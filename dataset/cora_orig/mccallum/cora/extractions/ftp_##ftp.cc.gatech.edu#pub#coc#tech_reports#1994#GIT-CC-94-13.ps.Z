URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1994/GIT-CC-94-13.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.94.html
Root-URL: 
Title: Avoiding Contention between Reads and Writes Using Dynamic Versioning  
Author: Sreenivas Gukal Edward Omiecinski Umakishore Ramachandran 
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Date: February 20, 1994  
Pubnum: GIT-CC-94/13  
Abstract: Abstract In this paper, we discuss a new approach to multi-version concur-rency control, called Dynamic Versioning, that avoids the data contention due to conflicts between Reads and Writes. A data item is allowed to have several committed versions and at most one uncommitted version. A conflict between a Read and a Write is resolved by imposing an order between the requesting transactions, and allowing the Read to access one of the committed versions. The space overhead is reduced to the minimum possible by making the versions dynamic; a version exists only as long as it may be accessed by an active transaction. Conditional lock compatibilities are used for providing serializable access to the multiple versions. The results from simulation studies indicate that the dynamic versioning method, with little space overhead (about 1% the size of the database), significantly reduces blocking (by 60% to 90%) compared to single-version two-phase locking. Lower blocking rates increase transaction throughput and reduce variance in transaction response times by better utilization of resources. This approach also reduces starvation of short transactions and subsumes previous methods proposed for supporting long-running queries. The dynamic versioning method can be easily incorporated into existing DBMS systems. The modifications required for the lock manager and the storage manager modules to implement dynamic versioning are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [Agar 87a] <author> Agrawal, R., Carey, M., Livny, M. </author> <title> Concurrency Control Performance Mod-elling: Alternatives and Implications, </title> <journal> ACM Transactions on Database Systems, </journal> <month> December </month> <year> 1987. </year>
Reference-contexts: The multi-version algorithm assumes that for a data item there is a single committed version, used by readers, and several uncommitted versions simultaneously being updated by the writers. This view is unrealistic since usually a writer has to first read a data item before deciding to update <ref> [Agar 87a] </ref>. Since, in such a case, all the writers would have read the committed version, only one of the concurrent writers to a data item can be allowed to commit to ensure serializability. <p> The percentage of updates is important since the main goal of DV is to reduce blocking by allowing a writer and several readers to work on an item simultaneously. Earlier simulation studies (e.g., <ref> [Agar 87a] </ref>, [Agar 87b], [Pein 83]) considered about 25% updates. Here we vary the percentage of updates from 10 to 50. 1 We also used a uniform distribution of +30% to -30% for the number of references. <p> The significant difference here is that we are comparing two similar algorithms (both variants of two-phase locking), with comparable number of aborts but one method with far lower blocking rates than the other. 4.1 Design of the Simulation Experiments in terms of resource units (similar to <ref> [Agar 87a] </ref> and [Agar 87b]). Each resource unit contains one CPU and two disks. The number of resource units is varied to change the resource contention. A transaction, on initiation, joins the CPU queue. Transactions from the CPU queue are assigned to an available CPU on a first-come-first-serve basis.
Reference: [Agar 87b] <author> Agrawal, R., Carey, M., McVoy, L. </author> <title> The Performance of Alternative Strategies for Dealing with Deadlocks in Database Management Systems, </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> December </month> <year> 1987. </year>
Reference-contexts: The percentage of updates is important since the main goal of DV is to reduce blocking by allowing a writer and several readers to work on an item simultaneously. Earlier simulation studies (e.g., [Agar 87a], <ref> [Agar 87b] </ref>, [Pein 83]) considered about 25% updates. Here we vary the percentage of updates from 10 to 50. 1 We also used a uniform distribution of +30% to -30% for the number of references. The results are similar to the case when the number of references is constant. <p> The significant difference here is that we are comparing two similar algorithms (both variants of two-phase locking), with comparable number of aborts but one method with far lower blocking rates than the other. 4.1 Design of the Simulation Experiments in terms of resource units (similar to [Agar 87a] and <ref> [Agar 87b] </ref>). Each resource unit contains one CPU and two disks. The number of resource units is varied to change the resource contention. A transaction, on initiation, joins the CPU queue. Transactions from the CPU queue are assigned to an available CPU on a first-come-first-serve basis.
Reference: [Baye 80] <author> Bayer, R., Heller, H., Reiser, A. </author> <title> Parallelism and Recovery in Database Systems, </title> <journal> ACM Transactions on Database Systems, </journal> <month> June </month> <year> 1980. </year>
Reference-contexts: We also describe how the storage manager can efficiently maintain the multiple versions in the database and provide indexed access to the versions. 1.1 Related Work A two version two-phase locking algorithm is proposed by Bayer et al. <ref> [Baye 80] </ref>. The algorithm is based on the observation that, in a shadowing environment, a copy of 1 the data item is made and the updates are done to the copy. Hence, to avoid blocking, readers can be allowed to read the before value. <p> A similar method [Stea 81], which uses time-stamp based deadlock prevention, has been proposed for distributed databases. Two-version and multi-version two-phase locking methods are also presented in [Bern 87]. The two-version method is similar to that in <ref> [Baye 80] </ref>. The multi-version algorithm assumes that for a data item there is a single committed version, used by readers, and several uncommitted versions simultaneously being updated by the writers.
Reference: [Bern 87] <author> Bernstein, P.A., Hadzilacos, V., Goodman, N. </author> <title> Concurrency Control and Recovery in Database Systems, </title> <publisher> Addison-Wesley Pub. Co., </publisher> <year> 1987. </year>
Reference-contexts: A similar method [Stea 81], which uses time-stamp based deadlock prevention, has been proposed for distributed databases. Two-version and multi-version two-phase locking methods are also presented in <ref> [Bern 87] </ref>. The two-version method is similar to that in [Baye 80]. The multi-version algorithm assumes that for a data item there is a single committed version, used by readers, and several uncommitted versions simultaneously being updated by the writers.
Reference: [Bobe 92] <author> Bober, P., Carey, M. </author> <title> On Mixing Queries and Transactions via Multiversion Locking, </title> <booktitle> Proc. 8th International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: The multiple versions of a data item here consist of several committed versions and one uncommitted version. Maintaining several committed versions allows updating transactions to commit as soon as they complete, instead of waiting for readers of earlier versions to end. Multi-versioning methods to support queries are proposed in <ref> [Bobe 92] </ref> and [Moha 92]. Both methods treat all queries in a similar manner, irrespective of the lengths of the queries or amount of data accessed. In Section 5, we show that in some cases the above two methods may result in maintaining a large number of unnecessary versions. <p> Each data item has one stable version in the previous version period and may have at most two additional versions in the current version period 4 . The system periodically switches to a new version period and purges the additional versions. Bober et al., <ref> [Bobe 92] </ref> use multi-version timestamp ordering to synchronize queries. Each update to a data item creates a new version.
Reference: [Dewi 92] <author> DeWitt, D., Gray, J. </author> <title> Parallel Database Systems: The Future of High Performance Database Systems, </title> <journal> Communications of the ACM, </journal> <month> June </month> <year> 1992. </year>
Reference-contexts: Multi-version algorithms incur the overhead of maintaining additional versions. Our approach reduces this overhead to the minimum possible by keeping only the versions necessary for avoiding aborts and providing serializability. One of the important concurrency control problems <ref> [Dewi 92] </ref> is to prevent the execution of queries (read-only transactions) from affecting the concurrent update transactions. Multi-versioning algorithms have been suggested as a way to solve this problem [Pira 90]. However, in large databases, a long-running query may require maintaining a large number of versions. <p> In DV, the increase in blocking rates is marginal (from 17 blocked transactions at 1 resource unit to 19 blocked transactions at 8 resource units). 5 Supporting Queries One of the important problems in concurrency control <ref> [Dewi 92] </ref> is to prevent the execution of queries (read-only transactions) from affecting concurrent update transactions (i.e., transactions which also update some data items). One solution is to violate serializability and run the queries at degree 2 or degree 1 isolation [Gray 76].
Reference: [Gray 93] <author> Gray, J., Reuter, A. </author> <title> Transaction Processing: Concepts and Techniques, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: This reduces the overhead of accessing the earlier versions. To reduce the space requirements, only the incremental difference of a version may be stored. Usually a database page contains some free space to accommodate growing tuples or for inserting new tuples <ref> [Gray 93] </ref>. This space may be used for holding the dynamic versions. In the worst case, when there is no free space in the page, a page split may be required. <p> This might result in rollbacks, since different transactions may acquire read-locks at the same time on an item and then decide to modify the item. To avoid this situation, the dynamic versioning method contains an additional lock mode called update-lock mode <ref> [Gray 93] </ref>. A transaction which might modify an item, first acquires an update-lock on the item. If the transaction decides to modify the item, it upgrades the update-lock to a write-lock, else it downgrades the update-lock to a read-lock. <p> Hence at low data contention or low percentage of updates (which means less chance of repeated conflicts), DV has lower number of restarts. At higher data contentions and higher percentage of updates, DV results in more restarts. Note that in an actual system aborts are rare <ref> [Gray 93] </ref>.
Reference: [Gray 76] <author> Gray, J., Lorie, R., Putzolu, F., Traiger, I. </author> <title> Granularity of Locks and Degrees of Consistency in a Shared Data Base, Modelling in Data Base Systems, </title> <publisher> North Holland Publishing, </publisher> <year> 1976. </year> <month> 19 </month>
Reference-contexts: One solution is to violate serializability and run the queries at degree 2 or degree 1 isolation <ref> [Gray 76] </ref>. The problem becomes interesting if the queries wish to see a transaction consistent database. Here, we consider only the problem of running queries at degree 3 isolation, which ensures repeatable reads.
Reference: [Guka 93] <author> Gukal, S., Omiecinski, E., Ramachandran, U. </author> <title> LU Logging: An Efficient Transaction Recovery Method, </title> <type> Technical Report, </type> <institution> Georgia Institute of Technology, </institution> <address> GIT-CC 93/21, </address> <month> March </month> <year> 1993. </year>
Reference: [Moha 93] <author> Mohan, C. </author> <title> A Survey of DBMS Research Issues in Supporting Very Large Tables, </title> <booktitle> Proc. 4th Int'l Conference on Foundations of Data Organization & Algorithms, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: An obvious way of implementing this technique would be to consider each table in the database as a distinct part, and order separately the queries and transactions working on each table. However, there are several applications where single tables can exceed 64 GB in size <ref> [Moha 93] </ref>. In such cases, each table can be further partitioned based on key ranges or general predicates 3 . Carrying the idea further, even predicate locks can be used to determine conflicts between queries and update transactions. The space overhead decreases as the size of the partitions is reduced.
Reference: [Moha 92] <author> Mohan, C., Pirahesh, H., Lorie, R. </author> <title> Efficient and Flexible Methods for Transient Versioning of Records to Avoid Locking by Read-Only Transactions, </title> <booktitle> ACM SIGMOD, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Maintaining several committed versions allows updating transactions to commit as soon as they complete, instead of waiting for readers of earlier versions to end. Multi-versioning methods to support queries are proposed in [Bobe 92] and <ref> [Moha 92] </ref>. Both methods treat all queries in a similar manner, irrespective of the lengths of the queries or amount of data accessed. In Section 5, we show that in some cases the above two methods may result in maintaining a large number of unnecessary versions. <p> The third technique is best suited for long-running queries accessing small and well-defined parts of the database. Several methods have been proposed in the literature to support long-running queries. Due to space limitations, we compare DV with only two recent methods, which also use multi-versioning schemes. Mohan et al., <ref> [Moha 92] </ref> propose a method 3 The partitions are not physical. They are used only to identify which parts of the database a query or a transaction accesses. 16 which uses version periods.
Reference: [Pein 83] <author> Peinl, P., Reuter, A. </author> <title> Empirical Comparison of Database Concurrency Control Schemes, </title> <booktitle> Proc. 9th International Conference on Very Large Data Bases, </booktitle> <year> 1983. </year>
Reference-contexts: They also do not discuss issues such as version maintenance and access, secondary indices for the versions and space management. A simulation study <ref> [Pein 83] </ref> of this algorithm concluded that it performs almost the same as single-version two-phase locking for most of the work loads. A similar method [Stea 81], which uses time-stamp based deadlock prevention, has been proposed for distributed databases. <p> The percentage of updates is important since the main goal of DV is to reduce blocking by allowing a writer and several readers to work on an item simultaneously. Earlier simulation studies (e.g., [Agar 87a], [Agar 87b], <ref> [Pein 83] </ref>) considered about 25% updates. Here we vary the percentage of updates from 10 to 50. 1 We also used a uniform distribution of +30% to -30% for the number of references. The results are similar to the case when the number of references is constant.
Reference: [Pira 90] <author> Pirahesh, H., Mohan, C., Cheng, J., Liu, T.S., Selinger, P. </author> <title> Parallelism in Relational Database Systems: </title> <booktitle> Architectural Issues and Design Approaches, IEEE 2nd Int'l Symp. on Databases in Parallel and Distributed Systems, </booktitle> <month> July </month> <year> 1990. </year>
Reference-contexts: One of the important concurrency control problems [Dewi 92] is to prevent the execution of queries (read-only transactions) from affecting the concurrent update transactions. Multi-versioning algorithms have been suggested as a way to solve this problem <ref> [Pira 90] </ref>. However, in large databases, a long-running query may require maintaining a large number of versions.
Reference: [Schw 90] <author> Schwetman, H. </author> <title> CSIM Users Guide, </title> <month> March </month> <year> 1990. </year>
Reference-contexts: These statistics allow us to compare the two methods along several metrics. CSIM <ref> [Schw 90] </ref>, a process-oriented discrete simulation package, is used for all the simulations. Each transaction is modelled as a separate CSIM process. The number of transactions in the system is determined by the degree of multiprogramming. A closed queuing simulation model is followed.
Reference: [Stea 81] <author> Stearns, R.E., Rosenkrantz, </author> <title> D.J. Distributed Database Concurrency Controls Using Before-Values, </title> <booktitle> ACM SIGMOD, </booktitle> <month> April </month> <year> 1981. </year>
Reference-contexts: They also do not discuss issues such as version maintenance and access, secondary indices for the versions and space management. A simulation study [Pein 83] of this algorithm concluded that it performs almost the same as single-version two-phase locking for most of the work loads. A similar method <ref> [Stea 81] </ref>, which uses time-stamp based deadlock prevention, has been proposed for distributed databases. Two-version and multi-version two-phase locking methods are also presented in [Bern 87]. The two-version method is similar to that in [Baye 80].
References-found: 15

