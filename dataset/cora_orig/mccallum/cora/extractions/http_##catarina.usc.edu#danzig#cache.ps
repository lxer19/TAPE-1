URL: http://catarina.usc.edu/danzig/cache.ps
Refering-URL: http://www-plateau.cs.berkeley.edu/papers/1995/138/paper-59.html
Root-URL: 
Title: A Hierarchical Internet Object Cache  
Author: Anawat Chankhunthod Peter B. Danzig Chuck Neerdaels Michael F. Schwartz Kurt J. Worrell 
Address: Boulder  
Affiliation: Computer Science Department University of Southern California  Computer Science Department University of Colorado  
Abstract: This paper discusses the design and performance of a hierarchical proxy-cache designed to make Internet information systems scale better. The design was motivated by our earlier trace-driven simulation study of Internet traffic. We challenge the conventional wisdom that the benefits of hierarchical file caching do not merit the costs, and believe the issue merits reconsideration in the Internet environment. The cache implementation supports a highly concurrent stream of requests. We present performance measurements that show that our cache outperforms other popular Internet cache implementations by an order of magnitude under concurrent load. These measurements indicate that hierarchy does not measurably increase access latency. Our software can also be configured as a Web-server accelerator; we present data that our httpd-accelerator is ten times faster than Netscape's Netsite and NCSA 1.4 servers. Finally, we relate our experience fitting the cache into the increasingly complex and operational world of Internet information systems, including issues related to security, transparency to cache-unaware clients, and the role of file systems in support of ubiquitous wide-area information systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rafael Alonso and Matthew Blaze. </author> <title> Long-term caching strategies for very large distributed file systems. </title> <booktitle> Proceedings of the USENIX Summer Conference, </booktitle> <pages> pages 3-16, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Bestavros also explored a mechanism for distributing popular documents based on server knowledge [3]. There have also been a number of simulation studies of caching in large environments. Using trace-driven simulations Alonso and Blaze showed that server load could be reduced by 60-90% <ref> [1, 2] </ref>. Muntz and Honeyman showed that a caching hierarchy does not help for typical UNIX workloads [17].
Reference: [2] <author> Rafael Alonso and Matthew Blaze. </author> <title> Dynamic hierarchical caching for large-scale distributed file systems. </title> <booktitle> Proceedings of the Twelvth International Conference on Distributed Computing Systems, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: In contrast, the hierarchical caching studies of Blaze and Alonso <ref> [2] </ref> and Muntz and Honeyman [17] showed that hierarchical caches can, at best, achieve 20% hit rates and cut file server workload in half. We believe the different conclusions reached by our study and these two file system studies lay in the workloads traced. <p> We believe the different conclusions reached by our study and these two file system studies lay in the workloads traced. Our study traced wide-area FTP traffic from a switch near the NSFNET backbone. In contrast, Blaze and Alonso <ref> [2] </ref> and Muntz and Honeyman [17] traced LAN workstation file system traffic. While workstation file systems share a large, relatively static collection of files, such as gcc, the Internet exhibits a high degree of read-only sharing among a rapidly evolving set of popular objects. <p> While workstation file systems share a large, relatively static collection of files, such as gcc, the Internet exhibits a high degree of read-only sharing among a rapidly evolving set of popular objects. Because LAN utility files rarely change over a five day period, both [17] and <ref> [2] </ref> studies found little value of hierarchical caching over flat file caches at each workstation: After the first reference to a shared file, the file stayed in the local cache indefinitely and the upper-level caches saw low hit rates. <p> Bestavros also explored a mechanism for distributing popular documents based on server knowledge [3]. There have also been a number of simulation studies of caching in large environments. Using trace-driven simulations Alonso and Blaze showed that server load could be reduced by 60-90% <ref> [1, 2] </ref>. Muntz and Honeyman showed that a caching hierarchy does not help for typical UNIX workloads [17].
Reference: [3] <author> Azer Bestavros. </author> <title> Demand-Based Document Dissemination for the World-Wide Web. </title> <institution> Computer Science Department, Boston University, </institution> <month> February </month> <year> 1995. </year> <note> Available from ftp://cs-ftp.bu.edu/techreports/95-003-webserver-dissemination.ps.Z. </note>
Reference-contexts: Their choice was motivated by their finding that cache performance could be improved by biasing the cache replacement policy in favor of more heavily shared local documents. Bestavros also explored a mechanism for distributing popular documents based on server knowledge <ref> [3] </ref>. There have also been a number of simulation studies of caching in large environments. Using trace-driven simulations Alonso and Blaze showed that server load could be reduced by 60-90% [1, 2]. Muntz and Honeyman showed that a caching hierarchy does not help for typical UNIX workloads [17].
Reference: [4] <author> Nathaniel Borenstein and Ned Freed. </author> <title> RFC 1521: MIME (Multipurpose Internet Mail Extensions) part one: Mechanisms for specifying and describing the format of Internet message bodies, </title> <month> September </month> <year> 1993. </year>
Reference-contexts: While no Web client is completely cache-aware, most support access through IP firewalls. Clients send all their 2 MIME stands for Multipurpose Internet Mail Extensions. It was originally developed for multimedia mail systems <ref> [4] </ref>, but was later adopted by HTTP for passing typing and other meta data between clients and servers. requests to their proxy-server, and the proxy-server decides how best to resolve it. There are advantages and disadvantages to the cache-aware and cache-unaware approaches.
Reference: [5] <author> C. Mic Bowman, Peter B. Danzig, Darren R. Hardy, Udi Manber, and Michael F. Schwartz. </author> <title> The Harvest information discovery and access system. </title> <booktitle> Proceedings of the Second International World Wide Web Conference, </booktitle> <pages> pages 763-771, </pages> <month> October </month> <year> 1994. </year> <note> Available from ftp://ftp.cs.colorado.edu/pub/cs/techreports /schwartz/Harvest.Conf.ps.Z. </note>
Reference-contexts: Beyond distributing load away from server hot spots, caching can also save bandwidth, reduce latency, and protect the network from clients that erroneously loop and generate repeated requests [9]. This paper describes the design and performance of the Harvest <ref> [5] </ref> cache, which we designed to make Internet information services scale better. The cache implementation is optimized to support a highly concurrent stream of requests with minimal queuing for OS-level resources, using non-blocking I/O, application-level threading and virtual memory management, and a Domain Naming System (DNS) cache.
Reference: [6] <author> Paul M. E. De Bra and Reiner D. J. Post. </author> <title> Information Retrieval in the World-Wide Web: Making client-based searching feasible. </title> <note> Available from http://www.win.tue.nl/win/cs/is/reinpost/www94 /www94.html. </note>
Reference-contexts: A few years ago, we demonstrated that FTP access patterns exhibit significant sharing and calculated that as early as 1992, 30-50% of NSFNET traffic was caused by repeated access to read-only FTP objects [10]. There have also been several network object cache implementations, including the CERN cache [15], Lagoon <ref> [6] </ref>, and the Netscape client cache. Netscape currently uses a 5 MB default disk cache at each client, which can improve client performance, but a single user might not have a high enough hit rate to affect network traffic substantially.
Reference: [7] <author> Hans-Werner Braun and Kimberly Claffy. </author> <title> Web traffic characterization: an assessment of the impact of caching documents from NCSA's web server. </title> <booktitle> In Second International World Wide Web Conference, </booktitle> <month> Octo-ber </month> <year> 1994. </year>
Reference-contexts: Claffy and Braun reported similar statistics for WWW traffic <ref> [7] </ref>, which has displaced FTP traffic as the largest contributor to Internet traffic. . . Second, the cost of a cache miss is much lower for Internet information systems than it is for traditional caching applications. <p> Sites that know their hit rate can use these measurements to evaluate the gain themselves. Alternatively, the reader can compute expected savings based on hit rates provided by earlier wide-area network traffic measurement studies <ref> [10, 7] </ref>. Figures 3 and 4 show the cumulative distribution of response times for hits and misses respectively. Figure 3 also reports both the median and average response times. Note that CERN's response time tail extends out to several seconds, so its average is three times its median.
Reference: [8] <author> Vincent Cate. </author> <title> Alex a global filesystem. </title> <booktitle> Proceedings of the Usenix File Systems Workshop, </booktitle> <pages> pages 1-11, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Harvest is unique among these systems in its support for a caching hierarchy, and in its high performance implementation. Its hierarchical approach distributes and reduces traffic, and the non-blocking/non-forking architecture provides greater scalability. It can be used to increase server performance, client performance, or both. Cate's Alex file system <ref> [8] </ref>, completed before the explosive growth of the Web, exports a cache of anonymous FTP space via an NFS interface. For performance, Alex caches IP addresses, keeps object meta-data in memory, and caches FTP connections to remote servers to stream fetches to multiple files.
Reference: [9] <author> Peter B. Danzig, Katia Obraczka, and Anant Kumar. </author> <title> An analysis of wide-area name server traffic: A study of the Domain Name System. </title> <booktitle> ACM SIGCOMM 92 Conference, </booktitle> <pages> pages 281-292, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Beyond distributing load away from server hot spots, caching can also save bandwidth, reduce latency, and protect the network from clients that erroneously loop and generate repeated requests <ref> [9] </ref>. This paper describes the design and performance of the Harvest [5] cache, which we designed to make Internet information services scale better.
Reference: [10] <author> Peter B. Danzig, Michael F. Schwartz, and Richard S. Hall. </author> <title> A case for caching file objects inside internet-works. </title> <booktitle> ACM SIGCOMM 93 Conference, </booktitle> <pages> pages 239-248, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Because of its high performance, the Harvest cache can also be paired with existing HTTP servers (httpd's) to increase document server throughput by an order of magnitude. Individual caches can be interconnected hierarchically to mirror an internetwork's topology, implementing the design motivated by our earlier NSFNET trace-driven simulation study <ref> [10] </ref>. 1.1 Hierarchical Web versus File System Caches Our 1993 study of Internet traffic showed that hierarchical caching of FTP files could eliminate half of all file transfers over the Internet's wide-area network links. [10]. <p> to mirror an internetwork's topology, implementing the design motivated by our earlier NSFNET trace-driven simulation study <ref> [10] </ref>. 1.1 Hierarchical Web versus File System Caches Our 1993 study of Internet traffic showed that hierarchical caching of FTP files could eliminate half of all file transfers over the Internet's wide-area network links. [10]. In contrast, the hierarchical caching studies of Blaze and Alonso [2] and Muntz and Honeyman [17] showed that hierarchical caches can, at best, achieve 20% hit rates and cut file server workload in half. <p> In contrast to workstation file systems, FTP, WWW, and Gopher facilitate read-only sharing of autonomously owned and rapidly evolving object spaces. Hence, we found that over half of NSFNET FTP traffic is due to sharing of read-only objects <ref> [10] </ref> and, since Internet topology tends to be organized hierarchically, that hierarchical caching can yield a 50% hit rate and reduce server load dramatically. Claffy and Braun reported similar statistics for WWW traffic [7], which has displaced FTP traffic as the largest contributor to Internet traffic. . . <p> Sites that know their hit rate can use these measurements to evaluate the gain themselves. Alternatively, the reader can compute expected savings based on hit rates provided by earlier wide-area network traffic measurement studies <ref> [10, 7] </ref>. Figures 3 and 4 show the cumulative distribution of response times for hits and misses respectively. Figure 3 also reports both the median and average response times. Note that CERN's response time tail extends out to several seconds, so its average is three times its median. <p> Muntz and Honeyman showed that a caching hierarchy does not help for typical UNIX workloads [17]. A few years ago, we demonstrated that FTP access patterns exhibit significant sharing and calculated that as early as 1992, 30-50% of NSFNET traffic was caused by repeated access to read-only FTP objects <ref> [10] </ref>. There have also been several network object cache implementations, including the CERN cache [15], Lagoon [6], and the Netscape client cache.
Reference: [11] <author> Bestavros et al. </author> <title> Application-level document caching in the Internet. </title> <booktitle> Workshop on Services in Distributed and Networked Environments, </booktitle> <month> Summer </month> <year> 1995. </year> <note> Available from ftp://cs-ftp.bu.edu/techreports/95-002-web-client-caching.ps.Z, </note> <month> January </month> <year> 1995. </year>
Reference-contexts: That approach has the advantage that the choice of what to cache and where to place copies can be made using the server's global knowledge of reference behavior. In contrast, Bestavros et al. <ref> [11] </ref> explored the idea of letting clients make the choice about what to cache, based on application-level knowledge such as user profiles and locally configured descriptions of organizational boundaries.
Reference: [12] <author> James Gwertzman and Margo Seltzer. </author> <title> The case for geographical push-caching. </title> <booktitle> HotOS Conference, </booktitle> <year> 1994. </year> <note> Available as ftp://das-ftp.harvard.edu/techreports/tr-34-94.ps.gz. </note>
Reference-contexts: Also, Harvest implements a more general caching interface, allowing objects to be cached using a variety of access protocols (FTP, Gopher, and HTTP), while AFS only caches using the single AFS access protocol. Gwertzman and Seltzer investigated a mechanism called geographical push caching <ref> [12] </ref>, in which the server chooses to replicate documents as a function of observed traffic patterns. That approach has the advantage that the choice of what to cache and where to place copies can be made using the server's global knowledge of reference behavior.
Reference: [13] <author> John Howard, Michael Kazar, Sherri Menees, David Nichols, M. Satyanarayanan, Robert Sidebotham, and Michael West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Get-if-modified fetches are propagated correctly through the hierarchy. We believe that Web caches should not implement hierarchical invalidation schemes, but that sites exporting time-critical data should simply set appropriate expiration times. We say this because hierarchical invalidation, like AFS's call-back invalidations <ref> [13] </ref>, requires state, and the Internet's basic unreliability and scale complicate stateful services. Below, we present data from two additional studies that support this position. <p> AFS provides a wide-area file system environment, supporting whole file caching <ref> [13] </ref>. Unlike the Harvest cache, AFS handles cache consistency using a server callback scheme that exhibits scaling problems in an environment where objects can be globally popular.
Reference: [14] <author> Bulter Lampson. </author> <title> Hints for computer system design. </title> <journal> Operating Systems Review, </journal> <volume> 17(5) </volume> <pages> 33-48, </pages> <month> Oct 10-13, </month> <year> 1983. </year>
Reference-contexts: Second, while the httpd servers process forms and queries, the accelerator makes the simpler, common case fast <ref> [14] </ref>. Finally, while it may not be practical or feasible to change the myriad httpd implementations to use the more efficient techniques advocated in this paper, sites can easily deploy the accelerator along with their existing httpd. <p> It also demonstrates that HTTP is not an inherently slow protocol, but rather that many popular implementations have ignored the sage advice to make the common case fast <ref> [14] </ref>. Hierarchical caching distributes load away from server hot spots raised by globally popular information objects, reduces access latency, and protects the network from erroneous clients. High performance is particularly important for higher levels in the cache hierarchy, which may experience heavy service request rates.
Reference: [15] <author> Ari Luotonen, Henrik Frystyk, and Tim Berners-Lee. </author> <title> CERN HTTPD public domain full-featured hypertext/proxy server with caching, </title> <note> 1994. Available from http://info.cern.ch/hypertext/WWW /Daemon/Status.html. </note>
Reference-contexts: The number of files per directory can be increased to decrease directory size and reduce file system overhead. 3 Performance We now compare the performance of the Harvest cache against the CERN proxy-http cache <ref> [15] </ref> and evaluate the Harvest cache's httpd-accelerator's performance gain over the Netscape Netsite, NCSA 1.4, and CERN 3.0 Web servers. <p> A few years ago, we demonstrated that FTP access patterns exhibit significant sharing and calculated that as early as 1992, 30-50% of NSFNET traffic was caused by repeated access to read-only FTP objects [10]. There have also been several network object cache implementations, including the CERN cache <ref> [15] </ref>, Lagoon [6], and the Netscape client cache. Netscape currently uses a 5 MB default disk cache at each client, which can improve client performance, but a single user might not have a high enough hit rate to affect network traffic substantially.
Reference: [16] <author> Paul Mockapetris. </author> <title> RFC 1035: Domain names implementation and specification. </title> <type> Technical report, </type> <institution> University of Southern California Information Sciences Institute, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: We restrict our discussion here to wide-area network caching efforts. One of the earliest efforts to support caching in a wide-area network environment was the Domain Naming System <ref> [16] </ref>. While not a general file or object cache, the DNS supports caching of name lookup results from server to server and also from client to server (although the widespread BIND resolver client library does not provide client caching), using timeouts for cache consistency.
Reference: [17] <author> D. Muntz and P. Honeyman. </author> <title> Multi-level caching in distributed file systems or- your cache ain't nuthin' but trash. </title> <booktitle> Proceedings of the USENIX Winter Conference, </booktitle> <pages> pages 305-313, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: In contrast, the hierarchical caching studies of Blaze and Alonso [2] and Muntz and Honeyman <ref> [17] </ref> showed that hierarchical caches can, at best, achieve 20% hit rates and cut file server workload in half. We believe the different conclusions reached by our study and these two file system studies lay in the workloads traced. <p> We believe the different conclusions reached by our study and these two file system studies lay in the workloads traced. Our study traced wide-area FTP traffic from a switch near the NSFNET backbone. In contrast, Blaze and Alonso [2] and Muntz and Honeyman <ref> [17] </ref> traced LAN workstation file system traffic. While workstation file systems share a large, relatively static collection of files, such as gcc, the Internet exhibits a high degree of read-only sharing among a rapidly evolving set of popular objects. <p> While workstation file systems share a large, relatively static collection of files, such as gcc, the Internet exhibits a high degree of read-only sharing among a rapidly evolving set of popular objects. Because LAN utility files rarely change over a five day period, both <ref> [17] </ref> and [2] studies found little value of hierarchical caching over flat file caches at each workstation: After the first reference to a shared file, the file stayed in the local cache indefinitely and the upper-level caches saw low hit rates. <p> There have also been a number of simulation studies of caching in large environments. Using trace-driven simulations Alonso and Blaze showed that server load could be reduced by 60-90% [1, 2]. Muntz and Honeyman showed that a caching hierarchy does not help for typical UNIX workloads <ref> [17] </ref>. A few years ago, we demonstrated that FTP access patterns exhibit significant sharing and calculated that as early as 1992, 30-50% of NSFNET traffic was caused by repeated access to read-only FTP objects [10].
Reference: [18] <author> Mirjana Spasojevic, Mic Bowman, and Alfred Spec-tor. </author> <title> Information Sharing Over a Wide-Area File System. </title> <institution> Transarc Corporation, </institution> <month> July </month> <year> 1994. </year> <note> Available from ftp://grand.central.org/darpa/arpa2/papers /usenix95.ps. </note>
Reference-contexts: So the question naturally arises, Why not just use a file system and dump all of this Web silliness? For example, Transarc proposes AFS as a replacement for HTTP <ref> [18] </ref>. AFS clearly provides better caching, replication, management, and security properties than the current Web does. Yet, it never reached the point of exponential growth that characterizes parts of the Internet infrastructure, as has been the case with TCP/IP, DNS, FTP, Gopher, WWW, and many other protocols and services.

References-found: 18

