URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-219.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: dpwe@media.mit.edu  
Title: Hierarchic models of hearing 1993mar15 1 HIERARCHIC MODELS OF HEARING FOR SOUND SEPARATION AND RECONSTRUCTION  
Author: Dan Ellis Daniel P W Ellis 
Address: E15-368C Cambridge MA 02139  
Affiliation: MIT Media Lab  
Pubnum: Perceptual Computing Section  
Abstract: In building a machine to detect and segregate individual components in sound mixtures, the best example to copy is the human auditory system. Several models of auditory organization implement various rules of psychoacoustic grouping [Breg90]; we propose in addition to model auditory inference as exhibited in the well-known phonemic restoration illusion of [Warr70]. A hierarchy of abstracted features and source hypotheses similar to [Nawa92] allows reconstruction of obliterated detail which can then be used to recreate an idealized sound without corruption. A preliminary example of fitting a harmonic model to a noisy recording of a clarinet gives a very convincing resynthesis with the interference totally removed. However, there are many issues including the design of the representation and the control architecture still to be addressed in building a more general system. 
Abstract-found: 1
Intro-found: 1
Reference: [Breg90] <editor> AS Bregman (1990) Auditory Scene Analysis, </editor> <publisher> MIT Press </publisher>
Reference-contexts: This model must duplicate listeners judgements of the number and duration of sources, and also construct output sounds that listeners can identify as the components of the mixture. Any such system must include the known principles of auditory grouping, by which acoustic energy is fused into single objects <ref> [Breg90] </ref>. A number of researchers have produced computer models that exploit important cues such as synchronized energy onset and harmonic frequency relations, with good results in the organization and separation of real sounds (notably [Cook91], [Mell91], [Brow92]).
Reference: [Brow92] <institution> GJ Brown (1992) Computational auditory scene analysis: A representational approach, </institution> <type> PhD thesis CS-92-22, </type> <institution> CS dept, Univ. of Sheffield </institution>
Reference-contexts: A number of researchers have produced computer models that exploit important cues such as synchronized energy onset and harmonic frequency relations, with good results in the organization and separation of real sounds (notably [Cook91], [Mell91], <ref> [Brow92] </ref>). However, these systems lack a model of inference in auditory processing - using higher-level patterns to fill in details not specifically detected by the periphery. As a result, any reconstructions will have characteristic energy holes in time-frequency where cancellations between the original sounds prevented information extraction.
Reference: [Carv92] <author> N Carver, </author> <title> V Lesser (1992) Blackboard systems for knowledge-based signal understanding, in Symbolic and knowledge-based signal processing, </title> <editor> ed. AV Oppenheim & SH Nawab, </editor> <publisher> Prentice Hall </publisher>
Reference: [Cook91] <institution> MP Cooke (1991) Modelling auditory processing and organisation, </institution> <type> PhD thesis, </type> <institution> CS dept, Univ, of Sheffield </institution>
Reference-contexts: A number of researchers have produced computer models that exploit important cues such as synchronized energy onset and harmonic frequency relations, with good results in the organization and separation of real sounds (notably <ref> [Cook91] </ref>, [Mell91], [Brow92]). However, these systems lack a model of inference in auditory processing - using higher-level patterns to fill in details not specifically detected by the periphery. As a result, any reconstructions will have characteristic energy holes in time-frequency where cancellations between the original sounds prevented information extraction.
Reference: [Elli91] <author> DPW Ellis, BL Vercoe, </author> <title> TF Quatieri (1991) A perceptual representation of audio for co-channel source separation, </title> <booktitle> IEEE Workshop on Apps. of Sig. Proc. to Audio and Acous. </booktitle>
Reference-contexts: include an example of the reconstruction of which we are currently capable, in which the system is able to completely remove interfering impulsive noise from a recording of clarinet while retaining the natural quality of the original instrument. 3.1 Front-end model This system is built upon our constant-Q sinewave model <ref> [Elli91] </ref>, [Elli92], itself based upon the Sinusoid Transform system [McAu86]. The sinewave model represents sound as the energy-maxima contours in the output of a constant-Q filterbank. Resynthesis by using each magnitude/frequency contour pair as control inputs to a sine-wave oscillator has very good perceptual identity with the original sound.
Reference: [Elli92] <institution> DPW Ellis (1992) A perceptual representation of audio, MS thesis, EECS dept, MIT </institution>
Reference-contexts: an example of the reconstruction of which we are currently capable, in which the system is able to completely remove interfering impulsive noise from a recording of clarinet while retaining the natural quality of the original instrument. 3.1 Front-end model This system is built upon our constant-Q sinewave model [Elli91], <ref> [Elli92] </ref>, itself based upon the Sinusoid Transform system [McAu86]. The sinewave model represents sound as the energy-maxima contours in the output of a constant-Q filterbank. Resynthesis by using each magnitude/frequency contour pair as control inputs to a sine-wave oscillator has very good perceptual identity with the original sound.
Reference: [McAu86] <author> RJ McAulay, </author> <title> TF Quatieri (1986) Speech analysis/synthesis based on a sinusoidal representation IEEE Tr. </title> <journal> ASSP 34 </journal>
Reference-contexts: are currently capable, in which the system is able to completely remove interfering impulsive noise from a recording of clarinet while retaining the natural quality of the original instrument. 3.1 Front-end model This system is built upon our constant-Q sinewave model [Elli91], [Elli92], itself based upon the Sinusoid Transform system <ref> [McAu86] </ref>. The sinewave model represents sound as the energy-maxima contours in the output of a constant-Q filterbank. Resynthesis by using each magnitude/frequency contour pair as control inputs to a sine-wave oscillator has very good perceptual identity with the original sound.
Reference: [Mell91] <institution> DK Mellinger (1991) Event formation and separation in musical sound, </institution> <type> PhD thesis, </type> <institution> CCRMA, Stanford Univ. </institution>
Reference-contexts: A number of researchers have produced computer models that exploit important cues such as synchronized energy onset and harmonic frequency relations, with good results in the organization and separation of real sounds (notably [Cook91], <ref> [Mell91] </ref>, [Brow92]). However, these systems lack a model of inference in auditory processing - using higher-level patterns to fill in details not specifically detected by the periphery. As a result, any reconstructions will have characteristic energy holes in time-frequency where cancellations between the original sounds prevented information extraction.
Reference: [Nawa92] <author> SH Nawab, </author> <title> V Lesser (1992) Integrated signal processing and understanding of signals, in Symbolic and knowledge-based signal processing, </title> <editor> ed. AV Oppenheim & SH Nawab, </editor> <publisher> Prentice Hall </publisher>
Reference-contexts: In section two we explain our understanding of auditory inference with reference to the phonemic restoration illusion, and outline a corresponding computer model. A hierarchy of abstract representations, similar to the Sound Understanding Testbed of <ref> [Nawa92] </ref> both explains the illusion and provides a restoratory mechanism. Resynthesis is a taxing ultimate goal for the system; while abstraction generally strips detail and categorizes many instances into the same model, we cannot discard too much information and still be able to regenerate something recognizable as the original. <p> We note that any such hierarchy is not likely to be neatly structured or layered. In particular, feedback from later stages can be important for the efficiency of early processing <ref> [Nawa92] </ref>. 2.2 An analogous computational model Our proposed computational approach to sound analysis is shown as a block diagram in figure one.
Reference: [Quat90] <author> TF Quatieri, </author> <title> RG Danisewicz (1990) An approach to co-channel talker interference suppression using a sinusoidal model for speech, </title> <journal> IEEE Tr. ASSP 38(1) </journal>
Reference-contexts: As a result, any reconstructions will have characteristic energy holes in time-frequency where cancellations between the original sounds prevented information extraction. Previous systems addressing this loss have been limited to specialized domains such as the method for separating voiced speech described in <ref> [Quat90] </ref> which reconstructed whole time frames by interpolating between neighbors. In section two we explain our understanding of auditory inference with reference to the phonemic restoration illusion, and outline a corresponding computer model.
Reference: [Repp91] <institution> BH Repp (1991) Perceptual restoration of a missing speech sound: Auditory induction or illusion? Haskins Lab. Status Rpt. on Sp. Rsrch. SR-107/108 </institution>
Reference-contexts: This is preconscious processing, since the listener genuinely hears the missing sound, and, indeed, has difficulty judging the timing of the noise burst within the speech. Further experiments indicate that the process is simple illusion rather than detailed subtraction in the masked region <ref> [Repp91] </ref>.

References-found: 11

