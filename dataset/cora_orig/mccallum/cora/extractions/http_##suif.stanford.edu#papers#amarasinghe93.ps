URL: http://suif.stanford.edu/papers/amarasinghe93.ps
Refering-URL: http://suif.stanford.edu/papers/papers.html
Root-URL: 
Title: poration. Communication Optimization and Code Generation for Distributed Memory Machines  
Author: Saman P. Amarasinghe and Monica S. Lam 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Note: This research was supported in part by DARPA contract DABT63-91-K-0003, an NSF Young Investigator Award and a fellowship from Intel Cor  
Abstract: This paper presents several algorithms to solve code generation and optimization problems specific to machines with distributed address spaces. Given a description of how the computation is to be partitioned across the processors in a machine, our algorithms produce an SPMD (single program multiple data) program to be run on each processor. Our compiler generates the necessary receive and send instructions, optimizes the communication by eliminating redundant communication and aggregating small messages into large messages, allocates space locally on each processor, and translates global data addresses to local addresses. Our techniques are based on an exact data-ow analysis on individual array element accesses. Unlike data dependence analysis, this analysis determines if two dynamic instances refer to the same value, and not just to the same location. Using this information, our compiler can handle more exible data decompositions and find more opportunities for communication optimization than systems based on data dependence analysis. Our technique is based on a uniform framework, where data decompositions, computation decompositions and the data ow information are all represented as systems of linear inequalities. We show that the problems of communication code generation, local memory management, message aggregation and redundant data communication elimination can all be solved by projecting polyhedra represented by sets of inequalities onto lower dimensional spaces. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ancourt and F. Irigoin. </author> <title> Scanning Polyhedra with DO Loops. </title> <booktitle> In Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP, </booktitle> <pages> pp. 39-50, </pages> <month> April </month> <year> 1991. </year>
Reference: [2] <author> M. Ancourt. </author> <title> Generation automatique de codes de transfert pour multiprocesseurs a memoires locales. </title> <type> PhD thesis, </type> <institution> University of Paris VI, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: It is essential that we eliminate the redundant messages and amortize the message sending overhead by batching the communication. 6.1 Eliminating Redundant Communication Ancourt has also studied the problem of eliminating redundant communication <ref> [2] </ref>. Given a set of iterations and accesses, Ancourts algorithm can construct a set of loop nests that fetches all the data touched without any duplication. This algorithm is adequate for removing redundant traffic if no communication is required within the loop nest.
Reference: [3] <author> J. M. Anderson and M. S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of the SIGPLAN 93 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: The input language to our compiler is sequential FORTRAN-77, although many of the techniques developed are also applicable to optimizations within and especially across FORTRAN-90 statements [14]. Our data and computation decomposition phase tries to maximize parallelism and minimize communication <ref> [3] </ref>. If all the available parallelism cannot be exploited without communication, this phase first tries to trade off excess degrees of parallelism to eliminate communication. The algorithm understands doacross parallelism, where processors are organized as a pipeline and they synchronize and communicate during the course of the computation. <p> Let us use the simple example in Figure 2 to illustrate our technique. for t = 0 to T do X [i] = X [i - 3] Data dependence analysis on this program will produce the dependence vectors -[+, 3], <ref> [0, 3] </ref>-, meaning that the read access in iteration may be data dependent on all iterations such that , and . <p> Computation decompositions can be written as where is an extended unimodular matrix, , are integer vectors, B is an integer matrix and is a vector of symbolic constants such that . In our compiler, computation decompositions are generated automatically by an earlier phase <ref> [3] </ref>. For systems that rely on the user to specify the data decompositions, Theorem 1 shows how to derive computation decompositions from data decompositions.
Reference: [4] <author> F. Andr, O. Chron and J.-L. Pazat. </author> <title> Compiling Sequential Programs for Distributed Memory Parallel Computers with Pandore II. </title> <booktitle> In Third Workshop on Compilers for Parallel Computers, </booktitle> <pages> pp. 231-242, </pages> <month> July </month> <year> 1992. </year>
Reference: [5] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic, </publisher> <year> 1988. </year>
Reference-contexts: This paper presents three main results. First, we propose a value-centric approach to deriving the necessary communication for machines with a distributed address space. Previous approaches are location-centric: communication is derived from data decompositions; optimizations are performed using data dependence <ref> [5] </ref>, an analysis that determines if accesses may refer to the same location. We derive our code generation from computation decompositions using a data-ow analysis technique that is based on values instead of locations. <p> A read operation is dependent on a write operation as long as they may refer to the same location, even if none of the written values are used by any of the read instances <ref> [5] </ref>. The lack of value information may reduce the opportunities for parallelism and communication optimization.
Reference: [6] <author> W. Blume and R. Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks Programs. </title> <journal> In IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, no. 6, </volume> <pages> pp. 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: This outer loop can be parallelized by giving each processor a private copy of the work array. This optimization, known as array privatization, has been shown to be essential to parallelize many real programs successfully <ref> [6] </ref>. Data dependence analysis does not distinguish between the different instances of the accesses. The analyzer only knows that a dependence exists; it does not know which pairs of instances are dependent. When using data dependence information for communication optimizations, the maximum depth of the dependence determines the communication interval.
Reference: [7] <author> M. Bromley, S. Heller, T. McNerney and G. L. Steele Jr. </author> <title> Fortran at Ten Gigaops: The Connection Machine Convolution Compiler. </title> <booktitle> In Proceedings of ACM Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 145-164, </pages> <month> June </month> <year> 1991. </year>
Reference: [8] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <booktitle> In Third Workshop on Compilers for Parallel Computers, </booktitle> <pages> pp. 121-160, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: These algorithms can also be useful to enhance the memory system performance of machines with a shared address space. Many systems being developed rely on the user to supply the data decompositions. Languages such as High Performance FORTRAN [24], FORTRAN-D [16] and Vienna FORTRAN <ref> [8] </ref> allow the programmer to annotate the sequential program with data decompositions. The compiler is responsible for generating the computation decomposition and an SPMD (Single Program Multiple Data) program with explicit communication. We are developing a compiler system that automatically parallel-izes a sequential program for shared and distributed memory machines.
Reference: [9] <author> C. Koelbel. </author> <title> Compile-time generation of regular communication patterns. </title> <booktitle> In Proceedings of Supercomputing 91, </booktitle> <pages> pp. 101-110, </pages> <month> November </month> <year> 1991. </year>
Reference: [10] <author> P. Feautrier, </author> <title> Array expansion. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pp. 429-442, </pages> <year> 1988. </year>
Reference: [11] <author> P. Feautrier, </author> <title> Parametric integer programming. </title> <type> Technical Report 209, </type> <institution> Laboratoire Methodologie and Architecture Des Systems Informatiques, </institution> <month> January </month> <year> 1988. </year>
Reference: [12] <author> P. Feautrier. </author> <title> Dataow analysis of array and scalar references. </title> <journal> In International Journal of Parallel Programming, </journal> <volume> 20(1) </volume> <pages> 23-52, </pages> <month> February </month> <year> 1991. </year>
Reference: [13] <author> G. Gannon, W. Jalby and K Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> In Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: However, there is a prevalent form of reuse that can be incorporated and exploited easily within our model and that is the set of uniformly generated references <ref> [13] </ref>. Array index functions of uniformly generated references are affine functions of loop indices and symbolic constants, and they differ only in the constant terms.
Reference: [14] <author> G. R. Gao, R. Olsen, V. Sarkar and R. Thekkath. </author> <title> Collective loop fusion for array contraction. </title> <booktitle> In Proceedings of the Fifth Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <pages> pp. 171-181, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: We are developing a compiler system that automatically parallel-izes a sequential program for shared and distributed memory machines. The input language to our compiler is sequential FORTRAN-77, although many of the techniques developed are also applicable to optimizations within and especially across FORTRAN-90 statements <ref> [14] </ref>. Our data and computation decomposition phase tries to maximize parallelism and minimize communication [3]. If all the available parallelism cannot be exploited without communication, this phase first tries to trade off excess degrees of parallelism to eliminate communication.
Reference: [15] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> In IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Thus, the maximum depth information is useful for reducing the communication frequency. All the data accessed within the interval requiring communication are summarized by a regular section description <ref> [15] </ref>. In this way, the same data used multiple times within the interval are only transferred once.
Reference: [16] <author> S. Hiranandani, K. Kennedy and C. Tseng. </author> <title> Compiling Fortran D for MIMD Distributed-Memory Machines. </title> <journal> In Communications of ACM, </journal> <volume> vol. 35, no. 8, </volume> <pages> pp. 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: These algorithms can also be useful to enhance the memory system performance of machines with a shared address space. Many systems being developed rely on the user to supply the data decompositions. Languages such as High Performance FORTRAN [24], FORTRAN-D <ref> [16] </ref> and Vienna FORTRAN [8] allow the programmer to annotate the sequential program with data decompositions. The compiler is responsible for generating the computation decomposition and an SPMD (Single Program Multiple Data) program with explicit communication.
Reference: [17] <author> L. G. C. Hamey, J. A. Webb and I. C. Wu. </author> <title> An architecture independent programming language for low-level vision. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 48, no. 2, </volume> <pages> pp. 246-264, </pages> <month> November </month> <year> 1989. </year>
Reference: [18] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Proceedings of Frontiers 90: The Third Symposium on the Frontiers of Massively Parallel Computation. </booktitle> <pages> pp. 424-432. </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Data reorganizations such as matrix transposes are implemented using collective communication routines <ref> [18] </ref>. The algorithms presented in this paper focus on generating the code and communication between reorganizations. Within each region of code requiring no major data reorganization, there may still be fine-grained communication. Sections of the arrays may be replicated and allocated to different processors at different times.
Reference: [19] <author> D. E. Maydan, S. P. Amarasinghe and M. S. Lam. </author> <title> Array Data-Flow Analysis and its Use in Array Privatization. </title> <booktitle> In Proceedings of ACM Conference on Principles of Programming Languages, </booktitle> <pages> pp. 2-15. </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Data dependence analysis, on the other hand, cannot discriminate between writes to the same locations. 3.2 Using LWTs in a Distributed Memory Compiler We have developed an array privatization algorithm based on the LWT analysis <ref> [19] </ref>. In this algorithm, parallelization is based on only the data-ow dependences generated by the LWT information. If the parallelized loop carries any anti-dependences or output dependences, then privatization is necessary. <p> The problem of identifying which written values are live at exit is a sub-problem in calculating last write trees <ref> [19] </ref>. The set of inequalities generated by this sub-problem, in conjunction with the final data distribution, defines the communication set for finalization. 5 Code Generation Before we describe how to generate the computation and communication code, we first review the techniques of projection and scanning a polyhedron in Section 5.1. <p> The second complication arises from the fact that a projected image may contains points that do not correspond to a solution to the original system. In many cases, a simple test can determine that no such degeneracies are present <ref> [19] </ref>. 6.1.2 Redundant Communication Due to Group Reuse Detection of reuse between arbitrary accesses to the same matrix can be expensive. However, there is a prevalent form of reuse that can be incorporated and exploited easily within our model and that is the set of uniformly generated references [13].
Reference: [20] <author> D. E. Maydan, </author> <title> Accurate Analysis of Array References. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> September </month> <year> 1992. </year> <note> Published as CSL-TR-92-547. </note>
Reference: [21] <author> P. Mehrotra and J. Van Rosendale. </author> <title> High Level Programming of Distributed Memory Architectures. </title> <editor> In A. Nicolau, D. Gelernter, T. Gross and D. Padua editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pp. 364-384, </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference: [22] <author> K. Pingali and A. Rogers. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN 89 Conference on Program Language Design and Implementation, </booktitle> <pages> pp. 69-80, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Third, we have developed several communication optimizations within the value-oriented framework. These optimizations include eliminating redundant messages, aggregating messages, and hiding the communication latency by overlapping the communication with computation. These optimizations are essential to achieving an acceptable performance on distributed memory machines <ref> [22] </ref>. The organization of our paper is as follows. In Section 2, we describe the conventional way of generating communication code from user-specified data decompositions and some of the limitations of the approach. In Section 3, we describe our value-centric approach to communication generation.
Reference: [23] <author> A. Schrijver, </author> <title> Theory of Linear and Integer Programming, </title> <publisher> Wiley, </publisher> <address> Chichester 1986. </address>
Reference-contexts: We know that must be even. However, this constraint is not captured in the projected polyhedron and can be an odd number in . Projection of an n-dimensional polyhedron onto an ( )-dimensional space can be achieved using a single step of Fourier-Motz-kin elimination <ref> [23] </ref>. Fourier-Motzkin elimination can produce a large number of superuous constraints. We can determine if a constraint is superuous as follows. We replace the constraint in question with its negation, and if the new system does not have an integer solution then the constraint is superuous. <p> To check if a system has an integer solution we again use Fourier-Motzkin elimination. Since the Fourier-Motzkin elimination algorithm checks if a real solution exists for a system, a branch-and-bound technique is needed to check for the existence of integer solutions <ref> [23] </ref>. We have extended the technique of projection to handle some simple non-linear inequalities so that we can handle symbolic block sizes. We allow the coefficients in the linear inequalities to be of the form where are integers and are symbolic constants.
Reference: [24] <author> G. L. Steele. </author> <title> Proposal for alignment and distribution directives in HPF. Draft presented at HPF Forum meeting, </title> <month> June </month> <year> 1992. </year>
Reference-contexts: These algorithms can also be useful to enhance the memory system performance of machines with a shared address space. Many systems being developed rely on the user to supply the data decompositions. Languages such as High Performance FORTRAN <ref> [24] </ref>, FORTRAN-D [16] and Vienna FORTRAN [8] allow the programmer to annotate the sequential program with data decompositions. The compiler is responsible for generating the computation decomposition and an SPMD (Single Program Multiple Data) program with explicit communication.
Reference: [25] <author> S. Tjiang, M. Wolf, M. Lam, K. Pieper and J. Hennessy. </author> <title> Integrating scalar optimizations and parallelization. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau and D. Padua editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pp. 137-151, </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1992. </year>
Reference-contexts: If the bounds of are independent of , the data sent to each processor are identical. 7 A Detailed Example We have implemented the communication optimization and code generation algorithms described in this paper in the Stanford SUIF compiler system <ref> [25] </ref>. However, these algorithms have not been fully integrated with all the other phases in the compiler.
Reference: [26] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year> <note> Published as Rice COMP TR93-199. </note>
Reference: [27] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <journal> In SIGPLAN Notices, </journal> <volume> vol. 26, no. 6, </volume> <pages> pp. 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference: [28] <author> M. E. Wolf. </author> <title> Improving locality and parallelism in nested loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> August </month> <year> 1992. </year> <note> Published as CSL-TR-92-538. </note>
References-found: 28

