URL: http://www.cs.wustl.edu/~gokhale/Papers/ieee_tc.ps.gz
Refering-URL: http://www.cs.wustl.edu/~gokhale/WWW/papers.html
Root-URL: 
Email: fgokhale,schmidtg@cs.wustl.edu  
Title: Evaluating CORBA Latency and Scalability Over High-Speed ATM Networks  
Author: Aniruddha S. Gokhale and Douglas C. Schmidt 
Keyword: Distributed object computing, corba communication middleware performance, high-speed networks.  
Address: St. Louis, MO 63130  
Affiliation: Department of Computer Science Washington University  
Abstract: This paper provides two contributions to the study of corba performance over high-speed networks. First, we measure the latency of various types and sizes of oneway and twoway client requests using a pair of widely used implementations of two C++ implementations of corba - Orbix 2.1 and VisiBroker 2.0. Second, we use Orbix and VisiBroker to measure the scalability of corba servers in terms of the number of objects they can support efficiently. These experiments extend our previous work on corba performance for bandwidth-sensitive applications (such as satellite surveillance, medical imaging, and teleconferencing). Our results show that the latency for corba implementations is relatively high and server scalability is relatively low. Our latency experiments show that non-optimized internal buffering and presentation layer conversion overhead in corba implementations can cause substantial delay variance, which is unacceptable in many real-time or constrained-latency applications. Likewise, our scalability experiments reveal that neither Orbix nor VisiBroker can handle a large number of objects in a single server process. The paper concludes by outlining optimizations we are developing to overcome the performance limitations with existing corba implementations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David D. Clark and David L. Tennenhouse, </author> <title> "Architectural Considerations for a New Generation of Protocols," </title> <booktitle> in Proceedings of the Symposium on Communications Architectures and Protocols (SIGCOMM), </booktitle> <address> Philadelphia, PA, </address> <month> Sept. </month> <year> 1990, </year> <booktitle> ACM, </booktitle> <pages> pp. 200-208. </pages>
Reference-contexts: Existing orb implementations suffer from excessive intra-orb function calls, as shown in Section IV-C. To minimize intra-orb function calls requires sophisticated compiler optimizations such as integrated layer processing <ref> [1] </ref>. * Dynamic invocation overhead. dii performance drops as the size of requests increases. To minimize the dynamic invocation overhead requires allowing reuse of requests and minimizing the marshaling and data copying overhead involved with populating the requests with their intended parameters. V. <p> Existing orbs are not optimized to reduce the overhead of data copies. In addition, these orbs suffer from excessive intra-orb function call overhead as shown in Section IV-C. In contrast, tao uses advanced compiler techniques (such as program flow analysis [20], [21] and integrated layer processing (ilp)) <ref> [1] </ref> to automatically omit unnecessary data copies between the corba infrastructure and applications. In addition, ilp reduces the overhead of excessive intra-orb function calls. Most importantly, this streamlining can be performed without requiring modifications to the standard corba specification. * Inefficient presentation layer conversions. <p> The variation between request algorithms revealed that the server-side did not cache any information. We plan to incorporate caching behavior in ourtao orb to improve latency. * Presentation Layer and Data Copying. The presentation layer is a major bottleneck in high-performance communication subsystems <ref> [1] </ref>. This layer transforms typed data objects from higher-level representations to lower-level representations (marshaling) and vice versa (de-marshaling). <p> Conventional layered protocol stacks and distributed object middleware lack the flexibility and efficiency required to meet the quality of service requirements of diverse applications running over high-speed networks. One proposed remedy for this problem is to use Application Level Framing (alf) <ref> [1] </ref>, [30], [31] and Integrated Layer Processing (ILP) [1], [32], [33]. alf ensures that lower layer protocols deal with data in units specified by the application. ilp provides the implementor with the option of performing all data manipulations in one or two integrated processing loops, rather than manipulating the data sequentially. <p> Conventional layered protocol stacks and distributed object middleware lack the flexibility and efficiency required to meet the quality of service requirements of diverse applications running over high-speed networks. One proposed remedy for this problem is to use Application Level Framing (alf) <ref> [1] </ref>, [30], [31] and Integrated Layer Processing (ILP) [1], [32], [33]. alf ensures that lower layer protocols deal with data in units specified by the application. ilp provides the implementor with the option of performing all data manipulations in one or two integrated processing loops, rather than manipulating the data sequentially. [34] have shown that although ilp reduces the
Reference: [2] <author> Object Management Group, </author> <title> The Common Object Request Broker: Architecture and Specification, </title> <address> 2.0 edition, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: In both rpc toolkits and corba, this transformation process is performed by client-side stubs and server-side skeletons that are generated by interface definition language (idl) compilers. idl compilers translate interfaces written in an idl (such as Sun rpc XDR [29], dce ndr, or corba cdr <ref> [2] </ref>) to other forms such as a network wire format. Eliminating the overhead of presentation layer conversions requires highly optimized stub compilers (e.g., Universal Stub Compiler [16]) and the Flick idl compiler [17].
Reference: [3] <institution> Object Management Group, </institution> <month> CORBAServices: </month> <title> Common Object Services Specification, </title> <note> Revised Edition, 95-3-31 edition, </note> <month> Mar. </month> <year> 1995. </year>
Reference: [4] <author> Irfan Pyarali, Timothy H. Harrison, and Douglas C. Schmidt, </author> <title> "Design and Performance of an Object-Oriented Framework for High-Performance Electronic Medical Imaging," </title> <booktitle> USENIX Computing Systems, </booktitle> <volume> vol. 9, no. 4, </volume> <month> November/December </month> <year> 1996. </year>
Reference: [5] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> "Measuring the Performance of Communication Middleware on High-Speed Networks," </title> <booktitle> in Proceedings of SIGCOMM '96, </booktitle> <address> Stanford, CA, </address> <month> August </month> <year> 1996, </year> <booktitle> ACM, </booktitle> <pages> pp. 306-317. </pages>
Reference-contexts: These figures reveal that as the sender buffer size increases the marshaling and data copying overhead also grows <ref> [5] </ref>, [6], thereby increasing latency. <p> VisiBroker Latency for Sending Structs Using Twoway DII 9 overhead of marshaling and demarshaling the BinStructs. These sources of overhead reduce the receiver's performance, thereby triggering the flow control mechanisms of the transport protocol, which impede the sender's progress. <ref> [5] </ref>, [6] precisely pinpoint the marshaling and data copying overheads when transferring richly-typed data using sii and dii. The latency for sending octets is significantly less than that for BinStructs due to significantly lower overhead of presentation layer conversions. <p> They also show that the socket layer overhead is more significant on the receiver side. [28] discusses the TCP NODELAY option, which allows tcp to send small packets as soon as possible to reduce latency. Earlier work [7], <ref> [5] </ref>, [6] using untyped data and typed data in a similar corba/atm testbed as the one in this paper reveal that the low-level C socket version and the C++ socket wrapper versions of ttcp are roughly equivalent for a given socket queue size. <p> The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [22]. Our earlier results <ref> [5] </ref>, [6] have presented detailed measurements of presentation layer overhead for transmitting richly-typed data. Our results for sending structs reveal that with increasing sender buffer sizes, the marshaling overhead increases, thereby increasing the latency.
Reference: [6] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> "The Performance of the CORBA Dynamic Invocation Interface and Dynamic Skeleton Interface over High-Speed ATM Networks," </title> <booktitle> in Proceedings of GLOBECOM '96, </booktitle> <address> London, England, </address> <month> November </month> <year> 1996, </year> <journal> IEEE, </journal> <pages> pp. 50-56. </pages>
Reference-contexts: These figures reveal that as the sender buffer size increases the marshaling and data copying overhead also grows [5], <ref> [6] </ref>, thereby increasing latency. <p> VisiBroker Latency for Sending Structs Using Twoway DII 9 overhead of marshaling and demarshaling the BinStructs. These sources of overhead reduce the receiver's performance, thereby triggering the flow control mechanisms of the transport protocol, which impede the sender's progress. [5], <ref> [6] </ref> precisely pinpoint the marshaling and data copying overheads when transferring richly-typed data using sii and dii. The latency for sending octets is significantly less than that for BinStructs due to significantly lower overhead of presentation layer conversions. <p> Most importantly, this streamlining can be performed without requiring modifications to the standard corba specification. * Inefficient presentation layer conversions. Existing orbs are not optimized to generate efficient stubs and skeletons. As a result, they incur excessive marshaling and demarshaling overhead ([5], <ref> [6] </ref> and this paper in Figures 13 Fig. 21. Demultiplexing Strategies 17 and 18). In contrast, tao produces and configures multiple encoding/decoding strategies for corba interface definition language (idl) descriptions. <p> They also show that the socket layer overhead is more significant on the receiver side. [28] discusses the TCP NODELAY option, which allows tcp to send small packets as soon as possible to reduce latency. Earlier work [7], [5], <ref> [6] </ref> using untyped data and typed data in a similar corba/atm testbed as the one in this paper reveal that the low-level C socket version and the C++ socket wrapper versions of ttcp are roughly equivalent for a given socket queue size. <p> The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [22]. Our earlier results [5], <ref> [6] </ref> have presented detailed measurements of presentation layer overhead for transmitting richly-typed data. Our results for sending structs reveal that with increasing sender buffer sizes, the marshaling overhead increases, thereby increasing the latency.
Reference: [7] <author> Douglas C. Schmidt, Timothy H. Harrison, and Ehab Al-Shaer, </author> <title> "Object-Oriented Components for High-speed Network Programming," </title> <booktitle> in Proceedings of the 1 st Conference on Object-Oriented Technologies and Systems, </booktitle> <address> Monterey, CA, June 1995, </address> <publisher> USENIX. </publisher>
Reference-contexts: They show that increasing the socket buffer sizes improves the ipc performance. They also show that the socket layer overhead is more significant on the receiver side. [28] discusses the TCP NODELAY option, which allows tcp to send small packets as soon as possible to reduce latency. Earlier work <ref> [7] </ref>, [5], [6] using untyped data and typed data in a similar corba/atm testbed as the one in this paper reveal that the low-level C socket version and the C++ socket wrapper versions of ttcp are roughly equivalent for a given socket queue size.
Reference: [8] <author> Douglas C. Schmidt, David Levine, and Timothy H. Harri-son, </author> <title> "An ORB Endsystem Architecture for Hard Real-Time Scheduling," </title> <month> Feb. </month> <year> 1997, </year> <note> Submitted to OMG in response to RFI ORBOS/96-09-02. </note>
Reference-contexts: Figure 19 depicts the optimizations we are employing to eliminate the bottlenecks with existing orbs identified in Section III. These optimizations are being integrated into a high-performance, real-time orb called tao [18], <ref> [8] </ref>. The research issue we are addressing in tao is how to provide end-to-end quality of service guarantees to corba-compliant applications and services.
Reference: [9] <author> Kenneth Birman and Robbert van Renesse, </author> <title> "RPC Considered Inadequate," </title> <booktitle> in Reliable Distributed Computing with the Isis Toolkit, </booktitle> <pages> pp. 68-78. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <year> 1994. </year>
Reference: [10] <author> USNA, TTCP: </author> <title> a test of TCP and UDP Performance, </title> <month> Dec </month> <year> 1984. </year>
Reference: [11] <author> Sudheer Dharnikota, Kurt Maly, and C. M. Overstreet, </author> <title> "Performance Evaluation of TCP(UDP)/IP over ATM networks," </title> <institution> Department of Computer Science, </institution> <type> Technical Report CSTR 94 23, </type> <institution> Old Dominion University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: In general, less attention has been paid to integrating the following topics related to communication middleware: * Transport Protocol Performance over ATM Networks. The underlying transport protocols used by the orb must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. <ref> [11] </ref>, [12], [13] present results on performance of tcp/ip (and udp/ip [11]) on atm networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size). <p> The underlying transport protocols used by the orb must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. <ref> [11] </ref>, [12], [13] present results on performance of tcp/ip (and udp/ip [11]) on atm networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size). <p> This work indicates that in addition to the host architecture and host network interface, parameters configurable in software (like tcp window size, socket queue size, and user data size) significantly affect tcp throughput. <ref> [11] </ref> shows that udp performs better than tcp over atm networks, which is attributed to redundant tcp processing overhead on highly-reliable atm links. [11] also describes techniques to tune tcp to be a less bulky protocol so that its performance can be comparable to udp. <p> to the host architecture and host network interface, parameters configurable in software (like tcp window size, socket queue size, and user data size) significantly affect tcp throughput. <ref> [11] </ref> shows that udp performs better than tcp over atm networks, which is attributed to redundant tcp processing overhead on highly-reliable atm links. [11] also describes techniques to tune tcp to be a less bulky protocol so that its performance can be comparable to udp.
Reference: [12] <author> Minh DoVan, Louis Humphrey, Geri Cox, and Carl Ravin, </author> <title> "Initial Experience with Asynchronous Transfer Mode for Use in a Medical Imaging Network," </title> <journal> Journal of Digital Imaging, </journal> <volume> vol. 8, no. 1, </volume> <pages> pp. 43-48, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: In general, less attention has been paid to integrating the following topics related to communication middleware: * Transport Protocol Performance over ATM Networks. The underlying transport protocols used by the orb must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. [11], <ref> [12] </ref>, [13] present results on performance of tcp/ip (and udp/ip [11]) on atm networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size).
Reference: [13] <author> K. Modeklev, E. Klovning, and O. Kure, </author> <title> "TCP/IP Behavior in a High-Speed Local ATM Network Environment," </title> <booktitle> in Pro ceedings of the 19 th Conference on Local Computer Networks, </booktitle> <address> Minneapolis, MN, Oct. 1994, </address> <publisher> IEEE, </publisher> <pages> pp. 176-185. </pages>
Reference-contexts: The underlying transport protocols used by the orb must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. [11], [12], <ref> [13] </ref> present results on performance of tcp/ip (and udp/ip [11]) on atm networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size).
Reference: [14] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> "Evaluating the Performance of Demultiplexing Strategies for Real-time CORBA," </title> <note> in Submitted to GLOBECOM '97, , Phoenix, AZ, </note> <month> November </month> <year> 1997, </year> <note> IEEE. </note>
Reference-contexts: Existing orbs utilize inefficient and inflexible demultiplexing strategies based on layered demultiplexing as explained in Section IV-C and shown in Figure 21 (A). In contrast, tao utilizes active delayered demultiplexing and explicit dynamic linking <ref> [14] </ref> shown in Figure 21 (C), which makes it possible to adapt and configure optimal strategies for dispatching client requests within orb endsystems and corba bridges. * Excessive data copying and intra-orb calls. Existing orbs are not optimized to reduce the overhead of data copies. <p> Layered multiplexing and demultiplexing is generally disparaged for high-performance communication systems [35] due to the additional overhead incurred at each layer.[15] describes a fast and flexible message de-multiplexing strategy based on dynamic code generation. <ref> [14] </ref> evaluates the performance of alternative demultiplex-ing strategies for real-time corba. Our results for latency measurements have shown that with increasing number of objects, the latency increases. This is partly due to the additional overhead of demulti-plexing the request to the appropriate method of the appropriate object. <p> Our results for latency measurements have shown that with increasing number of objects, the latency increases. This is partly due to the additional overhead of demulti-plexing the request to the appropriate method of the appropriate object. We propose to use a delayered demultiplex-ing architecture <ref> [14] </ref> that can select optimal demultiplexing strategies based on compile-time and run-time analysis of corba idl interfaces. VII. Concluding Remarks An important class of applications (such as avionics, distributed interactive simulation, and telecommunication systems) require scalable, low latency communication.
Reference: [15] <author> Dawson R. Engler and M. Frans Kaashoek, "DPF: </author> <title> Fast, Flexible Message Demultiplexing using Dynamic Code Generation," </title> <booktitle> in Proceedings of ACM SIGCOMM '96 Conference in Computer Communication Review, </booktitle> <institution> Stanford University, California, USA, </institution> <month> August </month> <year> 1996, </year> <pages> pp. 53-59, </pages> <publisher> ACM Press. </publisher>
Reference-contexts: Much of the receiver-side overhead occurs from inefficient demultiplexing and presentation layer conversions (particularly for passing richly-typed data (e.g., structs). Eliminating the demultiplex-ing overhead requires delayered strategies and fast, flexible message demultiplexing <ref> [15] </ref>. Eliminating the presentation layer overhead requires optimized stub generators [16], [17] for richly-typed data. * Demultiplexing overhead. The Orbix demultiplex-ing performs worse than VisiBroker demultiplexing since Orbix uses a linear search strategy based on string comparisons for operation demultiplexing.
Reference: [16] <author> Sean W. O'Malley, Todd A. Proebsting, and Allen B. Montz, </author> <title> "USC: A Universal Stub Compiler," </title> <booktitle> in Proceedings of the Symposium on Communications Architectures and Protocols (SIG-COMM), </booktitle> <address> London, UK, </address> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: Much of the receiver-side overhead occurs from inefficient demultiplexing and presentation layer conversions (particularly for passing richly-typed data (e.g., structs). Eliminating the demultiplex-ing overhead requires delayered strategies and fast, flexible message demultiplexing [15]. Eliminating the presentation layer overhead requires optimized stub generators <ref> [16] </ref>, [17] for richly-typed data. * Demultiplexing overhead. The Orbix demultiplex-ing performs worse than VisiBroker demultiplexing since Orbix uses a linear search strategy based on string comparisons for operation demultiplexing. <p> Eliminating the overhead of presentation layer conversions requires highly optimized stub compilers (e.g., Universal Stub Compiler <ref> [16] </ref>) and the Flick idl compiler [17]. The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [22].
Reference: [17] <author> Eric Eide, Kevin Frei, Bryan Ford, Jay Lepreau, and Gary Lind-strom, "Flick: </author> <title> A Flexible, Optimizing IDL Compiler," </title> <booktitle> in Proceedings of ACM SIGPLAN '97 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <address> Las Vegas, NV, </address> <month> June </month> <year> 1997, </year> <note> ACM. </note>
Reference-contexts: Much of the receiver-side overhead occurs from inefficient demultiplexing and presentation layer conversions (particularly for passing richly-typed data (e.g., structs). Eliminating the demultiplex-ing overhead requires delayered strategies and fast, flexible message demultiplexing [15]. Eliminating the presentation layer overhead requires optimized stub generators [16], <ref> [17] </ref> for richly-typed data. * Demultiplexing overhead. The Orbix demultiplex-ing performs worse than VisiBroker demultiplexing since Orbix uses a linear search strategy based on string comparisons for operation demultiplexing. <p> Eliminating the overhead of presentation layer conversions requires highly optimized stub compilers (e.g., Universal Stub Compiler [16]) and the Flick idl compiler <ref> [17] </ref>. The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [22]. Our earlier results [5], [6] have presented detailed measurements of presentation layer overhead for transmitting richly-typed data.
Reference: [18] <author> Douglas C. Schmidt, Aniruddha Gokhale, Tim Harrison, and Guru Parulkar, </author> <title> "A High-Performance Endsystem Architecture for Real-time CORBA," </title> <journal> IEEE Communications Magazine, </journal> <volume> vol. 14, no. 2, </volume> <month> February </month> <year> 1997. </year>
Reference-contexts: Figure 19 depicts the optimizations we are employing to eliminate the bottlenecks with existing orbs identified in Section III. These optimizations are being integrated into a high-performance, real-time orb called tao <ref> [18] </ref>, [8]. The research issue we are addressing in tao is how to provide end-to-end quality of service guarantees to corba-compliant applications and services.
Reference: [19] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> "Optimizing the Performance of the CORBA Internet Inter-ORB Protocol Over ATM," </title> <institution> in Washington University Technical Report #WUCS-97-10, </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: TAO IIOP ORB Core The central focus of our tao orb effort is a portable and feature-rich corba kernel that implements the standard corba Internet Inter-Operability Protocol (iiop) as shown in Figure 20. Our iiop kernel is based on a highly optimized implementation of SunSoft's implementation of the iiop <ref> [19] </ref>. <p> This is attributed to the additional overhead of presentation layer conversions. We are currently working towards eliminating presentation layer overhead by using measurement and principle driven optimizations <ref> [19] </ref>. Our optimizations are based on principles such as eliminating waste, precomputing and storing to avoid unnecessary repetitive computation, and optimizing for the common case. In general, our latency experiments indicate that corba implementations have not been optimized to support low-latency quality of service.
Reference: [20] <author> Jong-Deok Choi, Ron Cytron, and Jeanne Ferrante, </author> <title> "Automatic Construction of Sparse Data Flow Evaluation Graphs," </title> <booktitle> in Conference Record of the Eighteenth Annual ACE Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1991. </year>
Reference-contexts: Existing orbs are not optimized to reduce the overhead of data copies. In addition, these orbs suffer from excessive intra-orb function call overhead as shown in Section IV-C. In contrast, tao uses advanced compiler techniques (such as program flow analysis <ref> [20] </ref>, [21] and integrated layer processing (ilp)) [1] to automatically omit unnecessary data copies between the corba infrastructure and applications. In addition, ilp reduces the overhead of excessive intra-orb function calls.
Reference: [21] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Weg-man, and F. Kenneth Zadeck, </author> <title> "Efficiently Computing Static Single Assignment Form and the Control Dependence Graph," </title> <journal> in ACM Transactions on Programming Languages and Systems. ACM, </journal> <month> October </month> <year> 1991. </year>
Reference-contexts: Existing orbs are not optimized to reduce the overhead of data copies. In addition, these orbs suffer from excessive intra-orb function call overhead as shown in Section IV-C. In contrast, tao uses advanced compiler techniques (such as program flow analysis [20], <ref> [21] </ref> and integrated layer processing (ilp)) [1] to automatically omit unnecessary data copies between the corba infrastructure and applications. In addition, ilp reduces the overhead of excessive intra-orb function calls. Most importantly, this streamlining can be performed without requiring modifications to the standard corba specification. * Inefficient presentation layer conversions.
Reference: [22] <author> Phillip Hoschka and Christian Huitema, </author> <title> "Automatic Generation of Optimized Code for Marshalling Routines," </title> <booktitle> in IFIP Conference of Upper Layer Protocols, Architectures and Applications ULPAA'94, </booktitle> <address> Barcelona, Spain, 1994, </address> <publisher> IFIP. </publisher>
Reference-contexts: Demultiplexing Strategies 17 and 18). In contrast, tao produces and configures multiple encoding/decoding strategies for corba interface definition language (idl) descriptions. Each strategy can be configured for different time/space tradeoffs between compiled vs. interpreted corba idl stubs and skeletons <ref> [22] </ref>, and the application's use of parameters (e.g., pass-without-touching, read-only, mutable). * Non-optimized buffering algorithms used for network reads and writes. Existing orbs utilize non-optimized internal buffers for writing to and reading from the network, as shown in Section IV-C. <p> The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) <ref> [22] </ref>. Our earlier results [5], [6] have presented detailed measurements of presentation layer overhead for transmitting richly-typed data. Our results for sending structs reveal that with increasing sender buffer sizes, the marshaling overhead increases, thereby increasing the latency.
Reference: [23] <author> R. Gopalakrishnan and G. Parulkar, </author> <title> "A Real-time Upcall Facil ity for Protocol Processing with QoS Guarantees," </title> <booktitle> in 15 th Symposium on Operating System Principles (poster session), </booktitle> <address> Copper Mountain Resort, Boulder, CO, </address> <month> Dec. </month> <year> 1995, </year> <note> ACM. </note>
Reference-contexts: In contrast, tao utilizes optimal buffer choices to reduce this overhead; We are currently implementing tao within a prototype real-time os developed at Washington University. This real-time os is characterized by features that include (1) a Real-Time Upcall (rtu) scheduling mechanism <ref> [23] </ref>, which achieves end-to-end real-time scheduling and (2) the apic [24] atm/host network interface, which provides a zero-copy mechanism that eliminates the excessive data copying overhead.
Reference: [24] <author> Zubin D. Dittia, Jr. Jerome R. Cox, and Guru M. Parulkar, </author> <title> "Design of the APIC: A High Performance ATM Host-Network Interface Chip," </title> <booktitle> in IEEE INFOCOM '95, </booktitle> <address> Boston, USA, April 1995, </address> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 179-187. </pages>
Reference-contexts: This real-time os is characterized by features that include (1) a Real-Time Upcall (rtu) scheduling mechanism [23], which achieves end-to-end real-time scheduling and (2) the apic <ref> [24] </ref> atm/host network interface, which provides a zero-copy mechanism that eliminates the excessive data copying overhead.
Reference: [25] <author> Douglas C. Schmidt and Tatsuya Suda, </author> <title> "An Object-Oriented Framework for Dynamically Configuring Extensible Distributed Communication Systems," </title> <journal> IEE/BCS Distributed Systems Engineering Journal (Special Issue on Configurable Distributed Systems), </journal> <volume> vol. 2, </volume> <pages> pp. 280-293, </pages> <month> December </month> <year> 1994. </year> <month> 16 </month>
Reference-contexts: This real-time os is characterized by features that include (1) a Real-Time Upcall (rtu) scheduling mechanism [23], which achieves end-to-end real-time scheduling and (2) the apic [24] atm/host network interface, which provides a zero-copy mechanism that eliminates the excessive data copying overhead. In addition, the ace framework <ref> [25] </ref> is used to implement the tao orb. ace contains flexible, reusable, efficient, and portable object-oriented components that automate common orb communication tasks involving event demultiplexing, event handler dispatching, connection establishment, routing, dynamic configuration of orb services, and concurrency control. VI. <p> We are currently implementing a lightweight orb called tao that eliminates these overheads. The source code for the various tests performed in this paper is made available through the ace <ref> [25] </ref> software distribution at www.cs.wustl.edu/~schmidt/ACE.html. Acknowledgments We like to thank iona and Visigenic for their help in supplying the corba implementations used for these tests. Both companies are currently working to eliminate the latency overhead and scalability limitations described in this paper.
Reference: [26] <author> Jonathan Kay and Joseph Pasquale, </author> <title> "The Importance of Non-Data Touching Processing Overheads in TCP/IP," </title> <booktitle> in Proceedings of SIGCOMM '93, </booktitle> <address> San Francisco, CA, </address> <month> September </month> <year> 1993, </year> <booktitle> ACM, </booktitle> <pages> pp. 259-269. </pages>
Reference-contexts: They also show that the tcp delay characteristics are predictable and that it varies with the throughput. <ref> [26] </ref> present detailed measurements of various categories of processing overhead times of tcp/ip and udp/ip.
Reference: [27] <author> Christos Papadopoulos and Gurudatta Parulkar, </author> <title> "Experimental Evaluation of SUNOS IPC and TCP/IP Protocol Implementation," </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 199-216, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The authors show that most messages sent are short (less than 200 bytes). They claim that these overheads are hard to eliminate and techniques such as Integrated Layer Processing can be used to reduce the overhead. <ref> [27] </ref> present performance results of the sunos ipc and tcp/ip implementations. They show that increasing the socket buffer sizes improves the ipc performance.
Reference: [28] <editor> S. J. Le*er, M.K. McKusick, M.J. Karels, and J.S. </editor> <title> Quarterman, The Design and Implementation of the 4.3BSD UNIX Operating System, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: They show that increasing the socket buffer sizes improves the ipc performance. They also show that the socket layer overhead is more significant on the receiver side. <ref> [28] </ref> discusses the TCP NODELAY option, which allows tcp to send small packets as soon as possible to reduce latency.
Reference: [29] <author> Sun Microsystems, "XDR: </author> <title> External Data Representation Standard," Network Information Center RFC 1014, </title> <month> June </month> <year> 1987. </year>
Reference-contexts: In both rpc toolkits and corba, this transformation process is performed by client-side stubs and server-side skeletons that are generated by interface definition language (idl) compilers. idl compilers translate interfaces written in an idl (such as Sun rpc XDR <ref> [29] </ref>, dce ndr, or corba cdr [2]) to other forms such as a network wire format. Eliminating the overhead of presentation layer conversions requires highly optimized stub compilers (e.g., Universal Stub Compiler [16]) and the Flick idl compiler [17].
Reference: [30] <author> Isabelle Chrisment, </author> <title> "Impact of ALF on Communication Subsystems Design and Performance," </title> <booktitle> in First International Workshop on High Performance Protocol Architectures, HIPPARCH '94, </booktitle> <institution> Sophia Antipolis, France, </institution> <month> December </month> <year> 1994, </year> <institution> INRIA France. </institution>
Reference-contexts: Conventional layered protocol stacks and distributed object middleware lack the flexibility and efficiency required to meet the quality of service requirements of diverse applications running over high-speed networks. One proposed remedy for this problem is to use Application Level Framing (alf) [1], <ref> [30] </ref>, [31] and Integrated Layer Processing (ILP) [1], [32], [33]. alf ensures that lower layer protocols deal with data in units specified by the application. ilp provides the implementor with the option of performing all data manipulations in one or two integrated processing loops, rather than manipulating the data sequentially. [34]
Reference: [31] <author> Atanu Ghosh, Jon Crowcroft, Michael Fry, and Mark Hand-ley, </author> <title> "Integrated Layer Video Decoding and Application Layer Framed Secure Login: General Lessons from Two or Three Very Different Applications," </title> <booktitle> in First International Workshop on High Performance Protocol Architectures, HIPPARCH '94, </booktitle> <institution> Sophia Antipolis, France, </institution> <month> December </month> <year> 1994, </year> <institution> INRIA France. </institution>
Reference-contexts: Conventional layered protocol stacks and distributed object middleware lack the flexibility and efficiency required to meet the quality of service requirements of diverse applications running over high-speed networks. One proposed remedy for this problem is to use Application Level Framing (alf) [1], [30], <ref> [31] </ref> and Integrated Layer Processing (ILP) [1], [32], [33]. alf ensures that lower layer protocols deal with data in units specified by the application. ilp provides the implementor with the option of performing all data manipulations in one or two integrated processing loops, rather than manipulating the data sequentially. [34] have
Reference: [32] <author> M. Abbott and L. Peterson, </author> <title> "Increasing Network Throughput by Integrating Protocol Layers," </title> <journal> ACM Transactions on Networking, </journal> <volume> vol. 1, no. 5, </volume> <month> October </month> <year> 1993. </year>
Reference-contexts: One proposed remedy for this problem is to use Application Level Framing (alf) [1], [30], [31] and Integrated Layer Processing (ILP) [1], <ref> [32] </ref>, [33]. alf ensures that lower layer protocols deal with data in units specified by the application. ilp provides the implementor with the option of performing all data manipulations in one or two integrated processing loops, rather than manipulating the data sequentially. [34] have shown that although ilp reduces the number
Reference: [33] <author> Antony Richards, Ranil De Silva, Anne Fladenmuller, Aruna Seneviratne, and Michael Fry, </author> <title> "The Application of ILP/ALF to Configurable Protocols," </title> <booktitle> in First International Workshop on High Performance Protocol Architectures, HIPPARCH '94, </booktitle> <institution> Sophia Antipolis, France, </institution> <month> December </month> <year> 1994, </year> <institution> INRIA France. </institution>
Reference-contexts: One proposed remedy for this problem is to use Application Level Framing (alf) [1], [30], [31] and Integrated Layer Processing (ILP) [1], [32], <ref> [33] </ref>. alf ensures that lower layer protocols deal with data in units specified by the application. ilp provides the implementor with the option of performing all data manipulations in one or two integrated processing loops, rather than manipulating the data sequentially. [34] have shown that although ilp reduces the number of
Reference: [34] <author> Torsten Braun and Christophe Diot, </author> <title> "Protocol Implementation Using Integrated Layer Processnig," </title> <booktitle> in Proceedings of the Symposium on Communications Architectures and Protocols (SIG-COMM). ACM, </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: [30], [31] and Integrated Layer Processing (ILP) [1], [32], [33]. alf ensures that lower layer protocols deal with data in units specified by the application. ilp provides the implementor with the option of performing all data manipulations in one or two integrated processing loops, rather than manipulating the data sequentially. <ref> [34] </ref> have shown that although ilp reduces the number of memory accesses, it does not reduce the number of cache misses compared to a carefully designed non-ilp implementation. A major limitation of ilp described in [34] is its applicability to only non-ordering constrained protocol functions and its uses of macros that <p> data manipulations in one or two integrated processing loops, rather than manipulating the data sequentially. <ref> [34] </ref> have shown that although ilp reduces the number of memory accesses, it does not reduce the number of cache misses compared to a carefully designed non-ilp implementation. A major limitation of ilp described in [34] is its applicability to only non-ordering constrained protocol functions and its uses of macros that restrict the protocol implementation from being dynamically adapted to changing requirements.
Reference: [35] <author> David L. Tennenhouse, </author> <title> "Layered Multiplexing Considered Harmful," </title> <booktitle> in Proceedings of the 1 st International Workshop on High-Speed Networks, </booktitle> <month> May </month> <year> 1989. </year>
Reference-contexts: In addition, conventional corba implementations utilize several extra levels of demultiplexing at the application layer to associate incoming client requests with the appropriate object implementation and method (as shown in Figure 3). Layered multiplexing and demultiplexing is generally disparaged for high-performance communication systems <ref> [35] </ref> due to the additional overhead incurred at each layer.[15] describes a fast and flexible message de-multiplexing strategy based on dynamic code generation. [14] evaluates the performance of alternative demultiplex-ing strategies for real-time corba. Our results for latency measurements have shown that with increasing number of objects, the latency increases.
References-found: 35

