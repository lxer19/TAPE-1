URL: http://www.cs.toronto.edu/~ftp/pub/reports/na/cs-91-255.ps.Z
Refering-URL: http://www.cs.toronto.edu/NA/reports.html
Root-URL: 
Title: The Parallel Solution of ABD Systems Arising in Numerical Methods for BVPs for ODEs  
Author: K. R. Jackson and R. N. Pancer K. R. Jackson and R. N. Pancer 
Note: c Copyright  This work was supported in part by the Natural  
Address: Toronto, Ontario, Canada M5S 1A4  Ontario.  
Affiliation: Computer Science Department University of Toronto  Sciences and Engineering Research Council of Canada and the Information Technology Research Centre of  
Date: 255/91 May 1992.  May 1992.  
Pubnum: Technical Report No.  
Abstract-found: 0
Intro-found: 1
Reference: [Asch 89] <author> U.M. Ascher and P.S.Y. Chan, </author> <title> On parallel methods for boundary value ODEs, </title> <institution> University of British Columbia, Department of Computer Science, </institution> <type> Technical Report 89-19, </type> <year> 1989. </year>
Reference-contexts: Lentini [Lent 89] suggests performing Gaussian elimination with pivoting simultaneously from both ends of the matrix. This leads to a stable algorithm utilizing two processors, but it cannot be generalized for greater parallelism. Finally, Ascher and Chan <ref> [Asch 89] </ref> propose solving the ABD system by first forming the normal equations. The resulting system is symmetric, positive-definite, block-tridiagonal and can be solved stably in O (log M ) block-steps using block cyclic reduction [Hell 76]. <p> This results in a compacted system of order P that can be solved using cyclic reduction as shown above. Using this technique, O (log M ) complexity is attainable with as few as P = M= log M processors. See <ref> [Asch 89] </ref> for more details. Unfortunately, none of these algorithms is appropriate for solving ABD systems arising from BVPs. As pointed out in [Asch 89], they are equivalent to compactification|an algorithm that is known to be unstable because it fails to decouple the fast increasing and decreasing fundamental solution modes. <p> Using this technique, O (log M ) complexity is attainable with as few as P = M= log M processors. See <ref> [Asch 89] </ref> for more details. Unfortunately, none of these algorithms is appropriate for solving ABD systems arising from BVPs. As pointed out in [Asch 89], they are equivalent to compactification|an algorithm that is known to be unstable because it fails to decouple the fast increasing and decreasing fundamental solution modes. <p> In order to be competitive with SLF the transformation cannot be expensive, and it should not have an adverse effect on the condition of the system (c.f. the normal equations method in <ref> [Asch 89] </ref>). The rescaling algorithm described below is based on such a transformation. <p> All of the problems considered in this section are linear|each is of the form y 0 (t) = A (t)y (t) + q (t); t 2 [a; b]; where y; q 2 R n and A 2 R nfin . Problem 1 (Ascher and Chan <ref> [Asch 89] </ref>) a = 0; b = 1; n = 2; A (t) = cos 2!t ! + sin 2!t # " e t ; subject to the boundary conditions y 1 (0) = 1; y 1 (1) = e. <p> Of particular interest in the multiple shooting table is the ABD arising from shooting over M = 16 intervals. This system is so poorly conditioned that Ascher and Chan's normal equations method <ref> [Asch 89] </ref> breaks down.
Reference: [Asch 88] <author> U.M. Ascher, R.M.M. Mattheij, and R.D. Russell, </author> <title> Numerical Solution of Boundary Value Problems for Ordinary Differential Equations, </title> <publisher> Prentice Hall, </publisher> <address> Engle-wood Cliffs, </address> <year> 1988. </year>
Reference-contexts: There are several other issues that need be addressed in the numerical solution of BVPs, including selecting an appropriate mesh, choosing a specific formula for the residual function, and determining a convergence criteria for (5). See <ref> [Asch 88] </ref> for details. Much parallelism is inherent in the approach outlined above. For example, the residual components of (Y (q) ) can be evaluated independently, and the block-pairs [S i ; T i ] of J (q) can be constructed independently each time J (q) is re-evaluated. <p> 2 j] = 6 6 6 6 6 B a B b g V 1 I 1 . . . . . . 3 7 7 7 7 5 This second form of the Jacobian arises naturally, for example, in multiple shooting and codes based on implicit Runge-Kutta formulas (see <ref> [Asch 88] </ref>). <p> However, the kinematic eigenvalues are 3:3583 and 6:3583, so this ODE has an exponential dichotomy and the problem is well-posed. (See <ref> [Asch 88] </ref> for a discussion of these concepts.) 32 Problem #4 ; discretization: midpoint finite difference M = 32 M = 64 M = 256 M = 1024 K (full jac.) .2D+03 .1D+02 .2D+02 .7D+02 BlkCR .60D-13 (.17D-01) .32D-03 (.17D-02) .17D+00 (.17D+00) .31D-01 (.31D-01) SLF -QR .46D-13 (.17D-01) .36D-13 (.14D-02) .32D-13
Reference: [Asch 81] <author> U.M. Ascher, J. Christiansen, and R.D. Russell, </author> <title> Collocation software for boundary-value ODEs, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 7 (1981), </volume> <pages> pp. 209-222. </pages>
Reference-contexts: In codes that do not use condensation the resulting ABD system can be much larger, and its factorization and solution an even bigger bottleneck. In COLSYS <ref> [Asch 81] </ref>, for example, the ABD factorization often accounts for more than 50% of the total execution time [Muir 91]. These statistics emphasize the importance of developing a parallel ABD solver.
Reference: [Bade 87] <author> G. Bader and U.M. Ascher, </author> <title> A new basis implementation for a mixed order boundary value ODE solver, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 8 (1987), </volume> <pages> pp. 483-500. </pages>
Reference-contexts: For example, the residual components of (Y (q) ) can be evaluated independently, and the block-pairs [S i ; T i ] of J (q) can be constructed independently each time J (q) is re-evaluated. In [Benn 90], a parallel version of COLNEW <ref> [Bade 87] </ref> is implemented and the speed-up achievable by parallelizing the Jacobian set-up phase is investigated. A high percentage (60-80%) of the total execution time in COLNEW is spent during set-up where large blocks are "condensed" to form (6).
Reference: [Benn 90] <author> K.R. Bennett and G. Fairweather, PCOLNEW: </author> <title> A parallel boundary-value ODE code for shared-memory machines, </title> <institution> University of Kentucky, Center for Computational Sciences, </institution> <type> Technical Report CCS-90-8, </type> <year> 1990. </year> <month> 33 </month>
Reference-contexts: Much parallelism is inherent in the approach outlined above. For example, the residual components of (Y (q) ) can be evaluated independently, and the block-pairs [S i ; T i ] of J (q) can be constructed independently each time J (q) is re-evaluated. In <ref> [Benn 90] </ref>, a parallel version of COLNEW [Bade 87] is implemented and the speed-up achievable by parallelizing the Jacobian set-up phase is investigated. A high percentage (60-80%) of the total execution time in COLNEW is spent during set-up where large blocks are "condensed" to form (6). <p> A high percentage (60-80%) of the total execution time in COLNEW is spent during set-up where large blocks are "condensed" to form (6). Condensing in parallel, therefore, is very effective and in fact shifts the most time-consuming phase of the code: run-time profiling in <ref> [Benn 90] </ref> shows that the factorization and solution of the condensed ABD system becomes the bottleneck in the parallel implementation. In codes that do not use condensation the resulting ABD system can be much larger, and its factorization and solution an even bigger bottleneck.
Reference: [deBo 80] <author> C. de Boor and R. Weiss, </author> <title> SOLVEBLOK: A package for solving almost block diagonal linear systems, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 6 (1980), </volume> <pages> pp. 80-87. </pages>
Reference-contexts: These statistics emphasize the importance of developing a parallel ABD solver. Efficient sequential codes for solving ABD systems have been available for several years| two examples are SOLVEBLOK <ref> [deBo 80] </ref> and COLROW [Diaz 83]. Both codes perform the factorization in O (M n 3 ) time. SOLVEBLOK eliminates by rows and COLROW uses a variation of alternate row and column elimination to avoid fill-in. Stability is achieved by a pivoting strategy which controls element growth during the factorization. <p> 91a] * Structured QR ! Wright's structured orthogonal factorization algorithm, * Structured LU ! Wright's structured Gaussian elimination algorithm, * ROWPP ! a plain row partial-pivoting code, * DECOMP/SOLVE ! routines from the PASVA codes [Lent 77], and two sequential codes designed for problems with separated boundary conditions only: SOLVEBLOK <ref> [deBo 80] </ref> and COLROW [Diaz 83]. Structured QR and Structured LU are similar to SLF -QR and SLF -GE/rpp, respectively.
Reference: [Diaz 83] <author> J.C. Diaz, G. Fairweather, and P. Keast, COLROW and ARCECO: </author> <title> Fortran packages for solving certain block diagonal linear systems by modified alternate row and column elimination, </title> <journal> ACM Trans. Math. Software, </journal> <volume> 9 (1983), </volume> <pages> pp. 358-377. </pages>
Reference-contexts: These statistics emphasize the importance of developing a parallel ABD solver. Efficient sequential codes for solving ABD systems have been available for several years| two examples are SOLVEBLOK [deBo 80] and COLROW <ref> [Diaz 83] </ref>. Both codes perform the factorization in O (M n 3 ) time. SOLVEBLOK eliminates by rows and COLROW uses a variation of alternate row and column elimination to avoid fill-in. Stability is achieved by a pivoting strategy which controls element growth during the factorization. <p> ! Wright's structured orthogonal factorization algorithm, * Structured LU ! Wright's structured Gaussian elimination algorithm, * ROWPP ! a plain row partial-pivoting code, * DECOMP/SOLVE ! routines from the PASVA codes [Lent 77], and two sequential codes designed for problems with separated boundary conditions only: SOLVEBLOK [deBo 80] and COLROW <ref> [Diaz 83] </ref>. Structured QR and Structured LU are similar to SLF -QR and SLF -GE/rpp, respectively.
Reference: [Gear 88] <author> C.W. Gear, </author> <title> Massive parallelism across the method in ODEs, </title> <institution> University of Illinois, Computer Science Department, </institution> <type> Technical Report UIUCDCS-R-88-1442, </type> <year> 1988. </year>
Reference-contexts: This recurrence is stable and can be computed in parallel using cyclic reduction ([Sche 84]). In fact, this is precisely the recurrence that arises when solving a stable linear IVP. As noted in <ref> [Gear 88] </ref>, fast methods for the solution of linear IVPs can be constructed from fast algorithms for solving (28). Now, if the ABD system (8) could be transformed and the equations recast as (28), the analysis in [Sche 84] and [Gear 88] would apply and block cyclic reduction could be used <p> As noted in <ref> [Gear 88] </ref>, fast methods for the solution of linear IVPs can be constructed from fast algorithms for solving (28). Now, if the ABD system (8) could be transformed and the equations recast as (28), the analysis in [Sche 84] and [Gear 88] would apply and block cyclic reduction could be used on this problem. In order to be competitive with SLF the transformation cannot be expensive, and it should not have an adverse effect on the condition of the system (c.f. the normal equations method in [Asch 89]).
Reference: [Golu 83] <author> G. Golub and C. van Loan, </author> <title> Matrix Computations, </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1983. </year>
Reference-contexts: block-row of (7) through by T 1 i (providing T 1 i exists and is not too badly conditioned)|a process which is obviously highly parallel. 2 Block Cyclic Reduction Cyclic reduction was originally proposed as a stable sequential algorithm for solving symmetric positive-definite tridiagonal linear systems ([Hock 65], [Hock 70], <ref> [Golu 83] </ref>). It does, however, have considerable inherent parallelism and can easily be adapted to solve ABD systems. To this end, consider the second form of the Jacobian (8).
Reference: [Hell 76] <author> D. Heller, </author> <title> Some aspects of the cyclic reduction algorithm for block tridiagonal linear systems, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 13 (1976), </volume> <pages> pp. 484-496. </pages>
Reference-contexts: Finally, Ascher and Chan [Asch 89] propose solving the ABD system by first forming the normal equations. The resulting system is symmetric, positive-definite, block-tridiagonal and can be solved stably in O (log M ) block-steps using block cyclic reduction <ref> [Hell 76] </ref>. The drawback here, of course, is that by forming the normal equations explicitly the 2-norm condition number of the original system is squared. Wright was the first to present stable parallel algorithms that attain the theoretically optimal speed-up for this problem.
Reference: [Hock 65] <author> R. Hockney, </author> <title> A Fast Direct Solution of Poisson's Equation Using Fourier Analysis, </title> <journal> J. ACM, </journal> <volume> 12 (1965), </volume> <pages> pp. 95-113. </pages>
Reference: [Hock 70] <author> R. Hockney, </author> <title> The Potential Calculation and Some Applications, </title> <journal> Meth. Comput. Phys., </journal> <volume> 9 (1970), </volume> <pages> pp. 135-211. </pages>
Reference-contexts: the i-th block-row of (7) through by T 1 i (providing T 1 i exists and is not too badly conditioned)|a process which is obviously highly parallel. 2 Block Cyclic Reduction Cyclic reduction was originally proposed as a stable sequential algorithm for solving symmetric positive-definite tridiagonal linear systems ([Hock 65], <ref> [Hock 70] </ref>, [Golu 83]). It does, however, have considerable inherent parallelism and can easily be adapted to solve ABD systems. To this end, consider the second form of the Jacobian (8).
Reference: [Lent 77] <author> M. Lentini and V. Pereyra, </author> <title> An adaptive finite difference solver for nonlinear two-point boundary value problems with mild boundary layers, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 14 (1977), </volume> <pages> pp. 91-111. </pages>
Reference-contexts: to fl-shaped Jacobian, along with those of four algorithms discussed in [Wrig 90b] and [Wrig 91a] * Structured QR ! Wright's structured orthogonal factorization algorithm, * Structured LU ! Wright's structured Gaussian elimination algorithm, * ROWPP ! a plain row partial-pivoting code, * DECOMP/SOLVE ! routines from the PASVA codes <ref> [Lent 77] </ref>, and two sequential codes designed for problems with separated boundary conditions only: SOLVEBLOK [deBo 80] and COLROW [Diaz 83]. Structured QR and Structured LU are similar to SLF -QR and SLF -GE/rpp, respectively.
Reference: [Lent 89] <author> M. Lentini, </author> <title> Parallel solution of special large block tridiagonal systems: </title> <journal> TPBVP, </journal> <note> manuscript (1989). </note>
Reference-contexts: In addition, the authors found that even when a problem could be solved stably with this method, the speed-up achieved was less than expected. Lentini <ref> [Lent 89] </ref> suggests performing Gaussian elimination with pivoting simultaneously from both ends of the matrix. This leads to a stable algorithm utilizing two processors, but it cannot be generalized for greater parallelism. Finally, Ascher and Chan [Asch 89] propose solving the ABD system by first forming the normal equations.
Reference: [Matt 85] <author> R.M.M. Mattheij, </author> <title> Decoupling and stability of algorithms for boundary value problems, </title> <journal> SIAM Review, </journal> <volume> 27 (1985), </volume> <pages> pp. 1-44. </pages>
Reference-contexts: (.23D+10) .37D+11 (.37D+11) .48D+11 (.48D+11) SLF -QR .48D-13 (.59D-09) .99D-13 (.20D-09) .14D-13 (.25D-09) .21D-13 (.16D-09) SLF -GE/rpp .18D-13 (.59D-09) .25D-13 (.20D-09) .89D-14 (.25D-09) .71D-14 (.16D-09) GSC -RS =+1 .13D-12 (.59D-09) .83D-13 (.20D-09) .37D-12 (.25D-09) .60D-12 (.16D-09) GSC -RS-fl =+1 .12D-12 (.59D-09) .20D-12 (.20D-09) .44D-12 (.25D-09) .17D-11 (.16D-09) Problem 2B (Mattheij <ref> [Matt 85] </ref>) Same as Problem 2A, but with coupled boundary conditions y 1 (0) = 1 y 2 (0) + y 2 () = 1 + e : Since the linear system is not banded for this problem, DGBCO cannot be used for the condition number estimate and algebraic error.
Reference: [Muir 91] <author> P. Muir, </author> <title> Private communication. </title>
Reference-contexts: In codes that do not use condensation the resulting ABD system can be much larger, and its factorization and solution an even bigger bottleneck. In COLSYS [Asch 81], for example, the ABD factorization often accounts for more than 50% of the total execution time <ref> [Muir 91] </ref>. These statistics emphasize the importance of developing a parallel ABD solver. Efficient sequential codes for solving ABD systems have been available for several years| two examples are SOLVEBLOK [deBo 80] and COLROW [Diaz 83]. Both codes perform the factorization in O (M n 3 ) time.
Reference: [Panc 92] <author> R.N. Pancer, </author> <title> The parallel solution of ABD systems arising in numerical methods for BVPs for ODEs, </title> <type> Ph.D. thesis, </type> <institution> University of Toronto, </institution> <note> (in progress). </note>
Reference-contexts: As pointed out in [Wrig 91b], SLF-LU when implemented as described in x3.3 is potentially unstable on certain problems when the partitions become large. Whether or not cyclic reduction is a more stable alternative for these problems will be investigated in <ref> [Panc 92] </ref>. 4 Global Stability Control As shown in the previous section, SLF-transformations can be used with partitioning and cyclic reduction to provide a stable algorithm for solving ABD systems. With as few as M= log M processors, a parallel complexity of O (log M ) block-steps is attainable. <p> At the time of writing, the only ABDs known to require this dynamic shift are themselves very poorly conditioned. These issues will be investigated further in <ref> [Panc 92] </ref>. 16 The total flop-count for this algorithm|including operations required for the transfor- mation, cyclic reduction, back-solve and mesh recovery (30)|is approximately 1=2 that of SLF-LU. <p> The discrepancy in operation counts between Structured LU and SLF -GE/rpp can be attributed to a slightly different implementation. The details of the implementation are discussed in <ref> [Panc 92] </ref>. If P M processors are available the operation counts may be reduced by overlapping some of the computation. <p> However, this preprocessing stage may not be necessary as preliminary numerical experiments have shown GSC -RS can be applied directly to (7). This will be investigated further in <ref> [Panc 92] </ref>. 24 Table 1: Operation counts and storage requirements for various ABD solvers Algorithm Operation Count Storage SLF -QR ( M 3 n 3 + (15R + 30)n 2 ] 4M n 2 M 3 n 3 + (15R + 30)n 2 ] + log P [ 46 +3n 3
Reference: [Papr 89] <author> M. Paprzycki and I. Gladwell, </author> <title> Solving almost block diagonal systems on parallel computers, </title> <note> SMU Mathematics Department Technical Report 89-18, </note> <year> 1989. </year>
Reference-contexts: Wright and Pereyra [Wrig 90a] present a block factorization algorithm which is essentially equivalent to compactification|an algorithm known to be potentially unstable ([Asch 88, page 153]). They propose using the parallel algorithm initially, and when instability is detected, switching to a more stable sequential method. Paprzycki and Gladwell <ref> [Papr 89] </ref> describe a "tearing" algorithm in which the original ABD is partitioned into several smaller matrices, each of which is ABD. These represent sub-BVPs which can be solved independently, after which the solutions are combined.
Reference: [Sche 84] <editor> U. Schendel, </editor> <title> Introduction to Numerical Methods for Parallel Computers, </title> <publisher> Ellis Horwood, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: As noted in [Gear 88], fast methods for the solution of linear IVPs can be constructed from fast algorithms for solving (28). Now, if the ABD system (8) could be transformed and the equations recast as (28), the analysis in <ref> [Sche 84] </ref> and [Gear 88] would apply and block cyclic reduction could be used on this problem.
Reference: [Wrig 90a] <author> S.J. Wright and V. Pereyra, </author> <title> Adaptation of a two-point boundary value problem solver to a vector-multiprocessor environment, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11 (1990), </volume> <pages> pp. 425-449. </pages>
Reference-contexts: Surpressing the dependence on n, therefore, the theoretically best parallel complexity is O (log M ) block-steps. Most parallel algorithms proposed recently, however, either do not achieve the optimal speed-up or suffer from poor stability properties. Wright and Pereyra <ref> [Wrig 90a] </ref> present a block factorization algorithm which is essentially equivalent to compactification|an algorithm known to be potentially unstable ([Asch 88, page 153]). They propose using the parallel algorithm initially, and when instability is detected, switching to a more stable sequential method.
Reference: [Wrig 90b] <author> S.J. Wright, </author> <title> Stable parallel algorithms for two-point boundary value problems, </title> <type> Preprint MCS-P178-0990, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory (1990). </institution> <note> To appear in SIAM J. </note> <institution> Sci. Stat. Comput. </institution> <year> (1992). </year> <month> 34 </month>
Reference-contexts: The drawback here, of course, is that by forming the normal equations explicitly the 2-norm condition number of the original system is squared. Wright was the first to present stable parallel algorithms that attain the theoretically optimal speed-up for this problem. In <ref> [Wrig 90b] </ref> a "structured orthogonal factorization" is described which|when imbedded in a cyclic reduction algorithm|solves the ABD system stably in O (log M ) block-steps with as few as M= log M processors. <p> Stability is assured since the algorithm is essentially a QR-factorization applied to a row and column permuted version of the original ABD. In [Wrig 91a] the local orthogonal transformations used in <ref> [Wrig 90b] </ref> are replaced by Gauss transformations with row partial-pivoting resulting in a substantial speed-up even though the number of block-steps is the same. <p> In x3 the block multiplications used in the unmodified algorithm are replaced by Stable Local Factorization (SLF ) transformations. These transformations give rise to stable parallel algorithms for solving ABD systems|a thorough stability analysis is given in <ref> [Wrig 90b] </ref> and [Wrig 91a]. In addition, the algorithms are extended to increase parallelism in the back-solve stage by making better use of available 3 processors during the decomposition stage. <p> These Stable Local Factorization (SLF ) transformations give rise to stable parallel algorithms for solving ABD systems. Each avoids the instability inherent in (14) by controlling the growth of elements during the reduction. One uses orthogonal factorization, the other Gaussian elimination with row partial-pivoting. Their stability is discussed in <ref> [Wrig 90b] </ref> and [Wrig 91a]. <p> I 3 7 7 7 7 7 7 7 23 * SLF -GE/rpp-N ! SLF -GE/rpp with push to N -shaped Jacobian, * GSC -RS ! global stability control using rescaling, * GSC -RS-fl ! GSC -RS with push to fl-shaped Jacobian, along with those of four algorithms discussed in <ref> [Wrig 90b] </ref> and [Wrig 91a] * Structured QR ! Wright's structured orthogonal factorization algorithm, * Structured LU ! Wright's structured Gaussian elimination algorithm, * ROWPP ! a plain row partial-pivoting code, * DECOMP/SOLVE ! routines from the PASVA codes [Lent 77], and two sequential codes designed for problems with separated boundary
Reference: [Wrig 91a] <author> S.J. Wright, </author> <title> Stable parallel elimination for boundary value ODEs, </title> <type> Preprint MCS--P229-0491, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory (1991). </institution>
Reference-contexts: Stability is assured since the algorithm is essentially a QR-factorization applied to a row and column permuted version of the original ABD. In <ref> [Wrig 91a] </ref> the local orthogonal transformations used in [Wrig 90b] are replaced by Gauss transformations with row partial-pivoting resulting in a substantial speed-up even though the number of block-steps is the same. <p> In x3 the block multiplications used in the unmodified algorithm are replaced by Stable Local Factorization (SLF ) transformations. These transformations give rise to stable parallel algorithms for solving ABD systems|a thorough stability analysis is given in [Wrig 90b] and <ref> [Wrig 91a] </ref>. In addition, the algorithms are extended to increase parallelism in the back-solve stage by making better use of available 3 processors during the decomposition stage. <p> Each avoids the instability inherent in (14) by controlling the growth of elements during the reduction. One uses orthogonal factorization, the other Gaussian elimination with row partial-pivoting. Their stability is discussed in [Wrig 90b] and <ref> [Wrig 91a] </ref>. <p> 7 7 7 7 7 7 23 * SLF -GE/rpp-N ! SLF -GE/rpp with push to N -shaped Jacobian, * GSC -RS ! global stability control using rescaling, * GSC -RS-fl ! GSC -RS with push to fl-shaped Jacobian, along with those of four algorithms discussed in [Wrig 90b] and <ref> [Wrig 91a] </ref> * Structured QR ! Wright's structured orthogonal factorization algorithm, * Structured LU ! Wright's structured Gaussian elimination algorithm, * ROWPP ! a plain row partial-pivoting code, * DECOMP/SOLVE ! routines from the PASVA codes [Lent 77], and two sequential codes designed for problems with separated boundary conditions only: SOLVEBLOK <p> (.73D+68) .15D+69 (.15D+69) .99D+70 (.99D+70) SLF -QR .96D-10 (.17D-06) .37D-12 (.17D-09) .23D-13 (.92D-10) .40D-14 (.53D-10) SLF -GE/rpp .57D-10 (.17D-06) .13D-12 (.17D-09) .84D-14 (.92D-10) .36D-14 (.53D-10) GSC -RS =+1 .23D-09 (.17D-06) .92D-12 (.17D-09) .12D-12 (.92D-10) .54D-13 (.53D-10) GSC -RS-fl =+1 .23D-09 (.17D-06) .92D-12 (.17D-09) .13D-12 (.92D-10) .13D-12 (.53D-10) Problem 2A (Wright <ref> [Wrig 91a] </ref>) a = 0; b = ; n = 3; A (t) = 6 1 19 cos 2t 0 1 + 19 sin 2t 1 + 19 sin 2t 0 1 + 19 cos 2t 7 2 4 e t 3 5 ; subject to the boundary conditions y 1 <p> multiple shooting with DE M = 16 M = 32 M = 64 M = 128 BlkCR .10D+10 .23D+10 .37D+11 .48D+11 SLF -QR .59D-09 .20D-09 .25D-09 .16D-09 SLF -GE/rpp .59D-09 .20D-09 .25D-09 .16D-09 GSC -RS =+1 .59D-09 .20D-09 .25D-09 .16D-09 GSC -RS-fl =+1 .59D-09 .20D-09 .25D-09 .16D-09 Problem 3A (Wright <ref> [Wrig 91a] </ref>) a = 0; b = 1; n = 5; A (t) = 6 6 6 4 0 2 cos 2! 2 t 0 ! 2 + 2 sin 2! 2 t 0 0 ! 2 + 2 sin 2! 2 t 0 2 cos 2! 2 t 0 3 <p> (.88D+65) .64D+68 (.64D+68) .99D+70 (.99D+70) SLF -QR .55D-10 (.98D-07) .20D-12 (.21D-09) .20D-13 (.94D-10) .49D-14 (.54D-10) SLF -GE/rpp .38D-10 (.98D-07) .18D-12 (.21D-09) .12D-13 (.94D-10) .36D-14 (.54D-10) GSC -RS =+1 .18D-09 (.98D-07) .23D-11 (.21D-09) .21D-12 (.94D-10) .42D-13 (.54D-10) GSC -RS-fl =+1 .18D-09 (.98D-07) .23D-11 (.21D-09) .47D-12 (.94D-10) .29D-12 (.54D-10) Problem 3B (Wright <ref> [Wrig 91a] </ref>) Same as Problem 3A, but with coupled boundary conditions y 1 (0) = 1 5y 1 (0) + y 1 (1) = 5 + e 4y 2 (1) + 5y 5 (1) = e: Only the true error is shown. 31 Problem #3B ; discretization: midpoint finite difference M
Reference: [Wrig 91b] <author> S.J. Wright, </author> <title> A collection of problems for which Gaussian elimination with partial pivoting is unstable, </title> <type> Manuscript, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory (1991). </institution> <month> 35 </month>
Reference-contexts: The algorithm is equivalent to Gaussian elimination with restriced row partial-pivoting applied to the original ABD, and is stable for a wide range of problems, although rapid error growth can arise in some cases <ref> [Wrig 91b] </ref>. The algorithms described in x3 of this paper were discovered independently and are similar to those of Wright. Each is based on applying a variation of block cyclic reduction directly to (6) (i.e. the normal equations are not formed). <p> The cost of applying cyclic reduction to each partition is the same as that of the algorithm in x3.3. Which algorithm is better in terms of stability, however, is currently an open question. As pointed out in <ref> [Wrig 91b] </ref>, SLF-LU when implemented as described in x3.3 is potentially unstable on certain problems when the partitions become large.
References-found: 23

