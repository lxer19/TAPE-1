URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/papers/coling92.ps
Refering-URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/index.html
Root-URL: http://www.cs.nyu.edu
Title: Linguistic Knowledge Generator  
Author: Satoshi SEKINE Sofia ANANIADOU Jeremy J.CARROLL Jun'ichi TSUJII 
Address: PO Box 88, Manchester M60 1QD, United Kingdom  
Affiliation: Tokyo Information and Communications Research Laboratory Matsushita Electric Industrial Co.,Ltd.  Centre for Computational Linguistics University of Manchester Institute of Science and Technology  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Ralph Grishman: </author> <title> Discovery Procedures for Sublanguage Selectional Patterns: Initial Experiments Comp. </title> <note> Linguistics Vol.12 No.3 (1986) </note>
Reference-contexts: Semantic classification schemes for words, for example, usually reflect ontologies of subject domains so that we cannot expect a single classification scheme to be effective across different domains. To treat different sublanguages requires different word classification schemes. We have to construct appropriate schemes for given sublanguages from scratch <ref> [1] </ref>. It has also been reported that not only knowledge concerned with extra-linguistic domains but also syntactic knowledge, such as subcategorization frames of verbs (which is usually conceived as a part of general language knowledge), often varies from one sublanguage to another [2]. <p> C T;i : Credit of instance-tuple T with identification number i. <ref> [0; 1] </ref> V T : Plausibility value of a hypothesis-tuple T in cycle g. [0; 1] D g (w a ; w b ) : distance between words, w a and w b in cycle g. [0; 1] Algorithm The following explanation of the algorithm assumes that the inputs are sentences. <p> C T;i : Credit of instance-tuple T with identification number i. <ref> [0; 1] </ref> V T : Plausibility value of a hypothesis-tuple T in cycle g. [0; 1] D g (w a ; w b ) : distance between words, w a and w b in cycle g. [0; 1] Algorithm The following explanation of the algorithm assumes that the inputs are sentences. 1. <p> C T;i : Credit of instance-tuple T with identification number i. <ref> [0; 1] </ref> V T : Plausibility value of a hypothesis-tuple T in cycle g. [0; 1] D g (w a ; w b ) : distance between words, w a and w b in cycle g. [0; 1] Algorithm The following explanation of the algorithm assumes that the inputs are sentences. 1. For a sentence we use a simple grammar to find all tuples possibly used. Each instance-tuple is then given credit in proportion to the number of competing tuples.
Reference: [2] <author> Sofia Ananiadou: </author> <title> Sublanguage studies as the Basis for Computer Support for Multilingual Communication Proceedings of Termplan `90, </title> <address> Kuala Lumpur (1990) </address>
Reference-contexts: It has also been reported that not only knowledge concerned with extra-linguistic domains but also syntactic knowledge, such as subcategorization frames of verbs (which is usually conceived as a part of general language knowledge), often varies from one sublanguage to another <ref> [2] </ref>. Though re-usability of linguistic knowledge is currently and intensively prescribed [3], our contention is that the adaptation of existing knowledge requires processes beyond mere re-use. That is, 1.
Reference: [3] <editor> A Zampolli: </editor> <booktitle> Reusable Linguistic Resources (Invited paper) 5th Conference of the E.A.C.L. </booktitle> <year> (1991) </year>
Reference-contexts: Though re-usability of linguistic knowledge is currently and intensively prescribed <ref> [3] </ref>, our contention is that the adaptation of existing knowledge requires processes beyond mere re-use. That is, 1. There are some types of knowledge which we have to discover from scratch, and which should be integrated with already existing knowledge. fl SEKINE is currently a visitor at U.M.I.S.T. sekine@ccl.umist.ac.uk 2.
Reference: [4] <author> Kenneth Ward Church: </author> <title> A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text 2nd Conference on A.N.L.P (1988) </title>
Reference-contexts: The very fact that this trial and error process is time consuming and not always satisfactory indicates that human introspection alone cannot effectively reveal regularities or closure properties of sublanguages. There have been some proposals to aid this procedure by using programs in combination with huge corpora <ref> [4] </ref> [5] [6] [7]. But the acquisition programs in these reports require huge amounts of sample texts in given domains which often makes these methods unrealistic in actual application environments.
Reference: [5] <author> Donald Hindle and Mats Rooth: </author> <booktitle> Structural Ambiguity and Lexical Relations 29th Conference of the A.C.L. </booktitle> <year> (1991) </year>
Reference-contexts: The very fact that this trial and error process is time consuming and not always satisfactory indicates that human introspection alone cannot effectively reveal regularities or closure properties of sublanguages. There have been some proposals to aid this procedure by using programs in combination with huge corpora [4] <ref> [5] </ref> [6] [7]. But the acquisition programs in these reports require huge amounts of sample texts in given domains which often makes these methods unrealistic in actual application environments.
Reference: [6] <author> Smaja and McKeown: </author> <title> Automatically Extracting and Representing Collocations for language generation. </title> <booktitle> 28th Conference of the A.C.L. </booktitle> <year> (1991) </year>
Reference-contexts: The very fact that this trial and error process is time consuming and not always satisfactory indicates that human introspection alone cannot effectively reveal regularities or closure properties of sublanguages. There have been some proposals to aid this procedure by using programs in combination with huge corpora [4] [5] <ref> [6] </ref> [7]. But the acquisition programs in these reports require huge amounts of sample texts in given domains which often makes these methods unrealistic in actual application environments.
Reference: [7] <author> Uri Zernik and Paul Jacobs: </author> <title> Tagging for Learning: Collecting Thematic Relations from Corpus 13th COLING-90 (1990) </title>
Reference-contexts: There have been some proposals to aid this procedure by using programs in combination with huge corpora [4] [5] [6] <ref> [7] </ref>. But the acquisition programs in these reports require huge amounts of sample texts in given domains which often makes these methods unrealistic in actual application environments.
Reference: [8] <editor> S.Sekine, J.J.Carroll, S. Ananiadou, J.Tsujii: </editor> <booktitle> Automatic Learning for Semantic Collocation 3rd Conference on A.N.L.P. </booktitle> <year> (1992) </year>
Reference-contexts: In this scenario, the ALPSC treats only sequences of nouns, but it can generally applied for any structure of syntactic relationships. It is an unique program with the following points <ref> [8] </ref>: 1. it does not need a training corpus, which is one of the bottle necks of some other learning programs 2. it learns by using a combination of linguistic knowledge and statistical analysis 3. it uses a parser which produces all possible anal yses 4. it works as a relaxation
References-found: 8

