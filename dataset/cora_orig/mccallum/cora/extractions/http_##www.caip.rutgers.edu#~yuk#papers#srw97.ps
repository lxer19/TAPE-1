URL: http://www.caip.rutgers.edu/~yuk/papers/srw97.ps
Refering-URL: 
Root-URL: 
Title: Development of The RU Hub4 system  
Author: C. Che, D. Yuk, S. Chennoukh, L. Flanagan 
Address: Piscataway, NJ 08855  
Affiliation: CAIP Center Rutgers University  
Abstract: This paper describes preliminary development of a broadcast news transcribing system for this year's Hub4 evaluation. The recognition system uses CROWNS (developed at RU for the 1995 Hub3 tasks) with several modifications to handle the news programming task. Features such as model adaptation have been added to quickly provide acoustic models thought appropriate for the new task, even though the environment-dependent data are limited. The architecture of decoding is changed from one pass to multi-pass that can handle higher order language models more efficiently. Due to the short development period before evaluation, the preliminary system for this year's Hub4 test has produced a higher error rate than expected. In fact, its performance is found to be worse than our previous system when compared on the baselin broadcast speech. We have continued investigation since the test and performed diagnostic experiments. Results and error analysis are given in this report. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> C. Che, </author> <title> Development of CROWNS: CAIP Recognizer of Words N' Sentences, </title> <booktitle> DARPA Speech Recognition WorkShop, </booktitle> <address> Harriman, NY 1996, p. </address> <pages> 112-116 </pages>
Reference-contexts: Compared to last year's Hub3 test, the Hub4 task presents a more challenging problem with its great variety in speaking styles, channel conditions and presence of background noise and music. We modified our existing system "CROWNS" <ref> [1] </ref> for this year's evaluation. To address the various focus conditions, features such as model adaptation are added to our system. The decoding strategy is also changed from last year's single-pass Viterbi beam search to a multi-pass word graph decoder that can efficiently handle higher order language models.
Reference: 2. <author> C.J.Leggetter and P.C. Woodland, </author> <title> Flexible Speaker adaptation Using Maximum Likelihood Linear Regression, </title> <booktitle> Proceedings of he Spoken Language System Technology Workshop, </booktitle> <pages> pp. 110-115, </pages> <month> Jan. </month> <year> 1995 </year>
Reference-contexts: Trigrams are used to rescore the upper 500 hypotheses, and output the top one as the result. 3. ADAPTATION To make use of the available 60 hours worth of transcribed acoustic materials, a Maximum Likelihood Linear Regression transform based approach <ref> [2] </ref> is used. Due to the time constraint, only material from F0 and F3 conditions are used to adapt our seed model Set 1 and Set 3 with a global transform. The adapted models are then used in the final system decoding without performing any development test run. 4.
Reference: 3. <author> D. Paul and B. Necioglu, </author> <title> The Lincoln Large-Vocabulary Stack-Decoder HMM CSR, </title> <booktitle> Proc ICASSP, </booktitle> <volume> Vol II, </volume> <pages> pp. 660-663, </pages> <address> Minneapolis, </address> <year> 1993 </year>
Reference-contexts: Density of the graph is further reduced by prun ing according to the best full path likelihood. 3. N Best rescoring: A final N best pass is used in producing alternative hypotheses. We use a word level Afl algorithm <ref> [3] </ref> to search the word graph. The implementation is straight-forward since the graph already contains likelihood and boundaries for every word. Trigrams are used to rescore the upper 500 hypotheses, and output the top one as the result. 3.
Reference: 4. <author> P.C. Woodland and S.J. Young, </author> <title> The HTK Tied State Continuous Speech Recognition, </title> <booktitle> Proc. EuroSpeech, </booktitle> <volume> Vol. 3, </volume> <pages> pp. 2207-2210, </pages> <address> Berlin, </address> <year> 1993 </year>
Reference-contexts: Each state of the tri phone model use mixtures of Gaussians as output distributions. Transition probabilities are fixed. To obtain a more robust and reliable estimate for the huge number of Gaussian mixture used in the system, state tying are used with the furthest neighborhood tying suggested by <ref> [4] </ref>.
Reference: 5. <author> Lee-K.F., </author> <title> Automatic Speech Recognition: The Development of the SPHINX System, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1989. </year>
Reference: 6. <author> H.Murveit, J.Butzberger, V.Digalakis and M. Wein-traub, </author> <title> Large-Vocabulary Dictation Using SRI's DECIPHER (TM) Speech Recognition System: </title> <booktitle> Progressive-Search Techniques, ICASSP, </booktitle> <volume> Vol II, </volume> <pages> pp. 319-322, </pages> <address> Min-neapolis, </address> <year> 1993 </year>
Reference: 7. <author> V.Steinbiss, B.-H.Tran and H.Ney, </author> <title> Improvements in Beam Search, </title> <booktitle> Proc, Int.Conf. on Spoken Language Processing, </booktitle> <pages> pp. 2143-2146, </pages> <address> Yokohama, </address> <month> Sep, </month> <year> 1994 </year>
Reference: 8. <author> S. Ortmanns, H.Ney, F.Seide and I.Lindam, </author> <title> A Comparison of Time Conditioned and Word Conditioned Search Techniques for Large Vocabulary Speech Recognition, </title> <booktitle> Proc, Int.Conf. on Spoken Language Processing, </booktitle> <address> Philadelphia, PA, </address> <month> Oct, </month> <year> 1996 </year>
Reference: 9. <author> G.Antoniol, F.Brugnara, M.Cettolo and M.Federico, </author> <title> Language Model Representations for Beam Search Decoding, </title> <booktitle> Proc ICASSP, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 588-591, </pages> <address> Detroit, MI, </address> <month> May </month> <year> 1995 </year>
Reference: 10. <author> M.Oerder and H.Ney, </author> <title> Word Graphs: An Efficient Interface Between Continuous Speech Recognition and Language Understanding, </title> <booktitle> Proc ICASSP, </booktitle> <volume> Vol. II, </volume> <pages> pp. 119-122, </pages> <address> Minneapolis, MN, </address> <month> April, </month> <year> 1993 </year>
References-found: 10

