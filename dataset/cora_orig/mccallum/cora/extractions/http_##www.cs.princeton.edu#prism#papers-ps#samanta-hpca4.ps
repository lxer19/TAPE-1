URL: http://www.cs.princeton.edu/prism/papers-ps/samanta-hpca4.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Email: frudro, bilas, jpsg@cs.princeton.edu, iftode@cs.rutgers.edu  
Phone: 2  
Title: Home-based SVM protocols for SMP clusters: Design and Performance  
Author: Rudrajit Samanta Angelos Bilas Liviu Iftode and Jaswinder Pal Singh 
Address: Princeton, NJ 08544  Piscataway, NJ 08855  
Affiliation: 1 Department of Computer Science, Princeton University  Department of Computer Science, Rutgers University  
Abstract: As small-scale shared memory multiprocessors proliferate in the market, it is very attractive to construct large-scale systems by connecting smaller multiprocessors together in software using efficient commodity network interfaces and networks. Using a shared virtual memory (SVM) layer for this purpose preserves the attractive shared memory programming abstraction across nodes. In this paper: * We describe home-based SVM protocols that support symmetric multiprocessor (SMP) nodes, taking advantage of the intra-node hardware cache coherence and synchronization mechanisms. Our protocols take no special advantage of the network interface and network except as a fast communication link, and as such are very portable. We present the key design tradeoffs, discuss our choices, and describe key data structures that enable us to implement these choices quite simply. * We present an implementation on a network of 4-way Intel PentiumPro SMPs interconnected with Myrinet, and provide performance results. * We explore the advantages of SMP nodes over uniprocessor nodes with this protocol, as well as other performance tradeoffs, through both real implementation and simulation as appropriate, since both have im portant roles to play. We find one approach to deliver good parallel performance on many real applications (at least at the scale we examine) and to improve performance over SVM across uniprocessor nodes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bilas, L. Iftode, D. Martin, and J. Singh. </author> <title> Shared virtual memory across SMP nodes using automatic update: Protocols and performance. </title> <type> Technical Report TR-517-96, </type> <address> Prince-ton, NJ, </address> <month> Mar. </month> <year> 1996. </year>
Reference-contexts: SVM on SMPs is argued to be a promising direction in [5]. In [14] the authors find that a reserved protocol processor in not required. Earlier versions of this work through simulations can be found in <ref> [1, 2] </ref>. The SoftFLASH system [8] which has goals similar to us, but uses a single-writer protocol (modeled after the FLASH protocol). The protocol is not as aggressive as other multiple-writer lazy SVM protocols. The system exhibits 2 For Ocean the best case is always four processors per node.
Reference: [2] <author> A. Bilas, L. Iftode, and J. P. Singh. </author> <title> Comparison of shared virtual memory across uniprocessor and SMP nodes. </title> <booktitle> In IMA Workshop on Parallel Algorithms and Parallel Systems, </booktitle> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: A more detailed description of the simulator can be found in <ref> [2] </ref>. The simulated architecture (Figure 6) is similar to the implementation platform. It assumes a cluster of c processor SMPs connected with a commodity interconnect like Myrinet [4]. Contention is modeled at all levels except the network links. <p> SVM on SMPs is argued to be a promising direction in [5]. In [14] the authors find that a reserved protocol processor in not required. Earlier versions of this work through simulations can be found in <ref> [1, 2] </ref>. The SoftFLASH system [8] which has goals similar to us, but uses a single-writer protocol (modeled after the FLASH protocol). The protocol is not as aggressive as other multiple-writer lazy SVM protocols. The system exhibits 2 For Ocean the best case is always four processors per node.
Reference: [3] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A virtual memory mapped network interface for the shrimp multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142 153, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The best known all-software SVM system (across uniprocessors) is TreadMarks from Rice University [15]. Recently, however, all-software versions of multiple-writer home based protocols which were originally developed for systems with hardware support for automatic write prop-agation <ref> [3, 10] </ref> also have been developed. The idea, proposed in [11], is to compute diffs as in previous all-software protocols, but to propagate the diffs to the home at a release point and apply them eagerly, keeping the home up to date. <p> In a preliminary evaluation on the Intel Paragon multiprocessor [26], this software home-based protocol (called HLRC) was found to outperform earlier distributed all-software protocols, just as home-based protocols with hardware write propagation <ref> [3, 17, 9] </ref> were found to earlier, and is thus attractive. A simple way to build an SVM system across SMPs is to treat each processor as a separate node in a uniprocessor-node SVM protocol, using the SMP hardware simply to accelerate message passing [5].
Reference: [4] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, and W.-K. Su. Myrinet: </author> <title> A gigabit-per-second local area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1):2936, </volume> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: A more detailed description of the simulator can be found in [2]. The simulated architecture (Figure 6) is similar to the implementation platform. It assumes a cluster of c processor SMPs connected with a commodity interconnect like Myrinet <ref> [4] </ref>. Contention is modeled at all levels except the network links. In our simulations, we try to be very close to a realistic system, but we sometimes set parameters to avoid certain artifactual limitations of the real system.
Reference: [5] <author> A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software versus hardware shared-memory implementation: A case study. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 106 117, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: A simple way to build an SVM system across SMPs is to treat each processor as a separate node in a uniprocessor-node SVM protocol, using the SMP hardware simply to accelerate message passing <ref> [5] </ref>. Other previous implementations have examined single-writer protocols based on directory approaches [8]. The results from the latter implementation were not very optimistic. <p> Especially, if bandwidth is increased as well, the performance of applications that are traditionally difficult for SVM (i.e. Radix) becomes decent 2 . 6 Related work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [5, 14, 8, 18, 23] </ref> for different protocols. HLRC and LRC have been compared for some applications on the Intel Paragon in [26]. SVM on SMPs is argued to be a promising direction in [5]. In [14] the authors find that a reserved protocol processor in not required. <p> HLRC and LRC have been compared for some applications on the Intel Paragon in [26]. SVM on SMPs is argued to be a promising direction in <ref> [5] </ref>. In [14] the authors find that a reserved protocol processor in not required. Earlier versions of this work through simulations can be found in [1, 2]. The SoftFLASH system [8] which has goals similar to us, but uses a single-writer protocol (modeled after the FLASH protocol).
Reference: [6] <author> C. Dubnicki, A. Bilas, K. Li, and J. Philbin. </author> <title> Design and implementation of virtual memory-mapped communication on myrinet. </title> <booktitle> In Proceedings of the 1997 International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: Actual node-to-network bandwidth is usually constrained by the 132MB/s I/O bus. All nodes are connected directly to an 8-way switch. 4.1 Basic Software and Costs For basic communication we use the Virtual Memory Mapped Communication (VMMC-2) library for the Myrinet network <ref> [6] </ref>. This general-purpose, memory-mapped communication layer provides a minimum one-way end-to-end latency of approximately 10 s and a peak bandwidth of about 100MB/s on this hardware. The SMP nodes run the SMP version of the Linux operating system (version 2.0.24).
Reference: [7] <author> A. Erlichson, B. Nayfeh, J. Singh, and K. Olukotun. </author> <title> The benefits of clustering in shared address space multiprocessors: An applications-driven investigation. </title> <booktitle> In Supercomputing '95, </booktitle> <pages> pages 176186, </pages> <year> 1995. </year>
Reference-contexts: When using SMP nodes, the greater the size of the node relative to the total number of processors, the more the communication and synchronization that can be contained cheaply within the node, and the greater the benefits of prefetching, cache-to-cache sharing, and overlapping working sets <ref> [7] </ref>; at the same time, however, higher demands are placed on the cross-node communication architecture.
Reference: [8] <author> A. Erlichson, N. Nuckolls, G. Chesson, and J. Hennessy. SoftFLASH: </author> <title> analyzing the performance of clustered distributed virtual shared memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 210220, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: A simple way to build an SVM system across SMPs is to treat each processor as a separate node in a uniprocessor-node SVM protocol, using the SMP hardware simply to accelerate message passing [5]. Other previous implementations have examined single-writer protocols based on directory approaches <ref> [8] </ref>. The results from the latter implementation were not very optimistic. A multiple-writer directory based scheme was implemented concurrently with our work, and takes great advantage of the features of the DEC Memory Channel network interface and network [23]. <p> Costs on the implementation plat form. All costs are in s, excpept if otherwise noted. The cost of the page-fetch operation is much faster than previous implementations of SVM systems, for instance TreadMarks [16] reports over 1900 s (for a 4KB page) and SoftFlash <ref> [8] </ref> states 1164s (for a 16KB page at the Kernel level on an SGI Challenge). Cashmere-2L [23] reports the page fetch operation to cost about 800 s. <p> Especially, if bandwidth is increased as well, the performance of applications that are traditionally difficult for SVM (i.e. Radix) becomes decent 2 . 6 Related work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [5, 14, 8, 18, 23] </ref> for different protocols. HLRC and LRC have been compared for some applications on the Intel Paragon in [26]. SVM on SMPs is argued to be a promising direction in [5]. In [14] the authors find that a reserved protocol processor in not required. <p> SVM on SMPs is argued to be a promising direction in [5]. In [14] the authors find that a reserved protocol processor in not required. Earlier versions of this work through simulations can be found in [1, 2]. The SoftFLASH system <ref> [8] </ref> which has goals similar to us, but uses a single-writer protocol (modeled after the FLASH protocol). The protocol is not as aggressive as other multiple-writer lazy SVM protocols. The system exhibits 2 For Ocean the best case is always four processors per node.
Reference: [9] <author> R. Gillett. </author> <title> Memory channel network for pci. </title> <booktitle> In Proceedings of Hot Interconnects '95 Symposium, </booktitle> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: In a preliminary evaluation on the Intel Paragon multiprocessor [26], this software home-based protocol (called HLRC) was found to outperform earlier distributed all-software protocols, just as home-based protocols with hardware write propagation <ref> [3, 17, 9] </ref> were found to earlier, and is thus attractive. A simple way to build an SVM system across SMPs is to treat each processor as a separate node in a uniprocessor-node SVM protocol, using the SMP hardware simply to accelerate message passing [5].
Reference: [10] <author> L. Iftode, C. Dubnicki, E. W. Felten, and K. Li. </author> <title> Improving release-consistent shared virtual memory using automatic update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: The best known all-software SVM system (across uniprocessors) is TreadMarks from Rice University [15]. Recently, however, all-software versions of multiple-writer home based protocols which were originally developed for systems with hardware support for automatic write prop-agation <ref> [3, 10] </ref> also have been developed. The idea, proposed in [11], is to compute diffs as in previous all-software protocols, but to propagate the diffs to the home at a release point and apply them eagerly, keeping the home up to date. <p> HLRC uses a home node for each shared page at which updates from writers of that page are collected. This protocol is inspired by the design of Automatic Update Release Consistency (AURC) <ref> [10] </ref>. The difference is that updates are detected in software and propagated using diffs unlike in AURC, so HLRC does not require any special hardware support.
Reference: [11] <author> L. Iftode, J. Singh, and K. Li. </author> <title> Scope consistency: a bridge between release consistency and entry consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: The best known all-software SVM system (across uniprocessors) is TreadMarks from Rice University [15]. Recently, however, all-software versions of multiple-writer home based protocols which were originally developed for systems with hardware support for automatic write prop-agation [3, 10] also have been developed. The idea, proposed in <ref> [11] </ref>, is to compute diffs as in previous all-software protocols, but to propagate the diffs to the home at a release point and apply them eagerly, keeping the home up to date.
Reference: [12] <author> L. Iftode, J. P. Singh, and K. Li. </author> <title> Understanding application performance on shared virtual memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: We now briefly describe the basic characteristics of each application that are relevant to the use of SMP nodes. A more detailed classification and description of the application behavior for SVM systems with uniprocessor nodes is presented in <ref> [12] </ref>. The applications can be divided in two groups, regular and irregular. The regular applications are FFT, LU and Ocean. Their common characteristic is that they are single-writer applica tions a given word of data is written only by the processor to which it is assigned. <p> There is no need to compute diffs for these applications. Protocol action is required only to fetch pages. The applications have different inherent and induced (at page granularity) communication patterns <ref> [24, 12] </ref>, which affect their performance and the impact on SMP nodes. FFT: The all-to-all, read-based communication in FFT is essentially a transposition of a matrix of complex numbers. <p> Ocean-rowwise shows a higher than expected computation time, which results in a lower speedup. Finally, the applications for the bad category are Barnes-rebuild and FFT. These applications are known to be sore points for most SVM implementations <ref> [12] </ref>. Barnes-rebuild uses a huge number of locks and has a fine grained data access pattern, both of which are weak points of most page-based SVM systems. FFT has extremely bursty communication at barriers and places a serious strain on network bandwidth and latency. <p> This is due to the near-neighbor sharing pattern in Ocean. Neighbors are often on the same SMP, so the sharing is much cheaper. Also, the fragmentation (wasted communication) problems observed at column-oriented boundaries with uniprocessor nodes <ref> [12] </ref> are greatly alleviated when the neighbors are in the same SMP. This application uses a large number of barriers to synchronize between phases and time-steps of the computation.
Reference: [13] <author> D. Jiang, H. Shan, and J. P. Singh. </author> <title> Application restructuring and performance portability on shared virtual memory and hardware-coherent multiprocessors. </title> <booktitle> In Sixth ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Ocean: The communication pattern is largely nearest-neighbor and iterative on a regular grid. We run the contiguous (4-d array) version of Ocean as well as one that partitions the grid into blocks of rows rather than square subgrids <ref> [13] </ref>. The irregular applications in our suite are Barnes, Radix, Raytrace, Volrend, Water-nsquared and Water-spatial. Barnes: Access patterns are irregular and fine-grained. We use two versions of Barnes. The first version (Barnes-rebuild) uses the tree-building algorithm in SPLASH-2. The second version (Barnes-spatial) [13] is optimized for SVM. <p> into blocks of rows rather than square subgrids <ref> [13] </ref>. The irregular applications in our suite are Barnes, Radix, Raytrace, Volrend, Water-nsquared and Water-spatial. Barnes: Access patterns are irregular and fine-grained. We use two versions of Barnes. The first version (Barnes-rebuild) uses the tree-building algorithm in SPLASH-2. The second version (Barnes-spatial) [13] is optimized for SVM. It avoids locking entirely during tree building at the cost of load imbalance in that phase. Radix: The radix sorting application exhibits highly scattered and irregular remote writes. <p> This disappears in the SMP configuration and performance improves. Volrend is imbalanced because of the different portions of work related to each part of the scene. The version we use does not implement task stealing <ref> [13] </ref>. Using SMP nodes reduces the imbalances not in computation time but in memory stalls and data wait time. However, although local locks are cheap, remote locks are still expensive in HLRC-SMP. The combination of data sharing and cheaper synchronization makes Water-spatial exhibit a big improvement with HLRC-SMP.
Reference: [14] <author> M. Karlsson and P. Stenstrom. </author> <title> Performance evaluation of cluster-based multiprocessor built from atm switches and bus-based multiprocessor servers. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Especially, if bandwidth is increased as well, the performance of applications that are traditionally difficult for SVM (i.e. Radix) becomes decent 2 . 6 Related work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [5, 14, 8, 18, 23] </ref> for different protocols. HLRC and LRC have been compared for some applications on the Intel Paragon in [26]. SVM on SMPs is argued to be a promising direction in [5]. In [14] the authors find that a reserved protocol processor in not required. <p> HLRC and LRC have been compared for some applications on the Intel Paragon in [26]. SVM on SMPs is argued to be a promising direction in [5]. In <ref> [14] </ref> the authors find that a reserved protocol processor in not required. Earlier versions of this work through simulations can be found in [1, 2]. The SoftFLASH system [8] which has goals similar to us, but uses a single-writer protocol (modeled after the FLASH protocol).
Reference: [15] <author> P. Keleher, A. Cox, S. Dwarkadas, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115132, </pages> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: The trends in the marketplace make this two-level approach attractive. A network of bus-based, symmetric multiprocessors (SMPs) forms our own next-generation SVM infrastructure, and we focus on SMP nodes in this paper. The best known all-software SVM system (across uniprocessors) is TreadMarks from Rice University <ref> [15] </ref>. Recently, however, all-software versions of multiple-writer home based protocols which were originally developed for systems with hardware support for automatic write prop-agation [3, 10] also have been developed.
Reference: [16] <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 1321, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In this section we briefly review HLRC, discuss the design choices for the protocol extensions to support SMP nodes, and present the specific choices made in our implementation. 2.1 Home-based LRC Protocol Home-based Lazy Release Consistency (HLRC) [26] is a variation of LRC <ref> [16] </ref> which uses a software write detection and diff-based write propagation scheme. HLRC uses a home node for each shared page at which updates from writers of that page are collected. This protocol is inspired by the design of Automatic Update Release Consistency (AURC) [10]. <p> Writers can discard their diffs as soon as they are dispatched. Home nodes apply arriving diffs to the relevant pages as soon as they arrive, and immediately discard them. A vector time-stamp approach is used to preserve the lazy release consistency model <ref> [16] </ref>. During a page fault following a coherence invalidation (performed at acquires), the faulting node fetches the up-to-date version of a whole page from the home node. <p> Costs on the implementation plat form. All costs are in s, excpept if otherwise noted. The cost of the page-fetch operation is much faster than previous implementations of SVM systems, for instance TreadMarks <ref> [16] </ref> reports over 1900 s (for a 4KB page) and SoftFlash [8] states 1164s (for a 16KB page at the Kernel level on an SGI Challenge). Cashmere-2L [23] reports the page fetch operation to cost about 800 s.
Reference: [17] <author> L. Kontothanassis, G. Hunt, R. Stets, N. Hardavellas, M. Cierniak, S. Parthasarathy, W. Meira, S. Dwarkadas, and M. Scott. </author> <title> VM-based shared memory on low-latency, remote-memory-access networks. </title> <booktitle> Proc., 24th Annual Int'l. Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: In a preliminary evaluation on the Intel Paragon multiprocessor [26], this software home-based protocol (called HLRC) was found to outperform earlier distributed all-software protocols, just as home-based protocols with hardware write propagation <ref> [3, 17, 9] </ref> were found to earlier, and is thus attractive. A simple way to build an SVM system across SMPs is to treat each processor as a separate node in a uniprocessor-node SVM protocol, using the SMP hardware simply to accelerate message passing [5].
Reference: [18] <author> D. Scales and K. Gharachorloo. </author> <title> Towards transparent and efficient software distributed shared memory. </title> <booktitle> In Proceedings of the Sixteenth Symposium on Operating Systems Principles, </booktitle> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: A multiple-writer directory based scheme was implemented concurrently with our work, and takes great advantage of the features of the DEC Memory Channel network interface and network [23]. There has also been an SMP-node implementation of a fine-grained approach to software shared memory <ref> [18] </ref>. <p> Especially, if bandwidth is increased as well, the performance of applications that are traditionally difficult for SVM (i.e. Radix) becomes decent 2 . 6 Related work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [5, 14, 8, 18, 23] </ref> for different protocols. HLRC and LRC have been compared for some applications on the Intel Paragon in [26]. SVM on SMPs is argued to be a promising direction in [5]. In [14] the authors find that a reserved protocol processor in not required. <p> This idea was first implemented in the Blizzard-S system [21]. There is currently an implementation of Shasta that runs on a cluster of Alpha SMP nodes <ref> [18] </ref>. This implementation does show promising performance. However, it is not page-based shared virtual memory. It has control-data structure requirements that are dependent on the number of processors which may lead to scalability problems for large number of processors.
Reference: [19] <author> D. Scales, K. Gharachorloo, and C. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: The platform is a cluster of Alpha SMPs connected by the Memory Channel network. The protocol takes great advantage of particular features found on the Memory Channel network interface that are not found in other interfaces (such as Myrinet). The Shasta system <ref> [19] </ref> performs the coherence in software at cache-line granularity by instrumenting the executable to insert access control operations before shared loads and stores. This idea was first implemented in the Blizzard-S system [21]. There is currently an implementation of Shasta that runs on a cluster of Alpha SMP nodes [18].
Reference: [20] <author> I. Schoinas, B. Falsafi, A. Lebeck, S. Reinhardt, J. Larus, and D. Wood. </author> <title> Fine-grain access for distributed shared memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297306, </pages> <month> Oct. </month> <year> 1994. </year>
Reference: [21] <author> A. Sharma, A. T. Nguyen, J. Torellas, M. Michael, and J. Carbajal. Augmint: </author> <title> a multiprocessor simulation environment for intel x86 architectures. </title> <type> Technical report, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: The Shasta system [19] performs the coherence in software at cache-line granularity by instrumenting the executable to insert access control operations before shared loads and stores. This idea was first implemented in the Blizzard-S system <ref> [21] </ref>. There is currently an implementation of Shasta that runs on a cluster of Alpha SMP nodes [18]. This implementation does show promising performance. However, it is not page-based shared virtual memory.
Reference: [22] <author> R. Stets, S. Dwarkadas, N. Hardavellas, G. Hunt, L. Kon-tothanassis, S. Parthasarathy, and M. Scott. Cashmere-2L: </author> <title> Software Coherent Shared Memory on a Clustered Remote-Write Network. </title> <booktitle> In Proc. of the 16th ACM Symp. on Operating Systems Principles (SOSP-16), </booktitle> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: We use the simulator to analyze such tradeoffs in great detail. 5.1 Simulated Platforms For the simulation study of SMP clustering tradeoffs and detailed performance evaluation the simulation environment we use is built on top of augmint <ref> [22] </ref>, an execution driven simulator using the x86 instruction set and runs on x86 systems. A more detailed description of the simulator can be found in [2]. The simulated architecture (Figure 6) is similar to the implementation platform.
Reference: [23] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> Methodological considerations and characterization of the SPLASH-2 parallel application suite. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Other previous implementations have examined single-writer protocols based on directory approaches [8]. The results from the latter implementation were not very optimistic. A multiple-writer directory based scheme was implemented concurrently with our work, and takes great advantage of the features of the DEC Memory Channel network interface and network <ref> [23] </ref>. There has also been an SMP-node implementation of a fine-grained approach to software shared memory [18]. <p> The required global TLB flushes for each page invalidation can prove to be expensive in this scheme. However, since all threads within the same SMP have the same state for a given page, local acquire operations are very cheap. The first alternative has been shown to perform poorly in <ref> [23] </ref> due to high overheads. HLRC-SMP is used as an intermediate model that achieves both hardware sharing and laziness within the SMP node. Hardware sharing and (most) synchronization is achieved by having the processes in an SMP share a physical address space. <p> HLRC-SMP uses the first approach, since it allows more opportunities for load balancing the protocol processing across processors and because it leads to a cleaner implementation. Finally, HLRC-SMP implements hierarchical barriers and TLB invalidations are avoided with a scheme very similar to the 2-way-diffing described in <ref> [23] </ref>. 2.2.2 Implementation This section presents the data structures that allow us to easily implement the above choices in the HLRC-SMP, and how the synchronization and page fault operations are managed using them. 2.2.3 Data structures To illustrate the data structures, we first need to define some key terms. <p> Essentially a safe page fetch diffs the page returned by the home with the twin of the page and applies these updates to the current local page as well as the current twin for this page. This scheme is similar to the 2 way diffing used in Cashmere-2L <ref> [23] </ref>. 3 Applications We use several applications from the SPLASH-2 [24] application suite to evaluate the HLRC-SMP protocol. We now briefly describe the basic characteristics of each application that are relevant to the use of SMP nodes. <p> The cost of the page-fetch operation is much faster than previous implementations of SVM systems, for instance TreadMarks [16] reports over 1900 s (for a 4KB page) and SoftFlash [8] states 1164s (for a 16KB page at the Kernel level on an SGI Challenge). Cashmere-2L <ref> [23] </ref> reports the page fetch operation to cost about 800 s. Our system benefits from the very low latency of the VMMC-2 implementation and of Myrinet. 4.2 Performance In this section we present some preliminary performance results evaluating the HLRC-SMP implementation on this 4fi4 platform. <p> Especially, if bandwidth is increased as well, the performance of applications that are traditionally difficult for SVM (i.e. Radix) becomes decent 2 . 6 Related work Several papers have discussed the design and performance of shared virtual memory for SMPs <ref> [5, 14, 8, 18, 23] </ref> for different protocols. HLRC and LRC have been compared for some applications on the Intel Paragon in [26]. SVM on SMPs is argued to be a promising direction in [5]. In [14] the authors find that a reserved protocol processor in not required. <p> The processors in Alewife are very slow. The Cashmere-2L system <ref> [23] </ref> at Rochester is the closest to our system. It uses a multi-writer, directory-based scheme that is an eager variant of release consistency. This is unlike our lazy, vector time-stamp based scheme. The platform is a cluster of Alpha SMPs connected by the Memory Channel network.
Reference: [24] <author> D. Yeung, J. Kubiatowicz, and A. Agarwal. MGS: </author> <title> a multi-grain shared memory system. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: This scheme is similar to the 2 way diffing used in Cashmere-2L [23]. 3 Applications We use several applications from the SPLASH-2 <ref> [24] </ref> application suite to evaluate the HLRC-SMP protocol. We now briefly describe the basic characteristics of each application that are relevant to the use of SMP nodes. A more detailed classification and description of the application behavior for SVM systems with uniprocessor nodes is presented in [12]. <p> There is no need to compute diffs for these applications. Protocol action is required only to fetch pages. The applications have different inherent and induced (at page granularity) communication patterns <ref> [24, 12] </ref>, which affect their performance and the impact on SMP nodes. FFT: The all-to-all, read-based communication in FFT is essentially a transposition of a matrix of complex numbers.
Reference: [25] <author> Y. Zhou, L. Iftode, and K. Li. </author> <title> Performance evaluation of two home-based lazy release consistency protocols for shared virtual memory systems. </title> <booktitle> In Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: We use a multiple-writer home-based protocol based on lazy release consistency (LRC). The network interface we use provides lower latency and higher bandwidth. We use 4KB page and the safe page fetch helps us avoid the TLB synchronization problem. The Multigrain Shared Memory System (MGS) <ref> [25] </ref> built on top of the MIT Alewife machine uses a protocol very similar to the SoftFLASH system.
References-found: 25

