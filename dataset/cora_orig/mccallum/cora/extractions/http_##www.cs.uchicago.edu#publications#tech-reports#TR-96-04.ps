URL: http://www.cs.uchicago.edu/publications/tech-reports/TR-96-04.ps
Refering-URL: http://cs-www.uchicago.edu/publications/tech-reports/
Root-URL: 
Email: kahn@cs.uchicago.edu, swain@cs.uchicago.edu, peterp@cs.uchicago.edu, and firby@cs.uchicago.edu  
Title: Real-time Gesture Recognition with the Perseus System  
Author: Roger E. Kahn, Michael J. Swain, Peter N. Prokopowicz, and R. James Firby 
Note: supported this work.  
Address: 1100 East 58th Street Chicago, Illinois 60637  
Affiliation: The University of Chicago Computer Science Department  The University of Chicago Computer Science Department  
Pubnum: Technical Report TR-96-04  
Abstract: Interpersonal communication involves more than simply spoken information. Gestures are commonly used to more efficiently and precisely communicate. An important gesture because of its descriptive power and frequency of use is pointing. To produce a more natural and powerful human-robot interface, we have developed a purposive visual architecture called Perseus and have used it to locate objects a person is pointing to. With Perseus, in real-time, we are able to determine when a person enters the scene, track the relevant parts of the person including the hands and head, and recognize when she is pointing. Once the person points, the object pointed to is located. The Perseus architecture allows knowledge about the task and context to be used at all levels of visual analysis for improved performance. This knowledge is explicitly represented in the Perseus system to facilitate the extension of Perseus to other tasks and environments. In this paper we describe Perseus and how it is used to solve this task. Previous work on Perseus is extended by describing a more sophisticated representation of the person and superior segmenting and tracking methods. Experiments showing the success of the Perseus system with numerous naive users in varied environments is presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. L. Barron, D. J. Fleet, S. S. Beauchemin, and T. A. Burkitt. </author> <title> Performance of optical flow techniques. </title> <journal> Computer Vision and Pattern Recognition, </journal> <pages> pages 236242, </pages> <year> 1992. </year>
Reference-contexts: pyramid the following 5 types of early maps are computed based on the parameters specified by the ORs and markers (see figures 0.2 and 0.3): * Intensity - greyscale map describing each pixels intensity * Edge describes which points in the scene are edges [3]. * Motion describes the motion <ref> [1, 28] </ref> of each point in the scene. * Disparity computes the disparity [5] of each point in the scene. * Color performs color histogram backprojection [23, 13, 12] over the scene. 0.3.3 Object Representations The pointing task requires representations for various objects like the person and the item being pointed
Reference: [2] <author> R. P. Bonasso, E. Huber, and D. Kortenkamp. </author> <title> Recognizing and interpreting gestures within the context of an intelligent robot control architecture. </title> <booktitle> AAAI Fall Symposium on Embodied Language and Action, </booktitle> <year> 1995. </year>
Reference: [3] <author> J. F. Canny. </author> <title> A computational approach to edge detection. </title> <journal> Pattern Analysis and Machine Intelligence, </journal> <volume> 8(6), </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: From this pyramid the following 5 types of early maps are computed based on the parameters specified by the ORs and markers (see figures 0.2 and 0.3): * Intensity - greyscale map describing each pixels intensity * Edge describes which points in the scene are edges <ref> [3] </ref>. * Motion describes the motion [1, 28] of each point in the scene. * Disparity computes the disparity [5] of each point in the scene. * Color performs color histogram backprojection [23, 13, 12] over the scene. 0.3.3 Object Representations The pointing task requires representations for various objects like the
Reference: [4] <author> D. Chapman. </author> <title> Vision, Instruction, and Action. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: While addressing each of these issues the system must still maintain real-time performance. The Perseus system (see figure 0.1) is composed of 6 types of components: feature maps, object representations (ORs), markers <ref> [4] </ref>, visual routines [26], a segmentation map, and a long term visual memory (LTVM). Here we give a brief overview of each type of component; in the following sections we will discuss them in detail. A reactive execution system [8] interfaces to Perseus by calling visual routines [26, 21, 4]. <p> Here we give a brief overview of each type of component; in the following sections we will discuss them in detail. A reactive execution system [8] interfaces to Perseus by calling visual routines <ref> [26, 21, 4] </ref>. Visual routines are the top level structures in the Perseus system, addressing complex visual tasks like waiting for a person to enter the scene or finding the area a person points to. When visual routines are called, they are passed parameters. <p> Another important part of the pointing task is tracking; the person's head and hands must be tracked to recognize when the pointing gesture occurs. When an OR needs to track something, it instantiates a marker <ref> [4] </ref> and parameterizes the marker with a tracking function. Visual routines and ORs may then query the marker when they need to know where the object is. Markers track objects until either they lose them or a signal is received explicitly saying to stop tracking. <p> Thus specialization allows the higher level planner to refer to a particular object in the world and to have a reference to the visual representation of that object. 0.3.4 Markers Markers <ref> [4] </ref> track an object or a subpart of an object. They are created by ORs and used by ORs and visual routines when the retinotopic coordinates of an object are needed. When a marker is created, it is instantiated with a function for tracking the object.
Reference: [5] <author> I. J. Cox, S. Hingorani, B. M. Maggs, and S. B. Rao. </author> <title> Stereo without disparity gradient smoothing: a bayesian sensor fusion solution. </title> <booktitle> British Machine Vision Conference, </booktitle> <pages> pages 337346, </pages> <year> 1992. </year>
Reference-contexts: parameters specified by the ORs and markers (see figures 0.2 and 0.3): * Intensity - greyscale map describing each pixels intensity * Edge describes which points in the scene are edges [3]. * Motion describes the motion [1, 28] of each point in the scene. * Disparity computes the disparity <ref> [5] </ref> of each point in the scene. * Color performs color histogram backprojection [23, 13, 12] over the scene. 0.3.3 Object Representations The pointing task requires representations for various objects like the person and the item being pointed to.
Reference: [6] <author> T. Darrell, P. Maes, B. Blumberg, and A. Pentland. </author> <title> A novel environment for situated vision and behavior. Workshop On Visual Behaviors: </title> <journal> Computer Vision and Pattern Recognition, </journal> <pages> pages 6872, </pages> <year> 1994. </year>
Reference-contexts: In order to be useful, a gesture recognition system must run in real-time. One such system under development is the ALIVE system <ref> [6] </ref>. ALIVE provides a magic-mirror that allows the user to see himself in a simulated environment with artificial agents.
Reference: [7] <author> E. D. Dickmanns and V. Graefe. </author> <title> Dynamic monocular machine vision. Machine Vision and Applications, </title> <address> 1:223240, </address> <year> 1988. </year>
Reference-contexts: This involves using vision to perform a variety of functions in a non-engineered environment in real-time <ref> [15, 7] </ref>. To improve its performance, Perseus uses knowledge about the task and environment. The first part of the task requires recognizing if a person is in the scene and locating him.
Reference: [8] <author> R. J. Firby. </author> <title> Adaptive Execution in Complex Dynamic Worlds. </title> <type> PhD thesis, </type> <institution> Yale, </institution> <year> 1989. </year>
Reference-contexts: Furthermore, knowledge about the environment and task is explicitly represented in such a way that it can easily be re-used in tasks other than pointing. Finally, Perseus provides a clean interface to symbolic higher level systems like the RAP reactive execution system <ref> [8] </ref>. In this paper we describe the Perseus architecture in detail and show how it can be used to take advantage of situational knowledge to better understand the pointing gesture. <p> Here we give a brief overview of each type of component; in the following sections we will discuss them in detail. A reactive execution system <ref> [8] </ref> interfaces to Perseus by calling visual routines [26, 21, 4]. Visual routines are the top level structures in the Perseus system, addressing complex visual tasks like waiting for a person to enter the scene or finding the area a person points to. <p> This type of interface is essential for Perseus be used by a symbolic planning and execution systems like the RAP system <ref> [8] </ref>. Initially the LTVM contains an entry for each type of OR in the Perseus system. These ORs have not been specialized. New ORs may be added to LTVM by the higher level system, visual routines, or methods.
Reference: [9] <author> R. J. Firby, R. E. Kahn, P. N. Prokopowicz, and M. J. Swain. </author> <title> An architecture for vision and action. </title> <booktitle> International Joint Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: This allows it to understand the scene more accurately and efficiently. A third issue of importance in the design of Perseus is that it have a clean interface to a higher level system, such as a reactive execution system, so that it may be embedded into real-world applications <ref> [9] </ref>. While addressing each of these issues the system must still maintain real-time performance. The Perseus system (see figure 0.1) is composed of 6 types of components: feature maps, object representations (ORs), markers [4], visual routines [26], a segmentation map, and a long term visual memory (LTVM).
Reference: [10] <author> R. J. Firby, R. E. Kahn, P. N. Prokopowicz, and M. J. Swain. </author> <title> Collecting trash: A test of purposive vision. </title> <booktitle> Workshop on Vision for Robots, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: This is accomplished by locating small closed regions in the edge map. Once an object is located, certain properties of the region are measured and stored in the OR. These properties include the regions height, width, aspect ratio, edge density, and intensity <ref> [10] </ref>. The locate method then places a marker on the region parameterized with a tracking function that continually re-locates the region by checking that its properties remain consistent. Subsequent calls to locate will also require that the regions properties be similar to those stored in the OR. <p> The objects found include soda cans or various brands, paper and plastic cups of differing colors, and crum 11 pled pieces of paper <ref> [10] </ref>. The people using the system varied in appearance, clothing, and height. When testing the system objects were successfully found when people pointed to their side, when facing the objects, in front of them, and behind them 0.4.
Reference: [11] <author> W. T. Freeman and C. D. Weissman. </author> <title> Television control by hand gestures. </title> <type> Technical Report TR-94-24, </type> <institution> Mitsubishi Electronic Research Laboratories, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The background, floor, and lights ORs are used to aid in segmentation for the person OR. The small-isolated-object OR represents the object to be pointed to. In the following section we describe the implementation of these ORs. * Background The background OR represents the permanent objects in the scene <ref> [11, 18] </ref>. Since people are the only moving objects in our environment, this includes all points except those belonging to people. The background representation [11, 18] has a single method called segment which continuously locates all background points and updates the global segmentation map with the background segmentation. <p> In the following section we describe the implementation of these ORs. * Background The background OR represents the permanent objects in the scene <ref> [11, 18] </ref>. Since people are the only moving objects in our environment, this includes all points except those belonging to people. The background representation [11, 18] has a single method called segment which continuously locates all background points and updates the global segmentation map with the background segmentation.
Reference: [12] <author> B. V. Funt and G. D. Finlayson. </author> <title> Color constant color indexing. </title> <journal> IEEE Pattern Analysis and Machine Intelligence, </journal> <note> in press. </note>
Reference-contexts: One such map uses the color histogram backprojection <ref> [23, 12, 13] </ref> algorithm to compute feature values. In this map the features are points whose colors signify the existence of a particular object of interest, and therefore it must be parameterized with these colors. <p> Intensity - greyscale map describing each pixels intensity * Edge describes which points in the scene are edges [3]. * Motion describes the motion [1, 28] of each point in the scene. * Disparity computes the disparity [5] of each point in the scene. * Color performs color histogram backprojection <ref> [23, 13, 12] </ref> over the scene. 0.3.3 Object Representations The pointing task requires representations for various objects like the person and the item being pointed to.
Reference: [13] <author> G. Healey and D. Slater. </author> <title> Using illumination invariant color histogram descriptors for recognition. </title> <journal> Computer Vision and Pattern Recognition, </journal> <pages> pages 355360, </pages> <year> 1994. </year>
Reference-contexts: One such map uses the color histogram backprojection <ref> [23, 12, 13] </ref> algorithm to compute feature values. In this map the features are points whose colors signify the existence of a particular object of interest, and therefore it must be parameterized with these colors. <p> Intensity - greyscale map describing each pixels intensity * Edge describes which points in the scene are edges [3]. * Motion describes the motion [1, 28] of each point in the scene. * Disparity computes the disparity [5] of each point in the scene. * Color performs color histogram backprojection <ref> [23, 13, 12] </ref> over the scene. 0.3.3 Object Representations The pointing task requires representations for various objects like the person and the item being pointed to.
Reference: [14] <author> I. Horswill. Polly: </author> <title> A vision-based artificial agent. </title> <journal> American Association for Artificial Intelligence, </journal> <year> 1993. </year>
Reference-contexts: This segmentation plays an important role in segmenting the person by marking points that are not part of her. * Floor Non-textured floors are represented by the floor OR <ref> [14] </ref>. This OR has a segment method used to continuously segment the floor from the rest of the scene and update the global segmentation map with the floor segmentation.
Reference: [15] <author> I. Horswill. </author> <title> Specialization of Perceptual Processes. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference-contexts: This involves using vision to perform a variety of functions in a non-engineered environment in real-time <ref> [15, 7] </ref>. To improve its performance, Perseus uses knowledge about the task and environment. The first part of the task requires recognizing if a person is in the scene and locating him.
Reference: [16] <author> D. P. Huttenlocher and W. J. Rucklidge. </author> <title> A multi-resolution technique for comparing images using the haussdorf distance. </title> <type> Technical Report CUCS TR 92-1321, </type> <institution> Department of Computer Science Cornell University, </institution> <year> 1992. </year>
Reference-contexts: More sophisticated human-robot interactions such as handing objects to the robot are being investigated. In addition to increasing the number of gestures Perseus can recognize, the number of ORs should be increased so it can identify more than just small isolated objects. We currently have Haussdorf <ref> [16] </ref> based ORs for recognizing trash bins, doors and signs. Other useful ORs may recognize other robots, chairs, and desks. 0.7 Conclusion We have presented the Perseus system, a purposive visual architecture that has been used to recognize the pointing gesture.
Reference: [17] <author> T. Poggio K. Sung. </author> <title> Example-based learning for view-based human face detection. </title> <type> Technical Report 1521, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1994. </year>
Reference-contexts: Foveating on the hand with a zoom lens when tracking would enable us to use more sophisticated tracking methods that can track the hand as it moves in front of the body. Another interesting addition to the person OR would involve allowing it to acquire facial information <ref> [17] </ref> so it could be specialized to recognize particular people. With this capability the robotic system would be able to interact with multiple people in a more sensible way. The ability to locate and track the persons head and hands makes recognizing the pointing gesture quite simple.
Reference: [18] <author> R. E. Kahn and M. J. Swain. </author> <title> Background detection for segmentation. Animate Agent Project Working Note 8, </title> <institution> University of Chicago, </institution> <month> Augest </month> <year> 1995. </year>
Reference-contexts: The background, floor, and lights ORs are used to aid in segmentation for the person OR. The small-isolated-object OR represents the object to be pointed to. In the following section we describe the implementation of these ORs. * Background The background OR represents the permanent objects in the scene <ref> [11, 18] </ref>. Since people are the only moving objects in our environment, this includes all points except those belonging to people. The background representation [11, 18] has a single method called segment which continuously locates all background points and updates the global segmentation map with the background segmentation. <p> In the following section we describe the implementation of these ORs. * Background The background OR represents the permanent objects in the scene <ref> [11, 18] </ref>. Since people are the only moving objects in our environment, this includes all points except those belonging to people. The background representation [11, 18] has a single method called segment which continuously locates all background points and updates the global segmentation map with the background segmentation.
Reference: [19] <author> R. E. Kahn and M. J. Swain. </author> <title> Understanding people pointing: </title> <booktitle> The perseus system. International Symposium on Computer Vision, </booktitle> <month> November </month> <year> 1995. </year> <month> 15 </month>
Reference-contexts: Additionally, people are able to point to an object more accurately than they can give a verbal description of its location. To produce a more efficient, accurate, and natural human-machine interface we have used the Perseus architecture <ref> [19] </ref> to implement a system that is able to locate a person in a scene, track his significant body parts such as his head and hands, notice when he points, and identify which object he points to. <p> In this paper we describe the Perseus architecture in detail and show how it can be used to take advantage of situational knowledge to better understand the pointing gesture. This paper extends our previous work with Perseus <ref> [19] </ref> by presenting a more sophisticated representation of the person that includes the shape of his body and arms as well as the relative position of the arms, body, head and feet. Better methods for tracking the hands and head are also presented.
Reference: [20] <author> R. E. Kahn, M. J. Swain, and R. J. Firby. </author> <title> The datacube server. Animate Agent Project Working Note 2, </title> <institution> University of Chicago, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: The computation of our feature maps is performed primarily on a DataCube MV200 using the DataCube Server <ref> [20] </ref>. Using this machine we grab frames from the cameras and compute a Gaussian pyramid with 5 levels of resolution. Each level in the pyramid is half the height and width of the previous level.
Reference: [21] <author> M. H. J. Romanycia. </author> <title> The design and control of visual routines for the computation of simple geometric properties and relations. </title> <type> Technical Report 87-34, </type> <institution> University of British Colombia Department of Computer Science, </institution> <year> 1987. </year>
Reference-contexts: Here we give a brief overview of each type of component; in the following sections we will discuss them in detail. A reactive execution system [8] interfaces to Perseus by calling visual routines <ref> [26, 21, 4] </ref>. Visual routines are the top level structures in the Perseus system, addressing complex visual tasks like waiting for a person to enter the scene or finding the area a person points to. When visual routines are called, they are passed parameters.
Reference: [22] <author> A. J. Roy and R. J. Firby. </author> <title> Resolving noun phrases with a situated agent. </title> <journal> Submitted to American Association of Artificial Intelligence, </journal> <year> 1996. </year>
Reference-contexts: Here we describe the visual routines used in the pointing task. There are multiple circumstances that could lead to the higher level system looking for an object pointed to. A natural language system, such as Romper <ref> [22] </ref>, may receive a command like, pick up the Pepsi can, which refers to a particular object. Alternately, the interaction may involve a more vague reference like pick that up.
Reference: [23] <author> M. J. Swain and D. H. Ballard. </author> <title> Color indexing. </title> <journal> International Journal of Computer Vision, </journal> <volume> 7:1132, </volume> <year> 1991. </year>
Reference-contexts: One such map uses the color histogram backprojection <ref> [23, 12, 13] </ref> algorithm to compute feature values. In this map the features are points whose colors signify the existence of a particular object of interest, and therefore it must be parameterized with these colors. <p> Intensity - greyscale map describing each pixels intensity * Edge describes which points in the scene are edges [3]. * Motion describes the motion [1, 28] of each point in the scene. * Disparity computes the disparity [5] of each point in the scene. * Color performs color histogram backprojection <ref> [23, 13, 12] </ref> over the scene. 0.3.3 Object Representations The pointing task requires representations for various objects like the person and the item being pointed to.
Reference: [24] <author> A. Treisman and G. Gelade. </author> <title> A feature-integration theory of attention. </title> <journal> Cognitive Psychology, </journal> <volume> 12:97 136, </volume> <year> 1980. </year>
Reference-contexts: These few types of features are used by the many independent ORs and markers in the pointing task. To efficiently share information about features, the lowest level of the Perseus system is a set of feature maps <ref> [24, 25] </ref>. Some of the functions ORs and markers use the feature maps for are: detecting the person, segmenting him from the background, locating his face, tracking his hands, and identifying the object pointed to. These maps are retinotopic interpretations of the scene that describe the location of features.
Reference: [25] <author> J. K. Tsotsos. </author> <title> Analyzing vision at the complexity level. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 13:423469, </volume> <year> 1990. </year>
Reference-contexts: These few types of features are used by the many independent ORs and markers in the pointing task. To efficiently share information about features, the lowest level of the Perseus system is a set of feature maps <ref> [24, 25] </ref>. Some of the functions ORs and markers use the feature maps for are: detecting the person, segmenting him from the background, locating his face, tracking his hands, and identifying the object pointed to. These maps are retinotopic interpretations of the scene that describe the location of features.
Reference: [26] <author> S. Ullman. </author> <title> Visual routines. </title> <journal> Cognition, </journal> <volume> 18:97159, </volume> <year> 1984. </year>
Reference-contexts: While addressing each of these issues the system must still maintain real-time performance. The Perseus system (see figure 0.1) is composed of 6 types of components: feature maps, object representations (ORs), markers [4], visual routines <ref> [26] </ref>, a segmentation map, and a long term visual memory (LTVM). Here we give a brief overview of each type of component; in the following sections we will discuss them in detail. A reactive execution system [8] interfaces to Perseus by calling visual routines [26, 21, 4]. <p> Here we give a brief overview of each type of component; in the following sections we will discuss them in detail. A reactive execution system [8] interfaces to Perseus by calling visual routines <ref> [26, 21, 4] </ref>. Visual routines are the top level structures in the Perseus system, addressing complex visual tasks like waiting for a person to enter the scene or finding the area a person points to. When visual routines are called, they are passed parameters. <p> The two resulting ORs have the same internal can properties as data, but also contain different markers. Hence they represent two instances of the same type of object. 0.3.7 Visual Routines The interface between Perseus and the higher level system is a library of visual routines <ref> [26] </ref>. Visual routines use ORs, markers, and other visual routines to perform visual tasks. When a visual routine is called, it is passed a set of parameters which may include symbols referencing ORs in the LTVM.
Reference: [27] <author> A. Wilson and A. Bobick. </author> <title> Configuration states for the representation and recognition of gesture. </title> <booktitle> International Workshop on Automatic Face and Gesture Recognition, </booktitle> <year> 1995. </year>
Reference-contexts: Also, unless the arms and hands are extended from the body in particular positions, the proximity spaces collapse to her shoulders. This representation may require significant modifications if it were to be used for other tasks or to represent other gestures. Wilson and Bobick <ref> [27] </ref> model gestures as view-point dependent motions of the body used for communication. Since gestures are intended to communicate information, they assume people will actively try to make them easy to understand by orienting them in a standard way with respect to the intended recipient.
Reference: [28] <author> J. Woodfill. </author> <title> Motion Vision and Tracking for Robots in Dynamic, Unstructured Environments. </title> <type> PhD thesis, </type> <institution> Standford University, </institution> <year> 1992. </year>
Reference-contexts: pyramid the following 5 types of early maps are computed based on the parameters specified by the ORs and markers (see figures 0.2 and 0.3): * Intensity - greyscale map describing each pixels intensity * Edge describes which points in the scene are edges [3]. * Motion describes the motion <ref> [1, 28] </ref> of each point in the scene. * Disparity computes the disparity [5] of each point in the scene. * Color performs color histogram backprojection [23, 13, 12] over the scene. 0.3.3 Object Representations The pointing task requires representations for various objects like the person and the item being pointed <p> If the head is verified, a marker is placed on it with a tracking method that continually performs correlation based tracking in a small region surrounding the head; the motion map is used to determine exactly how large the region searched by the tracking function should be <ref> [28] </ref>. If the head isn't found in the uppermost region, the other regions in the segmentation that are high enough to be part of the head are examined until either a head is detected or all regions have been inspected.
Reference: [29] <author> Christopher Wren, Ali Azarbayejani, Trevor Darrell, and Alex Pentland. Pfinder: </author> <title> Real-time tracking of the human body. Media Laboratory Perceptual Computing Section 353, </title> <institution> Massachusetts Institute of Technology, </institution> <year> 1995. </year> <month> 16 </month>
Reference-contexts: For instance, he may wave for an agent to come to him; once the agent gets close to the user in the magic-mirror he can rub its belly to make it giggle. Unlike our system, ALIVE has no explicit representation of the person <ref> [29] </ref> beyond a segmentation. The person is segmented using background subtraction and the segmentation is compared with a spatiotemporal filter corresponding to waving or pointing for gesture recognition. Wren et. al.[29] are extending the ALIVE system to use a more explicit representation of people in the Pfinder project.
References-found: 29

