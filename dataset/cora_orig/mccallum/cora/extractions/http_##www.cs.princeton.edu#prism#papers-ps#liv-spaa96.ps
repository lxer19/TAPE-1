URL: http://www.cs.princeton.edu/prism/papers-ps/liv-spaa96.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Title: Scope Consistency A Bridge between Release Consistency and Entry Consistency  
Author: Liviu Iftode, Jaswinder Pal Singh and Kai Li 
Address: Princeton, NJ 08544  
Affiliation: Department of Computer Science, Princeton University,  
Abstract: This paper proposes a new consistency model for such systems, called Scope Consistency (ScC), which offers most of the performance advantages of the EC model without requiring explicit bindings between data and synchronization variables. Instead, ScC dynamically detects the associations implied by the programmer, using a programming interface similar to that of RC or LRC. We propose two ScC protocols: one that uses hardware support for fine-grained remote writes (automatic updates or AU) and the other, an all-software protocol. We compare the AU-based ScC protocol with Automatic Update Release Consistency (AURC), a modified LRC protocol that takes also advantage of automatic update support. AURC already improves performance substantially over an all-software LRC protocol. For three of the five applications we used, ScC further improves the speedups achieved by AURC by about 10%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve, A. L. Cox, S. Dwarkadas, R. Rajamony, and W. Zwaenepoel. </author> <title> A Comparison of Entry Consistency and Lazy Release Consistency Implementation. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: We can say that EC binds data to locks explicitly while in ScC the binding is implicit and dynamically created by the memory accesses. Global Synchronization. The explicit binding in EC has difficulty including global synchronization in the model <ref> [1] </ref>. The alternatives are: (1) not to bind data to barriers at all; (2) to have explicit binding as for locks; (3) to bind the entire address space. The first approach substantially limits the intuition of shared memory, since barriers do not cause modifications to become visible. <p> On the other hand, although the communication volume may be higher in ScC than in EC due to page fragmentation, the page size transfers allow ScC to benefit from potential prefetching <ref> [1] </ref>. Programming Effort. ScC is potentially easier to program than EC because in most cases it doesn't require any change to RC programs. <p> The Memory Channel [13] is a network interface similar to SHRIMP which allows remote memory to be mapped into the local virtual address space and have writes propagated automatically, but without a corresponding local memory update. LRC and EC have been compared in recent studies <ref> [32, 1] </ref>. Although their results cannot be directly compared with ours, since they compared LRC with EC while we compared AURC with ScC, a few points are worth noting. In the cases when EC wins, the performance improvement over LRC is on average 10-20%.
Reference: [2] <author> S.V. Adve and M.D. Hill. </author> <title> Weak Ordering-A New Definition. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: unimproved ones were already ScC, while the other two applications needed program changes to comply with ScC. 2 Relaxed Consistency Models A consistency model is essentially a contract between the memory system and the parallel program which specifies the order in which memory accesses can be executed by the system <ref> [2] </ref>. Relaxed consistency models define a set of sufficient conditions or rules such that if the program obeys these rules, the system guarantees sequentially consistent memory behavior. <p> Ordinary memory accesses to different locations are ordered only with respect to synchronization accesses and can be completely reordered between them. The following two definitions are reformulated after [9, 27, 11]. A complete set of definitions for terms used in consistency models can be found in <ref> [2] </ref>. * A write is performed with respect to a process P when a read issued to the same address by P will return the value stored by that write (or a subsequent write to the same location). * A read is performed with respect to a process P when the <p> It is an important area for future research to formalize these conditions and then prove they are indeed sufficient to define ScC-correct programs (as has been done for other relaxed consistency models <ref> [2, 12] </ref>). acquire (L1) release (L1) acquire (L1) P0 P1 ScC propagates Y LRC propagates X and Y (LRC) guaranteed Y=1 guaranteed X=1 X = 1 a = Y Example II RC to ScC, using LRC as an example.
Reference: [3] <author> B.N. Bershad and M.J. Zekauskas. Midway: </author> <title> Shared Memory Parallel Programming with Entry Consistency for Distributed Memory Multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Systems that preserve coherence at a large granularity, such as shared virtual memory systems, tend to suffer from false sharing and extra communication. Several relaxed memory consistency models <ref> [11, 8, 20, 3] </ref> have been proposed to alleviate the performance in shared virtual memory systems. However, models with increasing effectiveness also increase programming complexity. The challenge is to define models that maximize the performance benefits of relaxed consistency while minimizing the additional programming requirements. <p> The consistency model can be further relaxed by exploiting the association of shared data objects with synchronization variables (locks or barriers). For example, Entry consistency (EC) <ref> [3] </ref> lets the acquire operation obtain modifications only to the data that have been explicitly associated with the acquired synchronization variable | not all the shared data that have been modified by other processors | thus reducing unnecessary traffic compared to LRC or AURC. <p> This is particularly important in systems that preserve coherence in software and at a large granularity. Unfortunately, relaxed consistency models increase programming complexity, leading to a tradeoff with performance. In the following subsections we revisit release consistency [11] and entry consistency <ref> [3] </ref>, two important consistency models which are supported in software shared memory systems. 2.1 Release Consistency The Release Consistency (RC) model was initially introduced for hardware cache-coherent multiprocessor [11]. It has since become the cornerstone of software shared virtual memory systems [8, 20, 19, 17, 21]. <p> By not propagating modification information globally at a release, but postponing it until the next acquire, LRC further reduces the extra communication due to false sharing. 2.2 Entry Consistency The Entry Consistency (EC) model was proposed for the Midway software shared memory system <ref> [3] </ref>. Midway is a variable- or region-based rather than a page-based system. Entry consistency requires each ordinary shared variable to be explicitly associated with some synchronization variable such as a lock or barrier which protects access to that variable (Figure 2). <p> In non-exclusive mode, multiple processes can acquire the same lock but they can only read the associated variables. The consistency rules for entry consistency are the following <ref> [3] </ref>: 1. Before an acquire is allowed to perform, all updates to the guarded shared data must be performed with respect to the process. 2. When a lock is acquired in exclusive mode, no other process may acquire that lock, not even in non-exclusive mode. 3. <p> Adding more locks is however, an alternative solution to adding explicit scopes in ScC, and its performance benefit would be very similar to the one in EC. 4 Comparison with RC and EC Scope consistency is a relaxed consistency model situated between RC [11] and EC <ref> [3] </ref>. Let us compare it with each, using an LRC protocol to represent RC. 4.1 ScC versus RC Both ScC and RC do not require explicit binding of data to synchronization. However, ScC assumes an implicit binding of memory accesses to scope determined from the program structure. <p> Both schemes are particularly effective for applications that use a lot of point to point or mutually exclusive (lock) synchronization. However the solutions proposed by the two schemes are quite different. Binding. In EC <ref> [3] </ref>, the binding between data objects and synchronization objects is specified explicitly by the programmer. In fact, the solution proposed by EC is for variable-based software shared memory systems, close to object-based or region-based systems, not for transparent page-based systems. <p> The first approach substantially limits the intuition of shared memory, since barriers do not cause modifications to become visible. It requires the programmer to employ read-only locks to cause modifications to be visible after a barrier. This is the scheme used in Midway <ref> [3] </ref>. The second approach is difficult to program because it may require multiple bindings if the barriers are used to separate different phases of the computation. The third approach requires high communication in a update-based protocol like EC uses. <p> But they are also more difficult to program since the programmer must establish many more bindings for each lock. Determining a good binding granularity is the task of the programmer. The communication granularity depends on how write collection is implemented. Midway <ref> [3, 32] </ref> uses software dirty bits so the amount of communication is strictly proportional to the size of the updated data. In ScC the binding granularity is determined by the write detection granularity, which is of word size for both automatic update and all-software write detection schemes. <p> New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [20]. The TreadMarks system [19] is an example of state-of-the-art implementations of shared virtual memory on stock hardware. It uses LRC and allows for multiple writers to a page. Entry consistency <ref> [3] </ref>, a different consistency model in which shared data are explicitly associated with some synchronization variable, further reduces the impact of false sharing. Software-only techniques have been proposed to reduce false sharing itself by providing coherence in software at fine granularity.
Reference: [4] <author> R. Bisiani and M. Ravishankar. </author> <title> PLUS: A Distributed Shared-Memory System. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 115-124, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Blizzard-S [28] and Shasta [26] rewrite an existing executable file to insert a state table lookup before shared-memory references. Alternative directory-based shared virtual memory protocols for memory mapped network interfaces supporting automatic update as well as hardware remote reads were proposed for Cashmere [21]. The Plus <ref> [4] </ref>, Galactica Net [18], Merlin [25] and its successor SESAME [30] systems implement hardware-based shared memory using a type of write-through mechanism which is similar in some ways to automatic update.
Reference: [5] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: RC and LRC protocols may be implemented entirely in software or can be modified to take advantage of new network interfaces that provide hardware support for word-level automatic updates of remote copies of shared data upon writes <ref> [13, 5] </ref>. Automatic Update Release Consistency (AURC) [16] is one such protocol that improves performance substantially over all-software LRC, by using the automatic update mechanism provided in the SHRIMP network interface [5, 6]. <p> Automatic Update Release Consistency (AURC) [16] is one such protocol that improves performance substantially over all-software LRC, by using the automatic update mechanism provided in the SHRIMP network interface <ref> [5, 6] </ref>. The additional programming requirements imposed by RC-based protocols are small: Application programs simply need to mark all their synchronizations as acquire or release operations as appropriate. The consistency model can be further relaxed by exploiting the association of shared data objects with synchronization variables (locks or barriers). <p> For example if local page A at processor 0 is mapped to page B at processor 1, then all updates which are performed locally on page A are also automatically propagated to page B at node 1. The network interface of the SHRIMP multicomputer built at Princeton <ref> [5, 6, 10] </ref>, implements automatic update by mapping the selected pages on write-through caches and snooping all write traffic on the local memory bus. Writes to local memory pages which are mapped out on remote pages, are intercepted and forwarded to the remote destination by the SHRIMP network interface.
Reference: [6] <author> Matthias Blumrich, Cezary Dubnick, Edward Felten, and Kai Li. </author> <title> Protected, User-Level DMA for the SHRIMP Network Interface. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Automatic Update Release Consistency (AURC) [16] is one such protocol that improves performance substantially over all-software LRC, by using the automatic update mechanism provided in the SHRIMP network interface <ref> [5, 6] </ref>. The additional programming requirements imposed by RC-based protocols are small: Application programs simply need to mark all their synchronizations as acquire or release operations as appropriate. The consistency model can be further relaxed by exploiting the association of shared data objects with synchronization variables (locks or barriers). <p> For example if local page A at processor 0 is mapped to page B at processor 1, then all updates which are performed locally on page A are also automatically propagated to page B at node 1. The network interface of the SHRIMP multicomputer built at Princeton <ref> [5, 6, 10] </ref>, implements automatic update by mapping the selected pages on write-through caches and snooping all write traffic on the local memory bus. Writes to local memory pages which are mapped out on remote pages, are intercepted and forwarded to the remote destination by the SHRIMP network interface.
Reference: [7] <author> J. B. Carter, J. K. Bennett, and Willy Zwaenepoel. </author> <title> Techniques for Reducing Consistency-Related Communication in Distributed Shared-Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-244, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: The challenge is to define models that maximize the performance benefits of relaxed consistency while minimizing the additional programming requirements. Protocols based on Release Consistency (RC) [11] are accepted to offer a reasonable tradeoff between performance and programming complexity for shared virtual memory <ref> [7] </ref>. RC guarantees memory consistency only at synchronization points, which are marked as acquire or release operations. Modifications to shared data are globally performed [11] no later than the next release operation. <p> The release consistency model [11] reduces the impact of false sharing in shared virtual memory <ref> [8, 7] </ref>. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [20]. The TreadMarks system [19] is an example of state-of-the-art implementations of shared virtual memory on stock hardware. It uses LRC and allows for multiple writers to a page.
Reference: [8] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Systems that preserve coherence at a large granularity, such as shared virtual memory systems, tend to suffer from false sharing and extra communication. Several relaxed memory consistency models <ref> [11, 8, 20, 3] </ref> have been proposed to alleviate the performance in shared virtual memory systems. However, models with increasing effectiveness also increase programming complexity. The challenge is to define models that maximize the performance benefits of relaxed consistency while minimizing the additional programming requirements. <p> It has since become the cornerstone of software shared virtual memory systems <ref> [8, 20, 19, 17, 21] </ref>. Release consistency distinguishes between ordinary memory accesses and synchronization accesses defined as either acquire or release. To make this possible the program must be properly labeled [11] (Figure 1). Only the synchronization accesses are strictly ordered (either sequentially [22] or processor consistent [14]). <p> The release consistency model [11] reduces the impact of false sharing in shared virtual memory <ref> [8, 7] </ref>. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [20]. The TreadMarks system [19] is an example of state-of-the-art implementations of shared virtual memory on stock hardware. It uses LRC and allows for multiple writers to a page.
Reference: [9] <author> M. Dubois, C. Scheurich, and F. Briggs. </author> <title> Memory Access Buffering in Multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Only the synchronization accesses are strictly ordered (either sequentially [22] or processor consistent [14]). Ordinary memory accesses to different locations are ordered only with respect to synchronization accesses and can be completely reordered between them. The following two definitions are reformulated after <ref> [9, 27, 11] </ref>.
Reference: [10] <author> E.W. Felten, R.D. Alpert, A. Bilas, M.A. Blumrich, D.W. Clark, S. Damianakis, C. Dubnicki, L. Iftode, and K. Li. </author> <title> Early Experience with Message-Passing on the SHRIMP Mul-ticomputer. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: For example if local page A at processor 0 is mapped to page B at processor 1, then all updates which are performed locally on page A are also automatically propagated to page B at node 1. The network interface of the SHRIMP multicomputer built at Princeton <ref> [5, 6, 10] </ref>, implements automatic update by mapping the selected pages on write-through caches and snooping all write traffic on the local memory bus. Writes to local memory pages which are mapped out on remote pages, are intercepted and forwarded to the remote destination by the SHRIMP network interface.
Reference: [11] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Systems that preserve coherence at a large granularity, such as shared virtual memory systems, tend to suffer from false sharing and extra communication. Several relaxed memory consistency models <ref> [11, 8, 20, 3] </ref> have been proposed to alleviate the performance in shared virtual memory systems. However, models with increasing effectiveness also increase programming complexity. The challenge is to define models that maximize the performance benefits of relaxed consistency while minimizing the additional programming requirements. <p> However, models with increasing effectiveness also increase programming complexity. The challenge is to define models that maximize the performance benefits of relaxed consistency while minimizing the additional programming requirements. Protocols based on Release Consistency (RC) <ref> [11] </ref> are accepted to offer a reasonable tradeoff between performance and programming complexity for shared virtual memory [7]. RC guarantees memory consistency only at synchronization points, which are marked as acquire or release operations. Modifications to shared data are globally performed [11] no later than the next release operation. <p> Protocols based on Release Consistency (RC) <ref> [11] </ref> are accepted to offer a reasonable tradeoff between performance and programming complexity for shared virtual memory [7]. RC guarantees memory consistency only at synchronization points, which are marked as acquire or release operations. Modifications to shared data are globally performed [11] no later than the next release operation. Lazy Release Consistency (LRC) [20] is a relaxed implementation of RC in which coherence actions are postponed from the release to the next acquire operation. <p> This is particularly important in systems that preserve coherence in software and at a large granularity. Unfortunately, relaxed consistency models increase programming complexity, leading to a tradeoff with performance. In the following subsections we revisit release consistency <ref> [11] </ref> and entry consistency [3], two important consistency models which are supported in software shared memory systems. 2.1 Release Consistency The Release Consistency (RC) model was initially introduced for hardware cache-coherent multiprocessor [11]. It has since become the cornerstone of software shared virtual memory systems [8, 20, 19, 17, 21]. <p> In the following subsections we revisit release consistency <ref> [11] </ref> and entry consistency [3], two important consistency models which are supported in software shared memory systems. 2.1 Release Consistency The Release Consistency (RC) model was initially introduced for hardware cache-coherent multiprocessor [11]. It has since become the cornerstone of software shared virtual memory systems [8, 20, 19, 17, 21]. Release consistency distinguishes between ordinary memory accesses and synchronization accesses defined as either acquire or release. To make this possible the program must be properly labeled [11] (Figure 1). <p> initially introduced for hardware cache-coherent multiprocessor <ref> [11] </ref>. It has since become the cornerstone of software shared virtual memory systems [8, 20, 19, 17, 21]. Release consistency distinguishes between ordinary memory accesses and synchronization accesses defined as either acquire or release. To make this possible the program must be properly labeled [11] (Figure 1). Only the synchronization accesses are strictly ordered (either sequentially [22] or processor consistent [14]). Ordinary memory accesses to different locations are ordered only with respect to synchronization accesses and can be completely reordered between them. The following two definitions are reformulated after [9, 27, 11]. <p> Only the synchronization accesses are strictly ordered (either sequentially [22] or processor consistent [14]). Ordinary memory accesses to different locations are ordered only with respect to synchronization accesses and can be completely reordered between them. The following two definitions are reformulated after <ref> [9, 27, 11] </ref>. <p> value stored by that write (or a subsequent write to the same location). * A read is performed with respect to a process P when the issuing of a write to the same address by P cannot affect the value returned by the read The consistency conditions for release consistency <ref> [11] </ref> can be formulated as following: 1. Before an ordinary memory access is allowed to perform with respect to any other process, all previous acquire operations must have completed successfully. 2. <p> Adding more locks is however, an alternative solution to adding explicit scopes in ScC, and its performance benefit would be very similar to the one in EC. 4 Comparison with RC and EC Scope consistency is a relaxed consistency model situated between RC <ref> [11] </ref> and EC [3]. Let us compare it with each, using an LRC protocol to represent RC. 4.1 ScC versus RC Both ScC and RC do not require explicit binding of data to synchronization. However, ScC assumes an implicit binding of memory accesses to scope determined from the program structure. <p> The effects on overall speedup are smaller, about 12%, because of the increase in protocol overhead and communication due to explicit scopes. 7 Related Work The concept of shared virtual memory was proposed in 1986 [23, 24]. The release consistency model <ref> [11] </ref> reduces the impact of false sharing in shared virtual memory [8, 7]. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [20]. The TreadMarks system [19] is an example of state-of-the-art implementations of shared virtual memory on stock hardware.
Reference: [12] <author> P.B. Gibbons, M. Merritt, and K. Gharachorloo. </author> <title> Proving Sequential Consistency of High-Performance Shared Memories. </title> <booktitle> In Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991. </year>
Reference-contexts: It is an important area for future research to formalize these conditions and then prove they are indeed sufficient to define ScC-correct programs (as has been done for other relaxed consistency models <ref> [2, 12] </ref>). acquire (L1) release (L1) acquire (L1) P0 P1 ScC propagates Y LRC propagates X and Y (LRC) guaranteed Y=1 guaranteed X=1 X = 1 a = Y Example II RC to ScC, using LRC as an example.
Reference: [13] <author> R. Gillett, M. Collins, and D. Pimm. </author> <title> Overview of Network Memory Channel for PCI. </title> <booktitle> In Proceedings of the IEEE Spring COMPCON '96, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: RC and LRC protocols may be implemented entirely in software or can be modified to take advantage of new network interfaces that provide hardware support for word-level automatic updates of remote copies of shared data upon writes <ref> [13, 5] </ref>. Automatic Update Release Consistency (AURC) [16] is one such protocol that improves performance substantially over all-software LRC, by using the automatic update mechanism provided in the SHRIMP network interface [5, 6]. <p> Writes to local memory pages which are mapped out on remote pages, are intercepted and forwarded to the remote destination by the SHRIMP network interface. The DEC Memory Channel <ref> [13] </ref> allows modifications to be explicitly propagated at word level through I/O writes. This automatic update feature provides adequate support for a shared virtual memory implementation. AURC [16] is a protocol which implements LRC [20] with automatic update. <p> The Plus [4], Galactica Net [18], Merlin [25] and its successor SESAME [30] systems implement hardware-based shared memory using a type of write-through mechanism which is similar in some ways to automatic update. The Memory Channel <ref> [13] </ref> is a network interface similar to SHRIMP which allows remote memory to be mapped into the local virtual address space and have writes propagated automatically, but without a corresponding local memory update. LRC and EC have been compared in recent studies [32, 1].
Reference: [14] <author> James R. Goodman and Philip J. Woest. </author> <title> The Wisconsin Mul-ticube: A New Large-Scale Cache-Coherent Multiprocessor. </title> <booktitle> In Proceedings of the 15th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 422-431, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Release consistency distinguishes between ordinary memory accesses and synchronization accesses defined as either acquire or release. To make this possible the program must be properly labeled [11] (Figure 1). Only the synchronization accesses are strictly ordered (either sequentially [22] or processor consistent <ref> [14] </ref>). Ordinary memory accesses to different locations are ordered only with respect to synchronization accesses and can be completely reordered between them. The following two definitions are reformulated after [9, 27, 11].
Reference: [15] <author> S.A. Herrod. </author> <title> TangoLite; A Multiprocessor Simulation Environment. </title> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1994. </year>
Reference-contexts: These home-based protocols have been shown to substantially outperform earlier distributed approaches to LRC [17, 33]. To understand the performance implications we implemented an AU-based ScC and an AURC protocol within the TangoLite simulation framework <ref> [15] </ref>. We conducted detailed simulation studies with five Splash-2 [31] applications, as well as a synthetic benchmark designed to emphasize a communication pattern that illustrates the benefits of ScC. ScC reduces the number of page faults in the synthetic benchmark by 45%. <p> To evaluate the performance impact of scope consistency, we implemented both the AU-based ScC and AURC protocols on top of a shared virtual memory multiprocessor simulator. The simulator interfaces with the TangoLite execution-driven reference generator <ref> [15] </ref>. The architectural parameters (Table 1) we use are essentially those of the SHRIMP multicomputer, which has a network interface that supports automatic update. The simulator handles contention in detail both at the memory bus level as well as in the network interface between the incoming and outgoing traffic.
Reference: [16] <author> L. Iftode, C. Dubnicki, E. W. Felten, and Kai Li. </author> <title> Improving Release-Consistent Shared Virtual Memory using Automatic Update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: RC and LRC protocols may be implemented entirely in software or can be modified to take advantage of new network interfaces that provide hardware support for word-level automatic updates of remote copies of shared data upon writes [13, 5]. Automatic Update Release Consistency (AURC) <ref> [16] </ref> is one such protocol that improves performance substantially over all-software LRC, by using the automatic update mechanism provided in the SHRIMP network interface [5, 6]. <p> 1 Copy 2 Copy N-1 OWNER node 1 node 2 node N-1 node 1 node 2 node N-1 (a) communication from the local copies to the owner's copy (b) communication from the owner's copy to the local copies node Nnode N developed for lazy release consistency: Automatic-Update Release Consistency (AURC) <ref> [16] </ref> and Home-based Lazy Release Consistency (HLRC) [33] respectively. We begin by describing the hardware support called automatic update. 5.1 Automatic Update Automatic update can be viewed as a write-through between two local memories. <p> The DEC Memory Channel [13] allows modifications to be explicitly propagated at word level through I/O writes. This automatic update feature provides adequate support for a shared virtual memory implementation. AURC <ref> [16] </ref> is a protocol which implements LRC [20] with automatic update. <p> The acquirer uses the update lists to make its address space scope consistent by selectively invalidating the obsolete pages. 5.3 All-software ScC Protocol The hardware support for automatic update is very valuable for shared virtual memory independent of scope consistency <ref> [16, 17] </ref>, and several shared virtual memory systems have been built using this feature (see [16, 21]). However, ScC can also be built on top of an all-software home-based protocol, without automatic update support. <p> lists to make its address space scope consistent by selectively invalidating the obsolete pages. 5.3 All-software ScC Protocol The hardware support for automatic update is very valuable for shared virtual memory independent of scope consistency [16, 17], and several shared virtual memory systems have been built using this feature (see <ref> [16, 21] </ref>). However, ScC can also be built on top of an all-software home-based protocol, without automatic update support. All-software home-based LRC protocols have been shown to perform and scale much better than the traditional, homeless LRC [33]. The home-based LRC protocol (HLRC) [33] is inspired by AURC [16]. <p> However, ScC can also be built on top of an all-software home-based protocol, without automatic update support. All-software home-based LRC protocols have been shown to perform and scale much better than the traditional, homeless LRC [33]. The home-based LRC protocol (HLRC) [33] is inspired by AURC <ref> [16] </ref>. The basic scheme consists of using diffs, in the absence of AU support, to detect the updates, propagate them eagerly to the home at the release, and apply them to the home of the page. <p> Contention in the network itself is not simulated but this doesn't affect the comparison much since the amount of AU-based traffic is the same for both protocols. Results from previous studies <ref> [16, 17] </ref> show that AURC substantially outperforms the all-software "homeless" (non home-based) LRC protocol running on the same hardware (the latter does not exploit the automatic update feature). We evaluate the performance gained from ScC for different classes of sharing patterns in real applications.
Reference: [17] <author> L. Iftode, J. P. Singh, and Kai Li. </author> <title> Understanding Application Performance on Shared Virtual Memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: These home-based protocols have been shown to substantially outperform earlier distributed approaches to LRC <ref> [17, 33] </ref>. To understand the performance implications we implemented an AU-based ScC and an AURC protocol within the TangoLite simulation framework [15]. We conducted detailed simulation studies with five Splash-2 [31] applications, as well as a synthetic benchmark designed to emphasize a communication pattern that illustrates the benefits of ScC. <p> It has since become the cornerstone of software shared virtual memory systems <ref> [8, 20, 19, 17, 21] </ref>. Release consistency distinguishes between ordinary memory accesses and synchronization accesses defined as either acquire or release. To make this possible the program must be properly labeled [11] (Figure 1). Only the synchronization accesses are strictly ordered (either sequentially [22] or processor consistent [14]). <p> The acquirer uses the update lists to make its address space scope consistent by selectively invalidating the obsolete pages. 5.3 All-software ScC Protocol The hardware support for automatic update is very valuable for shared virtual memory independent of scope consistency <ref> [16, 17] </ref>, and several shared virtual memory systems have been built using this feature (see [16, 21]). However, ScC can also be built on top of an all-software home-based protocol, without automatic update support. <p> Contention in the network itself is not simulated but this doesn't affect the comparison much since the amount of AU-based traffic is the same for both protocols. Results from previous studies <ref> [16, 17] </ref> show that AURC substantially outperforms the all-software "homeless" (non home-based) LRC protocol running on the same hardware (the latter does not exploit the automatic update feature). We evaluate the performance gained from ScC for different classes of sharing patterns in real applications.
Reference: [18] <author> Andrew W. Wilson Jr. Richard P. LaRowe Jr. and Marc J. Teller. </author> <title> Hardware Assist for Distributed Shared Memory. </title> <booktitle> In Proceedings of 13th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 246-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Blizzard-S [28] and Shasta [26] rewrite an existing executable file to insert a state table lookup before shared-memory references. Alternative directory-based shared virtual memory protocols for memory mapped network interfaces supporting automatic update as well as hardware remote reads were proposed for Cashmere [21]. The Plus [4], Galactica Net <ref> [18] </ref>, Merlin [25] and its successor SESAME [30] systems implement hardware-based shared memory using a type of write-through mechanism which is similar in some ways to automatic update.
Reference: [19] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: It has since become the cornerstone of software shared virtual memory systems <ref> [8, 20, 19, 17, 21] </ref>. Release consistency distinguishes between ordinary memory accesses and synchronization accesses defined as either acquire or release. To make this possible the program must be properly labeled [11] (Figure 1). Only the synchronization accesses are strictly ordered (either sequentially [22] or processor consistent [14]). <p> Savings in page invalidations produce savings in communication cost only if the number of data fetching messages and the data traffic are proportional to the number of page faults not to the number of modifications. In traditional diff-based systems like TreadMarks <ref> [19] </ref>, the data traffic consists of diffs, thus it is proportional to the number of updates. A diff is a run-length encoding of the modifications performed by one process on a shared page. <p> When a processor which is not the home has a page miss due to the invalidation, the local copy is updated by transferring the entire page from the home on demand (see Figure 9.a). AURC uses vector timestamps <ref> [19] </ref> to partially order the synchronization intervals and the page versions. <p> Thus, a page miss does not have to go to multiple writers of the page to get the diffs, as in earlier LRC implementations like TreadMarks <ref> [19] </ref>. <p> The release consistency model [11] reduces the impact of false sharing in shared virtual memory [8, 7]. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [20]. The TreadMarks system <ref> [19] </ref> is an example of state-of-the-art implementations of shared virtual memory on stock hardware. It uses LRC and allows for multiple writers to a page. Entry consistency [3], a different consistency model in which shared data are explicitly associated with some synchronization variable, further reduces the impact of false sharing.
Reference: [20] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Systems that preserve coherence at a large granularity, such as shared virtual memory systems, tend to suffer from false sharing and extra communication. Several relaxed memory consistency models <ref> [11, 8, 20, 3] </ref> have been proposed to alleviate the performance in shared virtual memory systems. However, models with increasing effectiveness also increase programming complexity. The challenge is to define models that maximize the performance benefits of relaxed consistency while minimizing the additional programming requirements. <p> RC guarantees memory consistency only at synchronization points, which are marked as acquire or release operations. Modifications to shared data are globally performed [11] no later than the next release operation. Lazy Release Consistency (LRC) <ref> [20] </ref> is a relaxed implementation of RC in which coherence actions are postponed from the release to the next acquire operation. <p> It has since become the cornerstone of software shared virtual memory systems <ref> [8, 20, 19, 17, 21] </ref>. Release consistency distinguishes between ordinary memory accesses and synchronization accesses defined as either acquire or release. To make this possible the program must be properly labeled [11] (Figure 1). Only the synchronization accesses are strictly ordered (either sequentially [22] or processor consistent [14]). <p> P 0 writes X first and then Y ; Y is written in the critical section guarded by lock L 1 , while X is written outside it. P 1 acquires L 1 after P 0 has released it. Under LRC <ref> [20] </ref>, after acquiring L 1 P 1 is guaranteed to see all writes which occurred at P 0 before the release of L 1 , so the new value of both X and Y are guaranteed to be visible. <p> The DEC Memory Channel [13] allows modifications to be explicitly propagated at word level through I/O writes. This automatic update feature provides adequate support for a shared virtual memory implementation. AURC [16] is a protocol which implements LRC <ref> [20] </ref> with automatic update. The basic approach in AURC is to use automatic update mappings to merge updates from multiple writers on the same page into the home's copy of that page (see and the home copy is always kept up-to-date by automatic update. <p> The release consistency model [11] reduces the impact of false sharing in shared virtual memory [8, 7]. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency <ref> [20] </ref>. The TreadMarks system [19] is an example of state-of-the-art implementations of shared virtual memory on stock hardware. It uses LRC and allows for multiple writers to a page.
Reference: [21] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Using Memory-Mapped Network Interfaces to Improve the Performance of Distributed Shared Memory. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: It has since become the cornerstone of software shared virtual memory systems <ref> [8, 20, 19, 17, 21] </ref>. Release consistency distinguishes between ordinary memory accesses and synchronization accesses defined as either acquire or release. To make this possible the program must be properly labeled [11] (Figure 1). Only the synchronization accesses are strictly ordered (either sequentially [22] or processor consistent [14]). <p> lists to make its address space scope consistent by selectively invalidating the obsolete pages. 5.3 All-software ScC Protocol The hardware support for automatic update is very valuable for shared virtual memory independent of scope consistency [16, 17], and several shared virtual memory systems have been built using this feature (see <ref> [16, 21] </ref>). However, ScC can also be built on top of an all-software home-based protocol, without automatic update support. All-software home-based LRC protocols have been shown to perform and scale much better than the traditional, homeless LRC [33]. The home-based LRC protocol (HLRC) [33] is inspired by AURC [16]. <p> Blizzard-S [28] and Shasta [26] rewrite an existing executable file to insert a state table lookup before shared-memory references. Alternative directory-based shared virtual memory protocols for memory mapped network interfaces supporting automatic update as well as hardware remote reads were proposed for Cashmere <ref> [21] </ref>. The Plus [4], Galactica Net [18], Merlin [25] and its successor SESAME [30] systems implement hardware-based shared memory using a type of write-through mechanism which is similar in some ways to automatic update.
Reference: [22] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocessor Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <year> 1979. </year>
Reference-contexts: Release consistency distinguishes between ordinary memory accesses and synchronization accesses defined as either acquire or release. To make this possible the program must be properly labeled [11] (Figure 1). Only the synchronization accesses are strictly ordered (either sequentially <ref> [22] </ref> or processor consistent [14]). Ordinary memory accesses to different locations are ordered only with respect to synchronization accesses and can be completely reordered between them. The following two definitions are reformulated after [9, 27, 11].
Reference: [23] <author> K. Li. </author> <title> Shared Virtual Memory on Loosely-coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> October </month> <year> 1986. </year> <note> Tech Report YALEU-RR-492. </note>
Reference-contexts: The effects on overall speedup are smaller, about 12%, because of the increase in protocol overhead and communication due to explicit scopes. 7 Related Work The concept of shared virtual memory was proposed in 1986 <ref> [23, 24] </ref>. The release consistency model [11] reduces the impact of false sharing in shared virtual memory [8, 7]. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [20].
Reference: [24] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: The effects on overall speedup are smaller, about 12%, because of the increase in protocol overhead and communication due to explicit scopes. 7 Related Work The concept of shared virtual memory was proposed in 1986 <ref> [23, 24] </ref>. The release consistency model [11] reduces the impact of false sharing in shared virtual memory [8, 7]. New coherence protocols that improve the performance of release consistency in this context include lazy release consistency [20].
Reference: [25] <author> Creve Maples. </author> <title> A High-Performance, Memory-Based Interconnection System For Multicomputer Environments. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 295-304, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Alternative directory-based shared virtual memory protocols for memory mapped network interfaces supporting automatic update as well as hardware remote reads were proposed for Cashmere [21]. The Plus [4], Galactica Net [18], Merlin <ref> [25] </ref> and its successor SESAME [30] systems implement hardware-based shared memory using a type of write-through mechanism which is similar in some ways to automatic update.
Reference: [26] <author> D.J. Scales, K. Gharachorloo, and C.A. Thekkath. </author> <title> Shasta: A Low Overhead, Software-Only Approach for Supporting Fine-Grain Shared Memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Entry consistency [3], a different consistency model in which shared data are explicitly associated with some synchronization variable, further reduces the impact of false sharing. Software-only techniques have been proposed to reduce false sharing itself by providing coherence in software at fine granularity. Blizzard-S [28] and Shasta <ref> [26] </ref> rewrite an existing executable file to insert a state table lookup before shared-memory references. Alternative directory-based shared virtual memory protocols for memory mapped network interfaces supporting automatic update as well as hardware remote reads were proposed for Cashmere [21].
Reference: [27] <author> C. Scheurich and M. Dubois. </author> <title> Correct memory operation of cache-based multiprocessors. </title> <booktitle> In Proceedings of the 14th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 234-243, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Only the synchronization accesses are strictly ordered (either sequentially [22] or processor consistent [14]). Ordinary memory accesses to different locations are ordered only with respect to synchronization accesses and can be completely reordered between them. The following two definitions are reformulated after <ref> [9, 27, 11] </ref>.
Reference: [28] <author> I. Schoinas, B. Falsafi, A.R. Lebeck, S.K. Reinhardt, J.R. Larus, and D.A. Wood. </author> <title> Fine-grain Access for Distributed Shared Memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Entry consistency [3], a different consistency model in which shared data are explicitly associated with some synchronization variable, further reduces the impact of false sharing. Software-only techniques have been proposed to reduce false sharing itself by providing coherence in software at fine granularity. Blizzard-S <ref> [28] </ref> and Shasta [26] rewrite an existing executable file to insert a state table lookup before shared-memory references. Alternative directory-based shared virtual memory protocols for memory mapped network interfaces supporting automatic update as well as hardware remote reads were proposed for Cashmere [21].
Reference: [29] <author> J.P. Singh, J.L. Hennessy, and A. Gupta. </author> <title> Implications of Hierarchical N-Body Methods for Multiprocessor Architecture. </title> <type> Technical Report CSL-TR-92-506, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: The programs for this evaluation were selected from the Splash-2 suite [31] to cover distinct cases of interest. The problem sizes used are the default sizes indicated in Splash-2. We also developed a simplified N-body kernel representing a simple pairwise force calculation phase <ref> [29] </ref>, for which we arranged data distribution to cause a lot of false sharing (see description bellow). This is intended to artificially showcase the potential benefits of ScC.
Reference: [30] <author> Larry D. Wittie, Gudjon Hermannsson, and Ai Li. </author> <title> Eager Sharing for Efficient Massive Parallelism. </title> <booktitle> In Proceedings of the 1992 International Conference on Parall el Processing, </booktitle> <pages> pages 251-255, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Alternative directory-based shared virtual memory protocols for memory mapped network interfaces supporting automatic update as well as hardware remote reads were proposed for Cashmere [21]. The Plus [4], Galactica Net [18], Merlin [25] and its successor SESAME <ref> [30] </ref> systems implement hardware-based shared memory using a type of write-through mechanism which is similar in some ways to automatic update.
Reference: [31] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> Methodological Considerations and Characterization of the SPLASH-2 Parallel Application Suite. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: These home-based protocols have been shown to substantially outperform earlier distributed approaches to LRC [17, 33]. To understand the performance implications we implemented an AU-based ScC and an AURC protocol within the TangoLite simulation framework [15]. We conducted detailed simulation studies with five Splash-2 <ref> [31] </ref> applications, as well as a synthetic benchmark designed to emphasize a communication pattern that illustrates the benefits of ScC. ScC reduces the number of page faults in the synthetic benchmark by 45%. For three of the five real applications, ScC improves the speedup achieved by AURC by about 10%. <p> We evaluate the performance gained from ScC for different classes of sharing patterns in real applications. The programs for this evaluation were selected from the Splash-2 suite <ref> [31] </ref> to cover distinct cases of interest. The problem sizes used are the default sizes indicated in Splash-2. We also developed a simplified N-body kernel representing a simple pairwise force calculation phase [29], for which we arranged data distribution to cause a lot of false sharing (see description bellow). <p> Our N-body kernel, a simplified force calculation from an O (n) N-body algorithm using particles and space cells, displays such false sharing. It is based loosely on the Water-spatial application in Splash-2 <ref> [31] </ref>. The computational domain is divided into cells which are distributed among processors. Each processor computes the interactions of the particles belonging to its cell with particles from nearby cells.
Reference: [32] <author> M.J. Zekauskas, W.A. Sawdon, , and B.N.Bershad. </author> <title> Software Write Detection for a Distributed Shared Memory. </title> <booktitle> In Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <pages> pages 87-100, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: But they are also more difficult to program since the programmer must establish many more bindings for each lock. Determining a good binding granularity is the task of the programmer. The communication granularity depends on how write collection is implemented. Midway <ref> [3, 32] </ref> uses software dirty bits so the amount of communication is strictly proportional to the size of the updated data. In ScC the binding granularity is determined by the write detection granularity, which is of word size for both automatic update and all-software write detection schemes. <p> The Memory Channel [13] is a network interface similar to SHRIMP which allows remote memory to be mapped into the local virtual address space and have writes propagated automatically, but without a corresponding local memory update. LRC and EC have been compared in recent studies <ref> [32, 1] </ref>. Although their results cannot be directly compared with ours, since they compared LRC with EC while we compared AURC with ScC, a few points are worth noting. In the cases when EC wins, the performance improvement over LRC is on average 10-20%.
Reference: [33] <author> Y. Zhou, L. Iftode, and K. Li. </author> <title> Performance Evaluation of Two Home-Based Lazy Release Consistency Protocols for Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: These home-based protocols have been shown to substantially outperform earlier distributed approaches to LRC <ref> [17, 33] </ref>. To understand the performance implications we implemented an AU-based ScC and an AURC protocol within the TangoLite simulation framework [15]. We conducted detailed simulation studies with five Splash-2 [31] applications, as well as a synthetic benchmark designed to emphasize a communication pattern that illustrates the benefits of ScC. <p> 1 node 2 node N-1 node 1 node 2 node N-1 (a) communication from the local copies to the owner's copy (b) communication from the owner's copy to the local copies node Nnode N developed for lazy release consistency: Automatic-Update Release Consistency (AURC) [16] and Home-based Lazy Release Consistency (HLRC) <ref> [33] </ref> respectively. We begin by describing the hardware support called automatic update. 5.1 Automatic Update Automatic update can be viewed as a write-through between two local memories. <p> However, ScC can also be built on top of an all-software home-based protocol, without automatic update support. All-software home-based LRC protocols have been shown to perform and scale much better than the traditional, homeless LRC <ref> [33] </ref>. The home-based LRC protocol (HLRC) [33] is inspired by AURC [16]. The basic scheme consists of using diffs, in the absence of AU support, to detect the updates, propagate them eagerly to the home at the release, and apply them to the home of the page. <p> However, ScC can also be built on top of an all-software home-based protocol, without automatic update support. All-software home-based LRC protocols have been shown to perform and scale much better than the traditional, homeless LRC <ref> [33] </ref>. The home-based LRC protocol (HLRC) [33] is inspired by AURC [16]. The basic scheme consists of using diffs, in the absence of AU support, to detect the updates, propagate them eagerly to the home at the release, and apply them to the home of the page.
References-found: 33

