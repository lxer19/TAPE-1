URL: http://www.cns.ed.ac.uk/people/mark/papers/intro.ps
Refering-URL: http://www.cns.ed.ac.uk/people/mark/intro/intro.html
Root-URL: 
Phone: 2  3  4  
Title: Introduction to Radial Basis Function Networks  
Author: Mark J. L. Orr 
Web: http://www.cns.ed.ac.uk/people/mark.html  http://www.cns.ed.ac.uk/people/mark/intro/intro.html  http://www.cns.ed.ac.uk/people/mark/rbf.tar.Z  http://www.cns.ed.ac.uk/people/mark/manual.ps  
Note: A package of Matlab routines 3 which implement the algorithms described herein together with a PostScript user manual 4 are also available. 1  
Date: April 1996  
Address: 2, Buccleuch Place, Edinburgh EH8 9LW, Scotland  
Affiliation: Centre for Cognitive Science, University of Edinburgh,  
Abstract: This document is an introduction to radial basis function (RBF) networks, a type of artificial neural network for application to problems of supervised learning (e.g. regression, classification and time series prediction). It is available in either PostScript or hyper-text 2 . 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. M. Allen. </author> <title> The relationship between variable selection and data augmentation and a method for prediction. </title> <journal> Technometrics, </journal> <volume> 16(1) </volume> <pages> 125-127, </pages> <year> 1974. </year>
Reference-contexts: An extreme variant of this is to split the p patterns into a training set of size p 1 and a test of size 1 and average the squared error on the left-out pattern over the p possible ways of obtaining such a partition <ref> [1] </ref>. This is called leave-one-out (LOO) cross-validation. The advantage is that all the data can be used for training none has to be held back in a separate test set. <p> Let f i (x i ) be the prediction of the model for the i-th pattern in the training set (section 2) after it has been trained on the p 1 other patterns. Then leave-one-out (LOO) cross-validation <ref> [1] </ref> predicts the error variance ^ 2 1 p X (^y i f i (x i )) : Let H i be the design matrix (equation 4.3) of the reduced training set, A 1 i be its variance matrix (equation 6.7) and ^y i its output vector so that the optimal
Reference: [2] <author> L. Breiman and J. Friedman. </author> <title> Predicting multivariate responses in multiple linear regression. </title> <type> Technical report, </type> <institution> Department of Statistics, University of Cal-ifornia, Berkeley, </institution> <year> 1994. </year>
Reference-contexts: This adds more mathematics but little extra insight to the special case of univariate output so, for simplicity, we will confine our attention to the latter. Note, however, that multiple outputs can be treated in a special way in order to reduce redundancy <ref> [2] </ref>.
Reference: [3] <author> D.S. Broomhead and D. Lowe. </author> <title> Multivariate functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 321-355, </pages> <year> 1988. </year>
Reference-contexts: In principle, they could be employed in any sort of model (linear or nonlinear) and any sort of network (single-layer or 9 multi-layer). However, since Broomhead and Lowe's 1988 seminal paper <ref> [3] </ref>, radial basis function networks (RBF networks) have traditionally been associated with radial functions in a single-layer network such as shown in figure 3. f (x) t h j (x) . . . t x 1 . . . t x n * 6 @ @ @ @I w j @
Reference: [4] <author> S. Chen, C.F.N. Cowan, and P.M. Grant. </author> <title> Orthogonal least squares learning for radial basis function networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(2) </volume> <pages> 302-309, </pages> <year> 1991. </year>
Reference-contexts: That is the point at which to cease adding to the network. See section 7.4 for an illustration. 7.1 Orthogonal Least Squares Forward selection is a relatively fast algorithm but it can be speeded up even further using a technique called orthogonal least squares <ref> [4] </ref>. This is a Gram-Schmidt orthogonalisation process [12] which ensures that each new column added to the design matrix of the growing subset is orthogonal to all previous columns. This simplifies the equation for the change in sum-squared-error and results in a more efficient algorithm.
Reference: [5] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4(1) </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference-contexts: This score, which tells us how good the average prediction is, can be broken down into two components <ref> [5] </ref>, namely MSE = (y (x) hf (x)i) + (f (x) hf (x)i) E The first part is the bias and the second part is the variance. If hf (x)i = y (x) for all x then the model is unbiased (the bias is zero).
Reference: [6] <author> G.H. Golub, M. Heath, and G. Wahba. </author> <title> Generalised cross-validation as a method for choosing a good ridge parameter. </title> <journal> Technometrics, </journal> <volume> 21(2) </volume> <pages> 215-223, </pages> <year> 1979. </year>
Reference-contexts: Its cousin, generalised cross-validation (GCV) <ref> [6] </ref>, is more convenient and is ^ 2 p ^y &gt; P 2 ^y (trace (P)) 2 : (5.2) The similarity with leave-one-out cross-validation (equation 5.1) is apparent. <p> Then there are also bootstrap methods [14]. Our approach will be to use the most convenient method, as long as there are no serious objections to it, and the most convenient method is generalised cross validation (GCV) <ref> [6] </ref>. It leads to the simplest optimisation formulae, especially in local optimisation (section 6.4). Since all the model selection criteria depend nonlinearly on we need a method of nonlinear optimisation.
Reference: [7] <author> C. Gu and G. Wahba. </author> <title> Minimising GCV/GML scores with multiple smoothing parameters via the Newton method. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(2) </volume> <pages> 383-398, </pages> <year> 1991. </year>
Reference-contexts: Since all the model selection criteria depend nonlinearly on we need a method of nonlinear optimisation. We could use any of the standard techniques for this, such as the Newton method, and in fact that has been done <ref> [7] </ref>.
Reference: [8] <author> J. Hertz, A. Krough, and R.G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1991. </year>
Reference-contexts: Logistic functions, of the sort h (x) = 1 + exp (b &gt; x b 0 ) are popular in artificial neural networks, particularly in multi-layer perceptrons (MLPs) <ref> [8] </ref>. A familiar example, almost the simplest polynomial, is the straight line f (x) = a x + b which is a linear model whose two basis functions are h 1 (x) = 1 ; and whose weights are w 1 = b and w 2 = a. <p> In the 1980's, when neural networks became popular, weight decay was one of a number of techniques `invented' to help prune unimportant network connections. However, it was soon recognised <ref> [8] </ref> that weight decay involves adding the same penalty term to the sum-squared-error as in ridge regression. Weight-decay and ridge regression are equivalent. While it is admittedly crude, I like ridge regression because it is mathematically and computationally convenient and consequently other forms of regularisation are rather ignored here. <p> It is interesting to compare subset selection with the standard way of optimis-ing neural networks. The latter involves the optimisation, by gradient descent, of a nonlinear sum-squared-error surface in a high-dimensional space defined by the network parameters <ref> [8] </ref>. In RBF networks (section 3.1) the network parameters are the centres, sizes and hidden-to-output weights.
Reference: [9] <author> R.R. Hocking. </author> <title> The analysis and selection of variables in linear regression. </title> <journal> Bio-metrics, </journal> <volume> 32 </volume> <pages> 1-49, </pages> <year> 1976. </year>
Reference-contexts: However, the ordinary least squares versions of the selection criteria can always be obtained simply by setting all the regularisation parameters to zero and remembering that the projection matrix is idempotent (P 2 = P). For reviews of model selection see the two articles by Hocking <ref> [9, 10] </ref> and chapter 17 of Efron and Tisbshirani's book [14]. 5.1 Cross-Validation If data is not scarce then the set of available input-output measurements can be divided into two parts one part for training and one part for testing.
Reference: [10] <author> R.R. Hocking. </author> <title> Developments in linear regression methodology: 1959-1982 (with discussion). </title> <journal> Technometrics, </journal> <volume> 25 </volume> <pages> 219-249, </pages> <year> 1983. </year>
Reference-contexts: However, the ordinary least squares versions of the selection criteria can always be obtained simply by setting all the regularisation parameters to zero and remembering that the projection matrix is idempotent (P 2 = P). For reviews of model selection see the two articles by Hocking <ref> [9, 10] </ref> and chapter 17 of Efron and Tisbshirani's book [14]. 5.1 Cross-Validation If data is not scarce then the set of available input-output measurements can be divided into two parts one part for training and one part for testing.
Reference: [11] <author> A.E. Hoerl and R.W. Kennard. </author> <title> Ridge regression: Biased estimation for nonorthogonal problems. </title> <journal> Technometrics, </journal> <volume> 12(3) </volume> <pages> 55-67, </pages> <year> 1970. </year>
Reference-contexts: Tikhonov's work only became widely known in the West after the publication in 1977 of his book [29]. Meanwhile, two American statisticians, Arthur Hoerl and Robert Kennard, published a paper in 1970 <ref> [11] </ref> on ridge regression, a method for solving badly conditioned linear regression problems. Bad conditioning means numerical difficulties in performing the matrix inverse necessary to obtain the variance matrix (equation 4.4).
Reference: [12] <author> R.A. Horn and C.R. Johnson. </author> <title> Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1985. </year>
Reference-contexts: See section 7.4 for an illustration. 7.1 Orthogonal Least Squares Forward selection is a relatively fast algorithm but it can be speeded up even further using a technique called orthogonal least squares [4]. This is a Gram-Schmidt orthogonalisation process <ref> [12] </ref> which ensures that each new column added to the design matrix of the growing subset is orthogonal to all previous columns. This simplifies the equation for the change in sum-squared-error and results in a more efficient algorithm. <p> The inverse of an orthogonal matrix is its own transpose. If V is orthogonal then V &gt; V = V V &gt; = I m : Any matrix can be decomposed into the product of two orthogonal matrices and a diagonal one. This is called singular value decomposition (SVD) <ref> [12] </ref>. For example, the design matrix (equation 4.3) H 2 R pfim decomposes into H = U V &gt; ; where U 2 R pfip and V 2 R mfim are orthogonal and 2 R pfim is diagonal. <p> square of size m and none of its columns (rows) are linear combinations of its other columns (rows) then there exists a unique matrix, A 1 , called the inverse of A, which satisfies A 1 A = I m ; The following are two useful lemmas for matrix inversion <ref> [12] </ref>. They find frequent use whenever the design matrix (equation 4.3) is partitioned, as for example, in any of the incremental operations (section 4.3).
Reference: [13] <author> A.S. Weigend, </author> <title> A.S.M. Mangeas, and A.N. Srivastava. Nonlinear gated experts for time series: Discovering regimes and avoiding overfitting. </title> <journal> International Journal of Neural Systems, </journal> <volume> 6 </volume> <pages> 373-399, </pages> <year> 1995. </year>
Reference-contexts: This is one of a number of complications which make time series prediction a more difficult problem than straight regression or classification. Others include regime switching and asynchronous sampling <ref> [13] </ref>. 7 3 Linear Models A linear model for a function y (x) takes the form f (x) = j=1 The model f is expressed as a linear combination of a set of m fixed functions (often called basis functions by analogy with the concept of a vector being composed of
Reference: [14] <author> B. Efron and R.J. Tibshirani. </author> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman and Hall, </publisher> <year> 1993. </year>
Reference-contexts: For reviews of model selection see the two articles by Hocking [9, 10] and chapter 17 of Efron and Tisbshirani's book <ref> [14] </ref>. 5.1 Cross-Validation If data is not scarce then the set of available input-output measurements can be divided into two parts one part for training and one part for testing. In this way several different models, all trained on the training set, can be compared on the test set. <p> GCV is one of a number of criteria which all involve an adjustment to the average mean-squared-error over the training set (equation 4.8). There are several other criteria which share this form <ref> [14] </ref>. The unbiased estimate of variance (UEV), which we met in a previous section (4.4), is ^ 2 ^y &gt; P 2 ^y ; (5.3) where fl is the effective number of parameters (equation 4.10). <p> The popular choices are leave-one-out cross-validation, generalised cross-validation, final prediction error and Bayesian information criterion. Then there are also bootstrap methods <ref> [14] </ref>. Our approach will be to use the most convenient method, as long as there are no serious objections to it, and the most convenient method is generalised cross validation (GCV) [6]. It leads to the simplest optimisation formulae, especially in local optimisation (section 6.4). <p> In this case it is probably necessary to resort to bootstrap techniques <ref> [14] </ref> to estimate ^ W. 44 A.6 The Projection Matrix The prediction of the output at any of the training set inputs by the linear model (section 3) using equation (equation 4.1) for the weight vector is f (x i ) = j=1 = h &gt; where h &gt; i is
Reference: [15] <author> B. Fritzke. </author> <title> Growing cell structures a self-organizing network for unsupervised and supervised learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(9) </volume> <pages> 1441-1460, </pages> <year> 1994. </year> <month> 66 </month>
Reference-contexts: Most of the mathematical details are put in an appendix (section A). 5 For alternative approaches see, for example, the work of Platt [24] and associates [21] and of Fritzke <ref> [15] </ref>. 4 2 Supervised Learning A ubiquitous problem in statistics with applications in many areas is to guess or estimate a function from some example input-output pairs with little or no knowledge of the form of the function.
Reference: [16] <author> C. Bishop. </author> <title> Improving the generalisation properties of radial basis function neural networks. </title> <journal> Neural Computation, </journal> <volume> 3(4) </volume> <pages> 579-588, </pages> <year> 1991. </year>
Reference-contexts: While it is admittedly crude, I like ridge regression because it is mathematically and computationally convenient and consequently other forms of regularisation are rather ignored here. If the reader is interested in higher-order regularisation I suggest looking at [25] for a general overview and <ref> [16] </ref> for a specific example (second-order regularisation in RBF networks). We next describe ridge regression from the perspective of bias and variance (section 6.1) and how it affects the equations for the optimal weight vector (appendix A.4), the variance matrix (appendix A.5) and the projection matrix (appendix A.6).
Reference: [17] <author> D.J.C. MacKay. </author> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 415-447, </pages> <year> 1992. </year>
Reference-contexts: However, things are not as simple when ridge regression is used. Although there are still m weights in the model, what John Moody calls the effective number of parameters [18] (and David MacKay calls the number of good parameter measurements <ref> [17] </ref>) is less than m and depends on the size of the regularisation parameter (s). The simplest formula for this number, fl, is fl = p trace (P) ; (4.10) which is consistent with both Moody's and MacKay's formulae, as shown in appendix A.8. <p> In standard ridge regression (section 6.1) A 1 = H &gt; H + I m so the effective number of parameters (equation 4.10) is fl = trace = trace = trace = m trace This is the same as MacKay's equation (4.9) in <ref> [17] </ref>.
Reference: [18] <author> J.E. Moody. </author> <title> The effective number of parameters: An analysis of generalisation and regularisation in nonlinear learning systems. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P. Lippmann, editors, </editor> <booktitle> Neural Information Processing Systems 4, </booktitle> <pages> pages 847-854. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1992. </year>
Reference-contexts: However, things are not as simple when ridge regression is used. Although there are still m weights in the model, what John Moody calls the effective number of parameters <ref> [18] </ref> (and David MacKay calls the number of good parameter measurements [17]) is less than m and depends on the size of the regularisation parameter (s). <p> If the eigenvalues of the matrix H &gt; H are f j g m j=1 then fl = m trace m X 1 = j=1 1 + j = j=1 + j which is the same as Moody's equation (18) in <ref> [18] </ref>. 52 A.9 Leave-one-out Cross-validation We prove equation 5.1 for leave-one-out (LOO) cross-validation error prediction. Let f i (x i ) be the prediction of the model for the i-th pattern in the training set (section 2) after it has been trained on the p 1 other patterns.
Reference: [19] <author> M.J.L. Orr. </author> <title> Local Smoothing of Radial Basis Function Networks (long version). </title> <booktitle> In International Symposium on Artificial Neural Networks, </booktitle> <address> Hsinchu, Taiwan, </address> <year> 1995. </year>
Reference-contexts: In contrast to the case of standard ridge regression (section 6.2), there is an analytic solution for the optimal value of j based on GCV minimisation <ref> [19] </ref> - no re-estimation is necessary (see appendix A.11). The trouble is that there are m 1 other parameters to optimise and each time one j is optimised it changes 26 the optimal value of each of the others. <p> Optimising all the parameters together has to be done as a kind of re-estimation, doing one at a time and then repeating until they all converge <ref> [19] </ref>. When j = 1 the two projection matrices, P and P j are equal. This means that if the optimal value of j is 1 then the j-th basis function can be removed from the network. <p> The main difference is that any column can be removed from H m whereas in the previous section the new column was inserted in a fixed position (right at the end of H m , just after the m-th column). As noted in <ref> [19] </ref>, the projection matrix (appendix A.6) is invariant to a permutation of the columns of the design matrix (equation 4.3) which is why it does not matter in which position a new column is inserted; putting it at the end is as good a choice as any other. <p> Some increase in efficiency can be gained by updating only the quantities needed to calculate the five coefficients (a, b, c, ff and fi) such as P ^y and P H, rather than P itself <ref> [19] </ref>.
Reference: [20] <author> M.J.L. Orr. </author> <title> Regularisation in the Selection of Radial Basis Function Centres. </title> <journal> Neural Computation, </journal> <volume> 7(3) </volume> <pages> 606-623, </pages> <year> 1995. </year>
Reference-contexts: Two words of warning: they don't always work as effectively as in this example and UEV is inferior to GCV as a selection criteria <ref> [20] </ref> (and probably to the others as well). 22 6 Ridge Regression Around the middle of the 20th century the Russian theoretician Andre Tikhonov was working on the solution of ill-posed problems. <p> Since all the model selection criteria depend nonlinearly on we need a method of nonlinear optimisation. We could use any of the standard techniques for this, such as the Newton method, and in fact that has been done [7]. Alternatively <ref> [20] </ref>, we can exploit the fact that when the derivative of the GCV error prediction is set to zero, the resulting equation can be manipulated so that only ^ appears on the left hand side (see appendix A.10), ^ = ^y &gt; P 2 ^y trace ^w &gt; A 1 ^w <p> = ~ H &gt; ~ H m H &gt; and the unorthogonalised optimal weight (equation 4.5) are then related by ^w m = U 1 (see appendix A.13). 7.2 Regularised Forward Selection Forward selection and standard ridge regression (section 6.1) can be combined, leading to a modest improvement in performance <ref> [20] </ref>. In this case the key equation (7.5) involves the regularisation parameter . <p> Since new additions will cause a change in the optimal value anyway, there is little point in running the re-estimation formula (equation 6.5) to convergence. Good practical results have been obtained by performing only a single iteration of the re-estimation formula after each new selection <ref> [20] </ref>. 7.3 Regularised Orthogonal Least Squares Orthogonal least squares (section7.1) works because after factoring H m the orthog-onalised variance matrix (equation 4.4) is diagonal. <p> In this example the two methods chose similar (but not identical) subsets and ended with very close test set errors. However, on average, over a large number of similar training sets, the regularised version performs slightly better than the plain vanilla version <ref> [20] </ref>. 37 A Appendices A.1 Notational Conventions Scalars are represented by italicised letters such as y or . Vectors are represented by bold lower case letters such as x and . The first component of x (a vector) is x 1 (a scalar).
Reference: [21] <author> V. Kadirkamanathan and M. Niranjan. </author> <title> A function estimation approach to sequential learning with neural networks. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 954-975, </pages> <year> 1993. </year>
Reference-contexts: After that we cover ridge regression (section 6) in more detail and lastly we look at forward selection (section 7) for building networks. Most of the mathematical details are put in an appendix (section A). 5 For alternative approaches see, for example, the work of Platt [24] and associates <ref> [21] </ref> and of Fritzke [15]. 4 2 Supervised Learning A ubiquitous problem in statistics with applications in many areas is to guess or estimate a function from some example input-output pairs with little or no knowledge of the form of the function.
Reference: [22] <author> C. Mallows. </author> <title> Some comments on C p . Technometrics, </title> <booktitle> 15 </booktitle> <pages> 661-675, </pages> <year> 1973. </year>
Reference-contexts: The unbiased estimate of variance (UEV), which we met in a previous section (4.4), is ^ 2 ^y &gt; P 2 ^y ; (5.3) where fl is the effective number of parameters (equation 4.10). A version of Mallow's C p <ref> [22] </ref> and also a special case of Akaike's information criterion is final prediction error (FPE) ^ 2 1 UEV = p fl p Schwarz's Bayesian information criterion (BIC) [28] is ^ 2 1 UEV = p fl p Generalised-cross validation can also be written in terms of fl instead of trace
Reference: [23] <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor, editors. </editor> <title> Machine Learning: Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <address> London, </address> <year> 1994. </year>
Reference-contexts: After training the network responds to a new pattern with continuous values in each component of the output vector which can be interpreted as being proportional to class probability and used to generate a class assignment. For a comparison of various types of algorithms on different classification problems see <ref> [23] </ref>. Time series prediction is concerned with estimating the next value and future values of a sequence such as : : : ; y t3 ; y t2 ; y t1 ; ?; ?; : : : but usually not as a explicit function of time.
Reference: [24] <author> J. Platt. </author> <title> A resource-allocating network for function interpolation. </title> <journal> Neural Computation, </journal> <volume> 3(2) </volume> <pages> 213-225, </pages> <year> 1991. </year>
Reference-contexts: After that we cover ridge regression (section 6) in more detail and lastly we look at forward selection (section 7) for building networks. Most of the mathematical details are put in an appendix (section A). 5 For alternative approaches see, for example, the work of Platt <ref> [24] </ref> and associates [21] and of Fritzke [15]. 4 2 Supervised Learning A ubiquitous problem in statistics with applications in many areas is to guess or estimate a function from some example input-output pairs with little or no knowledge of the form of the function.
Reference: [25] <author> W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: We do use nonlinear optimisation but only for the regularisation parameter (s) in ridge regression (section 6) and the optimal subset of basis functions in forward selection (section 7). We also avoid the kind of expensive nonlinear gradient descent algorithms (such as the conjugate gradient and variable metric methods <ref> [25] </ref>) that are employed in explicitly nonlinear networks. <p> Bad conditioning means numerical difficulties in performing the matrix inverse necessary to obtain the variance matrix (equation 4.4). It is also a symptom of an ill-posed regression problem in Tikhonov's sense and Hoerl & Kennard's method was in fact a crude form of regu-larisation, known now as zero-order regularisation <ref> [25] </ref>. In the 1980's, when neural networks became popular, weight decay was one of a number of techniques `invented' to help prune unimportant network connections. However, it was soon recognised [8] that weight decay involves adding the same penalty term to the sum-squared-error as in ridge regression. <p> Weight-decay and ridge regression are equivalent. While it is admittedly crude, I like ridge regression because it is mathematically and computationally convenient and consequently other forms of regularisation are rather ignored here. If the reader is interested in higher-order regularisation I suggest looking at <ref> [25] </ref> for a general overview and [16] for a specific example (second-order regularisation in RBF networks).
Reference: [26] <author> J.O. Rawlings. </author> <title> Applied Regression Analysis. </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Pacific Grove, CA, </address> <year> 1988. </year>
Reference-contexts: The solution is the so-called normal equation <ref> [26] </ref>, ^w = A 1 H &gt; ^y ; (4.5) and ^w = [ ^w 1 ^w 2 : : : ^w m ] &gt; is the vector of weights which minimises the cost function (equation 4.2). The use of these equations is illustrated with a simple example (section 4.5). <p> An alternative strategy is to compare models made up of different subsets of basis functions drawn from the same fixed set of candidates. This is called subset selection in statistics <ref> [26] </ref>. To find the best subset is usually intractable | there are 2 M 1 subsets in a set of size M | so heuristics must be used to search a small but hopefully interesting fraction of the space of all subsets.
Reference: [27] <author> W.S. Sarle. </author> <title> Neural Networks and Statistical Models. </title> <booktitle> In Proceedings of the Nineteenth Annual SAS Users Group International Conference, </booktitle> <pages> pages 1538-1550, </pages> <address> Cary, NC, </address> <year> 1994. </year>
Reference-contexts: However, the fashion for neural networks, which started in the mid-80's, has given rise to new names for concepts already familiar to statisticians <ref> [27] </ref>. Table 1 gives some examples.
Reference: [28] <author> G. Schwarz. </author> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: A version of Mallow's C p [22] and also a special case of Akaike's information criterion is final prediction error (FPE) ^ 2 1 UEV = p fl p Schwarz's Bayesian information criterion (BIC) <ref> [28] </ref> is ^ 2 1 UEV = p fl p Generalised-cross validation can also be written in terms of fl instead of trace (P), using the equation for the effective number of parameters (4.10). ^ 2 p ^y &gt; P 2 ^y 20 UEV, FPE, GCV and BIC are all in
Reference: [29] <author> A.N. Tikhonov and V.Y. Arsenin. </author> <title> Solutions of Ill-Posed Problems. </title> <publisher> Winston, </publisher> <address> Washington, </address> <year> 1977. </year> <month> 67 </month>
Reference-contexts: It is necessary to supply extra information (or assumptions) and the mathematical technique Tikhonov developed for this is known as regularisation. Tikhonov's work only became widely known in the West after the publication in 1977 of his book <ref> [29] </ref>. Meanwhile, two American statisticians, Arthur Hoerl and Robert Kennard, published a paper in 1970 [11] on ridge regression, a method for solving badly conditioned linear regression problems. Bad conditioning means numerical difficulties in performing the matrix inverse necessary to obtain the variance matrix (equation 4.4). <p> This will be the case if f (x) is highly sensitive to the peculiarities (such as noise and the choice of sample points) of each particular training set and it is this sensitivity which causes regression problems to be ill-posed in the Tikhonov <ref> [29] </ref> sense. Often, however, the variance can be significantly reduced by deliberately introducing a small amount of bias so that the net effect is a reduction in mean-squared-error. Introducing bias is equivalent to restricting the range of functions for which a model can account.
References-found: 29

