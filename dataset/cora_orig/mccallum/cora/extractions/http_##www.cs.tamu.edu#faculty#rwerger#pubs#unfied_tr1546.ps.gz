URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/unfied_tr1546.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/
Root-URL: http://www.cs.tamu.edu
Email: torrella@cs.uiuc.edu, rwerger@cs.tamu.edu  
Title: A Unified Approach to Speculative Parallelization of Loops in DSM Multiprocessors  
Author: Ye Zhang, Lawrence Rauchwerger, and Josep Torrellas 
Keyword: scalable shared-memory multiprocessors, cache coherence protocols, run-time paralleliza-tion, speculative execution, reduction parallelization.  
Address: -zhang2,  
Affiliation: University of Illinois and Texas A&M University  
Abstract: Speculative parallel execution of statically non-analyzable codes on Distributed Shared-Memory (DSM) multiprocessors is challenging because of the long latency and memory distribution present. However, such an approach may well be the best way of speeding up codes whose dependences can not be compiler analyzed. In this paper, we have extended past work by proposing a hardware scheme for the speculative parallel execution of loops that have a modest number of cross-iteration dependences. In this case, when a dependence violation is detected, we locally repair the state. Then, depending on the situation, we either re-execute one out-of-order iteration or, restart parallel execution from that point on. The general algorithm, called the Unified Privatization and Reduction algorithm (UPAR), privatizes, on demand, at cache-line level, executes reductions in parallel, merges the last values and partial results of reductions on-the-fly with minimum residual work at loop end. UPAR allows for completely dynamic scheduling and does not get slowed down if the working set of an iteration is larger than the cache size. Simulations indicate good speedups relative to sequential execution. The hardware support for reduction optimizations brings, on average, 50% performance improvement and can be used both in speculative and normal execution. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Asenjo, E. Gutierrez, Y. Lin, D. Padua, B. Pottenger, and E. Zapata. </author> <title> On the automatic parallelization of sparse and irregular fortran codes. </title> <type> Technical Report 1512, </type> <institution> Center for Supercomputing Research and Development, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: On this inner loop we have applied the reduction optimization algorithm and obtained very good results. For the loop move3 goto100 in Dsmc3d, privatization removes all dependences <ref> [1] </ref> but, due to its sparse nature, causes high initialization and final merging overhead. By treating the tested array as a shared array and applying the UPAR algorithm we obtain good overall results. Spark98 is a sparse matrix and dense vector multiplication C kernel. Rmv needs only reduction optimization.
Reference: [2] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: To determine whether or not the order of the iterations affects the semantics of the loop, we need to analyze the data dependences across iterations (or cross-iteration dependences) <ref> [2] </ref>. There are three types of data dependences, namely flow (read after write), anti (write after read), and output (write after write). If there are no anti, output, or flow dependences across iterations, the loop can be executed in parallel. Such a loop is called a doall loop.
Reference: [3] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 3(3):540, </volume> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Adm and Track are Perfect Club <ref> [3] </ref> codes, Euler and Dsmc3d are HPF-2 applications [8], and Rmv is a Spark98 kernel [18], vml and mml are from Sparse BLAS library [7].
Reference: [4] <author> W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoeflinger, T. Lawrence, J. Lee, D. Padua, Y. Paek, B. Pottenger, L. Rauchwerger, and P. Tu. </author> <title> Advanced Program Restructuring for High-Performance Computers with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12):7882, </volume> <month> December </month> <year> 1996. </year>
Reference-contexts: Finally we will discuss how flow dependences are detected and the actions that the system initiates to continue a correct execution. 3.1 General System Functionality 3.1.1 At Compile Time Our system uses Polaris <ref> [4] </ref>, a state of the art Fortran parallelizing compiler that first performs a static analysis and qualifies loops as parallel, serial and potentially parallel. All arrays and scalars proven independent or privatizable by the compiler are transformed for parallel execution. <p> The modeled multiprocessor has the hardware support discussed in the previous sections. The simulated applications have been pre-processed with the Polaris <ref> [4] </ref> parallelizing compiler that has been specifically enhanced to transform selected loops for speculative run-time parallelization. The modeled architecture has 200-MHz 4-way dynamically-scheduled superscalar processor, which have 4 integer, 2 load-store, and 2 floating point function units, 32 entries instruction queue, and both 32 integer and floating point registers.
Reference: [5] <author> W. Blume and R. Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks T M Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6):643656, </volume> <month> November </month> <year> 1992. </year>
Reference-contexts: Thus, in order to realize the full potential of parallel computing it has become clear that static (compile time) analysis must be complemented by new methods capable of automatically extracting parallelism at runtime <ref> [5, 6, 9] </ref>. Runtime techniques can succeed where static compilation fails because they have complete information about the access pattern. For example, input dependent or dynamic data distribution, 1 memory accesses guarded by runtime dependent conditions, and subscript expressions can all be analyzed unambiguously at runtime.
Reference: [6] <author> W. J. Camp, S. J. Plimpton, B. A. Hendrickson, and R. W. Leland. </author> <title> Massively parallel methods for engineering and science problems. </title> <journal> Comm. ACM, </journal> <volume> 37(4):3141, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: Typical examples of applications containing such loops are complex simulations such as SPICE for circuit simulation, DYNA3D and PRONTO3D for structural mechanics modeling, GAUSSIAN and DMOL for quantum mechanical simulation of molecules, CHARMM and DISCOVER for molecular dynamics simulation of organic systems, and FIDAP for modeling complex fluid flows <ref> [6] </ref>. Thus, in order to realize the full potential of parallel computing it has become clear that static (compile time) analysis must be complemented by new methods capable of automatically extracting parallelism at runtime [5, 6, 9]. <p> Thus, in order to realize the full potential of parallel computing it has become clear that static (compile time) analysis must be complemented by new methods capable of automatically extracting parallelism at runtime <ref> [5, 6, 9] </ref>. Runtime techniques can succeed where static compilation fails because they have complete information about the access pattern. For example, input dependent or dynamic data distribution, 1 memory accesses guarded by runtime dependent conditions, and subscript expressions can all be analyzed unambiguously at runtime.
Reference: [7] <author> I. Duff, M. Marrone, G. Radiacti, and C. Vittoli. </author> <title> A set of Level 3 Basic Linear Algebra Subprograms for Sparse Matrices. </title> <type> Technical Report RAL-TR-95-049, </type> <institution> Rutherford Appleton Laboratory, </institution> <year> 1995. </year>
Reference-contexts: Adm and Track are Perfect Club [3] codes, Euler and Dsmc3d are HPF-2 applications [8], and Rmv is a Spark98 kernel [18], vml and mml are from Sparse BLAS library <ref> [7] </ref>.
Reference: [8] <author> I. Duff, R. Schreiber, and P. Havlak. </author> <title> Hpf-2 scope of activities and motivating applications. </title> <type> Technical Report CRPC-TR94492, </type> <institution> Rice University, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Adm and Track are Perfect Club [3] codes, Euler and Dsmc3d are HPF-2 applications <ref> [8] </ref>, and Rmv is a Spark98 kernel [18], vml and mml are from Sparse BLAS library [7].
Reference: [9] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science 589. Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 6583, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Thus, in order to realize the full potential of parallel computing it has become clear that static (compile time) analysis must be complemented by new methods capable of automatically extracting parallelism at runtime <ref> [5, 6, 9] </ref>. Runtime techniques can succeed where static compilation fails because they have complete information about the access pattern. For example, input dependent or dynamic data distribution, 1 memory accesses guarded by runtime dependent conditions, and subscript expressions can all be analyzed unambiguously at runtime. <p> This type of reduction is sometimes called an update. There are several known parallel methods for performing reduction operations. One method is to transform the do loop into a doall and enclose the access to the reduction variable in an unordered critical section <ref> [9, 29] </ref>, or, equivalently, perform an atomic fetch-and-add. The drawbacks of this method are that it is not scalable and that it requires potentially expensive synchronizations.
Reference: [10] <author> S. Gopal, T. N. Vijaykumar, J. E. Smith, and G. S. Sohi. </author> <title> Speculative Versioning Cache. </title> <booktitle> In Proceedings of the 4th International Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: The results with stalling on displacement are labeled as DPL in Figure 6.4.2 and cumulative in-order finish effect. 20 7 Related Work Some work related to ours is four schemes that support speculative parallelization inside a multiprocessor chip <ref> [10, 13, 11, 22] </ref>. These schemes are relatively similar to each other. The cache coherence protocol inside a chip is extended with versions or time stamps similar to ours. Parallelism is exploited by running one task (for example one loop iteration) on each of the processors on chip.
Reference: [11] <author> Lance Hammond, Mark Willey, and Kunle Olukotun. </author> <title> Data speculation support for a chip multiprocessor. </title> <booktitle> In ASPLOS8, </booktitle> <year> 1998. </year>
Reference-contexts: The results with stalling on displacement are labeled as DPL in Figure 6.4.2 and cumulative in-order finish effect. 20 7 Related Work Some work related to ours is four schemes that support speculative parallelization inside a multiprocessor chip <ref> [10, 13, 11, 22] </ref>. These schemes are relatively similar to each other. The cache coherence protocol inside a chip is extended with versions or time stamps similar to ours. Parallelism is exploited by running one task (for example one loop iteration) on each of the processors on chip.
Reference: [12] <author> Ken Kennedy. </author> <title> Compiler technology for machine-independent programming. </title> <journal> Int. J. Paral. Prog., </journal> <volume> 22(1):7998, </volume> <month> February </month> <year> 1994. </year>
Reference-contexts: Programs exhibiting this kind of behavior account for more than 50% of all Fortran applications <ref> [12] </ref> and encompass most C codes.
Reference: [13] <author> V. Krishnan and J. Torrellas. </author> <title> Hardware and Software Support for Speculative Execution of Sequential Binaries on a Chip-Multiprocessor. </title> <booktitle> In Proceedings of the 1998 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1998. </year>
Reference-contexts: The results with stalling on displacement are labeled as DPL in Figure 6.4.2 and cumulative in-order finish effect. 20 7 Related Work Some work related to ours is four schemes that support speculative parallelization inside a multiprocessor chip <ref> [10, 13, 11, 22] </ref>. These schemes are relatively similar to each other. The cache coherence protocol inside a chip is extended with versions or time stamps similar to ours. Parallelism is exploited by running one task (for example one loop iteration) on each of the processors on chip.
Reference: [14] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 869876, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: The drawbacks of this method are that it is not scalable and that it requires potentially expensive synchronizations. A scalable method can be obtained by noting that a reduction operation is an associative and 4 commutative recurrence and can thus be parallelized using a recursive doubling algorithm <ref> [14, 16] </ref>. In this case, the reduction variable is privatized in the transformed doall. A scalar is then produced using the partial results computed in each processor as operands for a reduction operation (with the same operator) across the processors (Figure 3-(c)).
Reference: [15] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> LCM: Memory system support for parallel language implementation. </title> <booktitle> In ASPLOS6, </booktitle> <pages> pages 208218, </pages> <year> 1994. </year>
Reference-contexts: Our cache line reduction optimization scheme is similar to the one proposed in <ref> [15] </ref>. 8 Conclusions Speculative parallel execution of statically non-analyzable codes on Distributed Shared-Memory (DSM) multiprocessors is challenging because of the long latency and memory distribution present. However, such an approach may well be the best way of speeding up codes whose dependences can not be compiler analyzed.
Reference: [16] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hyper-cubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year> <month> 22 </month>
Reference-contexts: The drawbacks of this method are that it is not scalable and that it requires potentially expensive synchronizations. A scalable method can be obtained by noting that a reduction operation is an associative and 4 commutative recurrence and can thus be parallelized using a recursive doubling algorithm <ref> [14, 16] </ref>. In this case, the reduction variable is privatized in the transformed doall. A scalar is then produced using the partial results computed in each processor as operands for a reduction operation (with the same operator) across the processors (Figure 3-(c)).
Reference: [17] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The cache sizes have been purposely selected so small in order to scale with the reduced working sets of the chosen applications. Real-life working sets could not be used because they would have required impractically long simulation times. The caches are kept coherent with a DASH-like cache coherence protocol <ref> [17] </ref>. Each node has part of the global memory and the corresponding section of the directory. We have modeled the contention in the whole system with the exception of the global network, which is abstracted away as a constant latency.
Reference: [18] <author> D. O'Hallaron, J. Shewchuk, and T. Gross. </author> <title> Architectural implications of a family of irregular applications. </title> <type> Technical Report CMU-CS-97-189, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> November </month> <year> 1997. </year>
Reference-contexts: Adm and Track are Perfect Club [3] codes, Euler and Dsmc3d are HPF-2 applications [8], and Rmv is a Spark98 kernel <ref> [18] </ref>, vml and mml are from Sparse BLAS library [7].
Reference: [19] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29:11841201, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: Restructuring, or parallelizing, compilers address these problems by detecting and exploiting parallelism in sequential programs written in conventional languages. Although compiler techniques for the automatic detection of parallelism have been studied extensively over the last two decades (see, e.g., <ref> [19, 24] </ref>), current parallelizing compilers cannot extract a significant fraction of the available parallelism in a loop if it has a complex and/or statically unknown access pattern. Programs exhibiting this kind of behavior account for more than 50% of all Fortran applications [12] and encompass most C codes.
Reference: [20] <author> L. Rauchwerger, N. Amato, and D. Padua. </author> <title> A scalable method for run-time loop parallelization. </title> <journal> IJPP, </journal> <volume> 26(6):537576, </volume> <month> July </month> <year> 1995. </year>
Reference-contexts: For example, input dependent or dynamic data distribution, 1 memory accesses guarded by runtime dependent conditions, and subscript expressions can all be analyzed unambiguously at runtime. Recently, new software techniques for the automatic parallelization of loops have been introduced <ref> [21, 20] </ref> which take a more aggressive approach: they speculate about the parallelism of access patterns, generate, and, then execute the loops speculatively in parallel, (alternatively they can inspect the access pattern first) and verify the validity of the parallelization by means of a parallel algorithm.
Reference: [21] <author> Lawrence Rauchwerger and David A. Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization and Reduction Parallelization. </title> <booktitle> In Proceedings of the SIGPLAN 1995 Conference on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <pages> pages 218232, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: For example, input dependent or dynamic data distribution, 1 memory accesses guarded by runtime dependent conditions, and subscript expressions can all be analyzed unambiguously at runtime. Recently, new software techniques for the automatic parallelization of loops have been introduced <ref> [21, 20] </ref> which take a more aggressive approach: they speculate about the parallelism of access patterns, generate, and, then execute the loops speculatively in parallel, (alternatively they can inspect the access pattern first) and verify the validity of the parallelization by means of a parallel algorithm. <p> The time to flush all dirty lines at the end of the loop is proportional to per processor cache size. This time can be dramatically shorter than the usual software cross-processor last value or reduction operation <ref> [21] </ref>, which is proportional to the data size (in the best case) or array dimensions (the worst case for sparse accesses). * Undo log maintenance. In order to be able to restart loop execution from any point of failure, the state of the loop's working space must be kept consistent.
Reference: [22] <author> J. G. Steffan and T. C. Mowry. </author> <title> The Potential for Using Thread-Level Data Speculation to Facilitate Automatic Parallelization. </title> <booktitle> In Proceedings of the 4th International Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: The results with stalling on displacement are labeled as DPL in Figure 6.4.2 and cumulative in-order finish effect. 20 7 Related Work Some work related to ours is four schemes that support speculative parallelization inside a multiprocessor chip <ref> [10, 13, 11, 22] </ref>. These schemes are relatively similar to each other. The cache coherence protocol inside a chip is extended with versions or time stamps similar to ours. Parallelism is exploited by running one task (for example one loop iteration) on each of the processors on chip.
Reference: [23] <author> J. Veenstra and R. Fowler. MINT: </author> <title> A Front End for Efficient Simulation of Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the Second International Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of Computer and Telecommunication Systems (MASCOTS'94), pages 201207, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: the end of a speculative execution and for recycling the undo log (most of the undo log recycling is done in hardware only overflows are in handled in software). 16 6 Performance Evaluation 6.1 Simulation Environment Our evaluation is based on execution-driven simulations of a CC-NUMA shared-memory multiprocessor using MINT <ref> [23] </ref>. The modeled multiprocessor has the hardware support discussed in the previous sections. The simulated applications have been pre-processed with the Polaris [4] parallelizing compiler that has been specifically enhanced to transform selected loops for speculative run-time parallelization.
Reference: [24] <author> M. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: Restructuring, or parallelizing, compilers address these problems by detecting and exploiting parallelism in sequential programs written in conventional languages. Although compiler techniques for the automatic detection of parallelism have been studied extensively over the last two decades (see, e.g., <ref> [19, 24] </ref>), current parallelizing compilers cannot extract a significant fraction of the available parallelism in a loop if it has a complex and/or statically unknown access pattern. Programs exhibiting this kind of behavior account for more than 50% of all Fortran applications [12] and encompass most C codes.
Reference: [25] <author> Y. Zhang, L. Rauchwerger, and J. Torrellas. </author> <title> Speculative Parallel Execution of Loops with Cross-Iteration Dependences in DSM Multiprocessors. </title> <type> Technical Report 1536, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> July </month> <year> 1997. </year>
Reference-contexts: Cross-processor flow dependences are serviced when possible and, when not possible, recover from any dependence violation by re-executing a minimal amount of work. We note that the work presented in this paper represents a significant improvement over our previous work <ref> [27, 25, 28, 26] </ref>. It introduces a unified privatization and reduction algorithm (UPAR) that is capable to remove any memory related dependences and parallelize reduction without using a specific choice of coherence algorithm. <p> However, such an approach may well be the best way of speeding up codes whose dependences can not be compiler analyzed. In this paper, we have extended past work <ref> [27, 25, 28, 26] </ref> by proposing a hardware scheme for the speculative parallel execution of loops that have a modest number of cross-iteration dependences. In this case, when a dependence violation is detected, we locally repair the state.
Reference: [26] <author> Y. Zhang, L. Rauchwerger, and J. Torrellas. </author> <title> A Unified Approach to Speculative Parallelization of Loops in DSM Multiprocessors. </title> <type> Technical Report 1550, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1998. </year>
Reference-contexts: Cross-processor flow dependences are serviced when possible and, when not possible, recover from any dependence violation by re-executing a minimal amount of work. We note that the work presented in this paper represents a significant improvement over our previous work <ref> [27, 25, 28, 26] </ref>. It introduces a unified privatization and reduction algorithm (UPAR) that is capable to remove any memory related dependences and parallelize reduction without using a specific choice of coherence algorithm. <p> When this happens some iterations will have to be re-executed. All updates of shared and private memory (off-cache) will be handled by the directories. We will now consider several scenarios in more detail. The full description and communication algorithm can be found in <ref> [26] </ref>. Read Request. If a request hits in cache and was written by the same iteration (Wr tag set) (it is privatizable) data is serviced from cache. In all other cases the request is considered a Read-in. Read-in. <p> However, such an approach may well be the best way of speeding up codes whose dependences can not be compiler analyzed. In this paper, we have extended past work <ref> [27, 25, 28, 26] </ref> by proposing a hardware scheme for the speculative parallel execution of loops that have a modest number of cross-iteration dependences. In this case, when a dependence violation is detected, we locally repair the state.
Reference: [27] <author> Y. Zhang, L. Rauchwerger, and J. Torrellas. </author> <title> Hardware for Speculative Run-Time Parallelization in Distributed Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of High Performance Computer Architecture 1998, </booktitle> <address> (HPCA-4), </address> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: Cross-processor flow dependences are serviced when possible and, when not possible, recover from any dependence violation by re-executing a minimal amount of work. We note that the work presented in this paper represents a significant improvement over our previous work <ref> [27, 25, 28, 26] </ref>. It introduces a unified privatization and reduction algorithm (UPAR) that is capable to remove any memory related dependences and parallelize reduction without using a specific choice of coherence algorithm. <p> However, such an approach may well be the best way of speeding up codes whose dependences can not be compiler analyzed. In this paper, we have extended past work <ref> [27, 25, 28, 26] </ref> by proposing a hardware scheme for the speculative parallel execution of loops that have a modest number of cross-iteration dependences. In this case, when a dependence violation is detected, we locally repair the state.
Reference: [28] <author> Y. Zhang, L. Rauchwerger, and J. Torrellas. </author> <title> Speculative Parallel Execution of Loops with Cross-Iteration Dependences in DSM Multiprocessors. </title> <booktitle> In To appear in Proceedings of the 5th International Symposium on High-Performance Computer Architecture, </booktitle> <month> January </month> <year> 1999. </year>
Reference-contexts: Cross-processor flow dependences are serviced when possible and, when not possible, recover from any dependence violation by re-executing a minimal amount of work. We note that the work presented in this paper represents a significant improvement over our previous work <ref> [27, 25, 28, 26] </ref>. It introduces a unified privatization and reduction algorithm (UPAR) that is capable to remove any memory related dependences and parallelize reduction without using a specific choice of coherence algorithm. <p> However, such an approach may well be the best way of speeding up codes whose dependences can not be compiler analyzed. In this paper, we have extended past work <ref> [27, 25, 28, 26] </ref> by proposing a hardware scheme for the speculative parallel execution of loops that have a modest number of cross-iteration dependences. In this case, when a dependence violation is detected, we locally repair the state.
Reference: [29] <author> H. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1991. </year> <month> 23 </month>
Reference-contexts: This type of reduction is sometimes called an update. There are several known parallel methods for performing reduction operations. One method is to transform the do loop into a doall and enclose the access to the reduction variable in an unordered critical section <ref> [9, 29] </ref>, or, equivalently, perform an atomic fetch-and-add. The drawbacks of this method are that it is not scalable and that it requires potentially expensive synchronizations. <p> this problem has been handled at compiletime by syntactically pattern matching the loop statements with a template of a generic reduction, and then performing a data dependence analysis of the variable under scrutiny to guarantee that it is not used anywhere else in the loop except in the reduction statement <ref> [29] </ref>. In the cases where data dependence analysis cannot be performed at compile time, reductions have to be validated at run-time.
References-found: 29

