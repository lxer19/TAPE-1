URL: ftp://ftpipr.ira.uka.de/pub/projects/BRA7274_B-Learn_II/papers/ewlr95vk.ps.gz
Refering-URL: http://wwwipr.ira.uka.de/~kaiser/events/ewlr3/ewlr3prog.html
Root-URL: 
Email: volker@ls8.informatik.uni-dortmund.de  sklorz@informatik.rwth-aachen.de  
Title: Representing, Learning, and Executing Operational Concepts  
Author: Volker Klingspor and Stefan Sklorz 
Keyword: Key Words. Operational concepts, mobile robots, combining sensing and action, adaptivity, in ductive logic programming  
Note: Has been moved to:  Member of the Graduiertenkolleg  
Address: VIII, D-44221 Dortmund, Germany  V, D-52056 Aachen, Germany  RWTH Aachen  
Affiliation: University of Dortmund, Computer Science Dept., LS  RWTH Aachen, Computer Science Dept.  Informatik und Technik,  
Abstract: On the one hand side operational concepts enable the user to get information about what a robot has been done and on the other hand side, they enable the user to control the robot. In this way, they function as elements of a high level command language, easy to understand by human users. Operational concepts should be general enough to be applied in different but similar environments like office rooms even if the environment is totally unknown. They combine sensing and action of a mobile robot situated in the real world without the need of additional environmental knowledge. In this paper, we complete the representation of operational concepts and show, how this work is combined with results presented previously. Furthermore, we sketch the learning of concept descriptions to reach a high adaptivity of the robot and we suggest the application of these concepts for planning and control. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rudiger Dillmann, Jurgen Kreuziger, and Frank Wall-ner. </author> <title> PRIAMOS an experimental platform for reflexive navigation. </title> <editor> In Groen, Hirose, and Thorpe, editors, IAS-3: </editor> <booktitle> Intelligent Autonomous Systems, chapter 18, </booktitle> <pages> pages 174-183. </pages> <publisher> IOS Press, </publisher> <year> 1993. </year>
Reference-contexts: A) The robot we use to apply the ideas about operational concepts is Priamos, developed at the University of Karlsruhe <ref> [1] </ref>. Priamos moves on four separately controlled Mecanum-wheels, allowing the robot to move in three degrees of freedom. 24 sonar sensors are mounted on the robot, three of them at each side and three at each corner, all in the same height.
Reference: [2] <author> A. Elfes. </author> <title> A sensor-based mapping and navigation system. </title> <booktitle> In Proc. of the IEEE Intern. Conf. on Robotics and Automation, </booktitle> <year> 1986. </year>
Reference-contexts: So, the systems are not much adaptive and they cannot react to unforeseen events. To reach a higher adaptivity, the plans used for navigation can be acquired automatically while moving in the environment <ref> [2, 21] </ref>, e.g., by detecting free and occupied areas. To enable the use of cognitive elements in these maps, like, e.g., an occupied area representing a desk, the classification of the perceptions is necessary. In those approaches, this work is mostly done by the user.
Reference: [3] <author> Richard E. Fikes and Nils J. Nilsson. </author> <title> STRIPS: a new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> 2(3/4):189-208, 1971. 
Reference-contexts: How to build up a representation for such concepts, how to acquire them, and how to apply them will be presented in this paper. Many approaches to command and control mobile systems base on the early work of Fikes and Nils-son <ref> [3] </ref>. This way of action planning is located on a cognitive high level allowing a human user to command the robot easily and to understand the generated plans. To be able to plan and to execute the plans, the environment must be known. <p> In the STRIPS-approach <ref> [3] </ref>, operators are presented as structures with a list of preconditions, of facts to be deleted, and facts to be added; add- and delete-list define the new situation.
Reference: [4] <author> R. James Firby. </author> <title> Building symbolic primitives with continuous control routines. </title> <editor> In James Hendler, editor, </editor> <booktitle> Proc. of the first Intern. Conf. on AI Planning Systems (AIPS-92), </booktitle> <pages> pages 62-69, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, the first of these systems are fully autonomous, there is no possibility for a user to influence the behavior during runtime. Additionally, these system cannot communicate in a human adequate way with the user, because they lack a cognitive level. By Firby <ref> [4] </ref>, an approach is presented controlling different behaviors by switching on or off, resp., some of the stimuli-response-rules, to perform an externally requested behavior. This approach does not allow the class-fication of the perceptions.
Reference: [5] <author> Stevan Harnad. </author> <title> The symbol grounding problem. </title> <journal> Physica D, </journal> <volume> 42 </volume> <pages> 335-346, </pages> <year> 1990. </year>
Reference-contexts: Second, the features used for classifying the objects should depend directly on the real world 1 In our context, situation do not refer to a single instant of time, but to a time interval. data, they should by anchored in the environment <ref> [5, 23] </ref>. This problem encloses the hard problem of feature generation, because the quality of these basic features determines the powerfulness of the overall representation.
Reference: [6] <author> Nicolas Helft. </author> <title> Induction as nonmonotonic inference. </title> <booktitle> In Proceedings of the 1st International Conference on Knowledge Representation and Reasoning, </booktitle> <year> 1989. </year>
Reference-contexts: The description must cover all positive examples and none of the negative ones [16]. Algorithms of the second group try to find regularities in a set of formulas. In contrast to the first group, these algorithms do not necessarily cover all examples of a concept <ref> [6] </ref>. These views have in common restricted first order logic as representation formalism. We apply rdt in the way described by Helft [6], and we only learn from positive examples 5 . The learner rdt is integrated in the knowledge acquisition tool mobal [13]. <p> In contrast to the first group, these algorithms do not necessarily cover all examples of a concept <ref> [6] </ref>. These views have in common restricted first order logic as representation formalism. We apply rdt in the way described by Helft [6], and we only learn from positive examples 5 . The learner rdt is integrated in the knowledge acquisition tool mobal [13]. It uses a extended function free horn logic as representation formalism, also allowing negated literals.
Reference: [7] <author> M. Kaiser, V. Klingspor, J. del R. Millan, M. Accame, F. Wallner, and R. Dillmann. </author> <title> Achieving intelligence in mobility incorporating learning capabilities in real-world mobile robots. </title> <journal> IEEE-Expert (special track on Intelligent Robotic Systems), </journal> <note> 1995. to appear. </note>
Reference-contexts: With this set, our system cannot reach the flexibility and the robustness of behavior based systems. However, coupling both approaches seems to make sense. In <ref> [7] </ref>, a possible coupling of different learning approaches for different navigation tasks, inter alia the approach presented here, is described. Acknowledgement This work is partially funded by the European Community under the project B-Learn II (P7274) and the Ministry for Sciences and Research of the German federal state Nordrhein-Westfalen.
Reference: [8] <author> Jorg-Uwe Kietz and Stefan Wrobel. </author> <title> Controlling the complexity of learning in logic through syntactic and task-oriented models. </title> <booktitle> In Muggleton [15], chapter 16, </booktitle> <pages> pages 335-360. </pages> <note> Also available as Arbeitspapiere der GMD No. 503, </note> <year> 1991. </year>
Reference-contexts: In this section, we show how to learn descriptions for perception-integrating action features and operational concepts, using logic based inductive learning. Learning perceptual features is described in [14, 10]. The learning algorithm used is rdt (Rule Discovery Tool) <ref> [8] </ref>, one of the algorithms of the area inductive logic programming (ILP) [15]. ILP-algorithms can be divided in two main groups, dependent on their objective.
Reference: [9] <author> Volker Klingspor and Katharina Morik. </author> <title> Towards concept formation grounded on perception and action of a mobile robot. </title> <booktitle> In Proc. of the 4th Intern. Conference on Intelligent Autonomous Systems, </booktitle> <year> 1995. </year> <note> In press. </note>
Reference-contexts: Moreover, the concepts are executable, i.e., the robot must be able to perform the action intended by the concept. Because of this capability we call them operational concepts, distinguishing them from the concepts usually occurring in machine learning literature. In <ref> [9] </ref>, two main aspects of operational concepts necessary to achieve these properties are presented in detail: First, the concepts must integrate action and perceptions, where actions are perception-oriented and perceptions are action-oriented.
Reference: [10] <author> Volker Klingspor, Katharina Morik, and Anke Rieger. </author> <title> Learning operational concepts from sensor data of a mobile robot. </title> <note> (submitted to Machine Learning Journal), </note> <month> September </month> <year> 1994. </year>
Reference-contexts: The sensing of these features during the same interval of time or with a short delay during an action of the robot allows to conclude objects. Morik, Klingspor, and Rieger describe the representation of these features on different levels of abstraction and how to learn descriptions for them <ref> [14, 10] </ref>. In particular, they distinguish basic perceptual features, sensor and sensor group features, and action-oriented perceptual features, each of them defined in terms of the next lower level. Action-oriented perceptual features represent objects of the environment, considering the path of the robot relative to this object. <p> The basic features of this representation can be delivered incrementally during the trip of the robot. In this section, we show how to learn descriptions for perception-integrating action features and operational concepts, using logic based inductive learning. Learning perceptual features is described in <ref> [14, 10] </ref>. The learning algorithm used is rdt (Rule Discovery Tool) [8], one of the algorithms of the area inductive logic programming (ILP) [15]. ILP-algorithms can be divided in two main groups, dependent on their objective.
Reference: [11] <editor> Maja J. Mataric. </editor> <title> Interaction and Intelligent Behavior. </title> <type> PhD thesis, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology (MIT), Cam-bridge, Massachusetts, </institution> <year> 1994. </year>
Reference-contexts: In those approaches, this work is mostly done by the user. The weaknesses of the approaches described previously is the necessity of reasoning exclusively on a high and very complex level, leading to slow and unflexible systems. Behavior based systems <ref> [12, 11] </ref> are the contrasting approaches. Their behavior is determined by a set of very simple stimuli-response-rules, i.e., rules that trigger a specific action depending on the sensed pattern.
Reference: [12] <editor> Jean-Arcady Meyer and Stewart W. Wilson, editors. </editor> <booktitle> From Animals to Animats Proceeding of the First International Conference on Simulation on Adaptive Behavior, </booktitle> <address> MA, 1991. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: In those approaches, this work is mostly done by the user. The weaknesses of the approaches described previously is the necessity of reasoning exclusively on a high and very complex level, leading to slow and unflexible systems. Behavior based systems <ref> [12, 11] </ref> are the contrasting approaches. Their behavior is determined by a set of very simple stimuli-response-rules, i.e., rules that trigger a specific action depending on the sensed pattern.
Reference: [13] <author> K. Morik, S. Wrobel, J.-U. Kietz, and W. Emde. </author> <title> Knowledge Acquisition and Machine Learning Theory, Methods, and Applications. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1993. </year>
Reference-contexts: These views have in common restricted first order logic as representation formalism. We apply rdt in the way described by Helft [6], and we only learn from positive examples 5 . The learner rdt is integrated in the knowledge acquisition tool mobal <ref> [13] </ref>. It uses a extended function free horn logic as representation formalism, also allowing negated literals. A special feature of rdt is the use of rule schemata, i.e., formulas with predicate variables instead of predicate symbols. They define sets of formulas of the same syntax.
Reference: [14] <author> Katharina Morik and Anke Rieger. </author> <title> Learning action-oriented perceptual features for robot navigation. </title> <editor> In Attilio Giordana, editor, </editor> <booktitle> Workshop notes: Learning Robots of the ECML-93, </booktitle> <year> 1993. </year> <note> Also available as Research Report 3, </note> <institution> Univ. Dortmund, Informatik VIII, D-44221 Dortmund. </institution>
Reference-contexts: Learning will then lead to a high adaptivity of the system. Results of learning perceptual features are already presented <ref> [14] </ref>, here we only describe the approach for learning action features and integrating concepts (Sect. 3). <p> The sensing of these features during the same interval of time or with a short delay during an action of the robot allows to conclude objects. Morik, Klingspor, and Rieger describe the representation of these features on different levels of abstraction and how to learn descriptions for them <ref> [14, 10] </ref>. In particular, they distinguish basic perceptual features, sensor and sensor group features, and action-oriented perceptual features, each of them defined in terms of the next lower level. Action-oriented perceptual features represent objects of the environment, considering the path of the robot relative to this object. <p> In Sect. 4 we show our ideas on using concepts for planning, execution, and control. 3 Learning Operational Concepts In the previous section, we described a representation of operational concepts, which is compatible with the representation of perceptual features described by <ref> [14] </ref>. The basic features of this representation can be delivered incrementally during the trip of the robot. In this section, we show how to learn descriptions for perception-integrating action features and operational concepts, using logic based inductive learning. Learning perceptual features is described in [14, 10]. <p> The basic features of this representation can be delivered incrementally during the trip of the robot. In this section, we show how to learn descriptions for perception-integrating action features and operational concepts, using logic based inductive learning. Learning perceptual features is described in <ref> [14, 10] </ref>. The learning algorithm used is rdt (Rule Discovery Tool) [8], one of the algorithms of the area inductive logic programming (ILP) [15]. ILP-algorithms can be divided in two main groups, dependent on their objective.
Reference: [15] <author> Stephen Muggleton. </author> <title> Inductive Logic Programming. Number 38 in APIC series. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: Learning perceptual features is described in [14, 10]. The learning algorithm used is rdt (Rule Discovery Tool) [8], one of the algorithms of the area inductive logic programming (ILP) <ref> [15] </ref>. ILP-algorithms can be divided in two main groups, dependent on their objective. Algorithms of the first group try to induce a concept description from a set of positive and negative examples of the concept and additional background knowledge.
Reference: [16] <author> Gordon D. Plotkin. </author> <title> A further note on inductive gen-eralisation. </title> <journal> Machine Intelligence, </journal> <volume> 6 </volume> <pages> 101-124, </pages> <year> 1971. </year>
Reference-contexts: Algorithms of the first group try to induce a concept description from a set of positive and negative examples of the concept and additional background knowledge. The description must cover all positive examples and none of the negative ones <ref> [16] </ref>. Algorithms of the second group try to find regularities in a set of formulas. In contrast to the first group, these algorithms do not necessarily cover all examples of a concept [6]. These views have in common restricted first order logic as representation formalism. <p> For testing, rdt counts the number of covered positive and negative examples als well as the number of uncovered examples 5 Under specific conditions, rdt can also be used as a learner in the sense of Plotkin <ref> [16] </ref>. and additional derivable knowledge. From these values, rdt evaluates whether the tested hypothesis can be accepted, should be further specialized, or is already to special. The learning tasks can now be formulated: LT 1: Learning Percept.-Integr.
Reference: [17] <author> Stefan Sklorz. </author> <title> Reprasentation operationaler Begriffe zum Lernen aus Roboter-Sensordaten. </title> <type> Master's thesis, </type> <institution> Universitat Dortmund, </institution> <year> 1995. </year> <note> in German. </note>
Reference-contexts: Thus, we use the computation from a move to move during the learning phase and vice versa during the performance phase. This transformation as well as the computation of a move is easy and is therefore not described in this paper, details can be found in <ref> [17] </ref>. Fig. 2 illustrates the difference, the black spot at the robot marks its absolute front. At the next higher level of the hierarchy, perception integrating actions are represented. Before describing them in detail, we present two examples, illustrated in Fig. 3.
Reference: [18] <author> Horst Spandl and Knut Pitschke. </author> <title> Lernen von Makro-Trajektoren fur einen autonomen, </title> <journal> mobilen Roboter. Kunstliche Intelligenz, </journal> <volume> 1 </volume> <pages> 12-16, </pages> <year> 1991. </year>
Reference-contexts: The graphs found by the first approach can be stored and used as macro operators in further situations, corresponding to the work on learning macro-trajectories <ref> [18] </ref>, but on a conceptual level. 5 Conclusion In this paper, we presented an approach to represent actions and to integrate them with perceptions to define operational concepts in a first order representation formalism.
Reference: [19] <author> L. Steels. </author> <title> Emergent frame recognition and its use in artificial creatures. </title> <booktitle> In Proc. of the 12 th IJCAI, </booktitle> <pages> pages 1219-1224, </pages> <address> Sidney, Australia, </address> <year> 1991. </year>
Reference-contexts: By Firby [4], an approach is presented controlling different behaviors by switching on or off, resp., some of the stimuli-response-rules, to perform an externally requested behavior. This approach does not allow the class-fication of the perceptions. Steels <ref> [19] </ref> presented an approach to classify the perceptions of a robot by emergent frame recognition, determing which frame become active depending on the data gathered by the robot. However, the frames cannot be used vice versa to control the robot. In our approach, we want to go one step further.
Reference: [20] <author> Luc Steels. </author> <title> Towards a theory of emergent functionality. </title> <booktitle> In Meyer and Wilson [12], </booktitle> <pages> pages 451-461. </pages>
Reference-contexts: The intelligent behavior of these systems is reached by an effect called emergency, i.e., that the different simple rules create a complex behavior when they work together (see <ref> [20] </ref> for a deeper discussion about emergent and hierarchical systems). In these approaches, no global knowledge about the environment is represented, thus they are very well suited for unknown environments. Because of the simple reactive rules, these sys tems can react very fast to sudden changes of the environment.
Reference: [21] <author> Frank Wallner, Michael Kaiser, Holger Friedrich, and Rudiger Dillmann. </author> <title> Integration of topological and geometrical planning in a learning mobile robot. </title> <booktitle> In Proc. of IROS-94, </booktitle> <year> 1994. </year>
Reference-contexts: So, the systems are not much adaptive and they cannot react to unforeseen events. To reach a higher adaptivity, the plans used for navigation can be acquired automatically while moving in the environment <ref> [2, 21] </ref>, e.g., by detecting free and occupied areas. To enable the use of cognitive elements in these maps, like, e.g., an occupied area representing a desk, the classification of the perceptions is necessary. In those approaches, this work is mostly done by the user.
Reference: [22] <author> Stephanie Wessel. </author> <title> Lernen qualitativer Merkmale aus numerischen Robotersensordaten. </title> <type> Master's thesis, </type> <institution> Universitat Dortmund, </institution> <year> 1995. </year> <note> in German. </note>
Reference-contexts: Wessel developed a system for incrementally calculating these basic features by find more or less linear sequences in the data, and to learn the exact value of "more or less" <ref> [22] </ref>. The concepts are represented on different levels of abstraction, the process of classification passes through multiple, differently detailed levels to eliminate faulty measurements and noise. The different elements of this representation hierarchy are described in Section 2, accompanied by some examples.
Reference: [23] <author> Stefan Wrobel. </author> <title> Towards a model of grounded concept formation. </title> <booktitle> In Proc. 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 712-719, </pages> <address> Los Altos, CA, 1991. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Second, the features used for classifying the objects should depend directly on the real world 1 In our context, situation do not refer to a single instant of time, but to a time interval. data, they should by anchored in the environment <ref> [5, 23] </ref>. This problem encloses the hard problem of feature generation, because the quality of these basic features determines the powerfulness of the overall representation.
References-found: 23

