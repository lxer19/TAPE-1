URL: ftp://lifelong.learning.cs.cmu.edu/pub/papers/proposal.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/josullvn/mosaic/research.html
Root-URL: 
Email: josullvn@cs.cmu.edu  
Title: Transfer of Learned Knowledge in Life-Long Learning Agents  
Author: Joseph O'Sullivan Tom Mitchell (Chair) Sebastian Thrun Manuela Veloso Jude Shavlik 
Note: Thesis Committee:  
Date: February 1997  
Affiliation: School Of Computer Science Carnegie Mellon University  (University of Wisconsin)  
Abstract: Previous work has demonstrated that the performance of machine learning algorithms can be improved by exploiting various forms of knowledge, such as domain theories. More recently, it has been recognized that some forms of knowledge can in turn be learned in particular, action models and task-specific internal representations. Using learned knowledge as a source of learning improvement can be particularly appropriate for agents that face many tasks. Over a long lifetime, an agent can amortize effort expended in learning knowledge by reducing the number of examples required to learn further tasks. In developing such a lifelong learning agent, a number of research issues arise, including: will an agent benefit from learned knowledge, can an agent exploit multiple sources of learned knowledge, how should the agent adapt as a new task arrives, how might the order of task arrival impact learning, and how can such an agent be built? I propose that an agent can be constructed which learns knowledge and exploits that knowledge to effectively improve further learning by reducing the number of examples required to learn. I intend to study the transfer of learned knowledge by life-long learning agents within a neural network based architecture capable of increasing capacity with the number of tasks faced. This proposal describes an appropriate architecture, based on preliminary work in controlled settings. This work has shown that learned knowledge can reduce the number of examples required to learn novel tasks and that combining previously separate mechanisms can yield a synergistic improvement on learning ability. It has also explored how capacity can be expanded as new tasks arise over time and how the order in which tasks arise can be exploited with a graded curriculum. This preliminary work will be applied to a life-long learning agent and extended by carrying out experimental studies of a simulated robot agent in a controlled environment and of a real-world mobile robot agent in Wean Hall. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Yaser S. Abu-Mostafa. </author> <title> Hints. </title> <booktitle> Neural Computation, </booktitle> <address> 7:637671, </address> <year> 1995. </year>
Reference-contexts: The proposed life-long learning agent improves its ability to learn thanks to combining two mechanisms. MTL [4, 10] is a generalization of work on hints <ref> [1, 47] </ref>. With hints, extra tasks are important learnable features of a main task, and are supplied as network outputs to pass domain specific knowledge to the network.
Reference: [2] <author> Philip E Agre and David Chapmann. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In AAAI-87, </booktitle> <year> 1987. </year>
Reference-contexts: 1. Introduction One common factor exists in the majority of previous work on agents <ref> [54, 2] </ref>: As the agent ages, and further new tasks arise, the agent does not improve its ability to learn those tasks. <p> Here, we will explore how a curriculum can be used to bias learning, by inserting constraints on the search space of the possible hypotheses considered by the agent. Agents have been embedded within simulated worlds since PENGI <ref> [2] </ref>. Such simulated worlds have been invaluable as a controlled means of investigating the performance of an agent [27, 53], and in this work we too rely on the simulated environment described in section 3.3 for basic validation of results.
Reference: [3] <author> Minoru Asada, Shoichi Noda, Sukoya Tawaratsumida, and Koh Hosoda. </author> <title> Purposive behavior acquisition for a real robot by vision-based reinforcement learnin. </title> <booktitle> Machine Learning, </booktitle> <address> 12((2/3)):279303, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Previously, curriculums have been used to insert procedural bias into a learning agent. Typically, such as in <ref> [3] </ref>, a robot is trained on easy missions (tasks), and gradually moved to more difficult ones. Here, we will explore how a curriculum can be used to bias learning, by inserting constraints on the search space of the possible hypotheses considered by the agent.
Reference: [4] <author> Jonathan Baxter. </author> <title> Learning Internal Representations. </title> <type> PhD thesis, </type> <institution> The Flinders University of South Australia, </institution> <year> 1994. </year>
Reference-contexts: Recently the machine learning community has started focusing on this aspect of a learning system, the ability to exploit previously learned knowledge for the learning of new tasks. For instance, algorithms which learn useful representations for a particular domain <ref> [9, 4] </ref> and algorithms which learn domain specific models for use in further learning [31, 50, 36] have recently been developed (as will be discussed in detail in Section 4). <p> Internal representations can be shared between tasks in a single domain, with the constraints on knowledge transferred, by using a simple yet highly effective mechanism, Multitask Learning (MTL) <ref> [10, 9, 4] </ref>. In MTL, a single network is trained to predict multiple tasks from the same domain using back-propagation. MTL in operation is conceptually simple: 1. Create a single network with n task outputs, with sufficient hidden units. 2. Train on n tasks. 3. <p> A life-long learning agent would provide a flexible means of specializing to a domain of interest, especially domains that are difficult to model in advance [24, 17, 33]. The proposed life-long learning agent improves its ability to learn thanks to combining two mechanisms. MTL <ref> [4, 10] </ref> is a generalization of work on hints [1, 47]. With hints, extra tasks are important learnable features of a main task, and are supplied as network outputs to pass domain specific knowledge to the network.
Reference: [5] <author> D. Beymer and T. Poggio. </author> <title> Face recognition from one model view. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: For an exhaustive list see the introduction to [34], but some salient work includes learning an appropriate inductive learning algorithm for a domain [37], optimizing algorithm control parameters to a domain such as distance metrics [51], or generating synthetic data according to domain rules <ref> [33, 5] </ref>. Work has been done on the serial transfer of knowledge. [36] has explored transferring knowledge between networks, but with a primary goal of improving learning speed, rather than improved generalization. Indeed, in [35], negative results are reported for generalization performance.
Reference: [6] <author> Leo Breiman. </author> <title> Bagging predictors. </title> <type> Technical report, </type> <institution> University of California- Berkeley, </institution> <year> 1994. </year>
Reference-contexts: Alternatively, improving bias by using ensembles of neural networks is appealing. Computational intensive techniques exist (e.g. <ref> [6] </ref>) to reduce variance error by using groups of networks. In addition, there are a number of directions in which to expand the application of the architecture most notably, applying the learning mechanisms within a reinforcement learning framework, to improve the ability to learn task policies. 5 3.
Reference: [7] <author> Rodney Brooks and Maja Mataric. </author> <title> Real Robots, Real Learning Problems. In Robot Learning, chapter 8, </title> <publisher> pages 193213. Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Related Work The importance of using constraints provided by domain-related knowledge in the learning process has long been acknowledged [8]. In fact, systems where built-in knowledge restricts what can be learned have arguably seen the most effective learning agents <ref> [7, 33] </ref>. Without built-in knowledge, there are fundamental limits on the generalization accuracy that can be expected from a learner that learns just from examples [12, 40].
Reference: [8] <author> J. G. Carbonell, R. S. Mitchalski, and T. M. Mitchell. </author> <title> General Issues in Machine Learning. </title> <editor> In R. S. Mitchalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, chapter 1, </booktitle> <pages> pages 324. </pages> <publisher> Tioga Publishing, </publisher> <address> Palo Alto, </address> <year> 1983. </year>
Reference-contexts: Related Work The importance of using constraints provided by domain-related knowledge in the learning process has long been acknowledged <ref> [8] </ref>. In fact, systems where built-in knowledge restricts what can be learned have arguably seen the most effective learning agents [7, 33]. Without built-in knowledge, there are fundamental limits on the generalization accuracy that can be expected from a learner that learns just from examples [12, 40].
Reference: [9] <author> Rich Caruana. </author> <title> Learning Many Related Tasks At the Same Time With Backpropagation. </title> <booktitle> In Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> December </month> <year> 1994. </year>
Reference-contexts: Recently the machine learning community has started focusing on this aspect of a learning system, the ability to exploit previously learned knowledge for the learning of new tasks. For instance, algorithms which learn useful representations for a particular domain <ref> [9, 4] </ref> and algorithms which learn domain specific models for use in further learning [31, 50, 36] have recently been developed (as will be discussed in detail in Section 4). <p> Internal representations can be shared between tasks in a single domain, with the constraints on knowledge transferred, by using a simple yet highly effective mechanism, Multitask Learning (MTL) <ref> [10, 9, 4] </ref>. In MTL, a single network is trained to predict multiple tasks from the same domain using back-propagation. MTL in operation is conceptually simple: 1. Create a single network with n task outputs, with sufficient hidden units. 2. Train on n tasks. 3.
Reference: [10] <author> Rich Caruana. </author> <title> Algorithms and applications for multitask learning. </title> <editor> In Lorenza Saitta, editor, </editor> <booktitle> 13th International Conference on Machine Learning, </booktitle> <pages> pages 8796, </pages> <address> Bari, Italy, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: Internal representations can be shared between tasks in a single domain, with the constraints on knowledge transferred, by using a simple yet highly effective mechanism, Multitask Learning (MTL) <ref> [10, 9, 4] </ref>. In MTL, a single network is trained to predict multiple tasks from the same domain using back-propagation. MTL in operation is conceptually simple: 1. Create a single network with n task outputs, with sufficient hidden units. 2. Train on n tasks. 3. <p> Train on n tasks. 3. Monitor performance by performing cross-validation on each individual output during training. MTL frequently outperforms single-task learning wherein a network is devoted to each task, with no sharing of internal representations (see section 3 and <ref> [10] </ref>). <p> A life-long learning agent would provide a flexible means of specializing to a domain of interest, especially domains that are difficult to model in advance [24, 17, 33]. The proposed life-long learning agent improves its ability to learn thanks to combining two mechanisms. MTL <ref> [4, 10] </ref> is a generalization of work on hints [1, 47]. With hints, extra tasks are important learnable features of a main task, and are supplied as network outputs to pass domain specific knowledge to the network. <p> EBNN [50, 49] extends earlier work that exploited hand crafted slope information during learning [43] by itself learning slope constraints. Both of these methods have been applied with success in various application domains (EBNN to chess, object recognition, robot control, see [50], MTL to object recognition, medical diagnostics, see <ref> [10] </ref>) but prior to this research combining these approaches had not been explored, nor had a life-long learning agent been constructed from these approaches (although the idea of a life-long learning agent is from [49]).
Reference: [11] <author> Rich Caruana. </author> <title> Multitask Learning. </title> <type> PhD thesis, </type> <institution> School Of Computer Science, </institution> <address> CMU, </address> <note> To appear. </note>
Reference-contexts: A limited amount of evaluation data hampered a statistical study of Serial MTL, and so an artificial problem setting was constructed. 3.2.1. The Peaks Domain A simple problem setting, that allowed for easy statistical evaluation, was constructed (with <ref> [11] </ref>) to test serial MTL.
Reference: [12] <author> T. G. Dietterich. </author> <title> Limitations of inductive learning. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 124128. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: In fact, systems where built-in knowledge restricts what can be learned have arguably seen the most effective learning agents [7, 33]. Without built-in knowledge, there are fundamental limits on the generalization accuracy that can be expected from a learner that learns just from examples <ref> [12, 40] </ref>. This results in it being increasingly difficult for a system to 14 generalize well as the the complexity of the learning task increases [15, 52].
Reference: [13] <author> S. E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation architecture. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Here, I am prepared to investigate a variety of neural network management techniques in order to reduce the usual drawbacks of local minima, concept drift, etc. In my preliminary work, I have take an ad hoc initial approach, and should consider the work from automatic neural construction (such as <ref> [13, 14] </ref>) for superior approaches for network expansion, whether to periodically do network restructuring, either by pruning weights and nodes using a technique such as OBD or SBP [18, 25], and splitting single networks into multiple networks containing only highly related tasks (I previously have discovered that the notion of task
Reference: [14] <author> M Frean. </author> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <booktitle> Neural Computation, </booktitle> <address> 2:198209, </address> <year> 1991. </year>
Reference-contexts: Here, I am prepared to investigate a variety of neural network management techniques in order to reduce the usual drawbacks of local minima, concept drift, etc. In my preliminary work, I have take an ad hoc initial approach, and should consider the work from automatic neural construction (such as <ref> [13, 14] </ref>) for superior approaches for network expansion, whether to periodically do network restructuring, either by pruning weights and nodes using a technique such as OBD or SBP [18, 25], and splitting single networks into multiple networks containing only highly related tasks (I previously have discovered that the notion of task
Reference: [15] <author> D. Haussler. </author> <title> Decision Theoretic Generalizations of the PAC Model for Neural Net and other Learning Applications. </title> <type> Inform. </type> <institution> Comput., 100:78150, </institution> <year> 1992. </year>
Reference-contexts: Without built-in knowledge, there are fundamental limits on the generalization accuracy that can be expected from a learner that learns just from examples [12, 40]. This results in it being increasingly difficult for a system to 14 generalize well as the the complexity of the learning task increases <ref> [15, 52] </ref>. A life-long learning agent would provide a flexible means of specializing to a domain of interest, especially domains that are difficult to model in advance [24, 17, 33]. The proposed life-long learning agent improves its ability to learn thanks to combining two mechanisms.
Reference: [16] <editor> Miroslav Kubat and Gerhard Widmer, editors. </editor> <booktitle> ICML-96 Workshop on Learning in Context-Sensitive Domains, </booktitle> <address> Bari, Italy, </address> <month> July </month> <year> 1996. </year> <month> 19 </month>
Reference-contexts: For instance, the agent's camera may be nudged, causing every image to be shifted slightly. Or the agent's bearings may loosen, causing the action models to have to gradually shift. A large body of research is on going in this area (see <ref> [16] </ref>), and this has been addressed specifically for robot agents [26]. This latter approach, of monitoring the relevance of training examples, could be applied on top of our approach.
Reference: [17] <author> J. Laird, E. Yager, C. Tuck, and M. Hucka. </author> <title> Learning in tele-autonomous systems using Soar. </title> <booktitle> In Proceedings of the 1989 NASA Conference of Space Telerobotics, </booktitle> <year> 1989. </year>
Reference-contexts: A life-long learning agent would provide a flexible means of specializing to a domain of interest, especially domains that are difficult to model in advance <ref> [24, 17, 33] </ref>. The proposed life-long learning agent improves its ability to learn thanks to combining two mechanisms. MTL [4, 10] is a generalization of work on hints [1, 47].
Reference: [18] <author> Y. LeCun, J. S. Denker, and S. A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: my preliminary work, I have take an ad hoc initial approach, and should consider the work from automatic neural construction (such as [13, 14]) for superior approaches for network expansion, whether to periodically do network restructuring, either by pruning weights and nodes using a technique such as OBD or SBP <ref> [18, 25] </ref>, and splitting single networks into multiple networks containing only highly related tasks (I previously have discovered that the notion of task relevance can be important in knowledge transfer [51]). Given an adequate architecture, there is the issue of suitable metrics.
Reference: [19] <author> R. Maclin and Jude W. Shavlik. </author> <title> Creating advice-taking reinforcement learners. </title> <booktitle> Machine Learning, </booktitle> <address> 22(1-3):251281, </address> <year> 1996. </year>
Reference-contexts: Ideally, as i increases, learning task i will always be easier. In RATLE, indications can be seen of a law of diminishing returns whereby a 2nd piece of advice is not as good as for the 1st piece <ref> [19] </ref> this can be explained by the two pieces of advice being redundant, but the thesis research will provide grounds for a full investigation of this issue. <p> FourEyes is not itself able to act, and so is unable to use action models as a source of domain knowledge. RATLE <ref> [19] </ref> is a reinforcement learning agent, operating in a simulated world, that incorporates advice from an external advisor during learning, to improve its ability to learn. It does not improve its ability to learn itself, yet is highly appropriate for many problem settings where an external advisor is available.
Reference: [20] <author> H. Marase and S. Nayar. </author> <title> Visual learning and recognition of 3-d objects from appearance. </title> <journal> International Journal of Computer Vision, </journal> <volume> 14:524, </volume> <year> 1994. </year>
Reference-contexts: Followings the ideas in <ref> [20, 21] </ref>, we have previously had experience with appearance-based recognition tasks in this domain, discriminating within an 12 object database, based on around 30 examples of each object [51].
Reference: [21] <author> Barlett Mel. Seemore: </author> <title> A view-based approach to 3-d object recognition using multiple visual cues. </title> <editor> In M.C. Mozer D.S. Touretzky and M.E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Followings the ideas in <ref> [20, 21] </ref>, we have previously had experience with appearance-based recognition tasks in this domain, discriminating within an 12 object database, based on around 30 examples of each object [51].
Reference: [22] <author> C. J. Merz and P. M. Murphy. </author> <title> Uci repository of machine learning databases. </title> <address> [http://www.ics.uci.edu/ mlearn/MLRepository.html], 1996. Irvine, CA: </address> <institution> University of Cali-fornia, Department of Information and Computer Science. </institution>
Reference-contexts: Converting and documenting data from the experimental studies into a format suitable for the UCI Machine Learning Archive <ref> [22] </ref>. Life-long learning agents are a young field, and would be furthered by the availability of a shared test-bed. 17 6. Conclusions It is expected that this thesis will demonstrate life-long learning agents, by implementing such agents on mobile robots, and allowing them to exist in simulated and real-world domains.
Reference: [23] <author> T. P. Minka and R. W. </author> <title> Picard. Interactive learning using a society of models. </title> <type> Technical Report 349, </type> <institution> M.I.T. Media Laboratory Perceptual Computing Section, </institution> <year> 1996. </year> <title> Submitted to Special Issue of Pattern Recognition on Image Database: Classification and Retrieval. </title>
Reference-contexts: In our case, we care about any future tasks in the same domain, and utilize sources of bias which are more widely applicable, namely action models and shared internal representations. FourEyes <ref> [23] </ref> is a visual database query system that learns to describe components of visual data (sky, grass, etc) driven by user annotation of scenes. As with our domain, examples are expensive, and FourEyes attempts to learn domain bias during learning, to reduce the future need for examples.
Reference: [24] <author> Tom M. Mitchell. </author> <title> Becoming increasingly reactive. </title> <booktitle> In Proceedings of the Eight National Conference on Artificial Intelligence, </booktitle> <pages> pages 10511059. </pages> <publisher> AAAI Press/MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: A life-long learning agent would provide a flexible means of specializing to a domain of interest, especially domains that are difficult to model in advance <ref> [24, 17, 33] </ref>. The proposed life-long learning agent improves its ability to learn thanks to combining two mechanisms. MTL [4, 10] is a generalization of work on hints [1, 47].
Reference: [25] <author> John Moody. </author> <title> Prediction risk and architecture selection for neural networks. </title> <editor> In V. Cherkassky, J.H. Friedman, and H. Wechsler, editors, </editor> <title> From Statistics to Neural Networks: Theory and Pattern Recognition Applications, </title> <booktitle> NATO ASI Series F. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: my preliminary work, I have take an ad hoc initial approach, and should consider the work from automatic neural construction (such as [13, 14]) for superior approaches for network expansion, whether to periodically do network restructuring, either by pruning weights and nodes using a technique such as OBD or SBP <ref> [18, 25] </ref>, and splitting single networks into multiple networks containing only highly related tasks (I previously have discovered that the notion of task relevance can be important in knowledge transfer [51]). Given an adequate architecture, there is the issue of suitable metrics.
Reference: [26] <author> Andrew W. Moore. </author> <title> Efficient Memory-based Learning for Robot Control. </title> <type> PhD thesis, </type> <institution> Trinity Hall, University of Cambridge, </institution> <month> March </month> <year> 1991. </year> <note> Computer Science Technical Report 209. </note>
Reference-contexts: However, neural networks have very rarely been applied to long living agent settings, and in serial learning paradigms. Typically, memory-based machine learning mechanisms (such as in <ref> [26] </ref>) are utilized. Here, I am prepared to investigate a variety of neural network management techniques in order to reduce the usual drawbacks of local minima, concept drift, etc. <p> Or the agent's bearings may loosen, causing the action models to have to gradually shift. A large body of research is on going in this area (see [16]), and this has been addressed specifically for robot agents <ref> [26] </ref>. This latter approach, of monitoring the relevance of training examples, could be applied on top of our approach.
Reference: [27] <author> Nils J. Nilsson. </author> <title> Teleo-reactive programs for agent control. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1:139158, </volume> <month> Jan </month> <year> 1994. </year>
Reference-contexts: Agents have been embedded within simulated worlds since PENGI [2]. Such simulated worlds have been invaluable as a controlled means of investigating the performance of an agent <ref> [27, 53] </ref>, and in this work we too rely on the simulated environment described in section 3.3 for basic validation of results. More recently, researchers have been moving agents from such simulated environments onto real-world robots navigating office buildings [45] and picking up objects [28].
Reference: [28] <author> Stefan Nolfi and Domenico Parisi. </author> <title> Evolving non-trivial behaviors on real robots: an autonomous robot that picks up object. </title> <editor> In M. Gori and G. Soda, editors, </editor> <booktitle> Proceedings of Fourth Congress of the Italian Association of Artificial Intelligence, Topics in Artificial Intelligence, </booktitle> <pages> pages 243254. </pages> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: More recently, researchers have been moving agents from such simulated environments onto real-world robots navigating office buildings [45] and picking up objects <ref> [28] </ref>. We consider it an important goal to demonstrate that a life-long learning agent can be effective as an real-world agent. 5.
Reference: [29] <author> Joseph O'Sullivan. </author> <title> Towards a robot learning architecture. </title> <editor> In Wei-Min Shen, editor, </editor> <booktitle> Learning Action Models Papers from the 1993 AAAI Workshop, Technical Report WS-93-06, </booktitle> <pages> pages 4751. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA 94025, </address> <year> 1993. </year>
Reference-contexts: In addition to being a domain in which we have extensive experience <ref> [29, 30, 31, 45] </ref>, the domain has the interesting properties that there exists a clear tradeoff between the cost of gathering new training examples versus that of extracting more information from current knowledge, and that the robot's perception of the world is easy to define, yet complex to model, posing a
Reference: [30] <author> Joseph O'Sullivan and Karen Haigh. </author> <title> Xavier Manual. </title> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> http://www.cs.cmu.edu/Xavier, March 1994. </address> <month> 20 </month>
Reference-contexts: In addition to being a domain in which we have extensive experience <ref> [29, 30, 31, 45] </ref>, the domain has the interesting properties that there exists a clear tradeoff between the cost of gathering new training examples versus that of extracting more information from current knowledge, and that the robot's perception of the world is easy to define, yet complex to model, posing a <p> Carnegie Mellon University <ref> [30] </ref>. It is built on a 4 wheeled omni-directional base, 21 inches in 13 (a) (b) diameter.
Reference: [31] <author> Joseph O'Sullivan, Tom M. Mitchell, and Sebastian B. Thrun. </author> <title> Explanation Based Learning for Mobile Robot Perception. </title> <editor> In Katsushi Ikeuchi and Manuela Veloso, editors, </editor> <title> Symbolic Visual Learning. </title> <publisher> Oxford University Press, </publisher> <year> 1996. </year>
Reference-contexts: For instance, algorithms which learn useful representations for a particular domain [9, 4] and algorithms which learn domain specific models for use in further learning <ref> [31, 50, 36] </ref> have recently been developed (as will be discussed in detail in Section 4). Such algorithms have been described as lifelong learning algorithms [42, 48], or discussed in the context of learning to learn or inductive transfer [34]. <p> Action models can be exploited when learning novel tasks by using Explanation Based Neural Networks (EBNN) to bias the hypotheses considered during learning in favor of those that satisfy the predictions of the action models <ref> [49, 50, 31] </ref>. EBNN takes a 3 step approach to training a neural network from examples of a task and such action models: 1. <p> The quality of the learned knowledge will determine the accuracy of the approximation, but I have found that even weak action models can be exploited by EBNN <ref> [31] </ref>. that modify its view of the world. In its environment are tasks for it to learn. In this example, knowledge is transfered by exploiting the fact that Door 1 (S) Door 0 (Forward1M (S)), and by a common hidden layer sharing internal representations. <p> It is important to estimate what those benefits would be, and whether these improvements would complement or subsume each other. The experimental setting chosen to investigate this was originally used to investigate properties of EBNN <ref> [31] </ref>. As a robot moves along a corridor in Wean Hall, it repeatedly takes snapshots of the current state of the world. Each snapshot, a combination of 24 sonar readings and a coarse (10x10 pixel) camera image, makes up the input state S from which the tasks are predicted. <p> In addition to being a domain in which we have extensive experience <ref> [29, 30, 31, 45] </ref>, the domain has the interesting properties that there exists a clear tradeoff between the cost of gathering new training examples versus that of extracting more information from current knowledge, and that the robot's perception of the world is easy to define, yet complex to model, posing a
Reference: [32] <author> Micheal J. Pazzani and Dennis Kibler. </author> <title> The role of prior knowledge in inductive learning. </title> <booktitle> Machine Learning, </booktitle> <address> 9:5497, </address> <year> 1992. </year>
Reference-contexts: Serial MTL as discussed here appears to be an effective, if ad hoc, solution. We note that there have been some more negative results on the use of prior knowledge in learning <ref> [32] </ref> presents a simple example where prior knowledge hinders learning in FOCL, and [51] demonstrates that it is important for two tasks to be related, in order for them to share bias.
Reference: [33] <author> Dean Pomerleau. </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Related Work The importance of using constraints provided by domain-related knowledge in the learning process has long been acknowledged [8]. In fact, systems where built-in knowledge restricts what can be learned have arguably seen the most effective learning agents <ref> [7, 33] </ref>. Without built-in knowledge, there are fundamental limits on the generalization accuracy that can be expected from a learner that learns just from examples [12, 40]. <p> A life-long learning agent would provide a flexible means of specializing to a domain of interest, especially domains that are difficult to model in advance <ref> [24, 17, 33] </ref>. The proposed life-long learning agent improves its ability to learn thanks to combining two mechanisms. MTL [4, 10] is a generalization of work on hints [1, 47]. <p> For an exhaustive list see the introduction to [34], but some salient work includes learning an appropriate inductive learning algorithm for a domain [37], optimizing algorithm control parameters to a domain such as distance metrics [51], or generating synthetic data according to domain rules <ref> [33, 5] </ref>. Work has been done on the serial transfer of knowledge. [36] has explored transferring knowledge between networks, but with a primary goal of improving learning speed, rather than improved generalization. Indeed, in [35], negative results are reported for generalization performance.
Reference: [34] <author> Lorien Pratt and Sebastian Thrun, </author> <title> editors. Learning To Learn. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1997. </year>
Reference-contexts: Such algorithms have been described as lifelong learning algorithms [42, 48], or discussed in the context of learning to learn or inductive transfer <ref> [34] </ref>. An important contribution to these methods would be to embed them in a domain as an agent. The longer such an agent lives, the more knowledge it can learn, and the more opportunities will exist to exploit that knowledge. <p> Additional mechanisms exist to exploit domain knowledge, suitable for life-long learning agents, which could be explored in future work. For an exhaustive list see the introduction to <ref> [34] </ref>, but some salient work includes learning an appropriate inductive learning algorithm for a domain [37], optimizing algorithm control parameters to a domain such as distance metrics [51], or generating synthetic data according to domain rules [33, 5].
Reference: [35] <author> Lorien Y. Pratt. </author> <title> Transferring Previously Learned Back-Propagation Neural Networks to New Learning Tasks. </title> <type> PhD thesis, </type> <institution> Rutgers University, Department of Computer Science, </institution> <month> May </month> <year> 1993. </year> <note> Technical Report ML-TR-37. </note>
Reference-contexts: Work has been done on the serial transfer of knowledge. [36] has explored transferring knowledge between networks, but with a primary goal of improving learning speed, rather than improved generalization. Indeed, in <ref> [35] </ref>, negative results are reported for generalization performance. A life-long learning agent will have to face the the difficulties acknowledged with serial learning; preventing catastrophic forgetting while attempting to maintain a mutual benefit between tasks. Serial MTL as discussed here appears to be an effective, if ad hoc, solution.
Reference: [36] <author> Lorien Y. Pratt. </author> <title> Experiments on the Transfer of Knowledge between Neural Networks. In Computational Learning Theory and Natural Learning Systems, Constraints and Prospects, </title> <booktitle> chapter 14, </booktitle> <pages> pages 523560. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: For instance, algorithms which learn useful representations for a particular domain [9, 4] and algorithms which learn domain specific models for use in further learning <ref> [31, 50, 36] </ref> have recently been developed (as will be discussed in detail in Section 4). Such algorithms have been described as lifelong learning algorithms [42, 48], or discussed in the context of learning to learn or inductive transfer [34]. <p> Work has been done on the serial transfer of knowledge. <ref> [36] </ref> has explored transferring knowledge between networks, but with a primary goal of improving learning speed, rather than improved generalization. Indeed, in [35], negative results are reported for generalization performance.
Reference: [37] <author> L. Rendell, R. Seshu, and D. Tcheng. </author> <title> Layered concept-learning and dynamically-variable bias management. </title> <booktitle> In Proceedings of IJCAI-87, </booktitle> <pages> pages 308314, </pages> <year> 1987. </year>
Reference-contexts: Additional mechanisms exist to exploit domain knowledge, suitable for life-long learning agents, which could be explored in future work. For an exhaustive list see the introduction to [34], but some salient work includes learning an appropriate inductive learning algorithm for a domain <ref> [37] </ref>, optimizing algorithm control parameters to a domain such as distance metrics [51], or generating synthetic data according to domain rules [33, 5].
Reference: [38] <author> Mark Ring. </author> <title> Two methods for hierarchy learning in reinforcement enviroments. </title> <booktitle> In From Animals to Animats 2: Proceedings of the 2nd International Conference on Simulation of Adaptive Behaviour:. </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: This latter approach, of monitoring the relevance of training examples, could be applied on top of our approach. However, we feel that the research issues outlined are the priorities for this thesis, and will only consider the issue of concept drift if time allows. 15 Both CHILD <ref> [38] </ref> and IS [41, 55] are existing agents which learn to learn in simulated maze--like environments. Both solve complicated reinforcement learning tasks, and transfer the skills learned to similar but more complex tasks, improving learning ability.
Reference: [39] <author> David E. Rumelhart, Geoffery E.Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In David E. Rymelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing. Vol I+II. </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The basic learning mechanism is that of neural networks trained by a variant of Backpropagation <ref> [39] </ref>. Each example supplied to the agent is classified as one of a number of existing tasks, or as a novel task, in which case the capacity of the architecture is expanded. Without any further knowledge, an agent based on this architecture is a learning agent.
Reference: [40] <author> C. Schaffer. </author> <title> Overfitting avoidance as bias. </title> <booktitle> Machine Learning, </booktitle> <address> 10:153178, </address> <year> 1993. </year>
Reference-contexts: In fact, systems where built-in knowledge restricts what can be learned have arguably seen the most effective learning agents [7, 33]. Without built-in knowledge, there are fundamental limits on the generalization accuracy that can be expected from a learner that learns just from examples <ref> [12, 40] </ref>. This results in it being increasingly difficult for a system to 14 generalize well as the the complexity of the learning task increases [15, 52].
Reference: [41] <author> Jurgen Schmidhuber, Jieyu Zhao, and Marrco. </author> <title> Simple principles of metalearning. </title> <type> Technical Report IDSIA-69-96, </type> <address> IDSIA, Corso Elvezia 36, CH-6900-LUGABO, Switzerland, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: However, we feel that the research issues outlined are the priorities for this thesis, and will only consider the issue of concept drift if time allows. 15 Both CHILD [38] and IS <ref> [41, 55] </ref> are existing agents which learn to learn in simulated maze--like environments. Both solve complicated reinforcement learning tasks, and transfer the skills learned to similar but more complex tasks, improving learning ability.
Reference: [42] <author> Bart Selman, Rodney A. Brooks, Thomas Dean, Eric Horvitz, Tom M. Mitchell, and Nils J. Nilsson. </author> <title> Challenge problems for artificial intelligence. </title> <booktitle> In Proceedings of AAAI-96, </booktitle> <address> Menlo Park, CA, </address> <month> August </month> <year> 1996. </year> <note> AAAI Press. </note>
Reference-contexts: Such algorithms have been described as lifelong learning algorithms <ref> [42, 48] </ref>, or discussed in the context of learning to learn or inductive transfer [34]. An important contribution to these methods would be to embed them in a domain as an agent.
Reference: [43] <author> Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. </author> <title> Tangent prop a formalism for specifying selected invariances in an adaptive network. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lipmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 895903. </pages> <publisher> Morgan Kaufmann, </publisher> <month> December </month> <year> 1992. </year>
Reference-contexts: MTL recognizes that when learning many related tasks, a Backpropagation network can use these tasks as inductive bias for each other and thus learn better. EBNN [50, 49] extends earlier work that exploited hand crafted slope information during learning <ref> [43] </ref> by itself learning slope constraints.
Reference: [44] <author> Reid Simmons. </author> <title> The curvature-velocity method for local obstacle avoidance. </title> <booktitle> In International Conference on Robotics and Automation, </booktitle> <address> Minneapolis MN, </address> <month> April </month> <year> 1996. </year> <month> 21 </month>
Reference-contexts: In addition, Amelia has a 4 degree of movement gripper, allowing for some simple manipulation tasks. Amelia has a large number of basic capabilities which have been tested over the past years: a low level navigation system for obstacle avoidance <ref> [44] </ref>, a high level system for topological map based navigation [46], and a large repertoire of elementary actions including trash collection, marker tracking and wall following.
Reference: [45] <author> Reid Simmons, Richard Goodwin, Karen Haigh, Sven Koenig, and Joseph O'Sullivan. </author> <title> A modular architecture for office delivery robots. </title> <booktitle> In The First International Conference on Autonomous Agents, </booktitle> <month> Feb </month> <year> 1997. </year>
Reference-contexts: In addition to being a domain in which we have extensive experience <ref> [29, 30, 31, 45] </ref>, the domain has the interesting properties that there exists a clear tradeoff between the cost of gathering new training examples versus that of extracting more information from current knowledge, and that the robot's perception of the world is easy to define, yet complex to model, posing a <p> More recently, researchers have been moving agents from such simulated environments onto real-world robots navigating office buildings <ref> [45] </ref> and picking up objects [28]. We consider it an important goal to demonstrate that a life-long learning agent can be effective as an real-world agent. 5.
Reference: [46] <author> Reid Simmons and Sven Koenig. </author> <title> Probabilistic navigation in partially observable environments. </title> <booktitle> In International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Amelia has a large number of basic capabilities which have been tested over the past years: a low level navigation system for obstacle avoidance [44], a high level system for topological map based navigation <ref> [46] </ref>, and a large repertoire of elementary actions including trash collection, marker tracking and wall following. Followings the ideas in [20, 21], we have previously had experience with appearance-based recognition tasks in this domain, discriminating within an 12 object database, based on around 30 examples of each object [51].
Reference: [47] <author> Steven C. Suddarth and Alistair D. C. Holden. </author> <title> Symbolic-neural systems and the use of hints for developing complex systems. </title> <journal> Int. J. Man-Machine Studies, </journal> <volume> 35:291311, </volume> <year> 1991. </year>
Reference-contexts: The proposed life-long learning agent improves its ability to learn thanks to combining two mechanisms. MTL [4, 10] is a generalization of work on hints <ref> [1, 47] </ref>. With hints, extra tasks are important learnable features of a main task, and are supplied as network outputs to pass domain specific knowledge to the network.
Reference: [48] <author> Sebastian Thrun. </author> <title> Lifelong learning: A case study. </title> <type> Technical Report CMU-CS-95-208, </type> <institution> School Of Computer Science, Carnegie Mellon University, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Such algorithms have been described as lifelong learning algorithms <ref> [42, 48] </ref>, or discussed in the context of learning to learn or inductive transfer [34]. An important contribution to these methods would be to embed them in a domain as an agent.
Reference: [49] <author> Sebastian Thrun. </author> <title> Explanation-Based Neural Network Learning: A Lifelong Learning Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1996. </year> <note> to appear. </note>
Reference-contexts: Action models can be exploited when learning novel tasks by using Explanation Based Neural Networks (EBNN) to bias the hypotheses considered during learning in favor of those that satisfy the predictions of the action models <ref> [49, 50, 31] </ref>. EBNN takes a 3 step approach to training a neural network from examples of a task and such action models: 1. <p> MTL recognizes that when learning many related tasks, a Backpropagation network can use these tasks as inductive bias for each other and thus learn better. EBNN <ref> [50, 49] </ref> extends earlier work that exploited hand crafted slope information during learning [43] by itself learning slope constraints. <p> (EBNN to chess, object recognition, robot control, see [50], MTL to object recognition, medical diagnostics, see [10]) but prior to this research combining these approaches had not been explored, nor had a life-long learning agent been constructed from these approaches (although the idea of a life-long learning agent is from <ref> [49] </ref>). Additional mechanisms exist to exploit domain knowledge, suitable for life-long learning agents, which could be explored in future work.
Reference: [50] <author> Sebastian B. Thrun and Tom M. Mitchell. </author> <title> Integrating inductive neural network learning and explanation-based learning. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <address> Chamberry, France, </address> <month> July </month> <year> 1993. </year> <pages> IJCAI. </pages>
Reference-contexts: For instance, algorithms which learn useful representations for a particular domain [9, 4] and algorithms which learn domain specific models for use in further learning <ref> [31, 50, 36] </ref> have recently been developed (as will be discussed in detail in Section 4). Such algorithms have been described as lifelong learning algorithms [42, 48], or discussed in the context of learning to learn or inductive transfer [34]. <p> Action models can be exploited when learning novel tasks by using Explanation Based Neural Networks (EBNN) to bias the hypotheses considered during learning in favor of those that satisfy the predictions of the action models <ref> [49, 50, 31] </ref>. EBNN takes a 3 step approach to training a neural network from examples of a task and such action models: 1. <p> MTL recognizes that when learning many related tasks, a Backpropagation network can use these tasks as inductive bias for each other and thus learn better. EBNN <ref> [50, 49] </ref> extends earlier work that exploited hand crafted slope information during learning [43] by itself learning slope constraints. <p> EBNN [50, 49] extends earlier work that exploited hand crafted slope information during learning [43] by itself learning slope constraints. Both of these methods have been applied with success in various application domains (EBNN to chess, object recognition, robot control, see <ref> [50] </ref>, MTL to object recognition, medical diagnostics, see [10]) but prior to this research combining these approaches had not been explored, nor had a life-long learning agent been constructed from these approaches (although the idea of a life-long learning agent is from [49]).
Reference: [51] <author> Sebastian B. Thrun and Joseph O'Sullivan. </author> <title> Discovering Structure in Multiple Learning Tasks: The TC Algorithm. </title> <booktitle> In Proceedings of ICML, </booktitle> <address> Torino, Italy, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: whether to periodically do network restructuring, either by pruning weights and nodes using a technique such as OBD or SBP [18, 25], and splitting single networks into multiple networks containing only highly related tasks (I previously have discovered that the notion of task relevance can be important in knowledge transfer <ref> [51] </ref>). Given an adequate architecture, there is the issue of suitable metrics. The priority of course is to reduce the number of examples required to learn novel tasks. <p> Followings the ideas in [20, 21], we have previously had experience with appearance-based recognition tasks in this domain, discriminating within an 12 object database, based on around 30 examples of each object <ref> [51] </ref>. As such, we believe a suitable family of tasks in this domain is that of recognizing objects with which agent can interact. A second, more ambitious family of tasks, would be actively searching for or interacting with those objects. <p> For an exhaustive list see the introduction to [34], but some salient work includes learning an appropriate inductive learning algorithm for a domain [37], optimizing algorithm control parameters to a domain such as distance metrics <ref> [51] </ref>, or generating synthetic data according to domain rules [33, 5]. Work has been done on the serial transfer of knowledge. [36] has explored transferring knowledge between networks, but with a primary goal of improving learning speed, rather than improved generalization. <p> Serial MTL as discussed here appears to be an effective, if ad hoc, solution. We note that there have been some more negative results on the use of prior knowledge in learning [32] presents a simple example where prior knowledge hinders learning in FOCL, and <ref> [51] </ref> demonstrates that it is important for two tasks to be related, in order for them to share bias. <p> As with our domain, examples are expensive, and FourEyes attempts to learn domain bias during learning, to reduce the future need for examples. This domain bias is an adaption of the weighting for nearest neighbor learning, something we ourselves have considered previously <ref> [51] </ref>, and is similar to the shared internal representation used here. FourEyes is not itself able to act, and so is unable to use action models as a source of domain knowledge.
Reference: [52] <author> L. G. Valiant. </author> <title> A Theory of the Learnable. </title> <journal> Comm. ACM, </journal> <volume> 27:11341142, </volume> <year> 1984. </year>
Reference-contexts: Without built-in knowledge, there are fundamental limits on the generalization accuracy that can be expected from a learner that learns just from examples [12, 40]. This results in it being increasingly difficult for a system to 14 generalize well as the the complexity of the learning task increases <ref> [15, 52] </ref>. A life-long learning agent would provide a flexible means of specializing to a domain of interest, especially domains that are difficult to model in advance [24, 17, 33]. The proposed life-long learning agent improves its ability to learn thanks to combining two mechanisms.
Reference: [53] <author> S. Vere and T. Bickmore. </author> <title> A basic agent. </title> <booktitle> Computational Intelligence, </booktitle> <address> 6:4160, </address> <year> 1990. </year>
Reference-contexts: Agents have been embedded within simulated worlds since PENGI [2]. Such simulated worlds have been invaluable as a controlled means of investigating the performance of an agent <ref> [27, 53] </ref>, and in this work we too rely on the simulated environment described in section 3.3 for basic validation of results. More recently, researchers have been moving agents from such simulated environments onto real-world robots navigating office buildings [45] and picking up objects [28].
Reference: [54] <author> M. Wooldridge and N. R. Jennings. </author> <title> Intelligent agents: </title> <journal> Theory and practice. Knowledge Engineering Review, </journal> <volume> 10(2), </volume> <year> 1995. </year>
Reference-contexts: 1. Introduction One common factor exists in the majority of previous work on agents <ref> [54, 2] </ref>: As the agent ages, and further new tasks arise, the agent does not improve its ability to learn those tasks.
Reference: [55] <author> Jieyu Zhao and Juergen Schmidhuber. </author> <title> Incremental self-improvement for lifelong multi-agent reinforcement learning. </title> <booktitle> In Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Cape Cod, USA, </address> <year> 1996. </year> <month> 22 </month>
Reference-contexts: However, we feel that the research issues outlined are the priorities for this thesis, and will only consider the issue of concept drift if time allows. 15 Both CHILD [38] and IS <ref> [41, 55] </ref> are existing agents which learn to learn in simulated maze--like environments. Both solve complicated reinforcement learning tasks, and transfer the skills learned to similar but more complex tasks, improving learning ability.
References-found: 55

