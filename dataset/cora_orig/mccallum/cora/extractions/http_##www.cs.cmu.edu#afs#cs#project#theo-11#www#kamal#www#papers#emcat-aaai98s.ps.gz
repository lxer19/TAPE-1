URL: http://www.cs.cmu.edu/afs/cs/project/theo-11/www/kamal/www/papers/emcat-aaai98s.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/project/theo-11/www/kamal/www/resume.html
Root-URL: 
Email: knigam@cs.cmu.edu  mccallum@cs.cmu.edu  thrun@cs.cmu.edu  mitchell+@cs.cmu.edu  
Title: Learning to Classify Text from Labeled and Unlabeled Documents  
Author: Kamal Nigam Andrew McCallum zy Sebastian Thrun Tom Mitchell 
Address: Pittsburgh, PA 15213  4616 Henry Street Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  Justsystem Pittsburgh Research Center  
Note: Submitted (1/98) to the 15th National Conference on Artificial Intelligence (AAAI-98).  
Abstract: This paper shows that the accuracy of learned text classifiers can be improved by augmenting small numbers of labeled training documents with a large pool of unlabeled documents. This is significant because in many important text classification problems obtaining classification labels is expensive, while large quantities of unlabeled documents are readily available. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text, based on combining Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error up to 30%. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Cheeseman, P., and Stutz, J. </author> <year> 1996. </year> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In Fayyad, U., ed., </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle>
Reference-contexts: Ghahramani and Jordan have used EM with mixture models to fill in missing values (Ghahramani & Jordan 1994). The emphasis of their work was on missing feature values, where we focus on augmenting a very small but complete set of labeled data. The AutoClass project <ref> (Cheeseman & Stutz 1996) </ref> investigated the combination of the EM algorithm with an underlying model of a Naive Bayes classifier. The emphasis of their research was the discovery of novel clusterings for unsupervised learning over unlabeled data. AutoClass has not been applied to text or classification.
Reference: <author> Cohen, W., and Singer, Y. </author> <year> 1997. </year> <title> Context-sensitive learning methods for text categorization. </title> <booktitle> In Proceedings of ACM SIGIR Conference. </booktitle>
Reference: <author> Craven, M.; Freitag, D.; McCallum, A.; Mitchell, T.; Nigam, K.; and Quek, C. </author> <year> 1998. </year> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <type> Technical report, </type> <institution> School of Computer Science, </institution> <address> CMU. </address>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles (Lewis & Gale 1994) and web pages <ref> (Craven et al. 1998) </ref>, automatically learn the reading interests of users (Pazzani, Muramatsu, & Billsus 1996; Lang 1995), and automatically sort electronic mail (Lewis & Knowles 1997). <p> Best performance was obtained with no feature selection, and by normalizing word counts by document length. On each trial, 20% of the documents were randomly selected for placement in the test set. The WebKB data set <ref> (Craven et al. 1998) </ref> contains 8145 web pages gathered from university computer science departments. For four departments, all web pages were included; additionally, there are many pages from an assortment of other universities. The pages are divided into seven categories: student, faculty, staff, course, project, department and other. <p> We did not use stemming or a stoplist; we found that using a stoplist actually hurt performance because, for example, "my" is the fourth-ranked word by information gain, and is an excellent indicator of a student homepage. As done previously <ref> (Craven et al. 1998) </ref>, we use only the 2000 most informative words, as measured by information gain. Accuracy results presented below are an average of four test/train splits, holding out and testing on each complete university in turn.
Reference: <author> Dempster, A. P.; Laird, N. M.; and Rubin, D. B. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM. algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> Series B 39 </volume> <pages> 1-38. </pages>
Reference-contexts: The specific approach we describe here is based on a combination of two well-known learning algorithms: the naive Bayes classifier (Lewis & Ringuette 1994) and the Expectation Maximization (EM) algorithm <ref> (Dempster, Laird, & Rubin 1977) </ref>. The naive Bayes algorithm is a one of a class of statistical text classifiers that use word frequencies as features. Other examples include Rocchio (Salton 1991; Rocchio 1971), regression models (Yang & Chute 1993), k-nearest-neighbor (Yang & Pederson 1997) and Support Vector Machines (Joachims 1997b). <p> Recalculate the classifier parameters 0j and j given the probabilistically assigned labels (Equa tions 8 and 9). Table 1: The Algorithm l c (jD; z) = + i=1 j=1 (13) Note that z i is already known for labeled documents. Dempster <ref> (Dempster, Laird, & Rubin 1977) </ref> shows that by working with the expected value of z at iteration k, denoted by Q (k) , we can find a local maximum for l (jD) by iterating the following two steps: * E-step: Set Q (k) = E [zjD; (k) ]. * M-step: Set
Reference: <author> Domingos, P., and Pazzani, M. </author> <year> 1997. </year> <title> Beyond independence: Conditions for the optimality of the simple bayesian classifier. </title> <booktitle> Machine Learning 29 </booktitle> <pages> 103-130. </pages>
Reference-contexts: Domingos and Pazzani shed some light on why the word independence assumption can be so violated, yet performance remains so good <ref> (Domingos & Pazzani 1997) </ref>. Using EM to Incorporate Unlabeled Data Now we address the question of how to combine labeled and unlabeled training documents to produce a good estimate of . This is a special case of the more general missing values formulation, as presented by (Ghahra-mani & Jordan 1994). <p> However, the complexity of natural language text will not soon be completely captured by statistical models. It is interesting then, to consider the sensitivity of a classifier's model to data inconsistent with that model. Domingos and Pazzani analyze this for the word independence assumption <ref> (Domingos & Pazzani 1997) </ref>. With our results on Reuters21578 we begin to study cases in which the data violate the assumptions about mixture models and the correspondence between components and classes. The results suggest exploring the use of more complex mixture models that better correspond to textual data distributions.
Reference: <author> Ghahramani, Z., and Jordan, M. </author> <year> 1994. </year> <title> Supervised learning from incomplete data via an EM approach. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS 6). </booktitle> <publisher> Morgan Kauffman Publishers. </publisher>
Reference-contexts: Ghahramani and Jordan have used EM with mixture models to fill in missing values <ref> (Ghahramani & Jordan 1994) </ref>. The emphasis of their work was on missing feature values, where we focus on augmenting a very small but complete set of labeled data.
Reference: <author> Joachims, T. </author> <year> 1997a. </year> <title> A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In International Conference on Machine Learning (ICML). </booktitle>
Reference-contexts: We present experimental results with three different text corpora. 1 In two cases we restrict ourselves to the more populous classes of a data set in order to have enough unlabeled data to perform meaningful experiments. The 20 Newsgroups data set <ref> (Joachims 1997a) </ref>, col lected by Ken Lang, consists of 20,017 articles divided almost evenly among 20 different UseNet discussion 1 All three of these data sets are available on the Internet. See http://www.cs.cmu.edu/~textlearning and http://www.research.att.com/~lewis. groups.
Reference: <author> Joachims, T. </author> <year> 1997b. </year> <title> Text categorization with Support Vector Machines: Learning with many relevant features. </title> <type> Technical Report LS8-Report, </type> <institution> University of Dortmund. </institution>
Reference-contexts: The naive Bayes algorithm is a one of a class of statistical text classifiers that use word frequencies as features. Other examples include Rocchio (Salton 1991; Rocchio 1971), regression models (Yang & Chute 1993), k-nearest-neighbor (Yang & Pederson 1997) and Support Vector Machines <ref> (Joachims 1997b) </ref>. EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
Reference: <author> Lang, K. </author> <year> 1995. </year> <title> Newsweeder: Learning to filter netnews. </title> <booktitle> In International Conference on Machine Learning (ICML), </booktitle> <pages> 331-339. </pages>
Reference-contexts: Take, for example, the task of learning which newsgroup articles are of interest to a person reading UseNet news, as examined by Lang <ref> (Lang 1995) </ref>. After reading and classifying about 1000 articles, precision of the learned classifier was about 50% for the top 10% of documents ranked by the classifier.
Reference: <author> Lewis, D., and Gale. </author> <year> 1994. </year> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of ACM SIGIR Conference. </booktitle>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles <ref> (Lewis & Gale 1994) </ref> and web pages (Craven et al. 1998), automatically learn the reading interests of users (Pazzani, Muramatsu, & Billsus 1996; Lang 1995), and automatically sort electronic mail (Lewis & Knowles 1997). <p> The specific approach we describe here is based on a combination of two well-known learning algorithms: the naive Bayes classifier <ref> (Lewis & Ringuette 1994) </ref> and the Expectation Maximization (EM) algorithm (Dempster, Laird, & Rubin 1977). The naive Bayes algorithm is a one of a class of statistical text classifiers that use word frequencies as features. <p> Approaches differ in their methods for selecting the unlabeled example to classify. Three such examples are relevance sampling (Salton & Buckley 1990), uncertainty sampling <ref> (Lewis & Gale 1994) </ref>, and a "Query By Committee" approach (Liere & Tadepalli 1997). On several corpora, some other statistical text classifiers (Yang & Pederson 1997; Joachims 1997b; Co-hen & Singer 1997) have been shown to mildly outperform naive Bayes in text classification.
Reference: <author> Lewis, D. D., and Knowles, K. A. </author> <year> 1997. </year> <title> Threading electronic mail: A preliminary study. </title> <booktitle> Information Processing and Management 33(2) </booktitle> <pages> 209-217. </pages>
Reference-contexts: These text classification algorithms have been used to automatically catalog news articles (Lewis & Gale 1994) and web pages (Craven et al. 1998), automatically learn the reading interests of users (Pazzani, Muramatsu, & Billsus 1996; Lang 1995), and automatically sort electronic mail <ref> (Lewis & Knowles 1997) </ref>. One key difficulty with these current algorithms, and the issue addressed by this paper, is that they require a large, often prohibitive, number of labeled training examples to learn accurately.
Reference: <author> Lewis, D., and Ringuette, M. </author> <year> 1994. </year> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> 81-93. </pages>
Reference-contexts: There are statistical text learning algorithms that can be trained to approximately classify documents, given a sufficient set of labeled training examples. These text classification algorithms have been used to automatically catalog news articles <ref> (Lewis & Gale 1994) </ref> and web pages (Craven et al. 1998), automatically learn the reading interests of users (Pazzani, Muramatsu, & Billsus 1996; Lang 1995), and automatically sort electronic mail (Lewis & Knowles 1997). <p> The specific approach we describe here is based on a combination of two well-known learning algorithms: the naive Bayes classifier <ref> (Lewis & Ringuette 1994) </ref> and the Expectation Maximization (EM) algorithm (Dempster, Laird, & Rubin 1977). The naive Bayes algorithm is a one of a class of statistical text classifiers that use word frequencies as features. <p> Approaches differ in their methods for selecting the unlabeled example to classify. Three such examples are relevance sampling (Salton & Buckley 1990), uncertainty sampling <ref> (Lewis & Gale 1994) </ref>, and a "Query By Committee" approach (Liere & Tadepalli 1997). On several corpora, some other statistical text classifiers (Yang & Pederson 1997; Joachims 1997b; Co-hen & Singer 1997) have been shown to mildly outperform naive Bayes in text classification.
Reference: <author> Liere, and Tadepalli. </author> <year> 1997. </year> <title> Active learning with committees for text categorization. </title> <booktitle> In AAAI-97. </booktitle>
Reference-contexts: Approaches differ in their methods for selecting the unlabeled example to classify. Three such examples are relevance sampling (Salton & Buckley 1990), uncertainty sampling (Lewis & Gale 1994), and a "Query By Committee" approach <ref> (Liere & Tadepalli 1997) </ref>. On several corpora, some other statistical text classifiers (Yang & Pederson 1997; Joachims 1997b; Co-hen & Singer 1997) have been shown to mildly outperform naive Bayes in text classification.
Reference: <author> Miller, D. J., and Uyar, H. S. </author> <year> 1997. </year> <title> A mixture of experts classifier with learning based on both labelled and unlabelled data. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS 9). </booktitle>
Reference-contexts: Previous supervised algorithms for learning to classify from text do not incorporate unlabeled data. A similar approach was used by Miller and Uyar <ref> (Miller & Uyar 1997) </ref> for non-text data sources. We adapt this approach for the naive Bayes text classifier and conduct a thorough empirical analysis.
Reference: <author> Pazzani, M. J.; Muramatsu, J.; and Billsus, D. </author> <year> 1996. </year> <title> Syskill & Webert: Identifying interesting Web sites. </title> <booktitle> In AAAI-96. </booktitle>
Reference: <author> Rocchio, J. </author> <year> 1971. </year> <title> Relevance feedback in information retrieval. In The SMART Retrieval System:Experiments in Automatic Document Processing. </title> <publisher> Prentice Hall. </publisher> <pages> chapter 14, 313-323. </pages>
Reference: <author> Salton, G., and Buckley, C. </author> <year> 1990. </year> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science 41(4) </journal> <pages> 288-297. </pages>
Reference-contexts: Approaches differ in their methods for selecting the unlabeled example to classify. Three such examples are relevance sampling <ref> (Salton & Buckley 1990) </ref>, uncertainty sampling (Lewis & Gale 1994), and a "Query By Committee" approach (Liere & Tadepalli 1997). On several corpora, some other statistical text classifiers (Yang & Pederson 1997; Joachims 1997b; Co-hen & Singer 1997) have been shown to mildly outperform naive Bayes in text classification.
Reference: <author> Salton, G. </author> <year> 1991. </year> <title> Developments in automatic text retrieval. </title> <booktitle> Science 253 </booktitle> <pages> 974-979. </pages>
Reference: <author> Shahshahani, B., and Landgrebe, D. </author> <year> 1994. </year> <title> The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon. </title> <journal> IEEE Trans. on Geoscience and Remote Sensing 32(5) </journal> <pages> 1087-1095. </pages>
Reference: <author> Vapnik, V. </author> <year> 1982. </year> <title> Estimations of dependences based on statistical data. </title> <publisher> Springer Publisher. </publisher>
Reference-contexts: Given these underlying assumptions of how the data is produced, the task of learning a text classifier consists of forming an estimate for given a set of data and their associated class labels. The Bayes optimal estimates for the word probabilities jn are <ref> (Vapnik 1982) </ref> 1 + i=1 N (w n ; d i )P (c j jd i ) P jV j P jDj : (8) where N (w n ; d i ) is the count of the number of times word w n occurs in document d i and where P
Reference: <author> Yang, Y., and Chute, C. G. </author> <year> 1993. </year> <title> An application of least squares fit mapping to text information retrieval. </title> <booktitle> In Proceedings of the Sixteenth Annual International ACM SIGIR Conference. </booktitle>
Reference-contexts: The naive Bayes algorithm is a one of a class of statistical text classifiers that use word frequencies as features. Other examples include Rocchio (Salton 1991; Rocchio 1971), regression models <ref> (Yang & Chute 1993) </ref>, k-nearest-neighbor (Yang & Pederson 1997) and Support Vector Machines (Joachims 1997b). EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
Reference: <author> Yang, Y., and Pederson, J. </author> <year> 1997. </year> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In ICML-97, </booktitle> <pages> 412-420. </pages>
Reference-contexts: The naive Bayes algorithm is a one of a class of statistical text classifiers that use word frequencies as features. Other examples include Rocchio (Salton 1991; Rocchio 1971), regression models (Yang & Chute 1993), k-nearest-neighbor <ref> (Yang & Pederson 1997) </ref> and Support Vector Machines (Joachims 1997b). EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data.
References-found: 22

