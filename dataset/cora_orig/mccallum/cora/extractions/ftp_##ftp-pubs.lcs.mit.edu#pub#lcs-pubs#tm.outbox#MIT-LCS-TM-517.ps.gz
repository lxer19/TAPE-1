URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tm.outbox/MIT-LCS-TM-517.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/catatm.html
Root-URL: 
Title: CRL: High-Performance All-Software Distributed Shared Memory  
Author: Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: This paper introduces the C Region Library (CRL), a new all-software distributed shared memory (DSM) system. CRL requires no special compiler, hardware, or operating system support beyond the ability to send and receive messages. It provides a simple, portable shared address space programming model that is capable of delivering good performance on a wide range of multiprocessor and distributed system architectures. We have developed CRL implementations for two platforms: the CM-5, a commercial multi-computer, and the MIT Alewife machine, an experimental multiprocessor offering efficient support for both message passing and shared memory. We present results for up to 128 processors on the CM-5 and up to 32 processors on Alewife. In a set of controlled experiments, we demonstrate that CRL is the first all-software DSM system capable of delivering performance competitive with hardware DSMs. CRL achieves speedups within 30% of those provided by Alewife's native support for shared memory, even for challenging applications (Barnes-Hut) and small problem sizes.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk L. Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Ken Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Using the CM-5 implementation of CRL, we have run applications on systems with up to 128 processors. CRL is the first all-software DSM system capable of delivering performance competitive with hardware DSMs. To demonstrate this fact, we ported our CRL implementation to the MIT Alewife machine <ref> [1] </ref>. Since Alewife provides efficient hardware support for both message passing and shared memory communication styles [23], the performance of applications running under CRL (using only message passing for communication) can be readily compared to the performance of the same applications when hardware-supported shared memory is used instead. <p> Since this latency is on par with that measured for the CM-5, we expect that the performance of our CRL on the CM-5 is indicative of what should be possible for implementations targeting networks of workstations using current- or next-generation technology. 4.2 Alewife Alewife <ref> [1] </ref> is an experimental distributed memory multiprocessor. The basic Alewife architecture consists of processor/memory nodes communicating over a packet-switched interconnection network organized as a low-dimensional mesh (see Figure 2).
Reference: [2] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The basic Alewife architecture consists of processor/memory nodes communicating over a packet-switched interconnection network organized as a low-dimensional mesh (see Figure 2). Each processor/memory node consists of a Sparcle processor <ref> [2] </ref>, an off-the-shelf floating-point unit (FPU), a 64-kilobyte unified instruction/data cache (direct mapped, 16-byte lines), eight megabytes of DRAM, the local portion of the interconnection network, and a Communications and Memory Management Unit (CMMU).
Reference: [3] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: distributed systems. 7 Related Work Except for the notion of mapping and unmapping regions, the programming interface CRL presents to the end user is similar to that provided by Shared Regions [35]; the same basic notion of synchronized access (operations) to regions (objects) also exists in other DSM programming systems <ref> [3, 11] </ref>. The Shared Regions work arrived at this interface from a different set of constraints; their goal was to provide software coherence mechanisms on machines that support non-cache-coherent shared memory in hardware. <p> All synchronization must be effected through hardware DSM mechanisms. In contrast, CRL is an all-software DSM system in which all communication and synchronization is implemented using software DSM techniques. Several all-software DSM systems that employ an object-based approach have been developed (e.g., Amber [13], Orca <ref> [3] </ref>). Like CRL, these systems effect coherence at the level of application-defined regions of memory (objects). Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. <p> Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. Existing systems of this type either require the use of an entirely new object-oriented language <ref> [3, 20] </ref> or only allow the use of a subset of an existing one [13]. In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam [36] have described SAM, a shared object system for distributed memory machines.
Reference: [4] <author> Henri E. Bal and M. Frans Kaashoek. </author> <title> Object Distribution in Orca using Compile-Time and Run-Time Techniques. </title> <booktitle> In Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA'93), </booktitle> <pages> pages 162-177, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Other types of DSM systems (e.g., those in which threads of computation are migrated to the data they reference) are also possible <ref> [4, 7, 13, 19] </ref>; a suitably generalized version of the classification scheme presented here could likely be applied to these systems as well. We classify systems by three basic mechanisms required to implement DSM and whether or not those mechanisms are implemented in hardware or software.
Reference: [5] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In Proceedings of the 38th IEEE Computer Society International Conference (COMPCON'93), </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: From this perspective, CRL provides sequential consistency for read and write operations in the same sense that a sequentially consistent hardware-based DSM does for individual loads and stores. In terms of individual loads and stores, CRL provides a memory/coherence model similar to entry <ref> [5] </ref> or release consistency [17]. Loads and stores to global data are only allowed within properly synchronized sections (operations), and modifications to a region are only made visible to other processors after the appropriate release operation (a call to rgn_end_write). <p> Annotations similar to those required by CRL are necessary in aggressive hardware DSM implementations (e.g., those providing release consistency) when writing to shared data. CRL requires such annotations whether reading or writing shared data, similar to entry consistency <ref> [5] </ref>. Based on our experience with the applications described in this paper, we feel that the additional programming overhead of doing so is minimal. Therefore, we believe CRL is an effective approach to providing a distributed shared memory abstraction. Calls to CRL library functions also provide a performance advantage. <p> These primitives are significantly different than standard shared memory models. Like SAM, CRL is implemented as a portable C library. Both CRL and SAM achieve good performance on distributed memory machines. Midway is a software DSM system based on entry consistency <ref> [5] </ref>. Both mostly software and all-software version of Midway have been implemented [46]. CRL differs from Midway in provided a simpler programming model that bundles synchronization and data access.
Reference: [6] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: We have developed an implementation of CRL for Thinking Machines' CM-5 family of multiprocessors. Because today's networks of workstations offer interprocessor communication performance rivaling that of the CM-5 <ref> [6, 30, 41, 43] </ref>, we believe that the performance of our CRL implementation for the CM-5 is indicative of what should be possible for an implementation targeting networks of workstations using current technology. Using the CM-5 implementation of CRL, we have run applications on systems with up to 128 processors. <p> While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prenegotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [6, 30, 41, 43] </ref>. For example, Thekkath et al. [42] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a FORE ATM network.
Reference: [7] <author> Martin C. Carlisle, Anne Rogers, John H. Reppy, and Laurie J. Hendren. </author> <title> Early Experiences with Olden. </title> <booktitle> In Conference Record of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Other types of DSM systems (e.g., those in which threads of computation are migrated to the data they reference) are also possible <ref> [4, 7, 13, 19] </ref>; a suitably generalized version of the classification scheme presented here could likely be applied to these systems as well. We classify systems by three basic mechanisms required to implement DSM and whether or not those mechanisms are implemented in hardware or software.
Reference: [8] <author> John B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Mostly Software Many software DSM systems are actually mostly software systems in which the hit/miss check functionality is implemented in hardware (e.g., by leveraging off of virtual memory protection mechanisms). Typical examples of mostly software systems include Ivy [28], Munin <ref> [8] </ref>, and TreadMarks [14]; coherence units in these systems are the size of virtual memory pages. Blizzard [37] implements a similar scheme on the CM-5 at the granularity of individual cache lines. <p> In terms of comparing message passing and shared memory, most other previous work has either compared the performance of applications written and tuned specifically for each programming model <ref> [8, 12] </ref> or looked at the performance gains made possible by augmenting a hardware DSM system with message passing primitives [23].
Reference: [9] <author> David L. Chaiken and Anant Agarwal. </author> <title> Software-Extended Coherent Shared Memory: Performance and Cost. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314-324, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Mostly Hardware As discussed in Section 4, Alewife implements a mostly hardware DSM system the processor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected <ref> [9] </ref>. Dir 1 SW and its variations [18, 45] are also mostly hardware schemes. The Stanford FLASH multiprocessor [25] and Wisconsin Typhoon architecture [33] represent a different kind of mostly hardware DSM system. <p> Alewife provides efficient support for both coherent shared-memory and message-passing communication styles. Shared memory support is provided through an implementation of the LimitLESS cache coherence scheme <ref> [9] </ref>: limited sharing of memory blocks (up to five remote readers) is supported in hardware; higher-degree sharing is handled by trapping the processor on the home memory node and extending the small hardware directory in software. <p> Unlike Midway and Cid, CRL runs on two large-scale platforms and has been shown to deliver performance competitive with hardware DSM systems. Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware <ref> [9, 18, 45] </ref>, mostly software vs. all-software [37, 46]), but to our knowledge, only Cox et al. [14] have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [10] <author> Rohit Chandra, Kourosh Gharachorloo, Vijayaraghavan Soundararajan, and Anoop Gupta. </author> <title> Performance Evaluation of Hybrid Hardware and Software Distributed Shared Memory Protocols. </title> <booktitle> In Proceedings of the Eighth International Conference on Supercomputing, </booktitle> <pages> pages 274-288, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Chandra et al. <ref> [10] </ref> propose a hybrid DSM protocol in which annotations similar to those described in this paper are used to demark access to regions of shared data.
Reference: [11] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> Data Locality and Load Balancing in COOL. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 249-259, </pages> <month> May </month> <year> 1993. </year> <month> 22 </month>
Reference-contexts: distributed systems. 7 Related Work Except for the notion of mapping and unmapping regions, the programming interface CRL presents to the end user is similar to that provided by Shared Regions [35]; the same basic notion of synchronized access (operations) to regions (objects) also exists in other DSM programming systems <ref> [3, 11] </ref>. The Shared Regions work arrived at this interface from a different set of constraints; their goal was to provide software coherence mechanisms on machines that support non-cache-coherent shared memory in hardware.
Reference: [12] <author> Satish Chandra, James R. Larus, and Anne Rogers. </author> <booktitle> Where is Time Spent in Message-Passing and Shared--Memory Programs? In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 61-73, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: In terms of comparing message passing and shared memory, most other previous work has either compared the performance of applications written and tuned specifically for each programming model <ref> [8, 12] </ref> or looked at the performance gains made possible by augmenting a hardware DSM system with message passing primitives [23].
Reference: [13] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Other types of DSM systems (e.g., those in which threads of computation are migrated to the data they reference) are also possible <ref> [4, 7, 13, 19] </ref>; a suitably generalized version of the classification scheme presented here could likely be applied to these systems as well. We classify systems by three basic mechanisms required to implement DSM and whether or not those mechanisms are implemented in hardware or software. <p> All synchronization must be effected through hardware DSM mechanisms. In contrast, CRL is an all-software DSM system in which all communication and synchronization is implemented using software DSM techniques. Several all-software DSM systems that employ an object-based approach have been developed (e.g., Amber <ref> [13] </ref>, Orca [3]). Like CRL, these systems effect coherence at the level of application-defined regions of memory (objects). Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. <p> Existing systems of this type either require the use of an entirely new object-oriented language [3, 20] or only allow the use of a subset of an existing one <ref> [13] </ref>. In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam [36] have described SAM, a shared object system for distributed memory machines.
Reference: [14] <author> Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Rajamony, and Willy Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Mostly Software Many software DSM systems are actually mostly software systems in which the hit/miss check functionality is implemented in hardware (e.g., by leveraging off of virtual memory protection mechanisms). Typical examples of mostly software systems include Ivy [28], Munin [8], and TreadMarks <ref> [14] </ref>; coherence units in these systems are the size of virtual memory pages. Blizzard [37] implements a similar scheme on the CM-5 at the granularity of individual cache lines. <p> In spite of the fact that CRL is an all-software DSM system, it performs comparably with existing mostly software DSM systems. The CM-5 CRL speedup for Water (4.8 on eight processors) is slightly better than that reported for TreadMarks <ref> [14] </ref>, a second-generation page-based mostly software DSM system (4.0 on eight processors, the largest configuration results have been reported for) 2 . 5.2.3 Barnes-Hut The Barnes-Hut application is also taken from the SPLASH-2 application suite; it employs hierarchical n-body techniques to simulate the evolution of a system of bodies under the <p> As was the case with Water, applications like Barnes-Hut are often run for a large number of 2 The SPLASH-2 version of Water used in this paper incorporates the M-Water modifications suggested by Cox et al. <ref> [14] </ref>. iterations, so the steady-state time per iteration is an appropriate measure of running time. Since the startup transients in Barnes-Hut persist through the first two iterations, we determine running time by running the application for four iterations and taking the average of the third and fourth iteration times. <p> Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware [9, 18, 45], mostly software vs. all-software [37, 46]), but to our knowledge, only Cox et al. <ref> [14] </ref> have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [15] <author> Alan L. Cox and Robert J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: CRL could be provided on such systems using the same implementation techniques and defining rgn_map and rgn_unmap to be null macros. A number of other approaches to providing coherence in software on top of non-cache-coherent shared-memory hardware have also been explored <ref> [15, 22] </ref>. Like the Shared Regions work, these research efforts differ from that described in this paper both in the type of hardware platform targeted (non-cache-coherent shared memory vs. message passing) and the use of simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided).
Reference: [16] <author> A. Geist, A. Beguelin, J. J. Dongarra, W. Jiang, R. Manchek, and V. S. Sunderam. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In spite of this fact, message passing environments such as PVM <ref> [16] </ref> and MPI [29] are often the de facto standards for programming multicomputers and networks of workstations.
Reference: [17] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: From this perspective, CRL provides sequential consistency for read and write operations in the same sense that a sequentially consistent hardware-based DSM does for individual loads and stores. In terms of individual loads and stores, CRL provides a memory/coherence model similar to entry [5] or release consistency <ref> [17] </ref>. Loads and stores to global data are only allowed within properly synchronized sections (operations), and modifications to a region are only made visible to other processors after the appropriate release operation (a call to rgn_end_write).
Reference: [18] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 262-273, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Mostly Hardware As discussed in Section 4, Alewife implements a mostly hardware DSM system the processor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected [9]. Dir 1 SW and its variations <ref> [18, 45] </ref> are also mostly hardware schemes. The Stanford FLASH multiprocessor [25] and Wisconsin Typhoon architecture [33] represent a different kind of mostly hardware DSM system. <p> Unlike Midway and Cid, CRL runs on two large-scale platforms and has been shown to deliver performance competitive with hardware DSM systems. Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware <ref> [9, 18, 45] </ref>, mostly software vs. all-software [37, 46]), but to our knowledge, only Cox et al. [14] have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [19] <author> Wilson C. Hsieh, Paul Wang, and William E. Weihl. </author> <title> Computation Migration: Enhancing Locality for Distributed-Memory Parallel Systems. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 239-248, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Other types of DSM systems (e.g., those in which threads of computation are migrated to the data they reference) are also possible <ref> [4, 7, 13, 19] </ref>; a suitably generalized version of the classification scheme presented here could likely be applied to these systems as well. We classify systems by three basic mechanisms required to implement DSM and whether or not those mechanisms are implemented in hardware or software.
Reference: [20] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concert Efficient Runtime Support for Concurrent Object-Oriented Programming Languages on Stock Hardware. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Any necessary synchronization, data replication, or thread migration functionality is provided automatically at the entry and exit of methods on shared objects. Existing systems of this type either require the use of an entirely new object-oriented language <ref> [3, 20] </ref> or only allow the use of a subset of an existing one [13]. In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam [36] have described SAM, a shared object system for distributed memory machines.
Reference: [21] <author> Alexander C. Klaiber and Henry M. Levy. </author> <title> A Comparison of Message Passing and Shared Memory Architectures for Data Parallel Programs. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 94-105, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Experimental results comparing hardware and software DSM performance are shown for up to 32 processors (Alewife); software DSM results are shown for up to 128 processors (CM-5). Klaiber and Levy <ref> [21] </ref> describe a set of experiments in which data-parallel (C*) applications are compiled such that all interprocessor communication is provided through a very simple library interface.
Reference: [22] <author> Leonidas I. Kontothanassis and Michael L. Scott. </author> <title> Software Cache Coherence for Large Scale Multiprocessors. </title> <booktitle> In Proceedings of the First Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 286-295, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: CRL could be provided on such systems using the same implementation techniques and defining rgn_map and rgn_unmap to be null macros. A number of other approaches to providing coherence in software on top of non-cache-coherent shared-memory hardware have also been explored <ref> [15, 22] </ref>. Like the Shared Regions work, these research efforts differ from that described in this paper both in the type of hardware platform targeted (non-cache-coherent shared memory vs. message passing) and the use of simulation to obtain controlled comparisons with cache-coherent hardware DSM (when such a comparison is provided).
Reference: [23] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: CRL is the first all-software DSM system capable of delivering performance competitive with hardware DSMs. To demonstrate this fact, we ported our CRL implementation to the MIT Alewife machine [1]. Since Alewife provides efficient hardware support for both message passing and shared memory communication styles <ref> [23] </ref>, the performance of applications running under CRL (using only message passing for communication) can be readily compared to the performance of the same applications when hardware-supported shared memory is used instead. <p> In terms of comparing message passing and shared memory, most other previous work has either compared the performance of applications written and tuned specifically for each programming model [8, 12] or looked at the performance gains made possible by augmenting a hardware DSM system with message passing primitives <ref> [23] </ref>. Such research addresses a different set of issues than those discussed in this paper, which takes a shared memory programming model as a given and provides a controlled comparison of hardware and software implementations of distributed shared memory.
Reference: [24] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 195-206, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: In general, Alewife's shared memory system performs quite well, enabling speedups comparable to or better than similarly scalable systems. In addition to providing support for coherent shared memory, Alewife provides the processor with direct access to the interconnection network for sending and receiving messages <ref> [24] </ref>. Efficient mechanisms are provided for sending and receiving both short (register-to-register) and long (memory-to-memory, block transfer) messages. Using Alewife's message-passing mechanisms, a processor can send a message with just a few user-level instructions.
Reference: [25] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hen-nessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Dir 1 SW and its variations [18, 45] are also mostly hardware schemes. The Stanford FLASH multiprocessor <ref> [25] </ref> and Wisconsin Typhoon architecture [33] represent a different kind of mostly hardware DSM system.
Reference: [26] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992. </year> <month> 23 </month>
Reference-contexts: and CRL versions of several applications available for anonymous ftp sometime during the summer of 1995. 4 Experimental Platforms This section describes the two platforms that are used for the experiments described in this paper: Thinking Machines' CM-5 family of multiprocessors and the MIT Alewife machine. 4.1 CM-5 The CM-5 <ref> [26] </ref> is a commercially available message-passing multicomputer with relatively efficient support for low-overhead, fine-grained message passing. The experiments described in this paper were run on a 128-node CM-5 system running version 7.3 Final of the CMOST operating system and version 3.2 of the CMMD message-passing library.
Reference: [27] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Using these three characteristics, we obtain the following breakdown of the spectrum of DSM implementation techniques that have been discussed in the literature. All-Hardware In all-hardware DSM systems, all three of these mechanisms are implemented in specialized hardware; the Stanford DASH multiprocessor <ref> [27] </ref> is a typical all-hardware system. Mostly Hardware As discussed in Section 4, Alewife implements a mostly hardware DSM system the processor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected [9].
Reference: [28] <author> Kai Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In Proceedings of the International Conference on Parallel Computing, </booktitle> <pages> pages 94-101, </pages> <year> 1988. </year>
Reference-contexts: Mostly Software Many software DSM systems are actually mostly software systems in which the hit/miss check functionality is implemented in hardware (e.g., by leveraging off of virtual memory protection mechanisms). Typical examples of mostly software systems include Ivy <ref> [28] </ref>, Munin [8], and TreadMarks [14]; coherence units in these systems are the size of virtual memory pages. Blizzard [37] implements a similar scheme on the CM-5 at the granularity of individual cache lines.
Reference: [29] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: In spite of this fact, message passing environments such as PVM [16] and MPI <ref> [29] </ref> are often the de facto standards for programming multicomputers and networks of workstations. We believe that this is primarily due to the fact that these systems require no special hardware, compiler, or operating system support, thus enabling them to run entirely at user level on unmodified, stock systems.
Reference: [30] <author> Ron Minnich, Dan Burns, and Frank Hady. </author> <title> The Memory-Integrated Network Interface. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 11-20, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: We have developed an implementation of CRL for Thinking Machines' CM-5 family of multiprocessors. Because today's networks of workstations offer interprocessor communication performance rivaling that of the CM-5 <ref> [6, 30, 41, 43] </ref>, we believe that the performance of our CRL implementation for the CM-5 is indicative of what should be possible for an implementation targeting networks of workstations using current technology. Using the CM-5 implementation of CRL, we have run applications on systems with up to 128 processors. <p> While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prenegotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [6, 30, 41, 43] </ref>. For example, Thekkath et al. [42] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a FORE ATM network.
Reference: [31] <author> Rishiyur S. Nikhil. Cid: </author> <title> A Parallel, Shared-memory C for Distributed-Memory Machines. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: CRL differs from Midway in provided a simpler programming model that bundles synchronization and data access. To the best of our knowledge, Midway has only been implemented on a small cluster of workstations connected with an ATM network. 20 Cid <ref> [31] </ref>, like CRL, is an all-software DSM system in which coherence is effected on regions (global objects) according to source code annotations provided by the programmer. Cid differs from the current CRL implementation in its potentially richer support for multithreading, automatic data placement, and load balancing.
Reference: [32] <author> Rishiyur S. Nikhil. </author> <type> Personal communication, </type> <month> March </month> <year> 1995. </year>
Reference-contexts: Cid differs from the current CRL implementation in its potentially richer support for multithreading, automatic data placement, and load balancing. To date, Cid has only been implemented and run on a small cluster of workstations connected by FDDI <ref> [32] </ref>. Unlike Midway and Cid, CRL runs on two large-scale platforms and has been shown to deliver performance competitive with hardware DSM systems.
Reference: [33] <author> Steve K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Dir 1 SW and its variations [18, 45] are also mostly hardware schemes. The Stanford FLASH multiprocessor [25] and Wisconsin Typhoon architecture <ref> [33] </ref> represent a different kind of mostly hardware DSM system.
Reference: [34] <author> Edward Rothberg, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: for Water because the data set is relatively small (a few hundred kilobytes) and is likely to remain manageable even for larger problem sizes. 5.2.1 Blocked LU Blocked LU implements LU factorization of a dense matrix; the version reported on here is based on one described by Rothberg et al. <ref> [34] </ref>. In the CRL version of the code, a region is created for each block of the matrix to be factored. The results presented here are for a 500x500 matrix using 10x10 blocks; thus the size of each region is 800 bytes. CRL, Alewife SM) on up to 32 processors.
Reference: [35] <author> Harjinder S. Sandhu, Benjamin Gamsa, and Songnian Zhou. </author> <title> The Shared Regions Approach to Software Cache Coherence on Multiprocessors. </title> <booktitle> In Proceedings of the Fourth Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 229-238, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: the art in that domain would not only help CRL but likely have broad impact across the spectrum of distributed systems. 7 Related Work Except for the notion of mapping and unmapping regions, the programming interface CRL presents to the end user is similar to that provided by Shared Regions <ref> [35] </ref>; the same basic notion of synchronized access (operations) to regions (objects) also exists in other DSM programming systems [3, 11].
Reference: [36] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The Design and Evaluation of a Shared Object System for Distributed Memory Machines. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In contrast, CRL is not language specific; the basic CRL interface could easily be provided in any imperative programming language. Scales and Lam <ref> [36] </ref> have described SAM, a shared object system for distributed memory machines. SAM is based on a new set of primitives that are motivated by optimizations commonly used on distributed memory machines. These primitives are significantly different than standard shared memory models.
Reference: [37] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Typical examples of mostly software systems include Ivy [28], Munin [8], and TreadMarks [14]; coherence units in these systems are the size of virtual memory pages. Blizzard <ref> [37] </ref> implements a similar scheme on the CM-5 at the granularity of individual cache lines. By manipulating the error correcting code bits associated with every memory block, Blizzard can control access on a cache-line by cache-line basis. <p> All-Software In an all-software DSM system, all three of the mechanisms identified above are implemented entirely in software. Several researchers have recently reported on experiences with all-software DSM systems obtained by modifying mostly software DSM systems such that the hit/miss check functionality is provided in software <ref> [37, 46] </ref>. 3 Generally speaking, increased use of software to provide shared-memory functionality tends to decrease application performance because processor cycles spent implementing memory system functionality might otherwise have been spent in application code. <p> Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware [9, 18, 45], mostly software vs. all-software <ref> [37, 46] </ref>), but to our knowledge, only Cox et al. [14] have published results from a relatively controlled comparison of hardware and software DSM systems. <p> Such research addresses a different set of issues than those discussed in this paper, which takes a shared memory programming model as a given and provides a controlled comparison of hardware and software implementations of distributed shared memory. Schoinas et al. <ref> [37] </ref> describe a taxonomy of shared-memory systems that is similar in spirit to that provided in Section 2.
Reference: [38] <author> Jaswinder Pal Singh, Anoop Gupta, and Marc Levoy. </author> <title> Parallel Visualization Algorithms: Performance and Architectural Implications. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 45-55, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: be managed without adversely affecting performance for relatively simple applications (e.g., those that communicate infrequently or have relatively simple communication patterns), the task can be far more difficult for large, complex applications, particularly those in which data is shared at a fine granularity or according to irregular, dynamic communication patterns <ref> [38, 39] </ref>. In spite of this fact, message passing environments such as PVM [16] and MPI [29] are often the de facto standards for programming multicomputers and networks of workstations. <p> In fact, Barnes-Hut and related hierarchical n-body methods present a challenging enough communication workload that they have been used by some authors as the basis of an argument in favor of aggressive hardware support for cache-coherent shared memory <ref> [38, 39] </ref>. Once again, Alewife SM delivers the best performance, achieving a speedup of 16.9 on 32 processors, but Alewife CRL is not far behind with a speedup of 12.0.
Reference: [39] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Implications of Hierarchical N-body Methods for Multiprocessor Architecture. </title> <type> Technical Report CSL-TR-92-506, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: be managed without adversely affecting performance for relatively simple applications (e.g., those that communicate infrequently or have relatively simple communication patterns), the task can be far more difficult for large, complex applications, particularly those in which data is shared at a fine granularity or according to irregular, dynamic communication patterns <ref> [38, 39] </ref>. In spite of this fact, message passing environments such as PVM [16] and MPI [29] are often the de facto standards for programming multicomputers and networks of workstations. <p> In fact, Barnes-Hut and related hierarchical n-body methods present a challenging enough communication workload that they have been used by some authors as the basis of an argument in favor of aggressive hardware support for cache-coherent shared memory <ref> [38, 39] </ref>. Once again, Alewife SM delivers the best performance, achieving a speedup of 16.9 on 32 processors, but Alewife CRL is not far behind with a speedup of 12.0.
Reference: [40] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The current implementation of CRL supports SPMD-like (single program, multiple data) applications in which a single user thread or process runs on each processor in the system. Interprocessor synchronization can be effected through region operations, barriers, broadcasts, and reductions. Many shared memory applications (e.g., the SPLASH application suites <ref> [40] </ref>) are written in this style. An initial version of CRL that supports multiple user threads per processor has recently become operational. 3.3 Memory/Coherence Model The simplest explanation of the coherence model provided by CRL considers entire operations on regions as indivisible units. <p> The results presented here are for a problem size of 4,096 bodies (one-quarter of the suggested base problem size). Other application parameters (Dt and ) are scaled appropriately for the smaller problem size <ref> [40] </ref>. Barnes-Hut represents a challenging communication workload.
Reference: [41] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to Low-Latency Communication on High-Speed Networks. </title> <journal> Transactions on Computer Systems, </journal> <pages> pages 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We have developed an implementation of CRL for Thinking Machines' CM-5 family of multiprocessors. Because today's networks of workstations offer interprocessor communication performance rivaling that of the CM-5 <ref> [6, 30, 41, 43] </ref>, we believe that the performance of our CRL implementation for the CM-5 is indicative of what should be possible for an implementation targeting networks of workstations using current technology. Using the CM-5 implementation of CRL, we have run applications on systems with up to 128 processors. <p> While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prenegotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [6, 30, 41, 43] </ref>. For example, Thekkath et al. [42] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a FORE ATM network.
Reference: [42] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Separating Data and Control Transfer in Distributed Operating Systems. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-11, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality [6, 30, 41, 43]. For example, Thekkath et al. <ref> [42] </ref> describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a FORE ATM network. They report round-trip times of 45 microseconds (1125 cycles) to read 40 bytes of data from a remote processor.
Reference: [43] <author> Thorsten von Eicken, Anindya Basu, </author> <title> and Vineet Buch. Low-Latency Communication Over ATM Networks Using Active Messages. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 46-53, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: We have developed an implementation of CRL for Thinking Machines' CM-5 family of multiprocessors. Because today's networks of workstations offer interprocessor communication performance rivaling that of the CM-5 <ref> [6, 30, 41, 43] </ref>, we believe that the performance of our CRL implementation for the CM-5 is indicative of what should be possible for an implementation targeting networks of workstations using current technology. Using the CM-5 implementation of CRL, we have run applications on systems with up to 128 processors. <p> While this approach cuts the effective transfer bandwidth roughly in half, it provides significantly reduced latencies for small transfers by avoiding the need for prenegotiation with the receiving node. Networks of workstations with interprocessor communication performance rivaling that of the CM-5 are rapidly becoming reality <ref> [6, 30, 41, 43] </ref>. For example, Thekkath et al. [42] describe the implementation of a specialized data-transfer mechanism implemented on a pair of 25 MHz DECstations connected with a FORE ATM network.
Reference: [44] <author> Thorsten von Eicken, David Culler, Seth Goldstein, and Klaus Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year> <month> 24 </month>
Reference-contexts: In both cases, all communication is effected using active messages <ref> [44] </ref>. CRL is implemented as a library against which user programs are linked; it is written entirely in 7 C. Both CM-5 and Alewife versions can be compiled from a single set of sources with conditionally compiled sections to handle machine-specific details (e.g., different message-passing interfaces).
Reference: [45] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual InternationalSymposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Mostly Hardware As discussed in Section 4, Alewife implements a mostly hardware DSM system the processor-side mechanisms are always implemented in hardware, but memory-side support is handled in software when widespread sharing is detected [9]. Dir 1 SW and its variations <ref> [18, 45] </ref> are also mostly hardware schemes. The Stanford FLASH multiprocessor [25] and Wisconsin Typhoon architecture [33] represent a different kind of mostly hardware DSM system. <p> Unlike Midway and Cid, CRL runs on two large-scale platforms and has been shown to deliver performance competitive with hardware DSM systems. Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware <ref> [9, 18, 45] </ref>, mostly software vs. all-software [37, 46]), but to our knowledge, only Cox et al. [14] have published results from a relatively controlled comparison of hardware and software DSM systems.
Reference: [46] <author> Matthew J. Zekauskas, Wayne A. Sawdon, and Brian N. Bershad. </author> <title> Software Write Detection for a Distributed Shared Memory. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 87-100, </pages> <month> November </month> <year> 1994. </year> <month> 25 </month>
Reference-contexts: All-Software In an all-software DSM system, all three of the mechanisms identified above are implemented entirely in software. Several researchers have recently reported on experiences with all-software DSM systems obtained by modifying mostly software DSM systems such that the hit/miss check functionality is provided in software <ref> [37, 46] </ref>. 3 Generally speaking, increased use of software to provide shared-memory functionality tends to decrease application performance because processor cycles spent implementing memory system functionality might otherwise have been spent in application code. <p> Systems that take advantage of more complex hardware or operating system functionality (e.g., page-based mostly software DSM systems) are worthy of study, but can suffer a performance penalty because of inefficient interfaces for accessing such features <ref> [46] </ref>. <p> Like SAM, CRL is implemented as a portable C library. Both CRL and SAM achieve good performance on distributed memory machines. Midway is a software DSM system based on entry consistency [5]. Both mostly software and all-software version of Midway have been implemented <ref> [46] </ref>. CRL differs from Midway in provided a simpler programming model that bundles synchronization and data access. <p> Several researchers have reported results comparing the performance of systems at adjacent levels of the classification presented in Section 2 (e.g., all-hardware vs. mostly hardware [9, 18, 45], mostly software vs. all-software <ref> [37, 46] </ref>), but to our knowledge, only Cox et al. [14] have published results from a relatively controlled comparison of hardware and software DSM systems.
References-found: 46

