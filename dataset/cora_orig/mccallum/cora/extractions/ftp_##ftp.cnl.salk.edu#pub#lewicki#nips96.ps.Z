URL: ftp://ftp.cnl.salk.edu/pub/lewicki/nips96.ps.Z
Refering-URL: http://www.cnl.salk.edu/~lewicki/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: lewicki@salk.edu  terry@salk.edu  
Title: Bayesian Unsupervised Learning of Higher Order Structure  
Author: Michael S. Lewicki Terrence J. Sejnowski 
Address: 10010 N. Torrey Pines Rd. La Jolla, CA 92037  
Affiliation: The Salk Institute Howard Hughes Medical Institute Computational Neurobiology Lab  
Abstract: Multilayer architectures such as those used in Bayesian belief networks and Helmholtz machines provide a powerful framework for representing and learning higher order statistical relations among inputs. Because exact probability calculations with these models are often intractable, there is much interest in finding approximate algorithms. We present an algorithm that efficiently discovers higher order structure using EM and Gibbs sampling. The model can be interpreted as a stochastic recurrent network in which ambiguity in lower-level states is resolved through feedback from higher levels. We demonstrate the performance of the algorithm on bench mark problems.
Abstract-found: 1
Intro-found: 1
Reference: <author> Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. </author> <year> (1995). </year> <title> The Helmholtz machine. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 889-904. </pages>
Reference: <author> Foldiak, P. </author> <year> (1989). </year> <title> Adaptive network for optimal linear feature extraction. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 401-405, </pages> <address> Washington, D. C. </address>
Reference: <author> Frey, B. J., Hinton, G. E., and Dayan, P. </author> <year> (1995). </year> <title> Does the wake-sleep algorithm produce good density estimators? In Touretzky, </title> <editor> D. S., Mozer, M., and Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 661-667, </pages> <address> San Mateo. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. </author> <year> (1995). </year> <title> The wake-sleep algorithm for unsupervised neural networks. </title> <journal> Science, </journal> <volume> 268(5214) </volume> <pages> 1158-1161. </pages>
Reference-contexts: higher than the optimal cost of less than 9 bits, because top units cannot capture the fact that they are mutually exclusive. 6 Discussion The methods we have described work well on these simple benchmark problems and scale well to larger problems such as the handwritten digits example used in <ref> (Hinton et al., 1995) </ref>. We believe there are two main reasons why the algorithm described here runs considerably faster than other Gibbs sampling based methods. The first is that there is no need to collect state statistics for each pattern.
Reference: <author> Hinton, G. E. and Sejnowski, T. J. </author> <year> (1986). </year> <title> Learning and relearning in Boltzmann machines. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 7, </volume> <pages> pages 282-317. </pages> <publisher> MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: The first unit in the third layer is active when the `k' is present. The second, fourth, and fifth units have learned to represent the `+', `=', and `2' respectively, with the remaining unit acting as a bias. The Shifter Problem. The shifter problem <ref> (Hinton and Sejnowski, 1986) </ref>, explained in figure 3, is important because the structure that must be discovered is in the higher order input correlations. This example also illustrates the importance of allowing high level states to influence low level states to determine the most probable internal representation.
Reference: <author> Lauritzen, S. L. and Spiegelhalter, D. J. </author> <year> (1988). </year> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> J. Royal Statistical Soc. Series B Methodological, </journal> <volume> 50(2) </volume> <pages> 157-224. </pages>
Reference: <author> Neal, R. M. </author> <year> (1992). </year> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56(1) </volume> <pages> 71-113. </pages>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference: <author> Saul, L. K., Jaakkola, T., and Jordan, M. I. </author> <year> (1996). </year> <title> Mean field theory for sigmoid belief networks. </title> <journal> J. Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 61-76. </pages>
Reference-contexts: If the causal structure to be learned is inherently ambiguous, e.g. in modeling the causal structure of medical symptoms, Gibbs sampling will be slow and better performance can be obtained with wake-sleep learning (Hinton et al., 1995; Frey et al., 1995) or mean field approximations <ref> (Saul et al., 1996) </ref>. There are many natural situations when there is ambiguity in low level features. This ambiguity can only be resolved by integrating the contextual information which itself is derived from the ambiguous simple features.
References-found: 9

