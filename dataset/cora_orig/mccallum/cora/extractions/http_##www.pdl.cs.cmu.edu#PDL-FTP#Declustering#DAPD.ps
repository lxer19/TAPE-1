URL: http://www.pdl.cs.cmu.edu/PDL-FTP/Declustering/DAPD.ps
Refering-URL: http://www.pdl.cs.cmu.edu/Publications/publications.html
Root-URL: 
Title: Architectures and Algorithms for On-Line Failure Recovery in Redundant Disk Arrays  
Author: Mark Holland Garth A. Gibson Daniel P. Siewiorek 
Address: 5000 Forbes Ave. Pittsburgh, PA 15213-3890  5000 Forbes Ave. Pittsburgh, PA 15213-3890  5000 Forbes Ave. Pittsburgh, PA 15213-3890  
Affiliation: Department of Electrical and Computer Engineering Carnegie Mellon University  School of Computer Science Carnegie Mellon University  School of Computer Science Carnegie Mellon University  
Note: Draft copy submitted to the Journal of Distributed and Parallel Databases. A revised copy is published in this journal, vol. 2 no.  
Email: mark.holland@ece.cmu.edu  garth.gibson@cs.cmu.edu  dan.siewiorek@cs.cmu.edu  
Phone: (412) 268-5237  (412) 268-5890  (412) 268-2570  
Date: 3, July 1994..  
Abstract-found: 0
Intro-found: 1
Reference: [ANSI86] <institution> American National Standard for Information Systems -- Small Computer System Interface (SCSI), ANSI X3.132-1986, </institution> <address> New York NY, </address> <year> 1986. </year>
Reference: [ANSI91] <institution> American National Standard for Information Systems -- High Performance Parallel Interface -- Mechanical, Electrical, and Signalling Protocol Specification, ANSI X3.183-1991, </institution> <address> New York NY, </address> <year> 1991. </year>
Reference: [Arulpragasam80] <author> J. Arulpragasam and R. Swarz, </author> <title> A Design for State Preservation on Storage Unit Failure, </title> <booktitle> Proceedings of the International Symposium on Fault Tolerant Computing, </booktitle> <year> 1980, </year> <pages> pp. 47-52. </pages>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring [Bit-ton88, Copeland89, Hsiao91], or by parity encoding <ref> [Arulpragasam80, Gibson93, Kim86, Park86, Patterson88] </ref>. In the former, one or more duplicate copies of each user data unit are stored on separate disks.
Reference: [Bitton88] <author> D. Bitton and J. Gray, </author> <title> Disk Shadowing, </title> <booktitle> Proceedings of the 14th Conference on Very Large Data Bases, </booktitle> <year> 1988, </year> <pages> pp. 331-338. </pages>
Reference-contexts: Another optimization also applies: since there are two copies of every data unit, it is possible to improve the performance of the array on read accesses by selecting the closer of the two copies at the time the access is initiated <ref> [Bitton88] </ref>. Our simulator contains an accurate disk model, and so we implement this as follows: when a read access is initiated, the simulator locates the two copies that can be read and then computes the completion time of the request for each of the two possible accesses.
Reference: [Cao93] <author> P. Cao, S.B. Lim, S. Venkataraman, and J. Wilkes, </author> <title> The TickerTAIP parallel RAID architecture, </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 52-63. </pages>
Reference-contexts: The controller functionality can also be distributed amongst the disks of the array <ref> [Cao93] </ref>. As disks get smaller [Gibson92], the large cables used by SCSI and other bus interfaces become increasingly unattractive. The system sketched in Figure 1b offers an alternative. It uses high-bandwidth serial links for disk interconnection.
Reference: [Chee90] <author> Y.M. Chee, C. Colbourn, D. Kreher, </author> <title> Simple t-designs with v &lt; 30, </title> <editor> Ars Combinatoria, v. </editor> <volume> 29, </volume> <year> 1990. </year>
Reference-contexts: The construction of BIBDs is an active area of research in combinatorial theory, and there exists no technique that allows the direct construction of a design with an arbitrarily-specified set of parameters. Instead, designs are generated on a case-by-case basis, and tables of known designs <ref> [Hanani75, Hall86, Chee90, Mathon90] </ref> are published and periodically updated. These tables are dense when v is small (less than about 45), but become gradually sparser as v increases.
Reference: [Chen90a] <author> P. Chen, et. al., </author> <title> An Evaluation of Redundant Arrays of Disks using an Amdahl 5890, </title> <booktitle> Proceedings of the Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1990, </year> <pages> pp. 74-85. </pages>
Reference-contexts: Studies <ref> [Chen90a, Gray90] </ref> have shown that, due to superior performance on small read and write operations, a mirrored array, also known as RAID level 1, can deliver higher performance to OLTP workloads than can a parity-based array.
Reference: [Chen90b] <author> P. Chen and D. Patterson, </author> <title> Maximizing Performance in a Striped Disk Array, </title> <booktitle> Proceedings of International Symposium on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: When a systems goals cannot be met using any C G 141414 such configuration, then, of course use the random-permutation algorithm. Section 9 discusses the problem of configuring very large arrays. 5. Evaluation methodology All analyses in this paper were done using an event-driven disk-array simulator called raidSim <ref> [Chen90b, Lee91] </ref>, originally developed for the RAID project at U.C. Berkeley [Katz89]. It consists of four primary components, illustrated in Figure 7. The top level of abstraction contains a synthetic reference generator. Table 2a shows the workload generated for our simulations. <p> Larger stripe units have been recommended for varied workloads because they reduce the probability that small requests require service from multiple disks arms while still allowing parallel transfer for requests large enough to benefit substantially <ref> [Chen90b] </ref>. Our prior study showed that the piggybacking and user-writes options had a measurable effect on reconstruction time.
Reference: [Copeland89] <author> G. Copeland and T. Keller, </author> <title> A Comparison of High-Availability Media Recovery Techniques, </title> <booktitle> Proceedings of the ACM Conference on Management of Data, </booktitle> <year> 1989, </year> <pages> pp. 98-109. </pages>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring <ref> [Bit-ton88, Copeland89, Hsiao91] </ref>, or by parity encoding [Arulpragasam80, Gibson93, Kim86, Park86, Patterson88]. In the former, one or more duplicate copies of each user data unit are stored on separate disks. <p> For G=2 the declustered parity scheme is similar to separately-developed declustered mirroring approaches in that two copies of each data unit are maintained, with the mirror data being distributed over the other disks in the array <ref> [Copeland89, Hsiao91] </ref>. At the other extreme, G=C (a = 1.0), parity declustering is equivalent to RAID level 5. Many of the performance plots in subsequent sections are presented with a on the x-axis. 4.
Reference: [Dibble90] <author> P. Dibble, </author> <title> A Parallel Interleaved File System, </title> <institution> University of Rochester Technical Report 334, </institution> <year> 1990. </year>
Reference-contexts: Layout goodness criteria Extending from non-declustered disk array layout research <ref> [Lee90, Dibble90] </ref>, we have identified six criteria for a good disk array layout. 1. Single failure correcting. No two stripe units in the same parity stripe may reside on the same physical disk. This is the basic characteristic of any single-failure-tolerating redundancy organization.
Reference: [Fibre91] <institution> Fibre Channel -- Physical Layer, ANSI X3T9.3 Working Document, Revision 2.1, </institution> <month> May </month> <year> 1991. </year> <month> 404040 </month>
Reference-contexts: It uses high-bandwidth serial links for disk interconnection. This architecture scales to large arrays more easily because it eliminates the need for the array controller to incorporate a large number of string controllers. While serial-interface disks are not yet common, standards for them are emerging (P1394 [IEEE93], Fibre Channel <ref> [Fibre91] </ref>, DQDB [IEEE89]).
Reference: [Gibson92] <author> G. Gibson, </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: The controller functionality can also be distributed amongst the disks of the array [Cao93]. As disks get smaller <ref> [Gibson92] </ref>, the large cables used by SCSI and other bus interfaces become increasingly unattractive. The system sketched in Figure 1b offers an alternative. It uses high-bandwidth serial links for disk interconnection.
Reference: [Gibson93] <author> G. Gibson and D. Patterson, </author> <title> Designing Disk Arrays for High Data Reliability, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> January, </month> <year> 1993. </year>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring [Bit-ton88, Copeland89, Hsiao91], or by parity encoding <ref> [Arulpragasam80, Gibson93, Kim86, Park86, Patterson88] </ref>. In the former, one or more duplicate copies of each user data unit are stored on separate disks. <p> The assignment of parity blocks to disks rotates across the array in order to avoid hot-spot contention; since every update to a data unit implies that a parity unit must also be updated, the distribution of parity across disks should be balanced. Because disk failures are detectable <ref> [Patterson88, Gibson93] </ref>, arrays of disks constitute an erasure channel [Peterson72], and so a parity code can correct any single disk failure. <p> Once reconstruction is complete, the array can again tolerate the loss of any single disk, and so is again fault-free, albeit with a diminished number of on-line spare disks until the faulty drives can be physically replaced. Gibson and Patterson <ref> [Gibson93] </ref> show that a small number of spare disks suffice to provide a high degree of protection against data loss in relatively large arrays (&gt;70 disks). Although the above organization can be easily extended to tolerate multiple disk failures, this paper focuses on single-failure toleration. 3. <p> In arrays in which groups of disks have a common failure mode, such as power or data cabling, this criteria should be extended to prohibit the allocation of stripe units from one parity stripe to two or more disks sharing that common failure mode <ref> [Schulze89, Gibson93] </ref>. 2. Distributed recovery workload. When any disk fails, its user workload should be evenly distributed across all other disks in the array. When replaced or repaired, its reconstruction workload should also be evenly distributed. 3. Distributed parity. <p> Assuming that the likelihood of failure of each disk is independent of that of each other disk; that is, that there are no dependent disk failure modes in the system, Gibson <ref> [Gibson93] </ref> models the mean time to data loss as where MTTF disk is the mean time to failure for each disk, N groups is the number of groups in the array, N diskspergroup is the number of disks in one group (N diskspergroup =G in RAID level 5 arrays and N <p> A primary consideration in the construction of large single-group arrays is their susceptibility to data loss arising from failures in equipment other than the disks <ref> [Gibson93] </ref>. For example, if the bus-connected disk array architecture shown in Figure 1a provides only one path to each disk but shares this path over multiple disks, the failure of a path renders multiple disks unavailable, if not damaged, for long periods of time.
Reference: [Gray90] <author> G. Gray, B. Horst, and M. Walker, </author> <title> Parity Striping of Disc Arrays: Low-Cost Reliable Storage with Acceptable Throughput, </title> <booktitle> Proceedings of the Conference on Very Large Data Bases, </booktitle> <year> 1990, </year> <pages> pp. 148-160. </pages>
Reference-contexts: Studies <ref> [Chen90a, Gray90] </ref> have shown that, due to superior performance on small read and write operations, a mirrored array, also known as RAID level 1, can deliver higher performance to OLTP workloads than can a parity-based array.
Reference: [Hall86] <author> M. Hall, </author> <title> Combinatorial Theory (2nd Edition), </title> <publisher> Wiley-Interscience, </publisher> <year> 1986. </year>
Reference-contexts: As suggested by Muntz and Lui, a layout with this property can be derived from a balanced incomplete block design <ref> [Hall86] </ref>. This section shows how such a layout may be implemented. A block design is an arrangement of v distinct objects into b tuples 3 , each containing k elements, such that each object appears in exactly r tuples, and each pair of objects appears in exactly l p tuples. <p> The construction of BIBDs is an active area of research in combinatorial theory, and there exists no technique that allows the direct construction of a design with an arbitrarily-specified set of parameters. Instead, designs are generated on a case-by-case basis, and tables of known designs <ref> [Hanani75, Hall86, Chee90, Mathon90] </ref> are published and periodically updated. These tables are dense when v is small (less than about 45), but become gradually sparser as v increases.
Reference: [Hanani75] <author> H. Hanani, </author> <title> Balanced Incomplete Block Designs and Related Designs, Discrete Mathematics, </title> <editor> v. </editor> <volume> 11, </volume> <year> 1975. </year>
Reference-contexts: The construction of BIBDs is an active area of research in combinatorial theory, and there exists no technique that allows the direct construction of a design with an arbitrarily-specified set of parameters. Instead, designs are generated on a case-by-case basis, and tables of known designs <ref> [Hanani75, Hall86, Chee90, Mathon90] </ref> are published and periodically updated. These tables are dense when v is small (less than about 45), but become gradually sparser as v increases. <p> These tables are dense when v is small (less than about 45), but become gradually sparser as v increases. Recalling that the layout equates v with C and k with G, Hanani <ref> [Hanani75] </ref>, for example, gives a table of designs that can be used to generate a layout for any value of G given C not larger than 43, and for many, but not all, combinations with larger C.
Reference: [Holland92] <author> M. Holland and G. Gibson, </author> <title> Parity Declustering for Continuous Operation in Redundant Disk Arrays, </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1992, </year> <pages> pp. 23-25. </pages>
Reference-contexts: Disk saturation may be unacceptable for OLTP applications because they mandate a minimum level of responsiveness; the TPC-A benchmark [TPCA89], for example, requires that 90% of all transactions complete in under two seconds. Long queueing delays caused by disk saturation can violate these requirements. The declustered parity <ref> [Muntz90, Holland92, Merchant92] </ref> disk array organization addresses this problem. This scheme reduces a surviving disks load increase from over 100% to an arbitrarily small percentage by increasing the amount of error correcting information that is stored in the array. <p> 13.9 ms Seek time model: (ms, cyls = seek distance in cylinders-1) 2.0 ms min, 12.5 ms average, 25 ms max Track skew: 4 sectors Cylinder skew: 17 sectors MTTF: 150,000 hours 2.0 0.01 cyls 0.46 cyls++ 161616 vices writes in the user request stream as well as reconstruction writes <ref> [Holland92] </ref>. The problem with this algorithm is that it is unable to consistently utilize all disk bandwidth not absorbed by user accesses. <p> These deficiencies can be partially overcome by parallelizing this algorithm, that is, by simultaneously reconstructing a set of P parity stripes instead of just one <ref> [Holland92, Holland93] </ref>, but this does not guarantee that the reconstruction process will absorb all the available disk bandwidth. <p> Enabling an option shifts workload from the surviving disks to the replacement disk: redirecting reads shifts user-read workload, piggybacking writes shifts reconstruction workload, and enabling user writes to the replacement shifts user-write workload. In a previous paper <ref> [Holland92] </ref> we analyzed the performance of these options using the stripe 313131 oriented reconstruction algorithm, a 50% write workload, and a smaller striping unit (4 KB).
Reference: [Holland93] <author> M. Holland, G. Gibson, and D. Siewiorek, </author> <title> Fast, On-Line Failure Recovery in Redundant Disk Arrays, </title> <booktitle> Proceedings of the International Symposium on Fault-Tolerant Computing, </booktitle> <year> 1993. </year>
Reference-contexts: These deficiencies can be partially overcome by parallelizing this algorithm, that is, by simultaneously reconstructing a set of P parity stripes instead of just one <ref> [Holland92, Holland93] </ref>, but this does not guarantee that the reconstruction process will absorb all the available disk bandwidth.
Reference: [Hou93] <author> R. Hou, J. Menon, and Y. Patt, </author> <title> Balancing I/O Response Time and Disk Rebuild Time in a RAID5 Disk Array, </title> <booktitle> Proceedings of the Hawaii International Conference on Systems Sciences, </booktitle> <year> 1993, </year> <pages> pp. 70-79. </pages>
Reference: [Hsiao91] <author> H.-I. Hsiao and D.J. DeWitt, </author> <title> A Performance Study of Three High-Availability Data Replication Strategies, </title> <booktitle> Proceedings of the International Conference on Parallel and Distributed Information Systems, </booktitle> <year> 1991, </year> <pages> pp. 18-28. </pages>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring <ref> [Bit-ton88, Copeland89, Hsiao91] </ref>, or by parity encoding [Arulpragasam80, Gibson93, Kim86, Park86, Patterson88]. In the former, one or more duplicate copies of each user data unit are stored on separate disks. <p> For G=2 the declustered parity scheme is similar to separately-developed declustered mirroring approaches in that two copies of each data unit are maintained, with the mirror data being distributed over the other disks in the array <ref> [Copeland89, Hsiao91] </ref>. At the other extreme, G=C (a = 1.0), parity declustering is equivalent to RAID level 5. Many of the performance plots in subsequent sections are presented with a on the x-axis. 4.
Reference: [IBM0661] <author> IBM Corporation, </author> <title> IBM 0661 Disk Drive Product Description, Model 370, First Edition, Low End Storage Products, </title> <type> 504/114-2, </type> <year> 1989. </year>
Reference-contexts: Table 2c shows the characteristics of the 314 MB, 3 1/2 inch diameter IBM 0661 Model 370 (Lightning) disks on which our simulations are based <ref> [IBM0661] </ref>. At the lowest level of abstraction in raidSim is an event-driven simulator, which is invoked to cause simulated time to pass. All simulation results reported represent averages over five independently seeded simulation runs.
Reference: [IEEE89] <institution> Proposed IEEE Standard 802.6 -- Distributed Queue Dual Bus (DQDB) -- Metropolitan Area Network, Draft D7, IEEE 802.6 Working Group, </institution> <year> 1989. </year>
Reference-contexts: This architecture scales to large arrays more easily because it eliminates the need for the array controller to incorporate a large number of string controllers. While serial-interface disks are not yet common, standards for them are emerging (P1394 [IEEE93], Fibre Channel [Fibre91], DQDB <ref> [IEEE89] </ref>).
Reference: [IEEE93] <institution> IEEE High Performance Serial Bus Specification, </institution> <address> P1394/Draft 6.2v0, New York, NY, </address> <month> June, </month> <year> 1993. </year>
Reference-contexts: It uses high-bandwidth serial links for disk interconnection. This architecture scales to large arrays more easily because it eliminates the need for the array controller to incorporate a large number of string controllers. While serial-interface disks are not yet common, standards for them are emerging (P1394 <ref> [IEEE93] </ref>, Fibre Channel [Fibre91], DQDB [IEEE89]).
Reference: [Katz89] <author> R. Katz, et. al., </author> <title> A Project on High Performance I/O Subsystems, </title> <journal> ACM Computer Architecture News, </journal> <volume> Vol. 17(5), </volume> <year> 1989, </year> <pages> pp. 24-31. </pages>
Reference-contexts: Section 9 discusses the problem of configuring very large arrays. 5. Evaluation methodology All analyses in this paper were done using an event-driven disk-array simulator called raidSim [Chen90b, Lee91], originally developed for the RAID project at U.C. Berkeley <ref> [Katz89] </ref>. It consists of four primary components, illustrated in Figure 7. The top level of abstraction contains a synthetic reference generator. Table 2a shows the workload generated for our simulations. This workload is based on access statistics measured on an airline-reservation OLTP system [Ramakrishnan92].
Reference: [Kim86] <author> M. Kim, </author> <title> Synchronized Disk Interleaving, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 35 (11), </volume> <year> 1986, </year> <pages> pp. 978-988. </pages>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring [Bit-ton88, Copeland89, Hsiao91], or by parity encoding <ref> [Arulpragasam80, Gibson93, Kim86, Park86, Patterson88] </ref>. In the former, one or more duplicate copies of each user data unit are stored on separate disks.
Reference: [Lee90] <author> E. Lee, </author> <title> Software and Performance Issues in the Implementation of a RAID Prototype, </title> <institution> University of California Technical Report UCB/CSD 90/573, </institution> <year> 1990. </year>
Reference-contexts: Layout goodness criteria Extending from non-declustered disk array layout research <ref> [Lee90, Dibble90] </ref>, we have identified six criteria for a good disk array layout. 1. Single failure correcting. No two stripe units in the same parity stripe may reside on the same physical disk. This is the basic characteristic of any single-failure-tolerating redundancy organization.
Reference: [Lee91] <author> E. Lee and R. Katz, </author> <title> Performance Consequences of Parity Placement in Disk Arrays, </title> <booktitle> Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991, </year> <pages> pp. 190-199. </pages>
Reference-contexts: As illustrated in Figure 6, it is possible to meet criterion six by employing a user-data mapping similar to Lees left-symmetric layout for non-declustered arrays <ref> [Lee91] </ref>, but this causes the layout to violate criterion 5. This mapping works by assigning each successive user data block to the first available data unit on each successive disk, thereby guaranteeing that criterion 6 is met. <p> When a systems goals cannot be met using any C G 141414 such configuration, then, of course use the random-permutation algorithm. Section 9 discusses the problem of configuring very large arrays. 5. Evaluation methodology All analyses in this paper were done using an event-driven disk-array simulator called raidSim <ref> [Chen90b, Lee91] </ref>, originally developed for the RAID project at U.C. Berkeley [Katz89]. It consists of four primary components, illustrated in Figure 7. The top level of abstraction contains a synthetic reference generator. Table 2a shows the workload generated for our simulations.
Reference: [Livny87] <author> M. Livny, S. Khoshafian, H. Boral, </author> <title> Multi-disk Management Algorithms, </title> <booktitle> Proceedings of the ACM Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1987, </year> <pages> pp. 69-77. </pages>
Reference-contexts: The algorithms and analyses presented in this paper apply to all array control implementations. broken down into blocks and striped across the disks to allow for concurrent access by independent processes <ref> [Livny87] </ref>. The shaded blocks, labelled Pi, store the parity (cumulative exclusive-or) computed over corresponding data blocks, labelled Di.0 through Di.3.
Reference: [Mathon90] <author> R. Mathon and A. Rosa, </author> <title> Tables of Parameters of BIBDs with r &lt; 41 Including Existence, Enumeration and Resolvability Results: An Update, </title> <editor> Ars Combinatoria, v. </editor> <volume> 30, </volume> <year> 1990. </year> <month> 414141 </month>
Reference-contexts: The construction of BIBDs is an active area of research in combinatorial theory, and there exists no technique that allows the direct construction of a design with an arbitrarily-specified set of parameters. Instead, designs are generated on a case-by-case basis, and tables of known designs <ref> [Hanani75, Hall86, Chee90, Mathon90] </ref> are published and periodically updated. These tables are dense when v is small (less than about 45), but become gradually sparser as v increases.
Reference: [Menon92a] <author> J. Menon and J. Kasson, </author> <title> Methods for Improved Update Performance of Disk Arrays, </title> <booktitle> Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <year> 1992, </year> <pages> pp. 74-83. </pages>
Reference-contexts: Unfortunately, mirroring is substantially more expensive -- its storage overhead for redundancy is 100%; that is, four or more times larger than that of typical parity encoded arrays. Furthermore, recent studies <ref> [Stodolsky93, Menon92a, Rosenblum91] </ref> have demonstrated techniques that allow the small-write performance of parity-based arrays to approach that of mirroring. This paper, therefore, focuses on parity-based arrays, but includes a comparison to mirroring where meaningful. Section 2 of this paper provides background on redundant disk arrays.
Reference: [Menon92b] <author> J. Menon and D. Mattson, </author> <title> Comparison of Sparing Alternative for Disk Arrays, </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: It provides half the benefit for improving degraded and recon 7. This bottleneck may be eliminated allowing reconstruction time to be further reduced by distributing the capacity of spare disks throughout the array <ref> [Menon92b] </ref>. 0.0 0.2 0.4 0.6 0.8 1.0 Declustering Ratio (a) 0.000 0.002 0.004 10-year data loss probability Declustering Mirroring 0.0 0.2 0.4 0.6 0.8 1.0 Declustering Ratio (a) 0.000 0.002 0.004 10-year data loss probability Declustering Mirrroring 20 DISKS 40 DISKS 303030 struction-mode performance and nearly all the benefit for reducing <p> Each of these approaches would expand the range of configurations that can be implemented using the block-design-based layout presented in this paper. Finally, implementing distributed sparing <ref> [Menon92b] </ref> in a declustered array could eliminate the replacement disk as the reconstruction bottleneck for low values of the declustering ratio (a), and perhaps yield extremely fast reconstruction (on the order of tens of seconds).
Reference: [Menon93] <author> J. Menon and J. Cortney, </author> <title> The Architecture of a Fault-Tolerant Cached RAID Controller, </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 76-86. </pages>
Reference-contexts: and because of the relatively high cost of redundancy in RAID level 1 arrays, this paper focuses on architectures derived from the RAID level 5 organization. disk busses are often duplicated (indicated by the dotted lines in Figure 1) so that they do not represent a single point of failure <ref> [Menon93] </ref>. The controller functionality can also be distributed amongst the disks of the array [Cao93]. As disks get smaller [Gibson92], the large cables used by SCSI and other bus interfaces become increasingly unattractive. The system sketched in Figure 1b offers an alternative. It uses high-bandwidth serial links for disk interconnection.
Reference: [Merchant92] <author> A. Merchant and P. Yu, </author> <title> Design and Modeling of Clustered RAID, </title> <booktitle> Proceedings of the International Symposium on Fault-Tolerant Computing, </booktitle> <year> 1992, </year> <pages> pp. 140-149. </pages>
Reference-contexts: Disk saturation may be unacceptable for OLTP applications because they mandate a minimum level of responsiveness; the TPC-A benchmark [TPCA89], for example, requires that 90% of all transactions complete in under two seconds. Long queueing delays caused by disk saturation can violate these requirements. The declustered parity <ref> [Muntz90, Holland92, Merchant92] </ref> disk array organization addresses this problem. This scheme reduces a surviving disks load increase from over 100% to an arbitrarily small percentage by increasing the amount of error correcting information that is stored in the array. <p> In this section we discuss goals for a disk array layout, present our layout for declus-tered parity based on balanced incomplete block designs, and contrast it to a layout proposed by Merchant and Yu <ref> [Merchant92] </ref> which supports more configurations of large arrays, at the cost of higher complexity. 4.1. Layout goodness criteria Extending from non-declustered disk array layout research [Lee90, Dibble90], we have identified six criteria for a good disk array layout. 1. Single failure correcting. <p> D20 D30 D21 D31 P D22 D18 P D4 D14 4 6 121212 4.3. Layouts based on random permutations Merchant and Yu <ref> [Merchant92] </ref> have independently developed an array layout strategy for declustered parity disk arrays. This section briey describes their layout strategy and compares it to the block-design based approach developed above. <p> This is the final trade-off: data reliability against cost. a=0.25 and a=0.5, the IBM Lightning drives described in Table 2, and reconstruction times given in using an analytical model such as that of Merchant and Yu <ref> [Merchant92] </ref> or Muntz and Lui [Muntz90]. Where this is too large a risk, the array must be partitioned into multiple independent groups.
Reference: [Mills92] <author> W.H. Mills and R.C. Mullin, </author> <title> Coverings and Packings, Chapter 9 in Contemporary Design Theory: A Collection of Surveys, </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1992, </year> <pages> pp. 371-399. </pages>
Reference-contexts: Second, the block-design based layout could be made much more general by relaxing the requirements on the tuples used for layout. For example, it might be possible to derive a balanced layout from a packing or covering <ref> [Mills92] </ref> instead of an actual block design, or a layout might be derived from a design in which the number of objects per tuple is not constant. Each of these approaches would expand the range of configurations that can be implemented using the block-design-based layout presented in this paper.
Reference: [Muntz90] <author> R. Muntz and J. Lui, </author> <title> Performance Analysis of Disk Arrays Under Failure, </title> <booktitle> Proceedings of the Conference on Very Large Data Bases, </booktitle> <year> 1990, </year> <pages> pp. 162-173. </pages>
Reference-contexts: Disk saturation may be unacceptable for OLTP applications because they mandate a minimum level of responsiveness; the TPC-A benchmark [TPCA89], for example, requires that 90% of all transactions complete in under two seconds. Long queueing delays caused by disk saturation can violate these requirements. The declustered parity <ref> [Muntz90, Holland92, Merchant92] </ref> disk array organization addresses this problem. This scheme reduces a surviving disks load increase from over 100% to an arbitrarily small percentage by increasing the amount of error correcting information that is stored in the array. <p> Work reducing variations to reconstruction algorithms Muntz and Lui <ref> [Muntz90] </ref> identified two simple modifications to a reconstruction algorithm, each intended to improve reconstruction performance or duration by reducing the total work required of surviving disks. <p> This is the final trade-off: data reliability against cost. a=0.25 and a=0.5, the IBM Lightning drives described in Table 2, and reconstruction times given in using an analytical model such as that of Merchant and Yu [Merchant92] or Muntz and Lui <ref> [Muntz90] </ref>. Where this is too large a risk, the array must be partitioned into multiple independent groups.
Reference: [Park86] <author> A. Park and K. Balasubramanian, </author> <title> Providing Fault Tolerance in Parallel Secondary Storage Systems, </title> <institution> Princeton University Technical Report CS-TR-057-86, </institution> <year> 1986. </year>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring [Bit-ton88, Copeland89, Hsiao91], or by parity encoding <ref> [Arulpragasam80, Gibson93, Kim86, Park86, Patterson88] </ref>. In the former, one or more duplicate copies of each user data unit are stored on separate disks.
Reference: [Patterson88] <author> D. Patterson, G. Gibson, and R. Katz, </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the Conference on Management of Data, </booktitle> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: Fault-tolerance in a data storage subsystem is generally achieved either by disk mirroring [Bit-ton88, Copeland89, Hsiao91], or by parity encoding <ref> [Arulpragasam80, Gibson93, Kim86, Park86, Patterson88] </ref>. In the former, one or more duplicate copies of each user data unit are stored on separate disks. <p> In the former, one or more duplicate copies of each user data unit are stored on separate disks. In the latter, commonly known as Redundant Arrays of Inexpensive 2 Disks (RAID) Levels 3, 4, and 5 <ref> [Patterson88] </ref>, a small portion (as large a 25%, but often much smaller) of the arrays physical storage is used to store an error correcting code computed over the file systems data. <p> Redundant disk arrays Patterson, Gibson, and Katz <ref> [Patterson88] </ref> present a taxonomy of redundant disk array architectures, RAID levels 1 through 5. Of these, RAID levels 3 is best at providing large amounts of data to a single requestor with high bandwidth, while RAID levels 1 and 5 are most appropriate for highly concurrent access to shared files. <p> The assignment of parity blocks to disks rotates across the array in order to avoid hot-spot contention; since every update to a data unit implies that a parity unit must also be updated, the distribution of parity across disks should be balanced. Because disk failures are detectable <ref> [Patterson88, Gibson93] </ref>, arrays of disks constitute an erasure channel [Peterson72], and so a parity code can correct any single disk failure. <p> Our comparison examines arrays with a total of 20 or 40 disks. We use this range of sizes because arrays of these sizes are not much larger than many current disk array products available with 5. Following the terminology of Patterson, Gibson, and Katz <ref> [Patterson88] </ref>, a group in a single-failure tolerating array is a set of disks that participate in a redundancy encoding to tolerate at most one concurrent failure. In this sense an array with parity declustered over all disks is a single group. 212121 multi-group RAID level 5.
Reference: [Peterson72] <author> W. Peterson and E. Weldon Jr., </author> <title> Error-Correcting Codes, second edition, </title> <publisher> MIT Press, </publisher> <year> 1972. </year>
Reference-contexts: Because disk failures are detectable [Patterson88, Gibson93], arrays of disks constitute an erasure channel <ref> [Peterson72] </ref>, and so a parity code can correct any single disk failure.
Reference: [Ramakrishnan92] <author> K. Ramakrishnan, P. Biswas, and R. Karedla, </author> <title> Analysis of File I/O Traces in Commercial Computing Environments, </title> <booktitle> Proceedings of the Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1992, </year> <pages> pp. 78-90. </pages>
Reference-contexts: Thus, for OLTP environments, only a small minority of the user accesses touch more than one data unit, and the fraction of reads that access more than G-1 units is even smaller <ref> [Ramakrishnan92] </ref>. Therefore the benefits of achieving criterion six in our layout would be marginal at best in OLTP workloads. <p> Berkeley [Katz89]. It consists of four primary components, illustrated in Figure 7. The top level of abstraction contains a synthetic reference generator. Table 2a shows the workload generated for our simulations. This workload is based on access statistics measured on an airline-reservation OLTP system <ref> [Ramakrishnan92] </ref>. The requests produced by this workload generator are sent to a RAID striping driver, whose function is to translate each user request into the corresponding set of disk accesses. Table 2b shows the configuration of our extended version of this striping driver.
Reference: [Rosenblum91] <author> M. Rosenblum and J. Ousterhout, </author> <title> The Design and Implementation of a Log-Structured File System, </title> <booktitle> Proceedings of the Symposium on Operating System Principles, </booktitle> <year> 1991, </year> <pages> pp. 1-15. </pages>
Reference-contexts: Unfortunately, mirroring is substantially more expensive -- its storage overhead for redundancy is 100%; that is, four or more times larger than that of typical parity encoded arrays. Furthermore, recent studies <ref> [Stodolsky93, Menon92a, Rosenblum91] </ref> have demonstrated techniques that allow the small-write performance of parity-based arrays to approach that of mirroring. This paper, therefore, focuses on parity-based arrays, but includes a comparison to mirroring where meaningful. Section 2 of this paper provides background on redundant disk arrays.
Reference: [Schulze89] <author> M. Schulze, G. Gibson, R. Katz, and D. Patterson, </author> <title> How Reliable is a RAID?, </title> <booktitle> Proceedings of COMPCON, </booktitle> <year> 1989, </year> <pages> pp. 118-123. </pages>
Reference-contexts: In arrays in which groups of disks have a common failure mode, such as power or data cabling, this criteria should be extended to prohibit the allocation of stripe units from one parity stripe to two or more disks sharing that common failure mode <ref> [Schulze89, Gibson93] </ref>. 2. Distributed recovery workload. When any disk fails, its user workload should be evenly distributed across all other disks in the array. When replaced or repaired, its reconstruction workload should also be evenly distributed. 3. Distributed parity.
Reference: [Stodolsky93] <author> D. Stodolsky, G. Gibson, and M. Holland, </author> <title> Parity Logging: Overcoming the Small-Write Problem in Redundant Disk Arrays, </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 64-75. </pages>
Reference-contexts: Unfortunately, mirroring is substantially more expensive -- its storage overhead for redundancy is 100%; that is, four or more times larger than that of typical parity encoded arrays. Furthermore, recent studies <ref> [Stodolsky93, Menon92a, Rosenblum91] </ref> have demonstrated techniques that allow the small-write performance of parity-based arrays to approach that of mirroring. This paper, therefore, focuses on parity-based arrays, but includes a comparison to mirroring where meaningful. Section 2 of this paper provides background on redundant disk arrays. <p> First, parity-based redun 393939 dant disk arrays exhibit small-write performance that is up to a factor of four worse than non-redundant arrays, and a factor of two worse than mirrored arrays, and so it would be highly desirable to combine parity declustering with parity logging <ref> [Stodolsky93] </ref>, which is a technique for improving this small-write performance in disk arrays. Second, the block-design based layout could be made much more general by relaxing the requirements on the tuples used for layout.
Reference: [TPCA89] <author> The TPC-A Benchmark: </author> <title> A Standard Specification, Transaction Processing Performance Council, </title> <year> 1989. </year>
Reference-contexts: The latter are preferable for OLTP-class applications, since OLTP is often characterized by a large number of independent processes concurrently requesting access to relatively small units of data <ref> [TPCA89] </ref>. <p> Disk saturation may be unacceptable for OLTP applications because they mandate a minimum level of responsiveness; the TPC-A benchmark <ref> [TPCA89] </ref>, for example, requires that 90% of all transactions complete in under two seconds. Long queueing delays caused by disk saturation can violate these requirements. The declustered parity [Muntz90, Holland92, Merchant92] disk array organization addresses this problem. <p> It causes criterion 5 to be violated because successive user data blocks are assigned to differing parity stripes. Since typical OLTP transactions access data in small units <ref> [TPCA89] </ref>, large accesses account for a small fraction of the total workload, typically deriving from decision-support or array-maintenance functions rather than actual transactions.
References-found: 43

