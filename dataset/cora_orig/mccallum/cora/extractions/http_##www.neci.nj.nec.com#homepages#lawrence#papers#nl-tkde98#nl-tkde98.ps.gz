URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/nl-tkde98/nl-tkde98.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers2.html
Root-URL: 
Email: flawrence,giles,sandiwayg@research.nj.nec.com  
Title: Natural Language Grammatical Inference with Recurrent Neural Networks  
Author: Steve Lawrence, C. Lee Giles Sandiway Fong 
Keyword: recurrent neural networks, natural language processing, grammatical inference, government-and-binding theory, gradient descent, simulated annealing, principles-and parameters framework, automata extraction.  
Address: 4 Independence Way Princeton, NJ 08540  College Park, MD 20742.  
Affiliation: NEC Research Institute  Institute for Advanced Computer Studies, University of Maryland,  
Note: Accepted for publication, IEEE Transactions on Knowledge and Data Engineering.  Also with the  
Abstract: This paper examines the inductive inference of a complex grammar with neural networks specifically, the task considered is that of training a network to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. Neural networks are trained, without the division into learned vs. innate components assumed by Chomsky, in an attempt to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. How a recurrent neural network could possess linguistic capability, and the properties of various common recurrent neural network architectures are discussed. The problem exhibits training behavior which is often not present with smaller grammars, and training was initially difficult. However, after implementing several techniques aimed at improving the convergence of the gradient descent backpropagation-through- time training algorithm, significant learning was possible. It was found that certain architectures are better able to learn an appropriate grammar. The operation of the networks and their training is analyzed. Finally, the extraction of rules in the form of deterministic finite state automata is investigated. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert B. Allen. </author> <title> Sequential connectionist networks for answering simple questions about a microworld. </title> <booktitle> In 5th Annual Proceedings of the Cognitive Science Society, </booktitle> <pages> pages 489495, </pages> <year> 1983. </year>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical inference [9, 21, 19, 20, 68]. Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: <ref> [1, 12, 24, 58, 59] </ref>. Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology [51, 41, 40] and role assignment [42, 58].
Reference: [2] <author> Etienne Barnard and Elizabeth C. Botha. </author> <title> Back-propagation uses prior information efficiently. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(5):794802, </volume> <month> September </month> <year> 1993. </year>
Reference-contexts: equalized (by randomly removing examples from the higher frequency class) in all training and test sets in order to reduce any effects due to differing a priori class probabilities (when the number of samples per class varies between classes there may be a bias towards predicting the more common class <ref> [3, 2] </ref>). 4 Neural Network Models and Data Encoding The following architectures were investigated. Architectures 1 to 3 are topological restrictions of 4 when the number of hidden nodes is equal and in this sense may not have the representational capability of model 4.
Reference: [3] <author> Etienne Barnard and David Casasent. </author> <title> A comparison between criterion functions for linear classifiers, with an application to neural nets. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 19(5):10301041, </volume> <year> 1989. </year> <month> 22 </month>
Reference-contexts: equalized (by randomly removing examples from the higher frequency class) in all training and test sets in order to reduce any effects due to differing a priori class probabilities (when the number of samples per class varies between classes there may be a bias towards predicting the more common class <ref> [3, 2] </ref>). 4 Neural Network Models and Data Encoding The following architectures were investigated. Architectures 1 to 3 are topological restrictions of 4 when the number of hidden nodes is equal and in this sense may not have the representational capability of model 4.
Reference: [4] <author> E.B. Baum and F. Wilczek. </author> <title> Supervised learning of probability distributions by neural networks. </title> <editor> In D.Z. An-derson, editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <pages> pages 5261, </pages> <address> New York, </address> <year> 1988. </year> <journal> American Institute of Physics. </journal>
Reference-contexts: Activation Function. Symmetric sigmoid functions (e.g. tanh) often improve convergence over the standard logistic function. For our particular problem we found that the difference was minor and that the logistic function resulted in better performance as shown in table 4. 7. Cost Function. The relative entropy cost function <ref> [4, 29, 57, 26, 27] </ref> has received particular attention and has a natural interpretation in terms of learning probabilities [36].
Reference: [5] <author> M.P. Casey. </author> <title> The dynamics of discrete-time computation, with application to recurrent neural networks and finite state machine extraction. </title> <booktitle> Neural Computation, </booktitle> <address> 8(6):11351178, </address> <year> 1996. </year>
Reference-contexts: It should be noted that this DFA extraction method may be applied to any discrete-time recurrent net, regardless of order or hidden layers. Recently, the extraction process has been proven to converge and extract any DFA learned or encoded by the neural network <ref> [5] </ref>. The extracted DFAs depend on the quantization level, q. We extracted DFAs using values of q starting from 3 and used standard minimization techniques to compare the resulting automata [28]. We passed the training and test data sets through the extracted DFAs.
Reference: [6] <author> N.A. Chomsky. </author> <title> Three models for the description of language. </title> <journal> IRE Transactions on Information Theory, </journal> <volume> IT2:113124, </volume> <year> 1956. </year>
Reference-contexts: In the Chomsky hierarchy of phrase structured grammars, the simplest grammar and its associated automata are regular grammars and finite-state-automata (FSA). However, it has been firmly established <ref> [6] </ref> that the syntactic structures of natural language cannot be parsimoniously described by regular languages. <p> This can be done by clustering the activation values of the recurrent state neurons [46]. The automata extracted with this process can only recognize regular grammars 13 . However, natural language <ref> [6] </ref> cannot be parsimoniously described by regular languages certain phenomena (e.g. center embedding) are more compactly described by context-free grammars, while others (e.g. crossed-serial dependencies and agreement) are better described by context-sensitive grammars.
Reference: [7] <author> N.A. Chomsky. </author> <title> Lectures on Government and Binding. </title> <publisher> Foris Publications, </publisher> <year> 1981. </year>
Reference-contexts: In the light of such examples and the fact that such contrasts crop up not just in English but in other languages (for example, the stubborn contrast also holds in Dutch), some linguists (chiefly Chomsky <ref> [7] </ref>) have hypothesized that it is only reasonable that such knowledge is only partially acquired: the lack of variation found across speakers, and indeed, languages for certain classes of data suggests that there exists a fixed component of the language system.
Reference: [8] <author> N.A. Chomsky. </author> <title> Knowledge of Language: Its Nature, Origin, and Use. </title> <type> Prager, </type> <year> 1986. </year>
Reference-contexts: and Its Acquisition Certainly one of the most important questions for the study of human language is: How do people unfailingly manage to acquire such a complex rule system? A system so complex that it has resisted the efforts of linguists to date to adequately describe in a formal system <ref> [8] </ref>. A couple of examples of the kind of knowledge native speakers often take for granted are provided in this section.
Reference: [9] <author> A. Cleeremans, D. Servan-Schreiber, and J.L. McClelland. </author> <title> Finite state automata and simple recurrent networks. </title> <booktitle> Neural Computation, </booktitle> <address> 1(3):372381, </address> <year> 1989. </year>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical inference <ref> [9, 21, 19, 20, 68] </ref>. Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: [1, 12, 24, 58, 59].
Reference: [10] <author> C. Darken and J.E. Moody. </author> <title> Note on learning rate schedules for stochastic optimization. </title> <editor> In R.P. Lippmann, J.E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 832838. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Moody and Darken have proposed search then converge learning rate schedules of the form <ref> [10, 11] </ref>: (t) = 1 + t (1) where (t) is the learning rate at time t, 0 is the initial learning rate, and t is a constant.
Reference: [11] <author> C. Darken and J.E. Moody. </author> <title> Towards faster stochastic gradient search. </title> <booktitle> In Neural Information Processing Systems 4, </booktitle> <pages> pages 10091016. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Moody and Darken have proposed search then converge learning rate schedules of the form <ref> [10, 11] </ref>: (t) = 1 + t (1) where (t) is the learning rate at time t, 0 is the initial learning rate, and t is a constant.
Reference: [12] <author> J.L. Elman. </author> <title> Structured representations and connectionist models. </title> <booktitle> In 6th Annual Proceedings of the Cognitive Science Society, </booktitle> <pages> pages 1725, </pages> <year> 1984. </year>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical inference [9, 21, 19, 20, 68]. Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: <ref> [1, 12, 24, 58, 59] </ref>. Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology [51, 41, 40] and role assignment [42, 58].
Reference: [13] <author> J.L. Elman. </author> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <booktitle> Machine Learning, </booktitle> <address> 7(2/3):195226, </address> <year> 1991. </year>
Reference-contexts: The task considered here differs from these in that the grammar is more complex. The recurrent neural networks investigated in this paper constitute complex, dynamical systems it has been shown that recurrent networks have the representational power required for hierarchical solutions <ref> [13] </ref>, and that they are Turing equivalent. 2.2 Language and Its Acquisition Certainly one of the most important questions for the study of human language is: How do people unfailingly manage to acquire such a complex rule system? A system so complex that it has resisted the efforts of linguists to <p> This paper considers replacing the inference algorithm with a neural network and the grammar is that of the English language. The simple grammar used by Elman <ref> [13] </ref> shown in table 1 contains some of the structures in the complete English grammar: e.g. agreement, verb argument structure, interactions with relative clauses, and recursion. S ! NP VP . NP ! PropN j N j N RC VP ! V (NP) N ! boy j girl j cat... <p> S ! NP VP . NP ! PropN j N j N RC VP ! V (NP) N ! boy j girl j cat... PropN ! John j Mary V ! chase j feed j see... Table 1. A simple grammar encompassing a subset of the English language (from <ref> [13] </ref>). NP = noun phrase, VP = verb phrase, PropN = proper noun, RC = relative clause, V = verb, N = noun, and S = the full sentence. In the Chomsky hierarchy of phrase structured grammars, the simplest grammar and its associated automata are regular grammars and finite-state-automata (FSA). <p> Narendra and Parthasarathy [44]. A recurrent network with feedback connections from each output node to all hidden nodes. The N&P network architecture has also been studied by Jordan [33, 34] the network is called N&P in this paper in line with [30]. 3. Elman <ref> [13] </ref>. A recurrent network with feedback from each hidden node to all hidden nodes. When training the Elman network backpropagation-through-time is used rather than the truncated version used by Elman, i.e. in this paper Elman network refers to the architecture used by Elman but not the training algorithm. 4. <p> This enabled the networks to focus on the simpler data first. Elman suggests that the initial training constrains later training in a useful way <ref> [13] </ref>. However, for our problem, the use of sectioning has consistently decreased performance as shown in table 4. We have also investigated the use of simulated annealing. Simulated annealing is a global optimization method [32, 35].
Reference: [14] <author> P. Frasconi and M. Gori. </author> <title> Computational capabilities of local-feedback recurrent networks acting as finite-state machines. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(6), </volume> <year> 1996. </year> <pages> 1521-1524. </pages>
Reference-contexts: FGS networks 20 state is state 1 at the bottom left and the accepting state is state 17 at the top right. All strings which do not reach the accepting state are rejected. have recently been shown to be the most computationally limited <ref> [14] </ref>. Elman networks are just a special case of W&Z networks the fact that the Elman and W&Z networks are the top performers is not surprising. However, theoretically why the Elman network outperformed the W&Z network is an open question.
Reference: [15] <author> P. Frasconi, M. Gori, M. Maggini, and G. </author> <title> Soda. Unified integration of explicit rules and learning by example in recurrent networks. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 7(2):340346, </volume> <year> 1995. </year>
Reference-contexts: In addition symbolic knowledge can be inserted into recurrent neural networks and even refined after training <ref> [15, 47, 45] </ref>. The ordered triple of a discrete Markov process (fstate; input ! next-stateg) can be extracted from a RNN 16 FGS Elman N&P Narendra & Parthasarathy and Williams & Zipser. and used to form an equivalent deterministic finite state automata (DFA).
Reference: [16] <author> P. Frasconi, M. Gori, and G. </author> <title> Soda. Local feedback multilayered networks. </title> <booktitle> Neural Computation, </booktitle> <address> 4(1):120130, </address> <year> 1992. </year>
Reference-contexts: It is expected that the Frasconi-Gori-Soda (FGS) architecture will be unable to perform the task and it has been included primarily as a control case. 1. Frasconi-Gori-Soda (FGS) locally recurrent networks <ref> [16] </ref>. A multilayer perceptron augmented with local feedback around each hidden node. The local-output version has been used. The FGS network has also been studied by [43] the network is called FGS in this paper in line with [63]. 2. Narendra and Parthasarathy [44]. <p> in the small input window case, where the networks are required to form a grammar in order to perform well. 5 Gradient Descent and Simulated Annealing Learning Backpropagation-through-time [66] 5 has been used to train the globally recurrent networks 6 , and the gradient descent algorithm described by the authors <ref> [16] </ref> was used for the FGS network. The standard gradient descent algorithms were found to be impractical for this problem 7 . The techniques described below for improving convergence were investigated.
Reference: [17] <author> K.S. Fu. </author> <title> Syntactic Pattern Recognition and Applications. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J, </address> <year> 1982. </year>
Reference-contexts: knowledge, positive and negative examples are used (a second argument for innateness is that it is not possible to learn the grammar without negative examples). 3 Data We first provide a brief introduction to formal grammars, grammatical inference, and natural language; for a thorough introduction see Harrison [25] and Fu <ref> [17] </ref>. <p> with the procedures that can be used to infer the syntactic or production rules of an unknown grammar G based on a finite set of strings I from L (G), the language generated by G, and possibly also on a finite set of strings from the complement of L (G) <ref> [17] </ref>. This paper considers replacing the inference algorithm with a neural network and the grammar is that of the English language.
Reference: [18] <author> M. Gasser and C. Lee. </author> <title> Networks that learn phonology. </title> <type> Technical report, </type> <institution> Computer Science Department, Indiana University, </institution> <year> 1990. </year>
Reference-contexts: Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: [1, 12, 24, 58, 59]. Neural network models have been shown to be able to account for a variety of phenomena in phonology <ref> [23, 61, 62, 18, 22] </ref>, morphology [51, 41, 40] and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. [64, 65, 19] on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [19] <author> C. Lee Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <booktitle> Neural Computation, </booktitle> <address> 4(3):393405, </address> <year> 1992. </year>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical inference <ref> [9, 21, 19, 20, 68] </ref>. Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: [1, 12, 24, 58, 59]. <p> Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology [51, 41, 40] and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. <ref> [64, 65, 19] </ref> on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex. <p> The algorithm we use for automata extraction (from <ref> [19] </ref>) works as follows: after the network is trained (or even during training), we apply a procedure for extracting what the network has learned i.e., the network's current conception of what DFA it has learned.
Reference: [20] <author> C. Lee Giles, C.B. Miller, D. Chen, G.Z. Sun, H.H. Chen, and Y.C. Lee. </author> <title> Extracting and learning an unknown grammar with recurrent neural networks. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 317324, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical inference <ref> [9, 21, 19, 20, 68] </ref>. Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: [1, 12, 24, 58, 59].
Reference: [21] <author> C. Lee Giles, G.Z. Sun, H.H. Chen, Y.C. Lee, and D. Chen. </author> <title> Higher order recurrent networks & grammatical inference. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 380387, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher> <pages> 23 </pages>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical inference <ref> [9, 21, 19, 20, 68] </ref>. Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: [1, 12, 24, 58, 59].
Reference: [22] <author> M. Hare. </author> <title> The role of similarity in Hungarian vowel harmony: A connectionist account. </title> <type> Technical Report CRL 9004, </type> <institution> Centre for Research in Language, University of California, </institution> <address> San Diego, </address> <year> 1990. </year>
Reference-contexts: Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: [1, 12, 24, 58, 59]. Neural network models have been shown to be able to account for a variety of phenomena in phonology <ref> [23, 61, 62, 18, 22] </ref>, morphology [51, 41, 40] and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. [64, 65, 19] on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [23] <author> M. Hare, D. Corina, and G.W. Cottrell. </author> <title> Connectionist perspective on prosodic structure. </title> <type> Technical Report CRL Newsletter Volume 3 Number 2, </type> <institution> Centre for Research in Language, University of California, </institution> <address> San Diego, </address> <year> 1989. </year>
Reference-contexts: Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: [1, 12, 24, 58, 59]. Neural network models have been shown to be able to account for a variety of phenomena in phonology <ref> [23, 61, 62, 18, 22] </ref>, morphology [51, 41, 40] and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. [64, 65, 19] on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [24] <author> Catherine L. Harris and J.L. Elman. </author> <title> Representing variable information with simple recurrent networks. </title> <booktitle> In 6th Annual Proceedings of the Cognitive Science Society, </booktitle> <pages> pages 635642, </pages> <year> 1984. </year>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical inference [9, 21, 19, 20, 68]. Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: <ref> [1, 12, 24, 58, 59] </ref>. Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology [51, 41, 40] and role assignment [42, 58].
Reference: [25] <author> M.H. Harrison. </author> <title> Introduction to Formal Language Theory. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1978. </year>
Reference-contexts: of using innate knowledge, positive and negative examples are used (a second argument for innateness is that it is not possible to learn the grammar without negative examples). 3 Data We first provide a brief introduction to formal grammars, grammatical inference, and natural language; for a thorough introduction see Harrison <ref> [25] </ref> and Fu [17].
Reference: [26] <author> S. Haykin. </author> <title> Neural Networks, A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: However, significant convergence was not obtained as shown in table 4. 4. Weight Initialization. Random weights are initialized with the goal of ensuring that the sigmoids do not start out in saturation but are not very small (corresponding to a flat part of the error surface) <ref> [26] </ref>. In addition, several (20) sets of random weights are tested and the set which provides the best performance on the training data is chosen. In our experiments on the current problem, it was found that these techniques do not make a significant difference. 5. Learning Rate Schedules. <p> Activation Function. Symmetric sigmoid functions (e.g. tanh) often improve convergence over the standard logistic function. For our particular problem we found that the difference was minor and that the logistic function resulted in better performance as shown in table 4. 7. Cost Function. The relative entropy cost function <ref> [4, 29, 57, 26, 27] </ref> has received particular attention and has a natural interpretation in terms of learning probabilities [36]. <p> All inputs were within the range zero to one. All target outputs were either 0.1 or 0.9. Bias inputs were used. The best of 20 random weight sets was chosen based on training set performance. Weights were initialized as shown in Haykin <ref> [26] </ref> where weights are initialized on a node by node basis as uniformly distributed random numbers in the range (2:4=F i ; 2:4=F i ) where F i is the fan-in of neuron i. The logistic output activation function was used. The quadratic cost function was used.
Reference: [27] <author> J.A. Hertz, A. Krogh, and R.G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1991. </year>
Reference-contexts: Activation Function. Symmetric sigmoid functions (e.g. tanh) often improve convergence over the standard logistic function. For our particular problem we found that the difference was minor and that the logistic function resulted in better performance as shown in table 4. 7. Cost Function. The relative entropy cost function <ref> [4, 29, 57, 26, 27] </ref> has received particular attention and has a natural interpretation in terms of learning probabilities [36].
Reference: [28] <author> J.E. Hopcroft and J.D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1979. </year>
Reference-contexts: Eventually this process must terminate since there are only a finite number of partitions available; and, in practice, many of the partitions are never reached. The derived DFA can then be reduced to its minimal DFA using standard minimization algorithms <ref> [28] </ref>. It should be noted that this DFA extraction method may be applied to any discrete-time recurrent net, regardless of order or hidden layers. Recently, the extraction process has been proven to converge and extract any DFA learned or encoded by the neural network [5]. <p> The extracted DFAs depend on the quantization level, q. We extracted DFAs using values of q starting from 3 and used standard minimization techniques to compare the resulting automata <ref> [28] </ref>. We passed the training and test data sets through the extracted DFAs. We found that the extracted automata correctly classified 95% of the training data and 60% of the test data for q = 7.
Reference: [29] <author> J. </author> <title> Hopfield. Learning algorithms and probability distributions in feed-forward and feed-back networks. </title> <booktitle> Proceedings of the National Academy of Science, </booktitle> <address> 84:84298433, </address> <year> 1987. </year>
Reference-contexts: Activation Function. Symmetric sigmoid functions (e.g. tanh) often improve convergence over the standard logistic function. For our particular problem we found that the difference was minor and that the logistic function resulted in better performance as shown in table 4. 7. Cost Function. The relative entropy cost function <ref> [4, 29, 57, 26, 27] </ref> has received particular attention and has a natural interpretation in terms of learning probabilities [36].
Reference: [30] <author> B. G. Horne and C. Lee Giles. </author> <title> An experimental comparison of recurrent neural networks. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 697704. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Narendra and Parthasarathy [44]. A recurrent network with feedback connections from each output node to all hidden nodes. The N&P network architecture has also been studied by Jordan [33, 34] the network is called N&P in this paper in line with <ref> [30] </ref>. 3. Elman [13]. A recurrent network with feedback from each hidden node to all hidden nodes. <p> Additionally, investigation of the data suggests that 100% correct classification on the training data with only two word inputs would not be possible unless the networks were able to learn significant aspects of the grammar. Another comparison of recurrent neural network architectures, that of Giles and Horne <ref> [30] </ref>, compared various networks on randomly generated 6 and 64-state finite memory machines.
Reference: [31] <author> L. Ingber. </author> <title> Very fast simulated re-annealing. </title> <booktitle> Mathematical Computer Modelling, </booktitle> <address> 12:967973, </address> <year> 1989. </year>
Reference-contexts: For example, the canonical Japanese word order is simply ungrammatical in English. Hence, it would be extremely surprising if an English-trained model accepts Japanese, i.e. it is expected that a network trained 11 The adaptive simulated annealing code by Lester Ingber <ref> [31, 32] </ref> was used. 13 on English will not generalize to Japanese data. This is what we find all models resulted in no significant generalization on the Japanese data (50% error on average). Five simulations were performed for each architecture.
Reference: [32] <author> L. Ingber. </author> <title> Adaptive simulated annealing (ASA). </title> <type> Technical report, </type> <institution> Lester Ingber Research, </institution> <address> McLean, VA, </address> <year> 1993. </year>
Reference-contexts: Elman suggests that the initial training constrains later training in a useful way [13]. However, for our problem, the use of sectioning has consistently decreased performance as shown in table 4. We have also investigated the use of simulated annealing. Simulated annealing is a global optimization method <ref> [32, 35] </ref>. When minimizing a function, any downhill step is accepted and the process repeats from this new point. An uphill step may also be accepted. It is therefore possible to escape from local minima. <p> For example, the canonical Japanese word order is simply ungrammatical in English. Hence, it would be extremely surprising if an English-trained model accepts Japanese, i.e. it is expected that a network trained 11 The adaptive simulated annealing code by Lester Ingber <ref> [31, 32] </ref> was used. 13 on English will not generalize to Japanese data. This is what we find all models resulted in no significant generalization on the Japanese data (50% error on average). Five simulations were performed for each architecture.
Reference: [33] <author> M.I. Jordan. </author> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the Ninth Annual conference of the Cognitive Science Society, </booktitle> <pages> pages 531546. </pages> <publisher> Lawrence Erlbaum, </publisher> <year> 1986. </year>
Reference-contexts: Narendra and Parthasarathy [44]. A recurrent network with feedback connections from each output node to all hidden nodes. The N&P network architecture has also been studied by Jordan <ref> [33, 34] </ref> the network is called N&P in this paper in line with [30]. 3. Elman [13]. A recurrent network with feedback from each hidden node to all hidden nodes.
Reference: [34] <author> M.I. Jordan. </author> <title> Serial order: A parallel distributed processing approach. </title> <type> Technical Report ICS Report 8604, </type> <institution> Institute for Cognitive Science, University of California at San Diego, La Jolla, </institution> <address> CA, </address> <month> May </month> <year> 1986. </year>
Reference-contexts: Narendra and Parthasarathy [44]. A recurrent network with feedback connections from each output node to all hidden nodes. The N&P network architecture has also been studied by Jordan <ref> [33, 34] </ref> the network is called N&P in this paper in line with [30]. 3. Elman [13]. A recurrent network with feedback from each hidden node to all hidden nodes.
Reference: [35] <author> S. Kirkpatrick and G.B. Sorkin. </author> <title> Simulated annealing. </title> <editor> In Michael A. Arbib, editor, </editor> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 876878. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: Elman suggests that the initial training constrains later training in a useful way [13]. However, for our problem, the use of sectioning has consistently decreased performance as shown in table 4. We have also investigated the use of simulated annealing. Simulated annealing is a global optimization method <ref> [32, 35] </ref>. When minimizing a function, any downhill step is accepted and the process repeats from this new point. An uphill step may also be accepted. It is therefore possible to escape from local minima.
Reference: [36] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: Cost Function. The relative entropy cost function [4, 29, 57, 26, 27] has received particular attention and has a natural interpretation in terms of learning probabilities <ref> [36] </ref>.
Reference: [37] <author> H. Lasnik and J. Uriagereka. </author> <title> A Course in GB Syntax: Lectures on Binding and Empty Categories. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference-contexts: by push-down automata, while others (e.g. crossed-serial dependencies and agreement) are better described by context-sensitive grammars which are recognized by linear bounded automata [50]. 3.2 Data The data used in this work consists of 552 English positive and negative examples taken from an introductory GB-linguistics textbook by Lasnik and Uriagereka <ref> [37] </ref>. Most of these examples are organized into minimal pairs like the example I am eager for John to win/*I am eager John to win above. The minimal nature of the changes involved suggests that the dataset may represent an especially difficult task for the models.
Reference: [38] <author> Steve Lawrence, Sandiway Fong, and C. Lee Giles. </author> <title> Natural language grammatical inference: A comparison of recurrent neural networks and machine learning methods. </title> <editor> In Stefan Wermter, Ellen Riloff, and Gabriele Scheler, editors, </editor> <title> Symbolic, Connectionist, and Statistical Approaches to Learning for Natural Language Processing, </title> <booktitle> Lecture notes in AI, </booktitle> <pages> pages 3347. </pages> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: We analyze the operation of the networks and investigate a rule approximation of what the recurrent network has learned specifically, the extraction of rules in the form of deterministic finite state automata. Previous work <ref> [38] </ref> has compared neural networks with other machine learning paradigms on this problem this work focuses on recurrent neural networks, investigates additional networks, analyzes the operation of the networks and the training algorithm, and investigates rule extraction.
Reference: [39] <author> Y. Le Cun. </author> <title> Efficient learning and second order methods. </title> <booktitle> Tutorial presented at Neural Information Processing Systems 5, </booktitle> <year> 1993. </year>
Reference-contexts: Batch update attempts to follow the true gradient, whereas a stochastic path is followed using stochastic update. Stochastic update is often much quicker than batch update, especially with large, redundant datasets <ref> [39] </ref>. Additionally, the stochastic path may help the network to escape from local minima. However, the error can jump around without converging unless the learning rate is reduced, most second order methods do not work well with stochastic update, and stochastic update is harder to parallelize than batch [39]. <p> redundant datasets <ref> [39] </ref>. Additionally, the stochastic path may help the network to escape from local minima. However, the error can jump around without converging unless the learning rate is reduced, most second order methods do not work well with stochastic update, and stochastic update is harder to parallelize than batch [39]. Batch update provides guaranteed convergence (to local minima) and works better with second order techniques. However it can be very slow, and may converge to very poor local minima.
Reference: [40] <author> L.R. Leerink and M. Jabri. </author> <title> Learning the past tense of English verbs using recurrent neural networks. </title> <editor> In Peter Bartlett, Anthony Burkitt, and Robert Williamson, editors, </editor> <booktitle> Australian Conference on Neural Networks, </booktitle> <pages> pages 222226. </pages> <institution> Australian National University, </institution> <year> 1996. </year> <month> 24 </month>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology <ref> [51, 41, 40] </ref> and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. [64, 65, 19] on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [41] <author> B. MacWhinney, J. Leinbach, R. Taraban, and J. McDonald. </author> <title> Language learning: cues or rules? Journal of Memory and Language, </title> <address> 28:255277, </address> <year> 1989. </year>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology <ref> [51, 41, 40] </ref> and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. [64, 65, 19] on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [42] <author> R. Miikkulainen and M. Dyer. </author> <title> Encoding input/output representations in connectionist cognitive systems. </title> <editor> In D. S. Touretzky, G. E. Hinton, and T. J. Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 188195, </pages> <address> Los Altos, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology [51, 41, 40] and role assignment <ref> [42, 58] </ref>. Induction of simpler grammars has been addressed often e.g. [64, 65, 19] on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [43] <author> M.C. Mozer. </author> <title> A focused backpropagation algorithm for temporal pattern recognition. </title> <journal> Complex Systems, </journal> <volume> 3(4):349381, </volume> <month> August </month> <year> 1989. </year>
Reference-contexts: Frasconi-Gori-Soda (FGS) locally recurrent networks [16]. A multilayer perceptron augmented with local feedback around each hidden node. The local-output version has been used. The FGS network has also been studied by <ref> [43] </ref> the network is called FGS in this paper in line with [63]. 2. Narendra and Parthasarathy [44]. A recurrent network with feedback connections from each output node to all hidden nodes.
Reference: [44] <author> K.S. Narendra and K. Parthasarathy. </author> <title> Identification and control of dynamical systems using neural networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(1):427, </volume> <year> 1990. </year>
Reference-contexts: Frasconi-Gori-Soda (FGS) locally recurrent networks [16]. A multilayer perceptron augmented with local feedback around each hidden node. The local-output version has been used. The FGS network has also been studied by [43] the network is called FGS in this paper in line with [63]. 2. Narendra and Parthasarathy <ref> [44] </ref>. A recurrent network with feedback connections from each output node to all hidden nodes. The N&P network architecture has also been studied by Jordan [33, 34] the network is called N&P in this paper in line with [30]. 3. Elman [13].
Reference: [45] <author> C.W. Omlin and C.L. Giles. </author> <title> Constructing deterministic finite-state automata in recurrent neural networks. </title> <journal> Journal of the ACM, </journal> <volume> 45(6):937, </volume> <year> 1996. </year>
Reference-contexts: In addition symbolic knowledge can be inserted into recurrent neural networks and even refined after training <ref> [15, 47, 45] </ref>. The ordered triple of a discrete Markov process (fstate; input ! next-stateg) can be extracted from a RNN 16 FGS Elman N&P Narendra & Parthasarathy and Williams & Zipser. and used to form an equivalent deterministic finite state automata (DFA). <p> The hypothesis is that during training, the network begins to partition (or quantize) its state space into fairly well-separated, distinct regions or clusters, which represent corresponding states in some finite state 19 automaton (recently, it has been proved that arbitrary DFAs can be stably encoded into recurrent neural networks <ref> [45] </ref>). One simple way of finding these clusters is to divide each neuron's range into q partitions of equal width. Thus for N hidden neurons, there exist q N possible partition states.
Reference: [46] <author> C.W. Omlin and C.L. Giles. </author> <title> Extraction of rules from discrete-time recurrent neural networks. Neural Networks, </title> <address> 9(1):4152, </address> <year> 1996. </year>
Reference-contexts: This can be done by clustering the activation values of the recurrent state neurons <ref> [46] </ref>. The automata extracted with this process can only recognize regular grammars 13 . However, natural language [6] cannot be parsimoniously described by regular languages certain phenomena (e.g. center embedding) are more compactly described by context-free grammars, while others (e.g. crossed-serial dependencies and agreement) are better described by context-sensitive grammars. <p> Automata extraction may also be useful for improving the performance of the system via an iterative combination of rule extraction and rule insertion. Significant learning time improvements can be achieved by training networks with prior knowledge <ref> [46] </ref>.
Reference: [47] <author> C.W. Omlin and C.L. Giles. </author> <title> Rule revision with recurrent neural networks. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 8(1):183188, </volume> <year> 1996. </year>
Reference-contexts: In addition symbolic knowledge can be inserted into recurrent neural networks and even refined after training <ref> [15, 47, 45] </ref>. The ordered triple of a discrete Markov process (fstate; input ! next-stateg) can be extracted from a RNN 16 FGS Elman N&P Narendra & Parthasarathy and Williams & Zipser. and used to form an equivalent deterministic finite state automata (DFA).
Reference: [48] <author> F. Pereira and Y. Schabes. </author> <title> Inside-outside re-estimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th annual meeting of the ACL, </booktitle> <pages> pages 128135, </pages> <address> Newark, </address> <year> 1992. </year>
Reference-contexts: The most successful stochastic language models have been based on finite-state descriptions such as n-grams or hidden Markov models. However, finite-state models cannot represent hierarchical structures as found in natural language 1 <ref> [48] </ref>. In the past few years several recurrent neural network architectures have emerged 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems. The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical <p> structures as found in natural language 1 <ref> [48] </ref>. In the past few years several recurrent neural network architectures have emerged 1 The inside-outside re-estimation algorithm is an extension of hidden Markov models intended to be useful for learning hierarchical systems. The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical inference [9, 21, 19, 20, 68]. Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: [1, 12, 24, 58, 59].
Reference: [49] <author> D. M. Pesetsky. </author> <title> Paths and Categories. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1982. </year>
Reference-contexts: Obviously, a word, e.g. to, may be part of more than one part-of-speech. The tagging resulted in several contradictory and duplicated sentences. Various methods 3 Following classical GB theory, these classes are synthesized from the theta-grids of individual predicates via the Canonical Structural Realization (CSR) mechanism of Pesetsky <ref> [49] </ref>. 6 were tested to deal with these cases, however they were removed altogether for the results reported here.
Reference: [50] <author> J.B. Pollack. </author> <title> The induction of dynamical recognizers. </title> <booktitle> Machine Learning, </booktitle> <address> 7:227252, </address> <year> 1991. </year>
Reference-contexts: Certain phenomena (e.g. center embedding) are more compactly described by context-free grammars which are recognized by push-down automata, while others (e.g. crossed-serial dependencies and agreement) are better described by context-sensitive grammars which are recognized by linear bounded automata <ref> [50] </ref>. 3.2 Data The data used in this work consists of 552 English positive and negative examples taken from an introductory GB-linguistics textbook by Lasnik and Uriagereka [37].
Reference: [51] <author> D.E. Rumelhart and J.L. McClelland. </author> <title> On learning the past tenses of English verbs. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 2, chapter 18, </volume> <pages> pages 216271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology <ref> [51, 41, 40] </ref> and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. [64, 65, 19] on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [52] <author> J. W. Shavlik. </author> <title> Combining symbolic and neural learning. </title> <booktitle> Machine Learning, </booktitle> <address> 14(3):321331, </address> <year> 1994. </year>
Reference-contexts: worse because the error surface presents greater difficulty to the training method. 7 Automata Extraction The extraction of symbolic knowledge from trained neural networks allows the exchange of information between connectionist and symbolic knowledge representations and has been of great interest for understanding what the neural network is actually doing <ref> [52] </ref>. In addition symbolic knowledge can be inserted into recurrent neural networks and even refined after training [15, 47, 45].
Reference: [53] <author> H.T. Siegelmann. </author> <title> Computation beyond the turing limit. </title> <booktitle> Science, </booktitle> <address> 268:545548, </address> <year> 1995. </year>
Reference-contexts: Only recurrent neural networks are investigated for computational reasons. Computationally, recurrent neural networks are more powerful than feedforward networks and some recurrent architectures have been shown to be at least Turing equivalent <ref> [53, 54] </ref>. We investigate the properties of various popular recurrent neural network architectures, in particular Elman, Narendra & Parthasarathy (N&P) and Williams & Zipser (W&Z) recurrent networks, and also Frasconi-Gori-Soda (FGS) locally recurrent networks.
Reference: [54] <author> H.T. Siegelmann, B.G. Horne, and C.L. Giles. </author> <title> Computational capabilities of recurrent NARX neural networks. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics Part B, </journal> <volume> 27(2):208, </volume> <year> 1997. </year>
Reference-contexts: Only recurrent neural networks are investigated for computational reasons. Computationally, recurrent neural networks are more powerful than feedforward networks and some recurrent architectures have been shown to be at least Turing equivalent <ref> [53, 54] </ref>. We investigate the properties of various popular recurrent neural network architectures, in particular Elman, Narendra & Parthasarathy (N&P) and Williams & Zipser (W&Z) recurrent networks, and also Frasconi-Gori-Soda (FGS) locally recurrent networks. <p> It is not surprising that the Elman network outperforms the FGS and N&P networks. The computational power of Elman networks has been shown to be at least Turing equivalent [55], where the N&P networks have been shown to be Turing equivalent <ref> [54] </ref> but to within a linear slowdown. FGS networks 20 state is state 1 at the bottom left and the accepting state is state 17 at the top right.
Reference: [55] <author> H.T. Siegelmann and E.D. Sontag. </author> <title> On the computational power of neural nets. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50(1):132150, </volume> <year> 1995. </year>
Reference-contexts: From best to worst performance, the architectures were: Elman, W&Z, N&P and FGS. It is not surprising that the Elman network outperforms the FGS and N&P networks. The computational power of Elman networks has been shown to be at least Turing equivalent <ref> [55] </ref>, where the N&P networks have been shown to be Turing equivalent [54] but to within a linear slowdown. FGS networks 20 state is state 1 at the bottom left and the accepting state is state 17 at the top right.
Reference: [56] <author> P. Simard, M.B. Ottaway, and D.H. Ballard. </author> <title> Analysis of recurrent backpropagation. </title> <editor> In D. Touretzky, G. Hinton, and T. Sejnowski, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 103112, </pages> <address> San Mateo, 1989. (Pittsburg 1988), </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Simulated annealing makes very few assumptions regarding the function to be optimized, and is therefore quite robust with respect to non-quadratic error surfaces. Previous work has shown the use of simulated annealing for finding the parameters of a recurrent network model to improve performance <ref> [56] </ref>. For comparison with the gradient descent based algorithms the use of simulated annealing has been investigated in order to train exactly the same Elman network as has been successfully trained to 100% correct training set classification using backpropagation-through-time (details are in section 6). <p> No significant results were obtained from these trials 11 . The use of simulated annealing has not been found to improve performance as in Simard et al. <ref> [56] </ref>. However, their problem was the parity problem using networks with only four hidden units whereas the networks considered in this paper have many more parameters. This result provides an interesting comparison to the gradient descent backpropagation-through-time (BPTT) method.
Reference: [57] <author> S.A. Solla, E. Levin, and M. Fleisher. </author> <title> Accelerated learning in layered neural networks. </title> <journal> Complex Systems, </journal> <volume> 2:625639, </volume> <year> 1988. </year>
Reference-contexts: Activation Function. Symmetric sigmoid functions (e.g. tanh) often improve convergence over the standard logistic function. For our particular problem we found that the difference was minor and that the logistic function resulted in better performance as shown in table 4. 7. Cost Function. The relative entropy cost function <ref> [4, 29, 57, 26, 27] </ref> has received particular attention and has a natural interpretation in terms of learning probabilities [36].
Reference: [58] <author> M. F. St. John and J.L. McClelland. </author> <title> Learning and applying contextual constraints in sentence comprehension. </title> <journal> Artificial Intelligence, </journal> <volume> 46:546, </volume> <year> 1990. </year>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical inference [9, 21, 19, 20, 68]. Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: <ref> [1, 12, 24, 58, 59] </ref>. Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology [51, 41, 40] and role assignment [42, 58]. <p> Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology [51, 41, 40] and role assignment <ref> [42, 58] </ref>. Induction of simpler grammars has been addressed often e.g. [64, 65, 19] on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [59] <author> Andreas Stolcke. </author> <title> Learning feature-based semantics with simple recurrent networks. </title> <type> Technical Report TR-90015, </type> <institution> International Computer Science Institute, Berkeley, California, </institution> <month> April </month> <year> 1990. </year> <month> 25 </month>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical inference [9, 21, 19, 20, 68]. Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: <ref> [1, 12, 24, 58, 59] </ref>. Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology [51, 41, 40] and role assignment [42, 58].
Reference: [60] <author> M. Tomita. </author> <title> Dynamic construction of finite-state automata from examples using hill-climbing. </title> <booktitle> In Proceedings of the Fourth Annual Cognitive Science Conference, </booktitle> <pages> pages 105108, </pages> <address> Ann Arbor, MI, </address> <year> 1982. </year>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology [51, 41, 40] and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. [64, 65, 19] on learning Tomita languages <ref> [60] </ref>. The task considered here differs from these in that the grammar is more complex.
Reference: [61] <author> D. S. Touretzky. </author> <title> Rules and maps in connectionist symbol processing. </title> <type> Technical Report CMU-CS-89-158, </type> <institution> Carnegie Mellon University: Department of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1989. </year>
Reference-contexts: Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: [1, 12, 24, 58, 59]. Neural network models have been shown to be able to account for a variety of phenomena in phonology <ref> [23, 61, 62, 18, 22] </ref>, morphology [51, 41, 40] and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. [64, 65, 19] on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [62] <author> D. S. Touretzky. </author> <title> Towards a connectionist phonology: The many maps approach to sequence manipulation. </title> <booktitle> In Proceedings of the 11th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 188195, </pages> <year> 1989. </year>
Reference-contexts: Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: [1, 12, 24, 58, 59]. Neural network models have been shown to be able to account for a variety of phenomena in phonology <ref> [23, 61, 62, 18, 22] </ref>, morphology [51, 41, 40] and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. [64, 65, 19] on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [63] <author> A.C. Tsoi and A.D. </author> <title> Back. Locally recurrent globally feedforward networks: A critical review of architectures. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2):229239, </volume> <year> 1994. </year>
Reference-contexts: Frasconi-Gori-Soda (FGS) locally recurrent networks [16]. A multilayer perceptron augmented with local feedback around each hidden node. The local-output version has been used. The FGS network has also been studied by [43] the network is called FGS in this paper in line with <ref> [63] </ref>. 2. Narendra and Parthasarathy [44]. A recurrent network with feedback connections from each output node to all hidden nodes. The N&P network architecture has also been studied by Jordan [33, 34] the network is called N&P in this paper in line with [30]. 3. Elman [13].
Reference: [64] <author> R.L. Watrous and G.M. Kuhn. </author> <title> Induction of finite state languages using second-order recurrent networks. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 309316, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology [51, 41, 40] and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. <ref> [64, 65, 19] </ref> on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [65] <author> R.L. Watrous and G.M. Kuhn. </author> <title> Induction of finite-state languages using second-order recurrent networks. </title> <booktitle> Neural Computation, </booktitle> <address> 4(3):406, </address> <year> 1992. </year>
Reference-contexts: Neural network models have been shown to be able to account for a variety of phenomena in phonology [23, 61, 62, 18, 22], morphology [51, 41, 40] and role assignment [42, 58]. Induction of simpler grammars has been addressed often e.g. <ref> [64, 65, 19] </ref> on learning Tomita languages [60]. The task considered here differs from these in that the grammar is more complex.
Reference: [66] <author> R.J. Williams and J. Peng. </author> <title> An efficient gradient-based algorithm for on-line training of recurrent network trajec-tories. </title> <booktitle> Neural Computation, </booktitle> <address> 2(4):490501, </address> <year> 1990. </year>
Reference-contexts: Thus, we are most interested in the small input window case, where the networks are required to form a grammar in order to perform well. 5 Gradient Descent and Simulated Annealing Learning Backpropagation-through-time <ref> [66] </ref> 5 has been used to train the globally recurrent networks 6 , and the gradient descent algorithm described by the authors [16] was used for the FGS network. The standard gradient descent algorithms were found to be impractical for this problem 7 .
Reference: [67] <author> R.J. Williams and D. Zipser. </author> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <booktitle> Neural Computation, </booktitle> <address> 1(2):270280, </address> <year> 1989. </year>
Reference-contexts: When training the Elman network backpropagation-through-time is used rather than the truncated version used by Elman, i.e. in this paper Elman network refers to the architecture used by Elman but not the training algorithm. 4. Williams and Zipser <ref> [67] </ref>. A recurrent network where all nodes are connected to all other nodes. Diagrams of these architectures are shown in figures 1 to 4. <p> However, due to the computational complexity of the task 8 , it was not possible to perform as many simulations as 5 Backpropagation-through-time extends backpropagation to include temporal aspects and arbitrary connection topologies by considering an equivalent feedforward network created by unfolding the recurrent network in time. 6 Real-time <ref> [67] </ref> recurrent learning (RTRL) was also tested but did not show any significant convergence for the present problem. 7 Without modifying the standard gradient descent algorithms it was only possible to train networks which operated on a large temporal input window.
Reference: [68] <author> Z. Zeng, R.M. Goodman, and P. Smyth. </author> <title> Learning finite state machines with self-clustering recurrent networks. </title> <booktitle> Neural Computation, </booktitle> <address> 5(6):976990, </address> <year> 1993. </year> <month> 26 </month>
Reference-contexts: The algorithm is currently only practical for relatively small grammars [48]. 2 which have been used for grammatical inference <ref> [9, 21, 19, 20, 68] </ref>. Recurrent neural networks have been used for several smaller natural language problems, e.g. papers using the Elman network for natural language tasks include: [1, 12, 24, 58, 59].
References-found: 68

