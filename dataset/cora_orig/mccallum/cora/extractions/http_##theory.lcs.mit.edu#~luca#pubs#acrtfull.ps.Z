URL: http://theory.lcs.mit.edu/~luca/pubs/acrtfull.ps.Z
Refering-URL: http://theory.lcs.mit.edu/~luca/papers.html
Root-URL: 
Title: Weak Random Sources, Hitting Sets, and BPP Simulations  
Author: Alexander E. Andreev Andrea E. F. Clementi Jose D. P. Rolim Luca Trevisan -January 
Keyword: Key Words and Phrases: Derandomization, Imperfect Sources of Randomness, Hitting Sets, Randomized Computations, Expander Graphs. Abbreviated Title: BPP Simulations using Weak Random Sources.  
Web: 68Q10, 11K45.  
Note: AMS Subject Classification:  
Date: 26, 1998  
Abstract: We show how to simulate any BPP algorithm in polynomial time using a weak random source of r bits and min-entropy r fl for any fl &gt; 0. This follows from a more general result about sampling with weak random sources. Our result matches an information-theoretic lower bound and solves a question that has been open for some years. The previous best results were a polynomial time simulation of RP [Saks, Srinivasan and Zhou 1995] and a quasi-polynomial time simulation of BPP [Ta-Shma 1996]. Departing significantly from previous related works, we do not use extractors; instead, we use the OR-disperser of [Saks, Srinivasan, and Zhou 1995] in combination with a tricky use of hitting sets borrowed from [Andreev, Clementi, and Rolim 1996].
Abstract-found: 1
Intro-found: 1
Reference: [ACR97a] <author> A.E. Andreev, A.E.F. Clementi, and J.D.P. </author> <title> Rolim. A new general de-randomization method. </title> <journal> Journal of the ACM, </journal> <note> to appear, 1997. Preliminary version in Proc. of ICALP'96. </note>
Reference-contexts: Informally speaking, in the context of derandomization, pseudorandom generators play the same role 2 of extractors and hitting sets generators play that of dispersers. A recent result of Andreev et al. <ref> [ACR97a] </ref> shows how to deterministically simulate BPP algorithms using hitting set generators. This suggests that perhaps dispersers could be used to simulate BPP with weak random sources. <p> The main technical result of <ref> [ACR97a] </ref> can be stated as follows. Lemma 1 ([ACR97a]) For any choice of constants *; ffi &gt; 0, there is a deterministic algorithm that, given access to a quick ffi-HSG, and given in input any circuit C of size n returns in poly (n) time a value D such that jPr <p> Lemma 1 immediately implies the following general derandomization result. Theorem 2 If for some ffi &gt; 0 a quick ffi-HSG exists, then P = BPP. Andreev et al. <ref> [ACR97a] </ref> prove Lemma 1 by constructing a set S of size poly (n) that is *-discrepant for C, i.e. such that Pr x2S [C (x) = 1] approximates the value Pr x [C (x) = 1] up to an additive error *. <p> Thus, proving the theorem amounts to find a set S that passes the test. This task is solved in <ref> [ACR97a] </ref> by means of a rather involved (and inherently sequential) algorithm. The algorithm indeed proves a somewhat stronger result than Lemma 1 and has been also used in [ACR97b] in a different context. For the sake of proving Lemma 1 it might however be over-kill. <p> The definition below is a slight variant of the definition of quick ffi-HSG of price O (log n) given in <ref> [ACR97a] </ref>. Definition 11 (Hitting Set Generator) A quick ffi-HSG is a polynomial-time algorithm H that, on input a number n in unary, returns a multiset H (n) f0; 1g n that is ffi-hitting for the set ff : f0; 1g n ! f0; 1g : L (f ) ng. <p> By using random walks on expander graphs instead that simple repetition, An-dreev et al. [ACR97b] show that, for c &gt; 1=2, even the existence of a (12 cn )-HSG is an equivalent condition. 3 The Discrepancy Test In this section we describe the discrepancy test of Andreev et al. <ref> [ACR97a] </ref>. We present a slight variation of the proof of [ACR97a] that the test is sound, and also prove a "completeness" property of the test. <p> simple repetition, An-dreev et al. [ACR97b] show that, for c &gt; 1=2, even the existence of a (12 cn )-HSG is an equivalent condition. 3 The Discrepancy Test In this section we describe the discrepancy test of Andreev et al. <ref> [ACR97a] </ref>. We present a slight variation of the proof of [ACR97a] that the test is sound, and also prove a "completeness" property of the test. <p> In this test, the set S is tested to be *-discrepant for f by using the auxiliary (hitting) set H. Theorem 13 (Soundness of disc-test <ref> [ACR97a] </ref>) A constant c 1 exists such that, for any * &gt; 0, integer n, function f : f0; 1g n ! f0; 1g, sets S; H f0; 1g n , if disc-test (f; S; H; *) = 1 and H is ffi-hitting for the set of functions g such that <p> := minfp (a; f; S) : a 2 H [ f ~ 0gg; p max := maxfp (a; f; S) : a 2 H [ f ~ 0gg; if p max p min * then return (1) else return (0) end Theorem 13 is the core of the results of <ref> [ACR97a] </ref>. Note that it says that a set H with a certain one-sided pseudorandom property (the hitting property) can be used to test S for a two-sided pseudorandom property (the discrepancy property). <p> However H has to be hitting for a whole set of functions while S is tested for discrepancy on a single function (i.e. f ). So, roughly speaking, the theorem trades-off "globality" versus "two-sidedness". The version of Theorem 13 proved in <ref> [ACR97a] </ref> requires f to be computable by a small circuit and H to be ffi-hitting for a family of functions of low circuit complexity. Here we have no requirement on f and H is required to be hitting for a set of functions directly "related" to f .
Reference: [ACR97b] <author> A.E. Andreev, A.E.F. Clementi, and J.D.P. </author> <title> Rolim. Worst-case hardness suffices for derandomization: A new method for hardness vs randomness trade-offs. </title> <booktitle> In Proceedings of the 24th International Colloquium on Automata, Languages and Programming, </booktitle> <pages> pages 177-187. </pages> <publisher> LNCS 1256, Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: Pseudorandom Generators and Hitting Sets A more ambitious goal than simulating BPP with weak random sources is the deterministic simulation of BPP. Research on this subject tries to isolate reasonable complexity assumptions under which deterministic simulations of randomized algorithms are possible <ref> [Y82, BM84, Nis90, BFNW93, NW94, IW97, ACR97b] </ref>. In some cases, combinatorial objects developed in the study of weak random sources have been used to give derandomization [NZ96]. Here we revert this connection, and use a derandomization method to take full advantage from a weak random source. <p> Thus, proving the theorem amounts to find a set S that passes the test. This task is solved in [ACR97a] by means of a rather involved (and inherently sequential) algorithm. The algorithm indeed proves a somewhat stronger result than Lemma 1 and has been also used in <ref> [ACR97b] </ref> in a different context. For the sake of proving Lemma 1 it might however be over-kill. Our Results We show how to use dispersers and weak random sources to simulate BPP in polynomial time and to even solve a more general sampling problem. <p> This simplified proof is presented in a preliminary version of this paper [ACRT97] and also in an appendix of the final version of the paper of Andreev et al. <ref> [ACR97b] </ref>. More recently, Lance Fortnow has observed that an even simpler proof of Theorem 2 can be given by using a previous result of Lautemann [L83]. Fortnow's proof of Theorem 2 does not use the discrepancy test. <p> By using random walks on expander graphs instead that simple repetition, An-dreev et al. <ref> [ACR97b] </ref> show that, for c &gt; 1=2, even the existence of a (12 cn )-HSG is an equivalent condition. 3 The Discrepancy Test In this section we describe the discrepancy test of Andreev et al. [ACR97a]. <p> In contrast, the proof of Lemma 1 appeared in <ref> [ACR97b] </ref> seems to be inherently sequential. Andreev et al. [ACR97b] have recently used the NC proof of Theorem 2 in order to provide sufficient conditions (in terms of worst-case circuit complexity) for NC = BPNC. <p> In contrast, the proof of Lemma 1 appeared in <ref> [ACR97b] </ref> seems to be inherently sequential. Andreev et al. [ACR97b] have recently used the NC proof of Theorem 2 in order to provide sufficient conditions (in terms of worst-case circuit complexity) for NC = BPNC.
Reference: [ACRT97] <author> A.E. Andreev, A.E.F. Clementi, J.D.P. Rolim, and L. Trevisan. </author> <title> Weak random sources, hitting sets, and BPP simulations. </title> <booktitle> In Proceedings of the 38th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 264-272, </pages> <year> 1997. </year>
Reference-contexts: The idea of generating candidate discrepancy sets S 1 ; : : :; S k and then applying the discrepancy test to them also yields a simple proof of Lemma 1. This simplified proof is presented in a preliminary version of this paper <ref> [ACRT97] </ref> and also in an appendix of the final version of the paper of Andreev et al. [ACR97b]. More recently, Lance Fortnow has observed that an even simpler proof of Theorem 2 can be given by using a previous result of Lautemann [L83]. <p> Thus, our method provides also an efficient simulation of BPNC algorithms using weak random sources. Likewise, the proof of Lemma 1 as appeared in a preliminary version of this paper <ref> [ACRT97] </ref>, as well as the proof of Theorem 2 described in Section 5, implies an NC simulation of randomized algorithms when both the algorithm and the hitting set are given as oracles. In contrast, the proof of Lemma 1 appeared in [ACR97b] seems to be inherently sequential.
Reference: [BFNW93] <author> L. Babai, L. Fortnow, N. Nisan, and A. Wigderson. </author> <title> BPP has subexponential time sim ulations unless EXPTIME has publishable proofs. </title> <journal> Computational Complexity, </journal> <volume> 3(4) </volume> <pages> 307-318, </pages> <year> 1993. </year>
Reference-contexts: Pseudorandom Generators and Hitting Sets A more ambitious goal than simulating BPP with weak random sources is the deterministic simulation of BPP. Research on this subject tries to isolate reasonable complexity assumptions under which deterministic simulations of randomized algorithms are possible <ref> [Y82, BM84, Nis90, BFNW93, NW94, IW97, ACR97b] </ref>. In some cases, combinatorial objects developed in the study of weak random sources have been used to give derandomization [NZ96]. Here we revert this connection, and use a derandomization method to take full advantage from a weak random source.
Reference: [BM84] <author> M. Blum and S. Micali. </author> <title> How to generate cryptographically strong sequences of pseudo random bits. </title> <journal> SIAM Journal of Computing, </journal> <volume> 13(4) </volume> <pages> 850-864, </pages> <year> 1984. </year> <note> Preliminary version in Proc. of FOCS'82. </note>
Reference-contexts: Pseudorandom Generators and Hitting Sets A more ambitious goal than simulating BPP with weak random sources is the deterministic simulation of BPP. Research on this subject tries to isolate reasonable complexity assumptions under which deterministic simulations of randomized algorithms are possible <ref> [Y82, BM84, Nis90, BFNW93, NW94, IW97, ACR97b] </ref>. In some cases, combinatorial objects developed in the study of weak random sources have been used to give derandomization [NZ96]. Here we revert this connection, and use a derandomization method to take full advantage from a weak random source.
Reference: [BR94] <author> M. Bellare and J. Rompel. </author> <title> Randomness-efficient oblivious sampling. </title> <booktitle> In Proceedings of the 35th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 276-287, </pages> <year> 1994. </year>
Reference-contexts: The source of this non-obliviousness is the selection of a good set S j among the candidates S 1 ; : : : ; S k . As a result, our sampling algorithm is not oblivious according to the definition of Bellare and Rompel <ref> [BR94] </ref>, however it is non-adaptive. See [G97] for definitions of these notions and for a survey on sampling.
Reference: [CG88] <author> B. Chor and O. Goldreich. </author> <title> Unbiased bits from sources of weak randomness and proba bilistic communication complexity. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(2) </volume> <pages> 230-261, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: Since it is questionable whether truly random sources really exist, much research has been devoted in the last decade to find weaker notions of randomness that are still sufficient to run BPP algorithms in polynomial time <ref> [VV85, SV86, Vaz86, Vaz87, CG88, Zuc90] </ref>. <p> Several definitions of weak random source have been proposed in the literature, the most general being the following <ref> [CG88, Zuc90] </ref>: for fl &gt; 0, an (r; r fl )-source is a random source that fl An extended abstract of this paper appears in the Proceedings of the 38-th IEEE Symposium on Foundations of Computer Science. y andreev@mntn.msk.su. University of Moscow. z clementi@dsi.uniroma1.it.
Reference: [CW89] <author> A. Cohen and A. Wigderson. Dispersers, </author> <title> deterministic amplification, and weak random sources. </title> <booktitle> In Proceedings of the 30th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 14-19, </pages> <year> 1989. </year>
Reference-contexts: A polynomial-time simulation of BPP using weak random sources of min-entropy r fl for any fixed fl &gt; 0 was one of the major open questions in the field. It is not difficult to show that to simulate RP by means of a weak random source, OR dispersers <ref> [CW89] </ref> (from now on, we will simply call them dispersers) are sufficient.
Reference: [ESY84] <author> S. Even, A. Selman, and Y. Yacoby. </author> <title> The complexity of promise problems with applica tions to public-key cryptography. </title> <journal> Information and Control, </journal> <volume> 2 </volume> <pages> 159-173, </pages> <year> 1984. </year>
Reference-contexts: We first have to introduce some new notation. For a set S and a property we denote by 9 + x 2 S:(x) the statement "at least half the elements of S have property ." A promise problem <ref> [ESY84] </ref> is a pair of disjoint sets of strings (Y; N ). An algorithm A solves a promise problem (Y; N ) if A accepts any element of Y and rejects any element of N .
Reference: [G97] <author> O. Goldreich. </author> <title> A sample of samplers | A computational perspective on sampling. </title> <booktitle> Elec tronic Colloquium on Computational Complexity TR97-020, </booktitle> <year> 1997. </year>
Reference-contexts: As a result, our sampling algorithm is not oblivious according to the definition of Bellare and Rompel [BR94], however it is non-adaptive. See <ref> [G97] </ref> for definitions of these notions and for a survey on sampling. Our main result can be stated in the following way Theorem 3 (Main Theorem) For any fl &gt; 0, there exist a polynomial p and a deterministic algorithm A such that the following holds. <p> The proof of Theorem 3 contained in this paper can be easily generalized to the case of such functions. We choose however to state and prove only the case of Boolean functions since proofs are cleaner and since, as proved in <ref> [G97] </ref>, sampling real-valued functions is reducible to sampling Boolean functions. We can thus get the following result as a corollary of Theorem 3 and of [G97, Theorem 5.5]. Corollary 24 For any fl &gt; 0, there exist a polynomial p and a deterministic algorithm A such that the following holds. <p> We choose however to state and prove only the case of Boolean functions since proofs are cleaner and since, as proved in [G97], sampling real-valued functions is reducible to sampling Boolean functions. We can thus get the following result as a corollary of Theorem 3 and of <ref> [G97, Theorem 5.5] </ref>. Corollary 24 For any fl &gt; 0, there exist a polynomial p and a deterministic algorithm A such that the following holds.
Reference: [IW97] <author> R. Impagliazzo and A. Wigderson. </author> <title> P= BPP if E requires exponential circuits: Deran domizing the XOR lemma. </title> <booktitle> In Proceedings of the 29th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 220-229, </pages> <year> 1997. </year> <month> 14 </month>
Reference-contexts: Pseudorandom Generators and Hitting Sets A more ambitious goal than simulating BPP with weak random sources is the deterministic simulation of BPP. Research on this subject tries to isolate reasonable complexity assumptions under which deterministic simulations of randomized algorithms are possible <ref> [Y82, BM84, Nis90, BFNW93, NW94, IW97, ACR97b] </ref>. In some cases, combinatorial objects developed in the study of weak random sources have been used to give derandomization [NZ96]. Here we revert this connection, and use a derandomization method to take full advantage from a weak random source.
Reference: [L83] <author> C. Lautemann. </author> <title> BPP and the Polynomial Hierarchy. </title> <journal> Information Processing Letters, </journal> <volume> 17 </volume> <pages> 215-217, </pages> <year> 1983. </year>
Reference-contexts: More recently, Lance Fortnow has observed that an even simpler proof of Theorem 2 can be given by using a previous result of Lautemann <ref> [L83] </ref>. Fortnow's proof of Theorem 2 does not use the discrepancy test. To the best of our understanding, this new proof does not extend to the context of dispersers and weak random sources, and it seems that we still need the discrepancy test in order to prove Theorem 3. <p> and only if there is a polynomial time algorithm A (; ) and a polynomial p () such that for any x of length n x 2 Y ) 9 + y 2 f0; 1g p (n) :A (x; y) = 1 We will use the following result of Lautemann <ref> [L83] </ref> (that is an improvement on a previous result by Sipser [S83]). Theorem 22 (Lautemann [L83]) If L 2 BPP then there exists a polynomial time computable Boolean function A (; ; ) and two polynomials p () and q () such that for any x of lenght n x 2 <p> p () such that for any x of length n x 2 Y ) 9 + y 2 f0; 1g p (n) :A (x; y) = 1 We will use the following result of Lautemann <ref> [L83] </ref> (that is an improvement on a previous result by Sipser [S83]). Theorem 22 (Lautemann [L83]) If L 2 BPP then there exists a polynomial time computable Boolean function A (; ; ) and two polynomials p () and q () such that for any x of lenght n x 2 L ) 9 + y 2 f0; 1g p (n) :8z 2 f0; 1g q
Reference: [LV90] <author> M. Li and P. Vitany. </author> <title> Kolmogorov complexity and its applications. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A, </booktitle> <pages> pages 187-254. </pages> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: For example, K U (f jf ) = O (1). As usual, if we fix another universal Turing machine U 0 it holds K U 0 (gjf ) = K U (gjf )+fi (1). We will usually omit the subscript. See e.g. <ref> [LV90] </ref> for an introduction to Kolmogorov complexity. In this paper we only use the obvious fact that, for any fixed f , the number of functions g such that K (gjf ) k is at most 2 k .
Reference: [MR95] <author> R. Motwani and P. Raghavan. </author> <title> Randomized Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction Randomized algorithms are often the simpler ones to solve a given problem, or the most efficient, or both (see <ref> [MR95] </ref>). For some problems, including primality testing and approximation of # P-complete counting problems, only randomized solutions are known. The practical applicability of such randomized methods depends on the effective possibility for an algorithm to access truly random bits.
Reference: [Nis90] <author> N. Nisan. </author> <title> Using Hard Problems to Create Pseudorandom Generators. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> ACM Distinguished Dissertations. </note>
Reference-contexts: Pseudorandom Generators and Hitting Sets A more ambitious goal than simulating BPP with weak random sources is the deterministic simulation of BPP. Research on this subject tries to isolate reasonable complexity assumptions under which deterministic simulations of randomized algorithms are possible <ref> [Y82, BM84, Nis90, BFNW93, NW94, IW97, ACR97b] </ref>. In some cases, combinatorial objects developed in the study of weak random sources have been used to give derandomization [NZ96]. Here we revert this connection, and use a derandomization method to take full advantage from a weak random source.
Reference: [Nis96] <author> N. Nisan. </author> <title> Extracting randomness: How and why. </title> <booktitle> In Proceedings of the 11th IEEE Conference on Computational Complexity, </booktitle> <pages> pages 44-58, </pages> <year> 1996. </year>
Reference-contexts: This construction is somewhat easier to obtain, and Saks et al. [SSZ95] give indeed a disperser with d = poly (n), for any constant fl &gt; 0, allowing for a polynomial time simulation of RP. See <ref> [Nis96] </ref> for a complete survey on extractors, dispersers, and weak random sources. Pseudorandom Generators and Hitting Sets A more ambitious goal than simulating BPP with weak random sources is the deterministic simulation of BPP. <p> The main novelty in our result has been the use of dispersers in a context where extractors seemed to be necessary. Extractors have other applications besides the use of weak random sources (see, e.g., <ref> [Nis96] </ref>). It could be the case that techniques similar to ours can give stronger results or simplified proofs in these other applications as well.
Reference: [NW94] <author> N. Nisan and A. Wigderson. </author> <title> Hardness vs randomness. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 49 </volume> <pages> 149-167, </pages> <year> 1994. </year> <note> Preliminary version in Proc. of FOCS'88. </note>
Reference-contexts: Pseudorandom Generators and Hitting Sets A more ambitious goal than simulating BPP with weak random sources is the deterministic simulation of BPP. Research on this subject tries to isolate reasonable complexity assumptions under which deterministic simulations of randomized algorithms are possible <ref> [Y82, BM84, Nis90, BFNW93, NW94, IW97, ACR97b] </ref>. In some cases, combinatorial objects developed in the study of weak random sources have been used to give derandomization [NZ96]. Here we revert this connection, and use a derandomization method to take full advantage from a weak random source.
Reference: [NZ96] <author> N. Nisan and D. Zuckerman. </author> <title> Randomness is linear in space. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52(1) </volume> <pages> 43-52, </pages> <year> 1996. </year> <note> Preliminary version in Proc. of STOC'93. </note>
Reference-contexts: The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to <p> Research on this subject tries to isolate reasonable complexity assumptions under which deterministic simulations of randomized algorithms are possible [Y82, BM84, Nis90, BFNW93, NW94, IW97, ACR97b]. In some cases, combinatorial objects developed in the study of weak random sources have been used to give derandomization <ref> [NZ96] </ref>. Here we revert this connection, and use a derandomization method to take full advantage from a weak random source.
Reference: [S83] <author> M. Sipser. </author> <title> A complexity theoretic approach to randomness. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 330-335, </pages> <year> 1983. </year>
Reference-contexts: ) and a polynomial p () such that for any x of length n x 2 Y ) 9 + y 2 f0; 1g p (n) :A (x; y) = 1 We will use the following result of Lautemann [L83] (that is an improvement on a previous result by Sipser <ref> [S83] </ref>).
Reference: [SSZ95] <author> M. Saks, A. Srinivasan, and S. Zhou. </author> <title> Explicit dispersers with polylog degree. </title> <booktitle> In Pro ceedings of the 27th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 479-488, </pages> <year> 1995. </year>
Reference-contexts: The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to <p> This construction is somewhat easier to obtain, and Saks et al. <ref> [SSZ95] </ref> give indeed a disperser with d = poly (n), for any constant fl &gt; 0, allowing for a polynomial time simulation of RP. See [Nis96] for a complete survey on extractors, dispersers, and weak random sources. <p> p max p min *. 2 4 Proof of Theorem 3 The starting point of our proof is the following easy observation: If we have a set I f0; 1g N such that Pr x [x 2 I] &gt; 1=2, then using a weak random source and the dispersers of <ref> [SSZ95] </ref>, we can generate a polynomial-sized (in N ) set of vectors x 1 ; : : : ; x k such that, with high probability, fx 1 ; : : : ; x k g " I 6= ;. This is formalized in Corollary 19 below. <p> As a consequence, the set S i H i has (with high probability) the hitting property required by Theorem 13. We start by quoting the disperser construction of Saks, Srinivasan, and Zhou. 9 Theorem 18 (Construction of dispersers <ref> [SSZ95] </ref>) For any 0 &lt; &lt; ff 1, for any sufficiently large r, and for any 2 r ff T 2 r , there exists an efficient construction of a (2 r ; 2 r ; T )-disperser G = (V; W; E) of degree poly (r).
Reference: [SSZ97] <author> M. Saks, A. Srinivasan, and S. Zhou. </author> <type> Personal communication, </type> <month> March </month> <year> 1997. </year>
Reference-contexts: We also emphasize that our simulation runs in NC. This is due to the parallel nature of our construction and to the fact that it is possible to give an NC construction of the SSZ-dispersers <ref> [SSZ97] </ref>. Thus, our method provides also an efficient simulation of BPNC algorithms using weak random sources.
Reference: [SV86] <author> M. Santha and U. Vazirani. </author> <title> Generating quasi-random sequences from slightly random sources. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 33 </volume> <pages> 75-87, </pages> <year> 1986. </year>
Reference-contexts: Since it is questionable whether truly random sources really exist, much research has been devoted in the last decade to find weaker notions of randomness that are still sufficient to run BPP algorithms in polynomial time <ref> [VV85, SV86, Vaz86, Vaz87, CG88, Zuc90] </ref>.
Reference: [SZ94] <author> A. Srinivasan and D. Zuckerman. </author> <title> Computing with very weak random sources. </title> <booktitle> In Proceedings of the 35th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 264-275, </pages> <year> 1994. </year>
Reference-contexts: The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to
Reference: [TS96] <author> A. Ta-Shma. </author> <title> On extracting randomness from weak random sources. </title> <booktitle> In Proceedings of the 28th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 276-285, </pages> <year> 1996. </year>
Reference-contexts: The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to <p> However, the best present construction of extractors for fixed fl &gt; 0 and r = poly (m) has d = n log (k) n <ref> [TS96] </ref>. This implies a quasi-polynomial time simulation of BPP. A polynomial-time simulation of BPP using weak random sources of min-entropy r fl for any fixed fl &gt; 0 was one of the major open questions in the field.
Reference: [Vaz86] <author> U. Vazirani. </author> <title> Randomness, Adversaries and Computation. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1986. </year>
Reference-contexts: Since it is questionable whether truly random sources really exist, much research has been devoted in the last decade to find weaker notions of randomness that are still sufficient to run BPP algorithms in polynomial time <ref> [VV85, SV86, Vaz86, Vaz87, CG88, Zuc90] </ref>.
Reference: [Vaz87] <author> U. Vazirani. </author> <title> Efficiency considerations in using semi-random sources. </title> <booktitle> In Proceedings of the 19th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 160-168, </pages> <year> 1987. </year>
Reference-contexts: Since it is questionable whether truly random sources really exist, much research has been devoted in the last decade to find weaker notions of randomness that are still sufficient to run BPP algorithms in polynomial time <ref> [VV85, SV86, Vaz86, Vaz87, CG88, Zuc90] </ref>.
Reference: [VV85] <author> U. Vazirani and V. Vazirani. </author> <title> Random polynomial time is equal to slightly random poly nomial time. </title> <booktitle> In Proceedings of the 26th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 417-428, </pages> <year> 1985. </year>
Reference-contexts: Since it is questionable whether truly random sources really exist, much research has been devoted in the last decade to find weaker notions of randomness that are still sufficient to run BPP algorithms in polynomial time <ref> [VV85, SV86, Vaz86, Vaz87, CG88, Zuc90] </ref>.
Reference: [Y82] <author> A.C. Yao. </author> <title> Theory and applications of trapdoor functions. </title> <booktitle> In Proceedings of the 23rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 80-91, </pages> <year> 1982. </year> <month> 15 </month>
Reference-contexts: Pseudorandom Generators and Hitting Sets A more ambitious goal than simulating BPP with weak random sources is the deterministic simulation of BPP. Research on this subject tries to isolate reasonable complexity assumptions under which deterministic simulations of randomized algorithms are possible <ref> [Y82, BM84, Nis90, BFNW93, NW94, IW97, ACR97b] </ref>. In some cases, combinatorial objects developed in the study of weak random sources have been used to give derandomization [NZ96]. Here we revert this connection, and use a derandomization method to take full advantage from a weak random source.
Reference: [Zuc90] <author> D. Zuckerman. </author> <title> General weak random sources. </title> <booktitle> In Proceedings of the 31st IEEE Sympo sium on Foundations of Computer Science, </booktitle> <pages> pages 534-543, </pages> <year> 1990. </year>
Reference-contexts: Since it is questionable whether truly random sources really exist, much research has been devoted in the last decade to find weaker notions of randomness that are still sufficient to run BPP algorithms in polynomial time <ref> [VV85, SV86, Vaz86, Vaz87, CG88, Zuc90] </ref>. <p> Several definitions of weak random source have been proposed in the literature, the most general being the following <ref> [CG88, Zuc90] </ref>: for fl &gt; 0, an (r; r fl )-source is a random source that fl An extended abstract of this paper appears in the Proceedings of the 38-th IEEE Symposium on Foundations of Computer Science. y andreev@mntn.msk.su. University of Moscow. z clementi@dsi.uniroma1.it. <p> The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to
Reference: [Zuc96a] <author> D. Zuckerman. </author> <title> Randomness-optimal sampling, extractors and constructive leader elec tion. </title> <booktitle> In Proceedings of the 28th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 286-295, </pages> <year> 1996. </year>
Reference: [Zuc96b] <author> D. Zuckerman. </author> <title> Simulating BPP using a general weak random source. </title> <journal> Algorithmica, </journal> 16(4/5):367-391, 1996. Preliminary version in Proc. of FOCS'91. <volume> 16 </volume>
Reference-contexts: The procedure that computes the sample space starting from the output of the source is independent of the algorithm that we want to derandomize. This simulation is basically equivalent <ref> [Zuc90, Zuc96b, NZ96, SZ94, SSZ95, TS96] </ref> to a bipartite graph G = (V; W; E) having 2 r nodes in the left component V , 2 m nodes in the right component W , degree d and such that if we select a node v in the left component according to
References-found: 31

