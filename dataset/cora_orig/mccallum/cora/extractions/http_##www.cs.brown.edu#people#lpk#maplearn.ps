URL: http://www.cs.brown.edu/people/lpk/maplearn.ps
Refering-URL: http://www.cs.brown.edu/people/lpk/
Root-URL: http://www.cs.brown.edu/
Title: Learning Dynamics: System Identification for Perceptually Challenged Agents  
Author: Kenneth Basye Thomas Dean Leslie Pack Kaelbling 
Note: This work was supported in part by a National Science Foundation Presidential Young Investigator Award IRI-8957601, by the Air Force and the Advanced Research Projects Agency of the Department of Defense under Contract No. F30602-91-C-0041, and by the National Science foundation in conjunction with the Advanced Research Projects Agency of the Department of Defense under Contract No. IRI-8905436. This work was supported in part by a National Science Foundation National Young Investigator Award.  
Address: Worcester, MA 01610  Providence, RI 02912  
Affiliation: Department of Mathematics and Computer Science Clark University,  Department of Computer Science Brown University,  
Abstract: From the perspective of an agent, the input/output behavior of the environment in which it is embedded can be described as a dynamical system. Inputs correspond to the actions executable by the agent in making transitions between states of the environment. Outputs correspond to the perceptual information available to the agent in particular states of the environment. We view dynamical system identification as inference of deterministic finite-state automata from sequences of input/output pairs. The agent can influence the sequence of input/output pairs it is presented by pursuing a strategy for exploring the environment. We identify two sorts of perceptual errors: errors in perceiving the output of a state and errors in perceiving the inputs actually carried out in making a transition from one state to another. We present efficient, high-probability learning algorithms for a number of system identification problems involving such errors. We also present the results of empirical investigations applying these algorithms to learning spatial representations. 
Abstract-found: 1
Intro-found: 1
Reference: [ Angluin, 1978 ] <author> Dana Angluin. </author> <title> On the complexity of minimum inference of regular sets. </title> <journal> Information and Control, </journal> <volume> 39 </volume> <pages> 337-350, </pages> <year> 1978. </year>
Reference-contexts: Gold's algorithm relies on the learner having the ability to reset the automaton to the initial state at any time. The general problem of inferring the smallest automaton consistent with a given set of input/output pairs is NP-complete <ref> [ Angluin, 1978, Gold, 1978 ] </ref> .
Reference: [ Angluin, 1987 ] <author> Dana Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year> <month> 43 </month>
Reference: [ Bachrach, 1992 ] <author> Jonathan R. Bachrach. </author> <title> Connectionist modeling and control of finite state environments. </title> <type> Technical Report 92-6, </type> <institution> University of Massachusetts at Amherst Department of Computer and Information Science, </institution> <year> 1992. </year>
Reference-contexts: Several researchers have approached learning finite-state systems using neural networks. For example, Servan-Schreiber et al. [ Servan-Schreiber et al., 1989 ] used a recurrent network to learn finite-state grammars, and Bachrach <ref> [ Bachrach, 1992 ] </ref> used a neural net to implement one of the the Rivest and Schapire algorithms mentioned above.
Reference: [ Basye et al., 1989a ] <author> Kenneth Basye, Thomas Dean, and Jeffrey Scott Vitter. </author> <title> Coping with uncertainty in map learning. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 663-668. IJCAI, </pages> <year> 1989. </year>
Reference-contexts: Thus, it is possible that all states that are not landmarks have the same label. With regard 5 The work described in this section was carried out jointly with Jeff Vitter and is described in more detail in <ref> [ Basye et al., 1989a ] </ref> . 19 to movement, we assume that the agent takes the intended action with probability &gt; 1 2 , but allow any static distribution over actions that are in error.
Reference: [ Basye et al., 1989b ] <author> Kenneth Basye, Thomas Dean, and Jeffrey Scott Vitter. </author> <title> Coping with uncertainty in map learning. </title> <type> Technical Report CS-89-27, </type> <institution> Brown University Department of Computer Science, </institution> <year> 1989. </year>
Reference-contexts: For instance, the indistinguishable states might be partitioned further into equivalence classes so that one could uniquely designate a state by specifying its equivalence class and some radius from a particular global landmark (e.g., the bookstore just across the street from the Chrysler building). In <ref> [ Basye et al., 1989b ] </ref> , we show how to modify the above approach and try to completely learn the environment by first completely learning local neighborhoods of each landmark. 5.3 Stochastic Actions and Observations In this section, we consider the case in which there is error in both the
Reference: [ Basye, 1992 ] <author> Kenneth J. Basye. </author> <title> A Framework for Map Construction. </title> <type> PhD thesis, </type> <institution> Brown University Department of Computer Science, </institution> <address> Providence, Rhode Island, </address> <year> 1992. </year>
Reference-contexts: For each path, maintain a count of successful traversals. 4. Return all paths from the list whose count from step 3 is above a threshold. 5.2.3 Analysis We now state without proof (the detailed proof can be found in <ref> [ Basye, 1992 ] </ref> ) a series of lemmas leading up to the result that this algorithm is correct and has polynomial sample complexity. <p> In Section 6 we discuss one set of noise models that satisfy this requirment. We can characterize the number of observations required by the algorithm as a function of the separation. The proof is omitted, but can be found elsewhere <ref> [ Kaelbling et al., 1992, Basye, 1992 ] </ref> . It uses Hoeffding's inequality to show that after a large enough number of samples, it is very likely that the observation with the largest sample frequency is also the observation with the largest true frequency. <p> Our simulations were based on the problem faced by mobile robots in learning about their spatial environment. In this section we briefly describe a real system for robotic map building based on the CMFO algorithm from Section 5.3; more detail is available elsewhere <ref> [ Basye, 1992 ] </ref> . 37 = 0:5 = 0:6 = 0:9 = 1:0 values of for algorithm cmfo on the CIT 4 environment with the oriented junction-type similarity partition error model 38 = 0:5 = 0:6 = 0:9 = 1:0 values of for algorithm cmfo on the CIT 4 environment
Reference: [ Dean et al., 1990 ] <author> Thomas Dean, Kenneth Basye, Robert Chekaluk, Seungseok Hyun, Moises Lejter, and Margaret Randazza. </author> <title> Coping with uncertainty in a control system for navigation and exploration. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1010-1015. </pages> <booktitle> American Association for Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: Clearly there are tradeoffs involved in learning dynamical models; exactly what is worth learning will depend on the tasks of the agent. Again, we avoid addressing those tradeoffs in this paper (but see <ref> [ Dean et al., 1990 ] </ref> ) in order to focus on basic issues in how uncertainty in observation affects learning. 2 Modeling Dynamical Systems with Automata We model dynamical systems as deterministic finite-state automata (DFAs).
Reference: [ Dean et al., 1992a ] <author> Thomas Dean, Dana Angluin, Kenneth Basye, Sean Engelson, Leslie Kaelbling, Evangelos Kokkevis, and Oded Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence. American Association for Artificial Intelligence, </booktitle> <year> 1992. </year>
Reference-contexts: That is, it terminates with the automaton in a state and returns a signature for 2 The work described in this section was carried out jointly with Dana Angluin and Sean Engelson, and is described in more detail in <ref> [ Dean et al., 1992a ] </ref> . 11 that state that is probably correct. The procedure is parameterized in such a way that it can be run longer in order to guarantee correctness with a higher probability.
Reference: [ Dean et al., 1992b ] <author> Thomas Dean, Dana Angluin, Kenneth Basye, Sean Engelson, Leslie Kaelbling, Evangelos Kokkevis, and Oded Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <type> Technical Report CS-92-27, </type> <institution> Brown University Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: The detailed algorithm and proof of correctness is in <ref> [ Dean et al., 1992b ] </ref> . Suppose for a moment that Localize always returns the agent to the same state and that the agent can always determine when it is in a state that it has visited before. <p> We now describe the results of experiments with Localize that indicate that this is so. There are similar results for the complete automaton inference algorithm provided in <ref> [ Dean et al., 1992b ] </ref> . 30 i. iii. iv. 31 states in the environment Our result requires environments with distinguishing sequences. We hypothesize that many natural environments, and hallway environments in particular, possess short distinguishing sequences.
Reference: [ Feller, 1970 ] <author> William Feller. </author> <title> An Introduction to Probability Theory and its Applications. </title> <publisher> Wiley, </publisher> <address> New York, third edition, </address> <year> 1970. </year> <note> revised printing. </note>
Reference-contexts: Any matrix that is doubly stochastic has a uniform stationary probability distribution <ref> [ Feller, 1970 ] </ref> ; that is, in the limit each state is visited with probability 1=jQj. The next lemma concerns the rate at which the distribution of state visitations ap proaches the uniform stationary distribution.
Reference: [ Gill, 1961 ] <author> Arthur Gill. </author> <title> State-identification experiments in finite automata. </title> <journal> Information and Computation, </journal> <volume> 4 </volume> <pages> 132-154, </pages> <year> 1961. </year> <month> 44 </month>
Reference-contexts: Both distinguishing and homing sequences may be either preset or adaptive <ref> [ Gill, 1961 ] </ref> . An adaptive sequence is one in which the next action in the sequence is determined by the previous outputs, thus it is really a tree of actions whose branches correspond to possible outputs. <p> The robotic system described earlier was designed with this knowledge in mind. With regard to problems involving spatial exploration, there may be other assumptions that can be made which allow even better results. 41 The solutions to problems involving non-unique outputs generally involve sequences, in particular distingushing sequences. Gill <ref> [ Gill, 1961 ] </ref> provides a way to construct preset distinguishing sequences when they exist, but these may have length exponential in the number of states and the algorithm requires a complete description of the automaton.
Reference: [ Gold, 1972 ] <author> E. Mark Gold. </author> <title> System identification via state characterization. </title> <journal> Automatica, </journal> <volume> 8 </volume> <pages> 621-636, </pages> <year> 1972. </year>
Reference: [ Gold, 1978 ] <author> E. Mark Gold. </author> <title> Complexity of automaton identification from given sets. </title> <journal> Information and Control, </journal> <volume> 37 </volume> <pages> 302-320, </pages> <year> 1978. </year>
Reference-contexts: Gold's algorithm relies on the learner having the ability to reset the automaton to the initial state at any time. The general problem of inferring the smallest automaton consistent with a given set of input/output pairs is NP-complete <ref> [ Angluin, 1978, Gold, 1978 ] </ref> .
Reference: [ Kaelbling et al., 1992 ] <author> Leslie Kaelbling, Kenneth Basye, and Thomas Dean. </author> <title> Learning labelled graphs from noisy data. </title> <booktitle> In Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems. </booktitle> <institution> Center for Systems Science, </institution> <year> 1992. </year>
Reference-contexts: In Section 6 we discuss one set of noise models that satisfy this requirment. We can characterize the number of observations required by the algorithm as a function of the separation. The proof is omitted, but can be found elsewhere <ref> [ Kaelbling et al., 1992, Basye, 1992 ] </ref> . It uses Hoeffding's inequality to show that after a large enough number of samples, it is very likely that the observation with the largest sample frequency is also the observation with the largest true frequency.
Reference: [ Mihail, 1989 ] <author> Milena Mihail. </author> <title> Conductance and convergence of Markov chains: A combinatorial treatment of expanders. </title> <booktitle> In Proceedings of the 31st ACM Symposium on Foundations of Computer Science, </booktitle> <pages> pages 526-531, </pages> <year> 1989. </year>
Reference-contexts: Define the discrepancy, t , to be the L 2 norm of the difference between and x (t). This result is a direct consequence of Mihail's result <ref> [ Mihail, 1989 ] </ref> on the convergence of Markov chains.
Reference: [ Moore, 1956 ] <author> Edward F. Moore. </author> <title> Gedanken-experiments on sequential machines. </title> <booktitle> In Automata Studies, </booktitle> <pages> pages 129-153. </pages> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1956. </year>
Reference-contexts: The class of automata indistinguishable from a given strongly-connected automaton M has a unique (up to isomorphism) reduced member that is also strongly connected <ref> [ Moore, 1956 ] </ref> ; this automaton has the minimum number of states. We consistently assume that the environments we are attempting to learn are reduced, since there would be no experiment we could perform that would tell us otherwise. <p> If multiple sequences are allowed, this construction can build a tree with the start state as the root. For this reason, research has concentrated on finding the smallest automaton (in terms of jQj) that agrees with a given set of data. Moore <ref> [ Moore, 1956 ] </ref> showed that if the input/output pairs are assumed to have come from a reduced, strongly connected automaton, then inference of the smallest consistent automaton yields a result that is isomorphic to the original automaton.
Reference: [ Pitt and Warmuth, 1989 ] <author> Leonard Pitt and Manfred K. Warmuth. </author> <title> The minimum consistent DFA problem cannot be approximated within any polynomial. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theoretical Computing, </booktitle> <pages> pages 421-432, </pages> <year> 1989. </year>
Reference-contexts: The general problem of inferring the smallest automaton consistent with a given set of input/output pairs is NP-complete [ Angluin, 1978, Gold, 1978 ] . Indeed, even finding an automaton polynomially close to the smallest is intractable assuming P 6= NP <ref> [ Pitt and Warmuth, 1989 ] </ref> . 9 Angluin [ 1987 ] , building on the work of Gold, provides a polynomial-time algorithm for inferring the smallest automaton given the ability to reset the automaton and a source of counterexamples.
Reference: [ Rivest and Schapire, 1987 ] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Diversity-based inference of finite automata. </title> <booktitle> In Proceedings of the 29th ACM Symposium on Foundations of Computer Science, </booktitle> <pages> pages 78-87, </pages> <year> 1987. </year>
Reference-contexts: Schapire [ 1989 ] show how to make use of a homing sequence as a substitute for a reset and how to dispense with both the reset and the source of counterexamples in the case in which a distinguishing sequence is either provided or can be learned in polynomial time <ref> [ Rivest and Schapire, 1987, Schapire, 1991 ] </ref> . Several researchers have approached learning finite-state systems using neural networks.
Reference: [ Rivest and Schapire, 1989 ] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theoretical Computing, </booktitle> <pages> pages 411-420, </pages> <year> 1989. </year>
Reference: [ Schapire, 1991 ] <author> Robert E. Schapire. </author> <title> The design and analysis of efficient learning algorithms. </title> <type> Technical Report MIT/LCS/TR-493, </type> <institution> MIT Laboratory for Computer Science, </institution> <year> 1991. </year>
Reference-contexts: Schapire [ 1989 ] show how to make use of a homing sequence as a substitute for a reset and how to dispense with both the reset and the source of counterexamples in the case in which a distinguishing sequence is either provided or can be learned in polynomial time <ref> [ Rivest and Schapire, 1987, Schapire, 1991 ] </ref> . Several researchers have approached learning finite-state systems using neural networks.
Reference: [ Servan-Schreiber et al., 1989 ] <author> David Servan-Schreiber, Axel Cleeremans, and James L. McClelland. </author> <title> Learning sequential structure in simple recurrent networks. </title> <editor> In David Touret-zky, editor, </editor> <booktitle> Advances in Neural Information Processing, </booktitle> <volume> volume 1. </volume> <publisher> Morgan Kaufman, </publisher> <year> 1989. </year>
Reference-contexts: Several researchers have approached learning finite-state systems using neural networks. For example, Servan-Schreiber et al. <ref> [ Servan-Schreiber et al., 1989 ] </ref> used a recurrent network to learn finite-state grammars, and Bachrach [ Bachrach, 1992 ] used a neural net to implement one of the the Rivest and Schapire algorithms mentioned above.
Reference: [ Sutton, 1990 ] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and re-acting based on approximating dynamic programming. </title> <booktitle> In Proceedings 7th International Conference on Machine Learning, </booktitle> <year> 1990. </year>
Reference-contexts: There are environments, however, in which having some sort of a model can help enormously. Perhaps, the clearest example of the utility of a model is in learning maps of large-scale 2 space to support path planning. A dynamical model can also speed learning plans <ref> [ Sutton, 1990 ] </ref> by allowing an agent to simulate its actions and the environment's reactions. Clearly there are tradeoffs involved in learning dynamical models; exactly what is worth learning will depend on the tasks of the agent.
Reference: [ Yannakakis and Lee, 1991 ] <author> Mihalis Yannakakis and David Lee. </author> <title> Testing finite state machines. </title> <booktitle> In Proceedings of the 23rd ACM Symposium on Theoretical Computing, </booktitle> <pages> pages 476-485. </pages> <publisher> ACM Press, </publisher> <year> 1991. </year> <month> 46 </month>
Reference-contexts: In particular, Dean et al. [ 1992b ] show that learning such a sequence is not possible in polynomial time for environments as general as those we are considering here. 18 Further, the results of Yannakakis and Lee <ref> [ Yannakakis and Lee, 1991 ] </ref> indicate that even computing preset distinguishing sequences for general environments is hard, although they provide an efficient way to compute adaptive distinguishing sequences. However, it is clear that an adaptive sequence will not work in place of a preset one in the Localize procedure. <p> Gill [ Gill, 1961 ] provides a way to construct preset distinguishing sequences when they exist, but these may have length exponential in the number of states and the algorithm requires a complete description of the automaton. Yannakakis and Lee <ref> [ Yannakakis and Lee, 1991 ] </ref> show that the problem of determining whether an automaton has a preset distinguishing sequence is PSPACE-complete, but also give an efficient, constructive algorithm for determining whether an automaton has an adaptive distinguishing sequence.
References-found: 23

