URL: ftp://ftp.cs.uoregon.edu/pub/lo/larcs.ps.Z
Refering-URL: http://www.cs.uoregon.edu/research/DistributedComputing/archive.html
Root-URL: http://www.cs.uoregon.edu
Email: email: lo@cs.uoregon.edu  
Title: LaRCS: A Language for Describing Parallel Computations for the Purpose of Mapping  
Author: Virginia M. Lo Sanjay Rajopadhye Moataz A. Mohamed Samik Gupta, Bill Nitzberg, Jan Arne Telle, Xiaoxiong Zhong 
Note: This research was sponsored by Oregon Advanced Computing Institute (OACIS), a consortium of academic, industrial, and government agencies in the state of Oregon. Partially supported by NSF grant CCR-8808532 Partially supported by NSF grant MIP-8802454 Moataz Mohamed is currently a member  
Address: 97403-1202  
Affiliation: Dept. of Computer and Information Science University of Oregon Eugene, Oregon  of Computer Systems Lab, Stanford University  
Abstract: LaRCS is a graph description language which enables the programmer of parallel algorithms to specify information about the static and temporal communication behavior of parallel algorithms. The information contained in a LaRCS program is used for the mapping problem: to assign tasks in the parallel computation to processors, and to route inter-task messages along the links of the interconnection network. Many practical algorithms exhibit regular communication patterns and LaRCS provides a mechanism for describing this regularity in a compact, parameterized manner. Static communication topology is expressed in LaRCS code through node labels and simple communication functions on the nodes. Temporal communication behavior is represented by a notation we have developed called phase expressions. This paper introduces the LaRCS language and compares LaRCS with several existing graph description languages. We introduce a new graph theoretic model of parallel computation based on LaRCS called the Temporal Communication Graph; and we show how information provided by LaRCS plays an important role in map ping. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> References </institution>
Reference: [AK89] <author> S. B. Akers and B. Krishnamurthy. </author> <title> A group-theoretic model for symmetric interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38(4):555-566, </volume> <month> April </month> <year> 1989. </year>
Reference-contexts: Figure 16 illustrates our group theoretic contraction for the 8-node perfect broadcast parallel algorithm. Many interesting interconnection networks, e.g. hypercubes, toruses, cube-connected-cycles, are themselves based on Cayley graphs <ref> [AK89] </ref>.
Reference: [BC87] <author> D. A. Bailey and J. E. Cuny. </author> <title> Graph grammar based specification of interconnection structures for massively parallel computation. </title> <booktitle> In Proceedings of the Third International Workshop on Graph Grammars, </booktitle> <pages> pages 73-85, </pages> <year> 1987. </year>
Reference-contexts: However, It is not possible to specify which edges or nodes of that graph are being used at certain points in the computation. 4.5 Aggregate Rewriting Graph Grammars An Aggregate Rewriting (AR) Grammar is a special type of graph grammars developed at the University of Massachusetts <ref> [BC87, BC89] </ref>. AR grammars are used to help the 24 25 programmer visually specify process interconnection structures in a parallel programming environment. This is achieved by a graphical editor called ParaGraph.
Reference: [BC89] <author> D. A. Bailey and J. E. Cuny. </author> <title> Visual extensions to parallel programming languages. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 17-36, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: However, It is not possible to specify which edges or nodes of that graph are being used at certain points in the computation. 4.5 Aggregate Rewriting Graph Grammars An Aggregate Rewriting (AR) Grammar is a special type of graph grammars developed at the University of Massachusetts <ref> [BC87, BC89] </ref>. AR grammars are used to help the 24 25 programmer visually specify process interconnection structures in a parallel programming environment. This is achieved by a graphical editor called ParaGraph. <p> The language of an AR grammar represents a family of graphs. Recently, AR grammars have been used to describe some temporal computation patterns <ref> [BC89] </ref> such as pipeline patterns in a systolic matrix multiplication. AR grammars have been used to describe several well-known interconnection structures including hypercubes, complete binary trees, back trees and butterfly structures. Fig. 15 shows an AR grammar which describes binary hypercube structures. * Expressive power. <p> These functions may be fairly complicated. Moreover, if the production rules are not applied in a correct order, an AR grammar may generate graphs which are not wanted. * Dynamically evolving graphs. In <ref> [BC89] </ref>, a facility called a script is used to denote a permissible sequence of transformations. The specification of scripts is an additional overhead for the user. * Hard to instantiate. The parameters of graphs in an AR grammar are implicit.
Reference: [Ber83] <author> F. Berman. </author> <title> Edge grammars and parallel computataion. </title> <booktitle> In Proceedings of the 1983 Allerton Conference, </booktitle> <address> Urbana, Il, </address> ?? <year> 1983. </year>
Reference-contexts: LaRCS allows the user to explicitly define the unit communication volume associated with a communication edge. In the Prep-P sys tem, communication volume is estimated by the compiler from the program code. 4.3 Edge Grammars Edge grammars were introduced by Berman <ref> [Ber83] </ref>, [BS84], [BS87] as a means for describing graph families and as an abstraction for use in the mapping problem. Edge grammars define graph families by generating all the edges in the graph using conventional formal languages mechanisms.
Reference: [Bok87] <author> S. H. Bokhari. </author> <title> Assignment Problems in Parallel and Distributed Computing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference-contexts: The TCG model is suitable for mapping and scheduling techniques from both domains. Thus algorithms for static task assignment such as <ref> [Bok87] </ref> [BS89] [Lo88], [SER90] and scheduling algorithms for precedence-constrained graphs such as [Pol88] [Bro86] [ERL90] can be applied to the TCG model. The TCG augments these two models with the ability to explicitly capture regularity, allowing the development of specialized mapping and scheduling algorithms to exploit this regularity.
Reference: [Bro86] <author> J.C. Browne. </author> <title> Framework for fomulation and analysis of parallel computation structures. </title> <journal> Parallel Computing, </journal> <volume> 3 </volume> <pages> 1-9, </pages> <year> 1986. </year>
Reference-contexts: The TCG model is suitable for mapping and scheduling techniques from both domains. Thus algorithms for static task assignment such as [Bok87] [BS89] [Lo88], [SER90] and scheduling algorithms for precedence-constrained graphs such as [Pol88] <ref> [Bro86] </ref> [ERL90] can be applied to the TCG model. The TCG augments these two models with the ability to explicitly capture regularity, allowing the development of specialized mapping and scheduling algorithms to exploit this regularity. Below is an intuitive definition of the TCG.
Reference: [BS84] <author> F. Berman and G. Shannon. </author> <title> Edge grammars: Decidability results and formal language issues. </title> <booktitle> In Proceedings of the 1984 Allerton Conference, </booktitle> <address> Urbana, Il, </address> ?? <year> 1984. </year>
Reference-contexts: LaRCS allows the user to explicitly define the unit communication volume associated with a communication edge. In the Prep-P sys tem, communication volume is estimated by the compiler from the program code. 4.3 Edge Grammars Edge grammars were introduced by Berman [Ber83], <ref> [BS84] </ref>, [BS87] as a means for describing graph families and as an abstraction for use in the mapping problem. Edge grammars define graph families by generating all the edges in the graph using conventional formal languages mechanisms.
Reference: [BS87] <author> F. Berman and L. Snyder. </author> <title> On mapping parallel algorithms into parallel architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 4(5) </volume> <pages> 439-458, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: LaRCS allows the user to explicitly define the unit communication volume associated with a communication edge. In the Prep-P sys tem, communication volume is estimated by the compiler from the program code. 4.3 Edge Grammars Edge grammars were introduced by Berman [Ber83], [BS84], <ref> [BS87] </ref> as a means for describing graph families and as an abstraction for use in the mapping problem. Edge grammars define graph families by generating all the edges in the graph using conventional formal languages mechanisms.
Reference: [BS89] <author> F. Berman and B. Stramm. Prep-p: </author> <title> Evolution and overview. </title> <type> Technical Report CS89-158, </type> <institution> University of California at San Diego, </institution> <year> 1989. </year> <month> 38 </month>
Reference-contexts: The TCG model is suitable for mapping and scheduling techniques from both domains. Thus algorithms for static task assignment such as [Bok87] <ref> [BS89] </ref> [Lo88], [SER90] and scheduling algorithms for precedence-constrained graphs such as [Pol88] [Bro86] [ERL90] can be applied to the TCG model. The TCG augments these two models with the ability to explicitly capture regularity, allowing the development of specialized mapping and scheduling algorithms to exploit this regularity. <p> However, they have some disadvantages, some of which have been reported by the authors themselves <ref> [BS89] </ref>: * Inability to represent directed graphs. As currently defined, the grammar does not provide any means for distinguishing between the source (sender) and the destination (receiver) of each edge. * No explicit specification of size of graphs.
Reference: [ea90] <author> S. Borkar et. al. </author> <title> Supporting systolic and memory communication in iwarp. </title> <booktitle> In Proceedings International Symposium on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: Examples of such systems include Intel's iWarp Computer <ref> [ea90] </ref>, the INMOS Transputer, Snyder's ChiP architecture, and some systems with store and forward message routing. The comphase declaration identifies logically synchronous communication, i.e. message passing that can occur simultaneously at runtime.
Reference: [ERL90] <author> H. El-Rewini and T.G. Lewis. </author> <title> Scheduling parallel program tasks onto arbitrary target machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 138-153, </pages> <year> 1990. </year>
Reference-contexts: The TCG model is suitable for mapping and scheduling techniques from both domains. Thus algorithms for static task assignment such as [Bok87] [BS89] [Lo88], [SER90] and scheduling algorithms for precedence-constrained graphs such as [Pol88] [Bro86] <ref> [ERL90] </ref> can be applied to the TCG model. The TCG augments these two models with the ability to explicitly capture regularity, allowing the development of specialized mapping and scheduling algorithms to exploit this regularity. Below is an intuitive definition of the TCG.
Reference: [KK88] <author> Simon M. Kaplan and Gail E. Kaiser. Garp: </author> <title> Graph abstractions for concurrent programming. </title> <editor> In H. Ganzinger, editor, </editor> <booktitle> European Symposium on Programming, volume 300 of Lecture Notes in Computer Science, </booktitle> <pages> pages 191-205, </pages> <address> Heidelberg, March 1988. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: A form of graph grammars called Concurrent Abstraction Grammars (CAGs) are used by the GARP system to specify parallel computations. GARP (Graph Abstractions for Concurrent Programming) <ref> [KK88] </ref> is a model of parallel computation where tasks are spawned dynamically at runtime, and where the tasks that are spawned have a "regular" interconnection structure. The regularity of the subgraph of tasks that are spawned is described by a notation based on graph grammars. <p> The embedding rule specifies how the newly generated collection of tasks fits in with the rest of the task-graph. GARP provides the programmer with a graphical representation of the computation which is useful for visualization. An example of a quicksort using CAGs in GARP <ref> [KK88] </ref> is shown in Fig. 14. The corresponding CAG specifies not just one graph, but a set of graphs. Which one of these graphs corresponds to the task graph of a particular computation becomes known only at run time when the rewrite rules are applied and the graph is generated.
Reference: [Lo88] <author> V. M. Lo. </author> <title> Heuristic algorithms for task assignment in distributed systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(11) </volume> <pages> 1384-1397, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: The TCG model is suitable for mapping and scheduling techniques from both domains. Thus algorithms for static task assignment such as [Bok87] [BS89] <ref> [Lo88] </ref>, [SER90] and scheduling algorithms for precedence-constrained graphs such as [Pol88] [Bro86] [ERL90] can be applied to the TCG model. The TCG augments these two models with the ability to explicitly capture regularity, allowing the development of specialized mapping and scheduling algorithms to exploit this regularity.
Reference: [Lo91] <author> V. M. Lo. </author> <title> Temporal communication graphs: A new graph theoretic model for mapping and scheduling in distributed memory systems. </title> <booktitle> In Proceedings 6th Distributed Memory Computing Conference, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Below is an intuitive definition of the TCG. A formal definition of the TCG and discussion of its utility in parallel programming environments is given in <ref> [Lo91] </ref>. The TCG abstraction defines a parallel computation as a collection of processes involved in two activities: computation and communication. A compute phase is a set of processes involved in logically synchronous computation.
Reference: [LRG + 90a] <author> V. M. Lo, S. Rajopadhye, S. Gupta, D. Keldsen, M. A. Mohamed, and J. Telle. </author> <title> Mapping divide-and-conquer algorithms ato parallel architectures. </title> <booktitle> In Proceedings IEEE 1990 International Conference on Parallel Processing, </booktitle> <pages> pages III:128-135, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: In this example, we declare the generic comtype ring and then declare two instances: master_ring and slave_ring. Divide-and-conquer binomial tree in LaRCS The binomial tree has been shown to be an excellent task graph for efficient computation of parallel divide and conquer applications [Vui87] <ref> [LRG + 90a] </ref>. The binomial tree is defined recursively as follows: * The binomial tree B 0 consists of a single node. * The binomial tree B n is formed by connecting the root nodes of two identical bino mial trees B n1 .
Reference: [LRG + 90b] <author> V. M. Lo, S. Rajopadhye, S. Gupta, D. Keldsen, M. A. Mohamed, and J. Telle. OREGAMI: </author> <title> Software tools for mapping parallel algorithms to parallel architectures. </title> <booktitle> In Proceedings IEEE 1990 International Conference on Parallel Processing, </booktitle> <pages> pages II:88-92, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: An updated version of OREGAMI written in C for the Open Windows interface on Sun workstations should be available for distribution Fall 1991. Additional information about MAPPER and METRICS can be found in <ref> [LRG + 90b] </ref>. In this paper, we discuss the features of LaRCS and illustrate its ability to describe a wide range of parallel computations. In Sec. 2 we describe the key language features found in LaRCS: nodetypes, comtypes, comphases, and phase expressions. <p> discuss the general contribution of LaRCS to the mapping problem through its ability to represent families of regular computations in a space efficient way. 5.1 Contraction Based on LaRCS Comphases We have developed an algorithm for contracting certain highly symmetrical task graphs in time quadratic in the number of tasks <ref> [LRG + 90b] </ref>. The algorithm is based in group theory and will yield a perfectly load balanced and symmetrical contraction whenever the task graph is a Cayley graph. <p> Examples of such systems include Intel's iWarp Computer [ea90], the INMOS Transputer, Snyder's ChiP architecture, and some systems with store and forward message routing. The comphase declaration identifies logically synchronous communication, i.e. message passing that can occur simultaneously at runtime. The OREGAMI routing algorithm <ref> [LRG + 90b] </ref> attempts to find a disjoint set of routes to service the set of messages belonging to a single comphase. This is accomplished by posing the routing problem as a bi-partite matching problem. <p> In addition, we presume implicit barrier synchronization between phases in the computation of completion time. Details of this computation are beyond the scope of this paper, but are discussed in <ref> [LRG + 90b] </ref> and [SB90]. 5.5 LaRCS provides an Efficient Representation of Task Graphs The primary purpose of LaRCS is to describe the computation graph for the purpose of mapping. LaRCS' orientation towards the representation of regular parallel computations 34 enables it to efficiently describe families of computation graphs.
Reference: [MKS89] <author> J. Magee, J. Kramer, and M. Sloman. </author> <title> Constructing distributed systems in conic. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-15(6):663-675, </volume> <month> June </month> <year> 1989. </year>
Reference-contexts: In this section, we compare LaRCS to Conic, GDL, edge grammars, GARP and concurrent abstract grammars, ParaGraph and Attribute Rewrite Graph Grammars. Each of the comparisons highlights one or two specific features of LaRCS. 4.1 Conic The Conic distributed programming environment <ref> [MKS89] </ref>, developed at the Imperial College in London, provides tools for the hierarchical construction of distributed applications. The Conic programming language is a version of Pascal with extensions to support modularity and message-passing.
Reference: [Pol88] <author> C. D. Polychronopoulos. </author> <title> Parallel Programming and Compilers. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year> <month> 39 </month>
Reference-contexts: Thus, the TCG can be seen as a hybrid of the two predominant models of parallel computation: the static task graph model of Stone [Sto77], and the precedence-constrained (DAG) model <ref> [Pol88] </ref> used in multiprocessor scheduling and in the parallelization of sequential code. Task assignment and scheduling research utilizing these two models has more or less followed disjoint paths over the past two decades, in that techniques and algorithms developed for one model have not been applicable to the other. <p> The TCG model is suitable for mapping and scheduling techniques from both domains. Thus algorithms for static task assignment such as [Bok87] [BS89] [Lo88], [SER90] and scheduling algorithms for precedence-constrained graphs such as <ref> [Pol88] </ref> [Bro86] [ERL90] can be applied to the TCG model. The TCG augments these two models with the ability to explicitly capture regularity, allowing the development of specialized mapping and scheduling algorithms to exploit this regularity. Below is an intuitive definition of the TCG.
Reference: [SB90] <author> B. Stramm and F. Berman. </author> <title> How good is good? Technical Report CS90--169, </title> <institution> University of California at San Diego, Dept. of Computer Science and Engineering, </institution> <year> 1990. </year>
Reference-contexts: LaRCS is designed to be used with a variety of parallel pro gramming languages. * Implementation. The Conic environment has been in use for a number of years for programming of distributed system applications. 4.2 Prep-P and GDL Prep-P is an automatic mapping system developed by Berman <ref> [SB90] </ref> at UCSD which is quite similar to our OREGAMI system. Prep-P differs from OREGAMI in that the target architecture is a reconfigurable network, currently the CHiP network [Sny82]. The Prep-P system includes a parallel programming language `XX' and a graph description language GDL. <p> In addition, we presume implicit barrier synchronization between phases in the computation of completion time. Details of this computation are beyond the scope of this paper, but are discussed in [LRG + 90b] and <ref> [SB90] </ref>. 5.5 LaRCS provides an Efficient Representation of Task Graphs The primary purpose of LaRCS is to describe the computation graph for the purpose of mapping. LaRCS' orientation towards the representation of regular parallel computations 34 enables it to efficiently describe families of computation graphs.
Reference: [Sei85] <author> C. L. Seitz. </author> <title> The cosmic cube. </title> <journal> Communications of the ACM, </journal> <volume> 28(1) </volume> <pages> 22-33, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: The model of parallel computation underlying LaRCS is described more fully in Sec. 3. We now describe the components of a LaRCS program, using a parallel algorithm for the n-body problem as an example. This algorithm was developed for the Caltech Cosmic Cube <ref> [Sei85] </ref>. Fig. 2 gives a description of the the n-body problem and Fig. 3 gives the LaRCS code. 1. Name of algorithm and parameters. The parameters specify the size of this instance of the parallel algorithm. <p> The LaRCS comtype-declaration's volume field can be initialized to the number of bytes sent in a single message of that comtype. For example, based on the code for the Caltech n-body algorithm given in <ref> [Sei85] </ref>, the volume for the comtype ring is 108 bytes. This value would be specified by the user in the LaRCS code.
Reference: [SER90] <author> P. Sadayappan, F. Ercal, and J. Ramanujam. </author> <title> Clustering partitioning approaches to mapping parallel programs onto a hypercube. </title> <journal> Parallel Computing, </journal> <volume> 13 </volume> <pages> 1-16, </pages> <year> 1990. </year>
Reference-contexts: The TCG model is suitable for mapping and scheduling techniques from both domains. Thus algorithms for static task assignment such as [Bok87] [BS89] [Lo88], <ref> [SER90] </ref> and scheduling algorithms for precedence-constrained graphs such as [Pol88] [Bro86] [ERL90] can be applied to the TCG model. The TCG augments these two models with the ability to explicitly capture regularity, allowing the development of specialized mapping and scheduling algorithms to exploit this regularity.
Reference: [Sny82] <author> L. Snyder. </author> <title> Introduction to the configurable, highly parallel computer. </title> <journal> Computer, </journal> <volume> 15(1) </volume> <pages> 47-56, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: Prep-P differs from OREGAMI in that the target architecture is a reconfigurable network, currently the CHiP network <ref> [Sny82] </ref>. The Prep-P system includes a parallel programming language `XX' and a graph description language GDL. GDL is closely tied to `XX' in the sense that a one-to-one correspondence exists between the procedures and ports declared in the `XX' program and those used to describe the task graph in GDL.
Reference: [Sto77] <author> H. S. Stone. </author> <title> Multiprocessor scheduling with the aid of network flow algorithms. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-3(1):85-93, </volume> <month> January </month> <year> 1977. </year>
Reference-contexts: In our model the entire graph is known statically. We describe temporal behavior by identifying collections of edges that are "active" simultaneously, and by the pattern of this activity. The conventional static task graph used in the research on mapping algorithms to processors <ref> [Sto77] </ref> can be defined as the 4 union of all instantiated edges in a LaRCS program. The model of parallel computation underlying LaRCS is described more fully in Sec. 3. We now describe the components of a LaRCS program, using a parallel algorithm for the n-body problem as an example. <p> Both the TCG and its forerunner, the static task graph model of Stone <ref> [Sto77] </ref>, were designed for systems in which the programmer designs his or her program as a set of static and persistent parallel processes which communicate through explicit message-passing. <p> In terms of LaRCS, the nodetype, comtype, and comphase declarations describe the static components of the TCG, while the phase expression defines the temporal behavior. Thus, the TCG can be seen as a hybrid of the two predominant models of parallel computation: the static task graph model of Stone <ref> [Sto77] </ref>, and the precedence-constrained (DAG) model [Pol88] used in multiprocessor scheduling and in the parallelization of sequential code.

References-found: 24

