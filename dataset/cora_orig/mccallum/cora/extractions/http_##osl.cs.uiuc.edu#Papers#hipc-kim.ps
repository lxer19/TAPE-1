URL: http://osl.cs.uiuc.edu/Papers/hipc-kim.ps
Refering-URL: http://osl.cs.uiuc.edu/Papers/Parallel.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: f wooyoung j aghag@cs.uiuc.edu  Email: rajendrap@VNET.IBM.COM  
Title: Efficient Compilation of Concurrent Call/Return Communication in Actor-Based Programming Languages  
Author: W. Kim and G. A. Agha R. B. Panwar 
Web: URL: http://www-osl.cs.uiuc.edu  
Address: Urbana, IL 61801, USA  San Jose, CA 95141, USA  
Affiliation: Open Systems Laboratory University of Illinois at Urbana-Champaign  Application Dev. Technology Institute IBM Santa Teresa Labs  
Abstract: Concurrent call/return communication (CCRC) allows programmers to conveniently express a communication pattern where a sender invokes a remote operation and uses the result to continue its computation. The blocking semantics requires context switching for efficient utilization of computation resource. We present a compilation technique which allows programmers to use CCRC with the cost of non-blocking asynchronous communication plus minimum context switch cost. The technique transforms CCRCs into non-blocking asynchronous sends and encapsulates continuations into separate objects. A data flow analysis is used to guarantee that only necessary context is cached in continuation objects. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: It also describes an implementation technique of CCRC adopted in other systems and motivates the development of our compilation technique. 2.1 The Actor model Actors <ref> [1, 2] </ref> are fine-grained active objects which encapsulate a thread as well as data and procedures. Computation is expressed in terms of actors communicating each other through asynchronous message passing. Each actor is assigned a unique mail address which is used for other actors to send it messages.
Reference: [2] <author> G. Agha. </author> <title> Concurrent Object-Oriented Programming. </title> <journal> Communications of the ACM, </journal> <volume> 33(9) </volume> <pages> 125-141, </pages> <month> Septem-ber </month> <year> 1990. </year>
Reference-contexts: It also describes an implementation technique of CCRC adopted in other systems and motivates the development of our compilation technique. 2.1 The Actor model Actors <ref> [1, 2] </ref> are fine-grained active objects which encapsulate a thread as well as data and procedures. Computation is expressed in terms of actors communicating each other through asynchronous message passing. Each actor is assigned a unique mail address which is used for other actors to send it messages.
Reference: [3] <author> G. Agha, S. Frtlund, W. Kim, R. Panwar, A. Pat-terson, and D. Sturman. </author> <title> Abstraction and Modularity Mechanisms for Concurrent Computing. </title> <journal> IEEE Parallel and Distributed Technology: Systems and Applications, </journal> <volume> 1(2) </volume> <pages> 3-14, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: For a set of mutually data-independent request message sendings, it transforms each request into a non-blocking asynchronous send and separates out their join continuation <ref> [3, 11] </ref> into a join continuation closure (JCC) [10]. Join continuation is a continuation which is dependent simultaneously on multiple replies; it is executed as all the replies are available. JCC encapsulates a join continuation and caches the context for its execution.
Reference: [4] <author> G. Agha, W. Kim, and R. Panwar. </author> <title> Actor Languages for Specification of Parallel Computations. </title> <editor> In G. E. Blelloch, K. Mani Chandy, and S. Jagannathan, editors, </editor> <booktitle> DIMACS. Series in Discrete Mathematics and Theoretical Computer Science. </booktitle> <volume> vol 18. </volume> <booktitle> Specification of Parallel Algorithms, </booktitle> <pages> pages 239-258. </pages> <publisher> American Mathematical Society, </publisher> <year> 1994. </year>
Reference-contexts: Though simple and primitive, actor operators form a powerful set upon which to build a wide range of higher-level abstractions. 2.2 The language We incorporated CCRC in a high-level actor-based language Thal <ref> [4, 11] </ref> which also provides other high-level communication abstractions, such as local synchronization constraints, delegation, and multicast. Thal supports CCRC using two primitives, request and reply. request invokes a method on a remote actor and reply sends a result back to the requester.
Reference: [5] <author> R. S. Nikhil Arvind and K. K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 598-632, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: A technique similar to our base algorithm has been used in compilation of explicit message passing programs [8]. Data structures similar to join continuation closure (JCC) have been used in fine-grained functional languages <ref> [5, 7] </ref> and an explicit continuation passing style language [6]. CCRC's blocking yet concurrent semantics requires context switching for efficient utilization of computation resources. Our compilation technique minimizes the number of context switches necessary for implementing CCRC by transforming request sends into non-blocking asynchronous sends and separating join continuations into JCCs.
Reference: [6] <author> R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, A. Shaw, and Y. Zhou. Cilk: </author> <title> An Efficient Multithreaded Runtime System. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming PPOPP, </booktitle> <year> 1994. </year>
Reference-contexts: Otherwise, the computation blocks on the future until its value is available. Although simple and easy to implement, futures have a potential source of performance bottleneck: concurrent execution using futures requires context switching, an expensive operation without special hardware support (e.g., 52 sec in the TMC CM-5 <ref> [6] </ref>). Suppose an actor creates N children in response to a comp message in Figure 1. A future-based implementation will create N futures and assign them to the identifiers replies [i] for i ranging from 1 to N . <p> The compiler optimizes away all actor creations since Fibonacci actors are purely functional. Figure 5 has two sets of performance numbers, one without and the other with dynamic load balancing [10]. As a point of comparison, executing the Fibonacci of 33 using the Cilk system <ref> [6] </ref> takes 73.16 seconds and an optimized C version completes in 8.49 seconds on a single node of a TMC CM-5. The second example is an N-Queen problem. <p> A technique similar to our base algorithm has been used in compilation of explicit message passing programs [8]. Data structures similar to join continuation closure (JCC) have been used in fine-grained functional languages [5, 7] and an explicit continuation passing style language <ref> [6] </ref>. CCRC's blocking yet concurrent semantics requires context switching for efficient utilization of computation resources. Our compilation technique minimizes the number of context switches necessary for implementing CCRC by transforming request sends into non-blocking asynchronous sends and separating join continuations into JCCs.
Reference: [7] <author> D.E. Culler, A. Sah, K. E. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Threaded Abstract Machine. </title> <booktitle> In Proceedings of ASP-LOS, </booktitle> <pages> pages 166-175, </pages> <year> 1991. </year>
Reference-contexts: A technique similar to our base algorithm has been used in compilation of explicit message passing programs [8]. Data structures similar to join continuation closure (JCC) have been used in fine-grained functional languages <ref> [5, 7] </ref> and an explicit continuation passing style language [6]. CCRC's blocking yet concurrent semantics requires context switching for efficient utilization of computation resources. Our compilation technique minimizes the number of context switches necessary for implementing CCRC by transforming request sends into non-blocking asynchronous sends and separating join continuations into JCCs.
Reference: [8] <author> J. G. Holm, A. Lain, and P. Banerjee. </author> <title> Compilation of Scientific Programs into Multithreaded and Message Driven Computation. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <pages> pages 518-525, </pages> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: A technique similar to our base algorithm has been used in compilation of explicit message passing programs <ref> [8] </ref>. Data structures similar to join continuation closure (JCC) have been used in fine-grained functional languages [5, 7] and an explicit continuation passing style language [6]. CCRC's blocking yet concurrent semantics requires context switching for efficient utilization of computation resources.
Reference: [9] <author> R. H. Halstead Jr. </author> <title> Multilisp: A Language for Concurrent Symbolic Computation. </title> <journal> ACM TOPLAS, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <year> 1985. </year>
Reference-contexts: If it is necessary to enforce a certain processing order on messages, the order must be specified explicitly using separate programming constructs, such as local synchronization constraints [11]. 2.4 Futures A technique used in a number of systems to implement CCRC-like abstractions is futures <ref> [9] </ref>. Futures are a promise for a value a place holder whose value is yet to be defined. A future-based CCRC implementation creates a future for a request send and returns it as the result.
Reference: [10] <author> W. Kim and G. Agha. </author> <title> Efficient Support of Location Transparency in Concurrent Object-Oriented Programming Languages. </title> <booktitle> In Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: For a set of mutually data-independent request message sendings, it transforms each request into a non-blocking asynchronous send and separates out their join continuation [3, 11] into a join continuation closure (JCC) <ref> [10] </ref>. Join continuation is a continuation which is dependent simultaneously on multiple replies; it is executed as all the replies are available. JCC encapsulates a join continuation and caches the context for its execution. <p> Figure 1 shows a Thal version of an N-queen problem [13] which computes the number of the solutions. ("." represents request message send.) The Thal runtime system <ref> [10] </ref> is currently running on the TMC CM-5 [14]. ' $ behv NQueen method comp (col,diag1,diag2,maxcol,depth) | i, c, c1, sols, replies, where | replies = zeros (N); c = ((col|diag1)|diag2); c1 = ((c+1) & ~c); sols = 0; for i = 1, N do if (c1 &gt; maxcol) then break; <p> Finally, some bookkeeping code are generated to keep track of the number of requests that have actually been sent. 4 Performance We present the performance results of three applications which are written using CCRC. All the programs were written in Thal <ref> [10, 12] </ref> and executed on a 32-node partition of a TMC CM-5 [14]. Each node contains a 33 MHz Sparc processor. Table 1 has timings of primitive operations or the runtime system. <p> The compiler optimizes away all actor creations since Fibonacci actors are purely functional. Figure 5 has two sets of performance numbers, one without and the other with dynamic load balancing <ref> [10] </ref>. As a point of comparison, executing the Fibonacci of 33 using the Cilk system [6] takes 73.16 seconds and an optimized C version completes in 8.49 seconds on a single node of a TMC CM-5. The second example is an N-Queen problem.
Reference: [11] <author> W. Kim and G. Agha. THAL: </author> <title> A High-level Actor Language and its Compilation. </title> <note> submitted for publication, </note> <year> 1996. </year>
Reference-contexts: For a set of mutually data-independent request message sendings, it transforms each request into a non-blocking asynchronous send and separates out their join continuation <ref> [3, 11] </ref> into a join continuation closure (JCC) [10]. Join continuation is a continuation which is dependent simultaneously on multiple replies; it is executed as all the replies are available. JCC encapsulates a join continuation and caches the context for its execution. <p> Though simple and primitive, actor operators form a powerful set upon which to build a wide range of higher-level abstractions. 2.2 The language We incorporated CCRC in a high-level actor-based language Thal <ref> [4, 11] </ref> which also provides other high-level communication abstractions, such as local synchronization constraints, delegation, and multicast. Thal supports CCRC using two primitives, request and reply. request invokes a method on a remote actor and reply sends a result back to the requester. <p> Thus, programmers are discouraged to assume any specific message delivery order when using CCRC. If it is necessary to enforce a certain processing order on messages, the order must be specified explicitly using separate programming constructs, such as local synchronization constraints <ref> [11] </ref>. 2.4 Futures A technique used in a number of systems to implement CCRC-like abstractions is futures [9]. Futures are a promise for a value a place holder whose value is yet to be defined.
Reference: [12] <author> R. Panwar, W. Kim, and G. Agha. </author> <title> Parallel Implementations of Irregular Problems using High-level Actor Language. </title> <booktitle> In Proceedings of IPPS '96, </booktitle> <year> 1996. </year>
Reference-contexts: Finally, some bookkeeping code are generated to keep track of the number of requests that have actually been sent. 4 Performance We present the performance results of three applications which are written using CCRC. All the programs were written in Thal <ref> [10, 12] </ref> and executed on a 32-node partition of a TMC CM-5 [14]. Each node contains a 33 MHz Sparc processor. Table 1 has timings of primitive operations or the runtime system.
Reference: [13] <author> K. Taura. </author> <title> Design and Implementation of Concurrent Object-Oriented Programming Languages on Stock Multicomputers. </title> <type> Master's thesis, </type> <institution> The University of Tokyo, </institution> <month> Feburuary </month> <year> 1994. </year>
Reference-contexts: Thal supports CCRC using two primitives, request and reply. request invokes a method on a remote actor and reply sends a result back to the requester. Figure 1 shows a Thal version of an N-queen problem <ref> [13] </ref> which computes the number of the solutions. ("." represents request message send.) The Thal runtime system [10] is currently running on the TMC CM-5 [14]. ' $ behv NQueen method comp (col,diag1,diag2,maxcol,depth) | i, c, c1, sols, replies, where | replies = zeros (N); c = ((col|diag1)|diag2); c1 = ((c+1) <p> In our eval PEs w/o w/ 1 55.5 60.7 4 20.8 24.1 16 7.94 3.84 (a) Times (sec.) (b) Speedup PEs Times 1 186.4 4 58.0 16 15.2 (a) Times (sec.) (b) Speedup uation we used a version given in <ref> [13] </ref> which computes the number of the solutions for a given chess board (Figure 1). Each actor represents a queen placed on a square defined by a column and a row. An actor's ancestors plus the actor define a partial solution up to the column that the actor represents.
Reference: [14] <institution> Thinking Machine Corporation. </institution> <note> Connection Machine CM-5 Technical Summary, revised edition, </note> <month> November </month> <year> 1992. </year>
Reference-contexts: Figure 1 shows a Thal version of an N-queen problem [13] which computes the number of the solutions. ("." represents request message send.) The Thal runtime system [10] is currently running on the TMC CM-5 <ref> [14] </ref>. ' $ behv NQueen method comp (col,diag1,diag2,maxcol,depth) | i, c, c1, sols, replies, where | replies = zeros (N); c = ((col|diag1)|diag2); c1 = ((c+1) & ~c); sols = 0; for i = 1, N do if (c1 &gt; maxcol) then break; end if ((col | c1) == maxcol) then <p> All the programs were written in Thal [10, 12] and executed on a 32-node partition of a TMC CM-5 <ref> [14] </ref>. Each node contains a 33 MHz Sparc processor. Table 1 has timings of primitive operations or the runtime system. Operation Time send-and- local 0.45/0.67 dispatch remote 9.91 reply local 2.76 remote 9.26 continuation creation 2.27 deallocation 0.75 Table 1: Performance of primitive operations of Thal on the CM-5 (sec).
References-found: 14

