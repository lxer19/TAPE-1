URL: http://www.cs.utexas.edu/users/lorenzo/papers/paralex/design.ps
Refering-URL: http://www.cs.utexas.edu/users/lorenzo/publications.html
Root-URL: 
Title: Parallel Computing in Networks of Workstations with Paralex  
Author: Renzo Davoli Luigi Alberto Giachini Ozalp Babao glu Alessandro Amoroso Lorenzo Alvisi Piazza di Porta S. Donato, 
Affiliation: Laboratory for Computer Science University of Bologna  40127 Bologna (Italy)  
Date: October 1992 Revised December 1994  
Pubnum: Technical Report UBLCS-92-4  
Abstract-found: 0
Intro-found: 1
Reference: [1] <editor> J.-L. Gaudiot and L. Bic. </editor> <booktitle> Advanced Topics in Dataflow Computing, </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1991. </year>
Reference-contexts: Section 10 concludes the paper. UBLCS-92-4 2 2 The Paralex Programming Paradigm 2 The Paralex Programming Paradigm The choices made for programming paradigm and notation are fundamental in harnessing parallelism in a particular application domain [19]. The programming paradigm supported by Paralex can be classified as static data flow <ref> [1] </ref>. A Paralex program is composed of nodes and links. Nodes correspond to computations (functions, procedures, programs) and the links indicate flow of (typed) data.
Reference: [2] <author> R. Anand, D. Lea and D.W. Forslund. </author> <title> Using nigen++. </title> <type> Technical Report, </type> <institution> School of Computer and Information Science, Syracuse University, </institution> <month> January </month> <year> 1991. </year> <note> UBLCS-92-4 20 REFERENCES </note>
Reference-contexts: The basic programming abstraction in TOPSYS consists of tasks, mailboxes and semaphores realized through library routines. The emphasis of the system is a collection of graphical tools for performance monitoring and debugging. The idea of broadcast-based parallel programming is explored in the nigen++ system <ref> [2] </ref>. Using ISIS as the broadcast mechanism, nigen++ supports a programming style not unlike the Connection Machine a single master process distributes work to a collection of slave processes. This paradigm has been argued by Steele to simplify reasoning about asynchronous parallel computations without reducing their flexibility [40].
Reference: [3] <author> J. Asplin. </author> <title> Distributed Parallel Volume Rendering of Atmospheric Model Output. </title> <booktitle> In Proceedings, </booktitle> <volume> Visualisering '93, </volume> <pages> pages 66-76. </pages> <institution> SINTEF Industriell matematikk, Norges Tekniske Hxgskole, University of Link oping, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: As such, the visualization process may itself benefit from parallelization. As an instance of this problem domain, we have used Paralex to program a volume rendering application for visualization of atmospheric model output. The original application, called Stor-mView <ref> [3] </ref>, was built at the University of Troms in Norway as part of the StormCast distributed system [31] and has been used by meteorologists to visualize the scalar fields generated by the atmospheric model LAM50S developed by the Norwegian Meteorological Institute.
Reference: [4] <author> J. Asplin. Distribuert Parallell Volumvisualisering av Skalarfelt fra en Atmosffremodell. M.Sc. </author> <type> Thesis, </type> <institution> Department of Computer Science, University of Troms, Norway, </institution> <month> March </month> <year> 1994. </year>
Reference: [5] <author> O. Babao glu, L. Alvisi, A. Amoroso and R. Davoli. </author> <title> Mapping Parallel Computations onto Distributed Systems in Paralex. </title> <booktitle> In Proc. IEEE CompEuro '91 Conference, </booktitle> <address> Bologna, Italy, </address> <month> May </month> <year> 1991, </year> <pages> pp. 123-130. </pages>
Reference-contexts: As with the compiler, the loader can be invoked either manually as a command or through the graphical interface. During the loading phase, heuristics are used to solve the mapping problem where each node of the Paralex program is associated with a host of the distributed system <ref> [5] </ref>. <p> Since an optimal solution to this problem is computationally intractable, Paralex bases its mapping decisions on simple heuristics described in <ref> [5] </ref>. The units of our mapping decision are chains defined as sequences of nodes that have to be executed sequentially due to data dependence constraints. The initial mapping decisions, as well as modifications during execution, try to keep all nodes of a chain mapped to the same host.
Reference: [6] <author> R.G. Babb II. </author> <title> Parallel Processing with Large-Grain Data Flow Techniques. </title> <booktitle> IEEE Computer, </booktitle> <month> July </month> <year> 1984, </year> <pages> pp. 55-61. </pages>
Reference-contexts: Unlike classical data flow, nodes of a Paralex program carry out significant computations. This so-called large-grain data flow model <ref> [6] </ref> is motivated by the high-latency, low-bandwidth network that is typically available for communication in distributed systems. Only by limiting the communication-to-computation ratio to reasonable levels can we expect acceptable performance from parallel applications in such systems. <p> Some notable systems where RPC has been employed to permit mixed-language programming and support for heterogeneous architectures include Mercury [36], MLP [30] and HRPC [12]. 8.3 Graphical Programming Examples of systems that use graphical notations to express parallel computations include Fel [32], Poker [38], CODE [17], Alex [33], LGDF <ref> [6, 7] </ref> and apE [26]. None of these systems addresses fault tolerance nor provides a programming environment in the sense of Paralex. Of these, perhaps CODE comes closest to Paralex in design goals.
Reference: [7] <author> R.G. Babb II. </author> <title> Issues in the Specification and Design of Parallel Programs. </title> <booktitle> In Proc. 6th International Workshop on Software Specification and Design, </booktitle> <month> October </month> <year> 1991, </year> <pages> pp. 75-82. </pages>
Reference-contexts: Some notable systems where RPC has been employed to permit mixed-language programming and support for heterogeneous architectures include Mercury [36], MLP [30] and HRPC [12]. 8.3 Graphical Programming Examples of systems that use graphical notations to express parallel computations include Fel [32], Poker [38], CODE [17], Alex [33], LGDF <ref> [6, 7] </ref> and apE [26]. None of these systems addresses fault tolerance nor provides a programming environment in the sense of Paralex. Of these, perhaps CODE comes closest to Paralex in design goals.
Reference: [8] <author> A. Beguelin, J.J. Dongarra, G.A. Geist, R. Manchek and V.S. Sunderam. </author> <title> Graphical Development Tools for Network-Based Concurrent Supercomputing. </title> <booktitle> In Proc. Supercomputing '91, November 1991, </booktitle> <address> Albuquerque, New Mexico. </address>
Reference-contexts: This paradigm has been argued by Steele to simplify reasoning about asynchronous parallel computations without reducing their flexibility [40]. The system that perhaps comes closest to Paralex in its design goals and implementation is HeNCE <ref> [8, 9] </ref>. In HeNCE, the graphical representation of a computation captures the precedence relations among the various procedures. Data flow is implicit through syntactic matching of output names to parameter names. HeNCE graphs are dynamic in that subgraphs could be conditionally expanded, repeated or pipelined.
Reference: [9] <author> A. Beguelin, J.J. Dongarra, G.A. Geist and V.S. Sunderam. </author> <title> Visualization and Debugging in a Heterogeneous Environment. </title> <journal> IEEE Computer, </journal> <volume> vol. 26, no. 6, </volume> <month> June </month> <year> 1993, </year> <pages> pp. 88-95. </pages>
Reference-contexts: This paradigm has been argued by Steele to simplify reasoning about asynchronous parallel computations without reducing their flexibility [40]. The system that perhaps comes closest to Paralex in its design goals and implementation is HeNCE <ref> [8, 9] </ref>. In HeNCE, the graphical representation of a computation captures the precedence relations among the various procedures. Data flow is implicit through syntactic matching of output names to parameter names. HeNCE graphs are dynamic in that subgraphs could be conditionally expanded, repeated or pipelined.
Reference: [10] <author> T. Bemmerl, A. Bode, et al. </author> <title> TOPSYS Tools for Parallel Systems. </title> <institution> SFB-Bericht 342/9/90A, Technische Universit at M unchen, Munich, Germany, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: The user encapsulates data and computation as objects and the compiler performs all the necessary data flow analysis to permit parallel invocations whenever possible. The run-time system ensures correct object semantics even though all invocations are performed asynchronously. Yet another object-oriented approach to parallel programming is TOPSYS <ref> [10] </ref>. The basic programming abstraction in TOPSYS consists of tasks, mailboxes and semaphores realized through library routines. The emphasis of the system is a collection of graphical tools for performance monitoring and debugging. The idea of broadcast-based parallel programming is explored in the nigen++ system [2].
Reference: [11] <author> B.N. Bershad and M.J. Zekauskas. </author> <title> Shared Memory Parallel Programming with Entry Consistency for Distributed Memory Multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Distributed shared memory has emerged as a popular paradigm for programming parallel applications to run on a network of workstations. Systems that have been built based on this model include Amber [22], Munin [20] and Midway <ref> [11] </ref>. Ideally, the collection of distributed memory management systems collaborate to create the abstraction of a single, coherent shared address space.
Reference: [12] <author> B.N. Bershad, D.T. Ching, E.D. Lazowska, J. Sanislo and M. Schwartz. </author> <title> A Remote Procedure Call Facility for Interconnecting Heterogeneous Computer Systems. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> vol. SE-13, no. 6, </volume> <month> August </month> <year> 1987, </year> <pages> pp. 880-894. </pages>
Reference-contexts: Results of the remote computation are passed on to other nodes rather than being returned to the invoker. Some notable systems where RPC has been employed to permit mixed-language programming and support for heterogeneous architectures include Mercury [36], MLP [30] and HRPC <ref> [12] </ref>. 8.3 Graphical Programming Examples of systems that use graphical notations to express parallel computations include Fel [32], Poker [38], CODE [17], Alex [33], LGDF [6, 7] and apE [26]. None of these systems addresses fault tolerance nor provides a programming environment in the sense of Paralex.
Reference: [13] <author> K. Birman, R. Cooper, T. Joseph, K. Marzullo, M. Makpangou, K. Kane, F. Schmuck and M. Wood. </author> <title> The ISIS System Manual, </title> <type> Version 2.1. </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: For each node, the binaries for stub and the actual computation are combined only at link time while producing the executable module. The compiler must also address the two aspects of heterogeneity data representation and instruction sets. Paralex uses the ISIS toolkit <ref> [14, 13] </ref> as the infrastructure to realize a universal data representation. All data that is passed from one node to another during the computation are encapsulated as ISIS messages. Paralex automatically generates all necessary code for encoding-decoding basic data types (integer, real, character) and linearizing arrays of these basic types.
Reference: [14] <author> K. Birman. </author> <title> The Process Group Approach to Reliable Distributed Computing. </title> <journal> Comm. of the ACM, </journal> <volume> vol. 123, </volume> <month> December </month> <year> 1993, </year> <pages> pp. 36-53. </pages>
Reference-contexts: For each node, the binaries for stub and the actual computation are combined only at link time while producing the executable module. The compiler must also address the two aspects of heterogeneity data representation and instruction sets. Paralex uses the ISIS toolkit <ref> [14, 13] </ref> as the infrastructure to realize a universal data representation. All data that is passed from one node to another during the computation are encapsulated as ISIS messages. Paralex automatically generates all necessary code for encoding-decoding basic data types (integer, real, character) and linearizing arrays of these basic types.
Reference: [15] <author> A.D. Birrell and B.J. Nelson. </author> <title> Implementing Remote Procedure Calls. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> vol. 2, no. 1, </volume> <month> February </month> <year> 1984, </year> <pages> pp. 39-59. </pages>
Reference-contexts: Unlike Paralex, HeNCE has no automatic support for fault tolerance or dynamic load balancing. 8.2 Language and Architecture Heterogeneity Our use of stubs for remote communication and universal data representation as ISIS messages derives its inspiration from Remote Procedure Call (RPC) systems <ref> [15] </ref>. Our use of these techniques, however, is to permit flow of data across (potentially) heterogeneous architectures rather than flow of control.
Reference: [16] <author> J.C. Browne. </author> <title> Formulation and Programming of Parallel Computations: A Unified Approach. </title> <booktitle> In Proc. Int. Conf. Parallel Processing, </booktitle> <address> Los Alamitos, California, </address> <year> 1985, </year> <pages> pp. 624-631. </pages>
Reference-contexts: Of these, perhaps CODE comes closest to Paralex in design goals. In addition to a graphical notation for parallel programs, it supports software reuse through a subsystem called ROPE [18]. The programming paradigm of CODE is based on the model of Browne <ref> [16] </ref> where programs are represented as generalized dependency graphs with each node having three sets of dependencies: input data, output data and exclusion. CODE does not support cyclic graph structures and the module interfaces are defined through declarative language.
Reference: [17] <author> J.C. Browne, M. Azam and S. Sobek. </author> <title> CODE: A Unified Approach to Parallel Programming. </title> <journal> IEEE Software, </journal> <month> July </month> <year> 1989, </year> <pages> pp. 10-18. </pages>
Reference-contexts: Some notable systems where RPC has been employed to permit mixed-language programming and support for heterogeneous architectures include Mercury [36], MLP [30] and HRPC [12]. 8.3 Graphical Programming Examples of systems that use graphical notations to express parallel computations include Fel [32], Poker [38], CODE <ref> [17] </ref>, Alex [33], LGDF [6, 7] and apE [26]. None of these systems addresses fault tolerance nor provides a programming environment in the sense of Paralex. Of these, perhaps CODE comes closest to Paralex in design goals.
Reference: [18] <author> J.C. Browne, T. Lee and J. Werth. </author> <title> Experimental Evaluation of a Reusability-Oriented Parallel Programming Environment. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> vol. 16, no. 2, </volume> <month> February </month> <year> 1990, </year> <pages> pp. 111-120. </pages>
Reference-contexts: In the limit, interesting new parallel programs can be constructed by reusing existing sequential software and without having to rewrite a single line of traditional code. As such, Paralex also addresses the issue of software reusability <ref> [18] </ref>. The rest of the paper is organized as follows. The next section defines the programming model supported by Paralex. In Section 3 we give an overview of the principal components of Paralex and illustrate the user interface through examples. <p> None of these systems addresses fault tolerance nor provides a programming environment in the sense of Paralex. Of these, perhaps CODE comes closest to Paralex in design goals. In addition to a graphical notation for parallel programs, it supports software reuse through a subsystem called ROPE <ref> [18] </ref>. The programming paradigm of CODE is based on the model of Browne [16] where programs are represented as generalized dependency graphs with each node having three sets of dependencies: input data, output data and exclusion.
Reference: [19] <author> N. Carriero and D. Gelernter. </author> <title> How to Write Parallel Programs: A Guide to the Perplexed. </title> <journal> ACM Computing Surveys, </journal> <volume> vol. 21, no. 3, </volume> <month> September </month> <year> 1989, </year> <pages> pp. 323-358. </pages>
Reference-contexts: Section 9 discusses some of our design decisions and directions for future work. Section 10 concludes the paper. UBLCS-92-4 2 2 The Paralex Programming Paradigm 2 The Paralex Programming Paradigm The choices made for programming paradigm and notation are fundamental in harnessing parallelism in a particular application domain <ref> [19] </ref>. The programming paradigm supported by Paralex can be classified as static data flow [1]. A Paralex program is composed of nodes and links. Nodes correspond to computations (functions, procedures, programs) and the links indicate flow of (typed) data.
Reference: [20] <author> J.B. Carter, J.K. Bennett and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proc. Thirteenth ACM SIGOPS Symposium on Operating Systems Principles, </booktitle> <address> Pacific Grove, California, </address> <month> October </month> <year> 1991, </year> <pages> pp. 152-164. </pages>
Reference-contexts: Distributed shared memory has emerged as a popular paradigm for programming parallel applications to run on a network of workstations. Systems that have been built based on this model include Amber [22], Munin <ref> [20] </ref> and Midway [11]. Ideally, the collection of distributed memory management systems collaborate to create the abstraction of a single, coherent shared address space.
Reference: [21] <author> K.M. Chandy. </author> <title> Programming Parallel Computers. </title> <type> Technical Report Caltech-CS-TR-88-16, </type> <institution> Department of Computer Science, California Institute of Technology, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: Recent proposals by Valiant [41] and Steele [40] fall into the first category. Systems and notations such as Paralex, Par [23], UNITY <ref> [21] </ref>, Linda [28], CODE, P 3 L [24], Prelude [42] and Phase Abstractions [35] fall into the second camp. In the case of Paralex, we inherit the properties of the data flow notation and keep further goals for architecture independence rather modest.
Reference: [22] <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy and R.J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proc. Twelfth ACM SIGOPS Symposium on Operating Systems Principles, </booktitle> <address> Litchfield Park, Arizona, </address> <month> December </month> <year> 1989, </year> <pages> pp. 147-158. </pages>
Reference-contexts: Distributed shared memory has emerged as a popular paradigm for programming parallel applications to run on a network of workstations. Systems that have been built based on this model include Amber <ref> [22] </ref>, Munin [20] and Midway [11]. Ideally, the collection of distributed memory management systems collaborate to create the abstraction of a single, coherent shared address space.
Reference: [23] <author> M.H. Coffin and G.R. Andrews. </author> <title> Towards Architecture-Independent Parallel Programming. </title> <type> Technical Report TR 89-21a, </type> <institution> Department of Computer Science, University of Arizona, Tucson, Arizona, </institution> <month> December </month> <year> 1989. </year> <note> UBLCS-92-4 21 REFERENCES </note>
Reference-contexts: Recent proposals by Valiant [41] and Steele [40] fall into the first category. Systems and notations such as Paralex, Par <ref> [23] </ref>, UNITY [21], Linda [28], CODE, P 3 L [24], Prelude [42] and Phase Abstractions [35] fall into the second camp. In the case of Paralex, we inherit the properties of the data flow notation and keep further goals for architecture independence rather modest.
Reference: [24] <author> M. Danelutto, R. Di Meglio, S. Pelegatti and M. Vanneschi. </author> <title> High Level Language Constructs for Massively Parallel Computing. </title> <booktitle> In Proc. Sixth International Symposium on Computer and Information Sciences, </booktitle> <publisher> Elsevier North-Holland, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: Recent proposals by Valiant [41] and Steele [40] fall into the first category. Systems and notations such as Paralex, Par [23], UNITY [21], Linda [28], CODE, P 3 L <ref> [24] </ref>, Prelude [42] and Phase Abstractions [35] fall into the second camp. In the case of Paralex, we inherit the properties of the data flow notation and keep further goals for architecture independence rather modest.
Reference: [25] <author> K. Dixit. SPECulations: </author> <title> Defining the SPEC Benchmark. </title> <journal> SunTech Journal, </journal> <volume> vol. 4, no. 1, </volume> <month> January </month> <year> 1991, </year> <pages> pp. 53-65. </pages>
Reference-contexts: In the example of Figure 1, tosca and violetta are both based on m68020 but are not binary compatible. If present, the attribute spec is an indication of the raw processor power measured in SPECmarks <ref> [25] </ref>. The SPECmark value of a host is used by the mapping and dynamic load balancing algorithms to associate computations with hosts. If the SPECmark rating of a particular host is not available, Paralex assumes that it is lower than all the other hosts.
Reference: [26] <author> D.S. Dyer. </author> <title> A Dataflow Toolkit for Visualization. </title> <journal> IEEE Computer Graphics and Applications, </journal> <month> July </month> <year> 1990, </year> <pages> pp. 60-69. </pages>
Reference-contexts: where RPC has been employed to permit mixed-language programming and support for heterogeneous architectures include Mercury [36], MLP [30] and HRPC [12]. 8.3 Graphical Programming Examples of systems that use graphical notations to express parallel computations include Fel [32], Poker [38], CODE [17], Alex [33], LGDF [6, 7] and apE <ref> [26] </ref>. None of these systems addresses fault tolerance nor provides a programming environment in the sense of Paralex. Of these, perhaps CODE comes closest to Paralex in design goals. In addition to a graphical notation for parallel programs, it supports software reuse through a subsystem called ROPE [18].
Reference: [27] <author> E. </author> <title> Fairfield. </title> <type> Private communication. </type> <institution> Los Alamos National Laboratory, </institution> <address> New Mexico. </address>
Reference-contexts: As a consequence, developing parallel programs in such systems requires expertise not only in distributed computing, but also in fault tolerance. A large number of important applications (e.g., genome analysis) require days or weeks of computations on a network with dozens of workstations <ref> [27] </ref>. In these applications, many hours of computation can be wasted not only if there are genuine hardware failures, but also if one of the processors is turned off, rebooted or disconnected from the network.
Reference: [28] <author> D. Gelernter, N. Carriero, S. Chandran and S. Chang. </author> <title> Parallel Programming in Linda. </title> <booktitle> In Proc. Int. Conf. Parallel Processing, </booktitle> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1985, </year> <pages> pp. 255-263. </pages>
Reference-contexts: Recent proposals by Valiant [41] and Steele [40] fall into the first category. Systems and notations such as Paralex, Par [23], UNITY [21], Linda <ref> [28] </ref>, CODE, P 3 L [24], Prelude [42] and Phase Abstractions [35] fall into the second camp. In the case of Paralex, we inherit the properties of the data flow notation and keep further goals for architecture independence rather modest.
Reference: [29] <author> A.S. Grimshaw. </author> <title> Easy-to-Use Object-Oriented Parallel Processing with Mentat. </title> <journal> IEEE Computer, </journal> <volume> vol. 26, no. 5, </volume> <month> May </month> <year> 1993, </year> <pages> pp. 39-51. </pages>
Reference-contexts: The drawback is the increased complexity of the application and/or the system support in order to guarantee correctness despite weak consistency semantics provided by the shared memory abstraction. The shared memory approach to parallel computing is further explored by the Mentat system <ref> [29] </ref> in the context of an object-oriented programming language derived from C++. The user encapsulates data and computation as objects and the compiler performs all the necessary data flow analysis to permit parallel invocations whenever possible. The run-time system ensures correct object semantics even though all invocations are performed asynchronously.
Reference: [30] <author> R. Hayes, S.W. Manweiler and R. D. Schlichting. </author> <title> A Simple System for Constructing Distributed, Mixed Language Programs. </title> <journal> Software-Practice and Experience, </journal> <volume> vol. 18, no. 7, </volume> <month> July </month> <year> 1988, </year> <pages> pp. 641-660. </pages>
Reference-contexts: Results of the remote computation are passed on to other nodes rather than being returned to the invoker. Some notable systems where RPC has been employed to permit mixed-language programming and support for heterogeneous architectures include Mercury [36], MLP <ref> [30] </ref> and HRPC [12]. 8.3 Graphical Programming Examples of systems that use graphical notations to express parallel computations include Fel [32], Poker [38], CODE [17], Alex [33], LGDF [6, 7] and apE [26]. None of these systems addresses fault tolerance nor provides a programming environment in the sense of Paralex.
Reference: [31] <author> D. Johansen. StormCast: </author> <title> Yet Another Excercise in Distributed Computing. </title> <editor> In Frances Brazier and Dag Johansen, editors, </editor> <title> Distributed Open Systems. </title> <publisher> IEEE Computer Society Press, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: As an instance of this problem domain, we have used Paralex to program a volume rendering application for visualization of atmospheric model output. The original application, called Stor-mView [3], was built at the University of Troms in Norway as part of the StormCast distributed system <ref> [31] </ref> and has been used by meteorologists to visualize the scalar fields generated by the atmospheric model LAM50S developed by the Norwegian Meteorological Institute. StormView uses a technique called direct volume rendering through ray casting whereby information between the iso-surfaces can be visualized without building the intermediate polygon structure.
Reference: [32] <author> R.M. Keller and W-C.J. Yen. </author> <title> A Graphical Approach to Software Development Using Function Graphs. </title> <booktitle> In Proc. Compcon Spring 1981, </booktitle> <publisher> CS Press, Los Alamitos, California, </publisher> <pages> pp. 156-161. </pages>
Reference-contexts: Some notable systems where RPC has been employed to permit mixed-language programming and support for heterogeneous architectures include Mercury [36], MLP [30] and HRPC [12]. 8.3 Graphical Programming Examples of systems that use graphical notations to express parallel computations include Fel <ref> [32] </ref>, Poker [38], CODE [17], Alex [33], LGDF [6, 7] and apE [26]. None of these systems addresses fault tolerance nor provides a programming environment in the sense of Paralex. Of these, perhaps CODE comes closest to Paralex in design goals.
Reference: [33] <author> D. Kozen, T. Teitelbaum, W. Chen, J. Field, W. Pugh and B. Vander Zanden. ALEX: </author> <title> An Alexical Programming Language. In Visual Programming Languages, </title> <editor> Ed. Korfnage, </editor> <publisher> Plenum Press. </publisher>
Reference-contexts: Some notable systems where RPC has been employed to permit mixed-language programming and support for heterogeneous architectures include Mercury [36], MLP [30] and HRPC [12]. 8.3 Graphical Programming Examples of systems that use graphical notations to express parallel computations include Fel [32], Poker [38], CODE [17], Alex <ref> [33] </ref>, LGDF [6, 7] and apE [26]. None of these systems addresses fault tolerance nor provides a programming environment in the sense of Paralex. Of these, perhaps CODE comes closest to Paralex in design goals.
Reference: [34] <author> L. Lamport. </author> <title> Time, clocks and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> vol. 21, no. 7, </volume> <month> July </month> <year> 1978, </year> <pages> pp. 558-565. </pages>
Reference-contexts: Paralex cooperates with ISIS toward this goal by using a reliable multicast communication primitive that respects causality <ref> [34] </ref>. 5 Dynamic Load Balancing Before a Paralex program can be executed, each of the nodes (and their replicas, in case fault tolerance is required) must be associated with a host of the distributed system.
Reference: [35] <author> C. Lin and L. Snyder. </author> <title> Portable Parallel Programming: Cross Machine Comparisons for SIMPLE. </title> <booktitle> In Fifth SIAM Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: Recent proposals by Valiant [41] and Steele [40] fall into the first category. Systems and notations such as Paralex, Par [23], UNITY [21], Linda [28], CODE, P 3 L [24], Prelude [42] and Phase Abstractions <ref> [35] </ref> fall into the second camp. In the case of Paralex, we inherit the properties of the data flow notation and keep further goals for architecture independence rather modest.
Reference: [36] <author> B. Liskov, et al. </author> <title> Communication in the Mercury System. </title> <booktitle> In Proc. 21st Annual Hawaii Conference on System Sciences, </booktitle> <month> January </month> <year> 1988. </year>
Reference-contexts: Results of the remote computation are passed on to other nodes rather than being returned to the invoker. Some notable systems where RPC has been employed to permit mixed-language programming and support for heterogeneous architectures include Mercury <ref> [36] </ref>, MLP [30] and HRPC [12]. 8.3 Graphical Programming Examples of systems that use graphical notations to express parallel computations include Fel [32], Poker [38], CODE [17], Alex [33], LGDF [6, 7] and apE [26].
Reference: [37] <author> V. Ranjan and A. Fournier. </author> <title> Volume Models for Volumetric Data. </title> <journal> IEEE Computer, </journal> <volume> vol. 27, no. 7, </volume> <month> July </month> <year> 1994, </year> <pages> pp. 28-36. </pages>
Reference-contexts: Communication and fault tolerance support were based on version 3.1 of the ISIS toolkit. 7.1 Volume Rendering of Atmospheric Data For many scientific applications, the visualization of the results may be just as expensive as computing the results themselves. This is particularly true for computations that produce volumetric data <ref> [37] </ref>. For these applications, extensive computation is required to visualize the internal structure of three-dimensional object rather than simply their surfaces. As such, the visualization process may itself benefit from parallelization.
Reference: [38] <author> L. Snyder. </author> <title> Parallel Programming and the Poker Programming Environment. </title> <journal> IEEE Computer, </journal> <volume> vol. 17, no. 7, </volume> <month> July </month> <year> 1984, </year> <pages> pp. 27-36. </pages>
Reference-contexts: Some notable systems where RPC has been employed to permit mixed-language programming and support for heterogeneous architectures include Mercury [36], MLP [30] and HRPC [12]. 8.3 Graphical Programming Examples of systems that use graphical notations to express parallel computations include Fel [32], Poker <ref> [38] </ref>, CODE [17], Alex [33], LGDF [6, 7] and apE [26]. None of these systems addresses fault tolerance nor provides a programming environment in the sense of Paralex. Of these, perhaps CODE comes closest to Paralex in design goals.
Reference: [39] <author> S.M. Sobek. </author> <title> A Constructive Unifoed Model of Parallel Computation. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Sciences, University of Texas at Austin, Austin, Texas, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: CODE does not support cyclic graph structures and the module interfaces are defined through declarative language. The program modules along with the interface definitions are compiled with the TOAD (translator of a declaration) subsystem to produce executables for specific parallel architectures <ref> [39] </ref>. As such, CODE needs the source versions of the program modules and cannot compose parallel programs out of pre-compiled binaries or libraries.
Reference: [40] <author> G.L. Steele Jr. </author> <title> Making Asynchronous Parallelism Safe for the World. </title> <booktitle> In. Proc. 17th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1990, </year> <pages> pp. 218-231. </pages>
Reference-contexts: Using ISIS as the broadcast mechanism, nigen++ supports a programming style not unlike the Connection Machine a single master process distributes work to a collection of slave processes. This paradigm has been argued by Steele to simplify reasoning about asynchronous parallel computations without reducing their flexibility <ref> [40] </ref>. The system that perhaps comes closest to Paralex in its design goals and implementation is HeNCE [8, 9]. In HeNCE, the graphical representation of a computation captures the precedence relations among the various procedures. Data flow is implicit through syntactic matching of output names to parameter names. <p> There are two ways to address this problem: propose a model of parallel computation and hope that it will be accepted as the model, or propose programming languages that can be efficiently mapped to a variety of competing models. Recent proposals by Valiant [41] and Steele <ref> [40] </ref> fall into the first category. Systems and notations such as Paralex, Par [23], UNITY [21], Linda [28], CODE, P 3 L [24], Prelude [42] and Phase Abstractions [35] fall into the second camp. <p> This in turn facilitates reusability of existing sequential programs as building blocks for distributed parallel programs. We also note that this programming paradigm is in the spirit of that proposed by Steele <ref> [40] </ref> where a severely-restricted programming paradigm is advocated even for environments that support arbitrary asynchrony. Paralex computation graphs are static. The only source of dynamism in the computation is the cyclic execution of a subgraph. This same mechanism could also be used to realize conditional subgraph expansion.
Reference: [41] <author> L.G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> vol. 33, no. 8, </volume> <month> August </month> <year> 1990, </year> <pages> pp. 103-111. </pages>
Reference-contexts: There are two ways to address this problem: propose a model of parallel computation and hope that it will be accepted as the model, or propose programming languages that can be efficiently mapped to a variety of competing models. Recent proposals by Valiant <ref> [41] </ref> and Steele [40] fall into the first category. Systems and notations such as Paralex, Par [23], UNITY [21], Linda [28], CODE, P 3 L [24], Prelude [42] and Phase Abstractions [35] fall into the second camp.
Reference: [42] <author> W. Weihl, E. Brewer, A. Colbrook, C. Dellarocas, W. Hsieh, A. Joseph, C. Waldspurger and P. Wang. </author> <title> Prelude: A System for Portable Parallel Software. </title> <booktitle> In Proc. Conference on Parallel Architectures and Languages in Europe (PARLE'92), </booktitle> <year> 1992. </year> <note> UBLCS-92-4 22 </note>
Reference-contexts: Recent proposals by Valiant [41] and Steele [40] fall into the first category. Systems and notations such as Paralex, Par [23], UNITY [21], Linda [28], CODE, P 3 L [24], Prelude <ref> [42] </ref> and Phase Abstractions [35] fall into the second camp. In the case of Paralex, we inherit the properties of the data flow notation and keep further goals for architecture independence rather modest.
References-found: 42

