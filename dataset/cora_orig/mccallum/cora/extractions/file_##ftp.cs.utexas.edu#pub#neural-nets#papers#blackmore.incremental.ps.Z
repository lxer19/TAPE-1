URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/blackmore.incremental.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Email: justine,risto@cs.utexas.edu  
Title: Incremental Grid Growing: Encoding High-Dimensional Structure into a Two-Dimensional Feature Map  
Author: Justine Blackmore and Risto Miikkulainen 
Address: Austin, Austin, TX 78712-1188  
Affiliation: Department of Computer Sciences The University of Texas at  
Abstract: Knowledge of clusters and their relations is important in understanding high-dimensional input data with unknown distribution. Ordinary feature maps with fully connected, fixed grid topology cannot properly reflect the structure of clusters in the input space|there are no cluster boundaries on the map. Incremental feature map algorithms, where nodes and connections are added to or deleted from the map according to the input distribution, can overcome this problem. However, so far such algorithms have been limited to maps that can be drawn in 2-D only in the case of 2-dimensional input space. In the approach proposed in this paper, nodes are added incrementally to a regular, 2-dimensional grid, which is drawable at all times, irrespective of the dimensionality of the input space. The process results in a map that explicitly represents the cluster structure of the high-dimensional input. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Fritzke. </author> <title> "Let it grow|Self-organizing feature maps with problem dependent cell structure." </title> <booktitle> Proceedings of the International Conference on Artificial Neural Networks (Espoo, </booktitle> <address> Finland). Amsterdam; New York: </address> <publisher> North-Holland, </publisher> <year> 1991, </year> <pages> pp. 403-408. </pages>
Reference: [2] <author> B. Fritzke. </author> <title> "Unsupervised clustering with growing cell structures." </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (Seattle, WA), volume II. </booktitle> <address> Pis-cataway, NJ: </address> <publisher> IEEE, </publisher> <year> 1991, </year> <pages> pp. 531-536. </pages>
Reference: [3] <author> B. Fritzke. Wachsende Zellstrukturen|ein selbstorgan-isierendes neuronales Netzwerkmodell. </author> <type> PhD thesis, </type> <institution> Tech-nischen Fakultat, Universitat Erlangen-Nurnberg, Erlan-gen, Germany, </institution> <year> 1992. </year>
Reference: [4] <author> S. Jokusch. </author> <title> "A neural network which adapts its structure to a given set of patterns." </title> <editor> Rolf Eckmiller, Georg Hartmann, and Gert Hauske, Eds., </editor> <booktitle> Parallel Processing in Neural Systems and Computers. </booktitle> <address> Amsterdam; New York: </address> <publisher> North-Holland, </publisher> <year> 1990, </year> <pages> pp. 169-172. </pages>
Reference: [5] <author> J. Kangas, T. Kohonen, and J. Laaksonen. </author> <title> "Variants of self-organizing maps." </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1 </volume> <pages> 93-99, </pages> <year> 1990. </year>
Reference: [6] <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory, </title> <publisher> 3rd ed. </publisher> <address> Berlin; Heidelberg; New York: </address> <publisher> Springer, </publisher> <year> 1989. </year>
Reference: [7] <author> T. Kohonen. </author> <title> "The self-organizing map." </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78 </volume> <pages> 1464-1480, </pages> <year> 1990. </year>
Reference-contexts: The relative numbers of nodes throughout the structure reflect the uniform distribution of the input. Again, the overall topology and distribution of the input space is apparent in the simple 2-dimensional structure of the grid. The final example, the "spanning tree" of <ref> [7] </ref>, demonstrates the algorithm's ability to develop arbitrarily complex topologies for discrete inputs. In this example, the input consists of the 5-dimensional vectors listed in Fig. 5a. <p> 3 3 6 6 6 6 66 666 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 12 345 6 (b) Fig. 5: (a) The input set for Kohonen's spanning tree example <ref> [7] </ref>. (b) The minimal spanning tree of the data. Although the example may seem artificial, it illustrates the self-organizing feature map's capacity to represent the general topology of difficult-to-describe data sets. <p> The overall topology of the input space is encoded in the continuity of the network structure alone, before any node labelling has been done. (a) Fig. 6: Feature map representation for the spanning tree data. (a) Map derived by the standard self-organizing algorithm <ref> [7] </ref>. The map is hexagonally connected. The spanning tree structure is clearly present in the map; however, the full connectivity makes it difficult to extract exact neighborhood relations between units. (b) Map derived by the grid growing algorithm.
Reference: [8] <author> T. M. Martinetz and K. J. Schulten. </author> <title> "A `neural gas' network learns topologies." </title> <booktitle> Proceedings of the International Conference on Artificial Neural Networks (Espoo, </booktitle> <address> Finland). Amsterdam; New York: </address> <publisher> North-Holland, </publisher> <year> 1991, </year> <pages> pp. 397-402. </pages>
Reference: [9] <author> R. Miikkulainen and M. G. Dyer. </author> <title> "Natural language processing with modular neural networks and distributed lexicon." </title> <journal> Cognitive Science, </journal> <volume> 15 </volume> <pages> 343-399, </pages> <year> 1991. </year>
Reference-contexts: In addition, the application of the algorithm to various complex high-dimensional data sets is being investigated. For example, the algorithm should be useful in developing clusters for the representation of semantic features of words (e.g. <ref> [9, 11] </ref>). Data interpretation and knowledge representation in general is a most promising application of feature map algorithms, and incremental grid growing should prove particularly useful in such tasks.
Reference: [10] <author> H. J. Ritter. </author> <title> "Learning with the self-organizing map." </title> <booktitle> Proceedings of the International Conference on Artificial Neural Networks (Espoo, </booktitle> <address> Finland). Amsterdam; New York: </address> <publisher> North-Holland, </publisher> <year> 1991, </year> <pages> pp. 379-384. </pages>
Reference: [11] <author> H. J. Ritter and T. Kohonen. </author> <title> "Self-organizing semantic maps." </title> <journal> Biological Cybernetics, </journal> <volume> 61 </volume> <pages> 241-254, </pages> <year> 1989. </year>
Reference-contexts: In addition, the application of the algorithm to various complex high-dimensional data sets is being investigated. For example, the algorithm should be useful in developing clusters for the representation of semantic features of words (e.g. <ref> [9, 11] </ref>). Data interpretation and knowledge representation in general is a most promising application of feature map algorithms, and incremental grid growing should prove particularly useful in such tasks.
Reference: [12] <author> J. S. Rodriques and L. B. Almeida. </author> <title> "Improving the learning speed in topological maps of patterns." </title> <booktitle> Proceedings of the International Neural Networks Conference (Paris, </booktitle> <address> France). Dordrecht; Boston: </address> <publisher> Kluwer, </publisher> <year> 1990, </year> <pages> pp. 813-816. </pages>
Reference: [13] <author> L. Xu and E. Oja. </author> <title> "Adding top-down expectation into the learning procedure of self-organizing maps." </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (Washington, DC), volume II. </booktitle> <address> Hillsdale, NJ: Erl-baum, </address> <year> 1990, </year> <pages> pp. 531-534. </pages>
References-found: 13

