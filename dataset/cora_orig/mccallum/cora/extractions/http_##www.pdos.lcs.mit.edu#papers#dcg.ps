URL: http://www.pdos.lcs.mit.edu/papers/dcg.ps
Refering-URL: http://www.pdos.lcs.mit.edu/PDOS-papers.html
Root-URL: 
Title: DCG: An Efficient, Retargetable Dynamic Code Generation System  
Author: Dawson R. Engler Todd A. Proebsting 
Affiliation: Massachusetts Institute of Technology  University of Arizona  
Abstract: Dynamic code generation allows aggressive optimization through the use of runtime information. Previous systems typically relied on ad hoc code generators that were not designed for retargetability, and did not shield the client from machine-specific details. We present a system, dcg, that allows clients to specify dynamically generated code in a machine-independent manner. Our one-pass code generator is easily retargeted and extremely efficient (code generation costs approximately 350 instructions per generated instruction). Experiments show that dynamic code generation increases some application speeds by over an order of magnitude. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Mahedevan Ganapathi, and Steven W. K. Tjiang. </author> <title> Code generation using tree matching and dynamic programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 491-516, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Our system is two orders of magnitude faster than this. Outside the context of dynamic code generation, retargetable code generation is well studied. Two competing code generation strategies dominate re-targetable compilers: Register Transfer Language (RTL)-based rewriting rule systems [4], and tree pattern matching systems <ref> [1, 9, 6] </ref>.
Reference: [2] <author> David G. Bradlee, Robert R. Henry, and Susan J. Eggers. </author> <title> The Marion system for retargetable instruction scheduling. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Our experiments are conducted on a MIPS R3000 and a SPARC 10. 5.1 Matrix Multiplication Fast multiplication of matrices is important to many graphics and image processing applications. Often typedef int (*FPtr)(int); FPtr example () - Symbol arg <ref> [2] </ref>; /* argument vec sent to gen */ int ncalls = 0; /* number of calls made by plus1 */ arg [0] = sargi (); /* allocate symbol for 'x' */ dcg_param_alloc (arg, ncalls); /* associate with a virtual register (if possible) */ /* create and register IR tree for "return <p> each matrix.) Consider the example 3x3 matrix: 3 0 2 0 0 3 We want to emulate the following optimized C code that describes a dot-product customized for each row (for clarity, we elide the use of shifts and adds for strength reduction). int dot_row0 (int *b) return 3*b [0]+2*b <ref> [2] </ref>;- int dot_row1 (int *b) return 7*b [1]+4*b [2];- int dot_row2 (int *b) return 3*b [2]; - The following code is emitted by dcg for the tree specifying dot_row0: /* return 3 * b [0] + 2 * b [2]; */ addiu $sp, -152 # allocate AR lw $24, 0 ($4) <p> 0 2 0 0 3 We want to emulate the following optimized C code that describes a dot-product customized for each row (for clarity, we elide the use of shifts and adds for strength reduction). int dot_row0 (int *b) return 3*b [0]+2*b <ref> [2] </ref>;- int dot_row1 (int *b) return 7*b [1]+4*b [2];- int dot_row2 (int *b) return 3*b [2]; - The following code is emitted by dcg for the tree specifying dot_row0: /* return 3 * b [0] + 2 * b [2]; */ addiu $sp, -152 # allocate AR lw $24, 0 ($4) # load value of b [0] mul $25, <p> to emulate the following optimized C code that describes a dot-product customized for each row (for clarity, we elide the use of shifts and adds for strength reduction). int dot_row0 (int *b) return 3*b [0]+2*b <ref> [2] </ref>;- int dot_row1 (int *b) return 7*b [1]+4*b [2];- int dot_row2 (int *b) return 3*b [2]; - The following code is emitted by dcg for the tree specifying dot_row0: /* return 3 * b [0] + 2 * b [2]; */ addiu $sp, -152 # allocate AR lw $24, 0 ($4) # load value of b [0] mul $25, $24, 3 # 3 * b [0] <p> adds for strength reduction). int dot_row0 (int *b) return 3*b [0]+2*b <ref> [2] </ref>;- int dot_row1 (int *b) return 7*b [1]+4*b [2];- int dot_row2 (int *b) return 3*b [2]; - The following code is emitted by dcg for the tree specifying dot_row0: /* return 3 * b [0] + 2 * b [2]; */ addiu $sp, -152 # allocate AR lw $24, 0 ($4) # load value of b [0] mul $25, $24, 3 # 3 * b [0] lw $15, 8 ($4) # load value of b [2] mul $24, $15, 2 # 2 * b [2] add $25, $25, $24 # <p> the tree specifying dot_row0: /* return 3 * b [0] + 2 * b <ref> [2] </ref>; */ addiu $sp, -152 # allocate AR lw $24, 0 ($4) # load value of b [0] mul $25, $24, 3 # 3 * b [0] lw $15, 8 ($4) # load value of b [2] mul $24, $15, 2 # 2 * b [2] add $25, $25, $24 # add two results move $2, $25 # put in return register form: `maximum element size/percentage of zero elements'. addiu $sp, 152 # deallocate AR j $31 # return To test dynamic code generation in this setting, <p> [0] + 2 * b <ref> [2] </ref>; */ addiu $sp, -152 # allocate AR lw $24, 0 ($4) # load value of b [0] mul $25, $24, 3 # 3 * b [0] lw $15, 8 ($4) # load value of b [2] mul $24, $15, 2 # 2 * b [2] add $25, $25, $24 # add two results move $2, $25 # put in return register form: `maximum element size/percentage of zero elements'. addiu $sp, 152 # deallocate AR j $31 # return To test dynamic code generation in this setting, matrix multiplication was implemented using three algorithms. naive: A <p> In the machines we targeted, we estimate that this can degrade performance by up to 25%. Retargetable scheduling systems would help eliminate this performance penalty <ref> [2, 18] </ref>. Local code could be improved by peephole optimization and by special-casing leaf procedures. While the interface of dcg is more civilized than machine code, it can be improved. We are currently investigating two approaches.
Reference: [3] <author> Craig Chambers and David Ungar. </author> <title> Customization: Optimizing compiler technology for SELF, a dynamically-typed object-oriented programming language. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 146-160, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Unfortunately, their system was not easily retargetable and ran only on the Motorola 680x0 family. Implementations of languages that rely on dynamic type information benefit from this technology as well. Smalltalk [5] and Self <ref> [3] </ref>, for example, have both used dynamic code generation to optimize frequently executed routines. ParcPlace sells an implementation of of Smalltalk-80 that uses a dynamic code generator for SPARC, Motorola 68k and PowerPC, Intel x86, and other architectures.
Reference: [4] <author> Jack W. Davidson and Christopher W. Fraser. </author> <title> Code selection through object code optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(4) </volume> <pages> 7-32, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: Our system is two orders of magnitude faster than this. Outside the context of dynamic code generation, retargetable code generation is well studied. Two competing code generation strategies dominate re-targetable compilers: Register Transfer Language (RTL)-based rewriting rule systems <ref> [4] </ref>, and tree pattern matching systems [1, 9, 6].
Reference: [5] <author> Peter Deutsch and Alan M. Schiffman. </author> <title> Efficient implementation of the smalltalk-80 system. </title> <booktitle> In Proceedings of the 9th Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 297-302, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Unfortunately, their system was not easily retargetable and ran only on the Motorola 680x0 family. Implementations of languages that rely on dynamic type information benefit from this technology as well. Smalltalk <ref> [5] </ref> and Self [3], for example, have both used dynamic code generation to optimize frequently executed routines. ParcPlace sells an implementation of of Smalltalk-80 that uses a dynamic code generator for SPARC, Motorola 68k and PowerPC, Intel x86, and other architectures.
Reference: [6] <author> Helmut Emmelmann, Friedrich-Wilhelm Schroer, and Rudolf Landwehr. </author> <title> BEG|a generator for efficient back ends. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 227-237, </pages> <year> 1989. </year>
Reference-contexts: Our system is two orders of magnitude faster than this. Outside the context of dynamic code generation, retargetable code generation is well studied. Two competing code generation strategies dominate re-targetable compilers: Register Transfer Language (RTL)-based rewriting rule systems [4], and tree pattern matching systems <ref> [1, 9, 6] </ref>.
Reference: [7] <author> Christopher W. Fraser. </author> <title> A language for writing code generators. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 238-245, </pages> <year> 1989. </year>
Reference-contexts: src1, src2) STYPE (@bin, dst, src1, src2); - add addu and nor or 100000 100001 100100 100111 100101 Yields the following for add: #define add (dst, src1, src2) " STYPE (0x20, dst, src1, src2) The current specification languages are small, but not as concise as other code generator specification languages <ref> [7] </ref>. Future work will involve making the preprocessors more sophisticated. Few architectures with separate I/D caches require that the I cache be kept coherent with memory. Consequently, dynamic code generation requires that coherence be maintained manually.
Reference: [8] <author> Christopher W. Fraser and David R. Hanson. </author> <title> A code generation interface for ANSI C. </title> <journal> Software|Practice and Experience, </journal> <volume> 21(9) </volume> <pages> 963-988, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Our dynamic code generation system, dcg, portably and efficiently generates executable code at runtime. dcg client programs specify dynamically generated code using the compact, machine-independent intermediate representation (IR) of the lcc compiler <ref> [8] </ref>. Binary code is selected using BURS tree pattern-matching technology [17, 9]. The code generator is very efficient | the creation and translation of IR to binary instructions takes approximately 350 instructions per generated instruction. <p> The client invokes that code as an indirect call to a C procedure. To make client programs portable, they specify code using a machine-independent intermediate representation (IR) that is passed to dcg. The logical infrastructure of dcg is taken directly from an existing retargetable ANSI C compiler, lcc <ref> [8] </ref>. lcc's IR is smaller, simpler, and more easily understood than the obvious alternative, gcc's. The simplicity and regularity of the IR is important because this IR must be easily generated by client programs. <p> In addition, declarations of local variables and procedure arguments must be communicated to the code generator. Clients respect the code generation interface defined by lcc when invoking dcg. This interface is fully documented in <ref> [8] </ref>. dcg consists of library routines that simplify the creation of lcc IR nodes and typing information. Individual functions are provided for the construction of all legal IR nodes and correspond to lcc's 109-operator language (36 operations with 9 potential types).
Reference: [9] <author> Christopher W. Fraser, Robert R. Henry, and Todd A. Proebsting. </author> <title> BURG | fast optimal instruction selection and tree parsing. </title> <journal> SIGPLAN Notices, </journal> <volume> 27(4) </volume> <pages> 68-76, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Our dynamic code generation system, dcg, portably and efficiently generates executable code at runtime. dcg client programs specify dynamically generated code using the compact, machine-independent intermediate representation (IR) of the lcc compiler [8]. Binary code is selected using BURS tree pattern-matching technology <ref> [17, 9] </ref>. The code generator is very efficient | the creation and translation of IR to binary instructions takes approximately 350 instructions per generated instruction. <p> Our system is two orders of magnitude faster than this. Outside the context of dynamic code generation, retargetable code generation is well studied. Two competing code generation strategies dominate re-targetable compilers: Register Transfer Language (RTL)-based rewriting rule systems [4], and tree pattern matching systems <ref> [1, 9, 6] </ref>. <p> Code selection is done using burg, which uses Bottom-Up Rewrite System (BURS) technology to optimally translate an IR tree into machine instructions <ref> [9] </ref>. Instruction selection using dynamic programming and tree pattern matching is easily understood, automated, and quite fast. dcg omits any significant global optimizations and pipeline scheduling. Existing pipeline schedulers would have made the code generator slower and more complex.
Reference: [10] <author> Torbjorn Granlund and Peter L. Montgomery. </author> <title> Division by invariant integers using multiplication. </title> <booktitle> Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: We also implemented an optimized matrix scaling library. Multiplication by a runtime constant is reduced to shifts and adds. Division is strength reduced to multiplication (and then to shifts and adds) using the techniques described in <ref> [10] </ref>. The performance of multiplying a 1024x1024 integer matrix by a runtime constant improved by a factor of 4 on a SPARC 10, and 40% on a R3000.
Reference: [11] <author> SPARC International. </author> <title> The SPARC Architecture Manual. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey 07632, </address> <year> 1992. </year>
Reference-contexts: We have automated code generator retargeting by developing simple machine spec ification languages and preprocessors. The retar-get of the system to the MIPS R3000 from the SPARC took approximately 1 week. The current system runs on ABI compliant SPARC implementations (e.g., SPARC1, SPARC10, IPX) <ref> [11] </ref> and the MIPS R2000/R3000 series [12]. We describe the design and implementation of our system, and report preliminary tests of its efficiency. 2 Previous Work Many people have used dynamic code generation to exploit runtime data for creating highly efficient code that could not have been produced statically.
Reference: [12] <author> Gerry Kane and Joe Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: The retar-get of the system to the MIPS R3000 from the SPARC took approximately 1 week. The current system runs on ABI compliant SPARC implementations (e.g., SPARC1, SPARC10, IPX) [11] and the MIPS R2000/R3000 series <ref> [12] </ref>. We describe the design and implementation of our system, and report preliminary tests of its efficiency. 2 Previous Work Many people have used dynamic code generation to exploit runtime data for creating highly efficient code that could not have been produced statically.
Reference: [13] <author> David Keppel. </author> <title> A portable interface for on-the-fly instruction space modification. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 86-95, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Static enumeration of all cases would re quire over 1MB of code. Instead, they dynamically generate code for each case as needed. The dynamically generated code was up to an order of magnitude faster than the static code. Keppel addressed some issues relevant to retarget-ing dynamic code generation in <ref> [13] </ref>. He developed a portable system for modifying instruction spaces on a variety of machines. His system dealt with the difficulties presented by caches and operating system restrictions, but it did not address how to select and emit actual binary instructions.
Reference: [14] <author> David Keppel, Susan J. Eggers, and Robert R. Henry. </author> <title> A case for runtime code generation. </title> <type> Technical Report 91-11-04, </type> <institution> University of Washington, </institution> <year> 1991. </year>
Reference-contexts: We describe the design and implementation of our system, and report preliminary tests of its efficiency. 2 Previous Work Many people have used dynamic code generation to exploit runtime data for creating highly efficient code that could not have been produced statically. In <ref> [14] </ref>, Keppel, Eggers and Henry survey many advantageous uses for dynamic code generation. Massalin and Pu used dynamic code generation in their Synthesis Kernel to remove a layer of interpretation from operating system routines [19].
Reference: [15] <author> J.C. Mogul, R.F. Rashid, and M.J. Accetta. </author> <title> The packet filter: An efficient mechanism for user-level network code. </title> <booktitle> In Proc. of the Eleventh ACM Symposium on Operating System Principles, </booktitle> <pages> pages 39-51, </pages> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: lack of a corresponding difference on the SPARC is a result of prolific register window dumping in response to Fibonacci's recursive nature, where dcg's lack of pipeline scheduling is hidden in the overhead of bulk memory transfers. 5.3 Additional Experiments We implemented two packet filter engines for Mogul's packet-filter language <ref> [15] </ref>. The first is an extremely efficient byte-code interpreter that uses indirect jumps (a C extension provided by the GNU C compiler) to achieve efficient interpretation. The second uses dcg to generate code specialized for a given filter and run it directly (eliminating interpretation overhead).
Reference: [16] <author> Rob Pike, Bart N. Locanthi, and John F. Reiser. </author> <title> Hardware/software trade-offs for bitmap graphics on the blit. </title> <journal> Software|Practice and Experience, </journal> <volume> 15(2) </volume> <pages> 131-151, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: Their approach relies on sophisticated compiler analysis of programs to create efficient, "hard-wired" code emitter routines. No mention is made of the system's retargetability. Pike, Locanthi and Reiser exploited dynamic code generation to optimize bitblt, a bit-manipulation routine used in many windowing systems <ref> [16] </ref>. bitblt merges a source rectangle with a destination rectangle via logical bit operators. bitblt code to handle every possible case of bit boundaries on a word-oriented machine is slow because of its burdensome generality. Static enumeration of all cases would re quire over 1MB of code.
Reference: [17] <author> Todd A. Proebsting. </author> <title> Simple and efficient BURS table generation. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Our dynamic code generation system, dcg, portably and efficiently generates executable code at runtime. dcg client programs specify dynamically generated code using the compact, machine-independent intermediate representation (IR) of the lcc compiler [8]. Binary code is selected using BURS tree pattern-matching technology <ref> [17, 9] </ref>. The code generator is very efficient | the creation and translation of IR to binary instructions takes approximately 350 instructions per generated instruction.
Reference: [18] <author> Todd A. Proebsting and Christopher W. Fraser. </author> <title> Detecting pipeline structural hazards quickly. </title> <booktitle> In Proceedings of the 21th Annual Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1994. </year> <note> to appear. </note>
Reference-contexts: In the machines we targeted, we estimate that this can degrade performance by up to 25%. Retargetable scheduling systems would help eliminate this performance penalty <ref> [2, 18] </ref>. Local code could be improved by peephole optimization and by special-casing leaf procedures. While the interface of dcg is more civilized than machine code, it can be improved. We are currently investigating two approaches.
Reference: [19] <author> Calton Pu, Henry Massalin, and John Ioannidis. </author> <title> The synthesis kernel. </title> <journal> Computing Systems, </journal> <volume> 1(1) </volume> <pages> 11-32, </pages> <year> 1988. </year>
Reference-contexts: In [14], Keppel, Eggers and Henry survey many advantageous uses for dynamic code generation. Massalin and Pu used dynamic code generation in their Synthesis Kernel to remove a layer of interpretation from operating system routines <ref> [19] </ref>. Dynamic code generation made a single byte read/write 56 times faster and a paged-sized read/write 4 to 6 times faster in the Synthesis Kernel than in SunOS even though SunOS was running on a faster machine.
References-found: 19

