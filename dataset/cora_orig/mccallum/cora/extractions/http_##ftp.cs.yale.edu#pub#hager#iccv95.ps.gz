URL: http://ftp.cs.yale.edu/pub/hager/iccv95.ps.gz
Refering-URL: http://ftp.cs.yale.edu/pub/hager/
Root-URL: http://www.cs.yale.edu
Title: Calibration-Free Visual Control Using Projective Invariance  
Author: Gregory D. Hager 
Address: P.O. Box 208285 New Haven, CT, 06520  
Affiliation: Department of Computer Science Yale University,  
Abstract: In this article, recent work on projective geometry as applied to vision is used to extend this paradigm in two ways. First, it is shown how results from projective geometry can be used to perform online calibration. Second, results on projective invariance are used to define setpoints for visual control that are independent of viewing location. These ideas are illustrated through a number of examples and have been tested on an implemented system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.K. Allen, B. Yoshimi, and A. Timcenko. </author> <title> Hand-eye coordination for robotics tracking and grasping. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 33-70. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: However the use of only one camera places strong limitations on their capabilities. Systems employing feedback from stereo vision have been exhibited, but have focused on using reconstruction as the basis of the feedback system <ref> [1, 9] </ref>. As with all position-based systems, it is possible to exhibit cases where the accuracy of the system is affected by camera calibration errors.
Reference: [2] <author> F. Chaumette, P. Rives, and B. Espiau. </author> <title> Classification and realization of the different vision-based tasks. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 199-228. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: It has long been argued that proper use of visual measurements within a feedback loop can improve the accuracy and response of a hand-eye system [3]. Several authors have exhibited systems that employ visual feedback from a single end-effector mounted camera <ref> [5, 13, 2] </ref>. However the use of only one camera places strong limitations on their capabilities. Systems employing feedback from stereo vision have been exhibited, but have focused on using reconstruction as the basis of the feedback system [1, 9].
Reference: [3] <author> P. I. Corke. </author> <title> Visual control of robot manipulators| a review. </title> <editor> In K. Hashimoto, editor, </editor> <booktitle> Visual Servoing, </booktitle> <pages> pages 1-32. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year>
Reference-contexts: It has long been argued that proper use of visual measurements within a feedback loop can improve the accuracy and response of a hand-eye system <ref> [3] </ref>. Several authors have exhibited systems that employ visual feedback from a single end-effector mounted camera [5, 13, 2]. However the use of only one camera places strong limitations on their capabilities. <p> Define J e (x 1 ) = @q fi fi : This matrix is a slight generalization of what is commonly referred to as the image Jacobian in the visual servoing literature <ref> [3] </ref>. It relates motion of the robot to changes in the image error term. Since e is typically a nonlinear function of x 1 , J e depends on knowing this value.
Reference: [4] <author> O.D. Faugeras. </author> <title> What can be seen in three dimensions with an uncalibration stereo rig? In Computer Vision-ECCV '92, </title> <address> pages 563-578. </address> <publisher> Springer Verlag, </publisher> <year> 1993. </year>
Reference-contexts: These and other variations are discussed more fully in [7]. 2.3 Online Calibration Suppose that five points with known coordinates and an additional three points located elsewhere in the world are tracked in two cameras in general position. Following <ref> [4] </ref>, let A 1 = (1; 0; 0; 0) T ; A 2 = (0; 1; 0; 0) T ; : : : A 5 = (1; 1; 1; 1) T be a standard projective basis. <p> of A 5 ; it is possible to compute a matrix H depending on the essential matrix and the projective coordinates of A 5 in the image, a i = H (a 5 ; F )A i ; for any points A i expressed in the projective coordinate system (see <ref> [4] </ref> for details).
Reference: [5] <author> J.T. Feddema, C.S.G. Lee, and O.R. Mitchell. </author> <title> Weighted selection of image features for resolved rate visual feedback control. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 7(1) </volume> <pages> 31-47, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: It has long been argued that proper use of visual measurements within a feedback loop can improve the accuracy and response of a hand-eye system [3]. Several authors have exhibited systems that employ visual feedback from a single end-effector mounted camera <ref> [5, 13, 2] </ref>. However the use of only one camera places strong limitations on their capabilities. Systems employing feedback from stereo vision have been exhibited, but have focused on using reconstruction as the basis of the feedback system [1, 9].
Reference: [6] <author> Gregory D. Hager. </author> <title> Real-time feature tracking and projective invariance as a basis for hand-eye coordination. </title> <booktitle> In Proc. IEEE Conf. Comp. Vision and Patt. Recog., </booktitle> <pages> pages 533-539. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Recent work been shown that feedback-based approaches employing stereo vision which avoid recon struction can perform hand-eye coordination tasks with a positioning accuracy that is independent of hand-eye calibration errors <ref> [11, 6, 7] </ref>. The key idea is to define a visual error between the manipulator in its current and desired position in image coordinates. This error must have the property that zero error in two images implies the desired end-effector position has been reached regardless of camera location. <p> All image processing and visual control calculations are performed on the Sun workstation. Cartesian velocities are sent to the PC which converts them into coordinated joint motions at 140 hz. A custom tracking system written in C++ provides visual input for the controller. The system, more fully described in <ref> [6, 8] </ref>, provides fast edge detection on a memory-mapped framebuffer. In addition, it supports simultaneous tracking of multiple edge segments, and can also enforce constraints among segments. This makes it easy to develop, support and combine modular tracking applications.
Reference: [7] <author> Gregory D. Hager. </author> <title> Six DOF visual control of relative position. </title> <institution> DCS RR-1038, Yale University, </institution> <address> New Haven, CT, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Recent work been shown that feedback-based approaches employing stereo vision which avoid recon struction can perform hand-eye coordination tasks with a positioning accuracy that is independent of hand-eye calibration errors <ref> [11, 6, 7] </ref>. The key idea is to define a visual error between the manipulator in its current and desired position in image coordinates. This error must have the property that zero error in two images implies the desired end-effector position has been reached regardless of camera location. <p> Since J pr is a continuous function of any camera calibration parameters, all regulators based on it are locally calibration insensitive provided they utilize an appropriate image error. The following are specific instances of collineation-based controllers. More detail on their implementation is provided in <ref> [7] </ref>. <p> These and other variations are discussed more fully in <ref> [7] </ref>. 2.3 Online Calibration Suppose that five points with known coordinates and an additional three points located elsewhere in the world are tracked in two cameras in general position.
Reference: [8] <author> Gregory D. Hager, Sidd Puri, and Kentaro Toyama. </author> <title> A framework for real-time vision-based tracking using off-the-shelf hardware. </title> <institution> DCS RR-988, Yale University, </institution> <address> New Haven, CT, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: All image processing and visual control calculations are performed on the Sun workstation. Cartesian velocities are sent to the PC which converts them into coordinated joint motions at 140 hz. A custom tracking system written in C++ provides visual input for the controller. The system, more fully described in <ref> [6, 8] </ref>, provides fast edge detection on a memory-mapped framebuffer. In addition, it supports simultaneous tracking of multiple edge segments, and can also enforce constraints among segments. This makes it easy to develop, support and combine modular tracking applications.
Reference: [9] <author> Nicholas Hollinghurst and Roberto Cipolla. </author> <title> Uncalibrated stereo hand eye coordination. </title> <journal> Image and Vision Computing, </journal> <volume> 12(3) </volume> <pages> 187-192, </pages> <year> 1994. </year>
Reference-contexts: However the use of only one camera places strong limitations on their capabilities. Systems employing feedback from stereo vision have been exhibited, but have focused on using reconstruction as the basis of the feedback system <ref> [1, 9] </ref>. As with all position-based systems, it is possible to exhibit cases where the accuracy of the system is affected by camera calibration errors.
Reference: [10] <author> S. Hutchinson, G.D. Hager, and P. Corke. </author> <title> A tutorial introduction to visual servo control. </title> <institution> DCS RR-1068, Yale University, </institution> <address> New Haven, CT, </address> <month> February </month> <year> 1995. </year> <note> Submitted to IEEE Transactions on Robotics and Automation. </note>
Reference-contexts: and l = p 3 fi p 4 the corresponding image error is 1 There is a slight technical problem here because J pr is a 4 fi 6 matrix, but only has rank 3: In practice, this problem is solved using a decomposition of the Jacobian as described in <ref> [10] </ref>. and J pl (P 2 ; l; l) : This formulation controls four independent degrees of freedom, leaving rotations about L and translations along L unspecified. Other Variations The error functions defined above can be combined in a variety of other ways.
Reference: [11] <author> N. Maru, H. Kase, A. Nishikawa, and F. Miyazaki. </author> <title> Manipulator control by visual servoing with the stereo vision. </title> <booktitle> In IEEE Int. Workshop on Intelligent Robots and Systems, </booktitle> <pages> pages 1866-1870. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1993. </year>
Reference-contexts: Recent work been shown that feedback-based approaches employing stereo vision which avoid recon struction can perform hand-eye coordination tasks with a positioning accuracy that is independent of hand-eye calibration errors <ref> [11, 6, 7] </ref>. The key idea is to define a visual error between the manipulator in its current and desired position in image coordinates. This error must have the property that zero error in two images implies the desired end-effector position has been reached regardless of camera location.
Reference: [12] <author> J. Mundy and A. Zisserman. </author> <title> Geometric Invariance in Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1992. </year>
Reference-contexts: Planar positioning is also attractive because there are several image invariants that can be defined on planar points and lines <ref> [12] </ref>. Let P i = (x i ; y i ; 1) denote planar homogeneous coordinates of a point on a planar surface with projection p i = (u i ; v i ; 1).
Reference: [13] <author> N. Papanikolopoulos, P.K. Khosla, and T. Kanade. </author> <title> Vision and control techniques for robotic visual tracking. </title> <booktitle> In Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <pages> pages 857-864, </pages> <year> 1991. </year>
Reference-contexts: It has long been argued that proper use of visual measurements within a feedback loop can improve the accuracy and response of a hand-eye system [3]. Several authors have exhibited systems that employ visual feedback from a single end-effector mounted camera <ref> [5, 13, 2] </ref>. However the use of only one camera places strong limitations on their capabilities. Systems employing feedback from stereo vision have been exhibited, but have focused on using reconstruction as the basis of the feedback system [1, 9].
References-found: 13

