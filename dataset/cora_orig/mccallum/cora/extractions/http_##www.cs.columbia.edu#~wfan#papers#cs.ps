URL: http://www.cs.columbia.edu/~wfan/papers/cs.ps
Refering-URL: http://www.cs.columbia.edu/~wfan/research.html
Root-URL: http://www.cs.columbia.edu
Email: fwfan,pkc,salg@cs.columbia.edu  
Title: A Comparison between Combiner and Stacked Generalization  
Author: David W. Fan, Philip K. Chan and Salvatore J. Stolfo 
Keyword: machine learning, meta-learning, combiner and stacked generalization.  
Date: (212)939-7078  
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: Combiner and Stacked Generalization are two very similar meta-learning methods that combine predictions of multiple classifiers to improve accuracy of any single classifier. Both methods form a meta-level classifier from meta-data that are predictions of multiple classifiers on the same data items. The difference between these two approaches lies in the way meta-data is formed. In this paper, we compare stacked generalization and combiner in both acurracy and efficiency. We show that both methods improve the accuracy of any single classifier roughly at an equivalent level of accuracy. But combiner's accuracy is a little higher than that of stacked generalization. This is different from general anticipation. Moreover , we also see that the cost of stacked generalization is very large and may prevent it from being used on very large data sets. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: In the next section, we will present our experimental results to compare the acurracy and cost between combiner and stacked generalization. 3 Experiments Three inductive learning algorithms were used in our experiments. We obtained ID3 [10] and CART <ref> [1] </ref> as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. BAYES is a Bayesian classifier that is based on computing conditional probabilities (frequency distributions), which is described in [7]. BAYES was re-implemented in C.
Reference: [2] <author> W. Buntine and R. Caruana. </author> <title> Introduction to IND and Recursive Partitioning. </title> <institution> NASA Ames Research Center, </institution> <year> 1991. </year>
Reference-contexts: In the next section, we will present our experimental results to compare the acurracy and cost between combiner and stacked generalization. 3 Experiments Three inductive learning algorithms were used in our experiments. We obtained ID3 [10] and CART [1] as part of the IND package <ref> [2] </ref> from NASA Ames Research Center; both algorithms compute decision trees. BAYES is a Bayesian classifier that is based on computing conditional probabilities (frequency distributions), which is described in [7]. BAYES was re-implemented in C.
Reference: [3] <author> P. Chan and S. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. on Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference: [4] <author> P. Chan and S. Stofo. </author> <title> Experiments on Multistrategy Learning by Meta-learning. </title> <booktitle> In Proc. Second Intl. Conf. on Information & Knowledge Management, </booktitle> <pages> pages 314-323, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Combiner <ref> [4] </ref> and stacked generalization [13] are 2 meta-learning [4] methods. Meta-learning can be loosely defined as learning from information generated by a learner (s). It can also be viewed as the learning of meta-knowledge on the learned information. <p> 1 Introduction Combiner <ref> [4] </ref> and stacked generalization [13] are 2 meta-learning [4] methods. Meta-learning can be loosely defined as learning from information generated by a learner (s). It can also be viewed as the learning of meta-knowledge on the learned information. In meta-learning , the focus is on learning from the output of inductive learning (or learning-from-examples) systems. <p> That is, in meta-learning we are interested in the output of the learners, not the learners themselves. Meta-learning is a general technique to coalesce the results of multiple learners. Combiner <ref> [4] </ref> and stacked generalization [13] uses this idea to combine different learners to improve prediction accuracy. It involves applying multiple algorithms on the same set of data and the results of the learned concepts are combined by meta-learning. This process is called multistrategy hypothesis boosting [4]. <p> Combiner <ref> [4] </ref> and stacked generalization [13] uses this idea to combine different learners to improve prediction accuracy. It involves applying multiple algorithms on the same set of data and the results of the learned concepts are combined by meta-learning. This process is called multistrategy hypothesis boosting [4]. The goal is to achieve an overall accuracy that is higher than the accuracy obtained by any of the individual learning algorithms. In addition to combiner and stacked generalization, more multistrategy hypothesis strategies are discussed in [4]. <p> This process is called multistrategy hypothesis boosting <ref> [4] </ref>. The goal is to achieve an overall accuracy that is higher than the accuracy obtained by any of the individual learning algorithms. In addition to combiner and stacked generalization, more multistrategy hypothesis strategies are discussed in [4]. Other applications of meta-learning in distributed and parallel learning and learning speedup can be found in [5, 6] In Section 2, we compare the difference in mechanism between stacked generalization and combiner. <p> We train the base learning algorithms on y i , and ask the learned classifier to classify the left-out x i , and apply a scheme that is the same as combiner's class-combiner scheme to form one meta-learner training data item. 1 In another paper <ref> [4] </ref>, other composition schemes are addressed. 3 This process is repeated n times. The way to classify a data is the same as that of combiner. It is important to note the connection between combiner and stacking. <p> Another approach is to include the attribute vector of the data itself in the meta data. Different data will have different attribute vectors 3 , so there will be no conflict in the meta-data. This approach is called class-combiner-attribute. A detailed discussion about it can be found in <ref> [4] </ref>. One advantage of this method is that it will not require more base classifiers. However, this approach will on one hand add the complexity of meta data, on the other hand, it may reduce the effect of meta-learning by these extra attributes.
Reference: [5] <author> P. Chan and S. Stolfo. </author> <title> Toward multistrategy parallel and distributed learning in sequence analysis. </title> <booktitle> In Proc. First Intl. Conf. Intel. Sys. Mol. Biol., </booktitle> <pages> pages 65-73, </pages> <year> 1993. </year>
Reference-contexts: In addition to combiner and stacked generalization, more multistrategy hypothesis strategies are discussed in [4]. Other applications of meta-learning in distributed and parallel learning and learning speedup can be found in <ref> [5, 6] </ref> In Section 2, we compare the difference in mechanism between stacked generalization and combiner. Sections 3 and 4 presents our preliminary experimental result that shows their difference in accuracy is minimal, but there is a big difference in efficiency.
Reference: [6] <author> P. Chan and S. Stolfo. </author> <title> Toward parallel and distributed learning by meta-learning. </title> <booktitle> In Working Notes AAAI Work. Know. Disc. Databases, </booktitle> <pages> pages 227-240, </pages> <year> 1993. </year>
Reference-contexts: In addition to combiner and stacked generalization, more multistrategy hypothesis strategies are discussed in [4]. Other applications of meta-learning in distributed and parallel learning and learning speedup can be found in <ref> [5, 6] </ref> In Section 2, we compare the difference in mechanism between stacked generalization and combiner. Sections 3 and 4 presents our preliminary experimental result that shows their difference in accuracy is minimal, but there is a big difference in efficiency.
Reference: [7] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-285, </pages> <year> 1987. </year>
Reference-contexts: We obtained ID3 [10] and CART [1] as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. BAYES is a Bayesian classifier that is based on computing conditional probabilities (frequency distributions), which is described in <ref> [7] </ref>. BAYES was re-implemented in C. Two molecular biology sequence analysis data sets, obtained from the UCI Machine Learning Database, were used in our studies. They are of different sizes and difficulties.
Reference: [8] <author> M. Noordewier, G. Towell, and J. Shavlik. </author> <title> Training knowledge-based neural networks to recognize genes in dna sequences. </title> <booktitle> In Proc. NIPS-91, </booktitle> <pages> pages 530-536, </pages> <year> 1991. </year>
Reference-contexts: The training set, E, for this task has 17300 instances and the test set has 4325. The prediction accuracy of most of the learning algorithms today on SS is around 50%, so there is ample room for improvement. The DNA splice junction data set (SJ) <ref> [8] </ref>, courtesy of Noordewier, Towell, and Shavlik, contains sequences of nucleotides and the type of splice junction, if any, at the center of each sequence (three classes). Each sequence has 60 nucleotides with eight different values each (four base ones plus four combinations).
Reference: [9] <author> N. Qian and T. Sejnowski. </author> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> J. Mol. Biol., </journal> <volume> 202 </volume> <pages> 865-884, </pages> <year> 1988. </year> <month> 7 </month>
Reference-contexts: BAYES was re-implemented in C. Two molecular biology sequence analysis data sets, obtained from the UCI Machine Learning Database, were used in our studies. They are of different sizes and difficulties. The secondary protein structure data set (SS) <ref> [9] </ref>, courtesy of Qian and Sejnowski, contains sequences of amino acids and the secondary structures at the corresponding positions. There are three structures and 20 amino acids (21 attributes because of a spacer [9]) in the data. <p> They are of different sizes and difficulties. The secondary protein structure data set (SS) <ref> [9] </ref>, courtesy of Qian and Sejnowski, contains sequences of amino acids and the secondary structures at the corresponding positions. There are three structures and 20 amino acids (21 attributes because of a spacer [9]) in the data. The amino acid sequences were split into shorter sequences of length 13 according to a windowing technique used in [9]. The sequences were then divided into a disjoint training and test set, according to the distribution described in [9]. <p> There are three structures and 20 amino acids (21 attributes because of a spacer <ref> [9] </ref>) in the data. The amino acid sequences were split into shorter sequences of length 13 according to a windowing technique used in [9]. The sequences were then divided into a disjoint training and test set, according to the distribution described in [9]. The training set, E, for this task has 17300 instances and the test set has 4325. <p> amino acids (21 attributes because of a spacer <ref> [9] </ref>) in the data. The amino acid sequences were split into shorter sequences of length 13 according to a windowing technique used in [9]. The sequences were then divided into a disjoint training and test set, according to the distribution described in [9]. The training set, E, for this task has 17300 instances and the test set has 4325. The prediction accuracy of most of the learning algorithms today on SS is around 50%, so there is ample room for improvement.
Reference: [10] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: We will see how much this cost will increase from k=2 to k=n. In the next section, we will present our experimental results to compare the acurracy and cost between combiner and stacked generalization. 3 Experiments Three inductive learning algorithms were used in our experiments. We obtained ID3 <ref> [10] </ref> and CART [1] as part of the IND package [2] from NASA Ames Research Center; both algorithms compute decision trees. BAYES is a Bayesian classifier that is based on computing conditional probabilities (frequency distributions), which is described in [7]. BAYES was re-implemented in C.
Reference: [11] <author> S. Stolfo, Z. Galil, K. McKeown, and R. Mills. </author> <title> Speech recognition in parallel. </title> <booktitle> In Proc. Speech Nat. Lang. Work., </booktitle> <pages> pages 353-373. DARPA, </pages> <year> 1989. </year>
Reference: [12] <author> L. </author> <type> Breiman Stacked Regressions Technical Report, </type> <institution> Dept. of Statistics, University of California at Berleley, </institution> <year> 1992 </year>
Reference-contexts: Breiman <ref> [12] </ref> did some experiments in evaluating the performance of different k values in regression estimators and found k = 10 achieves comparable accuracy as n-fold. More extensive and systematic experiments on real data sets would yield better understanding of how the different values of k behave.

References-found: 12

