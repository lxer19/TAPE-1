URL: ftp://ftp.idsia.ch/pub/juergen/icann95.ps.gz
Refering-URL: http://www.idsia.ch/reports.html
Root-URL: 
Title: REINFORCEMENT DRIVEN INFORMATION ACQUISITION IN NON-DETERMINISTIC ENVIRONMENTS  
Author: p. - Jan Storck Sepp Hochreiter Jurgen Schmidhuber 
Keyword: Exploration, reinforcement learning, Q-learning, information gain, maximum likelihood models, non-deterministic Markovian environments, reinforcement directed in formation acquisition.  
Address: 80290 Munchen, Germany  Corso Elvezia 36 6900 Lugano, Switzerland  
Affiliation: Fakultat fur Informatik Technische Universitat Munchen  IDSIA  
Note: In Proc. ICANN'95,  
Abstract: For an agent living in a non-deterministic Markov environment (NME), what is, in theory, the fastest way of acquiring information about its statistical properties? The answer is: to design "optimal" sequences of "experiments" by performing action sequences that maximize expected information gain. This notion is implemented by combining concepts from information theory and reinforcement learning. Experiments show that the resulting method, reinforcement driven information acquisition, can explore certain NMEs much faster than conventional random exploration. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. B. Baum. </author> <title> Neural nets that learn in polynomial time from examples and queries. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1) </volume> <pages> 5-19, </pages> <year> 1991. </year>
Reference-contexts: What is an efficient strategy for acquiring a model of a non-deterministic Markov environment (NME)? Reinforcement driven information acquisition (RDIA), the method described in this paper, extends previous work on "query learning" and "experimental design" (see e.g. [4] for an overview, see <ref> [1, 7, 5, 8, 3] </ref> for more recent contributions) and "active exploration", e.g. [10, 9, 12]. The method combines the notion of information gain with the notion of reinforcement learning. The latter is used to devise exploration strategies that maximize the former. Experiments demonstrate significant advantages of RDIA. <p> We use Watkins' Q-learning [13] for this purpose: Q (S; a) is the agent's evaluation (initially zero) corresponding to the state/action pair (S; a). The central loop of the algorithm is as follows: 1. Observe current state S (t). Randomly choose p 2 <ref> [0; 1] </ref>. If p 2 [0; 1], randomly pick a (t). Otherwise pick a (t) such that Q (S (t); a (t)) is maximal. 2. Execute a (t), observe S (t + 1) and R (t). 3. <p> We use Watkins' Q-learning [13] for this purpose: Q (S; a) is the agent's evaluation (initially zero) corresponding to the state/action pair (S; a). The central loop of the algorithm is as follows: 1. Observe current state S (t). Randomly choose p 2 <ref> [0; 1] </ref>. If p 2 [0; 1], randomly pick a (t). Otherwise pick a (t) such that Q (S (t); a (t)) is maximal. 2. Execute a (t), observe S (t + 1) and R (t). 3.
Reference: [2] <author> K. Behnen and G. Neuhaus. Grundkurs Stochastik. B. G. </author> <title> Teubner, </title> <publisher> Stuttgart, </publisher> <year> 1984. </year>
Reference-contexts: Standard statistics tells us that the approximative confidence region of a multinomial distribution satisfies P (E 2 (p fl ijk ; p ijk ) &lt; c ij ) = 1 ff, where ff is a given confidence level (e.g. <ref> [2] </ref>, p. 301), and E 2 (p fl P n (p fl p ijk ijk estimators' weighted squared error (Pearson's 2 -distance for multinomial distributions is a standard way of measuring the estimators' deviations from the true probabilities, e.g. [2], p. 233, 301). <p> = 1 ff, where ff is a given confidence level (e.g. <ref> [2] </ref>, p. 301), and E 2 (p fl P n (p fl p ijk ijk estimators' weighted squared error (Pearson's 2 -distance for multinomial distributions is a standard way of measuring the estimators' deviations from the true probabilities, e.g. [2], p. 233, 301). Note that with confidence level ff, E 2 's upper bound is 2 n1;ff (the ff-quantile of the 2 n1 -distribution, which is independent of c ij !) divided by c ij . This upper bound is proportional to 1 c ij .
Reference: [3] <author> D. A. Cohn. </author> <title> Neural network exploration using optimal experiment design. </title> <editor> In J. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: What is an efficient strategy for acquiring a model of a non-deterministic Markov environment (NME)? Reinforcement driven information acquisition (RDIA), the method described in this paper, extends previous work on "query learning" and "experimental design" (see e.g. [4] for an overview, see <ref> [1, 7, 5, 8, 3] </ref> for more recent contributions) and "active exploration", e.g. [10, 9, 12]. The method combines the notion of information gain with the notion of reinforcement learning. The latter is used to devise exploration strategies that maximize the former. Experiments demonstrate significant advantages of RDIA. <p> This upper bound is proportional to 1 c ij . Decreasing E 2 's upper bound reduces the dispersions of the estimators, which is a central goal of optimal experiment design (e.g. <ref> [3, 4] </ref>). Hence, when it comes to choosing a new experiment, state/action pairs with small counters c ij are preferrable. However, in the case of partly deterministic environments it would be smarter to conduct less "deterministic" experiments than non-deterministic ones.
Reference: [4] <author> V. V. Fedorov. </author> <title> Theory of optimal experiments. </title> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: INTRODUCTION Efficient reinforcement learning requires to model the environment. What is an efficient strategy for acquiring a model of a non-deterministic Markov environment (NME)? Reinforcement driven information acquisition (RDIA), the method described in this paper, extends previous work on "query learning" and "experimental design" (see e.g. <ref> [4] </ref> for an overview, see [1, 7, 5, 8, 3] for more recent contributions) and "active exploration", e.g. [10, 9, 12]. The method combines the notion of information gain with the notion of reinforcement learning. The latter is used to devise exploration strategies that maximize the former. <p> This upper bound is proportional to 1 c ij . Decreasing E 2 's upper bound reduces the dispersions of the estimators, which is a central goal of optimal experiment design (e.g. <ref> [3, 4] </ref>). Hence, when it comes to choosing a new experiment, state/action pairs with small counters c ij are preferrable. However, in the case of partly deterministic environments it would be smarter to conduct less "deterministic" experiments than non-deterministic ones.
Reference: [5] <author> J. Hwang, J. Choi, S. Oh, and R. J. Marks II. </author> <title> Query-based learning applied to partially trained multilayer perceptrons. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(1) </volume> <pages> 131-136, </pages> <year> 1991. </year>
Reference-contexts: What is an efficient strategy for acquiring a model of a non-deterministic Markov environment (NME)? Reinforcement driven information acquisition (RDIA), the method described in this paper, extends previous work on "query learning" and "experimental design" (see e.g. [4] for an overview, see <ref> [1, 7, 5, 8, 3] </ref> for more recent contributions) and "active exploration", e.g. [10, 9, 12]. The method combines the notion of information gain with the notion of reinforcement learning. The latter is used to devise exploration strategies that maximize the former. Experiments demonstrate significant advantages of RDIA.
Reference: [6] <author> L. Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: But with a different environment, curiosity slows down exploitation. The "exploitation/exploration trade-off" remains an open problem. 2. Additional experimental comparisons. It will be interesting to compare RDIA to better competitors than random exploration, like e.g. Kaelbling's Interval Estimation algorithm <ref> [6] </ref>. 3. Function approximators. It also will be interesting to replace the Q-table by function approximators like backprop networks. Previous experimental work by various authors indicates that in certain environments this might improve performance, despite the fact that theoretical foundations of combinations of Q-learning and function approximators are still weak.
Reference: [7] <author> D. J. C. MacKay. </author> <title> Information-based objective functions for active data selection. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 550-604, </pages> <year> 1992. </year>
Reference-contexts: What is an efficient strategy for acquiring a model of a non-deterministic Markov environment (NME)? Reinforcement driven information acquisition (RDIA), the method described in this paper, extends previous work on "query learning" and "experimental design" (see e.g. [4] for an overview, see <ref> [1, 7, 5, 8, 3] </ref> for more recent contributions) and "active exploration", e.g. [10, 9, 12]. The method combines the notion of information gain with the notion of reinforcement learning. The latter is used to devise exploration strategies that maximize the former. Experiments demonstrate significant advantages of RDIA.
Reference: [8] <author> M. Plutowski, G. Cottrell, and H. White. </author> <title> Learning Mackey-Glass from 25 examples, plus or minus 2. </title> <editor> In J. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 1135-1142. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: What is an efficient strategy for acquiring a model of a non-deterministic Markov environment (NME)? Reinforcement driven information acquisition (RDIA), the method described in this paper, extends previous work on "query learning" and "experimental design" (see e.g. [4] for an overview, see <ref> [1, 7, 5, 8, 3] </ref> for more recent contributions) and "active exploration", e.g. [10, 9, 12]. The method combines the notion of information gain with the notion of reinforcement learning. The latter is used to devise exploration strategies that maximize the former. Experiments demonstrate significant advantages of RDIA.
Reference: [9] <author> J. H. Schmidhuber. </author> <booktitle> Curious model-building control systems. In Proc. International Joint Conference on Neural Networks, Singapore, </booktitle> <volume> volume 2, </volume> <pages> pages 1458-1463. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: for acquiring a model of a non-deterministic Markov environment (NME)? Reinforcement driven information acquisition (RDIA), the method described in this paper, extends previous work on "query learning" and "experimental design" (see e.g. [4] for an overview, see [1, 7, 5, 8, 3] for more recent contributions) and "active exploration", e.g. <ref> [10, 9, 12] </ref>. The method combines the notion of information gain with the notion of reinforcement learning. The latter is used to devise exploration strategies that maximize the former. Experiments demonstrate significant advantages of RDIA. Basic set-up / Q-Learning. An agent lives in a NME. <p> Goto 1. MODEL BUILDING WITH RDIA Our agent's task is to build a model of the transition probabilities p ijk . The problem is studied in isolation from goal-directed reinforcement learning tasks: RDIA embodies a kind of "unsupervised reinforcement learning". It extends recent previous work on "active exploration" (e.g. <ref> [10, 9, 12] </ref>). Previous approaches (1) were limited to deterministic environments (they did not address the general problem of learning a model of the statistical properties of a nondeterministic NME), and (2) were based on ad-hoc elements instead of building on concepts from information theory. Collecting ML estimates.
Reference: [10] <author> J. H. Schmidhuber. </author> <title> A possibility for implementing curiosity and boredom in model-building neural controllers. </title> <editor> In J. A. Meyer and S. W. Wilson, editors, </editor> <booktitle> Proc. of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats, </booktitle> <pages> pages 222-227. </pages> <publisher> MIT Press/Bradford Books, </publisher> <year> 1991. </year>
Reference-contexts: for acquiring a model of a non-deterministic Markov environment (NME)? Reinforcement driven information acquisition (RDIA), the method described in this paper, extends previous work on "query learning" and "experimental design" (see e.g. [4] for an overview, see [1, 7, 5, 8, 3] for more recent contributions) and "active exploration", e.g. <ref> [10, 9, 12] </ref>. The method combines the notion of information gain with the notion of reinforcement learning. The latter is used to devise exploration strategies that maximize the former. Experiments demonstrate significant advantages of RDIA. Basic set-up / Q-Learning. An agent lives in a NME. <p> Goto 1. MODEL BUILDING WITH RDIA Our agent's task is to build a model of the transition probabilities p ijk . The problem is studied in isolation from goal-directed reinforcement learning tasks: RDIA embodies a kind of "unsupervised reinforcement learning". It extends recent previous work on "active exploration" (e.g. <ref> [10, 9, 12] </ref>). Previous approaches (1) were limited to deterministic environments (they did not address the general problem of learning a model of the statistical properties of a nondeterministic NME), and (2) were based on ad-hoc elements instead of building on concepts from information theory. Collecting ML estimates.
Reference: [11] <institution> J. Storck. Reinforcement-Lernen und Modellbildung in nicht-deterministischen Umgebun-gen. Fortgeschrittenenpraktikum, Fakultat fur Informatik, Lehrstuhl Prof. Brauer, Tech-nische Universitat Munchen, </institution> <year> 1994. </year>
Reference-contexts: This is illustrated by additional experiments presented in <ref> [11] </ref>: in one environment described therein, exploration helps to speed up exploitation. But with a different environment, curiosity slows down exploitation. The "exploitation/exploration trade-off" remains an open problem. 2. Additional experimental comparisons. It will be interesting to compare RDIA to better competitors than random exploration, like e.g.
Reference: [12] <author> S. Thrun and K. Moller. </author> <title> Active exploration in dynamic environments. </title> <editor> In D. S. Lippman, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 531-538. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: for acquiring a model of a non-deterministic Markov environment (NME)? Reinforcement driven information acquisition (RDIA), the method described in this paper, extends previous work on "query learning" and "experimental design" (see e.g. [4] for an overview, see [1, 7, 5, 8, 3] for more recent contributions) and "active exploration", e.g. <ref> [10, 9, 12] </ref>. The method combines the notion of information gain with the notion of reinforcement learning. The latter is used to devise exploration strategies that maximize the former. Experiments demonstrate significant advantages of RDIA. Basic set-up / Q-Learning. An agent lives in a NME. <p> Goto 1. MODEL BUILDING WITH RDIA Our agent's task is to build a model of the transition probabilities p ijk . The problem is studied in isolation from goal-directed reinforcement learning tasks: RDIA embodies a kind of "unsupervised reinforcement learning". It extends recent previous work on "active exploration" (e.g. <ref> [10, 9, 12] </ref>). Previous approaches (1) were limited to deterministic environments (they did not address the general problem of learning a model of the statistical properties of a nondeterministic NME), and (2) were based on ad-hoc elements instead of building on concepts from information theory. Collecting ML estimates.
Reference: [13] <author> C. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <year> 1989. </year>
Reference-contexts: At time t, the goal is to maximize the discounted sum of future reinforcement P m k=0 fl k R (t + k + 1) (where 0 &lt; fl &lt; 1). We use Watkins' Q-learning <ref> [13] </ref> for this purpose: Q (S; a) is the agent's evaluation (initially zero) corresponding to the state/action pair (S; a). The central loop of the algorithm is as follows: 1. Observe current state S (t). Randomly choose p 2 [0; 1]. If p 2 [0; 1], randomly pick a (t).
References-found: 13

