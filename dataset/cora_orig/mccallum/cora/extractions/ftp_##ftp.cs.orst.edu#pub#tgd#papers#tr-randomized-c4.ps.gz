URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/tr-randomized-c4.ps.gz
Refering-URL: http://www.research.att.com/~schapire/boost.html
Root-URL: 
Email: tgd@cs.orst.edu  Editor:  
Title: An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization  
Author: THOMAS G. DIETTERICH 
Keyword: Decision trees, ensemble learning, bagging, boosting, C4.5, Monte Carlo methods  
Address: Corvallis, OR 97331  
Affiliation: Department of Computer Science, Oregon State University,  
Note: Machine Learning, 1-22 (1998) c 1998 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a "base" learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K. M. </author> <year> (1995). </year> <title> A comparison of methods for learning and combining evidence from multiple models. </title> <type> Tech. rep. 95-47, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution>
Reference: <author> Ali, K. M., & Pazzani, M. J. </author> <year> (1996). </year> <title> Error reduction through learning multiple descriptions. </title> <journal> Machine Learning, </journal> <volume> 24 (3), </volume> <pages> 173-202. </pages>
Reference: <author> Bauer, E., & Kohavi, R. </author> <year> (1997). </year> <title> An empirical comparison of voting classification algorithms: Bagging, boosting, and variants. </title> <type> Tech. rep., </type> <institution> Stanford University, Department of Computer Science. </institution>
Reference: <author> Breiman, L. </author> <year> (1994). </year> <title> Heuristics of instability and stabilization in model selection. </title> <type> Tech. rep. 416, </type> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA. </address>
Reference: <author> Breiman, L. </author> <year> (1996a). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24 (2), </volume> <pages> 123-140. </pages>
Reference: <author> Breiman, L. </author> <year> (1996b). </year> <title> Bias, variance, and arcing classifiers. </title> <type> Tech. rep. 460, </type> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA. </address>
Reference: <author> Dietterich, T. G. </author> <year> (1998). </year> <title> Approximate statistical tests for comparing supervised classification learning algorithms. </title> <booktitle> Neural Computation, </booktitle> <pages> 10. </pages>
Reference-contexts: If this confidence interval does not include zero, then the test concludes that there is a significant difference in performance between the algorithms. However, this test is known <ref> (Dietterich, 1998) </ref> to have elevated type I error (i.e., it will incorrectly find a significant difference more often than the 5% of the time indicated by the confidence level).
Reference: <author> Dietterich, T. G., & Kong, E. B. </author> <year> (1995). </year> <title> Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. </title> <type> Tech. rep., </type> <institution> Department 22 of Computer Science, Oregon State University, Corvallis, Oregon. </institution> <note> Available from ftp://ftp.cs.orst.edu/pub/tgd/papers/tr-bias.ps.gz. </note>
Reference-contexts: This is a very crude randomization technique. One can imagine more sophisticated methods that preferred to select splits with higher information gain. But our goal in this paper is to explore how well this simple method works. In a previous paper <ref> (Dietterich & Kong, 1995) </ref>, we reported promising results for this technique on five tasks. In this paper, we have performed a much more thorough experiment using 33 learning tasks. We compare randomized C4.5 to C4.5 alone, C4.5 with bagging, and C4.5 with Adaboost.M1 (boosting by weighting).
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1996). </year> <title> Experiments with a new boosting algorithm. </title>
Reference: <editor> In Saitta, L. (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 148-156 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R., & Kunz, C. </author> <year> (1997). </year> <title> Option decision trees with majority votes. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 161-169 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Maclin, R., & Opitz, D. </author> <year> (1997). </year> <title> An empirical evaluation of bagging and boosting. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 546-551 Cambridge, MA. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference: <author> Margineantu, D., & Dietterich, T. </author> <year> (1997). </year> <title> Pruning adaptive boosting. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning San Francisco, </booktitle> <address> CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In contrast, Adaboost is not a good choice in such applications. 12 One further way to gain insight into the behavior of these ensemble methods is to construct -error diagrams <ref> (as introduced by Margineantu & Dietterich, 1997) </ref>. These diagrams help visualize the accuracy and diversity of the individual classifiers constructed by the ensemble methods.
Reference: <author> Merz, C. J., & Murphy, P. M. </author> <year> (1996). </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: To implement boosting by weighting, we imported some of the improvements from C4.5 Release 8 for proper handling of continuous splits with weighted training examples. We employed 33 domains drawn from the UCI Repository <ref> (Merz & Murphy, 1996) </ref>. For all but three of the domains (shuttle, satimage, and phoneme), we performed a stratified 10-fold cross-validation to evaluate each of the three ensemble methods (as well as running C4.5 by itself).
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Empirical Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: For each domain and each algorithm configuration (C4.5 alone, randomized C4.5, bagged C4.5, and boosted C4.5), we used the test data to determine whether pruning was needed. Previous research has shown that pruning can make a substantial difference in algorithm performance <ref> (Quinlan, 1993) </ref>, and we did not want to have the pruning decision confound our algorithm comparison. In real applications, the choice of whether to prune could be made based on internal cross-validation within the training set.
Reference: <author> Quinlan, J. R. </author> <year> (1996). </year> <title> Bagging, boosting, </title> <booktitle> and C4.5. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 725-730 Cambridge, MA. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: The second method, boosting by weighting, can be used with base learning algorithms that can accept a weighted training set directly. With such algorithms, the entire training set S (with associated weights) is given to the base learning algorithm. Both methods have been shown to be very effective <ref> (Quinlan, 1996) </ref>. Both boosting and bagging will generate diverse classifiers only if the base learning algorithm is unstable|that is, if small changes to the training set cause large changes in the learned classifier.
References-found: 16

