URL: http://www.cs.monash.edu.au/~jono/TechReports/TR173.dgraph.ps
Refering-URL: http://www.cs.monash.edu.au/~jono/
Root-URL: 
Email: (jono@molly.cs.monash.edu.au)  
Title: Decision Graphs An Extension of Decision Trees  
Author: JONATHAN J. OLIVER 
Address: Clayton, Victoria, 3168, AUSTRALIA  
Affiliation: Department of Computer Science Monash University  
Abstract: Technical Report No: 92/173 (C) Jonathan Oliver 1992 Shortened appeared in AI and Statistics 1993[14] Abstract: In this paper, we examine Decision Graphs, a generalization of decision trees. We present an inference scheme to construct decision graphs using the Minimum Message Length Principle. Empirical tests demonstrate that this scheme compares favourably with other decision tree inference schemes. This work provides a metric for comparing the relative merit of the decision tree and decision graph formalisms for a particular domain. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L.R. Bahl, P.F. Brown, P.V. deSouza, and R.L. Mercer. </author> <title> A tree-based statistical language model for natural language speech recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 37 </volume> <pages> 1001-1008, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction In this paper, we examine the problem of inferring a decision procedure from a set of examples. We examine the decision graph <ref> [5, 1, 16, 15, 14] </ref>, a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) [24, 10, 25]. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20]. <p> If a tree uses high arity attributes (say arity &gt;= 10), then it will quickly fragment the data into many partitions (see Figure 2). The most commonly used method to avoid fragmentation of the training data is to construct subsets of the attribute's values (see Figure 3) <ref> [3, 11, 6, 1, 9] </ref>. 1 A similar argument applies if the subterm (C ^ D) appears higher in the tree than (A ^ B). 2 It has been noted [3, 18] that searching through the possible 2-way subsets is a computationally expensive operation. <p> (C ^ D) given in Figure 1 and Figure 4 are different since the decision tree partitions the object space into 7 categories, while the decision graph partitions the object space into 2 categories. 3 Decision graphs have been previously examined in the literature by [Chou] [5], [Bahl et. al.] <ref> [1] </ref> and [Mahoney and Mooney] [12]. [Mahoney and Mooney] used the following method to construct decision graphs: 1. Grow a decision tree. 2. Search the decision tree for related subtrees. 3. Merge the related subtrees to build a decision graph.
Reference: [2] <author> A.R. </author> <title> Barron and T.M. Cover. Minimum complexity density estimation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37 </volume> <pages> 1034-1054, </pages> <year> 1991. </year> <month> 12 </month>
Reference-contexts: For example, if we have a decision graph G, and we postulate two modifications to G, one which splits a node, the other which joins two nodes, then we can sensibly compare these modifications and say one modification is superior to another. * provides a theoretical guarantee. [Barron and Cover] <ref> [2] </ref> show that if the data is drawn from a population in which the probability distribution over classes can be exactly represented by a decision tree or decision graph function then, given sufficient data, an MML tree or graph inference algorithm is guaranteed to recover that function.
Reference: [3] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction In this paper, we examine the problem of inferring a decision procedure from a set of examples. We examine the decision graph [5, 1, 16, 15, 14], a generalization of the decision tree <ref> [3, 18] </ref>, and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) [24, 10, 25]. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20]. <p> If a tree uses high arity attributes (say arity &gt;= 10), then it will quickly fragment the data into many partitions (see Figure 2). The most commonly used method to avoid fragmentation of the training data is to construct subsets of the attribute's values (see Figure 3) <ref> [3, 11, 6, 1, 9] </ref>. 1 A similar argument applies if the subterm (C ^ D) appears higher in the tree than (A ^ B). 2 It has been noted [3, 18] that searching through the possible 2-way subsets is a computationally expensive operation. <p> used method to avoid fragmentation of the training data is to construct subsets of the attribute's values (see Figure 3) [3, 11, 6, 1, 9]. 1 A similar argument applies if the subterm (C ^ D) appears higher in the tree than (A ^ B). 2 It has been noted <ref> [3, 18] </ref> that searching through the possible 2-way subsets is a computationally expensive operation. If an attribute has arity M , then there will be 2 M1 1 non-trivial 2-way subsets to explore. This approach is therefore infeasible for dealing with high arity attributes. <p> This approach is therefore infeasible for dealing with high arity attributes. For example, with a problem such as protein secondary structure prediction, where an amino acid has 20 possible values, there would be 2 19 1 = 524; 287 2-way subsets to explore. [Breiman et. al.] <ref> [3] </ref> presented a linear time algorithm to find a locally optimal binary partition, when there are only 2 classes. [Chou] [6, 5] presented a fast clustering algorithm that finds locally optimal N-way subsets. However, this method does not offer a method for selection between different sized partitions.
Reference: [4] <author> G.J. Chaitin. </author> <title> On the length of programs for computing finite sequences. </title> <journal> J.A.C.M., </journal> <volume> 13 </volume> <pages> 547-549, </pages> <year> 1966. </year>
Reference-contexts: A theory is unacceptable (shown in Figure 6 (c)) if its explanation of the data is longer than the null explanation. The shortest explanation (shown in Figure 6 (d)) of T and D is considered the best explanation. If the data is totally random (in the sense of Chaitin <ref> [4] </ref>) then the best theory will be the null theory. 2.2 Comparing Theories When given two theories, T 1 and T 2 , about some data we may calculate the message length of the explanations provided by T 1 and T 2 .
Reference: [5] <author> P.A. Chou. </author> <title> Applications of Information Theory to Pattern Recognition and the Design of Decision Trees and Trellises. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: 1 Introduction In this paper, we examine the problem of inferring a decision procedure from a set of examples. We examine the decision graph <ref> [5, 1, 16, 15, 14] </ref>, a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) [24, 10, 25]. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20]. <p> such as protein secondary structure prediction, where an amino acid has 20 possible values, there would be 2 19 1 = 524; 287 2-way subsets to explore. [Breiman et. al.] [3] presented a linear time algorithm to find a locally optimal binary partition, when there are only 2 classes. [Chou] <ref> [6, 5] </ref> presented a fast clustering algorithm that finds locally optimal N-way subsets. However, this method does not offer a method for selection between different sized partitions. <p> (A ^ B) _ (C ^ D) given in Figure 1 and Figure 4 are different since the decision tree partitions the object space into 7 categories, while the decision graph partitions the object space into 2 categories. 3 Decision graphs have been previously examined in the literature by [Chou] <ref> [5] </ref>, [Bahl et. al.] [1] and [Mahoney and Mooney] [12]. [Mahoney and Mooney] used the following method to construct decision graphs: 1. Grow a decision tree. 2. Search the decision tree for related subtrees. 3. Merge the related subtrees to build a decision graph.
Reference: [6] <author> P.A. Chou. </author> <title> Optimal partitioning for classification and regression trees. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(4), </volume> <year> 1991. </year>
Reference-contexts: If a tree uses high arity attributes (say arity &gt;= 10), then it will quickly fragment the data into many partitions (see Figure 2). The most commonly used method to avoid fragmentation of the training data is to construct subsets of the attribute's values (see Figure 3) <ref> [3, 11, 6, 1, 9] </ref>. 1 A similar argument applies if the subterm (C ^ D) appears higher in the tree than (A ^ B). 2 It has been noted [3, 18] that searching through the possible 2-way subsets is a computationally expensive operation. <p> such as protein secondary structure prediction, where an amino acid has 20 possible values, there would be 2 19 1 = 524; 287 2-way subsets to explore. [Breiman et. al.] [3] presented a linear time algorithm to find a locally optimal binary partition, when there are only 2 classes. [Chou] <ref> [6, 5] </ref> presented a fast clustering algorithm that finds locally optimal N-way subsets. However, this method does not offer a method for selection between different sized partitions.
Reference: [7] <institution> P.A. </institution> <type> Chou. Personal Communication. </type> <year> 1992. </year>
Reference-contexts: Both groups applied partitioning algorithms (discussed in Section 1.3) if a split took the structure of the graph being constructed outside the predetermined structure. Fixing the structure of a decision graph a priori is a considerable disadvantage <ref> [7] </ref>, since often the shape of the decision graph provides insight into the domain under consideration. The decision graphs presented in Section 4 highlight this disadvantage. 4 The contribution of this paper is that it applies a minimum encoding technique (MML) to the construction of decision graphs.
Reference: [8] <author> D.L. Dowe, J.J. Oliver, L. Allison, C.S. Wallace, and T.I. Dix. </author> <title> A decision graph explanation of protein secondary structure prediction. </title> <booktitle> In Proceedings of the Hawaii International Conference on System Sciences (HICSS), Biotechnology Computing Track, </booktitle> <pages> pages 669-678, </pages> <year> 1993. </year>
Reference-contexts: Body Shape While ID3 had an error rate of 31% [23], the decision graph inference scheme reconstructed the original function, as shown in Figure 9, and hence had a 0% error rate. 4.2 Protein Data Set We tested decision graphs on some data sets generated from a protein structure database <ref> [8] </ref>. The data consisted of amino acid chains with a secondary structure specified at each point. Microbiologists can determine the amino acid chain of a protein, but finding the secondary structure (f Extended; Helix; Other g) which is related to the shape of the protein is quite difficult.
Reference: [9] <author> U.M. Fayyad and K.B. Irani. </author> <title> The attribute selection problem in decision tree generation. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <pages> pages 104-110, </pages> <year> 1992. </year>
Reference-contexts: If a tree uses high arity attributes (say arity &gt;= 10), then it will quickly fragment the data into many partitions (see Figure 2). The most commonly used method to avoid fragmentation of the training data is to construct subsets of the attribute's values (see Figure 3) <ref> [3, 11, 6, 1, 9] </ref>. 1 A similar argument applies if the subterm (C ^ D) appears higher in the tree than (A ^ B). 2 It has been noted [3, 18] that searching through the possible 2-way subsets is a computationally expensive operation.
Reference: [10] <author> M.P. Georgeff and C.S. Wallace. </author> <title> A general criterion for inductive inference. </title> <booktitle> In Proceedings of the 6th European Conference on Artificial Intelligence, </booktitle> <year> 1984. </year>
Reference-contexts: We examine the decision graph [5, 1, 16, 15, 14], a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) <ref> [24, 10, 25] </ref>. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20]. For the reader unfamiliar with minimum encoding methods (MML and MDL), a good introduction to the area is given by Georgeff [10]. <p> The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20]. For the reader unfamiliar with minimum encoding methods (MML and MDL), a good introduction to the area is given by Georgeff <ref> [10] </ref>. We formalize the problem of inferring a decision procedure from a set of examples as follows. The given data represents a set of objects. Each object is described in terms of the independent variables or "attributes". Each object has a class or "dependent variable" associated with it.
Reference: [11] <author> I. Kononenko, I. Bratko, and E. Roskar. </author> <title> Experiments in automatic learning of medical diagnostic rules. </title> <type> Technical report, </type> <institution> Jozef Stefan Institute, Ljubljana, </institution> <address> Yugoslavia, </address> <year> 1984. </year>
Reference-contexts: If a tree uses high arity attributes (say arity &gt;= 10), then it will quickly fragment the data into many partitions (see Figure 2). The most commonly used method to avoid fragmentation of the training data is to construct subsets of the attribute's values (see Figure 3) <ref> [3, 11, 6, 1, 9] </ref>. 1 A similar argument applies if the subterm (C ^ D) appears higher in the tree than (A ^ B). 2 It has been noted [3, 18] that searching through the possible 2-way subsets is a computationally expensive operation.
Reference: [12] <author> J.J. Mahoney and R.J. Mooney. </author> <title> Initializing ID5R with a domain theory: Some negative results. </title> <type> 91-154, </type> <institution> Department of Computer Science, University of Texas at Austin, Taylor Hall 2.124, Austin, Texas 78712-1188, USA, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Figure 1 and Figure 4 are different since the decision tree partitions the object space into 7 categories, while the decision graph partitions the object space into 2 categories. 3 Decision graphs have been previously examined in the literature by [Chou] [5], [Bahl et. al.] [1] and [Mahoney and Mooney] <ref> [12] </ref>. [Mahoney and Mooney] used the following method to construct decision graphs: 1. Grow a decision tree. 2. Search the decision tree for related subtrees. 3. Merge the related subtrees to build a decision graph. <p> The decision graphs presented in Section 4 highlight this disadvantage. 4 The contribution of this paper is that it applies a minimum encoding technique (MML) to the construction of decision graphs. The method described in this paper avoids the problems encountered by [Mahoney and Mooney] <ref> [12] </ref> since it does not search through decision trees for related subtrees.
Reference: [13] <author> C.J. Mathues and L.A. Rendell. </author> <title> Constructive induction on decision trees. </title> <booktitle> In IJCAI-89, </booktitle> <pages> pages 645-650, </pages> <year> 1989. </year>
Reference-contexts: The reason for this duplication is the disjunction in the proposition. In general, conjunctions can be described efficiently by decision trees, while disjunctions require a large tree to describe. This representational shortcoming of decision trees has been identified by [Pagallo and Haussler] [17] and [Mathues and Rendell] <ref> [13] </ref>, and has been termed the replication problem. [Pagallo and Haussler] demonstrate that many boolean functions with small DNF (disjunctive normal form) descriptions will be inefficiently described by decision trees [17]. <p> One solution to the replication problem is to allow decision nodes to contain features that are a function of one or more attributes. A feature consists of a conjunction of literals (where each literal is an attribute or a negated attribute). [Pagallo and Haussler] [17] and [Mathues and Rendell] <ref> [13] </ref> construct decision trees, and analyse the tree to determine likely candidate features. They then add these features to the list of attributes and build another tree. Both groups repeat this process until no more features can be added.
Reference: [14] <author> J.J. Oliver. </author> <title> Decision graphs an extension of decision trees. </title> <booktitle> In Proceedings of the Fourth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 343-350, </pages> <year> 1993. </year> <note> More extensive version available as TR 173, </note> <institution> Computer Science Department, Monash University, </institution> <address> Vic 3168, AUSTRALIA. </address>
Reference-contexts: 1 Introduction In this paper, we examine the problem of inferring a decision procedure from a set of examples. We examine the decision graph <ref> [5, 1, 16, 15, 14] </ref>, a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) [24, 10, 25]. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20].
Reference: [15] <author> J.J. Oliver, D.L. Dowe, and C.S. Wallace. </author> <title> Inferring decision graphs using the minimum message length principle. </title> <editor> In A. Adams and L. Sterling, editors, </editor> <booktitle> Proceedings of the 5th Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 361-367. </pages> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction In this paper, we examine the problem of inferring a decision procedure from a set of examples. We examine the decision graph <ref> [5, 1, 16, 15, 14] </ref>, a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) [24, 10, 25]. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20].
Reference: [16] <author> J.J. Oliver and C.S. Wallace. </author> <title> Inferring decision graphs. </title> <booktitle> In Proceedings of Workshop 8 | Evaluating and Changing Representation in Machine Learning IJCAI-91, </booktitle> <year> 1991. </year> <note> Also available as TR 170, </note> <institution> Computer Science Department, Monash University, </institution> <address> Vic 3168, AUSTRALIA. </address>
Reference-contexts: 1 Introduction In this paper, we examine the problem of inferring a decision procedure from a set of examples. We examine the decision graph <ref> [5, 1, 16, 15, 14] </ref>, a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) [24, 10, 25]. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20]. <p> Procedures for encoding decision trees have been described in [Wallace and Patrick] [25] and [Quinlan and Rivest] [20]. A procedure for encoding decision graphs has been described in [Oliver and Wallace] <ref> [16] </ref>. 2.2.3 Lookahead The hill climbing algorithm proposed in Figure 7 only considers one step modifications. Thus, the algorithm may get stuck in a local minimum, and not find the global minimum. <p> In these cases, the decision graph scheme used the MML metric to determine that a decision tree was more appropriate for this data set than a decision graph. In this section, we describe how this determination was made. The message length function described in [Oliver and Wallace] <ref> [16] </ref> has a parameter, P J the probability of a Join, which describes how disjunctive a decision graph is. P J effects the message length of a given decision graph.
Reference: [17] <author> G. Pagallo and D. Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: The reason for this duplication is the disjunction in the proposition. In general, conjunctions can be described efficiently by decision trees, while disjunctions require a large tree to describe. This representational shortcoming of decision trees has been identified by [Pagallo and Haussler] <ref> [17] </ref> and [Mathues and Rendell] [13], and has been termed the replication problem. [Pagallo and Haussler] demonstrate that many boolean functions with small DNF (disjunctive normal form) descriptions will be inefficiently described by decision trees [17]. <p> This representational shortcoming of decision trees has been identified by [Pagallo and Haussler] <ref> [17] </ref> and [Mathues and Rendell] [13], and has been termed the replication problem. [Pagallo and Haussler] demonstrate that many boolean functions with small DNF (disjunctive normal form) descriptions will be inefficiently described by decision trees [17]. The effect of the replication problem is that decision tree learning algorithms require a large amount of data to learn disjunctive functions. <p> One solution to the replication problem is to allow decision nodes to contain features that are a function of one or more attributes. A feature consists of a conjunction of literals (where each literal is an attribute or a negated attribute). [Pagallo and Haussler] <ref> [17] </ref> and [Mathues and Rendell] [13] construct decision trees, and analyse the tree to determine likely candidate features. They then add these features to the list of attributes and build another tree. Both groups repeat this process until no more features can be added.
Reference: [18] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction In this paper, we examine the problem of inferring a decision procedure from a set of examples. We examine the decision graph [5, 1, 16, 15, 14], a generalization of the decision tree <ref> [3, 18] </ref>, and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) [24, 10, 25]. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20]. <p> used method to avoid fragmentation of the training data is to construct subsets of the attribute's values (see Figure 3) [3, 11, 6, 1, 9]. 1 A similar argument applies if the subterm (C ^ D) appears higher in the tree than (A ^ B). 2 It has been noted <ref> [3, 18] </ref> that searching through the possible 2-way subsets is a computationally expensive operation. If an attribute has arity M , then there will be 2 M1 1 non-trivial 2-way subsets to explore. This approach is therefore infeasible for dealing with high arity attributes.
Reference: [19] <author> J.R. Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man Machine Studies, </journal> <pages> pages 221-234, </pages> <year> 1987. </year>
Reference-contexts: We compared it with C4 with Pessimistic Pruning <ref> [19] </ref>, and with two tree generation schemes that use the Minimum Message Length Principle, one by [Quinlan and Rivest] [20] and the other by [Wallace and Patrick] [25]. Results are given for five data sets "Hypo", "Discordant", "LED", "Endgame" and "xd6" in Table 1. <p> Results are given for five data sets "Hypo", "Discordant", "LED", "Endgame" and "xd6" in Table 1. These data sets are described in [Quinlan] <ref> [19] </ref>. 4.1 Learning Disjunctive Concepts When we tested our algorithm upon the "xd6" data set we found the Join operator was used extensively.
Reference: [20] <author> J.R. Quinlan and R.L. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: We examine the decision graph [5, 1, 16, 15, 14], a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) [24, 10, 25]. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) <ref> [21, 22, 20] </ref>. For the reader unfamiliar with minimum encoding methods (MML and MDL), a good introduction to the area is given by Georgeff [10]. We formalize the problem of inferring a decision procedure from a set of examples as follows. The given data represents a set of objects. <p> Procedures for encoding decision trees have been described in [Wallace and Patrick] [25] and [Quinlan and Rivest] <ref> [20] </ref>. A procedure for encoding decision graphs has been described in [Oliver and Wallace] [16]. 2.2.3 Lookahead The hill climbing algorithm proposed in Figure 7 only considers one step modifications. Thus, the algorithm may get stuck in a local minimum, and not find the global minimum. <p> We go on in Section 3.3 to adapt the Hill Climbing Algorithm given in Figure 7 to design a decision graph inference algorithm. 3.1 An Overview of a MMLP Decision Tree Generation Algorithm [Quinlan and Rivest] <ref> [20] </ref> proposed an algorithm for the generation of decision trees that consists of two phases, a Growing Phase and a Pruning Phase. The Growing Phase begins with the root of the decision tree being a leaf with all of the training set associated with it. <p> We compared it with C4 with Pessimistic Pruning [19], and with two tree generation schemes that use the Minimum Message Length Principle, one by [Quinlan and Rivest] <ref> [20] </ref> and the other by [Wallace and Patrick] [25]. Results are given for five data sets "Hypo", "Discordant", "LED", "Endgame" and "xd6" in Table 1.
Reference: [21] <author> J. Rissanen. </author> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11 </volume> <pages> 416-431, </pages> <year> 1983. </year>
Reference-contexts: We examine the decision graph [5, 1, 16, 15, 14], a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) [24, 10, 25]. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) <ref> [21, 22, 20] </ref>. For the reader unfamiliar with minimum encoding methods (MML and MDL), a good introduction to the area is given by Georgeff [10]. We formalize the problem of inferring a decision procedure from a set of examples as follows. The given data represents a set of objects.
Reference: [22] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference-contexts: We examine the decision graph [5, 1, 16, 15, 14], a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) [24, 10, 25]. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) <ref> [21, 22, 20] </ref>. For the reader unfamiliar with minimum encoding methods (MML and MDL), a good introduction to the area is given by Georgeff [10]. We formalize the problem of inferring a decision procedure from a set of examples as follows. The given data represents a set of objects.
Reference: [23] <editor> S.B. Thrun et al. </editor> <title> The monk's problems: A performance comparison of different learning algorithms. </title> <institution> CMU-CS-91-197, Carnegie Mellon University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Furthermore, the decision graph produced a decision procedure that was easier to identify from a human's perspective. We had similar success with the 1 st Monk's data set (reported in <ref> [23] </ref>). This data set is an artificial data set constructed from the function: J acket Color = Red _ Head Shape = Body Shape While ID3 had an error rate of 31% [23], the decision graph inference scheme reconstructed the original function, as shown in Figure 9, and hence had a <p> We had similar success with the 1 st Monk's data set (reported in <ref> [23] </ref>). This data set is an artificial data set constructed from the function: J acket Color = Red _ Head Shape = Body Shape While ID3 had an error rate of 31% [23], the decision graph inference scheme reconstructed the original function, as shown in Figure 9, and hence had a 0% error rate. 4.2 Protein Data Set We tested decision graphs on some data sets generated from a protein structure database [8].
Reference: [24] <author> C.S. Wallace and D.M. Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11 </volume> <pages> 185-194, </pages> <year> 1968. </year>
Reference-contexts: We examine the decision graph [5, 1, 16, 15, 14], a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) <ref> [24, 10, 25] </ref>. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20]. For the reader unfamiliar with minimum encoding methods (MML and MDL), a good introduction to the area is given by Georgeff [10].
Reference: [25] <author> C.S. Wallace and J.D. Patrick. </author> <title> Coding decision trees. </title> <note> To appear in Machine Learning, 1993. Also available as TR 153, </note> <institution> Computer Science Department, Monash University, </institution> <address> Vic 3168, AUSTRALIA. </address> <month> 13 </month>
Reference-contexts: We examine the decision graph [5, 1, 16, 15, 14], a generalization of the decision tree [3, 18], and propose a method to construct decision graphs based upon Wallace's Minimum Message Length Principle (MMLP) <ref> [24, 10, 25] </ref>. The MMLP is related to Rissanen's Minimum Description Length Principle (MDLP) [21, 22, 20]. For the reader unfamiliar with minimum encoding methods (MML and MDL), a good introduction to the area is given by Georgeff [10]. <p> Procedures for encoding decision trees have been described in [Wallace and Patrick] <ref> [25] </ref> and [Quinlan and Rivest] [20]. A procedure for encoding decision graphs has been described in [Oliver and Wallace] [16]. 2.2.3 Lookahead The hill climbing algorithm proposed in Figure 7 only considers one step modifications. <p> We compared it with C4 with Pessimistic Pruning [19], and with two tree generation schemes that use the Minimum Message Length Principle, one by [Quinlan and Rivest] [20] and the other by [Wallace and Patrick] <ref> [25] </ref>. Results are given for five data sets "Hypo", "Discordant", "LED", "Endgame" and "xd6" in Table 1. These data sets are described in [Quinlan] [19]. 4.1 Learning Disjunctive Concepts When we tested our algorithm upon the "xd6" data set we found the Join operator was used extensively. <p> In general, when there are N leaves, the Grow Graph procedure will explore N fi (N1) 2 possible joins. 10 5 Determining the Type of Model The reader will note that the decision graph scheme often gave results identical to the decision tree scheme proposed by [Wallace and Patrick] <ref> [25] </ref>. In these cases, the decision graph scheme used the MML metric to determine that a decision tree was more appropriate for this data set than a decision graph. In this section, we describe how this determination was made.
References-found: 25

