URL: http://www.cse.ogi.edu/~denni/Publications/mlpjets.ps.Z
Refering-URL: http://www.cse.ogi.edu/~denni/publications.html
Root-URL: http://www.cse.ogi.edu
Title: Using Neural Networks to Identify Jets  
Author: Leif Lonnblad Carsten Peterson and Thorsteinn Rognvaldsson 
Note: Nuclear Physics B  PACS numbers: 13.65.+i, 12.38Qk, 13.87.Fh 1 thepll@seldc52 (bitnet) leif@thep.lu.se (internet) 2 thepcap@seldc52 (bitnet) carsten@thep.lu.se (internet) 3 thepdr@seldc52 (bitnet) denni@thep.lu.se (internet)  
Address: Solvegatan 14A, S-22362 Lund, Sweden  
Affiliation: Department of Theoretical Physics, University of Lund  
Date: May 1990  349, 675 (1991)  
Abstract: A neural network method for identifying the ancestor of a hadron jet is presented. The idea is to find an efficient mapping between certain observed hadronic kinematical variables and the quark/gluon identity. This is done with a neuronic expansion in terms of a network of sigmoidal functions using a gradient descent procedure, where the errors are back-propagated through the network. With this method we are able to separate gluon from quark jets originating from Monte Carlo generated e + e events with ~ 85% accuracy. The result is independent on the MC model used. This approach for isolating the gluon jet is then used to study the so-called string effect. In addition, heavy quarks (b and c) in e + e reactions can be identified on the 50% level by just observing the hadrons. In particular we are able to separate b-quarks with an efficiency and purity, which is comparable with what is expected from vertex detectors. We also speculate on how the neural network method can be used to disentangle different hadronization schemes by compressing the dimensionality of the state space of hadrons. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <title> "Finding Gluon Jets with a Neural Trigger", </title> <note> LU-TP 90-3, submitted to Physical Review Letters. </note>
Reference-contexts: High energy physics contain many feature recognition problems, ranging from low-level trigger conditions in experimental setups, to extraction of theoretically relevant quantities in collected data. Needless to say, the demand for efficient feature extraction procedures will become more acute with increasing luminosity and energy. In a previous paper <ref> [1] </ref> preliminary results for gluon/quark separation in e + e reactions using the neural network approach by only considering energy and momentum information from the leading particles were reported. <p> More elaborate schemes have been suggested [14, 15, 16] with improved performance (~ 70 75%) as a result. Here we will use the back-propagation learning algorithm to do quark/gluon identification. Preliminary encouraging results with this method were reported in <ref> [1] </ref> using the ARIADNE Monte Carlo. <p> In order to check the model independence of our approach we have also made "mixed" runs, where we train on one MC model and test on another (see table 2). As can be 4 The numbers reported here are slightly lower than those in ref. <ref> [1] </ref>, since we here also allow for the possibility of secondary quarks. 7 data set with the L4 input option. (b). The first epoch displayed in detail. seen the approach is indeed very model independent. Two important questions need to be answered.
Reference: [2] <author> D. E. Rumelhart, G. E. Hinton and R. J. Williams, </author> <title> "Learning Internal Representations by Error Propagation", </title> <editor> in D. E. Rumelhart and J. L. McClelland (Eds.) </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition (Vol. </booktitle> <volume> 1), </volume> <publisher> MIT Press (1986). </publisher>
Reference-contexts: The adjustment of parameters , or learning, is done with a gradient descent method, e.g. back-propagation <ref> [2] </ref>. This paper is organized as follows. In Sect. 2 we briefly describe the back-propagation learning algorithm. Its derivation and details on parameter choices etc. can be found in Appendix A. <p> A frequently used procedure for accomplishing this is the back-propagation learning rule <ref> [2] </ref> where the least mean square error function E = 2 p i (p) (p) is minimized. Changing ! ij by gradient descent corresponds to [2] (see Appendix A) ! ij = ffi i h j + ff! old for the hidden to the output layer, where ffi i is given <p> A frequently used procedure for accomplishing this is the back-propagation learning rule <ref> [2] </ref> where the least mean square error function E = 2 p i (p) (p) is minimized. Changing ! ij by gradient descent corresponds to [2] (see Appendix A) ! ij = ffi i h j + ff! old for the hidden to the output layer, where ffi i is given by ffi i = (y i t i )g 0 (a i ) (7) Correspondingly for the input to the hidden layer one has !
Reference: [3] <author> L. Lonnblad, C. Peterson and T. Rognvaldsson, </author> <note> JETNET 1.0 program and manual [Available via BITNET request]. </note>
Reference-contexts: This paper is organized as follows. In Sect. 2 we briefly describe the back-propagation learning algorithm. Its derivation and details on parameter choices etc. can be found in Appendix A. Appendix B contains a brief description of a F77 software package, JETNET 1.0 <ref> [3] </ref>, that implements the back-propagation learning algorithm for jet identification studies. The MC models used to generate the events and the jet clustering algorithm are described in Sect. 3. Sect. 4 contains the gluon/quark and heavy quark identification in e + e -reactions. <p> This does not mean that two hidden layers necessarily is the optimal for every classification problem. More than two hidden layers may well lead to a solution with fewer units in all, or speed up learning. The option of several hidden layers is included in JETNET 1.0 <ref> [3] </ref>. 25 Appendix B All analysis in this paper were performed using the FORTRAN 77 subroutine package JETNET 1.0 developed by the authors. It implements all features described in the previous section. It is available on request via BITNET. The package includes a manual and examples of programs using JETNET.
Reference: [4] <author> D. E. </author> <note> Rumelhart (unpublished). </note>
Reference-contexts: However, it would be advantageous to have a more algorithmic method available, since one could then also analyze how the network captures the different features of the data set. One 3 such pruning procedure goes as follows <ref> [4] </ref>: Add to the error function (eq. (5)) a complexity term [4] E ! E + ij ij ij where the sum extends over all weights. For large weights j! ij j, the cost is , whereas for small weights it is zero. <p> However, it would be advantageous to have a more algorithmic method available, since one could then also analyze how the network captures the different features of the data set. One 3 such pruning procedure goes as follows <ref> [4] </ref>: Add to the error function (eq. (5)) a complexity term [4] E ! E + ij ij ij where the sum extends over all weights. For large weights j! ij j, the cost is , whereas for small weights it is zero. Hence the network gets pruned to only contain weights that are really needed to represent the problem.
Reference: [5] <author> L. Lonnblad, "ARIADNE-3, </author> <title> A Monte Carlo for QCD Cascades in the Colour Dipole Formulation", </title> <type> Lund preprint LU TP 89-10. </type>
Reference-contexts: We stress that this procedure should only be done in a "research mode" and not when doing the final neural network learning. 3 The Monte Carlo Data 3.1 The Models The Monte Carlo data used in our analysis were generated with three different generators; ARIADNE 3.1 <ref> [5] </ref>, HERWIG 3.4 [6] and JETSET 7.2 [7]. All three generators are able to give a good description of event shapes, multiplicities etc. at LEP energies [8], although they differ substantially in the detailed description of e.g. heavy quark fragmentation.
Reference: [6] <author> G. Marchesini and B. R. </author> <type> Webber, </type> <institution> Nucl. Phys. B310 (1988) 461; I. G. Knowles, Nucl. Phys. </institution> <month> B310 </month> <year> (1988) </year> <month> 571. </month>
Reference-contexts: We stress that this procedure should only be done in a "research mode" and not when doing the final neural network learning. 3 The Monte Carlo Data 3.1 The Models The Monte Carlo data used in our analysis were generated with three different generators; ARIADNE 3.1 [5], HERWIG 3.4 <ref> [6] </ref> and JETSET 7.2 [7]. All three generators are able to give a good description of event shapes, multiplicities etc. at LEP energies [8], although they differ substantially in the detailed description of e.g. heavy quark fragmentation. <p> However they all include coherence effects due to soft and colinear gluon emission, so even if the implementation is quite different (eg. ARIADNE uses a colour dipole cascade [10] whereas HERWIG and JETSET uses partonic cascades <ref> [6, 11] </ref>) the observable differences are quite small. All options and parameters in each generator were set to their default values when the event samples for the neural network analysis were produced. 3.2 Clustering the Data After generating an event, the jets are defined using a clustering algorithm.
Reference: [7] <author> T. Sjostrand, </author> <title> JETSET 7.2 program and manual. </title> <editor> See B. Bambah et al., </editor> <title> QCD Generators for LEP, </title> <publisher> CERN-TH.5466/89. </publisher>
Reference-contexts: this procedure should only be done in a "research mode" and not when doing the final neural network learning. 3 The Monte Carlo Data 3.1 The Models The Monte Carlo data used in our analysis were generated with three different generators; ARIADNE 3.1 [5], HERWIG 3.4 [6] and JETSET 7.2 <ref> [7] </ref>. All three generators are able to give a good description of event shapes, multiplicities etc. at LEP energies [8], although they differ substantially in the detailed description of e.g. heavy quark fragmentation.
Reference: [8] <editor> OPAL Collaboration, M. Z. Akrawy et. al., </editor> <title> "A Measurement of Global Event Shape Distributions in the Hadronic Decays of the Z 0 ", CERN Preprint, </title> <publisher> CERN-EP/90-48. </publisher>
Reference-contexts: All three generators are able to give a good description of event shapes, multiplicities etc. at LEP energies <ref> [8] </ref>, although they differ substantially in the detailed description of e.g. heavy quark fragmentation. The main difference is that HERWIG uses a cluster fragmentation scheme whereas JETSET and ARIADNE uses the Lund string fragmentation [9] as it is implemented in JETSET.
Reference: [9] <author> B. Andersson and G. Gustafson, Z. Phys. C3 (1980) 223; B. Andersson, G. Gustafson, G. Ingelman, T. Sjostrand, </author> <title> Phys. </title> <type> Rep. </type> <month> 97 </month> <year> (1983) </year> <month> 31. </month>
Reference-contexts: The main difference is that HERWIG uses a cluster fragmentation scheme whereas JETSET and ARIADNE uses the Lund string fragmentation <ref> [9] </ref> as it is implemented in JETSET. Among other things this results in softer fragmentation functions for heavy quarks in HERWIG. The generators also differ in the treatment of the perturbative stage of the jet evolution. <p> At 29 GeV where the string effect is dominated by the hadronization process, this is easily explained within the Lund model <ref> [9] </ref> simply by looking at the Lorentz boost back to the events c.m. after the string has been fragmented in its c.m. At higher energies this effect is reduced as the string effect becomes more dominated by the perturbative phase of the jet evolution [24].
Reference: [10] <author> G. </author> <title> Gustafson, </title> <journal> Phys. Lett. </journal> <note> B175 (1986) 453; G. </note> <author> Gustafson, U. </author> <type> Pettersson, </type> <institution> Nucl. Phys. </institution> <note> B306 (1988) 746; B. </note> <author> Andersson, G. Gustafson, L. Lonnblad, </author> <title> "Gluon Splitting in the Colour Dipole Formulation", </title> <note> Lund Preprint LU TP 89-9 ( to be published in Nucl. Phys.B). </note>
Reference-contexts: The generators also differ in the treatment of the perturbative stage of the jet evolution. However they all include coherence effects due to soft and colinear gluon emission, so even if the implementation is quite different (eg. ARIADNE uses a colour dipole cascade <ref> [10] </ref> whereas HERWIG and JETSET uses partonic cascades [6, 11]) the observable differences are quite small.
Reference: [11] <author> M. Bengtsson and T. </author> <title> Sjostrand, </title> <journal> Phys. Lett. </journal> <note> B185 (1987) 453; Nucl. Phys. B289 (1987) 810. </note>
Reference-contexts: However they all include coherence effects due to soft and colinear gluon emission, so even if the implementation is quite different (eg. ARIADNE uses a colour dipole cascade [10] whereas HERWIG and JETSET uses partonic cascades <ref> [6, 11] </ref>) the observable differences are quite small. All options and parameters in each generator were set to their default values when the event samples for the neural network analysis were produced. 3.2 Clustering the Data After generating an event, the jets are defined using a clustering algorithm.
Reference: [12] <editor> JADE Collaboration, W. Bartel et. al., Z. Phys. </editor> <month> C21 </month> <year> (1983) </year> <month> 37. </month>
Reference-contexts: It can shed light on the hadronization mechanism. For example experimental studies of the so-called string effect <ref> [12] </ref> needs identification of the gluon jet. Also a fairly precise identification of the gluon jet is required for establishing the existence of the 3-gluon coupling in e + e -annihilation [13, 14]. To date the gluon jet identification has been done by making various cuts on the kinematic variables. <p> To date the gluon jet identification has been done by making various cuts on the kinematic variables. The most straightforward one is to select the jet with smallest energy as the gluon <ref> [12] </ref>. This procedure, which is based on the underlying perturbative QCD matrix element, typically yields ~ 65% identification rate. More elaborate schemes have been suggested [14, 15, 16] with improved performance (~ 70 75%) as a result. Here we will use the back-propagation learning algorithm to do quark/gluon identification. <p> This striking property is the fact that the gluon jets often have smaller energy as predicted by the leading order QCD matrix element and utilized in ref. <ref> [12] </ref>. But the network is more subtle than just making an energy cut. The way the network solves the problem is very transparent if we look at the solution when the network is fed with a reduced 2-dimensional input, E jet and p jet . <p> Nevertheless we have included selective fields as an option in JETNET 1.0. 6 The String Effect Revisited Using the NN approach to identify gluon jets when measuring the string effect <ref> [12, 23] </ref> is potentially very dangerous. It is very likely that the network has already used the string effect when learning to recognize the gluon, in which case the measurements will be very model dependent. <p> In table 7 we also present results for the ratio r when looking only at K-mesons (and heavier hadrons). In ref. <ref> [12] </ref> it was shown that the string effect is strongly dependent on the particle masses, hence the ratio r is larger for heavy particles than for lighter.
Reference: [13] <author> M. Bengtsson and P. </author> <title> Zerwas, </title> <journal> Phys. Lett. </journal> <note> B208 (1988) 306. </note>
Reference-contexts: It can shed light on the hadronization mechanism. For example experimental studies of the so-called string effect [12] needs identification of the gluon jet. Also a fairly precise identification of the gluon jet is required for establishing the existence of the 3-gluon coupling in e + e -annihilation <ref> [13, 14] </ref>. To date the gluon jet identification has been done by making various cuts on the kinematic variables. The most straightforward one is to select the jet with smallest energy as the gluon [12].
Reference: [14] <author> Z. </author> <title> Fodor, </title> <journal> Phys. Rev. </journal> <volume> D40, </volume> <year> (1989) </year> <month> 3590. </month>
Reference-contexts: It can shed light on the hadronization mechanism. For example experimental studies of the so-called string effect [12] needs identification of the gluon jet. Also a fairly precise identification of the gluon jet is required for establishing the existence of the 3-gluon coupling in e + e -annihilation <ref> [13, 14] </ref>. To date the gluon jet identification has been done by making various cuts on the kinematic variables. The most straightforward one is to select the jet with smallest energy as the gluon [12]. <p> The most straightforward one is to select the jet with smallest energy as the gluon [12]. This procedure, which is based on the underlying perturbative QCD matrix element, typically yields ~ 65% identification rate. More elaborate schemes have been suggested <ref> [14, 15, 16] </ref> with improved performance (~ 70 75%) as a result. Here we will use the back-propagation learning algorithm to do quark/gluon identification. Preliminary encouraging results with this method were reported in [1] using the ARIADNE Monte Carlo.
Reference: [15] <author> Y. K. Kim et. </author> <title> al.,Phys. </title> <journal> Rev. Lett. </journal> <volume> 63 (1989) 1772. </volume> <pages> 29 </pages>
Reference-contexts: The most straightforward one is to select the jet with smallest energy as the gluon [12]. This procedure, which is based on the underlying perturbative QCD matrix element, typically yields ~ 65% identification rate. More elaborate schemes have been suggested <ref> [14, 15, 16] </ref> with improved performance (~ 70 75%) as a result. Here we will use the back-propagation learning algorithm to do quark/gluon identification. Preliminary encouraging results with this method were reported in [1] using the ARIADNE Monte Carlo.
Reference: [16] <author> L. Jones, </author> <title> "Towards A Systematic Jet Classification", </title> <institution> University of Illinios preprint ILL-(TH)-90-01. </institution>
Reference-contexts: The most straightforward one is to select the jet with smallest energy as the gluon [12]. This procedure, which is based on the underlying perturbative QCD matrix element, typically yields ~ 65% identification rate. More elaborate schemes have been suggested <ref> [14, 15, 16] </ref> with improved performance (~ 70 75%) as a result. Here we will use the back-propagation learning algorithm to do quark/gluon identification. Preliminary encouraging results with this method were reported in [1] using the ARIADNE Monte Carlo.
Reference: [17] <author> See e.g. R. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons (1973). </publisher>
Reference-contexts: Two important questions need to be answered. How well can the network in principle perform given a certain data set? What characteristics in (or correlations) does the network utilize? The Bayesian limit. The upper limit of performance is set by the Bayesian limit <ref> [17] </ref>, which is given by the minimal overlap between the two multidimensional distributions.
Reference: [18] <author> C. Peterson and E. Hartman, </author> <title> "Explorations of the Mean Field Theory Learning Algorithm", </title> <booktitle> Neural Networks, </booktitle> <month> 2 </month> <year> (1989) </year> <month> 475. </month>
Reference-contexts: As can be seen from table 3 the neural network prediction is just a few % below the theoretical limit. We feel confident that with proper adjustments and parameter fine-tuning of the learning algorithm the theoretical limit can be reached. We have tried using a "Manhattan" <ref> [18] </ref> updating algorithm (see Appendix A) when training the network since it has been shown to work better on inconsistent data sets but we found no improvement. Correlations used by the network. <p> It is then advisable to have more than one training pass between the updates. In all our applications we have chosen this updating frequency to be 10 patterns/update. Manhattan updating.In cases where there are many patterns/update it might be beneficial to replace the standard gradient descent with "Manhattan" updating <ref> [18] </ref> ! ij = sign [ @E ] (A14) In this case the learning is bounded and it is easier to find an appropriate value for , which should decrease with increasing learning.
Reference: [19] <author> O. </author> <title> Barring (The DELPHI Collaboration), </title> <type> private communication. </type>
Reference-contexts: The network was trained to find which one of three jets that is the gluon jet. MC models have been processed through the DELPHI detector simulator <ref> [19] </ref>. We find only a modest degradation in performance (~3 %), which is very encouraging. 4.2 Identifying Heavy Quarks Identifying heavy quarks from light quarks is another important pattern recognition task.
Reference: [20] <author> B. A. Kniehl, J. H. Kuhn, and R. G. Stuart, </author> <note> in Polarization at LEP (Vol. 1), CERN-88-06. </note>
Reference-contexts: On the other hand the purity levels are high. A more ambitious (and expensive) approach is to use a vertex detector to find the secondary vertex. With such a device one expects about 25% tagging efficiency for heavy quarks (c+b) with 20% background from light quarks <ref> [20] </ref>.
Reference: [21] <author> B. Denby et. al., </author> <title> "Neural Networks for Triggering", </title> <publisher> FERMILAB-CONF-90/20. </publisher>
Reference-contexts: Also a neural network model could be employed to select b-quarks decaying semileptonically as demonstrated in <ref> [21] </ref> using calorimeter signals. An interesting question is to what extent the identified b-quark jets contain -mesons.
Reference: [22] <author> G. Arnison et. al. </author> <title> (UA1 collaboration), "Analysis of the Fragmentation Properties of Quark and Gluon Jets at the CERN SPS p(p) Collider", </title> <institution> Nucl. Phys. </institution> <month> B276 </month> <year> (1986) </year> <month> 253-271. </month>
Reference-contexts: If the objective of b-tagging is to measure b b mixing then one needs to know the charge of the b ( b) jets. Without leptonic information this can be done fairly reliably 16 by considering various weighted charge measures <ref> [22] </ref>, i.e. Q 4 = i 1=3 where i runs over the L4 set and z = E i =E jet . 5 Large p T Production In hadron-hadron collisions the situation is different due to the lack of knowledge about the event structure.
Reference: [23] <author> B. Andersson, G. Gustafson, T. </author> <title> Sjostrand, </title> <journal> Phys. Lett. </journal> <note> B94 (1980) 211. </note>
Reference-contexts: Nevertheless we have included selective fields as an option in JETNET 1.0. 6 The String Effect Revisited Using the NN approach to identify gluon jets when measuring the string effect <ref> [12, 23] </ref> is potentially very dangerous. It is very likely that the network has already used the string effect when learning to recognize the gluon, in which case the measurements will be very model dependent.
Reference: [24] <author> Ya. I. Azimov, Yu. L. Dokshitzer, S. I. Troyan, V. A. Khoze, Sovj. J. </author> <type> Nucl. </type> <institution> Phys. </institution> <month> 43 </month> <year> (1986) </year> <month> 91. </month>
Reference-contexts: At higher energies this effect is reduced as the string effect becomes more dominated by the perturbative phase of the jet evolution <ref> [24] </ref>. A recent study [25] indicates that this becomes important already at 92 GeV.
Reference: [25] <author> V. A. Khoze and L. </author> <title> Lonnblad, </title> <journal> Phys. Lett. </journal> <note> B241 (1990) 123. </note>
Reference-contexts: At higher energies this effect is reduced as the string effect becomes more dominated by the perturbative phase of the jet evolution [24]. A recent study <ref> [25] </ref> indicates that this becomes important already at 92 GeV.
Reference: [26] <author> J. J. Hopfield and D. W. Tank, </author> <title> "Neural Computation of Decisions in Optimization Problems", </title> <booktitle> Biological Cybernetics 52 (1985) 141. </booktitle>
Reference-contexts: Needless to say, the success rate of the NN method will vary with different detectors. Much detailed work with various detectors needs to be done. One should mention that neural networks have also shown great promise for solving difficult optimization problems <ref> [26, 27] </ref>. Also in this application area the NN approach could be fruitful for high energy physics in real time track-finding. [28, 29]. Acknowledgements: We have benefitted from discussions with O. Barring, T. Sjostrand and P. M.
Reference: [27] <author> C. Peterson and B. Soderberg, </author> <title> "A New Method for Mapping Optimization Problems onto Neural Networks", </title> <booktitle> International Journal of Neural Systems 1 (1989) 3. </booktitle>
Reference-contexts: Needless to say, the success rate of the NN method will vary with different detectors. Much detailed work with various detectors needs to be done. One should mention that neural networks have also shown great promise for solving difficult optimization problems <ref> [26, 27] </ref>. Also in this application area the NN approach could be fruitful for high energy physics in real time track-finding. [28, 29]. Acknowledgements: We have benefitted from discussions with O. Barring, T. Sjostrand and P. M.
Reference: [28] <author> B. </author> <title> Denby,"Neural Networks and Cellular Automata in Experimental High Energy Physics" Comp. </title> <journal> Phys. Comm. </journal> <volume> 49 (1988) 429. </volume>
Reference-contexts: Much detailed work with various detectors needs to be done. One should mention that neural networks have also shown great promise for solving difficult optimization problems [26, 27]. Also in this application area the NN approach could be fruitful for high energy physics in real time track-finding. <ref> [28, 29] </ref>. Acknowledgements: We have benefitted from discussions with O. Barring, T. Sjostrand and P. M.
Reference: [29] <author> C. Peterson, </author> <title> "Track Finding with Neural Networks", </title> <booktitle> Nuclear Instruments and Methods A279 (1989) 537. </booktitle>
Reference-contexts: Much detailed work with various detectors needs to be done. One should mention that neural networks have also shown great promise for solving difficult optimization problems [26, 27]. Also in this application area the NN approach could be fruitful for high energy physics in real time track-finding. <ref> [28, 29] </ref>. Acknowledgements: We have benefitted from discussions with O. Barring, T. Sjostrand and P. M.
Reference: [30] <author> E. B. Baum and F. Wilczek, </author> <title> "Supervised Learning of Probability Distributions by Neural Networks" in D. </title> <editor> Z. Andersson (Ed.) </editor> <booktitle> Neural Information Processing Systems, American Institute of Physics, </booktitle> <address> N.Y. </address> <year> (1988). </year>
Reference-contexts: Entropy Error An alternative error measure to eq. (5) is the log-likelihood or entropy measure <ref> [30] </ref> E = p i (p) (p) For @E=@! ij one then gets for each pattern p @E = ^ ffi i h j (A6) with ^ ffi i = y i t i (A7) The only difference is the absence of g 0 (a i ) in eq. (A7).
Reference: [31] <author> S. Kullback, </author> <title> Information Theory and Statistics, </title> <publisher> Wiley, </publisher> <address> N.Y. </address> <year> (1959). </year>
Reference-contexts: The sigmoid updating rule of eq. (2) is then replaced by y i = y (a 1 ; a 2 ; :::; a n ; T ) = P satisfying n X y i = 1 (A9) A suitable error function for this representation is the Kullback measure <ref> [31] </ref> E = p i (p) t i (A10) For @E=@! ij one gets for each pattern p @E = k 1 @y k h j (A11) which with @y k = ffi ik y k y k y i (A12) together with P k t k = 1 gives @E
Reference: [32] <author> R. P. Lippman, </author> <title> "An Introduction to Computing with Neural Nets", </title> <journal> IEEE ASSP Magazine April (1987) 4-22. </journal> <volume> 30 </volume>
Reference-contexts: Number of hidden layers. We have tried using two hidden layers for the quark/gluon separation. No improvement of performance was found. In theory, any pattern classification task is realizable with at most two hidden layers <ref> [32] </ref>. This is because any reasonable function can be represented by a superposition of Gaussian-like "bumps" (similar to Fourier analysis). Combining two sigmoids, by using two hidden layers, produces such bumps. This does not mean that two hidden layers necessarily is the optimal for every classification problem.
References-found: 32

