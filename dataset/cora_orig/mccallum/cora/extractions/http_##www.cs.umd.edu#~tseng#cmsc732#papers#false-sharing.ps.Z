URL: http://www.cs.umd.edu/~tseng/cmsc732/papers/false-sharing.ps.Z
Refering-URL: http://www.cs.umd.edu/~tseng/cmsc732/papers.html
Root-URL: 
Email: tor@research.att.com  eggers@cs.washington.edu  
Title: Reducing False Sharing on Shared Memory Multiprocessors through Compile Time Data Transformations  
Author: Tor E. Jeremiassen Susan J. Eggers 
Address: 600 Mountain Ave. Murray Hill, New Jersey 07974  Seattle, Washington 98195  
Affiliation: AT&T Bell Laboratories  Department of Computer Science and Engineering University of Washington  
Abstract: We have developed compiler algorithms that analyze explicitly parallel programs and restructure their shared data to reduce the number of false sharing misses. The algorithms analyze per-process shared data accesses, pinpoint the data structures that are susceptible to false sharing and choose an appropriate transformation to reduce it. The transformations either group data that is accessed by the same processor or separate individual data items that are shared. This paper evaluates that technique. We show through simulation that our analysis successfully identifies the data structures that are responsible for most false sharing misses, and then transforms them without unduly decreasing spatial locality. The reduction in false sharing positively impacts both execution time and program scalability when executed on a KSR2. Both factors combine to increase the maximum achievable speedup for all programs, more than doubling it for several. Despite being able to only approximate actual inter-processor memory accesses, the compiler-directed transformations always outperform programmer efforts to eliminate false sharing. 
Abstract-found: 1
Intro-found: 1
Reference: [AG88] <author> A. Agarwal and A. Gupta. </author> <title> Memory-reference characteristics of multiprocessor applications under mach. </title> <booktitle> In SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 215-225, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: 1 Introduction On bus-based, shared memory multiprocessors, much of the "unnecessary" bus traffic, i.e., that which could be eliminated with better processor locality <ref> [AG88] </ref>, is coherency overhead caused by false sharing [TLH94, EJ91]. False sharing occurs when multiple processors access (both read and write) different words in the same cache block. Although the processors do not actually share data, they incur its costs, because coherency operations manipulate cache blocks. <p> up only 5% (on average) of the total running time. 3.2 Shared Data Transformations In order to eliminate false sharing, data must be restructured so that (1) data that are only, or overwhelmingly, accessed by one processor are grouped together, and (2) write shared data objects with no processor locality <ref> [AG88] </ref> do not share cache lines. Two transformations, originally devised for manual application, group and transpose and indirection [EJ91], address item (1); the third, pad and align, is well known and addresses item (2).
Reference: [Ban79] <author> J.P. Banning. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Sixth Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 29-41, </pages> <month> January </month> <year> 1979. </year>
Reference-contexts: The second performs non-concurrency analysis [MR93] interpro-cedurally by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [JE94]. The third stage performs an enhanced in-terprocedural, flow-insensitive, summary side-effect analysis <ref> [Bar78, Ban79, Mye81, CK88b] </ref> and static profiling on a per-process basis (based on the control flow determined in stage one) for each phase (determined in stage two).
Reference: [Bar78] <author> J. Barth. </author> <title> A practical interprocedural data flow analysis algorithm. </title> <journal> CACM, </journal> <volume> 21(9) </volume> <pages> 724-736, </pages> <month> September </month> <year> 1978. </year>
Reference-contexts: The second performs non-concurrency analysis [MR93] interpro-cedurally by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [JE94]. The third stage performs an enhanced in-terprocedural, flow-insensitive, summary side-effect analysis <ref> [Bar78, Ban79, Mye81, CK88b] </ref> and static profiling on a per-process basis (based on the control flow determined in stage one) for each phase (determined in stage two).
Reference: [BFS89] <author> W.J. Bolosky, R.P. Fitzgerald, </author> <title> and M.L. Scott. Simple but effective techniques for NUMA memory management. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 19-31, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Our application of padding differs in that we apply it only when indicated by the static analysis, as opposed to from feedback from off-line cache simulation profiles [TLH94], or based on programmer knowledge <ref> [BFS89] </ref>. Locks: Locks are also padded, to the size of the cache block, rather than allocated with the write-shared data they protect. Co-allocating locks and data [TLH94] improves spatial locality, but generates coherence traffic when there is contention for the locks. <p> Their transformations remove false sharing by improving processor locality. Two studies focused on reducing false sharing in pages rather than cache blocks. Bolosky et al. <ref> [BFS89] </ref> eliminated false sharing by coalescing objects into a larger object or padding individual objects to page boundaries, all manually. However, they do not quantify the effect of eliminating false sharing. Granston [Gra93] presented a theory to identify and eliminate page-level sharing between processors that occur in parallel do-loops.
Reference: [Car88] <author> F.J. Carrasco. </author> <title> A parallel maxflow implemen-tation. </title> <type> CS411 Project Report, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: The programs we collected had been hand-optimized for locality to varying degrees. In one group, that included Maxflow <ref> [Car88] </ref>, Pverify [MDWSV87] and 5 Infinite caches can be used to approximate very large (on the order of several megabytes) second level caches [Egg91].
Reference: [CK88a] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> (5):517-550, 1988. 
Reference-contexts: We improve upon traditional summary side-effect analysis in two respects. First, to improve its accuracy we allow multiple regular section descriptors <ref> [CK88a, HK91] </ref> and only merge them when very little or no information will be lost, or when the number of descriptors for a single array exceeds some small preset limit. (None of the arrays used in our benchmarks required more than 10 descriptors).
Reference: [CK88b] <author> K.D. Cooper and K. Kennedy. </author> <title> Interprocedural side-effect analysis in linear time. </title> <booktitle> In Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 57-66, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The second performs non-concurrency analysis [MR93] interpro-cedurally by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [JE94]. The third stage performs an enhanced in-terprocedural, flow-insensitive, summary side-effect analysis <ref> [Bar78, Ban79, Mye81, CK88b] </ref> and static profiling on a per-process basis (based on the control flow determined in stage one) for each phase (determined in stage two).
Reference: [DN87] <author> S. Devadas and A.R. </author> <title> Newton. Topological optimization of multiple level array logic. </title> <booktitle> In IEEE Transactions on Computer-Aided Design, </booktitle> <month> November </month> <year> 1987. </year>
Reference-contexts: Version refers to (N)ot optimized, (C)ompiler optimized, or (P)rogrammer optimized. Topopt <ref> [DN87] </ref>, no effort had been made to improve locality. For Pverify and Topopt, in particular, the programmers had constructed data structures to match their "natural" way of thinking about the semantics of the program algorithms, rather than for better memory system performance.
Reference: [DSR + 93] <author> M. Dubois, J. Skeppstedt, L. Ricciulli, K. Ra-mamurthy, and P. Stenstrom. </author> <title> The detection and elimination of useless misses in multiprocessors. </title> <booktitle> In 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 88-97, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: In contrast, on a slightly different workload, our transformations reduced the total miss rate by an average of 49% (on the unoptimized programs, also for 64 byte blocks, but with 12 processors). Dubois et al. <ref> [DSR + 93] </ref> reduced false sharing with hardware, either by delaying invalidations (at the sender, receiver or both) until special acquire or release instructions were executed, or by performing invalidations on a word basis. Delaying invalidations both at the sender and the receiver and invalidating cache subblocks consistently perform well.
Reference: [Egg91] <author> S.J. Eggers. </author> <title> Simplicity versus accuracy in a model of cache coherency overhead. </title> <journal> IEEE Transaction on Computers, </journal> <volume> 40(8) </volume> <pages> 893-906, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The programs we collected had been hand-optimized for locality to varying degrees. In one group, that included Maxflow [Car88], Pverify [MDWSV87] and 5 Infinite caches can be used to approximate very large (on the order of several megabytes) second level caches <ref> [Egg91] </ref>.
Reference: [EJ91] <author> S.J. Eggers and T.E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume I, </volume> <pages> pages 377-381, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction On bus-based, shared memory multiprocessors, much of the "unnecessary" bus traffic, i.e., that which could be eliminated with better processor locality [AG88], is coherency overhead caused by false sharing <ref> [TLH94, EJ91] </ref>. False sharing occurs when multiple processors access (both read and write) different words in the same cache block. Although the processors do not actually share data, they incur its costs, because coherency operations manipulate cache blocks. <p> Two transformations, originally devised for manual application, group and transpose and indirection <ref> [EJ91] </ref>, address item (1); the third, pad and align, is well known and addresses item (2). Group & Transpose: Group & transpose (Figure 2a) physically groups per-process data together by changing the layout of the data structures in memory. <p> For Pverify and Topopt, in particular, the programmers had constructed data structures to match their "natural" way of thinking about the semantics of the program algorithms, rather than for better memory system performance. To provide hand-optimized versions of these programs (Pverify and Topopt), we manually transformed them <ref> [EJ91] </ref>. In another group that comprised the original SPLASH benchmark suite [SWG91] (LocusRoute, Mp3d, Pthor, Water) and the SPLASH2 benchmarks (Fmm [SHHG93], Ra-diosity and Raytrace [SGL94]), programs had been highly optimized for locality, including eliminating false sharing.
Reference: [EKKL90] <author> S.J. Eggers, D.R. Keppel, E.J. Koldinger, and H.M. Levy. </author> <title> Techniques for efficient inline tracing on a shared-memory multiprocessor. </title> <booktitle> In Proceedings of the International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 37-47, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: False sharing reductions and other cache miss metrics were measured using trace-driven simulation. Each program was traced (both before and after shared data was transformed), using a software tracing tool for parallel programs <ref> [EKKL90] </ref>. Cache miss rates were analyzed with a multiprocessor simulator that emulates a simple, shared memory architecture. The processors are assumed to be RISC-like, with a 32 KB first level cache and an infinite second level cache 5 . We studied block sizes ranging from 4 to 256 bytes.
Reference: [GP91] <author> M. Gupta and D. A. Padua. </author> <title> Effects of program parallelization and stripmining transformations on cache performance in a multiprocessor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 301-304, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The second requires an invalid bit per word in the cache block, and causes more invalidations when the writes exhibit spatial locality. Several compiler approaches reorganize control structures rather than data. One group used workloads that consisted of either loops or library routines that have fine-grain parallelism <ref> [JD91, GP91, PC89] </ref>. Their studies recorded performance improvements only for the code fragments that were transformed. Therefore the results are overly optimistic with regard to the expected performance of executing entire programs. Ju and Dietz [JD91] restructured a program fragment of several loops accessing array elements. <p> Their restructuring algorithm applies loop transformations (such as loop distribution) and data layout transformations (accessing arrays in row or column major order), according to a coherency cost function. The restructuring provided a 25% improvement in execution time of the loops for a 64 KB cache. Gupta and Padua <ref> [GP91] </ref> also examined sequential programs that were automatically parallelized at the loop level. They strip-mined the loops to the size of the cache block and assigned each strip to a different processor.
Reference: [Gra93] <author> E. D. Granston. </author> <title> Toward a compile-time methodology for reducing false sharing and communication traffic in shared virtual memory systems. </title> <editor> In U. Banerjee, D. Gelern-ter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Sixth Workshop on Languages and Compilers for Parallelism, </booktitle> <pages> pages 273-289, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Two studies focused on reducing false sharing in pages rather than cache blocks. Bolosky et al. [BFS89] eliminated false sharing by coalescing objects into a larger object or padding individual objects to page boundaries, all manually. However, they do not quantify the effect of eliminating false sharing. Granston <ref> [Gra93] </ref> presented a theory to identify and eliminate page-level sharing between processors that occur in parallel do-loops.
Reference: [GW94] <author> M. Galles and E. Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <booktitle> In Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences, volume I: Architecture, </booktitle> <pages> pages 134-143, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Examples of such programs can be found in the Stanford SPLASH application suite [SWG91]. These programs currently execute on small to medium scale multiprocessors, both commercially (e.g., Sequent Symmetry [LT88], SGI Challenge <ref> [GW94] </ref>, SPARCcenter 2000 [M. 93], and the KSR2 [Ken94] ) and in research environments (e.g., DASH [LLG + 92], FLASH [LLG + 94]). The granularity of parallelism in these programs is coarse, on the level of an entire process.
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Specification. </title> <month> January </month> <year> 1993. </year>
Reference-contexts: Examples of these variables include induction variables of F ORALL loops in HPF <ref> [Hig93] </ref> and private variables, such as pid in Figure 1, in the fork/join model. Our current implementation targets programs that use the latter: coarse-grained, explicitly parallel C programs that execute on shared memory multiprocessors. Examples of such programs can be found in the Stanford SPLASH application suite [SWG91].
Reference: [HK91] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The side-effect analysis represents the sections of each array that each process accesses using bounded regular section descriptors 4 to describe the index expressions <ref> [HK91] </ref>. When a regular section descriptor contains a PDV in the index expressions, we test whether the descriptor identifies disjoint sections of the array for different values of the variable. The array is implicitly partitioned across processes if the sections are disjoint. <p> We improve upon traditional summary side-effect analysis in two respects. First, to improve its accuracy we allow multiple regular section descriptors <ref> [CK88a, HK91] </ref> and only merge them when very little or no information will be lost, or when the number of descriptors for a single array exceeds some small preset limit. (None of the arrays used in our benchmarks required more than 10 descriptors).
Reference: [JD91] <author> Y. Ju and H. Dietz. </author> <title> Reduction of cache coherence overhead by compiler data layout and loop transformation. </title> <editor> In U. Banerjee, D. Gel-ernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Fourth Workshop on Languages and Compilers for Parallelism, </booktitle> <pages> pages 344-358. </pages> <publisher> Springer Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: The second requires an invalid bit per word in the cache block, and causes more invalidations when the writes exhibit spatial locality. Several compiler approaches reorganize control structures rather than data. One group used workloads that consisted of either loops or library routines that have fine-grain parallelism <ref> [JD91, GP91, PC89] </ref>. Their studies recorded performance improvements only for the code fragments that were transformed. Therefore the results are overly optimistic with regard to the expected performance of executing entire programs. Ju and Dietz [JD91] restructured a program fragment of several loops accessing array elements. <p> Their studies recorded performance improvements only for the code fragments that were transformed. Therefore the results are overly optimistic with regard to the expected performance of executing entire programs. Ju and Dietz <ref> [JD91] </ref> restructured a program fragment of several loops accessing array elements. Their restructuring algorithm applies loop transformations (such as loop distribution) and data layout transformations (accessing arrays in row or column major order), according to a coherency cost function.
Reference: [JE92] <author> T.E. Jeremiassen and S.J. Eggers. </author> <title> Computing per-process summary side-effect information. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Fifth Workshop on Languages and Compilers for Parallelism, </booktitle> <pages> pages 175-91, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Therefore, it is important to carefully balance the tradeoff between processor and spatial locality, so as to maximize program performance. To this end we have developed and incorporated into the Parafrase-2 [PGH + 89] source-to-source restructurer a series of compiler algorithms <ref> [JE92, JE94] </ref> and a suite of data transformations. The algorithms analyze explicitly parallel programs; they produce information about each processor's memory reference patterns that identifies data structures susceptible to false sharing, decide whether transforming them will pay off and then choose appropriate transformations. This paper evaluates that technique. <p> This compiler analysis involves three separate stages. The first uses inter-procedural analysis of the control flow to determine which sections of code each process executes, and annotates the nodes of the control-flow graphs accordingly <ref> [JE92] </ref> 2 . The second performs non-concurrency analysis [MR93] interpro-cedurally by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [JE94].
Reference: [JE94] <author> T.E. Jeremiassen and S.J. Eggers. </author> <title> Static analysis of barrier synchronization in explicitly parallel programs. </title> <booktitle> In International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <pages> pages 171-180, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: Therefore, it is important to carefully balance the tradeoff between processor and spatial locality, so as to maximize program performance. To this end we have developed and incorporated into the Parafrase-2 [PGH + 89] source-to-source restructurer a series of compiler algorithms <ref> [JE92, JE94] </ref> and a suite of data transformations. The algorithms analyze explicitly parallel programs; they produce information about each processor's memory reference patterns that identifies data structures susceptible to false sharing, decide whether transforming them will pay off and then choose appropriate transformations. This paper evaluates that technique. <p> The second performs non-concurrency analysis [MR93] interpro-cedurally by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them <ref> [JE94] </ref>. The third stage performs an enhanced in-terprocedural, flow-insensitive, summary side-effect analysis [Bar78, Ban79, Mye81, CK88b] and static profiling on a per-process basis (based on the control flow determined in stage one) for each phase (determined in stage two).
Reference: [Jer95] <author> T.E. Jeremiassen. </author> <title> Using compile time analysis and transformations to reduce coherency traffic on shared memory multiprocessors. </title> <type> Ph.D. thesis, </type> <institution> University of Washington, </institution> <note> to be published, </note> <year> 1995. </year>
Reference-contexts: The most important constraints involve pointers and separate compilation (a full description will appear in <ref> [Jer95] </ref>). While our model allows for pointers, the full generality of pointers in C is restricted to reduce pointer aliasing of statically allocated data to that induced by pointer type parameters to functions.
Reference: [Ken94] <institution> Kendall Square Research. KSR/Series Princ-ples of Operations, </institution> <address> revision 7.0 edition, </address> <year> 1994. </year>
Reference-contexts: Examples of such programs can be found in the Stanford SPLASH application suite [SWG91]. These programs currently execute on small to medium scale multiprocessors, both commercially (e.g., Sequent Symmetry [LT88], SGI Challenge [GW94], SPARCcenter 2000 [M. 93], and the KSR2 <ref> [Ken94] </ref> ) and in research environments (e.g., DASH [LLG + 92], FLASH [LLG + 94]). The granularity of parallelism in these programs is coarse, on the level of an entire process. <p> The processors are assumed to be RISC-like, with a 32 KB first level cache and an infinite second level cache 5 . We studied block sizes ranging from 4 to 256 bytes. Execution times were measured on a 56-processor Kendall Square Research KSR2 <ref> [Ken94] </ref>. Each processor has a 512 KB first level cache, divided equally between data and instructions. The second level cache contains 32 MB, and uses a coherency unit of 128 bytes.
Reference: [KM92] <author> K. Kennedy and K. S. McKinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <pages> pages 276-283, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Note, for LocusRoute, Mp3d, Pthor and Water only programmer- and compiler-optimized versions were available, while for Maxflow, no programmer-optimized version was available. Wolf and Lam [WL91] and Kennedy and McKinley <ref> [KM92] </ref> do similar work, but on complete programs. They reorganize control to improve locality in the inner loops. They also detect a parallel loop, put it in the outermost legal position and tile (i.e., strip mine and interchange) it if it contains spatial locality.
Reference: [LLG + 92] <author> D. Lenoski, J. Laudon, K Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stan-ford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Examples of such programs can be found in the Stanford SPLASH application suite [SWG91]. These programs currently execute on small to medium scale multiprocessors, both commercially (e.g., Sequent Symmetry [LT88], SGI Challenge [GW94], SPARCcenter 2000 [M. 93], and the KSR2 [Ken94] ) and in research environments (e.g., DASH <ref> [LLG + 92] </ref>, FLASH [LLG + 94]). The granularity of parallelism in these programs is coarse, on the level of an entire process.
Reference: [LLG + 94] <author> D. Lenoski, J. Laudon, K Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These programs currently execute on small to medium scale multiprocessors, both commercially (e.g., Sequent Symmetry [LT88], SGI Challenge [GW94], SPARCcenter 2000 [M. 93], and the KSR2 [Ken94] ) and in research environments (e.g., DASH [LLG + 92], FLASH <ref> [LLG + 94] </ref>). The granularity of parallelism in these programs is coarse, on the level of an entire process.
Reference: [LT88] <author> R. Lovett and S. Thakkar. </author> <title> The symmetry multiprocessor system. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 303-310, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Our current implementation targets programs that use the latter: coarse-grained, explicitly parallel C programs that execute on shared memory multiprocessors. Examples of such programs can be found in the Stanford SPLASH application suite [SWG91]. These programs currently execute on small to medium scale multiprocessors, both commercially (e.g., Sequent Symmetry <ref> [LT88] </ref>, SGI Challenge [GW94], SPARCcenter 2000 [M. 93], and the KSR2 [Ken94] ) and in research environments (e.g., DASH [LLG + 92], FLASH [LLG + 94]). The granularity of parallelism in these programs is coarse, on the level of an entire process.
Reference: [M. 93] <editor> M. Cekleov, et. al. </editor> <booktitle> SPARCcenter 2000: Multiprocessing for the 90's. In IEEE COMPCON, </booktitle> <pages> pages 345-353, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Examples of such programs can be found in the Stanford SPLASH application suite [SWG91]. These programs currently execute on small to medium scale multiprocessors, both commercially (e.g., Sequent Symmetry [LT88], SGI Challenge [GW94], SPARCcenter 2000 <ref> [M. 93] </ref>, and the KSR2 [Ken94] ) and in research environments (e.g., DASH [LLG + 92], FLASH [LLG + 94]). The granularity of parallelism in these programs is coarse, on the level of an entire process.
Reference: [MDWSV87] <author> H-K. T. Ma, S. Devadas, R. Wei, and A. Sangiovanni-Vincentelli. </author> <title> Logic verification algorithms and their parallel implementation. </title> <booktitle> In Proceedings of the 24th Design Automation Conference, </booktitle> <pages> pages 283-290, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The programs we collected had been hand-optimized for locality to varying degrees. In one group, that included Maxflow [Car88], Pverify <ref> [MDWSV87] </ref> and 5 Infinite caches can be used to approximate very large (on the order of several megabytes) second level caches [Egg91].
Reference: [MR93] <author> S.P. Masticola and B.G. Ryder. </author> <title> Non--concurrency analysis. </title> <booktitle> In Fourth ACM SIG-PLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 129-138, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: This compiler analysis involves three separate stages. The first uses inter-procedural analysis of the control flow to determine which sections of code each process executes, and annotates the nodes of the control-flow graphs accordingly [JE92] 2 . The second performs non-concurrency analysis <ref> [MR93] </ref> interpro-cedurally by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [JE94].
Reference: [Mye81] <author> E. Myers. </author> <title> A precise inter-procedural data flow algorithm. </title> <booktitle> In Symposium on Principles of Programming Languages, </booktitle> <pages> pages 219-230, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: The second performs non-concurrency analysis [MR93] interpro-cedurally by examining the barrier synchronization pattern of the program, delineating the phases that cannot execute in parallel and computing the flow of control between them [JE94]. The third stage performs an enhanced in-terprocedural, flow-insensitive, summary side-effect analysis <ref> [Bar78, Ban79, Mye81, CK88b] </ref> and static profiling on a per-process basis (based on the control flow determined in stage one) for each phase (determined in stage two).
Reference: [PC89] <author> J.K. Peir and R. Cytron. </author> <title> Minimum distance: A method for partitioning recurrences for multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(8) </volume> <pages> 1203-1211, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: The second requires an invalid bit per word in the cache block, and causes more invalidations when the writes exhibit spatial locality. Several compiler approaches reorganize control structures rather than data. One group used workloads that consisted of either loops or library routines that have fine-grain parallelism <ref> [JD91, GP91, PC89] </ref>. Their studies recorded performance improvements only for the code fragments that were transformed. Therefore the results are overly optimistic with regard to the expected performance of executing entire programs. Ju and Dietz [JD91] restructured a program fragment of several loops accessing array elements. <p> They strip-mined the loops to the size of the cache block and assigned each strip to a different processor. The decline in miss ratios ranged from 4% to almost 60%, as block size was increased to 128 bytes. No execution times were reported. Peir and Cytron <ref> [PC89] </ref> partitioned loops to minimize inter-processor communication when processing recurrences. Their mechanism for partitioning utilizes loop unrolling and dependence vectors. Partitions are then scheduled on different processors.
Reference: [PGH + 89] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C.L. Lee, B. Leung, and D. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 39-48, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Therefore, it is important to carefully balance the tradeoff between processor and spatial locality, so as to maximize program performance. To this end we have developed and incorporated into the Parafrase-2 <ref> [PGH + 89] </ref> source-to-source restructurer a series of compiler algorithms [JE92, JE94] and a suite of data transformations.
Reference: [SGL94] <author> J.P. Singh, A. Gupta, and M. Levoy. </author> <title> Visualization algorithms: Performance and architectural implications. </title> <journal> IEEE Computer, </journal> <volume> 27(7) </volume> <pages> 45-55, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: To provide hand-optimized versions of these programs (Pverify and Topopt), we manually transformed them [EJ91]. In another group that comprised the original SPLASH benchmark suite [SWG91] (LocusRoute, Mp3d, Pthor, Water) and the SPLASH2 benchmarks (Fmm [SHHG93], Ra-diosity and Raytrace <ref> [SGL94] </ref>), programs had been highly optimized for locality, including eliminating false sharing. The SPLASH2 programs contained several easily identifiable data structures whose elements had been organized by processor (in our terminology, grouped and transposed), and padded.
Reference: [SHHG93] <author> J.P. Singh, C. Holt, J.L. Hennessy, and A. Gupta. </author> <title> A parallel adaptive fast multipole method. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 54-65, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: To provide hand-optimized versions of these programs (Pverify and Topopt), we manually transformed them [EJ91]. In another group that comprised the original SPLASH benchmark suite [SWG91] (LocusRoute, Mp3d, Pthor, Water) and the SPLASH2 benchmarks (Fmm <ref> [SHHG93] </ref>, Ra-diosity and Raytrace [SGL94]), programs had been highly optimized for locality, including eliminating false sharing. The SPLASH2 programs contained several easily identifiable data structures whose elements had been organized by processor (in our terminology, grouped and transposed), and padded.
Reference: [SWG91] <author> J.P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stan-ford University, </institution> <year> 1991. </year>
Reference-contexts: Our current implementation targets programs that use the latter: coarse-grained, explicitly parallel C programs that execute on shared memory multiprocessors. Examples of such programs can be found in the Stanford SPLASH application suite <ref> [SWG91] </ref>. These programs currently execute on small to medium scale multiprocessors, both commercially (e.g., Sequent Symmetry [LT88], SGI Challenge [GW94], SPARCcenter 2000 [M. 93], and the KSR2 [Ken94] ) and in research environments (e.g., DASH [LLG + 92], FLASH [LLG + 94]). <p> To provide hand-optimized versions of these programs (Pverify and Topopt), we manually transformed them [EJ91]. In another group that comprised the original SPLASH benchmark suite <ref> [SWG91] </ref> (LocusRoute, Mp3d, Pthor, Water) and the SPLASH2 benchmarks (Fmm [SHHG93], Ra-diosity and Raytrace [SGL94]), programs had been highly optimized for locality, including eliminating false sharing. The SPLASH2 programs contained several easily identifiable data structures whose elements had been organized by processor (in our terminology, grouped and transposed), and padded.
Reference: [TLH94] <author> J. Torrellas, M.S. Lam, and J.L. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(6) </volume> <pages> 651-663, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: 1 Introduction On bus-based, shared memory multiprocessors, much of the "unnecessary" bus traffic, i.e., that which could be eliminated with better processor locality [AG88], is coherency overhead caused by false sharing <ref> [TLH94, EJ91] </ref>. False sharing occurs when multiple processors access (both read and write) different words in the same cache block. Although the processors do not actually share data, they incur its costs, because coherency operations manipulate cache blocks. <p> Pad and align has been used to eliminate false sharing in both cache blocks and pages in other work. Our application of padding differs in that we apply it only when indicated by the static analysis, as opposed to from feedback from off-line cache simulation profiles <ref> [TLH94] </ref>, or based on programmer knowledge [BFS89]. Locks: Locks are also padded, to the size of the cache block, rather than allocated with the write-shared data they protect. Co-allocating locks and data [TLH94] improves spatial locality, but generates coherence traffic when there is contention for the locks. <p> it only when indicated by the static analysis, as opposed to from feedback from off-line cache simulation profiles <ref> [TLH94] </ref>, or based on programmer knowledge [BFS89]. Locks: Locks are also padded, to the size of the cache block, rather than allocated with the write-shared data they protect. Co-allocating locks and data [TLH94] improves spatial locality, but generates coherence traffic when there is contention for the locks. The processor that holds the busy lock loses exclusive ownership of its cache block, because of reads by waiting processors. <p> All data points are speedups relative to the uniprocessor execution of the unoptimized version. Note the different scales on the vertical axis. 6 Related Work The research that is most closely related to ours is Torrellas et al. <ref> [TLH94] </ref>, who reduced false sharing using a somewhat different set of transformations that were applied manually.
Reference: [WL91] <author> M.E. Wolf and M.S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Note, for LocusRoute, Mp3d, Pthor and Water only programmer- and compiler-optimized versions were available, while for Maxflow, no programmer-optimized version was available. Wolf and Lam <ref> [WL91] </ref> and Kennedy and McKinley [KM92] do similar work, but on complete programs. They reorganize control to improve locality in the inner loops. They also detect a parallel loop, put it in the outermost legal position and tile (i.e., strip mine and interchange) it if it contains spatial locality.
References-found: 37

