URL: http://www.cs.man.ac.uk/ai/Papers/magnus/opt_rate_prl.ps.gz
Refering-URL: http://www.cs.man.ac.uk/~magnus/magnus.html
Root-URL: http://www.cs.man.ac.uk
Title: Globally Optimal Parameters for On-Line Learning in Multilayer Neural Networks networks have attracted interest in
Author: David Saad and Magnus Rattray P M *(J; j M X g( n 
Date: 2  (1)  
Note: Feed-forward neural  and may be carried out by a variety of methods.  M  The error made by a student  been obtained [4] in the thermodynamic limit (N 1) and can be represented by  ik  
Address: B4 7ET, UK.  
Affiliation: Department of Computer Science and Applied Mathematics, Aston University, Birmingham  
Pubnum: 2  
Abstract: We present a framework for calculating globally optimal parameters, within a given time frame, for on-line learning in multilayer neural networks. We demonstrate the capability of this method by computing optimal learning rates in typical learning scenarios. A similar treatment allows one to determine the relevance of related training algorithms based on modifications to the basic gradient descent rule as well as to compare different training methods. One of the leading techniques in neural networks training, especially for large systems, is on-line learning of continuous functions via gradient descent on a differentiable error measure. This technique has been successfully applied to many real-world problems and is arguably the most commonly used neural networks training technique. Many variations of the basic algorithm have been suggested over the years, for instance, adding weight decay and momentum terms (for a review, see [2]). These modifications inevitably introduce new parameters which, in addition to the inherent stochasticity of the learning process, makes it very hard to assess their usefulness. A recent study [3-5], offers a framework for analytically examining different aspects of on-line learning scenarios. We will employ the same framework to suggest a method for calculating, within given time windows, globally optimal parameters. The method will be demonstrated on one of the natural parameters in gradient descent on-line learning, the learning rate, although it can easily be generalized to accommodate other parameters and learning rules, as well as discrete architectures. This method can also be employed to assess the usefulness of various modifications to the basic gradient descent rule, or even to compare the efficiency of different training techniques, by examining the optimal values assigned to the related coefficients. For instance, low optimal values, possibly in certain phases of the learning process, will indicate that these modifications are redundant. In this letter, we concentrate on maps from an N -dimensional input space 2 &lt; N onto a scalar i 2 &lt;, realized function of the hidden units, taken here to be the error function g(x) j erf(x= p 2), J j fJ i g 1iK is the set of input-to-hidden adaptive weights for the K hidden nodes and the hidden-to-output weights are set to 1. The activation of hidden node i under presentation of the input pattern is denoted x i = J i . This general configuration, usually referred to as the `soft committee machine' [3], represents most of the properties of general multilayer networks and can easily be extended to accommodate adaptive hidden-to-output weights [6]. Training examples are of the form ( ; i ) where = 1; 2; : : : ; P . The components of the independently drawn input vectors are uncorrelated random variables with zero mean and unit variance. The corresponding output i is given by a deterministic teacher of a similar configuration to the student except for a possible difference in the number n=1 g (B n ), where B j fB n g 1nM is the set of input-to-hidden adaptive weights for teacher hidden nodes. The activation of hidden node n under presentation of the input pattern is denoted y n = B n . We will use indices i; j; k; l : : : to refer to units in the student network and n; m; : : : for units in the teacher network. 
Abstract-found: 1
Intro-found: 0
References-found: 0

