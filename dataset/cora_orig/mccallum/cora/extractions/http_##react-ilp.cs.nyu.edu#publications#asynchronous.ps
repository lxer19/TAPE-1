URL: http://react-ilp.cs.nyu.edu/publications/asynchronous.ps
Refering-URL: http://react-ilp.cs.nyu.edu/publications/index.html
Root-URL: http://www.cs.nyu.edu
Email: aumann@wisdom.weizmann.ac.il.  kedem@cs.nyu.edu.  kpalem@watson.ibm.com palem@cims.nyu.edu.  
Title: Highly Efficient Asynchronous Execution of Large-Grained Parallel Programs  
Author: Y. Aumann Z. M. Kedem K. V. Palem M. O. Rabin 
Note: The research of this author was supported in part by NSF/DARPA under grant number CCR-89-06949 and by NSF under grant number CCR-91-03953.  
Address: Rehovot 76100, Israel.  New York University, 251 Mercer St., New York, NY 10012-1185, USA;  P. O. Box 704, Yorktown Heights, NY 10598, USA;  
Affiliation: Dept. Computer Science, The Weizmann Institute,  Department of Computer Science, Courant Institute of Mathematical Sciences,  IBM Research Division, T. J. Watson Research Center,  
Abstract: An n-thread parallel program P is large-grained if in every parallel step the computations on each of the threads are complex procedures requiring numerous processor instructions. This practically relevant style of programs differs from PRAM programs in its large granularity and the possibility that within a parallel step the computations on different threads may considerably vary in size. Let M be an n-processor asynchronous parallel system, with no restriction on the degree of asynchrony and without any specialized synchronization mechanisms. It is a challenging theoretical as well as practically important problem to ensure correct execution of P on such a parallel machine. Let P be a large-grained program requiring total work W for its execution on a synchronous n-processor parallel system. We present a transformation (compilation) of P into a program C(P) which correctly and efficiently effects the computation of P on the asynchronous machine M. Under moderate assumptions on the granularity of threads and the size of the program variables, execution of C(P) requires just O(W log fl n) expected total work, and the memory space overhead is a small multiplicative constant. This result is the first of its kind. The solution involves a number of new concepts and methods. These include methods for storing program and control variables, employing a combination x Aiken Computation Laboratory, Harvard University, Cam-bridge, MA 02138, USA; Institute of Computer Science, He-brew University, Jerusalem, Israel; rabin@das.harvard.edu. The research of this author was supported in part by NSF under grant number CCR-90-07677 and by ONR under contract number N0001491-J-1981. of error correction codes with phase-dependent hashing into memory. We feel that these methods for storing data will have significant practical applications to storage of data on Disk Arrays (RAIDS), as well as additional theoretical implications. The significance of the present work to parallel data-processing programs and large scale parallel numerical computations is obvious. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal, A.K. Chandra, and M. Snir, </author> <title> "On communication latency in PRAM computation," </title> <booktitle> Proc. ACM SPAA, </booktitle> <pages> 11-21, </pages> <year> 1989. </year>
Reference-contexts: In these papers the simulation is for a PRAM program, and the solutions are restricted to this case. The change in focus, from the PRAM to models of larger granularity, is advocated in recent practically motivated studies <ref> [1, 23, 8] </ref>. In these papers alternate computation models are presented, including the BSP model in [23] and the LogP model in [8]. Algorithm for such large-granularity models, as well as complexity analysis are presented in [1, 2, 17, 19]. <p> In these papers alternate computation models are presented, including the BSP model in [23] and the LogP model in [8]. Algorithm for such large-granularity models, as well as complexity analysis are presented in <ref> [1, 2, 17, 19] </ref>. In several of these papers (e.g. [8]) the problem of asynchrony is addressed. However, in each case there is either an assumption of strong system-primitives for synchronization, or a methodology of fixed assignment of tasks to processors.
Reference: [2] <author> A. Aggarwal, A.K. Chandra, and M. Snir, </author> <title> "Communication complexity of PRAMs," </title> <booktitle> Theoretical Computer Science, </booktitle> <pages> 3-28, </pages> <year> 1990. </year>
Reference-contexts: In these papers alternate computation models are presented, including the BSP model in [23] and the LogP model in [8]. Algorithm for such large-granularity models, as well as complexity analysis are presented in <ref> [1, 2, 17, 19] </ref>. In several of these papers (e.g. [8]) the problem of asynchrony is addressed. However, in each case there is either an assumption of strong system-primitives for synchronization, or a methodology of fixed assignment of tasks to processors.
Reference: [3] <author> Y. Aumann and Y. Matias, </author> <title> "Simulating the CRCW-PRAM on asynchronous shared memory systems," </title> <type> Personal Communication, </type> <year> 1993. </year>
Reference-contexts: We create the necessary hash functions and the method of Random Number Tables required for the random space allocation. A limited scheme of random space allocation in temporary tables was previously suggested in <ref> [3] </ref>, to achieve CRCW-PRAM simulation. 2 The Model and Definitions Parallel Programs We consider a parallel program P which is described by n computation threads T 1 ; : : : ; T n . The threads compute on shared program variables, V = fX; Y; Z; : : :g.
Reference: [4] <author> Y. Aumann and M. O. Rabin, </author> <title> "Clock construction in fully asynchronous parallel systems and PRAM simulation," </title> <booktitle> Proc. 33rd IEEE FOCS, </booktitle> <pages> 147-156, </pages> <year> 1992. </year>
Reference-contexts: Avoiding clobbers is the main and most subtle challenge in executing a parallel program on a general asynchronous machine. The problem of asynchronous execution has been considered in several previous papers ([6, 9, 7, 18, 13, 4]). Restricting our attention to those works which deal with unrestricted asynchronous behavior <ref> [13, 4] </ref>, we observe that these solutions are limited to programs of very fine granularity, i.e. PRAM programs. <p> There a randomized solution was given to the problem of simulating a synchronous PRAM on its asynchronous counter-part. The simulation entailed a O (log 3 n) multiplicative factor in work overhead, and O (log n) factor in space. This was further improved in <ref> [4] </ref>, obtaining a O (log 2 n) work overhead. In these papers the simulation is for a PRAM program, and the solutions are restricted to this case. The change in focus, from the PRAM to models of larger granularity, is advocated in recent practically motivated studies [1, 23, 8]. <p> The detailed protocols for all the above functions appear in Section 8. The Phase Clock The phase clock is implemented as two arrays of size O (n). The construction and the protocols for read i (Clock) and update (Clock) are, with slight changes, those given in <ref> [4] </ref>. The same strong reliability results given there, provably hold in the present context. However, in [4] the clock is used in an entirely different manner, namely as an engine driving the com putation. Reading the clock is performed by sampling O (log n) locations. <p> The construction and the protocols for read i (Clock) and update (Clock) are, with slight changes, those given in <ref> [4] </ref>. The same strong reliability results given there, provably hold in the present context. However, in [4] the clock is used in an entirely different manner, namely as an engine driving the com putation. Reading the clock is performed by sampling O (log n) locations.
Reference: [5] <author> H. Chernoff, </author> <title> "A measure of asymptotic efficiency for tests of of hypothesis based on sums of observations," </title> <journal> Ann. Math. Stat., </journal> <pages> 493-509, </pages> <year> 1952. </year>
Reference-contexts: The constructions we use and the proofs in this paper involve fine points of independence of random variables and estimates of sums of random variables <ref> [5, 22] </ref>. It is a pleasant surprise that establishing the probabilistic properties of one of our memory-layout scheme requires just linear algebra and finite-field theory.
Reference: [6] <author> R. Cole and O. Zajicek, </author> <title> "The APRAM: Incorporating asynchrony into the PRAM model," </title> <booktitle> Proc. ACM SPAA, </booktitle> <pages> 169-178, </pages> <year> 1989. </year>
Reference-contexts: We pursue this direction elsewhere. Previous and Related Work Dealing with asynchrony in the context of parallel shared memory systems has attracted considerable research effort in the past few years. Most of this research focuses on asynchronous variants of the PRAM. The A-PRAM (Asynchronous PRAM) was introduced in <ref> [6] </ref> and [9], together with several specific algorithms. Further results on this and related models are presented in [7, 18]. A general scheme for simulation of synchronous PRAMs by asynchronous PRAMs is given in [16].
Reference: [7] <author> R. Cole and O. Zajicek. </author> <title> "The expected advantage of asynchrony" Proc. </title> <booktitle> 2nd ACM SPAA, </booktitle> <pages> 85-94, </pages> <year> 1990. </year>
Reference-contexts: Most of this research focuses on asynchronous variants of the PRAM. The A-PRAM (Asynchronous PRAM) was introduced in [6] and [9], together with several specific algorithms. Further results on this and related models are presented in <ref> [7, 18] </ref>. A general scheme for simulation of synchronous PRAMs by asynchronous PRAMs is given in [16]. In these papers, however, it is either assumed that the system is equipped with explicit synchronization mechanisms, or that there are bounds on the asynchronous behavior.
Reference: [8] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. schauser, E. Santos, R. Subramonian, and T. von Eicken, </author> <title> "LogP: Towards a realistic model for parallel computation," </title> <booktitle> Proc. 4th ACM SIGPLAN PPoP, </booktitle> <year> 1993. </year>
Reference-contexts: In these papers the simulation is for a PRAM program, and the solutions are restricted to this case. The change in focus, from the PRAM to models of larger granularity, is advocated in recent practically motivated studies <ref> [1, 23, 8] </ref>. In these papers alternate computation models are presented, including the BSP model in [23] and the LogP model in [8]. Algorithm for such large-granularity models, as well as complexity analysis are presented in [1, 2, 17, 19]. <p> The change in focus, from the PRAM to models of larger granularity, is advocated in recent practically motivated studies [1, 23, 8]. In these papers alternate computation models are presented, including the BSP model in [23] and the LogP model in <ref> [8] </ref>. Algorithm for such large-granularity models, as well as complexity analysis are presented in [1, 2, 17, 19]. In several of these papers (e.g. [8]) the problem of asynchrony is addressed. <p> In these papers alternate computation models are presented, including the BSP model in [23] and the LogP model in <ref> [8] </ref>. Algorithm for such large-granularity models, as well as complexity analysis are presented in [1, 2, 17, 19]. In several of these papers (e.g. [8]) the problem of asynchrony is addressed. However, in each case there is either an assumption of strong system-primitives for synchronization, or a methodology of fixed assignment of tasks to processors. <p> It would be interesting to see the implications of this methodology to programs written in the style of the BSP [23] and the LogP <ref> [8] </ref> models of parallel computation. Closely related to the aforementioned question is the adaptation of our methodology to hierarchical memory systems, with multiple memory access cost functions, and to programs written so as to capitalize on memory locality properties of the computation.
Reference: [9] <author> P. B. Gibbons, </author> <title> "A more practical PRAM model," </title> <booktitle> Proc. ACM SPAA, </booktitle> <pages> 158-168, </pages> <year> 1989. </year>
Reference-contexts: Previous and Related Work Dealing with asynchrony in the context of parallel shared memory systems has attracted considerable research effort in the past few years. Most of this research focuses on asynchronous variants of the PRAM. The A-PRAM (Asynchronous PRAM) was introduced in [6] and <ref> [9] </ref>, together with several specific algorithms. Further results on this and related models are presented in [7, 18]. A general scheme for simulation of synchronous PRAMs by asynchronous PRAMs is given in [16].
Reference: [10] <author> M. Herlihy, </author> <title> "Impossibility and universality results for wait-free synchronization," </title> <booktitle> Proc. 7th ACM PODC, </booktitle> <pages> 276-290, </pages> <year> 1988. </year>
Reference-contexts: A general scheme for simulation of synchronous PRAMs by asynchronous PRAMs is given in [16]. In these papers, however, it is either assumed that the system is equipped with explicit synchronization mechanisms, or that there are bounds on the asynchronous behavior. In other papers (see <ref> [10] </ref>) atomic read-modify-write instructions are used to achieve synchronization and prevent clobbers. The case of general unrestricted asynchronous behavior, with only atomic reads and writes, was first considered in [13]. There a randomized solution was given to the problem of simulating a synchronous PRAM on its asynchronous counter-part.
Reference: [11] <author> P. Kanellakis and A. Shvartsman, </author> <title> "Efficient parallel algorithms can be made robust," </title> <booktitle> Proc. 8th ACM PODC, </booktitle> <pages> 211-221, </pages> <year> 1989. </year>
Reference-contexts: A special form of asynchrony is the behavior of (synchronous) processors with fail-stop faults. Such faults were modeled via the fail-stop PRAM in <ref> [11] </ref>, where efficient algorithms for specific and important problems were presented. The general problem of PRAM simulations on fail-stop models was solved in [15]. Subsequent extensions of these results to PRAMs with synchronous faults with restarts can be found in [12, 14].
Reference: [12] <author> P. Kanellakis and A. Shvartsman, </author> <title> "Efficient parallel algorothms on restartable fail-stop processors," </title> <booktitle> Proc. 10th ACM PODC, </booktitle> <pages> 23-36, </pages> <year> 1991. </year>
Reference-contexts: Such faults were modeled via the fail-stop PRAM in [11], where efficient algorithms for specific and important problems were presented. The general problem of PRAM simulations on fail-stop models was solved in [15]. Subsequent extensions of these results to PRAMs with synchronous faults with restarts can be found in <ref> [12, 14] </ref>. Clearly our results apply to this restricted case as well. Our schemes combine error correction with phase-dependent random space allocation, for permanent storage of data in shared memory, and for storing temporary control variables. <p> An execution E is said to expend W work units, if W is total amount of internal clock ticks, summed up over all processors, which have elapsed throughout the course of the execution. This measure is analogous to that used in <ref> [12, 16, 13, 14, 15] </ref>, and others. In our context we compare the amount of work that the execution of C (P) requires on M, to that required for the execution of P on an ideal synchronous parallel machine.
Reference: [13] <author> Z. M. Kedem, K. V. Palem, M. O. Rabin, and A. Raghunathan, </author> <title> "Efficient program transformation for resilient parallel computation via randomization," </title> <booktitle> Proc. 24th ACM STOC, </booktitle> <year> 1992. </year>
Reference-contexts: Avoiding clobbers is the main and most subtle challenge in executing a parallel program on a general asynchronous machine. The problem of asynchronous execution has been considered in several previous papers ([6, 9, 7, 18, 13, 4]). Restricting our attention to those works which deal with unrestricted asynchronous behavior <ref> [13, 4] </ref>, we observe that these solutions are limited to programs of very fine granularity, i.e. PRAM programs. <p> In other papers (see [10]) atomic read-modify-write instructions are used to achieve synchronization and prevent clobbers. The case of general unrestricted asynchronous behavior, with only atomic reads and writes, was first considered in <ref> [13] </ref>. There a randomized solution was given to the problem of simulating a synchronous PRAM on its asynchronous counter-part. The simulation entailed a O (log 3 n) multiplicative factor in work overhead, and O (log n) factor in space. <p> An execution E is said to expend W work units, if W is total amount of internal clock ticks, summed up over all processors, which have elapsed throughout the course of the execution. This measure is analogous to that used in <ref> [12, 16, 13, 14, 15] </ref>, and others. In our context we compare the amount of work that the execution of C (P) requires on M, to that required for the execution of P on an ideal synchronous parallel machine.
Reference: [14] <author> Z.M. Kedem, K.V. Palem, A. Raghunathan, and P.G. Spirakis, </author> <title> "Combining tentative and definite executions for very fast dependable parallel computing," </title> <booktitle> Proc. 23rd ACM STOC, </booktitle> <pages> 381-390, </pages> <year> 1991. </year>
Reference-contexts: Such faults were modeled via the fail-stop PRAM in [11], where efficient algorithms for specific and important problems were presented. The general problem of PRAM simulations on fail-stop models was solved in [15]. Subsequent extensions of these results to PRAMs with synchronous faults with restarts can be found in <ref> [12, 14] </ref>. Clearly our results apply to this restricted case as well. Our schemes combine error correction with phase-dependent random space allocation, for permanent storage of data in shared memory, and for storing temporary control variables. <p> An execution E is said to expend W work units, if W is total amount of internal clock ticks, summed up over all processors, which have elapsed throughout the course of the execution. This measure is analogous to that used in <ref> [12, 16, 13, 14, 15] </ref>, and others. In our context we compare the amount of work that the execution of C (P) requires on M, to that required for the execution of P on an ideal synchronous parallel machine.
Reference: [15] <author> Z.M. Kedem, K.V. Palem, and P.G. Spirakis, </author> <title> "Efficient robust parallel computations," </title> <booktitle> Proc. 22nd ACM STOC, </booktitle> <pages> 138-148, </pages> <year> 1990. </year>
Reference-contexts: A special form of asynchrony is the behavior of (synchronous) processors with fail-stop faults. Such faults were modeled via the fail-stop PRAM in [11], where efficient algorithms for specific and important problems were presented. The general problem of PRAM simulations on fail-stop models was solved in <ref> [15] </ref>. Subsequent extensions of these results to PRAMs with synchronous faults with restarts can be found in [12, 14]. Clearly our results apply to this restricted case as well. <p> set of program variables updated by F j . (Note that Aumann, Kedem, Palem, Rabin: Highly Efficient Asynchronous Execution : : : Page 3 these sets, as well as the procedure F j itself, may be dynamically determined at run time.) Independence is achieved (for any P using techniques from <ref> [15] </ref> for example) by requiring that ( j j ) " ( j j ) = ;: Thus, within a parallel step, no thread needs as input, an output of any other thread. Consequently, within every step the n threads can be independently executed by n different parallel processors. <p> An execution E is said to expend W work units, if W is total amount of internal clock ticks, summed up over all processors, which have elapsed throughout the course of the execution. This measure is analogous to that used in <ref> [12, 16, 13, 14, 15] </ref>, and others. In our context we compare the amount of work that the execution of C (P) requires on M, to that required for the execution of P on an ideal synchronous parallel machine.
Reference: [16] <author> C. Martel, R. Subramonian, and A. Park, </author> <title> "Asynchronous PRAMs are (almost) as good as synchronous PRAMs," </title> <booktitle> Proc. 31st IEEE FOCS, </booktitle> <pages> 590-599, </pages> <year> 1990. </year>
Reference-contexts: The A-PRAM (Asynchronous PRAM) was introduced in [6] and [9], together with several specific algorithms. Further results on this and related models are presented in [7, 18]. A general scheme for simulation of synchronous PRAMs by asynchronous PRAMs is given in <ref> [16] </ref>. In these papers, however, it is either assumed that the system is equipped with explicit synchronization mechanisms, or that there are bounds on the asynchronous behavior. In other papers (see [10]) atomic read-modify-write instructions are used to achieve synchronization and prevent clobbers. <p> An execution E is said to expend W work units, if W is total amount of internal clock ticks, summed up over all processors, which have elapsed throughout the course of the execution. This measure is analogous to that used in <ref> [12, 16, 13, 14, 15] </ref>, and others. In our context we compare the amount of work that the execution of C (P) requires on M, to that required for the execution of P on an ideal synchronous parallel machine.
Reference: [17] <author> K. Mehlhorn and U. Vishkin, </author> <title> "Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memories," </title> <journal> Acta Informatica, </journal> <pages> 339-374, </pages> <year> 1990. </year>
Reference-contexts: In these papers alternate computation models are presented, including the BSP model in [23] and the LogP model in [8]. Algorithm for such large-granularity models, as well as complexity analysis are presented in <ref> [1, 2, 17, 19] </ref>. In several of these papers (e.g. [8]) the problem of asynchrony is addressed. However, in each case there is either an assumption of strong system-primitives for synchronization, or a methodology of fixed assignment of tasks to processors.
Reference: [18] <author> N. Nishimura, </author> <title> "Asynchronous shared memory parallel computation," </title> <booktitle> Proc. of the 2nd ACM SPAA, </booktitle> <pages> 76-84, </pages> <year> 1990. </year>
Reference-contexts: Most of this research focuses on asynchronous variants of the PRAM. The A-PRAM (Asynchronous PRAM) was introduced in [6] and [9], together with several specific algorithms. Further results on this and related models are presented in <ref> [7, 18] </ref>. A general scheme for simulation of synchronous PRAMs by asynchronous PRAMs is given in [16]. In these papers, however, it is either assumed that the system is equipped with explicit synchronization mechanisms, or that there are bounds on the asynchronous behavior.
Reference: [19] <author> C.H. Papadimitriou and M. Yannakakis, </author> <title> "Towards an architecture-independent analysis of parallel algorithms," </title> <booktitle> Proc. 20th ACM STOC, </booktitle> <year> 1988. </year>
Reference-contexts: In these papers alternate computation models are presented, including the BSP model in [23] and the LogP model in [8]. Algorithm for such large-granularity models, as well as complexity analysis are presented in <ref> [1, 2, 17, 19] </ref>. In several of these papers (e.g. [8]) the problem of asynchrony is addressed. However, in each case there is either an assumption of strong system-primitives for synchronization, or a methodology of fixed assignment of tasks to processors.
Reference: [20] <author> M. O. Rabin, </author> <title> "Fingerprinting by random polynomials," </title> <type> Technical Report TR-15-81, </type> <institution> Center for Research in Computer Technology, Harvard University, </institution> <year> 1981. </year>
Reference-contexts: Remember that only writing a single memory word is an atomic operation. Hence, we must guard against the possibility that a tardy processor will clobber a word within a block. To this end, we employ a fingerprinting function F <ref> [20] </ref>. If F (B) has ! bits and jBj = b + 2, then provably the probability of detecting any change in (B; F (B)), is at least 1 (b + 2)=2 ! .
Reference: [21] <author> M. O. Rabin, </author> <title> "Efficient dispersal of information for security, load balancing and fault tolerance," </title> <journal> JACM, </journal> <pages> 335-348, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: The total work for executing P is measured by W = P 0&lt;K n W . Our efficiency will be measured by comparison to this W . Information Dispersal We shall employ the bulk error correction scheme introduced under the name: the Information Dispersal Algorithm (IDA) <ref> [21] </ref>. Let (m; k) be a pair of integers, where m &lt; k.
Reference: [22] <author> P. Raghavan, </author> <title> "Probabilistic construction of deterministic algorithms: Approximating packing integer programs," </title> <booktitle> Proc. 27th ACM STOC 10-18, </booktitle> <year> 1986. </year>
Reference-contexts: The constructions we use and the proofs in this paper involve fine points of independence of random variables and estimates of sums of random variables <ref> [5, 22] </ref>. It is a pleasant surprise that establishing the probabilistic properties of one of our memory-layout scheme requires just linear algebra and finite-field theory.
Reference: [23] <author> L. G. Valiant, </author> <title> "A bridging model for parallel computation," </title> <journal> CACM, </journal> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year> <title> Aumann, Kedem, Palem, Rabin: Highly Efficient Asynchronous Execution : : : Page 10 </title>
Reference-contexts: In these papers the simulation is for a PRAM program, and the solutions are restricted to this case. The change in focus, from the PRAM to models of larger granularity, is advocated in recent practically motivated studies <ref> [1, 23, 8] </ref>. In these papers alternate computation models are presented, including the BSP model in [23] and the LogP model in [8]. Algorithm for such large-granularity models, as well as complexity analysis are presented in [1, 2, 17, 19]. <p> The change in focus, from the PRAM to models of larger granularity, is advocated in recent practically motivated studies [1, 23, 8]. In these papers alternate computation models are presented, including the BSP model in <ref> [23] </ref> and the LogP model in [8]. Algorithm for such large-granularity models, as well as complexity analysis are presented in [1, 2, 17, 19]. In several of these papers (e.g. [8]) the problem of asynchrony is addressed. <p> We provide in the present paper a general method for compiling any given parallel program P so that it will run on an arbitrarily asynchronous parallel machine. It would be interesting to see the implications of this methodology to programs written in the style of the BSP <ref> [23] </ref> and the LogP [8] models of parallel computation. Closely related to the aforementioned question is the adaptation of our methodology to hierarchical memory systems, with multiple memory access cost functions, and to programs written so as to capitalize on memory locality properties of the computation.
References-found: 23

