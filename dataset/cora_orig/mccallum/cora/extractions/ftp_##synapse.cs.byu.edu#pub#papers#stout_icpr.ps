URL: ftp://synapse.cs.byu.edu/pub/papers/stout_icpr.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: A VLSI Implementation of a Parallel, Self-Organizing Learning Model  
Author: Matthew G. Stout George L. Rudolph Linton G. Salmon Tony R. Martinez 
Address: Provo, UT 84602 Provo, UT 84602  
Affiliation: Department of Department of Computer Science Electrical and Computer Engineering Brigham Young University Brigham Young University  
Abstract: This paper presents a VLSI implementation of the Priority Adaptive Self-Organizing Concurrent System (PASOCS) learning model that is built using a multi-chip module (MCM) substrate. Many current hardware implementations of neural network learning models are direct implementations of classical neural network structures|a large number of simple computing nodes connected by a dense number of weighted links. PASOCS is one of a class of ASOCS (Adaptive Self-Organizing Concurrent System) connectionist models whose overall goal is the same as classical neural networks models, but whose functional mechanisms differ significantly. This model has potential application in areas such as pattern recognition, robotics, logical inference, and dynamic control. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Intel Corporation. </author> <title> 80170NX Electrically Trainable Analog Neural Network, </title> <month> Jun </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Over the past few years, many companies and researchers have announced hardware implementations of neural networks <ref> [1, 2, 3, 4] </ref>. The integrated circuits (ICs) cited are representative of much current neural network implementation research. They are direct implementations of classical neural network structures| a large number of simple computing nodes connected by a dense number of weighted links [4, 5, 6].
Reference: [2] <author> American NeuraLogix, Inc. </author> <title> NLX420 Neural Processor Slice, </title> <month> Feb </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Over the past few years, many companies and researchers have announced hardware implementations of neural networks <ref> [1, 2, 3, 4] </ref>. The integrated circuits (ICs) cited are representative of much current neural network implementation research. They are direct implementations of classical neural network structures| a large number of simple computing nodes connected by a dense number of weighted links [4, 5, 6].
Reference: [3] <author> M. Wright. </author> <title> Neural-network IC architectures define suitable applications. </title> <booktitle> EDN, </booktitle> <pages> pages 84-90, </pages> <month> Jul </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Over the past few years, many companies and researchers have announced hardware implementations of neural networks <ref> [1, 2, 3, 4] </ref>. The integrated circuits (ICs) cited are representative of much current neural network implementation research. They are direct implementations of classical neural network structures| a large number of simple computing nodes connected by a dense number of weighted links [4, 5, 6].
Reference: [4] <author> M. Verleysen and P. G. A. Jespers. </author> <title> An analog VLSI implementation of hopfield's neural network. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 47-55, </pages> <month> Dec </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Over the past few years, many companies and researchers have announced hardware implementations of neural networks <ref> [1, 2, 3, 4] </ref>. The integrated circuits (ICs) cited are representative of much current neural network implementation research. They are direct implementations of classical neural network structures| a large number of simple computing nodes connected by a dense number of weighted links [4, 5, 6]. <p> The integrated circuits (ICs) cited are representative of much current neural network implementation research. They are direct implementations of classical neural network structures| a large number of simple computing nodes connected by a dense number of weighted links <ref> [4, 5, 6] </ref>. The node function is typically a variation on sum-of-products [7]. Early learning models and implementations support only static topologies in the sense that learning does not involve changes in the network topology. More recent models support dynamic topologies in learning, and therefore implementations should also support this.
Reference: [5] <author> D. Hammerstrom, T. Leen, and E. </author> <title> Means. Dynamics and VLSI implementation of self-organizing networks. </title> <booktitle> In Advanced Neural Computers, </booktitle> <pages> pages 185-92. </pages> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: The integrated circuits (ICs) cited are representative of much current neural network implementation research. They are direct implementations of classical neural network structures| a large number of simple computing nodes connected by a dense number of weighted links <ref> [4, 5, 6] </ref>. The node function is typically a variation on sum-of-products [7]. Early learning models and implementations support only static topologies in the sense that learning does not involve changes in the network topology. More recent models support dynamic topologies in learning, and therefore implementations should also support this.
Reference: [6] <author> U. </author> <title> Ramacher. Hardware concepts for neural networks. </title> <booktitle> In Advanced Neural Computers, </booktitle> <pages> pages 209-18. </pages> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: The integrated circuits (ICs) cited are representative of much current neural network implementation research. They are direct implementations of classical neural network structures| a large number of simple computing nodes connected by a dense number of weighted links <ref> [4, 5, 6] </ref>. The node function is typically a variation on sum-of-products [7]. Early learning models and implementations support only static topologies in the sense that learning does not involve changes in the network topology. More recent models support dynamic topologies in learning, and therefore implementations should also support this.
Reference: [7] <author> D. E. Rumelhart, J. L. McClelland, </author> <title> and the PDP Research Group. Explorations in Parallel Distributed Processing: A Handbook of Models, Programs, and Exercises. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: They are direct implementations of classical neural network structures| a large number of simple computing nodes connected by a dense number of weighted links [4, 5, 6]. The node function is typically a variation on sum-of-products <ref> [7] </ref>. Early learning models and implementations support only static topologies in the sense that learning does not involve changes in the network topology. More recent models support dynamic topologies in learning, and therefore implementations should also support this. The style of implementation presented here accomplishes this goal.
Reference: [8] <author> T. R. Martinez and D. M. Campbell. </author> <title> A self-adjusting dynamic logic module. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11(4) </volume> <pages> 303-13, </pages> <year> 1991. </year>
Reference-contexts: More recent models support dynamic topologies in learning, and therefore implementations should also support this. The style of implementation presented here accomplishes this goal. The implementation described is based on the connectionist architecture Adaptive Self-Organizing Concurrent System (ASOCS) <ref> [8, 9, 10, 11] </ref>. The primary goal of an ASOCS is similar to the goals of many decision-making connectionist systems|the system attempts to learn an arbitrary set of input-to-output vector mappings. However, an ASOCS differs from many other connectionist systems in the manner that this task is accomplished.
Reference: [9] <author> T. R. Martinez and D. M. Campbell. </author> <title> A self-organizing binary decision tree for incrementally defined rule based systems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(5) </volume> <pages> 1231-7, </pages> <month> Sep/Oct </month> <year> 1991. </year>
Reference-contexts: More recent models support dynamic topologies in learning, and therefore implementations should also support this. The style of implementation presented here accomplishes this goal. The implementation described is based on the connectionist architecture Adaptive Self-Organizing Concurrent System (ASOCS) <ref> [8, 9, 10, 11] </ref>. The primary goal of an ASOCS is similar to the goals of many decision-making connectionist systems|the system attempts to learn an arbitrary set of input-to-output vector mappings. However, an ASOCS differs from many other connectionist systems in the manner that this task is accomplished.
Reference: [10] <author> T. R. Martinez. </author> <title> Consistency and generalization of incrementally trained connectionist models. </title> <booktitle> In International Symposium on Circuits and Systems, </booktitle> <pages> pages 706-9, </pages> <year> 1990. </year>
Reference-contexts: More recent models support dynamic topologies in learning, and therefore implementations should also support this. The style of implementation presented here accomplishes this goal. The implementation described is based on the connectionist architecture Adaptive Self-Organizing Concurrent System (ASOCS) <ref> [8, 9, 10, 11] </ref>. The primary goal of an ASOCS is similar to the goals of many decision-making connectionist systems|the system attempts to learn an arbitrary set of input-to-output vector mappings. However, an ASOCS differs from many other connectionist systems in the manner that this task is accomplished. <p> In this way, an ASOCS can change its structure to suit a particular problem. This paper presents an implementation of a version of ASOCS called PASOCS (Priority ASOCS) <ref> [10, 11] </ref> that is built on a multi-chip module (MCM) interconnection substrate. Although this implementation is modeled specifically after the PASOCS model, the general design of this MCM-based system is versatile and can be modified to reflect other connectionist models [10, 11, 12]. <p> Although this implementation is modeled specifically after the PASOCS model, the general design of this MCM-based system is versatile and can be modified to reflect other connectionist models <ref> [10, 11, 12] </ref>. This model has potential application in areas such as pattern recognition, robotics, logical inference, and dynamic control. Due to space considerations, background information dealing with the PASOCS learning model is not presented here but can be found in [11, 13].
Reference: [11] <author> T. R. Martinez, D. M. Campbell, and B. W. Hughes. </author> <title> Priority ASOCS. </title> <journal> Journal of Artificial Neural Networks, </journal> <volume> 1(3), </volume> <year> 1994. </year>
Reference-contexts: More recent models support dynamic topologies in learning, and therefore implementations should also support this. The style of implementation presented here accomplishes this goal. The implementation described is based on the connectionist architecture Adaptive Self-Organizing Concurrent System (ASOCS) <ref> [8, 9, 10, 11] </ref>. The primary goal of an ASOCS is similar to the goals of many decision-making connectionist systems|the system attempts to learn an arbitrary set of input-to-output vector mappings. However, an ASOCS differs from many other connectionist systems in the manner that this task is accomplished. <p> In this way, an ASOCS can change its structure to suit a particular problem. This paper presents an implementation of a version of ASOCS called PASOCS (Priority ASOCS) <ref> [10, 11] </ref> that is built on a multi-chip module (MCM) interconnection substrate. Although this implementation is modeled specifically after the PASOCS model, the general design of this MCM-based system is versatile and can be modified to reflect other connectionist models [10, 11, 12]. <p> Although this implementation is modeled specifically after the PASOCS model, the general design of this MCM-based system is versatile and can be modified to reflect other connectionist models <ref> [10, 11, 12] </ref>. This model has potential application in areas such as pattern recognition, robotics, logical inference, and dynamic control. Due to space considerations, background information dealing with the PASOCS learning model is not presented here but can be found in [11, 13]. <p> This model has potential application in areas such as pattern recognition, robotics, logical inference, and dynamic control. Due to space considerations, background information dealing with the PASOCS learning model is not presented here but can be found in <ref> [11, 13] </ref>. The hardware implementation is described in Section 2, and the PASOCS MCM system, its test results, and ideas for related research are presented in Section 3. 2 Implementation The IC described in this section was fabricated in 2m digital CMOS.
Reference: [12] <author> G. Rudolph and T.R. Martinez. </author> <title> An efficient static topology for modeling ASOCS. </title> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 729-34, </pages> <year> 1991. </year>
Reference-contexts: Although this implementation is modeled specifically after the PASOCS model, the general design of this MCM-based system is versatile and can be modified to reflect other connectionist models <ref> [10, 11, 12] </ref>. This model has potential application in areas such as pattern recognition, robotics, logical inference, and dynamic control. Due to space considerations, background information dealing with the PASOCS learning model is not presented here but can be found in [11, 13].
Reference: [13] <author> M. G. Stout. </author> <title> Multi-chip module design for neural networks. </title> <type> Master's thesis, </type> <institution> Brigham Young University, </institution> <year> 1994. </year>
Reference-contexts: This model has potential application in areas such as pattern recognition, robotics, logical inference, and dynamic control. Due to space considerations, background information dealing with the PASOCS learning model is not presented here but can be found in <ref> [11, 13] </ref>. The hardware implementation is described in Section 2, and the PASOCS MCM system, its test results, and ideas for related research are presented in Section 3. 2 Implementation The IC described in this section was fabricated in 2m digital CMOS. <p> It should be noted that these are problems with the specific implementation of the ICs and not with the functionality of the PASOCS model or with the original conceptual design of the ICs, described in more detail in <ref> [13] </ref>. Despite these problems, most of the functions of the three-node PASOCS are functioning according to original design specifications. A detailed report of the test results can be found in [13]. In addition to the research reported here, ASOCS models other than PASOCS are also being investigated. <p> the functionality of the PASOCS model or with the original conceptual design of the ICs, described in more detail in <ref> [13] </ref>. Despite these problems, most of the functions of the three-node PASOCS are functioning according to original design specifications. A detailed report of the test results can be found in [13]. In addition to the research reported here, ASOCS models other than PASOCS are also being investigated. These and other more classical neural network models can benefit from the results presented in this paper. Current research seeks to extend the general ideas presented here to other models. <p> These and other more classical neural network models can benefit from the results presented in this paper. Current research seeks to extend the general ideas presented here to other models. Other related research seeks high-density interconnect technologies that can be exploited to create larger neural network systems <ref> [13, 14] </ref>. MCM interconnection and packaging techniques offer a promising solution to the high interconnect and processing element densities required for such hardware implementations. The MCM for this prototype was fabricated in the Integrated Microelectronics Laboratory at Brigham Young University.
Reference: [14] <author> M. G. Stout, G. L. Rudolph, L. G. Salmon, and T. R. Martinez. </author> <title> A multi-chip module implementation of a neural network. </title> <booktitle> In Proceedings of the IEEE Multi-Chip Module Conference, </booktitle> <pages> pages 20-5, </pages> <year> 1994. </year>
Reference-contexts: These and other more classical neural network models can benefit from the results presented in this paper. Current research seeks to extend the general ideas presented here to other models. Other related research seeks high-density interconnect technologies that can be exploited to create larger neural network systems <ref> [13, 14] </ref>. MCM interconnection and packaging techniques offer a promising solution to the high interconnect and processing element densities required for such hardware implementations. The MCM for this prototype was fabricated in the Integrated Microelectronics Laboratory at Brigham Young University.
References-found: 14

