URL: http://www.cs.utah.edu/~beazley/md.ps
Refering-URL: http://www.cs.utah.edu/~beazley/publications.html
Root-URL: 
Email: fdmb,pxlg@viking.lanl.gov  
Title: Message-Passing Multi-Cell Molecular Dynamics on the Connection Machine 5  
Author: D. M. Beazley and P. S. Lomdahl 
Note: To appear in Parallel Computing (1993) Present address:  OR 97403. To whom correspondence should be addressed.  
Date: April 27, 1993  
Address: Los Alamos, New Mexico 87545  
Affiliation: Theoretical Division and Advanced Computing Laboratory Los Alamos National Laboratory,  Department of Mathematics, University of Oregon, Eugene,  
Abstract: We present a new scalable algorithm for short-range molecular dynamics simulations on distributed memory MIMD multicomputer based on a message-passing multi-cell approach. We have implemented the algorithm on the Connection Machine 5 (CM-5) and demonstrate that meso-scale molecular dynamics with more than 10 8 particles is now possible on massively parallel MIMD computers. Typical runs show single particle update-times of 0:15s in 2 dimensions (2D) and approximately 1s in 3 dimensions (3D) on a 1024 node CM-5 without vector units, corresponding to more than 1.8 GFlops overall performance. We also present a scaling equation which agrees well with actually observed timings. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Molecular Dynamics Simulations of Statistical-Mechanical Systems, </institution> <note> Eds. </note> <author> G. Ciccotti and W. G. </author> <title> Hoover, </title> <publisher> North-Holland, </publisher> <address> Amsterdam (1986). </address>
Reference-contexts: 1 Introduction The molecular dynamics (MD) method <ref> [1, 2, 3] </ref> has been known for several decades and has been used successfully in atomistic simulation models of thousands of interacting particles to describe structural and dynamical properties of simple physical systems, such as liquids and solids. <p> We have implemented the Verlet algorithm and a stochastic Runge-Kutta algorithm. The Verlet algorithm is a multi-step integrator requiring only one force evaluation per time-step <ref> [1, 2] </ref>. Although the algorithm is a multi-step integrator, it has the advantage of being self starting since only the positions and velocities are needed initially. A second-order Runge-Kutta scheme that has been adapted to solve stochastic differential equations for Langevin dynamics [15] has also been implemented.
Reference: [2] <institution> Computer Simulations of Liquids, </institution> <note> M. </note> <editor> P. Allen and D. J. Tildesley. </editor> <publisher> Clarendon Press, </publisher> <address> Oxford (1987). </address>
Reference-contexts: 1 Introduction The molecular dynamics (MD) method <ref> [1, 2, 3] </ref> has been known for several decades and has been used successfully in atomistic simulation models of thousands of interacting particles to describe structural and dynamical properties of simple physical systems, such as liquids and solids. <p> We have implemented the Verlet algorithm and a stochastic Runge-Kutta algorithm. The Verlet algorithm is a multi-step integrator requiring only one force evaluation per time-step <ref> [1, 2] </ref>. Although the algorithm is a multi-step integrator, it has the advantage of being self starting since only the positions and velocities are needed initially. A second-order Runge-Kutta scheme that has been adapted to solve stochastic differential equations for Langevin dynamics [15] has also been implemented.
Reference: [3] <editor> Molecular Dynamics Simulations, </editor> <booktitle> Proc. of the 13th Taniguchi Symposium, Kashikojima, Japan (1990), </booktitle> <editor> Ed. F. Yonezawa, </editor> <publisher> Springer-Verlag, </publisher> <address> New York (1992). </address>
Reference-contexts: 1 Introduction The molecular dynamics (MD) method <ref> [1, 2, 3] </ref> has been known for several decades and has been used successfully in atomistic simulation models of thousands of interacting particles to describe structural and dynamical properties of simple physical systems, such as liquids and solids.
Reference: [5] <author> W. C. Swope and H. C. </author> <title> Andersen, </title> <journal> Phys. Rev. </journal> <volume> B 41, </volume> <month> 7042 </month> <year> (1990). </year>
Reference-contexts: However, new generations of supercomputers are rapidly changing this picture. Simulations with 10 6 atoms have been performed on conventional vector supercomputers <ref> [5] </ref>. However, it is parallel multicomputers that seem to hold the greatest potential for reaching beyond 10 9 atoms. Coarse-grained transputer systems have been used in successful 2D simulations with 10 6 atoms [6].
Reference: [6] <author> W. G. Hoover et al. </author> , <note> Phys. Rev. A 42, 5844 (1990). </note>
Reference-contexts: Simulations with 10 6 atoms have been performed on conventional vector supercomputers [5]. However, it is parallel multicomputers that seem to hold the greatest potential for reaching beyond 10 9 atoms. Coarse-grained transputer systems have been used in successful 2D simulations with 10 6 atoms <ref> [6] </ref>. Data parallel SIMD implementations on the CM-2 have also recently been presented [7, 8] which have the potential for multi-million particle simulations. In fact, recent work by Holian and collaborators [9, 10] have demonstrated 2D simulations for 1:6 fi 10 7 atoms on a 64K processor CM-200.
Reference: [7] <author> A. I. Mel'cuk, R. C. Giles, and H. Gould, </author> <title> Computers in Physics, </title> <note> May/June 1991, p. 311. </note>
Reference-contexts: However, it is parallel multicomputers that seem to hold the greatest potential for reaching beyond 10 9 atoms. Coarse-grained transputer systems have been used in successful 2D simulations with 10 6 atoms [6]. Data parallel SIMD implementations on the CM-2 have also recently been presented <ref> [7, 8] </ref> which have the potential for multi-million particle simulations. In fact, recent work by Holian and collaborators [9, 10] have demonstrated 2D simulations for 1:6 fi 10 7 atoms on a 64K processor CM-200. <p> The cut-off distance is r max = 1:711238. Unlike truncated LJ-potentials <ref> [7] </ref>, the LJ-spl potential has a continuous derivative at r max . The number of interacting neighbors for each particle depends on the value of the cut-off distance r max and the particle density . <p> The calculation of forces for all of the particles in a cell is a two step process. First, all of the forces between particles in the same cell are calculated. Next, forces from particles in the neighboring cells are calculated by following an interaction path <ref> [7] </ref> that describes how we compute the interactions with neighboring cells. In 2D the path is as shown in Fig. 2.
Reference: [8] <author> P. Tamayo, J. P. Mesirov, and B. M. </author> <title> Boghosian, </title> <booktitle> Proc. of Supercomputing 91, IEEE Computer Society (1991), </booktitle> <address> p. </address> <month> 462. </month>
Reference-contexts: However, it is parallel multicomputers that seem to hold the greatest potential for reaching beyond 10 9 atoms. Coarse-grained transputer systems have been used in successful 2D simulations with 10 6 atoms [6]. Data parallel SIMD implementations on the CM-2 have also recently been presented <ref> [7, 8] </ref> which have the potential for multi-million particle simulations. In fact, recent work by Holian and collaborators [9, 10] have demonstrated 2D simulations for 1:6 fi 10 7 atoms on a 64K processor CM-200.
Reference: [9] <author> B. L. Holian et al. </author> <note> Phys. Rev. A 43, 2655 (1991) and private communications. </note>
Reference-contexts: Coarse-grained transputer systems have been used in successful 2D simulations with 10 6 atoms [6]. Data parallel SIMD implementations on the CM-2 have also recently been presented [7, 8] which have the potential for multi-million particle simulations. In fact, recent work by Holian and collaborators <ref> [9, 10] </ref> have demonstrated 2D simulations for 1:6 fi 10 7 atoms on a 64K processor CM-200. Work on MIMD implementaions of large scale MD is now also starting to appear [11, 12]. <p> In the case where long-range interactions (like electrostatic and gravitational forces) can not be neglected, efficient parallel algorithms based on hierarchical tree structures are also becoming available [13]. As the prototypical short-range interaction potential we have taken the Lennard-Jones spline (LJ-spl) potential <ref> [9] </ref>. This potential has the usual Lennard-Jones 6-12 form modified with a cubic spline that makes the potential go exactly to zero at a distance r max . <p> For the LJ-spl potential above, this corresponds to approximately 9 2 neighbors in 2D and 21 3 in 3D. We should note that our algorithm is not dependent on the exact nature of the potential. More sophisticated many-body potentials like EAM <ref> [9] </ref> can be accommodated too. 3 Message-passing and the CM-5 The CM-5 [14] is a massively parallel multicomputer containing between 32 and 16384 processing nodes (PN), each of which consists of a 33 MHz SPARC processor, up to 32 Mbytes of memory and an optional 128 MFlop vector-processing unit (VU) for
Reference: [10] <author> N. J. Wagner, B. L. Holian, and A. F. Voter, </author> <note> Phys. Rev. A 45, 8457 (1992). </note>
Reference-contexts: Coarse-grained transputer systems have been used in successful 2D simulations with 10 6 atoms [6]. Data parallel SIMD implementations on the CM-2 have also recently been presented [7, 8] which have the potential for multi-million particle simulations. In fact, recent work by Holian and collaborators <ref> [9, 10] </ref> have demonstrated 2D simulations for 1:6 fi 10 7 atoms on a 64K processor CM-200. Work on MIMD implementaions of large scale MD is now also starting to appear [11, 12].
Reference: [11] <author> R. C. Giles and P. Tamayo, </author> <booktitle> Proc. of SHPCC'92, IEEE Computer Society (1992), </booktitle> <address> p. </address> <month> 240. </month>
Reference-contexts: In fact, recent work by Holian and collaborators [9, 10] have demonstrated 2D simulations for 1:6 fi 10 7 atoms on a 64K processor CM-200. Work on MIMD implementaions of large scale MD is now also starting to appear <ref> [11, 12] </ref>. In this paper we present a scalable parallel MD algorithm which allows for the simulation of at least 10 8 particles interacting via a relative short range potential. <p> This fact, allow us to update the data structures for a very large system of particles very quickly. The redistribution algorithm rebuilds all necessary data structures. There is no searching for neighbors or the construction of neighbor lists <ref> [11] </ref>. There is also an advantage in that this procedure is called after every time-step so we are assured that the data is in the proper layout before each force calculation. As mentioned earlier, the redistribution procedure tends to fragment data.
Reference: [12] <author> S. Plimpton and G. Heffelfinger, </author> <booktitle> Proc. of SHPCC'92, IEEE Computer Society (1992), </booktitle> <address> p. </address> <month> 246. </month>
Reference-contexts: In fact, recent work by Holian and collaborators [9, 10] have demonstrated 2D simulations for 1:6 fi 10 7 atoms on a 64K processor CM-200. Work on MIMD implementaions of large scale MD is now also starting to appear <ref> [11, 12] </ref>. In this paper we present a scalable parallel MD algorithm which allows for the simulation of at least 10 8 particles interacting via a relative short range potential.
Reference: [13] <author> Parallel Hierarchical N-body Methods, J. K. Salmon, Ph. D. </author> <type> Thesis, </type> <address> Caltech (1991). </address>
Reference-contexts: In this paper we will assume that this is the case and will be concerned exclusively with this short-range approximation. In the case where long-range interactions (like electrostatic and gravitational forces) can not be neglected, efficient parallel algorithms based on hierarchical tree structures are also becoming available <ref> [13] </ref>. As the prototypical short-range interaction potential we have taken the Lennard-Jones spline (LJ-spl) potential [9]. This potential has the usual Lennard-Jones 6-12 form modified with a cubic spline that makes the potential go exactly to zero at a distance r max .
Reference: [14] <editor> C. E. Leiserson et al. </editor> <booktitle> in Proc. of Symposium on Parallel and Distributed Algorithms '92, </booktitle> <address> June 1992, San Diego. </address>
Reference-contexts: We should note that our algorithm is not dependent on the exact nature of the potential. More sophisticated many-body potentials like EAM [9] can be accommodated too. 3 Message-passing and the CM-5 The CM-5 <ref> [14] </ref> is a massively parallel multicomputer containing between 32 and 16384 processing nodes (PN), each of which consists of a 33 MHz SPARC processor, up to 32 Mbytes of memory and an optional 128 MFlop vector-processing unit (VU) for 64 bit floating-point and integer arithmetic.
Reference: [15] <author> H. S. Greenside and E. </author> <title> Helfand, </title> <journal> Bell Syst. Tech. J. </journal> <volume> 60, </volume> <year> 1927 (1981). </year> <month> 17 </month>
Reference-contexts: Although the algorithm is a multi-step integrator, it has the advantage of being self starting since only the positions and velocities are needed initially. A second-order Runge-Kutta scheme that has been adapted to solve stochastic differential equations for Langevin dynamics <ref> [15] </ref> has also been implemented. The advantage of this algorithm is that it allows simulations within the canonical ensemble which has constant temperature. The disadvantage is that it requires two force evaluations per time-step. With no noise, this method reduces to the standard 2nd-order Runge-Kutta method for solving ODE's.
References-found: 14

