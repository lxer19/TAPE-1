URL: http://www.ncc.up.pt/~ltorgo/Papers/SCD.ps.gz
Refering-URL: http://www.ncc.up.pt/~ltorgo/Papers/list_pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Search-based Class Discretization  
Author: Lus Torgo Joo Gama 
Keyword: Regression, Classification, Discretization methods.  
Address: 823 4150 Porto Portugal  
Affiliation: University of Porto R. Campo Alegre,  
Email: email  email  
Phone: LIACC  Phone (+351) 2 6001672 Fax (+351) 2 6003654  
Web: ltorgo ncc.up.pt  jgama ncc.up.pt  WWW http://www.ncc.up.pt/liacc/ML  
Abstract: We present a methodology that enables the use of classification algorithms on regression tasks. We implement this method in system RECLA that transforms a regression problem into a classification one and then uses an existent classification system to solve this new problem. The transformation consists of mapping a continuous variable into an ordinal variable by grouping its values into an appropriate set of intervals. We use misclassification costs as a means to reflect the implicit ordering among the ordinal values of the new variable. We describe a set of alternative discretization methods and, based on our experimental results, justify the need for a search-based approach to choose the best method. Our experimental results confirm the validity of our search-based approach to class discretization, and reveal the accuracy benefits of adding misclassification costs. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman,L. , Friedman,J.H., Olshen,R.A. & Stone,C.J. </author> <year> (1984): </year> <title> Classification and Regression Trees , Wadsworth Int. Group, </title> <address> Belmont, California, USA, 1984. Bhattacharyya,G., Johnson,R. </address> <publisher> (1977) : Statistical Concepts and Methods . John Wiley & Sons. </publisher>
Reference: <author> Clark, P. and Niblett, T. </author> <title> (1988) : The CN2 induction algorithm. </title> <booktitle> In Machine Learning , 3 . Dillon,W. and Goldstein,M. </booktitle> <publisher> (1984) : Multivariate Analysis . John Wiley & Sons, Inc. </publisher>
Reference-contexts: We use a wrapper technique (John et al., 1994; Kohavi, 1995) as a method for finding near-optimal settings for this mapping task. We have tested our methodology on four regression domains with three different classification systems : C4.5 (Quinlan, 1993); CN2 <ref> (Clark & Nibblet, 1988) </ref>; and a linear discriminant (Fisher, 1936; Dillon & Goldstein, 1984). The results show the validity of our search-based approach and the gains in accuracy obtained by adding misclassification costs to classification algorithms.
Reference: <author> Fayyad, U.M., and Irani, </author> <title> K.B. (1993) : Multi-interval Discretization of Continuous-valued Attributes for Classification Learning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence (IJCAI-93) . Morgan Kaufmann Publishers. </booktitle>
Reference-contexts: Finally, we have introduced the use of misclassification costs to overcome the inadequacy of classification systems to deal with ordinal target variables. Previous work on continuous attribute discretization usually proceeds by trying to maximize the mutual information between the resulting discrete attribute and the classes <ref> (Fayyad & Irani, 1993) </ref>. This strategy is applicable only when the classes are given. Ours is a different problem, as we are determining which classes to consider. 6 Conclusions The method described in this paper enables the use of classification systems on regression tasks.
Reference: <author> Fisher, R.A. </author> <title> (1936) : The use of multiple measurements in taxonomic problems. </title> <note> Annals of Eugenics , 7 , 179-188. </note>
Reference: <author> Fix, E., Hodges, J.L. </author> <title> (1951) : Discriminatory analysis, nonparametric discrimination consistency properties. </title> <type> Technical Report 4, </type> <institution> Randolph Field, TX: US Air Force, School of Aviation Medicine. </institution>
Reference-contexts: We have tried to find out this effect by obtaining the results of some pure regression tools on the same data sets using the same experimental methodology. Table 5 shows the results obtained by a regression tree similar to CART (Breiman et al., 1984), a 3-nearest neighbor algorithm <ref> (Fix & Hodges, 1951) </ref> and a standard linear regression method : Dataset Algorithm MAE MAPE Servo Regression tree 0.43 - 0.4 34.0 - 9.9 3-NN 0.52 - 0.11 57.1 - 17.3 Linear Regression 0.87 - 0.07 104.5 - 22.7 Auto-Mpg Regression tree 2.6 - 0.3 11.22 - 0.9 3-NN 2.4 -
Reference: <author> John,G.H., Kohavi,R. and Pfleger, K. </author> <title> (1994) : Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the 11th IML . Morgan Kaufmann. </booktitle>
Reference: <author> Kohavi, R. </author> <title> (1995) : Wrappers for performance enhancement and oblivious decision graphs. </title> <type> PhD Thesis. </type>
Reference-contexts: The wrapper approach (John et al., 1994; Kohavi, 1995) is a well known strategy has been mainly used for feature subset selection (John et al., 1994) and parameter estimation <ref> (Kohavi, 1995) </ref>.
Reference: <author> Merz,C.J., Murphy,P.M. </author> <note> (1996) : UCI repository of machine learning databases [http://www.ics.uci.edu/MLReposiroty.html]. Irvine, </note> <institution> CA. University of California, Department of Information and Computer Science. </institution>
Reference: <author> Quinlan, J. R. </author> <note> (1993) : C4.5 : programs for machine learning . Morgan Kaufmann Publishers. </note>
Reference-contexts: We use a wrapper technique (John et al., 1994; Kohavi, 1995) as a method for finding near-optimal settings for this mapping task. We have tested our methodology on four regression domains with three different classification systems : C4.5 <ref> (Quinlan, 1993) </ref>; CN2 (Clark & Nibblet, 1988); and a linear discriminant (Fisher, 1936; Dillon & Goldstein, 1984). The results show the validity of our search-based approach and the gains in accuracy obtained by adding misclassification costs to classification algorithms.
Reference: <author> Stone, M. </author> <title> (1974) : Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society , B 36 , 111-147. </journal>
Reference-contexts: The RECLA system allows the user to explicitly select one of these methods. If this is not done the system automatically selects the one that gives better estimated results. The other important component of the wrapper approach is the evaluation strategy. We use a N-fold Cross Validation <ref> (Stone, 1974) </ref> estimation technique which is well-known for its reliable estimates of prediction error. This means that each time a new tentative set of intervals is generated RECLA uses an internal N-fold Cross Validation (CV) process to evaluate it.
Reference: <author> Weiss, S. and Indurkhya, N. </author> <title> (1993) : Rule-base Regression. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence , pp. </booktitle> <pages> 1072-1078. </pages>
Reference: <author> Weiss, S. and Indurkhya, N. </author> <title> (1995) : Rule-based Machine Learning Methods for Functional Prediction. </title> <journal> In Journal Of Artificial Intelligence Research (JAIR), </journal> <volume> volume 3, </volume> <month> pp.383-403. </month>
References-found: 12

