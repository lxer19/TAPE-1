URL: ftp://ftp.neuroinformatik.ruhr-uni-bochum.de/pub/manuscripts/articles/IEEEWeights.ps.gz
Refering-URL: http://www.cnl.salk.edu/~wiskott/Bibliographies/FaceProcessing.html
Root-URL: http://www.cnl.salk.edu/~wiskott/Bibliographies/FaceProcessing.html
Title: An Algorithm for the Learning of Weights in Discrimination Functions using a priori Constraints  
Author: Norbert Kruger 
Note: Accepted for "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)"  
Abstract: We introduce a learning algorithm for the weights in a very common class of discrimination functions usually called "weighted average". The learning algorithm can reduce the number of free variables by simple but effective a priori criteria about significant features. Here we apply our algorithm to three tasks of different dimensionality all concerned with face recognition. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.O. Berger, </author> <title> Statistical Decision Theory; foundations, concepts and methods (2en ed), </title> <address> New York, </address> <publisher> Springer, </publisher> <year> 1985. </year> <title> 2 The maximum function is only needed to ensure that the power function x ff 2 is defined. </title> <type> 7 </type>
Reference-contexts: We would like to stress that our algorithm is not restricted to face recognition, but is able to deal with any problem which fits into the formalism defined above. 2 Problems in choosing suitable discrimination func tions The approach of statistical decision theory is described in detail in <ref> [1] </ref> and [4]. Bayes' formula P (cjI) = P (I) gives the optimal discrimination function we can achieve.
Reference: [2] <author> J.G. Daugman, </author> <title> "Uncertainty relation for resolution in space, spatial frequency, and orien-tation optimized by 2D visual cortical filters", </title> <journal> Journal of the Optical Society of America vol. </journal> <volume> 2 (7), </volume> <pages> pp. </pages> <month> 1160-1169 </month> <year> (1985). </year>
Reference-contexts: simple and evident and are controlled by the learning which also gives the required flexibility to find a solution adapted to the problem. 3 The face recognition system As basic visual feature we use a local image descriptor in the form of a vector I s;o (x; y) called "jet" <ref> [2, 11] </ref>. Each component (s; o) of a jet is a Gabor wavelet of specific frequency s and orientation o, extracted from the image at a definite point (x; y). We are employing Gabor wavelets of 5 different frequencies and 8 different orientations for a total of 40 complex components.
Reference: [3] <author> C. De Boor, </author> <title> A practical Guide to Splines, </title> <address> New York, </address> <publisher> Springer Verlag, </publisher> <year> 1978. </year>
Reference-contexts: We have to find functions j i 2 C [a i ; b i ]; i : 1; : : : ; 3 which are suitable as parametrization functions. A very general approach is to approximate j i with splines (see <ref> [3] </ref> for a more precise description of splines). We have done this for task 1 and task 2. A spline can be defined with different numbers of free variables.
Reference: [4] <author> T.S. Ferguson, </author> <title> Mathematical statistics: A decision theoritic approach, </title> <address> New York, </address> <publisher> Academic Press, </publisher> <year> 1967. </year>
Reference-contexts: We would like to stress that our algorithm is not restricted to face recognition, but is able to deal with any problem which fits into the formalism defined above. 2 Problems in choosing suitable discrimination func tions The approach of statistical decision theory is described in detail in [1] and <ref> [4] </ref>. Bayes' formula P (cjI) = P (I) gives the optimal discrimination function we can achieve.
Reference: [5] <author> K. Fukunaga, </author> <title> Introduction to statistical pattern recognition (2nd ed), </title> <address> Boston, </address> <publisher> Academic Press, </publisher> <address> Boston 1990. </address>
Reference-contexts: problem arises to estimate the parameters P (c) and the conditional densities P (Ijc) which are N -dimensional functions for every c. 1 Therefore a priori assumptions about the P (Ijc) usually have to be made and one has great difficulties if these assumptions are not fulfilled (see for example <ref> [5, 12] </ref>). Another discrimination function used in applications is the Mahalanobis metric (see e.g. [7]) which is a linear function. But also in this case there are many problems in determining the free parameters of the metric.
Reference: [6] <author> R.M. Gray, </author> <title> Vector Quantization, </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 1(2) </volume> <pages> 4-29, </pages> <year> 1984. </year>
Reference-contexts: Sim k (c; I) may for example be the distance between a representative of class c and the input I in a specific feature as for example in vector quantization <ref> [6] </ref>. Very often the final discrimination function is similar to Sim tot (c; I) = k=1 n and it is said that I belongs to class c if Sim tot (c; I) is maximal for c. <p> For the more significant features k for the recognition of a representative of class c we expect fi c k to be high, otherwise to be low. In many pattern recognition tasks (e.g., <ref> [11, 6] </ref>) a discrimination function of type (1) is used. Here we introduce an algorithm to extend (1) to (2) and we give a learning rule for the free parameters fi c k .
Reference: [7] <author> T. Kohonen, </author> <title> Self-Organisation and associative memory (3r. </title> <editor> ed.), </editor> <booktitle> Springer Series in Information Science 8, </booktitle> <address> Heidelberg 1989. </address>
Reference-contexts: Another discrimination function used in applications is the Mahalanobis metric (see e.g. <ref> [7] </ref>) which is a linear function. But also in this case there are many problems in determining the free parameters of the metric.
Reference: [8] <author> N. Kruger. </author> <title> "An algorithm for the Learning of Weights in Discrimination Functions", </title> <type> IR-INI 08-95 (Technical Report). </type>
Reference-contexts: We calculate the weighted average over these similarities for each graph representing a certain pose. The pose corresponding to the graph yielding the highest total similarity is chosen as the correct one. The pose estimation algorithm is described more precisely in <ref> [8, 9] </ref>. 4 Description of the algorithm for finding weights for nodes In this section we describe the basic idea of our algorithm on the example of learning weights for the nodes of the graphs for face discrimination. <p> In section 5 we will use different functions for j 1 ; j 2 and j 3 . The extension to learn a weight matrix fi (k;s;o) for all jet components is straightforward and is described more precisly in <ref> [8] </ref>. The basic difference to the algorithm described above is the choice of the -expressions. For the learning of the fi (k;s;o) we use as the differences of the similarities of two faces of the same person and to a different person in each jet component. <p> For pose estimation we learn class dependent weights fi c k (where c represents the classes frontal, half profile and profile) by setting to the difference of the node similarities achieved on a picture of a face with correct pose and incorrect pose (for details see <ref> [8] </ref>). 5 Results Our complete data set contains more than 1500 pictures of approximately 350 persons. The poses frontal, half profile and profile exist for most of the persons in the data set. We found out that the weights depend on the actual task. <p> If we approximate the j i with splines the learning takes approximately 15 minutes. The learning of weights for all jet components takes approximately 12 hours and the learning for the pose estimation problem takes less than 5 minutes. In <ref> [8] </ref> the results of our simulations are discussed in more detail. 6 Conclusion We introduced a learning algorithm for weights in discrimination functions and we applied this algorithm to very different tasks in face recognition.
Reference: [9] <author> N. Kruger, G. Peters, C. v.d. Malsburg, </author> <title> "Object Recognition with Banana Wavelets", </title> <note> accepted for ESANN-97. </note>
Reference-contexts: We think that the right mixture of a priori knowledge and learning has to be found. That means that the a priori assumptions have to be very general, effective, and applicable in many situations (see also <ref> [9, 10] </ref>). One of the advantages of our algorithm is the mixture of a priori knowledge and learning. <p> We calculate the weighted average over these similarities for each graph representing a certain pose. The pose corresponding to the graph yielding the highest total similarity is chosen as the correct one. The pose estimation algorithm is described more precisely in <ref> [8, 9] </ref>. 4 Description of the algorithm for finding weights for nodes In this section we describe the basic idea of our algorithm on the example of learning weights for the nodes of the graphs for face discrimination. <p> But if all of the features are not suitable to recognize elements of the different classes, the transformation we can learn with our algorithm will give less improvement. Then a more complex transformation has to be performed, i.e., the feature extraction itself has to be improved. In recent work <ref> [9] </ref> we utilized a priori constraints similar to our criteria C1 and C2 to derive more efficient features. Acknowledgement: We like to thank Laurenz Wiskott, Jan Vorbruggen, Thomas Maurer and Christoph von der Malsburg for very fruitful discussions.
Reference: [10] <author> N. Kruger, M. Potzsch. T. Maurer, M. Rinne, </author> <title> "Estimation of Face Position and Pose with Labeled Graphs", </title> <booktitle> Proceedings of the BMVC 1996. </booktitle>
Reference-contexts: We think that the right mixture of a priori knowledge and learning has to be found. That means that the a priori assumptions have to be very general, effective, and applicable in many situations (see also <ref> [9, 10] </ref>). One of the advantages of our algorithm is the mixture of a priori knowledge and learning.
Reference: [11] <author> M. Lades, J.C. Vorbruggen, J. Buhmann, J. Lange, C. von der Malsburg, R.P. Wurtz, W. Konen, </author> <title> "Distortion Invariant Object Recognition in the Dynamik Link Architecture", </title> <journal> IEEE Transactions on Computers, </journal> <volume> 42(3) </volume> <pages> 300-311, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: k 2 IR N . (In speech recognition I k can, for example, represent the Fourier transformation in a certain time interval in a specific frequency band [14]; in image processing I k could be the filter response of a wavelet-like filter at a certain position in the grey-level picture <ref> [11, 15] </ref>). In discrimination the input I has to be assigned to a specific class c. The extracted features are used to evaluate certain similarities to the different classes. <p> For the more significant features k for the recognition of a representative of class c we expect fi c k to be high, otherwise to be low. In many pattern recognition tasks (e.g., <ref> [11, 6] </ref>) a discrimination function of type (1) is used. Here we introduce an algorithm to extend (1) to (2) and we give a learning rule for the free parameters fi c k . <p> simple and evident and are controlled by the learning which also gives the required flexibility to find a solution adapted to the problem. 3 The face recognition system As basic visual feature we use a local image descriptor in the form of a vector I s;o (x; y) called "jet" <ref> [2, 11] </ref>. Each component (s; o) of a jet is a Gabor wavelet of specific frequency s and orientation o, extracted from the image at a definite point (x; y). We are employing Gabor wavelets of 5 different frequencies and 8 different orientations for a total of 40 complex components.
Reference: [12] <author> M.S. Landy, L.T.Maloney, E.B. Johnsten, M. Young, </author> <title> "Measurement and modeling of depth cue combinations: in defense of weak fusion", </title> <journal> Vision Research, </journal> <volume> Vol. 35, No. 35, pp:389-412, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: problem arises to estimate the parameters P (c) and the conditional densities P (Ijc) which are N -dimensional functions for every c. 1 Therefore a priori assumptions about the P (Ijc) usually have to be made and one has great difficulties if these assumptions are not fulfilled (see for example <ref> [5, 12] </ref>). Another discrimination function used in applications is the Mahalanobis metric (see e.g. [7]) which is a linear function. But also in this case there are many problems in determining the free parameters of the metric.
Reference: [13] <author> J.A. Nelder, R. Maed, </author> <title> "A simplex method for function minimization". </title> <journal> Computer Journal, </journal> <volume> vol. 7, </volume> <pages> pp. 308-313, </pages> <year> 1996. </year>
Reference-contexts: Because of the large number of parameters we had difficulties in generalization, i.e., we achieved a large improvement for the training data but a decrease of performance on the test data. In this paper we use the simplex algorithm <ref> [13] </ref> as optimization algorithm. The simplex algorithm finds minima of a multi-dimensional function f : IR n ! IR without using any information about the derivative of the function f . Furthermore the simplex algorithm is more robust against local minima than many other optimization algorithms. <p> 1 ; : : : ; ff N ) and we get Q (J (T 1 ; ff 1 ; : : : ; ff N ); : : : ; J (T n ; ff 1 ; : : : ; ff N )): We apply the simplex algorithm <ref> [13] </ref> to this function Q which now depends on N parameters. It is assumed that N is much smaller than n. Therefore, we have a reduction of dimensionality from n to N .
Reference: [14] <author> L.R. Rabiner, R.W. Schafer, </author> <title> Digital Processing of Speech Signals, </title> <institution> Bell Laboratories 1978. </institution>
Reference-contexts: In feature extraction an input I is transformed into a vector I k 2 IR N . (In speech recognition I k can, for example, represent the Fourier transformation in a certain time interval in a specific frequency band <ref> [14] </ref>; in image processing I k could be the filter response of a wavelet-like filter at a certain position in the grey-level picture [11, 15]). In discrimination the input I has to be assigned to a specific class c.

References-found: 14

