URL: ftp://ftp.cs.wisc.edu/sohi/papers/1995/micro.zcl.ps.gz
Refering-URL: http://www.cs.wisc.edu/~sohi/sohi.html
Root-URL: 
Email: faustin,sohig@cs.wisc.edu  
Title: Zero-Cycle Loads: Microarchitecture Support for Reducing Load Latency  
Author: Todd M. Austin Gurindar S. Sohi 
Address: 1210 W. Dayton Street Madison, WI 53706  
Affiliation: University of Wisconsin-Madison  
Abstract: Untolerated load instruction latencies often have a significant impact on overall program performance. As one means of mitigating this effect, we present an aggressive hardware-based mechanism that provides effective support for reducing the latency of load instructions. Through the judicious use of instruction predecode, base register caching, and fast address calculation, it becomes possible to complete load instructions up to two cycles earlier than traditional pipeline designs. For a pipeline with one cycle data cache access, this results in what we term a zero-cycle load. A zero-cycle load produces a result prior to reaching the execute stage of the pipeline, allowing subsequent dependent instructions to issue unfettered by load dependencies. Programs executing on processors with support for zero-cycle loads experience significantly fewer pipeline stalls due to load instructions and increased overall performance. We present two pipeline designs supporting zero-cycle loads: one for pipelines with a single stage of instruction decode, and another for pipelines with multiple decode stages. We evaluate these designs in a number of contexts: with and without software support, in-order vs. out-of-order issue, and on architectures with many and few registers. We find that our approach is quite effective at reducing the impact of load latency, even more so on architectures with in-order issue and few registers. 
Abstract-found: 1
Intro-found: 1
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Use of this addressing mode can be directly attributed to codes where strength reduction of array subscript expressions <ref> [ASU86] </ref> is not possible or fails, resulting in many large index variable offsets. We can improve the design by not speculating loads using this addressing mode. We instead compute the effective address during the decode stage of the pipeline and access the data cache in the execute stage.
Reference: [APS95] <author> T. M. Austin, D. N. Pnevmatikatos, and G. S. Sohi. </author> <title> Streamlin ing data cache access with fast address calculation. </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Instruction predecode reduces the latency to identify and align the loads in a group of fetched instructions. Base register caching provides the necessary high-bandwidth access to base and index register values early in the pipeline. Second, we employ fast address calculation <ref> [APS95] </ref> to reduce the latency of data cache access. Fast address calculation is a stateless set index predictor that allows address calculation and data cache access to proceed in parallel. <p> If the BRIC misses, an entry is replaced after the base and index register values have been read from the integer register file. Loads that use the stack or global pointers are executed quite frequently <ref> [APS95] </ref> we can increase the effective capacity of the BRIC by using an alternate means to supply these loads with a base register value. As shown in Figure 2, two registers are used to cache the global and stack pointer values. <p> We call this limited form of addition carry-free addition as this operation ignores any carries that may have been generated in or propagated into the set index portion of the address calculation. Because many offsets are small <ref> [APS95] </ref>, the set index portion of the offset will often be zero, allowing this fast computation to succeed. <p> By the end of the first decode stage, this design and the single decode stage design converge. In the second decode stage, fast address calculation is used to compute the effective address and the data cache is accessed. 2.3 Further Design Considerations We found in a earlier report <ref> [APS95] </ref> that fast address calculation for register+register mode accesses fails often. Use of this addressing mode can be directly attributed to codes where strength reduction of array subscript expressions [ASU86] is not possible or fails, resulting in many large index variable offsets. <p> We added software support to GCC and GLD to improve the prediction performance of fast address calculation. A complete description of our optimizations as well as the parameters used when compiling the codes can be found in <ref> [APS95] </ref>. In short, our software support works to align pointers and reduce the size of load offsets, two transformations which improve the prediction accuracy of fast address calculation. <p> Our approach can be viewed as essentially an extension of this pipeline design. Memory access is pulled back one more stage, to the stage prior to the execute stage. Because we employ fast address calculation, we can do this without increasing the number of address-use hazards. In <ref> [APS95] </ref>, we presented the design and evaluation of fast address calculation. In this work, we extended the latency reduction capability of our original approach by combining it with a mechanism for early issue. Comparing the baseline results of this paper with those in [APS95], we have roughly doubled the performance improvement <p> In <ref> [APS95] </ref>, we presented the design and evaluation of fast address calculation. In this work, we extended the latency reduction capability of our original approach by combining it with a mechanism for early issue. Comparing the baseline results of this paper with those in [APS95], we have roughly doubled the performance improvement for the integer codes and nearly quadrupled the improvement for the floating point codes.
Reference: [AVS93] <author> T. M. Austin, T.N. Vijaykumar, and G. S. Sohi. </author> <title> Knapsack: A zero-cycle memory hierarchy component. </title> <type> Technical Report TR 1189, </type> <institution> Computer Sciences Department, UW-Madison, Madison, WI, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: At cache misses, memory operand specifiers within instructions were replaced with direct stack cache addresses. When the partially decoded instructions were executed, operands in the stack cache could be accessed as quickly as registers. In <ref> [AVS93] </ref>, the knapsack memory component is presented. Software support was used to place data into the power-of-two aligned knapsack region, providing zero-cycle access to these variables when made with the architecturally-defined knapsack pointer. Zero-cycle access was limited primarily to global data.
Reference: [BC91] <author> J.-L. Baer and T.-F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> Supercomputing '91, </booktitle> <pages> pages 176-186, </pages> <year> 1991. </year>
Reference-contexts: the speedups are better than those found on the 32 register architecture due to the excellent performance of the many extra stack and global accesses. 5 Related Work The application of early issue as a means of reducing load latency has been gainfully applied in a number of previous works <ref> [BC91, EV93, GM93] </ref>. The approach used in each of these works is quite similar. An address predictor mechanism, which is a variant of the load delta table [EV93], generates addresses early in the pipeline, allowing loads to be initiated earlier than the execute stage.
Reference: [CCH + 87] <author> F. Chow, S. Correll, M. Himelstein, E. Killian, and L. </author> <title> We ber. How many addressing modes are enough. </title> <booktitle> Conference Proceedings of the Second International Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 117-121, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: The addressing mode field specifies either a register+constant or register+register addressing (if supported in the ISA). The base register type is one of the following: SP load, GP load, or other load. SP load and GP load specifies a load using the stack or global pointer <ref> [CCH + 87] </ref> as a base register, respectively. Other load specifies a load using a register other than the stack or global pointer as a base register. The offset field specifies the offset of the load if it is contained as an immediate value in the instruction.
Reference: [DM82] <author> D. R. Ditzel and H. R. McLellan. </author> <title> Register allocation for free: the C machine stack cache. </title> <booktitle> In 1st International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 48-56, </pages> <address> Palo Alto, CA, </address> <month> March </month> <year> 1982. </year>
Reference-contexts: This observation is further supported by the possibility that the load delta table may perform better on codes where fast address calculation performs poorly (e.g., poorly structured numeric codes). The C Machine <ref> [DM82] </ref> used a novel approach to implement zero-cycle access to stack frame variables. At cache misses, memory operand specifiers within instructions were replaced with direct stack cache addresses. When the partially decoded instructions were executed, operands in the stack cache could be accessed as quickly as registers.
Reference: [EV93] <author> R. J. Eickemeyer and S. Vassiliadis. </author> <title> A load-instruction unit for pipelined processors. </title> <journal> IBM J. Res. Develop., </journal> <volume> 37(4) </volume> <pages> 547-564, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: the speedups are better than those found on the 32 register architecture due to the excellent performance of the many extra stack and global accesses. 5 Related Work The application of early issue as a means of reducing load latency has been gainfully applied in a number of previous works <ref> [BC91, EV93, GM93] </ref>. The approach used in each of these works is quite similar. An address predictor mechanism, which is a variant of the load delta table [EV93], generates addresses early in the pipeline, allowing loads to be initiated earlier than the execute stage. <p> The approach used in each of these works is quite similar. An address predictor mechanism, which is a variant of the load delta table <ref> [EV93] </ref>, generates addresses early in the pipeline, allowing loads to be initiated earlier than the execute stage. The load delta table tracks both the previous address accessed by a particular load and one or more computed stride values used to predict a load's next effective address.
Reference: [FP91] <author> M. Farrens and A. Park. </author> <title> Dynamic base register caching: A technique for reducing address bus width. </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <volume> 19(3) </volume> <pages> 128-137, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: A better alternative for these designs may be to adapt the BRIC as a means for caching register values. We and others <ref> [FP91] </ref> have found a significant amount of temporal locality in base and index register accesses. A small cache, on the order of 4 to 8 entries provides the necessary bandwidth to register values without increasing the number of ports on the existing integer register file. <p> This result is to be expected since register file accesses have a significant amount of temporal locality <ref> [FP91] </ref>. Table 3 shows the results of detailed timing simulations. IPC's of the baseline simulations and speedups are shown for both the one and two decode stage implementation. All experiments were performed with the in-order issue processor model.
Reference: [GM93] <author> M. Golden and T. Mudge. </author> <title> Hardware support for hiding cache latency. </title> <institution> CSE-TR-152-93, University of Michigan, Dept. of Elect. Eng. and Comp. Sci., </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: the speedups are better than those found on the 32 register architecture due to the excellent performance of the many extra stack and global accesses. 5 Related Work The application of early issue as a means of reducing load latency has been gainfully applied in a number of previous works <ref> [BC91, EV93, GM93] </ref>. The approach used in each of these works is quite similar. An address predictor mechanism, which is a variant of the load delta table [EV93], generates addresses early in the pipeline, allowing loads to be initiated earlier than the execute stage.
Reference: [Gwe94a] <author> L. Gwennap. </author> <title> Digital leads the pack with 21164. </title> <journal> Micropro cessor Report, </journal> <volume> 8(12) </volume> <pages> 1-10, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: 2.2 Implementation with Multiple Decode Stages The increased complexity of instruction decode created by wider issue widths and faster clock speeds has forced many recent designs to increase the number of pipeline stages between instruction fetch and execute. (Stages which we collectively call decode stages.) For example, the DEC 21164 <ref> [Gwe94a] </ref> has three decode stages and the MIPS R10000 [Gwe94b] has two. Adding more decode stages increases the mispredicted branch penalty, however, architects have compensated for this penalty by increasing branch prediction accuracy through such means as larger branch target buffers or more effective predictors, e.g., two-level adaptive.
Reference: [Gwe94b] <author> L. Gwennap. </author> <title> MIPS R10000 uses decoupled architecture. </title> <journal> Mi croprocessor Report, </journal> <volume> 8(14) </volume> <pages> 18-22, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: of instruction decode created by wider issue widths and faster clock speeds has forced many recent designs to increase the number of pipeline stages between instruction fetch and execute. (Stages which we collectively call decode stages.) For example, the DEC 21164 [Gwe94a] has three decode stages and the MIPS R10000 <ref> [Gwe94b] </ref> has two. Adding more decode stages increases the mispredicted branch penalty, however, architects have compensated for this penalty by increasing branch prediction accuracy through such means as larger branch target buffers or more effective predictors, e.g., two-level adaptive.
Reference: [Hsu94] <author> P. Y.-T. Hsu. </author> <title> Designing the TFP microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 14(2) </volume> <pages> 23-33, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: It also removes the load-use hazard that occurs in the traditional 5-stage pipeline, instead introducing an address-use hazard. The address-use hazard stalls the pipeline for one cycle if the computation of the base register value is immediately followed by a dependent load or store. The R8000 (TFP) processor <ref> [Hsu94] </ref> uses a similar approach. Our approach can be viewed as essentially an extension of this pipeline design. Memory access is pulled back one more stage, to the stage prior to the execute stage.
Reference: [Jou89] <author> N. P. Jouppi. </author> <title> Architecture and organizational tradeoffs in the design of the MultiTitan CPU. </title> <booktitle> Proceedings of the 16st Annual International Symposium on Computer Architecture, </booktitle> <volume> 17(3) </volume> <pages> 281-289, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In [AVS93], the knapsack memory component is presented. Software support was used to place data into the power-of-two aligned knapsack region, providing zero-cycle access to these variables when made with the architecturally-defined knapsack pointer. Zero-cycle access was limited primarily to global data. Jouppi <ref> [Jou89] </ref> proposed a pipeline that performed ALU operations and memory access in the same stage. The pipeline employs a separate address generation pipeline stage, pushing the execution of ALU instructions and cache access to the same pipeline stage. This organization increases the mispredicted branch penalty by one cycle.
Reference: [KH92] <author> G. Kane and J. Heinrich. </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: It is important to note that these optimizations are not required, and their implementation is quite simple, totalling less than 1000 lines of C code. 4.1.2 Simulation Tools All experiments were performed on an extended (virtual) MIPS-like architecture. The architecture implements a superset of the MIPS-I instruction set <ref> [KH92] </ref>, with the following extensions: * extended addressing modes: register+register and post increment and decrement are included * no architected delay slots Our baseline simulator is detailed in Table 1.
Reference: [Soh90] <author> G. S. Sohi. </author> <title> Instruction issue logic for high-performance, inter ruptible, multiple functional unit, pipelined computers. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(3) </volume> <pages> 349-359, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The simulator supports both in-order and out-of-order issue execution models. The in-order issue model provides no renaming and stalls whenever any data hazard occurs on registers. The out-of-order issue model employs a 16 entry register update unit <ref> [Soh90] </ref> to rename values and hold results of pending instructions. Loads and stores are placed into an 8 entry queue. Stores execute when all operands are ready.
Reference: [WJ94] <author> S. J.E. Wilton and N. P. Jouppi. </author> <title> An enhanced access and cycle time model for on-chip caches. </title> <type> Tech report 93/5, </type> <institution> DEC Western Research Lab, </institution> <year> 1994. </year>
Reference-contexts: On-chip caches are organized as wide two-dimensional arrays of memory cells (as shown in Figure 3). This geometry minimizes access time by reducing wire lengths. Each row of a cache array typically contains one or more data blocks <ref> [WRP92, WJ94] </ref>. To access a word in the cache, the set index portion of the effective address is used to read an entire cache row from the data array and a tag value from the tag array.
Reference: [WRP92] <author> T. Wada, S. Rajan, and S. A. Pyzybylski. </author> <title> An analytical access time model for on-chip cache memories. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> 27(8) </volume> <pages> 1147-1156, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: On-chip caches are organized as wide two-dimensional arrays of memory cells (as shown in Figure 3). This geometry minimizes access time by reducing wire lengths. Each row of a cache array typically contains one or more data blocks <ref> [WRP92, WJ94] </ref>. To access a word in the cache, the set index portion of the effective address is used to read an entire cache row from the data array and a tag value from the tag array.
References-found: 17

