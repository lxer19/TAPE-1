URL: http://theory.lcs.mit.edu/~shanir/st95.ps
Refering-URL: http://theory.lcs.mit.edu/tds/dds.html
Root-URL: 
Title: Elimination Trees and the Construction of Pools and Stacks (Preliminary Version)  
Author: Nir Shavit Dan Touitou 
Date: May 9, 1995  
Affiliation: MIT and Tel-Aviv University  Tel-Aviv University  
Abstract: Shared pools and stacks are two coordination structures with a history of applications ranging from simple producer/consumer buffers to job-schedulers and procedure stacks. This paper introduces elimination trees, a novel form of diffracting trees that offer pool and stack implementations with superior response (on average constant) under high loads, while guaranteeing logarithmic time "deterministic" termination under sparse request patterns. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: A stack is a pool with LIFO ordering. The literature offers us a variety of possible pool implementations. On the one hand there are queue-lock based solutions such as of Anderson <ref> [1] </ref> and Mellor-Crummey and Scott [8], which offer good performance under sparse access patterns, but scale poorly since they offer little or no potential for parallelism in high load situations. <p> levels */ for i:=1 to k do place := random (1,size_i); him := register_to_memory_swap (Prism_i [place],mypid); if not_empty (him) then (his_b,his_type) := Location [him]; if (his_b = b) and (his_type = TOKEN or his_type= ANTI-TOKEN) then if compare_and_swap (Location [mypid],(b,TOKEN), EMPTY) then if compare_and_swap (Location [him],(b,his_type), BY_TOKEN) then return (b-&gt;OutputWire <ref> [1] </ref> or ELIMINATED if his_type = ANTI-TOKEN) else Location [mypid] := (b,TOKEN); else return (b-&gt;OutputWire [0] or ELIMINATED if Location [mypid] = BY_ANTI_TOKEN) repeat b-&gt;Spin times /* wait in hope of being collided with */ if Location [mypid] = BY_TOKEN then return b-&gt;OutputWire [0]; if Location [mypid] = BY_ANTI_TOKEN then return
Reference: [2] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: Of course, the tree structure is needed since one could still have long sequences of enqueues only. We compared the performance of elimination trees to other known methods using the Proteus Parallel Hardware Simulator [7] in a shared memory architecture similar to the Alewife machine of Agarwal et al. <ref> [2] </ref>. We found that elimination trees scale substantially better than all methods known to perform well under sparse loads, including queue-locks, software combining trees [4] and diffracting trees. <p> the length of the computation that the above code implements an elimination balancer. 2.2 Performance We evaluated the performance of our elimination tree based pool construction relative to other known methods by running a collection of benchmarks on a simulated 256 processors distributed-shared-memory machine similar to the MIT Alewife machine <ref> [2] </ref> of Agarwal et. al. Our simulations were performed using Proteus a multiprocessor simulator developed by Brewer, Dellarocas, Colbrook and Weihl [7].
Reference: [3] <author> J. Aspnes, M.P. Herlihy, and N. Shavit. </author> <title> Counting Networks. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 41, No. </volume> <month> 5 (September </month> <year> 1994), </year> <pages> pp. 1020-1048. </pages>
Reference-contexts: In fact, our tree construction is a novel form of a counting network <ref> [3] </ref> based counter, that allows decrement (anti-token) operations in addition to standard increment (token) operations. <p> Our formal model follows that of Aspnes, Herlihy, and Shavit <ref> [3] </ref> using the I/O-automata of Lynch and Tuttle [11]. An elimination balancer is a routing element with one input wire x and two output wires y 0 and y 1 . Tokens and anti-tokens arrive on the balancer's input wire at arbitrary times, and are output on its output wires. <p> A Stack [w] tree constructed from stack elimination balancers has the gap step property on its output wires, that is, in any quiescent state: 0 (y i y i ) (y j y j ) 1 In fact, the Stack [w] tree is a novel form of a counting tree/network <ref> [3, 16] </ref>, that allows both increment (token) and decrement (anti-token) operations. Proof: Follows from the step property for counting trees (Theorem 5.5 of [16]) by replacing the step property (on tokens) for regular balancers by the gap step property (on token/anti-token difference) for stack elimination balancers.
Reference: [4] <author> J.R. Goodman, M.K. Vernon, and P.J. Woest. </author> <title> Efficient Synchronization Primitives for Large-Scale Cache-Coherent multiprocessors. </title> <booktitle> In Proceedings of the 3rd AS-PLOS, </booktitle> <pages> pages 64-75. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1989. </year>
Reference-contexts: We found that elimination trees scale substantially better than all methods known to perform well under sparse loads, including queue-locks, software combining trees <ref> [4] </ref> and diffracting trees. They are inferior to the probabilistic techniques of [6, 13] (though in many cases not by much), especially for job distribution ap plications where a typical processor is the dequeuer of its latest enqueue. <p> 0 Procedure Enqueue (el:elements); Function Dequeue () returns elements; i:= fetch_and_increment (NQcounter); i:= fetch_and_increment (DQcounter); repeat repeat flag:= compare_and_swap (Pool [i],NULL,el); repeat el := Pool [i] until el &lt;&gt; NULL; until flag= TRUE; flag := compare_and_swap (Pool [i],el,NULL); until flag= TRUE; return el; combining tree protocol of Goodman et al. <ref> [4] </ref>, modified according to [5]. Optimal width means that when n processors participate in the simulation, a tree of width n=2 will be used.
Reference: [5] <author> M. Herlihy, B.H. Lim and N. Shavit. </author> <title> Low Contention Load Balancing on Large Scale Multiprocessors. </title> <booktitle> Proceedings of the 3rd Annual ASM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> July 1992, San Diego, CA. </address> <note> Full version available as a DEC TR. </note>
Reference-contexts: Function Dequeue () returns elements; i:= fetch_and_increment (NQcounter); i:= fetch_and_increment (DQcounter); repeat repeat flag:= compare_and_swap (Pool [i],NULL,el); repeat el := Pool [i] until el &lt;&gt; NULL; until flag= TRUE; flag := compare_and_swap (Pool [i],el,NULL); until flag= TRUE; return el; combining tree protocol of Goodman et al. [4], modified according to <ref> [5] </ref>. Optimal width means that when n processors participate in the simulation, a tree of width n=2 will be used.
Reference: [6] <author> R.D. Blumofe, and C.E. Leiserson. </author> <title> Sheduling Multi-threaded Computations by Work Stealing. </title> <booktitle> In Proceeding of the 35th Symposium on Foundations of Computer Science, </booktitle> <pages> pages 365-368, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Shared pools and stacks are two structures that offer a potential solution to such coordination problems, with a history of applications ranging from simple producer/consumer buffers to job-schedulers <ref> [6] </ref> and procedure stacks [17]. A pool (also called a pile [13], global pool [6] or a producer/consumer buffer) is a concurrent data-type which supports the abstract operations: enqueue (e) - adds element e to the pool, and dequeue (fl) deletes and returns some element e from the pool. <p> Shared pools and stacks are two structures that offer a potential solution to such coordination problems, with a history of applications ranging from simple producer/consumer buffers to job-schedulers <ref> [6] </ref> and procedure stacks [17]. A pool (also called a pile [13], global pool [6] or a producer/consumer buffer) is a concurrent data-type which supports the abstract operations: enqueue (e) - adds element e to the pool, and dequeue (fl) deletes and returns some element e from the pool. A stack is a pool with LIFO ordering. <p> On the other hand there are wonderfully simple and effective randomized work-pile and job-stealing techniques of Blumofe and Leis 1 Contact Author: E-mail: shanir@theory.lcs.mit.edu erson <ref> [6] </ref> and Rudolph, Slivkin-Allaluf and Upfal [13], that offer good expected response time under high loads, but very poor performance as access patterns become sparse (their expected response time becomes linear in n thenumber of processors in the system as opposed to that of a "deterministic" queue-lock based pool that is <p> We found that elimination trees scale substantially better than all methods known to perform well under sparse loads, including queue-locks, software combining trees [4] and diffracting trees. They are inferior to the probabilistic techniques of <ref> [6, 13] </ref> (though in many cases not by much), especially for job distribution ap plications where a typical processor is the dequeuer of its latest enqueue. However, our empirical evidence suggests that elimination trees provide up to a factor of 30 better response time than [13] under sparse loads. <p> This paper presents shared memory implementations of elimination trees, and uses them for constructing pools and almost-LIFO stacks. We are currently developing message passing versions. 2 Pools A pool (also called a pile [13], centralized "pool" <ref> [6] </ref> or a producer/consumer buffer) is a concurrent data-type which supports the abstract operations: enqueue (e) adds element e to the pool, and dequeue (fl) deletes and returns some element e from the pool. Assume for simplicity that all enqueued elements e are unique. <p> A successful operation is one that is guaranteed to return an answer within finite (in our construction, bounded) time. Note that the decentralized techniques of [13] and <ref> [6] </ref> implement a weaker "probabilistic" pool definition, where condition P2 is replaced by a randomized guarantee that dequeue operations succeed. 2.0.1 Elimination Trees Our pool implementation is based on the abstract notion of an elimination tree, a special form of the diffracting tree data structures introduced by Shavit and Zemach in <p> Naturally if the local pool is empty the dequeuing process waits until the pool is filled and then access it. The elimination tree is thus a load-balanced coordination medium among a distributed collection of pools. It differs from elegant randomized constructions of <ref> [6, 12, 13] </ref> in its deterministic dequeue termination guarantee and in performance. While work in an individual balancer is relatively high, each enqueue or dequeue request passes at most log w balancers both under high and under low loads. <p> While work in an individual balancer is relatively high, each enqueue or dequeue request passes at most log w balancers both under high and under low loads. This is not the case for <ref> [12, 13, 6] </ref> which provides exceptionally good behavior at high loads but can guarantee only an an expected (n) behavior under sparse access patterns. 2.1 Elimination Balancers The scalable performance of our pool constructions depends on providing an efficient implementation of an elimination balancer. Diffracting balancers were introduced in [16]. <p> LIFO-based scheduling will not only eliminate in many cases excessive task creation, but it will also prevent processors from attempting to dequeue and execute a task which depends on the results of other tasks [17]. Blumofe and Leiserson <ref> [6] </ref> provide a scheduler based on a version of the RSU algorithm having LIFO-ish behavior on a local level. We present here a construction of an almost stack.
Reference: [7] <author> E.A. Brewer, C.N. Dellarocas, A. Colbrook and W.E. Weihl. Proteus: </author> <title> A High-Performance Parallel-Architecture Simulator. </title> <type> MIT Technical Report /MIT/LCS/TR-561, </type> <month> September </month> <year> 1991. </year>
Reference-contexts: Of course, the tree structure is needed since one could still have long sequences of enqueues only. We compared the performance of elimination trees to other known methods using the Proteus Parallel Hardware Simulator <ref> [7] </ref> in a shared memory architecture similar to the Alewife machine of Agarwal et al. [2]. We found that elimination trees scale substantially better than all methods known to perform well under sparse loads, including queue-locks, software combining trees [4] and diffracting trees. <p> Our simulations were performed using Proteus a multiprocessor simulator developed by Brewer, Dellarocas, Colbrook and Weihl <ref> [7] </ref>. Our preliminary results include several synthetic benchmarks. 2.2.1 Produce-Consume benchmark We begin by comparing under high loads various deterministic pool constructions which are known to guarantee good enqueue /dequeue time when the load is low (sparse access patterns).
Reference: [8] <author> J.M. </author> <title> Mellor-Crummey and M.L. Scott Synchronization without Contention. </title> <booktitle> In Proceedings of the 4th International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: A stack is a pool with LIFO ordering. The literature offers us a variety of possible pool implementations. On the one hand there are queue-lock based solutions such as of Anderson [1] and Mellor-Crummey and Scott <ref> [8] </ref>, which offer good performance under sparse access patterns, but scale poorly since they offer little or no potential for parallelism in high load situations. <p> Each of the toggle bit locations is protected by an MCS-queue-lock <ref> [8] </ref>. A process shepherding a token or anti-token through the balancer decides on which wire to exit according to the value of the respective token or anti-token toggle bit, 0 to the left and 1 to the right, toggling the bit as it leaves. <p> Our implementation also uses standard AquireLock and ReleaseLock procedures to enter and exit the MCS queue-lock <ref> [8] </ref>. Initially, processor p announces the arrival of its token at node b, by writing b and its token type to location [p]. <p> most efficient pool implementations are attained when using shared counting to load balance and control access to a shared array (see We thus realized the centralized pool given in Figure 4, when the NQcounter and DQcounter are implemented using two counters of the following type: MCS The MCS lock of <ref> [8] </ref>, whose response time is linear in the number of concurrent requests. Each processor locks the shared counter, increments it, and then unlocks it. The code was taken directly from the article, and implemented using atomic operations: register to memory swap and compare and swap oper ations.
Reference: [9] <author> G.H. Pfister and A. Norton. </author> <title> `Hot Spot' contention and combining in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(11):933-938, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: In fact, our tree construction is a novel form of a counting network [3] based counter, that allows decrement (anti-token) operations in addition to standard increment (token) operations. However, this simple approach is bound to fail since the toggle bit at root of the tree will be a hot-spot <ref> [9, 10] </ref> and a sequential bottleneck that is no better than a centralized stack implementation. The problem is overcome by placing a diffracting prism [16] structure in front of the toggle bit inside every balancer.
Reference: [10] <author> D. Gawlick. </author> <title> Processing 'hot spots' in high performance systems. </title> <booktitle> In Proceedings COMPCON'85, </booktitle> <year> 1985. </year>
Reference-contexts: In fact, our tree construction is a novel form of a counting network [3] based counter, that allows decrement (anti-token) operations in addition to standard increment (token) operations. However, this simple approach is bound to fail since the toggle bit at root of the tree will be a hot-spot <ref> [9, 10] </ref> and a sequential bottleneck that is no better than a centralized stack implementation. The problem is overcome by placing a diffracting prism [16] structure in front of the toggle bit inside every balancer.
Reference: [11] <author> N.A. Lynch and M.R. Tuttle. </author> <title> Hierarchical Correctness Proofs for Distributed Algorithms. </title> <booktitle> In Sixth ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <month> August </month> <year> 1987, </year> <pages> pp. 137-151. </pages> <note> Full version available as MIT Technical Report MIT/LCS/TR-387. </note>
Reference-contexts: Our formal model follows that of Aspnes, Herlihy, and Shavit [3] using the I/O-automata of Lynch and Tuttle <ref> [11] </ref>. An elimination balancer is a routing element with one input wire x and two output wires y 0 and y 1 . Tokens and anti-tokens arrive on the balancer's input wire at arbitrary times, and are output on its output wires.
Reference: [12] <author> R. Luling, and B. Monien. </author> <title> A Dynamic Distributed Load Balancing Algorithm with Provable Good Performance. </title> <booktitle> In Proceedings of the 5rd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 164-173, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Naturally if the local pool is empty the dequeuing process waits until the pool is filled and then access it. The elimination tree is thus a load-balanced coordination medium among a distributed collection of pools. It differs from elegant randomized constructions of <ref> [6, 12, 13] </ref> in its deterministic dequeue termination guarantee and in performance. While work in an individual balancer is relatively high, each enqueue or dequeue request passes at most log w balancers both under high and under low loads. <p> While work in an individual balancer is relatively high, each enqueue or dequeue request passes at most log w balancers both under high and under low loads. This is not the case for <ref> [12, 13, 6] </ref> which provides exceptionally good behavior at high loads but can guarantee only an an expected (n) behavior under sparse access patterns. 2.1 Elimination Balancers The scalable performance of our pool constructions depends on providing an efficient implementation of an elimination balancer. Diffracting balancers were introduced in [16]. <p> However, unlike our the multi-layered balancer constructions, they do not continue to scale well at higher levels of concurrency. 2.2.3 Response time benchmark The randomized workpiles method of Rudolph, Silvkin-Allalouf and Upfal (RSU) [13] and later improvements by Luling and Monien <ref> [12] </ref> are surprisingly simple: RSU Processors enqueue task in a private task queue. Before dequeuing a task, every processor flips a coin and executes a load balancing procedure with probability 1=l where l is the size of its private task queue.
Reference: [13] <author> L. Rudolph, M. Slivkin, and E. Upfal. </author> <title> A Simple Load Balancing Scheme for Task Allocation in Parallel Machines. </title> <booktitle> In Proceedings of the 3rd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 237-245, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Shared pools and stacks are two structures that offer a potential solution to such coordination problems, with a history of applications ranging from simple producer/consumer buffers to job-schedulers [6] and procedure stacks [17]. A pool (also called a pile <ref> [13] </ref>, global pool [6] or a producer/consumer buffer) is a concurrent data-type which supports the abstract operations: enqueue (e) - adds element e to the pool, and dequeue (fl) deletes and returns some element e from the pool. A stack is a pool with LIFO ordering. <p> On the other hand there are wonderfully simple and effective randomized work-pile and job-stealing techniques of Blumofe and Leis 1 Contact Author: E-mail: shanir@theory.lcs.mit.edu erson [6] and Rudolph, Slivkin-Allaluf and Upfal <ref> [13] </ref>, that offer good expected response time under high loads, but very poor performance as access patterns become sparse (their expected response time becomes linear in n thenumber of processors in the system as opposed to that of a "deterministic" queue-lock based pool that is linear in the number of participating <p> Our empirical results show that unlike diffracting trees, and in spite of the fact that elimination trees offer a "deterministic" guarantee of coordination, 1 they scale like the "probabilistic" methods <ref> [13] </ref>, providing improved response time as the load on them increases. <p> We found that elimination trees scale substantially better than all methods known to perform well under sparse loads, including queue-locks, software combining trees [4] and diffracting trees. They are inferior to the probabilistic techniques of <ref> [6, 13] </ref> (though in many cases not by much), especially for job distribution ap plications where a typical processor is the dequeuer of its latest enqueue. However, our empirical evidence suggests that elimination trees provide up to a factor of 30 better response time than [13] under sparse loads. <p> However, our empirical evidence suggests that elimination trees provide up to a factor of 30 better response time than <ref> [13] </ref> under sparse loads. Finally, we present evidence that our new elimination balancer design offers a more scalable diffracting balancer construction even in cases where no collisions are possible. In summary, we believe elimination trees offer a new approach to produce/consume coordination problems. <p> In summary, we believe elimination trees offer a new approach to produce/consume coordination problems. This paper presents shared memory implementations of elimination trees, and uses them for constructing pools and almost-LIFO stacks. We are currently developing message passing versions. 2 Pools A pool (also called a pile <ref> [13] </ref>, centralized "pool" [6] or a producer/consumer buffer) is a concurrent data-type which supports the abstract operations: enqueue (e) adds element e to the pool, and dequeue (fl) deletes and returns some element e from the pool. Assume for simplicity that all enqueued elements e are unique. <p> A successful operation is one that is guaranteed to return an answer within finite (in our construction, bounded) time. Note that the decentralized techniques of <ref> [13] </ref> and [6] implement a weaker "probabilistic" pool definition, where condition P2 is replaced by a randomized guarantee that dequeue operations succeed. 2.0.1 Elimination Trees Our pool implementation is based on the abstract notion of an elimination tree, a special form of the diffracting tree data structures introduced by Shavit and <p> Naturally if the local pool is empty the dequeuing process waits until the pool is filled and then access it. The elimination tree is thus a load-balanced coordination medium among a distributed collection of pools. It differs from elegant randomized constructions of <ref> [6, 12, 13] </ref> in its deterministic dequeue termination guarantee and in performance. While work in an individual balancer is relatively high, each enqueue or dequeue request passes at most log w balancers both under high and under low loads. <p> While work in an individual balancer is relatively high, each enqueue or dequeue request passes at most log w balancers both under high and under low loads. This is not the case for <ref> [12, 13, 6] </ref> which provides exceptionally good behavior at high loads but can guarantee only an an expected (n) behavior under sparse access patterns. 2.1 Elimination Balancers The scalable performance of our pool constructions depends on providing an efficient implementation of an elimination balancer. Diffracting balancers were introduced in [16]. <p> However, unlike our the multi-layered balancer constructions, they do not continue to scale well at higher levels of concurrency. 2.2.3 Response time benchmark The randomized workpiles method of Rudolph, Silvkin-Allalouf and Upfal (RSU) <ref> [13] </ref> and later improvements by Luling and Monien [12] are surprisingly simple: RSU Processors enqueue task in a private task queue. Before dequeuing a task, every processor flips a coin and executes a load balancing procedure with probability 1=l where l is the size of its private task queue.
Reference: [14] <author> Krithi Ramamrithan and Calton Pu. </author> <title> A Formal Characterization of Epsilon Serializability. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <note> 1994, to appear. </note>
Reference-contexts: Clearly, the gap step property implies the pool balancing property on the balancer's output wires. 6 This notion is remotely related to *-serializablity <ref> [14] </ref>, which allows individual database read transactions to return partially inconsistent states. We design Stack [w] as a counting tree [16] (a special case of the structure with regular token routing balancers replaced by token/anti-token routing Stack elimination balancers.
Reference: [15] <author> M. Herlihy and J.M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <booktitle> In ACM Transaction on Programming Languages and Systems, </booktitle> <volume> 12(3), </volume> <pages> pages 463-492, </pages> <month> July 199 </month>
Reference: [16] <author> N. Shavit and A. Zemach. </author> <title> Diffracting Trees. </title> <booktitle> In Proceed--ings of the Annual Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Diffracting trees <ref> [16] </ref> have been proposed as a reasonable middle-of-the-road solution. <p> However, this simple approach is bound to fail since the toggle bit at root of the tree will be a hot-spot [9, 10] and a sequential bottleneck that is no better than a centralized stack implementation. The problem is overcome by placing a diffracting prism <ref> [16] </ref> structure in front of the toggle bit inside every balancer. Pairs of tokens attempt to "collide" on independent locations in the prism, diffracting in a coordinated manner one to the 0-wire and one to the 1-wire, thus leaving the balancer without ever having to toggle the shared bit. <p> implement a weaker "probabilistic" pool definition, where condition P2 is replaced by a randomized guarantee that dequeue operations succeed. 2.0.1 Elimination Trees Our pool implementation is based on the abstract notion of an elimination tree, a special form of the diffracting tree data structures introduced by Shavit and Zemach in <ref> [16] </ref>. Our formal model follows that of Aspnes, Herlihy, and Shavit [3] using the I/O-automata of Lynch and Tuttle [11]. An elimination balancer is a routing element with one input wire x and two output wires y 0 and y 1 . <p> Diffracting balancers were introduced in <ref> [16] </ref>. Our shared memory construction of a diffracting elimination balancer, apart from providing a mechanism for token/anti-token elimination, also improves on the performance of the original diffracting balancer design. While a regular diffracting balancer [16] is constructed from a single prism array and a toggle bit, the elimination balancer we use <p> Diffracting balancers were introduced in <ref> [16] </ref>. Our shared memory construction of a diffracting elimination balancer, apart from providing a mechanism for token/anti-token elimination, also improves on the performance of the original diffracting balancer design. While a regular diffracting balancer [16] is constructed from a single prism array and a toggle bit, the elimination balancer we use in our pool construction (see lefthand side of Figure 2) has a sequence of prism arrays and two toggle bits, one for tokens and one for anti-tokens 2 . <p> The reader can easily convince herself that this suffices to guarantee the pool-balancing property. However, if many tokens were to attempt to access the same toggle bit concurrently, the bit would quickly become a hot spot. The solution presented in <ref> [16] </ref> is to add a prism array in front of each toggle bit. Before accessing the bit, the process shep-herding the token selects a location l in the prism uniformly at random, hoping to "collide" with another token which selected l. <p> If such a diffracting collision does not occur, the process toggles the bit as above and leaves accordingly. As proved in <ref> [16] </ref>, the combination of diffracted tokens and toggling tokens behaves exactly as if all tokens toggled the bit, because if any two diffracted tokens were to access the bit instead, after they both toggled it the bit state would anyhow return to its initial state. <p> If it is too high, tokens will miss each other, lowering the number of successful eliminations, and causing contention on the toggle bits. If it is too low, to many processes will collide on the same prism entry, creating a hot-spot. Unlike the single prism array of <ref> [16] </ref>, we found it more effective to pass a token through a series of prisms of decreasing size, thus increasing the chances of a collision. <p> Optimal width means that when n processors participate in the simulation, a tree of width n=2 will be used. DTree A Diffracting Tree of width 32, using the optimized parameters of <ref> [16] </ref>, whose response time is logarithmic in w = 32 which is smaller than the maximal number of processors. and compare it to : ETree A Pool [32] elimination tree based pool, whose response time is logarithmic in w = 32 which is smaller than the maximal number of processors. <p> is 3.14 nodes (38.9% of the request access the leaf pools) and for 256 processors 2.082 (only 8.95% of the request eventually access the pools at the leaves). 2.2.2 Counting Benchmark Our new multi-layered prism approach is slightly more costly but scales better than the original single prism construction of <ref> [16] </ref>. <p> Clearly, the gap step property implies the pool balancing property on the balancer's output wires. 6 This notion is remotely related to *-serializablity [14], which allows individual database read transactions to return partially inconsistent states. We design Stack [w] as a counting tree <ref> [16] </ref> (a special case of the structure with regular token routing balancers replaced by token/anti-token routing Stack elimination balancers. <p> A Stack [w] tree constructed from stack elimination balancers has the gap step property on its output wires, that is, in any quiescent state: 0 (y i y i ) (y j y j ) 1 In fact, the Stack [w] tree is a novel form of a counting tree/network <ref> [3, 16] </ref>, that allows both increment (token) and decrement (anti-token) operations. Proof: Follows from the step property for counting trees (Theorem 5.5 of [16]) by replacing the step property (on tokens) for regular balancers by the gap step property (on token/anti-token difference) for stack elimination balancers. <p> Proof: Follows from the step property for counting trees (Theorem 5.5 of <ref> [16] </ref>) by replacing the step property (on tokens) for regular balancers by the gap step property (on token/anti-token difference) for stack elimination balancers. An almost-stack is constructed as with the pool data structure by placing sequential "local stacks" at the leaves of a Stack [w] tree.
Reference: [17] <author> K. Taura, S. Matsuoka, and A. Yonezawa. </author> <title> An Efficient Implementation Scheme of Concurrent Object-Oriented Languages on Stock Multicomputers. </title> <booktitle> In Proceedings of the 4th Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 218-228, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Shared pools and stacks are two structures that offer a potential solution to such coordination problems, with a history of applications ranging from simple producer/consumer buffers to job-schedulers [6] and procedure stacks <ref> [17] </ref>. A pool (also called a pile [13], global pool [6] or a producer/consumer buffer) is a concurrent data-type which supports the abstract operations: enqueue (e) - adds element e to the pool, and dequeue (fl) deletes and returns some element e from the pool. <p> LIFO-based scheduling will not only eliminate in many cases excessive task creation, but it will also prevent processors from attempting to dequeue and execute a task which depends on the results of other tasks <ref> [17] </ref>. Blumofe and Leiserson [6] provide a scheduler based on a version of the RSU algorithm having LIFO-ish behavior on a local level. We present here a construction of an almost stack.
References-found: 17

