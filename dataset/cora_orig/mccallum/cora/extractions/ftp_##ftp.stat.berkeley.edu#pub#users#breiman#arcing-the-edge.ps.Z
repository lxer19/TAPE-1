URL: ftp://ftp.stat.berkeley.edu/pub/users/breiman/arcing-the-edge.ps.Z
Refering-URL: http://www.research.att.com/~schapire/boost.html
Root-URL: 
Note: 1, Introduction 1.1 Background  
Abstract: ARCING THE EDGE Leo Breiman Technical Report 486 , Statistics Department University of California, Berkeley CA. 94720 Abstract Recent work has shown that adaptively reweighting the training set, growing a classifier using the new weights, and combining the classifiers constructed to date can significantly decrease generalization error. Procedures of this type were called arcing by Breiman[1996]. The first successful arcing procedure was introduced by Freund and Schapire[1995,1996] and called Adaboost. In an effort to explain why Adaboost works, Schapire et.al. [1997] derived a bound on the generalization error of a convex combination of classifiers in terms of the margin. We introduce a function called the edge, which differs from the margin only if there are more than two classes. A framework for understanding arcing algorithms is defined. In this framework, we see that the arcing algorithms currently in the literature are optimization algorithms which minimize some function of the edge. A relation is derived between the optimal reduction in the maximum value of the edge and the PAC concept of weak learner. Two algorithms are described which achieve the optimal reduction. Tests on both synthetic and real data cast doubt on the Schapire et.al. There is recent empirical evidence that significant reductions in generalization error can be gotten by growing a number of different classifiers on the same training set and letting these vote for the best class. Freund and Schapire ([1995], [1996] ) proposed an algorithm called AdaBoost which adaptively reweights the training set in a way based on the past history of misclassifications, constructs a new classifier using the current weights, and uses the misclassification rate of this classifier to determine the size of its vote. In a number of empirical studies on many data sets using trees (CART or C4.5) as the base classifier (Drucker and Cortes[1995], Quinlan[1996], Freud and Schapire[1996], Breiman[1996]) AdaBoost produced dramatic decreases in generalization error compared to using a single tree. Error rates were reduced to the point where tests on some well-known data sets gave the result that CART plus AdaBoost did significantly better than any other of the commonly used classification methods (Breiman[1996] ). Meanwhile, empirical results showed that other methods of adaptive resampling (or reweighting) and combining (called "arcing" by Breiman [1996]) also led to low test set error rates. An algorithm called arc-x4 (Breiman[1996]) gave error rates almost identical to Adaboost. Ji and Ma[1997] worked with classifiers consisting of randomly selected hyperplanes and using a different method of adaptive resampling and unweighted voting, also got low error rates. Thus, there are a least three arcing algorithms extant, all of which give excellent classification accuracy. explanation.
Abstract-found: 1
Intro-found: 0
Reference: <author> Breiman, L. </author> <title> [1996] Bias, Variance, and Arcing Classifiers, </title> <type> Technical Report 460, </type> <institution> Statistics Department, </institution> <note> University of California (available at www.stat.berkeley.edu) Breiman, </note> <editor> L. </editor> <booktitle> [1996b] Bagging predictors , Machine Learning 26, </booktitle> <volume> No. 2, </volume> <pages> pp. 123-140 Drucker, </pages> <editor> H. and Cortes, C. </editor> <title> [1995] Boosting decision trees, </title> <booktitle> Advances in Neural Information Processing Systems Vol. </booktitle> <volume> 8, </volume> <pages> pp. 479-485. </pages>
Reference: <author> Freund, Y. and Schapire, R. </author> <title> [1995] A decision-theoretic generalization of online learning and an application to boosting. </title> <note> to appear , Journal of Computer and System Sciences. </note>
Reference: <author> Freund, Y. and Schapire, R. </author> <title> [1996] Experiments with a new boosting algorithm, </title> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pp. 148-156 Ji, </pages> <editor> C. </editor> <title> and Ma, S[1997] Combinations of weak classifiers, </title> <journal> Special Issue of Neural Networks and Pattern Recognition, IEEE Trans. Neural Networks, </journal> <volume> Vol. 8, </volume> <pages> pp. </pages> <note> 32-42 Kong, </note> <author> E. and Dietterich, T., </author> <title> [1996] Error-correcting output coding corrects bias and variance, </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 313-321 14 Quinlan, </pages> <editor> J.R.[1996] Bagging, Boosting, and C4.5, </editor> <booktitle> Proceedings of AAAI'96 National Conference, on Artificial Intelligence, </booktitle> <pages> pp. 725-730. </pages>
Reference: <author> Schapire, R.,Freund, Y., Bartlett, P., and Lee, </author> <note> W[1997] Boosting the margin, (available at http://www.research.att.com/~yoav look under "publications".) </note>
References-found: 4

