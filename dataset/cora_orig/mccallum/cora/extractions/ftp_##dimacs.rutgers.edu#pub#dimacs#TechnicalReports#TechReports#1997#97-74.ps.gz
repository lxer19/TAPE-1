URL: ftp://dimacs.rutgers.edu/pub/dimacs/TechnicalReports/TechReports/1997/97-74.ps.gz
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1997.html
Root-URL: http://www.cs.rutgers.edu
Email: e-mail: amuchnik@int.glasnet.ru  email: an@romash.mccme.ru ver@mech.math.msu.su  email: shen@mccme.ru  
Title: Upper semi-lattice of binary strings with the relation is simple conditional to  
Author: by Andrei Muchnik Andrei Romashchenko Nikolai Vereshagin Alexander Shen 
Note: Rutgers University DIMACS is a partnership of Rutgers University, Princeton University, AT&T Labs-Research, Bell Labs and Bellcore. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Address: Moscow, Russia 109004  Vorobjewy Gory, Moscow, Russia 119899  Transmission  
Affiliation: Institute of New Technologies, 10 Nizhnyaya Radischewskaya  Dept. of Mathematical Logic and Theory of Algorithms Moscow State University  Institute of Problems of Information  
Abstract: DIMACS Technical Report 97-74 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G.J. Chaitin. </author> <title> "On the length of programs for computing finite binary sequences." </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> vol. 16 (1969), </volume> <pages> pp. 145-159. </pages>
Reference-contexts: An invariance theorem by Solomonoff, Kolmogorov and Chaitin <ref> [8, 3, 1] </ref> states that there exists universal conditional description method, that is, 0 such that for any there is a constant c for which K 0 (xjy) K (xjy) + c for all x; y. This theorem is easy to show.
Reference: [2] <author> P. Gacs, J. Korner. </author> <title> "Common information is far less than mutual information." </title> <journal> Problems of Control and Information Theory, </journal> <volume> vol. 2 (1973), </volume> <pages> pp. 149-162. </pages>
Reference-contexts: Our next results deal with common information. Gacs and Korner were first who pointed out that the quantity of mutual information I (x : y) may be large when the strings x and y have nothing in common. More specifically, Gacs and Korner in <ref> [2] </ref> proved that there exist x n and y n of length n such that the mutual information I (x n : y n ) has linear growth cn and the complexity of any common lower bound of x n and y n is O ( p n) (Another proof appeared <p> Our proof of this fact is simpler than the rather technical proof of the corresponding fact in <ref> [2] </ref>. If two sequences have greatest common lower bound, its complexity, defined up to O (log n) term, is called common information in those sequences, as opposed to mutual information I (x n : y n ). <p> Hence K (u n jz n ) = K (hz n ; u n i) K (z n ) + O (log n) O (log n). 2 If x and y do have the greatest common lower bound z, we call, after Gacs and Korner <ref> [2] </ref>, the complexity of z the common information in x and y. The common information is defined up to logarithmic term. Many pairs of sequences have the lowest common upper bound. Let us give an example of such a pair.
Reference: [3] <author> A.N. </author> <title> Kolmogorov. "Three approaches to the quantitative definition of information". Problems Inform. </title> <journal> Transmission, </journal> <volume> vol. 1 (1965), No. 1, </volume> <pages> pp. 1-7. </pages>
Reference-contexts: An invariance theorem by Solomonoff, Kolmogorov and Chaitin <ref> [8, 3, 1] </ref> states that there exists universal conditional description method, that is, 0 such that for any there is a constant c for which K 0 (xjy) K (xjy) + c for all x; y. This theorem is easy to show.
Reference: [4] <author> S. Lang. </author> <title> Algebra. </title> <publisher> Addison-Wesley, </publisher> <year> 1965. </year>
Reference-contexts: We can choose ff in such a way that, moreover, any element in F q can be represented in the form t + sff 2 for some t; s 2 F p . Why? The multiplicative group of the field F q is cyclic (see <ref> [4, page 184] </ref>), therefore the square of any its generator does not belong to F p . Let us take as ff any such generator. Then ff 2 = e + f ff, where e; f 2 F p and f 6= 0.
Reference: [5] <author> M. Li, P. Vitanyi. </author> <title> An introduction to Kolmogorov complexity and its applications. </title> <publisher> Springer, </publisher> <year> 1997. </year>
Reference-contexts: Given the string l (p)pq we can find both p and q, then find x and y, and finally find hx; yi. It turns out that vice versa K (y) + K (xjy) K (hx; yi) + O (log K (x) + log K (y)) (see <ref> [5] </ref>, or [10]), thus K (y) + K (xjy) = K (hx; yi) + O (log K (x) + log K (y)): (1) - 4 - We will use this equation several times.
Reference: [6] <author> An.A. Muchnik, </author> <title> "On the extraction of common information of two strings." </title> <booktitle> Abstracts of talks at the First World Congress of Bernoulli Society, </booktitle> <address> Moscow, </address> <publisher> Nauka, </publisher> <year> 1986, </year> <note> p. 453. 4 Recently answered by the second author - 17 </note> - 
Reference-contexts: that there exist x n and y n of length n such that the mutual information I (x n : y n ) has linear growth cn and the complexity of any common lower bound of x n and y n is O ( p n) (Another proof appeared in <ref> [6] </ref>.) Using another approach, we present three examples of sequences x n and y n of length n such that I (x n : y n ) has linear growth and the complexity of any common lower bound of x and y is O (log n). <p> We prove (in a sense non-constructively) that there exist x; y with given complexities K (x n ), K (y n ), I (x n : y n ) having minimum possible set M xy . (Our proof is a generalization of the proof in <ref> [6, 7] </ref>.) We study then what is the set M xy for one interesting particular pair (x; y). The paper is organized as follows. In Section 2, we recall basic definitions. In Section 3, we prove that there are sequences having no greatest common lower bound.
Reference: [7] <author> An.A. Muchnik. </author> <title> "Common information". </title> <type> Manuscript, </type> <year> 1996. </year>
Reference-contexts: We prove (in a sense non-constructively) that there exist x; y with given complexities K (x n ), K (y n ), I (x n : y n ) having minimum possible set M xy . (Our proof is a generalization of the proof in <ref> [6, 7] </ref>.) We study then what is the set M xy for one interesting particular pair (x; y). The paper is organized as follows. In Section 2, we recall basic definitions. In Section 3, we prove that there are sequences having no greatest common lower bound. <p> Similar to 2). 2 It turns out that there exists x; y such that M x;y = M min . This can by proved using the method of <ref> [7] </ref>. Theorem 7 There are sequences x, y such that M x;y = M min . Proof. The proof easily deduces from the following lemma.
Reference: [8] <author> R.J. Solomonoff. </author> <title> "A formal theory of inductive inference, part 1 and part 2". </title> <journal> Inform. Control, </journal> <volume> vol. 7 (1964), </volume> <pages> pp. 1-22, 224-254. </pages>
Reference-contexts: An invariance theorem by Solomonoff, Kolmogorov and Chaitin <ref> [8, 3, 1] </ref> states that there exists universal conditional description method, that is, 0 such that for any there is a constant c for which K 0 (xjy) K (xjy) + c for all x; y. This theorem is easy to show.
Reference: [9] <author> V. A. Uspensky, A. Shen. </author> <title> "Relations between varieties of Kolmogorov complexities." </title> <journal> Mathematical Systems Theory, </journal> <volume> vol. 29 (1996), no. 3, </volume> <pages> pp. 271-292. </pages>
Reference-contexts: complexities of all strings involved; for instance the well known commutativeness law for information states that I (x : y) I (y : x) = O (log K (x) + log K (y)) 2 Another reason is that different versions of Kolmogorov complexity are equivalent up to logarithmic term (see <ref> [9] </ref>). Therefore the relation K (xjy) c (log K (x)+ log K (y)) is more interesting than the relation K (xjy) c. Another, more simple, way to make the results rigorous is to consider infinite sequences of binary strings rather then single strings.
Reference: [10] <author> A.K. Zvonkin, L.A. Levin, </author> <title> "The complexity of finite objects and the development of the concepts of information and randomness by means of the theory of algorithms." </title> <journal> Russian Math. Surveys, </journal> <volume> vol. 25 (1970), No. 6. </volume> <pages> pp. 83-124. </pages>
Reference-contexts: Given the string l (p)pq we can find both p and q, then find x and y, and finally find hx; yi. It turns out that vice versa K (y) + K (xjy) K (hx; yi) + O (log K (x) + log K (y)) (see [5], or <ref> [10] </ref>), thus K (y) + K (xjy) = K (hx; yi) + O (log K (x) + log K (y)): (1) - 4 - We will use this equation several times.
References-found: 10

