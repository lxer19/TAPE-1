URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/chill-compling-97.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/chill.html
Root-URL: 
Email: zelle@zelle.drake.edu mooney@cs.utexas.edu  
Title: An Inductive Logic Programming Method for Corpus-based Parser Construction  
Author: John M. Zelle Raymond J. Mooney 
Date: March 18, 1997  
Address: Moines, IA 50311 Austin, TX 78712  
Affiliation: Dept. of Math and Computer Science Dept. of Computer Sciences Drake University University of Texas Des  
Abstract: Empirical methods for building natural language systems has become an important area of research in recent years. Most current approaches are based on propositional learning algorithms and have been applied to the problem of acquiring broad-coverage parsers for relatively shallow (syntactic) representations. This paper outlines an alternative empirical approach based on techniques from a subfield of machine learning known as Inductive Logic Programming (ILP). ILP algorithms, which learn relational (first-order) rules, are used in a parser acquisition system called Chill that learns rules to control the behavior of a traditional shift-reduce parser. Using this approach, Chill is able to learn parsers for a variety of different types of analyses, from traditional syntax trees to more meaning-oriented case-role and database query forms. Experimental evidence shows that Chill performs comparably to propositional learning systems on similar tasks, and is able to go beyond the broad-but-shallow paradigm and learn mappings directly from sentences into useful semantic representations. In a complete database-query application, parsers learned by Chill outperform an existing hand-crafted system, demonstrating the promise of empricial techniques for automating the construction certain NLP systems. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Abramson, H., & Dahl, V. </author> <year> (1989). </year> <title> Logic Grammars. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference: <author> Berwick, B. </author> <year> (1985). </year> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Black, E., & et. al. </author> <year> (1991). </year> <title> A procedure for quantitatively comparing the syntactic coverage of English grammars. </title> <booktitle> In Proceedings of the Fourth DARPA Speech and Natural Language Workshop, </booktitle> <pages> pp. 306-311. </pages>
Reference-contexts: Another accuracy measure, which has been used in evaluating systems that bracket the input sentence into unlabeled constituents, is the proportion of constituents in the generated parse that do not cross any constituent boundaries in the correct tree <ref> (Black & et. al., 1991) </ref>.
Reference: <author> Black, E., Jelineck, F., Lafferty, J., Magerman, D., Mercer, R., & Roukos, S. </author> <year> (1993). </year> <title> Towards history-based grammars: Using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 31-37 Columbus, Ohio. </address>
Reference: <author> Black, E., Lafferty, J., & Roukaos, S. </author> <year> (1992). </year> <title> Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 185-192 Newark, Delaware. </address>
Reference: <author> Bod, R. </author> <year> (1993). </year> <title> Using an annotated language corpus as a virtual stochastic grammar. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <month> 778-783. </month> <title> Borland International (1988). Turbo Prolog 2.0 Reference Guide. </title> <booktitle> Borland International, </booktitle> <address> Scotts Valley, CA. </address>
Reference: <author> Brill, E. </author> <year> (1993). </year> <title> Automatic grammar induction and parsing free text: A transformation-based approach. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 259-265 Columbus, Ohio. </address>
Reference-contexts: Following in the footsteps of speech recognition research, these methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods. Although several current methods learn some symbolic structures such as decision trees (Black, Jelineck, Lafferty, Magerman, Mercer, & Roukos, 1993; Magerman, 1995) and transformations <ref> (Brill, 1993) </ref>, more traditional statistical methods dominate. In fact, the term "parsing" is frequently restricted to the process of determining the syntactic structure of a sentence as a hierarchy of labeled constituents. Sometimes only the hierarchy itself is considered, resulting in unlabeled bracketings. <p> Statistical approaches relying on n-grams or probabilistic context-free grammars would have difficulty due to the large number of terminal symbols (around 400) appearing in the modest-sized training corpus. The data for lexical selection would be too sparse to adequately train the pre-defined models. Similarly, the transformational approach of <ref> (Brill, 1993) </ref> is limited to bracketing strings of lexical classes, not words. Chill's ability to learn something useful in the untagged rests on the learning mechanism's ability to automatically construct and attend to just those features of the input that are most useful in guiding parsing.
Reference: <author> Briscoe, T., & Carroll, J. </author> <year> (1993). </year> <title> Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. </title> <journal> Computational Linguistics, </journal> <volume> 19 (1), </volume> <pages> 25-59. </pages>
Reference: <author> Califf, M. E., & Mooney, R. </author> <year> (1997). </year> <title> Relational learning of pattern-match rules for information extraction. </title> <note> submitted. </note>
Reference-contexts: By inducing unbounded relational patterns 51 that characterize the context surrounding particular phrases, concise rules for extracting information can be learned from a training corpus of filled templates <ref> (Califf & Mooney, 1997) </ref>. A significant problem for empirical NLP in general is corpus construction. Annotating large corpora with parse trees, logical forms, or filled templates, is a difficult and time consuming task.
Reference: <author> Cameron-Jones, R. M., & Quinlan, J. R. </author> <year> (1994). </year> <title> Efficient top-down induction of logic programs. </title> <journal> SIGART Bulletin, </journal> <volume> 5 (1), </volume> <pages> 33-42. </pages>
Reference: <author> Charniak, E., & Carroll, G. </author> <year> (1994). </year> <title> Context-sensitive statistics for improved grammatical language models. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence Seattle, </booktitle> <address> WA. </address>
Reference-contexts: One major difference is the type of analysis provided. Chill learns parsers that produce complete, labeled parse trees; other systems have learned to produced simple bracketings of input sentences (Periera & Shabes, 1992; Brill, 1993), or probabilistic language models which assign sentences probabilities <ref> (Charniak & Carroll, 1994) </ref>. Another dimension of variation is the type of input provided to the learning system. While Chill requires only a suitably annotated corpus, other approaches have utilized an existing, complex, hand-crafted grammar that over-generates (Black et al., 1993; Black, Lafferty, & Roukaos, 1992).
Reference: <author> Charniak, E., Hendrickson, C., Jacobson, N., & Perkowitz, M. </author> <year> (1993). </year> <title> Equations for part-of-speech tagging. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 784-789 Washington, D.C. </address>
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Text categorization and relational learning. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 124-132 San Francisco, CA. </address> <publisher> Morgan Kaufman. </publisher> <address> 54 Cohn, D., </address> <note> Atlas, </note> <author> L., & Ladner, R. </author> <year> (1994). </year> <title> Improving generalization with active learning. </title> <journal> Machine Learning, </journal> <volume> 15 (2), </volume> <pages> 201-221. </pages>
Reference-contexts: Detailed experimental comparisons of ILP and feature-based induction have demonstrated the advantages of relational representations in two language related tasks, text categorization <ref> (Cohen, 1995) </ref> and generating the past tense of an English verb (Mooney & Califf, 1995). Our research attempts to bridge the gap between rational and empirical approaches to NLP by applying ILP to the problem of parser acquisition. The main advantage of this approach is its flexibility.
Reference: <author> Engelson, S., & Dagan, I. </author> <year> (1996). </year> <title> Minimizing manual annotation cost in supervised training from corpora. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics Santa Cruz, </booktitle> <address> CA. </address>
Reference-contexts: Not all training examples are equally useful to a learning system, and by carefully selecting useful examples, annotation effort can be dramatically reduced. This approach has been successfully employed in training part-of-speech taggers <ref> (Engelson & Dagan, 1996) </ref> and we intend to explore it's use in parsing and information extraction. 8 Conclusions This paper has presented Chill, a system that uses relational learning methods to induce traditional, shift-reduce parsers from corpora, complementing the results of recent statistical methods.
Reference: <author> Fillmore, C. J. </author> <year> (1968). </year> <title> The case for case. In Bach, </title> <editor> E., & Harms, R. T. (Eds.), </editor> <booktitle> Universals in Linguistic Theory. </booktitle> <publisher> Holt, Reinhart and Winston, </publisher> <address> New York. </address>
Reference-contexts: Additional information on how the same general framework is applied to produce syntactic parse trees and logical queries is provided in section 5. 3 Acquiring a Case-role Parser with Chill 3.1 The Mapping Problem Case theory <ref> (Fillmore, 1968) </ref> decomposes a sentence into a proposition represented by the main verb and various arguments such as agent, patient, and instrument, represented by noun phrases. The basic mapping problem is to decide which sentence constituents fill which roles.
Reference: <author> Goodman, J. </author> <year> (1996a). </year> <title> Efficient algorithms for parsing the DOP model. </title> <booktitle> In Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pp. </pages> <address> 143-152 Philadelphia, PA. </address>
Reference-contexts: While the metrics are certainly useful in making rough comparisons, the results must be interpreted with caution. Often systems are not run on identical corpora, and even when the corpora are the same, they may be "cleaned up" in different ways <ref> (Goodman, 1996a) </ref>. It is also unclear how one should compare systems that are tuned for optimizing different metrics (e.g., focusing on bracketings rather than labeled parse trees) (Goodman, 1996b). It is even less clear that these measures accurately reflect actual differences in performance of real language processing tasks.
Reference: <author> Goodman, J. </author> <year> (1996b). </year> <title> Parsing algorithms and metrics. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 177-183 Santa Cruz, CA. </address>
Reference-contexts: It is also unclear how one should compare systems that are tuned for optimizing different metrics (e.g., focusing on bracketings rather than labeled parse trees) <ref> (Goodman, 1996b) </ref>. It is even less clear that these measures accurately reflect actual differences in performance of real language processing tasks. It is not necessarily the case that a system scoring 80% on some parsing metric will produce better final application results than one achieving 70%.
Reference: <author> Hindle, D., & Rooth, M. </author> <year> (1993). </year> <title> Structural ambiguity and lexical relations. </title> <journal> Computational Linguistics, </journal> <volume> 19 (1), </volume> <pages> 103-120. </pages>
Reference: <author> Kijsirikul, B., Numao, M., & Shimura, M. </author> <year> (1992). </year> <title> Discrimination-based constructive induction of logic programs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 44-49 San Jose, CA. </address>
Reference-contexts: In general, separating the positive and negative examples may require simultaneously constraining multiple variables. In the table above, X and Z taken together do separate the examples even though they do not do so individually. Chill uses an approach similar to Champ <ref> (Kijsirikul, Numao, & Shimura, 1992) </ref>, employing a greedy algorithm to find a small set of variables that are sufficient for separation.
Reference: <author> Kuhn, R., & De Mori, R. </author> <year> (1995). </year> <title> The application of semantic classification trees to natural language understanding. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17 (5), </volume> <pages> 449-460. </pages>
Reference: <author> Lavrac, N., & Dzeroski, S. </author> <year> (1994). </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Hor-wood. </publisher>
Reference: <author> Lehman, J. F. </author> <year> (1994). </year> <title> Toward the essential nature of satistical knowledge in sense resolution. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 734-741 Seattle, WA. </address>
Reference: <author> Lehnert, W., & Sundheim, B. </author> <year> (1991). </year> <title> A performance evaluation of text-analysis technologies. </title> <journal> AI Magazine, </journal> <volume> 12 (3), </volume> <pages> 81-94. </pages>
Reference-contexts: We are currently developing techniques that automatically acquire this lexicon from the training corpus using a symbolic induction algorithm (Thompson, 1995). We are also exploring the use of ILP methods in constructing rules for information extraction (the task of identifying specific items in a natural language text <ref> (Lehnert & Sundheim, 1991) </ref>). ILP techniques can potentially induce more complex patterns than previous learning methods applied to this task (Riloff, 1996; Soderland, Fisher, Aseltine, & Lehnert, 1995).
Reference: <author> Lewis, D. D., & Catlett, J. </author> <year> (1994). </year> <title> Heterogeneous uncertainty sampling for supervised learning. </title> <booktitle> In Proceed--ings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 148-156 San Francisco, CA. </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Magerman, D. M. </author> <year> (1995). </year> <title> Statistical decision-tree models for parsing. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 276-283 Cambridge, MA. </address>
Reference: <author> Magerman, D. M. </author> <year> (1994). </year> <title> Natural Lagnuage Parsing as Statistical Pattern Recognition. </title> <type> Ph.D. thesis, </type> <institution> Stanford University. </institution>
Reference: <author> Manning, C. D. </author> <year> (1993). </year> <title> Automatic acquisition of a large subcategorization dictionary from corpora. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 235-242 Columbus, Ohio. </address>
Reference-contexts: The empirical alternative, which has been very successful in speech recognition, replaces hand-generated rules with models obtained automatically by training over language corpora. Corpus-based methods may be used to augment the knowledge of a traditional parser, for example by acquiring new case-frames for verbs <ref> (Manning, 1993) </ref> or acquiring models to resolve lexical or attachment ambiguities (Lehman, 1994; Hindle & Rooth, 1993). More radical approaches attempt to replace the hand-crafted components altogether, extracting all required linguistic knowledge directly from suitable corpora.
Reference: <author> Marcus, M. </author> <year> (1980). </year> <title> A Theory of Syntactic Recognition for Natural Language. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Marcus, M., Santorini, B., & Marcinkiewicz, M. </author> <year> (1993). </year> <title> Building a large annotated corpus of English: The Penn treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19 (2), </volume> <pages> 313-330. </pages>
Reference-contexts: They must devise a training corpus that demonstrates the type of NL analysis required. For example, if the desired system is a syntactic parser, then the required corpus is a large sampling of text paired with the desired syntactic parse trees. Such a corpus is sometimes called a treebank <ref> (Marcus, Santorini, & Marcinkiewicz, 1993) </ref>. Although some systems have used raw (unannotated) text for grammar acquisition, those employing annotations have produced more accurate parsers (Periera & Shabes, 1992). The second task, acquisition, is a machine learning problem.
Reference: <author> McClelland, J. L., & Kawamoto, A. H. </author> <year> (1986). </year> <title> Mechanisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Merialdo, B. </author> <year> (1994). </year> <title> Tagging English text with a probabilistic model. </title> <journal> Computational Linguistics, </journal> <volume> 20 (2), </volume> <pages> 155-172. </pages>
Reference: <author> Miikkulainen, R., & Dyer, M. G. </author> <year> (1991). </year> <title> Natural language processing with modular PDP networks and distributed lexicon. </title> <journal> Cognitive Science, </journal> <volume> 15, </volume> <pages> 343-399. </pages>
Reference: <author> Miller, S., Bobrow, R., Ingria, R., & Schwartz, R. </author> <year> (1994). </year> <title> Hidden understanding models of natural language. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 25-32. </pages>
Reference-contexts: So far, empirical techniques have been employed largely in the attempt to create broad-coverage parsers for relatively shallow representations, as in part-of-speech tagging (Merialdo, 1994; Charniak, Hendrickson, Jacobson, & Perkowitz, 1993) and the induction of stochastic context-free grammars (Periera & Shabes, 1992) or transition networks <ref> (Miller, Bobrow, Ingria, & Schwartz, 1994) </ref>. Following in the footsteps of speech recognition research, these methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods.
Reference: <author> Miller, S., Stallard, D., Bobrow, R., & Schwartz, R. </author> <year> (1996). </year> <title> A fully statistical approach to natural language interfaces. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 55-61 Santa Cruz, CA. </address> <note> 56 Mooney, </note> <author> R. J., & Califf, M. E. </author> <year> (1995). </year> <title> Induction of first-order decision lists: Results on learning the past tense of English verbs. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3, </volume> <pages> 1-24. </pages>
Reference-contexts: Training on 900 of these sentences produced a parser which achieved 61% exact-match accuracy on the remaining 100 sentences. This approach was recently extended to construct a complete interface with separate statistically trained modules for syntactic, semantic and discourse analysis <ref> (Miller, Stallard, Bobrow, & Schwartz, 1996) </ref>. However, the mapping to a final semantic representation employs two separate modules, requiring each training sentence to be labeled with both a parse tree and a semantic frame.
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 281-297. </pages> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: ILP methods have successfully induced small programs for sorting and list manipulation (Quinlan & Cameron-Jones, 1993) as well as produced encouraging results on important applications such as predicting protein secondary structure <ref> (Muggleton, King, & Sternberg, 1992) </ref>. Detailed experimental comparisons of ILP and feature-based induction have demonstrated the advantages of relational representations in two language related tasks, text categorization (Cohen, 1995) and generating the past tense of an English verb (Mooney & Califf, 1995). <p> The overall effect is a compression of the concept definition, replacing many specific instances with a few general clauses from which the instances can be derived. A successful representative of this class is Muggleton and Feng's Golem <ref> (Muggleton & Feng, 1992) </ref>. Like Foil, Golem may be viewed as a greedy set covering algorithm, where new clauses are hypothesized 8 by considering least-general generalizations (LGGs) of more specific clauses (Plotkin, 1970).
Reference: <author> Muggleton, S., King, R., & Sternberg, M. </author> <year> (1992). </year> <title> Protein secondary structure prediction using logic-based machine learning. </title> <journal> Protein Engineering, </journal> <volume> 5 (7), </volume> <pages> 647-657. </pages>
Reference-contexts: ILP methods have successfully induced small programs for sorting and list manipulation (Quinlan & Cameron-Jones, 1993) as well as produced encouraging results on important applications such as predicting protein secondary structure <ref> (Muggleton, King, & Sternberg, 1992) </ref>. Detailed experimental comparisons of ILP and feature-based induction have demonstrated the advantages of relational representations in two language related tasks, text categorization (Cohen, 1995) and generating the past tense of an English verb (Mooney & Califf, 1995). <p> The overall effect is a compression of the concept definition, replacing many specific instances with a few general clauses from which the instances can be derived. A successful representative of this class is Muggleton and Feng's Golem <ref> (Muggleton & Feng, 1992) </ref>. Like Foil, Golem may be viewed as a greedy set covering algorithm, where new clauses are hypothesized 8 by considering least-general generalizations (LGGs) of more specific clauses (Plotkin, 1970).
Reference: <author> Muggleton, S. H. (Ed.). </author> <year> (1992). </year> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference-contexts: ILP methods have successfully induced small programs for sorting and list manipulation (Quinlan & Cameron-Jones, 1993) as well as produced encouraging results on important applications such as predicting protein secondary structure <ref> (Muggleton, King, & Sternberg, 1992) </ref>. Detailed experimental comparisons of ILP and feature-based induction have demonstrated the advantages of relational representations in two language related tasks, text categorization (Cohen, 1995) and generating the past tense of an English verb (Mooney & Califf, 1995). <p> The overall effect is a compression of the concept definition, replacing many specific instances with a few general clauses from which the instances can be derived. A successful representative of this class is Muggleton and Feng's Golem <ref> (Muggleton & Feng, 1992) </ref>. Like Foil, Golem may be viewed as a greedy set covering algorithm, where new clauses are hypothesized 8 by considering least-general generalizations (LGGs) of more specific clauses (Plotkin, 1970).
Reference: <author> Periera, F., & Shabes, Y. </author> <year> (1992). </year> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 128-135 Newark, Delaware. </address>
Reference-contexts: Such a corpus is sometimes called a treebank (Marcus, Santorini, & Marcinkiewicz, 1993). Although some systems have used raw (unannotated) text for grammar acquisition, those employing annotations have produced more accurate parsers <ref> (Periera & Shabes, 1992) </ref>. The second task, acquisition, is a machine learning problem. Given a suitable training corpus, learning algorithms are employed to automatically construct a parser that can map subsequent inputs into the de 2 sired representation. The parser acquisition problem is depicted in Figure 1. <p> So far, empirical techniques have been employed largely in the attempt to create broad-coverage parsers for relatively shallow representations, as in part-of-speech tagging (Merialdo, 1994; Charniak, Hendrickson, Jacobson, & Perkowitz, 1993) and the induction of stochastic context-free grammars <ref> (Periera & Shabes, 1992) </ref> or transition networks (Miller, Bobrow, Ingria, & Schwartz, 1994). Following in the footsteps of speech recognition research, these methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods.
Reference: <author> Plotkin, G. D. </author> <year> (1970). </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B., & Michie, D. (Eds.), </editor> <booktitle> Machine Intelligence (Vol. </booktitle> <volume> 5). </volume> <publisher> Elsevier North-Holland, </publisher> <address> New York. </address>
Reference-contexts: A successful representative of this class is Muggleton and Feng's Golem (Muggleton & Feng, 1992). Like Foil, Golem may be viewed as a greedy set covering algorithm, where new clauses are hypothesized 8 by considering least-general generalizations (LGGs) of more specific clauses <ref> (Plotkin, 1970) </ref>. A clause G subsumes a clause C if there is a substitution for the variables in G that make the literals in G a subset of the literals in C. Informally, we could turn C into G by dropping some conditions and changing some constants to variables.
Reference: <author> Quinlan, J. R., & Cameron-Jones, R. M. </author> <year> (1993). </year> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pp. 3-20 Vienna. </pages>
Reference-contexts: Due to the expressiveness of first-order logic, ILP methods can learn relational and recursive concepts that cannot be represented in the feature-based languages assumed by most machine-learning algorithms. ILP methods have successfully induced small programs for sorting and list manipulation <ref> (Quinlan & Cameron-Jones, 1993) </ref> as well as produced encouraging results on important applications such as predicting protein secondary structure (Muggleton, King, & Sternberg, 1992).
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 (3), </volume> <pages> 239-266. </pages>
Reference: <author> Riloff, E. </author> <year> (1996). </year> <title> An empirical study of automated dictionary construction for information extraction in three domains. </title> <journal> Artificial Intelligence, </journal> <volume> 85, </volume> <pages> 101-134. </pages>
Reference: <author> Simmons, R. F., & Yu, Y. </author> <year> (1992). </year> <title> The acquisition and use of context dependent grammars for English. </title> <journal> Computational Linguistics, </journal> <volume> 18 (4), </volume> <pages> 391-418. </pages>
Reference-contexts: sentences, the prepositional phrase could be attached to the verb (making fork an instrument of ate) or the object (cheese is an accompaniment of pasta), and semantic knowledge is required to make the correct assignment. 3.2 Constructing the Overly-General Parser Our system adopts a simple shift-reduce framework for case-role mapping <ref> (Simmons & Yu, 1992) </ref>. The process is best illustrated by way of example.
Reference: <author> Soderland, S., Fisher, D., Aseltine, J., & Lehnert, W. </author> <year> (1995). </year> <title> Crystal: Inducing a conceptual dictionary. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1314-1319. </pages>
Reference: <author> Thompson, C. A. </author> <year> (1995). </year> <title> Acquisition of a lexicon from semantic representations of sentences. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 335-337 Cambridge, MA. </address>
Reference-contexts: When parsing into logical form, Chill currently requires a semantic lexicon that maps words into predicates and/or terms that are then composed to produce a final output. We are currently developing techniques that automatically acquire this lexicon from the training corpus using a symbolic induction algorithm <ref> (Thompson, 1995) </ref>. We are also exploring the use of ILP methods in constructing rules for information extraction (the task of identifying specific items in a natural language text (Lehnert & Sundheim, 1991)).
Reference: <author> Tomita, M. </author> <year> (1986). </year> <title> Efficient Parsing for Natural Language. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference: <author> Warren, D., & Pereira, F. </author> <year> (1982). </year> <title> An efficient easily adaptable system for interpreting natural language queries. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> 8 (3-4), </volume> <pages> 110-122. </pages>
Reference: <author> Zelle, J. M. </author> <year> (1995). </year> <title> Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. </title> <type> Ph.D. thesis, </type> <institution> University of Texas, Austin, TX. </institution> <note> Also appears as Artificial Intelligence Laboratory Technical Report AI 96-249. </note>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994a). </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 343-351 New Brunswick, NJ. </address>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994b). </year> <title> Inducing deterministic Prolog parsers from treebanks: A machine learning approach. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 748-753 Seattle, WA. </address> <month> 58 </month>
Reference-contexts: Our initial experiments used this simple representation of parsing actions <ref> (Zelle & Mooney, 1994b) </ref>. However, improved results were obtained by specializing the operators, effectively increasing the number of operators, but reducing the complexity of the control-rule induction task for each operator. The basic idea was to index the operators based on some relevant portion of the parsing context.
References-found: 50

