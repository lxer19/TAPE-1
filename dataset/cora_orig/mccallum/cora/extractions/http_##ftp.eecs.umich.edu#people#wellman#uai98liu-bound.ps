URL: http://ftp.eecs.umich.edu/people/wellman/uai98liu-bound.ps
Refering-URL: http://ftp.eecs.umich.edu/people/wellman/
Root-URL: http://www.eecs.umich.edu
Email: fchaolin,wellmang@umich.edu  
Title: Using Qualitative Relationships for Bounding Probability Distributions  
Author: Chao-Lin Liu and Michael P. Wellman 
Affiliation: University of Michigan AI Laboratory  
Date: July 1998  
Address: Madison, WI, USA,  Ann Arbor, Michigan 48109, USA  
Note: In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI-98), pages 346-353,  
Abstract: We exploit qualitative probabilistic relationships among variables for computing bounds of conditional probability distributions of interest in Bayesian networks. Using the signs of qualitative relationships, we can implement abstraction operations that are guaranteed to bound the distributions of interest in the desired direction. By evaluating incrementally improved approximate networks, our algorithm obtains monotonically tightening bounds that converge to exact distributions. For supermodular utility functions, the tightening bounds monotonically reduce the set of admissible decision alternatives as well. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Berger, J. O. </author> <year> 1985. </year> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: x 2 : u (d 1 ; x 2 )+u (d 2 ; x 1 ) u (d 1 ; x 1 )+ A common example of supermodular functions is the utility function defined as the negative of the squared-error loss function L (d; x) = (d x) 2 (cf. <ref> (Berger 1985) </ref>). The tightening intervals for the expected values of X imply that the range of D in which the optimal d exists is decreasing, thereby helping decision makers to focus on fewer alterna tives of D in applications with monotone decisions.
Reference: <author> Boddy, M., and Dean, T. L. </author> <year> 1994. </year> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence 67:245285. </journal>
Reference: <author> Chrisman, L. </author> <year> 1995. </year> <title> Incremental conditioning of lower and upper probabilities. </title> <journal> International Journal of Approximate Reasoning 13:125. </journal>
Reference-contexts: A CDF F (x) is a lower bound of F (x), if F (x) F SD F (x). Notice that lower and upper probabilities <ref> (Chrisman 1995) </ref> and bounds of CDFs are related concepts. With the bounds of CDFs, we may define lower and upper probabilities of X. Let M denote the event that x i &lt; X x j .
Reference: <author> Chrisman, L. </author> <year> 1996. </year> <title> Propagation of 2-monotone lower probabilities on an undirected graph. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 168184. </pages>
Reference: <author> Cozman, F. </author> <year> 1997. </year> <title> Robustness analysis of Bayesian networks with local convex sets of distributions. </title> <booktitle> In Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 108115. </pages>
Reference: <author> Dagum, P., and Luby, M. </author> <year> 1993. </year> <title> Approximating probabilistic inference in Bayesian belief networks is NP-hard. </title> <journal> Artificial Intelligence 60:141153. </journal>
Reference-contexts: 1 Introduction Approximation techniques have gained increasing interest among those employing Bayesian networks for probabilistic reasoning, despite the fact that computing a desired probability distribution to a fixed degree of accuracy has been shown to be NP-hard <ref> (Dagum & Luby 1993) </ref>. Approximation techniques offer reasonable prospects of significant accuracy, and increased opportunity to consider applications larger than we could otherwise. For instance, approximation techniques can be useful for applications that need to respond to requests for solutions under time constraints.
Reference: <author> D'Ambrosio, B. </author> <year> 1993. </year> <title> Incremental probabilistic inference. </title> <booktitle> In Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, 301308. </booktitle> <address> Washington, DC: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Dechter, R. </author> <year> 1997. </year> <title> Mini-buckets: A general scheme for generating approximations in automated reasoning. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 12971302. </pages>
Reference-contexts: Our algorithm requires the existence of qualitative relationships among some variables. Some algorithms use the maximal and minimal values of a set of numbers in computing the desired bounds. For instance, the mini-buckets algorithm uses maximizing and minimizing functions to bound the values of functions for mini-buckets <ref> (Dechter 1997) </ref>. Our algorithm is more close to Poole's method (1997), but we use bounds of conditional CDFs and Poole uses bounds of conditional probability val ues. Our algorithm is different from existing algorithms in some aspects.
Reference: <author> Draper, D. L., and Hanks, S. </author> <year> 1994. </year> <title> Localized partial evaluation of belief networks. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <volume> 170 177. </volume>
Reference-contexts: For instance, bounded conditioning computes bounds of probabilities by limiting the number of cutset instances used in computation (Horvitz, Suermondt, & Cooper 1989). Localized partial evaluation computes intervals of probability values by ignoring selected nodes in the network <ref> (Draper & Hanks 1994) </ref>. One advantage of computing probability intervals over point-valued approximations is that intervals explicitly specify a range that contains exact solutions, thereby providing information about bounds of possible errors. Algorithms that compute point-valued approximations typically do not provide comparable information. <p> The incremental term computation method takes advantage of the idea of more probable instances but uses a different strategy to compute the bounds (D'Ambrosio 1993). The localized partial evaluation algorithm removes selected nodes from networks to compute probability intervals and recovers selected nodes to improve intervals <ref> (Draper & Hanks 1994) </ref>. Our algorithm ignores some distinction of states of selected variables to compute approximations and recovers distinction of states to improve the approximations. Similar to some other algorithms, ours assumes special numerical properties of the underlying distributions of Bayesian networks. <p> Another difference is that we compute bounds using point-valued information rather than propagating bounds in the computation <ref> (Draper & Hanks 1994) </ref>. Finally, we require the networks be specified with point-valued probabilistic information. We compute bounds of CDFs rather than exact CDFs for saving computational cost. The imprecision is an artifact of the computation algorithm.
Reference: <author> Druzdzel, M. J., and Henrion, M. </author> <year> 1993. </year> <title> Efficient reasoning in qualitative probabilistic networks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 548553. </pages>
Reference-contexts: We operationalize Theorem 1 using the state-space abstraction methods. To compute bounds of the desired CDFs F (zje), we can abstract the state space of any node A that meets the conditions of the theorem. We may apply the inference algorithms for QPNs <ref> (Druzdzel & Henrion 1993) </ref> to locate those nodes whose children have unambiguous qualitative relationships with Z as specified in the first condition of the theorem. We apply (1) to assign the CPT of A, and we apply (2) or (3) to assign the CPTs of the child nodes of A.
Reference: <author> Fishburn, P. C., and Vickson, R. G. </author> <year> 1978. </year> <title> Theoretical foundations of stochastic dominance. </title> <editor> In Whitmore, G. A., and Findlay, M. C., eds., </editor> <title> Stochastic Dominance: An Approach to Decision Making Under Risk, </title> <type> 39113. </type> <address> Lexing-ton, MA: D. C. </address> <publisher> Heath and Company. </publisher>
Reference-contexts: Each arc in the network is marked with a signpositive (+), negative (), or ambiguous (?)denoting the sign of the qualitative probabilistic relationship between its terminal nodes. The interpretation of such qualitative influences is based on first-order stochastic dominance (F SD) <ref> (Fishburn & Vickson 1978) </ref>. Let F (x) and F 0 (x) denote two CDFs of a random variable X. <p> The expected value of a variable Z is R Z z dF (zje). Therefore, by Theorem 3, the intervals for the expected value of Z must tighten, if we compute the expected value using the lower and upper bounds of F (zje). Theorem 3 (cf. <ref> (Fishburn & Vickson 1978) </ref>) Let g (x) be a monotonically increasing function of a random variable X, and F (x) and F 0 (x) denote two cumulative distribution functions of X. Then, F (x) F SD F 0 (x) iff R g (x)dF (x) g (x)dF 0 (x).
Reference: <author> Henrion, M. </author> <year> 1988. </year> <title> Propagating uncertainty in Bayesian networks by probabilistic logic sampling. </title> <editor> In Lemmer, J. F., and Kanal, L. N., eds., </editor> <booktitle> Uncertainty in Artificial Intelligence 2. </booktitle> <publisher> Elsevier Science Publishers. </publisher> <pages> 149163. </pages>
Reference: <author> Horvitz, E. </author> <year> 1990. </year> <title> Computation and Action Under Bounded Resources. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University. </institution>
Reference: <author> Horvitz, E.; Suermondt, H. J.; and Cooper, G. F. </author> <year> 1989. </year> <title> Bounded conditioning: Flexible inference for decisions under scarce resources. </title> <booktitle> In Proceedings of the Fifth Workshop on Uncertainty in Artificial Intelligence, </booktitle> <pages> 182193. </pages>
Reference-contexts: Another popular approach is to compute bounds or intervals of the desired probability distributions. For instance, bounded conditioning computes bounds of probabilities by limiting the number of cutset instances used in computation <ref> (Horvitz, Suermondt, & Cooper 1989) </ref>. Localized partial evaluation computes intervals of probability values by ignoring selected nodes in the network (Draper & Hanks 1994). <p> For instance, bounded conditioning ignores some cutset instance to compute probability bounds and consider more instances to improve the bounds <ref> (Horvitz, Suermondt, & Cooper 1989) </ref>. Search-based algorithms search more probable assignments of all variables and use these instances to compute probability bounds. The bounds can be improved by considering more instances that are less probable (Poole 1993).
Reference: <author> Jaakkola, T. S., and Jordan, M. I. </author> <year> 1996. </year> <title> Computing upper and lower bounds on likelihoods in intractable networks. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 340348. </pages>
Reference: <author> Liu, C.-L., and Wellman, M. P. </author> <year> 1998. </year> <title> Incremental tradeoff resolution in qualitative probabilistic networks. </title> <booktitle> In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence. </booktitle>
Reference-contexts: For instance, bounds of probability distributions can be used to resolve ambiguous qualitative relationships between variables in QPNs. We report applications of bounds to this task in another paper <ref> (Liu & Wellman 1998) </ref>. We may also combine the ISSA algorithm with inference algorithms with QPNs to return purely qualitative solu tions and incrementally more precise numerical solutions. Assume a Bayesian network in which links are already marked with qualitative signs.
Reference: <author> Luo, C.; Yu, C.; Lobo, J.; Wang, G.; and Pham, T. </author> <year> 1996. </year> <title> Computation of best bounds of probabilities from uncertain data. </title> <journal> Computational Intelligence 12:541566. </journal>
Reference: <author> Neal, R. M. </author> <year> 1993. </year> <title> Probabilistic Inference Using Markov Chain Monte Carlo Methods. </title> <type> Ph.D. Dissertation, </type> <institution> University of Toronto, Canada. </institution>
Reference: <author> Neapolitan, R. E. </author> <year> 1990. </year> <title> Probabilistic Reasoning in Expert Systems: Theory and Algorithms. </title> <publisher> Wiley. </publisher>
Reference-contexts: The strengthening of F (yjx) can be carried out by using the values in the conditional probability tables (CPT) associated with Y . In the following theorem statements, we use ancestral ordering of nodes as defined below. Definition 3 (cf. <ref> (Neapolitan 1990) </ref>) Let J denote a set of nodes fJ 1 ; : : : ; J n g in a Bayesian network. [J 1 ; : : : ; J n ] is an ancestral ordering of the nodes in J if for every J i 2 J all the
Reference: <author> Pearl, J. </author> <year> 1987. </year> <title> Evidential reasoning using stochastic simulation of causal models. </title> <booktitle> Artificial Intelligence 32:245 257. </booktitle>
Reference: <author> Poole, D. </author> <year> 1993. </year> <title> Average-case analysis of a search algorithm for estimating prior and posterior probabilities in Bayesian networks with extreme probabilities. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 606612. </pages>
Reference-contexts: Search-based algorithms search more probable assignments of all variables and use these instances to compute probability bounds. The bounds can be improved by considering more instances that are less probable <ref> (Poole 1993) </ref>. The incremental term computation method takes advantage of the idea of more probable instances but uses a different strategy to compute the bounds (D'Ambrosio 1993).
Reference: <author> Poole, D. </author> <year> 1997. </year> <title> Probabilistic partial evaluation: Exploiting rule structure in probabilistic inference. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 12841291. </pages>
Reference: <author> Ross, S. M. </author> <year> 1983. </year> <title> Introduction to Stochastic Dynamic Programming. </title> <publisher> Academic Press. </publisher>
Reference-contexts: Let the function ffi u (x) choose the value of decision variable D to maximize the utility u for a variable x. ffi u (x) = arg max u (d; x) It can be shown that ffi u (x) increases monotonically in x if u is a supermodular function <ref> (Ross 1983) </ref>.
Reference: <author> Thone, H.; Guntzer, U.; and Kieling, W. </author> <year> 1992. </year> <title> Towards precision of probabilistic bounds propagation. </title> <booktitle> In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence, </booktitle> <volume> 315322. </volume> <editor> van Engelen, R. A. </editor> <year> 1997. </year> <title> Approximating Bayesian belief networks by arc removal. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 19(8):916920. </journal>
Reference: <author> Walley, P. </author> <year> 1991. </year> <title> Statistical Reasoning with Imprecise Probabilities. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Wellman, M. P. </author> <year> 1990. </year> <title> Fundamental concepts of qualitative probabilistic networks. </title> <booktitle> Artificial Intelligence 44:257 303. </booktitle>
Reference-contexts: The extended algorithm takes advantage of qualitative relationships among variables in the computation. The qualitative relationships that summarize special quantitative dependence relationships among variables are as originally defined for qualitative probabilistic networks (QPNs) <ref> (Wellman 1990) </ref>. When variables in Bayesian networks exhibit these special quantitative relationships, it is possible to compute bounds of conditional CDFs of interest using such relationships. We report conditions under which the extended ISSA algorithm can compute bounds of condi-tional CDFs, and show that these bounds tighten monotonically with iterations. <p> In Section 5, we describe some applications of the bounds, and, in Section 6, we compare and contrast our algorithm with existing algorithms designed for computing probability intervals. 2 Background 2.1 Qualitative relationships The qualitative relationships we employ are based on those defined for qualitative probabilistic networks (QPNs) <ref> (Wellman 1990) </ref>. QPNs are abstractions of Bayesian networks, with conditional probability tables summarized by the signs of qualitative relationships between variables. Each arc in the network is marked with a signpositive (+), negative (), or ambiguous (?)denoting the sign of the qualitative probabilistic relationship between its terminal nodes. <p> Then, F (x) F SD F 0 (x) iff R g (x)dF (x) g (x)dF 0 (x). The tightening intervals for expected values can be use-ful for applications that use monotone decisions <ref> (Wellman 1990) </ref>.
Reference: <author> Wellman, M. P., and Liu, C.-L. </author> <year> 1994. </year> <title> State-space abstraction for anytime evaluation of probabilistic networks. </title> <booktitle> In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 567574. </pages>
Reference-contexts: For instance, stochastic simulation algorithms compute approximations of desired distributions with random numbers sampled based on the given Bayesian network (Pearl 1987; Henrion 1988; Neal 1993). Some other algorithms compute approximations by ignoring information that specifies the exact distribution in the given network, for example, state-space abstraction <ref> (Wellman & Liu 1994) </ref> and arc removal (van Engelen 1997). Another popular approach is to compute bounds or intervals of the desired probability distributions. For instance, bounded conditioning computes bounds of probabilities by limiting the number of cutset instances used in computation (Horvitz, Suermondt, & Cooper 1989). <p> In contrast, the algorithms that compute point-valued approximations usually expect the approximations to improve with the allocated computation time only on average. In this paper, we extend our iterative state-space abstraction (ISSA) algorithm <ref> (Wellman & Liu 1994) </ref> to compute the bounds of cumulative distribution functions (CDFs) of interest. The extended algorithm takes advantage of qualitative relationships among variables in the computation. The qualitative relationships that summarize special quantitative dependence relationships among variables are as originally defined for qualitative probabilistic networks (QPNs) (Wellman 1990). <p> As we mention in Section 4.1, the existence of a particular qualitative relationship between A and nodes in Y facilitates, but is not required for, the application of the theorems. 4 State-space abstraction In previous work, we report an iterative state-space abstraction (ISSA) algorithm for approximate evaluation of Bayesian networks <ref> (Wellman & Liu 1994) </ref>. The ISSA algorithm aggregates states of variables into superstates to construct abstract versions of the original Bayesian networks (OBNs) that specify exact probability distributions. We use these abstract Bayesian networks (ABNs) to compute point-valued approximations of the probability distributions of interest.
Reference: <author> Wellman, M. P.; Ford, M.; and Larson, K. </author> <year> 1995. </year> <title> Path planning under time-dependent uncertainty. </title> <booktitle> In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> 532539. </pages>
Reference-contexts: In addition, our theorems can be used to compute the bounds of travel costs on stochastically consistent transportation networks defined in <ref> (Wellman, Ford, & Larson 1995) </ref>.
References-found: 28

