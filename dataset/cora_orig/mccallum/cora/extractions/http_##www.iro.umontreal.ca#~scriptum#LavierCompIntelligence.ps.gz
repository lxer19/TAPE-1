URL: http://www.iro.umontreal.ca/~scriptum/LavierCompIntelligence.ps.gz
Refering-URL: http://www.iro.umontreal.ca/~scriptum/PublicationsMembres.html
Root-URL: http://www.iro.umontreal.ca
Title: Using a Functional Language for Parsing and Semantic Processing  
Author: Guy Lapalme Fabrice Lavier 
Keyword: syntax, semantic, parsing, functional programming  
Date: July 9, 1997  
Address: CP 6128, Succ "A" Montreal Quebec Canada  
Affiliation: Departement d'informatique et de recherche operationnelle Universite de Montreal  
Pubnum: H3C 3J7  
Abstract: This paper shows how a non-strict functional programming language with polymorphic typing can be used to define grammar rules and semantic evaluation along the lines of Montague. This approach provides a unified formalism needing no preprocessing or postprocessing to the functional language itself: parsing and semantics are declared naturally using function definition and evaluation is done by lambda application. We show that by changing only the model we can, after parsing, compute either the truth value of a sentence or its parse tree. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Bird and P. Wadler. </author> <title> Introduction to Functional Programming. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: For further details, one should consult [22]. Bird and Wadler <ref> [1] </ref> provide an excellent introduction to functional programming of this kind. The rest of this section can be skipped by someone already familiar with Miranda or not interested in implementing the basic parsing mechanisms. 2.2 Basic elements Values are of three primitive types, called num, bool, and char.
Reference: [2] <author> L. Bolc. </author> <title> The Design of Interpreters, Compilers and Editors for Augmented Transition Networks. </title> <publisher> Springer-Verlag, </publisher> <year> 1983. </year>
Reference-contexts: Lambda calculus can be dealt using functional programming languages like Lisp or Scheme. Unfortunately those languages do not really offer a declarative style of programming except by using special purpose interpreters like Augmented Transition Networks <ref> [2] </ref>.
Reference: [3] <author> W.H. Burge. </author> <title> Recursive Programming Techniques. </title> <publisher> Addison-Wesley, </publisher> <year> 1978. </year>
Reference-contexts: In our case, following <ref> [3] </ref> and [26], we represent a parser as a function that accepts a list of tokens and returns either an empty list as indication of failure or a list of results each comprising an interpretation of the tokens accepted (e.g. a parse tree) and the list of tokens remaining in the <p> For example, we can define a binary tree as bin_tree * ::= Nil | Node2 (bin_tree *) (bin_tree *) where the different alternatives are separated by a vertical bar (|). 6 3 Syntactic Analysis We now show how these tools can be applied for building natural language parsers. Following <ref> [3] </ref> and [26], a parser is defined as a function that accepts an input pair (v,xs) where v is a value to be transformed by the parser and an input string xs; it returns either an empty list to signal failure or a list of pairs (v',xs') where v' is the <p> example, a rule of the form a ! c j a b can be transformed into a ! c fbg* but the values are combined on the left using a function f f : : : (f (f c b 1 )b 2 ) : : : b n Following <ref> [3, 21] </ref>, this transformation can be abstracted into a function using list comprehension as follows: recsequencing::(*-&gt;*->*-&gt;*) -&gt; parser * -&gt; parser * -&gt; parser * recsequencing f p q (v,xs) = arb_q (p (v,xs)) where arb_q [] = [] ++ vxs which is specialized in the same way as for sq. <p> We used a very simple parsing algorithm which is the classical depth first with backtracking approach as introduced by Burge <ref> [3] </ref> and Wadler [26]. This technique is now a "standard" practice in many applications of functional programming [24, 14, 21] but to our knowledge we are the first with Frost [7] to apply it in the context of natural language processing instead of the usual artificial (computer) languages. <p> Frost and Launchbury [7] have shown how a functional lazy language can be useful in this aspect and we intend to pursue in that direction. The parsers shown here are a stream-based implementation of top-down backtrack parsing following the ideas of Burge <ref> [3] </ref> and Wadler [26] but are not the most efficient implementation of context-free parsing. We are quite sure that it would be simple to implement a non-backtrack parser in a functional language and in this way get an order of magnitude in speed-up.
Reference: [4] <author> Robin Cooper. </author> <title> Quantification and Semantic Theory. </title> <publisher> Reidel, </publisher> <address> Dordrecht, </address> <year> 1983. </year>
Reference: [5] <author> D.R. Dowty, R. E. Wall, and S. Peters. </author> <title> Introduction to Montague Semantics. </title> <booktitle> Studies in Linguistics and Philosophy. </booktitle> <address> D. </address> <publisher> Reidel, </publisher> <year> 1985. </year>
Reference-contexts: One of the most fruitful avenue for a declarative definition of semantic processing has been the Montague Grammars <ref> [5, 18] </ref> approach which relies on lambda calculus (i.e. function definition, manipulation and application). Lambda calculus can be dealt using functional programming languages like Lisp or Scheme. Unfortunately those languages do not really offer a declarative style of programming except by using special purpose interpreters like Augmented Transition Networks [2]. <p> We take as first example, the"L0e" language defined in <ref> [5, p.23] </ref> with the following grammar where terminal symbols are given in quotes and non-terminal ones are in italic. n ! "Sadie" j "Liz" j "Hank" vi ! "snores" j "sleeps" j "is boring" vt ! "loves" j "hates" j "is taller than" neg ! "it is not the case that" <p> Phrase structure rules are used to combine these basic meanings into new functions which are then evaluated according to a model. As the application of typed lambda abstractions is the fundamental tool of Miranda, it is ideal for implementing Montague semantics. 4.1 Elements of truth-conditional semantics Dowty <ref> [5, p.42] </ref> gives the following "essential ingredients" of a truth conditional semantics: 1. "A set of things which can be assigned as semantic values. [...] these are (1) a set of individuals, (2) a set of truth values, (3) various functions contructed out of these by means of set theory." In <p> We give in comments the equation numbers used in pages 25-35 of Dowty <ref> [5] </ref>. (&), ("/) and (~) are the standard Miranda functions for the logical and, or and not of boolean values. f0 "Sadie" = E A_Sadat f0 "Liz" = E QE_II f0 "Hank" = E H_Kissinger f0 "Mary" = E MMonroe || Vi's f0 "snores"= Fet snoref || (2-13) where snoref x <p> y = x=y The above "ingredients" form a model defined as "a model is an ordered pair hA; F i where A is a set of individuals and F is a function which assigns semantic values of the appropriate sort to the basic expression"<ref> [5, p. 45] </ref> Model M 0 of [5, p. 45] is defined as: m =(everybody,f0,show_semantic) everybody = [E A_Sadat, E QE_II, E H_Kissinger, E MMonroe] show_semantic :: semantic -&gt; [char] show_semantic (T x) = show x || print only truth value where we add, for obvious computational reasons, a function to transform a truth value to a printable <p> We differ in our interests, they are more concerned with the fundamentals of their higher-order logic in the framework of Prolog while we are focused on the semantics but starting from a functional language. 5 Semantics with individual variables Dowty <ref> [5, p 69-70] </ref> introduces variables in natural language sentence by means of common nouns (here man, woman and fish) that become bound in the context of quantifiers (here every, some and the). Variables are then refered to by that followed by the common noun.
Reference: [6] <author> J. Fairbairn. </author> <title> Making form follow function: an exercise in functional programming style. </title> <journal> Software-Practice and Experience, </journal> <volume> 17(6) </volume> <pages> 379-386, </pages> <year> 1987. </year>
Reference-contexts: This function can be used as follows () indicates the value returned by the call): 3 search 'd' [('a',1),('b',2),('c',3),('a',4),('b',5),('d',6)] ) <ref> [6] </ref> search 'b' [('a',1),('b',2),('c',3),('a',4),('b',5),('d',6)] ) [2,5] Lists can also be defined using "list comprehensions" giving a concise syntax for a rather general class of iterations over lists. <p> Using currying, we now define along the lines of <ref> [6] </ref> search_a_or_b = search_a $alt search_b so that search a or b [('a',1),('b',2),('c',3),('a',4),('b',5),('d',6)] ) [1,4,2,5] 2.5 Sequencing Sequentially combining two "searchs" means in fact combining all the results of the first search with the ones of the second.
Reference: [7] <author> R. Frost and J. Launchbury. </author> <title> Constructing natural language interpreters in a lazy functional language. </title> <journal> Computer Journal, </journal> <volume> 32(2) </volume> <pages> 108-121, </pages> <year> 1989. </year>
Reference-contexts: This technique is now a "standard" practice in many applications of functional programming [24, 14, 21] but to our knowledge we are the first with Frost <ref> [7] </ref> to apply it in the context of natural language processing instead of the usual artificial (computer) languages. The parsing strategy also corresponds to the analysis strategy used by a standard DCG grammar interpreter in Prolog where unification helps a lot for passing information between components. <p> As Miranda bears a much closer ressemblance with the -calculus formalism and this makes our approach more "natural". Frost and Launchbury <ref> [7] </ref> have also used a lazy functional language to implement a parser and a semantic evaluator inspired by the Montague compositional approach. But their semantics is extensional: in their case, the meaning of a word is a list of integers that serve as indices in a global set of properties. <p> We have only shown here how to evaluate a formula according to a model and this is accordance with the original idea of Montague semantics but one of the important goal of natural language processing is the possibility of deducing some new facts. Frost and Launchbury <ref> [7] </ref> have shown how a functional lazy language can be useful in this aspect and we intend to pursue in that direction.
Reference: [8] <author> R. A. Frost. </author> <title> Use of algebraic identities in the calculation of programs constructed as executable specifications of attribute grammars. </title> <institution> School of Computer Science, University of Windsor, </institution> <month> october </month> <year> 1989. </year>
Reference-contexts: In Prolog, partially specified structures can be easily constructed but in a functional language we can often achieve much of the same goal by returning a function to be applied later when its argument becomes known. Frost <ref> [8] </ref> has shown how attribute grammars can be "packaged" using higher-order functions in a lazy functional language. In our case, parsing is only a small component of our approach, we are more interested in the semantic part along the lines of Montague which is described in the following sections.
Reference: [9] <author> A. Gal, G. Lapalme, and P. St-Dizier. </author> <title> Prolog pour l'analyse automatique de la langue naturelle. </title> <publisher> Eyrolles, </publisher> <year> 1988. </year>
Reference-contexts: The functional has some advantages compared to logic grammars <ref> [20, 10, 9] </ref>: we have a unified functional framework for dealing with lexical, syntactic and semantic processing.
Reference: [10] <author> Gerald Gazdar and Chris Mellish. </author> <title> Natural Language Processing in PROLOG. </title> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction One of the problems to be solved within natural language processing is a smooth integration of semantic and syntactic processing. Logic Programming formalisms like Metamorphosis Grammars or Definite Clause Grammars <ref> [20, 10, 23] </ref> have achieved a goal of a declarative definition of syntax while integrating an operative definition of semantics by means of "evaluable predicates" or by simulation of semantic processing using meta-interpreters that evaluate data-structures built by the syntactic component. <p> The functional has some advantages compared to logic grammars <ref> [20, 10, 9] </ref>: we have a unified functional framework for dealing with lexical, syntactic and semantic processing.
Reference: [11] <author> J. R. Hobbs and S.J. Rosenshein. </author> <title> Making computational sense of Montague's intensional logic. </title> <journal> Artificial Intelligence, </journal> <volume> 9 </volume> <pages> 287-306, </pages> <year> 1978. </year>
Reference-contexts: Sadie VT loves N Liz CONJ or @ N Sadie VI snores @ N Sadie VT loves N Liz , Unanalyzed:["or","Sadie","snores"] 4.4 Discussion Montague semantics is often implemented via a special interpreter or processor that comes into play after parsing is completed but here we stay within a unique framework. <ref> [11] </ref> has shown how to "make computational sense of Montague's Intensional Logic" but in the framework of Lisp: expressions are given procedural interpretations but parsing is not discussed. As Miranda bears a much closer ressemblance with the -calculus formalism and this makes our approach more "natural".
Reference: [12] <author> Jerry R. Hobbs and Stuart M. Shieber. </author> <title> An algorithm for generating quantifier scopings. </title> <booktitle> Computational Linguistics, </booktitle> <address> 13(1-2):47-63, </address> <month> January-June </month> <year> 1987. </year>
Reference-contexts: It is reminiscent of the "Cooper Storage"[4] mechanism for generating quantifier scopings. We are aware that this ad-hoc approach is limited and we give some of its drawbacks at the end of this section but we are currently working on integrating the approach of Hobbs and Schieber <ref> [12] </ref> into this parser. The point we make here is that this functional approach to parsing and semantic processing can be adapted to deal elegantly with quantifiers.
Reference: [13] <editor> P. Hudak and P. Wadler (editors). </editor> <title> Report on the programming language Haskell, a non-strict purely functional language (Version 1.0). </title> <type> Technical Report YALEU/DCS/RR777, </type> <institution> Yale University, Department of Computer Science, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: language, it would be possible to translate our approach to a "pure" Lisp subset but a great part of the simplicity and efficiency of our approach rests upon the ease of specification of lazy evaluation and currying inherent in modern typed functional language such as Miranda [25] 1 and Haskell <ref> [13] </ref> where functions are defined by means of recursive equations. Unfortunately, it seems that this approach has been largely ignored by researchers interested in natural language processing.
Reference: [14] <author> Graham Hutton. </author> <title> Parsing using combinators. </title> <editor> In Kei Davis and John Hughes, editors, </editor> <booktitle> Functional Programming, Glasgow 1989, Workshops in Computing, </booktitle> <pages> pages 353-370. </pages> <publisher> Springer-Verlag, </publisher> <month> Aug </month> <year> 1989 1990. </year>
Reference-contexts: We used a very simple parsing algorithm which is the classical depth first with backtracking approach as introduced by Burge [3] and Wadler [26]. This technique is now a "standard" practice in many applications of functional programming <ref> [24, 14, 21] </ref> but to our knowledge we are the first with Frost [7] to apply it in the context of natural language processing instead of the usual artificial (computer) languages.
Reference: [15] <author> Guy Lapalme and Fabrice Lavier. </author> <title> Using a functional language for parsing and semantic process-ing. </title> <institution> Publication 715a, Departement d'informatique et de recherche operationnelle, Universite de Montreal, </institution> <year> 1990. </year>
Reference-contexts: In <ref> [15] </ref>, we describe in more detail some of the subtleties involved in the functional approach to parsing using the well known example of arithmetic expressions parsing. 3.2.4 Applying functions It is sometimes useful to transform the resulting values of a parser using a function.
Reference: [16] <author> Dale A. Miller and Gopalan Nadathur. </author> <title> Some uses of higher-order logic in computational linguistics. </title> <booktitle> In Proceedings of the 24th Annual Meeting of the ACL, </booktitle> <pages> pages 247-256, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Composition is achieved using set intersection and union. Our approach is more intensional as advocated by Montague: the "meanings" are functions in a typed -calculus and new meanings are obtained using function composition. Another interesting approach is the one taken by Miller and Nadathur <ref> [16] </ref> in Prolog which defines higher-order definite clauses called -terms.
Reference: [17] <author> R. Milner. </author> <title> A theory of type polymorphism in programming. </title> <journal> Journal of Computer and System Science, </journal> <volume> 17 </volume> <pages> 348-375, </pages> <year> 1978. </year>
Reference-contexts: It is a good programming practice though because it provides a good documentation for the human reader and allows the compiler to check our definitions and to pinpoint more precisely any discrepancy between what is defined and what was "intended". 5 Types can be polymorphic, in the sense of <ref> [17] </ref>. This means that a single definition can be used for arguments of different types provided we are consistent in their use. This is indicated by using the symbols * ** *** ... as an alphabet of generic type variables.
Reference: [18] <author> R. Montague. </author> <title> The proper treatment of quantification in ordinary english. In Approaches to Natural Language: </title> <booktitle> Proceedings of the 1970 Stanford Workshop on Grammar and Semantics. </booktitle> <address> D. </address> <publisher> Reidel, </publisher> <year> 1973. </year>
Reference-contexts: One of the most fruitful avenue for a declarative definition of semantic processing has been the Montague Grammars <ref> [5, 18] </ref> approach which relies on lambda calculus (i.e. function definition, manipulation and application). Lambda calculus can be dealt using functional programming languages like Lisp or Scheme. Unfortunately those languages do not really offer a declarative style of programming except by using special purpose interpreters like Augmented Transition Networks [2].
Reference: [19] <author> R. T. Oerhle, E. Bach, and D. Wheeler. </author> <title> Categorial Grammars and Natural Language Structures. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: This is a form of forward application found in categorial grammars <ref> [19] </ref>. model_parser == parser appl lit:: [char] -&gt; model_parser lit = literal f where f v x = itp x sq,recsq::model_parser -&gt; model_parser -&gt; model_parser sq = sequencing f recsq = recsequencing f f v v1 v2 m = appff (v1 m) (v2 m) 14 With these tools, we write the
Reference: [20] <author> F.C.N. Pereira and S.M. Shieber. </author> <title> Prolog and Natural Language Analysis. </title> <booktitle> CSLI Lecture Notes #10. </booktitle> <institution> Stanford University, </institution> <year> 1987. </year>
Reference-contexts: 1 Introduction One of the problems to be solved within natural language processing is a smooth integration of semantic and syntactic processing. Logic Programming formalisms like Metamorphosis Grammars or Definite Clause Grammars <ref> [20, 10, 23] </ref> have achieved a goal of a declarative definition of syntax while integrating an operative definition of semantics by means of "evaluable predicates" or by simulation of semantic processing using meta-interpreters that evaluate data-structures built by the syntactic component. <p> The functional has some advantages compared to logic grammars <ref> [20, 10, 9] </ref>: we have a unified functional framework for dealing with lexical, syntactic and semantic processing.
Reference: [21] <author> Chris Reade. </author> <title> Elements of Functional Programming. </title> <booktitle> International Computer Science Series. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1989. </year>
Reference-contexts: example, a rule of the form a ! c j a b can be transformed into a ! c fbg* but the values are combined on the left using a function f f : : : (f (f c b 1 )b 2 ) : : : b n Following <ref> [3, 21] </ref>, this transformation can be abstracted into a function using list comprehension as follows: recsequencing::(*-&gt;*->*-&gt;*) -&gt; parser * -&gt; parser * -&gt; parser * recsequencing f p q (v,xs) = arb_q (p (v,xs)) where arb_q [] = [] ++ vxs which is specialized in the same way as for sq. <p> We used a very simple parsing algorithm which is the classical depth first with backtracking approach as introduced by Burge [3] and Wadler [26]. This technique is now a "standard" practice in many applications of functional programming <ref> [24, 14, 21] </ref> but to our knowledge we are the first with Frost [7] to apply it in the context of natural language processing instead of the usual artificial (computer) languages.
Reference: [22] <institution> Research Software Limited. </institution> <note> Miranda System Manual. </note> <year> 1987. </year>
Reference-contexts: For further details, one should consult <ref> [22] </ref>. Bird and Wadler [1] provide an excellent introduction to functional programming of this kind.
Reference: [23] <editor> Patrick Saint-Dizier and Stan Szpakowicz, editors. </editor> <booktitle> Logic and Logic Grammars for Language Processing. Series in Artificial Intelligence. </booktitle> <publisher> Ellis Horwood, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction One of the problems to be solved within natural language processing is a smooth integration of semantic and syntactic processing. Logic Programming formalisms like Metamorphosis Grammars or Definite Clause Grammars <ref> [20, 10, 23] </ref> have achieved a goal of a declarative definition of syntax while integrating an operative definition of semantics by means of "evaluable predicates" or by simulation of semantic processing using meta-interpreters that evaluate data-structures built by the syntactic component.
Reference: [24] <author> Simon Thompson. </author> <title> Interactive functional programs. </title> <editor> In David A. Turner, editor, </editor> <booktitle> Research Topics in Functional Programming, University of Texas at Austin Year of Programming Series, </booktitle> <pages> pages 249-285. </pages> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: We used a very simple parsing algorithm which is the classical depth first with backtracking approach as introduced by Burge [3] and Wadler [26]. This technique is now a "standard" practice in many applications of functional programming <ref> [24, 14, 21] </ref> but to our knowledge we are the first with Frost [7] to apply it in the context of natural language processing instead of the usual artificial (computer) languages.
Reference: [25] <author> D.A. Turner. </author> <title> Miranda anon strict functional language with polymorphic types. </title> <editor> In P. Jouan-naud, editor, </editor> <booktitle> Conference on Functional Programming and Computer Architecture, Lecture Notes in Computer Science #201, </booktitle> <pages> pages 1-16, </pages> <year> 1985. </year>
Reference-contexts: using a pure functional language, it would be possible to translate our approach to a "pure" Lisp subset but a great part of the simplicity and efficiency of our approach rests upon the ease of specification of lazy evaluation and currying inherent in modern typed functional language such as Miranda <ref> [25] </ref> 1 and Haskell [13] where functions are defined by means of recursive equations. Unfortunately, it seems that this approach has been largely ignored by researchers interested in natural language processing.
Reference: [26] <author> P. Wadler. </author> <title> How to replace failure by a list of successes. </title> <editor> In P. Jouannaud, editor, </editor> <booktitle> Conference on Functional Programming and Computer Architecture, Lecture Notes in Computer Science #201, </booktitle> <pages> pages 114-127, </pages> <year> 1985. </year>
Reference-contexts: In our case, following [3] and <ref> [26] </ref>, we represent a parser as a function that accepts a list of tokens and returns either an empty list as indication of failure or a list of results each comprising an interpretation of the tokens accepted (e.g. a parse tree) and the list of tokens remaining in the input. <p> They also allow an alternative to the backtracking implementation technique for nondeterminism using the "list of successes" approach advocated by <ref> [26] </ref>. Miranda is a strongly typed language that enables a compile time checking of the program. Miranda allows for polymorphic types, so a single function definition can be used in many contexts while keeping a static checking of the program. <p> Following [3] and <ref> [26] </ref>, a parser is defined as a function that accepts an input pair (v,xs) where v is a value to be transformed by the parser and an input string xs; it returns either an empty list to signal failure or a list of pairs (v',xs') where v' is the value transformed <p> We used a very simple parsing algorithm which is the classical depth first with backtracking approach as introduced by Burge [3] and Wadler <ref> [26] </ref>. This technique is now a "standard" practice in many applications of functional programming [24, 14, 21] but to our knowledge we are the first with Frost [7] to apply it in the context of natural language processing instead of the usual artificial (computer) languages. <p> Frost and Launchbury [7] have shown how a functional lazy language can be useful in this aspect and we intend to pursue in that direction. The parsers shown here are a stream-based implementation of top-down backtrack parsing following the ideas of Burge [3] and Wadler <ref> [26] </ref> but are not the most efficient implementation of context-free parsing. We are quite sure that it would be simple to implement a non-backtrack parser in a functional language and in this way get an order of magnitude in speed-up.
Reference: [27] <author> D.A. Watt. </author> <title> Executable semantic descriptions. </title> <journal> Software-Practice and Experience, </journal> <volume> 16(1) </volume> <pages> 13-43, </pages> <year> 1986. </year>
Reference-contexts: define grammar rules and semantic evaluation along the lines of Montague within a unified formalism needing no preprocessing (like the DCGs) or postprocessing (like a semantic evaluator) to the functional language itself: parsing and semantics are declared naturally using function definition (lambda abstractions) and evaluation is done by lambda application. <ref> [27] </ref> used this kind of approach to develop the semantics of programming languages. The next section presents Miranda the functional language we choose for our experiments.
References-found: 27

