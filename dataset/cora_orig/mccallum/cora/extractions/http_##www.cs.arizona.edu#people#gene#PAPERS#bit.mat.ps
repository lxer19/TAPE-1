URL: http://www.cs.arizona.edu/people/gene/PAPERS/bit.mat.ps
Refering-URL: http://www.cs.arizona.edu/people/gene/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A Fast Bit-Vector Algorithm for Approximate String Matching Based on Dynamic Programming  
Author: Gene Myers 
Date: March 27, 1998  
Abstract: The approximate string matching problem is to find all locations at which a query of length m matches a substring of a text of length n with k-or-fewer differences. Simple and practical bit-vector algorithms have been designed for this problem, most notably the one used in agrep. These algorithms compute a bit representation of the current state-set of the k-difference automaton for the query, and asymptotically run in O(nmk=w) time where w is the word size of the machine (e.g. 32 or 64 in practice). Here we present an algorithm of comparable simplicity that requires only O(nm=w) time by virtue of computing a bit representation of the relocatable dynamic programming matrix for the problem. Thus the algorithm's performance is independent of k, and it is found to be more efficient than the previous results for many useful choices of k and small m. Moreover, because the algorithm is not dependent on k, it can be used to rapidly compute blocks of the dynamic programming matrix as in the 4-Russians algorithm of Wu, Manber, and Myers. This gives rise to an O(kn=w) expected-time algorithm for the case where m may be arbitrarily large. In practice this new algorithm, which computes a region of the d.p. matrix in 1 fi w blocks using the basic algorithm as a subroutine, is significantly faster than our previous 4-Russians algorithm, which computes the same region in 1 fi 5 blocks using table lookup. This performance improvement yields a code which is superior to all existing algorithms except for some filtration algorithms when k=m is sufficiently small.
Abstract-found: 1
Intro-found: 1
Reference: [BYG92] <author> R.A. Baeza-Yates and G.H. Gonnet. </author> <title> A new approach to text searching. </title> <journal> Communications of the ACM, </journal> <volume> 35 </volume> <pages> 74-82, </pages> <year> 1992. </year>
Reference-contexts: At around the same time, another new thread of practice-oriented results exploited the hardware parallelism of bit-vector operations. Letting w be the number of bits in a machine word, this sequence of results began with an O (ndm=we) algorithm for the exact matching case by Baeza-Yates and Gonnet <ref> [BYG92] </ref>, and culminated with an O (kndm=we) algorithm for the k-differences problem by Wu and Manber [WM92]. These authors were interested specifically in text-retrieval applications where m is quite small, small enough that the expression between the ceiling braces is 1.
Reference: [BYN96] <author> R.A. Baeza-Yates and G. Navarro. </author> <title> A faster algorithm for approximate string matching. </title> <booktitle> In Proc. 7th Symp. on Combinatorial Pattern Matching. </booktitle> <publisher> Springer LNCS 1075, </publisher> <pages> pages 1-23, </pages> <year> 1996. </year>
Reference-contexts: These authors were interested specifically in text-retrieval applications where m is quite small, small enough that the expression between the ceiling braces is 1. Under such circumstances the algorithms run in O (n) or O (kn) time, respectively. More recently, Baeza-Yates and Navarro <ref> [BYN96] </ref> have realized an O (ndkm=we) variation on the Wu/Manber algorithm, implying O (n) performance when mk = O (w). The final recent thrust has been the development of filter algorithms that eliminate regions of the text that cannot match the query. <p> Indeed only 17 bit operations are performed per character scanned. This is to be contrasted with the Wu/Manber bit-vector algorithm [WM92] which takes O (m + kn) under the prevailing assumption that m w. The Baeza-Yates/Navarro bit-vector algorithm <ref> [BYN96] </ref> has this same complexity under this assumption, but improves to O (m + n) time when one assumes m 2 w 2 (e.g., m 9 when w = 32 and m 14 when w = 64). Finally, consider the case where m is unrestricted. <p> An operation on such bit-vectors takes O (m=w) time. It then directly follows that the basic algorithm of this section runs in O (m + nm=w) time and O (m=w) space. This is to be contrasted with the previous bit-vector algorithms 7 <ref> [WM92, BYN96] </ref>, both of which take O (m + knm=w) time asymptotically. This leads us to say that our algorithm represents a true asymptotic improvement over previous bit-vector algorithms. 4 The Unrestricted Algorithm. The Blocks Model. <p> The first is a study of our basic bit-vector algorithm and the two previous bit-vector results <ref> [WM92, BYN96] </ref> for approximate string matching when m w. The second set of experiments involves all verification-capable algorithms that work when k and m are unrestricted. Experiments to determine the range of k=m for which filter algorithms are superior have not been performed in this preliminary study.
Reference: [CL92] <author> W.I. Chang and J. Lampe. </author> <title> Theoretical and empirical comparisons of approximate string matching algorithms. </title> <booktitle> In Proc. 3rd Symp. on Combinatorial Pattern Matching. </booktitle> <publisher> Springer LNCS 644, </publisher> <pages> pages 172-181, </pages> <year> 1992. </year>
Reference-contexts: The algorithm achieves its efficiency by computing only the region or zone of the underlying dynamic programming matrix that has entries less than or equal to k. Further refining this basic design, Chang and Lampe <ref> [CL92] </ref> went on to devise a faster algorithm which is conjectured to run in O (kn= ) expected-time where is the size of the underlying alphabet. <p> We may thus embed it in the zone paradigm of the Ukkonen algorithm, exactly as we did with the 4-Russians technique [WMM96]. The result is an O (kn=w) expected-time algorithm which we will show in practice outperforms both our previous work and that of Chang and Lampe <ref> [CL92] </ref> for all regions of the (k; ) parameter space. 2 2 Preliminaries Assume the query sequence is P = p 1 p 2 : : : p m , the text is T = t 1 t 2 : : : t n , and that we are given a <p> That is, if x j = maxfi : C (i; j) kg then the algorithm takes time proportional to the size of the zone Z (k) = [ n j=0 f (i; j) : i 2 [0; x j ]g. It was shown in <ref> [CL92] </ref> that the expected size of Z (k) is O (kn). Computing just the zone is easily accomplished with the observation that x j = maxfi : i x j1 + 1 and C (i; j) kg. <p> Our second set of experiments are aimed at comparing verification-capable algorithms that can accommodate unrestricted choices of k and m. In this case, we need only consider our block-based algorithm and the results of <ref> [CL92] </ref> and [WMM96], as all other competitors are already known to be dominated in practice by these two [WMM96].
Reference: [CL94] <author> W.I. Chang and E.L. Lawler. </author> <title> Sublinear expected time approximate matching and biological applications. </title> <journal> Algorithmica, </journal> <volume> 12 </volume> <pages> 327-344, </pages> <year> 1994. </year>
Reference-contexts: The final recent thrust has been the development of filter algorithms that eliminate regions of the text that cannot match the query. The results here can broadly divided into on-line algorithms (e.g. <ref> [WM92, CL94] </ref>) and off-line algorithms (e.g. [Mye94]) that are permitted to preprocess a presumably static text before performing a number of queries over it.
Reference: [GP90] <author> Z. Galil and K. Park. </author> <title> An improved algorithm for approximate string matching. </title> <journal> SIAM J. on Computing, </journal> <volume> 19 </volume> <pages> 989-999, </pages> <year> 1990. </year>
Reference-contexts: Subsequently this was refined to O (kn) expected-time by Ukkonen [Ukk85], then to O (kn) worst-case time, first with O (n) space by Landau and Vishkin [LV88], and later with O (m 2 ) space by Galil and Park <ref> [GP90] </ref>. fl Dept. of Computer Science, University of Arizona Tucson, AZ 85721 (e-mail: gene@cs.arizona.edu). Partially supported by NLM grant LM-04960 1 Of these early algorithms, the O (kn) expected-time algorithm was universally the best in prac-tice.
Reference: [LV88] <author> G.M. Landau and U. Vishkin. </author> <title> Fast string matching with k differences. </title> <journal> J. of Computer and System Sciences, </journal> <volume> 37 </volume> <pages> 63-78, </pages> <year> 1988. </year>
Reference-contexts: Sellers algorithm requires O (mn) time where m is the length of the query and n is the length of the text. Subsequently this was refined to O (kn) expected-time by Ukkonen [Ukk85], then to O (kn) worst-case time, first with O (n) space by Landau and Vishkin <ref> [LV88] </ref>, and later with O (m 2 ) space by Galil and Park [GP90]. fl Dept. of Computer Science, University of Arizona Tucson, AZ 85721 (e-mail: gene@cs.arizona.edu). Partially supported by NLM grant LM-04960 1 Of these early algorithms, the O (kn) expected-time algorithm was universally the best in prac-tice.
Reference: [MP80] <author> W.J. Masek and M. S. Paterson. </author> <title> A faster algorithm for computing string edit distances. </title> <journal> J. of Computer and System Sciences, </journal> <volume> 20 </volume> <pages> 18-31, </pages> <year> 1980. </year>
Reference-contexts: Further refining this basic design, Chang and Lampe [CL92] went on to devise a faster algorithm which is conjectured to run in O (kn= ) expected-time where is the size of the underlying alphabet. Next, Wu, Manber, and this author [WMM96] developed a practical realization of the 4-Russians approach <ref> [MP80] </ref> that when applied to Ukkonen's zone, gives an O (kn= log s) expected-time algorithm, given that O (s) space can be dedicated to a universal lookup table. <p> Formally, define the horizontal delta h [i; j] at (i; j) as C [i; j] C [i; j 1] and the vertical delta v [i; j] as C [i; j] C [i 1; j] for all (i; j) 2 [1; m] fi [1; n]. Lemma 1 <ref> [MP80, Ukk85] </ref>: For all i; j: v [i; j]; h [i; j] 2 f1; 0; 1g. <p> This is the basic observation behind Four Russians approaches to sequence comparison <ref> [MP80, WMM96] </ref>, where the output resulting from every possible input combination is pretabulated and then used to effect the computation of blocks as they are encountered in a particular problem instance.
Reference: [Mye94] <author> E.W. Myers. </author> <title> A sublinear algorithm for approximate keywords searching. </title> <journal> Algorithmica, </journal> <volume> 12 </volume> <pages> 345-374, </pages> <year> 1994. </year>
Reference-contexts: The final recent thrust has been the development of filter algorithms that eliminate regions of the text that cannot match the query. The results here can broadly divided into on-line algorithms (e.g. [WM92, CL94]) and off-line algorithms (e.g. <ref> [Mye94] </ref>) that are permitted to preprocess a presumably static text before performing a number of queries over it.
Reference: [Sel80] <author> P.H. Sellers. </author> <title> The theory and computations of evolutionary distances: </title> <journal> Pattern recognition. J. of Algorithms, </journal> <volume> 1 </volume> <pages> 359-373, </pages> <year> 1980. </year>
Reference-contexts: It has been intensively studied over the last twenty years. In its most common incarnation, the problem is to find substrings that match the query with k or fewer differences. The first algorithm addressing exactly this problem is attributable to Sellers <ref> [Sel80] </ref> although one might claim that it was effectively solved by work in the early 70's on string comparison. Sellers algorithm requires O (mn) time where m is the length of the query and n is the length of the text. <p> The approximate string matching problem is to find all positions j in T such that there is a suffix of T [1::j] matching P with k-or-fewer differences, i.e., j such that min g ffi (P; T [g::j]) k. The classic approach to this problem <ref> [Sel80] </ref> is to compute an (m + 1) fi (n + 1) dynamic programming (d.p.) matrix C [0::m; 0::n] for which C [i; j] = min g ffi (P [1::i]; T [g::j]) after an O (mn) time computation using the well-known recurrence: C [i; j] = minfC [i 1; j 1]
Reference: [Ukk85] <author> E. Ukkonen. </author> <title> Finding approximate patterns in strings. </title> <journal> J. of Algorithms, </journal> <volume> 6 </volume> <pages> 132-137, </pages> <year> 1985. </year>
Reference-contexts: Sellers algorithm requires O (mn) time where m is the length of the query and n is the length of the text. Subsequently this was refined to O (kn) expected-time by Ukkonen <ref> [Ukk85] </ref>, then to O (kn) worst-case time, first with O (n) space by Landau and Vishkin [LV88], and later with O (m 2 ) space by Galil and Park [GP90]. fl Dept. of Computer Science, University of Arizona Tucson, AZ 85721 (e-mail: gene@cs.arizona.edu). <p> Formally, define the horizontal delta h [i; j] at (i; j) as C [i; j] C [i; j 1] and the vertical delta v [i; j] as C [i; j] C [i 1; j] for all (i; j) 2 [1; m] fi [1; n]. Lemma 1 <ref> [MP80, Ukk85] </ref>: For all i; j: v [i; j]; h [i; j] 2 f1; 0; 1g. <p> There are several sequence comparison results that involve computing a region or zone of the underlying dynamic programming matrix, the first of which was <ref> [Ukk85] </ref>. Figure 6 depicts such a hypothetical zone and a tiling of it with 1 fi w blocks. Provided that one can still effectively delimit the zone while performing a block-based computation, using such a tiling gives a factor w speedup over the underlying zone algorithm.
Reference: [WM92] <author> S. Wu and U. Manber. </author> <title> Fast text searching allowing errors. </title> <journal> Communications of the ACM, </journal> <volume> 35 </volume> <pages> 83-91, </pages> <year> 1992. </year>
Reference-contexts: Letting w be the number of bits in a machine word, this sequence of results began with an O (ndm=we) algorithm for the exact matching case by Baeza-Yates and Gonnet [BYG92], and culminated with an O (kndm=we) algorithm for the k-differences problem by Wu and Manber <ref> [WM92] </ref>. These authors were interested specifically in text-retrieval applications where m is quite small, small enough that the expression between the ceiling braces is 1. Under such circumstances the algorithms run in O (n) or O (kn) time, respectively. <p> The final recent thrust has been the development of filter algorithms that eliminate regions of the text that cannot match the query. The results here can broadly divided into on-line algorithms (e.g. <ref> [WM92, CL94] </ref>) and off-line algorithms (e.g. [Mye94]) that are permitted to preprocess a presumably static text before performing a number of queries over it. <p> Indeed only 17 bit operations are performed per character scanned. This is to be contrasted with the Wu/Manber bit-vector algorithm <ref> [WM92] </ref> which takes O (m + kn) under the prevailing assumption that m w. <p> An operation on such bit-vectors takes O (m=w) time. It then directly follows that the basic algorithm of this section runs in O (m + nm=w) time and O (m=w) space. This is to be contrasted with the previous bit-vector algorithms 7 <ref> [WM92, BYN96] </ref>, both of which take O (m + knm=w) time asymptotically. This leads us to say that our algorithm represents a true asymptotic improvement over previous bit-vector algorithms. 4 The Unrestricted Algorithm. The Blocks Model. <p> The first is a study of our basic bit-vector algorithm and the two previous bit-vector results <ref> [WM92, BYN96] </ref> for approximate string matching when m w. The second set of experiments involves all verification-capable algorithms that work when k and m are unrestricted. Experiments to determine the range of k=m for which filter algorithms are superior have not been performed in this preliminary study.
Reference: [WMM96] <author> S. Wu, U. Manber, and G. Myers. </author> <title> A subquadratic algorithm for approximate limited expression matching. </title> <journal> Algorithmica, </journal> <volume> 15 </volume> <pages> 50-67, </pages> <year> 1996. </year> <title> 10 ("ChaLa") and Wu/Manber/Myers ("WMM"), with alphabet sizes (a) = 2, (b) = 4, (c) = 8, (d) = 16, </title> <journal> and (e) = 32. </journal> <volume> 11 </volume>
Reference-contexts: Further refining this basic design, Chang and Lampe [CL92] went on to devise a faster algorithm which is conjectured to run in O (kn= ) expected-time where is the size of the underlying alphabet. Next, Wu, Manber, and this author <ref> [WMM96] </ref> developed a practical realization of the 4-Russians approach [MP80] that when applied to Ukkonen's zone, gives an O (kn= log s) expected-time algorithm, given that O (s) space can be dedicated to a universal lookup table. <p> We may thus embed it in the zone paradigm of the Ukkonen algorithm, exactly as we did with the 4-Russians technique <ref> [WMM96] </ref>. <p> This is the basic observation behind Four Russians approaches to sequence comparison <ref> [MP80, WMM96] </ref>, where the output resulting from every possible input combination is pretabulated and then used to effect the computation of blocks as they are encountered in a particular problem instance. <p> Computing just the zone is easily accomplished with the observation that x j = maxfi : i x j1 + 1 and C (i; j) kg. A block-based algorithm for this O (kn) expected-time algorithm was devised and presented 8 in an earlier paper of ours <ref> [WMM96] </ref> where the blocks were computed in O (1) time using a 4--Russians lookup table. What we are proposing here, is to do exactly the same thing, except to use our bit-vector approach to compute 1 fi w blocks in O (1) time. <p> Our second set of experiments are aimed at comparing verification-capable algorithms that can accommodate unrestricted choices of k and m. In this case, we need only consider our block-based algorithm and the results of [CL92] and <ref> [WMM96] </ref>, as all other competitors are already known to be dominated in practice by these two [WMM96]. These three algorithms are all zone-based and when m is suitably large, the zone never reaches the last row of the d.p. matrix, so that running time does not depend on m. <p> In this case, we need only consider our block-based algorithm and the results of [CL92] and <ref> [WMM96] </ref>, as all other competitors are already known to be dominated in practice by these two [WMM96]. These three algorithms are all zone-based and when m is suitably large, the zone never reaches the last row of the d.p. matrix, so that running time does not depend on m.
References-found: 12

