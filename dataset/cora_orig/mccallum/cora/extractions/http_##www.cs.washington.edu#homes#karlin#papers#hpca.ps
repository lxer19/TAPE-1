URL: http://www.cs.washington.edu/homes/karlin/papers/hpca.ps
Refering-URL: http://www.cs.washington.edu/homes/karlin/papers.html
Root-URL: 
Title: Two Adaptive Hybrid Cache Coherency Protocols  
Author: Craig Anderson Anna R. Karlin 
Address: 1 Infinite Loop, MS 301-4G  Cupertino, CA 95014 Seattle, WA 98195-2350  
Affiliation: Apple Computer Dept. of Computer Science and Engineering  University of Washington  
Abstract: We present and evaluate adaptive, hybrid cache coherence protocols for bus-based, shared-memory multiprocessors. Such protocols are motivated by the observation that sharing patterns vary substantially be tween different programs and even cache blocks within the same program. Performance measurements across a range of parallel applications indicate that the adaptive protocols we present perform well compared to both Write-Invalidate and Write-Update protocols. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Anderson. </author> <title> Improving the Performance of Bus-Based Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1995. </year>
Reference-contexts: However, we ran the appropriate variants of the experiments described in this paper without read-snarfing as well. The results were not substantially different. For more information, see <ref> [1] </ref>. * C i , the cost in bus cycles of an invalidate trans action. * C u , the cost in bus cycles of an update transac tion. * C r , the cost in bus cycles of reading a cache block. * R, the invalidation ratio, defined as (C <p> We will discuss distributions for both 4 and 32 processors runs; unfortunately, in most cases space constraints allow us to include only the graphs for the 32 processor, 128K cache run. The additional graphs may be found in <ref> [1] </ref>. Gauss The most noticeable feature of the Gauss data is the large spike at a write length of 8 (Figure 1). The spike is expected due to the access patterns in the program.
Reference: [2] <author> C. Anderson and J.-L. Baer. </author> <title> Two techniques for improving the performance of bus-based multiprocessors. </title> <booktitle> In IEEE Int. Symp. on High Performance Computer Architecture, </booktitle> <pages> pages 264--275, </pages> <year> 1995. </year>
Reference-contexts: These results are not surprising, given the poor behavior of Pverify when using large block sizes <ref> [2] </ref>. While false sharing hurts performance under both WI and WU, it it much worse under WI, since the entire cache block is "ping-ponging" between the false sharing caches, rather than just the values actually being written. 7 Related Work There have been various studies of hybrid cache coherence protocols.
Reference: [3] <author> J. Archibald. </author> <title> A cache coherence approach for large multiprocessor systems. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 337-345, </pages> <year> 1988. </year>
Reference-contexts: Archibald introduced a protocol which improves over hybrid protocols in which the counter is associated with the reader (as opposed to the writer). In his protocol, a block is not automatically invalidated when its count reaches 0 <ref> [3] </ref>. Instead, a cache simply doesn't raise the shared line if the relevant block's count is 0. If some cache whose block count is not 0 indicates that it is keeping the block by raising the shared line, then no cache invalidates its block.
Reference: [4] <author> E. D. Brooks III, T. S. Axelrod, and G. A. Dar-mohray. </author> <title> The Cerberus multiprocessor simulator. </title> <editor> In G. Rodrigue, editor, </editor> <booktitle> Parallel Processing for Scientific Computing, </booktitle> <pages> pages 384-390. </pages> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: Additional storage is needed for the threshold lookup table. The additional storage needed makes LTS a less practical algorithm than Random Walk. 3 Methodology Using the Cerberus simulator <ref> [4] </ref>, we simulated the protocols described in the previous section. The WI protocol we used is the Illinois protocol [16], while the WU protocol we used is the Dragon protocol [15]. The Dragon protocol uses a shared line to detect if a block being updated is being actively shared.
Reference: [5] <author> F. Dahlgren. </author> <title> Boosting the performance of hybrid snooping cache protocols. </title> <booktitle> In Proc. 22nd Int. Symp. on Computer Architecture, </booktitle> <pages> pages 60-69, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: A processor executing a moderate or long write run will have to pay for several unnecessary updates before it can invalidate. This is a problem with using a static value for the invalidate threshold. More recently, Dahlgren compared Archibald's protocol and a slight variant of the snoopy-reading protocol <ref> [5] </ref>. He found that Archibald's protocol consistently outperformed snoopy-reading. He also found that by using read snarfing, write caching and (necessarily) a release consistency model, along with Archibald's protocol, substantially increased program performance of Archibald's protocol over a base architecture executing an invalidate protocol.
Reference: [6] <author> S. Devadas and A. R. </author> <title> Newton. Topological optimization of multiple level array logic. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <month> November </month> <year> 1987. </year>
Reference-contexts: MP3D is a 3-dimensional particle simulator which has poor locality and incurs a great deal of coherence misses. We ran MP3D on 20000 molecules for 100 steps (497 million instructions). Topopt performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm <ref> [6] </ref>. This application exhibits a fair amount of both true and false sharing [7]. We used the cpla1.lomim input file (1.7 billion instructions). Pverify determines whether two boolean circuits are functionally identical [14]. It exhibits a great deal of false sharing, even when using moderate-sized blocks [7].
Reference: [7] <author> S. Eggers and T. Jeremiassen. </author> <title> Eliminating false-sharing. </title> <booktitle> In Proc. of Int. Conf. on Parallel Processing, </booktitle> <pages> pages I-377-381, </pages> <year> 1991. </year>
Reference-contexts: We ran MP3D on 20000 molecules for 100 steps (497 million instructions). Topopt performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm [6]. This application exhibits a fair amount of both true and false sharing <ref> [7] </ref>. We used the cpla1.lomim input file (1.7 billion instructions). Pverify determines whether two boolean circuits are functionally identical [14]. It exhibits a great deal of false sharing, even when using moderate-sized blocks [7]. We ran Pverify on the circuits C880.21.berk1 and C880.21.berk2 (1 billion instructions). <p> This application exhibits a fair amount of both true and false sharing <ref> [7] </ref>. We used the cpla1.lomim input file (1.7 billion instructions). Pverify determines whether two boolean circuits are functionally identical [14]. It exhibits a great deal of false sharing, even when using moderate-sized blocks [7]. We ran Pverify on the circuits C880.21.berk1 and C880.21.berk2 (1 billion instructions). Our final applications is Barnes-Hut, from the SPLASH suite. Barnes exhibits a very low coherence miss rate, even when 64 processors are used. We simulated 128 bodies (111 million instructions).
Reference: [8] <author> S. Eggers and R. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In Proc. of 15th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 373-382, </pages> <year> 1988. </year>
Reference-contexts: A key program characteristic underlying these examples is the block write run, which is defined as a sequence of write references to a shared cache block, uninterrupted by either an access to that block by another processor, or replacement of the block <ref> [8] </ref>. In the first example above, there were many write runs of length one; in the second, a single write run of length n. In general, short write runs favor write-update protocols, while long write runs favor invalidate based protocols. <p> In our algorithms, updating is analogous to spinning, while invalidating is analogous to blocking (since it entails the subsequent large reread cost). 2 SR stands for "snoopy reading", the name given to the protocol by Eggers and Katz <ref> [8] </ref> These protocols are dynamic versions of the com-petitive protocol SR. They maintain a counter and an invalidation threshold T b for each cache block b. The counter associated to a block is initialized to the value of T b .
Reference: [9] <author> S. Eggers and R. Katz. </author> <title> Evaluating the performance of four snooping cache coherence protocols. </title> <booktitle> In Proc. of 16th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 2-15, </pages> <year> 1989. </year>
Reference-contexts: Eggers and Katz compared the performance of the hybrid protocol of [12], which they called "snoopy reading", with WU and WI <ref> [9] </ref>. Using a value of 3 for the invalidation threshold, they found that neither WU nor WI was uniformly superior and that snoopy reading consistently had performance which was intermediate between the two.
Reference: [10] <author> J. Goodman and P. Woest. </author> <title> The Wisconsin Multi-cube: A new large-scale cache coherent multiprocessor. </title> <booktitle> In Proc. of 15th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 422-431, </pages> <year> 1988. </year>
Reference-contexts: Thus it is important to reduce the number of bus busy cycles where possible. In this section, we discuss the bus-cycle costs of each coherence scheme, and describe the adaptive protocols that we will measure. All the results presented in this paper assume that the system implements read-snarfing 1 <ref> [17, 10] </ref>. In such a system, the cache snoop mechanism of every cache, and not only of the requesting cache, checks the address associated with all completed cache-to-cache read operations on the bus.
Reference: [11] <author> A. Karlin, K. Li, M. Manasse, and S. Owicki. </author> <title> Empirical studies of competitive spinning for a shared-memory multiprocessor. </title> <booktitle> In Proc. of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 41-55, </pages> <year> 1991. </year>
Reference-contexts: Both protocols are adapted from competitive thread-spinning algorithms found in <ref> [11] </ref>. In that paper, the algorithms decide when to switch between spinning a thread while waiting for a lock | an inexpensive operation that can be repeated many times | and blocking a thread, which is an expensive operation that is done only once.
Reference: [12] <author> A. Karlin, M. Manasse, L. Rudolf, and D. Sleator. </author> <title> Competitive snoopy caching. </title> <booktitle> In Proc. of 27th Symposium on Foundations of Computer Science, </booktitle> <pages> pages 244-254, </pages> <year> 1986. </year>
Reference-contexts: Karlin et al. proposed an algorithm whose choice of invalidate threshold did no worse than twice the bus cycle cost of the optimal off-line algorithm <ref> [12] </ref>. However, it often did not achieve the best performance. One reason why is that using a single, fixed value of invalidate threshold for all blocks and for all applications necessitates a compromise, since applications have widely varying write run distributions (which we show in section 4). <p> Eggers and Katz compared the performance of the hybrid protocol of <ref> [12] </ref>, which they called "snoopy reading", with WU and WI [9]. Using a value of 3 for the invalidation threshold, they found that neither WU nor WI was uniformly superior and that snoopy reading consistently had performance which was intermediate between the two.
Reference: [13] <author> A. Karlin, M. Manasse, L. Rudolf, and D. Sleator. </author> <title> Competitive snoopy caching. </title> <journal> Algorithmica, </journal> <volume> 3 </volume> <pages> 79-119, </pages> <year> 1988. </year>
Reference-contexts: One advantage of this protocol is that its worst-case performance is bounded: the number of bus cycles used by this protocol is provably within a factor of 2 of the number used by the optimal o*ine protocol <ref> [13] </ref>. 2.3 The Adaptive Protocols We now describe the two dynamic hybrid protocols. Both protocols are adapted from competitive thread-spinning algorithms found in [11].
Reference: [14] <author> H.-K. T. Ma, S. Devadas, R. Wei, and A. Sangiovanni-Vincentelli. </author> <title> Logic verification algorithms and their parallel implementation. </title> <booktitle> In Proceedings of the 24th Design Automation Conference, </booktitle> <pages> pages 283-290, </pages> <year> 1987. </year>
Reference-contexts: Topopt performs topological optimization on VLSI circuits using a parallel simulated annealing algorithm [6]. This application exhibits a fair amount of both true and false sharing [7]. We used the cpla1.lomim input file (1.7 billion instructions). Pverify determines whether two boolean circuits are functionally identical <ref> [14] </ref>. It exhibits a great deal of false sharing, even when using moderate-sized blocks [7]. We ran Pverify on the circuits C880.21.berk1 and C880.21.berk2 (1 billion instructions). Our final applications is Barnes-Hut, from the SPLASH suite. Barnes exhibits a very low coherence miss rate, even when 64 processors are used.
Reference: [15] <author> E. McCreight. </author> <title> The Dragon computer system: An early overview. </title> <type> Technical report, </type> <institution> Xerox Corp., </institution> <year> 1984. </year>
Reference-contexts: The additional storage needed makes LTS a less practical algorithm than Random Walk. 3 Methodology Using the Cerberus simulator [4], we simulated the protocols described in the previous section. The WI protocol we used is the Illinois protocol [16], while the WU protocol we used is the Dragon protocol <ref> [15] </ref>. The Dragon protocol uses a shared line to detect if a block being updated is being actively shared.
Reference: [16] <author> M. Papamarcos and J. Patel. </author> <title> A low overhead coherence solution for multiprocessors with private cache memories. </title> <booktitle> In Proc. of 11th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 348-354, </pages> <year> 1984. </year>
Reference-contexts: Additional storage is needed for the threshold lookup table. The additional storage needed makes LTS a less practical algorithm than Random Walk. 3 Methodology Using the Cerberus simulator [4], we simulated the protocols described in the previous section. The WI protocol we used is the Illinois protocol <ref> [16] </ref>, while the WU protocol we used is the Dragon protocol [15]. The Dragon protocol uses a shared line to detect if a block being updated is being actively shared.
Reference: [17] <author> Z. Segall and L. Rudolph. </author> <title> Dynamic decentralized cache schemes for an MIMD parallel processor. </title> <booktitle> In Proc. of 11th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 340-347, </pages> <year> 1984. </year>
Reference-contexts: Thus it is important to reduce the number of bus busy cycles where possible. In this section, we discuss the bus-cycle costs of each coherence scheme, and describe the adaptive protocols that we will measure. All the results presented in this paper assume that the system implements read-snarfing 1 <ref> [17, 10] </ref>. In such a system, the cache snoop mechanism of every cache, and not only of the requesting cache, checks the address associated with all completed cache-to-cache read operations on the bus.
Reference: [18] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We then ran the modified version of Gauss with a large cache so that the entire array would stay resident during the parallel section of the program. We call this program Badgauss. Cholesky is a program from the SPLASH benchmark suite for sparse matrix factorization <ref> [18] </ref>. Our simulation used the BCSSTK14 input file (65 million instructions). MP3D is another program from the SPLASH suite. MP3D is a 3-dimensional particle simulator which has poor locality and incurs a great deal of coherence misses. We ran MP3D on 20000 molecules for 100 steps (497 million instructions).
Reference: [19] <author> C. Thacker, D. Conroy, and L. Stewart. </author> <title> The Alpha demonstration unit: A high-performance multiprocessor for software and chip development. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4) </volume> <pages> 51-65, </pages> <year> 1992. </year>
Reference-contexts: This scheme also has the advantage that a succession of single writes from different processors will not cause other copies to be deleted. It does this without any additional hardware required. Veenstra and Fowler evaluated the cache coherence protocol used by the DEC Alpha AXP <ref> [19] </ref>, which uses the snoopy reading scheme with an invalidate threshold of 2 [20]. They reported that using a threshold of 2 resulted in shared blocks being invalidated too quickly, thus applications did not gain from selective updating.
Reference: [20] <author> J. Veenstra and R. Fowler. </author> <title> The prospects for on-line hybrid coherency protocols on bus-based multiprocessors. </title> <type> Technical Report 490, </type> <institution> University of Rochester, </institution> <year> 1994. </year>
Reference-contexts: It does this without any additional hardware required. Veenstra and Fowler evaluated the cache coherence protocol used by the DEC Alpha AXP [19], which uses the snoopy reading scheme with an invalidate threshold of 2 <ref> [20] </ref>. They reported that using a threshold of 2 resulted in shared blocks being invalidated too quickly, thus applications did not gain from selective updating. Increasing the invalidate threshold in order to get more updating behavior does have drawbacks, however.
References-found: 20

