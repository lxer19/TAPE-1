URL: http://www.ri.cmu.edu/afs/cs/project/cmcl/archive/GNectar-papers/95superstreams.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/usr/prs/WWW/papers.html
Root-URL: 
Title: Gigabit I/O for Distributed-Memory Machines: Architecture and Applications  
Author: Michael Hemy and Peter Steenkiste 
Date: January 9, 1996  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Distributed-memory systems have traditionally had great difficulty performing network I/O at rates proportional to their computational power. The problem is that the network interface has to support network I/O for a supercomputer, using computational and memory bandwidth resources similar to those of a workstation. As a result, the network interface becomes a bottleneck. We implemented an architecture for network I/O for the iWarp system with the following two key characteristics: first, application-specific tasks are off-loaded from the network interface to the distributed-memory system, and second, these tasks are performed in close cooperation with the application. The network interface has been used by several applications for over a year. In this paper we describe the network interface software that manages the communication between the iWarp distributed-memory system and the network interface, we validate the main features of our network interface architecture based on application experience, and we discuss how this architecture can be used by other distributed-memory systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Barnett, L. Shuler, R. van de Geijn, S. Gupta, D. G. Payne, and J. Watts. </author> <title> Interprocessor collective communication library (intercom). </title> <booktitle> In Proceedings of the IEEE Scalable High Performance Computing Conference, </booktitle> <pages> pages 357-364. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Some of these techniques are similar to those used by other groups. For example, data reorganization has been proposed to achieve high bandwidth access to disks in distributed-memory systems [2, 14, 17] and the data reorganization is in effect a collective communication operations <ref> [10, 1] </ref>. Other systems also rely on the network interface to perform protocol processing. However, while we run a relatively standard TCP/IP implementation on the network interface, others use outboard protocol processing (e.g. [22]), or a customized protocol implementation relying heavily on hardware support (e.g. [18]).
Reference: [2] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <address> Oregon, </address> <month> November </month> <year> 1993. </year> <month> ACM/IEEE. </month>
Reference-contexts: This architecture alleviates the load on the network interface which can then handle the messages more efficiently. Some of these techniques are similar to those used by other groups. For example, data reorganization has been proposed to achieve high bandwidth access to disks in distributed-memory systems <ref> [2, 14, 17] </ref> and the data reorganization is in effect a collective communication operations [10, 1]. Other systems also rely on the network interface to perform protocol processing.
Reference: [3] <author> Shekhar Borkar, Robert Cohn, George Cox, Sha Gleason, Thomas Gross, H. T. Kung, Monica Lam, Brian Moore, Craig Peterson, John Pieper, Linda Rankin, P. S. Tseng, Jim Sutton, John Urbanski, and Jon Webb. </author> <title> iwarp: An integrated solution to high-speed parallel computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 330-339, </pages> <address> Orlando, Florida, </address> <month> November </month> <year> 1988. </year> <journal> IEEE Computer Society and ACM SIGARCH. </journal>
Reference-contexts: The reason is that network I/O is typically supported through an I/O node with the same computational power as one of the compute nodes, i.e. a workstation-class node has to support network I/O for a supercomputer. The I/O architecture used for the HIPPI interface for the iWarp system <ref> [3] </ref>, described in [19], relies on a careful distribution of the network functions (e.g. protocol processing, data formatting and connection management) between the distributed-memory system and the network interface to achieve high-bandwidth network I/O.
Reference: [4] <author> Shekhar Borkar, Robert Cohn, George Cox, Thomas Gross, H.T. Kung, Monica Lam, Margie Levine, Brian Moore, Wire Moore, Craig Peterson, Jim Susman, Jim Sutton, John Urbanski, and Jon Webb. </author> <title> Supporting systolic and memory communication in iwarp. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 70-81, </pages> <address> Seattle, </address> <month> May </month> <year> 1990. </year> <note> ACM/IEEE. Also published as CMU Technical Report CMU-CS-90-197. </note>
Reference-contexts: 5. 7 4 The iWarp Streams Software In this section we give a more detailed description of the features that where implemented for the iWarp streams software. 4.1 Data Interface The data interface between the distributed-memory system and the network interface is based on the PCS and ESPL communication libraries <ref> [12, 4] </ref>. PCS is used to create application-specific connections, and ESPL is a fast spooling library that achieves bandwidths close to the 40 MBytes/second link rate, even for 8 short messages. Spooling is an iWarp mechanism for DMAing data directly to or from the node's memory.
Reference: [5] <author> Claudson Bornstein and Peter Steenkiste. </author> <title> Data reshuffling in support of fast I/O for distributed-memory machines. </title> <booktitle> In Proceedings of the Third International Symposium on High-Performance Distributed Computing, </booktitle> <pages> pages 227-235, </pages> <address> San Fransisco, </address> <month> August </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: This architecture does not imply that each application has to provide the code to transfer data to and from the network interface. Rather, libraries can be built for common data distributions (e.g. <ref> [5] </ref>). The components interact through a data interface and a control interface: * The data interface transfers data between the the network interface and the application on the distributed memory system. <p> In this case buffer management and header creation is significantly simplified. 4.4 Performance Performance numbers for the basic operations have been reported elsewhere. Data can be moved between the iWarp compute nodes and the iWarp interface node at rates close to 145 MBytes/second. <ref> [19, 5] </ref>. The 11 network interface can send data over the HIPPI network at 90 MBytes/second for 64 KByte packets [19]. <p> We had to use two routes and stripe the data across two links to the network interface to achieve this. The critical features in the streams software are the support for irregular data sets and for application specific data distributions. 5.3 Block Cyclic Distributions An earlier paper <ref> [5] </ref> describes how the creation of large contiguous data blocks using reshuffling in the distributed memory systems can be used to increase the communication bandwidth. The paper focuses on general block-cyclic data distributions, i.e. data distributions used by parallelizing FORTRAN compilers [21]. <p> Each pair of columns then sends a 16 Kbyte SU to the network interface where the data is merged to form a 64 Kbyte TU (i.e. striping over four links). close to the hardware bandwidth <ref> [5] </ref>, even for small block sizes. An important feature is that the data movement was optimized for the specific data distribution. 15 5.4 Stereo-Vision In the stereo-vision application developed at CMU by Jon Webb, multi-baseline video images from four cameras are correlated to generate a depth image [23].
Reference: [6] <author> R. L. Clay. </author> <title> Scheduling in the Presence of Uncertainty: Probabilistic Solution of the Assignment Problem. </title> <type> PhD thesis, </type> <institution> Department of Chemical Engineering, Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <year> 1991. </year>
Reference-contexts: Specifically, the iWarp is used to generate a complex probability manifold representing a plant. The generated data is then sent to the C-90 at PSC for analysis, and finally, the C90 and CM2 solve the resulting linear assignment problem using a heterogeneous solver <ref> [6] </ref>. We collected data for several application runs, corresponding to input sizes of 1k, 2k and 4k; the samples generated by iWarp in these runs correspond to 256 MB, 1 GB and 4 GB of data.
Reference: [7] <author> R. L. Clay and G.J. McRae. </author> <title> Solution of large-scale modeling and optimization problems using heterogeneous supercomputing systems. </title> <booktitle> In SuParCup 1991 proceedings, Mannheim, </booktitle> <address> Germany, </address> <year> 1991. </year>
Reference-contexts: The rest of the nodes have 512 12 KByte of memory. 5.1 Chemical Process Optimization The Chemical Process Optimization application was implemented by Robert Clay and Greg McRae of the Chemical Engineering Department at CMU. The application optimizes a system, for example a chemical plant, using a stochastic model <ref> [7] </ref> of the system. The stochastic optimization problem can be transformed into a deterministic problem using the certainty equivalence transformation [9].
Reference: [8] <author> Robert Clay and Peter Steenkiste. </author> <title> Distributing a chemical process optimization application over a gigabit network. In Proceedings of Supercomputing '95, </title> <note> page To appear. ACM/IEEE, </note> <month> December </month> <year> 1995. </year>
Reference-contexts: The application optimizes a system, for example a chemical plant, using a stochastic model [7] of the system. The stochastic optimization problem can be transformed into a deterministic problem using the certainty equivalence transformation [9]. Our implementation of the stochastic optimization problem <ref> [8] </ref> was distributed across a heterogeneous system comprising of: the Intel iWarp at the CMU campus and the Cray C-90 and TM CM2 at the Pittsburgh Supercomputer Center (PSC). Specifically, the iWarp is used to generate a complex probability manifold representing a plant.
Reference: [9] <author> G.B. Dantzig. </author> <title> Planning under uncertainty using parallel computing. </title> <type> Technical Report SOL 87-1, </type> <institution> Department of Operations Research, Stanford University, </institution> <year> 1987. </year>
Reference-contexts: The application optimizes a system, for example a chemical plant, using a stochastic model [7] of the system. The stochastic optimization problem can be transformed into a deterministic problem using the certainty equivalence transformation <ref> [9] </ref>. Our implementation of the stochastic optimization problem [8] was distributed across a heterogeneous system comprising of: the Intel iWarp at the CMU campus and the Cray C-90 and TM CM2 at the Pittsburgh Supercomputer Center (PSC).
Reference: [10] <author> The MPI Forum. </author> <title> MPI: A message passing interface. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 878-883, </pages> <address> Oregon, </address> <month> November </month> <year> 1993. </year> <month> ACM/IEEE. </month>
Reference-contexts: Some of these techniques are similar to those used by other groups. For example, data reorganization has been proposed to achieve high bandwidth access to disks in distributed-memory systems [2, 14, 17] and the data reorganization is in effect a collective communication operations <ref> [10, 1] </ref>. Other systems also rely on the network interface to perform protocol processing. However, while we run a relatively standard TCP/IP implementation on the network interface, others use outboard protocol processing (e.g. [22]), or a customized protocol implementation relying heavily on hardware support (e.g. [18]).
Reference: [11] <author> Thomas Gross and Peter Steenkiste. </author> <title> Architecture implications of high-speed I/O for distributed-memory computers. </title> <booktitle> In Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 176-185, </pages> <address> Manchester, England, </address> <month> July </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: Data sent or received over the network is typically distributed over the private memories of the nodes. This means that the communication software has to perform scatter or gather operations as part of the I/O operation <ref> [11] </ref>. The task of combining/distributing the data parallelizes well, hence we map it onto the distributed-memory system. This architecture alleviates the load on the network interface which can then handle the messages more efficiently. Some of these techniques are similar to those used by other groups.
Reference: [12] <author> S. Hinrichs. </author> <title> Programmed Communcation Service Tool Chain User's Guide. </title> <institution> Carnegie Mellon University, </institution> <note> release 2.8 edition, </note> <year> 1991. </year> <title> Now part of Intel RTS 3.0. </title>
Reference-contexts: 5. 7 4 The iWarp Streams Software In this section we give a more detailed description of the features that where implemented for the iWarp streams software. 4.1 Data Interface The data interface between the distributed-memory system and the network interface is based on the PCS and ESPL communication libraries <ref> [12, 4] </ref>. PCS is used to create application-specific connections, and ESPL is a fast spooling library that achieves bandwidths close to the 40 MBytes/second link rate, even for 8 short messages. Spooling is an iWarp mechanism for DMAing data directly to or from the node's memory.
Reference: [13] <author> Intel Corporation. </author> <title> Paragon X/PS Product Overview, </title> <month> March </month> <year> 1991. </year>
Reference-contexts: Due to limited memory resources on the iWarp, this cannot be achieved on the production system. 6 Applicability to Other Systems We looked at how the streams architecture can be used to support network I/O through the HIPPI interface of the Paragon system <ref> [13] </ref>. The Paragon is a distributed-memory system, like iWarp, but there are some significant differences in the architecture.
Reference: [14] <author> D. Kotz. </author> <title> Multiprocessor file system interface. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 194-201. </pages> <address> ACM/IEEE, </address> <month> January </month> <year> 1993. </year> <month> 18 </month>
Reference-contexts: This architecture alleviates the load on the network interface which can then handle the messages more efficiently. Some of these techniques are similar to those used by other groups. For example, data reorganization has been proposed to achieve high bandwidth access to disks in distributed-memory systems <ref> [2, 14, 17] </ref> and the data reorganization is in effect a collective communication operations [10, 1]. Other systems also rely on the network interface to perform protocol processing.
Reference: [15] <author> Doug C. Noll, J. C. Cohen, C. H. Meyer, and W. Schneider. </author> <title> Spiral k-space mri of cortical activation. </title> <journal> Journal of Magnetic Resonance Imaging, </journal> <volume> 5 </volume> <pages> 49-56, </pages> <year> 1995. </year>
Reference-contexts: The current application represents the first step in this process: obtaining reconstructed, processed and rendered images based on Magnetic Resonance Imaging (MRI) data in a timely manner <ref> [15] </ref>. Our implementation is mapped on three architectures: the iWarp system at CMU performs pixel classification (brain/non-brain) and surface triangularization [16], the C-90 at PSC performs scalar processing, and the Intel Paragon at CMU performs the rendering.
Reference: [16] <author> Doug C. Noll, Jon A. Webb, and Tom E. Warfel. </author> <title> Parallel fourier inversion by the scan-line method. </title> <journal> IEEE Transactions on Medical Imaging, </journal> <note> in press, </note> <year> 1995. </year>
Reference-contexts: The current application represents the first step in this process: obtaining reconstructed, processed and rendered images based on Magnetic Resonance Imaging (MRI) data in a timely manner [15]. Our implementation is mapped on three architectures: the iWarp system at CMU performs pixel classification (brain/non-brain) and surface triangularization <ref> [16] </ref>, the C-90 at PSC performs scalar processing, and the Intel Paragon at CMU performs the rendering. The input to the iWarp component of the application consists of 52 MRI slices with pixel values representing the probability of being brain tissue.
Reference: [17] <author> K. E. Seamons and M. Winslett. </author> <title> Physical schemas for large multidimensional arrays in scientific computing applications. </title> <booktitle> In Seventh International Working Conference on Scientific and Statistical Database Management, </booktitle> <pages> pages 218-227. </pages> <publisher> IEEE, </publisher> <month> September </month> <year> 1994. </year>
Reference-contexts: This architecture alleviates the load on the network interface which can then handle the messages more efficiently. Some of these techniques are similar to those used by other groups. For example, data reorganization has been proposed to achieve high bandwidth access to disks in distributed-memory systems <ref> [2, 14, 17] </ref> and the data reorganization is in effect a collective communication operations [10, 1]. Other systems also rely on the network interface to perform protocol processing.
Reference: [18] <author> Raj K. Singh, Stephen G. Tell, Shaun J. Bharrat, David Becker, and Vernon L. Chi. </author> <title> A programmable HIPPI interface for a graphics supercomputer. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 124-132, </pages> <address> Oregon, </address> <month> November </month> <year> 1993. </year> <month> ACM/IEEE. </month>
Reference-contexts: Other systems also rely on the network interface to perform protocol processing. However, while we run a relatively standard TCP/IP implementation on the network interface, others use outboard protocol processing (e.g. [22]), or a customized protocol implementation relying heavily on hardware support (e.g. <ref> [18] </ref>). This paper focuses on a key element of our architecture that has not been addressed systematically elsewhere: the management of connections and of data-flow between the distributed-memory system and the network interface.
Reference: [19] <author> Peter Steenkiste, Michael Hemy, Todd Mummert, and Brian Zill. </author> <title> Architecture and evaluation of a high-speed networking subsystem for distributed-memory systems. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture. IEEE, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: The I/O architecture used for the HIPPI interface for the iWarp system [3], described in <ref> [19] </ref>, relies on a careful distribution of the network functions (e.g. protocol processing, data formatting and connection management) between the distributed-memory system and the network interface to achieve high-bandwidth network I/O. We achieved sustained throughputs of 55 MBytes/sec for simple demo applications and 40 MBytes/sec in large heterogeneous computing applications. <p> The sequential network interface tends to become a bottleneck; most of the performance problems can be attributed to the functions corresponding to the transport, network, session and presentation layers of the OSI network model (Figure 2). In the iWarp HIPPI interface <ref> [19] </ref>, these problems are addressed by mapping each task onto the subsystem that is the most appropriate for it (network interface or distributed memory system): 1. Communication protocol processing (transport and network layers) generally does not parallelize well. <p> In this case buffer management and header creation is significantly simplified. 4.4 Performance Performance numbers for the basic operations have been reported elsewhere. Data can be moved between the iWarp compute nodes and the iWarp interface node at rates close to 145 MBytes/second. <ref> [19, 5] </ref>. The 11 network interface can send data over the HIPPI network at 90 MBytes/second for 64 KByte packets [19]. <p> Data can be moved between the iWarp compute nodes and the iWarp interface node at rates close to 145 MBytes/second. [19, 5]. The 11 network interface can send data over the HIPPI network at 90 MBytes/second for 64 KByte packets <ref> [19] </ref>. Actual network I/O performance for applications is reported in the next section. 5 Applications In this section we describe how four very different applications use the iWarp HIPPI interface to communicate. We discuss both the communication performance and how the applications use the streams software.
Reference: [20] <author> Peter A. Steenkiste, Brian D. Zill, H.T. Kung, Steven J. Schlick, Jim Hughes, Bob Kowalski, and John Mullaney. </author> <title> A host interface architecture for high-speed networks. </title> <booktitle> In Proceedings of the 4th IFIP Conference on High Performance Networks, </booktitle> <pages> pages A3 1-16, </pages> <address> Liege, Belgium, </address> <month> December </month> <year> 1992. </year> <title> IFIP, </title> <publisher> Elsevier. </publisher>
Reference-contexts: Communication protocol processing (transport and network layers) generally does not parallelize well. We map this task to the network interface, and we provide hardware support for time-critical 2 tasks such as data checksumming and buffering <ref> [20] </ref>. 2. Managing connections between the nodes in the distributed-memory system and the outside world through the network interface translates into a problem of allocating resources in the system (e.g. link bandwidth, buffer space, ..). We map this task to the streams software, as described in Section 3. 3.
Reference: [21] <author> Jaspal Subhlok, Jim Stichnoth, Dave O'Hallaron, and Thomas Gross. </author> <title> Programming task and data parallelism on a multicomputer. </title> <booktitle> In Proc. of the ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 13-22, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The paper focuses on general block-cyclic data distributions, i.e. data distributions used by parallelizing FORTRAN compilers <ref> [21] </ref>. Since block-cyclic distributions are very common, the communication between the distributed memory 14 system and the network interface (including data reshuffling) was implemented as a library that can be used by applications.
Reference: [22] <author> Richard Thompson. </author> <title> Los alamos multiple crossbar network crossbar interfaces. In Workshop on the Architecture and Implementation of High Performance Communication Subsystems. </title> <publisher> IEEE, </publisher> <month> February </month> <year> 1992. </year>
Reference-contexts: Other systems also rely on the network interface to perform protocol processing. However, while we run a relatively standard TCP/IP implementation on the network interface, others use outboard protocol processing (e.g. <ref> [22] </ref>), or a customized protocol implementation relying heavily on hardware support (e.g. [18]). This paper focuses on a key element of our architecture that has not been addressed systematically elsewhere: the management of connections and of data-flow between the distributed-memory system and the network interface.
Reference: [23] <author> Jon Webb. </author> <title> Latency and bandwidth considerations in parallel robotics image processing. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 230-239, </pages> <address> Oregon, </address> <month> November </month> <year> 1993. </year> <note> ACM/IEEE. 19 </note>
Reference-contexts: An important feature is that the data movement was optimized for the specific data distribution. 15 5.4 Stereo-Vision In the stereo-vision application developed at CMU by Jon Webb, multi-baseline video images from four cameras are correlated to generate a depth image <ref> [23] </ref>. As part of that application, a digital VCR was implemented to display the four images on a framebuffer.
References-found: 23

