URL: http://kmi.open.ac.uk/techreports/papers/kmi-tr-41.ps.gz
Refering-URL: http://kmi.open.ac.uk/kmi-abstracts/kmi-tr-41-abstract.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Efficient Parameter Learning in Bayesian Networks from Incomplete Databases  
Author: Marco Ramoni Paola Sebastiani 
Date: January 1997  
Affiliation: Knowledge Media Institute  
Pubnum: KMI-TR-41  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. L. Buntine. </author> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 159-225, </pages> <year> 1994. </year>
Reference-contexts: Although in their original concept bbns were mainly designed to encode the knowledge of human experts, their statistical roots soon prompted for the use of learning methods to extract them directly from databases of cases rather than from the insight of human domain experts <ref> [2, 1, 6] </ref>. A common assumption made by the current learning methods is that the database at hand is complete.
Reference: [2] <author> G.F. Cooper and E. Herskovitz. </author> <title> A bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: Although in their original concept bbns were mainly designed to encode the knowledge of human experts, their statistical roots soon prompted for the use of learning methods to extract them directly from databases of cases rather than from the insight of human domain experts <ref> [2, 1, 6] </ref>. A common assumption made by the current learning methods is that the database at hand is complete. <p> A common assumption made by the current learning methods is that the database at hand is complete. The reason behind this assumption is that, when facing missing data, exact Bayesian learning becomes intractable, namely its complexity is exponential in the number of missing data in the database <ref> [2] </ref>. Unfortunately, real-world databases are rarely complete: unreported, lost, and corrupted data are one of their distinguished features. Researchers in both Artificial Intelligence and Statistics developed different methods to learn conditional probabilities from databases with missing data.
Reference: [3] <author> R G Cowell, A.P. Dawid, and P. Sebastiani. </author> <title> A comparison of sequential learning methods for incomplete data. </title> <booktitle> In Bayesian Statistics 5, </booktitle> <pages> pages 533-542. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1996. </year> <title> Efficient Parameter Learning in Bayesian Networks from Incomplete Data </title>
Reference-contexts: Exact analysis would require the computation of the joint posterior distribution of the parameters given each possible completion of the database, and then mix these over all possible completions. This is apparently infeasible and approximate methods therefore are required. Deterministic methods as sequential updating <ref> [7, 3] </ref> or the em algorithm [4] provide a possible solution. However the need to process data one by one of the former, and the slow rate of convergence of the latter can impair their applicability to large databases.
Reference: [4] <author> A. Dempster, D. Laird, and Rubin D. </author> <title> Maximum likelihood from incomplete data via the em algorithm. </title> <journal> J. Roy. Statist. Soc. B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: This is apparently infeasible and approximate methods therefore are required. Deterministic methods as sequential updating [7, 3] or the em algorithm <ref> [4] </ref> provide a possible solution. However the need to process data one by one of the former, and the slow rate of convergence of the latter can impair their applicability to large databases.
Reference: [5] <author> D. Heckerman. </author> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, Microsoft Corporation, </institution> <year> 1995. </year>
Reference-contexts: Researchers in both Artificial Intelligence and Statistics developed different methods to learn conditional probabilities from databases with missing data. Best known techniques are currently Sequential updating, the em algorithm, and the Gibbs Sampling. For an updated review of these methods, see <ref> [5] </ref>. These solutions share a common strategy: they try to complete the database by inferring the missing data from the available information and then learn from the completed database.
Reference: [6] <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning bayesian networks: The combinations of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: Although in their original concept bbns were mainly designed to encode the knowledge of human experts, their statistical roots soon prompted for the use of learning methods to extract them directly from databases of cases rather than from the insight of human domain experts <ref> [2, 1, 6] </ref>. A common assumption made by the current learning methods is that the database at hand is complete.
Reference: [7] <author> D.J. Spiegelhalter and S.L. Lauritzen. </author> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 157-224, </pages> <year> 1990. </year>
Reference-contexts: Exact analysis would require the computation of the joint posterior distribution of the parameters given each possible completion of the database, and then mix these over all possible completions. This is apparently infeasible and approximate methods therefore are required. Deterministic methods as sequential updating <ref> [7, 3] </ref> or the em algorithm [4] provide a possible solution. However the need to process data one by one of the former, and the slow rate of convergence of the latter can impair their applicability to large databases.
Reference: [8] <author> A Thomas, D J Spiegelhalter, and W R Gilks. </author> <title> Bugs: A program to perform bayesian inference using gibbs sampling. </title> <booktitle> In Bayesian Statistics 4, </booktitle> <pages> pages 837-42. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1992. </year> <month> 11 </month>
Reference-contexts: The algorithm is iterated to reach stability, and then a sample from the joint posterior distribution is taken which can be used to provide empirical estimates of the posterior means <ref> [8] </ref>. Convergence rate and convergence detection are open issues of this stochastic method. These methods share a common strategy: they complete the database by inferring somehow the missing data from the available information and then learn from the completed database. <p> these experiments were to compare the accuracy of the parameter estimates provided by the Gibbs Sampling and our method as the available information in the database decreases. 4.1 Materials and Methods We compared an implementation of our method to the implementation of the Gibbs Sampling provided by the program BUGS <ref> [8] </ref>. In the following experiments, we used the implementation of BUGS version 0.5 running on a Sun Sparc 5 under SunOS 5.5 and an implementation of our method written in Common Lisp running on the same platform under CLISP version 1996/10/10.
References-found: 8

