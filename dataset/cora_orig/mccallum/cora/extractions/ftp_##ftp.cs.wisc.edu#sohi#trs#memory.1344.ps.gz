URL: ftp://ftp.cs.wisc.edu/sohi/trs/memory.1344.ps.gz
Refering-URL: http://www.cs.wisc.edu/mscalar/publications.html
Root-URL: 
Title: Data Memory Alternatives for Multiscalar Processors  
Author: Scott E. Breach, T. N. Vijaykumar, Sridhar Gopal, James E. Smith, Gurindar S. Sohi 
Address: 1210 West Dayton Street Madison, Wisconsin 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: This work considers data memory alternatives for multiscalar processors that can support the aggressive control and data speculative execution of loads and stores. We discuss the key issues that must be dealt with for such a data memory design and partition the design space of alternatives on the basis of composition, i.e. whether the storage for speculative and architectural versions is separate or aggregate, and on the basis of organization, i.e. whether the storage for speculative and architectural versions is shared or private. Moreover, we attempt to address a broad spectrum of solutions by considering two schemes in terms of centralized and distributed designs: a known scheme, the address resolution buffer which provides distinct speculative and architectural storage; and a novel scheme, the time-sequence cache which merges the speculative and architectural storage. We have performed a preliminary experimental evaluation of designs from opposite ends of the spectrum of solutions. Our experimental evidence from a simulation of a multiscalar processor with a centralized address resolution buffer and a distributed time-sequence cache shows (i) that hit latency is an important performance factor (even for a latency tolerant processor like a multiscalar processor) and (ii) that distributed schemes may trade-off hit rate for hit latency to improve performance over centralized schemes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Implementing sequential consistency in cache-based systems. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages 47-50, </pages> <year> 1990. </year>
Reference-contexts: This loss of order could lead to violations of sequential consistency. To avoid such violations, each load or store must be globally committed <ref> [1] </ref> in program order to provide a total order for memory accesses made by a parallel program.
Reference: [2] <author> Scott E. Breach, T. N. Vijaykumar, and Gurindar S. Sohi. </author> <title> The anatomy of the register file in a multiscalar processor. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 181-190, </pages> <year> 1994. </year>
Reference-contexts: A PU executes instructions on its own collection of pipelined functional units (2 simple integer FU, 1 complex integer FU, 1 floating point FU, 1 branch FU, and 1 memory FU) according to its class. The unidirectional point-to-point ring connecting the register files <ref> [2] </ref> of the PUs imposes a 1 cycle communication latency between units and matches the ring width to the issue width of the PU. Each PU has its own instruction cache with 32k of 2-way set-associative storage in 44 byte blocks.
Reference: [3] <author> Manoj Franklin and Gurindar S. Sohi. ARB: </author> <title> A hardware mechanism for dynamic reordering of memory references. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 45(5) </volume> <pages> 552-571, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: In this paper, we consider data memory alternatives that can support the aggressive execution model of a mul-tiscalar processor. We consider both a known scheme, the address resolution buffer <ref> [3] </ref> which provides distinct speculative and architectural storage, and a novel scheme, the time-sequence cache which merges speculative and architectural storage. Moreover, we consider both centralized and distributed designs for each scheme. <p> We consider a known scheme, the address resolution buffer <ref> [3] </ref>, and a novel scheme, the time-sequence cache. Moreover, we consider centralized and distributed designs for each. 5 5.1 Address Resolution Buffer In terms of the axes of the design space, an address resolution buffer (ARB) provides a data memory that is composed of separate speculative and architectural storage.
Reference: [4] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Two techniques to enhance the performance of memory consistency models. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 255-364, </pages> <year> 1991. </year>
Reference-contexts: Moreover, a store cannot be performed to data memory until it is ready to be globally committed. We employ the strategy used in the R10000 <ref> [13, 4] </ref> to allow the speculative execution of loads and detect a violation of sequential consistency, in which case all instructions from the offending load instruction must be squashed 1 .
Reference: [5] <author> James R. Goodman. </author> <title> Using cache memory to reduce processor-memory traffic. </title> <booktitle> In Proceedings of the 10th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 124-131, </pages> <year> 1983. </year> <month> 21 </month>
Reference-contexts: individual PU. (We use the term ARB to refer in general to both designs, and CARB or DARB to refer in particular to one design.) The ARB uses state bits to track the state of different versions of a word (in a manner similar to hardware cache coherence for multiprocessors <ref> [5] </ref>). The CARB uses centralized version control logic to trigger finite state machine transitions.
Reference: [6] <author> Quinn Jacobson, Steve Bennett, Nikhil Sharma, and James E. Smith. </author> <title> Control flow speculation in multiscalar processors. </title> <booktitle> In To Appear in Proceedings of the Third International Symposium on High-Performance Computer Architecture, </booktitle> <year> 1997. </year>
Reference-contexts: The global sequencer maintains a 1024 entry 2-way set associative cache of task descriptors. The control flow predictor of the global sequencer uses a dynamic path based scheme which selects from up to 4 task targets per prediction and keeps 7 path histories XOR-folded into a 15-bit path register <ref> [6] </ref>. The predictor storage consists of both a task target table and a task address table, each with 32k entries indexed by the path register. Each target table entry is a 2-bit counter and a 2-bit target. Each address table entry is a 2-bit counter and a 32-bit address.
Reference: [7] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <year> 1981. </year>
Reference-contexts: Benchmark 4x8KB 4x16KB compress 0.315 0.310 espresso 0.183 0.170 gcc 0.238 0.229 xlisp 0.303 0.295 Table 2: Snooping Bus Utilization for DTSC with 4 MSHRs <ref> [7] </ref> per bank. Disambiguation is performed at the byte-level. An access has a hit time of 1, 2, 3, or 4 cycles, with an additional penalty of 10 cycles for a miss supplied by the next level of the data memory (plus any bus contention).
Reference: [8] <author> D. Lilja, D. Marcovitz, and P.-C. Yew. </author> <title> Memory reference behavior and cache performance in a shared memory multiprocessor. </title> <type> Technical Report 836, </type> <institution> CSRD, University of Illinois, Urbana-Champaign, </institution> <month> December </month> <year> 1988. </year>
Reference-contexts: Though, for xlisp, both schemes perform nearly as well, the trend is reversed for espresso. The increase in the miss rates may be attributed to two factors. First, the distribution of storage in the DTSC causes reference spreading <ref> [8] </ref> across multiple TSCs leading to an increase in misses. Though multiple accesses in close proximity go to only one ARB and/or data cache in the CARB, these accesses may be spread across different TSCs of the DTSC destroying the locality of the accesses.
Reference: [9] <author> David A. Patterson and John L. Hennessy. </author> <title> Computer Architecture A Quantitative Approach, </title> <booktitle> chapter 8, </booktitle> <pages> pages 635-755. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: The private approach poses the well-known and complex problem of how to maintain coherence, in a manner that is transparent to a running program, among multiple copies of storage. For a small number of PUs, coherence protocols based on a snooping bus <ref> [9] </ref> have been well studied.
Reference: [10] <author> James E. Smith and Andrew R. Pleszkun. </author> <title> Implementation of precise interrupts in pipelined processors. </title> <booktitle> In Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 36-44, </pages> <month> June 17-19, </month> <year> 1985. </year>
Reference-contexts: in a multiprocessor system for a data memory design in general and briefly describe how these aspects can be provided for the memory designs we have considered, the ARB and the TSC. 7.1 Precise Interrupts We classify interrupts into two classes: program interrupts (i.e., traps or exceptions) and external interrupts <ref> [10] </ref>. In a multiscalar processor, an exception may be received by speculative as well as non-speculative tasks. An exception received by the non-speculative task at the head can be made precise by dealing with it in the same fashion as a superscalar processor.
Reference: [11] <author> Gurindar S. Sohi. </author> <title> Instruction issue logic for high performance, interruptable, multiple functional unit, pipelined computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(3) </volume> <pages> 349-359, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: all of the program code, except system calls, on a cycle-by-cycle basis. (The system calls are handled by trapping to the operating system of the simulation host.) 6.2 Configurations The multiscalar processor used in the experiments is a 4 PU configuration in which each PU is based on the RUU <ref> [11] </ref> and has been configured with 2-way out-of-order issue characteristics. A PU executes instructions on its own collection of pipelined functional units (2 simple integer FU, 1 complex integer FU, 1 floating point FU, 1 branch FU, and 1 memory FU) according to its class.
Reference: [12] <author> Gurindar S. Sohi, Scott E. Breach, and T. N. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: Correct execution causes tasks to be committed; whereas incorrect speculation causes tasks to be squashed. To maintain sequential semantics, tasks are committed in the original program order and squashed from the task at point of incorrect speculation onwards. More details of this entire process may be found in <ref> [12] </ref>. To facilitate an understanding of the memory requirements of multiscalar execution, consider Figure 1. In the figure, memory locations are shown along the Y-axis (memory locations A and B are singled out). Logical time, corresponding to sequential program execution order, is along the X-axis.
Reference: [13] <author> K.C. Yeager. </author> <title> MIPS R10000 superscalar microprocessor. </title> <booktitle> Micro, </booktitle> <month> April </month> <year> 1996. </year> <month> 22 </month>
Reference-contexts: Moreover, a store cannot be performed to data memory until it is ready to be globally committed. We employ the strategy used in the R10000 <ref> [13, 4] </ref> to allow the speculative execution of loads and detect a violation of sequential consistency, in which case all instructions from the offending load instruction must be squashed 1 .
References-found: 13

