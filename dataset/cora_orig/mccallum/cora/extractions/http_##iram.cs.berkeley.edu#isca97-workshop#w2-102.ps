URL: http://iram.cs.berkeley.edu/isca97-workshop/w2-102.ps
Refering-URL: http://iram.cs.berkeley.edu/isca97-workshop/
Root-URL: 
Email: jouppi@pa.dec.com  parthas@ece.rice.edu  
Title: The Relative Importance of Memory Latency, Bandwidth, and Branch Limits to Performance  
Author: Norman P. Jouppi Parthasarathy Ranganathan 
Address: 250 University Avenue Palo Alto, California 94301  Houston, Texas 77005  
Affiliation: Digital Equipment Corporation Western Research Lab  Dept. of Electrical and Computer Engineering Rice University  
Abstract: This study investigates the relative importance of memory latency, memory bandwidth, and branch predictability in determining limits to processor performance. We use an aggressive simulation model with few other limits to study the performance of SPEC92 benchmarks. Our basic machine model assumes a dynamically scheduled processor with a 16536 entry instruction window. Up to 16536 instructions of any type can be issued each cycle, subject to data dependencies. In systems with unlimited memory bandwidth and perfect branch predictability, we find that memory latency is not a significant limit to performance until it exceeds 100 to 200 cycles. Memory bandwidth is not usually a significant limit either. In systems with memory latency of 16 cycles and perfect branch predictability, many applications require less than 6 bytes per cycle, while all but one perform well if 100 bytes per cycle are available. Based on current trends in the semiconductor industry and current research in packaging and interface technology, we expect these latency requirements and bandwidth requirements to be achievable in future processors. By far, the biggest limit to performance was branch unpredictability. The use of one of the best existing branch predictors provided with very large table sizes resulted in performance three to five times less than that achievable with perfect branch prediction for many benchmarks. Additionally, many benchmarks had significantly lower performance than the perfect case even with a futuristic branch predictor (with a mix of 75% perfect and 25% existing) was simulated. These results suggest that improved branch prediction techniques are crucial for improving future uniprocessor performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A System for Building Customized Program Analysis Tools. </title> <booktitle> Proceedings of the ACM SIGPLAN `94 Conference on Programming Languages, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: We use trace-driven simulation for this study. Traces are collected at run-time using a flexible code-instrumentation interface provided by ATOM <ref> [1] </ref>. ATOM instruments the executable code to call the simulator before every instruction (we do not instrument special instructions like barriers, etc.). The instrumentation code converts the instructions into a dynamic flow graph which is then used to simulate the processor, memory, and branch prediction models.
Reference: [2] <author> David W. Wall. </author> <title> Limits of instruction-level parallelism. </title> <note> In WRL Research Report 93/6, </note> <month> November </month> <year> 1993. </year>
Reference-contexts: Some work assumed that all branches were perfectly predictable, which yielded very large bounds on available instruction-level parallelism [13]. Other work considered only parallelism within basic blocks, which resulted in small estimates of instruction level parallelism. More recent work has assumed some type of practical branch prediction <ref> [2, 23, 3] </ref>. As commercial processors issue ever greater numbers of instructions per cycle, the control flow dependence problem is likely to impose a greater limit on processor performance. In terms of recent nomenclature, this is the "Branch Wall" problem. <p> This model does not account for the clustering of misses in an application, which as our results show, has significant impact on the performance variation with latency. Finally, Wall's study on limits to instruction-level parallelism <ref> [2] </ref> is by far the most detailed study on the subject and studies the impact of various parameters like register renaming, alias analysis, branch and jump prediction, window size, cycle width, and functional unit latency on 18 benchmarks. <p> The instrumentation code converts the instructions into a dynamic flow graph which is then used to simulate the processor, memory, and branch prediction models. The simulation methodology is similar to that used in many previous studies <ref> [2, 3] </ref>. Sections 3.1, 3.2, and 3.3 describe the processor, memory, and branch prediction models used in this paper and Section 3.4 describes the applications used. 3.1 Processor Model The processor model implements an aggressive superscalar architecture based on the Alpha instruction-set architecture. <p> For example, loop interchange can result in instructions being moved past thousands of other instructions. We assume single cycle functional unit latency for all non-memory operations. Additionally, we assume that all memory references are checked at run-time for aliases (similar to the perfect alias analysis assumed in <ref> [2] </ref>). <p> Other currently proposed techniques to improve the performance of branches (e.g. guarding [33], fanout-based prediction <ref> [2] </ref>) are not expected to 3 Application Notes Input Load % Branch % Integer Applications Compress Reduction of size of file using Lempel-Ziv coding ref 26.2 13.34 Espresso Minimization of a boolean function bca 22.5 17.81 Eqntott Translation of a boolean equation to truth table ref 12.9 10.91 Floating point applications <p> The futuristic branch predictor can therefore be used as a reasonable approximation of the limits of branch prediction. The prediction strategies described above apply for conditional branches. We assume that indirect jumps (procedure returns) are predicted correctly using Return Address Registers (RARs) as in current processors. (Previous studies <ref> [2] </ref> have shown that we can get almost perfect jump prediction with a small amount of hardware.) If a simulated branch predictor mispredicts a branch, the simulator does not issue any instructions from after the branch until the branch condition is computed.
Reference: [3] <author> M Butler, T.-Y. Yeh, Y. Patt, M. Alsup, H. Scales, and M. Shebanow. </author> <title> Single instruction parallelism is greater than two. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276-286, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Some work assumed that all branches were perfectly predictable, which yielded very large bounds on available instruction-level parallelism [13]. Other work considered only parallelism within basic blocks, which resulted in small estimates of instruction level parallelism. More recent work has assumed some type of practical branch prediction <ref> [2, 23, 3] </ref>. As commercial processors issue ever greater numbers of instructions per cycle, the control flow dependence problem is likely to impose a greater limit on processor performance. In terms of recent nomenclature, this is the "Branch Wall" problem. <p> The instrumentation code converts the instructions into a dynamic flow graph which is then used to simulate the processor, memory, and branch prediction models. The simulation methodology is similar to that used in many previous studies <ref> [2, 3] </ref>. Sections 3.1, 3.2, and 3.3 describe the processor, memory, and branch prediction models used in this paper and Section 3.4 describes the applications used. 3.1 Processor Model The processor model implements an aggressive superscalar architecture based on the Alpha instruction-set architecture.
Reference: [4] <author> Kenneth Yeager, et. al. </author> <title> R10000 Superscalar Microprocessor. </title> <booktitle> In the Proceedings of Hot Chips VII, </booktitle> <month> August </month> <year> 1995. </year> <note> See also http://www.mips.com/HTMLs/T5 B.html. </note>
Reference-contexts: The processor supports non-blocking loads and stores, speculative execution, out-of-order execution, and register renaming. Such features already exist in current processor designs <ref> [4, 5, 6] </ref>. In addition, we assume an instruction window with 16536 entries and an issue rate restricted only by the window size. This window size was chosen because it was two orders of magnitude larger than the typical IPC in our limits simulations.
Reference: [5] <author> Intel Corporation. </author> <title> Pentium (r) Pro Family Developer's Manual. </title>
Reference-contexts: High-end processors have had access to pipelined memory systems since the 1960's [15, 16], and more recently even PC's have had the benefit of pipelined memory systems <ref> [5] </ref>. The assumption of non-pipelined memory systems puts a profoundly small and unrealistic limit on possible processor performance. In a non-pipelined memory system, the bandwidth is roughly equal to the bus width times the inverse of the latency. In pipelined memory systems, memory latency and bandwidth are independent variables. <p> The processor supports non-blocking loads and stores, speculative execution, out-of-order execution, and register renaming. Such features already exist in current processor designs <ref> [4, 5, 6] </ref>. In addition, we assume an instruction window with 16536 entries and an issue rate restricted only by the window size. This window size was chosen because it was two orders of magnitude larger than the typical IPC in our limits simulations.
Reference: [6] <author> Linley Gwennap. </author> <title> PA-8000 Combines Complexity and Speed. </title> <type> Microprocessor Report, 8(15) </type> <pages> 1-9, </pages> <month> November 14 </month> <year> 1994. </year>
Reference-contexts: The processor supports non-blocking loads and stores, speculative execution, out-of-order execution, and register renaming. Such features already exist in current processor designs <ref> [4, 5, 6] </ref>. In addition, we assume an instruction window with 16536 entries and an issue rate restricted only by the window size. This window size was chosen because it was two orders of magnitude larger than the typical IPC in our limits simulations.
Reference: [7] <author> Wm. A. Wulf and Sally A. McKee. </author> <title> Hitting the memory wall: Implications of the obvious. </title> <journal> In ACM Computer Architecture News. </journal> <volume> Vol. 23, No. 4, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: In this paper we compare the relative importance of the first three possible limits to performance. Possible limits to processor performance imposed by memory systems (the "Memory Wall") have recently received widespread attention. Unfortunately the original "Memory Wall" study <ref> [7] </ref> was based on very simplistic models, in particular a non-pipelined memory system with only one memory operation in progress at a time. <p> For our experiments, we look at three kinds of memory systems perfect, non-pipelined and pipelined. The perfect memory system is ideal in all respects. It has no misses and suffers from no bandwidth restrictions. The non-pipelined memory system is similar to the memory system assumed by <ref> [7] </ref>. It can service only one request at a time and the bandwidth is limited by the fraction of the bus width to the latency. Multiple requests cannot be overlapped with one another. The third kind of memory system which we simulate is a pipelined memory system. <p> The three curves in the figure represent the three memory systems considered in our study. The perfect memory system assumes an ideal memory system with all cache accesses being serviced in the first-level cache. The non-pipelined memory system models the memory system assumed in <ref> [7] </ref>. In this memory system, the bandwidth is the reciprocal of the latency times the cache line size, which implies that only one request can be serviced at a time. Subsequent requests will have to wait for all previous requests to be processed before they are serviced.
Reference: [8] <author> Eric E. Johnson. </author> <booktitle> Graffiti in "the memory wall". In ACM Computer Architecture News, </booktitle> <year> 1995. </year>
Reference: [9] <author> Po-Yung Chang, Eric Hao and Yale N. Patt. </author> <title> Alternative Implementations of Hybrid Branch Predictors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: Store bandwidth is usually about half the load bandwidth and can be provided by making the output bus somewhere between the same width or half the width of the input bus. 3.3 Branch Prediction Strategies We model various branch prediction strategies described previously in the literature <ref> [10, 9, 11, 12] </ref>. Figure 1 summarizes the branch prediction strategies used. The seven branch prediction strategies used in this study are bimodal, gshare, bimodal/gshare, pa/gshare, PA/Gshare, futuristic, and perfect. <p> Hybrid branch predictors use a two-bit saturating counter to choose between two branch prediction strategies and use the branch predictor that is more likely to be correct. All our hybrid branch predictors use a two level branch predictor selection algorithm similar to the one used in <ref> [9] </ref>. We simulated three hybrid predictors. The first (bimodal/gshare) uses a table of 4K two-bit counters to select between the bimodal and gshare predictors described above.
Reference: [10] <author> Scott McFarling. </author> <title> Combining branch predictors. </title> <note> In WRL Technical Note TN-36, 1993. 9 0 1 2 3 4 5 6 7 Latency (cycles) perf 1 3 10 32 100 320 1000 Bandwidth (bytes per cycle) 6 19 64 192 640 2048 perf - Branch prediction Perf Futr PA/G pa/g 2c/g gsh 2c </note> - 
Reference-contexts: Store bandwidth is usually about half the load bandwidth and can be provided by making the output bus somewhere between the same width or half the width of the input bus. 3.3 Branch Prediction Strategies We model various branch prediction strategies described previously in the literature <ref> [10, 9, 11, 12] </ref>. Figure 1 summarizes the branch prediction strategies used. The seven branch prediction strategies used in this study are bimodal, gshare, bimodal/gshare, pa/gshare, PA/Gshare, futuristic, and perfect.
Reference: [11] <author> Marius Evers, Po-Yung Chang, and Yale N. Patt. </author> <title> Using Hybrid Branch Predictors to Improve Branch Prediction Accuracy in the Presence of Context Switches. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 3-11, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Store bandwidth is usually about half the load bandwidth and can be provided by making the output bus somewhere between the same width or half the width of the input bus. 3.3 Branch Prediction Strategies We model various branch prediction strategies described previously in the literature <ref> [10, 9, 11, 12] </ref>. Figure 1 summarizes the branch prediction strategies used. The seven branch prediction strategies used in this study are bimodal, gshare, bimodal/gshare, pa/gshare, PA/Gshare, futuristic, and perfect. <p> Many benchmarks even had significantly lower performance than the perfect case when an aggressive futuristic branch predictor (a mix of 75% perfect and 25% existing branch predictor) was simulated. Other recent work on branch predictor performance for operating systems <ref> [27, 28, 11] </ref> and other work-loads suggests that branch prediction accuracies are likely to further decrease in such cases; our results on the importance of the branch wall therefore tend to err on the optimistic side (by underestimating the importance of the branch wall).
Reference: [12] <author> T.-Y.Yeh and Y.N.Patt. </author> <title> Alternative Implementations of Two-Level Adaptive Branch Prediction. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Store bandwidth is usually about half the load bandwidth and can be provided by making the output bus somewhere between the same width or half the width of the input bus. 3.3 Branch Prediction Strategies We model various branch prediction strategies described previously in the literature <ref> [10, 9, 11, 12] </ref>. Figure 1 summarizes the branch prediction strategies used. The seven branch prediction strategies used in this study are bimodal, gshare, bimodal/gshare, pa/gshare, PA/Gshare, futuristic, and perfect.
Reference: [13] <author> Todd Austin and Guri Sohi. </author> <title> Dynamic Dependency Analysis of Ordinary Programs. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Over the last three decades, work on instruction-level parallelism has made varying assumptions about control-flow hazards. Some work assumed that all branches were perfectly predictable, which yielded very large bounds on available instruction-level parallelism <ref> [13] </ref>. Other work considered only parallelism within basic blocks, which resulted in small estimates of instruction level parallelism. More recent work has assumed some type of practical branch prediction [2, 23, 3].
Reference: [14] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> second edition, </address> <year> 1992. </year>
Reference: [15] <author> J. E. Thornton. </author> <title> Design of a Computer The Control Data 6600. Scott, </title> <address> Foresman, </address> <year> 1970. </year>
Reference-contexts: Unfortunately the original "Memory Wall" study [7] was based on very simplistic models, in particular a non-pipelined memory system with only one memory operation in progress at a time. High-end processors have had access to pipelined memory systems since the 1960's <ref> [15, 16] </ref>, and more recently even PC's have had the benefit of pipelined memory systems [5]. The assumption of non-pipelined memory systems puts a profoundly small and unrealistic limit on possible processor performance.
Reference: [16] <author> L. J. Boland, G. D. Granito, A. U. Marcotte, B. U. Messina, and J. W.Smith. </author> <title> The IBM System/360 Model 91: Storage System. </title> <journal> In IBM Journal of Research and Development, </journal> <pages> pages 54-68, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: Unfortunately the original "Memory Wall" study [7] was based on very simplistic models, in particular a non-pipelined memory system with only one memory operation in progress at a time. High-end processors have had access to pipelined memory systems since the 1960's <ref> [15, 16] </ref>, and more recently even PC's have had the benefit of pipelined memory systems [5]. The assumption of non-pipelined memory systems puts a profoundly small and unrealistic limit on possible processor performance.
Reference: [17] <author> Keith I. Farkas and Norman P. Jouppi. </author> <title> Complexity/performance tTadeoffs with Non-blocking Loads. </title> <booktitle> In Proc. of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 211-222, </pages> <month> June </month> <year> 1994. </year>
Reference: [18] <author> A. Saulsbury and F. Pong and A. Nowatzk. </author> <title> Missing the Memory Wall: The Case for Processor Memory Integration. </title> <booktitle> In Proc. of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 90-101, </pages> <month> May </month> <year> 1996. </year>
Reference: [19] <author> Doug Burger, James Goodman, and Alain Kagi. </author> <title> Memory Bandwidth Limitations fo Future Microprocessors. </title> <booktitle> In Proc. of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 78-89, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: This has resulted in a compound improvement in microprocessor performance of about 100X per decade. Interconnects. The number of pins on a microprocessor has been increasing relatively slowly, at 16% per year <ref> [19] </ref>. Moreover, most PC's only have busses that run at a fraction of the CPU clock speed (e.g., 1/4, typically 50-66Mhz). <p> Burger, Goodman, and Kagi <ref> [19] </ref> show that techniques to reduce latency increase the bandwidth requirements of the system. They then present an excellent comparison of many different techniques for reducing memory bandwidth requirements.
Reference: [20] <author> H. M. Rein and M. Moller. </author> <title> Design Considerations for Very-High-Speed Si-Bipolar IC's Operating up to 50Gb/s. </title> <journal> In the IEEE Journal of Solid-State Circuits, </journal> <pages> pages 1076-1090, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: By using fully differential signaling, as is normally used in communications applications, even higher signaling rates can be achieved. The current highest speeds in communications applications approach 50GHz <ref> [20] </ref>. Recently, work on 4GHz differential signaling in 0.5um CMOS has begun [21]. Communication speeds in CMOS should scale with the processor clock frequency, assuming the package electrical characteristics are sufficient. Pins on microprocessor pin-grid array packages have typically presented very large impediments to very high-speed transmission line signaling.
Reference: [21] <author> William J. Dally and John Poulton. </author> <title> Transmitter Equalization for 4Gb/s Signalling. </title> <booktitle> In the proceedings of Hot Interconnects IV, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: By using fully differential signaling, as is normally used in communications applications, even higher signaling rates can be achieved. The current highest speeds in communications applications approach 50GHz [20]. Recently, work on 4GHz differential signaling in 0.5um CMOS has begun <ref> [21] </ref>. Communication speeds in CMOS should scale with the processor clock frequency, assuming the package electrical characteristics are sufficient. Pins on microprocessor pin-grid array packages have typically presented very large impediments to very high-speed transmission line signaling.
Reference: [22] <author> Hiroyuki Sakai, et. al. </author> <booktitle> A Millimeter-Wave Flip-Chip IC using Micro-Bump Bonding Technology In the proceedings of the 10 International Solid-State Circuits Conference, </booktitle> <pages> pages 408--409, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: However, ball-grid arrays with very small impedance discontinuities offer the potential for very large numbers of very-high quality interconnects. Small bumps have been used in microwave circuits for connections up to 70GHz <ref> [22] </ref>.
Reference: [23] <author> Kevin Theobald, Guang Gao, and Laurie Hendren. </author> <title> On the Limits of Program Parallelism and its Smoothability. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 10-19, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Some work assumed that all branches were perfectly predictable, which yielded very large bounds on available instruction-level parallelism [13]. Other work considered only parallelism within basic blocks, which resulted in small estimates of instruction level parallelism. More recent work has assumed some type of practical branch prediction <ref> [2, 23, 3] </ref>. As commercial processors issue ever greater numbers of instructions per cycle, the control flow dependence problem is likely to impose a greater limit on processor performance. In terms of recent nomenclature, this is the "Branch Wall" problem. <p> They also assumed instruction reordering comparable to the existing state-of-the-art. Even with these limits, most applications spent less than 20% of their time in memory related stalls. Theobald, et. al. <ref> [23] </ref>, in their study on the effects of resource limitations on program parallelism, looked at the impact of control dependences and memory latencies on performance. They evaluated five different branch prediction strategies. However, they used a probabilistic model for their memory system to mark random memory accesses as misses.
Reference: [24] <author> Mark Horowitz, et. al. </author> <title> PLL Design for a 500 MB/s Interface. </title> <booktitle> In the proceedings of the International Solid-State Circuits Conference, </booktitle> <pages> pages 160-161, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Interconnects. The number of pins on a microprocessor has been increasing relatively slowly, at 16% per year [19]. Moreover, most PC's only have busses that run at a fraction of the CPU clock speed (e.g., 1/4, typically 50-66Mhz). However, techniques such as the RAMBUS memory interface <ref> [24] </ref> have shown that it is possible to achieve signaling rates of about 2X faster than CPU clock rates in similar technology. By using fully differential signaling, as is normally used in communications applications, even higher signaling rates can be achieved.
Reference: [25] <institution> LSI Logic Corporation. LSI Logic Announces Flip Chip Technology. </institution> <note> PR Newswire Press Release, Septtember 30, </note> <year> 1996. </year>
Reference-contexts: Compress implies an incoming data bus width of 100 bytes operating at the processor clock frequency. 100 incoming bytes implies 800 I/Os (not including address or ECC). This and a smaller outgoing data bus are almost small enough to fit in currently announced packages of 1500 I/Os <ref> [25] </ref>. However in a decade, packages with even more I/Os should be available. These results are consistent with (but not implied) by the results of Burger, Goodman, and Kagi (BGK). The ratio of their second-level cache latency to the processor cycle time is similar to ours (20 vs. 16).
Reference: [26] <author> James E. Smith. </author> <title> A Study of Branch Prediction Strategies. </title> <booktitle> In Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 135-148, </pages> <year> 1981. </year>
Reference-contexts: The two graphs in the figure plot the performance (measured by the IPC) and the prediction accuracy obtained with the seven branch prediction models. This shows that the misprediction rate for compress has only been reduced by 29% when moving from 1981 branch predictor algorithms <ref> [26] </ref> to 1996 branch predictor algorithms. This is an annual rate of misprediction reduction of 2.2%. Improving branch predictor algorithms from 1981 to 1996 results in an IPC improvement of 13%, for an annual rate of IPC improvement of 0.83%.
Reference: [27] <author> Nicolas Gloy, Cliff Young, J. Bradley Chen, and Mike Smith. </author> <title> An Analysis of Dynamic Branch Prediction Schemes on System Workloads. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 12-21, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Many benchmarks even had significantly lower performance than the perfect case when an aggressive futuristic branch predictor (a mix of 75% perfect and 25% existing branch predictor) was simulated. Other recent work on branch predictor performance for operating systems <ref> [27, 28, 11] </ref> and other work-loads suggests that branch prediction accuracies are likely to further decrease in such cases; our results on the importance of the branch wall therefore tend to err on the optimistic side (by underestimating the importance of the branch wall).
Reference: [28] <author> Stuart Sechrest, Chih-Chieh Lee, and Trevor Mudge. </author> <title> Correlation and Aliasing in Dynamic Branch Predictors. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 22-32, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Many benchmarks even had significantly lower performance than the perfect case when an aggressive futuristic branch predictor (a mix of 75% perfect and 25% existing branch predictor) was simulated. Other recent work on branch predictor performance for operating systems <ref> [27, 28, 11] </ref> and other work-loads suggests that branch prediction accuracies are likely to further decrease in such cases; our results on the importance of the branch wall therefore tend to err on the optimistic side (by underestimating the importance of the branch wall).
Reference: [29] <author> Gurindar Sohi, Scott Breach, </author> <title> and T.N. </title> <booktitle> Vijaykumar. Multi-scalar Processors. In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Hence techniques which exploit larger-grain flow predictability at the procedure or fine-grain task level are very attractive alternatives for research. These techniques include multiscalar <ref> [29] </ref>, multithreading [30], or parallel processors on a single chip [31]. 6 Acknowledgments This work was supported by an internship at DEC Western Research Laboratory. Thanks to all the staff at DEC WRL for putting up with our simulations.
Reference: [30] <author> Dean Tullsen, Susan Eggers, Joel Emer, Henry Levy, Jack Lo, and Rebecca Stamm. </author> <title> Exploiting Choice: Instruction Fetch and Issue on an Implementable Simultaneous Mul-tithreading Processor. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191-202, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Hence techniques which exploit larger-grain flow predictability at the procedure or fine-grain task level are very attractive alternatives for research. These techniques include multiscalar [29], multithreading <ref> [30] </ref>, or parallel processors on a single chip [31]. 6 Acknowledgments This work was supported by an internship at DEC Western Research Laboratory. Thanks to all the staff at DEC WRL for putting up with our simulations.
Reference: [31] <author> Basem Nayfeh, Lance Hammond, and Kunle Olukotun. </author> <title> Evaluation of Design Alternatives for a Multiprocessor Microprocessor. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 67-77, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Hence techniques which exploit larger-grain flow predictability at the procedure or fine-grain task level are very attractive alternatives for research. These techniques include multiscalar [29], multithreading [30], or parallel processors on a single chip <ref> [31] </ref>. 6 Acknowledgments This work was supported by an internship at DEC Western Research Laboratory. Thanks to all the staff at DEC WRL for putting up with our simulations.
Reference: [32] <author> David Kroft. </author> <title> Lockup-free Instruction Fetch/Prefetch Cache Organization. </title> <booktitle> In Proceedings of the 8th International Symposium on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: The second column indicates the percentage of memory instructions in the program. The third column gives the miss rate expressed as a percentage. The value in parentheses represents the primary miss rate of the application. Primary misses <ref> [32] </ref> are misses to blocks that are not already being fetched by earlier (still outstanding) misses.
Reference: [33] <author> Scott. A. Mahlke, Richard E. Hank, Roger A. Bringmann, John C. Gyllenhaal, David M. Gallagher, and Wen-mei W. Hwu. </author> <title> Characterizing the Impact of Predicated Execution on Branch Prediction. </title> <booktitle> In Proceedings of the 27th International Symposium on Microarchitecture, </booktitle> <pages> pages 217-227, </pages> <month> Dec </month> <year> 1994. </year> <month> 11 </month>
Reference-contexts: Other currently proposed techniques to improve the performance of branches (e.g. guarding <ref> [33] </ref>, fanout-based prediction [2]) are not expected to 3 Application Notes Input Load % Branch % Integer Applications Compress Reduction of size of file using Lempel-Ziv coding ref 26.2 13.34 Espresso Minimization of a boolean function bca 22.5 17.81 Eqntott Translation of a boolean equation to truth table ref 12.9 10.91
References-found: 33

