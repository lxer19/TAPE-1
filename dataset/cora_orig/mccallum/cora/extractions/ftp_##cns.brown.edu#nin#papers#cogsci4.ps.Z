URL: ftp://cns.brown.edu/nin/papers/cogsci4.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Email: edelman@ai.mit.edu  nin@math.tau.ac.il  
Title: Learning as formation of low-dimensional representation spaces  
Author: Shimon Edelman Nathan Intrator 
Address: E25-201, Cambridge, MA 02142, USA  69978, Israel  
Affiliation: Center for Biological and Computational Learning MIT  School of Mathematical Sciences Tel Aviv University, Tel Aviv  
Abstract: Psychophysical findings accumulated over the past several decades indicate that perceptual tasks such as similarity judgment tend to be performed on a low-dimensional representation of the sensory data. Low dimensionality is especially important for learning, as the number of examples required for attaining a given level of performance grows exponentially with the dimensionality of the underlying representation space. Because of this curse of dimensionality, in shape categorization the high initial dimensionality of the sensory data must be reduced by a nontrivial computational process, which, ideally, should capture the intrinsic low-dimensional nature of families of visual shapes. We show how to make a connectionist system use class labels to learn a representation that fulfills this requirement, thereby facilitating shape categorization. Our results indicate that low-dimensional representations are best extracted in a learning task that combines discrimination and generalization constraints. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atick, J. J., Griffin, P. A., and Redlich, A. N. </author> <year> (1996). </year> <title> The vocabulary of shape: principal shapes for probing perception and neural response. </title> <journal> Network, </journal> <volume> 7 </volume> <pages> 1-5. </pages>
Reference-contexts: In the meanwhile, a useful common low-dimensional parameterization of shapes belonging to certain categories can be achieved via principal component analysis, as it was done here for the human heads; cf. <ref> (Atick et al., 1996) </ref>.
Reference: <author> Beck, J. </author> <year> (1972). </year> <title> Surface Color Perception. </title> <publisher> Cornell University Press, </publisher> <address> Ithaca, NY. </address>
Reference: <author> Bellman, R. E. </author> <year> (1961). </year> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Boynton, R. M. </author> <year> (1978). </year> <title> Color, hue, </title> <editor> and wavelength. In Carterette, E. C. and Friedman, M. P., editors, </editor> <booktitle> Handbook of Perception, volume V, </booktitle> <pages> pp 301-347. </pages> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Cohen, J. </author> <year> (1964). </year> <title> Dependency of the spectral reflectance curves of the Munsell color chips. </title> <journal> Psychonomic Sciences, </journal> <volume> 1 </volume> <pages> 369-370. </pages>
Reference: <author> Cortese, J. M. and Dyre, B. P. </author> <year> (1996). </year> <title> Perceptual similarity of shapes generated from Fourier Descriptors. </title> <journal> Journal of Experimental Psychology: Human Perception and Performance, </journal> <volume> 22 </volume> <pages> 133-143. </pages>
Reference: <author> Cottrell, G. W., Munro, P., and Zipser, D. </author> <year> (1987). </year> <title> Learning internal representations from gray-scale images: An example of extensional programming. </title> <booktitle> In Ninth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp 462-473, </pages> <address> Hillsdale. </address> <publisher> Erlbaum. </publisher>
Reference: <author> Cutzu, F. and Edelman, S. </author> <year> (1996). </year> <title> Faithful representation of similarities among three-dimensional shapes in human vision. </title> <booktitle> Proceedings of the National Academy of Science, </booktitle> <volume> 93 </volume> <pages> 12046-12050. </pages>
Reference: <author> De Valois, R. L. and De Valois, K. K. </author> <year> (1978). </year> <title> Neural coding of color. </title> <editor> In Carterette, E. C. and Friedman, M. P., editors, </editor> <booktitle> Handbook of Perception, volume V, </booktitle> <pages> pp 117-166. </pages> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Demartines, P. and Herault, J. </author> <year> (1996). </year> <title> Curvilinear component analysis: a self-organizing neural network for non linear mapping of data sets. Submitted to IEEE Transaction on Neural Networks. </title>
Reference: <author> DeMers, D. and Cottrell, G. </author> <year> (1993). </year> <title> Nonlinear dimensionality reduction. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pp 580-587. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> D'Zmura, M. and Iverson, G. </author> <year> (1997). </year> <title> A formal approach to color constancy: the recovery of surface and light source spectral properties using bilinear models. </title> <editor> In Dowling, C., Roberts, F., and Theuns, P., editors, </editor> <booktitle> Recent Progress in Mathematical Psychology. </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Edelman, S. </author> <year> (1995). </year> <title> Representation of similarity in 3D object discrimination. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 407-422. </pages>
Reference: <author> Edelman, S. </author> <year> (1997). </year> <title> Representation is representation of similarity. </title> <journal> Behavioral and Brain Sciences, </journal> <note> to appear. </note>
Reference-contexts: same problem arises in the various attempts to extend a popular tool for linear dimensionality reduction, principal component analysis (PCA), to handle nonlinear spaces. 5 A number of learning methods for topology-preserving di 4 A discussion of such quasiconformal mappings in the context of shape representation can be found in <ref> (Edelman and Duvdevani-Bar, 1997) </ref>. 5 An example of such an approach is clustering followed by (local) PCA (Leen and Kambhatla, 1994). mensionality reduction have been derived from the idea of a self-supervised auto-associative network (Elman and Zipser, 1988; DeMers and Cottrell, 1993; Demartines and H erault, 1996). <p> Because these methods are unsupervised, they extract representations that are not orthogonal to the irrelevant dimensions of the input space. As a result, these methods are less likely to find the target manifold <ref> (Intrator and Edelman, 1997) </ref>, which is defined, to a large extent, by the measurement-space directions to which it is orthogonal; see Figure 1. <p> unsupervised and totally supervised methods in that it uses a label that individuates a given data item, but does not require information regarding the relationship between the different items, let alone the 6 Experimentation with various architectures, including multilayer perceptrons and radial basis function networks, yielded similarly encouraging results; see <ref> (Intrator and Edelman, 1997) </ref> for details. the 18 heads obtained by placing a 3 fi 6 grid in the space of the two leading principal components of the original nine heads. <p> proved to be a good substrate for solving classification tasks on which the system has not been trained: the error rate on a nonlinear dichotomy involving the 18 classes was 0:02, compared to 0:07 obtained by a system trained specifically on that dichotomy, but using the raw multidimensional representation; see <ref> (Intrator and Edelman, 1997) </ref> for details. Middle: results for a 5-layer bottleneck MLP with 2 hidden units in the middle hidden layer, trained on the 18-way classification task. The test dichotomy error rate was 0:1, compared to 0:29 on the raw data.
Reference: <author> Edelman, S., Cutzu, F., and Duvdevani-Bar, S. </author> <year> (1996). </year> <title> Similarity to reference shapes as a basis for shape representation. </title> <editor> In Cottrell, G. W., editor, </editor> <booktitle> Proceedings of 18th Annual Conf. of the Cognitive Science Society, </booktitle> <pages> pp 260-265, </pages> <address> San Diego, CA. </address>
Reference-contexts: The main problem with MDS, considered as a method for massive dimensionality reduction rather than for exploration of experimental data in applied sciences, is its poor scaling with dimensionality <ref> (Intrator and Edelman, 1996) </ref>. <p> The performance of this method in recovering the row/column parametric structure of the 18 7 We tested this method also on another data set, consisting of parameterized fractal images <ref> (Intrator and Edelman, 1996) </ref>. classes seems to be especially amazing. 8 Moreover, the same structure was apparent in the LDR produced by a network that was trained on every second face class (faces 1, 3, 5, B, D, F, a, c, e), then tested on the full data set (see Figure
Reference: <author> Edelman, S. and Duvdevani-Bar, S. </author> <year> (1997). </year> <title> Similarity, connectionism, and the problem of representation in vision. </title> <journal> Neural Computation, </journal> <volume> 9 </volume> <pages> 701-720. </pages>
Reference-contexts: same problem arises in the various attempts to extend a popular tool for linear dimensionality reduction, principal component analysis (PCA), to handle nonlinear spaces. 5 A number of learning methods for topology-preserving di 4 A discussion of such quasiconformal mappings in the context of shape representation can be found in <ref> (Edelman and Duvdevani-Bar, 1997) </ref>. 5 An example of such an approach is clustering followed by (local) PCA (Leen and Kambhatla, 1994). mensionality reduction have been derived from the idea of a self-supervised auto-associative network (Elman and Zipser, 1988; DeMers and Cottrell, 1993; Demartines and H erault, 1996). <p> Because these methods are unsupervised, they extract representations that are not orthogonal to the irrelevant dimensions of the input space. As a result, these methods are less likely to find the target manifold <ref> (Intrator and Edelman, 1997) </ref>, which is defined, to a large extent, by the measurement-space directions to which it is orthogonal; see Figure 1. <p> unsupervised and totally supervised methods in that it uses a label that individuates a given data item, but does not require information regarding the relationship between the different items, let alone the 6 Experimentation with various architectures, including multilayer perceptrons and radial basis function networks, yielded similarly encouraging results; see <ref> (Intrator and Edelman, 1997) </ref> for details. the 18 heads obtained by placing a 3 fi 6 grid in the space of the two leading principal components of the original nine heads. <p> proved to be a good substrate for solving classification tasks on which the system has not been trained: the error rate on a nonlinear dichotomy involving the 18 classes was 0:02, compared to 0:07 obtained by a system trained specifically on that dichotomy, but using the raw multidimensional representation; see <ref> (Intrator and Edelman, 1997) </ref> for details. Middle: results for a 5-layer bottleneck MLP with 2 hidden units in the middle hidden layer, trained on the 18-way classification task. The test dichotomy error rate was 0:1, compared to 0:29 on the raw data.
Reference: <author> Elman, J. L. and Zipser, D. </author> <year> (1988). </year> <title> Learning the hidden structure of speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 4(83). </volume>
Reference-contexts: First, we asked whether a self-supervised 3-layer autoencoder, which aims at the best reconstruction of the inputs, can reveal the correct low-dimensional structure in the present case. Although in the linear case such networks do quite well, essentially by extracting the principal components of the data <ref> (Elman and Zipser, 1988) </ref>, the performance on the FACES data was poor (the network consistently converged to the mean of the data), presumably due to the nonlinearity introduced by the imaging step. Second, we experimented with a 5-layer nonlinear bottleneck autoencoder (Leen and Kambhatla, 1994), which, likewise, performed poorly.
Reference: <author> Gregson, R. A. M. </author> <year> (1975). </year> <title> Psychometrics of similarity. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Intrator, N. and Edelman, S. </author> <year> (1996). </year> <title> How to make a low-dimensional representation suitable for diverse tasks. </title> <journal> Connection Science, </journal> <volume> 8 </volume> <pages> 205-224. </pages>
Reference-contexts: The main problem with MDS, considered as a method for massive dimensionality reduction rather than for exploration of experimental data in applied sciences, is its poor scaling with dimensionality <ref> (Intrator and Edelman, 1996) </ref>. <p> The performance of this method in recovering the row/column parametric structure of the 18 7 We tested this method also on another data set, consisting of parameterized fractal images <ref> (Intrator and Edelman, 1996) </ref>. classes seems to be especially amazing. 8 Moreover, the same structure was apparent in the LDR produced by a network that was trained on every second face class (faces 1, 3, 5, B, D, F, a, c, e), then tested on the full data set (see Figure
Reference: <author> Intrator, N. and Edelman, S. </author> <year> (1997). </year> <title> Learning low dimensional representations of visual objects with extensive use of prior knowledge. </title> <publisher> in press. </publisher>
Reference-contexts: Because these methods are unsupervised, they extract representations that are not orthogonal to the irrelevant dimensions of the input space. As a result, these methods are less likely to find the target manifold <ref> (Intrator and Edelman, 1997) </ref>, which is defined, to a large extent, by the measurement-space directions to which it is orthogonal; see Figure 1. <p> unsupervised and totally supervised methods in that it uses a label that individuates a given data item, but does not require information regarding the relationship between the different items, let alone the 6 Experimentation with various architectures, including multilayer perceptrons and radial basis function networks, yielded similarly encouraging results; see <ref> (Intrator and Edelman, 1997) </ref> for details. the 18 heads obtained by placing a 3 fi 6 grid in the space of the two leading principal components of the original nine heads. <p> proved to be a good substrate for solving classification tasks on which the system has not been trained: the error rate on a nonlinear dichotomy involving the 18 classes was 0:02, compared to 0:07 obtained by a system trained specifically on that dichotomy, but using the raw multidimensional representation; see <ref> (Intrator and Edelman, 1997) </ref> for details. Middle: results for a 5-layer bottleneck MLP with 2 hidden units in the middle hidden layer, trained on the 18-way classification task. The test dichotomy error rate was 0:1, compared to 0:29 on the raw data.
Reference: <author> Judd, D. B., MacAdam, D. L., and Wyszecki, G. </author> <year> (1964). </year> <title> Spectral distribution of typical daylight as a function of correlated color temperature. </title> <journal> Journal of the Optical Society of America, </journal> <volume> 54 </volume> <pages> 1031-1040. </pages>
Reference: <author> Koontz, W. L. G. and Fukunaga, K. </author> <year> (1972). </year> <title> A nonlinear feature extraction algorithm using distance information. </title> <journal> IEEE Trans. Comput., </journal> <volume> 21 </volume> <pages> 56-63. </pages>
Reference: <author> Krumhansl, C. L. </author> <year> (1978). </year> <title> Concerning the applicability of geometric models to similarity data: the interrelationship between similarity and spatial density. </title> <journal> Psychological Review, </journal> <volume> 85 </volume> <pages> 445-463. </pages>
Reference: <author> Kruskal, J. B. </author> <year> (1964). </year> <title> Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. </title> <journal> Psychometrika, </journal> <volume> 29(1) </volume> <pages> 1-27. </pages>
Reference: <author> Leen, T. K. and Kambhatla, N. </author> <year> (1994). </year> <title> Fast non-linear dimension reduction. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pp 152-159. </pages> <publisher> Morgan Kauffman, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: component analysis (PCA), to handle nonlinear spaces. 5 A number of learning methods for topology-preserving di 4 A discussion of such quasiconformal mappings in the context of shape representation can be found in (Edelman and Duvdevani-Bar, 1997). 5 An example of such an approach is clustering followed by (local) PCA <ref> (Leen and Kambhatla, 1994) </ref>. mensionality reduction have been derived from the idea of a self-supervised auto-associative network (Elman and Zipser, 1988; DeMers and Cottrell, 1993; Demartines and H erault, 1996). Because these methods are unsupervised, they extract representations that are not orthogonal to the irrelevant dimensions of the input space. <p> Second, we experimented with a 5-layer nonlinear bottleneck autoencoder <ref> (Leen and Kambhatla, 1994) </ref>, which, likewise, performed poorly. The outcome of this experiment showed that self-supervised dimensionality reduction cannot recover a good LDR in the present case, illustrating the importance of guidance provided by the class labels.
Reference: <author> Medin, D. L., Goldstone, R. L., and Gentner, D. </author> <year> (1993). </year> <title> Respects for similarity. </title> <journal> Psychological Review, </journal> <volume> 100 </volume> <pages> 254-278. </pages>
Reference: <author> Nosofsky, R. M. </author> <year> (1992). </year> <title> Similarity scaling and cognitive process models. </title> <journal> Annual Review of Psychology, </journal> <volume> 43 </volume> <pages> 25-53. </pages>
Reference: <author> Shepard, R. N. </author> <year> (1966). </year> <title> Metric structures in ordinal data. </title> <journal> J. Math. Psychology, </journal> <volume> 3 </volume> <pages> 287-315. </pages>
Reference: <author> Shepard, R. N. </author> <year> (1980). </year> <title> Multidimensional scaling, tree-fitting, and clustering. </title> <journal> Science, </journal> <volume> 210 </volume> <pages> 390-397. </pages>
Reference: <author> Shepard, R. N. </author> <year> (1987). </year> <title> Toward a universal law of generalization for psychological science. </title> <journal> Science, </journal> <volume> 237 </volume> <pages> 1317-1323. </pages>
Reference: <author> Shepard, R. N. and Cermak, G. W. </author> <year> (1973). </year> <title> Perceptual-cognitive explorations of a toroidal set of free-form stimuli. </title> <journal> Cognitive Psychology, </journal> <volume> 4 </volume> <pages> 351-377. </pages>
Reference: <author> Stone, C. J. </author> <year> (1982). </year> <title> Optimal global rates of convergence for non-parametric regression. </title> <journal> Annals of statistics, </journal> <volume> 10 </volume> <pages> 1040-1053. </pages>
Reference: <author> Tversky, A. </author> <year> (1977). </year> <title> Features of similarity. </title> <journal> Psychological Review, </journal> <volume> 84 </volume> <pages> 327-352. </pages>
Reference: <author> Tversky, A. and Gati, I. </author> <year> (1982). </year> <title> Concerning the applicability of geometric models to similarity data: the interrelationship between similarity and spatial density. </title> <journal> Psychological Review, </journal> <volume> 89 </volume> <pages> 123-154. </pages>
Reference: <author> Webb, A. R. </author> <year> (1995). </year> <title> Multidimensional-scaling by iterative ma-jorization using radial basis functions. </title> <journal> Pattern Recognition, </journal> <volume> 28 </volume> <pages> 753-759. </pages>
References-found: 35

