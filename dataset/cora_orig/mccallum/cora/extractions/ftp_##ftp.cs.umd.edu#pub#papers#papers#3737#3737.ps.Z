URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3737/3737.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Iteration Space Slicing and Its Application to Communication Optimization  
Author: William Pugh Evan Rosser 
Keyword: program slicing, communication optimization, message coalescing, latency tolerance  
Note: This work is supported by an NSF PYI grant CCR-9157384 and by a Packard Fellowship.  
Address: College Park, MD 20742  
Affiliation: Inst. for Advanced Computer Studies Dept. of Computer Science Dept. of Computer Science Univ. of Maryland,  
Pubnum: UMIACS-TR-97-02  CS-TR-3737  
Email: pugh@cs.umd.edu ejr@cs.umd.edu  
Date: April, 1997  
Abstract: Program slicing is an analysis that answers questions such as "Which statements might affect the computation of variable v at statement s?" or "Which statements depend on the value of v computed in statement s?". The answers computed by program slicing are generally a set of statements. We introduce the idea of iteration spacing slicing: we refine program slicing to ask questions such as "Which iterations of which statements might effect the computation in iterations I of statement s?" or "Which iterations of which statements depend on the value computed by iterations I of statement s?". One application of this general-purpose technique is optimization of interprocessor communication in data-parallel compilers. For example, we can separate a code fragment into 1) those iterations that must be done before a send, 2) those iterations that don't need to be done before a send and don't depend on non-local data and 3), those iterations that depend on non-local data. We examine applications of iteration space slicing to communication optimizations in parallel executions of programs such as stencil computations and block-cyclic Gaussian elimination with partial pivoting. Copyright c fl1997 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. 
Abstract-found: 1
Intro-found: 1
Reference: [AKMC94] <author> Vikram S. Adve, Charles Koelbel, and John M. Mellor-Crummey. </author> <title> Performance analysis of data-parallel programs. </title> <type> Technical Report CRPC-TR94404, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: This also allows computations to run in a more loosely-coupled fashion, so that when one processor runs slightly ahead of the others, it is not held back. Special cases of this optimization have been discussed in <ref> [KMR90, HKT93, AKMC94] </ref>. Message aggregation Message coalescing (or aggregation) is an important transformation on many machines where the start-up time for a message is significant. <p> Their paper also discusses a number of issues related performing loop fusion and avoiding cache conflicts while doing so. We do not address those issues here, and might profit from using their methods. Adve, Koelbel, and Mellor-Crummey <ref> [AKMC94] </ref> note that splitting off the iterations that are needed before or depend on non-local computations can be beneficial, and discuss how this is done in simple stencil computations such as SOR. <p> The additional level of optimization across multiple time steps described in Section 5.4 is not discussed. Adve, Koelbel and Mellor-Crummey <ref> [AKMC94] </ref> describe an optimization for which "...no parallelizing compiler has attempted the more difficult analysis and reordering of computations needed to deduce the optimization of DGEFA from the original source." This optimization is a restricted case of the transformation described in Section 5.4, for a pure cyclic distribution. 8 Conclusion We
Reference: [HKMCS94] <author> Seema Hiranandani, Ken Kennedy, John Mellor-Crummey, and Ajay Sethi. </author> <title> Compilation techniques for block-cyclic distributions. </title> <booktitle> In Proc. of the 1994 International Conference on Supercomputing, </booktitle> <year> 1994. </year>
Reference-contexts: An iteration space slice can extract those computations which must be done before the send, and postpone the rest, which enables profitable message coalescing in situations in which it would otherwise introduce significant additional latency. A manual application of this optimization for block-cyclic Gaussian elimination was discussed in <ref> [HKMCS94] </ref> but automation of the transformation was beyond the capabilities of existing transformation systems. We derive both that transformation and a more sophisticated form that produces even better performance. <p> In all parallel codes, we assume that the transformations to limit the computations to those executed on each processor, and to limit the sends and receives to just the appropriate processors, are performed in a later step. 5.2 Naive Aggregation With a block-cyclic distribution, it appears attractive <ref> [HKMCS94] </ref> to coalesce the communication of the adjacent columns that belong to one processor. <p> The result is shown in Figure 5e. This slicing gives us code similar to that in Figure 4 (although that code doesn't include pivoting). This is equivalent to an optimization proposed in <ref> [HKMCS94] </ref>. 5.4 Slicing across multiple time steps Slicing the code within a time step permits us to coalesce messages without a huge penalty, and depending on machine details, may improve performance by itself. However, it does not hide the latency of that communication.
Reference: [HKT93] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Preliminary experiences with the fortran-d compiler. </title> <booktitle> In Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: This also allows computations to run in a more loosely-coupled fashion, so that when one processor runs slightly ahead of the others, it is not held back. Special cases of this optimization have been discussed in <ref> [KMR90, HKT93, AKMC94] </ref>. Message aggregation Message coalescing (or aggregation) is an important transformation on many machines where the start-up time for a message is significant. <p> Simple cases of this optimization have been discussed or implemented by a number of researchers <ref> [KMR90, HKT93] </ref> Hiranandani et.al.[HKMCS94] discuss a number of issues related to the generation of efficient code for block-cyclic data distributions.
Reference: [KMP + 95] <author> Wayne Kelly, Vadim Maslov, William Pugh, Evan Rosser, Tatiana Shpeisman, and David Wonna-cott. </author> <title> The Omega Library interface guide. </title> <type> Technical Report CS-TR-3445, </type> <institution> Dept. of Computer Science, University of Maryland, College Park, </institution> <month> March </month> <year> 1995. </year> <note> The Omega library is available from http://www.cs.umd.edu/projects/omega. </note>
Reference-contexts: These sets and relations can be represented and manipulated using the Omega library <ref> [KMP + 95] </ref>. Iteration space slicing is not simply a new kind of dependence analysis technique. Data dependence analysis finds pairs of statement instances which access the same memory, and that information can be used to analyze communication or prove legality of transformations. <p> The relations may involve free variables such as n in the following example: f [i] ! [i + 1] j 1 i &lt; n g. These free variables correspond to symbolic constants or parameters in the source program. Tuple relations and sets are represented using the Omega library <ref> [Pug92, PW95, PW93, KMP + 95] </ref> which is a collection of routines for manipulating affine constraints over integer variables. The Omega library allows tuple relations and sets to be manipulated using operations such as intersection, union, inverse, composition, and difference.
Reference: [KMR90] <author> Charles Koebel, Piyush Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machine s. </title> <booktitle> In Proc. of the 2nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: This also allows computations to run in a more loosely-coupled fashion, so that when one processor runs slightly ahead of the others, it is not held back. Special cases of this optimization have been discussed in <ref> [KMR90, HKT93, AKMC94] </ref>. Message aggregation Message coalescing (or aggregation) is an important transformation on many machines where the start-up time for a message is significant. <p> Simple cases of this optimization have been discussed or implemented by a number of researchers <ref> [KMR90, HKT93] </ref> Hiranandani et.al.[HKMCS94] discuss a number of issues related to the generation of efficient code for block-cyclic data distributions.
Reference: [KPR95] <author> Wayne Kelly, William Pugh, and Evan Rosser. </author> <title> Code generation for multiple mappings. </title> <booktitle> In The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 332-341, </pages> <address> McLean, Virginia, </address> <month> Febru-ary </month> <year> 1995. </year>
Reference-contexts: In traditional program slicing, the final significant issue is determining how to construct an executable slice [Wei84, Tip95]. Within our domain, this problem is very simple because we assume well-structured control flow. Given a set of iteration spaces, we can use techniques <ref> [KPR95] </ref> implemented within the Omega library to generate efficient code that enumerates those iteration spaces, executing the selected iterations in the same order with respect to other selected iterations.
Reference: [KPRS96] <author> Wayne Kelly, William Pugh, Evan Rosser, and Tatiana Shpeisman. </author> <title> Transitive closure of infinite graphs and its applications. </title> <journal> International J. of Parallel Programming, </journal> <volume> 24(6) </volume> <pages> 579-598, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: The key step in calculating an iteration space slice is to calculate the transitive closure of the data dependence graph of the program <ref> [KPRS96] </ref>; the transitive dependences then are applied to the iterations of interest to produce the slice. <p> Instead of edges being between statements in the graph, edges are actually between iterations of the statements; therefore, we need to compute the transitive closure of an infinite graph, because the iteration spaces may have symbolic bounds. In previous work <ref> [KPRS96] </ref>, we presented techniques for doing this and have implemented them within the Omega library. Computing iteration space slices that are exact in all cases is impossible (as discussed in Section 3 and [KPRS96], there is a reduction to an undecidable problem). <p> In previous work <ref> [KPRS96] </ref>, we presented techniques for doing this and have implemented them within the Omega library. Computing iteration space slices that are exact in all cases is impossible (as discussed in Section 3 and [KPRS96], there is a reduction to an undecidable problem). However, we can compute upper or lower bounds on a slice, as required. Alternative representations that would be cruder but faster could also be used, although we do not examine them in this paper. <p> This is not true in many general purpose codes, but is fairly accurate for many scientific numerical applications. 3 Transitive Closure In addition to the fairly standard operations supported within the Omega library, we also provide a transitive closure operator <ref> [KPRS96] </ref>. <p> However, in practice our techniques frequently do give us the exact answer. In <ref> [KPRS96] </ref> we found that in 2000 code fragments from our problem domain, we could compute the exact transitive dependences in 99% of the cases.
Reference: [Kri] <author> Jens Krinke. </author> <title> Program slicing page. </title> <address> http://www.cs.tu-bs.de/~krinke/Slicing/slicing.html. </address>
Reference-contexts: In these cases the aggregation without latency tolerance can be detrimental. 7 Related Work Although there is a large body of work on program slicing <ref> [Wei84, Tip95, Kri] </ref>, we are unaware of any work describing the concept of computing an iteration space slice. However, our work does not address many of the important issues that others have studied, such as complicated or irregular control flow and interprocedural slicing.
Reference: [Li92] <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proc. of the 1992 International Conference on Supercomputing, </booktitle> <pages> pages 313-322, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Z = g (X) Send X and Z Naive aggregation X = ... Send X and Z Y = f (X) Using slicing for k = 1 to n-1 do S1 a (i,k) = a (i,k) / a (k,k) S2 a (i,j) += - a (k,j)*a (i,k) <ref> [Ros90, TP92, PEH + 93, Li92] </ref> provide information required to support array privatization, but don't provide information on exactly which iteration produced or needs a value.
Reference: [MA97] <author> Maraig Manjikian and Tarek SS. Abdelrahman. </author> <title> Fusion of loops for parallelism and locality. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> February </month> <year> 1997. </year>
Reference-contexts: In Figures 1c and 1d, parallel code is shown for a block distribution of a and b, where lb and ub denote the boundaries of the array region owned by the local processor. Enabling parallelization of fused loops is discussed in <ref> [MA97] </ref>, and they provide a solution which only applies to programs with constant dependence distances. Tolerating message latency In order to hide message latency, compilers try to overlap computation with communication. <p> However, our work does not address many of the important issues that others have studied, such as complicated or irregular control flow and interprocedural slicing. Manjikian and Abdelraham <ref> [MA97] </ref> discussed the problem of loop-fusion creating loop-carried dependences and preventing parallelism. Their solution [MA97] works only in the presence of constant dependence distances and generates code that requires a barrier and will be inefficient if message latency is high. <p> However, our work does not address many of the important issues that others have studied, such as complicated or irregular control flow and interprocedural slicing. Manjikian and Abdelraham <ref> [MA97] </ref> discussed the problem of loop-fusion creating loop-carried dependences and preventing parallelism. Their solution [MA97] works only in the presence of constant dependence distances and generates code that requires a barrier and will be inefficient if message latency is high. Since their method requires constant dependence distances, it works well in the case of non-periodic stencil computations.
Reference: [PEH + 93] <author> David A. Padua, Rudolf Eigenmann, Jay Hoelflinger, Paul Peterson, Peng Tu, Stephen Weatherford, and Keith Faigin. </author> <title> Polaris: A new-generation parallelizing compiler for mpps. CSRD Rpt. </title> <type> 1306, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Z = g (X) Send X and Z Naive aggregation X = ... Send X and Z Y = f (X) Using slicing for k = 1 to n-1 do S1 a (i,k) = a (i,k) / a (k,k) S2 a (i,j) += - a (k,j)*a (i,k) <ref> [Ros90, TP92, PEH + 93, Li92] </ref> provide information required to support array privatization, but don't provide information on exactly which iteration produced or needs a value.
Reference: [Pug92] <author> William Pugh. </author> <title> The Omega test: a fast and practical integer programming algorithm for dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 8 </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The relations may involve free variables such as n in the following example: f [i] ! [i + 1] j 1 i &lt; n g. These free variables correspond to symbolic constants or parameters in the source program. Tuple relations and sets are represented using the Omega library <ref> [Pug92, PW95, PW93, KMP + 95] </ref> which is a collection of routines for manipulating affine constraints over integer variables. The Omega library allows tuple relations and sets to be manipulated using operations such as intersection, union, inverse, composition, and difference.
Reference: [PW93] <author> William Pugh and David Wonnacott. </author> <title> An exact method for analysis of value-based array data dependences. </title> <booktitle> In Lecture Notes in Computer Science 768: Sixth Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year> <note> Springer-Verlag. </note>
Reference-contexts: The relations may involve free variables such as n in the following example: f [i] ! [i + 1] j 1 i &lt; n g. These free variables correspond to symbolic constants or parameters in the source program. Tuple relations and sets are represented using the Omega library <ref> [Pug92, PW95, PW93, KMP + 95] </ref> which is a collection of routines for manipulating affine constraints over integer variables. The Omega library allows tuple relations and sets to be manipulated using operations such as intersection, union, inverse, composition, and difference.
Reference: [PW95] <author> William Pugh and David Wonnacott. </author> <title> Going beyond integer programming with the Omega test to eliminate false data dependences. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 204-211, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: The relations may involve free variables such as n in the following example: f [i] ! [i + 1] j 1 i &lt; n g. These free variables correspond to symbolic constants or parameters in the source program. Tuple relations and sets are represented using the Omega library <ref> [Pug92, PW95, PW93, KMP + 95] </ref> which is a collection of routines for manipulating affine constraints over integer variables. The Omega library allows tuple relations and sets to be manipulated using operations such as intersection, union, inverse, composition, and difference.
Reference: [Ros90] <author> Carl Rosene. </author> <title> Incremental Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Z = g (X) Send X and Z Naive aggregation X = ... Send X and Z Y = f (X) Using slicing for k = 1 to n-1 do S1 a (i,k) = a (i,k) / a (k,k) S2 a (i,j) += - a (k,j)*a (i,k) <ref> [Ros90, TP92, PEH + 93, Li92] </ref> provide information required to support array privatization, but don't provide information on exactly which iteration produced or needs a value.
Reference: [SHFG95] <author> M. Snir, P Hochschild, D. D. Frye, and K. J. Gildea. </author> <title> The communication software and parallel environment of the ibm sp2. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 205-221, </pages> <year> 1995. </year>
Reference-contexts: The interrupt-driven mode imposes an additional overhead on each communication, but permits non-blocking communication (when the switch is used in polled mode, no appreciable communication occurs unless both processors are simultaneously executing communication code <ref> [SHFG95] </ref>). The optimizations we describe are very dependent on having a communication system that supports non-blocking I/O that can be overlapped with computation. 6.1 Gaussian elimination with partial pivoting We ran three parallel versions of Gaussian elimination with partial pivoting using both a cyclic and block-cyclic distribution.
Reference: [Tip95] <author> Frank Tip. </author> <title> A survey of program slicing techniques. </title> <journal> Journal of Programming Languages, </journal> <volume> 3(3) </volume> <pages> 121-189, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Once we have the transitive closure of the dependences, computing the iterations reachable from the designated iterations is a straightforward application of the transitive dependences to the iterations of interest. In traditional program slicing, the final significant issue is determining how to construct an executable slice <ref> [Wei84, Tip95] </ref>. Within our domain, this problem is very simple because we assume well-structured control flow. <p> In these cases the aggregation without latency tolerance can be detrimental. 7 Related Work Although there is a large body of work on program slicing <ref> [Wei84, Tip95, Kri] </ref>, we are unaware of any work describing the concept of computing an iteration space slice. However, our work does not address many of the important issues that others have studied, such as complicated or irregular control flow and interprocedural slicing.
Reference: [TP92] <author> Peng Tu and David Padua. </author> <title> Array privatization for shared and distributed memory machines. </title> <booktitle> In Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: Z = g (X) Send X and Z Naive aggregation X = ... Send X and Z Y = f (X) Using slicing for k = 1 to n-1 do S1 a (i,k) = a (i,k) / a (k,k) S2 a (i,j) += - a (k,j)*a (i,k) <ref> [Ros90, TP92, PEH + 93, Li92] </ref> provide information required to support array privatization, but don't provide information on exactly which iteration produced or needs a value.
Reference: [Wei84] <author> M. Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 352-357, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: 1 Introduction Program slicing <ref> [Wei84] </ref> is an analysis that answers questions such as "Which statements might affect the computation of variable v at statement s?" or "Which statements depend on the value of v computed in statement s?". <p> Once we have the transitive closure of the dependences, computing the iterations reachable from the designated iterations is a straightforward application of the transitive dependences to the iterations of interest. In traditional program slicing, the final significant issue is determining how to construct an executable slice <ref> [Wei84, Tip95] </ref>. Within our domain, this problem is very simple because we assume well-structured control flow. <p> In these cases the aggregation without latency tolerance can be detrimental. 7 Related Work Although there is a large body of work on program slicing <ref> [Wei84, Tip95, Kri] </ref>, we are unaware of any work describing the concept of computing an iteration space slice. However, our work does not address many of the important issues that others have studied, such as complicated or irregular control flow and interprocedural slicing.
References-found: 19

