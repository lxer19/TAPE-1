URL: http://www.cs.umn.edu/Research/Agassiz/Paper/konas.wsc95.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Title: PROCESSOR SELF-SCHEDULING IN PARALLEL DISCRETE EVENT SIMULATION  
Author: Pavlos Konas Pen-Chung Yew 
Address: Mountain View, CA 94043, U.S.A.  Minneapolis, MN 55455-0159, U.S.A.  
Affiliation: Silicon Graphics Inc.  Department of Computer Science University of Minnesota  
Abstract: This paper describes a novel data structure and an algorithm for processor self-scheduling in parallel discrete event simulation. The presented data structure allows the efficient scheduling of future computations, it facilitates the inexpensive use of processor affinity information, it reduces the contention on the scheduling queue, and it integrates load balancing and locality management methods into a single mechanism. We use the behavioral simulation of a multiprocessor system to characterize the behavior of the proposed data structure and the associated scheduling algorithm. The results of our study show that it is important to maintain as detailed affinity information as possible and exploit this information at run time. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, T. </author> <year> 1991. </year> <title> Operating System Support for High Performance Multiprocessing. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seat-tle, Washington. </address>
Reference-contexts: It has also been shown that scheduling decisions should be fast introducing minimum overhead into the computation <ref> (Anderson 1991) </ref>. This means that affinity related decisions should use minimum information, and should not introduce significant overhead into the scheduling process. Otherwise, the performance of the parallel program would suffer (Anderson 1991). <p> It has also been shown that scheduling decisions should be fast introducing minimum overhead into the computation <ref> (Anderson 1991) </ref>. This means that affinity related decisions should use minimum information, and should not introduce significant overhead into the scheduling process. Otherwise, the performance of the parallel program would suffer (Anderson 1991). Furthermore, both analytical and experimental results have shown that a single, centralized scheduling queue will quickly become the system bottleneck as the number of participating processors increases (Squillante 1990).
Reference: <author> Chou, C., Bruell, S., and Jones, D. </author> <year> 1993. </year> <title> A Generalized Hold Model. </title> <booktitle> In Proceedings of the 1993 Winter Simulation Conference, </booktitle> <pages> 756-761. </pages>
Reference-contexts: These models allow us to study a data structure under different future event scheduling distributions, and have been extensively used in comparing different event queue implementations <ref> (Chou, Bruell, and Jones 1993) </ref>. Such a study will provide us with a more general characterization of the suitability of the presented data structure as a scheduling queue in general-purpose simulations. Second, we need to study the performance of diverse parallel simulators using this data structure as their scheduling queue.
Reference: <author> Fujimoto, R. </author> <year> 1990. </year> <title> Parallel Discrete Event Simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 31-53. </pages>
Reference-contexts: Third, most of the computations in architectural and logic-level simulations are fine- to medium-grain computations and, therefore, long memory access delays can introduce significant overheads into the parallel execution. Finally, in simulations based on the logical processes (LP) model <ref> (Fujimoto 1990) </ref> the distribution of the event queue across the LPs makes the scheduling queue (s) the only mechanism available for transforming conditionally activated LPs (scheduled computations) into unconditionally active LPs (ready to execute computations).
Reference: <author> Konas, P. </author> <year> 1994. </year> <title> Parallel Architectural Simulations on Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, Illinois. </institution>
Reference-contexts: Synchronous PDES methods have shown significant potential in the parallel execution of such simulations <ref> (Konas 1994) </ref>. On the other hand, asynchronous PDES methods (both conservative and optimistic) are less attractive due to the significant overheads they introduce in attempting to find parallelism during a simulation (Konas and Yew 1991). <p> In practice, the time wheel is (almost) guaranteed to have an average performance of O (1) in the simulation of architectural and logic-level designs. The reason is the behavior of these types of simulations. As has been observed in <ref> (Konas 1994, Soule 1992) </ref>, the most striking characteristic of architectural and logic-level simulations is the clock effect: peaks with considerable activity induced by the arrivals of the clock and of new input signals, followed by periods of diminishing activity as the signals propagate through the combinational elements of the simulated system. <p> In synchronous PDES simulation, a logical process is activated when a new value arrives at its inputs and no event already exists in the input queue of the LP for that simulated time <ref> (Konas 1994) </ref>. The activation of an LP results into its insertion in the scheduling queue of the processor that activated it. More specifically, when a processor activates an LP it executes the following steps. <p> A fourth simulation (nonuniform load) introduces nonuniformity in the computation requirements of the simulated components. In all simulations we pre-partition the system using both random (RND) and string-based (STR) partitioning methods <ref> (Konas 1994) </ref>, and turn self-scheduling on (DYNamic) and off (STATic). The overhead introduced into the computation by the load balancing step is (most of) the difference between the static and dynamic versions of each simulator.
Reference: <author> Konas, P. and Yew, P.-C. </author> <year> 1991. </year> <title> Parallel Discrete Event Simulation on Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 24th Annual Simulation Symposium, </booktitle> <pages> 134-148. </pages>
Reference-contexts: Synchronous PDES methods have shown significant potential in the parallel execution of such simulations (Konas 1994). On the other hand, asynchronous PDES methods (both conservative and optimistic) are less attractive due to the significant overheads they introduce in attempting to find parallelism during a simulation <ref> (Konas and Yew 1991) </ref>. The performance of a parallel simulation depends on the efficient utilization of the processors in a multiprocessor system. Two significant factors that determine the effective use of a multiprocessor are load balancing and locality management (Markatos 1993).
Reference: <author> Markatos, E. </author> <year> 1993. </year> <title> Scheduling for Locality in Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Rochester, Rochester, </institution> <address> New York. </address>
Reference-contexts: The performance of a parallel simulation depends on the efficient utilization of the processors in a multiprocessor system. Two significant factors that determine the effective use of a multiprocessor are load balancing and locality management <ref> (Markatos 1993) </ref>. Load balancing refers to the dynamic redistribution of the workload among the participating processors so that the load is continuously balanced across the multiprocessor. <p> Previous work in the area of self-scheduling of parallel programs has shown that, in the presence of nonuniform memory access (NUMA) characteristics in the host multiprocessor, we need to simultaneously address the issues of load balancing and of locality management if we are to achieve the most efficient parallel execution <ref> (Markatos 1993) </ref>. It has also been shown that scheduling decisions should be fast introducing minimum overhead into the computation (Anderson 1991). This means that affinity related decisions should use minimum information, and should not introduce significant overhead into the scheduling process.
Reference: <author> Razdan, R., Bischoff, G., and Ulrich, E. </author> <year> 1990. </year> <title> Exploitation of Periodicity in Logic Simulation of Synchronous Systems. </title> <booktitle> In Proceedings of the 1990 IEEE International Conference on Computer Aided Design, </booktitle> <pages> 62-65. </pages>
Reference-contexts: Previous work in the simulation area has shown that the most efficient data structure for event scheduling in simulations of detailed logic networks of large and active digital systems is the time-wheel <ref> (Razdan, Bischoff, and Ulrich 1990, Ulrich 1980, Ulrich 1983) </ref>.
Reference: <author> Riboe, J. </author> <year> 1990. </year> <title> Improvement of the Calendar Queue Algorithm. </title> <type> Technical report, </type> <institution> Department of Telecommunication and Computer Systems, The Royal Institute of Technology, Stockholm, Sweden. </institution>
Reference-contexts: The work presented here can be extended in two ways. First, it would be interesting to study the behavior and the performance of the presented data structure using analytical models, such as the Hold and the p-Hold <ref> (Riboe 1990) </ref> models. These models allow us to study a data structure under different future event scheduling distributions, and have been extensively used in comparing different event queue implementations (Chou, Bruell, and Jones 1993).
Reference: <author> Soule, L. </author> <year> 1992. </year> <title> Parallel Logic Simulation: An Evaluation of Centralized-Time and Distributed-Time Algorithms. </title> <type> PhD thesis, </type> <institution> Stanford University, Palo Alto, California. </institution>
Reference: <author> Squillante, M. </author> <year> 1990. </year> <title> Issues in Shared-Memory Multiprocessor Scheduling: A Performance Evaluation. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Washington, </institution> <address> Seattle, Washington. </address>
Reference-contexts: Otherwise, the performance of the parallel program would suffer (Anderson 1991). Furthermore, both analytical and experimental results have shown that a single, centralized scheduling queue will quickly become the system bottleneck as the number of participating processors increases <ref> (Squillante 1990) </ref>. The consensus of the previous work in scheduling thread-based and loop-parallel programs has been to avoid single, centralized data structures, and use affinity information when available.
Reference: <author> Ulrich, E. </author> <year> 1980. </year> <title> Table Lookup Techniques for Fast and Flexible Digital Logic Simulation. </title> <booktitle> In Proceedings of the 17th Design Automation Conference, </booktitle> <pages> 560-563. </pages>

References-found: 11

