URL: http://www-ai.ijs.si/MarkoBohanec/icml97.ps
Refering-URL: http://www-ai.ijs.si/MarkoBohanec/hmodres.html
Root-URL: 
Email: blaz.zupan@ijs.si  marko.bohanec@ijs.si  ivan.bratko@fri.uni-lj.si  janez.demsar@fri.uni-lj.si  
Title: Machine Learning by Function Decomposition  
Author: Blaz Zupan Marko Bohanec Ivan Bratko Janez Demsar 
Address: Ljubljana, Slovenia  Ljubljana, Slovenia  Ljubljana, Slovenia  Ljubljana Ljubljana, Slovenia  
Affiliation: Jozef Stefan Institute  Jozef Stefan Institute  Faculty of Computer and Information Sciences, and Jozef Stefan Institute  Faculty of Computer and Information Sciences University of  
Note: To appear in Proc. ICML-97  
Abstract: We present a new machine learning method that, given a set of training examples, induces a definition of the target concept in terms of a hierarchy of intermediate concepts and their definitions. This effectively decomposes the problem into smaller, less complex problems. The method is inspired by the Boolean function decomposition approach to the design of digital circuits. To cope with high time complexity of finding an optimal decomposition, we propose a suboptimal heuristic algorithm. The method, implemented in program HINT (HIerarchy Induction Tool), is experimentally evaluated using a set of artificial and real-world learning problems. It is shown that the method performs well both in terms of classification accuracy and discovery of meaningful concept hierarchies.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ashenhurst, R. L.: </author> <year> 1952, </year> <title> The decomposition of switching functions, </title> <type> Technical report, </type> <institution> Bell Laboratories BL-1(11), </institution> <address> pages 541-602. </address>
Reference-contexts: In this paper we present a method for developing a problem decomposition hierarchy from examples and investigate its applicability in machine learning. The method is based on function decomposition, an approach originally developed for the design of digital circuits <ref> (Ashenhurst 1952, Curtis 1962) </ref>. The goal is to decompose a function y = F (X) into y = G (A; H (B)), where X is a set of input attributes x 1 ; : : : ; x n , and y is the class variable.
Reference: <author> Biermann, A. W., Fairfield, J. and Beres, T.: </author> <year> 1982, </year> <title> Signature table systems and learning, </title> <journal> IEEE Trans. Syst. Man Cybern. </journal> <volume> 12(5), </volume> <pages> 635-648. </pages>
Reference: <author> Bohanec, M. and Rajkovic, V.: </author> <year> 1988, </year> <title> Knowledge acquisition and explanation for multi-attribute decision making, </title> <booktitle> 8th Intl Workshop on Expert Systems and their Applications, </booktitle> <address> Avignon, France, </address> <pages> pp. 59-78. </pages>
Reference-contexts: These were used to obtain a set of examples from which decomposition tried to reconstruct the original hierarchies. CAR evaluates cars based on their price and technical characteristics. This simple model was developed for educational purposes and is described in <ref> (Bohanec and Rajkovic 1988) </ref>. NURSERY is a real-world model developed to rank ap plications for nursery schools (Olave et al. 1989). The original datasets are noiseless. They completely cover the attribute space for all domains other than MONK1 and MONK2, where the coverage is 28.7% and 39.1%, respectively.
Reference: <author> Bohanec, M. and Rajkovic, V.: </author> <year> 1990, </year> <title> DEX: An expert system shell for decision support, </title> <booktitle> Sistemica 1(1), </booktitle> <pages> 145-157. </pages>
Reference-contexts: Michie (1995) emphasized the important role of structured induction and listed several real problems that were solved in this way. The concept hierarchy has also been used by a multi-attribute decision support expert system shell DEX <ref> (Bohanec and Rajkovic 1990) </ref>. There, a tree-like structure of variables is defined by a domain expert. DEX has been successfully applied in more than 50 realistic decision making problems. <p> MONK1 has an underlying concept (x 1 = x 2 ) OR x 5 = 1 and MONK2 the concept x i = 1 for exactly two choices of i 2 f1; : : : ; 6g. CAR and NURSERY For these two domains hierarchical classifiers in DEX <ref> (Bohanec and Ra-jkovic 1990) </ref> formalism already existed. These were used to obtain a set of examples from which decomposition tried to reconstruct the original hierarchies. CAR evaluates cars based on their price and technical characteristics.
Reference: <author> Curtis, H. A.: </author> <year> 1962, </year> <title> A New Approach to the Design of Switching Functions, </title> <publisher> Van Nostrand, </publisher> <address> Princeton, N.J. </address>
Reference: <author> Demsar, J., Zupan, B., Bohanec, M. and Bratko, I.: </author> <year> 1997, </year> <title> Constructing intermediate concepts by decomposition of real functions, </title> <booktitle> Proc. Euro-pean Conference on Machine Learning, ECML-97, </booktitle> <address> Prague. </address>
Reference-contexts: The decomposition approach as presented in this paper is limited by that there is no special mechanism for handling noise and continuous attributes. However, preliminary results on using an extended version of decomposition for continuously-valued data sets in <ref> (Demsar et al. 1997) </ref> and preliminary results on noise-handling extension strongly encourage further developments in this direction.
Reference: <author> Goldman, J. A., Ross, T. D. and Gadd, D. A.: </author> <year> 1995, </year> <title> Pattern theoretic learning, </title> <booktitle> AAAI Spring Symposium Series: Systematic Methods of Scientific Discovery. </booktitle>
Reference: <author> Luba, T.: </author> <year> 1995, </year> <title> Decomposition of multiple-valued functions, </title> <booktitle> 25th Intl. Symposium on Multiple-Valued Logic, Bloomigton, Indiana, </booktitle> <pages> pp. 256-261. </pages>
Reference: <author> Michalski, R. S.: </author> <year> 1986, </year> <title> Understanding the nature of learning: issues and research directions, </title> <editor> in R. Michalski, J. Carbonnel and T. Michell (eds), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <publisher> Kaufmann, </publisher> <address> Los Atlos, CA, </address> <pages> pp. 3-25. </pages>
Reference-contexts: Goldman et al. (1995) evaluate FLASH, a Boolean function decomposer, on a set of eight-attribute binary functions and show its robustness in comparison with C4.5 decision tree inducer. Feature discovery has been at large investigated by constructive induction <ref> (Michalski 1986) </ref>. Perhaps closest to the function decomposition method are the constructive induction systems that use a set of existing attributes and a set of predefined constructive operators to derive new attributes (Pfahringer 1994, Raga-van and Rendell 1993).
Reference: <author> Michie, D.: </author> <year> 1995, </year> <title> Problem decomposition and the learning of skills, </title> <editor> in N. Lavrac and S. Wrobel (eds), </editor> <booktitle> Machine Learning: ECML-95, Notes in Artificial Intelligence 912, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 17-31. </pages>
Reference: <author> Murphy, P. M. and Aha, D. W.: </author> <year> 1994, </year> <note> UCI Repository of machine learning databases [http:// www.ics.uci.edu/~mlearn/mlrepository.html]. Irvine, </note> <institution> CA: University of California, Department of Information and Computer Science. </institution>
Reference-contexts: While the definition of MIN and MAX is standard, the function AVG computes the average of its arguments and rounds it to the closest integer. LENSES A small domain taken from UCI machine learning repository <ref> (Murphy and Aha 1994) </ref>. Using patient age, spectacle prescription, astigmatism, and tear production rate each example describes whether the patient should wear soft or hard contact lenses or no lenses at all. <p> Using patient age, spectacle prescription, astigmatism, and tear production rate each example describes whether the patient should wear soft or hard contact lenses or no lenses at all. MONK1 and MONK2 Well-known six-attribute binary classification problems taken from the same repository <ref> (Murphy and Aha 1994, Thrun et al. 1991) </ref>. Attributes are 2 to 4-valued. MONK1 has an underlying concept (x 1 = x 2 ) OR x 5 = 1 and MONK2 the concept x i = 1 for exactly two choices of i 2 f1; : : : ; 6g.
Reference: <author> Olave, M., Rajkovic, V. and Bohanec, M.: </author> <year> 1989, </year> <title> An application for admission in public school systems, </title> <editor> in I. T. M. Snellen, W. B. H. J. van de Donk and J.-P. Baquiast (eds), </editor> <title> Expert Systems in Public Administration, </title> <publisher> Elsevier Science Publishers (North Holland), </publisher> <pages> pp. 145-160. </pages>
Reference-contexts: CAR evaluates cars based on their price and technical characteristics. This simple model was developed for educational purposes and is described in (Bohanec and Rajkovic 1988). NURSERY is a real-world model developed to rank ap plications for nursery schools <ref> (Olave et al. 1989) </ref>. The original datasets are noiseless. They completely cover the attribute space for all domains other than MONK1 and MONK2, where the coverage is 28.7% and 39.1%, respectively. Some other domain characteristics are given in Table 2. The decomposition used column multiplicity as a partition selection measure.
Reference: <author> Perkowski, M. A. et al.: </author> <year> 1995, </year> <title> Unified approach to functional decompositions of switching functions, </title> <type> Technical report, </type> <institution> Warsaw University of Technology and Eindhoven University of Technology. </institution>
Reference: <author> Pfahringer, B.: </author> <year> 1994, </year> <title> Controlling constructive induction in CiPF, </title> <editor> in F. Bergadano and L. D. Raedt (eds), </editor> <booktitle> Machine Learning: </booktitle> <address> ECML-94, </address> <publisher> Springer-Verlag, </publisher> <pages> pp. 242-256. </pages>
Reference-contexts: Feature discovery has been at large investigated by constructive induction (Michalski 1986). Perhaps closest to the function decomposition method are the constructive induction systems that use a set of existing attributes and a set of predefined constructive operators to derive new attributes <ref> (Pfahringer 1994, Raga-van and Rendell 1993) </ref>. Within machine learning, there are other approaches that are based on problem decomposition, but where the problem is decomposed by the expert and not discovered by a machine. A well-known example is structured induction (a term introduced by Donald Michie) applied by Shapiro (1987).
Reference: <author> Quinlan, J. R.: </author> <year> 1993, </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: For each p, the results are the average of 10 randomly chosen splits. The learning curve is compared to the one obtained by C4.5 inductive decision tree learner <ref> (Quinlan 1993) </ref> run on the same data. C4.5 used the default options except for -m1, which was observed to obtain a better classification accuracy than the default -m2. Accuracy is measured on unpruned decision trees for the same reason.
Reference: <author> Ragavan, H. and Rendell, L.: </author> <year> 1993, </year> <title> Lookahead feature construction for learning hard concepts, </title> <booktitle> Proc. Tenth International Machine Learning Conference, </booktitle> <publisher> Morgan Kaufman, </publisher> <pages> pp. 252-259. </pages>
Reference: <author> Samuel, A.: </author> <year> 1967, </year> <title> Some studies in machine learning using the game of checkers II: Recent progress, </title> <journal> IBM J. Res. Develop. </journal> <volume> 11, </volume> <pages> 601-617. </pages>
Reference-contexts: The paper is concluded by a summary and possible directions of further work. 2 RELATED WORK The decomposition approach to machine learning was used by a pioneer of artificial intelligence, A. Samuel. He proposed a method based on a signature table system <ref> (Samuel 1967) </ref> and successfully used it as an evaluation mechanism for his checkers playing programs. This approach was later improved by Biermann et al. (1982). Their method, however, did not address the problem of deriving the structure of concepts.
Reference: <author> Shapiro, A. D.: </author> <year> 1987, </year> <title> Structured induction in expert systems, </title> <publisher> Turing Institute Press in association with Addison-Wesley Publishing Company. </publisher>
Reference-contexts: 1 INTRODUCTION To solve a complex problem, one of the most general approaches is to decompose it into smaller, less complex and more manageable subproblems. In machine learning, this principle is a foundation for structured induction <ref> (Shapiro 1987) </ref>: instead of learning a single complex classification rule from examples, define a goal-subgoal hierarchy and learn the rules for each of the subgoals.
Reference: <author> Thrun, S. B. et al.: </author> <year> 1991, </year> <title> A performance comparison of different learning algorithms, </title> <type> Technical report, </type> <institution> Carnegie Mellon University CMU-CS-91-197. </institution>
Reference-contexts: It is also interesting to note that in MM4 C4.5's accuracy decreases with higher coverage of example space, which may be explained with decreased generalization. HINT was further tested on the data sets for MONK1 and MONK2 used in the detailed study of 25 machine learning algorithms <ref> (Thrun et al. 1991) </ref>. For both MONK1 and MONK2, the training set was the same as our original data set described above. The two test sets used in the study consisted of 432 examples that completely covered the attribute space. For MONK1, the accuracy of HINT is 100%. <p> For both MONK1 and MONK2, the training set was the same as our original data set described above. The two test sets used in the study consisted of 432 examples that completely covered the attribute space. For MONK1, the accuracy of HINT is 100%. In the study <ref> (Thrun et al. 1991) </ref>, this score was achieved by 9 learners: three variants of AQ17, Assistant Professional, mFOIL, CN2, two variants of Backpropagation, 0 20 40 60 80 100 60 100 * . . . . . . . . . . . . . . . . . . .
References-found: 19

