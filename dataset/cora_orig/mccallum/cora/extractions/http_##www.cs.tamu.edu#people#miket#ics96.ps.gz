URL: http://www.cs.tamu.edu/people/miket/ics96.ps.gz
Refering-URL: http://www.cs.tamu.edu/people/miket/
Root-URL: http://www.cs.tamu.edu
Email: fmiket,jcliug@cs.tamu.edu  
Title: An Efficient Steepest-Edge Simplex Algorithm for SIMD Computers  
Author: Michael E. Thomadakis and Jyh-Charn Liu 
Address: College Station, TX 77843-3112  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: This paper proposes a new implementation of the Primal and Dual Simplex algorithms for Linear Programming problems on massively parallel SIMD computers. The algorithms are based on the Steepest-Edge pivot selection method and the tableau representation of the constraint matrix. The parallel algorithm reduces communication overhead by maintaining local replicas of key portions of the tableau along with the sub-matrices on each one of the PEs. The Steepest-Edge method utilizes mainly local information to search for the next pivot element. Pivot rows and columns are efficiently broadcasted to PEs before pivot steps, utilizing the geometry of pipelined, toroidal mesh interconnection network. The proposed parallelization has optimal asymptotic speedup and scalability properties and experimental results show that as problem sizes increase the speedups obtained by MasPar's MP-1 and MP-2 models are in the order of 100 times, and 1000 times, respectively, over sequential Steepest-Edge Simplex running on high-end Unix workstations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agrawal, A., G. E. Blelloch, R. L. Krawitz, and C. A. Phillips, </author> <title> "Four Vector-Matrix Primitives," </title> <booktitle> in Proc. ACM Symp. on Parallel Alg. and Arch., </booktitle> <pages> pp. 292-302, </pages> <year> 1989. </year>
Reference-contexts: Their sparse simplex paralleliza-tion experienced high communication to computation ratio on the Touchstone machine due to strong data dependencies between PEs. Simplex algorithms for general LP problems on Single Instruction Mulitple Data (SIMD) computers have been reported by Agarwal et al. in <ref> [1] </ref>, and by Eckstein et al. in [5]. The early implementation in [1] had a disappointing performance. Eckstein et al. in [5] implemented prototypes of dense simplex and interior-point algorithms on a CM-2 machine. <p> Simplex algorithms for general LP problems on Single Instruction Mulitple Data (SIMD) computers have been reported by Agarwal et al. in <ref> [1] </ref>, and by Eckstein et al. in [5]. The early implementation in [1] had a disappointing performance. Eckstein et al. in [5] implemented prototypes of dense simplex and interior-point algorithms on a CM-2 machine. They concluded that interior point algorithms are relatively easy to implement on SIMD machines with commercially available library software.
Reference: [2] <author> Bazaraa, S., J. Jarvis and H. Sherali, </author> <title> Linear Programming and Network Flows, </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1990. </year>
Reference-contexts: The Simplex is the most widely used solution method for linear programming problems <ref> [2, 12, 14] </ref>. As in the solution of any large scale mathematical system, the computation time for large LP problems is a major concern. Recently, several researchers experimented with the parallelization of the Simplex algorithm, on vector, shared and distributed memory Multiple-Instruction Multiple-Data (MIMD) types of computer architectures.
Reference: [3] <author> Crowder, H. and J. M. Hatting, </author> <title> "Partially Normalized Pivot Selection in Linear Programming," </title> <journal> Math. Programming Study, </journal> <volume> Vol. 4, </volume> <pages> pp. 12-25, </pages> <publisher> North-Holland Publ. </publisher> <address> Comp. </address> <year> 1975. </year>
Reference-contexts: This 2 row x 0 i , column X j , and, element x ij for the scalar algorithm method has been employed in all previous simplex method parallelizations. Other pivot selection heuristics include the Bland's [14], the Least-Recently Considered [13], the maximum cost reduction [14], Partially Normalized <ref> [3] </ref>, the "DE-VEX" [8], and the Steepest-Edge [7] rules. We decided to use the Steepest-Edge, since according to our previous computational experience this method constantly guides simplex to take the fewest pivot steps before optimal solution, as compared to the other methods.
Reference: [4] <author> Dantzig, G. B., </author> <title> Linear Programming and Extensions, </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1963. </year>
Reference-contexts: Every LP problem in the general form can be expressed into an equivalent LP <ref> [4, 14] </ref> in the standard form. <p> The cost value z of the objective function at a BFS x is given by z = c B x B = c B b: It is well known <ref> [4, 14] </ref> that every BFS x 2 F is a vertex (i.e., a corner point) of convex polytope F , and that the optimal solution, denoted by x fl , is also a vertex of F . <p> From Eq. (4) it is clear that element x kl of the resulting tableau depends on elements x ij ; x il ; x kj , and x kl of the previous tableau. One of the most widely used pivot selection methods is the Dantzig's rule <ref> [4] </ref>, where simplex always selects as pivot column X j the one with the most negative reduced cost coefficient c j = minfc k &lt; 0 : k = 1; 2 : : : ; ng.
Reference: [5] <author> Eckstein, J., R. Qi, V. I. Ragulin and S. A. Zenios, </author> <title> "Data-Parallel Implementations of Dense Linear Programming Algorithms," </title> <type> Tech. Report, AH-PCRC Preprint 92-129, </type> <institution> U. of Minnesota, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Their sparse simplex paralleliza-tion experienced high communication to computation ratio on the Touchstone machine due to strong data dependencies between PEs. Simplex algorithms for general LP problems on Single Instruction Mulitple Data (SIMD) computers have been reported by Agarwal et al. in [1], and by Eckstein et al. in <ref> [5] </ref>. The early implementation in [1] had a disappointing performance. Eckstein et al. in [5] implemented prototypes of dense simplex and interior-point algorithms on a CM-2 machine. They concluded that interior point algorithms are relatively easy to implement on SIMD machines with commercially available library software. <p> Simplex algorithms for general LP problems on Single Instruction Mulitple Data (SIMD) computers have been reported by Agarwal et al. in [1], and by Eckstein et al. in <ref> [5] </ref>. The early implementation in [1] had a disappointing performance. Eckstein et al. in [5] implemented prototypes of dense simplex and interior-point algorithms on a CM-2 machine. They concluded that interior point algorithms are relatively easy to implement on SIMD machines with commercially available library software.
Reference: [6] <author> Finkel, R. A., </author> <title> "Large-Grain Parallelism: Three Case Studies," in The Characteristics of Parallel Algorithms, </title> <editor> Ed. L. H. Jamieson, </editor> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Recently, several researchers experimented with the parallelization of the Simplex algorithm, on vector, shared and distributed memory Multiple-Instruction Multiple-Data (MIMD) types of computer architectures. One of the earliest parallel tableau simplex methods on a small-scale distributed memory MIMD machines is by Finkel in <ref> [6] </ref>. His study showed that the overhead for distributing matrix elements for pivot operations, for finding minimum or maximum values among different PEs, and for inter-process synchronization, did not allow for any significant speedup.
Reference: [7] <author> Goldfarb, D. and J. K. Reid, </author> <title> "A Practicable Steepest-Edge Simplex Algorithm," </title> <journal> Mathematical Programming, </journal> <volume> Vol. 12, no. 3, </volume> <pages> pp. 361-371, </pages> <publisher> North-Holland Publ. </publisher> <address> Comp., </address> <month> Jun. </month> <year> 1977. </year>
Reference-contexts: Other pivot selection heuristics include the Bland's [14], the Least-Recently Considered [13], the maximum cost reduction [14], Partially Normalized [3], the "DE-VEX" [8], and the Steepest-Edge <ref> [7] </ref> rules. We decided to use the Steepest-Edge, since according to our previous computational experience this method constantly guides simplex to take the fewest pivot steps before optimal solution, as compared to the other methods.
Reference: [8] <author> Harris, P. M. J., </author> <title> "Pivot Selection Methods of the Devex LP Code," </title> <journal> Math. Programming Study Vol. </journal> <volume> 4, </volume> <publisher> North-Holland Publ. </publisher> <address> Comp., </address> <month> Dec. </month> <year> 1975. </year>
Reference-contexts: Other pivot selection heuristics include the Bland's [14], the Least-Recently Considered [13], the maximum cost reduction [14], Partially Normalized [3], the "DE-VEX" <ref> [8] </ref>, and the Steepest-Edge [7] rules. We decided to use the Steepest-Edge, since according to our previous computational experience this method constantly guides simplex to take the fewest pivot steps before optimal solution, as compared to the other methods.
Reference: [9] <author> Helgason, R. V., J. L. Kennington, and H. A. Zaki, </author> <title> "A Parallelization of the Simplex Method," </title> <journal> in Annals of Operations Research, </journal> <volume> Vol. 14, </volume> <pages> pp. 17-40, </pages> <year> 1988. </year>
Reference-contexts: They reported good scalability results on very small problem sizes. Stunkel [17] studied the performance of the tableau and the revised simplex on the iPSC/2 hypercube computer. Helgason et al. <ref> [9] </ref> discussed a way to implement the revised Simplex using sparse matrix methods on shared memory MIMD computer, without elaborating on actual implementation. Recently, Shu and Wu [15] and Shu [16] parallelized the explicit inverse and the LU decomposition of the basis simplex algorithms on iPSC/2 and Touchstone Delta computers.
Reference: [10] <author> Hwang, Kai, </author> <title> Advanced Computer Architecture: Parallelism, Scalability, Programmability, </title> <publisher> McGraw-Hill Inc., </publisher> <year> 1993. </year>
Reference: [11] <institution> MasPar Programming Language Reference Manual, MasPar Computer Corporation, </institution> <year> 1992. </year>
Reference: [12] <author> Murtagh, B. A., </author> <title> Advanced Linear Programming: Computation and Practice, </title> <publisher> McGraw-Hill Inc., </publisher> <year> 1981. </year>
Reference-contexts: The Simplex is the most widely used solution method for linear programming problems <ref> [2, 12, 14] </ref>. As in the solution of any large scale mathematical system, the computation time for large LP problems is a major concern. Recently, several researchers experimented with the parallelization of the Simplex algorithm, on vector, shared and distributed memory Multiple-Instruction Multiple-Data (MIMD) types of computer architectures.
Reference: [13] <author> Murty, K. G., </author> <title> Linear Programming, </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1983. </year>
Reference-contexts: This 2 row x 0 i , column X j , and, element x ij for the scalar algorithm method has been employed in all previous simplex method parallelizations. Other pivot selection heuristics include the Bland's [14], the Least-Recently Considered <ref> [13] </ref>, the maximum cost reduction [14], Partially Normalized [3], the "DE-VEX" [8], and the Steepest-Edge [7] rules. We decided to use the Steepest-Edge, since according to our previous computational experience this method constantly guides simplex to take the fewest pivot steps before optimal solution, as compared to the other methods.
Reference: [14] <author> Papadimitriou C. H. and K. Steiglitz, </author> <title> Combinatorial Optimization: Algorithms and Complexity, </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1982. </year>
Reference-contexts: The Simplex is the most widely used solution method for linear programming problems <ref> [2, 12, 14] </ref>. As in the solution of any large scale mathematical system, the computation time for large LP problems is a major concern. Recently, several researchers experimented with the parallelization of the Simplex algorithm, on vector, shared and distributed memory Multiple-Instruction Multiple-Data (MIMD) types of computer architectures. <p> Every LP problem in the general form can be expressed into an equivalent LP <ref> [4, 14] </ref> in the standard form. <p> The cost value z of the objective function at a BFS x is given by z = c B x B = c B b: It is well known <ref> [4, 14] </ref> that every BFS x 2 F is a vertex (i.e., a corner point) of convex polytope F , and that the optimal solution, denoted by x fl , is also a vertex of F . <p> This 2 row x 0 i , column X j , and, element x ij for the scalar algorithm method has been employed in all previous simplex method parallelizations. Other pivot selection heuristics include the Bland's <ref> [14] </ref>, the Least-Recently Considered [13], the maximum cost reduction [14], Partially Normalized [3], the "DE-VEX" [8], and the Steepest-Edge [7] rules. <p> This 2 row x 0 i , column X j , and, element x ij for the scalar algorithm method has been employed in all previous simplex method parallelizations. Other pivot selection heuristics include the Bland's <ref> [14] </ref>, the Least-Recently Considered [13], the maximum cost reduction [14], Partially Normalized [3], the "DE-VEX" [8], and the Steepest-Edge [7] rules. We decided to use the Steepest-Edge, since according to our previous computational experience this method constantly guides simplex to take the fewest pivot steps before optimal solution, as compared to the other methods.
Reference: [15] <author> Shu, Wei, and Min-You Wu, </author> <title> "Sparse Implementation of Revised Simplex Algorithms on Parallel Computers," </title> <booktitle> in Proc. of the Sixth SIAM Conf. on Par. Proc. for Sc. Comp., </booktitle> <volume> Vol. II, </volume> <pages> pp. 501-509, </pages> <year> 1993. </year>
Reference-contexts: Stunkel [17] studied the performance of the tableau and the revised simplex on the iPSC/2 hypercube computer. Helgason et al. [9] discussed a way to implement the revised Simplex using sparse matrix methods on shared memory MIMD computer, without elaborating on actual implementation. Recently, Shu and Wu <ref> [15] </ref> and Shu [16] parallelized the explicit inverse and the LU decomposition of the basis simplex algorithms on iPSC/2 and Touchstone Delta computers.
Reference: [16] <author> Shu, Wei, </author> <title> "Parallel Implementation of Sparse Simplex Algorithm," </title> <type> Manuscript, </type> <institution> Dept. of Computer Science , SUNY Buffalo, </institution> <year> 1994. </year>
Reference-contexts: Helgason et al. [9] discussed a way to implement the revised Simplex using sparse matrix methods on shared memory MIMD computer, without elaborating on actual implementation. Recently, Shu and Wu [15] and Shu <ref> [16] </ref> parallelized the explicit inverse and the LU decomposition of the basis simplex algorithms on iPSC/2 and Touchstone Delta computers. Both methods are very successful on uniprocessors, with the first being more suitable to LP problems with dense constraint matrices, and the second LP problems with sparse matrices.
Reference: [17] <author> Stunkel, C. B., </author> <title> "Linear Optimization via Message-based Parallel Processing," </title> <booktitle> Int'l Conf. on Par. Proc., </booktitle> <volume> Vol. III, </volume> <pages> pp. 264-271, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: They reported good scalability results on very small problem sizes. Stunkel <ref> [17] </ref> studied the performance of the tableau and the revised simplex on the iPSC/2 hypercube computer. Helgason et al. [9] discussed a way to implement the revised Simplex using sparse matrix methods on shared memory MIMD computer, without elaborating on actual implementation.
Reference: [18] <author> Wu, Y. and T. G. Lewis, </author> <title> "Performance of Parallel Simplex Algorithms," </title> <type> Tech. Report, </type> <institution> Dep't of Computer Science, Oregon State U., </institution> <year> 1988. </year> <month> 8 </month>
Reference-contexts: Wu Appeared in the 10th ACM International Conference on Supercomputing, pp. 286-293, hosted by the 2nd FCRC '96, May 20-28, 1996, Philadelphia, PA. and Lewis <ref> [18] </ref> presented two parallelizations of the revised simplex with explicit form of the basis inverse on a shared memory MIMD machine. They reported good scalability results on very small problem sizes. Stunkel [17] studied the performance of the tableau and the revised simplex on the iPSC/2 hypercube computer.
References-found: 18

