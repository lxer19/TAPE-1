URL: http://www.isi.edu/soar/jihie/papers/isi95-1.ps
Refering-URL: http://www.isi.edu/soar/jihie/chunking.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A Transformational Analysis of Expensive Chunks  
Author: Jihie Kim and Paul S. Rosenbloom 
Keyword: Key words: machine learning, utility problem, expensive chunks, Soar, EBL  
Address: 4676 Admiralty Way Marina del Rey, CA 90292, U.S.A.  
Affiliation: Information Sciences Institute and Computer Science Department University of Southern California  
Email: jihie@isi.edu, rosenbloom@isi.edu  
Phone: (310) 822-1510  
Web: (x769)  
Abstract: Many learning systems must confront the problem of run time after learning being greater than run time before learning. This utility problem has been a particular focus of research in explanation-based learning (EBL). This paper shows how the cost increase of a learned rule in an EBL system can be analyzed by characterizing the learning process as a sequence of transformations from a problem solving episode to a learned rule. The analysis of how the cost changes through the transformations can be a useful tool for revealing the sources of cost increase in the learning system. Once all of the sources are revealed, by avoiding these sources, the learned rule will never be expensive. That is, the cost of the learned rule will be bounded by the problem solving. We performed such a transformational analysis of chunking in Soar. The chunking process has been decomposed into a sequence of transformations from the problem solving to a chunk. By analyzing these transformations, we have identified a set of sources which can make the output chunk expensive. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. E. Prieditis and J. Mostow. Prolearn: </author> <title> Towards a prolog interpreter that learns. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 494-498, </pages> <year> 1987. </year> <month> 14 </month>
Reference-contexts: 1 Introduction Many learning systems must confront the problem of run time after learning being greater than run time before learning. This utility problem has been a particular focus of research in explanation-based learning (EBL). There have been approaches which are useful for producing cheaper rules <ref> [1, 2, 3, 4, 5, 6] </ref> or filtering out expensive rules [2, 7, 8, 9]. However, these approaches cannot generally guarantee that the cost of using the learned rules will always be bounded by the cost of the problem solving from which they are learned, given the same situation.
Reference: [2] <author> S. Minton. </author> <title> Quantitative results concerning the utility of explanation--based learning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 564-569, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Many learning systems must confront the problem of run time after learning being greater than run time before learning. This utility problem has been a particular focus of research in explanation-based learning (EBL). There have been approaches which are useful for producing cheaper rules <ref> [1, 2, 3, 4, 5, 6] </ref> or filtering out expensive rules [2, 7, 8, 9]. However, these approaches cannot generally guarantee that the cost of using the learned rules will always be bounded by the cost of the problem solving from which they are learned, given the same situation. <p> This utility problem has been a particular focus of research in explanation-based learning (EBL). There have been approaches which are useful for producing cheaper rules [1, 2, 3, 4, 5, 6] or filtering out expensive rules <ref> [2, 7, 8, 9] </ref>. However, these approaches cannot generally guarantee that the cost of using the learned rules will always be bounded by the cost of the problem solving from which they are learned, given the same situation. <p> best alternative among the candidate values, given the identifier G1 and the attribute object. 4 In addition, architectural actions that occurred during the problem solving episode are replaced in the PS-rule by dummy rules that have the same effect, much in the way that architectural axioms are used in Prodigy/EBL <ref> [2] </ref>. 7 processing differs significantly from the initial problem solving by being closed off from intermediate WMEs generated outside of this structure. 5 For example, the link between R3 and R4 through W22 means that no other WMEs except for those created by R3 are matched to the condition of R4.
Reference: [3] <author> P. Shell and J. Carbonell. </author> <title> Empirical and analytical performance of iterative operators. </title> <booktitle> In The 13th Annual Conference of The Cognitive Science Society, </booktitle> <pages> pages 898-902. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Many learning systems must confront the problem of run time after learning being greater than run time before learning. This utility problem has been a particular focus of research in explanation-based learning (EBL). There have been approaches which are useful for producing cheaper rules <ref> [1, 2, 3, 4, 5, 6] </ref> or filtering out expensive rules [2, 7, 8, 9]. However, these approaches cannot generally guarantee that the cost of using the learned rules will always be bounded by the cost of the problem solving from which they are learned, given the same situation.
Reference: [4] <author> Jude W. Shavlik. </author> <title> Aquiring recursive and iterative concepts with explanation-based learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 39-70, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Many learning systems must confront the problem of run time after learning being greater than run time before learning. This utility problem has been a particular focus of research in explanation-based learning (EBL). There have been approaches which are useful for producing cheaper rules <ref> [1, 2, 3, 4, 5, 6] </ref> or filtering out expensive rules [2, 7, 8, 9]. However, these approaches cannot generally guarantee that the cost of using the learned rules will always be bounded by the cost of the problem solving from which they are learned, given the same situation.
Reference: [5] <author> O. Etzioni. </author> <title> Why prodigy/ebl works. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 916-922, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Many learning systems must confront the problem of run time after learning being greater than run time before learning. This utility problem has been a particular focus of research in explanation-based learning (EBL). There have been approaches which are useful for producing cheaper rules <ref> [1, 2, 3, 4, 5, 6] </ref> or filtering out expensive rules [2, 7, 8, 9]. However, these approaches cannot generally guarantee that the cost of using the learned rules will always be bounded by the cost of the problem solving from which they are learned, given the same situation.
Reference: [6] <author> M. Tambe. </author> <title> Eliminating combinatorics from production match. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1991. </year>
Reference-contexts: 1 Introduction Many learning systems must confront the problem of run time after learning being greater than run time before learning. This utility problem has been a particular focus of research in explanation-based learning (EBL). There have been approaches which are useful for producing cheaper rules <ref> [1, 2, 3, 4, 5, 6] </ref> or filtering out expensive rules [2, 7, 8, 9]. However, these approaches cannot generally guarantee that the cost of using the learned rules will always be bounded by the cost of the problem solving from which they are learned, given the same situation. <p> Beta memories store partial instantiations of productions, that is, instantiations of initial subsequences of conditions. The partial instantiations are called tokens. Because match time per token is known to be approximately constant in Rete <ref> [17, 6] </ref> | and because counting tokens yields a measure that is independent of machines, optimizations, and implementation details | we will follow the standard practice established within the match-algorithm community and use the number of tokens, rather than time, as our comparative measure of match cost. 3 Transforming problem solving <p> At each stage from problem solving to chunks, match cost is evaluated by counting the number of tokens required during the match to generate the result. So far, the resulting experimental system has been applied to a simple Grid-task problem <ref> [6] </ref> which creates one subgoal to break a tie (impasse) among the candidate operators and creates a chunk. The results of this experiment are shown in table 1.
Reference: [7] <author> R. Greiner and I. Jurisica. </author> <title> A statistical approach to solving the ebl utility problem. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 241-248, </pages> <year> 1992. </year>
Reference-contexts: This utility problem has been a particular focus of research in explanation-based learning (EBL). There have been approaches which are useful for producing cheaper rules [1, 2, 3, 4, 5, 6] or filtering out expensive rules <ref> [2, 7, 8, 9] </ref>. However, these approaches cannot generally guarantee that the cost of using the learned rules will always be bounded by the cost of the problem solving from which they are learned, given the same situation.
Reference: [8] <author> J. Gratch and G. Dejong. Composer: </author> <title> A probabilistic solution to the utility problem in speed-up learning. </title> <booktitle> In Proceedings of the Tenth National Conference on Aritificial Intelligence, </booktitle> <pages> pages 235-240, </pages> <year> 1992. </year>
Reference-contexts: This utility problem has been a particular focus of research in explanation-based learning (EBL). There have been approaches which are useful for producing cheaper rules [1, 2, 3, 4, 5, 6] or filtering out expensive rules <ref> [2, 7, 8, 9] </ref>. However, these approaches cannot generally guarantee that the cost of using the learned rules will always be bounded by the cost of the problem solving from which they are learned, given the same situation.
Reference: [9] <author> S. Markovitch and P. D. Scott. </author> <title> Information filtering : Selection mecha nism in learning systems. </title> <journal> Machine Learning, </journal> <volume> 10(2) </volume> <pages> 113-151, </pages> <year> 1993. </year>
Reference-contexts: This utility problem has been a particular focus of research in explanation-based learning (EBL). There have been approaches which are useful for producing cheaper rules [1, 2, 3, 4, 5, 6] or filtering out expensive rules <ref> [2, 7, 8, 9] </ref>. However, these approaches cannot generally guarantee that the cost of using the learned rules will always be bounded by the cost of the problem solving from which they are learned, given the same situation.
Reference: [10] <author> P. S. Rosenbloom, J. E. Laird, A. Newell, and R. McCarl. </author> <title> A preliminary analysis of the soar architecture as a basis for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 47(1-3):289-325, </volume> <year> 1991. </year>
Reference-contexts: Analyses of these transformations then point out where extra cost is being added, and guide the proposal of alternatives that do not introduce such added costs. The focus of the analysis in this paper is chunking in Soar <ref> [10] </ref>. Soar is an architecture that combines general problem solving abilities with a chunking mechanism that is a variant of explanation-based learning [11]. <p> However, chunking employs only traces from task-definition rules; that is, rules that directly propose values of WMEs. Search-control rules, as distinguished from task-definition rules, suggest the relative worth of the proposed values. The search-control rules are missing in chunking <ref> [18, 10] </ref> (and other EBL systems [19]) based on the assumption that they only affect the efficiency, not the correctness of learned rules.
Reference: [11] <author> P. S. Rosenbloom and J. E. Laird. </author> <title> Mapping explanation-based generalization onto soar. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 561-567, </pages> <address> Philadelphia, 1986. </address> <publisher> AAAI. </publisher>
Reference-contexts: The focus of the analysis in this paper is chunking in Soar [10]. Soar is an architecture that combines general problem solving abilities with a chunking mechanism that is a variant of explanation-based learning <ref> [11] </ref>. <p> In addition, a parallel analysis of EBL and chunking should further clarify the relationship between the two. An earlier comparison related the four basic structural components (goal concept, domain theory, training example, operationality criterion) of the two systems <ref> [11] </ref>; however, a transformational analysis should allow us to go beyond this to a deeper analysis of the processes underlying the two algorithms.
Reference: [12] <author> J. Kim and P. S. Rosenbloom. </author> <title> Constraining learning with search control. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 174-181, </pages> <year> 1993. </year>
Reference-contexts: In the context of characterizing learning systems as a sequence of transformations, our prior work has revealed one source of added expensiveness: in chunking (and other EBL systems which use search control in the problem solving), eliminating search control in learning can increase the cost of the learned rules <ref> [12] </ref>. The critical consequence of the elimination of search control is that the learned rules are not constrained by the path actually taken in the problem space, and thus can perform an exponential amount of search even when the original problem-space search was highly directed (by the control rules). <p> The consequence of eliminating search control is that the E-chunk is not constrained by the path actually taken in the problem space, and thus can perform an exponential amount of search even when the original problem-space search was highly directed (by the control rules), as analyzed in <ref> [12] </ref>. In the above example, without constraining eval-operator to the best candidate | which has priority 1 | the number of tokens in the match of rule R3 increases from 7 to 14. Overall, the total number of tokens increases from 17 to 20. <p> Overall, the total number of tokens increases from 17 to 20. This is thus one of the star-marked (i.e., cost increasing) transformations in One promising way of avoiding this problem is to incorporate search control in chunking <ref> [12] </ref>. By incorporating search control in the explanation structure, the match process for the learned rule can focus on the path that was actually followed. 3.3 Constraining variables by instantiations ()I-chunk) The variabilization step in chunking is performed by examining the back-trace (explanation).
Reference: [13] <author> A. Segre and C. Elkan. </author> <title> A high-performance explanation-based learning algorithm. </title> <journal> Artificial Intelligence, </journal> <volume> 69 </volume> <pages> 1-50, </pages> <year> 1994. </year>
Reference-contexts: This analysis was based on one step (removal of search control) among the whole sequence of transformations. To reveal all sources of additional cost, we need a complete analysis of the whole sequence of transformations. This approach is similar in spirit to <ref> [13] </ref> in its use of a transformational analysis of the learning algorithm. However, the focus of their analysis and resulting algorithm development was on speedup rather than on boundedness, and on search-control-free EBL rather than on chunking. Section 2 of this article briefly reviews chunking in Soar.
Reference: [14] <author> T. M. Mitchell, R. M. Keller, and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization a unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Section 4 presents preliminary experimental results backing up the analysis in Section 3. Finally, Section 5 summarizes and discusses issues for future work. 2 2 Background In Soar, productions comprise the domain theory for EBL <ref> [14, 15] </ref>. Each production consists of a set of conditions and a set of actions. Conditions test working memory for the presence or absence of patterns of tuples, where each tuple consists of an object identifier, an attribute and a value.
Reference: [15] <author> G. F. DeJong and R. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: Section 4 presents preliminary experimental results backing up the analysis in Section 3. Finally, Section 5 summarizes and discusses issues for future work. 2 2 Background In Soar, productions comprise the domain theory for EBL <ref> [14, 15] </ref>. Each production consists of a set of conditions and a set of actions. Conditions test working memory for the presence or absence of patterns of tuples, where each tuple consists of an object identifier, an attribute and a value.
Reference: [16] <author> C. L. Forgy. </author> <title> Rete: A fast algorithm for the many pattern/many object pattern match problem. </title> <journal> Artificial Intelligence, </journal> <volume> 19(1) </volume> <pages> 17-37, </pages> <year> 1982. </year>
Reference-contexts: mean just the match cost of all of the rules that fired to generate the result (whether this be via multiple rules during the initial problem solving, or via a single chunk). 1 Because computing match cost is dependent on the match algorithm used, we briefly review the Rete algorithm <ref> [16] </ref> employed in Soar. Rete is one of the most efficient rule-match algorithms presently known. Its efficiency stems primarily from two key optimizations: sharing and state saving. Sharing of common conditions in a production, or across a set of productions, reduces the number of tests performed during match.
Reference: [17] <author> M. Tambe, D. Kalp, A. Gupta, C. L. Forgy, B. G. Milnes, and A. Newell. Soar/psm-e: </author> <title> Investigating match parallelism in a learning production sys tem. </title> <booktitle> In Proceedings of the ACM/SIGPLAN Symposium on Parallel Programming: Experience with applications, languages, and systems, </booktitle> <pages> pages 146-160, </pages> <year> 1988. </year>
Reference-contexts: Beta memories store partial instantiations of productions, that is, instantiations of initial subsequences of conditions. The partial instantiations are called tokens. Because match time per token is known to be approximately constant in Rete <ref> [17, 6] </ref> | and because counting tokens yields a measure that is independent of machines, optimizations, and implementation details | we will follow the standard practice established within the match-algorithm community and use the number of tokens, rather than time, as our comparative measure of match cost. 3 Transforming problem solving
Reference: [18] <author> J. E. Laird, P. S. Rosenbloom, and A. Newell. </author> <title> Overgeneralization dur-ing knowledge compilation in soar. </title> <booktitle> In Proceedings of the Workshop on Knowledge Compilation, </booktitle> <pages> pages 46-57, </pages> <year> 1986. </year>
Reference-contexts: However, chunking employs only traces from task-definition rules; that is, rules that directly propose values of WMEs. Search-control rules, as distinguished from task-definition rules, suggest the relative worth of the proposed values. The search-control rules are missing in chunking <ref> [18, 10] </ref> (and other EBL systems [19]) based on the assumption that they only affect the efficiency, not the correctness of learned rules.
Reference: [19] <author> S. Minton. </author> <type> Personal communication. </type> <year> 1993. </year> <month> 16 </month>
Reference-contexts: However, chunking employs only traces from task-definition rules; that is, rules that directly propose values of WMEs. Search-control rules, as distinguished from task-definition rules, suggest the relative worth of the proposed values. The search-control rules are missing in chunking [18, 10] (and other EBL systems <ref> [19] </ref>) based on the assumption that they only affect the efficiency, not the correctness of learned rules.
References-found: 19

