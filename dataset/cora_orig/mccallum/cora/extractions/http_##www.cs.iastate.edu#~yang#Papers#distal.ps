URL: http://www.cs.iastate.edu/~yang/Papers/distal.ps
Refering-URL: http://www.cs.iastate.edu/~yang/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fyang|parekh|honavarg@cs.iastate.edu  
Title: DistAl: An Inter-pattern Distance-based Constructive Learning Algorithm  
Author: Jihoon Yang, Rajesh Parekh and Vasant Honavar 
Keyword: neural networks, constructive learning algorithms, pattern classification  
Note: This research was partially supported by the National Science Foundation (through grant IRI-9409580) and the John Deere Foundation.  
Address: 226 Atanasoff Hall  Ames, IA 50011. U.S.A.  
Affiliation: Artificial Intelligence Research Group Department of Computer Science  Iowa State University  
Abstract: Multi-layer networks of threshold logic units offer an attractive framework for the design of pattern classification systems. A new constructive neural network learning algorithm (DistAl) based on inter-pattern distance is introduced. DistAl constructs a single hidden layer of hyperspherical threshold neurons. Each neuron is designed to exclude a cluster of training patterns belonging to the same class. The weights and thresholds of the hidden neurons are determined directly by comparing the inter-pattern distances of the training patterns. This offers a significant advantage over other constructive learning algorithms that use an iterative (and often time consuming) weight modification strategy to train individual neurons. The individual clusters (represented by the hidden neurons) are combined by a single output layer of threshold neurons. The speed of DistAl makes it a good candidate for datamining and knowledge acquisition from very large datasets. The paper presents results of experiments using several artificial and real-world datasets. The results demonstrate that DistAl compares favorably with other neural network learning algorithms for pattern classification. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy. </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference: [2] <author> V. Honavar. </author> <title> Machine learning: Principles and applications. </title> <editor> In J. Webster, editor, </editor> <booktitle> Encyclopedia of Electrical and Electronics Engineering. </booktitle> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1998. </year> <note> To appear. </note>
Reference: [3] <author> J. Bradshaw. </author> <title> Software Agents. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1997. </year>
Reference: [4] <author> V. Honavar. </author> <title> Intelligent agents. </title> <editor> In J. Williams and K. Sochats, editors, </editor> <booktitle> Encyclopedia of Information Technology. </booktitle> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1998. </year> <note> To appear. </note>
Reference: [5] <author> K. Balakrishnan and V. Honavar. </author> <title> Intelligent diagnosis systems. </title> <journal> International Journal of Intelligent Systems, </journal> <note> 1998. In press. </note>
Reference: [6] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Different distance metrics represent different 6 notions of distance in the pattern space. They also impose different inductive biases [7, 8] on the learning algorithm. Consequently, many researchers have investigated the use of alternative distance functions for instance-based learning <ref> [6, 44, 52, 56, 57] </ref>. The number and distribution of the clusters that result from specific choices of distance functions is a function of the distribution of the patterns as well as the clustering strategy used.
Reference: [7] <author> T. Mitchell. </author> <title> Machine Learning. </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1997. </year> <month> 23 </month>
Reference-contexts: The choice of an appropriate distance metric for the hidden layer neurons is critical to achieving a good performance. Different distance metrics represent different 6 notions of distance in the pattern space. They also impose different inductive biases <ref> [7, 8] </ref> on the learning algorithm. Consequently, many researchers have investigated the use of alternative distance functions for instance-based learning [6, 44, 52, 56, 57]. <p> The larger the search space, the more time the learning algorithm needs for learning a sufficiently accurate classification function <ref> [7, 58] </ref>. * number of examples needed: All other things being equal, the larger the number of attributes used to describe the patterns, the larger is the number of examples need to learn a classification function to a desired accuracy [7, 58]. * cost of classification: In many real-world pattern classification <p> learning algorithm needs for learning a sufficiently accurate classification function <ref> [7, 58] </ref>. * number of examples needed: All other things being equal, the larger the number of attributes used to describe the patterns, the larger is the number of examples need to learn a classification function to a desired accuracy [7, 58]. * cost of classification: In many real-world pattern classification tasks (e.g., medical diagnosis), some of the attributes may be observable symptoms and others might require diagnostic tests. Different diagnostic tests might have different costs as well as risks associated with them.
Reference: [8] <author> P. Langley. </author> <title> Elements of Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1995. </year>
Reference-contexts: The choice of an appropriate distance metric for the hidden layer neurons is critical to achieving a good performance. Different distance metrics represent different 6 notions of distance in the pattern space. They also impose different inductive biases <ref> [7, 8] </ref> on the learning algorithm. Consequently, many researchers have investigated the use of alternative distance functions for instance-based learning [6, 44, 52, 56, 57].
Reference: [9] <author> V. Honavar. </author> <title> Toward learning systems that integrate multiple strategies and representations. </title> <editor> In V. Honavar and L. Uhr, editors, </editor> <booktitle> Artificial Intelligence and Neural Networks: Steps Toward Principled Integration, </booktitle> <pages> pages 615-644. </pages> <publisher> Academic Press: </publisher> <address> New York., </address> <year> 1994. </year>
Reference: [10] <author> C-H. Chen, R. Parekh, J. Yang, K. Balakrishnan, and V. Honavar. </author> <title> Analysis of decision boundaries generated by constructive neural network learning algorithms. </title> <booktitle> In Proceedings of WCNN'95, </booktitle> <address> July 17-21, Washington D.C., </address> <booktitle> volume 1, </booktitle> <pages> pages 628-635, </pages> <year> 1995. </year>
Reference: [11] <author> S. Gallant. </author> <title> Neural Network Learning and Expert Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference: [12] <author> R. Parekh, J. Yang, and V. Honavar. </author> <title> Constructive neural network learning algorithms for multi-category real-valued pattern classification. </title> <type> Technical Report ISU-CS-TR97-06, </type> <institution> Department of Computer Science, Iowa State University, </institution> <year> 1997. </year> <note> (Submitted for review to the IEEE Transactions on Neural Networks). </note>
Reference: [13] <author> R. Parekh, J. Yang, and V. Honavar. </author> <title> MUpstart a constructive neural network learning algorithm for multi-category pattern classification. </title> <booktitle> In Proceedings of the IEEE/INNS International Conference on Neural Networks, ICNN'97, </booktitle> <pages> pages 1924-1929, </pages> <year> 1997. </year>
Reference: [14] <author> J. Yang, R. Parekh, and V. Honavar. </author> <title> MTiling a constructive neural network learning algorithm for multi-category pattern classification. </title> <booktitle> In Proceedings of the World Congress on Neural Networks'96, </booktitle> <pages> pages 182-187, </pages> <address> San Diego, </address> <year> 1996. </year>
Reference: [15] <author> V. Honavar. </author> <title> Structural learning. </title> <editor> In J. Webster, editor, </editor> <booktitle> Encyclopedia of Electrical and Electronics Engineering. </booktitle> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1998. </year> <note> To appear. </note>
Reference: [16] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. In Parallel Distributed Processing: Explorations into the Microstructure of Cognition, </title> <booktitle> volume 1 (Foundations). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year> <month> 24 </month>
Reference: [17] <author> P. Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year>
Reference: [18] <author> F. Rosenblatt. </author> <title> The perceptron: A probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> 65 </volume> <pages> 386-408, </pages> <year> 1958. </year>
Reference: [19] <author> N. Nilsson. </author> <title> The Mathematical Foundations of Learning Machines. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1965. </year>
Reference: [20] <author> W. Krauth and M. Mezard. </author> <title> Learning algorithms with optimal stability in neural networks. </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> 20:L745-L752, </volume> <year> 1987. </year>
Reference: [21] <author> J. Anlauf and M. Biehl. </author> <title> Properties of an adaptive perceptron algorithm. </title> <booktitle> In Parallel Processing in Neural Systems and Computers, </booktitle> <pages> pages 153-156. </pages> <year> 1990. </year>
Reference: [22] <author> M. Frean. </author> <title> Small Nets and Short Paths: Optimizing Neural Computation. </title> <type> PhD thesis, </type> <institution> Center for Cognitive Science, Edinburgh University, UK, </institution> <year> 1990. </year>
Reference: [23] <author> H. Poulard. </author> <title> Barycentric correction procedure: A fast method of learning threshold units. </title> <booktitle> In Proceedings of WCNN'95, </booktitle> <address> July 17-21, Washington D.C., </address> <booktitle> volume 1, </booktitle> <pages> pages 710-713, </pages> <year> 1995. </year>
Reference: [24] <author> B. Raffin and M. Gordon. </author> <title> Learning and generalization with minimerror, a temperature-dependent learning algorithm. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 1206-1224, </pages> <year> 1995. </year>
Reference: [25] <author> R. Reed. </author> <title> Pruning algorithms | a survey. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(5) </volume> <pages> 740-747, </pages> <year> 1993. </year>
Reference: [26] <author> R. Parekh, J. Yang, and V. Honavar. </author> <title> Pruning strategies for constructive neural network learning algorithms. </title> <booktitle> In Proceedings of the IEEE/INNS International Conference on Neural Networks, ICNN'97, </booktitle> <pages> pages 1960-1965, </pages> <year> 1997. </year>
Reference: [27] <author> V. Honavar. </author> <title> Generative Learning Structures and Processes for Generalized Connectionist Networks. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, </institution> <year> 1990. </year> <month> 25 </month>
Reference: [28] <author> V. Honavar and Uhr. L. </author> <title> Generative learning structures for generalized connectionist networks. </title> <journal> Information Sciences, </journal> <volume> 70(1-2):75-108, </volume> <year> 1993. </year>
Reference: [29] <author> R. Parekh and V. Honavar. </author> <title> Constructive theory refinement in knowledge based neural networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Anchorage, Alaska, </address> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: Some preliminary results based on experiments using DistAl to design mobile agents for text classification and retrieval from distributed document collections are reported in [64]. Constructive algorithms in general provide an natural framework for exploration of cumulative (life long) learning [70] and for knowledge-based theory refinement <ref> [29, 71] </ref>. An interesting direction for future research would be to explore the use of DistAl for this task using real-world datasets e.g., the genome data used in [29]. <p> Constructive algorithms in general provide an natural framework for exploration of cumulative (life long) learning [70] and for knowledge-based theory refinement [29, 71]. An interesting direction for future research would be to explore the use of DistAl for this task using real-world datasets e.g., the genome data used in <ref> [29] </ref>.
Reference: [30] <author> J. Nadal. </author> <title> Study of a growth algorithm for a feedforward network. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(1) </volume> <pages> 55-59, </pages> <year> 1989. </year>
Reference: [31] <author> S. Gallant. </author> <title> Perceptron based learning algorithms. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 179-191, </pages> <month> June </month> <year> 1990. </year>
Reference: [32] <author> M. Mezard and J. Nadal. </author> <title> Learning feed-forward networks: The tiling algorithm. </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> 22 </volume> <pages> 2191-2203, </pages> <year> 1989. </year>
Reference: [33] <author> M. Frean. </author> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209, </pages> <year> 1990. </year>
Reference-contexts: In case of Vowel dataset, the nearest neighbor algorithm [57] reports 20 a even higher accuracy than DistAl 5 The network size of three algorithms (perceptron cascade [34], cascade correlation [67], upstart <ref> [33] </ref>) for the two spirals problem is shown in [34]: 17.8 (perceptron cascade), 15.2 (cascade correlation), 91.4 (upstart). DistAl generated more compact networks with 7.7 hidden neurons. Table 5 shows that the combination of DistAl and feature subset selection yield fairly good results.
Reference: [34] <author> N. Burgess. </author> <title> A constructive algorithm that converges for real-valued input patterns. </title> <journal> International Journal of Neural Systems, </journal> <volume> 5(1) </volume> <pages> 59-66, </pages> <year> 1994. </year>
Reference-contexts: As we can see from Table 4, DistAl gave comparable results on most datasets (except Soylarge and Vowel). In case of Vowel dataset, the nearest neighbor algorithm [57] reports 20 a even higher accuracy than DistAl 5 The network size of three algorithms (perceptron cascade <ref> [34] </ref>, cascade correlation [67], upstart [33]) for the two spirals problem is shown in [34]: 17.8 (perceptron cascade), 15.2 (cascade correlation), 91.4 (upstart). DistAl generated more compact networks with 7.7 hidden neurons. Table 5 shows that the combination of DistAl and feature subset selection yield fairly good results. <p> In case of Vowel dataset, the nearest neighbor algorithm [57] reports 20 a even higher accuracy than DistAl 5 The network size of three algorithms (perceptron cascade <ref> [34] </ref>, cascade correlation [67], upstart [33]) for the two spirals problem is shown in [34]: 17.8 (perceptron cascade), 15.2 (cascade correlation), 91.4 (upstart). DistAl generated more compact networks with 7.7 hidden neurons. Table 5 shows that the combination of DistAl and feature subset selection yield fairly good results.
Reference: [35] <author> M. Marchand, M. Golea, and P. Rujan. </author> <title> A convergence theorem for sequential learning in two-layer perceptrons. </title> <journal> Europhysics Letters, </journal> <volume> 11(6) </volume> <pages> 487-492, </pages> <year> 1990. </year>
Reference-contexts: Then, they are fully connected to M output TLUs (1 for each output class) in an output layer. The representation of the patterns at the hidden layer is linearly separable <ref> [35] </ref>. Thus, an iterative perceptron learning rule can be used to train the output weights. However, the output weights can be directly set as follows: The weights between output and hidden neurons are chosen such that each hidden neuron overwhelms the effect of the hidden neurons generated later.
Reference: [36] <author> J. Yang and V. Honavar. </author> <title> Feature subset selection using a genetic algorithm. </title> <booktitle> In Proceedings of the Genetic Programming Conference, GP'97, </booktitle> <pages> pages 380-385, </pages> <address> Stanford University, CA, </address> <year> 1997. </year>
Reference-contexts: Genetic algorithms [59, 60, 61] offer a particularly promising approach to feature subset selection for a number of reasons <ref> [36, 37, 38] </ref>: * They do not have to rely on the often unrealistic monotonicity assumption. * They are particularly effective tools for exploring large search spaces for near-optimal solutions [59, 60, 61].
Reference: [37] <author> J. Yang and V. Honavar. </author> <title> Feature subset selection using a genetic algorithm. IEEE Expert (Sepcial Issue on Feature Transformation and Subset Selection), </title> <note> 1998. To appear. 26 </note>
Reference-contexts: The interested reader is referred to <ref> [37, 38] </ref> for discussion of a variety of alternative approaches to feature subset selection. <p> Genetic algorithms [59, 60, 61] offer a particularly promising approach to feature subset selection for a number of reasons <ref> [36, 37, 38] </ref>: * They do not have to rely on the often unrealistic monotonicity assumption. * They are particularly effective tools for exploring large search spaces for near-optimal solutions [59, 60, 61]. <p> Experiments were run using the rank-based selection strategy with the following parameter settings: Population size is 50; Number of generation is 300; The probability of crossover is 0.5; The probability of mutation is 0.01; The probability of selection of the highest ranked individual is 0.6. (See <ref> [37, 38] </ref> for detailed explanations on the experiments). 3 Experimental Evaluation of DistAl This section presents results of experiments using DistAl on several benchmark problems both with and without feature subset selection and compares them with the results presented by Wilson and Martinez in a recent paper [57]. <p> The results indicate that the networks constructed using GA-selected subset of features compare quite favorably with networks that use all of the features. In particular, feature subset selection resulted in significant improvement in generalization. For detailed explanation of implementation, related work and comparisons with other approaches see <ref> [37, 38] </ref>. 3.2.3 Document Datasets The same experimental setup was used as in Section 3.2.2. Table 6 shows that DistAl gives fairly good results for document classification as well. It gave reasonably high (over 80%) generalization accuracy for all datasets. <p> Consequently, its classification accuracy can be further improved by incorporating a suitable feature subset selection algorithm. This is borne out by the experiments using DistAl in conjunction with a genetic algorithm for feature subset selection <ref> [37, 38] </ref>. A potential disadvantage of DistAl is its need for maintaining the inter-pattern distance matrix during learning. The memory needed to store this matrix grows quadratically with the size of the training set.
Reference: [38] <author> J. Yang and V. Honavar. </author> <title> Feature subset selection using a genetic algorithm. In Feature Extraction, Construction and Selection A Data Mining Perspective. </title> <publisher> Kluwer: </publisher> <address> New York, </address> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: The interested reader is referred to <ref> [37, 38] </ref> for discussion of a variety of alternative approaches to feature subset selection. <p> Genetic algorithms [59, 60, 61] offer a particularly promising approach to feature subset selection for a number of reasons <ref> [36, 37, 38] </ref>: * They do not have to rely on the often unrealistic monotonicity assumption. * They are particularly effective tools for exploring large search spaces for near-optimal solutions [59, 60, 61]. <p> Experiments were run using the rank-based selection strategy with the following parameter settings: Population size is 50; Number of generation is 300; The probability of crossover is 0.5; The probability of mutation is 0.01; The probability of selection of the highest ranked individual is 0.6. (See <ref> [37, 38] </ref> for detailed explanations on the experiments). 3 Experimental Evaluation of DistAl This section presents results of experiments using DistAl on several benchmark problems both with and without feature subset selection and compares them with the results presented by Wilson and Martinez in a recent paper [57]. <p> The results indicate that the networks constructed using GA-selected subset of features compare quite favorably with networks that use all of the features. In particular, feature subset selection resulted in significant improvement in generalization. For detailed explanation of implementation, related work and comparisons with other approaches see <ref> [37, 38] </ref>. 3.2.3 Document Datasets The same experimental setup was used as in Section 3.2.2. Table 6 shows that DistAl gives fairly good results for document classification as well. It gave reasonably high (over 80%) generalization accuracy for all datasets. <p> Consequently, its classification accuracy can be further improved by incorporating a suitable feature subset selection algorithm. This is borne out by the experiments using DistAl in conjunction with a genetic algorithm for feature subset selection <ref> [37, 38] </ref>. A potential disadvantage of DistAl is its need for maintaining the inter-pattern distance matrix during learning. The memory needed to store this matrix grows quadratically with the size of the training set.
Reference: [39] <author> D. Aha. </author> <title> Incremental constructive induction: An instance-based approach. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 117-121, </pages> <address> Evanston, IL, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [40] <author> D. Aha, D. Kibler, and M. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference: [41] <author> P. Turney. </author> <title> Theoretical analyses of cross-validation error and voting in instance-based learning. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <pages> pages 331-360, </pages> <year> 1994. </year>
Reference: [42] <author> P. Domingos. </author> <title> Rule induction and instance-based learning: A unified approach. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <year> 1995. </year>
Reference: [43] <author> T. Cover and P. Hart. </author> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13 </volume> <pages> 21-27, </pages> <year> 1967. </year>
Reference: [44] <author> E. Diday. </author> <title> Recent progress in distance and similarity measures in pattern recognition. </title> <booktitle> In Proceedings of the Second International Joint Conference on Pattern Recognition, </booktitle> <pages> pages 534-539, </pages> <year> 1974. </year>
Reference-contexts: Different distance metrics represent different 6 notions of distance in the pattern space. They also impose different inductive biases [7, 8] on the learning algorithm. Consequently, many researchers have investigated the use of alternative distance functions for instance-based learning <ref> [6, 44, 52, 56, 57] </ref>. The number and distribution of the clusters that result from specific choices of distance functions is a function of the distribution of the patterns as well as the clustering strategy used.
Reference: [45] <author> B. Dasarathy. </author> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techiniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference: [46] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <year> 1986. </year> <month> 27 </month>
Reference: [47] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10(1) </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference: [48] <author> J. Kolodner. </author> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <year> 1993. </year>
Reference: [49] <author> G. Carpenter and S. Grossberg. </author> <title> Pattern Recognition by Self-Organizing Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference: [50] <author> A. Tversky. </author> <title> Features of similiarity. </title> <journal> Psychological Review, </journal> <volume> 84 </volume> <pages> 327-352, </pages> <year> 1977. </year>
Reference: [51] <author> R. Nosofsky. </author> <title> Attention, similarity, and the identification-categorization relationship. </title> <journal> Journal of Experimental Psychology: General, </journal> <volume> 115 </volume> <pages> 39-57, </pages> <year> 1986. </year>
Reference: [52] <author> G. Salton and M. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Different distance metrics represent different 6 notions of distance in the pattern space. They also impose different inductive biases [7, 8] on the learning algorithm. Consequently, many researchers have investigated the use of alternative distance functions for instance-based learning <ref> [6, 44, 52, 56, 57] </ref>. The number and distribution of the clusters that result from specific choices of distance functions is a function of the distribution of the patterns as well as the clustering strategy used.
Reference: [53] <author> D. Broomhead and D. Lowe. </author> <title> Multivariable functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 321-355, </pages> <year> 1988. </year>
Reference: [54] <author> M. Powell. </author> <title> Radial basis functions for multivariable interpolation: A review. </title> <editor> In J. Mason and M. Cox, editors, </editor> <booktitle> Algorithms for Approximation, </booktitle> <pages> pages 143-167. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1987. </year>
Reference: [55] <author> F. Girosi, M. Jones, and T. Poggio. </author> <title> Regularization theory and neural networks architectures. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 219-269, </pages> <year> 1995. </year>
Reference: [56] <author> B. Batchelor. </author> <title> Pattern Recognition: Ideas in Practice. </title> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Different distance metrics represent different 6 notions of distance in the pattern space. They also impose different inductive biases [7, 8] on the learning algorithm. Consequently, many researchers have investigated the use of alternative distance functions for instance-based learning <ref> [6, 44, 52, 56, 57] </ref>. The number and distribution of the clusters that result from specific choices of distance functions is a function of the distribution of the patterns as well as the clustering strategy used.
Reference: [57] <author> D. Wilson and T. Martinez. </author> <title> Improved heterogeneous distance functions. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 6 </volume> <pages> 1-34, </pages> <year> 1997. </year>
Reference-contexts: Different distance metrics represent different 6 notions of distance in the pattern space. They also impose different inductive biases [7, 8] on the learning algorithm. Consequently, many researchers have investigated the use of alternative distance functions for instance-based learning <ref> [6, 44, 52, 56, 57] </ref>. The number and distribution of the clusters that result from specific choices of distance functions is a function of the distribution of the patterns as well as the clustering strategy used. <p> Normalization of individual attributes overcomes this problem in the distance computation. Normalization can be achieved by dividing each pattern attribute by the range of possible values for that attribute, or by 4 times the standard deviation for that attribute <ref> [57] </ref>. Normalization also allows attributes with nominal and/or missing values to be considered in distance computation. The distance for attributes with nominal values (say with attribute values x and y) is computed as follows [57]: * Overlap: d ol (x; y) = 0 if x = y; 1 otherwise. * Value <p> of possible values for that attribute, or by 4 times the standard deviation for that attribute <ref> [57] </ref>. Normalization also allows attributes with nominal and/or missing values to be considered in distance computation. The distance for attributes with nominal values (say with attribute values x and y) is computed as follows [57]: * Overlap: d ol (x; y) = 0 if x = y; 1 otherwise. * Value difference: d vd (x; y) = c=1 fi fi N a;x;c N a;y fi fi q where - N a;x (N a;y ): number of patterns in the training set that have value x <p> is 0.6. (See [37, 38] for detailed explanations on the experiments). 3 Experimental Evaluation of DistAl This section presents results of experiments using DistAl on several benchmark problems both with and without feature subset selection and compares them with the results presented by Wilson and Martinez in a recent paper <ref> [57] </ref>. <p> Following comparisons should be interpreted in light of those considerations. The best results of DistAl are compared with the best results in <ref> [57] </ref>. The results in [57] are chosen since they are recent and also obtained by a nearest-neighbor algorithm with a 10-fold cross-validation. Table 4 summarizes the comparison. As we can see from Table 4, DistAl gave comparable results on most datasets (except Soylarge and Vowel). <p> Following comparisons should be interpreted in light of those considerations. The best results of DistAl are compared with the best results in <ref> [57] </ref>. The results in [57] are chosen since they are recent and also obtained by a nearest-neighbor algorithm with a 10-fold cross-validation. Table 4 summarizes the comparison. As we can see from Table 4, DistAl gave comparable results on most datasets (except Soylarge and Vowel). In case of Vowel dataset, the nearest neighbor algorithm [57] <p> <ref> [57] </ref> are chosen since they are recent and also obtained by a nearest-neighbor algorithm with a 10-fold cross-validation. Table 4 summarizes the comparison. As we can see from Table 4, DistAl gave comparable results on most datasets (except Soylarge and Vowel). In case of Vowel dataset, the nearest neighbor algorithm [57] reports 20 a even higher accuracy than DistAl 5 The network size of three algorithms (perceptron cascade [34], cascade correlation [67], upstart [33]) for the two spirals problem is shown in [34]: 17.8 (perceptron cascade), 15.2 (cascade correlation), 91.4 (upstart). DistAl generated more compact networks with 7.7 hidden neurons.
Reference: [58] <author> B. Natarajan. </author> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kauffman, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year> <month> 28 </month>
Reference-contexts: The larger the search space, the more time the learning algorithm needs for learning a sufficiently accurate classification function <ref> [7, 58] </ref>. * number of examples needed: All other things being equal, the larger the number of attributes used to describe the patterns, the larger is the number of examples need to learn a classification function to a desired accuracy [7, 58]. * cost of classification: In many real-world pattern classification <p> learning algorithm needs for learning a sufficiently accurate classification function <ref> [7, 58] </ref>. * number of examples needed: All other things being equal, the larger the number of attributes used to describe the patterns, the larger is the number of examples need to learn a classification function to a desired accuracy [7, 58]. * cost of classification: In many real-world pattern classification tasks (e.g., medical diagnosis), some of the attributes may be observable symptoms and others might require diagnostic tests. Different diagnostic tests might have different costs as well as risks associated with them.
Reference: [59] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Since exhaustive search over all possible subsets of features is computationally infeasible, most approaches make restrictive assumptions (e.g., monotonic-ity which states that the addition of features does not worsen classification accuracy) or use a variety of heuristics. Genetic algorithms <ref> [59, 60, 61] </ref> offer a particularly promising approach to feature subset selection for a number of reasons [36, 37, 38]: * They do not have to rely on the often unrealistic monotonicity assumption. * They are particularly effective tools for exploring large search spaces for near-optimal solutions [59, 60, 61]. <p> Genetic algorithms <ref> [59, 60, 61] </ref> offer a particularly promising approach to feature subset selection for a number of reasons [36, 37, 38]: * They do not have to rely on the often unrealistic monotonicity assumption. * They are particularly effective tools for exploring large search spaces for near-optimal solutions [59, 60, 61].
Reference: [60] <author> M. Mitchell. </author> <title> An Introduction to Genetic algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: Since exhaustive search over all possible subsets of features is computationally infeasible, most approaches make restrictive assumptions (e.g., monotonic-ity which states that the addition of features does not worsen classification accuracy) or use a variety of heuristics. Genetic algorithms <ref> [59, 60, 61] </ref> offer a particularly promising approach to feature subset selection for a number of reasons [36, 37, 38]: * They do not have to rely on the often unrealistic monotonicity assumption. * They are particularly effective tools for exploring large search spaces for near-optimal solutions [59, 60, 61]. <p> Genetic algorithms <ref> [59, 60, 61] </ref> offer a particularly promising approach to feature subset selection for a number of reasons [36, 37, 38]: * They do not have to rely on the often unrealistic monotonicity assumption. * They are particularly effective tools for exploring large search spaces for near-optimal solutions [59, 60, 61].
Reference: [61] <author> Z. Michalewicz. </author> <title> Genetic Algorithms + Data Structures = Evolution Programs. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <note> 3rd edition, </note> <year> 1996. </year>
Reference-contexts: Since exhaustive search over all possible subsets of features is computationally infeasible, most approaches make restrictive assumptions (e.g., monotonic-ity which states that the addition of features does not worsen classification accuracy) or use a variety of heuristics. Genetic algorithms <ref> [59, 60, 61] </ref> offer a particularly promising approach to feature subset selection for a number of reasons [36, 37, 38]: * They do not have to rely on the often unrealistic monotonicity assumption. * They are particularly effective tools for exploring large search spaces for near-optimal solutions [59, 60, 61]. <p> Genetic algorithms <ref> [59, 60, 61] </ref> offer a particularly promising approach to feature subset selection for a number of reasons [36, 37, 38]: * They do not have to rely on the often unrealistic monotonicity assumption. * They are particularly effective tools for exploring large search spaces for near-optimal solutions [59, 60, 61].
Reference: [62] <author> P. Murphy and D. Aha. </author> <title> Repository of machine learning databases. </title> <institution> Department of Information and Computer Science, University of California, </institution> <address> Irvine, CA, </address> <year> 1994. </year>
Reference-contexts: It also presents the performance of DistAl on a real-world document classification task. 3.1 Datasets Two artificial datasets (parity and two spirals) and a wide range of real-world datasets from the machine learning data repository at the University of California at Irvine <ref> [62] </ref> were chosen to test the performance of DistAl. DistAl is also used for classifying paper abstracts and news articles. The paper abstracts were chosen from three different sources: IEEE Expert magazine, Journal of Artificial Intelligence Research and Neural Computation. The news articles were obtained from Reuters dataset. <p> Instead, it computes the distance (using one of the pre-defined distance metrics) between each pattern pair and uses it to set the weights (and the thresholds) between hidden neurons and inputs. 5 The best results reported in the literature <ref> [62] </ref> is 56% for Vowel dataset. 21 The weights between the hidden and output neurons are set using a one-shot (as opposed to iterative) learning algorithm.
Reference: [63] <author> G. Salton. </author> <title> Developments in automatic text retrieval. </title> <journal> Science, </journal> <volume> 253 </volume> <pages> 974-979, </pages> <year> 1991. </year>
Reference-contexts: The news articles were obtained from Reuters dataset. Each document is represented in the form of a vector of numeric weights for each of the words (terms) in the vocabulary. The weights correspond to the term frequency and inverse document frequency (TFIDF) <ref> [63, 64] </ref> values for the corresponding words. The training sets for paper abstracts were generated based on the classification of the corresponding documents into two classes (interesting and not interesting) by two different individuals, resulting in two different data sets (Abstract1 and Abstract2).
Reference: [64] <author> J. Yang, P. Pai, V. Honavar, and L. Miller. </author> <title> Mobile intelligent agents for document classification and retrieval: A machine learning approach. </title> <booktitle> In 14th European Meeting on Cybernetics and Systems Research. Symposium on Agent Theory to Agent Implementation, </booktitle> <address> Vienna, Austria, </address> <year> 1998. </year>
Reference-contexts: The news articles were obtained from Reuters dataset. Each document is represented in the form of a vector of numeric weights for each of the words (terms) in the vocabulary. The weights correspond to the term frequency and inverse document frequency (TFIDF) <ref> [63, 64] </ref> values for the corresponding words. The training sets for paper abstracts were generated based on the classification of the corresponding documents into two classes (interesting and not interesting) by two different individuals, resulting in two different data sets (Abstract1 and Abstract2). <p> It gave reasonably high (over 80%) generalization accuracy for all datasets. Also, the GA-selected subset of features produced improved generalization accuracy with slightly larger network size. For detailed explanation of implementation, related work and comparisons with other approaches see <ref> [64] </ref>. 4 Summary and Discussion A fast inter-pattern distance-based constructive learning algorithm, DistAl, is introduced and its performance on a number of datasets is demonstrated. DistAl is different from other constructive learning algorithms in two aspects. <p> Some preliminary results based on experiments using DistAl to design mobile agents for text classification and retrieval from distributed document collections are reported in <ref> [64] </ref>. Constructive algorithms in general provide an natural framework for exploration of cumulative (life long) learning [70] and for knowledge-based theory refinement [29, 71]. An interesting direction for future research would be to explore the use of DistAl for this task using real-world datasets e.g., the genome data used in [29].
Reference: [65] <author> D. Koller and M. Sahami. </author> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In International Conference on Machine Learning, </booktitle> <pages> pages 170-178, </pages> <year> 1997. </year>
Reference-contexts: The classifications for news articles were given based on their topics (6, 4 and 8 classes) following <ref> [65] </ref>, resulting in three different datasets (Reuters1,Reuters2 and 18 Reuters3), respectively. Table 1 summarizes the characteristics of the datasets selected for our experiments. 3.2 Experimental Results DistAl is deterministic in the sense that its behavior is always identical for a given training set.
Reference: [66] <author> H. Andersen and A. Tsoi. </author> <title> A constructive algorithm for the training of a multilayer perceptron based on the genetic algorithm. </title> <journal> Complex Systems, </journal> <volume> 7 </volume> <pages> 249-268, </pages> <year> 1993. </year>
Reference-contexts: DistAl converged fairly quickly for almost all datasets. (See Section 2.6 for detailed analysis of time complexity). GA-MLP <ref> [66] </ref> is based on a genetic algorithm and thus it usually takes significant amount of time to get a quality solution. Cascade correlation [67] uses Quickprop [69]. Quickprop uses an iterative gradient descent method based on a second order heuristic. 19 accuracy of the network is computed.
Reference: [67] <author> S. Fahlman and C. Lebiere. </author> <title> The cascade correlation learning algorithm. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Neural Information Systems 2, </booktitle> <pages> pages 524-532. </pages> <address> Morgan-Kauffman, </address> <year> 1990. </year>
Reference-contexts: DistAl converged fairly quickly for almost all datasets. (See Section 2.6 for detailed analysis of time complexity). GA-MLP [66] is based on a genetic algorithm and thus it usually takes significant amount of time to get a quality solution. Cascade correlation <ref> [67] </ref> uses Quickprop [69]. Quickprop uses an iterative gradient descent method based on a second order heuristic. 19 accuracy of the network is computed. The current best generalization accuracy is stored in a pocket along with the network size. <p> As we can see from Table 4, DistAl gave comparable results on most datasets (except Soylarge and Vowel). In case of Vowel dataset, the nearest neighbor algorithm [57] reports 20 a even higher accuracy than DistAl 5 The network size of three algorithms (perceptron cascade [34], cascade correlation <ref> [67] </ref>, upstart [33]) for the two spirals problem is shown in [34]: 17.8 (perceptron cascade), 15.2 (cascade correlation), 91.4 (upstart). DistAl generated more compact networks with 7.7 hidden neurons. Table 5 shows that the combination of DistAl and feature subset selection yield fairly good results.
Reference: [68] <author> M. Golea and M. Marchand. </author> <title> A growth algorithm for neural network decision trees. </title> <journal> Europhysics Letters, </journal> <volume> 12(3) </volume> <pages> 205-210, </pages> <year> 1990. </year>
Reference: [69] <author> S. Fahlman. </author> <title> Faster-learning variations on backpropagation: an empirical study. </title> <editor> In D. Touretzky, G. Hinton, and T. Sejnowsky, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 38-51. </pages> <address> Morgan-Kauffman, </address> <year> 1988. </year> <month> 29 </month>
Reference-contexts: DistAl converged fairly quickly for almost all datasets. (See Section 2.6 for detailed analysis of time complexity). GA-MLP [66] is based on a genetic algorithm and thus it usually takes significant amount of time to get a quality solution. Cascade correlation [67] uses Quickprop <ref> [69] </ref>. Quickprop uses an iterative gradient descent method based on a second order heuristic. 19 accuracy of the network is computed. The current best generalization accuracy is stored in a pocket along with the network size.
Reference: [70] <author> S. Thrun. </author> <title> Lifelong learning: A case study. </title> <type> Technical Report CMU-CS-95-208, </type> <institution> Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: Some preliminary results based on experiments using DistAl to design mobile agents for text classification and retrieval from distributed document collections are reported in [64]. Constructive algorithms in general provide an natural framework for exploration of cumulative (life long) learning <ref> [70] </ref> and for knowledge-based theory refinement [29, 71]. An interesting direction for future research would be to explore the use of DistAl for this task using real-world datasets e.g., the genome data used in [29].

References-found: 70

