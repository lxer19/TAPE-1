URL: http://theory.lcs.mit.edu/~slonim/BS95.ps
Refering-URL: http://theory.lcs.mit.edu/~slonim/team.html
Root-URL: 
Email: bender@das.harvard.edu slonim@theory.lcs.mit.edu  
Title: The Power of Team Exploration: Two Robots Can Learn Unlabeled Directed Graphs  
Author: Michael A. Bender Donna K. Slonim 
Note: Supported by NSF Grant CCR-93-13775. Supported by NSF Grant CCR-93-10888 and NIH Grant 1 T32 HG00039-01.  
Date: September 6, 1995  
Address: 545 Technology Square Cambridge, MA 02138 Cambridge, MA 02139  
Affiliation: Aiken Computation Laboratory MIT Laboratory for Computer Science Harvard University  
Abstract: We show that two cooperating robots can learn exactly any strongly-connected directed graph with n indistinguishable nodes in expected time polynomial in n. We introduce a new type of homing sequence for two robots, which helps the robots recognize certain previously-seen nodes. We then present an algorithm in which the robots learn the graph and the homing sequence simultaneously by actively wandering through the graph. Unlike most previous learning results using homing sequences, our algorithm does not require a teacher to provide counterexamples. Furthermore, the algorithm can use efficiently any additional information available that distinguishes nodes. We also present an algorithm in which the robots learn by taking random walks. The rate at which a random walk on a graph converges to the stationary distribution is characterized by the conductance of the graph. Our random-walk algorithm learns in expected time polynomial in n and in the inverse of the conductance and is more efficient than the homing-sequence algorithm for high-conductance graphs. 
Abstract-found: 1
Intro-found: 1
Reference: [Ang81] <author> Dana Angluin. </author> <title> A note on the number of queries needed to identify regular languages. </title> <journal> Information and Control, </journal> <volume> 51 </volume> <pages> 76-87, </pages> <year> 1981. </year>
Reference-contexts: It then runs several copies of Angluin's algorithm [Ang87] for learning DFAs given a reset. Angluin has shown that any algorithm for actively learning DFAs requires an equivalence oracle <ref> [Ang81] </ref>. In this paper, we introduce a new type of homing sequence for two robots. Because of the strength of the homing sequence, our algorithm does not require an equivalence oracle. For any graph, the expected running time of our algorithm is O (d 2 n 5 ). <p> In fact, two robots on a graph define a DFA whose states are pairs of nodes in G and whose edges correspond to pairs of actions. Since the automata defined in this way form a restricted class of DFAs, our results are not inconsistent with Angluin's work <ref> [Ang81] </ref> showing that a teacher is necessary for learning general DFAs. Theorem 5 Every strongly-connected directed graph has a two-robot homing sequence. Proof: The following algorithm (based on that of Kohavi [Koh78, RS93]) constructs a homing sequence: Initially, let h be empty.
Reference: [Ang87] <author> Dana Angluin. </author> <title> Learning regular sets from queries and counterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: They present an algorithm for a single robot to learn minimal deterministic finite automata. With the help of an equivalence oracle, their algorithm learns a homing sequence, which it uses in place of a reset function. It then runs several copies of Angluin's algorithm <ref> [Ang87] </ref> for learning DFAs given a reset. Angluin has shown that any algorithm for actively learning DFAs requires an equivalence oracle [Ang81]. In this paper, we introduce a new type of homing sequence for two robots.
Reference: [AR94] <author> Y. Aumann and M. O. Rabin. </author> <title> Clock construction in fully asynchronous parallel systems and PRAM simulation. </title> <journal> Theoretical Computer Science, </journal> <volume> 128 </volume> <pages> 3-30, </pages> <year> 1994. </year>
Reference-contexts: However, the following corollary bounds the conditional probabilities. The proof of this corollary is exactly analogous to that of a similar corollary by Aumann and Rabin <ref> [AR94, Corollary 1] </ref>. Corollary 11 Let X 1 ; : : :; X m be 0/1 random variables (not necessarily independent), and let b j 2 f0; 1g for 1 j m.
Reference: [ABRS95] <author> Baruch Awerbuch, Margrit Betke, Ronald L.Rivest, and Mona Singh. </author> <title> Piecemeal graph exploration by a mobile robot. </title> <booktitle> In Proceedings of the 1995 ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 321-328, </pages> <address> Santa Cruz, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: In piecemeal learning, the learner must return to a fixed starting point from time to time during the learning process. Betke, Rivest, and Singh provide linear algorithms for learning grid graphs with rectangular obstacles [BRS93], and with Awerbuch <ref> [ABRS95] </ref> extend this work to show nearly-linear algorithms for general graphs. Rivest and Schapire [RS87, RS93] explore the problem of learning deterministic finite automata whose nodes are not distinguishable except by the observed output. We rely heavily on their results in this paper.
Reference: [BB + 90] <author> Paul Beame, Allan Borodin, Prabhakar Raghavan, Walter Ruzzo, and Martin Tompa. </author> <title> Time-space tradeoffs for undirected graph traversal. </title> <booktitle> In Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 429-430, </pages> <year> 1990. </year>
Reference-contexts: JAGs have been used primarily to prove space efficiency for st-connectivity algorithms, and they have recently resurfaced as a tool for analyzing time and space tradeoffs for graph traversal and connectivity problems (e.g. <ref> [BB + 90, Poo93, Edm93] </ref>). Universal traversal sequences have been used to provide upper and lower bounds for the exploration of undirected graphs. Certainly, a universal traversal sequence for the class of directed graphs could be used to learn individual graphs.
Reference: [Bet92] <author> Margrit Betke. </author> <title> Algorithms for exploring an unknown graph. </title> <type> Master's thesis. </type> <note> Published as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-536, </note> <month> March, </month> <year> 1992. </year>
Reference-contexts: They provide a learning algorithm whose competitive ratio (versus the optimal time to traverse all edges in the graph) is exponential in the deficiency of the graph <ref> [DP90, Bet92] </ref>. Betke, Rivest, and Singh introduce the notion of piecemeal learning of undirected graphs with labeled nodes. In piecemeal learning, the learner must return to a fixed starting point from time to time during the learning process.
Reference: [BRS93] <author> Margrit Betke, Ronald Rivest, and Mona Singh. </author> <title> Piecemeal learning of an unknown environment. </title> <booktitle> In Proceedings of the 1993 Conference on Computational Learning Theory, </booktitle> <pages> pages 277-286, </pages> <address> Santa Cruz, CA, </address> <month> July </month> <year> 1993. </year> <note> (Published as MIT AI-Memo 1474, CBCL-Memo 93; to be published in Machine Learning). </note>
Reference-contexts: In piecemeal learning, the learner must return to a fixed starting point from time to time during the learning process. Betke, Rivest, and Singh provide linear algorithms for learning grid graphs with rectangular obstacles <ref> [BRS93] </ref>, and with Awerbuch [ABRS95] extend this work to show nearly-linear algorithms for general graphs. Rivest and Schapire [RS87, RS93] explore the problem of learning deterministic finite automata whose nodes are not distinguishable except by the observed output. We rely heavily on their results in this paper.
Reference: [BK78] <author> Manuel Blum and Dexter Kozen. </author> <title> On the power of the compass (or, why mazes are easier to search than graphs). </title> <booktitle> In 19th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 132-142. </pages> <publisher> IEEE, </publisher> <year> 1978. </year>
Reference-contexts: They also prove, however, that no collection of finite automata can search every 3-dimensional maze. Blum and Kozen <ref> [BK78] </ref> improve this result to show that a single automaton with 2 pebbles can search a finite, 2-dimensional maze. Their results imply that mazes are strictly easier to search than planar graphs, since they also show that no single automaton with pebbles can search all planar graphs.
Reference: [BS77] <author> M. Blum and W. J. Sakoda. </author> <title> On the capability of finite automata in 2 and 3 dimensional space. </title> <booktitle> In 18th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 147-161. </pages> <publisher> IEEE, </publisher> <year> 1977. </year>
Reference-contexts: Rabin first proposed the idea of dropping pebbles to mark nodes [Rab67]. This suggestion led to a body of work exploring the searching capabilities of a finite automaton supplied with pebbles. Blum and Sakoda <ref> [BS77] </ref> consider the question of whether a finite set of finite automata can search a 2 or 3-dimensional obstructed grid.
Reference: [CBF + 93] <author> N. Cesa-Bianchi, Y. Freund, D. Helmbold, D. Haussler, R. Schapire, and M. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Several team learning papers explore the problems of combining the abilities of a number of different learners. Cesa-Bianchi et al. <ref> [CBF + 93] </ref> consider the task of learning a probabilistic binary sequence given the predictions of a set of experts on the same sequence. They show how to combine the prediction strategies of several experts to predict nearly as well as the best of the experts.
Reference: [CR80] <author> Stephen A. Cook and Charles W. Rackoff. </author> <title> Space lower bounds for maze threadability on restricted machines. </title> <journal> SIAM Journal on Computing, </journal> <volume> 9 </volume> <pages> 636-652, </pages> <year> 1980. </year>
Reference-contexts: Cook and Rackoff generalized the idea of pebbles to jumping automata for graphs (JAGs) <ref> [CR80] </ref>. A jumping automaton is equipped with pebbles that can be dropped to mark nodes and that can "jump" to the locations of other pebbles.
Reference: [DA + 92] <author> Thomas Dean, Dana Angluin, Kenneth Basye, Sean Engelson, Leslie Kaelbling, Evan-gelos Kokkevis, and Oded Maron. </author> <title> Inferring finite automata with stochastic output functions and an application to map learning. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 208-214, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Rivest and Schapire [RS87, RS93] explore the problem of learning deterministic finite automata whose nodes are not distinguishable except by the observed output. We rely heavily on their results in this paper. Their work has been extended by Freund et al. [FK + 93], and by Dean et al. <ref> [DA + 92] </ref>. Freund et al. analyze the problem of learning finite automata with average-case labelings by the observed output on a random string, while Dean et al. explore the problem of learning DFAs with a robot whose observations of the environment are not always reliable.
Reference: [DP90] <author> Xiaotie Deng and Christos H. Papadimitriou. </author> <title> Exploring an unknown graph. </title> <booktitle> In Proceedings of the 31st Symposium on Foundations of Computer Science, </booktitle> <volume> volume I, </volume> <pages> pages 355-361, </pages> <year> 1990. </year>
Reference-contexts: They provide a learning algorithm whose competitive ratio (versus the optimal time to traverse all edges in the graph) is exponential in the deficiency of the graph <ref> [DP90, Bet92] </ref>. Betke, Rivest, and Singh introduce the notion of piecemeal learning of undirected graphs with labeled nodes. In piecemeal learning, the learner must return to a fixed starting point from time to time during the learning process.
Reference: [DP + 91] <author> Robert Daley, Leonard Pitt, Mahendran Velauthapillai, and Todd Will. </author> <title> Relations be-tween probabilistic and team one-shot learners. </title> <booktitle> In Proceedings of COLT '91, </booktitle> <pages> pages 228-239. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [Edm93] <author> Jeff Edmonds. </author> <title> Time-space trade-offs for undirected st-connectivity on a JAG. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 718-727, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: JAGs have been used primarily to prove space efficiency for st-connectivity algorithms, and they have recently resurfaced as a tool for analyzing time and space tradeoffs for graph traversal and connectivity problems (e.g. <ref> [BB + 90, Poo93, Edm93] </ref>). Universal traversal sequences have been used to provide upper and lower bounds for the exploration of undirected graphs. Certainly, a universal traversal sequence for the class of directed graphs could be used to learn individual graphs.
Reference: [FK + 93] <author> Yoav Freund, Michael Kearns, Dana Ron, Ronitt Rubinfeld, Robert Schapire, and Linda Sellie. </author> <title> Efficient learning of typical finite automata from random walks. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 315-324, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Rivest and Schapire [RS87, RS93] explore the problem of learning deterministic finite automata whose nodes are not distinguishable except by the observed output. We rely heavily on their results in this paper. Their work has been extended by Freund et al. <ref> [FK + 93] </ref>, and by Dean et al. [DA + 92].
Reference: [Koh78] <author> Zvi Kohavi. </author> <title> Switching and Finite Automata Theory. </title> <publisher> McGraw-Hill, </publisher> <address> second edition, </address> <year> 1978. </year>
Reference-contexts: Theorem 5 Every strongly-connected directed graph has a two-robot homing sequence. Proof: The following algorithm (based on that of Kohavi <ref> [Koh78, RS93] </ref>) constructs a homing sequence: Initially, let h be empty. As long as there are two nodes u and v in G such that output (h,u) = output (h,v) but final (h,u) 6 final (h,v), let x be a lead-lag sequence whose output distinguishes final (h,u) from final (h,v).
Reference: [KS93] <author> Michael J. Kearns and H. Sebastian Seung. </author> <title> Learning from a population of hypotheses. </title> <booktitle> In Proceedings of COLT '93, </booktitle> <pages> pages 101-110, </pages> <year> 1993. </year>
Reference-contexts: They show how to combine the prediction strategies of several experts to predict nearly as well as the best of the experts. In a related paper, Kearns and Seung <ref> [KS93] </ref> explore the statistical problems of combining several independent hypotheses to learn a target concept from a known, restricted concept class.
Reference: [Mih89] <author> Milena Mihail. </author> <title> Conductance and convergence of Markov chains a combinatorial treatment of expanders. </title> <booktitle> In Proceedings of the 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 526-531, </pages> <year> 1989. </year>
Reference-contexts: Mihail <ref> [Mih89] </ref> shows that after a walk of length 2 log (2n=* 2 ), the L 1 norm of the distance between the current distribution P and the stationary distribution is at most * (i.e. P *).
Reference: [Moo56] <author> Edward F. Moore. </author> <booktitle> Gedanken-Experiments on Sequential Machines, </booktitle> <pages> pages 129-153. </pages> <publisher> Princeton University Press, </publisher> <year> 1956. </year> <title> Edited by C. </title> <editor> E. Shannon and J. </editor> <publisher> McCarthy. </publisher>
Reference-contexts: As a tool we introduce a family C = [ n C n of graphs called combination locks. 1 For a graph 1 Graphs of this sort have been used in theoretical computer science for many years (see <ref> [Moo56] </ref>, for example). More recently they have reemerged as tools to prove the hardness of learning problems.
Reference: [Poo93] <author> C.K. Poon. </author> <title> Space bounds for graph connectivity problems on node-named JAGs and node-ordered JAGs. </title> <booktitle> In Proceedings of the Thirty-Fourth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 218-227, </pages> <address> Palo Alto, California, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: JAGs have been used primarily to prove space efficiency for st-connectivity algorithms, and they have recently resurfaced as a tool for analyzing time and space tradeoffs for graph traversal and connectivity problems (e.g. <ref> [BB + 90, Poo93, Edm93] </ref>). Universal traversal sequences have been used to provide upper and lower bounds for the exploration of undirected graphs. Certainly, a universal traversal sequence for the class of directed graphs could be used to learn individual graphs.
Reference: [PW95] <author> James Gary Propp and David Bruce Wilson. </author> <title> Exact sampling with coupled Markov chains and applications to statistical mechanics. </title> <booktitle> To appear in the Proceedings of the 1995 Conference on Random Structures and Algorithms. </booktitle>
Reference: [Rab67] <author> Michael O. Rabin. </author> <title> Maze threading automata. </title> <month> October </month> <year> 1967. </year> <institution> Seminar talk presented at the University of California at Berkeley. </institution>
Reference-contexts: However, when equipped with a number of pebbles that can be used to mark nodes, the single robot's plight improves. Rabin first proposed the idea of dropping pebbles to mark nodes <ref> [Rab67] </ref>. This suggestion led to a body of work exploring the searching capabilities of a finite automaton supplied with pebbles. Blum and Sakoda [BS77] consider the question of whether a finite set of finite automata can search a 2 or 3-dimensional obstructed grid.
Reference: [RS87] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Diversity-based inference of finite automata. </title> <booktitle> In Proceedings of the Twenty-Eighth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 78-87, </pages> <address> Los Angeles, California, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: Betke, Rivest, and Singh provide linear algorithms for learning grid graphs with rectangular obstacles [BRS93], and with Awerbuch [ABRS95] extend this work to show nearly-linear algorithms for general graphs. Rivest and Schapire <ref> [RS87, RS93] </ref> explore the problem of learning deterministic finite automata whose nodes are not distinguishable except by the observed output. We rely heavily on their results in this paper. Their work has been extended by Freund et al. [FK + 93], and by Dean et al. [DA + 92].
Reference: [Rag89] <author> Prabhakar Raghavan. CS661: </author> <title> Lecture Notes on Randomized Algorithms. </title> <institution> Yale University, </institution> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Before we can prove the correctness of our algorithm, we need one more set of tools. Consider the following statement of Chernoff bounds from Raghavan <ref> [Rag89] </ref>. Lemma 10 Let X 1 ; : : :; X m be independent Bernoulli trials with E [X j ] = p j . Let the random variable X = P m j=1 X j , where = E [X] 0.
Reference: [RS93] <author> Ronald L. Rivest and Robert E. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103(2) </volume> <pages> 299-347, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Betke, Rivest, and Singh provide linear algorithms for learning grid graphs with rectangular obstacles [BRS93], and with Awerbuch [ABRS95] extend this work to show nearly-linear algorithms for general graphs. Rivest and Schapire <ref> [RS87, RS93] </ref> explore the problem of learning deterministic finite automata whose nodes are not distinguishable except by the observed output. We rely heavily on their results in this paper. Their work has been extended by Freund et al. [FK + 93], and by Dean et al. [DA + 92]. <p> Alternatively, the lagging robot can abandon its current node to catch up with the leader, but then it may not know how to return to that node. In spite of these difficulties, our algorithms successfully employ a lead-lag strategy. Our work also builds on techniques of Rivest and Schapire <ref> [RS93] </ref>. They present an algorithm for a single robot to learn minimal deterministic finite automata. With the help of an equivalence oracle, their algorithm learns a homing sequence, which it uses in place of a reset function. <p> In this section we suggest an alternative technique: we introduce a new type of homing sequence for two robots. Intuitively, a homing sequence is a sequence of actions whose observed output uniquely determines the final node reached in G. Rivest and Schapire <ref> [RS93] </ref> show how a single robot with a teacher can use homing sequences to learn strongly-connected minimal DFAs. The output at each node indicates whether that node is an accepting or rejecting state of the automaton. <p> Theorem 5 Every strongly-connected directed graph has a two-robot homing sequence. Proof: The following algorithm (based on that of Kohavi <ref> [Koh78, RS93] </ref>) constructs a homing sequence: Initially, let h be empty. As long as there are two nodes u and v in G such that output (h,u) = output (h,v) but final (h,u) 6 final (h,v), let x be a lead-lag sequence whose output distinguishes final (h,u) from final (h,v). <p> Thus, the total running time of the algorithm is O (d 2 n 6 ). 2 5.1 Improvements to the Algorithm The running time for Learn-Graph can be decreased significantly by using two-robot adaptive homing sequences. As in Rivest and Schapire <ref> [RS93] </ref>, an adaptive homing sequence is a decision tree, so the actions in later steps of the sequence depend on the output of earlier steps. With an adaptive homing sequence, only one map c needs to be discarded each time the homing sequence is improved.
Reference: [RR95] <author> Dana Ron and Ronitt Rubinfeld. </author> <title> Learning fallible deterministic finite automata. </title> <journal> Machine Learning, </journal> <volume> 18 </volume> <pages> 149-185, </pages> <month> February/March </month> <year> 1995. </year>
Reference-contexts: Freund et al. analyze the problem of learning finite automata with average-case labelings by the observed output on a random string, while Dean et al. explore the problem of learning DFAs with a robot whose observations of the environment are not always reliable. Ron and Rubinfeld <ref> [RR95] </ref> present algorithms for learning "fallible" DFAs, in which the data is subject to persistent random errors. Recently, Ron and Rubinfeld [RR95b] have shown that a teacher is unnecessary for learning finite automata with small cover time.
Reference: [RR95b] <author> Dana Ron and Ronitt Rubinfeld. </author> <title> Exactly learning automata with small cover time. </title> <booktitle> In Proceedings of the 1995 ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 427-436, </pages> <address> Santa Cruz, CA, </address> <month> July </month> <year> 1995. </year> <month> 26 </month>
Reference-contexts: Ron and Rubinfeld [RR95] present algorithms for learning "fallible" DFAs, in which the data is subject to persistent random errors. Recently, Ron and Rubinfeld <ref> [RR95b] </ref> have shown that a teacher is unnecessary for learning finite automata with small cover time. In our model a single robot is powerless because it is completely unable to distinguish one node from any other.
Reference: [Sav73] <author> Walter J. Savitch. </author> <title> Maze recognizing automata and nondeterministic tape complexity. </title> <journal> JCSS, </journal> <volume> 7 </volume> <pages> 389-403, </pages> <year> 1973. </year>
Reference-contexts: Their results imply that mazes are strictly easier to search than planar graphs, since they also show that no single automaton with pebbles can search all planar graphs. Savitch <ref> [Sav73] </ref> introduces the notion of a maze-recognizing automaton (MRA), which is a DFA with a finite number of distinguishable pebbles.
Reference: [SJ89] <author> Alistair Sinclair and Mark Jerrum. </author> <title> Approximate counting, uniform generation and rapidly mixing markov chains. </title> <journal> Information and Computation, </journal> <volume> 82(1) </volume> <pages> 93-133, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Two robots can learn specific classes of directed graphs more quickly, such as the class of graphs with high conductance. Conductance, a measure of the expansion properties of a graph, was introduced by Sinclair and Jerrum <ref> [SJ89] </ref>. The class of directed graphs with high conductance includes graphs with exponentially-large cover time. <p> In this section we define conductance and present an algorithm that runs more quickly than Learn-Graph for graphs with conductance greater than p 6.1 Conductance The conductance <ref> [SJ89] </ref> of a graph characterizes the rate at which a random walk on the graph converges to the stationary distribution .
Reference: [Smi94] <author> Carl H. Smith. </author> <title> Three decades of team learning. </title> <booktitle> To appear in Proceedings of the Fourth International Workshop on on Algorithmic Learning Theory. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1994. </year> <month> 27 </month>
Reference-contexts: Thus, our algorithms demonstrate that two robots are strictly more powerful than one. 1.1 Related Work Previous results showing the power of team learning are plentiful, particularly in the field of inductive inference (see Smith <ref> [Smi94] </ref> for an excellent survey). Several team learning papers explore the problems of combining the abilities of a number of different learners. Cesa-Bianchi et al. [CBF + 93] consider the task of learning a probabilistic binary sequence given the predictions of a set of experts on the same sequence.
References-found: 31

