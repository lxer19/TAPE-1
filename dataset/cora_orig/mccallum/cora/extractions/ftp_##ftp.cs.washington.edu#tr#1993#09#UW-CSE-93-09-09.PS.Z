URL: ftp://ftp.cs.washington.edu/tr/1993/09/UW-CSE-93-09-09.PS.Z
Refering-URL: http://www.cs.princeton.edu/~felten/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Protocol Compilation: High-Performance Communication for Parallel Programs  
Author: by Edward W. Felten 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Approved by (Co-Chairperson of Supervisory Committee) (Co-Chairperson of Supervisory Committee)  
Note: Program Authorized to Offer Degree Date  
Date: 1993  
Affiliation: University of Washington  
Abstract-found: 0
Intro-found: 1
Reference: [Abbott & Peterson 92] <author> M. B. Abbott and L. L. Peterson. </author> <title> A language-based approach to protocol implementation. </title> <booktitle> In Proceedings of ACM SIGCOMM '92 Conference, </booktitle> <pages> pages 27-38, </pages> <year> 1992. </year>
Reference-contexts: Like the language compilation work, the optimization research treats send and receive or higher-level operations as indivisible building blocks. These optimizations are valuable, but the resulting code could be improved further by using tailored protocols. 8.1.4 Efficient Protocol Implementation Techniques The x-kernel [Hutchinson & Peterson 91] and Morpheus <ref> [Abbott & Peterson 92] </ref> are two systems that facilitate the construction of efficient protocols. The x-kernel provides a software substrate through which small pieces of protocol code can be assembled into a single protocol.
Reference: [Agarwal et al. 91] <author> A. Agarwal, D. Chaiken, G. D'Souza, K. Johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, B.-H. Lim, G. Maa, D. Nussbaum, M. Parkin, and D. Yeung. </author> <title> The MIT Alewife machine: A large-scale distributed-memory multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: Since there were two kinds of machines, and programming model was assumed to follow directly from architecture, there were also two programming models. Three recent trends are making the shared-memory/message-passing dichotomy invalid. First, hardware designs are converging. The new generation of scalable shared-memory machines <ref> [Lenoski et al. 92, Agarwal et al. 91] </ref> have physically distributed memory and point-to-point communication networks. These machines look very much like the current generation of distributed-memory machines; the only difference is in how communication is caused.
Reference: [Amarasinghe & Lam 93] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In SIG-PLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 126-138, </pages> <month> June </month> <year> 1993. </year>
Reference: [Anderson & Snyder 91] <author> R. Anderson and L. Snyder. </author> <title> A comparison of shared and nonshared memory models of parallel computation. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 79(4) </volume> <pages> 480-487, </pages> <year> 1991. </year>
Reference-contexts: It is becoming increasingly clear that parallel programmers must face the same difficult issues whether they are programming shared-memory or distributed-memory machines <ref> [Snyder 86, Anderson & Snyder 91, Markatos & LeBlanc 92] </ref>. In view of these trends, we must develop a more sophisticated understanding of the tradeoff between shared memory and message-passing.
Reference: [Anderson et al. 89] <author> T. E. Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> The performance implications of thread management alternatives for shared memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <year> 1989. </year>
Reference-contexts: Because the processors are asynchronous, the read might happen before the write. To avoid this, some synchronization must take place between A's write and B's read. This synchronization imposes additional communication, which also counts as protocol overhead. Indeed, synchronization primitives can require a great deal of communication <ref> [Anderson et al. 89, Mellor-Crummey & Scott 91] </ref>. At first glance, it might appear unfair to classify synchronization as overhead. 1 Because DASH uses release consistency [Gharachorloo et al. 90], some of these communications might actually happen in parallel.
Reference: [Anderson et al. 91] <author> T. E. Anderson, H. M. Levy, B. N. Bershad, and E. D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Proceedings of 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <year> 1991. </year>
Reference-contexts: Since all communication operations are protected operations, the kernel can ensure that user-level processes play by the rules. One problem with putting communication in the kernel is that crossing protection boundaries is costly <ref> [Anderson et al. 92, Anderson et al. 91] </ref>. There are several reasons for this: * context switch costs: Crossing into the kernel requires that registers be saved and restored.
Reference: [Anderson et al. 92] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level manage 128 ment of parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Since all communication operations are protected operations, the kernel can ensure that user-level processes play by the rules. One problem with putting communication in the kernel is that crossing protection boundaries is costly <ref> [Anderson et al. 92, Anderson et al. 91] </ref>. There are several reasons for this: * context switch costs: Crossing into the kernel requires that registers be saved and restored.
Reference: [Bershad et al. 93] <author> B. N. Bershad, M. J. Zekauskas, and W. A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In 38th IEEE Computer Society Intl. Conf. (COMPCON), </booktitle> <pages> pages 524-533, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The second trend is toward increasingly sophisticated and varied parallel programming languages and models. These include data-parallel languages like High Performance Fortran [HPFF 93], C* [TMC 90], and Dino [Rosing 91]; shared virtual memory systems such as Ivy [Li & Hudak 86], Munin [Carter et al. 91] and Midway <ref> [Bershad et al. 93] </ref>; dataflow languages such as Id [Nikhil 88]; and stream-based systems like Strand [Foster & Taylor 90] and PCN [Foster et al. 92]. These systems have one thing in common: they provide the programmer with a single namespace for variables, but they run reasonably on distributed-memory machines.
Reference: [Blelloch et al. 93] <author> G. E. Blelloch, S. Chatterjee, J. C. Hardwick, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <booktitle> In Proceedings of Fourth SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 102-111, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: These languages include Dino [Rosing 91], Dataparallel C [Hatcher & Quinn 91], C* [Rose & Steele 87, TMC 90], CM-Fortran [TMC 89], Fortran D [Fox et al. 91, Tseng 93], Vienna Fortran [Chapman et al. 92], NESL <ref> [Blelloch et al. 93] </ref>, and many others. While several such compilers generate efficient message-passing code, they all treat send and receive as indivisible primitives.
Reference: [Bokhari 90] <author> S. Bokhari. </author> <title> Communication overhead on the intel iPSC-860 hypercube. </title> <type> Technical Report Interim Report 10, </type> <institution> ICASE, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: The routers are pipelined and use wormhole routing [Dally & Seitz 87] to rapidly set up a path for a message. The per-hop latency of these networks is so low that we can consider the interconnection topology irrelevant, except for its effect on contention for links <ref> [Bokhari 90] </ref>. The Intel Paragon's network can set up a path between any two nodes in less than one microsecond; once established, the path has a bandwidth of 200 megabytes per second. A different network design is used by the Thinking Machines CM-5 [TMC 91, Leiserson et al. 92].
Reference: [Bower et al. 88] <author> J. M. Bower, M. E. Nelson, M. A. Wilson, G. C. Fox, and W. Fur-manski. </author> <title> Piriform (olfactory) cortex model on the hypercube. </title> <booktitle> In Proceedings of Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 977-999, </pages> <year> 1988. </year>
Reference-contexts: I wrote this program. * Olfactory: This application simulates the olfactory cortex of a cat's brain. The program does a biologically accurate simulation of the behavior of each neuron, and the connections between them. This program was originally written by Jim Bower of the Caltech Biology Department and colleagues <ref> [Bower et al. 88] </ref>, and has been updated by Wojtek Furmanski and Roberto Battiti of the Caltech Concurrent Computing Program. * Md: This program carries out a molecular dynamics simulation. It simulates a set of molecules interacting via a Lennard-Jones force law.
Reference: [Brustoloni & Bershad 92] <author> J. C. Brustoloni and B. N. Bershad. </author> <title> Simple protocol processing for high-bandwidth low-latency networking. </title> <type> Technical Report CMU-CS-93-132, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Data corruption errors are so rare that they can be ignored | mistaking data corruption for congestion will not affect correctness and will have only a tiny effect on performance. Brustoloni and Bershad present a promising message-passing protocol for ATM networks <ref> [Brustoloni & Bershad 92] </ref>. Their protocol uses per-message pre-reservation to allocate buffer space for arriving messages. (They use a separate protocol for small messages.) Dropped packets cause the sender to retry with exponential backoff. The protocol compiler technique could be used to improve the performance of this protocol.
Reference: [Byrd & Delagi 88] <author> G. Byrd and B. Delagi. </author> <title> A performance comparison of shared variables versus message passing. </title> <booktitle> In Proceedings of Third International Conference on Supercomputing, </booktitle> <volume> volume 1, </volume> <pages> pages 1-7, </pages> <year> 1988. </year>
Reference: [Callahan & Kennedy 88] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <year> 1988. </year>
Reference: [Carriero & Gelernter 89] <author> N. Carriero and D. Gelernter. </author> <title> How to write parallel programs: A guide to the perplexed. </title> <journal> Computing Surveys, </journal> <volume> 21(3) </volume> <pages> 323-357, </pages> <month> September </month> <year> 1989. </year> <month> 129 </month>
Reference-contexts: NUMA systems do not experience the overhead of a cache coherence protocol, they do suffer from the cost of making repeated references to remote data, rather than caching them locally. 2.3.3 Protocol Overhead in Linda The third example is a multicomputer like the Intel DELTA, running the Linda coordination language <ref> [Carriero & Gelernter 89, Gelernter & Carriero 92] </ref>. In the Linda model, all communication is through a logically shared data structure called the tuple space. Processes add tuples to the tuple space via the out primitive, and access and remove existing tuples via the in and rd primitives.
Reference: [Carter et al. 91] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The second trend is toward increasingly sophisticated and varied parallel programming languages and models. These include data-parallel languages like High Performance Fortran [HPFF 93], C* [TMC 90], and Dino [Rosing 91]; shared virtual memory systems such as Ivy [Li & Hudak 86], Munin <ref> [Carter et al. 91] </ref> and Midway [Bershad et al. 93]; dataflow languages such as Id [Nikhil 88]; and stream-based systems like Strand [Foster & Taylor 90] and PCN [Foster et al. 92].
Reference: [Chapman et al. 92] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Vienna Fortran | a Fortran language extension for distributed memory systems. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-time Environments for Distributed Memory Machines. </title> <publisher> Elsevier Press, </publisher> <year> 1992. </year>
Reference-contexts: These languages include Dino [Rosing 91], Dataparallel C [Hatcher & Quinn 91], C* [Rose & Steele 87, TMC 90], CM-Fortran [TMC 89], Fortran D [Fox et al. 91, Tseng 93], Vienna Fortran <ref> [Chapman et al. 92] </ref>, NESL [Blelloch et al. 93], and many others. While several such compilers generate efficient message-passing code, they all treat send and receive as indivisible primitives.
Reference: [Culler et al. 93] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumette, T. von Eicken, and K. Yelick. </author> <title> Introduction to Split-C. </title> <note> Available by anonymous ftp from mammoth.berkeley.edu, </note> <year> 1993. </year>
Reference-contexts: For example, the DINO language [Rosing et al. 90, Rosing 91] offers primitives that read and write remote data structures. Similarly, the Split-C system <ref> [Culler et al. 93] </ref> extends local C-language program 16 ming with global pointers and global arrays. Global pointers specify an arbitrary memory location, possibly located remotely. Global pointers are supported at the language level; dereferencing a global pointer causes the appropriate remote-memory fetch or store to be performed.
Reference: [Dally & Seitz 87] <author> W. J. Dally and C. L. Seitz. </author> <title> Deadlock free message routing in multiprocessor interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):547-553, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: Each node must have two network interfaces, one for user-level programs, and one reserved for the kernel. Fortunately, we needn't implement two separate physical networks | the two logical networks can be multiplexed over a single physical network, using virtual channels <ref> [Dally & Seitz 87, Dally 92] </ref> as in the J-machine [Dally 90]. The CM-5 nearly satisfies these requirements. Its only deficiency is that it does not have a separate network for the kernel. However, the CM-5 can offer user-level communication if the operating system is slightly restricted. <p> The Intel network is implemented as a set of router chips connected directly by wires. The routers are pipelined and use wormhole routing <ref> [Dally & Seitz 87] </ref> to rapidly set up a path for a message. The per-hop latency of these networks is so low that we can consider the interconnection topology irrelevant, except for its effect on contention for links [Bokhari 90].
Reference: [Dally 87] <author> W. J. Dally. </author> <title> Wire-efficient VLSI multiprocessor communication networks. </title> <editor> In P. Losleben, editor, </editor> <booktitle> Proceedings of Stanford Conference on Advanced Research in VLSI, </booktitle> <pages> pages 391-415. </pages> <publisher> MIT Press, </publisher> <month> March </month> <year> 1987. </year>
Reference-contexts: In these systems, the nodes are connected in a two-dimensional grid topology. This topology offers a high bisection bandwidth and presents fewer packaging problems than other organizations <ref> [Dally 87] </ref>. Messages are routed using a non-adaptive, oblivious algorithm: a message first moves horizontally until it reaches the correct column, then it moves up or down to the destination [Sullivan & Brashkow 77]. The Intel network is implemented as a set of router chips connected directly by wires.
Reference: [Dally 90] <author> W. J. Dally. </author> <title> The J-machine system. </title> <editor> In P. Winston and S. Shellard, editors, </editor> <booktitle> Artificial Intelligence at MIT: Expanding Frontiers, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Fortunately, we needn't implement two separate physical networks | the two logical networks can be multiplexed over a single physical network, using virtual channels [Dally & Seitz 87, Dally 92] as in the J-machine <ref> [Dally 90] </ref>. The CM-5 nearly satisfies these requirements. Its only deficiency is that it does not have a separate network for the kernel. However, the CM-5 can offer user-level communication if the operating system is slightly restricted.
Reference: [Dally 92] <author> W. J. Dally. </author> <title> Virtual-channel flow control. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 194-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Each node must have two network interfaces, one for user-level programs, and one reserved for the kernel. Fortunately, we needn't implement two separate physical networks | the two logical networks can be multiplexed over a single physical network, using virtual channels <ref> [Dally & Seitz 87, Dally 92] </ref> as in the J-machine [Dally 90]. The CM-5 nearly satisfies these requirements. Its only deficiency is that it does not have a separate network for the kernel. However, the CM-5 can offer user-level communication if the operating system is slightly restricted.
Reference: [Derby et al. 90] <author> T. M. Derby, E. Eskow, R. K. Neves, M. Rosing, R. B. Schnabel, and R. P. Weaver. </author> <title> The DINO user's manual. </title> <type> Technical Report CU-CS-501-90, </type> <institution> University of Colorado, </institution> <month> November </month> <year> 1990. </year>
Reference: [Feeley & Levy 92] <author> M. J. Feeley and H. M. Levy. </author> <title> Distributed shared memory with versioned objects. </title> <booktitle> In Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA '92), </booktitle> <pages> pages 247-262, </pages> <month> October </month> <year> 1992. </year> <month> 130 </month>
Reference-contexts: By storing multiple "versions" of each memory location, we could in principle avoid the need to make the sender wait for the receiver <ref> [Feeley & Levy 92] </ref>. 2.3.2 Protocol Overhead in Remote-Access NUMA Systems The second example is a shared-memory system without caching. This is a system in which each memory location is located with some processor, and processors can access each other's memory directly.
Reference: [Felten & McNamee 92] <author> E. W. Felten and D. McNamee. </author> <note> Newthreads 2.0 user's guide, </note> <year> 1992. </year>
Reference-contexts: An MP0 job consists of a set of processes distributed over some set of processing nodes. There is usually one process per physical processor, although most multicom-puter operating systems support multiple processes, and systems like NewThreads <ref> [Felten & McNamee 92] </ref> support multiple threads of control within a process. Processes communicate by making calls to the MP0 communication library. At the simplest level, MP0 supports point-to-point transmission of messages between processes. A message consists of data of arbitrary size. Each message is marked with a tag.
Reference: [Fisher 81] <author> J. A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 30(7) </volume> <pages> 478-490, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Even if the synchronization in B cannot completely replace the barrier, it can at least reduce the amount of "artificial" synchronization that must be added to insulate A from C. A third way to reduce synchronization is to use trace scheduling <ref> [Fisher 81] </ref>. Originally developed to aid scheduling of instructions across basic-block boundaries in compilers, this technique works by "guessing" a sequence of patterns that will be executed, and then treating the entire sequence as if it were a single pattern.
Reference: [Foster & Taylor 90] <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: include data-parallel languages like High Performance Fortran [HPFF 93], C* [TMC 90], and Dino [Rosing 91]; shared virtual memory systems such as Ivy [Li & Hudak 86], Munin [Carter et al. 91] and Midway [Bershad et al. 93]; dataflow languages such as Id [Nikhil 88]; and stream-based systems like Strand <ref> [Foster & Taylor 90] </ref> and PCN [Foster et al. 92]. These systems have one thing in common: they provide the programmer with a single namespace for variables, but they run reasonably on distributed-memory machines.
Reference: [Foster et al. 92] <author> I. Foster, R. Olson, and S. Tuecke. </author> <title> Productive parallel programming: The PCN approach. </title> <journal> Scientific Programming, </journal> <volume> 1 </volume> <pages> 51-66, </pages> <year> 1992. </year>
Reference-contexts: Fortran [HPFF 93], C* [TMC 90], and Dino [Rosing 91]; shared virtual memory systems such as Ivy [Li & Hudak 86], Munin [Carter et al. 91] and Midway [Bershad et al. 93]; dataflow languages such as Id [Nikhil 88]; and stream-based systems like Strand [Foster & Taylor 90] and PCN <ref> [Foster et al. 92] </ref>. These systems have one thing in common: they provide the programmer with a single namespace for variables, but they run reasonably on distributed-memory machines. These systems demonstrate that the programmer can have the desirable features of shared memory, while still using a message-passing machine.
Reference: [Fox 88] <author> G. C. Fox. </author> <title> What have we learnt from using real parallel machines to solve real problems. </title> <booktitle> In Proceedings of Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 897-955, </pages> <year> 1988. </year>
Reference-contexts: In addition, the majority of successful scientific applications of parallel computers are amenable to data-parallel implementation. In a study of 84 scientific applications of high-performance computers, Fox found that 70 applications, or 83% of those considered, fit the data-parallel model <ref> [Fox 88] </ref>. In the data-parallel model, a program has a single locus of control | a single "program counter" that advances through the program. This single execution stream invokes small pieces of parallel code.
Reference: [Fox et al. 87] <author> G. C. Fox, A. J. G. Hey, and S. W. Otto. </author> <title> Matrix algorithms on the hypercube I: Matrix multiplication. </title> <journal> Parallel Computing, </journal> <volume> 4, </volume> <year> 1987. </year>
Reference-contexts: The sync-pong version uses synchronizing mode for all messages. The micro-benchmark experiments consisted of running all three versions of pong, for various message sizes. The macro-benchmarks consisted of these five application programs: * Matmult: This program multiplies two matrices, using the Fox/Hey/Otto algorithm <ref> [Fox et al. 88, Fox et al. 87] </ref>. This algorithm lays out the program's data, and orchestrates its communication, very carefully. Most of this application's communication consists of broadcasts of submatrices across rows of the machine, and vertical pipelined "rolls" of data.
Reference: [Fox et al. 88] <author> G. C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker. </author> <title> Solving Problems on Concurrent Processors. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: The "loosely lockstep" behavior of the programs generated by these compilers matches a common programming pattern on MIMD systems. This pattern is sometimes referred to as SPMD (single-program, multiple-data) parallelism. It is also the idea captured by Fox's "loosely synchronous" programs <ref> [Fox et al. 88] </ref> and Snyder's XYZ model [Griswold et al. 90]. These ideas, in turn, have spawned research on data-parallel languages with weaker synchronization [Larus et al. 92]; for example, processes may (logically) synchronize after a set of statements rather than after each instruction. <p> The sync-pong version uses synchronizing mode for all messages. The micro-benchmark experiments consisted of running all three versions of pong, for various message sizes. The macro-benchmarks consisted of these five application programs: * Matmult: This program multiplies two matrices, using the Fox/Hey/Otto algorithm <ref> [Fox et al. 88, Fox et al. 87] </ref>. This algorithm lays out the program's data, and orchestrates its communication, very carefully. Most of this application's communication consists of broadcasts of submatrices across rows of the machine, and vertical pipelined "rolls" of data. <p> This program was written by David Walker at Caltech. * Nbody: This program simulates the evolution of a set of stars, interacting through gravity. It uses the simple n 2 algorithm, in which each timestep requires all-to-all communication. All-to-all communication is implemented with a "bucket brigade" technique <ref> [Fox et al. 88] </ref>; the program uses double-buffering to overlap communication and local computation. I wrote this program. * Olfactory: This application simulates the olfactory cortex of a cat's brain. The program does a biologically accurate simulation of the behavior of each neuron, and the connections between them.
Reference: [Fox et al. 91] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141 (revised), </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: These languages include Dino [Rosing 91], Dataparallel C [Hatcher & Quinn 91], C* [Rose & Steele 87, TMC 90], CM-Fortran [TMC 89], Fortran D <ref> [Fox et al. 91, Tseng 93] </ref>, Vienna Fortran [Chapman et al. 92], NESL [Blelloch et al. 93], and many others. While several such compilers generate efficient message-passing code, they all treat send and receive as indivisible primitives.
Reference: [Geist et al. 91] <author> G. A. Geist, M. T. Heath, B. W. Peyton, and P. H. Worley. </author> <title> A users' guide to PICL, a portable instrumented communication library. </title> <type> Technical Report TM-11616, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: In some cases, these packages are part of the operating system kernel; Intel's NX [Pierce 88] and NCUBE's Vertex [NCUBE 87] are examples. Other packages are implemented as user-level libraries; these include Thinking Machines' CMMD [TMC 92], Parasoft's Express [Parasoft 88], and the public-domain packages PVM [Sunderam 92], PICL <ref> [Geist et al. 91] </ref>, and Zipcode [Skjellum & Leung 90]. These packages have slightly different purposes and features, but in most ways they are very similar. In response to the proliferation of message-passing packages, a standardization effort was launched.
Reference: [Gelernter & Carriero 92] <author> D. Gelernter and N. Carriero. </author> <title> Coordination languages and their significance. </title> <journal> Communications of the ACM, </journal> <volume> 35(2) </volume> <pages> 97-107, </pages> <month> February </month> <year> 1992. </year> <month> 131 </month>
Reference-contexts: NUMA systems do not experience the overhead of a cache coherence protocol, they do suffer from the cost of making repeated references to remote data, rather than caching them locally. 2.3.3 Protocol Overhead in Linda The third example is a multicomputer like the Intel DELTA, running the Linda coordination language <ref> [Carriero & Gelernter 89, Gelernter & Carriero 92] </ref>. In the Linda model, all communication is through a logically shared data structure called the tuple space. Processes add tuples to the tuple space via the out primitive, and access and remove existing tuples via the in and rd primitives.
Reference: [Gharachorloo et al. 90] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <year> 1990. </year>
Reference-contexts: This synchronization imposes additional communication, which also counts as protocol overhead. Indeed, synchronization primitives can require a great deal of communication [Anderson et al. 89, Mellor-Crummey & Scott 91]. At first glance, it might appear unfair to classify synchronization as overhead. 1 Because DASH uses release consistency <ref> [Gharachorloo et al. 90] </ref>, some of these communications might actually happen in parallel.
Reference: [Griswold et al. 90] <author> W. Griswold, G. Harrison, D. Notkin, and L. Snyder. </author> <title> Scalable abstractions for parallel programming. </title> <booktitle> In Proceedings of Fifth Distributed Memory Computing Conference, </booktitle> <year> 1990. </year>
Reference-contexts: The "loosely lockstep" behavior of the programs generated by these compilers matches a common programming pattern on MIMD systems. This pattern is sometimes referred to as SPMD (single-program, multiple-data) parallelism. It is also the idea captured by Fox's "loosely synchronous" programs [Fox et al. 88] and Snyder's XYZ model <ref> [Griswold et al. 90] </ref>. These ideas, in turn, have spawned research on data-parallel languages with weaker synchronization [Larus et al. 92]; for example, processes may (logically) synchronize after a set of statements rather than after each instruction.
Reference: [Hatcher & Quinn 91] <author> P. J. Hatcher and M. J. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: An Example The work of Hatcher and Quinn on compiling data-parallel programs illustrates the complexity of implementation tradeoffs. Hatcher and Quinn developed two compilers that translate a program written in a language called Dataparallel C into code executable on a parallel machine <ref> [Hatcher & Quinn 91] </ref>. One compiler generates code for 15 a shared-memory machine [Quinn et al. 90], and one for a message-passing machine [Hatcher et al. 91]. The programmer's interface, Dataparallel C, is identical on both platforms. <p> Recent research has shown how (SIMD) data-parallel programs can be run efficiently on MIMD hardware <ref> [Hatcher & Quinn 91, TMC 91] </ref>. This is done by using compile-time analysis to determine where and when synchronization and communication are needed to maintain the illusion of lockstep behavior in spite of physically unsynchronized processors. <p> Protocols must be built on top of these data movement systems. 119 8.1.2 Compiling to Message-Passing Code Many researchers have studied how to compile languages into message-passing code. These languages include Dino [Rosing 91], Dataparallel C <ref> [Hatcher & Quinn 91] </ref>, C* [Rose & Steele 87, TMC 90], CM-Fortran [TMC 89], Fortran D [Fox et al. 91, Tseng 93], Vienna Fortran [Chapman et al. 92], NESL [Blelloch et al. 93], and many others.
Reference: [Hatcher et al. 91] <author> P. J. Hatcher, A. J. Lapadula, R. R. Jones, M. J. Quinn, and R. J. Anderson. </author> <title> A production-quality C* compiler for hypercube multicomputers. </title> <booktitle> In Proceedings of Third SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 73-82, </pages> <year> 1991. </year>
Reference-contexts: Hatcher and Quinn developed two compilers that translate a program written in a language called Dataparallel C into code executable on a parallel machine [Hatcher & Quinn 91]. One compiler generates code for 15 a shared-memory machine [Quinn et al. 90], and one for a message-passing machine <ref> [Hatcher et al. 91] </ref>. The programmer's interface, Dataparallel C, is identical on both platforms. Dat-aparallel C provides the programmer with the abstraction of a single shared address space for variables. Thus, the programmer gets the sharing model that he wants. <p> Despite this careful management, compiler-generated programs typically communicate more than hand-written message-passing programs would, so communication performance is a particularly acute problem for compiler-generated code <ref> [Hatcher et al. 91] </ref>. Data-parallel languages are easier to compile than other parallel languages, because the single locus of control allows the compiler to deduce that all processes are in the same small section of code at the same time.
Reference: [Heath & Etheridge 91] <author> M. T. Heath and J. A. Etheridge. </author> <title> Visualizing performance of parallel programs. </title> <type> Technical Report TM-11813, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: We can represent a communication pattern by drawing a diagram of the communication history of each process within the pattern. Such diagrams are drawn by trace analysis tools like ParaGraph <ref> [Heath & Etheridge 91] </ref>, where they are called "space-time diagrams" or "Feynman diagrams." Figure 4.2 shows pseudocode for a one-dimensional FFT computation, and figure 4.3 shows the spacetime diagram of its communication pattern.
Reference: [Hillis & Steele 86] <author> W. D. Hillis and G. L. Steele Jr. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: This model is supported directly by SIMD machines, including the Connection Machine CM-1 and CM-2 architectures and the MasPar MP-1 and MP-2; the existence of these machines, which run only data-parallel code, has fostered the development of a rich variety of data-parallel algorithms <ref> [Hillis & Steele 86] </ref>. Recent research has shown how (SIMD) data-parallel programs can be run efficiently on MIMD hardware [Hatcher & Quinn 91, TMC 91].
Reference: [Hillis 85] <author> W. D. Hillis. </author> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference: [Hoare 85] <author> C. A. R. Hoare. </author> <title> Communicating Sequential Processes. </title> <publisher> Prentice Hall, </publisher> <year> 1985. </year>
Reference: [HPFF 93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: In the distributed-memory machines, messages are sent and received by software. The second trend is toward increasingly sophisticated and varied parallel programming languages and models. These include data-parallel languages like High Performance Fortran <ref> [HPFF 93] </ref>, C* [TMC 90], and Dino [Rosing 91]; shared virtual memory systems such as Ivy [Li & Hudak 86], Munin [Carter et al. 91] and Midway [Bershad et al. 93]; dataflow languages such as Id [Nikhil 88]; and stream-based systems like Strand [Foster & Taylor 90] and PCN [Foster et <p> A consortium of researchers and vendors is now developing a standardized language called High Performance Fortran (HPF) with some data-parallel features <ref> [HPFF 93] </ref>. HPF will be supported by all the major parallel system vendors, and will provide a common platform for researchers studying implementation of data-parallel languages. Compilers for data-parallel languages usually generate message-passing programs as output.
Reference: [Hutchinson & Peterson 91] <author> N. C. Hutchinson and L. L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year> <month> 132 </month>
Reference-contexts: Like the language compilation work, the optimization research treats send and receive or higher-level operations as indivisible building blocks. These optimizations are valuable, but the resulting code could be improved further by using tailored protocols. 8.1.4 Efficient Protocol Implementation Techniques The x-kernel <ref> [Hutchinson & Peterson 91] </ref> and Morpheus [Abbott & Peterson 92] are two systems that facilitate the construction of efficient protocols. The x-kernel provides a software substrate through which small pieces of protocol code can be assembled into a single protocol.
Reference: [Intel 91a] <institution> Intel Supercomputer Systems Division. Paragon XP/S Product Overview, </institution> <year> 1991. </year>
Reference-contexts: I/O devices are often connected directly to certain processing nodes. Some machines have a special "front-end" processor that is used for system administration. 22 Typical interconnection networks are exhibited by Intel's DELTA [Intel 91b] and Paragon <ref> [Intel 91a] </ref> systems. In these systems, the nodes are connected in a two-dimensional grid topology. This topology offers a high bisection bandwidth and presents fewer packaging problems than other organizations [Dally 87].
Reference: [Intel 91b] <institution> Intel Supercomputer Systems Division. </institution> <note> A Touchstone DELTA System Description, </note> <month> February </month> <year> 1991. </year>
Reference-contexts: I/O devices are often connected directly to certain processing nodes. Some machines have a special "front-end" processor that is used for system administration. 22 Typical interconnection networks are exhibited by Intel's DELTA <ref> [Intel 91b] </ref> and Paragon [Intel 91a] systems. In these systems, the nodes are connected in a two-dimensional grid topology. This topology offers a high bisection bandwidth and presents fewer packaging problems than other organizations [Dally 87].
Reference: [ISO 88] <author> ISO. </author> <title> Information Processing | Open Systems Interconnection | LOTOS: </title>
Reference-contexts: Future versions of the description language are likely to have an operation corresponding to local computation, with an optional estimate of the duration of the computation. Several other languages have been used to describe communication behavior and protocols. The best-known is LOTOS <ref> [ISO 88, van Eijk et al. 89, Logrippo et al. 92] </ref>, devised by ISO to specify and model the OSI standard protocol suite. LOTOS is much more ambitious and complex than PDL. Although LOTOS is in some sense a superset of PDL, expressing communication patterns in LOTOS would be overkill.
References-found: 47

