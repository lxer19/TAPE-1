URL: http://www.cacs.usl.edu/Departments/CACS/Publications/Raghavan/DRSS96.ps.Z
Refering-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Root-URL: http://www.cacs.usl.edu/~raghavan/raghavan-1.html
Title: DATA MINING: TRENDS IN RESEARCH AND DEVELOPMENT  
Author: Jitender S. Deogun, Vijay V. Raghavan*, Amartya Sarkar*, and Hayri 
Address: Lincoln, NE 68588, USA  Lafayette, LA 70504, USA  06532, TR  
Affiliation: Sever** The Department of Computer Science and Engineering, University of Nebraska-Lincoln  The Center for Advanced Computer Studies University of Southwestern Louisiana  The Department of Computer Science Hacettepe University, Beytepe, Ankara  
Abstract: Data mining is an interdisciplinary research area spanning several disciplines such as database systems, machine learning, intelligent information systems, statistics, and expert systems. Data mining has evolved into an important and active area of research because of theoretical challenges and practical applications associated with the problem of discovering (or extracting) interesting and previously unknown knowledge from very large real-world databases. Many aspects of data mining have been investigated in several related fields. But the problem is unique enough that there is a great need to extend these studies to include the nature of the contents of the real-world databases. In this chapter, we discuss the theory and foundational issues in data mining, describe data mining methods and algorithms, and review data mining applications. Since a major focus of this book is on rough sets and its applications to database mining, one full section is devoted to summarizing the state of rough sets as related to data mining of real-world databases. More importantly, we provide evidence showing that the theory of rough sets constitutes a sound basis for data mining applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. J. Frawley, G. Piatetsky-Shapiro, and C. J. Matheus, </author> <title> "Knowledge discovery 32 Chapter 1 databases: An overview," in Knowledge Discovery in Databases (G. </title> <editor> Piatetsky-Shapiro and W. J. Frawley, </editor> <booktitle> eds.), </booktitle> <pages> pp. 1-27, </pages> <address> Cambridge, MA: </address> <publisher> AAAI/MIT, </publisher> <year> 1991. </year>
Reference-contexts: 1 INTRODUCTION It is estimated that the amount of information in the world doubles every 20 months <ref> [1] </ref>; that is, many scientific, government and corporate information systems are being overwhelmed by a flood of data that are generated and stored routinely, which grow into large databases amounting to giga (and even tera) bytes of data [2]. <p> If the knowledge model is integrated/related to a data base within a DBMS, then it should also address issues related to the management of data such as data security, viewing levels of data, transaction management, and the use of general database functions/facilities <ref> [1, 3] </ref>. 3 THEORETICAL AND FOUNDATIONAL ISSUES The data (or instance space) is represented by a relation , which is the predominant structure adopted in either machine learning or database systems. Each tuple in a relation corresponds to an entity (also known as object, instance or background fact). <p> In the case of nominal attributes, all pairs of values are assigned an interval in advance. Then the closeness of an object to a rule is determined over the interval <ref> [0; 1] </ref> by using partial differences of attribute values. 6 KNOWLEDGE DISCOVERY SYSTEMS A knowledge discovery system that is capable of operating on large, real-world databases, is referred to as a knowledge discovery in databases (KDD) system.
Reference: [2] <author> R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and B. Swami, </author> <title> "An interval classifier for database mining applications," </title> <booktitle> in Proceedings of the 18th VLDB Conference, </booktitle> <address> (Vancouver, British Columbia, Canada), </address> <pages> pp. 560-573, </pages> <year> 1992. </year>
Reference-contexts: that the amount of information in the world doubles every 20 months [1]; that is, many scientific, government and corporate information systems are being overwhelmed by a flood of data that are generated and stored routinely, which grow into large databases amounting to giga (and even tera) bytes of data <ref> [2] </ref>. These databases contain potential gold mine of valuable information, but it is beyond human ability to analyze such massive amounts of data and elicit meaningful patterns.
Reference: [3] <author> C. J. Matheus, P. K. Chan, and G. Piatetsky-Shapiro, </author> <title> "Systems for knowledge discovery in databases," </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> vol. 5, no. 6, </volume> <pages> pp. 903-912, </pages> <year> 1993. </year>
Reference-contexts: Unfortunately, the database technology of today offers little functionality to explore data in such a fashion. At the same time KD techniques for intelligent data analysis are not yet mature for large data sets <ref> [3] </ref>. Furthermore, the fact that data has been organized and collected around the needs of organizational activities may pose a real difficulty in locating relevant data for knowledge discovery techniques from diverse sources. <p> It may be noted that data mining is a distinct descipline and its objectives are different from the goals and emphases of the individual fields. Data mining may, however, heavily use theories and developments of these fields <ref> [5, 3, 6, 7, 8] </ref>. In the following we present basic differences (and/or similarities) between data mining and various allied research areas. In developing database systems to manage uncertain (or imprecise) information as well as certain (or precise) information, several extensions to relational model have been suggested [9, 10, 11]. <p> If the knowledge model is integrated/related to a data base within a DBMS, then it should also address issues related to the management of data such as data security, viewing levels of data, transaction management, and the use of general database functions/facilities <ref> [1, 3] </ref>. 3 THEORETICAL AND FOUNDATIONAL ISSUES The data (or instance space) is represented by a relation , which is the predominant structure adopted in either machine learning or database systems. Each tuple in a relation corresponds to an entity (also known as object, instance or background fact). <p> In summary, data mining is a practical problem that drives theoretical studies toward understanding and reasoning about large and existing data. Matheus et al. used the tradeoff between `versatility' and `autonomy' for evaluating a KDD system <ref> [3] </ref>. They have argued that an ideal KDD system would handle knowledge discovery tasks autonomously while being applicable across many domains. While progress is being made in the direction of automatically acquiring knowledge needed for for guiding and controlling the knowledge discovery process, the ideal system remains far from reach.
Reference: [4] <editor> U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Some researchers view KDD as a broader discipline, with data mining as one component dealing with knowledge discovery methods <ref> [4] </ref>. Data Mining 3 evaluation of data mining results is summarized. In Section 4, we classify data mining queries into four categories: data dependency, classification, clustering and characterization. A variety of data mining methods available to handle each of these query classes are presented. <p> rough set based approaches to data mining are presented. 2 A PERSPECTIVE ON DATA MINING AND RELATED RESEARCH AREAS Data mining is a promising interdisciplinary area of research shared by several fields such as database systems, machine learning, intelligent information systems, statistics, data warehousing and knowledge acquisition in expert systems <ref> [4] </ref>. It may be noted that data mining is a distinct descipline and its objectives are different from the goals and emphases of the individual fields. Data mining may, however, heavily use theories and developments of these fields [5, 3, 6, 7, 8]. <p> KDW is "ideal for exploratory data analysis by a user knowledgeable in both data and operation of discovery tools". However such heavy reliance on the user has given the system a low ranking on the autonomy scale. Explora <ref> [4, 71] </ref> is another KDD system that incorporates a variety of search strategies to adapt discovery processes to the requirements of applications. It operates by performing a graph search through a network of patterns, searching for instances of interesting patterns. <p> in the decision process| individually each rule is treated as a piece of uncertain evidence and hence worth a little in the process of decision making; however, along with similar other rules, it can provide a substantial input to the decision making process. 6.2 Application-specific Systems Commercial systems, like CoverStory <ref> [4] </ref>, Spotlight [76] and KEFIR [4], have been developed to discover knowledge in databases using the concept of deviations. Deviations are powerful because they provide a simple way of identifying interesting pattern in the data. <p> each rule is treated as a piece of uncertain evidence and hence worth a little in the process of decision making; however, along with similar other rules, it can provide a substantial input to the decision making process. 6.2 Application-specific Systems Commercial systems, like CoverStory <ref> [4] </ref>, Spotlight [76] and KEFIR [4], have been developed to discover knowledge in databases using the concept of deviations. Deviations are powerful because they provide a simple way of identifying interesting pattern in the data. <p> The systems are fully automated once the initial domain knowledge has been set up. However, limited applicability has forced them a low ranking on the versatility scale. R-MINI <ref> [4] </ref> is yet another system that primarily utilizes classification techniques and also deviation detection, to some extent, to extract useful information from noisy domains, such as financial markets. <p> Once the rules are derived, the user can select a subset of rules to display or remove from display, specify an ordering of the rules or specify a grouping or clustering of the rules. In the scientific domain SKICAT <ref> [4] </ref> has been developed for automating the reduction and analysis of large astronomical data. The SKICAT system employs a supervised classification technique and is intended to automatically catalog and analyze celestial objects, given digitized sky images (plates).
Reference: [5] <author> R. Krishnamurty and T. Imielinski, </author> <title> "Research directions in knowledge discovery," </title> <booktitle> SIGMOD RECORD, </booktitle> <volume> vol. 20, </volume> <pages> pp. 76-78, </pages> <year> 1991. </year>
Reference-contexts: It may be noted that data mining is a distinct descipline and its objectives are different from the goals and emphases of the individual fields. Data mining may, however, heavily use theories and developments of these fields <ref> [5, 3, 6, 7, 8] </ref>. In the following we present basic differences (and/or similarities) between data mining and various allied research areas. In developing database systems to manage uncertain (or imprecise) information as well as certain (or precise) information, several extensions to relational model have been suggested [9, 10, 11].
Reference: [6] <author> A. Silberschatz, M. Stonebraker, and J. Ullman, </author> <title> "Database systems: achievements and opportunities," </title> <type> Tech. Rep. </type> <institution> TR-90-22, University of Texas at Austin, Department of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: It may be noted that data mining is a distinct descipline and its objectives are different from the goals and emphases of the individual fields. Data mining may, however, heavily use theories and developments of these fields <ref> [5, 3, 6, 7, 8] </ref>. In the following we present basic differences (and/or similarities) between data mining and various allied research areas. In developing database systems to manage uncertain (or imprecise) information as well as certain (or precise) information, several extensions to relational model have been suggested [9, 10, 11].
Reference: [7] <author> K. C. C. Chan and A. K. C. Wong, </author> <title> "A statistical technique for extracting classificatory knowledge from databases," in Knowledge Discovery in Databases (G. </title> <editor> Piatetsky-Shapiro and W. J. Frawley, </editor> <booktitle> eds.), </booktitle> <pages> pp. 107-123, </pages> <address> Cambridge, MA: </address> <publisher> AAAI/MIT, </publisher> <year> 1991. </year>
Reference-contexts: It may be noted that data mining is a distinct descipline and its objectives are different from the goals and emphases of the individual fields. Data mining may, however, heavily use theories and developments of these fields <ref> [5, 3, 6, 7, 8] </ref>. In the following we present basic differences (and/or similarities) between data mining and various allied research areas. In developing database systems to manage uncertain (or imprecise) information as well as certain (or precise) information, several extensions to relational model have been suggested [9, 10, 11]. <p> The last few years have seen an increasing use of techniques in data mining that draw upon or are based on statistics; namely, in feature selection [12], data dependency involving two variables for constructing data dependency networks [13, 14], classification of objects based on descriptions <ref> [7] </ref>, discretization of continuous values [13, 15], data summarization [14], predicting missing values [16], etc. The motivation behind this trend can be explained by the fact that statistical techniques for data analysis are well developed and in some cases, we do not have any other means to apply. <p> It has also been observed that rules learned from corrupted training set perform better in classifying noisy test data than rules that are learned from noise free training set. Chan and Wong <ref> [7] </ref> have used statistical techniques to analyze the effect of noise.
Reference: [8] <author> V. V. Raghavan, H. Sever, and J. S. Deogun, </author> <title> "A system architecture for database mining applications," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Knowledge Discovery, </booktitle> <address> (Banff, Alberta, Canada), </address> <pages> pp. 73-77, </pages> <year> 1993. </year>
Reference-contexts: It may be noted that data mining is a distinct descipline and its objectives are different from the goals and emphases of the individual fields. Data mining may, however, heavily use theories and developments of these fields <ref> [5, 3, 6, 7, 8] </ref>. In the following we present basic differences (and/or similarities) between data mining and various allied research areas. In developing database systems to manage uncertain (or imprecise) information as well as certain (or precise) information, several extensions to relational model have been suggested [9, 10, 11].
Reference: [9] <author> S. K. Lee, </author> <title> "An extended relational database model for uncertain and imprecise information," </title> <booktitle> in Proceedings of the 18th VLDB conference, </booktitle> <address> (Vancouver, British Columbia, Canada), </address> <pages> pp. 211-218, </pages> <year> 1992. </year>
Reference-contexts: In the following we present basic differences (and/or similarities) between data mining and various allied research areas. In developing database systems to manage uncertain (or imprecise) information as well as certain (or precise) information, several extensions to relational model have been suggested <ref> [9, 10, 11] </ref>. The direction of such extensions include data representation as well as basic relational operations. <p> In developing database systems to manage uncertain (or imprecise) information as well as certain (or precise) information, several extensions to relational model have been suggested [9, 10, 11]. The direction of such extensions include data representation as well as basic relational operations. In Lee's approach <ref> [9] </ref>, the uncertainty associated with an attribute (treated as random variable) is represented using a probability distribution on the power set (basic probability assignment) of its domain instead of an atomic value, while a set of values is allowed for the representation of imprecise data. <p> For example, in the list of personal computers, the attribute that contains the model type of the sound cards would be null for some model of computers. Lee provides an approach to extend relational database model for uncertain and imprecise information <ref> [9] </ref>, where the traditional null value is handled by subdividing it into three cases such as unknown, inapplicable, and unknown or inapplicable.
Reference: [10] <author> B. P. Buckles and F. E. Petry, </author> <title> "A fuzzy model for relational databases," </title> <journal> Journal of Fuzzy Sets and Systems, </journal> <volume> vol. 7, no. 3, </volume> <pages> pp. 213-226, </pages> <year> 1982. </year>
Reference-contexts: In the following we present basic differences (and/or similarities) between data mining and various allied research areas. In developing database systems to manage uncertain (or imprecise) information as well as certain (or precise) information, several extensions to relational model have been suggested <ref> [9, 10, 11] </ref>. The direction of such extensions include data representation as well as basic relational operations.
Reference: [11] <author> D. Barbara, H. Garcia-Molina, and D. Porter, </author> <title> "The management of probabilistic data," </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> vol. 4, no. 5, </volume> <pages> pp. 487-502, </pages> <year> 1992. </year>
Reference-contexts: In the following we present basic differences (and/or similarities) between data mining and various allied research areas. In developing database systems to manage uncertain (or imprecise) information as well as certain (or precise) information, several extensions to relational model have been suggested <ref> [9, 10, 11] </ref>. The direction of such extensions include data representation as well as basic relational operations. <p> Since the concern is to capture only the uncertainty in the data, Barbara et al. have associated discrete probabilistic functions with the values of attributes <ref> [11] </ref>. An attribute in a relation may be 4 Chapter 1 deterministic or probabilistic in nature, while keys must be deterministic, which is a restriction imposed by the authors leading to simple relational operators. <p> In other words, he reduced the missing value problem to that of learning from inconsistent examples. He, then, used rough set theory to induce certain and possible rules. Using similar line of interpretation of missing values, Barbara et al. in <ref> [11] </ref> have interpreted missing values as uninteresting values of an attribute with which they have associated missing probability measures. Probabilistic relational operations would yield certain or possible probabilities (lower or upper bounds on the probability of a random variable) depending on whether missing probabilities are facilitated, or not.
Reference: [12] <author> C. Corinna, H. Drucker, D. Hoover, and V. Vapnik, </author> <title> "Capacity and complexity control in predicting the spread between barrowing and lending interest rates," </title> <booktitle> in The First International Conference on Knowledge Discovery and Data Mining (U. </booktitle> <editor> Fayyad and R. Uthurusamy, eds.), </editor> <address> (Montreal, Quebec, Canada), </address> <pages> pp. 51-76, </pages> <month> aug </month> <year> 1995. </year>
Reference-contexts: For example, identifying probabilistic relationships in data can be useful in discovering functional or production-rule relationships in the data. The last few years have seen an increasing use of techniques in data mining that draw upon or are based on statistics; namely, in feature selection <ref> [12] </ref>, data dependency involving two variables for constructing data dependency networks [13, 14], classification of objects based on descriptions [7], discretization of continuous values [13, 15], data summarization [14], predicting missing values [16], etc. <p> If the evaluation criterion to derive the decision model is monotonic, then the training error can be controlled [37, 42]. In the process of estimating validation error, the concept of bootstrapping over test set may be used <ref> [12, 56] </ref>. Note that dividing the samples into training and test sets is an important problem and must be solved in a way that the distributions of the two sets are close to each other.
Reference: [13] <author> N. Zhong and S. Ohsuga, </author> <title> "Discovering concept clusters by decomposing databases," </title> <journal> Data & Knowledge Engineering, </journal> <volume> vol. 12, </volume> <pages> pp. 223-244, </pages> <year> 1994. </year>
Reference-contexts: The last few years have seen an increasing use of techniques in data mining that draw upon or are based on statistics; namely, in feature selection [12], data dependency involving two variables for constructing data dependency networks <ref> [13, 14] </ref>, classification of objects based on descriptions [7], discretization of continuous values [13, 15], data summarization [14], predicting missing values [16], etc. <p> The last few years have seen an increasing use of techniques in data mining that draw upon or are based on statistics; namely, in feature selection [12], data dependency involving two variables for constructing data dependency networks [13, 14], classification of objects based on descriptions [7], discretization of continuous values <ref> [13, 15] </ref>, data summarization [14], predicting missing values [16], etc. The motivation behind this trend can be explained by the fact that statistical techniques for data analysis are well developed and in some cases, we do not have any other means to apply. <p> Horizontal reduction is related to merging identical tuples following either the substitution of an attribute value by its higher level value in a pre-defined generalization hierarchy of categorical values of the attribute [22] or the quantization (or discretization) of continuous (or numeric) values <ref> [13, 15, 23] </ref>. Vertical reduction is realized by either applying some feature selection methods or using attribute dependency graph [24]. We consider vertical reduction as a part of methods for handling redundant data, in Section 3.5. We elaborate on some notable studies on horizontal reduction in the following. <p> Han et al. [22], for example, utilize generalization hierarchies of attributes' values in their inductive learning method to characterize a concept or discriminate it from other concepts. In another approach, Zhong & Ohsuga <ref> [13] </ref> have focused on the conditional distributions of two discrete attributes to form a basis for hierarchical model learning. They have transformed the instance space of two discrete attributes to a probability space, represented by a probability distribution matrix.
Reference: [14] <author> G. Piatetsky-Shapiro and C. J. Matheus, </author> <title> "Knowledge discovery workbench for exploring business databases," </title> <journal> International Journal of Intelligent Systems, </journal> <volume> vol. 7, </volume> <pages> pp. 675-686, </pages> <year> 1992. </year> <title> Data Mining 33 </title>
Reference-contexts: The last few years have seen an increasing use of techniques in data mining that draw upon or are based on statistics; namely, in feature selection [12], data dependency involving two variables for constructing data dependency networks <ref> [13, 14] </ref>, classification of objects based on descriptions [7], discretization of continuous values [13, 15], data summarization [14], predicting missing values [16], etc. <p> have seen an increasing use of techniques in data mining that draw upon or are based on statistics; namely, in feature selection [12], data dependency involving two variables for constructing data dependency networks [13, 14], classification of objects based on descriptions [7], discretization of continuous values [13, 15], data summarization <ref> [14] </ref>, predicting missing values [16], etc. The motivation behind this trend can be explained by the fact that statistical techniques for data analysis are well developed and in some cases, we do not have any other means to apply. <p> Many of the classical multivariate analysis techniques, viz., principal components, factor analysis, discriminant analysis are special cases of projection pursuit method. As a final note, it may be worth pointing out that one could also use random sampling methods <ref> [14] </ref>, along with the horizontal pruning methods [22]. 3.6 Dynamic Data A fundamental characteristic of databases that are online is that they are dynamic; that is, their contents are ever changing. This situation has several important implications for the Knowledge Discovery (KD) method. <p> In KDW <ref> [14] </ref>, Shapiro & Matheus have utilized the idea of probabilistic dependency between two discrete attributes. This information provides the weight and direction of the arc between nodes characterized by the two attributes. <p> Unfortunately such technique does not work well when the patterns are time varying. Alternatively, interactive cluster techniques may be applied, which combine the computer's computational power with a human's knowledge. In Shapiro & Matheus's paper on knowledge discovery workbench <ref> [14] </ref>, a tool for line clustering of points involving numerical values of two attributes is discussed, as a part of data visualization. That is an example of the kind of interaction that can take place between a human expert and a data mining tool. <p> Similarly, it would have been better if more complex KGOs had been implemented on the kernel consisting of primitive KGOs, rather than collecting all KGOs in one menu. The Knowledge Discovery Workbench (KDW) <ref> [14] </ref> is a collection of tools for interactive analysis of large databases. Many of its design principles and characteristics are similar to those of INLEN. The pattern extraction algorithms range from clustering to classification to deviation detection.
Reference: [15] <author> U. M. Fayyad and K. B. Irani, </author> <title> "Multi interval discretization of continuous attributes for classification learning," </title> <booktitle> in Proceedings of 13th International Joint Conference on Artificial Intelligence (R. Bajcsy, </booktitle> <publisher> ed.), </publisher> <pages> pp. 1022-1027, </pages> <publisher> Morgan Kauffmann, </publisher> <year> 1993. </year>
Reference-contexts: The last few years have seen an increasing use of techniques in data mining that draw upon or are based on statistics; namely, in feature selection [12], data dependency involving two variables for constructing data dependency networks [13, 14], classification of objects based on descriptions [7], discretization of continuous values <ref> [13, 15] </ref>, data summarization [14], predicting missing values [16], etc. The motivation behind this trend can be explained by the fact that statistical techniques for data analysis are well developed and in some cases, we do not have any other means to apply. <p> Horizontal reduction is related to merging identical tuples following either the substitution of an attribute value by its higher level value in a pre-defined generalization hierarchy of categorical values of the attribute [22] or the quantization (or discretization) of continuous (or numeric) values <ref> [13, 15, 23] </ref>. Vertical reduction is realized by either applying some feature selection methods or using attribute dependency graph [24]. We consider vertical reduction as a part of methods for handling redundant data, in Section 3.5. We elaborate on some notable studies on horizontal reduction in the following. <p> In other words, their criteria of discretization fail to take into consideration the relationship between pre-assigned classes and interval boundaries. Both Ching et al. [23] and Fayyad & Irani <ref> [15] </ref> suggest class dependent discretization algorithms. Note that the whole idea here is to reduce the number of attribute values without destroying the interdependence relationship between the class and attribute values. Class-dependent discretization of Ching et al. [23] consists of three main processes: interval initialization, interval improvement, and interval reduction. <p> The boundary improvement process, which uses an interdependence criterion given by a normalized class-attribute mutual information, considers all possible local adjustments on the boundary set to ensure a good estimation of global optimal interdependence. The last process combines statistically insignificant intervals. Fayyad & Irani in <ref> [15] </ref> formally prove that the information entropy minimization criterion of ID3, used for binary splitting of continuous valued attributes, always selects a value between two examples of different classes in the sequence of sorted examples with respect to increasing order of that attribute values, i.e., the selected value is actually a
Reference: [16] <author> J. F. Elder-IV and D. </author> <title> Pregibon, "A statistical perspective on KDD," </title> <booktitle> in The First International Conference on Knowledge Discovery and Data Mining (U. </booktitle> <editor> Fayyad and R. Uthurusamy, eds.), </editor> <address> (Montreal, Quebec, Canada), </address> <pages> pp. 87-93, </pages> <month> aug </month> <year> 1995. </year>
Reference-contexts: use of techniques in data mining that draw upon or are based on statistics; namely, in feature selection [12], data dependency involving two variables for constructing data dependency networks [13, 14], classification of objects based on descriptions [7], discretization of continuous values [13, 15], data summarization [14], predicting missing values <ref> [16] </ref>, etc. The motivation behind this trend can be explained by the fact that statistical techniques for data analysis are well developed and in some cases, we do not have any other means to apply. <p> The EM algorithm assumes that the missing values are missing at random, but the importance of this method lies in its underlying message| even when the data is complete, it is often useful to treat the data as a missing value problem for computational purposes <ref> [16] </ref>. 3.4 Incomplete Data Suppose each object in the universe of discourse is described or characterized by the values of a set of attributes.
Reference: [17] <author> S. K. M. Wong, W. Ziarko, and R. L. Ye, </author> <title> "Comparison of rough set and statistical methods in inductive learning," </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> vol. 24, </volume> <pages> pp. 53-72, </pages> <year> 1986. </year>
Reference-contexts: The primary limitation is the inability to recognize and generalize relationships, such as the set inclusion, that capture structural aspects of a data set, as a result of being entirely confined to arithmetic manipulations of probability measures <ref> [17, 18] </ref>. The chi-square test is used, for example, by some decision-tree based systems during tree pruning to determine whether a node should be branched [19]. It is also used to select a good set of features with which to perform the learning process [20].
Reference: [18] <author> W. Ziarko, </author> <title> "The discovery, analysis, and representation of data dependencies in databases," in Knowledge Discovery in Databases (G. </title> <editor> Piatetsky-Shapiro and W. J. Frawley, eds.), </editor> <address> Cambridge, MA: </address> <publisher> AAAI/MIT, </publisher> <year> 1991. </year>
Reference-contexts: The primary limitation is the inability to recognize and generalize relationships, such as the set inclusion, that capture structural aspects of a data set, as a result of being entirely confined to arithmetic manipulations of probability measures <ref> [17, 18] </ref>. The chi-square test is used, for example, by some decision-tree based systems during tree pruning to determine whether a node should be branched [19]. It is also used to select a good set of features with which to perform the learning process [20]. <p> A plausible remedy for this problem is to design an incremental method and separate the summary and the result of a method from one to another. Ziarko, in <ref> [18] </ref>, has used the strength of a decision rule as a part of the summary of the decision algorithm.
Reference: [19] <author> J. R. Quinlan, </author> <title> "Induction of decision trees," </title> <journal> Machine Learning, </journal> <volume> vol. 1, </volume> <pages> pp. 81-106, </pages> <year> 1986. </year>
Reference-contexts: The chi-square test is used, for example, by some decision-tree based systems during tree pruning to determine whether a node should be branched <ref> [19] </ref>. It is also used to select a good set of features with which to perform the learning process [20]. Despite its popularity, it should be noted that the chi-square test only tells us whether an attribute, as a whole, is helpful in determining the class membership of an object. <p> When the database contains missing attribute values, either the values can be discarded or an attempt can be made to replace them with the most likely values <ref> [19] </ref>. These are the ideas adopted by Quinlan [19] for inductive decision trees. In [31] it is suggested to construct rules that predict the value of the missing attribute, Data Mining 9 based on the value of other attributes in the example, and the class information. <p> When the database contains missing attribute values, either the values can be discarded or an attempt can be made to replace them with the most likely values <ref> [19] </ref>. These are the ideas adopted by Quinlan [19] for inductive decision trees. In [31] it is suggested to construct rules that predict the value of the missing attribute, Data Mining 9 based on the value of other attributes in the example, and the class information. <p> In the rest of this subsection, we review work on inductive decision trees aimed at making them suitable for incomplete data. ID3-like algorithms <ref> [19, 34, 35] </ref>, during the process of inducing decision trees as well as of refining induced decision trees, implicitly assume that enough information is available in the data to decide exactly how each object should be classified.
Reference: [20] <author> M. James, </author> <title> Classification Algorithms. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: The chi-square test is used, for example, by some decision-tree based systems during tree pruning to determine whether a node should be branched [19]. It is also used to select a good set of features with which to perform the learning process <ref> [20] </ref>. Despite its popularity, it should be noted that the chi-square test only tells us whether an attribute, as a whole, is helpful in determining the class membership of an object. <p> This type of exhaustive search would be appropriate only if l is small and J is computationally inexpensive. Greedy approaches like stepwise backward/forward techniques <ref> [20, 35] </ref>, dynamic programming [41], and branch and bound algorithm [42] are non-exhaustive and efficient search techniques, which can be applied with some feature selection criterion.
Reference: [21] <author> T. Mitchell, </author> <title> "Generalization as search," </title> <journal> Artificial Intelligence, </journal> <volume> vol. 18, </volume> <pages> pp. 203-226, </pages> <year> 1982. </year>
Reference-contexts: For example, candidate elimination algorithm <ref> [21] </ref>, a tuple oriented learning technique from examples, aims to search the version space, whose size is doubly-exponential in the number of attributes, of training examples to induce a generalized concept that is satisfied by all of the positive examples and none of the negative examples.
Reference: [22] <author> J. Han, Y. Cai, and N. Cercone, </author> <title> "Knowledge discovery in databases: An attribute-oriented approach," </title> <booktitle> in Proceedings of the 18th VLDB Conference, </booktitle> <address> (Van-couver, British Columbia, Canada), </address> <pages> pp. 547-559, </pages> <year> 1992. </year>
Reference-contexts: Horizontal reduction is related to merging identical tuples following either the substitution of an attribute value by its higher level value in a pre-defined generalization hierarchy of categorical values of the attribute <ref> [22] </ref> or the quantization (or discretization) of continuous (or numeric) values [13, 15, 23]. Vertical reduction is realized by either applying some feature selection methods or using attribute dependency graph [24]. We consider vertical reduction as a part of methods for handling redundant data, in Section 3.5. <p> Many of the classical multivariate analysis techniques, viz., principal components, factor analysis, discriminant analysis are special cases of projection pursuit method. As a final note, it may be worth pointing out that one could also use random sampling methods [14], along with the horizontal pruning methods <ref> [22] </ref>. 3.6 Dynamic Data A fundamental characteristic of databases that are online is that they are dynamic; that is, their contents are ever changing. This situation has several important implications for the Knowledge Discovery (KD) method. <p> Concept hierarchies (or more generally dependency networks) are based on a partial ordering of propositions (or predicates), which are usually expressed as unary formulas. 14 Chapter 1 Such structures may be a part of the background knowledge. Han et al. <ref> [22] </ref>, for example, utilize generalization hierarchies of attributes' values in their inductive learning method to characterize a concept or discriminate it from other concepts. In another approach, Zhong & Ohsuga [13] have focused on the conditional distributions of two discrete attributes to form a basis for hierarchical model learning. <p> On the other hand, the characterization query describes common features of a class regardless of the characteristics of other classes. The former kind of description is called discriminating while the latter is called characterizing. A typical example of characterization method can be found in <ref> [22] </ref>. Han et al., in their attribute based learning framework called DBLEARN [22], utilize concept hierarchies, which constitute background knowledge, during the generalization process. A relation that represents intermediate (or final) learning results is called an intermediate (or a final) generalized relation. <p> The former kind of description is called discriminating while the latter is called characterizing. A typical example of characterization method can be found in <ref> [22] </ref>. Han et al., in their attribute based learning framework called DBLEARN [22], utilize concept hierarchies, which constitute background knowledge, during the generalization process. A relation that represents intermediate (or final) learning results is called an intermediate (or a final) generalized relation. <p> This is one of the strategies used in the attribute oriented approach for inductive concept learning <ref> [22] </ref>. Since an attribute-oriented learning technique operates on relations, its strategies can be easily adapted to rough classifiers to reduce the size of some categorical attributes. Uncertainty in data In the algebraic space, rough set theory approximates given concept (s) using lower and upper sets of the concept (s).
Reference: [23] <author> J. Ching, A. Wong, and K. Chan, </author> <title> "Class-dependent discretization for inductive learning from continuous and mixed mode data," </title> <journal> IEEE Trans. Knowledge and Data Eng., </journal> <volume> vol. 17, no. 7, </volume> <pages> pp. 641-651, </pages> <year> 1995. </year>
Reference-contexts: Horizontal reduction is related to merging identical tuples following either the substitution of an attribute value by its higher level value in a pre-defined generalization hierarchy of categorical values of the attribute [22] or the quantization (or discretization) of continuous (or numeric) values <ref> [13, 15, 23] </ref>. Vertical reduction is realized by either applying some feature selection methods or using attribute dependency graph [24]. We consider vertical reduction as a part of methods for handling redundant data, in Section 3.5. We elaborate on some notable studies on horizontal reduction in the following. <p> In other words, their criteria of discretization fail to take into consideration the relationship between pre-assigned classes and interval boundaries. Both Ching et al. <ref> [23] </ref> and Fayyad & Irani [15] suggest class dependent discretization algorithms. Note that the whole idea here is to reduce the number of attribute values without destroying the interdependence relationship between the class and attribute values. Class-dependent discretization of Ching et al. [23] consists of three main processes: interval initialization, interval <p> Both Ching et al. <ref> [23] </ref> and Fayyad & Irani [15] suggest class dependent discretization algorithms. Note that the whole idea here is to reduce the number of attribute values without destroying the interdependence relationship between the class and attribute values. Class-dependent discretization of Ching et al. [23] consists of three main processes: interval initialization, interval improvement, and interval reduction. <p> The ratio of the sizes of the training set to the test set is then determined from the bias and the variance of the estimated error [57]. For classification with mixed mode data <ref> [23] </ref>, the mutual information, between a class and an attribute, can be combined to determine the membership of an unknown object under the assumption that the given attributes are independent.
Reference: [24] <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Vertical reduction is realized by either applying some feature selection methods or using attribute dependency graph <ref> [24] </ref>. We consider vertical reduction as a part of methods for handling redundant data, in Section 3.5. We elaborate on some notable studies on horizontal reduction in the following.
Reference: [25] <author> D. Stashuk and R. Naphan, </author> <title> "Probabilistic inference based classification applied to myoelectric signal decomposition," </title> <journal> IEEE Trans. on Biomedical Engineering, </journal> <month> June </month> <year> 1992. </year>
Reference-contexts: A variation of that method is the use of Shannon's entropy theory such that the entropy scheme determines the interval boundaries by making the total gain of information from the observed occurrences in each interval equal. This procedure is called `even information intervals quantization' method <ref> [25] </ref>. The obvious drawback of such a procedure is that there may be a large amount of information loss, because the cut points would not necessarily be on boundaries of pre-defined classes.
Reference: [26] <author> J. Quinlan and R. Rivest, </author> <title> "Inferring decision trees using the minumum description length principle," </title> <journal> Information and Computation, </journal> <volume> vol. 80, </volume> <pages> pp. 227-248, </pages> <year> 1989. </year>
Reference-contexts: Such a problem can be expressed in terms of Bayesian decision strategy, involving, for example, probability-of-error criterion. The decision criterion has been estimated using the minimum description length principle (MDLP) <ref> [26] </ref>. 3.2 Noisy Data Non-systematic errors, which can occur during data-entry or collection of data, are usually referred to as noise.
Reference: [27] <author> J. R. Quinlan, </author> <title> "The effect of noise on concept learning," in Machine Learning: An Artificial Intelligence Approach (R. </title> <editor> Michalski, J. Carbonell, and T. Mitchell, eds.), </editor> <volume> vol. 2, </volume> <pages> pp. 149-166, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kauffmann Inc., </publisher> <year> 1986. </year>
Reference-contexts: This implies that a knowledge discovery method should be less sensitive to noise in the data set. This problem has been extensively investigated 8 Chapter 1 for variations of inductive decision trees, depending on where and how much the noise occurs <ref> [27] </ref>. If a training sample is corrupted with noise, the system should be able to identify and ignore it. <p> Presence of noise in the class information of training set affects the accuracy of generated rules; hence an attempt should be made to eliminate noise that affects the class information of the objects in the training set. Quinlan <ref> [27] </ref> has performed experiments to investigate the effect of noise on classifying examples from the test set. The experimental results indicate that for some systems adding substantial noise to the data results in low level of misclassification of unseen examples (test set).
Reference: [28] <author> T. Luba and R. Lasocki, </author> <title> "On unknown attribute values in functional dependencies," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Soft Computing, </booktitle> <address> (San Jose, CA), </address> <pages> pp. 490-497, </pages> <year> 1994. </year>
Reference-contexts: Other than this work, which does not offer any solution for existing data, we have not come across any work that deals with null values, though there are some recent studies on unknown values <ref> [28, 29, 30] </ref>. When the database contains missing attribute values, either the values can be discarded or an attempt can be made to replace them with the most likely values [19]. These are the ideas adopted by Quinlan [19] for inductive decision trees.
Reference: [29] <author> J. W. Grzymala-Busse, </author> <title> "On the unknown attribute values in learning from examples," </title> <booktitle> in Proceedings of Methodologies for Intelligent Systems (Z. </booktitle> <editor> W. Ras and M. Zemankowa, eds.), </editor> <booktitle> Lecture Notes in AI, </booktitle> <volume> 542, </volume> <pages> pp. 368-377, </pages> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1991. </year> <note> 34 Chapter 1 </note>
Reference-contexts: Other than this work, which does not offer any solution for existing data, we have not come across any work that deals with null values, though there are some recent studies on unknown values <ref> [28, 29, 30] </ref>. When the database contains missing attribute values, either the values can be discarded or an attempt can be made to replace them with the most likely values [19]. These are the ideas adopted by Quinlan [19] for inductive decision trees. <p> These rules can then be used to "fill in" the missing attribute values and the resulting data set could be used to construct the descriptions. Grzymala-Busse <ref> [29] </ref>, citing the drawbacks of the approaches given above, has transformed a given decision table with unknown values to a new and possibly inconsistent decision table, in which every attribute value is known, by replacing the unknown value of an attribute with all possible values of that attribute.
Reference: [30] <author> B. Thiesson, </author> <title> "Accelerated quantification of bayesian networks with incomplete data," </title> <booktitle> in The First International Conference on Knowledge Discovery and Data Mining (U. </booktitle> <editor> Fayyad and R. Uthurusamy, eds.), </editor> <address> (Montreal, Quebec, Canada), </address> <pages> pp. 306-311, </pages> <month> aug </month> <year> 1995. </year>
Reference-contexts: Other than this work, which does not offer any solution for existing data, we have not come across any work that deals with null values, though there are some recent studies on unknown values <ref> [28, 29, 30] </ref>. When the database contains missing attribute values, either the values can be discarded or an attempt can be made to replace them with the most likely values [19]. These are the ideas adopted by Quinlan [19] for inductive decision trees. <p> Probabilistic relational operations would yield certain or possible probabilities (lower or upper bounds on the probability of a random variable) depending on whether missing probabilities are facilitated, or not. In <ref> [30] </ref>, the problem of missing value is solved using the EM algorithm.
Reference: [31] <author> J. R. Quinlan, </author> <title> "Unknown attribute values in induction," </title> <booktitle> in Proceedings of the Sixth International Machine Learning Workshop (A. </booktitle> <editor> M. Segre, ed.), </editor> <address> (San Mateo, CA), </address> <pages> pp. 164-168, </pages> <publisher> Morgan Kaufmann Pub., </publisher> <year> 1989. </year>
Reference-contexts: When the database contains missing attribute values, either the values can be discarded or an attempt can be made to replace them with the most likely values [19]. These are the ideas adopted by Quinlan [19] for inductive decision trees. In <ref> [31] </ref> it is suggested to construct rules that predict the value of the missing attribute, Data Mining 9 based on the value of other attributes in the example, and the class information.
Reference: [32] <author> S. K. M. Wong and W. Ziarko, </author> <title> "Comparison of the probabilistic approximate classification and fuzzy set model," </title> <journal> Fuzzy Sets and Systems, </journal> <volume> no. 21, </volume> <pages> pp. 357-362, </pages> <year> 1982. </year>
Reference-contexts: Another approach is based on the rough set theory which provides the lower and upper approximations of a concept depending on how relationship between two 10 Chapter 1 different partitions of a finite universe of discourse is defined. If this relationship is probabilistic in nature, Wong and Ziarko <ref> [32] </ref> demonstrated that the generalized notion of rough sets can indeed be conveniently described by the concept of fuzzy sets when proper fuzzy set operations are employed.
Reference: [33] <author> Y. Y. Yao and K. M. Wong, </author> <title> "A decision theoretic framework for approximating concepts," </title> <journal> International Journal Man-Machine Studies, </journal> <volume> vol. 37, </volume> <pages> pp. 793-809, </pages> <year> 1992. </year>
Reference-contexts: If this relationship is probabilistic in nature, Wong and Ziarko [32] demonstrated that the generalized notion of rough sets can indeed be conveniently described by the concept of fuzzy sets when proper fuzzy set operations are employed. In a related study <ref> [33] </ref>, Wong and Yao introduced a Bayesian decision theoretic framework which provides a plausible unification of the fuzzy set and rough set approaches for approximating a concept.
Reference: [34] <author> J. Mingers, </author> <title> "An empirical comparison of selection measures for decision tree induction," </title> <journal> Machine Learning, </journal> <volume> vol. 3, </volume> <pages> pp. 319-342, </pages> <year> 1989. </year>
Reference-contexts: In the rest of this subsection, we review work on inductive decision trees aimed at making them suitable for incomplete data. ID3-like algorithms <ref> [19, 34, 35] </ref>, during the process of inducing decision trees as well as of refining induced decision trees, implicitly assume that enough information is available in the data to decide exactly how each object should be classified.
Reference: [35] <author> M. Modrzejewski, </author> <title> "Feature selection using rough sets theory," </title> <booktitle> in Machine Learning: Proceedings of ECML-93 (P. </booktitle> <editor> B. Brazdil, </editor> <publisher> ed.), </publisher> <pages> pp. 213-226, </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: In the rest of this subsection, we review work on inductive decision trees aimed at making them suitable for incomplete data. ID3-like algorithms <ref> [19, 34, 35] </ref>, during the process of inducing decision trees as well as of refining induced decision trees, implicitly assume that enough information is available in the data to decide exactly how each object should be classified. <p> This type of exhaustive search would be appropriate only if l is small and J is computationally inexpensive. Greedy approaches like stepwise backward/forward techniques <ref> [20, 35] </ref>, dynamic programming [41], and branch and bound algorithm [42] are non-exhaustive and efficient search techniques, which can be applied with some feature selection criterion.
Reference: [36] <author> R. Uthurusamy, U. Fayyad, and S. Spangler, </author> <title> "Learning useful rules from inconclusive data," in Knowledge Discovery in Databases (G. </title> <editor> Piatetsky-Shapiro and W. J. Frawley, eds.), </editor> <address> Cambridge, MA: </address> <publisher> AAAI/MIT, </publisher> <year> 1991. </year>
Reference-contexts: Hence, for some time, inconclusive objects in a training set, i.e., objects having the same description and yet different class labels, have been interpreted as noise either in their descriptions or in their labels. Uthurusamy et al. in <ref> [36] </ref> have argued that this assumption is not valid in the first place on the ground that inconclusive data sets are different from noisy data set, especially when descriptions of objects are incomplete to arrive at certain conclusions.
Reference: [37] <author> J. S. Deogun, V. V. Raghavan, and H. </author> <title> Sever, "Exploiting upper approximations in the rough set methodology," </title> <booktitle> in The First International Conference on Knowledge Discovery and Data Mining (U. </booktitle> <editor> Fayyad and R. Uthurusamy, eds.), </editor> <address> (Montreal, Quebec, Canada), </address> <pages> pp. 69-74, </pages> <month> aug </month> <year> 1995. </year>
Reference-contexts: The importance of feature selection in a broader sense is not only to reduce the search space, but also to speed up the processes of both concept learning and classifying objects and to improve the quality of classification <ref> [37, 38, 39, 40] </ref>. It is well known that searching for the smallest subset of features in the feature space takes time that is bounded by O (2 l J); where: l is the number of features, and J is the computational effort required to evaluate each subset. <p> of either individual features or combinations of features are computed with respect to some feature selection criteria (or measures) such as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics [43, 44]; Shan-non's entropy criterion, classification accuracy, or classification quality based on dice coefficient in pattern recognition and machine learning <ref> [37, 45, 46] </ref>. Projection Pursuit technique can also be used on the data to find "interesting low dimensional projections of a high dimensional point cloud by numerically maximizing a certain objective function or projection index" [47]. <p> If the evaluation criterion to derive the decision model is monotonic, then the training error can be controlled <ref> [37, 42] </ref>. In the process of estimating validation error, the concept of bootstrapping over test set may be used [12, 56]. <p> In terms of these new definitions, feature selection problem can be re-expressed as finding a -reduct of CON in S: A stepwise backward algorithm to find a -reduct of a given feature set was introduced by Deogun et al. in <ref> [37] </ref> on the premise that the quality of upper classifier decreases as the feature set is pruned down.
Reference: [38] <author> K. Kira and L. Rendell, </author> <title> "The feature selection problem: Tradational methods and a new algorithm," </title> <booktitle> in Proceedings of AAAI-92, </booktitle> <pages> pp. 129-134, </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: The importance of feature selection in a broader sense is not only to reduce the search space, but also to speed up the processes of both concept learning and classifying objects and to improve the quality of classification <ref> [37, 38, 39, 40] </ref>. It is well known that searching for the smallest subset of features in the feature space takes time that is bounded by O (2 l J); where: l is the number of features, and J is the computational effort required to evaluate each subset. <p> The usual remedy for this problem is to map nonquantitative (nominal) values into a numerical scale and use a distance function for the evaluation. For example, Kira & Rendell suggested a binary scale and the they used it in their Relief algorithm for feature selection <ref> [38] </ref>. Using more domain knowledge, Slowinski & Stefanowiski in [59] have suggested a distance measure based on mapping the difference between two values of an attribute into a well-ordered scale consisting of indifferent, weakly indifferent, strictly different, and excessively different symbols (or intervals).
Reference: [39] <author> H. Almuallim and T. Dietterich, </author> <title> "Learning with many irrelevant features," </title> <booktitle> in Proceedings of AAAI-91, </booktitle> <address> (Menlo Park, CA), </address> <pages> pp. 547-552, </pages> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: The importance of feature selection in a broader sense is not only to reduce the search space, but also to speed up the processes of both concept learning and classifying objects and to improve the quality of classification <ref> [37, 38, 39, 40] </ref>. It is well known that searching for the smallest subset of features in the feature space takes time that is bounded by O (2 l J); where: l is the number of features, and J is the computational effort required to evaluate each subset.
Reference: [40] <author> Z. Pawlak, K. Slowinski, and R. Slowinski, </author> <title> "Rough classification of patients after highly selective vagotomy for duodenal ulcer," </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> vol. 24, </volume> <pages> pp. 413-433, </pages> <year> 1986. </year>
Reference-contexts: The importance of feature selection in a broader sense is not only to reduce the search space, but also to speed up the processes of both concept learning and classifying objects and to improve the quality of classification <ref> [37, 38, 39, 40] </ref>. It is well known that searching for the smallest subset of features in the feature space takes time that is bounded by O (2 l J); where: l is the number of features, and J is the computational effort required to evaluate each subset.
Reference: [41] <author> C. Y. Chang, </author> <title> "Dynamic programming as applied to feature subset selection in a pattern recognition system," </title> <journal> IEEE Trans. Syst., Man, Cybern., </journal> <volume> vol. SMC-3, </volume> <pages> pp. 166-171, </pages> <year> 1973. </year>
Reference-contexts: This type of exhaustive search would be appropriate only if l is small and J is computationally inexpensive. Greedy approaches like stepwise backward/forward techniques [20, 35], dynamic programming <ref> [41] </ref>, and branch and bound algorithm [42] are non-exhaustive and efficient search techniques, which can be applied with some feature selection criterion.
Reference: [42] <author> P. M. Narendra and K. Fukunaga, </author> <title> "A branch and bound algorithm for feature subset selection," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. c-26, no. 9, </volume> <pages> pp. 917-922, </pages> <year> 1977. </year>
Reference-contexts: This type of exhaustive search would be appropriate only if l is small and J is computationally inexpensive. Greedy approaches like stepwise backward/forward techniques [20, 35], dynamic programming [41], and branch and bound algorithm <ref> [42] </ref> are non-exhaustive and efficient search techniques, which can be applied with some feature selection criterion. <p> If the evaluation criterion to derive the decision model is monotonic, then the training error can be controlled <ref> [37, 42] </ref>. In the process of estimating validation error, the concept of bootstrapping over test set may be used [12, 56].
Reference: [43] <author> R. A. Devijver and J. Kittler, </author> <title> Pattern Recognation: A statistical approach. </title> <publisher> Lon-don: Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: For near-optimal solutions or optimal solutions in special cases, weights of either individual features or combinations of features are computed with respect to some feature selection criteria (or measures) such as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics <ref> [43, 44] </ref>; Shan-non's entropy criterion, classification accuracy, or classification quality based on dice coefficient in pattern recognition and machine learning [37, 45, 46]. <p> Clustering Query: We call unsupervised partitioning of tuples of a relational table a clustering query (also known as unsupervised learning in the context of inductive learning). There are numerous clustering algorithms ranging from the traditional methods of pattern recognition to clustering techniques in machine learning <ref> [43, 58] </ref>. User-defined parameters such as the number of clusters or the maximum number of tuples within a cluster can influence the result of 16 Chapter 1 a clustering query. Clustering queries may be helpful for the following two reasons.
Reference: [44] <author> A. J. Miller, </author> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall, </publisher> <year> 1990. </year> <title> Data Mining 35 </title>
Reference-contexts: For near-optimal solutions or optimal solutions in special cases, weights of either individual features or combinations of features are computed with respect to some feature selection criteria (or measures) such as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics <ref> [43, 44] </ref>; Shan-non's entropy criterion, classification accuracy, or classification quality based on dice coefficient in pattern recognition and machine learning [37, 45, 46].
Reference: [45] <author> U. M. Fayyad and K. B. Irani, </author> <title> "The attribute selection problem in decision tree generation," </title> <booktitle> in Proceedings of AAAI-92, </booktitle> <pages> pp. 104-110, </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: of either individual features or combinations of features are computed with respect to some feature selection criteria (or measures) such as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics [43, 44]; Shan-non's entropy criterion, classification accuracy, or classification quality based on dice coefficient in pattern recognition and machine learning <ref> [37, 45, 46] </ref>. Projection Pursuit technique can also be used on the data to find "interesting low dimensional projections of a high dimensional point cloud by numerically maximizing a certain objective function or projection index" [47].
Reference: [46] <author> P. Baim, </author> <title> "A method for attribute selection in inductive learning systems," </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 10, no. 4, </volume> <pages> pp. 888-896, </pages> <year> 1988. </year>
Reference-contexts: of either individual features or combinations of features are computed with respect to some feature selection criteria (or measures) such as Bhattacharya coefficient, divergence, Kolmogorov variational distance, etc., in statistics [43, 44]; Shan-non's entropy criterion, classification accuracy, or classification quality based on dice coefficient in pattern recognition and machine learning <ref> [37, 45, 46] </ref>. Projection Pursuit technique can also be used on the data to find "interesting low dimensional projections of a high dimensional point cloud by numerically maximizing a certain objective function or projection index" [47].
Reference: [47] <author> P. J. Huber, </author> <title> "Projection pursuit," </title> <journal> Annals of Statistics, </journal> <volume> vol. 13, no. 2, </volume> <pages> pp. 435-475, </pages> <year> 1985. </year>
Reference-contexts: Projection Pursuit technique can also be used on the data to find "interesting low dimensional projections of a high dimensional point cloud by numerically maximizing a certain objective function or projection index" <ref> [47] </ref>. These "interesting" projections could then be further analyzed to check for some unspec 12 Chapter 1 ified, unanticipated structures in the data. The projection pursuit methods are unaffected by the curse of dimensionality; however, they are poorly suited to deal with non-linear structures.
Reference: [48] <author> R. Agrawal, T. Imielinski, and A. Swami, </author> <title> "Database mining: A performance perspective," </title> <journal> IEEE Trans. Knowledge and Data Eng., </journal> <volume> vol. 5, no. 6, </volume> <pages> pp. 914-924, </pages> <year> 1993. </year>
Reference-contexts: This type of query along with interval classification has been suggested by Agrawal et al. in <ref> [48] </ref>. They represent knowledge as a set of rules, denoted by r : F (o) ) G (o); where: F is a conjunction of unary formulas, G is a unary formula. <p> Each rule r is associated with a confidence factor c; 0 c 1; which shows the strength of the rule r: The knowledge units considered in <ref> [48] </ref> are equivalent to the notion of ID3 trees, except that continuous values are partitioned into intervals in contrast to ID3 that uses binary splitting for this purpose.
Reference: [49] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <year> 1973. </year>
Reference-contexts: Difference between two classes may be described by discriminating descriptions such as decision trees and decision lists. Many empirical learning algorithms, such as decision tree inducers, neural networks and genetic algorithms are designed to produce discriminating descriptions. This subject has extensively been investigated in the literature <ref> [49, 50, 51, 52, 53] </ref> and is the primary task in inductive learning. Note that this type of inductive learning can potentially help in predicting the future. In order to predict the future, known results from the past should be used as much as possible.
Reference: [50] <author> S. Salzberg, </author> <title> Learning with Nested Generalized Exemplars. </title> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1990. </year>
Reference-contexts: Difference between two classes may be described by discriminating descriptions such as decision trees and decision lists. Many empirical learning algorithms, such as decision tree inducers, neural networks and genetic algorithms are designed to produce discriminating descriptions. This subject has extensively been investigated in the literature <ref> [49, 50, 51, 52, 53] </ref> and is the primary task in inductive learning. Note that this type of inductive learning can potentially help in predicting the future. In order to predict the future, known results from the past should be used as much as possible.
Reference: [51] <author> S. M. Weiss and C. A. </author> <title> Kulikowski, Computer Systems that Learn. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Difference between two classes may be described by discriminating descriptions such as decision trees and decision lists. Many empirical learning algorithms, such as decision tree inducers, neural networks and genetic algorithms are designed to produce discriminating descriptions. This subject has extensively been investigated in the literature <ref> [49, 50, 51, 52, 53] </ref> and is the primary task in inductive learning. Note that this type of inductive learning can potentially help in predicting the future. In order to predict the future, known results from the past should be used as much as possible.
Reference: [52] <author> R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, </author> <title> Machine Learning: </title> <booktitle> An Artificial Intelligence Approach. </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga, </publisher> <year> 1983. </year>
Reference-contexts: Difference between two classes may be described by discriminating descriptions such as decision trees and decision lists. Many empirical learning algorithms, such as decision tree inducers, neural networks and genetic algorithms are designed to produce discriminating descriptions. This subject has extensively been investigated in the literature <ref> [49, 50, 51, 52, 53] </ref> and is the primary task in inductive learning. Note that this type of inductive learning can potentially help in predicting the future. In order to predict the future, known results from the past should be used as much as possible.
Reference: [53] <author> J. Shavlik and T. Diettrich, </author> <booktitle> Readings in Machine Learning. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Difference between two classes may be described by discriminating descriptions such as decision trees and decision lists. Many empirical learning algorithms, such as decision tree inducers, neural networks and genetic algorithms are designed to produce discriminating descriptions. This subject has extensively been investigated in the literature <ref> [49, 50, 51, 52, 53] </ref> and is the primary task in inductive learning. Note that this type of inductive learning can potentially help in predicting the future. In order to predict the future, known results from the past should be used as much as possible.
Reference: [54] <author> S. Muggleton, A. Srinivasan, and M. Bain, </author> <title> "Compression, significance and accuracy," </title> <booktitle> in Proceedings of 9th International Workshop on Machine Learning, </booktitle> <address> (ML92), (Aberdeen, Scotland), </address> <publisher> Morgan Kauffmann, </publisher> <year> 1992. </year>
Reference-contexts: Classification accuracy (or classification error) is then measured as the fraction of objects/instances in test data that are incorrectly classified. There have been indications that the accuracy of a rule (as measured on training set) may not be a good indicator of its accuracy in general <ref> [54] </ref>. This is especially true on noisy data; DNF concept learners typically learn a few reliable disjuncts and many unreliable disjuncts each of which covers a small number of positive training examples [55].
Reference: [55] <author> R. Holte, L. Acker, and B. Porter, </author> <title> "Concept learning and the problem of small disjuncts," </title> <booktitle> in Proceedings of 11th International Joint Conference on Artificial Intelligence, </booktitle> <address> (Detroit, MI), </address> <publisher> Morgan Kauffmann, </publisher> <year> 1989. </year>
Reference-contexts: This is especially true on noisy data; DNF concept learners typically learn a few reliable disjuncts and many unreliable disjuncts each of which covers a small number of positive training examples <ref> [55] </ref>. If the evaluation criterion to derive the decision model is monotonic, then the training error can be controlled [37, 42]. In the process of estimating validation error, the concept of bootstrapping over test set may be used [12, 56]. <p> According to a reported study <ref> [55] </ref>, DNF concept learning algorithms may induce many unreliable disjuncts each of which covers a small number of positive training examples. Since rough classifiers can be viewed as a DNF concept learner, and the study to incorporate the unified quality measure into post-pruning process can be well justified.
Reference: [56] <author> B. Efron and R. Tibshirani, </author> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman & Hall, </publisher> <year> 1993. </year>
Reference-contexts: If the evaluation criterion to derive the decision model is monotonic, then the training error can be controlled [37, 42]. In the process of estimating validation error, the concept of bootstrapping over test set may be used <ref> [12, 56] </ref>. Note that dividing the samples into training and test sets is an important problem and must be solved in a way that the distributions of the two sets are close to each other.
Reference: [57] <author> K. Fukunaga and R. Hayes, </author> <title> "Effects of sample size in classifier design," </title> <journal> IEEE Trans. on Pattern analysis and Machine Intelligence, </journal> <volume> vol. 11, no. 8, </volume> <pages> pp. 873-885, </pages> <year> 1985. </year>
Reference-contexts: The ratio of the sizes of the training set to the test set is then determined from the bias and the variance of the estimated error <ref> [57] </ref>. For classification with mixed mode data [23], the mutual information, between a class and an attribute, can be combined to determine the membership of an unknown object under the assumption that the given attributes are independent.
Reference: [58] <author> M. P. D. Fisher and P. Langley, </author> <title> Concept Formation, Knowledge and Experience in Unsupervised Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Clustering Query: We call unsupervised partitioning of tuples of a relational table a clustering query (also known as unsupervised learning in the context of inductive learning). There are numerous clustering algorithms ranging from the traditional methods of pattern recognition to clustering techniques in machine learning <ref> [43, 58] </ref>. User-defined parameters such as the number of clusters or the maximum number of tuples within a cluster can influence the result of 16 Chapter 1 a clustering query. Clustering queries may be helpful for the following two reasons.
Reference: [59] <author> R. Slowinski and J. Stefanowiski, </author> <title> "Rough classification with valued closeness relation," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Knowledge Discovery, </booktitle> <address> (San Jose, CA), </address> <year> 1995. </year>
Reference-contexts: =) x 2 X; 18 Chapter 1 ii. x 2 BN D A (X) =) unknown; and iii. x 2 N EG A (X) =) x is not in X: Note that if x is not in one of regions, then a decision may be taken on using closeness heuristic <ref> [59] </ref>, provided that each region and object have some type of descriptions. For the sake of simplicity, the decision algorithm D A (X) is assumed to be a set of decision rules, where each rule gives positive answer. There are two approximation methods defined in algebraic approximation spaces: a. <p> For example, Kira & Rendell suggested a binary scale and the they used it in their Relief algorithm for feature selection [38]. Using more domain knowledge, Slowinski & Stefanowiski in <ref> [59] </ref> have suggested a distance measure based on mapping the difference between two values of an attribute into a well-ordered scale consisting of indifferent, weakly indifferent, strictly different, and excessively different symbols (or intervals).
Reference: [60] <author> J. S. Deogun, V. V. Raghavan, and H. </author> <title> Sever, "Rough set based classification methods and extended decision tables," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Soft Computing, </booktitle> <address> (San Jose, California), </address> <pages> pp. 302-309, </pages> <year> 1994. </year> <note> 36 Chapter 1 </note>
Reference-contexts: Deogun at al. in <ref> [60] </ref> have proposed a unification of these two measures, which is the normalized size of intersection between approximated concept, X; and its positive region in an approximation space A; P OS A (X); as formalized below. <p> This problem has been the subject of numerous studies on developing rough approximation methods based on different defini Data Mining 21 tions of positive (and boundary) regions <ref> [60, 62, 63, 64] </ref>. For example, in the elementary set approximation of an unknown concept [60], an elementary set is mapped to the positive region of an unknown concept if its degree of membership is bigger than a user defined threshold value. <p> This problem has been the subject of numerous studies on developing rough approximation methods based on different defini Data Mining 21 tions of positive (and boundary) regions [60, 62, 63, 64]. For example, in the elementary set approximation of an unknown concept <ref> [60] </ref>, an elementary set is mapped to the positive region of an unknown concept if its degree of membership is bigger than a user defined threshold value. <p> In the rough set literature, the terms `inconsistent' and `nondeterministic' decision algorithms (or rules) are used interchangeably, though they are different concepts. The `inconsistency' is attributed to the result of a classification method while the `nondeterminism' is attributed to the interpretation of that result. As shown in <ref> [60] </ref>, inconsistent decision algorithms, under an appropriate representation structure, can be interpreted deterministically as well as nondeter-ministically. This is an important result, particularly when the background knowledge is incomplete and dynamic. Redundant data Redundant data can be eliminated by pruning insignificant attributes with respect to a certain problem at hand. <p> Similarly, a further refinement of antecedent parts of rules in a decision algorithm is a part of the summary if the decision algorithm is persistent in the system and the background knowledge from which the decision algorithm has been induced is dynamic. Deogun et al. in <ref> [60] </ref> extended decision tables to represent upper classifiers such that each tuple contains a special and composed field, called incremental information, which contains the number of objects that satisfy condition part of a decision rule and the number of objects being classified correctly by the same decision rule. <p> Incremental rough approximation: This is a must feature that has to be provided for if the decision algorithm is to be persistent in the rough set model and the background knowledge is dynamic. One of the claims made by Deogun et al. in <ref> [60] </ref> is that evolving rough classifier schemes can be developed, if the decision table is accommodated with a composite increment field that contains frequencies of rows.
Reference: [61] <author> W. Ziarko and N. Shan, "KDD-R: </author> <title> a comprehensive system for knowledge discovery in databases using rough sets," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Soft Computing, </booktitle> <address> (San Jose, California), </address> <pages> pp. 164-173, </pages> <year> 1994. </year>
Reference-contexts: For example, in KDD-R system, the data preprocessing unit discretizes the numerical attributes either by applying user-supplied discretization formula or by using an automatic discretization algorithm <ref> [61] </ref>. Alternatively, horizontal reduction of a very large data set table may use a generalization hierarchy of attributes to merge identical tuples, after the substitution of an attribute value, by its higher level concept in the generalization hierarchy. <p> In terms of completeness of the rules, it is noted that All Rules provide the most complete set while the error rates of the rule sets induced by the machine learning options are the worst. System KDD-R <ref> [61] </ref> is a software providing a collection of rough sets based tools for comprehensive data analysis. <p> Towards this direction, there have already been some reported works on using the rough set methodology based knowledge discovery tools on off-line data; KDD-R, an experimental open tool box <ref> [61] </ref>; LERS, a machine learning system from examples [74]; and DataLogic/R [73], a commercial product for data mining and decision support. In the following, we present future research directions that are critical for data mining applications.
Reference: [62] <author> J. D. Katzberg and W. Ziarko, </author> <title> "Variable precision rough sets with asymmetric bounds," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Knowledge Discovery, </booktitle> <address> (Banff, Alberta, Canada), </address> <pages> pp. 163-190, </pages> <year> 1993. </year>
Reference-contexts: This problem has been the subject of numerous studies on developing rough approximation methods based on different defini Data Mining 21 tions of positive (and boundary) regions <ref> [60, 62, 63, 64] </ref>. For example, in the elementary set approximation of an unknown concept [60], an elementary set is mapped to the positive region of an unknown concept if its degree of membership is bigger than a user defined threshold value. <p> It is the process of reducing an information system such that the set of attributes of the reduced information system is independent and no attribute can be eliminated further without losing some information from the system, the result of which is called reduct <ref> [62, 65] </ref>. Given the fact that exhaustive search over the attribute space is exponential in the number of attributes it might not always be computationally feasible to search for the minimum size reduct of attributes.
Reference: [63] <author> Y. Y. Yao and X. Li, </author> <title> "Uncertainty reasoning with interval-set algebra," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Knowledge Discovery, </booktitle> <address> (Banff, Alberta, Canada), </address> <pages> pp. 191-201, </pages> <year> 1993. </year>
Reference-contexts: This problem has been the subject of numerous studies on developing rough approximation methods based on different defini Data Mining 21 tions of positive (and boundary) regions <ref> [60, 62, 63, 64] </ref>. For example, in the elementary set approximation of an unknown concept [60], an elementary set is mapped to the positive region of an unknown concept if its degree of membership is bigger than a user defined threshold value.
Reference: [64] <author> R. R. Hashemi, B. A. Pearce, W. G. Hinson, M. G. Paule, and J. F. Young, </author> <title> "IQ estimation of monkeys based on human data using rough sets," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Soft Computing, </booktitle> <address> (San Jose, California), </address> <pages> pp. 400-407, </pages> <year> 1994. </year>
Reference-contexts: This problem has been the subject of numerous studies on developing rough approximation methods based on different defini Data Mining 21 tions of positive (and boundary) regions <ref> [60, 62, 63, 64] </ref>. For example, in the elementary set approximation of an unknown concept [60], an elementary set is mapped to the positive region of an unknown concept if its degree of membership is bigger than a user defined threshold value.
Reference: [65] <author> Z. Pawlak, </author> <title> "Rough classification," </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> vol. 20, </volume> <pages> pp. 469-483, </pages> <year> 1984. </year>
Reference-contexts: It is the process of reducing an information system such that the set of attributes of the reduced information system is independent and no attribute can be eliminated further without losing some information from the system, the result of which is called reduct <ref> [62, 65] </ref>. Given the fact that exhaustive search over the attribute space is exponential in the number of attributes it might not always be computationally feasible to search for the minimum size reduct of attributes.
Reference: [66] <author> R. Kohavi and B. Frasca, </author> <title> "Useful feature subsets and rough set reducts," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Soft Computing, </booktitle> <address> (San Jose, California), </address> <pages> pp. 310-317, </pages> <year> 1994. </year>
Reference-contexts: Furthermore, finding just a single reduct of the attributes may be too restrictive for some data analysis problems, which is one of the arguments stated in Kohavi & Frasca's paper <ref> [66] </ref>. One plausible approach is to utilize the idea of -reduct as described below.
Reference: [67] <author> J. S. Deogun, V. V. Raghavan, and H. </author> <title> Sever, "Rough set model for database mining applications," </title> <type> Tech. Rep. </type> <institution> TR-94-6-10, The University of Southwestern Louisiana, The Center for Advanced Computer Studies, </institution> <year> 1994. </year>
Reference-contexts: The hypothesis testing and association between values of an attribute can easily be solved by the rough set methodology (see Deogun et al. <ref> [67] </ref>). A recent theoretical paper by Kent [68] extends the notions of approximation and rough equality to formal concept analysis.
Reference: [68] <author> R. E. Kent, </author> <title> "Rough concept analysis," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Knowledge Discovery, </booktitle> <address> (Banff, Alberta, Canada), </address> <pages> pp. 245-253, </pages> <year> 1993. </year>
Reference-contexts: The hypothesis testing and association between values of an attribute can easily be solved by the rough set methodology (see Deogun et al. [67]). A recent theoretical paper by Kent <ref> [68] </ref> extends the notions of approximation and rough equality to formal concept analysis. An immediate result of this study, in our data mining context, is to be able to use the rough set methodology for the characterization of a concept (or more generally for concept exploration).
Reference: [69] <author> J. Berry, </author> <title> "Database marketing," </title> <booktitle> Business Week, </booktitle> <pages> pp. 56-62, </pages> <month> September 5 </month> <year> 1994. </year>
Reference-contexts: Knowledge discovery in databases is changing the face of today's business world and has opened up new frontiers in the realm of science. In the business world, one of the most successful and widespread application of KDD is "Database Marketing" <ref> [69] </ref>. Marketers are collecting mountains of information about customers, looking for patterns among existing customer preferences and using that knowledge to predict future customer behavior and to craft a marketing message that targets such potential customers.
Reference: [70] <author> K. A. Kaufmann, R. S. Michalski, and L. Kerschberg, </author> <title> "Mining for knowledge in databases: Goals and general description of the INLEN system," in Knowledge Discovery in Databases (W. </title> <editor> J. Frawley, G. Piatetsky-Shapiro, and C. J. Matheus, eds.), </editor> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: The INLEN system <ref> [70] </ref>, which is partially operational, combines database, knowledge base, and a wide spectrum of machine learning techniques to assist a data analysis expert to extract new or better knowledge from the database or/and knowledge base and discover interesting regularities in the database.
Reference: [71] <author> P. Hoschka and W. Klosgen, </author> <title> "A support system for interpreting statistical data," in Knowledge Discovery in Databases (G. </title> <editor> Piatetsky-Shapiro and W. J. Frawley, </editor> <booktitle> eds.), </booktitle> <pages> pp. 325-345, </pages> <address> Cambridge, MA: </address> <publisher> AAAI/MIT, </publisher> <year> 1991. </year>
Reference-contexts: KDW is "ideal for exploratory data analysis by a user knowledgeable in both data and operation of discovery tools". However such heavy reliance on the user has given the system a low ranking on the autonomy scale. Explora <ref> [4, 71] </ref> is another KDD system that incorporates a variety of search strategies to adapt discovery processes to the requirements of applications. It operates by performing a graph search through a network of patterns, searching for instances of interesting patterns.
Reference: [72] <institution> Integrated Solutions, Ltd., Hampshire, England, Clementine Software for Data Mining. </institution>
Reference-contexts: A user of Explora experiences a moderately high degree of versatility and autonomy. One of the leading data mining toolkit of modern era, that has been subjected to diverse applications, is Clementine <ref> [72] </ref>. Clementine is built on the technologies of neural networks and rule induction and hence can automatically identify the relationships in the data and generate rules to apply to future cases.
Reference: [73] <author> A. J. Szladow, "DataLogic/R: </author> <title> for database mining and decision support," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Knowledge Discovery, </booktitle> <address> (Banff, Alberta, Canada), p. 511, </address> <year> 1993. </year>
Reference-contexts: Clementine has been applied to verify incoming foreign exchange stock price data, model skin corrosivity, select locations for retail outlets, anticipating toxic health hazards, and predicting audiences for television programs for the British Broadcasting Corporation (BBC). DataLogic/R <ref> [73] </ref> is another software designed to perform multiple tasks in data analysis, knowledge discovery and reasoning from data and is based on the concept of rough set analysis. The analysis and pattern discovery involves elimination of redundant attributes, elimination of redundant data and generation of classification rules. <p> Towards this direction, there have already been some reported works on using the rough set methodology based knowledge discovery tools on off-line data; KDD-R, an experimental open tool box [61]; LERS, a machine learning system from examples [74]; and DataLogic/R <ref> [73] </ref>, a commercial product for data mining and decision support. In the following, we present future research directions that are critical for data mining applications.
Reference: [74] <author> J. W. Grzymala-Busse, </author> <title> "The rule induction system LERS Q: a version for personal computers," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Knowledge Discovery, </booktitle> <address> (Banff, Alberta, Canada), p. 509, </address> <year> 1993. </year> <title> Data Mining 37 </title>
Reference-contexts: By varying the roughness, DataLogic/R can discover knowledge at different levels of detail. DataLogic/R has been used successfully in the "automated discovery of control rules for N O X and SO 2 emissions from utility boilers", and market analysis. The system LERS (Learning from Examples based on Rough Sets) <ref> [74, 75] </ref> induces a set of rules from examples given in the form of a decision table. The examples in the table are described by the values of attributes and are characterized by a value of a decision, as assigned by an expert. <p> Towards this direction, there have already been some reported works on using the rough set methodology based knowledge discovery tools on off-line data; KDD-R, an experimental open tool box [61]; LERS, a machine learning system from examples <ref> [74] </ref>; and DataLogic/R [73], a commercial product for data mining and decision support. In the following, we present future research directions that are critical for data mining applications. <p> A less restrictive version of the problem, which is known as unknown attribute values, has been studied by Grzymala-Busse and implemented in the LERS, a machine learning system <ref> [74] </ref>. Characterization query: Even though data dependency analysis within the rough set methodology can be applied to characterize concepts, it lacks of an explicit context dimension that is very important notion when a knowledge model contains a set/hierarchy of persistent concepts.
Reference: [75] <author> D. M. Grzymala-Busse and J. W. Grzymala-Busse, </author> <title> "Comparison of machine learning and knowledge acquisition methods of rule induction based on rough sets," </title> <booktitle> in Proceedings of the International Workshop on Rough Sets and Knowledge Discovery, </booktitle> <address> (Banff, Alberta, Canada), </address> <pages> pp. 297-306, </pages> <year> 1993. </year>
Reference-contexts: By varying the roughness, DataLogic/R can discover knowledge at different levels of detail. DataLogic/R has been used successfully in the "automated discovery of control rules for N O X and SO 2 emissions from utility boilers", and market analysis. The system LERS (Learning from Examples based on Rough Sets) <ref> [74, 75] </ref> induces a set of rules from examples given in the form of a decision table. The examples in the table are described by the values of attributes and are characterized by a value of a decision, as assigned by an expert.
Reference: [76] <author> T. Anand and G. Kahn, </author> <title> "Spotlight: A data explanation system," </title> <booktitle> in Proceedings of the Eighth IEEE Conference on Applied AI, </booktitle> <address> (Washington, D.C.), </address> <pages> pp. 2-8, </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: decision process| individually each rule is treated as a piece of uncertain evidence and hence worth a little in the process of decision making; however, along with similar other rules, it can provide a substantial input to the decision making process. 6.2 Application-specific Systems Commercial systems, like CoverStory [4], Spotlight <ref> [76] </ref> and KEFIR [4], have been developed to discover knowledge in databases using the concept of deviations. Deviations are powerful because they provide a simple way of identifying interesting pattern in the data.
Reference: [77] <author> K. Hatonen, M. Klemettinen, H. Mannila, and P. Ronkinen, </author> <title> "Knowledge discovery from telecommunications network alarm databases," </title> <booktitle> in Proceedings of the 12th International Conference on Data Engineering (C. </booktitle> <editor> Bogdan, ed.), </editor> <address> (New Orleans, LA), </address> <month> feb/mar </month> <year> 1996. </year>
Reference-contexts: Once this rate is known, rules can be regenerated "every n years from the immediate past data so as to continue holding up the predictive performance". Knowledge Discovery techniques using associative rules has been explored in TASA (Telecommunication Network Alarm Sequence Analyzer) <ref> [77] </ref>. It is an experimental knowledge discovery system developed for predicting faults in a telecommunication network. An alarm in a telecommunication network occurs whenever a part of the system behaves abnormally. A network typically generates 200-1000 alarms per day.
Reference: [78] <author> R. Wille, </author> <title> "Restructuring lattice theory: An approach based on hierarchies on concepts," in Ordered Sets (I. </title> <editor> Rival, ed.), Dordrecht-Boston: </editor> <publisher> Reidel, </publisher> <year> 1982. </year>
Reference-contexts: For example, characterization of the concept `Windows' within the context of `product' is certainly different from that of the one within the context of `sale'. This subject has been formally studied by Wille <ref> [78] </ref> and used for concept modeling. We believe that this study can be further extended to capture approximate characterization of concepts. In summary, data mining is a practical problem that drives theoretical studies toward understanding and reasoning about large and existing data.
References-found: 78

