URL: http://www.cs.jhu.edu/~sheppard/cs.605.755/papers/paper12-2.ps
Refering-URL: http://www.cs.jhu.edu/~sheppard/cs.605.755/sched.html
Root-URL: http://www.cs.jhu.edu
Email: maja@cs.brandeis.edu  
Phone: tel: (617) 736-2708, fax: (617) 736-2741  
Title: Learning Social Behaviors  
Author: Maja J Mataric 
Address: Waltham, MA 02254  
Affiliation: Volen Center for Complex Systems Computer Science Department Brandeis University  
Abstract: This paper discusses the challenges of learning to behave socially in a group of greedy agents. We build on our previous work, which introduced a methodology for synthesizing basic behaviors that serve as a substrate for generating a large repertoire of higher-level group interactions. In this paper we describe how, given the substrate, greedy agents can learn social rules that benefit the group as a whole. While this is a well-defined problem for rational agents, it is less so in dynamic, noisy situated domains. We describe three sources of reinforcement and show their necessity for learning non-greedy social rules. We then demonstrate the learning approach on a group of four mobile robots learning to yield and share information in a foraging task. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Axelrod, R. </author> <year> (1984), </year> <title> The Evolution of Cooperation, </title> <publisher> Basic Books, </publisher> <address> NY. </address>
Reference: <author> Brooks, R. A. </author> <year> (1987), </year> <title> A Hardware Retargetable Distributed Layered Architecture for Mobile Robot Control, </title> <booktitle> in `IEEE International Conference on Robotics and Automation', </booktitle> <address> Raleigh, NC, </address> <pages> pp. 106-110. </pages>
Reference: <author> Brooks, R. A. </author> <year> (1990), </year> <title> The Behavior Language; User's Guide, </title> <type> Technical Report AIM-1127, </type> <institution> MIT Artificial Intelligence Lab. </institution>
Reference-contexts: Section 9 lists the experiments that were performed and Section 10 gives the results. Finally, Section 11 describes continuing work and concludes the paper. 2 Related Work Previous work in behavior-based control has addressed the problem of learning behavior selection <ref> (Maes & Brooks 1990) </ref>, learning box pushing (Mahadevan & Connell 1991), and learning navigation on an individual robot (Millan 1994) using reinforcement learning (RL). Very few examples of multi-robot learning have been demonstrated so far. <p> The robots are programmed in the Behavior Language, a parallel programming language based on the Subsump-tion Architecture <ref> (Brooks 1990, Brooks 1987) </ref>. Their control systems are collections of parallel, concurrently active behaviors.
Reference: <author> Chapman, D. & Kaelbling, L. P. </author> <year> (1991), </year> <title> Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons, </title> <booktitle> in `Proceedings, IJCAI-91', </booktitle> <address> Sydney, Australia. </address>
Reference: <author> Chase, I. D. </author> <year> (1982), </year> <title> `Dynamics of Hierarchy Formation: The Sequential Development of Dominance Relationships', </title> <booktitle> Behaviour 80, </booktitle> <pages> 218-240. </pages>
Reference-contexts: Specifically, only a few classes of group sizes and prototypical relations among agents are relevant for any given type of interaction. The canonical form of social relation is the dominance hierarchy or pecking order, ubiquitous in animal societies, from hermit crabs and chickens to primates and people <ref> (Chase 1982, Chase & Rohwer 1987) </ref>. Besides mating, the majority of animal social interaction focuses on establishing and maintaining pecking orders (Cheney & Seyfarth 1990). Although no directly derivable evolutionary benefit has been proven, hierarchies certainly serve to ritualize and thus simplify social interactions.
Reference: <author> Chase, I. D. & Rohwer, S. </author> <year> (1987), </year> <title> `Two Methods for Quantifying the Development of Dominance Hierarchies in Large Groups with Application to Harris' Sparrows', </title> <booktitle> Animal Behavior 35, </booktitle> <pages> 1113-1128. </pages>
Reference-contexts: Although no directly derivable evolutionary benefit has been proven, hierarchies certainly serve to ritualize and thus simplify social interactions. Furthermore, some biological data supports the hypothesis that the number of levels in dominance hierarchies is bounded and relatively stable across a large spectrum of species <ref> (Chase & Rohwer 1987) </ref>.
Reference: <author> Cheney, D. L. & Seyfarth, R. M. </author> <year> (1990), </year> <title> How Monkeys See the World, </title> <publisher> The University of Chicago Press, Chicago. </publisher>
Reference-contexts: The canonical form of social relation is the dominance hierarchy or pecking order, ubiquitous in animal societies, from hermit crabs and chickens to primates and people (Chase 1982, Chase & Rohwer 1987). Besides mating, the majority of animal social interaction focuses on establishing and maintaining pecking orders <ref> (Cheney & Seyfarth 1990) </ref>. Although no directly derivable evolutionary benefit has been proven, hierarchies certainly serve to ritualize and thus simplify social interactions.
Reference: <author> Davis, J. M. </author> <year> (1973), </year> <title> Imitation: A Review and Critique, </title> <editor> in Bateson & Klopfer, eds, `Perspectives in Ethology', </editor> <volume> Vol. 1, </volume> <publisher> Plenum Press. </publisher>
Reference-contexts: Observed behavior of conspecifics can serve as negative as well as positive reinforcement. For example, animals quickly learn not to eat food that has had a poisonous effect on others, and to avoid those that have been dangerous to other members of the group <ref> (Davis 1973, Gould 1982, McFar-land 1987) </ref>. In a sense, vicarious learning is a means of distributing trials over multiple agents so that one agent need not perform them all.
Reference: <author> Deneubourg, J. L., Goss, S., Pasteels, J. M., Fresneau, D. & Lachaud, J. P. </author> <year> (1987), </year> <title> `Self-Organization Mechanisms in Ant Societies, II: Learning in Foraging and Division of Labor', From Individual to Collective Behavior in Social Insects 54, </title> <type> 177-196. </type>
Reference: <author> Durfee, E. H., Lee, J. & Gmytrasiewicz, P. J. </author> <year> (1993), </year> <title> Overeager Reciprocal Rationality and Mixed Strategy Equilibria, </title> <booktitle> in `Proceedings, AAAI-93', Wash-ington, DC, </booktitle> <pages> pp. 225-230. </pages>
Reference: <editor> Gasser, L. & Huhns, M. N. </editor> <booktitle> (1989), Distributed Artificial Intelligence, </booktitle> <publisher> Pitman, London. </publisher>
Reference-contexts: Functionally different agents can create lasting interference over conflicting goals. An agent can undo the work of another not out of immediate need for the resources but due to some higher-level goal such as, for instance, establishing dominance. Goal competition is studied primarily by the Distributed AI community <ref> (Gasser & Huhns 1989) </ref> whose approaches usually involve predicting other agents' goals and intentions, thus requiring agents to maintain mod els of each other's internal state (e.g. Huber & Durfee (1993), Miceli & Cesta (1993)). Such prediction abilities require computational resources that do not scale well with increased group sizes.
Reference: <author> Gould, J. L. </author> <year> (1982), </year> <title> Ethology; The Mechanisms and Evolution of Behavior, </title> <editor> W. W. </editor> <publisher> Norton & Co., </publisher> <address> NY. </address>
Reference: <author> Huber, M. J. & Durfee, E. H. </author> <year> (1993), </year> <title> Observational Uncertainty in Plan Recognition Among Interacting Robots, </title> <booktitle> in `Proceedings, IJCAI-93 Workshop on Dynamically Interacting Robots', </booktitle> <address> Chambery, France, </address> <pages> pp. 68-75. </pages>
Reference: <author> Kraus, S. </author> <year> (1993), </year> <title> Agents Contracting Tasks in Non-Collaborative Environments, </title> <booktitle> in `Proceedings, AAAI-93', </booktitle> <address> Washington, DC, </address> <pages> pp. 243-248. </pages>
Reference: <author> Langton, C. G. </author> <year> (1989), </year> <title> Artificial Life, </title> <publisher> Addison-Wesley. 9 MacLennan, </publisher> <editor> B. J. </editor> <year> (1990), </year> <title> Evolution of Communication in a Population of Simple Machines, </title> <institution> Technical Report Computer Science Department Technical Report CS-90-104, University of Tennessee. </institution>
Reference: <author> Maes, P. & Brooks, R. A. </author> <year> (1990), </year> <title> Learning to Coordinate Behaviors, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Boston, MA, </address> <pages> pp. 796-802. </pages>
Reference-contexts: Section 9 lists the experiments that were performed and Section 10 gives the results. Finally, Section 11 describes continuing work and concludes the paper. 2 Related Work Previous work in behavior-based control has addressed the problem of learning behavior selection <ref> (Maes & Brooks 1990) </ref>, learning box pushing (Mahadevan & Connell 1991), and learning navigation on an individual robot (Millan 1994) using reinforcement learning (RL). Very few examples of multi-robot learning have been demonstrated so far.
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1991), </year> <title> Automatic Programming of Behavior-based Robots using Reinforcement Learning, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Pittsburgh, PA, </address> <pages> pp. 8-14. </pages>
Reference-contexts: Section 9 lists the experiments that were performed and Section 10 gives the results. Finally, Section 11 describes continuing work and concludes the paper. 2 Related Work Previous work in behavior-based control has addressed the problem of learning behavior selection (Maes & Brooks 1990), learning box pushing <ref> (Mahadevan & Connell 1991) </ref>, and learning navigation on an individual robot (Millan 1994) using reinforcement learning (RL). Very few examples of multi-robot learning have been demonstrated so far. Mataric (1994b) demonstrated learning higher-level group behaviors by selecting among basis primitives, the work on which this paper builds.
Reference: <author> Mataric, M. J. </author> <year> (1992), </year> <title> Designing Emergent Behaviors: From Local Interactions to Collective Intelligence, </title> <editor> in J.-A. Meyer, H. Roitblat & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats: International Conference on Simulation of Adaptive Behavior'. </booktitle>
Reference-contexts: Sharing information consists of learning when to broadcast position information and when to receive and store it. The social rules are expressed within the robots' natural habitat and the context of their usual routines, in this case foraging. The foraging context was chosen because our previous work <ref> (Mataric 1992, Mataric 1993) </ref> provided the basis behavior repertoire to which social rules could easily be added. The built-in foraging behavior consists of a finite state controller which, in response to mutually-exclusive conditions consisting of internal state, activates appropriate basis behaviors from the robots' repertoire.
Reference: <author> Mataric, M. J. </author> <year> (1993), </year> <title> Kin Recognition, Similarity, and Group Behavior, </title> <booktitle> in `Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society', </booktitle> <address> Boulder, Colorado, </address> <pages> pp. 705-710. </pages>
Reference-contexts: 1 Introduction Our work focuses on synthesizing complex group behaviors from simple social interactions between individuals <ref> (Mataric 1993, Mataric 1995) </ref>. We introduced a methodology for selecting and designing a set of basis behaviors that serves as a substrate for a large repertoire of higher-level interactions through the application of two general combination operators.
Reference: <author> Mataric, M. J. </author> <year> (1994a), </year> <title> Interaction and Intelligent Behavior, </title> <type> Technical Report AI-TR-1495, </type> <institution> MIT Artificial Intelligence Lab. </institution>
Reference-contexts: We postulated that all three of the above described forms of reinforcement are necessary for learning social rules in our domain, as well as possibly others with qualitatively similar types of interactions. Individual reinforcement alone, while effective for learning specific tasks (e.g., foraging <ref> (Mataric 1994a) </ref>), is not sufficient for learning social rules because it, by definition, maximizes individual benefit. Many social rules do not immediately and directly benefit the individual and, in most cases, have a delaying effect on its individual reinforcement. Therefore, they require a non-greedy approach.
Reference: <author> Mataric, M. J. </author> <year> (1994b), </year> <title> Reward Functions for Accelerated Learning, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> `Proceedings of the Eleventh International Conference on Machine Learning (ML-94)', </booktitle> <publisher> Morgan Kauff-man Publishers, Inc., </publisher> <address> New Brunswick, NJ, </address> <pages> pp. 181-189. </pages>
Reference-contexts: This type of reinforcement is inherent in any learning task, but its availability varies depending on the specific agent and environment. In most tasks, the agent can maintain some measure of progress that is critical for efficient learning <ref> (Mataric 1994b) </ref>. The second type of reinforcement comes from observing the behavior of conspecifics. Similar behavior in a peer is considered to be a source of positive reinforcement. <p> The rest of the paper describes the experiments, starting with the experimental environment. 6 The Experimental Setup The experimental environment is designed for performing a variety of group behavior experiments. It has been used for verifying work on basis behaviors and for demonstrating learning to forage <ref> (Mataric 1994b) </ref> and to be be objects. The robots' sensory capabilities include piezoelectric bump and gripper sensors, infra-red sensors for collision detection, proprioceptive sensors for maintaining drive and gripper-motor current, voltage, and position, and a radio transmitter for communication and absolute positioning. have socially. <p> Basis behaviors include avoidance, dispersion, searching for pucks, picking up pucks, homing, and sleeping. Our earlier work has shown how foraging itself can be learned <ref> (Mataric 1994b) </ref>. This work demonstrates how it can be improved with social rules. It is important to note that the alternative to behaving socially is not behaving randomly, but behaving greedily. <p> Given that most environments are not of this kind, this algorithm is bound to work faster. It has been successfully used to acquire the basic, greedy strategy for individual foraging <ref> (Mataric 1994b) </ref>. The motivation for using progress estimators (also called internal critics) rather than a delayed reinforcement algorithm such as Q-learning comes from the non-Markovian uncertainty and inconsistency properties of the group situated agent domain.
Reference: <author> Mataric, M. J. </author> <year> (1995), </year> <title> `Designing and Understanding Adaptive Group Behavior', Adaptive Behavior. </title>
Reference-contexts: Unlike foraging, which can be achieved with a single agent, the pushing experiments require tight cooperation and sharing of the agents' limited local sensory and effector resources <ref> (Mataric, Nilsson & Simsarian 1995) </ref>. At least one social rule or convention, turn-taking, was involved in our experiments in learning cooperative box-pushing (Simsarian & Mataric 1995). We are interested in exploring such tightly coupled cooperative tasks as a domain for studying social rules. <p> Unlike foraging, which can be achieved with a single agent, the pushing experiments require tight cooperation and sharing of the agents' limited local sensory and effector resources (Mataric, Nilsson & Simsarian 1995). At least one social rule or convention, turn-taking, was involved in our experiments in learning cooperative box-pushing <ref> (Simsarian & Mataric 1995) </ref>. We are interested in exploring such tightly coupled cooperative tasks as a domain for studying social rules.
Reference: <author> Mataric, M. J., Nilsson, M. & Simsarian, K. T. </author> <year> (1995), </year> <booktitle> Cooperative Multi-Robot Box-Pushing, in `Proceedings, </booktitle> <address> IROS-95', </address> <publisher> IEEE Computer Society Press, </publisher> <address> Pittsburgh, PA. </address>
Reference-contexts: Unlike foraging, which can be achieved with a single agent, the pushing experiments require tight cooperation and sharing of the agents' limited local sensory and effector resources <ref> (Mataric, Nilsson & Simsarian 1995) </ref>. At least one social rule or convention, turn-taking, was involved in our experiments in learning cooperative box-pushing (Simsarian & Mataric 1995). We are interested in exploring such tightly coupled cooperative tasks as a domain for studying social rules. <p> Unlike foraging, which can be achieved with a single agent, the pushing experiments require tight cooperation and sharing of the agents' limited local sensory and effector resources (Mataric, Nilsson & Simsarian 1995). At least one social rule or convention, turn-taking, was involved in our experiments in learning cooperative box-pushing <ref> (Simsarian & Mataric 1995) </ref>. We are interested in exploring such tightly coupled cooperative tasks as a domain for studying social rules.
Reference: <author> McFarland, D. </author> <year> (1985), </year> <title> Animal Behavior, </title> <publisher> Benjamin Cummings, </publisher> <address> Menlo Park, CA. </address>
Reference-contexts: Social learning is the process of acquiring new behavior patterns in a social context, by learning from con-specifics. Also called observational learning, it is ubiquitous in nature <ref> (McFarland 1985, McFarland 1987) </ref> and the propensity for it appears to be innate. Animals imprint, mimic, and imitate adults of their own kind instinctively, often without obtaining direct rewards or even successfully achieving the goal of the behavior (Mc-Farland 1985, Gould 1982). <p> However, the difficult of certain learning problems, such as particularly altruistic social rules, indicates that those are best learned genetically. Data from biology seems to support this intuition, since animals do not appear to learn altruism toward their kin but instead are endowed with it <ref> (McFarland 1985) </ref>. The presented results are preliminary, and are only a glimpse at the wide variety of social rules that can be learned, and forms of social reinforcement that are worth exploring.
Reference: <author> McFarland, D. </author> <year> (1987), </year> <title> The Oxford Companion to Animal Behavior, </title> <publisher> in `Oxford, University Press'. </publisher>
Reference: <author> Miceli, M. & Cesta, A. </author> <year> (1993), </year> <title> Strategic Social Planning: Looking for Willingness in Multi-Agent Domains, </title> <booktitle> in `Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society', </booktitle> <address> Boulder, Colorado, </address> <pages> pp. 741-746. </pages>
Reference: <author> Millan, J. D. R. </author> <year> (1994), </year> <title> Learning Reactive Sequences from Basic Reflexes, </title> <booktitle> in `Proceedings, Simulation of Adaptive Behavior SAB-94', </booktitle> <publisher> The MIT Press, </publisher> <address> Brighton, England, </address> <pages> pp. 266-274. </pages>
Reference-contexts: Finally, Section 11 describes continuing work and concludes the paper. 2 Related Work Previous work in behavior-based control has addressed the problem of learning behavior selection (Maes & Brooks 1990), learning box pushing (Mahadevan & Connell 1991), and learning navigation on an individual robot <ref> (Millan 1994) </ref> using reinforcement learning (RL). Very few examples of multi-robot learning have been demonstrated so far. Mataric (1994b) demonstrated learning higher-level group behaviors by selecting among basis primitives, the work on which this paper builds.
Reference: <author> Parker, L. E. </author> <year> (1994), </year> <title> Heterogeneous Multi-Robot Cooperation, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference: <author> Simsarian, K. T. & Mataric, M. J. </author> <year> (1995), </year> <title> Learning to Cooperate Using Two Six-Legged Mobile Robots, </title> <booktitle> in `Proceedings, Third European Workshop of Learning Robots', </booktitle> <address> Heraklion, Crete, Greece. </address>
Reference-contexts: Unlike foraging, which can be achieved with a single agent, the pushing experiments require tight cooperation and sharing of the agents' limited local sensory and effector resources <ref> (Mataric, Nilsson & Simsarian 1995) </ref>. At least one social rule or convention, turn-taking, was involved in our experiments in learning cooperative box-pushing (Simsarian & Mataric 1995). We are interested in exploring such tightly coupled cooperative tasks as a domain for studying social rules. <p> Unlike foraging, which can be achieved with a single agent, the pushing experiments require tight cooperation and sharing of the agents' limited local sensory and effector resources (Mataric, Nilsson & Simsarian 1995). At least one social rule or convention, turn-taking, was involved in our experiments in learning cooperative box-pushing <ref> (Simsarian & Mataric 1995) </ref>. We are interested in exploring such tightly coupled cooperative tasks as a domain for studying social rules.
Reference: <author> Sutton, R. </author> <year> (1988), </year> <title> `Learning to Predict by Method of Temporal Differences', </title> <booktitle> Machine Learning 3(1), </booktitle> <pages> 9-44. </pages>
Reference-contexts: The algorithm relies on estimating progress at least intermittently. If progress measurements are not available, and the reward function is an impulse at goal only, then the algorithm reduces to one-step temporal differencing <ref> (Sutton 1988) </ref>, which has been proven to converge, however slowly. Given that most environments are not of this kind, this algorithm is bound to work faster. It has been successfully used to acquire the basic, greedy strategy for individual foraging (Mataric 1994b).
Reference: <author> Tan, M. </author> <year> (1993), </year> <title> Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents, in `Proceedings, Tenth International Conference on Machine Learning', </booktitle> <address> Amherst, MA, </address> <pages> pp. 330-337. </pages>
Reference-contexts: Parker (1994) implemented a non-RL memory-based style of parameter-learning for adjusting activation thresholds used to perform task allocation in a multi-robot system. Various examples of simulated multi-agent systems have been used to study reinforcement learning (e.g., <ref> (Tan 1993) </ref>). Those and most other 1 domains in which reinforcement learning has been ap-plied, the learning agent attempts to acquire an effective policy for individual (greedy) payoff.
Reference: <author> Tomasello, M., Kruger, A. C. & Rather, H. H. </author> <year> (1992), </year> <note> `Cultural Learning', to appear in The Journal of Brain and Behavior Sciences. 10 </note>
Reference-contexts: True imitation is distinct from mimicry and social facilitation. Mimicry is the ability to repeat some aspects of the behavior of another agent, without having the mimicing agent understand the goal of the behavior or of the internal state of the agent being mimicked <ref> (Tomasello, Kruger & Rather 1992) </ref>. Finally, social facilitation refers to the process of selectively expressing a behavior which is already a part of the agent's repertoire. Social settings offer a plethora of information useful for social learning in any of the above forms.
References-found: 32

