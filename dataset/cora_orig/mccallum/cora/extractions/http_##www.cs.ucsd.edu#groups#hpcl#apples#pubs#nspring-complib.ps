URL: http://www.cs.ucsd.edu/groups/hpcl/apples/pubs/nspring-complib.ps
Refering-URL: http://www.cs.ucsd.edu/users/rich/publications.html
Root-URL: http://www.cs.ucsd.edu
Title: Application Level Scheduling of Gene Sequence Comparison on Metacomputers  
Author: Neil Spring Rich Wolski 
Abstract: This paper investigates the efficacy of Application-Level Scheduling (AppLeS) [3] for a parallel gene sequence library comparison application in production metacomputing settings. We compare an AppLeS-enhanced version of the application to an original implementation designed and tuned to use the native scheduling mechanisms of Mentat [6] a meta-computing software infrastructure. The experimental data shows that the AppLeS versions outperform the best Mentat versions over a range of problem sizes and computational settings. The structure of the AppLeS we have defined for this application does not depend on the scheduling algorithms that it uses. Instead, the AppLeS scheduler considers the uncertainty associated with the information it uses in its scheduling decisions to choose between the static placement of computation, and the dynamic assignment of computation during execution. We propose that this framework is general enough to represent the class of metacom-puting applications that are organized as a master and set of parallel slaves, in which the master distributes uncomputed work. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Altschul, W. Gish, W. Miller, E. Myers, and D. Lipman. </author> <title> Basic local alignment search tool. </title> <journal> Journal of Molecular Biology, </journal> <volume> 215 </volume> <pages> 403-10, </pages> <year> 1990. </year>
Reference-contexts: It is likely that many of the matching pairs of sequences share little function or origin, and attempting to separate these from relevant matches by parameter tuning is generally worthwhile. FASTA is just one fast heuristic method used to compare sequences. Others include Smith-Waterman [14], and BLAST <ref> [1] </ref>. Choosing a particular algorithm and tuning its parameters effects the tradeoff between sensitivity (finding distantly related sequences), selectivity (discriminating unrelated sequences), and speed. 2.2 Library Comparison Gene sequence library comparison consists of many individual sequence comparisons.
Reference: [2] <author> I. Banicescu and S. F. Hummel. </author> <title> Balancing processor loads and exploiting data locality in irregular computation. </title> <type> Technical Report RC 19934, </type> <institution> IBM, </institution> <year> 1995. </year>
Reference-contexts: If the uncertainties are high, the reverse is true. The algorithms used for placement and replacement, however, are not specified by the model. Much work, for example, has been done to improve dynamic scheduling techniques <ref> [11, 2, 12, 10] </ref>. The scheduler need only be able to consider the non-amortizable overhead cost of each in order to determine its benefit relative to a given placement algorithm. Similarly, a variety of static placement algorithms such as time-balancing or recursive bisection may be employed.
Reference: [3] <author> F. Berman, R. Wolski, S. Figueira, J. Schopf, and G. Shao. </author> <title> Application level scheduling on distributed heterogeneous networks. </title> <booktitle> In Proceedings of Supercomputing 1996, </booktitle> <year> 1996. </year>
Reference-contexts: D570, Issued by ESC/ENS under contract #F19628-96-C-0020. y email: nspring@cs.ucsd.edu, rich@cs.ucsd.edu ing the resident operating systems, languages and language libraries, storage devices, etc. on each) it is often termed metacomputing <ref> [4, 8, 15, 3] </ref>. To take the fullest advantage of the shared, heterogeneous resources of a high-performance metacomputer, a parallel application's components (tasks, inter-task communication, I/O) must be carefully scheduled. <p> Resource heterogeneity adds an additional complication since the way in which an application uses a particular resource or resource type dramatically affects the performance that resource can actually deliver to the application. Application Level Scheduling (AppLeS) <ref> [3] </ref> is an approach to metacomputer application scheduling in which each application is integrated with a customized scheduler. <p> members of a regular Mentat class, the Mentat scheduler chooses where worker objects will be started, and will start as many worker objects as necessary. 4 AppLeS Application Level Scheduling (AppLeS) combines dynamic system performance information with application specific models and user specified parameters in order to produce better schedules <ref> [3] </ref>. "Better" is defined by the user, but in this case will be taken to mean decreased run time. In this section, we will describe the application model used, and how the scheduler can improve application performance during each phase of execution. <p> Note, however, that AppLeS can consider the delay associated with different object-to-host mappings, and choose only those resources (and the time at which they should be used) so that the execution time of the program is optimized. To decide on a schedule, the AppLeS employs a time-balancing heuristic <ref> [3] </ref> which attempts to make all worker tasks finish simultaneously.
Reference: [4] <author> I. Foster and C. Kesselman. Globus: </author> <title> A metacomputing infrastructure toolkit. </title> <journal> International Journal of Supercomputer Applications, </journal> <year> 1997. </year> <title> average of 30 execution times. Error bars cover the range. </title>
Reference-contexts: D570, Issued by ESC/ENS under contract #F19628-96-C-0020. y email: nspring@cs.ucsd.edu, rich@cs.ucsd.edu ing the resident operating systems, languages and language libraries, storage devices, etc. on each) it is often termed metacomputing <ref> [4, 8, 15, 3] </ref>. To take the fullest advantage of the shared, heterogeneous resources of a high-performance metacomputer, a parallel application's components (tasks, inter-task communication, I/O) must be carefully scheduled.
Reference: [5] <author> A. Grimshaw. </author> <title> The mentat run-time system: Support for medium grain parallel computation. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <pages> pages 1064-1073, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Complib, the library-comparison implementation we discuss in the next section, exploits this parallelism for performance. target arrays must be compared to form a two dimensional array of results structures. 3 Complib Complib is a metacomputer application for comparing biological sequence libraries using the FASTA algorithm, written in Mentat <ref> [6, 5] </ref>. Mentat is both a run-time system for metacomputer resource management and a programming language based on C++. Parallel tasks in this object-oriented system are contained within Mentat objects, and these Mentat objects are distributed on the different machines in the metacomputer.
Reference: [6] <author> A. Grimshaw. </author> <title> Easy-to-use object-oriented parallel programming with mentat. </title> <booktitle> IEEE Computer, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: In this paper, we describe an application level sched-uler designed for a parallel gene sequence library comparison application written for the Mentat <ref> [6] </ref> prototype meta-computing system. By default, Mentat employs a variant of workstealing [7], to schedule all applications that execute within its environment. <p> Complib, the library-comparison implementation we discuss in the next section, exploits this parallelism for performance. target arrays must be compared to form a two dimensional array of results structures. 3 Complib Complib is a metacomputer application for comparing biological sequence libraries using the FASTA algorithm, written in Mentat <ref> [6, 5] </ref>. Mentat is both a run-time system for metacomputer resource management and a programming language based on C++. Parallel tasks in this object-oriented system are contained within Mentat objects, and these Mentat objects are distributed on the different machines in the metacomputer.
Reference: [7] <author> A. Grimshaw and V. Vivas. </author> <title> Falcon: A distributed scheduler for mimd architectures. </title> <booktitle> In Proceedings of the Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pages 149-163, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: In this paper, we describe an application level sched-uler designed for a parallel gene sequence library comparison application written for the Mentat [6] prototype meta-computing system. By default, Mentat employs a variant of workstealing <ref> [7] </ref>, to schedule all applications that execute within its environment. We contrast the performance of this system-provided default method with an AppLeS-determined static schedule derived at runtime, and an AppLeS hybrid method based on a combination of runtime scheduling and workstealing. This comparison is important for two reasons. <p> The collector object stores a partially assembled array of results between functions, and the library objects store the genome library to be distributed. 3.3 Mentat System Scheduling Policy Currently, complib relies on the Mentat scheduler <ref> [7] </ref>. Chunks of a parameterizable maximum size are placed on a system-managed queue. This block-style data distribution is shown in Figure 4. Effectively, when objects run out of work, they request a pair of chunks, one from the source library and another from the target from this queue.
Reference: [8] <author> A. S. Grimshaw, W. A. Wulf, J. C. French, A. C. Weaver, and P. F. Reynolds. Legion: </author> <title> The next logical step towrd a nationwide virtual computer. </title> <type> Technical Report CS-94-21, </type> <institution> University of Virginia, </institution> <year> 1994. </year>
Reference-contexts: D570, Issued by ESC/ENS under contract #F19628-96-C-0020. y email: nspring@cs.ucsd.edu, rich@cs.ucsd.edu ing the resident operating systems, languages and language libraries, storage devices, etc. on each) it is often termed metacomputing <ref> [4, 8, 15, 3] </ref>. To take the fullest advantage of the shared, heterogeneous resources of a high-performance metacomputer, a parallel application's components (tasks, inter-task communication, I/O) must be carefully scheduled.
Reference: [9] <author> M. Harchol-Balter and A. Downey. </author> <title> Exploiting process lifetime distributions for dynamic load balancing. </title> <booktitle> In Proceedings of the 1996 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Based on the relationship between the observed data and the normal quantiles, we chose to use a normal distribution to model the dedicated execution time of FASTA. Expected CPU availability, however, is not normally distributed <ref> [9] </ref> as shown in Figure 7. To predict the percentage of available CPU occupancy that will be possible for each worker task, we use the Network Weather Service (NWS) [17, 18].
Reference: [10] <author> S. Hummel, J. Schmidt, R. Uma, and J. Wein. </author> <title> Load-sharing in heterogeneous systems via weighted factoring. </title> <booktitle> In Proc. 8th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 318-328, </pages> <year> 1996. </year>
Reference-contexts: If the uncertainties are high, the reverse is true. The algorithms used for placement and replacement, however, are not specified by the model. Much work, for example, has been done to improve dynamic scheduling techniques <ref> [11, 2, 12, 10] </ref>. The scheduler need only be able to consider the non-amortizable overhead cost of each in order to determine its benefit relative to a given placement algorithm. Similarly, a variety of static placement algorithms such as time-balancing or recursive bisection may be employed.
Reference: [11] <author> S. Lucco. </author> <title> Adaptive Parallel Programs. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: If the uncertainties are high, the reverse is true. The algorithms used for placement and replacement, however, are not specified by the model. Much work, for example, has been done to improve dynamic scheduling techniques <ref> [11, 2, 12, 10] </ref>. The scheduler need only be able to consider the non-amortizable overhead cost of each in order to determine its benefit relative to a given placement algorithm. Similarly, a variety of static placement algorithms such as time-balancing or recursive bisection may be employed.
Reference: [12] <author> C. Polychronopoulos and D. Kuck. </author> <title> A practical scheduling scheme for parallel computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Placement is the process of allocating work to processors once, before execution begins, using the best performance estimates that are available. Replacement is the allocation of the remaining work using a dynamic technique, in this case, Guided Self-Scheduling (GSS) <ref> [12] </ref>. This strategy is shown in Figure 8 for comparison with Figures 5 and ??. GSS allocates successively smaller chunks of the computation in order to avoid overhead early on, while still providing parameterizably even finishing time. <p> If the uncertainties are high, the reverse is true. The algorithms used for placement and replacement, however, are not specified by the model. Much work, for example, has been done to improve dynamic scheduling techniques <ref> [11, 2, 12, 10] </ref>. The scheduler need only be able to consider the non-amortizable overhead cost of each in order to determine its benefit relative to a given placement algorithm. Similarly, a variety of static placement algorithms such as time-balancing or recursive bisection may be employed.
Reference: [13] <author> J. M. Schopf and F. Berman. </author> <title> Performance prediction in production environments. </title> <booktitle> In Proceedings of IPPS/SPDP '98, </booktitle> <year> 1998. </year> <note> to appear. </note>
Reference-contexts: Use of such Quality of Information or QoIn metrics is the subject of other, on-going research efforts within the AppLeS research group at UCSD <ref> [13] </ref>. 5.2 Complib Execution Performance The largest performance improvement provided by application level scheduling of complib was seen on the medium problem size on the medium sized cluster (Figure 14). AppLeS run-time static scheduling ran an average of 59.7% (63.8 s) faster than Mentat complib.
Reference: [14] <author> T. Smith and M. Waterman. </author> <title> Identification of common molecular subsequences. </title> <journal> Journal of Molecular Biology, </journal> <volume> 147 </volume> <pages> 195-197, </pages> <year> 1981. </year>
Reference-contexts: It is likely that many of the matching pairs of sequences share little function or origin, and attempting to separate these from relevant matches by parameter tuning is generally worthwhile. FASTA is just one fast heuristic method used to compare sequences. Others include Smith-Waterman <ref> [14] </ref>, and BLAST [1]. Choosing a particular algorithm and tuning its parameters effects the tradeoff between sensitivity (finding distantly related sequences), selectivity (discriminating unrelated sequences), and speed. 2.2 Library Comparison Gene sequence library comparison consists of many individual sequence comparisons.
Reference: [15] <author> T. Tannenbaum and M. Litzkow. </author> <title> The condor distributed processing system. </title> <journal> Dr. Dobbs Journal, </journal> <month> February </month> <year> 1995. </year>
Reference-contexts: D570, Issued by ESC/ENS under contract #F19628-96-C-0020. y email: nspring@cs.ucsd.edu, rich@cs.ucsd.edu ing the resident operating systems, languages and language libraries, storage devices, etc. on each) it is often termed metacomputing <ref> [4, 8, 15, 3] </ref>. To take the fullest advantage of the shared, heterogeneous resources of a high-performance metacomputer, a parallel application's components (tasks, inter-task communication, I/O) must be carefully scheduled.
Reference: [16] <author> R. Wolski. </author> <title> Dynamically forecasting network performance to support dynamic scheduling using the network weather service. </title> <booktitle> In Proc. 6th IEEE Symp. on High Performance Distributed Computing, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: Since the distribution of CPU availability values is often multimodal, (see Figure 7) we do not assume normality, but rather use an estimate of variance provided by the NWS prediction modules <ref> [16] </ref>. The NWS tracks the error associated with each of the predictions of resource performance it supplies. We incorporate that information into the calculation of the placement/replacement boundary by using a multiple of the mean percent prediction error as a confidence interval about the prediction.
Reference: [17] <author> R. Wolski. </author> <title> Dynamically forecasting network performance using the network weather service. Cluster Computing, </title> <note> 1998. available from http://www.cs.ucsd.edu/users/rich/publications.html. </note>
Reference-contexts: Expected CPU availability, however, is not normally distributed [9] as shown in Figure 7. To predict the percentage of available CPU occupancy that will be possible for each worker task, we use the Network Weather Service (NWS) <ref> [17, 18] </ref>. The NWS is a distributed performance monitoring and forecasting facility designed to provide schedulers with predictions of the deliverable performance of resources to applications. Currently, the NWS provides CPU availability forecasts, and available network bandwidth and latency predictions to the sched-uler.
Reference: [18] <author> R. Wolski, N. Spring, and C. Peterson. </author> <title> Implementing a performance forecasting system for metacomputing: The network weather service. </title> <booktitle> In Proceedings of Supercomputing 1997, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: Expected CPU availability, however, is not normally distributed [9] as shown in Figure 7. To predict the percentage of available CPU occupancy that will be possible for each worker task, we use the Network Weather Service (NWS) <ref> [17, 18] </ref>. The NWS is a distributed performance monitoring and forecasting facility designed to provide schedulers with predictions of the deliverable performance of resources to applications. Currently, the NWS provides CPU availability forecasts, and available network bandwidth and latency predictions to the sched-uler.
References-found: 18

