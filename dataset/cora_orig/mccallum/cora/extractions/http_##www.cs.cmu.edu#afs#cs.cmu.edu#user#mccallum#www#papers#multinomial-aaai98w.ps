URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mccallum/www/papers/multinomial-aaai98w.ps
Refering-URL: 
Root-URL: 
Email: mccallum@justresearch.com  knigam@cs.cmu.edu  
Title: A Comparison of Event Models for Naive Bayes Text Classification  
Author: Andrew McCallum zy Kamal Nigam 
Address: 4616 Henry Street Pittsburgh, PA 15213  Pittsburgh, PA 15213  
Affiliation: Just Research  School of Computer Science Carnegie Mellon University  
Abstract: Recent work in text classification has used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes|providing on average a 27% reduction in error over the multi-variate Bernoulli model at any vocabulary size. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Use Equations 2 and 3 for the multi-variate Bernoulli event model. Use Equations 5 and 6 for the multinomial Feature Selection When reducing the vocabulary size, feature selection is done by selecting words that have highest average mutual information with the class variable <ref> (Cover and Thomas 1991) </ref>. This method works well with text and has been used often (Yang and Pederson 1997; Joachims 1997; Craven et al. 1998). <p> Average mutual information is the difference between the entropy of the class variable, H (C), and the entropy of the class variable conditioned on the absence or presence of the word, H (CjW t ) <ref> (Cover and Thomas 1991) </ref>: I (C; W t ) = H (C) H (CjW t ) (8) X P (c) log (P (c)) X P (f t ) c2C = c2C f t 2f0;1g P (c)P (f t ) ; where P (c), P (f t ) and P (c; f
Reference: <author> M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and S. Slattery. </author> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <booktitle> In AAAI-98, </booktitle> <year> 1998. </year>
Reference-contexts: When tokenizing this data, we skip the UseNet headers (thereby discarding the subject line); tokens are formed from contiguous alphabetic characters with no stemming. The resulting vocabulary, after removing words that occur only once or on a stoplist, has 62258 words. The WebKB data set <ref> (Craven et al. 1998) </ref> contains web pages gathered from university computer science departments. The pages are divided into seven categories: student, faculty, staff, course, project, department and other. In this paper, we use the four most populous entity-representing categories: student, faculty, course and project, all together containing 4199 pages.
Reference: <author> P. Domingos and M. Pazzani. </author> <title> On the optimality of the simple Bayesian classifier under zero-one loss. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 103-130, </pages> <year> 1997. </year>
Reference: <author> Nir Friedman, Dan Geiger, and Moises Goldszmidt. </author> <title> Bayesian network classifiers. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 131-163, </pages> <year> 1997. </year>
Reference: <author> Jerome H. Friedman. </author> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery, </title> <booktitle> 1 </booktitle> <pages> 55-77, </pages> <year> 1997. </year>
Reference: <author> Louise Guthrie and Elbert Walker. </author> <title> Document classification by machine: Theory and practice. </title> <booktitle> In Proceedings of COLING-94, </booktitle> <year> 1994. </year> <title> Thorsten Joachims. A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In ICML-97, </booktitle> <year> 1997. </year> <title> Thorsten Joachims. Text categorization with Support Vector Machines: Learning with many relevant features. </title> <booktitle> In ECML-98, </booktitle> <year> 1998. </year>
Reference: <author> T. Kalt and W. B. Croft. </author> <title> A new probabilistic model of text classification and retrieval. </title> <type> Technical Report IR-78, </type> <institution> University of Massachusetts Center for Intelligent Information Retrieval, </institution> <year> 1996. </year> <note> http://ciir.cs.umass.edu/publications/index.shtml. </note>
Reference: <author> Daphne Koller and Mehran Sahami. </author> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <year> 1997. </year>
Reference: <author> Pat Langley, Wayne Iba, and Kevin Thompson. </author> <title> An analysis of Bayesian classifiers. </title> <booktitle> In AAAI-92, </booktitle> <year> 1992. </year>
Reference: <author> Leah S. Larkey and W. Bruce Croft. </author> <title> Combining classifiers in text categorization. </title> <booktitle> In SIGIR-96, </booktitle> <year> 1996. </year>
Reference: <author> D. Lewis and W. Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In SIGIR-94, </booktitle> <year> 1994. </year>
Reference: <author> David D. Lewis. </author> <title> An evaluation of phrasal and clustered representations on a text categorization task. </title> <booktitle> In SIGIR-92, </booktitle> <year> 1992. </year>
Reference: <author> David Lewis. </author> <title> Naive (Bayes) at forty: </title> <booktitle> The independence asssumption in information retrieval. In ECML'98: Tenth European Conference On Machine Learning, </booktitle> <year> 1998. </year>
Reference: <author> Hang Li and Kenji Yamanishi. </author> <title> Document classification using a finite mixture model. </title> <booktitle> In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1997. </year>
Reference: <author> Ray Liere and Prasad Tadepalli. </author> <title> Active learning with committees for text categorization. </title> <booktitle> In AAAI-97, </booktitle> <year> 1997. </year>
Reference: <author> Andrew McCallum, Ronald Rosenfeld, Tom Mitchell, and Andrew Ng. </author> <title> Improving text clasification by shrinkage in a hierarchy of classes. </title> <booktitle> In ICML-98, </booktitle> <year> 1998. </year>
Reference: <author> Tom M. Mitchell. </author> <title> Machine Learning. </title> <address> WCB/McGraw-Hill, </address> <year> 1997. </year>
Reference: <author> Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom Mitchell. </author> <title> Learning to classify text from labeled and unlabeled documents. </title> <booktitle> In AAAI-98, </booktitle> <year> 1998. </year>
Reference: <author> S. E. Robertson and K. Sparck-Jones. </author> <title> Relevance weighting of search terms. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 27 </volume> <pages> 129-146, </pages> <year> 1976. </year>
Reference: <author> Mehran Sahami, Susan Dumais, David Heckerman, and Eric Horvitz. </author> <title> A bayesian approach to filtering junk e-mail. </title> <booktitle> In AAAI-98 Workshop on Learning for Text Categorization, </booktitle> <year> 1998. </year>
Reference: <author> Mehran Sahami. </author> <title> Learning limited dependence Bayesian classifiers. </title> <booktitle> In KDD-96: Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 335-338. </pages> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference: <author> V. Vapnik. </author> <title> Estimations of dependences based on statistical data. </title> <publisher> Springer Publisher, </publisher> <year> 1982. </year>
Reference-contexts: The parameters of a mixture component are written w t jc j = P (w t jc j ; ), where 0 w t jc j 1. We can calculate Bayes-optimal estimates for these probabilities by straightforward counting of events, supplemented by a prior <ref> (Vapnik 1982) </ref>. We use the Laplacean prior, priming each word's absence and presence count with a count of one to avoid probabilities of zero or one. Define P (c j jd i ) 2 f0; 1g as given by the document's class label.
Reference: <author> Yiming Yang and Jan Pederson. </author> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In ICML-97, </booktitle> <year> 1997. </year>
References-found: 23

