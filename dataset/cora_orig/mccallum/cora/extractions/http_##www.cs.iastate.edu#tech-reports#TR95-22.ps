URL: http://www.cs.iastate.edu/tech-reports/TR95-22.ps
Refering-URL: http://www.cs.iastate.edu/tech-reports/catalog.html
Root-URL: 
Email: balakris@cs.iastate.edu, honavar@cs.iastate.edu  
Title: Intelligent Diagnosis Systems  
Author: Karthik Balakrishnan Vasant Honavar 
Address: Ames, Iowa 50011-1040  
Affiliation: Artificial Intelligence Research Group Department of Computer Science Iowa State University  
Abstract: This paper examines and compares several different approaches to the design of intelligent systems for diagnosis and advising applications. These include expert systems or knowledge-based systems, case-based reasoning systems, truth (or reason) maintenance systems, statistical pattern classification systems, decision trees, and artificial neural networks (or connectionist systems). The key aspects of each approach are demonstrated through the design of a system for a simple automobile fault diagnosis task. The paper also discusses the domain characteristics and design and performance requirements that influence the choice of a specific technique (or a combination of techniques) for a given application.
Abstract-found: 1
Intro-found: 1
Reference: <author> Buchanan, B. G., & Wilkins, D. C. </author> <year> (1993). </year> <title> Readings in Knowledge Acquisition. </title> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Chen, C-H., & Honavar, V. </author> <year> (1995a). </year> <title> A Neural Architecture for High-Speed Database Query Processing. </title> <journal> International Journal of Microcomputer Applications. </journal> <note> (to appear draft available as ISU-CS-TR95-11, </note> <institution> Department of Computer Science, Iowa State University, Ames, IA). </institution>
Reference: <author> Chen, C-H., & Honavar, V. </author> <year> (1995b). </year> <title> A Neural Memory Architecture for Content as Well as Address-Based Storage and Recall. </title> <type> Tech. </type> <institution> rept. ISU-CS-TR95-03. Department of Computer Science, Iowa State University, Ames, Iowa. </institution>
Reference: <author> Dean, T., Allen, J., & Aloimonos, Y. </author> <year> (1995). </year> <booktitle> Artificial Intelligence Theory and Practice. </booktitle> <address> Redwood City, CA: Benjamin/Cummings. </address>
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference: <author> Durkin, J. </author> <year> (1994). </year> <title> Expert Systems Design and Development. </title> <address> New York, NY: </address> <publisher> Macmillan Publishing Company. </publisher>
Reference: <author> Forbus, K. D., & de Kleer, J. </author> <year> (1993). </year> <title> Building Problem Solvers. </title> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Based on further information (e.g., concerning the functioning of Headlights), one could possibly diagnose the exact cause. This is the principle behind Model-Based diagnosis systems (Mozetic, 1992; Durkin, 1994; Puppe, 1993). Diagnosis using truth-maintenance systems <ref> (Forbus & de Kleer, 1993) </ref> (see Section 5 below) is an example of the model-based approach. In many practical scenarios, precise models of the domain may be unavailable. <p> Since these assumptions have to be retracted, we remove H from D. Once these assumptions are retracted from the database, we can add the new fact p without any contradictions. This is the functioning of an ATMS. The above system can be easily adapted for diagnosis <ref> (Forbus & de Kleer, 1993) </ref>. The database D corresponds to a set A of assumptions about the proper functioning of components, and a set C of facts describing the behavior of the system being diagnosed, in terms of its components. <p> The computation of all diagnoses for an observed fault is known to be NP-complete which makes its use in its most general form impractical in most real-world applications because of its computational overhead. One alternative is to use focusing mechanisms for computing just a few of the probable diagnoses <ref> (Forbus & de Kleer, 1993) </ref>. Another possibility is to use known polynomial time algorithms for computing the first k diagnoses (for a given k) (Mozetic, 1992).
Reference: <author> Fu, K. S. </author> <year> (1982). </year> <title> Syntactic Pattern Recognition and Applications. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Fukunaga, K. </author> <year> (1990). </year> <title> Introduction to Statistical Pattern Recognition. </title> <address> New York: </address> <publisher> Academic Press. </publisher> <address> 34 Gallant, S. </address> <year> (1993). </year> <title> Neural Network Learning and Expert Systems. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Genesereth, M. R., & Nilsson, N. J. </author> <year> (1987). </year> <booktitle> Logical Foundations of Artificial Intelligence. </booktitle> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ginsberg, M. </author> <year> (1993). </year> <booktitle> Essentials of Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kauf-mann Publishers, Inc. </publisher>
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Algorithms in Search, Optimization and Machine Learning. </title> <address> New York, NY: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Gonzalez, R. C., & Thomason, M. G. </author> <year> (1978). </year> <title> Syntactic Pattern Recognition: An Introduction. </title> <address> Reading, MA: </address> <publisher> Addison Wesley. </publisher>
Reference: <editor> Goonatilake, S., & Khebbal, S. (eds). </editor> <year> (1995). </year> <title> Intelligent Hybrid Systems. </title> <address> West Sussex: </address> <publisher> John Wiley. </publisher>
Reference: <author> Hassoun, M. H. </author> <year> (1995). </year> <booktitle> Fundamentals of Artificial Neural Networks. </booktitle> <address> Boston, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Holland, J. </author> <year> (1992). </year> <booktitle> Adaptation in Natural and Artificial Systems. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Honavar, V. </author> <year> (1994). </year> <title> Toward Learning Systems That Integrate Different Strategies and Representations. Pages 561-580 of: Honavar, </title> <editor> V., & Uhr, L. (eds), </editor> <title> Artificial Intelligence and Neural Networks: Steps Toward Principled Integration. </title> <address> San Diego, CA: </address> <publisher> Academic Press. </publisher>
Reference: <author> Honavar, V., & Uhr, L. </author> <year> (1993). </year> <title> Generative Learning Structures and Processes for Generalized Connectionist Networks. </title> <journal> Information Sciences, </journal> <volume> 70, </volume> <pages> 75-108. </pages>
Reference: <author> Honavar, V., & Uhr, L. (eds). </author> <year> (1994). </year> <title> Artificial Intelligence and Neural Networks Steps toward Principled Integration. </title> <address> San Diego, CA: </address> <publisher> Academic Press. </publisher>
Reference: <author> Hutchinson, A. </author> <year> (1994). </year> <title> Algorithmic Learning. </title> <address> New York, NY: </address> <publisher> Oxford University Press. </publisher>
Reference: <author> Kolodner, J. </author> <year> (1993). </year> <title> Case-Based Reasoning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: A simple example of an expert system for diagnosis is presented in Section 4. One approach which attempts to circumvent the difficult task of extracting and cod 5 ifying the domain knowledge of the expert relies on buiding a large repository of sample diagnoses or cases <ref> (Kolodner, 1993) </ref>. When presented with a diagnostic problem, the system attempts to solve the problem by identifying one or more scenarios with known diagnoses from its repository of cases. <p> Since then, the field has matured considerably. For a detailed exposition the interested reader is referred to Kolodner <ref> (Kolodner, 1993) </ref>. In a CBR system, knowledge is stored in the form of cases. Kolodner (Kolodner, 1993) defines a case as follows: A case is a contextualized piece of knowledge representing an experience that teaches a lesson fundamental to achieving the goals of the reasoner A case is thus, a situation <p> Since then, the field has matured considerably. For a detailed exposition the interested reader is referred to Kolodner <ref> (Kolodner, 1993) </ref>. In a CBR system, knowledge is stored in the form of cases. Kolodner (Kolodner, 1993) defines a case as follows: A case is a contextualized piece of knowledge representing an experience that teaches a lesson fundamental to achieving the goals of the reasoner A case is thus, a situation that was experienced in the past, and resulted in some action.
Reference: <editor> Koza, J. </editor> <booktitle> (1992). Genetic Programming. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Kung, S. Y. </author> <year> (1993). </year> <title> Digital Neural Networks. </title> <address> New York: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Langley, P. </author> <year> (1995). </year> <title> Elements of Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kauffman. 35 Luger, </publisher> <editor> G. F., & Stubblefield, W. A. </editor> <booktitle> (1993). Artificial Intelligence. </booktitle> <address> New York, NY: Benjamin/Cummings. </address>
Reference: <author> Medsker, L. </author> <year> (1994). </year> <title> Hybrid Neural Network and Expert Systems. </title> <address> New York: </address> <publisher> Kluwer. </publisher>
Reference: <author> Michalewicz, Z. </author> <year> (1992). </year> <title> Genetic Algorithms + Data Structures = Evolution Programs. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> Theory and Methodology of Inductive Learning. </title> <editor> In: Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (eds), </editor> <booktitle> Machine Learning AnArtificial Intelligence Approach. </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga. </publisher>
Reference: <author> Miclet, L. </author> <year> (1986). </year> <title> Structural methods in pattern recognition. </title> <address> New York, NY: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Mozetic, I. </author> <year> (1992). </year> <title> Model-Based Diagnosis: An Overview. Pages 419-430 of: </title> <editor> Marik, V., Stepankova., O., & Trappl, R. (eds), </editor> <booktitle> Advanced Topics in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Consider a scenario in which a model of the domain is available. Such a model explicitly represents the structure of the system, i.e., its constituent components 4 and their organization <ref> (Mozetic, 1992) </ref>. A diagnosis problem arises when the system's observed behavior conflicts with the system's expected behavior. And the task is to identify the (faulty) system component (s) that explain the anomaly. <p> One alternative is to use focusing mechanisms for computing just a few of the probable diagnoses (Forbus & de Kleer, 1993). Another possibility is to use known polynomial time algorithms for computing the first k diagnoses (for a given k) <ref> (Mozetic, 1992) </ref>. Yet another practical approach to reducing the computational complexity of model-based diagnosis involves the use of several levels of abstraction to focus the search for diagnoses.
Reference: <author> Natarajan, B. </author> <year> (1991). </year> <title> Machine Learning: A Theoretical Approach. </title> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Nilsson, N. J. </author> <year> (1965). </year> <title> Mathematical Foundations of Learning Machines. </title> <address> Palo Alto, CA: </address> <note> Morgan Kaufmann. (Reprinted in 1992). </note>
Reference-contexts: A two-class training set made of patterns is said to be linearly separable if there exits a weight vector W ? for a threshold neuron so that the neuron correctly classifies every pattern in the training set <ref> (Nilsson, 1965) </ref>, as shown in Figure 10. In this case, the neuron implements a decision hyperplane in the pattern space defined by W X = 0. Clearly, when the training set is not linearly separable, a single threshold neuron will not be able to classify the entire training set correctly.
Reference: <author> Parekh, R. G., Yang, J., & Honavar, V. </author> <year> (1995). </year> <title> Multi-Category Constructive Neural Network Algorithms for Pattern Classification. </title> <institution> Ames, Iowa: </institution> <type> Tech. Rep. </type> <institution> ISU CS-TR 95-15, Department of Computer Science, Iowa State University. </institution>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Puppe, F. </author> <year> (1993). </year> <title> Systematic Introduction to Expert Systems Knowledge Representations and Problem Solving Methods. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <author> Quinlan, R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Inductive approaches are particularly suited for domains in which principles are hard to formalize (weather prediction, medical diagnosis etc.) or where experts are few (space exploration, nuclear waste cleanup etc.). Some successful inductive learning systems include decision trees <ref> (Quinlan, 1993) </ref> (see Section 8) neural networks (Gallant, 1993; Kung, 1993; Hassoun, 1995) (see Section 9), and statistical pattern recognition systems (Duda & Hart, 1973; Fukunaga, 1990) (see Section 10). 4 Expert Systems Expert Systems (Durkin, 1994; Puppe, 1993; Stefik, 1995) are programs that model the expertise (knowledge) and reasoning capabilities <p> These are: decision trees, some simple classes of artificial neural networks (or connectionist networks) and some elementary statistical approaches. The following sections discuss these systems in more detail. 19 8 Decision Trees ID3 <ref> (Quinlan, 1993) </ref> is an inductive method in which the classification rule is expressed in the form of a decision tree. <p> In decision tree construction, it simply means that given a set of examples, the procedure should construct, in some reasonable sense of the term, the simplest decision tree that correctly classifies the training set. The ID3 <ref> (Quinlan, 1993) </ref> algorithm uses an information-based method for constructing the decision tree. This procedure works by determining the information-gain associated with branching on a particular attribute. <p> However, the information-based approach can occasionally construct trees that may be counter-intuitive even to experts in that domain. Other drawbacks associated with the use of decision tree construction algorithms such as ID3 and some possible remedies are documented in <ref> (Quinlan, 1993) </ref>.
Reference: <author> Rich, E., & Knight, K. </author> <year> (1991). </year> <booktitle> Aritificial Intelligence. </booktitle> <address> New York, NY: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Russell, S., & Norvig, P. </author> <year> (1995). </year> <title> Artificial Intelligence A Modern Approach. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Schank, R. </author> <year> (1982). </year> <title> Dynamic Memory: A theory of learning in computers and people. </title> <address> New York, NY: </address> <publisher> Cambridge University Press. 36 Schank, </publisher> <editor> R., & Abelson, R. </editor> <year> (1977). </year> <title> Scripts, plans, goals and understanding. </title> <address> Northvale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Shannon, C. </author> <year> (1948). </year> <title> A Mathematical Theory of Communication. </title> <journal> Bell System Technical Journal, 27(July and October), </journal> <pages> 379-423, 623-656. </pages>
Reference-contexts: However, this approach can be easily extended to handle multiple classes as will be demonstrated using the automobile diagnosis example later on. The expected information (number of questions that need to be asked) required to classify the given input can be shown to be (refer <ref> (Shannon, 1948) </ref> for details) - I (p; n) = p + n p p + n n (1) where p and n are the number of examples in the training set that belong to class 1 and class 2 respectively.
Reference: <author> Shavlik, J. </author> <year> (1994). </year> <title> A Framework for Combining Symbolic and Neural Learning. Pages 561-580 of: Honavar, </title> <editor> V., & Uhr, L. (eds), </editor> <title> Artificial Intelligence and Neural Networks: Steps Toward Principled Integration. </title> <address> San Diego, CA: </address> <publisher> Academic Press. </publisher>
Reference: <author> Shavlik, J., & Dietterich, T. </author> <year> (1990). </year> <booktitle> Readings in Machine Learning. </booktitle> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Simon, H. A. </author> <year> (1983). </year> <title> Why should machines learn? In: </title> <editor> Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (eds), </editor> <booktitle> Machine Learning AnArtificial Intelligence Approach. </booktitle> <address> Palo Alto, CA: </address> <publisher> Tioga. </publisher>
Reference: <author> Stefik, M. </author> <year> (1995). </year> <title> Introduction to Knowledge Systems. </title> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sun, R., & Bookman, L. (eds). </author> <year> (1994). </year> <title> Computational Architectures Integrating Symbolic and Neural Processes. </title> <address> New York: </address> <publisher> Kluwer. </publisher>
Reference: <author> Tanimoto, S. L. </author> <year> (1995). </year> <title> Elements of Artificial Intelligence Using Common Lisp. </title> <address> New York: </address> <publisher> Computer Science Press. </publisher>
Reference-contexts: A broad range of tools have been developed for efficient probabilistic reasoning using what are called belief networks (Pearl, 1988; Dean et al., 1995; Russell & Norvig, 1995). Other, not necessarily statistical techniques for reasoning under uncertainty include Dempster-Schaefer calculus, fuzzy logic, and related methods <ref> (Tanimoto, 1995) </ref>. In summary, statistical approaches such as the one outlined above rely on the avail-abililty of the necessary class-conditional probabilities (or a representative set of samples from which such probabilities can be estimated).
Reference: <author> Uhr, L. </author> <year> (1973). </year> <title> Pattern Recognition, </title> <booktitle> Learning and Thought. </booktitle> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Winston, P. </author> <year> (1992). </year> <booktitle> Artificial Intelligence. </booktitle> <address> New York, NY: </address> <publisher> Addison-Wesley. </publisher> <pages> 37 </pages>
References-found: 47

