URL: http://www.ncc.up.pt/~ltorgo/Papers/KRT.ps.gz
Refering-URL: http://www.ncc.up.pt/~ltorgo/Papers/list_pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Kernel Regression Trees  
Author: Lus Torgo 
Keyword: Propositional learning, regression trees, instance-based methods. Category Full Paper  
Address: 823 4150 Porto Portugal  
Affiliation: University of Porto R. Campo Alegre,  
Email: email  
Phone: LIACC  Phone (+351) 2 6001672 Fax (+351) 2 6003654  
Web: ltorgo ncc.up.pt WWW: http://www.up.pt/~ltorgo  
Abstract: This paper presents a novel method for learning in domains with continuous target variables. The method integrates regression trees with kernel regression models. The integration is done by adding kernel regressors at the tree leaves producing what we call kernel regression trees. The approach is motivated by the goal of trying to take advantage of the different biases of the two regression methodologies. The presented method is implemented. Kernel regression trees are comprehensible and accurate regression models of the data. Experimental comparisons on both artificial and real world domains revealed the superiority of kernel regression trees when compared to the two individual approaches. The use of kernel regression at the trees leaves gives a significant performance gain. Moreover, a good performance level is achieved with much smaller trees than if kernel regression was not used. Compared to kernel regression our method improves its comprehensibility and execution time. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. </author> <title> (1990) : A study of instance-based learning algorithms for supervised learning tasks: Mathematical, empirical, and psychological evaluations. </title> <type> PhD Thesis. Technical Report 90-42. </type> <institution> University of California at Irvine, Department of Information and Computer Science. </institution>
Reference-contexts: Several techniques exist that try to overcome these limitations. Feature weighing can help to the reduce the influence of irrelevant features by better tuning the distance function (Wettshereck et al. 1996). Sampling techniques, indexing schemes and instance prototypes are some of the methods used with large samples <ref> (Aha, 1990) </ref>. Still, local regression methods have a strong limitation when it comes to getting a better insight of the structure of the domain being studied (Breiman et al., 1984). The resulting regression models have low interpretability. 3. Kernel Regression Trees Our proposed method integrates regression trees with kernel regression. <p> Time complexity is just one of the facets of computational efficiency. Another important issue is storage requirements. HTL (like kernel regression methods) saves all training instances distributed by the tree leaves. As we have referred in section 2.2, several techniques exist to overcome this problem <ref> (Aha, 1990) </ref>. These techniques could easily be applied within HTL which would reduce its storage requirements. An obvious thing to do is to take advantage of the instances generalization provided by the tree. Similar approaches were followed in EACH (Salzsberg, 1991) and RISE (Domingos, 1996) systems.
Reference: <author> Aha, D. </author> <title> (1992) : Generalizing from case studies : A case study. </title> <booktitle> In Proceedings of the 9th International Conference on Machine Learning. </booktitle> <editor> Sleeman,D. & Edwards,P. (eds.). </editor> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Atkeson,C.G., Moore,A.W., Schaal,S. </author> <title> (in press) : Locally Weighted Learning. </title> <note> Special issue on lazy learning. </note>
Reference: <author> Aha, D. </author> <title> (Ed.). </title> <journal> Artificial Intelligence Review </journal> . 
Reference: <author> Bentley,J.L. </author> <title> (1975) : Multidimensional binary search trees used for associative searching. </title> <booktitle> Communications of the ACM , 18 (9), </booktitle> <pages> 509-517. </pages>
Reference: <author> Bohanec, M., Bratko.I. </author> <title> (1994) : Trading Accuracy for Simplicity in Decision Trees. </title> <booktitle> In Machine Learning , 15 - 3, </booktitle> <pages> 223-250. </pages> <publisher> Kluwer Academic Publishers. </publisher> <address> Brazdil,P. , Gama,J., </address> <month> Henery,B. </month> <title> (1994) : Characterizing the applicability of classification algorithms using metalevel learning. </title> <booktitle> In Proceedings of the European Conference on Machine Learning (ECML-94). </booktitle>
Reference-contexts: Moreover, the experiments revealed that kernel regression trees achieve significantly better accuracy than regression trees with smaller trees. This indicates that our hybrid method provides a better tradeoff between accuracy and simplicity which is considered highly relevant with real world applications <ref> (Bohanec & Bratko, 1994) </ref>. In the next section we briefly describe the regression problem and present the two approaches we propose to integrate. We then present the kernel regression tree approach on section 3.
Reference: <editor> Bergadano,F. & De Raedt,L. (eds.). </editor> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> 784. </volume> <publisher> Springer-Verlag. </publisher> <address> Brazdil,P. </address> <month> Torgo,L. </month> <title> (1990) : Knowledge Acquisition via Knowledge Integration. In Current Trends in Knowledge Acquisition . Wielinga,B. </title> <editor> et al. (eds.). </editor> <publisher> IOS Press. </publisher> <address> Breiman,L. </address> <booktitle> (1996) : Bagging Predictors. In Machine Learning , 24 , (p.123-140). </booktitle> <publisher> Kluwer Academic Publishers. </publisher> <address> Breiman,L. , Friedman,J.H., Olshen,R.A. & Stone,C.J. </address> <year> (1984): </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth Int. Group, </publisher> <address> Belmont, California, USA, </address> <year> 1984. </year>
Reference: <author> Broadley, C. E. </author> <title> (1995) : Recursive automatic bias selection for classifier construction. Machine Learning , 20 , 63-94. </title> <publisher> Kluwer Academic Publishers. </publisher> <month> Cattlet,J. </month> <title> (1991) : Megainduction : a test flight. </title> <booktitle> In Proceedings of the 8th International Conference on Machine Learning. </booktitle> <editor> Birnbaum,L. & Collins,G. (eds.). </editor> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Experimental comparisons of different learning methods on various real world problems have shown the difficulty of selecting a method that performs better on all domains (Michie et al., 1994). This is sometimes called the selective superiority problem <ref> (Broadley , 1995) </ref>. Several authors have tried to automatically identify the applicability of the different methods (Aha, 1992; Brazdil et al., 1994).
Reference: <author> Charfield, C. </author> <title> (1983) : Statistics for technology (third edition). </title> <publisher> Chapman and Hall, Ltd. </publisher>
Reference: <author> Cleveland, </author> <title> W.S. (1979) : Robust locally weighted regression and smoothing scatterplots. </title> <journal> Journal of the American Statistical Association , 74 , 829-836. </journal>
Reference: <author> Cleveland,W.S., Loader,C.R. </author> <title> (1995) : Smoothing by Local Regression: Principles and Methods (with discussion). In Computational Statistics . Deng,K., Moore,A.W. (1995) : Multiresolution Instance-based Learning. In Proceedings of IJCAI95. Domingos,P. (1996) : Unifying Instance-based and Rule-based Induction. </title> <booktitle> In Machine Learning , 24 -2, </booktitle> <pages> 141-168. </pages> <publisher> Kluwer Academic Publishers. </publisher> <month> Fan,J. </month> <title> (1995) : Local Modelling. EES update: </title> <institution> written for the Encyclopidea of Statistics Science. </institution>
Reference: <author> Fix, E., Hodges, J.L. </author> <title> (1951) : Discriminatory analysis, nonparametric discrimination consistency properties. </title> <type> Technical Report 4, </type> <institution> Randolph Filed, TX: US Air Force, School of Aviation Medicine. Freund,Y., </institution> <month> Schapire,R.E. </month> <title> (1995) : A decision-theoretic generalization of online learning and an application to boosting. </title> <institution> Technical Report . AT & T Bell Laboratories. </institution>
Reference-contexts: These global parametric approaches have been widely used and give good predictive results when the assumed model correctly fits the data. Modern statistical approaches to regression include k-nearest neighbors <ref> (Fix & Hodges, 1951) </ref>, kernel regression (Watson, 1964; Nadaraya, 1964), local regression (Stone, 1977; Cleveland, 1979), radial basis functions, neural networks, projection pursuit regression (Friedman & Stuetzle, 1981), adaptive splines (Friedman, 1991), and others. ML researchers have always been mainly concerned with classification problems.
Reference: <author> Friedman, J. </author> <title> (1991) : Multivariate Adaptative Regression Splines. </title> <journal> In Annals of Statistics , 19:1. </journal>
Reference-contexts: Modern statistical approaches to regression include k-nearest neighbors (Fix & Hodges, 1951), kernel regression (Watson, 1964; Nadaraya, 1964), local regression (Stone, 1977; Cleveland, 1979), radial basis functions, neural networks, projection pursuit regression (Friedman & Stuetzle, 1981), adaptive splines <ref> (Friedman, 1991) </ref>, and others. ML researchers have always been mainly concerned with classification problems. The few existent systems dealing with regression usually build axis orthogonal partitions of the input space and fit a parametric model within each of these partitions.
Reference: <author> Friedman,J.H., Stuetzle,W. </author> <title> (1981) : Projection pursuit regression. </title> <journal> Journal of the American Statistical Association , 76 (376), </journal> <pages> 817-823. </pages> <address> Hastie,T., </address> <month> Loader,C. </month> <title> (1993) : Local Regression: Automatic Kernel Carpentry. </title> <journal> In Statistical Science, </journal> <volume> 8 , (p.120-143). </volume>

Reference: <author> Nadaraya, </author> <title> E.A. (1964) : On estimating regression. Theory of Probability and its Applications , 9 :141-142. Niblett,T., Bratko,I. (1986) : Learning decision rules in noisy domain. </title> <publisher> Expert Systems , 86 . Cambridge University Press. </publisher>
Reference: <author> Quinlan, J. R. </author> <note> (1993) : C4.5 : programs for machine learning . Morgan Kaufmann Publishers. </note>
Reference-contexts: RETIS (Karalic, 1992) and M5 (Quinlan, 1992) are able to use linear models of the predictor variables. We will see how our system HTL adds a further degree of smoothness by using kernel regressors at the tree leaves. Regression trees obtain good predictive accuracy on many domains <ref> (Quinlan, 1993) </ref>. However, the simple models used on their leaves have some limitations regarding the kind of functions they are able to approximate. <p> One problem of this approach is that the number I needs to estimated which can be computationally demanding (Torgo & Gama, 1996). The local modeling used in this system is not elaborated as ours due to the absence of kernel functions, feature weights and flexible bandwidths. M5 system <ref> (Quinlan, 1993) </ref> also uses regression trees and k-nearest neighbors. However, this system performs prediction combination instead of integrating the two methodologies like in HTL. This means that two models are independently obtained and their predictions combined. This has strong implications in terms of comprehensibility as mentioned before.
Reference: <author> Quinlan, J.R. </author> <year> (1992): </year> <title> Learning with Continuos Classes. </title> <booktitle> In Proceedings of the 5th Australian Joint Conference on Artificial Intelligence . Singapore: World Scientific, </booktitle> <year> 1992. </year>
Reference-contexts: Some of the existent approaches to regression trees differ on this later issue. CART (Breiman et al., 1984) assigns a constant to the leaves (the average y value). RETIS (Karalic, 1992) and M5 <ref> (Quinlan, 1992) </ref> are able to use linear models of the predictor variables. We will see how our system HTL adds a further degree of smoothness by using kernel regressors at the tree leaves. Regression trees obtain good predictive accuracy on many domains (Quinlan, 1993).
Reference: <author> Quinlan, J.R. </author> <year> (1992): </year> <title> Learning with Continuos Classes. </title> <booktitle> In Proceedings of the 5th Australian Joint Conference on Artificial Intelligence . Singapore: World Scientific, </booktitle> <year> 1992. </year>
Reference-contexts: Some of the existent approaches to regression trees differ on this later issue. CART (Breiman et al., 1984) assigns a constant to the leaves (the average y value). RETIS (Karalic, 1992) and M5 <ref> (Quinlan, 1992) </ref> are able to use linear models of the predictor variables. We will see how our system HTL adds a further degree of smoothness by using kernel regressors at the tree leaves. Regression trees obtain good predictive accuracy on many domains (Quinlan, 1993).
Reference: <author> Quinlan,J.R. </author> <title> (1993) : Combining Instance-based and Model-based Learning. </title> <booktitle> Proceedings of the 10th ICML. </booktitle> <publisher> Morgan Kaufmann. </publisher> <address> Robnik-Sikonja,M., </address> <month> Kononenko,I. </month> <title> (1996) : Contextsensitive attribute estimation in regression. </title> <booktitle> Proceedings of the ICML-96 Workshop on Learning in ContextSensitive Domains. Salzberg,S. (1991) : A nearest hyperrectangle learning method. Machine Learning , 6 -3, </booktitle> <pages> 251-276. </pages> <publisher> Kluwer Academic Publishers. </publisher> <address> Smyth,P., Gray,A., </address> <month> Fayyad,U.M. </month> <title> (1995) : Retrofitting Decision Tree Classifiers using Kernel Density Estimation. </title> <booktitle> Proceedings of the 12th International Conference Machine Learning. </booktitle> <editor> Prieditis,A., Russel,S. (Eds.). </editor> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Stone, C.J. </author> <title> (1977) : Consistent nonparametric regression. </title> <journal> The Annals of Statistics , 5 , 595-645. </journal>
Reference: <author> Torgo, L. </author> <title> (1995) : Data Fitting with Rule-based Regression. </title> <booktitle> In Proceedings of the 2nd international workshop on Artificial Intelligence Techniques (AIT95) , Zizka,J. </booktitle> <editor> and Brazdil,P. (eds.). </editor> <address> Brno, Czech Republic. Torgo,L. </address> <month> Gama,J. </month> <title> (1996) : Regression by Classification. </title> <note> To appear in Proceedings of SBIA96. Springer-Verlag. </note> <author> Utgoff,P. </author> <title> (1989) : Incremental induction of decision trees. </title> <booktitle> Machine Learning , 4 -2, </booktitle> <pages> 161-186. </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Watson, G.S. </author> <title> (1964) : Smooth Regression Analysis. </title> <journal> Sankhya: The Indian Journal of Statistics , Series A, </journal> <pages> 26 : 359-372. </pages>
Reference: <author> Weiss, S. and Indurkhya, N. </author> <title> (1993) : Rule-base Regression. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence , pp. </booktitle> <pages> 1072-1078. </pages>
Reference: <author> Weiss, S. and Indurkhya, N. </author> <title> (1995) : Rule-based Machine Learning Methods for Functional Prediction. </title> <journal> In Journal Of Artificial Intelligence Research (JAIR), </journal> <volume> volume 3, </volume> <month> pp.383-403. </month> <title> Wettschereck,D. (1994) : A study of distance-based machine learning algorithms. </title> <type> PhD thesis. </type> <institution> Oregon State University. Wettschereck,D., Aha,D.W., </institution> <month> Mohri,T. </month> <title> (in press) : A review and empirical evaluation of feature weighting methods for a class of lazzy learning algorithms. Special issue on lazy learning. </title> <editor> Aha, D. (Ed.). </editor> <booktitle> Artificial Intelligence Review </booktitle> . 
Reference: <author> Wolpert,D.H. </author> <title> (1992) : Stacked Generalization. </title> <booktitle> In Neural Networks , 5 , (p.241-259). </booktitle>
References-found: 25

