URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1998/tr-98-028.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1998.html
Root-URL: http://www.icsi.berkeley.edu
Title: Learning from data: general issues and special applications of Radial Basis Function networks deductive types
Author: A. Baraldi N. A. Borghese 
Keyword: Key words: Inductive  
Note: and  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  1947 Center Street, Suite 600, Berkeley, CA 94704-1198, Ph.:  via f.lli Cervi 93 20090 Segrate (Milano), Italy Ph.:  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  International Computer Science Institute,  Laboratory of  Istituto Neuroscienze Bioimmagini C.N.R.  
Pubnum: TR-98-028  
Email: baraldi@icsi.berkeley.edu  borghese@inb.mi.cnr.it  
Phone: (510) 643-9153 FAX (510) 643-7684  +1+510+643-9153, Fx.: +1+510+643-7684,  +39+2+; Fax: +39+2+,  
Date: August 1998  
Abstract: In the first part of this work some important issues regarding the use of data-driven learning systems are discussed. Next, a special category of learning systems known as artificial Neural Networks (NNs) is presented. Our attention is focused on a specific class of NNs, termed Radial Basis Function (RBF) networks, which are widely employed in classification and function regression tasks. A constructive RBF network, termed Hierarchical RBF (HRBF) model, is proposed. An application where the HRBF model is applied to reconstruct a continuous 3-D surface from range data samples is presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. Cherkassky and F. Mulier, </author> <title> Learning From Data: Concepts, Theory, and Methods, </title> <publisher> Wiley: </publisher> <address> New York, </address> <year> 1998. </year>
Reference-contexts: 1 Learning from data Modern science describes physical, biological and social systems in terms of first-principle models (e.g., Newton's laws of mechanics) to be verified on the basis of experimental data <ref> [1] </ref>. However, in many applications the underlying first principles are unknown or the systems under study are too complex to be described analytically. In the absence of first principles, experimental data can be used to derive unknown (input, output) dependencies between a system's variables. <p> In recent years the use of low-cost sensors and computers has increased the availability and usability of large experimental data sets. Thus, there has been a shift from classical modelling, based on first principles, to the development of models from data <ref> [1] </ref>. The latter approach is known as data-driven learning [1], or learning-by-examples [2]. In humans, this type of learning mechanism is involved, for example, with the simplest cognitive processes in infants and with commonsense reasoning and spontaneous generalization in adults [2]. <p> Thus, there has been a shift from classical modelling, based on first principles, to the development of models from data <ref> [1] </ref>. The latter approach is known as data-driven learning [1], or learning-by-examples [2]. In humans, this type of learning mechanism is involved, for example, with the simplest cognitive processes in infants and with commonsense reasoning and spontaneous generalization in adults [2]. <p> Predictive learning tasks. Whereas deductive learning provides special cases (e.g., output values) from general models, the goal of predictive learning, also called inductive learning or specific-to-general type of inference, is to provide good generalization (i.e., a good model) from a finite set of particular labeled cases (supervised training examples) <ref> [1] </ref>. In detail, on the basis of the supervised training set, a predictive learning system selects the approximating function that minimizes a cost function for unobserved (future) samples from a wide class of candidated functions [1]. <p> (i.e., a good model) from a finite set of particular labeled cases (supervised training examples) <ref> [1] </ref>. In detail, on the basis of the supervised training set, a predictive learning system selects the approximating function that minimizes a cost function for unobserved (future) samples from a wide class of candidated functions [1]. Two classes of predictive learning tasks are: (a) Function regression, where the task is to estimate an unknown continuous function, capable of giving good predictions for unobserved data, from eventually noisy supervised training samples. When training examples are not affected by noise, regression problems are called function interpolation problems [1]. <p> <ref> [1] </ref>. Two classes of predictive learning tasks are: (a) Function regression, where the task is to estimate an unknown continuous function, capable of giving good predictions for unobserved data, from eventually noisy supervised training samples. When training examples are not affected by noise, regression problems are called function interpolation problems [1]. <p> To improve the performance of learning systems in minimizing the actual risk on the basis of a limited amount of data, quantitative bounds and equations relating the actual risk to the empirical risk, model complexity and the number of training examples have been investigated in recent years <ref> [1] </ref>, [4]. 2.2 Inductive principles The complexity of a learning system increases with the number of independent and adjustable parameters, also termed degrees of freedom, to be adapted during the learning process. <p> This is tantamount to saying that the choice of and ![ ^ f (w; x)] is a problem of model selection, i.e., a task in which a model of optimal complexity is chosen for the given finite data set <ref> [1] </ref>. <p> As correct specification of priors is difficult to accomplish in practice, predictive learning systems become more robust and flexible if the regularization parameter is chosen on the basis of the training set (for example, by means of cross-validation <ref> [1] </ref>, [5]) for a given type of user-defined penalty functional ![ ^ f (w; x)]. Several statistical techniques, besides the regularization inductive principle, have proposed different bounds and equations relating actual risk to empirical risk, model complexity and the number of training examples within the framework of predictive learning. <p> This parameter, known as the VC dimension, is a characteristic of the set of approximating functions supported by the learning system and is a measure of model complexity <ref> [1] </ref>, [10]. In Eq. (7), function [ h M ], called the VC confidence term or confidence interval, decreases monotonically with size M of the training set and monotonically increases with h. <p> For function regression problems VC theory provides the following bound, which is useful for model selection <ref> [1] </ref> h 0:8 for min 4 M where (1 ) 2 [0; 1] is called the confidence level. <p> For function regression problems VC theory provides the following bound, which is useful for model selection [1] h 0:8 for min 4 M where (1 ) 2 <ref> [0; 1] </ref> is called the confidence level. <p> Like parametric learning systems, they employ simple learning procedures, but their complexity increases with the size of the training set. Learning systems where the problem is to estimate both optimal model complexity and model parameters are called semiparametric <ref> [1] </ref>, [6]. Unlike parametric learning algorithms, semiparametric systems are not restricted to specific functional forms. Moreover, the complexity of semiparametric systems must increase with the complexity of the problem being solved rather than with the size of the training set as for non-parametric systems. <p> A special case of semiparametric systems is the class of constructive procedures, also called growing algorithms, where model complexity increases dynamically and the choice 5 among candidated solutions is based on their performance over the entire collection of training examples <ref> [1] </ref>, [4]. 2.4 Curse of dimensionality In previous sections it has been shown why, in predictive learning, meaningful function estimation is possible only for sufficiently smooth target functions. A smoothness constraint essentially defines possible function behaviors in local neighborhoods of the input space [1]. <p> over the entire collection of training examples <ref> [1] </ref>, [4]. 2.4 Curse of dimensionality In previous sections it has been shown why, in predictive learning, meaningful function estimation is possible only for sufficiently smooth target functions. A smoothness constraint essentially defines possible function behaviors in local neighborhoods of the input space [1]. It is obvious that the accuracy of function estimation depends on having enough training samples within the local neighborhood specified by a smoothness constraint in the input space. Unfortunately, the number of samples yielding the same density increases exponentially with the dimensionality of the input space [1], [6]. <p> the input space <ref> [1] </ref>. It is obvious that the accuracy of function estimation depends on having enough training samples within the local neighborhood specified by a smoothness constraint in the input space. Unfortunately, the number of samples yielding the same density increases exponentially with the dimensionality of the input space [1], [6]. This effect is known as the "curse of dimensionality." As an example of the "curse of dimensionality," consider that any function estimator increases its number of adjustable parameters with the dimensionality of input space. <p> complexity requires sample size to be at least 10 times the number of free parameters in the model [16]. 2.5 Classification of methods for regression One possible way to classify methods for estimating continuous-valued functions from noisy samples is to develop a taxonomy based on dictionary representation versus kernel representation <ref> [1] </ref>. <p> If basis functions are fixed, parameterization (9) becomes linear with respect to parameters w which can be estimated from the training set by means of linear least squares methods <ref> [1] </ref>, [6]. 6 In kernel representation, a continuous-valued function estimator is expressed as a distance- weighted combination of observed output values ^y (x) = h=1 where (input, output) pairs (x h ; y h ), h = 1; :::; M , are the observed examples that make up the supervised training <p> If p = 2, Eq. (11) provides the Euclidean distance. Kernel function k (d (a; b)) usually (but not always) satisfies the following properties <ref> [1] </ref> k (d (a; b)) 0: Nonnegative. (12a) k (d (a; b)) = k (d (b; a)): Radially symmetric. (12b) k (d (a; a)) = max. <p> This means that when Eq. (14) employs a Gaussian weighting function then no exact interpolation is pursued because not every observed input value is mapped exactly onto its corresponding target value. It is important to stress that a clear distinction between dictionary and kernel methods as proposed in <ref> [1] </ref> is rather obscure in the rest of the literature where the term "kernel function" is often employed to identify localized basis functions in dictionary methods and density function estimators. 2.6 Orthogonal basis functions An interesting case where the computation of the weights of a dictionary representation (9) is significantly simplified <p> If an orthonormal basis is employed in Eq. (9), and equally-spaced samples are collected in the supervised training set, then minimization of the sum-of-squares error with respect to weight w i , i 2 f1; Cg, yields <ref> [1] </ref> @R (w) = R h P C i 2 @w i Z 4 y (x) j=1 3 2 y (x)g i (x)dx + 2 j=1 Z Z i.e., Z Eq. (17) shows that in signal processing, where the goal is to find a compact and accurate representation of a known <p> This is tantamount to stating that C is considered a regularization parameter <ref> [1] </ref>. Both MLPs and RBF networks have been proved to work as universal approximators, i.e., they are capable of approximating any arbitrary mapping between multidimensional spaces to any degree of precision if they are provided with a sufficient number C of processing units [21]. <p> when its adjustable parameter does not tend to either zero or infinity); and v) the use of unsupervised learning methods can be quite successful in practice when the distribution of the input patterns is highly non-uniform and/or its effective di mensionality is small, i.e., correlation between input variables is high <ref> [1] </ref>. <p> Definition 1. The Pass Band of a realistic low-pass filter h (x) is defined as the interval in which the amplitude spectrum of filter FT, jH (-)j, is bounded in range <ref> [d 1 ; 1] </ref>, i.e., jH (-)j 2 [d 1 ; 1] when - 2 [0; cutoff]; (28) where cutoff, called cut-off frequency, is such that jH (-cutoff )j = d 1 , where 0 &lt;< d 1 &lt; 1 is a functional parameter. <p> Definition 1. The Pass Band of a realistic low-pass filter h (x) is defined as the interval in which the amplitude spectrum of filter FT, jH (-)j, is bounded in range <ref> [d 1 ; 1] </ref>, i.e., jH (-)j 2 [d 1 ; 1] when - 2 [0; cutoff]; (28) where cutoff, called cut-off frequency, is such that jH (-cutoff )j = d 1 , where 0 &lt;< d 1 &lt; 1 is a functional parameter. <p> Unfortunately, this choice may cause a waste of resources when the highest frequency content of the target function is concentrated (localized) in a narrow region of the input space (i.e., when the target 21 signal is non-stationary <ref> [1] </ref>). In this case fewer Gaussians, featuring a larger spread, can be used to reconstruct the signal in those domain intervals where the scale of y (x) is larger.
Reference: [2] <author> R. Serra and G. Zanarini, </author> <title> Complex Systems and Cognitive Processes, </title> <publisher> Springer-Verlag: </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: Thus, there has been a shift from classical modelling, based on first principles, to the development of models from data [1]. The latter approach is known as data-driven learning [1], or learning-by-examples <ref> [2] </ref>. In humans, this type of learning mechanism is involved, for example, with the simplest cognitive processes in infants and with commonsense reasoning and spontaneous generalization in adults [2]. <p> The latter approach is known as data-driven learning [1], or learning-by-examples <ref> [2] </ref>. In humans, this type of learning mechanism is involved, for example, with the simplest cognitive processes in infants and with commonsense reasoning and spontaneous generalization in adults [2]. <p> In this context the term distributed means that NNs consist of a multeplicity of simple and mutually interconnected processing elements. Owing to these properties NNs are also termed complex systems <ref> [2] </ref>. The role of NNs is to determine (input, output) dependencies that are unknown, or too complex to be described analytically, on the basis of a finite training set of examples. <p> An important consequence of this observation is that, in the connectionist approach, data-driven learning cannot be studied independently of the physical support of the cognitive system. This is in contrast with the study of syntactic (symbolic) high-level information processing systems based on learning-by-rule mechanisms <ref> [2] </ref>. In these systems a set of decision rules representing application-domain specific background knowledge (i.e., independent of training data) may be explicitly taught to the system by an external supervisor.
Reference: [3] <author> N. A. Borghese, G. Ferrigno, G. Baroni, A. Pedotti, S. Ferrari and R. Savare, "Autoscan: </author> <title> a flexible and portable 3D scanner," </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> pp. 2-5, </pages> <month> May/June </month> <year> 1998. </year>
Reference-contexts: A constructive RBF network, termed Hierarchical RBF (HRBF) model, is presented. An application where the HRBF model is applied to reconstruct a continuous 3-D surface from range data samples is proposed <ref> [3] </ref>. 2 Issues in learning from data This section briefly discusses some important issues regarding the use of data-driven learning systems. 2.1 Actual risk and empirical risk In Section 1, an inductive learning machine is described as an adaptive system whose goal is to select, from a set of candidated functions, <p> Dimension Z is measured in mm. The error of the real-time image processor (the Elite system <ref> [3] </ref>) is below 1 mm. To avoid removal of useful information, the average local error threshold * is set to 0.5. Since frequency MAX of the target surface is unknown, then the Gaussian interval 4 max cannot be derived from Eq. (40). Let us estimate 4 max as follows.
Reference: [4] <author> T. Mitchell, </author> <title> Machine Learning, </title> <publisher> McGraw-Hill: </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: To improve the performance of learning systems in minimizing the actual risk on the basis of a limited amount of data, quantitative bounds and equations relating the actual risk to the empirical risk, model complexity and the number of training examples have been investigated in recent years [1], <ref> [4] </ref>. 2.2 Inductive principles The complexity of a learning system increases with the number of independent and adjustable parameters, also termed degrees of freedom, to be adapted during the learning process. <p> According to the qualitative principle of Occam's razor, a sound basis for generalizing beyond a given set of examples is to prefer the simplest hypothesis that fits observed data <ref> [4] </ref>, [6]. This principle states that to be effective, the cost function minimized by an inductive learning system should provide a trade-off between how well the model fits the training data and model complexity. <p> A special case of semiparametric systems is the class of constructive procedures, also called growing algorithms, where model complexity increases dynamically and the choice 5 among candidated solutions is based on their performance over the entire collection of training examples [1], <ref> [4] </ref>. 2.4 Curse of dimensionality In previous sections it has been shown why, in predictive learning, meaningful function estimation is possible only for sufficiently smooth target functions. A smoothness constraint essentially defines possible function behaviors in local neighborhoods of the input space [1]. <p> Takes on its maximum when a = b: (12c) lim t!1 k (t) = 0: Localized function [6]. (12d) Kernel methods are examples of instance- or memory-based learning strategies because they are based on storing all training data <ref> [4] </ref>, [11]. The basic idea behind kernel representation is that when a new query instance is presented, the target value is estimated only on the basis of training examples near the query point, as training examples are individually weighted by their distance from the query point. <p> This also means that unlike Eq. (9), which is designed to perform well over the entire instance space, instance-based learning approaches actually construct a different approximation to the target function for each distinct query instance <ref> [4] </ref>. For the sake of simplicity, let us consider a mapping between two 1-D spaces, i.e., x h 2 R, y h 2 R, h = 1; :::; M . <p> generated by a target function, which is smooth and noise-free, while the input data is corrupted by additive Gaussian noise [6], [25]; (e) Kernel regression, in which a noise-affected function is estimated by means of the Gaussian kernel-based method for density estimation [6]. (f) Distance-weighted regression methods, also termed instance-based <ref> [4] </ref>, or memory-based learning methods [11] (see Section 2.5). Instance-based methods construct only a local approximation to the target function that applies in the neighborhood of the new query instance, and never construct an approximation designed to perform well over the entire input space [4]. <p> Distance-weighted regression methods, also termed instance-based <ref> [4] </ref>, or memory-based learning methods [11] (see Section 2.5). Instance-based methods construct only a local approximation to the target function that applies in the neighborhood of the new query instance, and never construct an approximation designed to perform well over the entire input space [4]. RBF networks employing localized RBFs to construct approximations that perform well over the entire input space can be considered an interesting bridge between instance-based and neural network learning algorithms [4]. (g) Bayes optimal classifiers, where the density of the input data is expressed in terms of a mixture distribution whose <p> neighborhood of the new query instance, and never construct an approximation designed to perform well over the entire input space <ref> [4] </ref>. RBF networks employing localized RBFs to construct approximations that perform well over the entire input space can be considered an interesting bridge between instance-based and neural network learning algorithms [4]. (g) Bayes optimal classifiers, where the density of the input data is expressed in terms of a mixture distribution whose components are the basis functions [4], [6]. 4 Learning techniques for RBF networks The traditional view of RBF networks where the centers of RBFs are regarded as templates has led <p> to construct approximations that perform well over the entire input space can be considered an interesting bridge between instance-based and neural network learning algorithms <ref> [4] </ref>. (g) Bayes optimal classifiers, where the density of the input data is expressed in terms of a mixture distribution whose components are the basis functions [4], [6]. 4 Learning techniques for RBF networks The traditional view of RBF networks where the centers of RBFs are regarded as templates has led to the development of hybrid learning schemes whose first stage performs unsupervised clustering. <p> The sampling rate of the regular grid at level one is eventually computed from the data on the basis of linear filtering theory; for next layers, the sampling rate doubles the sampling rate value of the lower level. 5 Stopping criterion: back-tracking through cross-validation (non-greedy algorithm) <ref> [4] </ref>. 6 (Batch) Gradient Descent of the sum-of-squares error; 7 Gradient Descent of the Class-conditional Variance; 8 Stochastic (on-line) Gradient Descent of the sum-of-squares error; 17 does not consist of a step edge nor is the filter response within each band flat.
Reference: [5] <author> R. Kohavi, </author> <title> "A study of cross-validation and bootstrap for accuracy estimation and model selection," </title> <booktitle> in Proc. IJCAI-93, Int'l Joint Conferences on Artificial Intelligence Inc., </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year> <pages> 25 26 27 28 </pages>
Reference-contexts: As correct specification of priors is difficult to accomplish in practice, predictive learning systems become more robust and flexible if the regularization parameter is chosen on the basis of the training set (for example, by means of cross-validation [1], <ref> [5] </ref>) for a given type of user-defined penalty functional ![ ^ f (w; x)]. Several statistical techniques, besides the regularization inductive principle, have proposed different bounds and equations relating actual risk to empirical risk, model complexity and the number of training examples within the framework of predictive learning.
Reference: [6] <author> C. M. Bishop, </author> <title> Neural Networks for Pattern Recognition, </title> <publisher> Clarendon Press: </publisher> <address> Oxford (UK), </address> <year> 1995. </year>
Reference-contexts: According to the qualitative principle of Occam's razor, a sound basis for generalizing beyond a given set of examples is to prefer the simplest hypothesis that fits observed data [4], <ref> [6] </ref>. This principle states that to be effective, the cost function minimized by an inductive learning system should provide a trade-off between how well the model fits the training data and model complexity. <p> For example, in function regression tasks, the regularization term may penalize candidated functions that are not smooth, i.e., those presenting large oscillations, by computing <ref> [6] </ref> ![ ^ f (w; x)] = d 2 ^ f (w; x) ! 2 Under the classical Bayesian approach, both and ![ ^ f (w; x)] are chosen on the basis of a priori knowledge exclusively, which is to say that by definition the observed data are not used for <p> For example, it has been shown that in the limit of an infinite data set, a cost function computed as the sum-of-squared differences between target and estimated outputs is proportional to the sum of a bias term with a variance term <ref> [6] </ref>, [8], i.e., R act (w) / [bias (w)] 2 + variance (w): (6) In Eq. (6) the bias term measures the extent to which the average (over an infinite data set) of the approximating function differs from the target function. <p> For example, an approximating function that is closely fitted to training data (e.g., that exactly interpolates them) will tend to have a small bias. Conversely, in Eq. (6) the variance term measures the extent to which the approximating function is sensitive to the particular choice of the data set <ref> [6] </ref>, [7]. For example, an approximating function that is closely fitted to training data will tend to have a large variance. <p> Like parametric learning systems, they employ simple learning procedures, but their complexity increases with the size of the training set. Learning systems where the problem is to estimate both optimal model complexity and model parameters are called semiparametric [1], <ref> [6] </ref>. Unlike parametric learning algorithms, semiparametric systems are not restricted to specific functional forms. Moreover, the complexity of semiparametric systems must increase with the complexity of the problem being solved rather than with the size of the training set as for non-parametric systems. <p> The disadvantage of semiparametric learning systems is that they are computationally intensive compared to the simple procedures required by both parametric and non-parametric systems <ref> [6] </ref>. One typical example of a semiparametric learning system is the mixture distribution model for density function estimation [6]. <p> The disadvantage of semiparametric learning systems is that they are computationally intensive compared to the simple procedures required by both parametric and non-parametric systems <ref> [6] </ref>. One typical example of a semiparametric learning system is the mixture distribution model for density function estimation [6]. <p> It is obvious that the accuracy of function estimation depends on having enough training samples within the local neighborhood specified by a smoothness constraint in the input space. Unfortunately, the number of samples yielding the same density increases exponentially with the dimensionality of the input space [1], <ref> [6] </ref>. This effect is known as the "curse of dimensionality." As an example of the "curse of dimensionality," consider that any function estimator increases its number of adjustable parameters with the dimensionality of input space. <p> If basis functions are fixed, parameterization (9) becomes linear with respect to parameters w which can be estimated from the training set by means of linear least squares methods [1], <ref> [6] </ref>. 6 In kernel representation, a continuous-valued function estimator is expressed as a distance- weighted combination of observed output values ^y (x) = h=1 where (input, output) pairs (x h ; y h ), h = 1; :::; M , are the observed examples that make up the supervised training set. <p> Takes on its maximum when a = b: (12c) lim t!1 k (t) = 0: Localized function <ref> [6] </ref>. (12d) Kernel methods are examples of instance- or memory-based learning strategies because they are based on storing all training data [4], [11]. <p> approach, the symbolic approach implies that high-level cognitive systems can be studied independently of the structure and functioning of their physical support. 3.1 Advantages and drawbacks of neural networks Many of the important issues concerning the application of NNs can be introduced by considering the simpler case of polynomial estimators <ref> [6] </ref>. Let us consider a set of M (input, output) data pairs (x h ; y h ); h = 1; :::; M , where x h = (x h;1 ; :::; x h;n i ) 2 R n i and y h 2 R. <p> ;i 2 ;:::;i P x i 1 x i 2 :::x i P : 9 Eq. (19) is an example of a linear model, i.e., a learning system supporting candidated functions that depend linearly on the free parameter vector w, although they may not depend linearly on input variables x <ref> [6] </ref>. In the P th-order polynomial the number of degrees of freedom grows as n P i , which represents a dramatic growth in the complexity of the model as the dimensionality of the input space, n i , increases. <p> is to estimate an approximating function as a superposition of non-linear `hidden functions' of adjustable parameters that are adapted to the data during the training process, so that the number of such hidden functions increases with the complexity of the mapping rather than with the dimensionality of the input space <ref> [6] </ref>. As a consequence, the number of free parameters in non-linear NN models typically increases linearly, or quadratically, with the number of input variables n i [6]. To provide efficient scaling between model complexity and dimensionality of the input space, non-linear NN models exploit two common properties of real data. <p> so that the number of such hidden functions increases with the complexity of the mapping rather than with the dimensionality of the input space <ref> [6] </ref>. As a consequence, the number of free parameters in non-linear NN models typically increases linearly, or quadratically, with the number of input variables n i [6]. To provide efficient scaling between model complexity and dimensionality of the input space, non-linear NN models exploit two common properties of real data. <p> Although MLPs and RBF networks play similar roles, their architectural properties as well as their supervised learning techniques are quite different. In MLPs <ref> [6] </ref>: 1. The feed-forward network architecture consists of up to three layers of processing units, termed input, hidden and output layer respectively. Hidden layers are all the layers between the input and output layer. 10 2. <p> All parameters of an MLP network are adjusted according to a single-stage global training strategy, termed error back-propagation learning algorithm, which is based on the supervised training set [16]. 1 In RBF networks <ref> [6] </ref>, [17], [18]: 1. The feed-forward network architecture is made up of only two layers. The first hidden layer consists of C processing units while the second output layer linearly combines the activations provided by the hidden layer to form the outputs. 2. <p> An RBF whose output tends to zero when distance d (x; v j ) goes to infinity is termed localized RBF <ref> [6] </ref>. <p> a smooth interpolation function in which the number of RBFs is determined by the complexity of the mapping to be approximated rather than by the size of the training data set, RBF networks employ a number C of basis functions that must be less than number M of training samples <ref> [6] </ref>, [17]. This is tantamount to stating that C is considered a regularization parameter [1]. <p> These critical training examples are termed support vectors. (c) Regularization theory, in which a mapping function is determined by minimizing a cost function designed to penalize mappings that are not smooth <ref> [6] </ref>, [17]. (d) Noise interpolation theory, in which the observable output data is generated by a target function, which is smooth and noise-free, while the input data is corrupted by additive Gaussian noise [6], [25]; (e) Kernel regression, in which a noise-affected function is estimated by means of the Gaussian kernel-based <p> mapping function is determined by minimizing a cost function designed to penalize mappings that are not smooth <ref> [6] </ref>, [17]. (d) Noise interpolation theory, in which the observable output data is generated by a target function, which is smooth and noise-free, while the input data is corrupted by additive Gaussian noise [6], [25]; (e) Kernel regression, in which a noise-affected function is estimated by means of the Gaussian kernel-based method for density estimation [6]. (f) Distance-weighted regression methods, also termed instance-based [4], or memory-based learning methods [11] (see Section 2.5). <p> theory, in which the observable output data is generated by a target function, which is smooth and noise-free, while the input data is corrupted by additive Gaussian noise <ref> [6] </ref>, [25]; (e) Kernel regression, in which a noise-affected function is estimated by means of the Gaussian kernel-based method for density estimation [6]. (f) Distance-weighted regression methods, also termed instance-based [4], or memory-based learning methods [11] (see Section 2.5). <p> construct approximations that perform well over the entire input space can be considered an interesting bridge between instance-based and neural network learning algorithms [4]. (g) Bayes optimal classifiers, where the density of the input data is expressed in terms of a mixture distribution whose components are the basis functions [4], <ref> [6] </ref>. 4 Learning techniques for RBF networks The traditional view of RBF networks where the centers of RBFs are regarded as templates has led to the development of hybrid learning schemes whose first stage performs unsupervised clustering. <p> The EM algorithm tries to maximize the likelihood of input vectors, p (x), represented as a mixture distribution of basis functions p (j), j 2 f1; Cg, according to the expansion rule p (x) = P C j p (xjj)p (j), where the number of basis functions C is user-defined <ref> [6] </ref>, [26]. Other examples of unsupervised methods are the clustering algorithms based on histogram analysis where, from the sequence of input patterns, prototype vectors are extracted as statistical regularities capable of minimizing a requantization error (e.g., Hard c-means [27], Fuzzy c-means [28], LBG-U [29]). <p> When the cost function is the sum-of-squares error, output weights can be determined by using a pseudo-inverse solution of a linear problem, which is therefore also fast <ref> [6] </ref>. Otherwise, a slower gradient descent approach can be employed. <p> the distribution of RBFs in the input space as computed by the unsupervised technique does not reflect the local complexity of the classification or regression problem at hand; e.g., unsupervised methods may form clusters of input vectors that are closely spaced in the input space but belong to different classes <ref> [6] </ref>, [23], [30], [31]. <p> On the other hand, our result is consistent with the heuristic criterion employed by Bishop, where the spread parameter of hidden units in RBF networks is roughly equal to twice the average spacing between the centres of unequally spaced GRBFs <ref> [6] </ref>. <p> Since size M of the supervised training set must satisfy condition M C (see Section 3.2), then output weights can be computed by considering Eq. (35) as a linear model (see Section 3.1) which can be solved by pseudo-inverse techniques <ref> [6] </ref>, [7], or algorithms which are numerically more stable, such 20 as the Singular Value Decomposition [38]. A better scheme, one that allows elimination of outliers, has recently been proposed [39]. However, these techniques are computationally demanding and may cause numerical and memory allocation problems for large networks.
Reference: [7] <author> L. Xu, A. Krzyzak and A. Yuille, </author> <title> "On radial basis function nets and kernel regression: Statistical consistency, convergence rates, and receptive field size," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 4, </volume> <pages> pp. 609-628, </pages> <year> 1994. </year>
Reference-contexts: Conversely, in Eq. (6) the variance term measures the extent to which the approximating function is sensitive to the particular choice of the data set [6], <ref> [7] </ref>. For example, an approximating function that is closely fitted to training data will tend to have a large variance. <p> 0:2743-G =2 = 0:1371-G ; (38a) 0:2743 &lt; cutoff 0:2743 = max= 0:6831 Eq. (38) can be written as 2 0:6831 = -G 0:1874 : (39) Eq. (39), where &gt; 1:36624, is more restrictive than the empirical criterion = 4, which is typically employed in Parzen window-based density function estimators <ref> [7] </ref>, [37]. The difference is significant in terms of amount of overlap (oversampling [12]) between two 19 consecutive Gaussians, which increases from 68.2% to 73.3%. <p> Since size M of the supervised training set must satisfy condition M C (see Section 3.2), then output weights can be computed by considering Eq. (35) as a linear model (see Section 3.1) which can be solved by pseudo-inverse techniques [6], <ref> [7] </ref>, or algorithms which are numerically more stable, such 20 as the Singular Value Decomposition [38]. A better scheme, one that allows elimination of outliers, has recently been proposed [39]. However, these techniques are computationally demanding and may cause numerical and memory allocation problems for large networks.
Reference: [8] <author> S. Geman and D. Geman, </author> <title> "Stochastic relaxation, Gibbs distribution, and the Bayesian restoration of images," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. PAMI-6, no. 6, </volume> <pages> pp. 721-741, </pages> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: For example, it has been shown that in the limit of an infinite data set, a cost function computed as the sum-of-squared differences between target and estimated outputs is proportional to the sum of a bias term with a variance term [6], <ref> [8] </ref>, i.e., R act (w) / [bias (w)] 2 + variance (w): (6) In Eq. (6) the bias term measures the extent to which the average (over an infinite data set) of the approximating function differs from the target function. <p> Thus, minimization of Eq. (6) provides a natural trade-off between minimizing both bias and variance: by smoothing the approximating function we can decrease the variance, but if the smoothing is taken too far then the bias becomes large. This quantitative principle is called the bias-variance trade-off <ref> [8] </ref>. 4 Additional quantitative properties of predictive learning systems have been proposed by statistical learning theory, also known as the Vapnik-Chervonenkis (VC) theory, in the last 25 years [9].
Reference: [9] <author> V. Vapnik, </author> <title> The Nature of Statistical Learning Theory, </title> <publisher> Springer-Verlag: </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: This quantitative principle is called the bias-variance trade-off [8]. 4 Additional quantitative properties of predictive learning systems have been proposed by statistical learning theory, also known as the Vapnik-Chervonenkis (VC) theory, in the last 25 years <ref> [9] </ref>. For example, in the case of two-class (binary) pattern recognition problems, the bound R act (w) R emp (w) + h holds if parameter h is finite. <p> Vice versa, when M is small, Eq. (7) shows that to minimize the actual risk the combination of the empirical risk with the confidence term ought to be minimized, i.e., the VC-dimension h should also be minimized <ref> [9] </ref>, [10]. For function regression problems VC theory provides the following bound, which is useful for model selection [1] h 0:8 for min 4 M where (1 ) 2 [0; 1] is called the confidence level.
Reference: [10] <author> C. Burges, </author> <title> "A tutorial on Support Vector Machines for pattern recognition," submitted to Data Mining and Knowledge Discovery, </title> <year> 1998. </year>
Reference-contexts: This parameter, known as the VC dimension, is a characteristic of the set of approximating functions supported by the learning system and is a measure of model complexity [1], <ref> [10] </ref>. In Eq. (7), function [ h M ], called the VC confidence term or confidence interval, decreases monotonically with size M of the training set and monotonically increases with h. <p> Vice versa, when M is small, Eq. (7) shows that to minimize the actual risk the combination of the empirical risk with the confidence term ought to be minimized, i.e., the VC-dimension h should also be minimized [9], <ref> [10] </ref>. For function regression problems VC theory provides the following bound, which is useful for model selection [1] h 0:8 for min 4 M where (1 ) 2 [0; 1] is called the confidence level.
Reference: [11] <author> C. G. Atkeson, S. A. Schall and A. W. Moore, </author> <title> "Locally weighted learning," </title> <journal> AI Review, </journal> <volume> vol. 11, </volume> <pages> pp. 11-73, </pages> <year> 1997. </year>
Reference-contexts: Takes on its maximum when a = b: (12c) lim t!1 k (t) = 0: Localized function [6]. (12d) Kernel methods are examples of instance- or memory-based learning strategies because they are based on storing all training data [4], <ref> [11] </ref>. The basic idea behind kernel representation is that when a new query instance is presented, the target value is estimated only on the basis of training examples near the query point, as training examples are individually weighted by their distance from the query point. <p> If data are noisy, exact interpolation is not desirable, and Eq. (14) should employ a weighting scheme based on finite values of the kernel function to guarantee smooth interpolation between training points <ref> [11] </ref>. <p> which is smooth and noise-free, while the input data is corrupted by additive Gaussian noise [6], [25]; (e) Kernel regression, in which a noise-affected function is estimated by means of the Gaussian kernel-based method for density estimation [6]. (f) Distance-weighted regression methods, also termed instance-based [4], or memory-based learning methods <ref> [11] </ref> (see Section 2.5). Instance-based methods construct only a local approximation to the target function that applies in the neighborhood of the new query instance, and never construct an approximation designed to perform well over the entire input space [4]. <p> For example, a common choice in digital filtering theory is d 1 = 2=2, corresponding to a maximum attenuation in the Pass Band equal to 3 db <ref> [11] </ref>. Definition 2. <p> However, the hypothesis of equally spaced samples is rarely satisfied in real problems. When a number M C of either equally- or unequally-spaced samples is available, then values of the target function at Gaussian centers can be estimated, for example, by means of a locally distance-weighted regression model <ref> [11] </ref>.
Reference: [12] <author> M. Porat and Y. Zeevi, </author> <title> "The generalized gabor scheme of image representation in biological and machine vision," </title> <journal> IEEE Trans. Pattern Anal. Machine Intell., </journal> <volume> vol. 10, no. 4, </volume> <pages> pp. 452-467, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: When these conditions are satisfied the signal-specific weighting coefficients w j , j = 1; :::; C, define uniquely signal y (x)) <ref> [12] </ref>, [13]. <p> The difference is significant in terms of amount of overlap (oversampling <ref> [12] </ref>) between two 19 consecutive Gaussians, which increases from 68.2% to 73.3%.
Reference: [13] <author> P. Burt and E. Adelson, </author> <title> "The Laplacian pyramid as a compact image code," </title> <journal> IEEE Trans. on Communications, </journal> <volume> vol. COM-31, no. 4, </volume> <pages> pp. 532-540, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: When these conditions are satisfied the signal-specific weighting coefficients w j , j = 1; :::; C, define uniquely signal y (x)) [12], <ref> [13] </ref>.
Reference: [14] <author> J. B. Martens, </author> <title> "The Hermite transform Applications," </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. ASSP-38, </volume> <pages> pp. 1607-1618, </pages> <year> 1990. </year>
Reference-contexts: cannot be employed directly but must be estimated from the training set of equally-spaced (input, output) examples, i.e., w i = M h=1 Examples of an orthonormal basis include Fourier series, where a stationary signal is synthesized (reconstructed) as a combination of sinusoidal waveforms, Hermite polynomials employed in polynomial transforms <ref> [14] </ref>, and wavelets which are employed, for example, in 1-D and 2-D signal compression [15]. 3 Introduction to neural networks Artificial Neural Networks (NNs) are parametrized and distributed systems capable of learning from data.
Reference: [15] <author> G. Strang and T. Nguyen, </author> <title> Wavelets and Filter Banks, </title> <publisher> Wellesley-Cambridge Press: </publisher> <address> Wellesley (MA), </address> <year> 1997. </year>
Reference-contexts: (input, output) examples, i.e., w i = M h=1 Examples of an orthonormal basis include Fourier series, where a stationary signal is synthesized (reconstructed) as a combination of sinusoidal waveforms, Hermite polynomials employed in polynomial transforms [14], and wavelets which are employed, for example, in 1-D and 2-D signal compression <ref> [15] </ref>. 3 Introduction to neural networks Artificial Neural Networks (NNs) are parametrized and distributed systems capable of learning from data. In this context the term distributed means that NNs consist of a multeplicity of simple and mutually interconnected processing elements.
Reference: [16] <author> D. E. Rumelhart, G. E. Hinton and R. J. Williams, </author> <title> "Learning internal representations by error propagation," in Parallel Distributed Processing, </title> <editor> D. E. Rumelhart and J. L. McClelland, Eds., </editor> <volume> vol. 1, ch. 8, </volume> <pages> pp. 318-362, </pages> <publisher> MIT Press: </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: A specific relationship between sample size and model complexity is shown in Eq. (8). A heuristic rule between sample size and model complexity requires sample size to be at least 10 times the number of free parameters in the model <ref> [16] </ref>. 2.5 Classification of methods for regression One possible way to classify methods for estimating continuous-valued functions from noisy samples is to develop a taxonomy based on dictionary representation versus kernel representation [1]. <p> All parameters of an MLP network are adjusted according to a single-stage global training strategy, termed error back-propagation learning algorithm, which is based on the supervised training set <ref> [16] </ref>. 1 In RBF networks [6], [17], [18]: 1. The feed-forward network architecture is made up of only two layers. The first hidden layer consists of C processing units while the second output layer linearly combines the activations provided by the hidden layer to form the outputs. 2. <p> Thus, GRBFs 1 This optimization algorithm is called back-propagation because the output error is "back-propagated" through the network all the way down to the free parameters of the input layer <ref> [16] </ref>. It is their simple and easily implementable learning strategy that makes conventional MLPs the most popular NN model [34]. 11 are equivalent to small patches (Gaussian data windows) covering the input space.
Reference: [17] <author> F. Girosi, M. Jones and T. Poggio, </author> <title> "Regularization theory and neural network architectures," </title> <journal> Neural Computation, </journal> <volume> vol. 7, </volume> <pages> pp. 219-269, </pages> <year> 1995. </year>
Reference-contexts: All parameters of an MLP network are adjusted according to a single-stage global training strategy, termed error back-propagation learning algorithm, which is based on the supervised training set [16]. 1 In RBF networks [6], <ref> [17] </ref>, [18]: 1. The feed-forward network architecture is made up of only two layers. The first hidden layer consists of C processing units while the second output layer linearly combines the activations provided by the hidden layer to form the outputs. 2. <p> smooth interpolation function in which the number of RBFs is determined by the complexity of the mapping to be approximated rather than by the size of the training data set, RBF networks employ a number C of basis functions that must be less than number M of training samples [6], <ref> [17] </ref>. This is tantamount to stating that C is considered a regularization parameter [1]. <p> These critical training examples are termed support vectors. (c) Regularization theory, in which a mapping function is determined by minimizing a cost function designed to penalize mappings that are not smooth [6], <ref> [17] </ref>. (d) Noise interpolation theory, in which the observable output data is generated by a target function, which is smooth and noise-free, while the input data is corrupted by additive Gaussian noise [6], [25]; (e) Kernel regression, in which a noise-affected function is estimated by means of the Gaussian kernel-based method
Reference: [18] <author> J. Moody and C. Darken, </author> <title> "Fast learning in networks of locally-tuned processing units," </title> <journal> Neural Computation, </journal> <volume> vol. 1, </volume> <pages> pp. 281-294, </pages> <year> 1989. </year>
Reference-contexts: All parameters of an MLP network are adjusted according to a single-stage global training strategy, termed error back-propagation learning algorithm, which is based on the supervised training set [16]. 1 In RBF networks [6], [17], <ref> [18] </ref>: 1. The feed-forward network architecture is made up of only two layers. The first hidden layer consists of C processing units while the second output layer linearly combines the activations provided by the hidden layer to form the outputs. 2. <p> devised to determine the network weights, it has been proved that when this technique is applied, for example, to GRBF networks, it is unsuccessful because [34]: i) it does not ensure that GRBFs will remain localized, i.e., spread parameters may become very large and GRBF responses may become very broad <ref> [18] </ref>; and ii) it has practically no effect on the positions (centers) of GRBFs. To overcome these limitations Karayiannis has introduced new types of localized RBFs suitable for gradient descent learning [34]. <p> Structural Parameters Synaptic strategy no. of RBFs fg fSpreadg weights [41] 1 , [42] 1 EGI and ER D D EGD 8 one-stage [24] ES 3 ES 3 F E [35] ES 4 ES 4 F 4 E two-stage [31] EGI 5 EGD 7 EGD 7 EPI or EGD 6 <ref> [18] </ref> F D H EPI D: (Input) Data-driven; E: Error-driven; ER: Error-driven Removal of localized RBFs; ES: Error-driven Selection of localized RBF; EGD: Error Gradient Descent; EGI: Error-driven Generation/Insertion of localized RBFs; EPI: Error-driven Pseudo-Inverse solution of a linear problem; F: Fixed (user-defined); H: Heuristic criterion; 1 Fritzke's algorithm is termed
Reference: [19] <author> T. Poggio, V. Torre and C. Koch, </author> <title> "Computational vision and regularization theory," </title> <journal> Nature, </journal> <volume> vol. 317, </volume> <pages> pp. 314-319, </pages> <year> 1985. </year>
Reference-contexts: Among localized RBF types, GRBFs are preferred for two main reasons: a) they have a number of useful analytical properties, e.g., a Gaussian function is factorizable; as a consequence, their implementation in parallel hardware is straightforward, which makes GRBFs particularly attractive for real-time network implementations <ref> [19] </ref>; and b) they are claimed to constitute a processing module common in the human nervous system [20].
Reference: [20] <author> T. Poggio, </author> <title> "A theory of how the brain might work," </title> <booktitle> Cold Spring Harbor Symposium on Quantitative Biology, </booktitle> <pages> pp. 899-910, </pages> <year> 1990. </year>
Reference-contexts: they have a number of useful analytical properties, e.g., a Gaussian function is factorizable; as a consequence, their implementation in parallel hardware is straightforward, which makes GRBFs particularly attractive for real-time network implementations [19]; and b) they are claimed to constitute a processing module common in the human nervous system <ref> [20] </ref>.
Reference: [21] <author> J. Park and I. Sandberg, </author> <title> "Universal approximation using radial-basis-function networks," </title> <journal> Neural Computation, </journal> <volume> vol. 3, no. 2, </volume> <pages> p. 246-257, </pages> <year> 1991. </year> <month> 29 </month>
Reference-contexts: Both MLPs and RBF networks have been proved to work as universal approximators, i.e., they are capable of approximating any arbitrary mapping between multidimensional spaces to any degree of precision if they are provided with a sufficient number C of processing units <ref> [21] </ref>.
Reference: [22] <author> J. Jang and C. Sun, </author> <title> "Functional equivalence between radial basis function networks and fuzzy inference systems," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 156-159, </pages> <year> 1993. </year>
Reference-contexts: approximation tech niques An important property of RBF networks is that they form a unifying link between a number of different learning systems including: 12 (a) Zero-order Sugeno fuzzy systems (where the ouput of each fuzzy rule is a constant), which are equivalent to RBF networks under certain mild restrictions <ref> [22] </ref>, [23]. (b) Support Vector Machines (SVMs), whose training always finds a global minimum of a cost function for a classification task. An SVM is largely characterized by the choice of a kernel function (i.e., a weighting function).
Reference: [23] <author> B. Fritzke, </author> <title> "Incremental neuro-fuzzy systems," </title> <booktitle> Proc. SPIE's Optical Science, Engineering and Instrumentation '97: Applications of Fuzzy Logic Technology IV, </booktitle> <address> San Diego, </address> <month> July </month> <year> 1997. </year>
Reference-contexts: tech niques An important property of RBF networks is that they form a unifying link between a number of different learning systems including: 12 (a) Zero-order Sugeno fuzzy systems (where the ouput of each fuzzy rule is a constant), which are equivalent to RBF networks under certain mild restrictions [22], <ref> [23] </ref>. (b) Support Vector Machines (SVMs), whose training always finds a global minimum of a cost function for a classification task. An SVM is largely characterized by the choice of a kernel function (i.e., a weighting function). <p> distribution of RBFs in the input space as computed by the unsupervised technique does not reflect the local complexity of the classification or regression problem at hand; e.g., unsupervised methods may form clusters of input vectors that are closely spaced in the input space but belong to different classes [6], <ref> [23] </ref>, [30], [31]. <p> According to their insertion criterion, growing RBF networks can be divided into scatter-partitioning RBF networks <ref> [23] </ref>, [31], where centers and spread parameters of RBFs are adjustable, and grid-partitioning RBF networks, where the input space is partitioned into a regular grid of identical patches whose size is known a priori, i.e., center and spread parameter values of the hidden layer are neither error- nor data-driven [35].
Reference: [24] <author> B. Scholkopf, K. Sung, C. Burges, F. Girosi, P. Niyogi, T. Poggio and V. Vapnik, </author> <title> "Comparing Support Vector Machines with Gaussian kernels to radial basis function classifiers," </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> vol. 45, no. 11, </volume> <pages> pp. 2758-2765, </pages> <year> 1997. </year>
Reference-contexts: When this kernel function is a Gaussian, then SVM is called SVM RBFs, where the number of RBFs, their centers and their interpolation coefficients are all determined automatically by the SVM training and have a simple geometric interpretation <ref> [24] </ref>. The traditional view of RBF networks has been one in which the centers of RBFs are regarded as templates or prototypes. In line with this view it is reasonable to exploit a clustering heuristic to train the first layer of RBF networks. <p> In real implementations of a low-pass filter the transition between the two bands 16 Table 1: Learning Algorithm Structural Parameters Synaptic strategy no. of RBFs fg fSpreadg weights [41] 1 , [42] 1 EGI and ER D D EGD 8 one-stage <ref> [24] </ref> ES 3 ES 3 F E [35] ES 4 ES 4 F 4 E two-stage [31] EGI 5 EGD 7 EGD 7 EPI or EGD 6 [18] F D H EPI D: (Input) Data-driven; E: Error-driven; ER: Error-driven Removal of localized RBFs; ES: Error-driven Selection of localized RBF; EGD: Error
Reference: [25] <author> M. Cannon and J. E. Slotine, </author> <title> "Space-frequency localized basis function networks for nonlinear system estimation and control," </title> <journal> Neurocomputing, </journal> <volume> vol. 9, </volume> <pages> pp. 293-342, </pages> <year> 1995. </year>
Reference-contexts: function is determined by minimizing a cost function designed to penalize mappings that are not smooth [6], [17]. (d) Noise interpolation theory, in which the observable output data is generated by a target function, which is smooth and noise-free, while the input data is corrupted by additive Gaussian noise [6], <ref> [25] </ref>; (e) Kernel regression, in which a noise-affected function is estimated by means of the Gaussian kernel-based method for density estimation [6]. (f) Distance-weighted regression methods, also termed instance-based [4], or memory-based learning methods [11] (see Section 2.5). <p> However, they offer the advantage of employing the input space gridding mechanism to reduce the number of free parameters in the network <ref> [25] </ref>. This advantage is fully exploited in the Hierarchical Radial Basis Functions (HRBF) network model where an error-driven mechanism identifies useful RBFs at the crossings of a hierarchy of grid partitions of the input space [35].
Reference: [26] <author> A. P. Dempster, N. M. Laird and D. B. Rubin, </author> <title> "Maximum likelihood from incomplete data via the EM algorithm," </title> <journal> J. Royal Statist. Soc. Ser. B, </journal> <volume> vol. 39, </volume> <pages> pp. 1-38, </pages> <year> 1977. </year>
Reference-contexts: EM algorithm tries to maximize the likelihood of input vectors, p (x), represented as a mixture distribution of basis functions p (j), j 2 f1; Cg, according to the expansion rule p (x) = P C j p (xjj)p (j), where the number of basis functions C is user-defined [6], <ref> [26] </ref>. Other examples of unsupervised methods are the clustering algorithms based on histogram analysis where, from the sequence of input patterns, prototype vectors are extracted as statistical regularities capable of minimizing a requantization error (e.g., Hard c-means [27], Fuzzy c-means [28], LBG-U [29]).
Reference: [27] <author> A. K. Jain and R. C. Dubes, </author> <title> Algorithms for clustering data, </title> <publisher> Prentice Hall: </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1988. </year>
Reference-contexts: Other examples of unsupervised methods are the clustering algorithms based on histogram analysis where, from the sequence of input patterns, prototype vectors are extracted as statistical regularities capable of minimizing a requantization error (e.g., Hard c-means <ref> [27] </ref>, Fuzzy c-means [28], LBG-U [29]). Once parameters of the hidden layer have been estimated by the first stage, the second training stage of the hybrid learning procedure determines the values of the output weights by minimizing a cost functional on the basis of the supervised training set.
Reference: [28] <author> J. C. Bezdek and N. R. Pal, </author> <title> "Two soft relatives of learning vector quantization," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 8, no. 5, </volume> <pages> pp. 729-743, </pages> <year> 1995. </year>
Reference-contexts: Other examples of unsupervised methods are the clustering algorithms based on histogram analysis where, from the sequence of input patterns, prototype vectors are extracted as statistical regularities capable of minimizing a requantization error (e.g., Hard c-means [27], Fuzzy c-means <ref> [28] </ref>, LBG-U [29]). Once parameters of the hidden layer have been estimated by the first stage, the second training stage of the hybrid learning procedure determines the values of the output weights by minimizing a cost functional on the basis of the supervised training set.
Reference: [29] <author> B. Fritzke, </author> <title> "The LBG-U method for vector quantization an improvement over LBG inspired from neural networks," </title> <journal> Neural Processing Letters, </journal> <volume> vol. 5, no. 1, </volume> <pages> pp. 83-111, </pages> <year> 1997. </year>
Reference-contexts: Other examples of unsupervised methods are the clustering algorithms based on histogram analysis where, from the sequence of input patterns, prototype vectors are extracted as statistical regularities capable of minimizing a requantization error (e.g., Hard c-means [27], Fuzzy c-means [28], LBG-U <ref> [29] </ref>). Once parameters of the hidden layer have been estimated by the first stage, the second training stage of the hybrid learning procedure determines the values of the output weights by minimizing a cost functional on the basis of the supervised training set.
Reference: [30] <author> E. Alpaydn, </author> <title> "Soft vector quantization and the EM algorithm," Neural Networks, </title> <publisher> in press, </publisher> <year> 1998. </year>
Reference-contexts: of RBFs in the input space as computed by the unsupervised technique does not reflect the local complexity of the classification or regression problem at hand; e.g., unsupervised methods may form clusters of input vectors that are closely spaced in the input space but belong to different classes [6], [23], <ref> [30] </ref>, [31].
Reference: [31] <author> N. Karayiannis, </author> <title> "Growing radial basis neural networks: Merging supervised and unsupervised learning with network growth techniques," </title> <journal> IEEE Trans. on Neural Neworks, </journal> <volume> vol. 8, no. 6, </volume> <pages> pp. 1492-1506, </pages> <year> 1997. </year>
Reference-contexts: RBFs in the input space as computed by the unsupervised technique does not reflect the local complexity of the classification or regression problem at hand; e.g., unsupervised methods may form clusters of input vectors that are closely spaced in the input space but belong to different classes [6], [23], [30], <ref> [31] </ref>. <p> there is no guarantee of improving the system's performance on a test set, i.e, on a set of unobserved labeled examples, because the unsupervised algorithm may locate the additional RBFs in regions of the input space where they are either useless or harmful in implementing the desired (input, output) mapping <ref> [31] </ref>. 14 To avoid this problem, the density of RBFs must be made independent of input vector density but dependent on the complexity of the desired (input, output) mapping. Thus, several error-driven (supervised) learning algorithms for RBF networks, either one- or two-stage, have been proposed in recent years. <p> According to their insertion criterion, growing RBF networks can be divided into scatter-partitioning RBF networks [23], <ref> [31] </ref>, where centers and spread parameters of RBFs are adjustable, and grid-partitioning RBF networks, where the input space is partitioned into a regular grid of identical patches whose size is known a priori, i.e., center and spread parameter values of the hidden layer are neither error- nor data-driven [35]. <p> the transition between the two bands 16 Table 1: Learning Algorithm Structural Parameters Synaptic strategy no. of RBFs fg fSpreadg weights [41] 1 , [42] 1 EGI and ER D D EGD 8 one-stage [24] ES 3 ES 3 F E [35] ES 4 ES 4 F 4 E two-stage <ref> [31] </ref> EGI 5 EGD 7 EGD 7 EPI or EGD 6 [18] F D H EPI D: (Input) Data-driven; E: Error-driven; ER: Error-driven Removal of localized RBFs; ES: Error-driven Selection of localized RBF; EGD: Error Gradient Descent; EGI: Error-driven Generation/Insertion of localized RBFs; EPI: Error-driven Pseudo-Inverse solution of a linear problem;
Reference: [32] <author> S. Medasani and R. Krishnapuram, </author> <title> "Determination of the number of components in Gaussian mixtures using agglomerative clustering," </title> <booktitle> Proc. Int. Conference on Neural Networks, </booktitle> <address> Houston, </address> <year> 1997, </year> <pages> pp. 1412-1417. </pages>
Reference-contexts: These disadvantages imply that when the number of RBFs (i.e., the number of adjustable parameters) is increased either by the user or by a constructive clustering algorithm in practical applications <ref> [32] </ref>, [33], there is no guarantee of improving the system's performance on a test set, i.e, on a set of unobserved labeled examples, because the unsupervised algorithm may locate the additional RBFs in regions of the input space where they are either useless or harmful in implementing the desired (input, output)
Reference: [33] <author> C. Yiu-Ming and L. Xu, </author> <title> "Some further studies on detection the number of clusters," </title> <booktitle> Proc. Int. Conference on Neural Networks, </booktitle> <address> Houston, </address> <year> 1997, </year> <pages> pp. 1479-1483. </pages>
Reference-contexts: These disadvantages imply that when the number of RBFs (i.e., the number of adjustable parameters) is increased either by the user or by a constructive clustering algorithm in practical applications [32], <ref> [33] </ref>, there is no guarantee of improving the system's performance on a test set, i.e, on a set of unobserved labeled examples, because the unsupervised algorithm may locate the additional RBFs in regions of the input space where they are either useless or harmful in implementing the desired (input, output) mapping
Reference: [34] <author> N. Karayiannis, </author> <title> "Reformulated radial basis neural networks trained by gradient descent," </title> <journal> IEEE Trans. on Neural Networks, </journal> <note> under review, </note> <year> 1998. </year>
Reference-contexts: Thus, GRBFs 1 This optimization algorithm is called back-propagation because the output error is "back-propagated" through the network all the way down to the free parameters of the input layer [16]. It is their simple and easily implementable learning strategy that makes conventional MLPs the most popular NN model <ref> [34] </ref>. 11 are equivalent to small patches (Gaussian data windows) covering the input space. <p> While for MLPs a simple optimization algorithm based on gradient descent of a sum-of-squares error has been devised to determine the network weights, it has been proved that when this technique is applied, for example, to GRBF networks, it is unsuccessful because <ref> [34] </ref>: i) it does not ensure that GRBFs will remain localized, i.e., spread parameters may become very large and GRBF responses may become very broad [18]; and ii) it has practically no effect on the positions (centers) of GRBFs. <p> To overcome these limitations Karayiannis has introduced new types of localized RBFs suitable for gradient descent learning <ref> [34] </ref>. Drawbacks of this supervised learning approach are that it does not guarantee convergence to the absolute minimum of the cost function and that it does not select model complexity (i.e., number C of hidden units must be user-defined). <p> This observation suggests the idea that to make RBF networks more popular, these networks must be provided with a single-stage simple and easily implementable error-driven learning algorithm analogous to the error back-propagation algorithm for MLPs <ref> [34] </ref>. 5 Introduction to Hierarchical Radial Basis Function net works In this section a function estimator called Hierarchical Radial Basis Function (HRBF) network is described. HRBF can be employed to approximate mappings between multidimensional spaces.
Reference: [35] <author> N. A. Borghese and S. Ferrari, </author> <title> "Hierarchical RBF networks and local parameters eti-mate," </title> <journal> Neurocomputing, </journal> <volume> vol. 19, </volume> <publisher> in press, </publisher> <year> 1998. </year>
Reference-contexts: networks [23], [31], where centers and spread parameters of RBFs are adjustable, and grid-partitioning RBF networks, where the input space is partitioned into a regular grid of identical patches whose size is known a priori, i.e., center and spread parameter values of the hidden layer are neither error- nor data-driven <ref> [35] </ref>. Unlike constructive grid-partitioning RBF networks, traditional (non-growing) grid-partitioning schemes, where all structural parameters are set on the basis of a rigid gridding of the input space, tend to be inefficient and not robust. <p> This advantage is fully exploited in the Hierarchical Radial Basis Functions (HRBF) network model where an error-driven mechanism identifies useful RBFs at the crossings of a hierarchy of grid partitions of the input space <ref> [35] </ref>. In other words, RBFs belonging to higher grids are dynamically added to the basic (first) layer of the HRBF network only when required by an error-driven local convergence criterion. <p> In real implementations of a low-pass filter the transition between the two bands 16 Table 1: Learning Algorithm Structural Parameters Synaptic strategy no. of RBFs fg fSpreadg weights [41] 1 , [42] 1 EGI and ER D D EGD 8 one-stage [24] ES 3 ES 3 F E <ref> [35] </ref> ES 4 ES 4 F 4 E two-stage [31] EGI 5 EGD 7 EGD 7 EPI or EGD 6 [18] F D H EPI D: (Input) Data-driven; E: Error-driven; ER: Error-driven Removal of localized RBFs; ES: Error-driven Selection of localized RBF; EGD: Error Gradient Descent; EGI: Error-driven Generation/Insertion of localized <p> Let us identify with 4 the distance between two consecutive centers of equally-spaced Gaussians and with -G = 1=4 their spatial frequency. In the discrete case of a finite number of equally-spaced Gaussians, Eq. (26) becomes <ref> [35] </ref> ^y (x) = 2 j=1 2 2 : (35) To provide a reasonable approximation of target function y (x), Eq. (35) must employ a uniform sampling interval 4 and a spread parameter which are constrained as follows. <p> To avoid overlapping of the spectrum components, the condition max &lt; -G =2; (37) must hold true <ref> [35] </ref>. <p> when the training set consists of M C examples regardless of whether they are equally- or unequally-spaced; and ii) it is capable of filtering out noisy points (outliers) since coefficients ^y fl ( j ), j = 1; :::; C; are estimated as a weighted average of the training examples <ref> [35] </ref>. From the computational standpoint, the HRBF learning procedure requires carrying out as many local estimates as the number of GRBFs. Therefore, the computational complexity of the model increases with complexity C of the mapping rather than with size M of the training set. <p> This is usually a two-step procedure: in the first step a very large set of data points (10,000 ffi 100,000) is sampled over the face, and in the second step a mathematical model is fitted to these points <ref> [35] </ref>. Owing to its computational efficiency and capability in performing smooth surface reconstruction, the HRBF network is considered suitable for use as the second stage of a 3-D human face reconstruction procedure.
Reference: [36] <author> G. C. Holst, </author> <title> Sampling, Aliasing, and Data Fidelity, </title> <publisher> SPIE Press: </publisher> <address> Bellingham (WA), </address> <year> 1998. </year>
Reference-contexts: y (x) (i.e., a signal featuring finite MAX) is sampled at a sampling interval 4x &lt; 1=2-MAX , then the signal can be (perfectly) reconstructed by 18 using an ideal low-pass filter (see Section 5.1.1), whose response in the input domain is a sinc function, according to the following equation <ref> [36] </ref> y (x) = j=1 sin 4x 4x +1 X y j sinc 4x ; (34) where (x j ; y j ) are the (input, output) observed examples such that x j = x 0 + j4x, j 2 f1; +1g, are equally-spaced sampling points, and kernel functions sinc (x
Reference: [37] <author> D. F. Specht, </author> <title> "Probabilistic neural networks," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 3, </volume> <pages> pp. 109-118, </pages> <year> 1990. </year>
Reference-contexts: =2 = 0:1371-G ; (38a) 0:2743 &lt; cutoff 0:2743 = max= 0:6831 Eq. (38) can be written as 2 0:6831 = -G 0:1874 : (39) Eq. (39), where &gt; 1:36624, is more restrictive than the empirical criterion = 4, which is typically employed in Parzen window-based density function estimators [7], <ref> [37] </ref>. The difference is significant in terms of amount of overlap (oversampling [12]) between two 19 consecutive Gaussians, which increases from 68.2% to 73.3%.
Reference: [38] <author> W. Press, W. Vetterling, S. Teukolsky and B. Flannery, </author> <title> Numerical Recipes in C, </title> <publisher> Cam-bridge University Press: </publisher> <address> Cambridge (MT), </address> <year> 1992. </year>
Reference-contexts: training set must satisfy condition M C (see Section 3.2), then output weights can be computed by considering Eq. (35) as a linear model (see Section 3.1) which can be solved by pseudo-inverse techniques [6], [7], or algorithms which are numerically more stable, such 20 as the Singular Value Decomposition <ref> [38] </ref>. A better scheme, one that allows elimination of outliers, has recently been proposed [39]. However, these techniques are computationally demanding and may cause numerical and memory allocation problems for large networks.
Reference: [39] <author> D. V. Sanchez, </author> <title> "Robustization of a learning method for RBF networks," </title> <journal> Neurocom-puting, </journal> <volume> vol. 9, </volume> <editor> p. </editor> <volume> 85, </volume> <year> 1995. </year>
Reference-contexts: A better scheme, one that allows elimination of outliers, has recently been proposed <ref> [39] </ref>. However, these techniques are computationally demanding and may cause numerical and memory allocation problems for large networks. Thus, to compute the output weights employed in Eq. (35) a specific strategy has been developed for the HRBF network.
Reference: [40] <author> B. Fritzke, </author> <title> "Growing-grid a self-organizing network with constant neighborhood range and adaptation strength," </title> <journal> Neural Processing Letters, </journal> <volume> vol. 2, no. 5, </volume> <pages> pp. 1-5, </pages> <year> 1995. </year>
Reference-contexts: Unlike traditional gridding procedures where no constructive mechanism is employed, the HRBF gridding procedure can be considered error-driven and one-stage. An approach similar to HRBF has been proposed by Fritzke using Kohonen maps <ref> [40] </ref>. The main difference is that Fritzke's system requires insertion of an entire row or column into the grid, while HRBF adds units selectively in those localized regions of the input space where the average local error criterion is not satisfied.
Reference: [41] <author> B. Fritzke, </author> <title> "Some competitive learning methods," Draft document, </title> <address> http://www.neuro-informatik.ruhr-uni-bochum.de/ini/VDM/ research/gsn/DemoGNG, </address> <year> 1998. </year>
Reference-contexts: According to this definition, frequency MAX separates the so-called Pass Band from the Stop Band of the filter. In real implementations of a low-pass filter the transition between the two bands 16 Table 1: Learning Algorithm Structural Parameters Synaptic strategy no. of RBFs fg fSpreadg weights <ref> [41] </ref> 1 , [42] 1 EGI and ER D D EGD 8 one-stage [24] ES 3 ES 3 F E [35] ES 4 ES 4 F 4 E two-stage [31] EGI 5 EGD 7 EGD 7 EPI or EGD 6 [18] F D H EPI D: (Input) Data-driven; E: Error-driven; ER: <p> Its hidden layer, termed Growing Neural Gas (GNG), is described in <ref> [41] </ref>; its output layer is described in [42]. 2 The type of RBFs is feasible for gradient descent learning. 3 Centers of selected RBFs correspond to support vectors belonging to the training data set. 4 Centers of selected RBFs correspond to grid-crossings of a hierarchy of regular grids featuring increasing sampling
Reference: [42] <author> B. Fritzke, </author> <title> "Growing cell structures A self-organizing network for unsupervised and supervised learning," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 9, </volume> <pages> pp. 1441-1460, </pages> <year> 1994. </year> <month> 31 </month>
Reference-contexts: According to this definition, frequency MAX separates the so-called Pass Band from the Stop Band of the filter. In real implementations of a low-pass filter the transition between the two bands 16 Table 1: Learning Algorithm Structural Parameters Synaptic strategy no. of RBFs fg fSpreadg weights [41] 1 , <ref> [42] </ref> 1 EGI and ER D D EGD 8 one-stage [24] ES 3 ES 3 F E [35] ES 4 ES 4 F 4 E two-stage [31] EGI 5 EGD 7 EGD 7 EPI or EGD 6 [18] F D H EPI D: (Input) Data-driven; E: Error-driven; ER: Error-driven Removal of <p> Its hidden layer, termed Growing Neural Gas (GNG), is described in [41]; its output layer is described in <ref> [42] </ref>. 2 The type of RBFs is feasible for gradient descent learning. 3 Centers of selected RBFs correspond to support vectors belonging to the training data set. 4 Centers of selected RBFs correspond to grid-crossings of a hierarchy of regular grids featuring increasing sampling rate.
References-found: 42

