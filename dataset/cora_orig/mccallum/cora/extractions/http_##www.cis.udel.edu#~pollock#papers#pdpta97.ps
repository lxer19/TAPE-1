URL: http://www.cis.udel.edu/~pollock/papers/pdpta97.ps
Refering-URL: http://www.cis.udel.edu/~jochen/passages/pubs.htm
Root-URL: http://www.cis.udel.edu
Email: ffenwick,pollockg@cis.udel.edu  
Phone: (302) 831-1953 (302) 831-8458 (Fax)  
Title: Optimizing the Use of Distributed Queues in Tuplespace  
Author: James B. Fenwick Jr. (presenting author) Lori L. Pollock 
Keyword: distributed tuplespace, generative communication, communication optimization, static analysis  
Address: 19716  
Affiliation: Department of Computer and Information Sciences University of Delaware, Newark, DE  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1986. </year>
Reference-contexts: This requires use-definition data flow information to identify all definitions of position that reach this use of position. All definitions of this use must come from a tuplespace operation of the same shared variable partition. However, the standard reaching definitions data flow <ref> [1] </ref> is insufficient when dealing with Linda programs. We have modified the reaching definitions data flow framework to accommodate the Linda operations [10].
Reference: [2] <author> Henri E. Bal. </author> <title> A comparative study of five parallel programming languages. </title> <booktitle> In Distributed Open Systems, </booktitle> <pages> pages 134-151. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: One of the most important aspects of tuplespace is the notion of distributed data structures <ref> [2] </ref>. However, the use of these distributed data structures raises old concerns [8] about the efficiency of tuplespace implementations on distributed memory architectures. Compile-time analysis can structure tuplespace to significantly reduce the time to find data in tuplespace [7] (i.e., tuplespace partitioning), and run-time strategies counteract some communication inefficiencies [3]. <p> and templates, which can be produced by processes on nodes throughout the machine, rendezvous at this one node. 3 Inefficiencies of Distributed Queues In a comparative study of several parallel programming languages, Bal observes that the concept of distributed data structures is an important contribution of the tuplespace communication model <ref> [2] </ref>. In their book on parallel programming, the developers of Linda utilize distributed queues, or streams, as a data structure central to the programming methods described [5]. It is our goal to automatically detect and improve the performance of this important way of using tuplespace.
Reference: [3] <author> Robert D. Bjornson. </author> <title> Linda on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: However, the use of these distributed data structures raises old concerns [8] about the efficiency of tuplespace implementations on distributed memory architectures. Compile-time analysis can structure tuplespace to significantly reduce the time to find data in tuplespace [7] (i.e., tuplespace partitioning), and run-time strategies counteract some communication inefficiencies <ref> [3] </ref>. While several researchers have demonstrated that a distributed tuplespace implementation can be efficient [9, 17, 6], there remains opportunity for improvement through compiler analysis that targets the underlying message passing [4, 12, 11, 16]. <p> Partitioning significantly reduces the time required to match templates to tuples. A successful strategy to distribute tuplespace uses a static hash function to map partitions to nodes in the parallel machine <ref> [3] </ref>.
Reference: [4] <author> Nicholas Carriero and David Gelernter. </author> <title> A foundation for advanced compile-time analysis of linda programs. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 389-404. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: While several researchers have demonstrated that a distributed tuplespace implementation can be efficient [9, 17, 6], there remains opportunity for improvement through compiler analysis that targets the underlying message passing <ref> [4, 12, 11, 16] </ref>. This paper provides concrete steps towards the advanced compile-time analysis and optimization of the use of distributed data structures in the shared tuplespace. Specifically, we present global program analysis algorithms for automatically detecting the distributed queue tuplespace data structure at compile time.
Reference: [5] <author> Nicholas Carriero and David Gelernter. </author> <title> How to Write Parallel Programs, A First Course. </title> <publisher> The MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Specifically, we present global program analysis algorithms for automatically detecting the distributed queue tuplespace data structure at compile time. Distributed queues, or streams, are central to many of the methods for parallel programming with the generative communication paradigm <ref> [5] </ref>. However, because a single access to these distributed tuplespace data structures involves a number of tuplespace operations in several tuplespace partitions, not necessarily localized within the program text, their automatic identification is difficult. <p> inefficiencies of distributed queues on distributed memory machines, describes how these data structures can be automatically detected and transformed at compile time, and presents the results of our implementation and evaluation of these analyses and transformations. 2 Linda The best known implementation of the generative communication model is Linda 1 <ref> [15, 5] </ref>. Our work is based on Linda although it applies more generally to any generative communication model implementation. <p> In their book on parallel programming, the developers of Linda utilize distributed queues, or streams, as a data structure central to the programming methods described <ref> [5] </ref>. It is our goal to automatically detect and improve the performance of this important way of using tuplespace. This section describes the distributed queue data structure and an improvement to the default tuplespace handling of this data structure.
Reference: [6] <author> Nicholas Carriero and David Gelernter. </author> <title> Learning from our successes. </title> <editor> In Janusz S. Kowalik and Lucio Grandinetti, editors, </editor> <booktitle> Software for Parallel Computation, volume 106 of NATO ASI Series F: Computer and Systems Sciences, </booktitle> <pages> pages 37-45. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Compile-time analysis can structure tuplespace to significantly reduce the time to find data in tuplespace [7] (i.e., tuplespace partitioning), and run-time strategies counteract some communication inefficiencies [3]. While several researchers have demonstrated that a distributed tuplespace implementation can be efficient <ref> [9, 17, 6] </ref>, there remains opportunity for improvement through compiler analysis that targets the underlying message passing [4, 12, 11, 16]. This paper provides concrete steps towards the advanced compile-time analysis and optimization of the use of distributed data structures in the shared tuplespace.
Reference: [7] <author> Nicholas John Carriero, Jr. </author> <title> Implementation of Tuple Space Machines. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> December </month> <year> 1987. </year>
Reference-contexts: However, the use of these distributed data structures raises old concerns [8] about the efficiency of tuplespace implementations on distributed memory architectures. Compile-time analysis can structure tuplespace to significantly reduce the time to find data in tuplespace <ref> [7] </ref> (i.e., tuplespace partitioning), and run-time strategies counteract some communication inefficiencies [3]. While several researchers have demonstrated that a distributed tuplespace implementation can be efficient [9, 17, 6], there remains opportunity for improvement through compiler analysis that targets the underlying message passing [4, 12, 11, 16].
Reference: [8] <author> C. Davidson. </author> <title> Technical correspondence on linda in contex. </title> <journal> In Communications of the ACM, </journal> <volume> volume 3210, </volume> <pages> pages 1249-1252. </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: One of the most important aspects of tuplespace is the notion of distributed data structures [2]. However, the use of these distributed data structures raises old concerns <ref> [8] </ref> about the efficiency of tuplespace implementations on distributed memory architectures. Compile-time analysis can structure tuplespace to significantly reduce the time to find data in tuplespace [7] (i.e., tuplespace partitioning), and run-time strategies counteract some communication inefficiencies [3].
Reference: [9] <author> A. Deshpande and M. Schultz. </author> <title> Efficient parallel programming with linda. </title> <booktitle> In Supercomputing '92 Proceedings, </booktitle> <pages> pages 238-244, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Compile-time analysis can structure tuplespace to significantly reduce the time to find data in tuplespace [7] (i.e., tuplespace partitioning), and run-time strategies counteract some communication inefficiencies [3]. While several researchers have demonstrated that a distributed tuplespace implementation can be efficient <ref> [9, 17, 6] </ref>, there remains opportunity for improvement through compiler analysis that targets the underlying message passing [4, 12, 11, 16]. This paper provides concrete steps towards the advanced compile-time analysis and optimization of the use of distributed data structures in the shared tuplespace.
Reference: [10] <author> James B. Fenwick, Jr. and Lori L. Pollock. </author> <title> Data flow in the presence of linda operations. </title> <note> Technical Report in progress. </note>
Reference-contexts: All definitions of this use must come from a tuplespace operation of the same shared variable partition. However, the standard reaching definitions data flow [1] is insufficient when dealing with Linda programs. We have modified the reaching definitions data flow framework to accommodate the Linda operations <ref> [10] </ref>. Figure 2 presents the algorithm for this analysis to detect distributed queues in tuplespace. 4.3 Compiler Transformation After usage of a distributed queue has been detected, the compiler needs to transform the program to realize improved performance.
Reference: [11] <author> James B. Fenwick, Jr. and Lori L. Pollock. </author> <title> Global compiler analysis for optimizing tuplespace communication on distributed systems. </title> <booktitle> In Eighth IASTED International Conference on Parallel and Distributed Computing and Systems (PDCS), </booktitle> <address> Chicago, IL, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: While several researchers have demonstrated that a distributed tuplespace implementation can be efficient [9, 17, 6], there remains opportunity for improvement through compiler analysis that targets the underlying message passing <ref> [4, 12, 11, 16] </ref>. This paper provides concrete steps towards the advanced compile-time analysis and optimization of the use of distributed data structures in the shared tuplespace. Specifically, we present global program analysis algorithms for automatically detecting the distributed queue tuplespace data structure at compile time. <p> Recalling the distributed queue tuplespace operations in the user process of Figure 1, a shared variable tuple indicates the queue element to be requested. Thus, the identification of a distributed queue crucially depends on the identification of the shared variable tuple coordinating access to the queue elements. In <ref> [11] </ref>, we present static analysis to identify tuplespace operations acting as a shared variable tuple that further requires a tuple counting data flow analysis [12].
Reference: [12] <author> James B. Fenwick, Jr. and Lori L. Pollock. </author> <title> Identifying tuple usage patterns in an optimizing linda compiler. </title> <booktitle> In Proceedings of Mid-Atlantic Workshop on Programming Languages and Systems (MASPLAS '96), </booktitle> <year> 1996. </year>
Reference-contexts: While several researchers have demonstrated that a distributed tuplespace implementation can be efficient [9, 17, 6], there remains opportunity for improvement through compiler analysis that targets the underlying message passing <ref> [4, 12, 11, 16] </ref>. This paper provides concrete steps towards the advanced compile-time analysis and optimization of the use of distributed data structures in the shared tuplespace. Specifically, we present global program analysis algorithms for automatically detecting the distributed queue tuplespace data structure at compile time. <p> Thus, the identification of a distributed queue crucially depends on the identification of the shared variable tuple coordinating access to the queue elements. In [11], we present static analysis to identify tuplespace operations acting as a shared variable tuple that further requires a tuple counting data flow analysis <ref> [12] </ref>. Given shared variable tuple identification, the next step in finding a distributed queue is to relate the shared variable partition with the tuplespace partition responsible for the queue elements.
Reference: [13] <author> James B. Fenwick, Jr. and Lori L. Pollock. </author> <title> Implementing an optimizing linda compiler using suif. </title> <booktitle> In Proceedings of SUIF Compiler Workshop. </booktitle> <address> Stanford, California, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: Proof of the safety of this transformation will be presented in the full paper. 5 Experimental Results We have built an optimizing Linda compiler <ref> [13] </ref> based on the SUIF compiler infrastructure [18], and a distributed tuplespace runtime system [14] that executes on a network of Sun 4 workstations connected by Ethernet.
Reference: [14] <author> James B. Fenwick, Jr. and Lori L. Pollock. </author> <title> Issues and experiences in implementing a distributed tuplespace. </title> <type> Technical Report TR 9706, </type> <institution> University of Delaware, </institution> <year> 1996. </year>
Reference-contexts: Proof of the safety of this transformation will be presented in the full paper. 5 Experimental Results We have built an optimizing Linda compiler [13] based on the SUIF compiler infrastructure [18], and a distributed tuplespace runtime system <ref> [14] </ref> that executes on a network of Sun 4 workstations connected by Ethernet.
Reference: [15] <author> D. Gelernter. </author> <title> Generative communication in linda. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(1) </volume> <pages> 80-112, </pages> <month> Jan. </month> <year> 1985. </year>
Reference-contexts: While the message passing paradigm of parallel programming works well for distributed memory systems, it requires the application programmer to program at a very low level in terms of process coordination and communication, much like assembly language programming for sequential machines. The generative communication <ref> [15] </ref> paradigm of parallel programming offers the simplicity of shared memory programming and only a small number of primitives for coordination, while also providing flexibility, power, and the potential to scale like message passing. <p> inefficiencies of distributed queues on distributed memory machines, describes how these data structures can be automatically detected and transformed at compile time, and presents the results of our implementation and evaluation of these analyses and transformations. 2 Linda The best known implementation of the generative communication model is Linda 1 <ref> [15, 5] </ref>. Our work is based on Linda although it applies more generally to any generative communication model implementation.
Reference: [16] <author> Kenneth Landry and John D. Arthur. </author> <title> Achieving asynchronous speedup while preserving synchronous semantics: An implementation of instructional footprinting in linda. </title> <booktitle> In The International Conference on Computer Languages, </booktitle> <pages> pages 55-63, </pages> <address> Toulouse, France, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: While several researchers have demonstrated that a distributed tuplespace implementation can be efficient [9, 17, 6], there remains opportunity for improvement through compiler analysis that targets the underlying message passing <ref> [4, 12, 11, 16] </ref>. This paper provides concrete steps towards the advanced compile-time analysis and optimization of the use of distributed data structures in the shared tuplespace. Specifically, we present global program analysis algorithms for automatically detecting the distributed queue tuplespace data structure at compile time.
Reference: [17] <author> Timothy G. Mattson. </author> <title> The efficiency of linda for general purpose scientific programming. </title> <journal> Scientific Programming, </journal> <volume> 3(1) </volume> <pages> 61-71, </pages> <year> 1994. </year>
Reference-contexts: Compile-time analysis can structure tuplespace to significantly reduce the time to find data in tuplespace [7] (i.e., tuplespace partitioning), and run-time strategies counteract some communication inefficiencies [3]. While several researchers have demonstrated that a distributed tuplespace implementation can be efficient <ref> [9, 17, 6] </ref>, there remains opportunity for improvement through compiler analysis that targets the underlying message passing [4, 12, 11, 16]. This paper provides concrete steps towards the advanced compile-time analysis and optimization of the use of distributed data structures in the shared tuplespace.
Reference: [18] <author> Stanford SUIF Compiler Group. </author> <title> The SUIF Parallelizing Compiler Guide. </title> <institution> Stanford University, </institution> <year> 1994. </year> <note> Version 1.0. </note>
Reference-contexts: Proof of the safety of this transformation will be presented in the full paper. 5 Experimental Results We have built an optimizing Linda compiler [13] based on the SUIF compiler infrastructure <ref> [18] </ref>, and a distributed tuplespace runtime system [14] that executes on a network of Sun 4 workstations connected by Ethernet.
Reference: [19] <author> Gregory V. Wilson. </author> <title> Practical Parallel Programming. Scientific and Engineering Computation Series. </title> <publisher> The MIT Press, </publisher> <year> 1995. </year> <month> 5 </month>
References-found: 19

