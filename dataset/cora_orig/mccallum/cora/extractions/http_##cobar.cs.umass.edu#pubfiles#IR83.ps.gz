URL: http://cobar.cs.umass.edu/pubfiles/IR83.ps.gz
Refering-URL: http://cobar.cs.umass.edu/pubfiles/
Root-URL: 
Email: flarkey,croftg@cs.umass.edu  
Title: Combining Classifiers in Text Categorization  
Author: Leah S. Larkey and W. Bruce Croft 
Address: Amherst, MA 01003-4610  
Affiliation: Center for Intelligent Information Retrieval Department of Computer Science, University of Massachusetts  
Abstract: Three different types of classifiers were investigated in the context of a text categorization problem in the medical domain: the automatic assignment of ICD9 codes to dictated inpatient discharge summaries. K-nearest-neighbor, relevance feedback, and Bayesian independence classifiers were applied individually and in combination. A combination of different classifiers produced better results than any single type of classifier. For this specific medical categorization problem, new query formulation and weighting methods used in the k-nearest-neighbor classifier improved performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Allan, L. Ballesteros, J. P. Callan, W. B. Croft, and Z. Lu. </author> <title> Recent experiments with INQUERY. </title> <editor> In D. K. Harmon, editor, </editor> <booktitle> The Fourth Text REtrieval Conference 8 (TREC-4), </booktitle> <address> Gaithersburg, MD, </address> <year> 1996. </year> <note> NIST special pub-lication. To appear. </note>
Reference-contexts: The relevance feedback algorithm was essentially the same as that used in TREC4 <ref> [1] </ref> and is more fully described there. Relevance feedback began with null queries. First, 40 terms were chosen by comparing their occurrences in relevant and non-relevant training documents. A weighted sum query was built from these 40 terms with weights from the Rocchio formula applied to INQUERY's weighting scheme.
Reference: [2] <author> C. Apte, F. Damerau, and S. M. Weiss. </author> <title> Automated learning of decision rules for text categorization. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 12(3) </volume> <pages> 233-251, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The most common approaches to text categorization use a large corpus of previously coded documents to infer codes for new documents. Algorithms include k-nearest-neighbor [7, 27, 19], Bayesian independence classifiers [14], relevance feedback [22], and rule-induction algorithms from machine learning, like decision trees <ref> [2, 17] </ref>. These categorization algorithms have been applied to many different subject domains, usually news stories, but also physics abstracts [10], Copyright c fl1996 by the Association for Computing Machinery, Inc. To appear in SIGIR'96. Also available as Technical Report IR-83, UMass Center for Intelligent Information Retrieval (CIIR).
Reference: [3] <author> N. Belkin, C. Cool, W. B. Croft, and J. P. Callan. </author> <title> The effect of multiple query representations on information retrieval system performance. </title> <booktitle> In Proceedings of ACM SIGIR'93, </booktitle> <pages> pages 339-346, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Past research in information retrieval has shown that one can improve retrieval effectiveness by using multiple representations in indexing and query formulation <ref> [28, 20, 3, 12] </ref> and by using multiple search strategies [6, 25, 8]. In this work, we investigate whether we can attain similar improvements in the domain of text categorization by combining different representations and classification methods.
Reference: [4] <author> C. Buckley and G. Salton. </author> <title> Optimization of relevance feedback weights. </title> <booktitle> In Proceedings of ACM SIGIR'95, </booktitle> <pages> pages 351-357, </pages> <year> 1995. </year>
Reference-contexts: A weighted sum query was built from these 40 terms with weights from the Rocchio formula applied to INQUERY's weighting scheme. Finally, the weights were adjusted using an iterative technique similar to that of Buckley and Salton and others <ref> [4, 21] </ref>. The relevance feedback classifier is very much like the Bayesian classifier. In our instantiation of the two approaches there are two major differences, concerning the use of term frequency and terms associated with nonrelevant training documents.
Reference: [5] <author> W. S. Cooper. </author> <title> Some inconsistencies and misnomers in probabilistic information retrieval. </title> <booktitle> In Proceedings of ACM SIGIR'91, </booktitle> <pages> pages 57-61, </pages> <year> 1991. </year>
Reference-contexts: Bayes theorem is used to estimate the probability of category membership for each category and each document. Probability estimates are based on the co-occurrence of categories and the selected features in the training corpus, and on an assumption of linked dependence of these co-occurrences <ref> [5] </ref>. Within this framework, we train a separate binary classifier for each ICD9 code, using a manually labeled training corpus of discharge summaries. 1.3 Relevance Feedback Relevance feedback has typically been used in information retrieval to improve existing queries. Usually, an original query is submitted to an information retrieval system.
Reference: [6] <author> W. B. Croft, T. J. Lucia, J. Cringean, and P. Willett. </author> <title> Retrieving documents by plausible inference: An experimental study. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 25(6) </volume> <pages> 599-614, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Past research in information retrieval has shown that one can improve retrieval effectiveness by using multiple representations in indexing and query formulation [28, 20, 3, 12] and by using multiple search strategies <ref> [6, 25, 8] </ref>. In this work, we investigate whether we can attain similar improvements in the domain of text categorization by combining different representations and classification methods. Our domain is the automatic assignment of ICD9 codes to dictated inpatient discharge summaries.
Reference: [7] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Because this coding determines reimbursement, it is important to accomplish this task as easily and as accurately as possible. The most common approaches to text categorization use a large corpus of previously coded documents to infer codes for new documents. Algorithms include k-nearest-neighbor <ref> [7, 27, 19] </ref>, Bayesian independence classifiers [14], relevance feedback [22], and rule-induction algorithms from machine learning, like decision trees [2, 17]. These categorization algorithms have been applied to many different subject domains, usually news stories, but also physics abstracts [10], Copyright c fl1996 by the Association for Computing Machinery, Inc. <p> Each possible code is a category, and we want to determine whether documents belong in each category, or more generally, the probability that a document belongs in each category. We use three different classification techniques, a k-nearest-neighbor <ref> [7] </ref> approach using the belief scores from INQUERY as the distance metric, Bayesian independence classifiers [14], and relevance feedback [22]. <p> Two subtasks, described in detail in [13], made up this part of the research: identifying document sections, and tuning the weights on the sections. Sections were identified heuristically. Weights were tuned using the tuning set divided into two sets with 255 documents each. We used a hill-climbing algorithm <ref> [7] </ref>, and accepted each successive change in weights that improved the first tuning set without hurting performance on the second tuning set. 2.3 Bayesian Independence Classifiers A set of 1068 classifiers were trained, one for each code that occurred 6 or more times in the training data, using the training corpus
Reference: [8] <author> E. A. Fox and J. A. Shaw. </author> <title> Combination of multiple searches. </title> <editor> In D. K. Harmon, editor, </editor> <booktitle> The Second Text REtrieval Conference (TREC-2), </booktitle> <pages> pages 243-252, </pages> <address> Gaithersburg, MD, </address> <year> 1994. </year> <note> NIST special publication 500-215. </note>
Reference-contexts: 1 Introduction Past research in information retrieval has shown that one can improve retrieval effectiveness by using multiple representations in indexing and query formulation [28, 20, 3, 12] and by using multiple search strategies <ref> [6, 25, 8] </ref>. In this work, we investigate whether we can attain similar improvements in the domain of text categorization by combining different representations and classification methods. Our domain is the automatic assignment of ICD9 codes to dictated inpatient discharge summaries.
Reference: [9] <author> N. Fuhr. </author> <title> Models for retrieval with probabilistic indexing. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 25(1) </volume> <pages> 55-72, </pages> <year> 1989. </year>
Reference-contexts: Various improvements to Maron's approach have been explored by other researchers <ref> [9, 16, 14] </ref>. We adopt a form of classifier very close to one used by Lewis [14]. Highlights of this probabilistic model are the following: A small set of features (stopped and stemmed terms) is selected separately for each code. Independent binary classifiers are trained for each ICD9 code. <p> Preliminary experiments showed that increasing the number of features above 40 per code did not improve performance. Classifiers were trained according to the probabilistic model described by Lewis [14], which was derived from a retrieval model proposed by Fuhr <ref> [9] </ref>. The model supports probabilistic indexing [9], however we implement a simplified version in which only estimates of 0 or 1 are used for the probability that a document has a feature. <p> Preliminary experiments showed that increasing the number of features above 40 per code did not improve performance. Classifiers were trained according to the probabilistic model described by Lewis [14], which was derived from a retrieval model proposed by Fuhr <ref> [9] </ref>. The model supports probabilistic indexing [9], however we implement a simplified version in which only estimates of 0 or 1 are used for the probability that a document has a feature.
Reference: [10] <author> N. Fuhr, S. Hartmann, G. Lustig, M. Schwantner, K. Tzeras, and G. Knorz. </author> <title> AIR/X a rule-based multistage indexing system for large subject fields. </title> <booktitle> In Proceedings of the RIAO '91, </booktitle> <pages> pages 606-623, </pages> <address> Barcelona Spain, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Algorithms include k-nearest-neighbor [7, 27, 19], Bayesian independence classifiers [14], relevance feedback [22], and rule-induction algorithms from machine learning, like decision trees [2, 17]. These categorization algorithms have been applied to many different subject domains, usually news stories, but also physics abstracts <ref> [10] </ref>, Copyright c fl1996 by the Association for Computing Machinery, Inc. To appear in SIGIR'96. Also available as Technical Report IR-83, UMass Center for Intelligent Information Retrieval (CIIR). This material is based on work supported by NRaD Contract Number N66001-94-D-6054. <p> Such an interactive system has proven useful to human experts in indexing physics abstracts <ref> [10] </ref> and may be useful in coding patient records. 4.3 Future Directions Our next step is to take advantage of yet another level of structure in these documents. <p> Another advantage of a two-level classifier would be to capture co-occurrence patterns among different classifiers, as Fuhr, et. al. do in the AIR/X system <ref> [10] </ref>. Our current models lose this information because classifiers for each code are completely independent of each other. 5 Acknowledgments We would like to thank David Aronow for his help in categorizing the section titles in the documents and David Fisher, Fang-Fang Feng, and Stephen Soderland for the NLP tagging.
Reference: [11] <editor> D. K. Harmon, editor. </editor> <booktitle> The Third Text REtrieval Conference (TREC-3), </booktitle> <address> Gaithersburg, MD, </address> <year> 1995. </year> <note> NIST special publication 500-225. </note>
Reference: [12] <author> J. Katzer, M. McGill, J. Tessier, W. Frakes, and P. Das-Gupta. </author> <title> A study of the overlap among document representations. </title> <journal> Information Technology: Research and Development, </journal> <volume> 1 </volume> <pages> 261-274, </pages> <year> 1982. </year>
Reference-contexts: 1 Introduction Past research in information retrieval has shown that one can improve retrieval effectiveness by using multiple representations in indexing and query formulation <ref> [28, 20, 3, 12] </ref> and by using multiple search strategies [6, 25, 8]. In this work, we investigate whether we can attain similar improvements in the domain of text categorization by combining different representations and classification methods.
Reference: [13] <author> L. S. Larkey and W. B. Croft. </author> <title> Automatic assignment of ICD9 codes to discharge summaries. </title> <type> Technical Report IR-64, </type> <institution> University of Massachusetts Center for Intelligent Information Retrieval, </institution> <year> 1995. </year>
Reference-contexts: Because the discharge summaries contain large amounts of text that are not relevant to the coding task, we have incorporated a method for differentially weighting sections that provide the most diagnostic evidence, described in <ref> [13] </ref>. For the Bayesian and relevance feedback classifiers, the documents are represented by a small set of features (terms, phrases), and they are selected by slightly different criteria. We do not try to make representations consistent across classifiers. <p> We have tested several different weighting methods for determining w ic , which are discussed in <ref> [13] </ref>. <p> Besides manipulating document-score weighting, we experimented with query formulation, turning the 187 test documents into structured queries using #wsum (weighted sum) and #sum operators, as in Figure 3. Two subtasks, described in detail in <ref> [13] </ref>, made up this part of the research: identifying document sections, and tuning the weights on the sections. Sections were identified heuristically. Weights were tuned using the tuning set divided into two sets with 255 documents each. <p> The model supports probabilistic indexing [9], however we implement a simplified version in which only estimates of 0 or 1 are used for the probability that a document has a feature. The model also considers features which are absent in the test document, which many models do not. (See <ref> [13] </ref> for more detail.) The classifier yields an estimate of the log probability that a code is assigned to a test document. We produce a ranked list of code candidates for each test document, ordered according to this probability.
Reference: [14] <author> D. Lewis. </author> <title> An evaluation of phrasal and clustered representations on a text categorization task. </title> <booktitle> In Proceedings of ACM SIGIR'92, </booktitle> <pages> pages 37-50, </pages> <year> 1992. </year>
Reference-contexts: Because this coding determines reimbursement, it is important to accomplish this task as easily and as accurately as possible. The most common approaches to text categorization use a large corpus of previously coded documents to infer codes for new documents. Algorithms include k-nearest-neighbor [7, 27, 19], Bayesian independence classifiers <ref> [14] </ref>, relevance feedback [22], and rule-induction algorithms from machine learning, like decision trees [2, 17]. These categorization algorithms have been applied to many different subject domains, usually news stories, but also physics abstracts [10], Copyright c fl1996 by the Association for Computing Machinery, Inc. To appear in SIGIR'96. <p> We use three different classification techniques, a k-nearest-neighbor [7] approach using the belief scores from INQUERY as the distance metric, Bayesian independence classifiers <ref> [14] </ref>, and relevance feedback [22]. Within each classification method, we experiment with ways to optimize performance that are specific to the characteristics of the ICD9 coding problem and the kinds of discharge summaries that make up our collection. Each classification method lends itself to different kinds of variations on representations. <p> Various improvements to Maron's approach have been explored by other researchers <ref> [9, 16, 14] </ref>. We adopt a form of classifier very close to one used by Lewis [14]. Highlights of this probabilistic model are the following: A small set of features (stopped and stemmed terms) is selected separately for each code. Independent binary classifiers are trained for each ICD9 code. <p> Various improvements to Maron's approach have been explored by other researchers [9, 16, 14]. We adopt a form of classifier very close to one used by Lewis <ref> [14] </ref>. Highlights of this probabilistic model are the following: A small set of features (stopped and stemmed terms) is selected separately for each code. Independent binary classifiers are trained for each ICD9 code. Bayes theorem is used to estimate the probability of category membership for each category and each document. <p> The exceptions were codes with few training examples, where fewer than forty terms met the criteria. Preliminary experiments showed that increasing the number of features above 40 per code did not improve performance. Classifiers were trained according to the probabilistic model described by Lewis <ref> [14] </ref>, which was derived from a retrieval model proposed by Fuhr [9]. The model supports probabilistic indexing [9], however we implement a simplified version in which only estimates of 0 or 1 are used for the probability that a document has a feature.
Reference: [15] <author> D. Lewis. </author> <title> Evaluating and optimizing autonomous text classification systems. </title> <booktitle> In Proceedings of ACM SIGIR'95, </booktitle> <pages> pages 246-254, </pages> <year> 1995. </year>
Reference-contexts: Instead we take advantage of the diversity of the representations when the classifiers are combined. These classification techniques yield a ranked list of codes (categories) for each document. A purely automatic coder would need cutoff criteria for which codes should actually get assigned. Lewis <ref> [15] </ref> has argued that in evaluating a classification system, one should use effectiveness measures based on estimates of class membership rather than measures based on rankings, like recall-precision.
Reference: [16] <author> D. D. Lewis. </author> <title> Representation and Learning in Information Retrieval. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <year> 1992. </year>
Reference-contexts: Various improvements to Maron's approach have been explored by other researchers <ref> [9, 16, 14] </ref>. We adopt a form of classifier very close to one used by Lewis [14]. Highlights of this probabilistic model are the following: A small set of features (stopped and stemmed terms) is selected separately for each code. Independent binary classifiers are trained for each ICD9 code.
Reference: [17] <author> D. D. Lewis and M. Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 81-93, </pages> <institution> University of Nevada, </institution> <address> Las Vegas, </address> <year> 1994. </year>
Reference-contexts: The most common approaches to text categorization use a large corpus of previously coded documents to infer codes for new documents. Algorithms include k-nearest-neighbor [7, 27, 19], Bayesian independence classifiers [14], relevance feedback [22], and rule-induction algorithms from machine learning, like decision trees <ref> [2, 17] </ref>. These categorization algorithms have been applied to many different subject domains, usually news stories, but also physics abstracts [10], Copyright c fl1996 by the Association for Computing Machinery, Inc. To appear in SIGIR'96. Also available as Technical Report IR-83, UMass Center for Intelligent Information Retrieval (CIIR).
Reference: [18] <author> M. Maron. </author> <title> Automatic indexing: An experimental inquiry. </title> <journal> Journal of the ACM, </journal> <volume> 8 </volume> <pages> 404-417, </pages> <year> 1961. </year>
Reference-contexts: We consider not only whether a category (code) is assigned to a retrieved document, but also whether that category is the principal diagnosis code for the retrieved document. 1.2 Bayesian Independence Classifier The Bayesian independence classifier was first proposed by Maron <ref> [18] </ref> as a way to estimate a probability that a category or key word should be assigned to a document, given the presence of "clue words" in the document. Various improvements to Maron's approach have been explored by other researchers [9, 16, 14].
Reference: [19] <author> B. Masand, G. Linoff, and D. Waltz. </author> <title> Classifying news stories using memory based reasoning. </title> <booktitle> In Proceedings of ACM SIGIR'92, </booktitle> <pages> pages 59-65, </pages> <year> 1992. </year>
Reference-contexts: Because this coding determines reimbursement, it is important to accomplish this task as easily and as accurately as possible. The most common approaches to text categorization use a large corpus of previously coded documents to infer codes for new documents. Algorithms include k-nearest-neighbor <ref> [7, 27, 19] </ref>, Bayesian independence classifiers [14], relevance feedback [22], and rule-induction algorithms from machine learning, like decision trees [2, 17]. These categorization algorithms have been applied to many different subject domains, usually news stories, but also physics abstracts [10], Copyright c fl1996 by the Association for Computing Machinery, Inc. <p> The already-coded documents make up an INQUERY database, and the to-be-coded coded documents (also referred to as test documents) are queries submitted to the database. A similar approach has been used for other classification tasks and is sometimes referred to as memory-based reasoning <ref> [19, 27] </ref>. Our approach is similar to that of Yang and Chute [30] except that we use IN-QUERY rather than cosine similarity for the similarity metric. We go beyond their work in representing the document as a structured query, and in combining k-nearest-neighbor with other classifiers.
Reference: [20] <author> T. B. Rajashekar and W. B. Croft. </author> <title> Combining automatic and manual index representations in probabilistic retrieval. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 6(4) </volume> <pages> 272-283, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Past research in information retrieval has shown that one can improve retrieval effectiveness by using multiple representations in indexing and query formulation <ref> [28, 20, 3, 12] </ref> and by using multiple search strategies [6, 25, 8]. In this work, we investigate whether we can attain similar improvements in the domain of text categorization by combining different representations and classification methods.
Reference: [21] <author> S. Robertson, S. Walker, S. Jones, M. Hancock-Beau-lieu, and M. Gatford. </author> <note> Okapi at TREC-3. In Harmon [11]. </note>
Reference-contexts: A weighted sum query was built from these 40 terms with weights from the Rocchio formula applied to INQUERY's weighting scheme. Finally, the weights were adjusted using an iterative technique similar to that of Buckley and Salton and others <ref> [4, 21] </ref>. The relevance feedback classifier is very much like the Bayesian classifier. In our instantiation of the two approaches there are two major differences, concerning the use of term frequency and terms associated with nonrelevant training documents.
Reference: [22] <author> J. Rocchio. </author> <title> Relevance feedback in information retrieval. </title> <editor> In G. Salton, editor, </editor> <title> The SMART Retrieval System - Experiments in Automatic Document Processing, chapter 14. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1971. </year>
Reference-contexts: The most common approaches to text categorization use a large corpus of previously coded documents to infer codes for new documents. Algorithms include k-nearest-neighbor [7, 27, 19], Bayesian independence classifiers [14], relevance feedback <ref> [22] </ref>, and rule-induction algorithms from machine learning, like decision trees [2, 17]. These categorization algorithms have been applied to many different subject domains, usually news stories, but also physics abstracts [10], Copyright c fl1996 by the Association for Computing Machinery, Inc. To appear in SIGIR'96. <p> We use three different classification techniques, a k-nearest-neighbor [7] approach using the belief scores from INQUERY as the distance metric, Bayesian independence classifiers [14], and relevance feedback <ref> [22] </ref>. Within each classification method, we experiment with ways to optimize performance that are specific to the characteristics of the ICD9 coding problem and the kinds of discharge summaries that make up our collection. Each classification method lends itself to different kinds of variations on representations. <p> The original query and terms from the indicated relevant documents are combined to produce a new query which is better at ranking relevant documents over nonrelevant documents. Term weights in the new query depend upon the occurrence of the terms in relevant and nonrelevant documents <ref> [22, 24] </ref>. Although the original query typically plays an important role in relevance feedback, it does not have to.
Reference: [23] <author> G. Salton. </author> <title> Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Average 11 point precision. Precision and recall have been standard measures of retrieval effectiveness in information retrieval <ref> [23] </ref>. When the task is retrieval, these measures are computed from the ranked list of documents retrieved for each query.
Reference: [24] <author> G. Salton and C. Buckley. </author> <title> Improving retrieval performance by relevance feedback. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(4) </volume> <pages> 288-297, </pages> <year> 1990. </year>
Reference-contexts: The original query and terms from the indicated relevant documents are combined to produce a new query which is better at ranking relevant documents over nonrelevant documents. Term weights in the new query depend upon the occurrence of the terms in relevant and nonrelevant documents <ref> [22, 24] </ref>. Although the original query typically plays an important role in relevance feedback, it does not have to.
Reference: [25] <author> J. A. Shaw and E. A. Fox. </author> <title> Combination of multiple searches. </title> <note> In Harmon [11]. </note>
Reference-contexts: 1 Introduction Past research in information retrieval has shown that one can improve retrieval effectiveness by using multiple representations in indexing and query formulation [28, 20, 3, 12] and by using multiple search strategies <ref> [6, 25, 8] </ref>. In this work, we investigate whether we can attain similar improvements in the domain of text categorization by combining different representations and classification methods. Our domain is the automatic assignment of ICD9 codes to dictated inpatient discharge summaries.
Reference: [26] <author> S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. </author> <title> CRYSTAL: Inducing a conceptual dictionary. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Our associates are using NLP techniques to tag phrases in the discharge summaries with five subtypes each of diagnoses and signs or symptoms <ref> [26] </ref>. Our hypothesis is that performance will be improved by giving more weight to these items in k-nearest-neighbor classification, or to consider such phrases as candidate features along with the single terms we now use in the Bayesian or relevance feedback classifiers.
Reference: [27] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: Because this coding determines reimbursement, it is important to accomplish this task as easily and as accurately as possible. The most common approaches to text categorization use a large corpus of previously coded documents to infer codes for new documents. Algorithms include k-nearest-neighbor <ref> [7, 27, 19] </ref>, Bayesian independence classifiers [14], relevance feedback [22], and rule-induction algorithms from machine learning, like decision trees [2, 17]. These categorization algorithms have been applied to many different subject domains, usually news stories, but also physics abstracts [10], Copyright c fl1996 by the Association for Computing Machinery, Inc. <p> The already-coded documents make up an INQUERY database, and the to-be-coded coded documents (also referred to as test documents) are queries submitted to the database. A similar approach has been used for other classification tasks and is sometimes referred to as memory-based reasoning <ref> [19, 27] </ref>. Our approach is similar to that of Yang and Chute [30] except that we use IN-QUERY rather than cosine similarity for the similarity metric. We go beyond their work in representing the document as a structured query, and in combining k-nearest-neighbor with other classifiers.
Reference: [28] <author> H. Turtle and W. B. Croft. </author> <title> Evaluation of an inference network-based retrieval model. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 9(3) </volume> <pages> 187-222, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Past research in information retrieval has shown that one can improve retrieval effectiveness by using multiple representations in indexing and query formulation <ref> [28, 20, 3, 12] </ref> and by using multiple search strategies [6, 25, 8]. In this work, we investigate whether we can attain similar improvements in the domain of text categorization by combining different representations and classification methods. <p> We are following several of these approaches to automatic coding, all of them incorporating INQUERY, a probabilistic information retrieval system based on an inference net model <ref> [28] </ref>. Each possible code is a category, and we want to determine whether documents belong in each category, or more generally, the probability that a document belongs in each category.
Reference: [29] <editor> C. van Rijsbergen. </editor> <booktitle> Information Retrieval. </booktitle> <address> Butter-worths, London, </address> <note> second edition, </note> <year> 1979. </year>
Reference-contexts: The resulting stemmed terms were the potential features for the classifiers. Second, up to 40 of these features were chosen for each classifier (code) according to mutual information <ref> [29] </ref>, subject to the following constraints: Terms must have length &gt;1, must contain at least one alphabetic character, must co-occur at least two times with the code, and cannot begin with a digit. Forty terms were obtained for most codes.
Reference: [30] <author> Y. Yang and C. G. Chute. </author> <title> An application of Expert Network to clinical classification and MEDLINE indexing. </title> <booktitle> In Proceedings of the Eighteenth Annual Symposium on Computer Applications in Medical Care, </booktitle> <pages> pages 157-161, </pages> <year> 1994. </year>
Reference-contexts: This material is based on work supported by NRaD Contract Number N66001-94-D-6054. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. and medical text <ref> [30] </ref>. We are following several of these approaches to automatic coding, all of them incorporating INQUERY, a probabilistic information retrieval system based on an inference net model [28]. <p> A similar approach has been used for other classification tasks and is sometimes referred to as memory-based reasoning [19, 27]. Our approach is similar to that of Yang and Chute <ref> [30] </ref> except that we use IN-QUERY rather than cosine similarity for the similarity metric. We go beyond their work in representing the document as a structured query, and in combining k-nearest-neighbor with other classifiers. <p> Clearly performance is better when each code has 25 or more training examples. 4.2 Comparison with other research How do these results compare to other attempts at automatic coding and categorization in the medical domain? Researchers at the Mayo Clinic <ref> [30] </ref> have used a method called ExpNet which is very similar to our k-nearest-neighbor classifier and which yields performance very similar to that of our k-nearest-neighbor classifier when applied to a problem with similar parameters.
References-found: 30

