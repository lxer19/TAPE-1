URL: http://www.cs.nps.navy.mil/people/faculty/xie/service_jhsn.ps
Refering-URL: 
Root-URL: 
Title: An Efficient Network Architecture Motivated by Application-Level QoS  
Author: Geoffrey G. Xie Simon S. Lam 
Keyword: application-level QoS, deterministic traffic model, burst scheduling, admission control, selective early ADU discard  
Address: Monterey, CA 93943 Austin, Texas 78712-1188  
Affiliation: Department of Computer Science Department of Computer Sciences Naval Postgraduate School The University of Texas at Austin  
Pubnum: CS-97-Xg-2  
Email: xie@cs.nps.navy.mil lam@cs.utexas.edu  
Date: May 1997  
Abstract: We consider application-level quality of service (QoS) requirements in design of networks that provide delay and loss guarantees. Using this approach, we have designed a novel network architecture that in many aspects is more efficient than ones motivated exclusively by packet QoS. An overview of our work is presented in this paper. The centerpiece of our design is a traffic model that enables delivery of application specific information to the network. Based on the traffic model, we have developed (i) efficient network techniques for providing application-level delay guarantees, (ii) an admission control algorithm for a statistical service that bounds application data losses below a specified value, and (iii) network techniques for managing application data losses to achieve fairness and protection of high priority data units marked by applications (e.g., I frames of MPEG applications). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jon C.R. Bennett and Hui Zhang. </author> <title> Hierarchical packet fair queueing algorithms. </title> <booktitle> In Proceedings of ACM SIGCOMM '96, </booktitle> <pages> pages 143156, </pages> <address> Stanford University, CA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Let C (bits/second) be the channel capacity dedicated to a statistical service with a target burst loss rate of p. (Note that it is straightforward to extend our design to a channel that is hierarchically shared by different agencies, by multiple classes of statistical services, etc. <ref> [1, 7, 20] </ref>). 4.1 Burst Level Admission Control Our traffic model, as mentioned earlier, makes it possible for the packets of a burst to be processed as a whole and independently from other bursts. Thus, separate admission control can be performed for each burst.
Reference: [2] <author> I. Cidon and R. Guerin. </author> <title> An investigation of application level performance in ATM networks. </title> <booktitle> In Proceedings of IEEE INFOCOM '95, </booktitle> <pages> pages 129137, </pages> <address> Boston, MA, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: As a result, the transport layer at the receiver can still deliver other ADUs to the application layer when it has to hold a particular ADU because some packets of that ADU are lost or out of order. In <ref> [2] </ref>, the relation between application-level QoS and network-level QoS was investigated in the context of an ATM network. The study concentrated on the impact of source peak transmission rate on message losses given a constant level of network cell losses.
Reference: [3] <author> David Clark and David L. Tennenhouse. </author> <title> Architectural considerations for a new generation of protocols. </title> <booktitle> In Proceedings of ACM SIGCOMM '90, </booktitle> <pages> pages 200208, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: There have been studies on how to bridge QoS at the network and application levels. However, they focused mainly on techniques that can be used at the end hosts (i.e. sender and receiver) to predict and/or enhance application-level QoS for a particular grade of network-level QoS. In <ref> [3] </ref>, the idea of application-level framing was proposed to avoid application layer throughput degradation at the receiver; such degradation is caused by the transport layer suspending delivery of data when some packets are lost or out of order.
Reference: [4] <author> T. Connolly, P. Amer, and P. Conrad. </author> <title> An extension to TCP: Partial order service. </title> <type> Technical report, </type> <month> Novem-ber </month> <year> 1994. </year> <title> Internet Draft, </title> <type> RFC 1693. </type>
Reference: [5] <author> Alan Demers, Srinivasan Keshav, and Scott Shenker. </author> <title> Analysis and simulation of a fair queuing algorithm. </title> <booktitle> In Proceedings of ACM SIGCOMM '89, </booktitle> <pages> pages 312, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Since then, the model has motivated the development of several techniques [11, 12, 18], refereed to as burst scheduling techniques, that are useful in design of efficient networks that provide application-level delay guarantees. Burst scheduling techniques are particularly useful when network channels employ rate-based packet service disciplines (e.g., <ref> [5, 14, 17, 21, 22] </ref>), which can be described generally as follows. Consider a particular channel. A reserved rate is allocated to each flow that shares the channel. Each packet in a flow is tagged, upon arrival, a priority (or deadline) computed using the rate allocated to the flow.
Reference: [6] <author> D.L. Tennenhouse et al. </author> <title> A survey of active network research. </title> <journal> IEEE Communications, </journal> <month> January </month> <year> 1997. </year>
Reference-contexts: In fact, our approach is consistent with the technology trend, in which the network is required to be more active, i.e. to perform more application specific computations <ref> [6] </ref>, for better and more flexible services.
Reference: [7] <author> Sally Floyd and Van Jacobson. </author> <title> Link-sharing and resource management models for packet networks. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 3(4):365386, </volume> <month> August </month> <year> 1995. </year>
Reference-contexts: Let C (bits/second) be the channel capacity dedicated to a statistical service with a target burst loss rate of p. (Note that it is straightforward to extend our design to a channel that is hierarchically shared by different agencies, by multiple classes of statistical services, etc. <ref> [1, 7, 20] </ref>). 4.1 Burst Level Admission Control Our traffic model, as mentioned earlier, makes it possible for the packets of a burst to be processed as a whole and independently from other bursts. Thus, separate admission control can be performed for each burst.
Reference: [8] <author> R. Guerin, H. Ahmadi, and M. Naghshineh. </author> <title> Equivalent capacity and its application to bandwidth allocation in high-speed networks. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 9(7):968981, </volume> <year> 1991. </year>
Reference: [9] <author> ITU-T Rec. I.371. </author> <title> Traffic Control and Congestion Control in B-ISDN. </title> <address> Perth, U.K., </address> <month> Nov. </month> <pages> 6-14, </pages> <year> 1995. </year>
Reference-contexts: Specifically, a burst is accepted only if the burst's reserved rate 4 does not exceed the channel's unallocated capacity; otherwise, the entire burst will be discarded. (Note that a similar mechanism, called the ATM Block Transfer (ABT) capability, is being standardized by ITU-T <ref> [9] </ref>.) With burst level admission control, packet losses are concentrated over a small number of bursts, and channel capacity is not wasted on delivery of partial bursts. Burst level admission control, as specified below, is an integral part of our admission control algorithm.
Reference: [10] <author> Sugih Jamin, Peter Danzig, Scott Shenker, and Lixia Zhang. </author> <title> A measurement-based admission control algorithm for integrated services packet networks. </title> <booktitle> In Proceedings of ACM SIGCOMM '95, </booktitle> <pages> pages 113, </pages> <month> August </month> <year> 1995. </year>
Reference: [11] <author> Simon S. Lam and Geoffrey G. Xie. </author> <title> Burst scheduling networks. Performance Evaluation, </title> <note> 1997. An abbreviated version in Proceedings of INFOCOM '95, April 1995. Available from http://www.cs.nps.navy.mil/people/faculty/xie/pub. </note>
Reference-contexts: We have taken a different approach, focusing directly on the network. In particular, we consider application-level requirements in design of networks that provide delay and loss guarantees <ref> [11, 12, 20, 1 19] </ref>. Using this approach, we have designed a novel network architecture that in many aspects is more efficient than ones motivated exclusively by packet QoS. An overview of our work is presented in this paper. <p> The results and specifications to be presented can be modified in a straightforward manner for networks where the packet size is variable, but bounded. In designing networks that provide delay guarantees to delay sensitive variable bit rate (VBR) flows, we introduced the concept of a burst <ref> [11] </ref>, which models a sequence of packets that encapsulate an application data unit. For video, for example, a burst models a sequence of packets that carry the encoded bits of a picture. <p> In summary, our traffic model is deterministic because the boundary and the exact bandwidth requirement of each ADU are encoded and accessible by the network. The burst concept and traffic model are particularly applicable to network design in support of application-level services <ref> [11, 20] </ref>. Specifically, they allow each ADU's packets to be processed as a whole and independently from other ADUs. Consequently much more predictable network performance for ADUs can be rendered. <p> burst m (in packets) m bandwidth requirement for burst m (in packets/second) 3 Burst Scheduling Techniques The first version of our deterministic traffic model was constructed as a component of a network architecture, called Burst Scheduling, that provides end-to-end delay and end-to-end jitter guarantees to packets of an VBR flow <ref> [11] </ref>. Since then, the model has motivated the development of several techniques [11, 12, 18], refereed to as burst scheduling techniques, that are useful in design of efficient networks that provide application-level delay guarantees. <p> Since then, the model has motivated the development of several techniques <ref> [11, 12, 18] </ref>, refereed to as burst scheduling techniques, that are useful in design of efficient networks that provide application-level delay guarantees. Burst scheduling techniques are particularly useful when network channels employ rate-based packet service disciplines (e.g., [5, 14, 17, 21, 22]), which can be described generally as follows. <p> Specifically, a reserved rate is not allocated to a flow until the first packet of a burst arrives, and the rate is subsequently deallocated when the last packet of the burst departs <ref> [11] </ref>. (At any time at most one burst in each flow is allocated its reserved rate.) As a result, the rate allocated to a VBR flow is variable, i.e. it changes from one burst to the next. <p> Rate-based packet service disciplines can be modified, in a straightforward manner, to support burst-based rate allocation. Their packet delay guarantees can also be generalized to burst delay guarantees while retaining the firewall property. We have done both for the Virtual Clock [22] service discipline <ref> [11] </ref>. 3.2 Application-Level Delay Guarantee: an Example As mentioned earlier, our traffic model enables burst scheduling techniques that are especially useful in providing application-level delay guarantees. Next we will illustrate this point with an example network architecture, which is described in detail in [11]. <p> for the Virtual Clock [22] service discipline <ref> [11] </ref>. 3.2 Application-Level Delay Guarantee: an Example As mentioned earlier, our traffic model enables burst scheduling techniques that are especially useful in providing application-level delay guarantees. Next we will illustrate this point with an example network architecture, which is described in detail in [11]. <p> Definition 2 (Flow Specification) <ref> [11] </ref> A flow conforms to the following conditions when entering the network * Packets in burst m satisfy a jitter timing constraint, namely: for l = 1; 2; : : : ; b m , 0 A (m; l) A (m; 1) m 4 where A (m; l) is the arrival <p> Such a jitter bound is necessary if a network is to provide a tight burst delay bound. Timing constraint (2) specifies a minimum separation between two consecutive burst arrivals in a flow. This is a form of source control <ref> [11] </ref>. 3.2.2 Packet Scheduler The packet scheduler at each channel uses the Virtual Clock service discipline [22], modified slightly to allow variable rate allocation. Specifically, it creates a new queue for each new flow, as well as a flow regulator for the queue [11]. <p> This is a form of source control <ref> [11] </ref>. 3.2.2 Packet Scheduler The packet scheduler at each channel uses the Virtual Clock service discipline [22], modified slightly to allow variable rate allocation. Specifically, it creates a new queue for each new flow, as well as a flow regulator for the queue [11]. There are three state variables associated with each queue: P , Q, and E, defined below. There are four variables associated with burst m: a m , m, m and u m . The flow regulator can read and write all of these variables. <p> Furthermore, at any time, only one virtual clock value is stored per flow, rather than one per packet. These improvements are made possible by considering application-level QoS, specifically by using the burst-based flow specification <ref> [11] </ref>. 3.2.3 Delay Bounds Next we present a theorem that summarizes the delay guarantees provided by a network that uses the example network architecture. <p> We assume that no channel has been overbooked, i.e. each flow is admitted at connection setup on the basis of its peak rate (i.e. the maximum bandwidth requirement of a burst). The following delay guarantee theorem holds for the flow. Theorem 1 <ref> [11] </ref> Let e k be the constant overhead, i.e. transmission delay plus propagation delay, associated with the channel from node k to k + 1. <p> ) arrives at node K + 1, has the following upper and lower bounds: D m m 1nm 1 g + k=1 D m (K 1) m K X e k : (6) We have evaluated the network architecture by performing a set of simulation experiments using MPEG video data <ref> [11] </ref>. In Figure 1, we show the end-to-end delay performance of a particular video sequence named Energizer. <p> This ensures that delay guarantees can be provided to bursts that are accepted by the node <ref> [11] </ref>.
Reference: [12] <author> Simon S. Lam and Geoffrey G. Xie. </author> <title> Group priority scheduling. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <month> April </month> <year> 1997. </year> <note> An abbreviated version in Proceedings of INFOCOM '96, </note> <month> march </month> <year> 1996. </year> <note> Available from http://www.cs.nps.navy.mil/people/faculty/xie/pub. 19 </note>
Reference-contexts: We have taken a different approach, focusing directly on the network. In particular, we consider application-level requirements in design of networks that provide delay and loss guarantees <ref> [11, 12, 20, 1 19] </ref>. Using this approach, we have designed a novel network architecture that in many aspects is more efficient than ones motivated exclusively by packet QoS. An overview of our work is presented in this paper. <p> Since then, the model has motivated the development of several techniques <ref> [11, 12, 18] </ref>, refereed to as burst scheduling techniques, that are useful in design of efficient networks that provide application-level delay guarantees. Burst scheduling techniques are particularly useful when network channels employ rate-based packet service disciplines (e.g., [5, 14, 17, 21, 22]), which can be described generally as follows. <p> We later discovered that, as illustrated in Figure 2, the cost for the adaptive heap search is dominated by the work due to priority changes when the channel utilization is high [18]. This discovery has motivated the idea of group priority <ref> [12, 18] </ref>. Specifically, consecutive packet arrivals in a flow are partitioned into groups. <p> In particular, we observed that large group sizes can be chosen for most bursts 8 such that the worst-case end-to-end burst delay of a VBR flow is unaffected by the use of group priority <ref> [12] </ref>. <p> bounds in (5) and (6) can be generalized as follows D m m 1nm g n g + k=1 D m (K 1) m K X e k ; (8) where g m is the group size used for burst m. (Methods for choosing appropriate group sizes are described in <ref> [12] </ref>.) Group priority has two advantages. First, with group priority, the scheduler updates less often its data structure for storing priority values (e.g., a heap). As a result, the amount of work for scheduler search is reduced. <p> Note that the average group size is one in the individual priority case. Secondly, we have discovered empirically that the use of group priority results in much better statistical performance (i.e. delay, queue size, and loss probability) for networks where some channels are heavily utilized <ref> [12] </ref>. <p> As a result, all bursts in a VBR 9 flow move through the network at a similar pace. This smoothes out the traffic inside the network, reducing the congestion at the channels that are severely overbooked <ref> [12] </ref>. 4 Two Level Admission Control The delay guarantee presented in the previous section is provided to every burst. Such a deterministic guarantee has a cost of low channel utilization because it requires peak rate reservation.
Reference: [13] <author> Richard G. Ogier, Nina T. Plotkin, and Irfan Khan. </author> <title> Neural network methods with traffic descriptor com-pression for call admission control. </title> <booktitle> In Proceedings of IEEE INFOCOM '96, </booktitle> <pages> pages 768776, </pages> <address> San Francisco, CA, </address> <month> April </month> <year> 1996. </year>
Reference: [14] <author> Abhay K. Parekh and Robert G. Gallager. </author> <title> A generalized processor sharing approach to flow control in integrated services networks: The single node case. </title> <journal> IEEE/ACM Trans. on Networking, </journal> <volume> 1(3):344357, </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: Since then, the model has motivated the development of several techniques [11, 12, 18], refereed to as burst scheduling techniques, that are useful in design of efficient networks that provide application-level delay guarantees. Burst scheduling techniques are particularly useful when network channels employ rate-based packet service disciplines (e.g., <ref> [5, 14, 17, 21, 22] </ref>), which can be described generally as follows. Consider a particular channel. A reserved rate is allocated to each flow that shares the channel. Each packet in a flow is tagged, upon arrival, a priority (or deadline) computed using the rate allocated to the flow.
Reference: [15] <author> A. Romanow and S. Floyd. </author> <title> The dynamics of TCP traffic over ATM networks. </title> <booktitle> In Proceedings of ACM SIGCOMM '94, </booktitle> <address> London, England, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Therefore, it is desirable that loss management within a node can facilitate protection of high priority ADUs marked by applications. In the remainder of this section, we will present our loss management techniques, which are based on selective early ADU discard (SEAD) [19]. Similar to early packet discard proposals <ref> [15, 16] </ref>, SEAD achieves the goals of fair loss distribution and protection of high priority ADUs by taking early control actions.
Reference: [16] <author> J.S. Turner. </author> <title> Maintaining high throughput during overload in ATM networks. </title> <booktitle> In Proceedings of IEEE INFOCOM '96, </booktitle> <pages> pages 287295, </pages> <address> San Francisco, CA, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Therefore, it is desirable that loss management within a node can facilitate protection of high priority ADUs marked by applications. In the remainder of this section, we will present our loss management techniques, which are based on selective early ADU discard (SEAD) [19]. Similar to early packet discard proposals <ref> [15, 16] </ref>, SEAD achieves the goals of fair loss distribution and protection of high priority ADUs by taking early control actions.
Reference: [17] <author> Geoffrey G. Xie and Simon S. Lam. </author> <title> Delay guarantee of Virtual Clock server. </title> <journal> IEEE/ACM Trans. on Networking, </journal> <volume> 3(6):683689, </volume> <month> December </month> <year> 1995. </year>
Reference-contexts: Since then, the model has motivated the development of several techniques [11, 12, 18], refereed to as burst scheduling techniques, that are useful in design of efficient networks that provide application-level delay guarantees. Burst scheduling techniques are particularly useful when network channels employ rate-based packet service disciplines (e.g., <ref> [5, 14, 17, 21, 22] </ref>), which can be described generally as follows. Consider a particular channel. A reserved rate is allocated to each flow that shares the channel. Each packet in a flow is tagged, upon arrival, a priority (or deadline) computed using the rate allocated to the flow.
Reference: [18] <author> Geoffrey G. Xie and Simon S. Lam. </author> <title> An efficient adaptive search algorithm for scheduling real-time traffic. </title> <booktitle> In Proceedings of 1996 IEEE International Conference on Network Protocols (ICNP '96), </booktitle> <pages> pages 1422, </pages> <address> Columbus, Ohio, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Since then, the model has motivated the development of several techniques <ref> [11, 12, 18] </ref>, refereed to as burst scheduling techniques, that are useful in design of efficient networks that provide application-level delay guarantees. Burst scheduling techniques are particularly useful when network channels employ rate-based packet service disciplines (e.g., [5, 14, 17, 21, 22]), which can be described generally as follows. <p> We developed a search algorithm based upon a novel data structure, called adaptive heap, which behaves like a heap most of the time, but adaptively changes its strategy when necessary to satisfy the 7 time bound <ref> [18] </ref>. The algorithm has optimal worst-case time performance and good average performance. We later discovered that, as illustrated in Figure 2, the cost for the adaptive heap search is dominated by the work due to priority changes when the channel utilization is high [18]. <p> necessary to satisfy the 7 time bound <ref> [18] </ref>. The algorithm has optimal worst-case time performance and good average performance. We later discovered that, as illustrated in Figure 2, the cost for the adaptive heap search is dominated by the work due to priority changes when the channel utilization is high [18]. This discovery has motivated the idea of group priority [12, 18]. Specifically, consecutive packet arrivals in a flow are partitioned into groups. <p> We later discovered that, as illustrated in Figure 2, the cost for the adaptive heap search is dominated by the work due to priority changes when the channel utilization is high [18]. This discovery has motivated the idea of group priority <ref> [12, 18] </ref>. Specifically, consecutive packet arrivals in a flow are partitioned into groups. <p> First, with group priority, the scheduler updates less often its data structure for storing priority values (e.g., a heap). As a result, the amount of work for scheduler search is reduced. The reduction can be very significant when the channel utilization is high, as we have observed in experiments <ref> [18] </ref>. In Figure 3, the result from one of the experiments is shown. Note that the average group size is one in the individual priority case.
Reference: [19] <author> Geoffrey G. Xie and Simon S. Lam. </author> <title> Admission control and loss management for an application-level statistical service. </title> <type> Technical Report CS-97-Xg-1, </type> <institution> Naval Postgraduate School, </institution> <month> January </month> <year> 1997. </year> <note> Available from http://www.cs.nps.navy.mil/people/faculty/xie/pub. </note>
Reference-contexts: We have developed an efficient admission control algorithm, which bounds the burst loss rate at a network channel below a specified value while achieving high channel utilization <ref> [19, 20] </ref>. 3 The algorithm is based on a novel approach, made possible by our traffic model, that combines admission control at the flow level and admission control at the burst level. 3 We have also investigated how to perform nodal allocation of an application's end-to-end ADU loss requirement. <p> See <ref> [19] </ref>. We assume that the path of each flow is fixed. For a particular flow, its path is a sequence of nodes, each of which consists of an outgoing channel, and a set of buffers for the channel where packets of different flows are queued. Consider a particular node. <p> Denote i (t) the reserved rate for the burst of flow i that is either allocated a rate or being processed by burst level admission control at time t. ( i (t) = 0 if there is no such burst for flow i at t.) In <ref> [19] </ref>, we derived, using a generalized venison of central limit theorem, the following sufficient condition to bound the burst loss rate at the node below p: Z p 1; (9) where Z p is the (1 p) percentile of the standard normal distribution, and Z = P M q i=1 V <p> We are currently investigating such techniques. 13 4.3 Empirical Evaluation of Admission Control Algorithm We have evaluated our two level admission control algorithm by performing a set of simulation experiments <ref> [19, 20] </ref>. The simulation configuration is shown in Figure 5. The nodes labeled by VS denote video sources, and VD their destination. Each video source generates 53-byte packets from a trace file obtained from a MPEG video sequence [19], and each MPEG frame (or picture) is considered an ADU. <p> The simulation configuration is shown in Figure 5. The nodes labeled by VS denote video sources, and VD their destination. Each video source generates 53-byte packets from a trace file obtained from a MPEG video sequence <ref> [19] </ref>, and each MPEG frame (or picture) is considered an ADU. The two level admission control algorithm is implemented for channel L1 with a target burst loss rate of p. <p> In particular, the ADU losses should be distributed evenly among all flows subscribing to the service and uniformly over the duration of each flow. We have developed loss management techniques, specifically simple modifications to the admission control algorithm, to enable the channel to anticipate and distribute ADU losses <ref> [19] </ref>. 6 Beside fair distribution of losses, such active loss management was also motivated by the fact that to many applications, some of their ADUs are more important than others. For example, to an MPEG decoder, I frames are more important than either P or B frames. <p> Therefore, it is desirable that loss management within a node can facilitate protection of high priority ADUs marked by applications. In the remainder of this section, we will present our loss management techniques, which are based on selective early ADU discard (SEAD) <ref> [19] </ref>. Similar to early packet discard proposals [15, 16], SEAD achieves the goals of fair loss distribution and protection of high priority ADUs by taking early control actions. <p> The advantage is that high utilization can be maintained at the same time that high priority ADUs have a loss rate much smaller than p. However this advantage does come with a price. Namely, those unmarked ADUs can experience more losses. In <ref> [19] </ref>, we showed that the overall ADU loss rate is still bounded with SEAD-1, albeit by a value a little larger than p. We have evaluated the performance of SEAD-1 by simulation experiments. The simulation configuration shown in Figure 5 was used. The channel capacity of L1 is 48 Mbps.
Reference: [20] <author> Geoffrey G. Xie and Simon S. Lam. </author> <title> Real-time block transfer under a link sharing hierarchy. </title> <booktitle> In Proceedings of IEEE INFOCOM '97, </booktitle> <address> Kobe, Japan, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: We have taken a different approach, focusing directly on the network. In particular, we consider application-level requirements in design of networks that provide delay and loss guarantees <ref> [11, 12, 20, 1 19] </ref>. Using this approach, we have designed a novel network architecture that in many aspects is more efficient than ones motivated exclusively by packet QoS. An overview of our work is presented in this paper. <p> In summary, our traffic model is deterministic because the boundary and the exact bandwidth requirement of each ADU are encoded and accessible by the network. The burst concept and traffic model are particularly applicable to network design in support of application-level services <ref> [11, 20] </ref>. Specifically, they allow each ADU's packets to be processed as a whole and independently from other ADUs. Consequently much more predictable network performance for ADUs can be rendered. <p> We have developed an efficient admission control algorithm, which bounds the burst loss rate at a network channel below a specified value while achieving high channel utilization <ref> [19, 20] </ref>. 3 The algorithm is based on a novel approach, made possible by our traffic model, that combines admission control at the flow level and admission control at the burst level. 3 We have also investigated how to perform nodal allocation of an application's end-to-end ADU loss requirement. <p> Let C (bits/second) be the channel capacity dedicated to a statistical service with a target burst loss rate of p. (Note that it is straightforward to extend our design to a channel that is hierarchically shared by different agencies, by multiple classes of statistical services, etc. <ref> [1, 7, 20] </ref>). 4.1 Burst Level Admission Control Our traffic model, as mentioned earlier, makes it possible for the packets of a burst to be processed as a whole and independently from other bursts. Thus, separate admission control can be performed for each burst. <p> We are currently investigating such techniques. 13 4.3 Empirical Evaluation of Admission Control Algorithm We have evaluated our two level admission control algorithm by performing a set of simulation experiments <ref> [19, 20] </ref>. The simulation configuration is shown in Figure 5. The nodes labeled by VS denote video sources, and VD their destination. Each video source generates 53-byte packets from a trace file obtained from a MPEG video sequence [19], and each MPEG frame (or picture) is considered an ADU.
Reference: [21] <author> Hui Zhang and Srinivasan Keshav. </author> <title> Comparison of rate-based service disciplines. </title> <booktitle> In Proceedings of ACM SIGCOMM '91, </booktitle> <pages> pages 113121, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Since then, the model has motivated the development of several techniques [11, 12, 18], refereed to as burst scheduling techniques, that are useful in design of efficient networks that provide application-level delay guarantees. Burst scheduling techniques are particularly useful when network channels employ rate-based packet service disciplines (e.g., <ref> [5, 14, 17, 21, 22] </ref>), which can be described generally as follows. Consider a particular channel. A reserved rate is allocated to each flow that shares the channel. Each packet in a flow is tagged, upon arrival, a priority (or deadline) computed using the rate allocated to the flow.
Reference: [22] <author> Lixia Zhang. VirtualClock: </author> <title> A new traffic control algorithm for packet switching networks. </title> <booktitle> In Proceedings of ACM SIGCOMM '90, pages 1929, </booktitle> <month> August </month> <year> 1990. </year> <month> 20 </month>
Reference-contexts: Since then, the model has motivated the development of several techniques [11, 12, 18], refereed to as burst scheduling techniques, that are useful in design of efficient networks that provide application-level delay guarantees. Burst scheduling techniques are particularly useful when network channels employ rate-based packet service disciplines (e.g., <ref> [5, 14, 17, 21, 22] </ref>), which can be described generally as follows. Consider a particular channel. A reserved rate is allocated to each flow that shares the channel. Each packet in a flow is tagged, upon arrival, a priority (or deadline) computed using the rate allocated to the flow. <p> Rate-based packet service disciplines can be modified, in a straightforward manner, to support burst-based rate allocation. Their packet delay guarantees can also be generalized to burst delay guarantees while retaining the firewall property. We have done both for the Virtual Clock <ref> [22] </ref> service discipline [11]. 3.2 Application-Level Delay Guarantee: an Example As mentioned earlier, our traffic model enables burst scheduling techniques that are especially useful in providing application-level delay guarantees. Next we will illustrate this point with an example network architecture, which is described in detail in [11]. <p> Timing constraint (2) specifies a minimum separation between two consecutive burst arrivals in a flow. This is a form of source control [11]. 3.2.2 Packet Scheduler The packet scheduler at each channel uses the Virtual Clock service discipline <ref> [22] </ref>, modified slightly to allow variable rate allocation. Specifically, it creates a new queue for each new flow, as well as a flow regulator for the queue [11]. There are three state variables associated with each queue: P , Q, and E, defined below. <p> Specifically, the flow regulator executes the procedure update (P ,E) only once per burst, i.e. for the first packet in each burst. For any other packet in burst m, it simply increments P by the value of 1= m , instead of executing the original Virtual Clock algorithm <ref> [22] </ref>. With the exception of the first packet of each burst, there is no need to store packet arrival times. Furthermore, at any time, only one virtual clock value is stored per flow, rather than one per packet. <p> The two level admission control algorithm is implemented for channel L1 with a target burst loss rate of p. Each video source makes a reservation with L1, and starts sending packets to the network only after the reservation is successful. Packets are scheduled based on their virtual clock values <ref> [22] </ref>. The channel capacity of L1 as well as the value of p were varied in different experiments. We ran each experiment for 10 seconds of simulated time. Channel utilization In Figure 6, we plot the channel utilization as a function of the target picture loss rate p.
References-found: 22

