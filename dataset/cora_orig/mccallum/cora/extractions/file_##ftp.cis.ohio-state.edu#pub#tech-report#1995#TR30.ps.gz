URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1995/TR30.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Title: Incremental Learning of Complex Temporal Patterns 1  
Author: DeLiang Wang and Budi Yuwono 
Address: Columbus, Ohio 43210-1277, USA  
Affiliation: Laboratory for AI Research, Department of Computer and Information Science and Center for Cognitive Science Department of Computer and Information Science The Ohio State University  
Abstract: 1 Technical Report: OSU-CISRC-6/95-TR30 Abstract A neural model for temporal pattern generation is used and analyzed for training with multiple complex sequences in a sequential manner. The network exhibits some degree of interference when new sequences are acquired. It is proven that the model is capable of incrementally learning a finite number of complex sequences. The model is then evaluated with a large set of highly correlated sequences. While the number of intact sequences increases linearly with the number of previously acquired sequences, the amount of retraining due to interference appears to be independent of the size of existing memory. The model is extended to include a chunking network which detects repeated subsequences between and within sequences. The chunking mechanism substantially reduces the amount of retraining in sequential training. Thus, the network investigated here constitutes an effective sequential memory. Various aspects of such a memory are discussed at the end of the paper. 1 The work described in this paper was supported in part by the NSF grants IRI-9211419, IRI-9423312, and 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Baram, </author> <title> "Memorizing binary vector sequences by a sparsely encoded network," </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> vol. 5(6), </volume> <pages> pp. 974-981, </pages> <year> 1994. </year>
Reference-contexts: Each detector encodes a sequence with the beginning component having the strongest weight, and the subsequent components having successively weaker weights. They claim that the network has an unusually high capacity. However, it is unclear how their network reads out the encoded sequences. Baram <ref> [1] </ref> presented a model for memorizing vector sequences using the Kanerva memory model [24]. The basic idea is similar to those models that are based on the Hopfield model.
Reference: [2] <author> J.M. Barnes and B.J. Underwood, </author> <title> "'Fate' of first-list associations in transfer theory," </title> <journal> J. Exp. Psychol., </journal> <volume> vol. 58, </volume> <pages> pp. 97-105, </pages> <year> 1959. </year>
Reference-contexts: Retroactive interference has been well documented in psychology [9], which occurs when learning a later event interferes with the recall of earlier information. In general, the similarity between the current event and memory items is responsible for retroactive interference <ref> [2] </ref>, [8], [4]. Retroactive interference exists in animals as well [35]. The existence of retroactive interference suggests that events are not independently stored in the brain, and related events are somehow associated in the memory.
Reference: [3] <author> Y. Bengio, P. Simard, and P. Frasconi, </author> <title> "Learning long-term dependencies with gradient descent is difficult," </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> vol. 5(2), </volume> <pages> pp. 157-166, </pages> <year> 1994. </year>
Reference-contexts: Though proposed remedies can alleviate the problem to a certain degree, the problem is not resolved. In multilayer perceptrons, a blended form of STM becomes increasingly ambiguous when temporal dependencies increase <ref> [3] </ref>. The use of high-order units in the Hopfield model entails a huge number of connections when long range temporal dependencies appear, or the model faces ambiguities [41].
Reference: [4] <author> G.H. Bower, S. Thompson-Schill, and E. Tulving, </author> <title> "Reducing retroactive interference: An interference analysis," </title> <journal> J. Exp. Psychol: Learning, Memory, and Cognition, </journal> <volume> vol. 20, </volume> <pages> pp. 51-66, </pages> <year> 1994. </year>
Reference-contexts: Retroactive interference has been well documented in psychology [9], which occurs when learning a later event interferes with the recall of earlier information. In general, the similarity between the current event and memory items is responsible for retroactive interference [2], [8], <ref> [4] </ref>. Retroactive interference exists in animals as well [35]. The existence of retroactive interference suggests that events are not independently stored in the brain, and related events are somehow associated in the memory.
Reference: [5] <author> G. Bradski, G.A. Carpenter, and S. Grossberg, </author> <title> "STORE working memory networks for storage and recall of arbitrary temporal sequences," </title> <journal> Biol. Cybern., </journal> <volume> vol. 71, </volume> <pages> pp. 469-480, </pages> <year> 1994. </year>
Reference-contexts: In multilayer perceptrons, a blended form of STM becomes increasingly ambiguous when temporal dependencies increase [3]. The use of high-order units in the Hopfield model entails a huge number of connections when long range temporal dependencies appear, or the model faces ambiguities [41]. More recently, Bradski et al. <ref> [5] </ref> proposed an STM model, and showed that both recency and primacy can be captured by such a model. In addition, their model creates new representations for repeated occurrences of the same symbol, thus capable of encoding complex sequences to a certain extent. <p> In this sense, the anticipation model exhibits similar properties as the adaptive resonance theory [7]. However, the anticipation model deals with temporal patterns, whereas ART deals with static patterns. A variant of ART which incorporates an STM model can store and recognize temporal sequences <ref> [5] </ref>. It is unclear how such a model is used for sequence generation tasks. Another difference between the anticipation model and ART is that the former allows overlaps in storing different temporal patterns, while the latter does not.
Reference: [6] <author> J. Buhmann and K. Schulten, </author> <title> "Noise-driven temporal association in neural networks," </title> <journal> Europhys. Lett., </journal> <volume> vol. 4, </volume> <pages> pp. 1205-1209, </pages> <year> 1987. </year>
Reference-contexts: Similarly, for temporal recall models based on the Hopfield associative memory, a temporal sequence is viewed as associations between consecutive components. These associations are stored in extended versions of the Hopfield model [40], <ref> [6] </ref>, [21]. To deal with longer temporal dependencies, high-order networks have been proposed [19]. One of the main problems with these two classes of model lies in their difficulty in retrieving complex temporal sequences, where the same part may occur many times in the sequence.
Reference: [7] <author> G.A. Carpenter and S. Grossberg, </author> <title> "A massively parallel architecture for a self-organizing neural pattern recognition machine," Computer Vision, Graphs, </title> <booktitle> and Imaging Processing, </booktitle> <volume> vol. 37, </volume> <pages> pp. 54-115, </pages> <year> 1987. </year>
Reference-contexts: We can say that there is a tradeoff between distributedness and interference. Models that use nonoverlapping representations, or local representations, do not exhibit the problem. For example, the adaptive resonance theory (ART, <ref> [7] </ref>) does not have the problem because each pattern corresponds to the weight vector of a unique unit and no overlapping is allowed between any two weight vectors. It is clear that during sequential learning, humans show some degree of interference. <p> The system is both adaptive and stable, and its long-term memory increases gradually as learning episodes extend. In this sense, the anticipation model exhibits similar properties as the adaptive resonance theory <ref> [7] </ref>. However, the anticipation model deals with temporal patterns, whereas ART deals with static patterns. A variant of ART which incorporates an STM model can store and recognize temporal sequences [5]. It is unclear how such a model is used for sequence generation tasks.
Reference: [8] <author> C.C. Chandler, </author> <title> "Accessing related events increases retroactive interference in a matching test," </title> <journal> J. Exp. Psychol: Learning, Memory, and Cognition, </journal> <volume> vol. 19, </volume> <pages> pp. 967-974, </pages> <year> 1993. </year>
Reference-contexts: Retroactive interference has been well documented in psychology [9], which occurs when learning a later event interferes with the recall of earlier information. In general, the similarity between the current event and memory items is responsible for retroactive interference [2], <ref> [8] </ref>, [4]. Retroactive interference exists in animals as well [35]. The existence of retroactive interference suggests that events are not independently stored in the brain, and related events are somehow associated in the memory.
Reference: [9] <editor> R.L. Crooks and J. Stein, </editor> <booktitle> Psychology: Science, behavior, and life. </booktitle> <address> Fort Worth, TX: </address> <publisher> Holt, Rinehart and Winston, </publisher> <year> 1991. </year>
Reference-contexts: It is clear that during sequential learning, humans show some degree of interference. Retroactive interference has been well documented in psychology <ref> [9] </ref>, which occurs when learning a later event interferes with the recall of earlier information. In general, the similarity between the current event and memory items is responsible for retroactive interference [2], [8], [4]. Retroactive interference exists in animals as well [35].
Reference: [10] <author> S. Diederich and M. Opper, </author> <title> "Learning of correlated patterns in spin-like glass networks by local learning rules," </title> <journal> Phys. Rev. Lett., </journal> <volume> vol. 58, </volume> <pages> pp. 949-952, </pages> <year> 1987. </year>
Reference-contexts: However, the Hopfield model has an exceedingly low capacity when storing correlated patterns that have overlapping structures [20]. The Hopfield model has been extended to deal with correlated patterns [25], and Diederich and Opper <ref> [10] </ref> proposed a local learning rule to acquire the necessary weights iteratively. The local learning rule used by them is similar to the perceptron learning rule. Thus, it appears that such a scheme for dealing with correlated patterns should suffer from catastrophic interference.
Reference: [11] <author> J.L. Elman, </author> <title> "Finding structure in time," </title> <journal> Cognit. Sci., </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference-contexts: The basic idea for the former class of models is to view a temporal sequence as a set of associations between consecutive components, and learn these associations as input-output pairs [23], <ref> [11] </ref>, [32]. To deal with temporal dependencies beyond immediate predecessors, part of the input layer is used to keep a blended form of history, behaving as short-term memory (STM). Similarly, for temporal recall models based on the Hopfield associative memory, a temporal sequence is viewed as associations between consecutive components. <p> For context recognition, the system can be extended to recognizing similar contexts once a context is learned (see [42], [43] for more discussions). Another level of generalization concerns with learning an underlying temporal structure, such as a grammar, from its exemplar sequences (see <ref> [11] </ref>, [33] for such studies). This kind of generalization is currently beyond the scope of the anticipation model. Yet another form of generalization has to do timing, or temporal duration. Wang and Arbib [43] have addressed this issue, and their proposed methods can be directly incorporated into the anticipation model.
Reference: [12] <author> R. Elmasri and S.B. Navathe, </author> <title> Fundamentals of database systems. </title> <address> Redwood City, CA: Benjamin/Cummings, </address> <year> 1994. </year>
Reference-contexts: This method of storing sequences is used for many applications, such as storing book titles 16 for library search and various pieces of music, and it is characteristic of the computer database approach <ref> [12] </ref>. The anticipation model differs from this method in several ways. First, our model supports not only sequence recall by its identifier but also partial recall by a context. Second, the computer method stores recurring subsequences as different copies, and the anticipation model stores them as a unique copy.
Reference: [13] <author> R.M. </author> <title> French, "Using semi-distributed representations to overcome catastrophic forgetting in connectionist networks," </title> <booktitle> in Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum, </publisher> <pages> pp. 173-178, </pages> <year> 1991. </year>
Reference-contexts: Many subsequent studies attempt to address the problem. Most of proposals amount to reducing overlapping in hidden layer representations by some techniques of orthogonalization, which have been used long ago for reducing crosstalks in associative memories (see Kortge [26], French <ref> [13] </ref>, [14], Kruschke [27], Sloman and Rumelhart [39], Sharkey and Sharkey [37]). Most of these proposals are verified only by small scale simulations, which, together with the lack of rigorous analysis, make it difficult to judge to what extent these proposed methods work.
Reference: [14] <author> R.M. </author> <title> French, "Dynamically contraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference," </title> <booktitle> in Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society, </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum, </publisher> <pages> pp. 335-340, </pages> <year> 1994. </year>
Reference-contexts: Many subsequent studies attempt to address the problem. Most of proposals amount to reducing overlapping in hidden layer representations by some techniques of orthogonalization, which have been used long ago for reducing crosstalks in associative memories (see Kortge [26], French [13], <ref> [14] </ref>, Kruschke [27], Sloman and Rumelhart [39], Sharkey and Sharkey [37]). Most of these proposals are verified only by small scale simulations, which, together with the lack of rigorous analysis, make it difficult to judge to what extent these proposed methods work.
Reference: [15] <author> R. Granger, J. Whitson, J. Larson, and G. Lynch, </author> <title> "Non-Hebbian properties of long-term potentiation enable high-capacity encoding of temporal sequences," </title> <journal> Proc. Natl. Acad. Sci. USA, </journal> <volume> vol. 91, </volume> <pages> pp. 10104-10108, </pages> <year> 1994. </year>
Reference-contexts: In addition, their model creates new representations for repeated occurrences of the same symbol, thus capable of encoding complex sequences to a certain extent. Granger et al. <ref> [15] </ref> proposed a biologically motivated model for encoding temporal sequences. Their model uses a non-Hebbian competitive learning rule that eventually develops sequence detectors at the end of sequence presentation. Each detector encodes a sequence with the beginning component having the strongest weight, and the subsequent components having successively weaker weights.
Reference: [16] <author> S. Grossberg, </author> <title> "Some networks that can learn, remember, and reproduce any number of complicated space-time patterns, 1," </title> <journal> J. Math. Mechan., </journal> <volume> vol. 19, </volume> <pages> pp. 53-91, </pages> <year> 1969. </year>
Reference-contexts: How does the anticipation model as a sequential memory compare with computer storage of symbol sequences (strings)? A typical computational scheme is to store each sequence independently, and recall it by its identifier. This is similar to Grossberg's outstar avalanche model <ref> [16] </ref>. In doing so, this method treats both simple sequences and complex sequences in the same way, and a complex sequence can be recalled just as easily as a simple one. Also, there is no interference when a new sequence is acquired.
Reference: [17] <author> S. Grossberg, </author> <title> "Adaptive pattern classification and universal recoding: I. Parallel development and coding of neural feature detectors," </title> <journal> Biol. Cybern., </journal> <volume> vol. 23, </volume> <pages> pp. 121-134, </pages> <year> 1976. </year>
Reference-contexts: The learning process proceeds as follows. An input sequence S is presented to the network one component at a time. In each time step, only one detector unit can be activated because all of the detector units form a winner-take-all network <ref> [17] </ref>. The winning detector, denoted as z, performs oneshot learning to recognize a sequence of most active components held in STM by adapting its connection weights to match the activity levels of the corresponding SR units - much like template matching. <p> V jk (t) = I j (t) ifk=1 (headunit) max [0,V j,k-1 (t-1)d] otherwise (A3) where d is a decay parameter. The detailed dynamics of winner-take-all competition in the detector layer can be found in <ref> [17] </ref>.
Reference: [18] <author> S. Grossberg, </author> <title> "Competitive learning: From interactive activation to adaptive resonance," </title> <journal> Cognit. Sci., </journal> <volume> vol. 11, </volume> <pages> pp. 23-63, </pages> <year> 1987. </year>
Reference-contexts: It turns out that incremental learning is particularly challenging to obtain. In multilayer perceptrons, it has been recognized that the network exhibits so called catastrophic interference, whereby later training disrupts the traces of previous training. It was pointed out by Grossberg <ref> [18] </ref>, and systematically revealed by McCloskey and Cohen [30] and Ratcliff [34]. Many subsequent studies attempt to address the problem.
Reference: [19] <author> I. Guyon, L. Personnaz, J.P. Nadal, and G. Dreyfus, </author> <title> "Storage and retrieval of complex sequences in neural networks," </title> <journal> Phys. Rev. A, </journal> <volume> vol. 38, </volume> <pages> pp. 6365-6372, </pages> <year> 1988. </year>
Reference-contexts: Similarly, for temporal recall models based on the Hopfield associative memory, a temporal sequence is viewed as associations between consecutive components. These associations are stored in extended versions of the Hopfield model [40], [6], [21]. To deal with longer temporal dependencies, high-order networks have been proposed <ref> [19] </ref>. One of the main problems with these two classes of model lies in their difficulty in retrieving complex temporal sequences, where the same part may occur many times in the sequence. Though proposed remedies can alleviate the problem to a certain degree, the problem is not resolved.
Reference: [20] <author> H. Hertz, A. Krogh, and R.G. Palmer, </author> <title> Introduction to the theory of neural computation. </title> <address> Redwood City, CA: </address> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Associative memories appear to be able to incorporate more patterns easily so long as the overall number of patterns does not exceed the memory capacity. However, the Hopfield model has an exceedingly low capacity when storing correlated patterns that have overlapping structures <ref> [20] </ref>. The Hopfield model has been extended to deal with correlated patterns [25], and Diederich and Opper [10] proposed a local learning rule to acquire the necessary weights iteratively. The local learning rule used by them is similar to the perceptron learning rule.
Reference: [21] <author> T.M. Heskes and S. Gielen, </author> <title> "Retrieval of pattern sequences at variable speeds in a neural network with delays," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 5, </volume> <pages> pp. 145-152, </pages> <year> 1992. </year>
Reference-contexts: Similarly, for temporal recall models based on the Hopfield associative memory, a temporal sequence is viewed as associations between consecutive components. These associations are stored in extended versions of the Hopfield model [40], [6], <ref> [21] </ref>. To deal with longer temporal dependencies, high-order networks have been proposed [19]. One of the main problems with these two classes of model lies in their difficulty in retrieving complex temporal sequences, where the same part may occur many times in the sequence.
Reference: [22] <institution> Proceedings of IEEE International Conference on Neural Networks, </institution> <address> Orlando, FL, </address> <year> 1994. </year> <month> 20 </month>
Reference-contexts: The database of the sequences used consists of the titles of all sessions that were held during the 1994 IEEE International Conference on Neural Networks (ICNN-94, see <ref> [22] </ref>). The model is evaluated in two phases, both of which use the sequential training procedure. In Phase I, we test the model using 11 sequences; in Phase II, we test the model using the whole set of 97 sequences.
Reference: [23] <author> M.I. Jordan, </author> <title> "Attractor dynamics and parallelism in a connectionist sequential machine," </title> <booktitle> in Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum, </publisher> <pages> pp. 531-546, </pages> <year> 1986. </year>
Reference-contexts: The basic idea for the former class of models is to view a temporal sequence as a set of associations between consecutive components, and learn these associations as input-output pairs <ref> [23] </ref>, [11], [32]. To deal with temporal dependencies beyond immediate predecessors, part of the input layer is used to keep a blended form of history, behaving as short-term memory (STM).
Reference: [24] <author> P. Kanerva, </author> <title> Sparse distributed memory. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: They claim that the network has an unusually high capacity. However, it is unclear how their network reads out the encoded sequences. Baram [1] presented a model for memorizing vector sequences using the Kanerva memory model <ref> [24] </ref>. The basic idea is similar to those models that are based on the Hopfield model. Baram's model uses second-order synapses to store the temporal associations between consecutive vectors in a sequence, and the model deals only with sequences that contain no repeating vectors.
Reference: [25] <author> I. Kantor and H. Sompolinsky, </author> <title> "Associative recall of memory without errors," </title> <journal> Phys. Rev. A, </journal> <volume> vol. 35, </volume> <pages> pp. 380-392, </pages> <year> 1987. </year>
Reference-contexts: However, the Hopfield model has an exceedingly low capacity when storing correlated patterns that have overlapping structures [20]. The Hopfield model has been extended to deal with correlated patterns <ref> [25] </ref>, and Diederich and Opper [10] proposed a local learning rule to acquire the necessary weights iteratively. The local learning rule used by them is similar to the perceptron learning rule. Thus, it appears that such a scheme for dealing with correlated patterns should suffer from catastrophic interference.
Reference: [26] <author> C.A. Kortge, </author> <title> "Episodic memory in connectionist networks," </title> <booktitle> in Proceedings of the Twelfth Annual Conference of the Cognitivie Science Society, </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum, </publisher> <pages> pp. 764-771, </pages> <year> 1990. </year>
Reference-contexts: Many subsequent studies attempt to address the problem. Most of proposals amount to reducing overlapping in hidden layer representations by some techniques of orthogonalization, which have been used long ago for reducing crosstalks in associative memories (see Kortge <ref> [26] </ref>, French [13], [14], Kruschke [27], Sloman and Rumelhart [39], Sharkey and Sharkey [37]). Most of these proposals are verified only by small scale simulations, which, together with the lack of rigorous analysis, make it difficult to judge to what extent these proposed methods work.
Reference: [27] <author> J.K. Kruschke, "ALCOVE: </author> <title> An exemplar-based model of category learning," </title> <journal> Psychol. Rev., </journal> <volume> vol. 99, </volume> <pages> pp. 22-44, </pages> <year> 1992. </year>
Reference-contexts: Many subsequent studies attempt to address the problem. Most of proposals amount to reducing overlapping in hidden layer representations by some techniques of orthogonalization, which have been used long ago for reducing crosstalks in associative memories (see Kortge [26], French [13], [14], Kruschke <ref> [27] </ref>, Sloman and Rumelhart [39], Sharkey and Sharkey [37]). Most of these proposals are verified only by small scale simulations, which, together with the lack of rigorous analysis, make it difficult to judge to what extent these proposed methods work.
Reference: [28] <author> K.S. Lashley, </author> <title> "The problem of serial order in behavior," in Cerebral mechanisms in behavior, L.A. </title> <editor> Jeffress, Ed. </editor> <address> New York: </address> <publisher> Wiley & Sons, </publisher> <pages> pp. 112-146, </pages> <year> 1951. </year>
Reference-contexts: 1 The work described in this paper was supported in part by the NSF grants IRI-9211419, IRI-9423312, and equipment grant CDA-9413962, and the ONR grant N00014-93-1-0335. 2 I. Introduction One of the fundamental aspects of natural intelligence is the ability to process temporal information <ref> [28] </ref>. Learning and recalling temporal patterns are closely associated with our ability to perceive and generate body movements, speech and language, music, etc. A considerable body of neural network literature is devoted to studying temporal pattern generation (see [29] and [41] for reviews).
Reference: [29] <author> R.P. Lippmann, </author> <title> "Review of neural networks for speech recognition," </title> <journal> Neural Comput., </journal> <volume> vol. 1, </volume> <pages> pp. 1-38, </pages> <year> 1989. </year>
Reference-contexts: Learning and recalling temporal patterns are closely associated with our ability to perceive and generate body movements, speech and language, music, etc. A considerable body of neural network literature is devoted to studying temporal pattern generation (see <ref> [29] </ref> and [41] for reviews). These models generally treat a temporal pattern as a sequence of discrete patterns, called a temporal sequence. Most of the models are based on either multilayer perceptrons with backpropagation training or the Hopfield model of associative memory.
Reference: [30] <author> M. McCloskey and N.J. Cohen, </author> <title> "Catastrophic interference in connectionist networks: The sequential learning problem," </title> <journal> Psychol. of Learning and Motivat., </journal> <volume> vol. 24, </volume> <pages> pp. 109-165, </pages> <year> 1989. </year>
Reference-contexts: It turns out that incremental learning is particularly challenging to obtain. In multilayer perceptrons, it has been recognized that the network exhibits so called catastrophic interference, whereby later training disrupts the traces of previous training. It was pointed out by Grossberg [18], and systematically revealed by McCloskey and Cohen <ref> [30] </ref> and Ratcliff [34]. Many subsequent studies attempt to address the problem.
Reference: [31] <author> G.A. Miller, </author> <title> "The magical number seven, plus or minus two: Some limits on our capacity for processing information," </title> <journal> Psychol. Rev., </journal> <volume> vol. 63, </volume> <pages> pp. 81-97, </pages> <year> 1956. </year>
Reference-contexts: It requires further investigation to incorporate the chunking idea of Wang and Arbib with the anticipation model so that the latter can acquire sequences whose set degree is much greater than r. Chunking is one of the fundamental characteristics of human information processing <ref> [31] </ref>, [38]. Though the present model and the model of Wang and Arbib [43] have addressed some aspects of chunking, the general issue of automatic chunking is very challenging and remains unsolved. It is not even clear what constitutes a chunk in general. <p> The anticipation model, through its mechanism of context learning, provides a physical basis for forming such chunks. On the other hand, this definition of a chunk does not capture the richness of general chunking. We understand that chunking depends critically on the STM capacity <ref> [31] </ref>. Different people, however, may have different ways of chunking the same sequence in order to overcome STM limitations and memorize the sequence. Chunking also depends on general knowledge.
Reference: [32] <author> M.C. Mozer, </author> <title> "Neural net architectures for temporal sequence processing," in Predicting the future and understanding the past, </title> <editor> A. Weigend and N. Gershenfeld, Ed. </editor> <address> Redwood City, CA: </address> <publisher> Addison-Wesley, </publisher> <pages> pp. 243-264, </pages> <year> 1993. </year>
Reference-contexts: The basic idea for the former class of models is to view a temporal sequence as a set of associations between consecutive components, and learn these associations as input-output pairs [23], [11], <ref> [32] </ref>. To deal with temporal dependencies beyond immediate predecessors, part of the input layer is used to keep a blended form of history, behaving as short-term memory (STM). Similarly, for temporal recall models based on the Hopfield associative memory, a temporal sequence is viewed as associations between consecutive components.
Reference: [33] <author> J.B. Pollack, </author> <title> "The induction of dynamic recognizers," </title> <journal> Machine Learning, </journal> <volume> vol. 7, </volume> <pages> pp. 227-252, </pages> <year> 1991. </year>
Reference-contexts: For context recognition, the system can be extended to recognizing similar contexts once a context is learned (see [42], [43] for more discussions). Another level of generalization concerns with learning an underlying temporal structure, such as a grammar, from its exemplar sequences (see [11], <ref> [33] </ref> for such studies). This kind of generalization is currently beyond the scope of the anticipation model. Yet another form of generalization has to do timing, or temporal duration. Wang and Arbib [43] have addressed this issue, and their proposed methods can be directly incorporated into the anticipation model.
Reference: [34] <author> R. Ratcliff, </author> <title> "Connectionist models of recognition memory: Constraints imposed by learning and forgetting function," </title> <journal> Psychol. Rev., </journal> <volume> vol. 97, </volume> <pages> pp. 285-308, </pages> <year> 1990. </year>
Reference-contexts: In multilayer perceptrons, it has been recognized that the network exhibits so called catastrophic interference, whereby later training disrupts the traces of previous training. It was pointed out by Grossberg [18], and systematically revealed by McCloskey and Cohen [30] and Ratcliff <ref> [34] </ref>. Many subsequent studies attempt to address the problem.
Reference: [35] <author> W.A. Rodriguez, </author> <title> L.S. Borbely, and R.S. Garcia, "Attenuation by contextual cues of retroactive interference of a conditional discrimination in rats," Animal Learn. </title> <journal> Behav., </journal> <volume> vol. 21, </volume> <pages> pp. 101-105, </pages> <year> 1993. </year>
Reference-contexts: Retroactive interference has been well documented in psychology [9], which occurs when learning a later event interferes with the recall of earlier information. In general, the similarity between the current event and memory items is responsible for retroactive interference [2], [8], [4]. Retroactive interference exists in animals as well <ref> [35] </ref>. The existence of retroactive interference suggests that events are not independently stored in the brain, and related events are somehow associated in the memory.
Reference: [36] <author> V. Ruiz de Angulo and C. Torras, </author> <title> "On-line learning with minimal degradation in feedforward networks," </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> vol. 6, </volume> <pages> pp. 657-668, </pages> <year> 1995. </year>
Reference-contexts: Contrasting capacity-independent incremental learning, catastrophic interference would require simultaneous retraining of the entire memory when a new item is to be learned. The cost of retraining when catastrophic interference occurs is prohibitive if the size of memory is not so small. Ruiz de Angulo and Torras <ref> [36] </ref> presented a study on sequential learning of multilayer perceptrons, and reported that their model can learn sequentially several most recent patterns. Although it is a better result than the original multilayer perceptrons, their model cannot be used to support a sizable memory.
Reference: [37] <author> N.E. Sharkey and A.J.C. Sharkey, </author> <title> "Understanding catastrophic interference in neural nets," </title> <type> Technical Report CS-94-4, </type> <institution> Department of Computer Science, University of Sheffield, </institution> <year> 1994. </year>
Reference-contexts: Most of proposals amount to reducing overlapping in hidden layer representations by some techniques of orthogonalization, which have been used long ago for reducing crosstalks in associative memories (see Kortge [26], French [13], [14], Kruschke [27], Sloman and Rumelhart [39], Sharkey and Sharkey <ref> [37] </ref>). Most of these proposals are verified only by small scale simulations, which, together with the lack of rigorous analysis, make it difficult to judge to what extent these proposed methods work.
Reference: [38] <author> H.A. Simon, </author> <title> "How big is a chunk?" Science, </title> <journal> vol. </journal> <volume> 183, </volume> <pages> pp. 482-488, </pages> <year> 1974. </year>
Reference-contexts: It requires further investigation to incorporate the chunking idea of Wang and Arbib with the anticipation model so that the latter can acquire sequences whose set degree is much greater than r. Chunking is one of the fundamental characteristics of human information processing [31], <ref> [38] </ref>. Though the present model and the model of Wang and Arbib [43] have addressed some aspects of chunking, the general issue of automatic chunking is very challenging and remains unsolved. It is not even clear what constitutes a chunk in general.
Reference: [39] <author> S.A. Sloman and D.E. Rumelhart, </author> <title> "Reducing interference in distributed memories through episodic gating," in From learning theory to connectionist theory: Essays in honor of William K. Estes, A.F. Healy, S.M. Kosslyn, and R.M. </title> <editor> Shiffrin, Ed. </editor> <address> Hillsdale, NJ: </address> <publisher> Erlbaum, </publisher> <pages> pp. 227-248, </pages> <year> 1992. </year>
Reference-contexts: Many subsequent studies attempt to address the problem. Most of proposals amount to reducing overlapping in hidden layer representations by some techniques of orthogonalization, which have been used long ago for reducing crosstalks in associative memories (see Kortge [26], French [13], [14], Kruschke [27], Sloman and Rumelhart <ref> [39] </ref>, Sharkey and Sharkey [37]). Most of these proposals are verified only by small scale simulations, which, together with the lack of rigorous analysis, make it difficult to judge to what extent these proposed methods work.
Reference: [40] <author> H. Sompolinsky and I. Kanter, </author> <title> "Temporal association in asymmetric neural networks," </title> <journal> Phys. Rev. Lett., </journal> <volume> vol. 57, </volume> <pages> pp. 2861-2864, </pages> <year> 1986. </year>
Reference-contexts: Similarly, for temporal recall models based on the Hopfield associative memory, a temporal sequence is viewed as associations between consecutive components. These associations are stored in extended versions of the Hopfield model <ref> [40] </ref>, [6], [21]. To deal with longer temporal dependencies, high-order networks have been proposed [19]. One of the main problems with these two classes of model lies in their difficulty in retrieving complex temporal sequences, where the same part may occur many times in the sequence.
Reference: [41] <author> D.L. Wang, </author> <title> "Temporal pattern processing," in Handbook of brain theory and neural networks, </title> <address> M.A. Arbib, Ed. Cambridge, MA: </address> <publisher> MIT Press, to appear, </publisher> <year> 1995. </year>
Reference-contexts: Learning and recalling temporal patterns are closely associated with our ability to perceive and generate body movements, speech and language, music, etc. A considerable body of neural network literature is devoted to studying temporal pattern generation (see [29] and <ref> [41] </ref> for reviews). These models generally treat a temporal pattern as a sequence of discrete patterns, called a temporal sequence. Most of the models are based on either multilayer perceptrons with backpropagation training or the Hopfield model of associative memory. <p> In multilayer perceptrons, a blended form of STM becomes increasingly ambiguous when temporal dependencies increase [3]. The use of high-order units in the Hopfield model entails a huge number of connections when long range temporal dependencies appear, or the model faces ambiguities <ref> [41] </ref>. More recently, Bradski et al. [5] proposed an STM model, and showed that both recency and primacy can be captured by such a model. In addition, their model creates new representations for repeated occurrences of the same symbol, thus capable of encoding complex sequences to a certain extent. <p> But, we point out that rate invariance, which refers to generalization to the global scaling of component duration, is a problem yet to be solved by neural networks (see also <ref> [41] </ref>) The basic idea of anticipation and the structure of the network have been argued to be plausible at the cognitive and the neural levels [44], and the discussions will not be repeated here. As stated previously, the interference exhibited in the anticipation model is generally consistent with retroactive interference.
Reference: [42] <author> D.L. Wang and M.A. Arbib, </author> <title> "Complex temporal sequence learning based on short-term memory," </title> <journal> Proc. IEEE, </journal> <volume> vol. 78, </volume> <pages> pp. 1536-1543, </pages> <year> 1990. </year>
Reference-contexts: Baram's model uses second-order synapses to store the temporal associations between consecutive vectors in a sequence, and the model deals only with sequences that contain no repeating vectors. Based on the idea of using STM for resolving ambiguities, Wang and Arbib <ref> [42] </ref> proposed a model for learning to recognize and generate complex temporal sequences. With an STM model, a complex sequence is acquired by a learning rule that associates the activity distribution in STM with a context detector (for a rigorous definition see Section II.A). <p> In sequence generation, the system can maintain relative timing among the components while changing overall rate. Recently we proposed an anticipation model for temporal pattern generation [44]. Similar to Wang and Arbib <ref> [42] </ref>, [43], an STM model is used for maintaining a context which is stored by a context detector. In learning a temporal sequence, the model actively anticipates the next 3 component. When the anticipation is correct, the model does nothing and continues to learn the rest of the sequence. <p> Finally, Section VI provides some general discussions about the anticipation model. II. Anticipation Model of Temporal Learning A. Terminology We follow the terminology introduced by Wang and Arbib <ref> [42] </ref>. Sequences are defined over a symbol set G, which contains all possible symbols, or spatial (static) patterns. Sequence S of length N over G is defined as p 1 -p 2 -...-p N , where p i (1 i N) G is a component of S. <p> The same detector may be used for anticipating the same symbol that occurs many times in a sequence. As a result, the system needs fewer detectors to learn complex sequences than the model of Wang and Arbib <ref> [42] </ref>, [43]. 6 III. Incremental Learning The previous study on the anticipation model is mainly concerned with the learning and storage of a single sequence [44]. A preliminary simulation presented in [44] indicates that the model has the ability of learning two sequences sequentially. <p> If desired, generalization can be integrated at the level of single component recognition and context recognition. For single component recognition, generalization can result directly from spatial pattern recognition. For context recognition, the system can be extended to recognizing similar contexts once a context is learned (see <ref> [42] </ref>, [43] for more discussions). Another level of generalization concerns with learning an underlying temporal structure, such as a grammar, from its exemplar sequences (see [11], [33] for such studies). This kind of generalization is currently beyond the scope of the anticipation model.
Reference: [43] <author> D.L. Wang and M.A. Arbib, </author> <title> "Timing and chunking in processing temporal order," </title> <journal> IEEE Trans. Syst. Man Cybern., </journal> <volume> vol. 23, </volume> <pages> pp. 993-1009, </pages> <year> 1993. </year>
Reference-contexts: After successful training, a beginning part of the sequence forms the context for activating the next component, and the newly activated component joins STM to form a context for activating yet a next component. This process continues until the entire sequence is generated. A later model <ref> [43] </ref> addressed the issues of time warping and chunking of subsequences. In particular, sequences can be recognized in a hierarchical manner and without being affected by presentation speed. Hierarchical recognition enables the system to recognize sequences whose temporal dependencies are much longer than the STM capacity. <p> In sequence generation, the system can maintain relative timing among the components while changing overall rate. Recently we proposed an anticipation model for temporal pattern generation [44]. Similar to Wang and Arbib [42], <ref> [43] </ref>, an STM model is used for maintaining a context which is stored by a context detector. In learning a temporal sequence, the model actively anticipates the next 3 component. When the anticipation is correct, the model does nothing and continues to learn the rest of the sequence. <p> The same detector may be used for anticipating the same symbol that occurs many times in a sequence. As a result, the system needs fewer detectors to learn complex sequences than the model of Wang and Arbib [42], <ref> [43] </ref>. 6 III. Incremental Learning The previous study on the anticipation model is mainly concerned with the learning and storage of a single sequence [44]. A preliminary simulation presented in [44] indicates that the model has the ability of learning two sequences sequentially. <p> After retraining them sequentially, all of the first 96 sequences had been learned successfully. To examine the amount of detector sharing, let us compare the detector use in the anticipation model with one in which no detector is shared by different components (see for example <ref> [43] </ref>). Without detector sharing, the number of detectors needed to acquire all of the 97 sequences in Table 2 is S i=1 97 |S i | 97, which equals 2520. As stated earlier, the anticipation model needed 1,088 detectors to learn all of the sequences. <p> This is implemented by introducing a layer of chunk detectors which operates in a way similar to context detection. The dual architecture self-organizes to perform chunking. It is interesting to compare the present chunking method with the hierarchical model of temporal sequence recognition of Wang and Arbib <ref> [43] </ref>. In their model, a sequence in a layer becomes a basic component in the next layer, and using this idea of chunking they show that detectors in a higher layer can recognize sequences much longer than the basic capacity of STM. <p> Chunking is one of the fundamental characteristics of human information processing [31], [38]. Though the present model and the model of Wang and Arbib <ref> [43] </ref> have addressed some aspects of chunking, the general issue of automatic chunking is very challenging and remains unsolved. It is not even clear what constitutes a chunk in general. In this paper, a chunk corresponds to a repeated subsequence. <p> If desired, generalization can be integrated at the level of single component recognition and context recognition. For single component recognition, generalization can result directly from spatial pattern recognition. For context recognition, the system can be extended to recognizing similar contexts once a context is learned (see [42], <ref> [43] </ref> for more discussions). Another level of generalization concerns with learning an underlying temporal structure, such as a grammar, from its exemplar sequences (see [11], [33] for such studies). This kind of generalization is currently beyond the scope of the anticipation model. <p> This kind of generalization is currently beyond the scope of the anticipation model. Yet another form of generalization has to do timing, or temporal duration. Wang and Arbib <ref> [43] </ref> have addressed this issue, and their proposed methods can be directly incorporated into the anticipation model.

References-found: 43

