URL: http://ciir.cs.umass.edu/info/psfiles/tepubs/aaai94_soderland.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/te.html
Root-URL: 
Email: soderlan@cs.umass.edu lehnert@cs.umass.edu  
Title: Corpus-Driven Knowledge Acquisition for Discourse Analysis  
Author: Stephen Soderland and Wendy Lehnert 
Address: Amherst, MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  
Date: 1994  
Note: Proceedings of the Twelfth National Conference on Artificial Intelligence,  
Abstract: The availability of large on-line text corpora provides a natural and promising bridge between the worlds of natural language processing (NLP) and machine learning (ML). In recent years, the NLP community has been aggressively investigating statistical techniques to drive part-of-speech taggers, but application-specific text corpora can be used to drive knowledge acquisition at much higher levels as well. In this paper we will show how ML techniques can be used to support knowledge acquisition for information extraction systems. It is often very difficult to specify an explicit domain model for many information extraction applications, and it is always labor intensive to implement hand-coded heuristics for each new domain. We have discovered that it is nevertheless possible to use ML algorithms in order to capture knowledge that is only implicitly present in a representative text corpus. Our work addresses issues traditionally associated with discourse analysis and intersentential inference generation, and demonstrates the utility of ML algorithms at this higher level of language analysis. The benefits of our work address the portability and scalability of information extraction (IE) technologies. When hand-coded heuristics are used to manage discourse analysis in an information extraction system, months of programming effort are easily needed to port a successful IE system to a new domain. We will show how ML algorithms can reduce this development time to a few days of automated corpus analysis without any resulting degradation of overall system performance. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ayuso, D.; Boisen, S.; Fox, H.; Gish, H.; Ingria, R.; and Weishedel, R. </author> <year> 1992. </year> <title> BBN: Description of the PLUM System as Used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference, </booktitle> <pages> 169-176. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Cardie, C. </author> <year> 1993. </year> <title> A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentence Analysis. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 798-803. </pages>
Reference: <author> Dolan, C. P.; Goldman, S. R.; Cuda, T. V.; Naka-mura, A. M. </author> <year> 1991. </year> <title> Hughes Trainable Text Skimmer: Description of the TTS System as used for MUC-3. </title> <booktitle> In Proceedings of the Third Message Understanding Conference. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Test Results The performance of Wrap-Up compared well with that of the official UMass/Hughes MUC-5 system, where output from the CIRCUS sentence analyzer was sent to TTG (Trainable Template Generator), a discourse component based on the Trainable Text Skimmer from the Hughes Research Laboratories <ref> (Dolan, et al. 1991, Lehnert et al. 1993) </ref>. Acquisition of domain knowledge by machine learning was at the heart of the TTG system, but it didn't go as far as Wrap-Up in being fully trainable.
Reference: <author> Hobbes, J.R.; Appelt, D.; Mabry, T.; Bear, J.; Israel, D. </author> <year> 1992. </year> <title> SRI International: Description of the Faustus System Used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference, </booktitle> <pages> 268-275. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Lehnert, W. </author> <year> 1990. </year> <title> Symbolic/Subsymbolic Sentence Analysis: Exploiting the Best of Two Worlds. </title> <booktitle> Advances in Connectionist and Neural Computation Theory. </booktitle> <volume> vol. 1., </volume> <pages> 151-158. </pages> <address> Norwood, NJ: </address> <publisher> Ablex Publishing. </publisher>
Reference-contexts: Wrap-Up transforms this set of tokens, discarding information judged irrelevant to the domain, merging tokens with related information, adding pointers between tokens, and adding inferred tokens and default slot values. Wrap-Up was tested using output extracted by the University of Massachusetts CIRCUS sentence analyzer <ref> (Lehnert 1990, Lehnert et al. 1992a, 1992b) </ref>, although it could be adapted to any sentence analyzer which uses linguistic patterns for extraction.
Reference: <author> Lehnert, W.; Cardie, C.; Fisher, D.; McCarthy, J.; Riloff, E.; Soderland, S. </author> <year> 1992a. </year> <title> University of Mas-sachusetts: MUC-4 Test Results and Analysis, 151-158. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Lehnert, W.; Cardie, C.; Fisher, D.; McCarthy, J.; Riloff, E.; Soderland, S. </author> <year> 1992b. </year> <title> University of Mas-sachusetts: Description of the CIRCUS System as Used for MUC-4, 282-288. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference. </booktitle> <publisher> Morgan Kauf-mann Publishers. </publisher>
Reference: <author> Lehnert, W.; Cardie, C.; Fisher, D.; McCarthy, J.; Riloff, E.; Soderland, S.; Feng, F.; Dolan, C.; Gold-man, S. </author> <year> 1993. </year> <title> University of Massachusetts: Description of the CIRCUS System as Used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Lehnert, W.G., and Sundheim, B. </author> <year> 1991. </year> <title> A Performance Evaluation of Text Analysis Technologies. </title> <journal> AI Magazine: </journal> <pages> 81-94. </pages>
Reference-contexts: Recent performance evaluations sponsored by ARPA have shown fl This research was supported by NSF Grant no. EEC-9209623, State/Industry/University Cooperative Research on Intelligent Information Retrieval. that a number of different parsing strategies can handle sentence-level information extraction with varying degrees of success <ref> (Lehnert and Sundheim 1991, Sund-heim 1991) </ref>. This paper will concentrate on the discourse level, by which we mean all processing that takes place after sentence analysis. <p> Test Results The performance of Wrap-Up compared well with that of the official UMass/Hughes MUC-5 system, where output from the CIRCUS sentence analyzer was sent to TTG (Trainable Template Generator), a discourse component based on the Trainable Text Skimmer from the Hughes Research Laboratories <ref> (Dolan, et al. 1991, Lehnert et al. 1993) </ref>. Acquisition of domain knowledge by machine learning was at the heart of the TTG system, but it didn't go as far as Wrap-Up in being fully trainable.
Reference: <author> Quinlan, J.R. </author> <year> 1986. </year> <title> Induction of Decision Trees. </title> <booktitle> Machine Learning (1): </booktitle> <pages> 81-106. </pages>
Reference-contexts: Applying Decision Tree Algorithms to the Problem Wrap-Up breaks discourse processing into a number of small decisions and builds a separate ID3 decision tree for each <ref> (Quinlan 1986) </ref>. The Lithography-Equipment-Links tree is typical of the 91 decision trees used for the micro-electronics domain. During discourse processing, Wrap-Up encodes an instance for each pair of extracted lithography and equipment objects, such as UV lithography and XLS stepper in the previous example. <p> ID3 recursively selects features to partition tree. The highlighted path is for an instance with stepper equipment, UV lithography, and equipment mentioned one sentence earlier than lithography. the training instances according to an information gain metric <ref> (p.89-90 Quinlan 1986) </ref>. The feature chosen as root of this tree is equipment-type. This feature alone is sufficient to classify instances with equipment-type such as modular-equipment, radiation-source, or etching-system, which have only negative instances.
Reference: <author> Riloff, E. </author> <year> 1993. </year> <title> Automatically Constructing a Dictionary for Information Extraction Tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 811-816. </pages>
Reference: <author> Sundheim, B. </author> <year> 1991. </year> <booktitle> Proceedings of the Third Message Understanding Conference (MUC-3). </booktitle> <publisher> Morgan Kauf-mann Publishers. </publisher>
Reference-contexts: Recent performance evaluations sponsored by ARPA have shown fl This research was supported by NSF Grant no. EEC-9209623, State/Industry/University Cooperative Research on Intelligent Information Retrieval. that a number of different parsing strategies can handle sentence-level information extraction with varying degrees of success <ref> (Lehnert and Sundheim 1991, Sund-heim 1991) </ref>. This paper will concentrate on the discourse level, by which we mean all processing that takes place after sentence analysis.
Reference: <author> Sundheim, B. </author> <year> 1992. </year> <booktitle> Proceedings of the Fourth Message Understanding Conference (MUC-4). </booktitle> <publisher> Morgan Kauf-mann Publishers. </publisher>
Reference: <author> Sundheim, B. </author> <year> 1993. </year> <booktitle> Proceedings of the Fifth Message Understanding Conference (MUC-5). </booktitle> <publisher> Morgan Kauf-mann Publishers. </publisher>
References-found: 14

