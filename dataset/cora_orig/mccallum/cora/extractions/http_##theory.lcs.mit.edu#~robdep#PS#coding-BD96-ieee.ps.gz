URL: http://theory.lcs.mit.edu/~robdep/PS/coding-BD96-ieee.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~robdep/papers.html
Root-URL: 
Email: fcarblu,robdepg@dia.unisa.it  
Title: New Bounds on the Expected Length of One-to-One Codes  
Author: Carlo Blundo and Roberto De Prisco 
Keyword: Index Terms Source coding, one-to-one codes, non-prefix codes.  
Address: 84081 Baronissi (SA), Italy.  
Affiliation: Dipartimento di Informatica ed Applicazioni Universita di Salerno,  
Abstract: In this correspondence we provide new bounds on the expected length L of a binary one-to-one code for a discrete random variable X with entropy H. We prove that L H log(H + 1) H log(1 + 1=H). This bound improves on previous results. Furthermore, we provide upper bounds on the expected length of the best code as function of H and the most likely source letter probability.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon and A. Orlitsky, </author> <title> "A Lower Bound on the Expected Length of One-to-One Codes", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-40, no. 5, </volume> <pages> pp. 1670-1672, </pages> <month> Sept. </month> <year> 1994. </year> <month> 10 </month>
Reference-contexts: It is easy to see that the length of the i-th shortest codeword is n i = blog ic: (2) All logarithms in this paper are to the base 2. We denote by L * the expected length of the best f0;1g * -encoding. Recently, Alon and Orlitsky <ref> [1] </ref> proved that L * H log (H + 1) log e: (3) Wyner [5] proved that L * H: (4) Bound (4) is achieved by the constant random variable. In this correspondence we prove that H L * + (L * + 1)H 1 extending the result of [4]. <p> The authors wish to thank Alon Orlitsky and one of the referees for useful comments on this correspondence. Alon Orlitsky pointed out that the bound of Theorem 2.2 can be obtained using the derivation of Lemma 1 in <ref> [1] </ref>, noticing that H = log E + (E 1) log (E=(E 1)), where E = E (X), and continuing as in that proof.
Reference: [2] <author> S. K. Leung-Yan-Cheong and T. M. </author> <title> Cover, "Some Equivalences Between Shannon Entropy and Kolmogorov Complexity", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-24, no. 3, </volume> <pages> pp. 331-338, </pages> <month> May </month> <year> 1978. </year>
Reference-contexts: It is easy to see that the length of the i-th shortest codeword is n i = log i + 1 : (8) We denote by L the expected length of the best f0;1g + -encoding. Leung-Yan-Cheong and Cover <ref> [2] </ref> proved the following bound L H log fl (H + 1) 6 (9) where log fl x 4 = log x + log log x + stopping at last positive term.
Reference: [3] <author> C. Shannon, </author> <title> "A Mathematical Theory of Communication", </title> <journal> Bell Syst. Tech. J., </journal> <volume> vol. 27, </volume> <pages> pp. 379-423, 623-656, </pages> <year> 1948 </year>
Reference-contexts: The entropy of X is denoted H (X) or simply H when X is clear from the context. Shannon <ref> [3] </ref> proved that the minimum expected length L pre of a prefix-free encoding of X satisfies H L pre H + 1. fl This work is partially supported by Italian Ministry of University and Research (M.U.R.S.T.) and by National Council for Research (C.N.R.). 1 Prefix-free codes are very useful as they
Reference: [4] <author> E. I. Verriest, </author> <title> "An Achievable Bound for Optimal Noiseless Coding of a Random Variable", </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> vol. IT-32, no. 4, </volume> <pages> pp. 592-594, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: In this correspondence we prove that H L * + (L * + 1)H 1 extending the result of <ref> [4] </ref>. Previous bound is achievable for any value of H. We also provide an explicit lower bound on L * , L * H log (H + 1) H log 1 + H : (6) improving on (3). We denote by H the binary entropy. <p> Alon and Orlitsky's bound (3) improves on previous bound (any lower bound for L * holds for L as well). Verriest <ref> [4] </ref> proved that when H 1 H L 1 + H 1 Since L L pre we have that L H + 1: (11) The above bound is achieved by the constant random variable. Since L L * , bound (6) holds for L, too. <p> In Section 3 we address the case of f0;1g + -encodings providing the upper and the lower bounds on L. 2 f0;1g -encodings In this section we consider the case of encodings that do use the empty codeword. We employ the technique of <ref> [4] </ref> based on Lagrange's multipliers. We will prove that L * is related to the entropy H by H L * + (L * + 1)H 1 3 Before showing how to obtain (14) we need some technical lemmas. <p> j=0 2 (18) 1 X j2 j p 1 1 p 1 j 1 X j (1 p 1 ) j = p 1 Substituting (18) in (17) after some algebra we get H + p 1 Setting x = 1 p 1 we obtain H = 1 x In <ref> [4] </ref> it has been proved that the function y (x) = H (x)+x 1x , defined in the interval [0; 1 [ is invertible. It follows that for any given value of the entropy H there exists an unique solution in [0; 1 [ of the equation (20).
Reference: [5] <author> A. D. Wyner, </author> <title> "An Upper Bound on the Entropy Series", </title> <journal> Inform. Control, </journal> <volume> vol. 20, </volume> <pages> pp. 176-181, </pages> <year> 1972. </year>
Reference-contexts: We denote by L * the expected length of the best f0;1g * -encoding. Recently, Alon and Orlitsky [1] proved that L * H log (H + 1) log e: (3) Wyner <ref> [5] </ref> proved that L * H: (4) Bound (4) is achieved by the constant random variable. In this correspondence we prove that H L * + (L * + 1)H 1 extending the result of [4]. Previous bound is achievable for any value of H.
References-found: 5

