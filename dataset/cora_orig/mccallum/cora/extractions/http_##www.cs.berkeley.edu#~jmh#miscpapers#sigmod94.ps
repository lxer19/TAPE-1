URL: http://www.cs.berkeley.edu/~jmh/miscpapers/sigmod94.ps
Refering-URL: http://www.cs.berkeley.edu/~jmh/papers.html
Root-URL: 
Email: joey@cs.wisc.edu  
Title: Practical Predicate Placement  
Author: Joseph M. Hellerstein 
Address: Berkeley  
Affiliation: University of California,  
Abstract: Recent work in query optimization has addressed the issue of placing expensive predicates in a query plan. In this paper we explore the predicate placement options considered in the Montage DBMS, presenting a family of algorithms that form successively more complex and effective optimization solutions. Through analysis and performance measurements of Montage SQL queries, we classify queries and highlight the simplest solution that will optimize each class correctly. We demonstrate limitations of previously published algorithms, and discuss the challenges and feasibility of implementing the various algorithms in a commercial-grade system. 
Abstract-found: 1
Intro-found: 1
Reference: [BMSU86] <author> Francois Bancilhon, David Maier, Yehoshua Sagiv, and Jeffrey D. Ullman. </author> <title> Magic Sets and other Strange Ways to Implement Logic Programs. </title> <booktitle> In Proc. 5th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, </booktitle> <pages> pages 1-15, </pages> <address> Cam-bridge, </address> <month> March </month> <year> 1986. </year>
Reference-contexts: One can do per-function caching instead, as proposed in [Jhi88] and [HS93a]. Function or predicate caches can be limited in size, using any of a variety of replacement schemes. Queries can be rewritten with Magic-Sets techniques <ref> [BMSU86, MFPR90] </ref> to avoid the issue of caching entirely, at the expense of extra joins and common subexpressions. Such alternatives do not form a focus of this paper, as this space of possible implementations is large and orthogonal to the space of optimization techniques explored here.
Reference: [CGK89] <author> Danette Chimenti, Ruben Gamboa, and Ravi Krishnamurthy. </author> <title> Towards an Open Architecture for LDL. </title> <booktitle> In Proc. 15th International Conference on Very Large Data Bases, </booktitle> <address> Amsterdam, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: A variety of papers have dealt with this problem in various forms, and two basic algorithms for query planning have resulted, one first described in <ref> [CGK89] </ref>, another in [Hel92]. As we will see in this paper, each of these approaches has limitations in practice, which necessitate some compromises. We add to these rather complex algorithms a family of simpler heuristics, illustrating for each one the class of queries which it can effectively optimize. <p> All performance numbers reported below are relative, not absolute. 3 Predicate Placement Algorithms Revisited Two basic approaches have been published for handling expensive predicate placement. The first approach was pioneered in the LDL logic database system <ref> [CGK89] </ref>, and was later proposed for an extended relational model in [YKY + 91]. We refer to this as the LDL algorithm. The other approach, called Predicate Migration, is outlined in [HS93a], and described in full in [Hel92]. Neither algorithm actually produces optimal plans in all scenarios.
Reference: [CS93] <author> Surajit Chaudhuri and Kyuseok Shim. </author> <title> Query Optimization in the Presence of Foreign Functions. </title> <booktitle> In Proc. 19th International Conference on Very Large Data Bases, </booktitle> <pages> pages 526-541, </pages> <address> Dublin, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Neither system considered pulling up subquery predicates from their lowest eligible position. [LH93] An orthogonal issue related to predicate placement is the problem of rewriting predicates into more efficient forms <ref> [CS93, CYY + 92] </ref>. In such semantic optimization work, the focus is on rewriting expensive predicates in terms of other, cheaper predicates. This is similar to the query rewrite facility of Starburst [PHH92].
Reference: [CYY + 92] <author> Hanxiong Chen, Xu Yu, Kazunori Yam-aguchi, Hiroyuki Kitagawa, Nobuo Ohbo, and Yuzuru Fujiwara. </author> <title> Decomposition | An Approach for Optimizing Queries Including ADT Functions. </title> <journal> Information Processing Letters, </journal> <volume> 43(6) </volume> <pages> 327-333, </pages> <year> 1992. </year>
Reference-contexts: Neither system considered pulling up subquery predicates from their lowest eligible position. [LH93] An orthogonal issue related to predicate placement is the problem of rewriting predicates into more efficient forms <ref> [CS93, CYY + 92] </ref>. In such semantic optimization work, the focus is on rewriting expensive predicates in terms of other, cheaper predicates. This is similar to the query rewrite facility of Starburst [PHH92].
Reference: [DKS92] <author> Weimin Du, Ravi Krishnamurthy, and Ming-Chien Shan. </author> <title> Query Optimization in Heterogeneous DBMS. </title> <booktitle> In Proc. 18th International Conference on Very Large Data Bases, </booktitle> <address> Vancouver, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Linear cost models have been evaluated experimentally for nested-loop and merge joins, and were found to be relatively accurate estimates of the performance of a variety of commercial systems <ref> [DKS92] </ref>. Unfortunately, our strict linear cost model does not apply to joins in which the primary join predicate is expensive. In such joins, the cost formula has an additional term c p fRgfSg, where c p is the cost of evaluating the expensive join predicate on a tuple.
Reference: [Hel92] <author> Joseph M. Hellerstein. </author> <title> Predicate Migration: Optimizing Queries With Expensive Predi 10 cates. </title> <type> Technical Report Sequoia 2000 92/13, </type> <institution> University of California, Berkeley, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: A variety of papers have dealt with this problem in various forms, and two basic algorithms for query planning have resulted, one first described in [CGK89], another in <ref> [Hel92] </ref>. As we will see in this paper, each of these approaches has limitations in practice, which necessitate some compromises. We add to these rather complex algorithms a family of simpler heuristics, illustrating for each one the class of queries which it can effectively optimize. <p> The intention of this work is to guide query optimizer developers in choosing a practical solution whose implementation and performance complexity is suited to their application domain. As a reference point, we describe our experience implementing the Predicate Migration algorithm <ref> [Hel92, HS93a] </ref> and simpler heuristics in the Montage Object-Relational DBMS (formerly called Miro [Sto93]). We compare the performance of the various heuristics on different classes of queries, attempting to highlight the simplest solution that works for each class. <p> The first approach was pioneered in the LDL logic database system [CGK89], and was later proposed for an extended relational model in [YKY + 91]. We refer to this as the LDL algorithm. The other approach, called Predicate Migration, is outlined in [HS93a], and described in full in <ref> [Hel92] </ref>. Neither algorithm actually produces optimal plans in all scenarios. <p> Instead of the global cost model, Montage uses a more flexible estimate of selectivity that can be different for each input, and a simpler (non-"global") estimate of cost per tuple of each input. These modifications do not affect the Predicate Migration algorithm or the proofs of optimality in <ref> [Hel92] </ref>. Given two relations R and S, and a join predicate J of selectivity s over them, we represent the selectivity of J over R as s fSg, where fSg is the number of tuples that are passed into the join from S. <p> This join order chosen by PullRank is not a good one, however, and results in the poor performance shown in 6 but with the costly selection pulled to the top. 4.4 Predicate Migration The details of the Predicate Migration algorithm are presented in <ref> [Hel92] </ref> , and we only review them here. The Predicate Migration algorithm repeatedly applies the Series-Parallel Algorithm using Parallel Chains [MS79] to each root-to-leaf path in the plan tree until no progress is made.
Reference: [HS93a] <author> Joseph M. Hellerstein and Michael Stone-braker. </author> <title> Predicate Migration: Optimizing Queries With Expensive Predicates. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The intention of this work is to guide query optimizer developers in choosing a practical solution whose implementation and performance complexity is suited to their application domain. As a reference point, we describe our experience implementing the Predicate Migration algorithm <ref> [Hel92, HS93a] </ref> and simpler heuristics in the Montage Object-Relational DBMS (formerly called Miro [Sto93]). We compare the performance of the various heuristics on different classes of queries, attempting to highlight the simplest solution that works for each class. <p> The first approach was pioneered in the LDL logic database system [CGK89], and was later proposed for an extended relational model in [YKY + 91]. We refer to this as the LDL algorithm. The other approach, called Predicate Migration, is outlined in <ref> [HS93a] </ref>, and described in full in [Hel92]. Neither algorithm actually produces optimal plans in all scenarios. <p> Yajima et al. [YKY + 91] suc 3 cessfully integrate the LDL algorithm with an IK-KBZ optimizer, but they use an exhaustive mechanism that requires time exponential in the number of expensive selections. 3.2 Predicate Migration Revisited The Predicate Migration algorithm presented in <ref> [HS93a] </ref> only works when join costs fit a linear cost model. In this section we update that cost model, and illustrate how that affects the practical applicability of Predicate Migration. The "global" cost model presented in [HS93a] proved to be inaccurate at modelling query plans in practice, and was discarded in <p> number of expensive selections. 3.2 Predicate Migration Revisited The Predicate Migration algorithm presented in <ref> [HS93a] </ref> only works when join costs fit a linear cost model. In this section we update that cost model, and illustrate how that affects the practical applicability of Predicate Migration. The "global" cost model presented in [HS93a] proved to be inaccurate at modelling query plans in practice, and was discarded in Montage. In the global cost model, the selectivity of a join node has the same effect on both inputs to the join. In practice this is inaccurate. <p> The selectivity of each selection predicate (i.e. the percentage of tuples expected to satisfy the predicate) is similarly estimated, and selections over a given relation are ordered in ascending order of the metric rank = selectivity 1 cost : Such ordering is easily shown to be optimal for selections <ref> [HS93a] </ref>, and intuitively makes sense: the lower the selectivity of the predicate, the earlier we wish to apply it, since it will filter out many tuples. Similarly, the cheaper the predicate, the earlier we wish to apply it, since its benefits may be reaped at a low cost. <p> In Montage, we avoid this through a predicate caching scheme, similar to the one proposed in <ref> [HS93a] </ref>, but different in a few key ways. Contrary to the assertions of [HS93a], predicate caching is not a requirement for using Predicate Migration. Choosing to use predicate caching merely requires changes in rank calculations, to reflect the selectivity of a join on values rather than on tuples. <p> In Montage, we avoid this through a predicate caching scheme, similar to the one proposed in <ref> [HS93a] </ref>, but different in a few key ways. Contrary to the assertions of [HS93a], predicate caching is not a requirement for using Predicate Migration. Choosing to use predicate caching merely requires changes in rank calculations, to reflect the selectivity of a join on values rather than on tuples. <p> Note that it does not cache the results of the expensive function beard color (as proposed in <ref> [HS93a] </ref>); instead it caches the results of the entire predicate. This is important because the return types of functions within predicates may be arbitrarily large derived objects | in the case of subquery functions, for example, they may be sets. <p> In such cases predicate caching should be avoided. This optimization is planned for Montage, but has not been implemented yet. Other alternatives exist to our predicate caching implementation, though ours seems to perform reasonably well for our purposes. One can do per-function caching instead, as proposed in [Jhi88] and <ref> [HS93a] </ref>. Function or predicate caches can be limited in size, using any of a variety of replacement schemes. Queries can be rewritten with Magic-Sets techniques [BMSU86, MFPR90] to avoid the issue of caching entirely, at the expense of extra joins and common subexpressions. <p> Adapting the Predicate Migration algorithm for use in a commercial system required some re-working of its cost model. Particularly, it was found that the "global" cost model of <ref> [HS93a] </ref> was inapplicable in practice. This paper presents a more realistic model for costs and selectivities, and integrates that model into the Predicate Migration algorithm. Some roughness remains in our selectivity estimations, and in our cost estimations for expensive joins.
Reference: [HS93b] <author> Wei Hong and Michael Stonebraker. </author> <title> Optimization of Parallel Query Execution Plans in XPRS. Distributed and Parallel Databases, </title> <journal> An International Journal, </journal> <volume> 1(1) </volume> <pages> 9-32, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: In the course of the paper, we will be using SQL queries to demonstrate the strengths and limitations of our algorithms. The database schema for these queries is based on that of Hong and Stonebraker <ref> [HS93b] </ref>, with cardinalities scaled up by a factor of 10. All tuples are 100 bytes wide. Attributes whose names start with the letter `u' are unindexed, while all other attributes have B-tree indices defined over them.
Reference: [IK84] <author> Toshihide Ibaraki and Tiko Kameda. </author> <title> Optimal Nesting for Computing N-relational Joins. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 9(3) </volume> <pages> 482-502, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: This does not integrate well with a System R-style optimization algorithm, however, since LDL increases the number of joins to order, and System R's complexity is exponential in the number of joins. Thus [KZ88] proposes using the polynomial-time IK-KBZ <ref> [IK84, KBZ86] </ref> approach for optimizing the join order. Unfortunately, both the System R and IK-KBZ optimization algorithms consider only left-deep plan trees, and no left-deep plan tree can model the optimal plan tree of Figure 1.
Reference: [Jhi88] <author> Anant Jhingran. </author> <title> A Performance Study of Query Optimization Algorithms on a Database System Supporting Procedures. </title> <booktitle> In Proc. 14th International Conference on Very Large Data Bases, </booktitle> <address> Los Angeles, </address> <month> August-September </month> <year> 1988. </year>
Reference-contexts: In such cases predicate caching should be avoided. This optimization is planned for Montage, but has not been implemented yet. Other alternatives exist to our predicate caching implementation, though ours seems to perform reasonably well for our purposes. One can do per-function caching instead, as proposed in <ref> [Jhi88] </ref> and [HS93a]. Function or predicate caches can be limited in size, using any of a variety of replacement schemes. Queries can be rewritten with Magic-Sets techniques [BMSU86, MFPR90] to avoid the issue of caching entirely, at the expense of extra joins and common subexpressions.
Reference: [KBZ86] <author> Ravi Krishnamurthy, Haran Boral, and Carlo Zaniolo. </author> <title> Optimization of Nonrecursive Queries. </title> <booktitle> In Proc. 12th International Conference on Very Large Data Bases, </booktitle> <pages> pages 128-137, </pages> <address> Kyoto, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: This does not integrate well with a System R-style optimization algorithm, however, since LDL increases the number of joins to order, and System R's complexity is exponential in the number of joins. Thus [KZ88] proposes using the polynomial-time IK-KBZ <ref> [IK84, KBZ86] </ref> approach for optimizing the join order. Unfortunately, both the System R and IK-KBZ optimization algorithms consider only left-deep plan trees, and no left-deep plan tree can model the optimal plan tree of Figure 1.
Reference: [KZ88] <author> Ravi Krishnamurthy and Carlo Zaniolo. </author> <title> Optimization in a Logic Based Language for Knowledge and Data Intensive Applications. </title> <editor> In Joachim W. Schmidt, Stefano Ceri, and M. Missikoff, editors, </editor> <booktitle> Proc. International Conference on Extending Data Base Technology, Advances in Database Technology - EDBT '88. Lecture Notes in Computer Science, </booktitle> <volume> Volume 303, </volume> <pages> Venice, </pages> <address> March 1988. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: At this point, the LDL approach applies a traditional join-ordering optimizer to plan the rewritten query. This does not integrate well with a System R-style optimization algorithm, however, since LDL increases the number of joins to order, and System R's complexity is exponential in the number of joins. Thus <ref> [KZ88] </ref> proposes using the polynomial-time IK-KBZ [IK84, KBZ86] approach for optimizing the join order. Unfortunately, both the System R and IK-KBZ optimization algorithms consider only left-deep plan trees, and no left-deep plan tree can model the optimal plan tree of Figure 1.
Reference: [LH93] <author> Guy M. Lohman and Laura M. Haas. </author> <type> Personal correspondence, </type> <month> November </month> <year> 1993. </year>
Reference-contexts: Neither system considered pulling up subquery predicates from their lowest eligible position. <ref> [LH93] </ref> An orthogonal issue related to predicate placement is the problem of rewriting predicates into more efficient forms [CS93, CYY + 92]. In such semantic optimization work, the focus is on rewriting expensive predicates in terms of other, cheaper predicates.
Reference: [MFPR90] <author> Inderpal Singh Mumick, Sheldon J. Finkel-stein, Hamid Pirahesh, and Raghu Ramakr-ishnan. </author> <title> Magic is Relevant. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 247-258, </pages> <address> Atlantic City, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: One can do per-function caching instead, as proposed in [Jhi88] and [HS93a]. Function or predicate caches can be limited in size, using any of a variety of replacement schemes. Queries can be rewritten with Magic-Sets techniques <ref> [BMSU86, MFPR90] </ref> to avoid the issue of caching entirely, at the expense of extra joins and common subexpressions. Such alternatives do not form a focus of this paper, as this space of possible implementations is large and orthogonal to the space of optimization techniques explored here.
Reference: [MS79] <author> C. L. Monma and J.B. Sidney. </author> <title> Sequencing with Series-Parallel Precedence Constraints. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4 </volume> <pages> 215-224, </pages> <year> 1979. </year>
Reference-contexts: The Predicate Migration algorithm repeatedly applies the Series-Parallel Algorithm using Parallel Chains <ref> [MS79] </ref> to each root-to-leaf path in the plan tree until no progress is made.
Reference: [Nau93] <author> Jeff Naughton. </author> <booktitle> Presentation at Fifth International High Performance Transaction Workshop, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: The lesson to be learned here is that benchmarking is absolutely crucial to thoroughly debugging a query optimizer. It has been noted that a variety of commercial products still produce very poor plans even on simple queries <ref> [Nau93] </ref>. Thus benchmarks | particularly complex query benchmarks such as TPC-D [TPC93] | are critical debugging tools for DBMS developers. In our case, we were able to easily compare our Predicate Migration implementation against various heuristics, to ensure that Predicate Migration always did at least as well as the heuristics.
Reference: [PHH92] <author> Hamid Pirahesh, Joseph M. Hellerstein, and Waqar Hasan. </author> <title> Extensible/Rule-Based Query Rewrite Optimization in Starburst. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 39-48, </pages> <address> San Diego, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: In such semantic optimization work, the focus is on rewriting expensive predicates in terms of other, cheaper predicates. This is similar to the query rewrite facility of Starburst <ref> [PHH92] </ref>.
Reference: [SAC + 79] <author> Patricia G. Selinger, M. Astrahan, D. Cham-berlin, Raymond Lorie, and T. Price. </author> <title> Access Path Selection in a Relational Database Management System. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Boston, </address> <month> June </month> <year> 1979. </year>
Reference-contexts: In practice this is inaccurate. Consider an equi-join of two relations R and S on primary keys, where R has cardinality 100 and S has cardinality 1000. According to <ref> [SAC + 79] </ref>, the selectivity of this join is 1 1000 , because we can expect each tuple of R to find one match in S.
Reference: [Sha86] <author> Leonard D. Shapiro. </author> <title> Join Processing in Database Systems with Large Main Memories. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 11(3) </volume> <pages> 239-264, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: Recall that we treat traditional simple predicates as being of zero-cost; similarly here we ignore the CPU costs associated with joins. Taking this into account, the costs of merge and hash joins given in <ref> [Sha86] </ref> fit our criterion. 2 For nested-loop join with an indexed inner 2 Actually, we ignore the p S savings available in merge join due to buffering.
Reference: [SK91] <author> Michael Stonebraker and Greg Kemnitz. </author> <title> The POSTGRES Next-Generation Database Management System. </title> <journal> Communications of the ACM, </journal> <volume> 34(10), </volume> <year> 1991. </year>
Reference-contexts: In this section we discuss the implementation experience, and some issues which arose in our experiments. The Montage "Object-Relational" DBMS is based on the publicly available POSTGRES system <ref> [SK91] </ref>. 4 For Query 5, PullAll used up all available swap space and never completed. This happened because PullAll pulled the costly selection on t3 above the costly join predicate.
Reference: [Sto93] <author> Michael Stonebraker. </author> <title> The Miro DBMS. </title> <booktitle> In Proc. ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: As a reference point, we describe our experience implementing the Predicate Migration algorithm [Hel92, HS93a] and simpler heuristics in the Montage Object-Relational DBMS (formerly called Miro <ref> [Sto93] </ref>). We compare the performance of the various heuristics on different classes of queries, attempting to highlight the simplest solution that works for each class. Table 1 provides a quick reference to the algorithms, their applicability and limitations.
Reference: [TPC93] <author> TPC. </author> <title> TPC Benchmark T M D (Decision Support). Working Draft 6.0, Transaction Processing Performance Council, </title> <month> August </month> <year> 1993. </year>
Reference-contexts: The lesson to be learned here is that benchmarking is absolutely crucial to thoroughly debugging a query optimizer. It has been noted that a variety of commercial products still produce very poor plans even on simple queries [Nau93]. Thus benchmarks | particularly complex query benchmarks such as TPC-D <ref> [TPC93] </ref> | are critical debugging tools for DBMS developers. In our case, we were able to easily compare our Predicate Migration implementation against various heuristics, to ensure that Predicate Migration always did at least as well as the heuristics.
Reference: [Ull88] <author> Jeffrey D. Ullman. </author> <booktitle> Principles of Database and Knowledge-Base Systems, </booktitle> <volume> volume 1. </volume> <publisher> Computer Science Press, </publisher> <year> 1988. </year>
Reference-contexts: These two supposedly distinct approaches to data management are converging on similar sets of new problems in query optimization and execution. One of the major problems faced by these systems is that the common relational heuristic of "selection pushdown" (see e.g. <ref> [Ull88] </ref>) is no longer advantageous in all situations. Selection pushdown requires query processing to perform selections before performing joins. fl Current address: Department of Computer Sciences, 1210 W. Dayton St., Madison, WI, 53706. This work was initiated while the author was at the University of California, Berkeley.
Reference: [YKY + 91] <author> Kenichi Yajima, Hiroyuki Kitagawa, Kazunori Yamaguchi, Nobuo Ohbo, and Yuzura Fujiwara. </author> <title> Optimization of Queries Including ADT Functions. </title> <booktitle> In Proc. 2nd International Symposium on Database Systems for Advanced Applications, </booktitle> <pages> pages 366-373, </pages> <address> Tokyo, </address> <month> April </month> <year> 1991. </year> <month> 11 </month>
Reference-contexts: All performance numbers reported below are relative, not absolute. 3 Predicate Placement Algorithms Revisited Two basic approaches have been published for handling expensive predicate placement. The first approach was pioneered in the LDL logic database system [CGK89], and was later proposed for an extended relational model in <ref> [YKY + 91] </ref>. We refer to this as the LDL algorithm. The other approach, called Predicate Migration, is outlined in [HS93a], and described in full in [Hel92]. Neither algorithm actually produces optimal plans in all scenarios. <p> A System R optimizer can be modified to explore the space of bushy trees, but this increases the complexity of the LDL algorithm yet further. No known modification of the IK-KBZ optimizer can handle bushy trees. Yajima et al. <ref> [YKY + 91] </ref> suc 3 cessfully integrate the LDL algorithm with an IK-KBZ optimizer, but they use an exhaustive mechanism that requires time exponential in the number of expensive selections. 3.2 Predicate Migration Revisited The Predicate Migration algorithm presented in [HS93a] only works when join costs fit a linear cost model.
References-found: 24

