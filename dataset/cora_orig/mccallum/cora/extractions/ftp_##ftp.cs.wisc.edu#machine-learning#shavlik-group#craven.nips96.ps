URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/craven.nips96.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/craven.nips96.ps.abstract.html
Root-URL: 
Email: craven@cs.wisc.edu, shavlik@cs.wisc.edu  
Title: Extracting Tree-Structured Representations of Trained Networks  
Author: Mark W. Craven and Jude W. Shavlik 
Address: 1210 West Dayton St. Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Note: Appears in Advances in Neural Information Processing Systems, Vol. 8. MIT Press, Cambridge, MA, 1996.  
Abstract: A significant limitation of neural networks is that the representations they learn are usually incomprehensible to humans. We present a novel algorithm, Trepan, for extracting comprehensible, symbolic representations from trained neural networks. Our algorithm uses queries to induce a decision tree that approximates the concept represented by a given network. Our experiments demonstrate that Trepan is able to produce decision trees that maintain a high level of fidelity to their respective networks while being comprehensible and accurate. Unlike previous work in this area, our algorithm is general in its applicability and scales well to large net works and problems with high-dimensional input spaces.
Abstract-found: 1
Intro-found: 1
Reference: <author> Alexander, J. A. & Mozer, M. C. </author> <year> (1995). </year> <title> Template-based algorithms for connectionist rule extraction. </title> <editor> In Tesauro, G., Touretzky, D., & Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 7). </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth and Brooks, </publisher> <address> Monterey, CA. </address>
Reference-contexts: The advantage of learning with queries, as opposed to ordinary training examples, is that they can be used to garner information precisely where it is needed during the learning process. Our algorithm, as shown in Table 1, is similar to conventional decision-tree algorithms, such as CART <ref> (Breiman et al., 1984) </ref>, and C4.5 (Quinlan, 1993), which learn directly from a training set. However, Trepan is substantially different from these conventional algorithms in number of respects, which we detail below. The Oracle.
Reference: <author> Craven, M. & Shavlik, J. </author> <year> (1993a). </year> <title> Learning symbolic rules using artificial neural networks. </title> <booktitle> In Proc. of the 10th International Conference on Machine Learning, </booktitle> <pages> (pp. 73-80), </pages> <address> Amherst, MA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1993b). </year> <title> Learning to predict reading frames in E. coli DNA sequences. </title> <booktitle> In Proc. of the 26th Hawaii International Conference on System Sciences, </booktitle> <pages> (pp. 773-782), </pages> <address> Wailea, HI. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: the Cleveland heart-disease data set (13 features, 303 examples) from the UC-Irvine database; a promoter data set (57 features, 468 examples) which is a more complex superset of the UC-Irvine one; and a data set in which the task is to recognize protein-coding regions in DNA (64 features, 20,000 examples) <ref> (Craven & Shavlik, 1993b) </ref>. We remove the physician-fee-freeze feature from the voting data set to make the problem more difficult. We conduct our experiments using a 10-fold cross validation methodology, except for in the protein-coding domain.
Reference: <author> Craven, M. W. & Shavlik, J. W. </author> <year> (1994). </year> <title> Using sampling and queries to extract rules from trained neural networks. </title> <booktitle> In Proc. of the 11th International Conference on Machine Learning, </booktitle> <pages> (pp. 37-45), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fu, L. </author> <year> (1991). </year> <title> Rule learning by searching on adapted nets. </title> <booktitle> In Proc. of the 9th National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 590-595), </pages> <address> Anaheim, CA. </address> <publisher> AAAI/MIT Press. </publisher>
Reference: <author> Gallant, S. I. </author> <year> (1993). </year> <title> Neural Network Learning and Expert Systems. </title> <publisher> MIT Press. </publisher>
Reference: <author> Hayashi, Y. </author> <year> (1991). </year> <title> A neural expert system with automated extraction of fuzzy if-then rules. </title> <editor> In Lippmann, R., Moody, J., & Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 3). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Hogg, R. V. & Tanis, E. A. </author> <year> (1983). </year> <title> Probability and Statistical Inference. </title> <publisher> MacMillan. </publisher>
Reference-contexts: To make this decision, Trepan determines the proportion of examples, p c , that fall into the most common class at a given node, and then calculates a confidence interval around this proportion <ref> (Hogg & Tanis, 1983) </ref>. The oracle is queried for additional examples until prob (p c &lt; 1 *) &lt; ffi, where * and ffi are parameters of the algorithm. Trepan also accepts a parameter that specifies a limit on the number of internal nodes in an extracted tree.
Reference: <author> McMillan, C., Mozer, M. C., & Smolensky, P. </author> <year> (1992). </year> <title> Rule induction through integrated symbolic and subsymbolic processing. </title> <editor> In Moody, J., Hanson, S., & Lippmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 4). </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murphy, P. M. & Pazzani, M. J. </author> <year> (1991). </year> <title> ID2-of-3: Constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> In Proc. of the 8th International Machine Learning Workshop, </booktitle> <pages> (pp. 183-187), </pages> <address> Evanston, IL. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For continuous features, we consider binary splits on thresholds, in the same manner as C4.5. The selected binary split serves as a seed for the m-of-n search process. This greedy search uses the gain ratio measure as its heuristic evaluation function, and uses the following two operators <ref> (Murphy & Pazzani, 1991) </ref>: * m-of-n+1 : Add a new value to the set, and hold the threshold constant. For example, 2-of-fa; bg =) 2-of-fa; b; cg. * m+1-of-n+1: Add a new value to the set, and increment the threshold. For example, 2-of-fa; b; cg =) 3-of-fa; b; c; dg.
Reference: <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Neural networks are limited in this respect, since they are usually difficult to interpret after training. In contrast to neural networks, the solutions formed by "symbolic" learning systems <ref> (e.g., Quinlan, 1993) </ref> are usually much more amenable to human comprehension. We present a novel algorithm, Trepan, for extracting comprehensible, symbolic representations from trained neural networks. Trepan queries a given network to induce a decision tree that describes the concept represented by the network. <p> Our algorithm, as shown in Table 1, is similar to conventional decision-tree algorithms, such as CART (Breiman et al., 1984), and C4.5 <ref> (Quinlan, 1993) </ref>, which learn directly from a training set. However, Trepan is substantially different from these conventional algorithms in number of respects, which we detail below. The Oracle. <p> Like the ID2-of-3 algorithm, Trepan uses a hill-climbing search process to construct its m-of-n splits. The search process begins by first selecting the best binary split at the current node; as in C4.5, Trepan uses the gain ratio criterion <ref> (Quinlan, 1993) </ref> to evaluate candidate splits. For two-valued features, a binary split separates examples according to their values for the feature. For discrete features with more than two values, we consider binary splits based on each allowable value of the feature (e.g., color=red?, color=blue?, ...).
Reference: <author> Saito, K. & Nakano, R. </author> <year> (1988). </year> <title> Medical diagnostic expert system based on PDP model. </title> <booktitle> In Proc. of the IEEE International Conference on Neural Networks, </booktitle> <pages> (pp. 255-262), </pages> <address> San Diego, CA. </address> <publisher> IEEE Press. </publisher>
Reference: <author> Sethi, I. K., Yoo, J. H., & Brickman, C. M. </author> <year> (1993). </year> <title> Extraction of diagnostic rules using neural networks. </title> <booktitle> In Proc. of the 6th IEEE Symposium on Computer-Based Medical Systems, </booktitle> <pages> (pp. 217-222), </pages> <address> Ann Arbor, MI. </address> <publisher> IEEE Press. </publisher>
Reference: <author> Setiono, R. & Liu, H. </author> <year> (1995). </year> <title> Understanding neural networks via rule extraction. </title> <booktitle> In Proc. of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> (pp. 480-485), </pages> <address> Montreal, Canada. </address>
Reference: <author> Silverman, B. W. </author> <year> (1986). </year> <title> Density Estimation for Statistics and Data Analysis. </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: In order to generate these random values, Trepan uses the training data to model each feature's marginal distribution. Trepan uses frequency counts to model the distributions of discrete-valued features, and a kernel density estimation method <ref> (Silverman, 1986) </ref> to model continuous features.
Reference: <author> Tan, A.-H. </author> <year> (1994). </year> <title> Rule learning and extraction with self-organizing neural networks. </title> <booktitle> In Proc. of the 1993 Connectionist Models Summer School. </booktitle> <publisher> Erlbaum. </publisher>
Reference: <author> Tchoumatchenko, I. & Ganascia, J.-G. </author> <year> (1994). </year> <title> A Bayesian framework to integrate symbolic and neural learning. </title> <booktitle> In Proc. of the 11th International Conference on Machine Learning, </booktitle> <pages> (pp. 302-308), </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Thrun, S. </author> <year> (1995). </year> <title> Extracting rules from artificial neural networks with distributed representations. </title> <editor> In Tesauro, G., Touretzky, D., & Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 7). </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1993). </year> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101. </pages>
References-found: 20

