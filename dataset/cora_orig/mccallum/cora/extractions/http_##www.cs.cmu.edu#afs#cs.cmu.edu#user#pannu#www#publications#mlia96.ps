URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/pannu/www/publications/mlia96.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/pannu/www/pannu.html
Root-URL: 
Email: pannu+@cs.cmu.edu, katia@cs.cmu.edu  
Title: Learning Text Filtering Preferences  
Author: Anandeep S. Pannu and Katia Sycara 
Address: Pittsburgh, PA 15213  
Affiliation: The Robotics Institute School of Computer Science Carnegie Mellon University  
Abstract: We describe a reusable agent that learns a model of the user's research interests for filtering conference announcements and request for proposals (RFPs) from the Web. For this task, there is a large volume of irrelevant documents and the proportion of relevant documents is very small. It is also critical that the agent not misclassify relevant documents. Information Retrieval and Neural Network techniques were utilized to learn the model of user's preferences. Learning was boot-strapped using papers and proposals the user had written as positive examples. The agent's performance at startup is quite high. Information retrieval and Neural Nets were used to train the agent and experimental performance results were obtained and reported. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Armstrong, R.; Freitag, D.; Joachims, T.; and Mitchell, T. </author> <year> 1995. </year> <title> Webwatcher: A learning apprentice for the world wide web. </title> <booktitle> In Proceedings of AAAI Spring Symposium on Information Gathering from Heterogenous Distributed Environments. </booktitle>
Reference-contexts: Introduction We describe a reusable Learning Personal Agent (LPA) that learns a user's research interests and filters Web-based conference announcements and RFPs. Learning Personal Agents have been used for information filtering from the WWW (Lang 1995), <ref> (Armstrong et al. 1995) </ref>, (Pazzani, Nguyen, & Mantik 1995).
Reference: <author> Lang, K. </author> <year> 1994. </year> <title> Newsweeder: An adaptive multiuser text filter. Research Summary, </title> <institution> Carnegie Mellon University. </institution>
Reference-contexts: Using Information Retrieval Based Filtering One of the techniques adopted for learning a user preference model was a standard well tested technique from Information Retrieval called term frequency-inverse document frequency weighting (tf-idf) (Salton 1989). This is a simple technique that is hard to beat <ref> (Lang 1994) </ref> and provides a well tested benchmark to which other learning models can be compared. Assume some dictionary vector ~ D where each element d i is a word. <p> However keyword tf-idf performance is the best when we consider classification of only relevant documents. The Neural Net performed the worst, though our NN performance is considerably better than Neural Net performance reported by others (e.g. <ref> (Lang 1994) </ref>). Despite the small number of training examples used (357) compared to those reported in other literature (for instance (Lang 1994) which used 20000 examples), we were able to get performance approaching 70% correct classification. <p> The Neural Net performed the worst, though our NN performance is considerably better than Neural Net performance reported by others (e.g. <ref> (Lang 1994) </ref>). Despite the small number of training examples used (357) compared to those reported in other literature (for instance (Lang 1994) which used 20000 examples), we were able to get performance approaching 70% correct classification. For the Cascade 2 neural network growing algorithm trained on a 1000 articles, Lang (Lang 1994) reported 25% correct classification. <p> <ref> (Lang 1994) </ref>). Despite the small number of training examples used (357) compared to those reported in other literature (for instance (Lang 1994) which used 20000 examples), we were able to get performance approaching 70% correct classification. For the Cascade 2 neural network growing algorithm trained on a 1000 articles, Lang (Lang 1994) reported 25% correct classification. In our case the Neural Network has approximately 3 times the performance achieved by the Cascade 2 algorithm. This is no doubt due to the fact that our text filtering task is more specialized than the task for (Lang 1994). <p> trained on a 1000 articles, Lang <ref> (Lang 1994) </ref> reported 25% correct classification. In our case the Neural Network has approximately 3 times the performance achieved by the Cascade 2 algorithm. This is no doubt due to the fact that our text filtering task is more specialized than the task for (Lang 1994). A trend seen in Figure 2 is that classification accuracy increases with user feedback. Though the keyword based approach shows a small decrease in overall classification accuracy, classification accuracy for relevant documents alone follows an increasing trend.
Reference: <author> Lang, K. </author> <year> 1995. </year> <title> Learning to filter netnews. </title> <booktitle> In Proceedings of the Machine Learning Conference 1995. </booktitle>
Reference-contexts: Introduction We describe a reusable Learning Personal Agent (LPA) that learns a user's research interests and filters Web-based conference announcements and RFPs. Learning Personal Agents have been used for information filtering from the WWW <ref> (Lang 1995) </ref>, (Armstrong et al. 1995), (Pazzani, Nguyen, & Mantik 1995).
Reference: <author> Pannu, A., and Sycara, K. </author> <year> 1996. </year> <title> A learning personal agent for text filtering and notification. </title> <note> Submitted to AAAI 96. </note>
Reference-contexts: In case the network output was greater than 0.5 for an expected value of 1, or less than 0.5 for an expected value of -1 the document was marked as being classified correctly. Otherwise the document was marked as being classified incorrectly. We used a coordination set (see <ref> (Pannu & Sycara 1996) </ref>) to determine the number of epochs to train the Neural Net for optimal performance. The results reported are for a 360 word vector of keywords, and training time was roughly comparable to the tf-idf algorithm using the keyword based approach.
Reference: <author> Pazzani, M.; Nguyen, L.; and Mantik, S. </author> <year> 1995. </year> <title> Learning from hotlists and coldlists : Towards a www information filtering and seeking agent. </title> <booktitle> In Proceedings of the Seventh International IEEE conference on Tools with AI. </booktitle> <publisher> IEEE Computer Press. </publisher>
Reference-contexts: Introduction We describe a reusable Learning Personal Agent (LPA) that learns a user's research interests and filters Web-based conference announcements and RFPs. Learning Personal Agents have been used for information filtering from the WWW (Lang 1995), (Armstrong et al. 1995), <ref> (Pazzani, Nguyen, & Mantik 1995) </ref>.
Reference: <author> Salton, G. </author> <year> 1989. </year> <title> Automatic Text Processing. The Transformation, Analysis and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Using Information Retrieval Based Filtering One of the techniques adopted for learning a user preference model was a standard well tested technique from Information Retrieval called term frequency-inverse document frequency weighting (tf-idf) <ref> (Salton 1989) </ref>. This is a simple technique that is hard to beat (Lang 1994) and provides a well tested benchmark to which other learning models can be compared. Assume some dictionary vector ~ D where each element d i is a word.
References-found: 6

