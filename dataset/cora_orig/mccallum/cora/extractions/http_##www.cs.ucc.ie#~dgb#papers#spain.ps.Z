URL: http://www.cs.ucc.ie/~dgb/papers/spain.ps.Z
Refering-URL: http://www.cs.ucc.ie/~dgb/publist.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fmiles,dgbg@minster.york.ac.uk  
Title: Learning unification-based grammars using the Spoken English Corpus  
Author: Miles Osborne and Derek Bridge 
Date: July 12, 1994  
Address: York, Heslington, York YO1 5DD, U. K.  
Affiliation: Department of Computer Science, University of  
Abstract: This paper describes a grammar learning system that combines model-based and data-driven learning within a single framework. Our results from learning grammars using the Spoken English Corpus (SEC) suggest that combined model-based and data-driven learning can produce a more plausible grammar than is the case when using either learning style in isolation.
Abstract-found: 1
Intro-found: 1
Reference: [Bak79] <author> J. K. Baker. </author> <title> Trainable grammars for speech recognition. </title> <editor> In D. H. Klatt and J. J. Wolf, editors, </editor> <booktitle> Speech Communication Papers for the 97 th Meeting of the Acoustical Society of America, </booktitle> <pages> pages 547-550. </pages> <year> 1979. </year>
Reference-contexts: Furthermore, the SEC is tagged and parsed, thus side-stepping the problems of constructing a suitable lexicon and of creating an evaluation corpus to determine the plausibility of the learnt grammars. In contrast to other researchers (for example <ref> [BMMS92, GLS87, Bak79, LY90, VB87] </ref>), we try to learn competence grammars and not performance grammars. We also try to learn grammars that assign linguistically plausible parses to sentences. Learning competence grammars that assign plausible parses is achieved by combining model-based and data-driven learning within a single framework [OB93b, OB93a].
Reference: [Ber85] <author> Robert C. Berwick. </author> <title> The acquisition of syntactic knowledge. </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: An example of purely model-based language learning is given by Berwick <ref> [Ber85] </ref>. More usually, though, the model is incomplete and this leads us to give it a different role in our architecture.
Reference: [BGL93] <editor> Ezra Black, Roger Garside, and Geoffrey Leech, editors. </editor> <title> Statistically driven computer grammars of English the IBM-Lancaster approach. </title> <address> Rodopi, </address> <year> 1993. </year>
Reference-contexts: These were pretrain (less than 20 sentences), train (60 sentences) and test (60 sentences). * A grammar, G, was used as the initial grammar. This was manually constructed and consisted of 97 unification-based rules with a terminal set of the CLAWS2 tagset <ref> [BGL93] </ref>. * The Model was configured to consist of 4 LP rules, 32 pairings of semantic types and corresponding syntactic categories, and a Head Feature Conven tion. * Pretrain was used to calculate scores of MDPs, thus providing an initial estimate of grammaticality for the data-driven learner. * Train was then
Reference: [BMMS92] <author> Eric Brill, David Magerman, Mitchell Marcus, and Beatrice San-torini. </author> <title> Deducing Linguistic Structure from the Statistics of Large Corpora. </title> <booktitle> In AAAI-92 Workshop Program: Statistically-Based NLP Techniques, </booktitle> <address> San Jose, California, </address> <year> 1992. </year>
Reference-contexts: Furthermore, the SEC is tagged and parsed, thus side-stepping the problems of constructing a suitable lexicon and of creating an evaluation corpus to determine the plausibility of the learnt grammars. In contrast to other researchers (for example <ref> [BMMS92, GLS87, Bak79, LY90, VB87] </ref>), we try to learn competence grammars and not performance grammars. We also try to learn grammars that assign linguistically plausible parses to sentences. Learning competence grammars that assign plausible parses is achieved by combining model-based and data-driven learning within a single framework [OB93b, OB93a].
Reference: [Cas88] <author> Claudia Casadio. </author> <title> Semantic Categories and the Development of Cate-gorial Grammars. </title> <editor> In Richard T. Oehrle, editor, </editor> <booktitle> Categorial Grammars and Natural Language Structures, </booktitle> <pages> pages 95-123. </pages> <address> D. </address> <publisher> Reidel, </publisher> <year> 1988. </year> <month> 9 </month>
Reference-contexts: An example of purely model-based language learning is given by Berwick [Ber85]. More usually, though, the model is incomplete and this leads us to give it a different role in our architecture. Our model currently consists of GPSG Linear Precedence (LP) rules [GKPS85], semantic types <ref> [Cas88] </ref>, a Head Feature Convention [GKPS85] and X-bar syntax [Jac77]. * LP rules are restrictions upon local trees. A local tree is a (sub)tree of depth one.
Reference: [CGBB88] <author> John Carroll, Claire Grover, Ted Briscoe, and Bran Boguraev. </author> <title> A Development Environment for Large Natural Language Grammars. </title> <type> Technical report number 127, </type> <institution> University of Cambridge Computer Laboratory, </institution> <year> 1988. </year>
Reference-contexts: We also try to learn grammars that assign linguistically plausible parses to sentences. Learning competence grammars that assign plausible parses is achieved by combining model-based and data-driven learning within a single framework [OB93b, OB93a]. The system is implemented to make use of the Grammar Development Environment (GDE) <ref> [CGBB88] </ref> and it augments the GDE with 3300 lines of Common Lisp. 1 Our aim in this paper is to show that combining both learning styles pro-duces a grammar that assigns more plausible parses than is the case for grammars learnt using either learning style in isolation.
Reference: [Cho81] <author> Noam Chomsky. </author> <title> Lectures on Government and Binding. </title> <publisher> Dordrecht: Foris, </publisher> <year> 1981. </year>
Reference-contexts: Future work will evaluate how much the learnt grammars overgenerate. We also intend investigating other constraints upon grammaticality, such as to be found in Government and Binding Theory <ref> [Cho81] </ref>, punctuation [Num90], or textuality [HH76, dBD81].
Reference: [CP82] <author> K. Church and R. Patil. </author> <title> Coping with syntactic ambiguity or how to put the block in the box on the table. </title> <journal> Computational Linguistics, </journal> <volume> 8 </volume> <pages> 139-49, </pages> <year> 1982. </year>
Reference-contexts: For example, using the binary super rule may lead to a number of parses equal (at least) to the Catalan series with respect to sentence length. This is because, as a worst case, the binary super rule will create all possible binary branching parses for some sentence <ref> [CP82] </ref>. In order to generate results therefore, steps were taken to place resource bounds upon the learning process. These bounds were to halt when n parses or m edges had been generated (n=1, m=3000) for some sentence. Increasing n leads to more ambiguous attachments being learnt.
Reference: [dBD81] <author> Robert de Beaugrande and Wolfgang Dressler. </author> <title> Introduction to Text Linguistics. </title> <publisher> Longman, </publisher> <year> 1981. </year>
Reference-contexts: Future work will evaluate how much the learnt grammars overgenerate. We also intend investigating other constraints upon grammaticality, such as to be found in Government and Binding Theory [Cho81], punctuation [Num90], or textuality <ref> [HH76, dBD81] </ref>.
Reference: [DWP81] <author> D.R. Dowty, R.E. Wall, and S. Peters. </author> <title> Introduction to Montague Semantics. </title> <address> D. </address> <publisher> Reidel Publishing Company, </publisher> <year> 1981. </year>
Reference-contexts: In our learning system, any putative rule that violates an LP rule is rejected. * We construct our syntax and semantics in tandem, adhering to the principle of compositionality, and pair a semantic rule to each syntactic rule <ref> [DWP81] </ref>. Our semantics uses the typed -calculus with extensional typing. For example, the syntactic rule: S ! NP VP is paired with the following semantic rule: 3 VP (NP) which should be read as `the functor VP takes the argument NP' 1 .
Reference: [GKPS85] <author> G. Gadzar, E. Klein, G.K. Pullum, and I.A. Sag. </author> <title> Generalized Phrase Structure Grammar. </title> <publisher> Harvard University Press, </publisher> <year> 1985. </year>
Reference-contexts: An example of purely model-based language learning is given by Berwick [Ber85]. More usually, though, the model is incomplete and this leads us to give it a different role in our architecture. Our model currently consists of GPSG Linear Precedence (LP) rules <ref> [GKPS85] </ref>, semantic types [Cas88], a Head Feature Convention [GKPS85] and X-bar syntax [Jac77]. * LP rules are restrictions upon local trees. A local tree is a (sub)tree of depth one. <p> More usually, though, the model is incomplete and this leads us to give it a different role in our architecture. Our model currently consists of GPSG Linear Precedence (LP) rules <ref> [GKPS85] </ref>, semantic types [Cas88], a Head Feature Convention [GKPS85] and X-bar syntax [Jac77]. * LP rules are restrictions upon local trees. A local tree is a (sub)tree of depth one. <p> Our model currently consists of GPSG Linear Precedence (LP) rules [GKPS85], semantic types [Cas88], a Head Feature Convention [GKPS85] and X-bar syntax [Jac77]. * LP rules are restrictions upon local trees. A local tree is a (sub)tree of depth one. An example of an LP rule might be <ref> [GKPS85, p.50] </ref>: [SUBCAT] ~ [SUBCAT] This rule should be read as `if the SUBCAT feature is instantiated (in a category of a local tree) then the SUBCAT feature of the linearly preceding category should not be instantiated'.
Reference: [GLS87] <editor> R. Garside, G. Leech, and G. Sampson, editors. </editor> <title> The Computational Analysis of English: A Corpus-based Approach. </title> <publisher> Longman, </publisher> <year> 1987. </year>
Reference-contexts: Furthermore, the SEC is tagged and parsed, thus side-stepping the problems of constructing a suitable lexicon and of creating an evaluation corpus to determine the plausibility of the learnt grammars. In contrast to other researchers (for example <ref> [BMMS92, GLS87, Bak79, LY90, VB87] </ref>), we try to learn competence grammars and not performance grammars. We also try to learn grammars that assign linguistically plausible parses to sentences. Learning competence grammars that assign plausible parses is achieved by combining model-based and data-driven learning within a single framework [OB93b, OB93a].
Reference: [HH76] <author> M. A. K. Halliday and Ruqaiya Hasan. </author> <title> Coherence in English. </title> <address> Long-man, </address> <year> 1976. </year>
Reference-contexts: Future work will evaluate how much the learnt grammars overgenerate. We also intend investigating other constraints upon grammaticality, such as to be found in Government and Binding Theory [Cho81], punctuation [Num90], or textuality <ref> [HH76, dBD81] </ref>.
Reference: [Jac77] <author> Ray S. Jackendoff. </author> <title> X-Bar Syntax: A Study of Phrase Structure. </title> <publisher> The M.I.T Press, </publisher> <year> 1977. </year>
Reference-contexts: More usually, though, the model is incomplete and this leads us to give it a different role in our architecture. Our model currently consists of GPSG Linear Precedence (LP) rules [GKPS85], semantic types [Cas88], a Head Feature Convention [GKPS85] and X-bar syntax <ref> [Jac77] </ref>. * LP rules are restrictions upon local trees. A local tree is a (sub)tree of depth one.
Reference: [JLG78] <author> S. Johansson, G. Leech, and H. Goodluck. </author> <title> Manual of Information to Accompany the Lancaster-Oslo/Bergen Corpus of British English, for Use with Digital Computers. </title> <type> Technical report, </type> <institution> Department of English, University of Oslo, </institution> <year> 1978. </year>
Reference-contexts: 1 Introduction In this paper, we present some results of our grammar learning system acquiring unification-based grammars using the Spoken English Corpus (SEC). The SEC is a collection of monologues for public broadcast and is small (circa 50,000 words) in comparison to other corpora, such as the Lancaster-Oslo-Bergen Corpus <ref> [JLG78] </ref>, but sufficiently large to demonstrate the capabilities of the learning system. Furthermore, the SEC is tagged and parsed, thus side-stepping the problems of constructing a suitable lexicon and of creating an evaluation corpus to determine the plausibility of the learnt grammars.
Reference: [Lee87] <author> Fanny Leech. </author> <title> An approach to probabilistic parsing. </title> <booktitle> MPhil Dissertation, </booktitle> <year> 1987. </year> <institution> University of Lancaster. </institution>
Reference-contexts: Other instantiations can be rejected. The higher the threshold, the fewer the number of rules accepted 3 . The approach we have described is a generalisation of the work of Leech, who uses a simple phrase structure grammar, whereas we use a unification-based grammar <ref> [Lee87] </ref>. 3 Method We predicted that the plausibility of grammars learnt using both model-based and data-driven learning would be better than the plausibility of grammars obtained by using either learning style in isolation.
Reference: [LG91] <author> Geoffrey Leech and Roger Garside. </author> <title> Running a grammar factory: The production of syntactically analysed corpora or "treebanks". </title> <editor> In Stig Johansson and Anna-Brita Stenstrom, editors, </editor> <title> English Computer Corpora: Selected Papers and Research Guide. </title> <publisher> Mouten de Gruyter, </publisher> <year> 1991. </year>
Reference-contexts: This can then be used in subsequent learning to score instantiations of the super rules. In pre-training the frequencies of mother-daughter pairs (MDPs) found in parses of sentences taken from the pre-training corpus are recorded <ref> [LG91] </ref>. For example, the tree (S (NP Sam) (VP (V laughs))) has the following MDPs: &lt;S,NP&gt; &lt;VP,V&gt; The frequencies of these MDPs in the parse trees of the pre-training corpus are noted.
Reference: [LY90] <author> K. Lari and S. J. Young. </author> <title> The estimation of stochastic context-free grammars using the Inside-Outside Algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56, </pages> <year> 1990. </year> <month> 10 </month>
Reference-contexts: Furthermore, the SEC is tagged and parsed, thus side-stepping the problems of constructing a suitable lexicon and of creating an evaluation corpus to determine the plausibility of the learnt grammars. In contrast to other researchers (for example <ref> [BMMS92, GLS87, Bak79, LY90, VB87] </ref>), we try to learn competence grammars and not performance grammars. We also try to learn grammars that assign linguistically plausible parses to sentences. Learning competence grammars that assign plausible parses is achieved by combining model-based and data-driven learning within a single framework [OB93b, OB93a].
Reference: [MM91] <author> D. Magerman and M. Marcus. Pearl: </author> <title> a probabilistic chart parser. </title> <booktitle> In Proceedings of the 2 nd International Workshop on Parsing Technologies, Cancun, Mexico, </booktitle> <pages> pages 193-199, </pages> <year> 1991. </year>
Reference-contexts: We take the geometric mean, rather than the product, to avoid penalising local trees that have more daughters over local trees that have fewer daughters <ref> [MM91] </ref>. * For interior trees of the form (B (C D)), the score of the local tree is: score (B) = gm (score (C) fi f (&lt; B; C &gt;); score (D) fi f (&lt; B; D &gt;)) (This does leave the problem of dealing with MDPs that arise in completed
Reference: [MW92] <author> David Magerman and Carl Weir. </author> <title> Efficiency, Robustness and Accuracy in Picky Chart Parsing. </title> <booktitle> In Proceedings of the 30 th ACL, </booktitle> <institution> University of Delaware, Newark, </institution> <address> Delaware, </address> <pages> pages 40-47, </pages> <year> 1992. </year>
Reference-contexts: Increasing n leads to more ambiguous attachments being learnt. The motivation for the m limit follows from Magerman and Weir who suggest that large numbers of edges being generated might correlate with ungrammaticality <ref> [MW92] </ref>. In effect, the parser spends a lot of time searching unsuccessfully for a parse and this is reflected in the large number of edges generated. The other constraint upon the system was that we only used the binary super rule during interleaved parsing and learning.
Reference: [Num90] <author> G. Numberg. </author> <title> The linguistics of punctuation. Center for the Study of Language and Information, </title> <year> 1990. </year>
Reference-contexts: Future work will evaluate how much the learnt grammars overgenerate. We also intend investigating other constraints upon grammaticality, such as to be found in Government and Binding Theory [Cho81], punctuation <ref> [Num90] </ref>, or textuality [HH76, dBD81].
Reference: [OB93a] <author> Miles Osborne and Derek Bridge. </author> <title> Inductive and deductive grammar learning: dealing with incomplete theories. In Grammatical Inference Colloquim, </title> <publisher> Essex University, </publisher> <year> 1993. </year>
Reference-contexts: We also try to learn grammars that assign linguistically plausible parses to sentences. Learning competence grammars that assign plausible parses is achieved by combining model-based and data-driven learning within a single framework <ref> [OB93b, OB93a] </ref>.
Reference: [OB93b] <author> Miles Osborne and Derek Bridge. </author> <title> Learning unification-based grammars and the treatment of undergeneration. </title> <booktitle> In Workshop on Machine Learning Techniques and Text Analysis, </booktitle> <address> Vienna, Austria, </address> <year> 1993. </year>
Reference-contexts: We also try to learn grammars that assign linguistically plausible parses to sentences. Learning competence grammars that assign plausible parses is achieved by combining model-based and data-driven learning within a single framework <ref> [OB93b, OB93a] </ref>.
Reference: [VB87] <author> Kurt Vanlehn and William Ball. </author> <title> A Version Space Approach to Learning Context-free Grammars. </title> <booktitle> Machine Learning, </booktitle> <address> 2.1:39-74, </address> <year> 1987. </year> <month> 11 </month>
Reference-contexts: Furthermore, the SEC is tagged and parsed, thus side-stepping the problems of constructing a suitable lexicon and of creating an evaluation corpus to determine the plausibility of the learnt grammars. In contrast to other researchers (for example <ref> [BMMS92, GLS87, Bak79, LY90, VB87] </ref>), we try to learn competence grammars and not performance grammars. We also try to learn grammars that assign linguistically plausible parses to sentences. Learning competence grammars that assign plausible parses is achieved by combining model-based and data-driven learning within a single framework [OB93b, OB93a].
References-found: 24

