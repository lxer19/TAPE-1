URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-29-97.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Title: Learning Team Strategies With Multiple Policy-Sharing Agents: A Soccer Case Study  
Author: Rafa l Sa lustowicz, Marco Wiering, Jurgen Schmidhuber 
Keyword: Multiagent Reinforcement Learning, Soccer, TD-Q Learning, Probabilistic Incremental Program Evolution  
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Pubnum: Technical Report IDSIA-29-97  
Email: e-mail: frafal, marco, juergeng@idsia.ch  
Phone: tel.: +41-91-9919838 fax: +41-91-9919839  
Date: January 1997  
Abstract: We use simulated soccer to study multiagent learning. Each team's players (agents) share action set and policy but may behave differently due to position-dependent inputs. All agents making up a team are rewarded or punished collectively in case of goals. We conduct simulations with varying team sizes, and compare two learning algorithms: TD-Q learning with linear neural networks (TD-Q) and Probabilistic Incremental Program Evolution (PIPE). TD-Q is based on evaluation functions (EFs) mapping input/action pairs to expected reward, while PIPE searches policy space directly. PIPE uses adaptive "probabilistic prototype trees" to synthesize programs that calculate action probabilities from current inputs. Our results show that TD-Q encounters several difficulties in learning appropriate shared EFs. PIPE, however, does not depend on EFs and can find good policies faster and more reliably. This suggests that in multiagent learning scenarios direct search through policy space can offer advantages over EF-based approaches. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Asada, M., Uchibe, E., Noda, S., Tawaratsumida, S., and Hosoda, K. </author> <year> (1994). </year> <title> A vision-based reinforcement learning for coordination of soccer playing behaviors. </title> <booktitle> In Proc. of AAAI-94 Workshop on AI and A-life and Entertainment, </booktitle> <pages> pages 16-21. </pages>
Reference: <author> Baluja, S. and Caruana, R. </author> <year> (1995). </year> <title> Removing the genetics from the standard genetic algorithm. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 38-46. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: TD-Q selects actions according to linear neural networks trained with the delta rule (Widrow and Hoff, 1960) to map player inputs to evaluations of alternative actions. PIPE is based on probability vector coding of program instructions (Schmidhuber, 1997), Population-Based Incremental Learning <ref> (Baluja and Caruana, 1995) </ref> and tree coding of programs used in variants of Genetic Programming (GP) (Cramer, 1985; Koza, 1992). PIPE synthesizes programs that calculate action probabilities from inputs. Experiences with programs are stored in "probabilistic prototype trees" that guide program synthesis. Soccer.
Reference: <author> Bertsekas, D. P. </author> <year> (1996). </year> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference: <author> Cramer, N. L. </author> <year> (1985). </year> <title> A representation for the adaptive generation of simple sequential programs. </title> <editor> In Grefenstette, J., editor, </editor> <booktitle> Proceedings of an International Conference on Genetic Algorithms and Their Applications, </booktitle> <address> Hillsdale NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Crites, R. and Barto, A. </author> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <editor> In Touretzky, D., Mozer, M., and Hasselmo, M., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 1017-1023, </pages> <address> Cambridge MA. </address> <publisher> MIT Press. 18 Dickmanns, </publisher> <editor> D., Schmidhuber, J. H., and Winklhofer, A. </editor> <year> (1987). </year> <title> Der genetische Algo--rithmus: Eine Implementierung in Prolog. </title> <institution> Fortgeschrittenenpraktikum, Institut fur Informatik, Lehrstuhl Prof. Radig, Technische Universitat Munchen. </institution>
Reference-contexts: TD-Q's problems are due to a combination of several reasons. (1) Partial observability. Q-learning assumes that the environment is fully observable; otherwise it is not guaranteed to work. Still, Q-learning variants already have been successfully applied to partially observable environments, e.g., <ref> (Crites and Barto, 1996) </ref>.
Reference: <author> Geman, S., Bienenstock, E., and Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference-contexts: Still, Q-learning variants already have been successfully applied to partially observable environments, e.g., (Crites and Barto, 1996). The POPs in our soccer simulations, however, seem too severe for the linear networks. (2) Too many trainable parameters (variance in the "bias-variance dilemma" <ref> (Geman et al., 1992) </ref> too high | more training games are needed). (3) Agent credit assignment problem (ACAP) (Weiss, 1996; Versino and Gambardella, 1997): how much did some agent contribute to team performance? ACAP 17 is particularly difficult in the case of multiagent soccer.
Reference: <author> Koza, J. R. </author> <year> (1992). </year> <title> Genetic Programming On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press. </publisher>
Reference-contexts: (8y 2 IR; y 6= 0: rlog (y)=log (abs (y)) and rlog (0) = 0), ~ i (p; t) i 1 i v denotes component i of a vector ~ i (p; t) with v components and R represents a generic random constant 2 [0;1) (see also "ephemeral random constant" <ref> (Koza, 1992) </ref>). Program Representation. Programs are encoded in n-ary trees, with n being the maximal number of function arguments. Each argument is calculated by a subtree. The trees are parsed depth first from left to right. Probabilistic Prototype Tree. A probabilistic prototype tree (PPT) is generally a complete n-ary tree.
Reference: <author> Levin, L. A. </author> <year> (1973). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference: <author> Levin, L. A. </author> <year> (1984). </year> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37. </pages>
Reference: <author> Li, M. and Vitanyi, P. M. B. </author> <year> (1993). </year> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer. </publisher>
Reference-contexts: Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning with linear neural nets (TD-Q) <ref> (Lin, 1993) </ref> and Probabilistic Incremental Program Evolution (PIPE) (Sa lustowicz and Schmidhuber, 1997). We choose PIPE and TD-Q because both have already been successfully applied to interesting single-agent tasks (TD-Q also because it is very popular). <p> At the end of each generation we prune all prototype trees as described in Section 3.1. 4 TD-Q Learning One of the most widely known and promising EF-based approaches to reinforcement learn ing is TD-Q learning. We use Lin's popular and successful TD () Q-variant <ref> (Lin, 1993) </ref>. 10 For efficiency reasons our TD-Q version uses linear neural nets (nets with hidden units require too much simulation time). <p> Similarly we select the second parameter if there is any. TD-Q learning. For both simple and complex simulations we use Lin's TD () Q-variant <ref> (Lin, 1993) </ref>. Each game consists of separate trials. For each player p there is a variable time-pointer t (p). At trial start we set t (p) to current game time t c . We increment t (p) after each cycle.
Reference: <author> Lin, L. J. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Pittsburgh. </institution>
Reference-contexts: Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning with linear neural nets (TD-Q) <ref> (Lin, 1993) </ref> and Probabilistic Incremental Program Evolution (PIPE) (Sa lustowicz and Schmidhuber, 1997). We choose PIPE and TD-Q because both have already been successfully applied to interesting single-agent tasks (TD-Q also because it is very popular). <p> At the end of each generation we prune all prototype trees as described in Section 3.1. 4 TD-Q Learning One of the most widely known and promising EF-based approaches to reinforcement learn ing is TD-Q learning. We use Lin's popular and successful TD () Q-variant <ref> (Lin, 1993) </ref>. 10 For efficiency reasons our TD-Q version uses linear neural nets (nets with hidden units require too much simulation time). <p> Similarly we select the second parameter if there is any. TD-Q learning. For both simple and complex simulations we use Lin's TD () Q-variant <ref> (Lin, 1993) </ref>. Each game consists of separate trials. For each player p there is a variable time-pointer t (p). At trial start we set t (p) to current game time t c . We increment t (p) after each cycle.
Reference: <author> Littman, M. L. </author> <year> (1994). </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 157-163. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Matsubara, H., Noda, I., and Hiraki, K. </author> <year> (1996). </year> <title> Learning of cooperative actions in multi-agent systems: a case study of pass play in soccer. </title> <booktitle> In AAAI-96 Spring Symposium on Adaptation, Coevolution and Learning in Multi-agent Systems, </booktitle> <pages> pages 63-67. </pages>
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 173-193. </pages>
Reference-contexts: If some multiagent cooperation task indeed can be solved by homogeneous agents then policy-sharing is quite natural as it allows for greatly reducing the number of adaptive free parameters. This tends to reduce the number of required training examples (learning time) and increase generalization performance <ref> (Nowlan and Hinton, 1992) </ref>. Challenges of Multiagent Learning. One challenge is the "partial observability problem" (POP): in general no learner's input will tell the learner everything about its environment (which includes other changing learners). This means that each learner's environment may change in an inherently unpredictable way.
Reference: <author> Sahota, M. </author> <year> (1993). </year> <title> Real-time intelligent behaviour in dynamic environments: Soccer-playing robots. </title> <type> Master's thesis, </type> <institution> University of British Columbia. </institution>
Reference: <author> Sa lustowicz, R. P. and Schmidhuber, J. </author> <year> (1997). </year> <title> Probabilistic incremental program evolution. Evolutionary Computation, </title> <note> to appear. See ftp://ftp.idsia.ch/pub/rafal/- PIPE.ps.gz. </note>
Reference-contexts: Members of this class are Levin search (Levin, 1973; Levin, 1984; Solomonoff, 1986; Li and Vitanyi, 1993; Schmidhuber, 1995; Wiering and Schmidhuber, 1996), Genetic Programming (GP) (Cramer, 1985; Dickmanns et al., 1987; Koza, 1992) and Probabilistic Incremental Program Evolution (PIPE) <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>. Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning with linear neural nets (TD-Q) (Lin, 1993) and Probabilistic Incremental Program Evolution (PIPE) (Sa lustowicz and Schmidhuber, 1997). <p> (GP) (Cramer, 1985; Dickmanns et al., 1987; Koza, 1992) and Probabilistic Incremental Program Evolution (PIPE) <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>. Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning with linear neural nets (TD-Q) (Lin, 1993) and Probabilistic Incremental Program Evolution (PIPE) (Sa lustowicz and Schmidhuber, 1997). We choose PIPE and TD-Q because both have already been successfully applied to interesting single-agent tasks (TD-Q also because it is very popular). <p> In simple simulations we set ASET := ASET S and ~ i (p; t) := ~ i s (p; t). In complex simulations we set ASET := ASET C and ~ i (p; t) := ~ i c (p; t). We use PIPE as described in <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>, except for "elitist learning" which we omit due to high environmental stochasticity. A PIPE alternative for searching program space would be genetic programming (GP) (Cramer, 1985; Dickmanns et al., 1987; Koza, 1992). <p> A PIPE alternative for searching program space would be genetic programming (GP) (Cramer, 1985; Dickmanns et al., 1987; Koza, 1992). We chose PIPE over GP because it compared favorably with Koza's GP variant in previous experiments <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>. Action Selection. Action selection depends on 5 (8) variables when simple (complex) actions are used: g 2 IR, A i 2 IR, 8i 2 ASET .
Reference: <author> Schmidhuber, J. </author> <year> (1995). </year> <title> Discovering solutions with low Kolmogorov complexity and high generalization capability. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 488-496. </pages> <publisher> Morgan Kauf-mann Publishers, </publisher> <address> San Francisco, CA. </address> <note> 19 Schmidhuber, J. </note> <year> (1997). </year> <title> A general method for incremental self-improvement and multi--agent learning in unrestricted environments. </title> <editor> In Yao, X., editor, </editor> <booktitle> Evolutionary Computation: Theory and Applications. </booktitle> <publisher> Scientific Publ. Co., Singapore. In press. </publisher>
Reference: <author> Schmidhuber, J. and Zhao, J. </author> <year> (1997). </year> <title> The success-story algorithm for multi-agent reinforcement learning. </title> <editor> In Weiss, G., editor, </editor> <booktitle> Distributed Artificial Intelligence meets Machine Learning. </booktitle> <address> Springer. </address> <note> To appear. </note>
Reference-contexts: Members of this class are Levin search (Levin, 1973; Levin, 1984; Solomonoff, 1986; Li and Vitanyi, 1993; Schmidhuber, 1995; Wiering and Schmidhuber, 1996), Genetic Programming (GP) (Cramer, 1985; Dickmanns et al., 1987; Koza, 1992) and Probabilistic Incremental Program Evolution (PIPE) <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>. Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning with linear neural nets (TD-Q) (Lin, 1993) and Probabilistic Incremental Program Evolution (PIPE) (Sa lustowicz and Schmidhuber, 1997). <p> (GP) (Cramer, 1985; Dickmanns et al., 1987; Koza, 1992) and Probabilistic Incremental Program Evolution (PIPE) <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>. Comparison. In our case study we compare two learning algorithms, each representative of its class: TD-Q learning with linear neural nets (TD-Q) (Lin, 1993) and Probabilistic Incremental Program Evolution (PIPE) (Sa lustowicz and Schmidhuber, 1997). We choose PIPE and TD-Q because both have already been successfully applied to interesting single-agent tasks (TD-Q also because it is very popular). <p> TD-Q selects actions according to linear neural networks trained with the delta rule (Widrow and Hoff, 1960) to map player inputs to evaluations of alternative actions. PIPE is based on probability vector coding of program instructions <ref> (Schmidhuber, 1997) </ref>, Population-Based Incremental Learning (Baluja and Caruana, 1995) and tree coding of programs used in variants of Genetic Programming (GP) (Cramer, 1985; Koza, 1992). PIPE synthesizes programs that calculate action probabilities from inputs. Experiences with programs are stored in "probabilistic prototype trees" that guide program synthesis. Soccer. <p> In simple simulations we set ASET := ASET S and ~ i (p; t) := ~ i s (p; t). In complex simulations we set ASET := ASET C and ~ i (p; t) := ~ i c (p; t). We use PIPE as described in <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>, except for "elitist learning" which we omit due to high environmental stochasticity. A PIPE alternative for searching program space would be genetic programming (GP) (Cramer, 1985; Dickmanns et al., 1987; Koza, 1992). <p> A PIPE alternative for searching program space would be genetic programming (GP) (Cramer, 1985; Dickmanns et al., 1987; Koza, 1992). We chose PIPE over GP because it compared favorably with Koza's GP variant in previous experiments <ref> (Sa lustowicz and Schmidhuber, 1997) </ref>. Action Selection. Action selection depends on 5 (8) variables when simple (complex) actions are used: g 2 IR, A i 2 IR, 8i 2 ASET .
Reference: <author> Solomonoff, R. </author> <year> (1986). </year> <title> An application of algorithmic probability to problems in artificial intelligence. </title> <editor> In Kanal, L. N. and Lemmer, J. F., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 473-491. </pages> <publisher> Elsevier Science Publishers. </publisher>
Reference: <author> Stone, P. and Veloso, M. </author> <year> (1995). </year> <title> Beating a defender in robotic soccer: Memory-based learning of a continuous function. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Stone, P. and Veloso, M. </author> <year> (1996). </year> <title> A layered approach to learning client behaviors in the robocup soccer server. </title> <note> Submitted to Applied Artificial Intelligence (AAI) in August 1996. </note>
Reference: <author> Versino, C. and Gambardella, L. M. </author> <year> (1997). </year> <title> Learning real team solutions. </title> <editor> In Weiss, G., editor, </editor> <booktitle> DAI Meets Machine Learning, Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag. In press. </publisher>
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College. </institution>
Reference: <author> Weiss, G. </author> <year> (1996). </year> <title> Adaptation and learning in multi-agent systems: Some remarks and a bibliography. </title> <editor> In Weiss, G. and Sen, S., editors, </editor> <booktitle> Adaptation and Learning in Multi-Agent Systems, </booktitle> <volume> volume 1042, </volume> <pages> pages 1-21. </pages> <note> Springer-Verlag, Lecture Notes in Artificial Intelligence. </note>
Reference: <author> Widrow, B. and Hoff, M. E. </author> <year> (1960). </year> <title> Adaptive switching circuits. </title> <booktitle> 1960 IRE WESCON Convention Record, </booktitle> <volume> 4 </volume> <pages> 96-104. </pages> <address> New York: </address> <note> IRE. Reprinted in Anderson and Rosenfeld [1988]. </note>
Reference-contexts: We choose PIPE and TD-Q because both have already been successfully applied to interesting single-agent tasks (TD-Q also because it is very popular). TD-Q selects actions according to linear neural networks trained with the delta rule <ref> (Widrow and Hoff, 1960) </ref> to map player inputs to evaluations of alternative actions. PIPE is based on probability vector coding of program instructions (Schmidhuber, 1997), Population-Based Incremental Learning (Baluja and Caruana, 1995) and tree coding of programs used in variants of Genetic Programming (GP) (Cramer, 1985; Koza, 1992). <p> Once all fist entries have been processed we start processing the second entries, etc. The nets are trained using the delta-rule <ref> (Widrow and Hoff, 1960) </ref> with learning rate Lr N . 5 Experiments We conduct two different types of simulations simple and complex. During simple simulations we use simple input vectors ~ i s (t; p) and simple actions from ASET S .
Reference: <author> Wiering, M. A. and Schmidhuber, J. </author> <year> (1996). </year> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 534-542. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: In principle, however, EFs are not necessary to find good or optimal policies. Sometimes, particularly in the presence of POPs, it makes more sense to search policy space directly instead of spending too much time on fine-tuning EFs <ref> (Wiering and Schmidhuber, 1996) </ref>. That's what PIPE does. Currently PIPE-like, EF-independent techniques seem more promising for complex multiagent learning scenarios, unless methods for overcoming TD-Q's problems are developed.
References-found: 26

