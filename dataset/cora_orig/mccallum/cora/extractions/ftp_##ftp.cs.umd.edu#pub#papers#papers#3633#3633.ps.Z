URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3633/3633.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: fedjlali,als,saltzg@cs.umd.edu  
Title: Interoperability of Data Parallel Runtime Libraries with  
Author: Meta-Chaos Guy Edjlali, Alan Sussman and Joel Saltz 
Address: College Park, MD 20742  
Affiliation: Department of Computer Science University of Maryland  
Abstract: This paper describes a framework for providing the ability to use multiple specialized data parallel libraries and/or languages within a single application. The ability to use multiple libraries is required in many application areas, such as multidisciplinary complex physical simulations and remote sensing image database applications. An application can consist of one program or multiple programs that use different libraries to parallelize operations on distributed data structures. The framework is embodied in a runtime library called Meta -Chaos that has been used to exchange data between data parallel programs written using High Performance Fortran, the Chaos and Multiblock Parti libraries developed at Maryland for handling various types of unstructured problems, and the runtime library for pC++, a data parallel version of C++ from Indiana University. Experimental results show that Meta-Chaos is able to move data between libraries efficiently, and that Meta-Chaos provides effective support for complex applications.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agrawal, A. Sussman, and J. Saltz. </author> <title> An integrated runtime and compile-time approach for paral-lelizing structured and block structured applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 6(7) </volume> <pages> 747-754, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: These libraries often provide capabilities that do not exist in widely available data parallel languages, such as High Performance Fortran (HPF) [14]. Examples of such data parallel runtime libraries include AMR++ and P++ [17, 20] for adaptive grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti <ref> [1] </ref> and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec [12], PETSc [10], ScaLAPACK [5, 6] and Syisda [3] for linear algebra operations. <p> A Region is an instantiation of a Region type, which must be defined by each data parallel library. For example, High Performance Fortran (HPF) [14] and Multiblock Parti <ref> [1, 25] </ref> utilize arrays as their main distributed data structure, therefore the Region type for them is a regularly distributed array section. Chaos [7, 13, 23] employs irregularly accessed arrays as its main distributed data structure, either through irregular data distributions or accesses through indirection arrays. <p> The first set of experiments, in Sections 5.1 and 5.2, shows two data parallel libraries exchanging data between a structured mesh (distributed by Multiblock Parti <ref> [1] </ref>) and an unstructured mesh (distributed by Chaos [7]), both within a single program and between two separate programs.
Reference: [2] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for structured and block structured applications. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: This scenario would occur, for example, in a multiblock computational fluid dynamics code, where inter-block boundaries must be updated at every time-step <ref> [2] </ref>. The copy operation can be completely expressed using Multiblock Parti functions, for both building the communication schedule and moving the data.
Reference: [3] <author> C. Bischof, S. Huss-Lederman, X. Sun, A. Tsao, and T. Turnbull. </author> <title> Parallel performance of a symmetric eigensolver based on the invariant subspace decomposition approach. </title> <booktitle> In Proceedings of Scalable High Performance Computing Conference, </booktitle> <year> 1994. </year>
Reference-contexts: for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec [12], PETSc [10], ScaLAPACK [5, 6] and Syisda <ref> [3] </ref> for linear algebra operations. Although this list is not exhaustive, it shows the amount of effort that has gone into developing data parallel runtime libraries optimized for various types of applications.
Reference: [4] <author> Francois Bodin, Peter Beckman, Dennis Gannon, Srinivas Narayana, and Shelby X. Yang. </author> <title> Distributed pC++: Basic ideas for an object parallel language. </title> <journal> Scientific Programming, </journal> <volume> 2(3), </volume> <month> Fall </month> <year> 1993. </year>
Reference-contexts: So far, implementations for several data parallel libraries have been completed, including the High Performance Fortran runtime library, the Maryland CHAOS and Multiblock Parti libraries for various types of irregular computations, and the pC++ <ref> [4] </ref> runtime library, Tulip, from Indiana University.
Reference: [5] <author> J. Choi, J. Dongarra, L. Ostrouchov, A. Petitet, D. Walker, and R. Whaley. </author> <title> The design and implementation of the ScaLAPACK, LU, QR, and Cholesky factorization routines. </title> <booktitle> Scientific Programming, </booktitle> <year> 1995. </year>
Reference-contexts: grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec [12], PETSc [10], ScaLAPACK <ref> [5, 6] </ref> and Syisda [3] for linear algebra operations. Although this list is not exhaustive, it shows the amount of effort that has gone into developing data parallel runtime libraries optimized for various types of applications.
Reference: [6] <author> Jaeyoung Choi, David W. Walker, and Jack J. Dongarra. </author> <title> The design of scalable software libraries for distributed memory concurrent computers. </title> <booktitle> In Proceedings of the Eighth International Parallel Processing Symposium, </booktitle> <pages> pages 792-799. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec [12], PETSc [10], ScaLAPACK <ref> [5, 6] </ref> and Syisda [3] for linear algebra operations. Although this list is not exhaustive, it shows the amount of effort that has gone into developing data parallel runtime libraries optimized for various types of applications.
Reference: [7] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication optimizations for irregular scientific computations on distributed memory architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-479, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Examples of such data parallel runtime libraries include AMR++ and P++ [17, 20] for adaptive grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos <ref> [7] </ref> and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec [12], PETSc [10], ScaLAPACK [5, 6] and Syisda [3] for linear algebra operations. <p> For example, High Performance Fortran (HPF) [14] and Multiblock Parti [1, 25] utilize arrays as their main distributed data structure, therefore the Region type for them is a regularly distributed array section. Chaos <ref> [7, 13, 23] </ref> employs irregularly accessed arrays as its main distributed data structure, either through irregular data distributions or accesses through indirection arrays. For Chaos a Region type would be a set of global array indices. A Region type is dependent on the requirements of the data parallel library. <p> The first set of experiments, in Sections 5.1 and 5.2, shows two data parallel libraries exchanging data between a structured mesh (distributed by Multiblock Parti [1]) and an unstructured mesh (distributed by Chaos <ref> [7] </ref>), both within a single program and between two separate programs.
Reference: [8] <author> Al Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 user's guide and reference manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The underlying communication layer required by Meta-Chaos is a point-to-point message passing library, such as MPI [24] or PVM <ref> [8] </ref>, or a vendor-specific library such as MPL for the IBM SP2. The rest of the paper is organized as follows.
Reference: [9] <author> U. Geuder, M. Hardtner, B. Worner, and R. </author> <title> Zin. Scalable execution control of grid-based scientific applications on parallel systems. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94). </booktitle> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: These libraries often provide capabilities that do not exist in widely available data parallel languages, such as High Performance Fortran (HPF) [14]. Examples of such data parallel runtime libraries include AMR++ and P++ [17, 20] for adaptive grid applications, Grids <ref> [9] </ref> for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec [12], PETSc [10], ScaLAPACK [5, 6] and Syisda
Reference: [10] <author> W. Gropp and B. Smith. </author> <title> Scalable, extensible, and portable numerical libraries. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 60-67. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year> <month> 23 </month>
Reference-contexts: for adaptive grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec [12], PETSc <ref> [10] </ref>, ScaLAPACK [5, 6] and Syisda [3] for linear algebra operations. Although this list is not exhaustive, it shows the amount of effort that has gone into developing data parallel runtime libraries optimized for various types of applications.
Reference: [11] <author> Rolf Hempel and Hubert Ritzdorf. </author> <title> The GMD communications library for grid-oriented problems. </title> <type> Technical Report 589, </type> <institution> GMD, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Examples of such data parallel runtime libraries include AMR++ and P++ [17, 20] for adaptive grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti [1] and GMD <ref> [11] </ref> for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec [12], PETSc [10], ScaLAPACK [5, 6] and Syisda [3] for linear algebra operations.
Reference: [12] <author> S. Hutchinson, J. Shadid, and R. Tuminaro. </author> <title> Aztec user's guide, version 1.0. </title> <type> Technical Report SAND95-1559, </type> <institution> Sandia National Laboratories, </institution> <month> Oct </month> <year> 1995. </year>
Reference-contexts: [17, 20] for adaptive grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec <ref> [12] </ref>, PETSc [10], ScaLAPACK [5, 6] and Syisda [3] for linear algebra operations. Although this list is not exhaustive, it shows the amount of effort that has gone into developing data parallel runtime libraries optimized for various types of applications.
Reference: [13] <author> Yuan-Shin Hwang, Bongki Moon, Shamik D. Sharma, Ravi Ponnusamy, Raja Das, and Joel H. Saltz. </author> <title> Runtime and language support for compiling adaptive irregular programs. </title> <journal> Software-Practice and Experience, </journal> <volume> 25(6) </volume> <pages> 597-621, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: For example, High Performance Fortran (HPF) [14] and Multiblock Parti [1, 25] utilize arrays as their main distributed data structure, therefore the Region type for them is a regularly distributed array section. Chaos <ref> [7, 13, 23] </ref> employs irregularly accessed arrays as its main distributed data structure, either through irregular data distributions or accesses through indirection arrays. For Chaos a Region type would be a set of global array indices. A Region type is dependent on the requirements of the data parallel library.
Reference: [14] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Therefore a number of runtime libraries targeted at particular application domains have been developed over the last several years. These libraries often provide capabilities that do not exist in widely available data parallel languages, such as High Performance Fortran (HPF) <ref> [14] </ref>. <p> A Region is an instantiation of a Region type, which must be defined by each data parallel library. For example, High Performance Fortran (HPF) <ref> [14] </ref> and Multiblock Parti [1, 25] utilize arrays as their main distributed data structure, therefore the Region type for them is a regularly distributed array section. Chaos [7, 13, 23] employs irregularly accessed arrays as its main distributed data structure, either through irregular data distributions or accesses through indirection arrays.
Reference: [15] <author> S.R. Kohn and S.B. Baden. </author> <title> A robust parallel programming model for dynamic non-uniform scientific computations. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 509-517. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Examples of such data parallel runtime libraries include AMR++ and P++ [17, 20] for adaptive grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX <ref> [15] </ref> for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec [12], PETSc [10], ScaLAPACK [5, 6] and Syisda [3] for linear algebra operations.
Reference: [16] <author> Antonio Lain and Prithviraj Banerjee. </author> <title> Exploiting spatial regularity in irregular iterative applications. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 820-826. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1995. </year>
Reference-contexts: Examples of such data parallel runtime libraries include AMR++ and P++ [17, 20] for adaptive grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR <ref> [16] </ref> for unstructured grid problems, GA [18] for computational chemistry and Aztec [12], PETSc [10], ScaLAPACK [5, 6] and Syisda [3] for linear algebra operations.
Reference: [17] <author> M. Lemke and D. Quinlan. </author> <title> P++, a C++ virtual shared grids based programming environment for architecture-independant development of structured grid applications. </title> <booktitle> In Proceedings of the Second Joint International Conference on Vector and Parallel Processing (CONPAR'92 - VAPP V), </booktitle> <year> 1992. </year>
Reference-contexts: These libraries often provide capabilities that do not exist in widely available data parallel languages, such as High Performance Fortran (HPF) [14]. Examples of such data parallel runtime libraries include AMR++ and P++ <ref> [17, 20] </ref> for adaptive grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec [12],
Reference: [18] <author> J. Nieplocha, R. Harrison, and R. Littlefield. </author> <title> Global arrays: a portable shared-memory programming model for distributed memory computers. </title> <booktitle> In Proceedings Supercomputing '94, </booktitle> <year> 1994. </year>
Reference-contexts: runtime libraries include AMR++ and P++ [17, 20] for adaptive grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA <ref> [18] </ref> for computational chemistry and Aztec [12], PETSc [10], ScaLAPACK [5, 6] and Syisda [3] for linear algebra operations. Although this list is not exhaustive, it shows the amount of effort that has gone into developing data parallel runtime libraries optimized for various types of applications.
Reference: [19] <author> Object Management Group. </author> <title> The Common Object Request Broker: Architecture and Specification, </title> <year> 1995. </year>
Reference-contexts: We plan to apply the framework-based approach to new application areas, and are currently studying ways to incorporate distributed data parallel objects into the CORBA <ref> [19] </ref> object model, so that data parallel programs could interoperate with distributed object systems. Meta-Chaos could be used as the underlying mechanism for such an extension.
Reference: [20] <author> R. Parsons. </author> <title> A++/P++ array classes for architecture independant finite difference computations. </title> <booktitle> In Proceedings of OONSKI'94, The Object-Oriented Numerics Conference, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: These libraries often provide capabilities that do not exist in widely available data parallel languages, such as High Performance Fortran (HPF) [14]. Examples of such data parallel runtime libraries include AMR++ and P++ <ref> [17, 20] </ref> for adaptive grid applications, Grids [9] for structured and unstructured grid problems, Multiblock Parti [1] and GMD [11] for multigrid and multiblock codes, LPARX [15] for codes using adaptive finite difference methods, Chaos [7] and PILAR [16] for unstructured grid problems, GA [18] for computational chemistry and Aztec [12],
Reference: [21] <author> M. Ranganathan, A. Acharya, G. Edjlali, A. Sussman, and J. Saltz. </author> <title> Runtime coupling of data-parallel programs. </title> <booktitle> In Proceedings of the 1996 International Conference on Supercomputing. </booktitle> <publisher> ACM Press, </publisher> <month> May </month> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Meta-Chaos can also be used to communicate between data parallel libraries in two different programs. Implicitly controlling the coupling between two data parallel programs, using Meta-Chaos to perform the communication, has been explored in another recent paper <ref> [21] </ref>. To show the performance of this feature, the same algorithm that was described in the previous section has been implemented in two separate programs.
Reference: [22] <author> Joel Saltz, Kathleen Crowley, Ravi Mirchandaney, and Harry Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 303-312, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Therefore communication schedules can be generated before the first iteration of the time-step loop and re-used for all time-steps. To effectively parallelize the algorithm, each parallel loop is transformed into two loops, an inspector 13 loop and an executor loop <ref> [22] </ref>. The inspector loop computes a communication schedule and is executed only once. The executor loop performs both the computation from the original loop and the communication required by the schedule, for every time-step.
Reference: [23] <author> Joel Saltz, Ravi Ponnusamy, Shamik D. Sharma, Bongki Moon, Yuan-Shin Hwang, Mustafa Uysal, and Raja Das. </author> <title> A manual for the CHAOS runtime library. </title> <institution> Technical Report CS-TR-3437 and UMIACS-TR-95-34, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: For example, High Performance Fortran (HPF) [14] and Multiblock Parti [1, 25] utilize arrays as their main distributed data structure, therefore the Region type for them is a regularly distributed array section. Chaos <ref> [7, 13, 23] </ref> employs irregularly accessed arrays as its main distributed data structure, either through irregular data distributions or accesses through indirection arrays. For Chaos a Region type would be a set of global array indices. A Region type is dependent on the requirements of the data parallel library.
Reference: [24] <author> Marc Snir, Steve W. Otto, Steven Huss-Lederman, David W. Walker, and Jack Dongarra. </author> <title> MPI: The Complete Reference. Scientific and Engineering Computation Series. </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: The underlying communication layer required by Meta-Chaos is a point-to-point message passing library, such as MPI <ref> [24] </ref> or PVM [8], or a vendor-specific library such as MPL for the IBM SP2. The rest of the paper is organized as follows.
Reference: [25] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the Multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> December </month> <year> 1993. </year> <month> 24 </month>
Reference-contexts: A Region is an instantiation of a Region type, which must be defined by each data parallel library. For example, High Performance Fortran (HPF) [14] and Multiblock Parti <ref> [1, 25] </ref> utilize arrays as their main distributed data structure, therefore the Region type for them is a regularly distributed array section. Chaos [7, 13, 23] employs irregularly accessed arrays as its main distributed data structure, either through irregular data distributions or accesses through indirection arrays.
References-found: 25

