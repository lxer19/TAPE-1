URL: http://neural-server.aston.ac.uk/Papers/postscript/NCRG_97_003.ps.Z
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00355.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Mixtures of Probabilistic Principal Component Analysers  
Author: Michael E. Tipping Christopher M. Bishop 
Address: Birmingham B4 7ET United Kingdom  
Affiliation: Neural Computing Research Group Dept of Computer Science Applied Mathematics Aston University  
Pubnum: Technical Report NCRG/97/003  
Email: M.E.Tipping@aston.ac.uk  C.M.Bishop@aston.ac.uk  
Phone: Tel: +44 (0)121 333 4631  
Date: June 11, 1997  
Web: http://www.ncrg.aston.ac.uk/  
Abstract: Principal component analysis (PCA) is one of the most popular techniques for processing, compressing and visualising data, although its effectiveness is limited by its global linearity. While nonlinear variants of PCA have been proposed, an alternative paradigm is to capture data complexity by a combination of local linear PCA projections. However, conventional PCA does not correspond to a probability density, and so there is no unique way to combine PCA models. Previous attempts to formulate mixture models for PCA have therefore to some extent been ad hoc. In this paper, PCA is formulated within a maximum-likelihood framework, based on a specific form of Gaussian latent variable model. This leads to a well-defined mixture model for probabilistic principal component analysers, whose parameters can be determined using an EM algorithm. We discuss the advantages of this model in the context of clustering, density modelling and local dimensionality reduction, and we demonstrate its application to image compression and handwritten digit recognition. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, T. W. </author> <year> (1963). </year> <title> Asymptotic theory for principal component analysis. </title> <journal> Annals of Mathematical Statistics 34, </journal> <pages> 122-148. </pages>
Reference: <author> Bartholomew, D. J. </author> <year> (1987). </year> <title> Latent Variable Models and Factor Analysis. </title> <address> London: Charles Griffin & Co. </address> <publisher> Ltd. </publisher>
Reference-contexts: Such a model may also be termed `generative', as data vectors t may be generated by sampling from the x and * distributions and applying (1). 2.2 Factor Analysis Perhaps the most common example of a latent variable model is that of statistical factor analysis <ref> (Bartholomew 1987) </ref>, in which the mapping y (x; w) is a linear function of x: t = Wx + + *; (2) where the latent variables x ~ N (0; I) are defined to have a unit covariance isotropic Gaussian distribution.
Reference: <author> Basilevsky, A. </author> <year> (1994). </year> <title> Statistical Factor Analysis and Related Methods. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Bishop, C. M. </author> <year> (1995). </year> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford: Clarendon Press. </publisher>
Reference-contexts: (t n ) then in Appendix C it is shown that we obtain parameter updates: e i = N n e i = n R ni t n n R ni Thus the updates for e i and e i correspond exactly to those of a standard Gaussian mixture formulation <ref> (Bishop 1995) </ref>.
Reference: <author> Bishop, C. M., M. Svensen, and C. K. I. Williams (1997). GTM: </author> <title> the generative topographic mapping. </title> <note> Accepted for publication in Neural Computation. Available as NCRG/96/015 from http://www.ncrg.aston.ac.uk/. </note>
Reference: <author> Bregler, C. and S. M. </author> <month> Omohundro </month> <year> (1995). </year> <title> Nonlinear image interpolation using manifold learning. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pp. 973-980. </pages> <address> Cambridge, Mass: </address> <publisher> MIT Press. </publisher>
Reference: <author> Broomhead, D. S., R. Indik, A. C. Newell, and D. A. </author> <title> Rand (1991). Local adaptive Galerkin bases for large-dimensional dynamical systems. </title> <type> Nonlinearity 4 (1), </type> <pages> 159-197. </pages>
Reference: <author> Dempster, A. P., N. M. Laird, and D. B. </author> <title> Rubin (1977). Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B 39 (1), </volume> <pages> 1-38. </pages>
Reference: <author> Dony, R. D. and S. </author> <title> Haykin (1995). Optimally adaptive transform coding. </title> <booktitle> IEEE Transactions on Image Processing 4 (10), </booktitle> <pages> 1358-1370. </pages>
Reference: <author> Hastie, T. and W. </author> <title> Stuetzle (1989). Principal curves. </title> <journal> Journal of the American Statistical Association 84, </journal> <pages> 502-516. </pages>
Reference-contexts: Because PCA only defines a linear projection of the data, the scope of its application is necessarily somewhat limited. This has naturally motivated various developments of nonlinear principal component analysis in an effort to retain a greater proportion of the variance using fewer components. Examples include principal curves <ref> (Hastie and Stuetzle 1989) </ref>, multi-layer auto-associative neural networks (Kramer 1991), the kernel-function approach of Webb (1996) and the generative topographic mapping, or GTM, of Bishop, Svensen, and Williams (1997).
Reference: <author> Hinton, G. E., P. Dayan, and M. </author> <month> Revow </month> <year> (1997). </year> <title> Modelling the manifolds of images of handwritten digits. </title> <booktitle> IEEE Transactions on Neural Networks 8 (1), </booktitle> <pages> 65-74. </pages>
Reference-contexts: A related model has recently been exploited for data visualization (Tipping and Bishop 1996), while a similar approach, based on the standard factor analysis diagonal () noise model, has been employed for handwritten digit recognition <ref> (Hinton et al. 1997) </ref>, although it does not implement PCA. We can develop an iterative EM algorithm for optimisation of all the parameters i , i , W i and 2 i .
Reference: <author> Hinton, G. E., M. Revow, and P. </author> <title> Dayan (1995). Recognizing handwritten digits using mixtures of linear models. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pp. 1015-1022. </pages> <address> Cambridge, Mass: </address> <publisher> MIT Press. </publisher>
Reference: <author> Hotelling, H. </author> <year> (1933). </year> <title> Analysis of a complex of statistical variables into principal components. </title> <journal> Journal of Educational Psychology 24, </journal> <pages> 417-441. </pages>
Reference: <author> Hull, J. J. </author> <year> (1994). </year> <title> A database for handwritten text recognition research. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 16, </journal> <pages> 550-554. </pages>
Reference-contexts: Hinton et al. (1997) gave an excellent discussion of the handwritten digit problem, and applied a mixture of PCA approach, using soft reconstruction-based clustering, to the classification of scaled and smoothed 8-by-8 gray-scale images taken from the CEDAR U.S. postal service database <ref> (Hull 1994) </ref>. The models were constructed using an 11,000-digit subset of the `br ' data set (which was further split into training and validation sets), and the `bs' test set was classified according to which model best reconstructed each digit.
Reference: <author> Jolliffe, I. T. </author> <year> (1986). </year> <title> Principal Component Analysis. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: 1 Introduction Principal component analysis (PCA) <ref> (Jolliffe 1986) </ref> has proven to be an exceedingly popular technique for dimensionality reduction and is discussed at length in most texts on multivariate analysis. Its many application areas include data compression, image analysis, visualization, pattern recognition, regression and time series prediction.
Reference: <author> Jordan, M. I. and R. A. </author> <title> Jacobs (1994). Hierarchical mixtures of experts and the EM algorithm. </title> <booktitle> Neural Computation 6 (2), </booktitle> <pages> 181-214. </pages>
Reference-contexts: However, an alternative paradigm to such global nonlinear approaches is to model nonlinear structure with a collection, or mixture, of local linear sub-models. This philosophy is an attractive one, motivating, for example, the `mixture of experts' technique for regression <ref> (Jordan and Jacobs 1994) </ref>. Previous attempts to construct mixtures of PCA models have proved problematic, due mainly to the absence of an overall differentiable cost function which can be minimised with respect to all the model parameters.
Reference: <author> Kambhatla, N. and T. K. </author> <title> Leen (1994). Fast non-linear dimension reduction. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pp. 152-159. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kramer, M. A. </author> <year> (1991). </year> <title> Nonlinear principal component analysis using autoassociative neural networks. </title> <journal> AIChE Journal 37 (2), </journal> <pages> 233-243. </pages>
Reference-contexts: This has naturally motivated various developments of nonlinear principal component analysis in an effort to retain a greater proportion of the variance using fewer components. Examples include principal curves (Hastie and Stuetzle 1989), multi-layer auto-associative neural networks <ref> (Kramer 1991) </ref>, the kernel-function approach of Webb (1996) and the generative topographic mapping, or GTM, of Bishop, Svensen, and Williams (1997). However, an alternative paradigm to such global nonlinear approaches is to model nonlinear structure with a collection, or mixture, of local linear sub-models.
Reference: <author> Krzanowski, W. J. and F. H. C. </author> <title> Marriott (1994). Multivariate Analysis Part I: Distributions, Ordination and Inference. </title> <publisher> London: Edward Arnold. </publisher>
Reference-contexts: To show this we consider the derivative of (10) with respect to W: @L = N (C 1 SC 1 W C 1 W); (12) which may be obtained from standard matrix differentiation results <ref> (e.g. see Krzanowski and Marriott 1994, pp 133) </ref>.
Reference: <author> Pearson, K. </author> <year> (1901). </year> <title> On lines and planes of closest fit to systems of points in space. </title> <journal> The London, Edinburgh and Dublin Philosophical Magazine and Journal of Science, </journal> <volume> Sixth Series 2, </volume> <pages> 559-572. </pages>
Reference: <author> Rao, C. R. </author> <year> (1955). </year> <title> Estimation and tests of significance in factor analysis. </title> <type> Psychometrika 20, </type> <pages> 93-111. </pages>
Reference-contexts: Nevertheless, certain links between the two methods have previously been noted. For instance, it has been observed that the factor loadings and the principal axes are quite similar in situations where the estimates of the elements of turn out to be approximately equal <ref> (e.g. Rao 1955) </ref>. Anderson (1963) further remarked that principal components emerge when the data is assumed to comprise a systematic component, plus an independent error term for each variable with common variance 2 , again implying that the diagonal elements of are identical.
Reference: <author> Rubin, D. B. and D. T. </author> <title> Thayer (1982). EM algorithms for ML factor analysis. </title> <type> Psychometrika 47 (1), </type> <pages> 69-76. </pages>
Reference-contexts: without loss of information, and reconstructed using ^ t n = W ML f (W ML ) T W ML g 1 ^ x n + . 3.3 An EM Algorithm For PCA By a simple extension of the EM formulation for parameter estimation in the standard linear factor model <ref> (Rubin and Thayer 1982) </ref>, we can obtain a principal component projection by max-imising the likelihood function (10).
Reference: <author> Tipping, M. E. and C. M. </author> <title> Bishop (1996). A hierarchical latent variable model for data visualization. </title> <type> Technical report NCRG/96/028, </type> <institution> Neural Computing Research Group, Aston University, Aston Street, Birmingham, </institution> <address> B4 7ET, UK. </address>
Reference-contexts: Note that a separate mean vector i is now associated with each of the M mixture components, along with the parameters W i and 2 i . A related model has recently been exploited for data visualization <ref> (Tipping and Bishop 1996) </ref>, while a similar approach, based on the standard factor analysis diagonal () noise model, has been employed for handwritten digit recognition (Hinton et al. 1997), although it does not implement PCA.
Reference: <author> Titterington, D. M., A. F. M. Smith, and U. E. </author> <month> Makov </month> <year> (1985). </year> <title> The Statistical Analysis of Finite Mixture Distributions. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Webb, A. R. </author> <year> (1996). </year> <title> An approach to nonlinear principal components-analysis using radially symmetrical kernel functions. Statistics and Computing 6 (2), 159-168. Mixtures of Probabilistic Principal Component Analysers 20 </title>
References-found: 25

