URL: ftp://ftp.cs.wisc.edu/wwt/sc94_paging.ps
Refering-URL: http://www.cs.wisc.edu/~wwt/wwt_papers.html
Root-URL: 
Email: wwt@cs.wisc.edu  
Title: Paging Tradeoffs in Distributed-Shared-Memory Multiprocessors  
Author: Douglas C. Burger, Rahmat S. Hyder, Barton P. Miller, David A. Wood 
Address: 1210 W. Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Note: Appears in: "Supercomputing '94," Nov. 1994. Reprinted by permission of IEEE.  
Abstract: Massively parallel processors have begun using commodity operating systems that support demand-paged virtual memory. To evaluate the utility of virtual memory, we measured the behavior of seven shared-memory parallel application programs on a simulated distributed-shared-memory machine. Our results (i) confirm the importance of gang CPU scheduling, (ii) show that a page-faulting processor should spin rather than invoke a parallel context switch, (iii) show that our parallel programs frequently touch most of their data, and (iv) indicate that memory, not just CPUs, must be "gang scheduled". Overall, our experiments demonstrate that demand paging has limited value on current parallel machines because of the applications' synchronization and memory reference patterns and the machines' high page-fault and parallel-context-switch overheads. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An evaluation of directory schemes for cache coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference-contexts: The nodes are connected by a point-to-point network (100 cycle constant latency) and coherence is maintained through a full-map directory protocol (i.e., dir n N B <ref> [1] </ref>). The address space is globally shared, with 4KB shared pages assigned to nodes round-robin. On a page fault, the target operating system selects a victim page using the Clock [5] algorithm. To maintain inclusion, the system invalidates all cached blocks from the victim page.
Reference: [2] <author> Tom Anderson. </author> <title> NOW: Distributed supercomputing on a network of workstations, September 1993. </title> <booktitle> Presentation at 1993 Fall ARPA HPC Software PI's meeting. </booktitle>
Reference-contexts: Furthermore, simple schemes that allocate fixed memory partitions on each node are unlikely to be effective, since many applications have unequal requirements for private pages. While there have been numerous proposals to manage processors globally <ref> [2] </ref>, we believe these are the first results indicating the importance of doing so for physical memory. Demand paging would become more attractive for parallel applications if we can decrease either the page fault service time (T pf ) or parallel context switch overhead (T pcs ).
Reference: [3] <author> Tom Anderson, David Culler, and David Patterson. </author> <title> A case for networks of workstations: NOW. </title> <type> Technical report, </type> <institution> Computer Science Division (EECS), University of California at Berkeley, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Recently, however, massively parallel supercomputers have begun running modified workstation operating systems: the Intel Paragon and Convex SPP-1 run modified versions of Mach OSF/1 AD and the Meiko CS-2 runs a modified version of Solaris. Furthermore, clusters of workstations are emerging as increasingly popular alternatives to dedicated parallel supercomputers <ref> [3, 18, 7] </ref>. Parallel applications on these systems must co-exist with the operating system's demand-paged virtual memory. In this paper we examine the performance of seven shared-memory scientific applications and argue that demand paging has limited value on distributed-shared-memory parallel computers.
Reference: [4] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report RNR-91-002 Revision 2, </type> <institution> Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: The benchmark programs are scientific applications drawn from a variety of sources, including the Stanford SPLASH benchmarks [15], the NAS benchmarks <ref> [4] </ref>, and the Schlumberger Corp. <p> Of the seven, Barnes, Locus, Mp3d, and Ocean are from the SPLASH benchmark suite [15]. Appbt is a locally-parallelized version of one of the NAS Parallel Benchmarks <ref> [4] </ref>. Laplace was developed at Wisconsin [17], and Wave is a proprietary code from the Schlumberger Corporation. The last column in Table 1 contains the total number of data pages touched by the applications.
Reference: [5] <author> F. J. Corbato. </author> <title> A paging experiment with the Multics system. </title> <type> Technical Report MAC-M-384, </type> <institution> MIT, </institution> <month> May </month> <year> 1968. </year>
Reference-contexts: The address space is globally shared, with 4KB shared pages assigned to nodes round-robin. On a page fault, the target operating system selects a victim page using the Clock <ref> [5] </ref> algorithm. To maintain inclusion, the system invalidates all cached blocks from the victim page. <p> In this experiment, each simulated processing node periodically invalidates a virtual memory page, to approximate the effect of having that page frame assigned to another process. Pages are selected for invalidation using the Clock <ref> [5] </ref> algorithm; pages that have not been referenced recently (as calculated by Clock), are candidates for invalidation. Pages are invalidated at random times (exponential interarrival time distribution with mean 512 ms). We timed the execution of each program in two ways.
Reference: [6] <author> P. J. Denning. </author> <title> The working set model of program behavior. </title> <journal> Communications of the ACM, </journal> <volume> 11(5) </volume> <pages> 323-333, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: We first analyze the applications using the Working Set model of program behavior <ref> [6] </ref>, which describes memory access patterns in terms of localities. A program's working set at time t with parameter t is defined as the set of pages touched by the program during the last t time units (t t; t).
Reference: [7] <author> J. Dongarra, G. A. Geist, R. Manchek, and V. S. Sunderam. </author> <title> Integrated PVM framework supports heterogeneous network computing. </title> <journal> Computers in Physics, </journal> <volume> 7(2) </volume> <pages> 166-174, </pages> <month> March-April </month> <year> 1993. </year>
Reference-contexts: Recently, however, massively parallel supercomputers have begun running modified workstation operating systems: the Intel Paragon and Convex SPP-1 run modified versions of Mach OSF/1 AD and the Meiko CS-2 runs a modified version of Solaris. Furthermore, clusters of workstations are emerging as increasingly popular alternatives to dedicated parallel supercomputers <ref> [3, 18, 7] </ref>. Parallel applications on these systems must co-exist with the operating system's demand-paged virtual memory. In this paper we examine the performance of seven shared-memory scientific applications and argue that demand paging has limited value on distributed-shared-memory parallel computers.
Reference: [8] <author> Derek L. Eager, John Zahorjan, and Edward D. Lazowska. </author> <title> Speedup versus efficiency in parallel systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(3) </volume> <pages> 408-423, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: To address this problem, some previous studies have argued that parallel machines should employ gang scheduling, where all processing nodes simultaneously execution time switch to the same parallel job [12, 10]. Others have argued for space-sharing <ref> [8] </ref>, where processing nodes are dedicated to a parallel program until it completes. Space sharing can also be thought of as the limiting case of gang scheduling, with the scheduling quantum set to infinity.
Reference: [9] <author> Liviu Iftode, Kai Li, and Karin Petersen. </author> <title> Memory servers for multicomputers. </title> <booktitle> In Proceedings of 1993 Spring CompCon, </booktitle> <pages> pages 538-547, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The high parallel context switch overhead on current MPP machines (e.g, the CM-5) is greater than the cycles lost by spinning. This is especially true when paging to a memory server <ref> [9] </ref> or fast paging device rather than a traditional disk. * The parallel programs we studied frequently access most of their data. <p> T pf could be reduced using standard techniques such as faster disks or dedicated paging memory (e.g., "solid-state disks"). Alternatively, Iftode, et al., have proposed dedicating some processing nodes as "memory servers" <ref> [9] </ref>, which use their physical memories as fast paging stores. Such an approach might make use of the memory of idle or underutilized workstations in a network or cluster of workstations. Faster parallel context switches would allow useful work to be overlapped with the page fault service time.
Reference: [10] <author> Scott T. </author> <title> Leutenegger. Issues in Multiprogrammed Multiprocessor Scheduling. </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Scheduling processors independently can magnify these delays by descheduling a process involved in the synchronization. To address this problem, some previous studies have argued that parallel machines should employ gang scheduling, where all processing nodes simultaneously execution time switch to the same parallel job <ref> [12, 10] </ref>. Others have argued for space-sharing [8], where processing nodes are dedicated to a parallel program until it completes. Space sharing can also be thought of as the limiting case of gang scheduling, with the scheduling quantum set to infinity.
Reference: [11] <author> Scott T. Leutenegger and Xian-He Sun. </author> <title> Distributed computing feasibility in a non-dedicated homogenous distributed system. </title> <booktitle> In Proceedings of ACM Supercomputing '93, </booktitle> <year> 1993. </year>
Reference-contexts: Even a single memory-constrained node can degrade perfor mance by over a factor of two. The frequency of (blocking or spinning) synchronization is key to determining the appropriate CPU and memory scheduling policies. Coarse-grain parallel applications|those with little synchronization| can be scheduled exactly as sequential tasks <ref> [11] </ref>. As synchronization grows more frequent, however, the impact of delaying any one node increases dramatically. A page fault on one node can cause cascading delays on other nodes.
Reference: [12] <author> John K. Ousterhout, Donald A. Scelza, and Pradeep S. Sindhu. </author> <title> Medusa: An experiment in distributed operating system structure. </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 92-105, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: Scheduling processors independently can magnify these delays by descheduling a process involved in the synchronization. To address this problem, some previous studies have argued that parallel machines should employ gang scheduling, where all processing nodes simultaneously execution time switch to the same parallel job <ref> [12, 10] </ref>. Others have argued for space-sharing [8], where processing nodes are dedicated to a parallel program until it completes. Space sharing can also be thought of as the limiting case of gang scheduling, with the scheduling quantum set to infinity.
Reference: [13] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wiscon-sin Wind Tunnel: Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The benchmark programs are scientific applications drawn from a variety of sources, including the Stanford SPLASH benchmarks [15], the NAS benchmarks [4], and the Schlumberger Corp. We simulated the execution of these programs on our hypothetical DSM system using the Wisconsin Wind Tunnel <ref> [13] </ref>. 2.1 A DSM Machine Model Our target hardware system contains 32 processing nodes, each with 33 Mhz SPARC CPU, 256-KB cache (4-way associative, 32-byte blocks, random replacement), and the local portion of the distributed shared memory. <p> All applications use a locally-modified version of the PARMACS macro package and assume a process-per-processor computation model (i.e., processes are always scheduled on the same processing node). 2.3 Simulation Environment The Wisconsin Wind Tunnel (WWT) <ref> [13] </ref> is a parallel, discrete-event simulator for cache-coherent, shared-memory multiprocessors that runs on a Thinking Machines CM-5.
Reference: [14] <author> Eric Sharakan. </author> <type> Personal communication., </type> <month> April </month> <year> 1994. </year>
Reference-contexts: Unfortunately, current massively parallel processors incur substantial overhead for a full parallel context switch. For example, the Thinking Machines CM-5 incurs a minimum overhead of 4 ms, with typical times closer to 10 ms <ref> [14] </ref>.
Reference: [15] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> Splash: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The benchmark programs are scientific applications drawn from a variety of sources, including the Stanford SPLASH benchmarks <ref> [15] </ref>, the NAS benchmarks [4], and the Schlumberger Corp. <p> Of the seven, Barnes, Locus, Mp3d, and Ocean are from the SPLASH benchmark suite <ref> [15] </ref>. Appbt is a locally-parallelized version of one of the NAS Parallel Benchmarks [4]. Laplace was developed at Wisconsin [17], and Wave is a proprietary code from the Schlumberger Corporation. The last column in Table 1 contains the total number of data pages touched by the applications.
Reference: [16] <author> D. Thiebaut and H.S. Stone. </author> <title> Footprints in the cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4) </volume> <pages> 305-329, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: The efficacy of the LUG policy depends highly on i) having a mix of parallel and sequential jobs and ii) the cache and TLB "footprint" <ref> [16] </ref> of the scheduled local process. While we believe such hybrid workloads may become common, characterizing their overheads is outside the scope of this paper.
Reference: [17] <author> F. Traenkle. </author> <title> Parallel programming models and boundary integral equation methods for microstructure electrostatics. </title> <type> Master's thesis, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1993. </year>
Reference-contexts: Of the seven, Barnes, Locus, Mp3d, and Ocean are from the SPLASH benchmark suite [15]. Appbt is a locally-parallelized version of one of the NAS Parallel Benchmarks [4]. Laplace was developed at Wisconsin <ref> [17] </ref>, and Wave is a proprietary code from the Schlumberger Corporation. The last column in Table 1 contains the total number of data pages touched by the applications.
Reference: [18] <author> Songnian Zhou, Jingwen Wang, Xiaohu Zheng, and Pierre Delisle. </author> <title> Utopia: A load sharing system for large, heterogeneous distributed computer systems. </title> <type> Technical report, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> April </month> <year> 1992. </year> <type> CSRI Technical Report #257. 9 10 </type>
Reference-contexts: Recently, however, massively parallel supercomputers have begun running modified workstation operating systems: the Intel Paragon and Convex SPP-1 run modified versions of Mach OSF/1 AD and the Meiko CS-2 runs a modified version of Solaris. Furthermore, clusters of workstations are emerging as increasingly popular alternatives to dedicated parallel supercomputers <ref> [3, 18, 7] </ref>. Parallel applications on these systems must co-exist with the operating system's demand-paged virtual memory. In this paper we examine the performance of seven shared-memory scientific applications and argue that demand paging has limited value on distributed-shared-memory parallel computers.
References-found: 18

