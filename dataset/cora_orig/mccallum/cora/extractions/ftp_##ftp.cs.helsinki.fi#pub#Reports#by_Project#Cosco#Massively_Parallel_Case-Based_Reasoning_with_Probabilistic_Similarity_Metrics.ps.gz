URL: ftp://ftp.cs.helsinki.fi/pub/Reports/by_Project/Cosco/Massively_Parallel_Case-Based_Reasoning_with_Probabilistic_Similarity_Metrics.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~tirri/publications.html
Root-URL: 
Email: email: Petri.Myllymaki@cs.Helsinki.FI, Henry.Tirri@cs.Helsinki.FI  
Title: Massively Parallel Case-Based Reasoning with Probabilistic Similarity Metrics  
Author: Petri Myllymaki and Henry Tirri 
Note: Notes in Artificial Intelligence. Springer Verlag, 1994.  
Address: P.O.Box 26, FIN-00014 University of Helsinki, Finland  
Affiliation: University of Helsinki, Department of Computer Science  
Abstract: We propose a probabilistic case-space metric for the case matching and case adaptation tasks. Central to our approach is a probability propagation algorithm adopted from Bayesian reasoning systems, which allows our case-based reasoning system to perform theoretically sound probabilistic reasoning. The same probability propagation mechanism actually offers a uniform solution to both the case matching and case adaptation problems. We also show how the algorithm can be implemented as a connectionist network, where efficient massively parallel case retrieval is an inherent property of the system. We argue that using this kind of an approach, the difficult problem of case indexing can be completely avoided. Pp. 144-154 in Topics in Case-Based Reasoning, edited by Stefan Wess, Klaus-Dieter Althoff and Michael M. Richter. Volume 837, Lecture 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G.F. Cooper, </author> <title> Probabilistic Inference using Belief Networks is NP-hard. </title> <type> Technical Report KSL-87-27, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <year> 1987. </year>
Reference-contexts: However, the network in Fig. 4a is not singly connected, which means that there are loops in the underlying network, if the direction of the arcs is disregarded. In this case, the problem of calculating the above mentioned probabilities can be shown to be NP-hard <ref> [1] </ref>. One approach to overcome this problem is to use stochastic simulation schemes such as Gibbs sampling [3] for approximating the outcome of the updating process. In our earlier work [7, 8, 12] we presented schemes for implementing Gibbs sampling on a connectionist network architecture.
Reference: 2. <author> DARPA, </author> <booktitle> Proceedings of the Workshop on Case-Based Reasoning 1988-1991. </booktitle> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Base Constructor Case Matcher Case Adaptor Ranked cases Matching metrics Adaptation metrics Case Base New cases Input case Output case User Learning Reasoning Fig. 1. The structure of our CBR system. task <ref> [2] </ref>. We adopt an alternative approach and show how to construct a massively parallel implementation of CBR confirming to the so called connectionist architecture (see e.g. [14]).
Reference: 3. <author> S. Geman and D. Geman, </author> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence 6 (1984), </journal> <pages> 721-741. </pages>
Reference-contexts: In this case, the problem of calculating the above mentioned probabilities can be shown to be NP-hard [1]. One approach to overcome this problem is to use stochastic simulation schemes such as Gibbs sampling <ref> [3] </ref> for approximating the outcome of the updating process. In our earlier work [7, 8, 12] we presented schemes for implementing Gibbs sampling on a connectionist network architecture.
Reference: 4. <author> H. Kitano and M. Yasunaga, </author> <title> Wafer Scale Integration for Massively Parallel Memory-Based Reasoning. Pp. </title> <booktitle> 850 - 856 in: Proc. of the Tenth National Conference on Artificial Intelligence (San Jose, </booktitle> <publisher> July 1992) AAAI Press/MIT Press, </publisher> <address> Menlo Park, CA, </address> <year> 1992. </year>
Reference-contexts: This offers a linear speed-up in the comparison process with respect to the number of computational elements available. This type of an ap proach allows matching of the input against millions of stored cases efficiently <ref> [4] </ref>. 3 Bayesian Case-Based Reasoning In our Bayesian framework, we assume that the problem domain knowledge is represented using values of m discrete attributes (random variables) A 1 ; : : : ; A m .
Reference: 5. <author> H. Kitano, </author> <title> Challenges of Massive Parallelism. Pp. </title> <booktitle> 813-834 in: Proceedings of IJCAI-93, the Thirteenth International Joint Conference on Artificial Intelligence (Chambery, </booktitle> <address> France, August 1993). </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: This type of memory-based or instance-based reasoning systems have recently shown very promising results in solving difficult real-world problems, such as speech-to-speech translation <ref> [5] </ref>. <p> This is especially interesting as it has been recently argued that massive parallelism has an fundamental effect on the development of AI in general <ref> [5] </ref>. In addition to using connectionist models for avoiding the case indexing problem, we will also propose a uniform solution to the problem of choosing proper metrics for case matching and adaptation.
Reference: 6. <author> S. L. Lauritzen and D. J. Spiegelhalter, </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> J. Royal Stat. Soc., Ser. B 1989. </journal> <note> Reprinted as pp. 415-448 in: Readings in Uncertain Reasoning (G. </note> <editor> Shafer and J. Pearl, eds.). </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: However, the problem of determining the so called annealing schedule has proven very hard in practice, resulting to slow convergence of the algorithm. On the other hand, for singly-connected networks, there exists a polynomial time algorithm for belief updating, developed by Pearl [13]. In the approach introduced in <ref> [6] </ref>, a given belief network is first transformed to a singly-connected network, which is then updated by using Pearl's algorithm. However, as the problem is NP-hard, the transformation process may take an exponential time.
Reference: 7. <author> P. Myllymaki, </author> <title> Bayesian Reasoning by Stochastic Neural Networks. Ph. Lic. </title> <type> Thesis, Report C-1993-67, </type> <institution> Department of Computer Science, University of Helsinki, </institution> <year> 1993. </year>
Reference-contexts: In this case, the problem of calculating the above mentioned probabilities can be shown to be NP-hard [1]. One approach to overcome this problem is to use stochastic simulation schemes such as Gibbs sampling [3] for approximating the outcome of the updating process. In our earlier work <ref> [7, 8, 12] </ref> we presented schemes for implementing Gibbs sampling on a connectionist network architecture. However, the problem of determining the so called annealing schedule has proven very hard in practice, resulting to slow convergence of the algorithm.
Reference: 8. <author> P. Myllymaki and P. Orponen, </author> <booktitle> Programming the Harmonium. </booktitle> <pages> Pp. </pages> <booktitle> 671-677 in: Proc. of the International Joint Conf. on Neural Networks (Singapore, Nov. 1991), </booktitle> <volume> Vol. 1. </volume> <publisher> IEEE, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: In this case, the problem of calculating the above mentioned probabilities can be shown to be NP-hard [1]. One approach to overcome this problem is to use stochastic simulation schemes such as Gibbs sampling [3] for approximating the outcome of the updating process. In our earlier work <ref> [7, 8, 12] </ref> we presented schemes for implementing Gibbs sampling on a connectionist network architecture. However, the problem of determining the so called annealing schedule has proven very hard in practice, resulting to slow convergence of the algorithm.
Reference: 9. <author> P. Myllymaki and H. Tirri, </author> <title> Bayesian Case-Based Reasoning with Neural Networks. Pp. </title> <booktitle> 422-427 in: Proc. of the IEEE International Conf. on Neural Networks (San Francisco, March 1993), </booktitle> <volume> Vol. 1. </volume> <publisher> IEEE, </publisher> <address> Piscataway, NJ, </address> <year> 1993. </year>
Reference-contexts: In an earlier paper <ref> [9] </ref> we discussed a related directed 6-layer feedforward neural network architecture, which has a more complex structure than the connectionistic network presented here, but used simpler computational elements.
Reference: 10. <author> P. Myllymaki and H. Tirri, </author> <title> Learning in neural networks with Bayesian prototypes. </title> <note> Pp. 60-64 in: Proceedings of SOUTHCON'94 (Orlando, </note> <month> March </month> <year> 1994). </year>
Reference-contexts: Initial attemps towards a theoretically solid learning algorithm for the Bayesian CBR model are reported in <ref> [10, 15] </ref>. 153
Reference: 11. <author> R. </author> <title> Neapolitan, Probabilistic Reasoning in Expert Systems. </title> <publisher> Wiley Interscience, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: To avoid resorting to such ad hoc heuristic solutions in the reasoning process, we propose that one uses methods developed for Bayesian probabilistic reasoning systems [13], <ref> [11] </ref>. In Section 3, we show how our Bayesian reasoning framework, which we call Bayesian case-based reasoning, offers us a uniform similarity met-rics for both the case matching and adaptation tasks.
Reference: 12. <author> P. Orponen, P. Floreen, P. Myllymaki, H. Tirri, </author> <title> A neural implementation of conceptual hierarchies with Bayesian reasoning. Pp. </title> <booktitle> 297-303 in: Proc. of the International Joint Conf. on Neural Networks (San Diego, </booktitle> <address> CA, </address> <month> June </month> <year> 1990), </year> <note> Vol. </note> <editor> I. </editor> <publisher> IEEE, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: In this case, the problem of calculating the above mentioned probabilities can be shown to be NP-hard [1]. One approach to overcome this problem is to use stochastic simulation schemes such as Gibbs sampling [3] for approximating the outcome of the updating process. In our earlier work <ref> [7, 8, 12] </ref> we presented schemes for implementing Gibbs sampling on a connectionist network architecture. However, the problem of determining the so called annealing schedule has proven very hard in practice, resulting to slow convergence of the algorithm.
Reference: 13. <author> J. Pearl, </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: To avoid resorting to such ad hoc heuristic solutions in the reasoning process, we propose that one uses methods developed for Bayesian probabilistic reasoning systems <ref> [13] </ref>, [11]. In Section 3, we show how our Bayesian reasoning framework, which we call Bayesian case-based reasoning, offers us a uniform similarity met-rics for both the case matching and adaptation tasks. <p> However, in our model the components can also have values between 0 and 1, which means that a case can be regarded as a "prototype" representation of a class of (in some sense) similar instances. Our case base can now be represented as a Bayesian belief network <ref> [13] </ref>, consisting of variables A 1 ; : : : ; A m and C 1 ; : : : ; C l (see Fig. 4a). <p> : ; P 0 (a 1n 1 ); P 0 (a 21 ); : : : ; P 0 (a 2n 2 ); : : : ; P 0 (a m1 ); : : : ; P 0 (a mn m )); where according to the idea of virtual evidence <ref> [13] </ref>, if there exists some initial evidence e for the value a ij of the attribute A i , the value P 0 (a ij ) should be set equal to the probability P (e j A i = a ij ). <p> However, the problem of determining the so called annealing schedule has proven very hard in practice, resulting to slow convergence of the algorithm. On the other hand, for singly-connected networks, there exists a polynomial time algorithm for belief updating, developed by Pearl <ref> [13] </ref>. In the approach introduced in [6], a given belief network is first transformed to a singly-connected network, which is then updated by using Pearl's algorithm. However, as the problem is NP-hard, the transformation process may take an exponential time. <p> In general this allows all stored cases to contribute to the adaptation by the amount justified by their original matching. Using the notation of Pearl in <ref> [13] </ref>, the task of the step 2 is to compute 151 a 11 a 1j a 1n 1 a i1 a ij a in i a i1 a mj a mn m A 11 A 1k A 1l A i1 A ik A il A m1 A mk A ml Q
Reference: 14. <author> D.E. </author> <title> Rumelhart and J.L.McClelland (eds.),Parallel distributed processing: </title> <journal> explorations in the microstructures of cognition. </journal> <volume> Vol 1,2. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The structure of our CBR system. task [2]. We adopt an alternative approach and show how to construct a massively parallel implementation of CBR confirming to the so called connectionist architecture (see e.g. <ref> [14] </ref>). Connectionist networks are constructed from a large amount of elements with an input fan order of magnitudes larger than in computational elements of conventional architectures.
Reference: 15. <author> H. Tirri and P. Myllymaki, </author> <title> MDL learning of probabilistic neural networks for discrete problem domains. </title> <booktitle> To be presented at the IEEE World Congress on Computational Intelligence (Orlando, </booktitle> <month> June </month> <year> 1994). </year> <title> This article was processed using the L a T E X macro package with LLNCS style 154 </title>
Reference-contexts: Initial attemps towards a theoretically solid learning algorithm for the Bayesian CBR model are reported in <ref> [10, 15] </ref>. 153
References-found: 15

