URL: http://www.aic.nrl.navy.mil/papers/1996/AIC-96-006.ps.Z
Refering-URL: http://mnemosyne.itc.it:1024/avesani/html/rds-sem.html
Root-URL: 
Title: A Review and Empirical Evaluation of Feature Weighting Methods for a Class of Lazy Learning Algorithms  
Author: Dietrich Wettschereck David W. Aha Takao Mohri 
Keyword: lazy learning, k-nearest neighbor, feature weights, empirical comparison  
Abstract: Many lazy learning algorithms are derivatives of the k-nearest neighbor (k-NN) classifier, which uses a distance function to generate predictions from stored instances. Several studies have shown that k-NN's performance is highly sensitive to the definition of its distance function. Many k-NN variants have been proposed to reduce this sensitivity by parameterizing the distance function with feature weights. However, these variants have not been categorized nor empirically compared. This paper reviews a class of weight-setting methods for lazy learning algorithms. We introduce a framework for distinguishing these methods and empirically compare them. We observed four trends from our experiments and conducted further studies to highlight them. Our results suggest that methods which use performance feedback to assign weight settings demonstrated three advantages over other methods: they require less pre-processing, perform better in the presence of interacting features, and generally require less training data to learn good settings. We also found that continuous weighting methods tend to outperform feature selection algorithms for tasks where some features are useful but less important than others.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1991). </year> <title> Incremental constructive induction: An instance-based approach. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning (pp. </booktitle> <pages> 117-121). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: QM2 also is sensitive to concept skew when ordering the new variables. 14 This algorithm performed best among their selected set of lazy algorithms, but hypotheses explaining why have not yet been investigated. Another lazy learning algorithm that supports feature transformation is IB3-CI <ref> (Aha, 1991) </ref>. This is a knowledge-intensive extension of the noise-tolerant IB3 algorithm (Aha et al., 1991). It uses a Bayesian approach, adapted from (Schlimmer, 1987), to direct its search through a space of logical feature combinations, and uses a competitive feature selection approach to assign binary weights. <p> Another lazy learning algorithm that supports feature transformation is IB3-CI (Aha, 1991). This is a knowledge-intensive extension of the noise-tolerant IB3 algorithm <ref> (Aha et al., 1991) </ref>. It uses a Bayesian approach, adapted from (Schlimmer, 1987), to direct its search through a space of logical feature combinations, and uses a competitive feature selection approach to assign binary weights.
Reference: <author> Aha, D. W. </author> <year> (1992). </year> <title> Tolerating noisy, irrelevant, and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 36, </volume> <pages> 267-287. </pages>
Reference-contexts: A different distance function could be applied at each query (query-based) (Atkeson et al., 1996a) or each instance (point-based) <ref> (Aha & Goldstone, 1992) </ref>. Similarly, the definition of feature difference (Equation 7) is one of many; functions other than absolute difference for continuous values and more elaborate functions for defining similarity on discrete features have been proposed (Stanfill & Waltz, 1986). <p> This procedure was expected to assign high weights to relevant features and lower weights to others. Salzberg found that, by selecting good values for , this algorithm consistently improved classification performance vs. Equation 9. Salzberg's algorithm influenced the design of IB4 <ref> (Aha, 1992) </ref>, which updates weight settings using 2 w (f ) = max CumulativeWeight f WeightNormalizer f 0:5; 0 ; (12) where CumulativeWeight f is assumed to asymptote to half of the WeightNormalizer f for seemingly irrelevant features. <p> Similarly, the updating algorithm for instance-specific weights has more influence when these instances are similar. GCM-ISW was shown to correlate significantly better with subject data than did its non-weighting and global weighting variants <ref> (Aha & Goldstone, 1992) </ref>. The subjects' target concept was designed such that feature relevance varied in different parts of the instance space. GCM-ISW's definition of similarity is not symmetric; frequently, it yields d (x; y) 6= d (y; x).
Reference: <author> Aha, D. W., & Bankert, R. L. </author> <year> (1994). </year> <title> Feature selection for case-based classification of cloud types: An empirical comparison. </title> <editor> In D. W. Aha (Ed.) </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: However, it is not clear to what extent they can recognize redundant or highly interacting features. For 2 IB4 learns a separate set of weights per concept, but this is ignored here to simplify the presentation. 8 example, IB4 performed poorly on a task with many partially relevant features <ref> (Aha & Bankert, 1994) </ref>. Batch Optimizers These weighting methods optimize feature weights by repeatedly processing instances. <p> Several researchers have recently used various feature selection methods for lazy learning algorithms. These methods select features using * an induced decision tree (Cardie, 1993; Kibler & Aha, 1987), * random mutation hill-climbing (Skalak, 1994), * parallel search (Moore & Lee, 1994), * beam search with stepwise selection <ref> (Aha & Bankert, 1994) </ref>, and * stepwise feature removal in oblivious decision trees (Langley & Sage, 1994). The first of these methods employs a preset weighting bias while the others exploit performance feedback. All report accuracy and/or speed improvements over 1-NN or k-NN.
Reference: <author> Aha, D. W., & Goldstone, R. L. </author> <year> (1992). </year> <title> Concept learning and flexible weighting. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 534-539). </pages> <address> Bloomington, </address> <publisher> IN: Lawrence Erlbaum. </publisher>
Reference-contexts: A different distance function could be applied at each query (query-based) (Atkeson et al., 1996a) or each instance (point-based) <ref> (Aha & Goldstone, 1992) </ref>. Similarly, the definition of feature difference (Equation 7) is one of many; functions other than absolute difference for continuous values and more elaborate functions for defining similarity on discrete features have been proposed (Stanfill & Waltz, 1986). <p> This procedure was expected to assign high weights to relevant features and lower weights to others. Salzberg found that, by selecting good values for , this algorithm consistently improved classification performance vs. Equation 9. Salzberg's algorithm influenced the design of IB4 <ref> (Aha, 1992) </ref>, which updates weight settings using 2 w (f ) = max CumulativeWeight f WeightNormalizer f 0:5; 0 ; (12) where CumulativeWeight f is assumed to asymptote to half of the WeightNormalizer f for seemingly irrelevant features. <p> Similarly, the updating algorithm for instance-specific weights has more influence when these instances are similar. GCM-ISW was shown to correlate significantly better with subject data than did its non-weighting and global weighting variants <ref> (Aha & Goldstone, 1992) </ref>. The subjects' target concept was designed such that feature relevance varied in different parts of the instance space. GCM-ISW's definition of similarity is not symmetric; frequently, it yields d (x; y) 6= d (y; x).
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference-contexts: QM2 also is sensitive to concept skew when ordering the new variables. 14 This algorithm performed best among their selected set of lazy algorithms, but hypotheses explaining why have not yet been investigated. Another lazy learning algorithm that supports feature transformation is IB3-CI <ref> (Aha, 1991) </ref>. This is a knowledge-intensive extension of the noise-tolerant IB3 algorithm (Aha et al., 1991). It uses a Bayesian approach, adapted from (Schlimmer, 1987), to direct its search through a space of logical feature combinations, and uses a competitive feature selection approach to assign binary weights. <p> Another lazy learning algorithm that supports feature transformation is IB3-CI (Aha, 1991). This is a knowledge-intensive extension of the noise-tolerant IB3 algorithm <ref> (Aha et al., 1991) </ref>. It uses a Bayesian approach, adapted from (Schlimmer, 1987), to direct its search through a space of logical feature combinations, and uses a competitive feature selection approach to assign binary weights.
Reference: <author> Ashley, K. D., & Rissland, E. L. </author> <year> (1988). </year> <title> Waiting on weighting: A symbolic least commitment approach. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence (pp. </booktitle> <pages> 239-244). </pages> <address> St. Paul, MN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Atkeson, C. </author> <year> (1989). </year> <title> Using local models to control movement. </title> <editor> In D. S. Touretzky (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Atkeson, C., Moore, A., & Schaal, S. </author> <year> (1996a). </year> <title> Locally weighted learning. </title> <journal> Artificial Intelligence Review, </journal> <note> this issue. </note>
Reference-contexts: First, we assume that instances are represented by a set of feature-value pairs rather than, for example, directed acyclic graphs, which are sometimes used in the case-based reasoning literature (Kolodner, 1993). Second, we focus on classification as the performance task and ignore issues concerning function learning <ref> (Atkeson et al., 1996a) </ref>, class density estimation (e.g., using parzen windows (Duda & Hart, 1973)), or problem solving (Kolodner, 1993). Furthermore, we assume classes are disjoint. 2.2 The k-NN Classifier A classifier inputs a query instance q and outputs a prediction for its class. <p> It could take on many other forms, such as by replacing 1 () with a function that relates classes or varies on each instance. Also, many different kernel functions (K ()) have been investigated <ref> (Atkeson et al., 1996a) </ref>. Third, while we examine various distance functions, we only examine the effects of replacing Equation 9 with weighting functions that assign unequal weights to features. Many other types of distance functions and weighting methods have been studied. <p> For example, rather than Euclidean or even Minkowskian, distance could be defined using a set-theoretic definition (Tversky, 1977; Biberman, 1994) or by a function other than one which sums independent contributions of the features. A different distance function could be applied at each query (query-based) <ref> (Atkeson et al., 1996a) </ref> or each instance (point-based) (Aha & Goldstone, 1992). Similarly, the definition of feature difference (Equation 7) is one of many; functions other than absolute difference for continuous values and more elaborate functions for defining similarity on discrete features have been proposed (Stanfill & Waltz, 1986).
Reference: <author> Atkeson, C., Moore, A., & Schaal, S. </author> <year> (1996b). </year> <title> Locally weighted learning for control. </title> <journal> Artificial Intelligence Review, </journal> <note> this issue. </note>
Reference: <author> Bakiri, G. </author> <year> (1991). </year> <title> Converting English text to speech: A machine learning approach. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Science, Oregon State University, Corvallis, OR. </institution> <note> 32 Bareiss, </note> <author> R. </author> <year> (1989). </year> <title> The experimental evaluation of a case-based learning apprentice. </title> <booktitle> In Proceedings of a Case-Based Reasoning Workshop (pp. </booktitle> <pages> 162-167). </pages> <address> Pensacola Beach, FL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Battiti, R. </author> <year> (1994). </year> <title> Using mutual information for selecting features in supervised neural net learning. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5, </volume> <pages> 537-550. </pages>
Reference: <author> Biberman, Y. </author> <year> (1994). </year> <title> A context similarity measure. </title> <booktitle> In Proceedings of the European Conference on Machine Learning (pp. </booktitle> <pages> 49-63). </pages> <address> Catania, Italy: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Bottou, L., & Vapnik, V. </author> <year> (1992). </year> <title> Local learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 888-900. </pages>
Reference-contexts: Defer: They store all training data and defer processing until queries are given that require replies. 2 2. Reply: Queries are answered by combining the training data, typically by using a local learning approach <ref> (Bottou & Vapnik, 1992) </ref> in which (1) instances are defined as points in a space, (2) a similarity function is defined on all pairs of these instances, and (3) a prediction function defines an answer to be a monotonic function of query similarity. 3.
Reference: <author> Bounds, D., Lloyd, P., & Mathew, B. </author> <year> (1990). </year> <title> A comparison of neural network and other pattern recognition approaches to the diagnosis of low back disorders. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 583-591. </pages>
Reference: <author> Broomhead, D. S., & Lowe, D. </author> <year> (1988). </year> <title> Multivariable functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> 2, </volume> <pages> 321-355. </pages>
Reference: <author> Cain, T., Pazzani, M. J., & Silverstein, G. </author> <year> (1991). </year> <title> Using domain knowledge to influence similarity judgement. </title> <booktitle> In Proceedings of the Case-Based Reasoning Workshop (pp. </booktitle> <pages> 191-202). </pages> <address> Washington, DC: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 25-32). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Carpenter, G.A., Grossberg, S., Markuzon, N., Reynolds, J.H., & Rosen, D.B. </author> <year> (1992). </year> <title> Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3, </volume> <pages> 693-713. </pages>
Reference: <author> Cleveland, W. S.,& Loader, C. </author> <year> (1994). </year> <title> Computational methods for local regression (Technical Report 11). </title> <address> Murray Hill, NJ: </address> <institution> AT&T Bell Laboratories, Statistics Department. </institution> <note> Available by FTP from netlib.att.com in /netlib/att/stat/doc/94/11.ps. </note>
Reference-contexts: They distinguish point-based and query-based weighting methods, based on whether distances are computed for each stored instance or dynamically, for a specific query, as is done in LWR algorithms <ref> (Cleveland & Loader, 1994) </ref>. Finally, several case-based reasoning researchers have advocated using local distance functions with pre-determined weight assignments (e.g., Ashley & Rissland, 1988; Skalak, 1992). We expect that local weighting methods will continue to be a fruitful area for future research. 3.5 Knowledge: None vs.
Reference: <author> Connell, M. E., & Utgoff, P. E. </author> <year> (1987). </year> <title> Learning to control a dynamic physical system. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 456-460). </pages> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cost, S., & Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 57-78. </pages>
Reference: <author> Cover, T. M., & Hart, P. E. </author> <year> (1967). </year> <title> Nearest neighbor pattern classification. </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory, </journal> <volume> 13, </volume> <pages> 21-27. </pages>
Reference-contexts: The second local weighting scheme removes this constraint by allowing feature weights to vary as a function of the instance. We discuss several examples of this approach below. The asymptotic error rate of first nearest neighbor is no more than twice that of the Bayes optimal classifier <ref> (Cover & Hart, 1967) </ref>. Short and Fukunaga (1980; 1981) and Fuku-naga and Flick (1982; 1984) utilized this fact to compute feature weights for a weighted distance function. They estimated the finite sample risk from the local neighborhood of a given instance.
Reference: <author> Cover, T. M., & Thomas, J. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <address> New York: </address> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: The MI of two variables is the reduction in uncertainty of one variable's value given knowledge of the other's value <ref> (Cover & Thomas, 1991) </ref>.
Reference: <author> Cover, T. M., & van Campenhout, J. M. </author> <year> (1977). </year> <title> On the possible orderings in the measurement selection problem. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 7, </volume> <pages> 657-661. </pages>
Reference: <author> Creecy, R. H., Masand, B. M., Smith, S. J., & Waltz, D. L. </author> <year> (1992). </year> <title> Trading MIPS and memory for knowledge engineering. </title> <journal> Communications of the ACM, </journal> <volume> 35, </volume> <pages> 48-64. </pages>
Reference: <author> Daelemans, W., Gills, S., & Durieux, G. </author> <year> (1993). </year> <title> Learnability and markedness in data-driven acquisition of stress (Technical Report 43). </title> <type> Tilburg, </type> <institution> Netherlands: Tilburg University, Institute for Language Technology and Artificial Intelligence. </institution>
Reference: <author> Daelemans, W., van den Bosch, A. </author> <year> (1992). </year> <title> Generalization performance of backpropagation learning on a syllabification task. </title> <booktitle> In Proceedings of TWLT3: Connectionism and Natural Language Processing (pp. </booktitle> <pages> 27-37). </pages> <address> Enschede, The Netherlands: </address> <note> Unpublished. </note>
Reference: <author> Dasarathy, B. V. (Ed.). </author> <year> (1991). </year> <title> Nearest neighbor(NN) norms: NN pattern classification techniques. </title> <publisher> Los Alamitos, </publisher> <address> CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: 1 Introduction The k-nearest neighbor (k-NN) classifier <ref> (Dasarathy, 1991) </ref> is the basis of many lazy learning algorithms. k-NN is purely lazy; it simply stores the entire training set and postpones all effort towards inductive generalization until classification time. k-NN generalizes by fl GMD (German National Research Center for Information Technology), Schlo Birlinghoven, 53754 Sankt Augustin, Germany, dietrich.wettschereck@gmd.de y
Reference: <author> Doak, J. </author> <year> (1992). </year> <title> An evaluation of feature selection methods and their application to computer security (Technical Report CSE-92-18). </title> <institution> Davis, CA: University of California, Department of Computer Science. </institution> <note> 33 Domingos, </note> <author> P. </author> <year> (1996). </year> <title> Context-sensitive feature selection for lazy learners. </title> <journal> Artificial Intelligence Review, </journal> <note> this issue. </note>
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: Second, we focus on classification as the performance task and ignore issues concerning function learning (Atkeson et al., 1996a), class density estimation (e.g., using parzen windows <ref> (Duda & Hart, 1973) </ref>), or problem solving (Kolodner, 1993). Furthermore, we assume classes are disjoint. 2.2 The k-NN Classifier A classifier inputs a query instance q and outputs a prediction for its class.
Reference: <author> Dudani, S. </author> <year> (1975). </year> <title> The distance-weighted k-nearest neighbor rule. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 6, </volume> <pages> 325-327. </pages>
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1993). </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1022-1029). </pages> <address> Chambery, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Friedman, J. H. </author> <year> (1994). </year> <title> Flexible metric nearest neighbor classification. </title> <note> Unpublished manuscript available by anonymous FTP from playfair.stanford.edu (see pub/friedman/README). </note>
Reference: <author> Fu, K. S. </author> <year> (1968). </year> <title> Sequential methods in pattern recognition and machine learning. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Fukunaga, K., & Flick, T. </author> <year> (1982). </year> <title> A parametrically-defined nearest neighbor distance measure. </title> <journal> Pattern Recognition Letters, </journal> <volume> 1, </volume> <pages> 3-5. </pages>
Reference: <author> Fukunaga, K., & Flick, T. </author> <year> (1984). </year> <title> An optimal global nearest neighbor metric. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6, </volume> <pages> 314-318. </pages>
Reference: <author> Gorman, R., & Sejnowski, T. </author> <year> (1988). </year> <title> Analysis of hidden units in a layered network trained to classify sonar targets. </title> <booktitle> Neural Networks, </booktitle> <volume> 1, </volume> <pages> 75-89. </pages>
Reference: <author> Hastie, T. J., & Tibshirani, R. J. </author> <year> (1994). </year> <title> Discriminant Adaptive Nearest Neighbor Classification. </title> <note> Unpublished manuscript available by anonymous FTP from playfair.stanford.edu as /pub/hastie/dann.ps.Z. </note>
Reference-contexts: Larger weight spaces could then be searched for those regions of instance space. Alternatively, this could provide a focus for extracting domain-specific knowledge (i.e., for those local regions) (e.g., Domingos, 1996), or for applying local feature transformation methods so as to reduce the size of the weight space <ref> (e.g., Hastie & Tibshirani, 1994) </ref>. As another example, comparative evaluations could be focussed according to the subcategory structure. Our trends did not address differences within sub-categories of the framework (i.e., a comparison of two types of preset bias methods), although we briefly addressed these issues in Section 4.4.1.
Reference: <author> Hayashi, C. </author> <year> (1952). </year> <title> On the prediction of phenomena from qualitative data and the quantification of qualitative data from the mathematical-statistical point of view. </title> <journal> Annals of the Institute of Statistical Mathematics, </journal> <volume> 3, </volume> <pages> 69-98. </pages>
Reference-contexts: QM2m (Mohri & Tanaka, 1994) is an example of this latter approach. It assigns the absolute values computed by Quantification Method II (QM2) <ref> (Hayashi, 1952) </ref> to set its feature weights in the following variant of Equation 6: 5 d (x; q) = u u X 0 X jw (f 0 ; f )j ffi (x f ; q f ) A (23) where F 0 is the set of transformed (i.e., new) features and
Reference: <author> John, G., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference (pp. </booktitle> <pages> 121-129). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Space constraints prevent a more detailed discussion for the other dimensions. 3.1 Bias: Performance vs. Preset The distinction between performance and preset biases is described as open loop and closed loop in statistics, and as wrapper and filter models in machine learning <ref> (e.g., John et al., 1994) </ref>. We view this as an issue of learning bias. Weighting methods that use feedback from the performance function during training attempt to incorporate the classifier's bias during weighting. Those that do not incorporate some alternative, preset bias.
Reference: <author> Kawaguchi, M. </author> <year> (1978). </year> <title> Introduction to Multivariate Analysis II (in Japanese). </title> <publisher> Morikita-Shuppan. </publisher>
Reference: <author> Kelly, J. D., Jr., & Davis, L. </author> <year> (1991). </year> <title> A hybrid genetic algorithm for classification. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 645-650). </pages> <address> Sydney, Australia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kibler, D., & Aha, D. W. </author> <year> (1987). </year> <title> Learning representative exemplars of concepts: An initial case study. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 24-30). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kira, K., & Rendell, L. A. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 249-256). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Kohavi, R., Langley, P., & Yun, Y. </author> <year> (1995). </year> <title> Heuristic search for feature weights in instance-based learning. </title> <note> Manuscript submitted for publication. </note>
Reference: <author> Kohonen, T., Barna, G., & Chrisley, R. </author> <year> (1988). </year> <title> Statistical pattern recognition with neural networks. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (pp. </booktitle> <pages> 61-88). </pages> <publisher> IEEE Press. </publisher>
Reference: <author> Kohonen, T. </author> <year> (1990). </year> <title> The self-organizing map. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78, </volume> <pages> 1464-1480. </pages>
Reference: <author> Kolodner, J. </author> <year> (1993). </year> <title> Case-based reasoning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <address> 34 Kononenko, I. </address> <year> (1994). </year> <title> Estimating attributes: Analysis and extensions of RELIEF. </title> <booktitle> In Proceedings of the 1994 European Conference on Machine Learning (pp. </booktitle> <pages> 171-182). </pages> <address> Catania, Italy: </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: We also constrain our study in two respects. First, we assume that instances are represented by a set of feature-value pairs rather than, for example, directed acyclic graphs, which are sometimes used in the case-based reasoning literature <ref> (Kolodner, 1993) </ref>. Second, we focus on classification as the performance task and ignore issues concerning function learning (Atkeson et al., 1996a), class density estimation (e.g., using parzen windows (Duda & Hart, 1973)), or problem solving (Kolodner, 1993). <p> than, for example, directed acyclic graphs, which are sometimes used in the case-based reasoning literature <ref> (Kolodner, 1993) </ref>. Second, we focus on classification as the performance task and ignore issues concerning function learning (Atkeson et al., 1996a), class density estimation (e.g., using parzen windows (Duda & Hart, 1973)), or problem solving (Kolodner, 1993). Furthermore, we assume classes are disjoint. 2.2 The k-NN Classifier A classifier inputs a query instance q and outputs a prediction for its class.
Reference: <author> Kruschke, J. K. </author> <year> (1992). </year> <title> ALCOVE: An exemplar-based connectionist model of category learning. </title> <journal> Psychological Review, </journal> <volume> 99, </volume> <pages> 22-44. </pages>
Reference: <author> Kshirsager, A. </author> <year> (1972). </year> <title> Multivariate Analysis. </title> <address> New York: </address> <publisher> Dekker. </publisher>
Reference-contexts: jV f j binary features before this transformation. 6 QM2 is preferable to PCA, which can lose information since (1) PCA orders principal components by decreasing functions of their input data variations and (2) the variable with the lowest variation might actually be the one with the highest predictive relevance <ref> (Kshirsager, 1972) </ref>. QM2 also is sensitive to concept skew when ordering the new variables. 14 This algorithm performed best among their selected set of lazy algorithms, but hypotheses explaining why have not yet been investigated. Another lazy learning algorithm that supports feature transformation is IB3-CI (Aha, 1991).
Reference: <author> Langley, P., & Iba, W. </author> <year> (1993). </year> <title> Average-case analysis of a nearest neighbor algorithm. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 889-894). </pages> <address> Chambery, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Langley, P., & Sage, S. </author> <year> (1994). </year> <title> Oblivious decision trees and abstract cases. </title> <editor> In D. W. Aha (Ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: These methods select features using * an induced decision tree (Cardie, 1993; Kibler & Aha, 1987), * random mutation hill-climbing (Skalak, 1994), * parallel search (Moore & Lee, 1994), * beam search with stepwise selection (Aha & Bankert, 1994), and * stepwise feature removal in oblivious decision trees <ref> (Langley & Sage, 1994) </ref>. The first of these methods employs a preset weighting bias while the others exploit performance feedback. All report accuracy and/or speed improvements over 1-NN or k-NN. Feature selection algorithms can often reduce the dimensionality of a learning task.
Reference: <author> Ling, X. C., & Wang, H. </author> <year> (1996). </year> <title> Towards optimal weights setting for the 1-nearest neighbour learning algorithm. </title> <journal> Artificial Intelligence Review, </journal> <note> this issue. </note>
Reference: <author> Lowe, D. </author> <year> (1995). </year> <title> Similarity metric learning for a variable-kernel classifier. </title> <journal> Neural Computation, </journal> <volume> 7, </volume> <pages> 72-85. </pages>
Reference-contexts: Batch Optimizers These weighting methods optimize feature weights by repeatedly processing instances. Some of these methods require only knowledge of the function value to be approximated at each problem state (e.g., simulated annealing and genetic algorithms) while others use knowledge of the function's gradient <ref> (e.g., Lowe, 1995) </ref>. Kelly and Davis (1991) and Skalak (1994) used genetic algorithms (GAs) to learn continuous feature weights for lazy learning algorithms. GAs loosely mimic processes of biological evolution.
Reference: <author> Lucassen, J., & Mercer, R. </author> <year> (1984). </year> <title> An information theoretic approach to the automatic determination of phonemic base forms. </title> <booktitle> In Proceedings of the International Conference on Acoustics Speech Signal Processing (42.5.1-42.5.4). </booktitle>
Reference: <author> Luce, R. D. </author> <year> (1963). </year> <title> Detection and recognition. </title> <editor> In R. D. Luce, R .R. Bush, & E. Galanger (Eds.), </editor> <booktitle> Handbook of mathematical psychology. </booktitle> <address> New York, NY: </address> <publisher> Wiley. </publisher>
Reference-contexts: The loss function can be arbitrarily complex (e.g., it could vary per class or instance). Second, k-NN's equation for computing posteriors (Equation 4) is a form of kernel regression in statistics (Nadaraya, 1964), or the probability choice model in cognitive psychology <ref> (Luce, 1963) </ref>. It could take on many other forms, such as by replacing 1 () with a function that relates classes or varies on each instance. Also, many different kernel functions (K ()) have been investigated (Atkeson et al., 1996a).
Reference: <author> McGill, W. </author> <year> (1955). </year> <title> Multivariate information transmission. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 1, </volume> <pages> 93-111. </pages>
Reference: <author> Medin, D. L., & Schaffer, M. M. </author> <year> (1978). </year> <title> Context theory of classification learning. </title> <journal> Psychological Review, </journal> <volume> 85, </volume> <pages> 207-238. </pages>
Reference-contexts: Domain-specific heuristics help determine the degree to which two features match. Feature weights can be subsequently modified by the domain expert whenever PROTOS fails to retrieve the correct case to a query. PROTOS' distance function uses a variant of the context model <ref> (Medin & Schaffer, 1978) </ref>: it subtracts, from 1.0, the contributions of non-matching features according to their relevance weights. Bareiss (1989) reported that PROTOS recorded higher accuracies than did knowledge-poor k-NN on the audiology task.
Reference: <editor> Michie, D., Spiegelhalter, D. J., & Taylor, C. C. (Eds.) </editor> <year> (1994). </year> <title> Machine learning, neural and statistical classification. </title> <publisher> London: Prentice Hall. </publisher>
Reference-contexts: Although large scale empirical comparisons exist for other classes of algorithms <ref> (e.g., Michie et al., 1994) </ref>, they do not exist for this class. Instead, most previous comparisons among feature weighting algorithms tend to focus on a specific pair of algorithms (e.g., Wettschereck & Dietterich, 1995; Kohavi et al., 1995).
Reference: <author> Mitchell, T. M. </author> <year> (1990). </year> <title> The need for biases in learning generalizations. </title> <editor> In J. W. Shavlik & T. G. Dietterich (Eds.), </editor> <booktitle> Readings in machine learning. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our objective is to bring attention to these weight learning variants, and their relative merits. No weight learning method can learn optimal weight settings for all learning tasks since each task requires a different learning bias for optimal performance <ref> (Mitchell, 1990) </ref>. Therefore, we empirically evaluate a subclass of weight learning methods, present general trends that contrast their capabilities, and investigate these trends in Section 4.
Reference: <author> Mitchell, T., Keller, R., & Kedar-Cabelli, S. </author> <year> (1986). </year> <title> Explanation-based learning: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 47-80. </pages>
Reference-contexts: Cain et al. (1991) use a domain theory of rules to assign instance-specific weights. Like other algorithms that define local distance functions, their CBR+EBL algorithm combines instance-specific with global feature weights. They assign instance-specific weights to discrete features using an explanation-based learning (EBL) approach <ref> (Mitchell et al., 1986) </ref>. Any feature appearing in an EBL tree that was generated to explain the instance's class is assigned a weight of 1. All other features for that instance are assigned an instance-specific weight of 0.
Reference: <author> Mohri, T., Nakamura, M., & Tanaka, H. </author> <year> (1993). </year> <title> Weather forecasting using memory-based reasoning. </title> <booktitle> In Second International Workshop on Parallel Processing for Artificial Intelligence (pp. </booktitle> <pages> 40-45). </pages>
Reference: <author> Mohri, T., & Tanaka, H. </author> <year> (1994). </year> <title> An optimal weighting criterion of case indexing for both numeric and symbolic attributes. </title> <editor> In D. W. Aha (Ed.), </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: They concluded that their weighted methods outperform non-weighted methods, but did not compare these two weighting methods. 10 Mohri et al. (1993) found that PCF is sensitive to concept distributions; it tends to classify too many instances according to the majority class. PCF performed poorly in their experiments <ref> (Mohri & Tanaka, 1994) </ref>. For example, the simpler CCF attained higher accuracies than PCF on six of eight tasks. Class projection Stanfill and Waltz (1986) introduced the value-difference metric (VDM), a more sophisticated similarity function defined for discrete features. <p> A substantial shortcoming of all methods that simply assign weights to individual features is their insensitivity to interacting or correlated features. This can be addressed either by using distance functions that combine weights (e.g., using upper triangular weight matrices) or by transforming the given representation before weighting features. QM2m <ref> (Mohri & Tanaka, 1994) </ref> is an example of this latter approach.
Reference: <author> Mohri, T., & Tanaka, H. </author> <year> (1995). </year> <title> Comparison between attribute weighting methods in memory-based reasoning and multivariate analysis. </title> <note> Manuscript submitted for publication. </note>
Reference: <author> Moore, A. W., & Lee, M. S. </author> <year> (1994). </year> <title> Efficient algorithms for minimizing cross validation error. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 190-198). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Several researchers have recently used various feature selection methods for lazy learning algorithms. These methods select features using * an induced decision tree (Cardie, 1993; Kibler & Aha, 1987), * random mutation hill-climbing (Skalak, 1994), * parallel search <ref> (Moore & Lee, 1994) </ref>, * beam search with stepwise selection (Aha & Bankert, 1994), and * stepwise feature removal in oblivious decision trees (Langley & Sage, 1994). The first of these methods employs a preset weighting bias while the others exploit performance feedback.
Reference: <author> Mucciardi, A. N., & Gose, E. E. </author> <year> (1971). </year> <title> A comparison of seven techniques for choosing subsets of pattern recognition properties. </title> <journal> IEEE Transaction on Computers, </journal> <volume> 20, </volume> <pages> 1023-1031. </pages>
Reference: <author> Murphy, P. </author> <year> (1995). </year> <title> UCI Repository of machine learning databases [Machine-readable data repository @ics.uci.edu]. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution> <note> 35 Myles, </note> <author> J., & Hand, D. </author> <year> (1990). </year> <title> The multi-class metric problem in nearest neighbor discrimination rules. </title> <journal> Pattern Recognition, </journal> <volume> 23, </volume> <pages> 1291-1297. </pages>
Reference-contexts: They obtained favorable results with their local weighting scheme as compared to standard 1-NN and Salzberg's (1991) EACH on four datasets from the UCI repository <ref> (Murphy, 1995) </ref>. Other purely local (and lazy) approaches were recently introduced by Hastie and Tib-shirani (1994) and by Friedman (1994). Hastie and Tibshirani (1994) compute a separate distance metric for each query through an iterative process. <p> We selected a parity task with 11 boolean features; seven are irrelevant while the sum of the other four determines the output (i.e., if the sum is an even number, then the class is 1 and otherwise is 0). The remaining datasets were drawn from the UC Irvine Repository <ref> (Murphy, 1995) </ref>. Some characteristics of these datasets are shown in Table 3. Additional dataset characteristics that are relevant to the evaluation are: * The LED Display and Waveform datasets are also constructed from a data generator.
Reference: <author> Nadaraya, E. A. </author> <year> (1964), </year> <title> On estimating regression. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 9, </volume> <pages> 141-142. </pages>
Reference-contexts: The loss function can be arbitrarily complex (e.g., it could vary per class or instance). Second, k-NN's equation for computing posteriors (Equation 4) is a form of kernel regression in statistics <ref> (Nadaraya, 1964) </ref>, or the probability choice model in cognitive psychology (Luce, 1963). It could take on many other forms, such as by replacing 1 () with a function that relates classes or varies on each instance. Also, many different kernel functions (K ()) have been investigated (Atkeson et al., 1996a).
Reference: <author> Poggio, T., & Girosi, F. </author> <year> (1990). </year> <title> Regularization algorithms for learning that are equivalent to multilayer networks. </title> <journal> Science, </journal> <volume> 247, </volume> <pages> 978-982. </pages>
Reference-contexts: Wettschereck and Diet-terich (1992) showed how LVQ-type algorithms can be used to adjust the coordinates of irrelevant input features for all stored exemplars such that they are identical, which effectively eliminates them. They showed this for generalized radial basis networks <ref> (Poggio & Girosi, 1990) </ref>, where the centers of basis functions are moved during training.
Reference: <author> Porter, B. W., Bareiss, R., & Holte, R. C. </author> <year> (1990). </year> <title> Knowledge acquisition and heuristic classification in weak-theory domains. </title> <journal> Artificial Intelligence, </journal> <volume> 45, </volume> <pages> 229-263. </pages>
Reference-contexts: Thus, they demonstrated the utility of using knowledge to set instance-specific weights for a lazy learning algorithm. PROTOS <ref> (Porter et al., 1990) </ref> is a sophisticated case-based reasoning system designed initially for a clinical audiology classification task. It builds a semantic network whose links relate features, instances, and classes. It uses feedback from the user to refine its knowledge.
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. </author> <year> (1992). </year> <title> Numerical Recipes in C. </title> <address> Cambridge, UK: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: Lowe (1995) employed this approach in the variable kernel similarity metric (VSM), which computes distances using a function similar to Equation 6. Feature weights are optimized using conjugate gradient <ref> (Press et al., 1992) </ref> to minimize summed leave-one-out classification error (LOOCE) on the training set. 3 The derivative of this error with respect to each feature weight is used to guide the conjugate gradient procedure. <p> For k-NN V SM , the number of training epochs was limited to the number of epochs required for minimization along one conjugate direction <ref> (see Press et al., 1992, Wettschereck, 1995a) </ref>. CCF, VDM, and MVDM have no free parameters. Fayyad and Irani's (1993) discretiza-tion algorithm was used to discretize continuous features for CCF, VDM, MVDM, MI (i.e., during feature weight computation), and Relief-F (i.e., when computing distance in the presence of missing feature values).
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: Daelemans and van den Bosch (1992) introduced an extension of this approach that assigns a feature's (normalized) information gain <ref> (Quinlan, 1986) </ref> as its weight rather than Equation 21: w (f ) = c j 2J X X p (c j jx f = v) log p (c j jx f = v)p (x f = v) (22) This equation subtracts the average information entropy of a feature from the information
Reference: <author> Ricci, F., & Avesani, P. </author> <year> (1995). </year> <title> Learning a local similarity metric for case-based reasoning. </title> <booktitle> In Proceedings of the First International Conference on Case-Based Reasoning (pp. </booktitle> <pages> 301-312). </pages> <address> Sesimbra, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Salzberg, S. L. </author> <year> (1991). </year> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 251-276. </pages>
Reference-contexts: Wettschereck and Dietterich (1995) used a simple approach to do this; it divides continuous features into a pre-determined number I of intervals, treating all values within a given interval as equal. They found that this batch weighting method improved the performance of EACH <ref> (Salzberg, 1991) </ref> compared to its online algorithm for setting weights (Equation 11). We use a similar approach in Section 4 that avoids the need to predetermine I. <p> Some studies have also compared weighting algorithms in a specific context. For example, Wettschereck and Di-etterich (1995) showed that a mutual information method traded off higher computational complexity for higher accuracies when compared with an online algorithm in the context of learning hyperrectangles <ref> (Salzberg, 1991) </ref>. Mohri and Tanaka (1994) reported a more extensive comparison, in which they review several feature weighting algorithms while motivating the introduction of QM2. This algorithm assigns weight values by optimizing specific statistical criteria.
Reference: <author> Satoh, K., & Okamoto, S. </author> <year> (1994). </year> <title> Toward PAC-learning of weights from qualitative distance information. </title>
Reference: <editor> In D. W. Aha (Ed.) </editor> <booktitle> Case-Based Reasoning: Papers from the 1994 Workshop (Technical Report WS-94-01). </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Schaffer, C. </author> <year> (1994). </year> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning (pp. </booktitle> <pages> 259-265). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schlimmer, J. C. </author> <year> (1987). </year> <title> Incremental adjustment of representations for learning. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 79-90). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Another lazy learning algorithm that supports feature transformation is IB3-CI (Aha, 1991). This is a knowledge-intensive extension of the noise-tolerant IB3 algorithm (Aha et al., 1991). It uses a Bayesian approach, adapted from <ref> (Schlimmer, 1987) </ref>, to direct its search through a space of logical feature combinations, and uses a competitive feature selection approach to assign binary weights. Aha (1991) reported good results for IB3-CI in comparison with lazy algorithms that do not perform representation change.
Reference: <author> Shannon, C. E. </author> <year> (1948). </year> <title> A mathematical theory of communication. </title> <journal> Bell Systems Technology Journal, </journal> <volume> 27, </volume> <pages> 379-423. </pages>
Reference: <author> Short, R., & Fukunaga, K. </author> <year> (1980). </year> <title> A new nearest neighbor distance measure. </title> <booktitle> In Proceedings of the Fifth International Conference on Pattern Recognition (pp. </booktitle> <pages> 81-86). </pages> <address> Los Alamitos, CA: </address> <publisher> IEEE Press. </publisher>
Reference: <author> Short, R., & Fukunaga, K. </author> <year> (1981). </year> <title> The optimal distance measure for nearest neighbor classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 27, </volume> <pages> 622-627. </pages>
Reference: <author> Simard, P., Le Cun, Y., & Denker, J. </author> <year> (1993). </year> <title> Efficient pattern recognition using a new transformation distance. </title> <editor> In Hanson, S. J., et al. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Approaches more closely related to the focus of this paper are algorithms that combine automated weight-learning components with domain intensive knowledge or heuristics (e.g., the set of possible transformations assumed when defining tangent distance functions <ref> (Simard et al., 1993) </ref>). 17 We described some knowledge-intensive algorithms earlier (e.g., IB3-CI), and briefly detail two more below. Cain et al. (1991) use a domain theory of rules to assign instance-specific weights. Like other algorithms that define local distance functions, their CBR+EBL algorithm combines instance-specific with global feature weights.
Reference: <author> Skalak, D. </author> <year> (1992). </year> <title> Representing cases as knowledge sources that apply local similarity metrics. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 325-330). </pages> <address> Blooming-ton, </address> <publisher> IN: Lawrence Erlbaum. </publisher>
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference (pp. </booktitle> <pages> 293-301). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Several researchers have recently used various feature selection methods for lazy learning algorithms. These methods select features using * an induced decision tree (Cardie, 1993; Kibler & Aha, 1987), * random mutation hill-climbing <ref> (Skalak, 1994) </ref>, * parallel search (Moore & Lee, 1994), * beam search with stepwise selection (Aha & Bankert, 1994), and * stepwise feature removal in oblivious decision trees (Langley & Sage, 1994). The first of these methods employs a preset weighting bias while the others exploit performance feedback.
Reference: <author> Stanfill, C., & Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 29, </volume> <pages> 1213-1228. </pages>
Reference-contexts: Similarly, the definition of feature difference (Equation 7) is one of many; functions other than absolute difference for continuous values and more elaborate functions for defining similarity on discrete features have been proposed <ref> (Stanfill & Waltz, 1986) </ref>. Finally, many other classes of weight-learning methods have been examined, frequently in the context of statistical regression, where distance functions on continuous functions have been carefully examined.
Reference: <author> Tan, M. </author> <year> (1993). </year> <title> Cost-sensitive learning of classification knowledge and its application in robotics. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 7-34. </pages>
Reference-contexts: For example, k-NN's constant loss function is not class sensitive; its value for L c j c j 0 is invariant for different pairs of classes. Cost-sensitive learning (Turney, 1995) variants of k-NN exist <ref> (e.g., Tan, 1993) </ref> that minimize a locally weighted error criterion (Vapnik, 1992). The loss function can be arbitrarily complex (e.g., it could vary per class or instance).
Reference: <author> Ting, K. M. </author> <year> (1994). </year> <title> Discretization of continuous-valued attributes and instance-based learning (Technical Report). </title> <address> Sydney, Australia, </address> <institution> University of Sydney, Basser Department of Computer Science. </institution> <note> 36 Turney, </note> <author> P. D. </author> <year> (1993). </year> <title> Exploiting context when learning to classify. </title> <booktitle> In Proceedings of the European Confer--ence on Machine Learning (pp. </booktitle> <pages> 402-407). </pages> <address> Vienna, Austria: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Turney, P. D. </author> <year> (1995). </year> <title> Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction algorithm. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 369-409. </pages>
Reference-contexts: For example, k-NN's constant loss function is not class sensitive; its value for L c j c j 0 is invariant for different pairs of classes. Cost-sensitive learning <ref> (Turney, 1995) </ref> variants of k-NN exist (e.g., Tan, 1993) that minimize a locally weighted error criterion (Vapnik, 1992). The loss function can be arbitrarily complex (e.g., it could vary per class or instance).
Reference: <author> Tversky, A. </author> <year> (1977). </year> <title> Features of similarity. </title> <journal> Psychological Review, </journal> <volume> 84, </volume> <pages> 327-352. </pages>
Reference: <author> Utgoff, P. E. </author> <year> (1989). </year> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 161-186. </pages> <editor> van den Bosch, A., & Daelemans, W. </editor> <year> (1993). </year> <type> Data-oriented methods for grapheme-to-phoneme conversion (Technical Report 42). </type> <institution> Tilburg, Netherlands: Tilburg University, Institute for Language Technology and Artificial Intelligence. </institution>
Reference-contexts: Several of these systems use some form of feature weighting that could be more closely compared with the algorithms reviewed here. Several other algorithms blur the distinction between lazy and eager processing. For example, some incremental decision tree induction algorithms retain specific instances <ref> (e.g., Utgoff, 1989) </ref>, and some algorithms combine rules with specific instances to represent concepts (e.g., Zhang, 1990).
Reference: <author> Vapnik, V. </author> <year> (1992). </year> <title> Principles of risk minimization for learning theory. </title> <editor> In R. P. Lippmann & J. E. Moody (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <address> Denver, </address> <publisher> CO: Morgan Kaufmann. </publisher>
Reference-contexts: Defer: They store all training data and defer processing until queries are given that require replies. 2 2. Reply: Queries are answered by combining the training data, typically by using a local learning approach <ref> (Bottou & Vapnik, 1992) </ref> in which (1) instances are defined as points in a space, (2) a similarity function is defined on all pairs of these instances, and (3) a prediction function defines an answer to be a monotonic function of query similarity. 3. <p> For example, k-NN's constant loss function is not class sensitive; its value for L c j c j 0 is invariant for different pairs of classes. Cost-sensitive learning (Turney, 1995) variants of k-NN exist (e.g., Tan, 1993) that minimize a locally weighted error criterion <ref> (Vapnik, 1992) </ref>. The loss function can be arbitrarily complex (e.g., it could vary per class or instance). Second, k-NN's equation for computing posteriors (Equation 4) is a form of kernel regression in statistics (Nadaraya, 1964), or the probability choice model in cognitive psychology (Luce, 1963).
Reference: <author> Volper, D. J., & Hampson, S. E. </author> <year> (1987). </year> <title> Learning and using specific instances. </title> <journal> Biological Cybernetics, </journal> <volume> 57, </volume> <pages> 57-71. </pages>
Reference: <author> Weiss, S. M., & Kapouleas, I. </author> <year> (1989). </year> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 781-787). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Weiss, S. M., & Kulikowski, C. A. </author> <year> (1991). </year> <title> Computer systems that learn: Classification and prediction methods from statistics, neural nets, </title> <booktitle> machine learning, and expert systems. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Equation 9's strong constraint provides the motivation for our survey and empirical study. Some implementation details need mentioning. If there is a tie among the maximal p (c j jq), then one of the most probable classes is randomly selected. k is set using leave-one-out cross-validation on X <ref> (Weiss & Kulikowski, 1991) </ref>, where ties are broken in favor of smaller values for k. We used a standard function (i.e., subtract the minimum and divide by the observed range) to normalize all continuous values. This ensures that the range of ffi () is [0; 1] for all features.
Reference: <author> Wettschereck, D. </author> <year> (1994). </year> <title> A study of distance-based machine learning algorithms. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Science, Oregon State University, Corvallis, OR. </institution> <note> Available via WWW at http://nathan.gmd.de/persons/dietrich.wettschereck.html Wettschereck, </note> <author> D. </author> <year> (1995a). </year> <title> A description of the mutual information approach and the variable similiarty metric (Technical Report 944). </title> <institution> Sankt Augustin, Germany, German National Research Center for Computer Science, Artificial Intelligence Research Division. </institution>
Reference-contexts: Lines represent the decision boundaries used to label the data. * The Cleveland, Hungarian, and Voting databases contain redundant features (i.e., some features can be removed in these datasets without any significant effect on the perfor mance of k-NN <ref> (Wettschereck, 1994) </ref>). In contrast, some datasets have no redundant features (e.g., Waveform, Isolet, and NETtalk). Table 3: Characteristics of the selected datasets. B = Boolean, C = Continuous, D = Dis crete. The relevant features in the datasets located above the horizontal divider are approx imately equally relevant.
Reference: <author> Wettschereck, D. </author> <year> (1995b). </year> <title> Weighted kNN versus majority kNN: </title> <type> A recommendation (Technical Report 943). </type> <institution> Sankt Augustin, Germany, German National Research Center for Computer Science, Artificial Intelligence Research Division. </institution>
Reference: <author> Wettschereck, D., & Dietterich, T. G. </author> <year> (1992). </year> <title> Improving the performance of radial basis function networks by learning center locations. </title> <editor> In J. Moody, S. Hanson, & R. Lippmann (Eds.), </editor> <booktitle> Neural Information Processing Systems 4. </booktitle> <address> Denver, </address> <publisher> CO: Morgan Kaufmann. </publisher>
Reference-contexts: For k-NN V SM , the number of training epochs was limited to the number of epochs required for minimization along one conjugate direction <ref> (see Press et al., 1992, Wettschereck, 1995a) </ref>. CCF, VDM, and MVDM have no free parameters. Fayyad and Irani's (1993) discretiza-tion algorithm was used to discretize continuous features for CCF, VDM, MVDM, MI (i.e., during feature weight computation), and Relief-F (i.e., when computing distance in the presence of missing feature values).
Reference: <author> Wettschereck, D., & Dietterich, T. G. </author> <year> (1995). </year> <title> An experimental comparison of the nearest neighbor and nearest hyperrectangle algorithms. </title> <journal> Machine Learning, </journal> <volume> 19, </volume> <pages> 5-28. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1990). </year> <title> Constructing a generalizer superior to NETtalk via a mathematical theory of generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 445-452. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1994). </year> <type> Personal communication. </type>
Reference: <author> Yau, H. C., & Manry, M. T. </author> <year> (1991). </year> <title> Iterative improvement of a nearest neighbor classifier. </title> <booktitle> Neural Networks, </booktitle> <volume> 4, </volume> <pages> 517-524. </pages>
Reference: <author> Zhang, J. </author> <year> (1990). </year> <title> A method that combines inductive learning with exemplar-based learning. </title> <booktitle> In Proceedings for Tools for Artificial Intelligence (pp. </booktitle> <pages> 31-37). </pages> <address> Herndon, VA: </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 37 </pages>
Reference-contexts: Several other algorithms blur the distinction between lazy and eager processing. For example, some incremental decision tree induction algorithms retain specific instances (e.g., Utgoff, 1989), and some algorithms combine rules with specific instances to represent concepts <ref> (e.g., Zhang, 1990) </ref>. Each architecture highlights a unique perspective on weighting features, which may provide insights not easily obtained when using a traditional k-NN architecture. 7 Conclusions In this paper we investigated issues on estimating feature weight parameters for the distance functions in a subclass of lazy learning algorithms.
References-found: 102

