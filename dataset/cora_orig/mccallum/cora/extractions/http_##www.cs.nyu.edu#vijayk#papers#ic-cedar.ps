URL: http://www.cs.nyu.edu/vijayk/papers/ic-cedar.ps
Refering-URL: http://www.cs.nyu.edu/vijayk/papers.html
Root-URL: http://www.cs.nyu.edu
Email: fzhang,vijayk,achieng@cs.uiuc.edu, ng@lcs.mit.edu  
Title: Optimizing COOP Languages: Study of a Protein Dynamics Program  
Author: Xingbin Zhang, Vijay Karamcheti, Tony Ng, and Andrew A. Chien 
Note: Appears in the Proceedings of IPPS'96.  
Affiliation: Department of Computer Science, University of Illinois at Urbana-Champaign  
Abstract: Fine-grained concurrent object-oriented programming (COOP) models can simplify the programming of irregular parallel applications but are often perceived as inefficient. In this paper, we study implementation techniques to obtain efficient parallel execution of fine-grained COOP languages using a medium-sized protein dynamics program. We found that even with high data locality and good sequential code efficiency, an implementation using only thread-oriented optimizations and software multithreading fails to achieve parallel efficiency. The two major sources of overhead are the lack of processor-level data reuse and fine-grained threads for remote object accesses. Two processor-oriented optimizations, processor-level caching and communication grouping, overcome these inefficiencies and achieve performance comparable to a highly-tuned SPMD program. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: We believe these optimizations can be automated in a compiler and permit similar irregular applications to be expressed in a natural COOP style and achieve good performance. 2. COOP Model and Implementation The Concert system supports a fine-grained COOP model based on Actors <ref> [1] </ref>. Computation is expressed as method invocations on objects or collections of objects. Method invocations conceptually operate within dynamically created threads that are inherently concurrent.
Reference: [2] <author> A. A. Chien and J. Dolby. </author> <title> The Illinois Concert system: A problem-solving environment for irregular applications. </title> <booktitle> In the Symposium on Parallel Computation and Problem Solving Environments., </booktitle> <year> 1994. </year>
Reference-contexts: Object-level concurrency control maintains sequential consistency of the object state in the global namespace, enabling the safe composition of concurrent operations and freeing the programmer from managing explicit locking. The Concert system <ref> [2] </ref> is a state-of-the-art implementation of a COOP programming model. On distributed memory machines, the execution model synthesizes a global namespace and hides latency of remote invocations via software multithreading on commercial single-threaded microprocessors.
Reference: [3] <author> D. Culler, et. al. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In ASPLOS-IV, </booktitle> <pages> pages 164-75, </pages> <year> 1991. </year>
Reference-contexts: We are currently developing a compiler framework using interprocedural analysis coupled with user annotations to automate the optimizations described here. Our work is related to several dynamic programming language systems <ref> [16, 14, 3] </ref> whose objective is to achieve high performance on distributed memory systems. Implementations of these systems have traditionally focused on communication overhead reductions and thread-oriented optimizations, relying on user-specified data distribution to reduce communication volume.
Reference: [4] <author> A. Y. Grama, et. al. </author> <title> Scalable parallel formulations of the Barnes-Hut method for n-body simulations. </title> <booktitle> In Supercomputing'94, </booktitle> <pages> pages 439-448, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Data placement that exploits the spatial locality in both the neighbor list and force computation phases is mandatory for parallel efficiency. We apply space-filling curves, used in astrophysics applications <ref> [4] </ref>, to distribute objects spatially as well as for dynamic load-balancing. In addition, a suite of locality-sensitive optimizations in the Concert system is applied to the base IC-CEDAR code. Speculative inlining and hybrid execution enable code optimizations for local execution.
Reference: [5] <author> CSAG. </author> <title> The ICC++ reference manual. Concurrent Systems Architecture Group Memo, </title> <month> June </month> <year> 1995. </year>
Reference-contexts: The force calculation requires reading the position and updating the force of interacting atoms, also inducing irregular data accesses. 2 All sample codes are shown using ICC++ <ref> [5] </ref>, a parallel C++ dialect supported by the Concert system. 2 cells.doall (neighbor_list, cutoff); // fanout to all cells ... void Cells::neighbor_list (float cutoff) - conc for (i = 0; i &lt; num_neighbors; i++) - neighbor = neighbor_cells [i]; conc for (j = 0; j &lt; num_groups; j++) conc for (k
Reference: [6] <author> J. Hermans and M. Carson. </author> <title> Cedar documentation. Unpublished manual for CEDAR, </title> <year> 1985. </year>
Reference-contexts: The Concert runtime implements a flexible hybrid execution model [12], which creates threads lazily and exposes a hierarchy of high performance COOP primitives [8] to allow compile-time specialization. 3. IC-CEDAR IC-CEDAR is a parallel version of CEDAR <ref> [6] </ref>, a sequential protein molecular dynamics program. Computation proceeds in discrete time steps with four computation phases, shown in Figure 1. During each time step, forces exerted on each atom are calculated and used to update the atom's position and velocity.
Reference: [7] <author> Y.-S. Hwang, et. al. </author> <title> Parallelizing molecular dynamics programs for distributed memory machines. </title> <journal> IEEE Comp. Sci. and Engr., </journal> <pages> pages 18-29, </pages> <month> Summer </month> <year> 1995. </year>
Reference-contexts: CM-5 execution times in seconds. 5.2. Comparison to Other Implementations Finally, to place our performance results in context, Table 4 compares the overall execution times of IC-CEDAR and CHARMM, a similar protein MD package <ref> [7] </ref>, on 64 nodes of the CRAY T3D. <p> We believe for these applications, compile-time global program transformations can achieve efficient node-level reuse and communication grouping for COOP languages. The inspector/executor execution model has been extensively studied for SPMD programs <ref> [15, 7] </ref> and has been shown to give good performance for irregular applications whose data access pattern is independent of the ongoing computation. Ongoing efforts to 5 compile such applications in parallel Fortran using this model share many issues with the required transformations here.
Reference: [8] <author> V. Karamcheti and A. Chien. </author> <title> Concert efficient run-time support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <year> 1993. </year>
Reference-contexts: The Concert runtime implements a flexible hybrid execution model [12], which creates threads lazily and exposes a hierarchy of high performance COOP primitives <ref> [8] </ref> to allow compile-time specialization. 3. IC-CEDAR IC-CEDAR is a parallel version of CEDAR [6], a sequential protein molecular dynamics program. Computation proceeds in discrete time steps with four computation phases, shown in Figure 1.
Reference: [9] <author> V. Karamcheti and A. A. Chien. </author> <title> A comparison of architectural support for messaging on the TMC CM-5 and the Cray T3D. </title> <booktitle> In ISCA'95, </booktitle> <year> 1995. </year>
Reference-contexts: A remote object access involves sending and receiving two messages and a handler execution, costing nearly 12 microseconds of total processor overhead on the T3D even with a low overhead communication layer <ref> [9] </ref>. In addition, a remote invocation causes processor synchronization overhead when the current computation blocks, requiring a thread context switch. Without effective node-level reuse, the overhead of communication and synchronization limits the parallel efficiency of the force kernel to 40% on 16 nodes.
Reference: [10] <author> F. H. McMahon. </author> <title> The Livermore Fortran kernels: a computer test of the numerical performance range. </title> <institution> UCRL-53745, LLNL, Livermore, California, </institution> <year> 1986. </year>
Reference-contexts: The Concert compiler [11, 13] implements a suite of interprocedural optimizations and can achieve sequential performance equal to that of C for a large number of programs including the Livermore kernels <ref> [10] </ref>, a demanding numerical benchmark. The Concert runtime implements a flexible hybrid execution model [12], which creates threads lazily and exposes a hierarchy of high performance COOP primitives [8] to allow compile-time specialization. 3. IC-CEDAR IC-CEDAR is a parallel version of CEDAR [6], a sequential protein molecular dynamics program.
Reference: [11] <author> J. Plevyak. </author> <title> Optimization of Object-Oriented and Concurrent Programs. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, Illinois. </institution> <note> In Preparation. </note>
Reference-contexts: The Concert system [2] is a state-of-the-art implementation of a COOP programming model. On distributed memory machines, the execution model synthesizes a global namespace and hides latency of remote invocations via software multithreading on commercial single-threaded microprocessors. The Concert compiler <ref> [11, 13] </ref> implements a suite of interprocedural optimizations and can achieve sequential performance equal to that of C for a large number of programs including the Livermore kernels [10], a demanding numerical benchmark.
Reference: [12] <author> J. Plevyak, et. al. </author> <title> A hybrid execution model for fine-grained languages on distributed memory multicom-puters. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <year> 1995. </year>
Reference-contexts: The Concert compiler [11, 13] implements a suite of interprocedural optimizations and can achieve sequential performance equal to that of C for a large number of programs including the Livermore kernels [10], a demanding numerical benchmark. The Concert runtime implements a flexible hybrid execution model <ref> [12] </ref>, which creates threads lazily and exposes a hierarchy of high performance COOP primitives [8] to allow compile-time specialization. 3. IC-CEDAR IC-CEDAR is a parallel version of CEDAR [6], a sequential protein molecular dynamics program. Computation proceeds in discrete time steps with four computation phases, shown in Figure 1.
Reference: [13] <author> J. Plevyak, et. al. </author> <title> Obtaining sequential efficiency in concurrent object-oriented programs. </title> <booktitle> In Proceedings of PoPL, </booktitle> <pages> pages 311-321, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: The Concert system [2] is a state-of-the-art implementation of a COOP programming model. On distributed memory machines, the execution model synthesizes a global namespace and hides latency of remote invocations via software multithreading on commercial single-threaded microprocessors. The Concert compiler <ref> [11, 13] </ref> implements a suite of interprocedural optimizations and can achieve sequential performance equal to that of C for a large number of programs including the Livermore kernels [10], a demanding numerical benchmark.
Reference: [14] <author> A. Rogers, et. al. </author> <title> Supporting dynamic data structures on distributed memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <year> 1995. </year>
Reference-contexts: We are currently developing a compiler framework using interprocedural analysis coupled with user annotations to automate the optimizations described here. Our work is related to several dynamic programming language systems <ref> [16, 14, 3] </ref> whose objective is to achieve high performance on distributed memory systems. Implementations of these systems have traditionally focused on communication overhead reductions and thread-oriented optimizations, relying on user-specified data distribution to reduce communication volume.
Reference: [15] <editor> J. Saltz, et. al. </editor> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: Figure 4 (a) shows that processor synchronization overhead still constitutes around 13% of the execution time in the node-cache version of the force kernel. We used the inspector/executor model <ref> [15] </ref>, in which an inspector determines all remote data accesses and performs the communication prior to local computation, to completely separate communication from computation. For the force kernel, processor synchronization overhead is dramatically reduced by grouping the synchronization of all outstanding requests, as shown in Figure 4 (a). <p> CHARMM 4 is 4 The CHARMM execution time, which we scaled down appropriately to get an identical number of time steps and neighbor list updates as used in IC-CEDAR, is from http://www.ki.si/parallel.html. a highly-tuned SPMD implementation in Fortran with calls to the CHAOS <ref> [15] </ref> runtime library for communication and load-balancing. To reduce the differences in the data sets, we increased the nonbonded cutoff for the IC-CEDAR input. Because the two codes use different simulation models, it is difficult to make a direct comparison. <p> We believe for these applications, compile-time global program transformations can achieve efficient node-level reuse and communication grouping for COOP languages. The inspector/executor execution model has been extensively studied for SPMD programs <ref> [15, 7] </ref> and has been shown to give good performance for irregular applications whose data access pattern is independent of the ongoing computation. Ongoing efforts to 5 compile such applications in parallel Fortran using this model share many issues with the required transformations here.
Reference: [16] <author> A. Yonezawa, </author> <title> editor. ABCL: An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> 6 </month>
Reference-contexts: We are currently developing a compiler framework using interprocedural analysis coupled with user annotations to automate the optimizations described here. Our work is related to several dynamic programming language systems <ref> [16, 14, 3] </ref> whose objective is to achieve high performance on distributed memory systems. Implementations of these systems have traditionally focused on communication overhead reductions and thread-oriented optimizations, relying on user-specified data distribution to reduce communication volume.
References-found: 16

