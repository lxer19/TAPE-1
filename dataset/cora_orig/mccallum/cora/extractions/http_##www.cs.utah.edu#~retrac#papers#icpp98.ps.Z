URL: http://www.cs.utah.edu/~retrac/papers/icpp98.ps.Z
Refering-URL: http://www.cs.utah.edu/~retrac/papers.html
Root-URL: 
Title: AS-COMA: An Adaptive Hybrid Shared Memory Architecture  
Author: Chen-Chi Kuo, John Carter, Ravindra Kuramkote, Mark Swanson 
Keyword: Distributed shared memory, multiprocessor computer architecture, memory architecture, CC-NUMA, S-COMA, hybrid.  
Note: Technical Areas: Architecture.  
Affiliation: University of Utah, Computer Systems Laboratory  
Abstract: Scalable shared memory multiprocessors traditionally use either a cache coherent nonuniform memory access (CC-NUMA) or simple cache-only memory architecture (S-COMA) memory architecture. Recently, hybrid architectures that combine aspects of both CC-NUMA and S-COMA have emerged. In this paper, we present two improvements over other hybrid architectures. The first improvement is a page allocation algorithm that prefers S-COMA pages at low memory pressures. Once the local free page pool is drained, additional pages are mapped in CC-NUMA mode until they suffer sufficient remote misses to warrant upgrading to S-COMA mode. The second improvement is a page replacement algorithm that dynamically backs off the rate of page remappings from CC-NUMA to S-COMA mode at high memory pressure. This design dramatically reduces the amount of kernel overhead and the number of induced cold misses caused by needless thrashing of the page cache. The resulting hybrid architecture is called adaptive S-COMA (AS-COMA). AS-COMA exploits the best of S-COMA and CC-NUMA, performing like an S-COMA machine at low memory pressure and like a CC-NUMA machine at high memory pressure. AS-COMA outperforms CC-NUMA under almost all conditions, and outperforms other hybrid architectures by up to 17% at low memory pressure and up to 90% at high memory pressure. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Because of the growing gap between microprocessor cycle times and main memory latencies, modern microprocessors incorporate a variety of latency-tolerating features such as fine-grained multithreading, lockup free caches, split transaction memory busses, and out-of-order execution <ref> [1, 11, 15] </ref>. These features reduce the performance bottleneck of both local and remote memory latencies by allowing the processor to perform useful work while memory is being accessed. <p> These features reduce the performance bottleneck of both local and remote memory latencies by allowing the processor to perform useful work while memory is being accessed. However, other than the fine-grained multithreading support of the Tera machine <ref> [1] </ref>, which requires a large amount of parallelism and an expensive and proprietary microprocessor, these techniques can hide only a fraction of the total memory latency. Therefore, it is important to develop memory architectures that reduce the overhead of remote memory access.
Reference: [2] <author> W.J. Bolosky, R.P. Fitzgerald, </author> <title> and M.L. Scott. Simple but effective techniques for NUMA memory management. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 19-31, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [5]. Unfortunately, these applications are fairly common [5, 14, 16]. Careful page allocation <ref> [2, 9] </ref>, migration [21], or replication [21] can alleviate this problem by carefully selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages.
Reference: [3] <author> S. Chandra, J.R. Larus, and A. Rogers. </author> <booktitle> Where is time spent in message-passing and shared-memory programs? In Proceedings of the 6th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 61-73, </pages> <month> October </month> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Note that our network model only accounts for input port contention. 4.2 Benchmark Programs We used six programs to conduct our study: barnes, fft, lu, ocean, and radix from the SPLASH-2 benchmark suite [22] and em3d from a shared memory implementation of the Split-C benchmark <ref> [4, 3] </ref>. Table 5 shows the inputs used for each test program. The column labeled Home pages indicates the number of shared data pages initially allocated at each node. These numbers indicate that each node manages from 0.5 megabytes (barnes) to 2 megabytes (lu, em3d, and ocean) of home data.
Reference: [4] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <booktitle> Parallel programming in spit-c. In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Note that our network model only accounts for input port contention. 4.2 Benchmark Programs We used six programs to conduct our study: barnes, fft, lu, ocean, and radix from the SPLASH-2 benchmark suite [22] and em3d from a shared memory implementation of the Split-C benchmark <ref> [4, 3] </ref>. Table 5 shows the inputs used for each test program. The column labeled Home pages indicates the number of shared data pages initially allocated at each node. These numbers indicate that each node manages from 0.5 megabytes (barnes) to 2 megabytes (lu, em3d, and ocean) of home data.
Reference: [5] <author> B. Falsafi and D.A. Wood. </author> <title> Reactive NUMA: A design for unifying S-COMA and CC-NUMA. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 229-240, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: In this paper, we concentrate on the second and third issues, namely reducing the frequency of remote memory accesses while ensuring that the software overhead required to do this remains modest. Previous studies have tended to ignore the impact of software overhead <ref> [5, 12, 16] </ref>, but our findings indicate that the effect of this factor can be dramatic. Scalable shared memory multiprocessors traditionally use either a cache coherent nonuniform memory access (CC-NUMA) architecture or a simple cache-only memory architecture (S-COMA) [16]. Each architecture performs well under different conditions, as follows. <p> Recently, hybrid architectures that combine aspects of both CC-NUMA and S-COMA have emerged, such as the Wisconsin reactive CC-NUMA (R-NUMA) <ref> [5] </ref> and the USC victim cache NUMA (VC-NUMA) [12]. Intuitively, these hybrid systems attempt to map the remote pages for which there are the highest number of conflict misses to local S-COMA pages, thereby eliminating the greatest number of expensive remote operations. <p> This design dramatically reduces the amount of kernel overhead and the number of induced cold misses caused by needless thrashing of the page cache. The resulting hybrid architecture is called adaptive S-COMA (AS-COMA). R-NUMA <ref> [5] </ref> and VC-NUMA [12] initially map all pages in CC-NUMA mode, and then identify remote pages that are suffering inordinate numbers of conflict misses to remote node, so-called hot pages. <p> Remote data can only be cached in the processor cache (s) or an optional remote access cache (RAC) on the DSM controller. Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs <ref> [5] </ref>. Unfortunately, these applications are fairly common [5, 14, 16]. <p> Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [5]. Unfortunately, these applications are fairly common <ref> [5, 14, 16] </ref>. Careful page allocation [2, 9], migration [21], or replication [21] can alleviate this problem by carefully selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages. <p> Sources of this overhead include the time spent context switching between the user application and the pageout daemon, flushing blocks from the victim page (s), and remapping pages. 2.4 Hybrid DSM Architectures Two hybrid CC-NUMA/S-COMA architectures have been proposed: R-NUMA <ref> [5] </ref> and VC-NUMA [12]. We describe these architectures in this section. The basic architecture of an R-NUMA machine [5] is that of a CC-NUMA machine. <p> context switching between the user application and the pageout daemon, flushing blocks from the victim page (s), and remapping pages. 2.4 Hybrid DSM Architectures Two hybrid CC-NUMA/S-COMA architectures have been proposed: R-NUMA <ref> [5] </ref> and VC-NUMA [12]. We describe these architectures in this section. The basic architecture of an R-NUMA machine [5] is that of a CC-NUMA machine. However, unlike CC-NUMA, which "wastes" local physical memory not required to hold home pages, R-NUMA uses this otherwise unused storage to cache frequently accessed remote pages, as in S 8 COMA. <p> By tracking refetch counts, it is able to select dynamically which CC-NUMA pages should populate the S-COMA cache based on access behavior. In a recent study <ref> [5] </ref>, R-NUMA's flexibility and intelligent selection of pages to map in S-COMA mode caused it to outperform the best of pure CC-NUMA and pure S-COMA by up to 37% on some applications. <p> However, although R-NUMA frequently outperforms both CC-NUMA and S-COMA, it was also observed to perform as much as 57% worse on some applications <ref> [5] </ref>. This poor performance can be attributed to two problems. First, R-NUMA initially maps all pages in CC-NUMA mode, and only upgrades them to S-COMA mode after some number of remote refetches occur, which introduces needless remote refetches when memory pressure is low. <p> We, therefore, model a single 8-kilobyte direct-mapped processor cache to compensate for the small size of the data sets, which is consistent with previous studies of hybrid architectures <ref> [5, 12] </ref>. We model a 4-bank main memory controller that can supply data from local memory in 58 cycles. The size of main memory and the amount of free memory used for page caching was varied to test the different models under varying memory pressures.
Reference: [6] <author> J. Laudon and D. Lenoski. </author> <title> The SGI Origin: A ccNUMA highly scalable server. </title> <booktitle> In SIGARCH97, </booktitle> <pages> pages 241-251, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: The designers of high-end commercial DSM systems such as the SUN UE10000 [18] and SGI Origin 2000 <ref> [6] </ref> have put considerable effort into reducing the remote memory latency by developing specialized high speed interconnects. These efforts can reduce the ratio of remote to local memory latency to as low as 2:1, but they require expensive hardware available only on high-end servers costing hundreds of thousands of dollars. <p> Each architecture performs well under different conditions, as follows. CC-NUMA is the most common DSM memory architecture. It is embodied by such machines as the Stanford DASH [7], SUN UE10000 [18], and SGI Origin 2000 <ref> [6] </ref>. In a CC-NUMA, shared physical memory is evenly distributed amongst the nodes in the machine, and each page of shared memory has a home location. The home node of data can be determined from its global physical address. <p> To reduce this overhead, designers of some such systems have adopted high speed interconnect to reduce (T remote ) <ref> [6, 13, 18] </ref>. 2.3 S-COMA In the S-COMA model [16], the DSM controller and operating system cooperate to provide access to remotely homed data. In S-COMA, a mapping from a global virtual address to a local physical address is created at the first page fault to that shared memory page.
Reference: [7] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Each architecture performs well under different conditions, as follows. CC-NUMA is the most common DSM memory architecture. It is embodied by such machines as the Stanford DASH <ref> [7] </ref>, SUN UE10000 [18], and SGI Origin 2000 [6]. In a CC-NUMA, shared physical memory is evenly distributed amongst the nodes in the machine, and each page of shared memory has a home location. The home node of data can be determined from its global physical address.
Reference: [8] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Processors can access any piece of global data by mapping a virtual address to the appropriate global physical address, but the amount of remote shared data that can be replicated on a node is limited by the size of a node's processor cache (s) and remote access cache (RAC) <ref> [8] </ref>. 2 Thus, CC-NUMA machines generally perform poorly when the rate of conflict or capacity misses is high, such as when a node's caches are too small to hold the entire remote working set or when the data access patterns and cache organization cause cached remote data to be purged frequently.
Reference: [9] <author> M. Marchetti, L. Kontothonassis, R. Bianchini, </author> <title> and M.L. Scott. Using simple page placement policies to reduce the code of cache fills in coherent shared-memory systems. </title> <booktitle> In Proceedings of the Ninth ACM/IEEE International Parallel Processing Symposium (IPPS), </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [5]. Unfortunately, these applications are fairly common [5, 14, 16]. Careful page allocation <ref> [2, 9] </ref>, migration [21], or replication [21] can alleviate this problem by carefully selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages. <p> All three hybrid architectures we study adopt BSD4.4's page allocation mechanism and paging policy [10] with minor modifications. Free min and free target (see Section 3) were set to 5% and 7% of total memory, respectively. We extended the first touch allocation algorithm <ref> [9] </ref> to distribute home pages equally to nodes by limiting the number of home pages that are allocated at each node to a proportional share of the total number of pages.
Reference: [10] <editor> M.K. Mckusick, K. Bostic, M.J. Karels, and J.S. Quarterman. </editor> <booktitle> The Design and Implementation of the 4.4BSD operating system, chapter 5 Memory Management, </booktitle> <pages> pages 117-190. </pages> <publisher> Addison-Wesley Publishing Company Inc, </publisher> <year> 1996. </year>
Reference-contexts: The modeled physical page size is 4 kilobytes. The VM system was modified to provide the page translation, allocation, and replacement support needed by the various distributed shared memory models. All three hybrid architectures we study adopt BSD4.4's page allocation mechanism and paging policy <ref> [10] </ref> with minor modifications. Free min and free target (see Section 3) were set to 5% and 7% of total memory, respectively.
Reference: [11] <institution> MIPS Technologies Inc. </institution> <note> MIPS R10000 Microprocessor User's Manual, Version 2.0, </note> <month> December </month> <year> 1996. </year>
Reference-contexts: Because of the growing gap between microprocessor cycle times and main memory latencies, modern microprocessors incorporate a variety of latency-tolerating features such as fine-grained multithreading, lockup free caches, split transaction memory busses, and out-of-order execution <ref> [1, 11, 15] </ref>. These features reduce the performance bottleneck of both local and remote memory latencies by allowing the processor to perform useful work while memory is being accessed.
Reference: [12] <author> A. Moga and M. Dubois. </author> <title> The effectiveness of SRAM network caches in clustered DSMs. </title> <booktitle> In Proceedings of the Fourth Annual Symposium on High Performance Computer Architecture, </booktitle> <year> 1998. </year>
Reference-contexts: In this paper, we concentrate on the second and third issues, namely reducing the frequency of remote memory accesses while ensuring that the software overhead required to do this remains modest. Previous studies have tended to ignore the impact of software overhead <ref> [5, 12, 16] </ref>, but our findings indicate that the effect of this factor can be dramatic. Scalable shared memory multiprocessors traditionally use either a cache coherent nonuniform memory access (CC-NUMA) architecture or a simple cache-only memory architecture (S-COMA) [16]. Each architecture performs well under different conditions, as follows. <p> Recently, hybrid architectures that combine aspects of both CC-NUMA and S-COMA have emerged, such as the Wisconsin reactive CC-NUMA (R-NUMA) [5] and the USC victim cache NUMA (VC-NUMA) <ref> [12] </ref>. Intuitively, these hybrid systems attempt to map the remote pages for which there are the highest number of conflict misses to local S-COMA pages, thereby eliminating the greatest number of expensive remote operations. All other remote pages are mapped in CC-NUMA mode. <p> This design dramatically reduces the amount of kernel overhead and the number of induced cold misses caused by needless thrashing of the page cache. The resulting hybrid architecture is called adaptive S-COMA (AS-COMA). R-NUMA [5] and VC-NUMA <ref> [12] </ref> initially map all pages in CC-NUMA mode, and then identify remote pages that are suffering inordinate numbers of conflict misses to remote node, so-called hot pages. <p> Sources of this overhead include the time spent context switching between the user application and the pageout daemon, flushing blocks from the victim page (s), and remapping pages. 2.4 Hybrid DSM Architectures Two hybrid CC-NUMA/S-COMA architectures have been proposed: R-NUMA [5] and VC-NUMA <ref> [12] </ref>. We describe these architectures in this section. The basic architecture of an R-NUMA machine [5] is that of a CC-NUMA machine. <p> When memory pressure is high, and the number of hot pages exceeds the number of free 9 pages available for caching them, this behavior results in frequent expensive page remappings for little value. This leads to performance worse than CC-NUMA, which never remaps pages. VC-NUMA <ref> [12] </ref> treats its RAC as a victim cache for the processor cache (s), i.e., only remote data evicted from the processor cache (s) is placed in its RAC. <p> We, therefore, model a single 8-kilobyte direct-mapped processor cache to compensate for the small size of the data sets, which is consistent with previous studies of hybrid architectures <ref> [5, 12] </ref>. We model a 4-bank main memory controller that can supply data from local memory in 58 cycles. The size of main memory and the amount of free memory used for page caching was varied to test the different models under varying memory pressures. <p> Thus, the results reported for VC-NUMA are only relevant for evaluating its relocation strategy, and not the value of treating the page cache as a victim cache <ref> [12] </ref>. Finally, Table 4 shows the minimum latency required to satisfy a load or store from various locations in the global memory hierarchy. <p> In particular, VC-NUMA only checks its backoff indicator when an average of two replacements per cached page have occurred, which is not sufficiently often to avoid thrashing. As shown in the previous study <ref> [12] </ref>, VC-NUMA does not significantly outperform R-NUMA until memory pressure exceeds 87.5%. Once again, AS-COMA's adaptive replacement algorithm detects thrashing as soon as it starts to occur, and the resulting backoff mechanism causes performance to degrade only slightly as memory pressure increases.
Reference: [13] <author> A. Nowatzyk, G. Aybay, M. Browne, E. Kelly, M. Parkin, B. Radke, and S. Vishin. </author> <title> The S3.mp scalable shared memory multiprocessor. </title> <booktitle> In Proceedings of the 1995 International Conference on Parallel Processing, </booktitle> <year> 1995. </year>
Reference-contexts: To reduce this overhead, designers of some such systems have adopted high speed interconnect to reduce (T remote ) <ref> [6, 13, 18] </ref>. 2.3 S-COMA In the S-COMA model [16], the DSM controller and operating system cooperate to provide access to remotely homed data. In S-COMA, a mapping from a global virtual address to a local physical address is created at the first page fault to that shared memory page.
Reference: [14] <author> S. E. </author> <title> Perl and R.L. Sites. Studies of Windows NT performance using dynamic execution traces. </title> <booktitle> In Proceedings of the Second Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 169-184, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [5]. Unfortunately, these applications are fairly common <ref> [5, 14, 16] </ref>. Careful page allocation [2, 9], migration [21], or replication [21] can alleviate this problem by carefully selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages.
Reference: [15] <author> V. Santhanam, E.H. Fornish, and W.-C. Hsu. </author> <title> Data prefetching on the HP PA-8000. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 264-273, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Because of the growing gap between microprocessor cycle times and main memory latencies, modern microprocessors incorporate a variety of latency-tolerating features such as fine-grained multithreading, lockup free caches, split transaction memory busses, and out-of-order execution <ref> [1, 11, 15] </ref>. These features reduce the performance bottleneck of both local and remote memory latencies by allowing the processor to perform useful work while memory is being accessed.
Reference: [16] <author> A. Saulsbury, T. Wilkinson, J. Carter, and A. Landin. </author> <title> An argument for Simple COMA. </title> <booktitle> In Proceedings of the First Annual Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 276-285, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: In this paper, we concentrate on the second and third issues, namely reducing the frequency of remote memory accesses while ensuring that the software overhead required to do this remains modest. Previous studies have tended to ignore the impact of software overhead <ref> [5, 12, 16] </ref>, but our findings indicate that the effect of this factor can be dramatic. Scalable shared memory multiprocessors traditionally use either a cache coherent nonuniform memory access (CC-NUMA) architecture or a simple cache-only memory architecture (S-COMA) [16]. Each architecture performs well under different conditions, as follows. <p> Previous studies have tended to ignore the impact of software overhead [5, 12, 16], but our findings indicate that the effect of this factor can be dramatic. Scalable shared memory multiprocessors traditionally use either a cache coherent nonuniform memory access (CC-NUMA) architecture or a simple cache-only memory architecture (S-COMA) <ref> [16] </ref>. Each architecture performs well under different conditions, as follows. CC-NUMA is the most common DSM memory architecture. It is embodied by such machines as the Stanford DASH [7], SUN UE10000 [18], and SGI Origin 2000 [6]. <p> S-COMA architectures employ any unused DRAM on a node as a cache for remote data <ref> [16] </ref>, which significantly increases the amount of storage available on each node for caching remote data. The performance of pure S-COMA machines is heavily dependent on the memory pressure of a particular application. <p> Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [5]. Unfortunately, these applications are fairly common <ref> [5, 14, 16] </ref>. Careful page allocation [2, 9], migration [21], or replication [21] can alleviate this problem by carefully selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages. <p> To reduce this overhead, designers of some such systems have adopted high speed interconnect to reduce (T remote ) [6, 13, 18]. 2.3 S-COMA In the S-COMA model <ref> [16] </ref>, the DSM controller and operating system cooperate to provide access to remotely homed data. In S-COMA, a mapping from a global virtual address to a local physical address is created at the first page fault to that shared memory page. <p> An improved hybrid architecture, motivated by the analysis above, that performs well regardless of memory pressure is discussed in the following section. 3 Adaptive S-COMA At low memory pressure, S-COMA outperforms CC-NUMA, but the converse is true at high memory pressure <ref> [16] </ref>. Thus, our goal when designing AS-COMA was to develop a memory architecture that performed like pure S-COMA when memory for page caching was plentiful, and like CC-NUMA when it is not. To exploit S-COMA's superior performance at low memory pressures, AS-COMA initially maps pages in S-COMA mode.
Reference: [17] <author> L.B. Stoller, R. Kuramkote, </author> <title> and M.R. Swanson. PAINT- PA instruction set interpreter. </title> <type> Technical Report UUCS-96-009, </type> <institution> University of Utah Computer Science Department, </institution> <month> September </month> <year> 1996. </year>
Reference: [18] <author> Sun Microsystems. </author> <title> Ultra Enterprise 10000 System Overview. </title> <address> http://www.sun.com/servers/datacenter/products/starfire. </address>
Reference-contexts: The designers of high-end commercial DSM systems such as the SUN UE10000 <ref> [18] </ref> and SGI Origin 2000 [6] have put considerable effort into reducing the remote memory latency by developing specialized high speed interconnects. <p> Each architecture performs well under different conditions, as follows. CC-NUMA is the most common DSM memory architecture. It is embodied by such machines as the Stanford DASH [7], SUN UE10000 <ref> [18] </ref>, and SGI Origin 2000 [6]. In a CC-NUMA, shared physical memory is evenly distributed amongst the nodes in the machine, and each page of shared memory has a home location. The home node of data can be determined from its global physical address. <p> To reduce this overhead, designers of some such systems have adopted high speed interconnect to reduce (T remote ) <ref> [6, 13, 18] </ref>. 2.3 S-COMA In the S-COMA model [16], the DSM controller and operating system cooperate to provide access to remotely homed data. In S-COMA, a mapping from a global virtual address to a local physical address is created at the first page fault to that shared memory page.
Reference: [19] <author> M. Swanson and L. Stoller. </author> <title> Shared memory as a basis for conservative distributed architectural simulation. </title> <booktitle> In Parallel and Distributed Simulation (PADS '97), </booktitle> <year> 1997. </year> <note> Submitted for publication. </note>
Reference: [20] <author> J.E. Veenstra and R.J. Fowler. Mint: </author> <title> A front end for efficient simulation of shared-memory multiprocessors. </title> <booktitle> In MASCOTS 1994, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: Nevertheless, AS-COMA is able to converge rapidly to either S-COMA or CC-NUMA mode, depending on the memory pressure. 4 Performance Evaluation 4.1 Experimental Setup All experiments were performed using an execution-driven simulation of the HP PA-RISC architecture called Paint (PA-interpreter)[17, 19]. Paint was derived from the Mint simulator <ref> [20] </ref>. Our simulation environment includes detailed simulation modules for a first level cache, system bus, memory controller, network interconnect, and DSM engine. It provides a multiprogrammed process model with support for operating system code, so the effects of OS/user code interactions are modeled.
Reference: [21] <author> B. Verghese, S. Devine, A. Gupta, and M. Rosenblum. </author> <title> Operating system support for improving data locality on CC-NUMA compute servers. </title> <booktitle> In Proceedings of the 7th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [5]. Unfortunately, these applications are fairly common [5, 14, 16]. Careful page allocation [2, 9], migration <ref> [21] </ref>, or replication [21] can alleviate this problem by carefully selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages. <p> Applications that suffer a large number of conflict misses to remote data, e.g., due to the limited amount of caching of remote data, perform poorly on CC-NUMAs [5]. Unfortunately, these applications are fairly common [5, 14, 16]. Careful page allocation [2, 9], migration <ref> [21] </ref>, or replication [21] can alleviate this problem by carefully selecting or modifying the choice of home node for a given page of data, but these techniques have to date only been successful for read-only or non-shared pages.
Reference: [22] <author> S.C. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year> <month> 25 </month>
Reference-contexts: switch topology, port contention (only) modeled Fall through delay: 4 cycles (ratio between remote to local memory access latencies - 3:1) Table 3 Cache and Network Characteristics For most of the SPLASH2 applications we studied, the data sets provided have a primary working set that fits in an 8-kbyte cache <ref> [22] </ref>. We, therefore, model a single 8-kilobyte direct-mapped processor cache to compensate for the small size of the data sets, which is consistent with previous studies of hybrid architectures [5, 12]. We model a 4-bank main memory controller that can supply data from local memory in 58 cycles. <p> The remote to local memory access ratio is about 3:1. Note that our network model only accounts for input port contention. 4.2 Benchmark Programs We used six programs to conduct our study: barnes, fft, lu, ocean, and radix from the SPLASH-2 benchmark suite <ref> [22] </ref> and em3d from a shared memory implementation of the Split-C benchmark [4, 3]. Table 5 shows the inputs used for each test program. The column labeled Home pages indicates the number of shared data pages initially allocated at each node.
References-found: 22

