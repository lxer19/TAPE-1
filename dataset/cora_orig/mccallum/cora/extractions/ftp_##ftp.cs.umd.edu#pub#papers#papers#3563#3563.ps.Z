URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3563/3563.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/authors/Anurag_Acharya-no-abs.html
Root-URL: 
Email: fgagan,acha,saltzg@cs.umd.edu  
Phone: (301)-405-2756  
Title: An Interprocedural Framework for Placement of Asynchronous I/O Operations  
Author: Gagan Agrawal and Anurag Acharya and Joel Saltz 
Address: College Park, MD 20742  
Affiliation: UMIACS and Department of Computer Science University of Maryland  
Abstract: Overlapping memory accesses with computations is a standard technique for improving performance on modern architectures, which have deep memory hierarchies. In this paper, we present a compiler technique for overlapping accesses to secondary memory (disks) with computation. We have developed an Interprocedural Balanced Code Placement (IBCP) framework, which performs analysis on arbitrary recursive procedures and arbitrary control flow and replaces synchronous I/O operations with a balanced pair of asynchronous operations. We demonstrate how this analysis is useful for applications which perform frequent and large accesses to secondary memory, including applications which snapshot or checkpoint their computations or out-of-core applications.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anurag Acharya, Mustafa Uysal, Robert Bennett, Assaf Mendelson, Mike Beynon, Jeff Hollingsworth, Joel Saltz, and Alan Sussman. </author> <title> Tuning the Performance of I/O Intensive Parallel Applications. </title> <note> Submitted to IOPADS'96, </note> <month> October </month> <year> 1995. </year>
Reference-contexts: Such codes need to access the secondary memory very frequently. Examples include several sensor data processing codes which perform out-of-core reductions on images which are several hundred MB in size. Restructuring the computation can often reduce the frequency and increase the granularity of secondary memory access <ref> [1, 6] </ref>, however, such codes can still spend significant amount of their time in I/O operations. 3 Problem Definition In this section, we define the interprocedural balanced code placement problem and motivate our analysis framework. <p> All our experiments were done on an IBM RS/6000 running AIX 3.2.5, with 64 MB of primary memory and one 2.2 GB IBM Starfire 7200 SCSI disk. The Starfire 7200 is rated at a maximum bandwidth of 8 MB/s; the maximum measured application-level file I/O bandwidth is 7.5 MB/s <ref> [1] </ref>. For all our experiments, we measured end-to-end execution time including a final fsync () to ensure that all data has been written to disk. This allowed us to include the cost of operating system operations. <p> To implement this out-of-core max-reduction efficiently, a bounding region of the image containing the pixels to be updated, is read, modified in-core and written back to disk. This template was loosely based on pathfinder, the AVHRR program from the NASA Goddard Distributed Active Archive Center <ref> [1] </ref>. It has a similar organization, the same memory requirement and processes its input in same size (500 KB) chunks. <p> Kotz has developed a similar technique, disk-directed I/O for performing collective I/O operations [18]. Other projects have focussed on benchmarking I/O intensive applications, including Crandall et. al at Illinois [11] and Acharya et. al at Maryland <ref> [1] </ref>. An important limitation of our current work has been to consider each array as a single entity, i.e., modification or reference to any element of an array is considered as mod/ref to the entire array.
Reference: [2] <author> Gagan Agrawal and Joel Saltz. </author> <title> Interprocedural communication optimizations for distributed memory compilation. </title> <booktitle> In Proceedings of the 7th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 283-299, </pages> <month> August </month> <year> 1994. </year> <note> Also available as University of Maryland Technical Report CS-TR-3264. </note>
Reference-contexts: In such scenarios, it will be profitable to perform analysis across procedure boundaries to prefetch data into the L2 cache. We believe that our analysis can be extended to perform prefetches for L2 caches as well. In separate work, we have worked on Interprocedural Partial Redundancy Elimination (IPRE) <ref> [2, 4] </ref> and other placement optimizations for distributed memory compilation [3].
Reference: [3] <author> Gagan Agrawal and Joel Saltz. </author> <title> Interprocedural compilation of irregular applications for distributed memory machines. </title> <booktitle> In Proceedings Supercomputing '95. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1995. </year> <note> To appear. Also available as University of Maryland Technical Report CS-TR-3447. </note>
Reference-contexts: We believe that our analysis can be extended to perform prefetches for L2 caches as well. In separate work, we have worked on Interprocedural Partial Redundancy Elimination (IPRE) [2, 4] and other placement optimizations for distributed memory compilation <ref> [3] </ref>. The analysis required for balanced code placement is significantly different from the analysis in IPRE for at least two reasons: First, IBCP analysis needs to ensure that there is exactly one occurrence of asynchronous operation corresponding to each occurrence of the synchronous operation.
Reference: [4] <author> Gagan Agrawal, Joel Saltz, and Raja Das. </author> <title> Interprocedural partial redundancy elimination and its application to distributed memory compilation. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 258-269. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1995. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 30, No. </volume> <pages> 6. </pages> <note> Also available as University of Maryland Technical Report CS-TR-3446 and UMIACS-TR-95-42. 23 </note>
Reference-contexts: In the case of out-of-core programs, the initial phase of the compilation can possibly generate redundant requests. In this case, some redundancy elimination analysis like Interprocedural Partial Redundancy Elimination (IPRE) <ref> [4] </ref> can be applied before applying the analysis we present in this paper. 4 4 Interprocedural Balanced Code Placement Framework In the previous section, we described the requirements of our interprocedural code placement framework. In this section, we present Interprocedural Balanced Code Placement (IBCP) framework, which performs such placement. <p> In such scenarios, it will be profitable to perform analysis across procedure boundaries to prefetch data into the L2 cache. We believe that our analysis can be extended to perform prefetches for L2 caches as well. In separate work, we have worked on Interprocedural Partial Redundancy Elimination (IPRE) <ref> [2, 4] </ref> and other placement optimizations for distributed memory compilation [3].
Reference: [5] <author> Graeme A. Bird. </author> <title> Molecular Gas Dynamics and the Direct Simulation of Gas Flows. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1994. </year>
Reference-contexts: In the former case, a series of program states are written to disks for post-processing. In the latter case, the state of the computation is periodically piped into an appropriate visualization tool. Examples of applications that require snapshoting include Direct Simulation Monte Carlo (DSMC) <ref> [5] </ref>, simulation of a flame sweeping through a volume [27], airplane wake simulations [19], etc. <p> Call P (B,d) Start op (x,y) End op (A,d) ..other computations .. End op (B,d) End Enddo End 5.1 Direct Simulation Monte Carlo Direction Simulation Monte Carlo is a well known technique for studying the interaction of particles in cells <ref> [5] </ref>. The program we used in our experiments, dsmc-3d was originally developed by Richard Wilmoth at NASA Langley [21]. To study the evolution of the process being simulated, it is useful to snapshot the position of all the particles after every time-step.
Reference: [6] <author> R. Bordawekar, A. Choudhary, K. Kennedy, C. Koelbel, and M. Paleczny. </author> <title> A model and compilation strategy for out-of-core data parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 1-10. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1995. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 30, No. </volume> <pages> 8. </pages>
Reference-contexts: The cost of data access increases rapidly with the depth of access. Achieving and sustaining good performance in presence of deep memory hierarchies is a very important problem and has received significant attention in the last few years. Several research projects have worked on code transformations to improve locality <ref> [6, 31, 32] </ref>. Most of the programs still spend considerable amount of their time in accessing data from memory at a deeper level in the hierarchy. The overhead of deep memory accesses can be reduced by using asynchronous operations overlapped with computations. <p> Such codes need to access the secondary memory very frequently. Examples include several sensor data processing codes which perform out-of-core reductions on images which are several hundred MB in size. Restructuring the computation can often reduce the frequency and increase the granularity of secondary memory access <ref> [1, 6] </ref>, however, such codes can still spend significant amount of their time in I/O operations. 3 Problem Definition In this section, we define the interprocedural balanced code placement problem and motivate our analysis framework. <p> In the applications which checkpoint or snapshot the progress of the computation, the programmer may explicitly insert synchronous write operations. In the out-of-core applications, an initial phase of the compilation may determine when disk operations need to be made and correspondingly may insert synchronous read or write operations <ref> [6, 26] </ref>. For our purpose, we consider original program text (or a compiler processed representation) which has explicit calls to read or write operations. We denote by Op, any such synchronous 3 operation, which the compiler may want to replace by a corresponding asynchronous or split--phase operation. <p> Neither of these groups have proposed any general techniques for placement of asynchronous operations or any interprocedural optimizations. Hand-compilation experiments have been presented to show performance gains from using asynchronous I/O <ref> [6] </ref>. Significant amount of work has been done on runtime libraries for optimizing Parallel I/O. PASSION library at Syracuse University is one such library for optimizing I/O accesses and uses the two phase I/O technique [7, 9].
Reference: [7] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 452-461. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Hand-compilation experiments have been presented to show performance gains from using asynchronous I/O [6]. Significant amount of work has been done on runtime libraries for optimizing Parallel I/O. PASSION library at Syracuse University is one such library for optimizing I/O accesses and uses the two phase I/O technique <ref> [7, 9] </ref>. Kotz has developed a similar technique, disk-directed I/O for performing collective I/O operations [18]. Other projects have focussed on benchmarking I/O intensive applications, including Crandall et. al at Illinois [11] and Acharya et. al at Maryland [1].
Reference: [8] <author> D. Callahan, Ken Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The overhead of deep memory accesses can be reduced by using asynchronous operations overlapped with computations. Substantial work has been done on compiler analysis for overlapping memory accesses with computations, mostly in the context of reducing overheads of cache stalls <ref> [8, 23] </ref>. Several classes of applications involve large and frequent accesses to data stored in the secondary memory. The need for reading or writing data in secondary memory can arise for fl This work was supported by NSF under grant No. ASC 9213821, by ONR under contract No. <p> Several researchers have developed compiler techniques for overlapping cache stalls with computation (also known as software prefetching). This includes the work of Mowry et al. as part of the SUIF system [23] and Callahan et al. at Rice University <ref> [8] </ref>. The amount of time required for cache misses is usually of the order of 10 cycles; much smaller than the disk latency, which of the order of 100,000 cycles. Therefore, for cache prefetching one only needs to consider overlap within a single loop, as they do.
Reference: [9] <author> Alok Choudhary, Rajesh Bordawekar, Michael Harry, Rakesh Krishnaiyer, Ravi Ponnusamy, Tarvin-der Singh, and Rajeev Thakur. </author> <title> PASSION: Parallel and scalable software for input-output. </title> <type> Technical Report SCCS-636, </type> <institution> NPAC, </institution> <month> September </month> <year> 1994. </year> <note> Also available as CRPC Report CRPC-TR94483. </note>
Reference-contexts: Hand-compilation experiments have been presented to show performance gains from using asynchronous I/O [6]. Significant amount of work has been done on runtime libraries for optimizing Parallel I/O. PASSION library at Syracuse University is one such library for optimizing I/O accesses and uses the two phase I/O technique <ref> [7, 9] </ref>. Kotz has developed a similar technique, disk-directed I/O for performing collective I/O operations [18]. Other projects have focussed on benchmarking I/O intensive applications, including Crandall et. al at Illinois [11] and Acharya et. al at Maryland [1].
Reference: [10] <author> K. Cooper and K. Kennedy. </author> <title> Interprocedural side-effect analysis in linear time. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: In the absence of aliasing, this information can easily be computed by flow--insensitive interprocedural analysis in time linear to the size of call graph of the program <ref> [10] </ref>. This information is used by the FSUM cs functions defined later. 4.2 Candidates for Placement We now introduce the format of the read and write operations that we assume.
Reference: [11] <author> P. E. Crandall, R. A. Aydt, A. C. Chien, and D. A. Reed. </author> <title> Input/Output characteristics of Scalable Parallel Applications. </title> <booktitle> In Proceedings Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: Checkpointing. Checkpointing is often required for long running applications which can get interrupted for a variety of reasons [12]. Checkpointing is also used for parametric studies, i.e., modifying some of the checkpointed values and restarting the computation <ref> [11] </ref>. Out-of-Core Computations. Several scientific and engineering computations operate on large data structures which do not fit into the main memory. Such codes need to access the secondary memory very frequently. <p> Kotz has developed a similar technique, disk-directed I/O for performing collective I/O operations [18]. Other projects have focussed on benchmarking I/O intensive applications, including Crandall et. al at Illinois <ref> [11] </ref> and Acharya et. al at Maryland [1]. An important limitation of our current work has been to consider each array as a single entity, i.e., modification or reference to any element of an array is considered as mod/ref to the entire array.
Reference: [12] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven Parallel I/O. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Checkpointing. Checkpointing is often required for long running applications which can get interrupted for a variety of reasons <ref> [12] </ref>. Checkpointing is also used for parametric studies, i.e., modifying some of the checkpointed values and restarting the computation [11]. Out-of-Core Computations. Several scientific and engineering computations operate on large data structures which do not fit into the main memory. Such codes need to access the secondary memory very frequently.
Reference: [13] <author> E. H. Gornish, E. D. Granston, and A. V. Veindenbaum. </author> <title> Compiler-directed data prefetching in multiprocessors with memory hierarchies. </title> <booktitle> In Proceedings of International Conference on SuperComputing, </booktitle> <pages> pages 354-368, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: This framework can also be used for placement of read and write operations, however, it is restricted to analysis and placement within a single procedure. Gornish et al. present methods for prefetching in shared memory multiprocessors <ref> [13] </ref>. Their techniques are also applicable for placing read operations early within a single procedure. However, their techniques apply only when the procedure has a simple loop structure. In contrast, our method does not impose any restrictions on the shapes of call graph and CFGs of the procedures.
Reference: [14] <author> Mary Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: A number of different program representations have been used for various interprocedural data flow problems. The most commonly used program representation is a Call Graph <ref> [14] </ref>. A call graph is a directed multi graph, which has a single node for each procedure in the program. A directed edge from node i to node j means that procedure i calls procedure j.
Reference: [15] <author> Mary Hall, John Mellor Crummey, Alan Carle, and Rene G Rodriguez. FIAT: </author> <title> A framework for interprocedural analysis and transformations. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 522-545. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: In contrast, our method does not impose any restrictions on the shapes of call graph and CFGs of the procedures. Several other projects have performed interprocedural optimizations for parallelism and for dealing with memory hierarchies. FIAT has been proposed as a general framework for interprocedural analysis <ref> [15] </ref>, but largely targets flow-insensitive problems. Compiler optimizations for improving I/O accesses have been addressed by at least two projects. The PASSION compiler (based upon Syracuse F90D system) performs loop transformations for improving locality in out-of-core applications [29].
Reference: [16] <author> Reinhard von Hanxleden and Ken Kennedy. </author> <title> Give-n-take a balanced code placement framework. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 107-120. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1994. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 29, No. </volume> <pages> 6. </pages>
Reference-contexts: Note that the first requirement mentioned above has two implications: * The occurrences of Start op and End op will match each other irrespective of the control flow paths taken during the execution of the program. This is also known as the balanced code placement property <ref> [16] </ref>. * In any execution of the program, there will exactly be the same number of asynchronous operations as there were synchronous operations, i.e., under no circumstances, the new placement will increase the number of I/O operations on any path. This is known as the safety property [22]. <p> Finally, we present the intraprocedural phase of our analysis. Data flow analysis has been a key method for performing various optimization in the program text, without imposing any restrictions on control-flow [20]. Various intraprocedural code placement frameworks like Partial Redundancy Elimination [22] and Give-N-Take <ref> [16] </ref> perform data flow analysis on Control Flow Graph (CFG) of a single procedure. A number of different program representations have been used for various interprocedural data flow problems. The most commonly used program representation is a Call Graph [14]. <p> Use of the same representation for IBCP framework establishes that this representation has wide applicability. Hanxleden and Kennedy have developed a general framework for communication placement, which includes performing early placement of sends and late placement of receives <ref> [16] </ref>. This framework can also be used for placement of read and write operations, however, it is restricted to analysis and placement within a single procedure. Gornish et al. present methods for prefetching in shared memory multiprocessors [13].
Reference: [17] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: An important limitation of our current work has been to consider each array as a single entity, i.e., modification or reference to any element of an array is considered as mod/ref to the entire array. We plan to augment our analysis with array section analysis <ref> [17] </ref> to improve its accuracy. Consider, for example, a loop iterating over the elements of an array, which is to be snapshot later.
Reference: [18] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Department of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Significant amount of work has been done on runtime libraries for optimizing Parallel I/O. PASSION library at Syracuse University is one such library for optimizing I/O accesses and uses the two phase I/O technique [7, 9]. Kotz has developed a similar technique, disk-directed I/O for performing collective I/O operations <ref> [18] </ref>. Other projects have focussed on benchmarking I/O intensive applications, including Crandall et. al at Illinois [11] and Acharya et. al at Maryland [1].
Reference: [19] <author> Kwan-Liu Ma and Z.C. Zheng. </author> <title> 3D visualization of unsteady 2D airplane wake vortices. </title> <booktitle> In Proceedings of Visualization'94, </booktitle> <pages> pages 124-31, </pages> <month> Oct </month> <year> 1994. </year>
Reference-contexts: In the latter case, the state of the computation is periodically piped into an appropriate visualization tool. Examples of applications that require snapshoting include Direct Simulation Monte Carlo (DSMC) [5], simulation of a flame sweeping through a volume [27], airplane wake simulations <ref> [19] </ref>, etc. The amount of data generated per time-step is often large, e.g., if one million particle are simulated in a particular run of the three-dimensional dsmc-3d code, then each snapshot is 24 MB (3 double precision numbers per particle). Checkpointing.
Reference: [20] <author> T.J. Marlowe and B.G. Ryder. </author> <title> Properties of data flow frameworks. </title> <journal> Acta Informatica, </journal> <volume> 28 </volume> <pages> 121-163, </pages> <year> 1990. </year>
Reference-contexts: Finally, we present the intraprocedural phase of our analysis. Data flow analysis has been a key method for performing various optimization in the program text, without imposing any restrictions on control-flow <ref> [20] </ref>. Various intraprocedural code placement frameworks like Partial Redundancy Elimination [22] and Give-N-Take [16] perform data flow analysis on Control Flow Graph (CFG) of a single procedure. A number of different program representations have been used for various interprocedural data flow problems.
Reference: [21] <author> B. Moon and J. Saltz. </author> <title> Adaptive runtime support for direct simulation Monte Carlo methods on distributed memory architectures. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 176-183. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year> <month> 24 </month>
Reference-contexts: End op (B,d) End Enddo End 5.1 Direct Simulation Monte Carlo Direction Simulation Monte Carlo is a well known technique for studying the interaction of particles in cells [5]. The program we used in our experiments, dsmc-3d was originally developed by Richard Wilmoth at NASA Langley <ref> [21] </ref>. To study the evolution of the process being simulated, it is useful to snapshot the position of all the particles after every time-step. This program is parameterized by the number of particles which governs the memory and I/O requirements of the program.
Reference: [22] <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Communi--cations of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: This is known as the safety property <ref> [22] </ref>. Note that we do not consider redundancy elimination as part of our analysis i.e., there is always exactly one asynchronous operation placed for each synchronous operation in the original program text. <p> Finally, we present the intraprocedural phase of our analysis. Data flow analysis has been a key method for performing various optimization in the program text, without imposing any restrictions on control-flow [20]. Various intraprocedural code placement frameworks like Partial Redundancy Elimination <ref> [22] </ref> and Give-N-Take [16] perform data flow analysis on Control Flow Graph (CFG) of a single procedure. A number of different program representations have been used for various interprocedural data flow problems. The most commonly used program representation is a Call Graph [14].
Reference: [23] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 62-73. </pages> <publisher> ACM Press, </publisher> <month> October </month> <year> 1992. </year>
Reference-contexts: The overhead of deep memory accesses can be reduced by using asynchronous operations overlapped with computations. Substantial work has been done on compiler analysis for overlapping memory accesses with computations, mostly in the context of reducing overheads of cache stalls <ref> [8, 23] </ref>. Several classes of applications involve large and frequent accesses to data stored in the secondary memory. The need for reading or writing data in secondary memory can arise for fl This work was supported by NSF under grant No. ASC 9213821, by ONR under contract No. <p> We also mention limitations of our current approach and the future directions we plan to take. Several researchers have developed compiler techniques for overlapping cache stalls with computation (also known as software prefetching). This includes the work of Mowry et al. as part of the SUIF system <ref> [23] </ref> and Callahan et al. at Rice University [8]. The amount of time required for cache misses is usually of the order of 10 cycles; much smaller than the disk latency, which of the order of 100,000 cycles.
Reference: [24] <author> E. Myers. </author> <title> A precise interprocedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 219-230, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Call Graph is a very concise representation and no information is available in a call graph about flow of control between different call sites within a single procedure. On the other extreme, Myer's SuperGraph <ref> [24] </ref> is a very detailed representation. SuperGraph is constructed by linking control flow graphs of procedures by inserting edges from call site in the caller to start node in callee.
Reference: [25] <author> R. Nance, R. Wilmoth, B. Moon, H. Hassan, and J. Saltz. </author> <title> Parallel DSMC solution of three-dimensional flow over a finite flat plate. </title> <booktitle> In Proceedings of the 6th AIAA/ASME Joint Thermophysics and Heat Transfer Conference, </booktitle> <address> Colorado Springs, CO, </address> <month> June </month> <year> 1994. </year>
Reference: [26] <author> M. Paleczny, K. Kennedy, and C. Koelbel. </author> <title> Compiler support for out-of-core arrays on parallel machines. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 110-118. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> February </month> <year> 1995. </year>
Reference-contexts: In the applications which checkpoint or snapshot the progress of the computation, the programmer may explicitly insert synchronous write operations. In the out-of-core applications, an initial phase of the compilation may determine when disk operations need to be made and correspondingly may insert synchronous read or write operations <ref> [6, 26] </ref>. For our purpose, we consider original program text (or a compiler processed representation) which has explicit calls to read or write operations. We denote by Op, any such synchronous 3 operation, which the compiler may want to replace by a corresponding asynchronous or split--phase operation. <p> The PASSION compiler (based upon Syracuse F90D system) performs loop transformations for improving locality in out-of-core applications [29]. Similar optimizations have also been performed as part of the Fortran D compilation system's support for out-of-core applications <ref> [26] </ref>. Neither of these groups have proposed any general techniques for placement of asynchronous operations or any interprocedural optimizations. Hand-compilation experiments have been presented to show performance gains from using asynchronous I/O [6]. Significant amount of work has been done on runtime libraries for optimizing Parallel I/O.
Reference: [27] <author> G. Patnaik, K. Kailasnath, and E.S. Oran. </author> <title> Effect of gravity on flame instabilities in premixed gases. </title> <journal> AIAA Journal, </journal> <volume> 29(12) </volume> <pages> 2141-8, </pages> <month> Dec </month> <year> 1991. </year>
Reference-contexts: In the latter case, the state of the computation is periodically piped into an appropriate visualization tool. Examples of applications that require snapshoting include Direct Simulation Monte Carlo (DSMC) [5], simulation of a flame sweeping through a volume <ref> [27] </ref>, airplane wake simulations [19], etc. The amount of data generated per time-step is often large, e.g., if one million particle are simulated in a particular run of the three-dimensional dsmc-3d code, then each snapshot is 24 MB (3 double precision numbers per particle). Checkpointing.
Reference: [28] <author> Mendel Rosenblum, Ed Bugnion, Stephen Alan Herrod, Emmett Witchel, and Anoop Gupta. </author> <title> The impact of architectural trends on operating system performance. </title> <booktitle> In Proceedings of Symposium on Operating System Principles, </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: In our case, we need to look at computation across procedure boundaries to allow for significant overlap. Future architecture trends show that micro-processors will have large Level 2 (L2) caches, and the miss penalty for L2 caches will of the order of 500 cycles <ref> [28] </ref>. In such scenarios, it will be profitable to perform analysis across procedure boundaries to prefetch data into the L2 cache. We believe that our analysis can be extended to perform prefetches for L2 caches as well.
Reference: [29] <author> Rajeev Thakur, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Compilation of out-of-core data parallel programs for distributed memory machines. </title> <booktitle> In Proceedings of the IPPS'94 Second Annual Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 54-72, </pages> <month> April </month> <year> 1994. </year> <journal> Also appears in ACM Computer Architecture News, </journal> <volume> Vol. 22, No. 4, </volume> <month> September </month> <year> 1994. </year>
Reference-contexts: FIAT has been proposed as a general framework for interprocedural analysis [15], but largely targets flow-insensitive problems. Compiler optimizations for improving I/O accesses have been addressed by at least two projects. The PASSION compiler (based upon Syracuse F90D system) performs loop transformations for improving locality in out-of-core applications <ref> [29] </ref>. Similar optimizations have also been performed as part of the Fortran D compilation system's support for out-of-core applications [26]. Neither of these groups have proposed any general techniques for placement of asynchronous operations or any interprocedural optimizations.
Reference: [30] <author> R.G. Wilmoth. </author> <title> Application of a parallel direct simulation monte carlo method to hypersonic rarefied flows. </title> <journal> AIAA Journal, </journal> <volume> 30(10) </volume> <pages> 2447-52, </pages> <month> Oct </month> <year> 1992. </year>
Reference: [31] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: The cost of data access increases rapidly with the depth of access. Achieving and sustaining good performance in presence of deep memory hierarchies is a very important problem and has received significant attention in the last few years. Several research projects have worked on code transformations to improve locality <ref> [6, 31, 32] </ref>. Most of the programs still spend considerable amount of their time in accessing data from memory at a deeper level in the hierarchy. The overhead of deep memory accesses can be reduced by using asynchronous operations overlapped with computations.
Reference: [32] <author> Michael Wolfe. </author> <title> Data dependence and program restructuring. </title> <journal> Journal of Supercomputing, </journal> <volume> 4(4) </volume> <pages> 321-344, </pages> <month> January </month> <year> 1991. </year> <month> 25 </month>
Reference-contexts: The cost of data access increases rapidly with the depth of access. Achieving and sustaining good performance in presence of deep memory hierarchies is a very important problem and has received significant attention in the last few years. Several research projects have worked on code transformations to improve locality <ref> [6, 31, 32] </ref>. Most of the programs still spend considerable amount of their time in accessing data from memory at a deeper level in the hierarchy. The overhead of deep memory accesses can be reduced by using asynchronous operations overlapped with computations.
References-found: 32

