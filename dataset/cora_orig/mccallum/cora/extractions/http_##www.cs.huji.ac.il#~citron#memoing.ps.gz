URL: http://www.cs.huji.ac.il/~citron/memoing.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~citron/index.html
Root-URL: http://www.cs.huji.ac.il
Title: Accelerating Multi-Media processing by Implementing Memoing in Multiplication  
Keyword: Division Units Keywords: memoing, lookup, multiplication, division, multi-media.  
Note: and  
Abstract: This paper introduces a technique that enables performing multi-cycle (multiplication, division, square-root ...) computations in a single cycle. The technique is based on the notion of memoing, saving the input and output of previous calculations and using the output if the input is encountered again. This technique is especially suitable for Multi-Media (MM) processing, rather than other types of applications. In MM applications the local entropy of the data tends to be low which results in repeated operations on the same datum. The inputs and outputs of assembly level operations are stored in cache-like lookup tables and accessed in parallel to the conventional computation. The scales of circuit integration nowadays enable us to build these lookup tables without putting a strain on avaliable chip area. A successful lookup gives the result of a multi-cycle computation in a single cycle, and a failed lookup doesn't necessitate a penalty in computation time. Results of simulations have shown that 43% of the floating-point multiplications and 50% of the floating point divisions, in Multi-Media applications, can be replicated without performing the calculations themselves, leading to an average speedup of up to 22%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> http://www.sgi.com/MIPS/products/r10k [2] http://www.mot.com/SPS/PowerPC/products [3] http://www.digital.com/info </institution>
Reference-contexts: Instructions that usually complete in multiple cycles (multiplication, division, square-root ...) can be made to complete in a single cycle when certain conditions are met. Current VLSI technology has made it possible to integrate microprocessors with superscalar capabilities of four instruction issues per cycle <ref> [1, 2, 3] </ref> and microprocessors with super-pipelining that have 9 stage deep pipelines [3]. But even with all the features mentioned above there are still instructions that take more than one cycle to complete. These are the mathematical functions of multiplication, division, root taking and other trigonometric and logarithmic functions. <p> Most of these instructions are computed using iterative algorithms [4] that by their nature are time consuming. Table 1 show the cycle times for floating point multiplication and division on several processors (Pentium Pro [5], Alpha 21164 [3], R10000 <ref> [1] </ref>, PPC 604e [2], UltraSparc-II [6] and PA8000 [7]). It is important to note that these numbers are the latencies for the given instructions. In all the above processors the multiplication unit is pipelined itself, which leads to a throughput of one cycle for consecutive multiplication instructions. <p> A 8K cache with a line size of 32 bytes contains 256 entries. This is well higher than our 16-32 entry memo-table and as mentioned in section 1 first level on-chip caches are reaching sizes of up to 64K <ref> [1, 2] </ref>. So addressing an entry should take less time than in a conventional cache. But the size of a tag in a cache is smaller than the size of a memo-table tag.
Reference: [4] <author> Hennessy J. L. and Patterson D. A., </author> <title> "Computer Architecture: A Quantitative Approach," </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo CA, </address> <year> 1990. </year> <note> [5] http://www.intel.com/design/ [6] http://www.sun.com/microelectronics/datasheets [7] http://www.hp.com/wsg/strategies </note>
Reference-contexts: But even with all the features mentioned above there are still instructions that take more than one cycle to complete. These are the mathematical functions of multiplication, division, root taking and other trigonometric and logarithmic functions. Most of these instructions are computed using iterative algorithms <ref> [4] </ref> that by their nature are time consuming. Table 1 show the cycle times for floating point multiplication and division on several processors (Pentium Pro [5], Alpha 21164 [3], R10000 [1], PPC 604e [2], UltraSparc-II [6] and PA8000 [7]). <p> 21164 MIPS R10000 PPC 604e UltraSparc-II PA8000 Multiplication 3 4 2 5 3 5 Division 39 31 40 31 22 31 Table 1: Cycle times of leading microprocessors These instructions can "throw a wrench" in the execution pipeline by introducing structural and data hazards and by resulting in out-of-order completion <ref> [4] </ref>. We propose to reduce the frequency of this problem by completing the above instructions in a single cycle, part of the time. The technique we use is called memoing [8]. <p> The conflicts are caused by the hashing scheme, in some applications (vsqrt, vcost, vgauss) nearly identical values are entered into the memo-table alternately causing a conflict miss on every lookup. Setting a set size of 2 solves this problem. 17 (LUT size is 32 entries). 3.3 Speedup Amdahl's law <ref> [4] </ref> tells us that the speedup obtained by using an enhancement depends on two factors: 1. The fraction of computation time in the original machine that can use the enhancement. This is called Fraction Enhanced (FE), it is always smaller than 1. 2.
Reference: [8] <author> Michie D., </author> <title> "Memo Functions and Machine Learning," </title> <booktitle> Nature 218, </booktitle> <pages> pp 19-22, </pages> <year> 1968. </year>
Reference-contexts: We propose to reduce the frequency of this problem by completing the above instructions in a single cycle, part of the time. The technique we use is called memoing <ref> [8] </ref>. The input and output of the instructions are stored in cache-like lookup tables and accessed in parallel to the conventional computation. A successful lookup gives the result of a multi-cycle computation in 3 a single cycle, and a failed lookup doesn't necessitate a penalty in computation time. <p> The next section will give a description of the memo-table. Section 3 will show the experiments performed and the results received in supporting this theory and section 4 will conclude and summarize this paper. 1.1 Related Work The concept of memoing was introduced by Michie <ref> [8] </ref>. The idea is to save the inputs and results of side-effect free functions in a table and reuse the results for matching inputs. Since then it has been used mainly in the context of declarative languages like Prolog and Lisp [9, 10].
Reference: [9] <author> L. Sterling and E. Shapiro, </author> <title> "The Art of Prolog, 2nd Ed.", </title> <publisher> MIT Press Cambridge MA, </publisher> <year> 1992. </year>
Reference-contexts: The idea is to save the inputs and results of side-effect free functions in a table and reuse the results for matching inputs. Since then it has been used mainly in the context of declarative languages like Prolog and Lisp <ref> [9, 10] </ref>. In computer arithmetic LUTs are used in division and square-root taking [11], for instance high-radix SRT [12] division uses a static table with predefined values to "guess" the next bits of the quotient (in fact this table caused the infamous "Pentium fdiv bug").
Reference: [10] <author> Abelson, H. and Sussman, G.J. </author> <title> Structure and Interpretation of Computer Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address> <year> 1985. </year>
Reference-contexts: The idea is to save the inputs and results of side-effect free functions in a table and reuse the results for matching inputs. Since then it has been used mainly in the context of declarative languages like Prolog and Lisp <ref> [9, 10] </ref>. In computer arithmetic LUTs are used in division and square-root taking [11], for instance high-radix SRT [12] division uses a static table with predefined values to "guess" the next bits of the quotient (in fact this table caused the infamous "Pentium fdiv bug").
Reference: [11] <author> P. Soderquist and M. Leeser, </author> <title> "An area/performance comparison of subtractive and multiplicative divide/square root implementations," </title> <booktitle> Proc. 12th IEEE Symp. Computer Arithmetic, </booktitle> <pages> pp. 132-139, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Since then it has been used mainly in the context of declarative languages like Prolog and Lisp [9, 10]. In computer arithmetic LUTs are used in division and square-root taking <ref> [11] </ref>, for instance high-radix SRT [12] division uses a static table with predefined values to "guess" the next bits of the quotient (in fact this table caused the infamous "Pentium fdiv bug").
Reference: [12] <author> Atkins, D.E. </author> <title> "Higher-radix division using estimates of the divisor and partial reminders," </title> <journal> IEEE Trans. on Computers C-17:10, 925-934,1968. </journal>
Reference-contexts: Since then it has been used mainly in the context of declarative languages like Prolog and Lisp [9, 10]. In computer arithmetic LUTs are used in division and square-root taking [11], for instance high-radix SRT <ref> [12] </ref> division uses a static table with predefined values to "guess" the next bits of the quotient (in fact this table caused the infamous "Pentium fdiv bug"). <p> And of course if several memo-table interfaces are to be implemented the design complexity rises to a point where the memo-table might have to be duplicated. But even in this case space can be saved as a memo-table is much smaller than a divider that incorporates the high-radix SRT <ref> [12] </ref> technique (section 2.4 expands this discussion). In this way it is possible for VLIW and super-scalar processors to increase their issue rate. 8 2.4 Cycle time & Die size The cycle time of a memo-table lookup is comparable to a cache lookup.
Reference: [13] <author> S. Richardson, </author> <title> "Exploiting Trivial and Redundant Computation", </title> <booktitle> Proc. of the 11th Symp. on Computer Arithmetic, </booktitle> <pages> pp. 220-227, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Our work isn't directly related to computer arithmetic in the sense that we don't propose any new algorithms or techniques for accelerating arithmetic computations. Our technique enables us to bypass the need to perform part of these computations altogether. The idea of exploiting redundant computation was introduced by Stevens <ref> [13] </ref> and further expanded by Flynn and Uberman [14]. Their simulations were performed on the SPEC, Perfect and NAS benchmarks [15]. Our tests showed us that looking at other families of applications would produce better results, the most likely candidate was Multi-Media with an emphasis on applications utilizing Image Processing.
Reference: [14] <author> S. Oberman, M. Flynn, </author> <title> "Reducing Division Latency with Reciprocal Caches", </title> <booktitle> Reliable Computing, </booktitle> <volume> Vol 2, no. 2, </volume> <pages> pages 147-153, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Our technique enables us to bypass the need to perform part of these computations altogether. The idea of exploiting redundant computation was introduced by Stevens [13] and further expanded by Flynn and Uberman <ref> [14] </ref>. Their simulations were performed on the SPEC, Perfect and NAS benchmarks [15]. Our tests showed us that looking at other families of applications would produce better results, the most likely candidate was Multi-Media with an emphasis on applications utilizing Image Processing.
Reference: [15] <author> Price W.J. </author> , <title> "A Benchmark Tutorial," </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 28-43, </pages> <month> October </month> <year> 1989. </year> <month> 23 </month>
Reference-contexts: Our technique enables us to bypass the need to perform part of these computations altogether. The idea of exploiting redundant computation was introduced by Stevens [13] and further expanded by Flynn and Uberman [14]. Their simulations were performed on the SPEC, Perfect and NAS benchmarks <ref> [15] </ref>. Our tests showed us that looking at other families of applications would produce better results, the most likely candidate was Multi-Media with an emphasis on applications utilizing Image Processing. <p> From floating point operands a 6 decimal place mantissa is composed and the least significant bits of this truncated mantissa are used. The system simulated consisted of memo-tables adjacent to the integer multiplier, fp multiplier and fp divider. The traces were taken from two sources. The Perfect Benchmarks <ref> [15] </ref> supply a suite of scientific applications. These applications are described in table 3. The Khoros development environment [19] supplies a suite of Image Processing (IP) and Digital Signal Processing (DSP) applications. These applications showed much higher hit ratios than the other applications, thus our targeting of Multi-Media applications. <p> A register instance is defined as each time a datum is written into a register. Reads to that register use that register instance. Franklin and Sohi show that for the SPEC <ref> [15] </ref> benchmarks a large number of register instances are used only once with the average use being about 2.
Reference: [16] <author> A. Sodani, G. Sohi, </author> <title> "Dynamic Instruction Reuse", </title> <booktitle> Proc. of the 24th Int. Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Our tests showed us that looking at other families of applications would produce better results, the most likely candidate was Multi-Media with an emphasis on applications utilizing Image Processing. Sodani and Sohi have introduced the concept of Dynamic Instruction Reuse <ref> [16] </ref>, where all instructions executed are inserted into a table called the Reuse Buffer (RB).
Reference: [17] <author> Cmelik R. and Keppel D., Shade: </author> <title> A Fast Instruction-Set Simulator for Execution Profiling, </title> <institution> Sun Microsystems Laboratories. </institution>
Reference-contexts: only 1066 of them are used), each entry can be any of 5 values (-2 - 2) so the lookup table alone takes up one KB. 3 Experiments and Results To verify the usefulness of the memo-table technique, we performed a series of experiments with an architecturally detailed simulator: Shade <ref> [17] </ref> a SPARC [18] (versions 8 & 9) instruction level simulator. Shade receives as input a binary executable and executes it natively on a SPARC compatible processor. Statistics are collected by breaking on specific instructions and storing register values in software simulated memo-tables.
Reference: [18] <author> R. Garner et al., </author> <title> "Scalable processor architecture (SPARC)," </title> <journal> COMP-CON, IEEE, </journal> <pages> pp 278-283, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: them are used), each entry can be any of 5 values (-2 - 2) so the lookup table alone takes up one KB. 3 Experiments and Results To verify the usefulness of the memo-table technique, we performed a series of experiments with an architecturally detailed simulator: Shade [17] a SPARC <ref> [18] </ref> (versions 8 & 9) instruction level simulator. Shade receives as input a binary executable and executes it natively on a SPARC compatible processor. Statistics are collected by breaking on specific instructions and storing register values in software simulated memo-tables.
Reference: [19] <author> D. Argiro and C. </author> <title> Gage, "Khoros User's Manual," </title> <editor> U. </editor> <address> of New Mexico, </address> <year> 1991. </year>
Reference-contexts: The system simulated consisted of memo-tables adjacent to the integer multiplier, fp multiplier and fp divider. The traces were taken from two sources. The Perfect Benchmarks [15] supply a suite of scientific applications. These applications are described in table 3. The Khoros development environment <ref> [19] </ref> supplies a suite of Image Processing (IP) and Digital Signal Processing (DSP) applications. These applications showed much higher hit ratios than the other applications, thus our targeting of Multi-Media applications. The specific applications are described in table 2.
Reference: [20] <author> M. Franklin and G.Sohi, </author> <title> "Register Traffic Analysis for Streamlining Inter-Operation Communication in Fine-Grain Parallel Processors," </title> <booktitle> Proc. of Micro 25, </booktitle> <pages> pp 236-245, </pages> <year> 1992. </year>
Reference-contexts: Both suites show a large potential for data reuse but only the MM suite can scale down to a size and associativity that are practical. The low hit ratios on the Perfect Club suite can be explained by the work of Franklin and Sohi <ref> [20] </ref>. A register instance is defined as each time a datum is written into a register. Reads to that register use that register instance.
Reference: [21] <author> T. Yeh and Y. Patt, </author> <title> "A Comparison of Dynamic Branch Predictors that Use Two Levels of Branch History," </title> <booktitle> Proc. of the 20th Int. Symp. on Computer Architecture, </booktitle> <pages> pp 191-201, </pages> <year> 1993. </year>
Reference-contexts: The table shows an average speedup of between 8% to 22%. Even taken at its face value a 8% speedup is comparable with the speedups attained by enhancing branch prediction and cache & TLB hit ratios <ref> [21, 22, 23] </ref>. 4 Conclusions In this paper we introduced a technique to reduce the average CPI of multiplication and division instructions. By using the concept of memoing we stored previous computations in look up tables we called memo-tables, and accessed them in parallel to computing an operation.
Reference: [22] <author> N. Joupi, </author> <title> "Cache Write Policies and Performances," </title> <booktitle> Proc. of the 20th Int. Symp. on Computer Architecture, </booktitle> <pages> pp 191-201, </pages> <year> 1993. </year>
Reference-contexts: The table shows an average speedup of between 8% to 22%. Even taken at its face value a 8% speedup is comparable with the speedups attained by enhancing branch prediction and cache & TLB hit ratios <ref> [21, 22, 23] </ref>. 4 Conclusions In this paper we introduced a technique to reduce the average CPI of multiplication and division instructions. By using the concept of memoing we stored previous computations in look up tables we called memo-tables, and accessed them in parallel to computing an operation.
Reference: [23] <author> J. Chen, A. Borg, N. Jouppi, </author> <title> "A Simulation Based Study of TLB Performance," </title> <booktitle> Proc. of the 18th Int. Symp. on Computer Architecture, </booktitle> <pages> pp 114-123, </pages> <year> 1991. </year>
Reference-contexts: The table shows an average speedup of between 8% to 22%. Even taken at its face value a 8% speedup is comparable with the speedups attained by enhancing branch prediction and cache & TLB hit ratios <ref> [21, 22, 23] </ref>. 4 Conclusions In this paper we introduced a technique to reduce the average CPI of multiplication and division instructions. By using the concept of memoing we stored previous computations in look up tables we called memo-tables, and accessed them in parallel to computing an operation.
References-found: 18

