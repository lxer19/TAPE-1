URL: ftp://ftp.cs.columbia.edu/pub/CAVE/papers/nayar/nayar-nene-murase-robot_ijra-96.ps.gz
Refering-URL: http://www.cs.columbia.edu/CAVE/video-demos.html
Root-URL: http://www.cs.columbia.edu
Title: Subspace Methods for Robot Vision  
Author: Shree K. Nayar, Sameer A. Nene, Hiroshi Murase 
Date: February, 1995  
Address: New York, N.Y. 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: 1 Submitted to the special issue of the IEEE Transactions on Robotics and Automation on Vision-Based Control of Robot Manipulators. This research was conducted at the Center for Research in Intelligent Systems, Department of Computer Science, Columbia University. It was supported in part by the David and Lucile Packard Fellowship and in part by ARPA Contract No. DACA 76-92-C-0007. Hiroshi Murase is with the NTT Basic Research Lab., Atsugi, Japan. 
Abstract-found: 1
Intro-found: 1
Reference: [Albus 75] <author> J. S. Albus, </author> <title> "A new approach to manipulator control: The cerebellar model," </title> <journal> Transactions of ASME, Journal of Dynamic Systems Measurement and Control, </journal> <volume> Vol. 97, </volume> <pages> pp. 220-227, </pages> <month> Sept. </month> <year> 1975. </year>
Reference-contexts: These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks [Kuperstien 87] [Mel 87] [Miller 89] [Walter et al. 90] to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) <ref> [Albus 75] </ref> [Miller 87].
Reference: [Allen et al. 92] <author> P. K. Allen, A. Timcenko, B. Yoshimi, and P. Michelman, </author> <title> "Trajectory Filtering and Prediction for Automated Tracking and Grasping of a Moving Object," </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <address> Nice, </address> <year> 1992, </year> <pages> pp. 1850-1856. </pages>
Reference-contexts: from geometric primitives such as edges, lines, vertices, and circles [Weiss et al. 87] 1 [Feddema et al. 91], [Koivo and Houshangi 91] [Hashimoto et al. 91] to optical flow esti-mates [Papanikolopoulos et al. 91] [Luo et al. 88] [Castano and Hutchinson 92] and object location estimates obtained using stereo <ref> [Allen et al. 92] </ref>. The control schemes used to drive the robot to its desired position vary from simple prediction algorithms employed to achieve computational efficiency to more sophisticated adaptive self-tuning controllers that account for the dynamics of the manipulator.
Reference: [Binford 87] <author> T. O. Binford, </author> <title> "Generalized Cylinder Representation," </title> <booktitle> Encyclopedia of Artificial Intelligence, </booktitle> <editor> S. C. Sahpiro, Ed., </editor> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <pages> pp. 321-323, </pages> <year> 1987. </year>
Reference-contexts: In most inspection applications, there exist a class of model objects that together define a range of permissible appearances. This would correspond to a closely packed bunch of curves in eigenspace. In such cases, a representation such as the generalized cylinder <ref> [Binford 87] </ref> may be used to compactly represent the tube in eigenspace that describes the class of permissible appearances.
Reference: [Castano and Hutchinson 92] <author> A. Castano and S. Hutchinson, </author> <title> "Hybrid Vision/Position Servo Control of a Robotic Manipulator," </title> <booktitle> Proc. of IEEE Intl. Conf. on Robotics and Automation, </booktitle> <pages> pp. 1264-1269, </pages> <address> Nice, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Image features used vary from geometric primitives such as edges, lines, vertices, and circles [Weiss et al. 87] 1 [Feddema et al. 91], [Koivo and Houshangi 91] [Hashimoto et al. 91] to optical flow esti-mates [Papanikolopoulos et al. 91] [Luo et al. 88] <ref> [Castano and Hutchinson 92] </ref> and object location estimates obtained using stereo [Allen et al. 92].
Reference: [Chin and Dyer 86] <author> R. T. Chin and C. R. Dyer, </author> <title> "Model-Based Recognition in Robot Vision," </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 18, No. 1, </volume> <pages> pp. 67-108, </pages> <year> 1986. </year> <month> 25 </month>
Reference-contexts: In order for the robot to interact with objects in its workspace, it requires a-priori models of the objects. Traditionally, robot vision systems have heavily relied on shape (CAD) models <ref> [Chin and Dyer 86] </ref>. Will shape representation suffice? After all, most vision applications deal with brightness images that are functions not only of shape but also other intrinsic scene properties such as reflectance and perpetually varying factors such as illumination.
Reference: [Feddema et al. 91] <author> J. Feddema, C.S.G. Lee, and O. Mitchell, </author> <title> "Weighted selection of image features for resolved rate visual feedback control," </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> Vol. 7, No. 1, </volume> <pages> pp. 31-47, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: The objective is to find the rotation and translation that must be applied to the end-effector to bring the features back to their desired positions in the image. Image features used vary from geometric primitives such as edges, lines, vertices, and circles [Weiss et al. 87] 1 <ref> [Feddema et al. 91] </ref>, [Koivo and Houshangi 91] [Hashimoto et al. 91] to optical flow esti-mates [Papanikolopoulos et al. 91] [Luo et al. 88] [Castano and Hutchinson 92] and object location estimates obtained using stereo [Allen et al. 92].
Reference: [Fukunaga 90] <author> K. Fukunaga, </author> <title> Introduction to Statistical Pattern Recognition, </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: Our appearance based approach to robot vision offers a solution to servoing that differs from previous work in two significant ways; (a) the method uses raw brightness images directly without the computation of image features, and (b) the learning algorithm is based on principal component analysis [Oja 83] <ref> [Fukunaga 90] </ref> rather than a large input/output mapping network. During the learning stage, a sizable image window is selected that represents the appearance of the object when the robot is in the desired position. <p> The obvious step is to take advantage of this redundancy and compress the large set to a low-dimensional representation that captures the key appearance characteristics of the visual workspace. A suitable compression technique is based on principal component analysis [Oja 83] <ref> [Fukunaga 90] </ref>, where the eigenvectors of the image set are computed and used as orthogonal bases for representing individual images.
Reference: [Hashimoto et al. 91] <author> K. Hashimoto, T. Kimoto, and H. Kimura, </author> <title> "Manipulator Control with Image-Based Visual Servo," </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <year> 1991, </year> <pages> pp. 2267-2271. </pages>
Reference-contexts: Image features used vary from geometric primitives such as edges, lines, vertices, and circles [Weiss et al. 87] 1 [Feddema et al. 91], [Koivo and Houshangi 91] <ref> [Hashimoto et al. 91] </ref> to optical flow esti-mates [Papanikolopoulos et al. 91] [Luo et al. 88] [Castano and Hutchinson 92] and object location estimates obtained using stereo [Allen et al. 92].
Reference: [Householder 64] <author> A. S. </author> <title> Householder, The theory of matrices in numerical analysis, </title> <publisher> Dover Publications, </publisher> <address> New York, </address> <year> 1964. </year>
Reference-contexts: The manifold generation module can be used for projecting image (or feature) sets to subspaces, B-spline interpolation [Rogers 90] of subspace projections to produce multivariate manifolds, dense resampling of manifolds, and orthogonalization <ref> [Householder 64] </ref> of multiple subspaces. Finally, the recognition module includes efficient search implementations [Nene and Nayar 95] that find manifold points which lie closest to novel input projections. All four modules can be accessed via an intuitive graphical interface built on X/Motif.
Reference: [Hummel 79] <author> R. A. Hummel, </author> <title> "Feature Detection Using Basis Functions," </title> <journal> Computer Graphics and Image Processing, </journal> <volume> Vol. 9, </volume> <pages> pp. 40-55, </pages> <year> 1979. </year>
Reference-contexts: A suitable compression technique is based on principal component analysis [Oja 83] [Fukunaga 90], where the eigenvectors of the image set are computed and used as orthogonal bases for representing individual images. Principal component analysis has been previously used in computer vision for deriving basis functions for feature detection <ref> [Hummel 79] </ref> [Lenz 87], representing human face images [Sirovich and Kirby 87], and recognizing face images [Turk and Pentland 91] [Pentland et al. 94].
Reference: [Koivo and Houshangi 91] <author> A. Koivo and N. Houshangi, </author> <title> "Real-time vision feedback for ser-voing robotics manipulator with self-tuning controller," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol. 21, No. 1, </volume> <pages> pp. 134-142, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: Image features used vary from geometric primitives such as edges, lines, vertices, and circles [Weiss et al. 87] 1 [Feddema et al. 91], <ref> [Koivo and Houshangi 91] </ref> [Hashimoto et al. 91] to optical flow esti-mates [Papanikolopoulos et al. 91] [Luo et al. 88] [Castano and Hutchinson 92] and object location estimates obtained using stereo [Allen et al. 92].
Reference: [Kuperstien 87] <author> M. Kuperstien, </author> <title> "Adaptive visual-motor coordination in multijoint robots using parallel architecture," </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <address> Raleigh, N.C., </address> <year> 1987, </year> <pages> pp. 1595-1602. </pages>
Reference-contexts: In addition, calibration of the vision sensor is not required as long as the sensor-robot configuration remains unaltered between learning and servoing. These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks <ref> [Kuperstien 87] </ref> [Mel 87] [Miller 89] [Walter et al. 90] to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) [Albus 75] [Miller 87].
Reference: [Lenz 87] <author> R. Lenz, </author> <title> "Optimal Filters for the Detection of Linear Patterns in 2-D and Higher Dimensional Images," </title> <journal> Pattern Recognition, </journal> <volume> Vol. 20, No. 2, </volume> <pages> pp. 163-172, </pages> <year> 1987. </year>
Reference-contexts: Principal component analysis has been previously used in computer vision for deriving basis functions for feature detection [Hummel 79] <ref> [Lenz 87] </ref>, representing human face images [Sirovich and Kirby 87], and recognizing face images [Turk and Pentland 91] [Pentland et al. 94]. Though, in general, all the eigenvectors of an image set are needed for perfect reconstruction of any particular image, only a few are sufficient for visual recognition.
Reference: [Luo et al. 88] <author> R. Luo, R. Mullen, and D. Wessel, </author> <title> "An Adaptive Robotic Tracking System using Optical Flow," </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <year> 1988, </year> <pages> pp. 568-573. </pages>
Reference-contexts: Image features used vary from geometric primitives such as edges, lines, vertices, and circles [Weiss et al. 87] 1 [Feddema et al. 91], [Koivo and Houshangi 91] [Hashimoto et al. 91] to optical flow esti-mates [Papanikolopoulos et al. 91] <ref> [Luo et al. 88] </ref> [Castano and Hutchinson 92] and object location estimates obtained using stereo [Allen et al. 92].
Reference: [Mel 87] <author> B. W. Mel, "MURPHY: </author> <title> A robot that learns by doing," </title> <booktitle> AIP Proceedings of Neural Information Processing System Conference, </booktitle> <address> Denver, CO, </address> <year> 1987. </year>
Reference-contexts: In addition, calibration of the vision sensor is not required as long as the sensor-robot configuration remains unaltered between learning and servoing. These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks [Kuperstien 87] <ref> [Mel 87] </ref> [Miller 89] [Walter et al. 90] to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) [Albus 75] [Miller 87].
Reference: [Mersch 84] <author> S. Mersch, </author> <title> "Polarized Lighting for Machine Vision Applications," </title> <booktitle> Proc. of RI/SME Third Annual Applied Machine Vision Conf., </booktitle> <pages> pp. 40-54, Schaumburg, </pages> <month> Feb. </month> <year> 1984. </year>
Reference-contexts: Diffuse reflections, on the other hand, tends to be unpolarized even under polarized illumination and hence are allowed to pass through to the sensor. The result is an image that is more or less devoid of specularities. This trick has been widely employed in visual inspection systems <ref> [Mersch 84] </ref>. 4 large image window (white box) of fixed size and position. (b) Alternatively, the contents of several windows of fixed sizes and shapes, scattered in the image, can be concatenated and treated as a single image vector.
Reference: [Miller 87] <author> W. T. Miller, </author> <title> "Sensor-based control of robotic manipulators using a general learning algorithm," </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> Vol. RA-3, No. 2, </volume> <pages> pp. 157-165, </pages> <month> April, </month> <year> 1987. </year>
Reference-contexts: These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks [Kuperstien 87] [Mel 87] [Miller 89] [Walter et al. 90] to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) [Albus 75] <ref> [Miller 87] </ref>.
Reference: [Miller 89] <author> W. T. Miller, </author> <title> "Real-time application of neural networks for sensor-based control of robots with vision," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol. 19, No. 4, </volume> <pages> pp. 825-831, </pages> <address> July/August, </address> <year> 1989. </year> <month> 26 </month>
Reference-contexts: In addition, calibration of the vision sensor is not required as long as the sensor-robot configuration remains unaltered between learning and servoing. These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks [Kuperstien 87] [Mel 87] <ref> [Miller 89] </ref> [Walter et al. 90] to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) [Albus 75] [Miller 87].
Reference: [Mukherjee and Nayar 95] <author> S. Mukherjee and S. K. Nayar, </author> <title> "Optimal RBF Networks for Visual Learning," </title> <booktitle> Proc. of International Conference on Computer Vision, to appear, </booktitle> <address> Boston, </address> <month> June, </month> <year> 1995. </year> <note> Also Tech. Rep. CUCS-01-95. </note>
Reference-contexts: In [Nene and Nayar 95], code for the above algorithm is given and the algorithm is demonstrated to be easier to implement than most existing algorithms with similar complexity. The second approach <ref> [Mukherjee and Nayar 95] </ref> uses three-layered radial basis function (RBF) networks [Poggio and Girosi 90] to learn the mapping between input points and task parameters. The complexity of the network approach depends on the number of networks used and their sizes. In [Mukherjee and Nayar 95] a novel framework is introduced <p> The second approach <ref> [Mukherjee and Nayar 95] </ref> uses three-layered radial basis function (RBF) networks [Poggio and Girosi 90] to learn the mapping between input points and task parameters. The complexity of the network approach depends on the number of networks used and their sizes. In [Mukherjee and Nayar 95] a novel framework is introduced that uses the wavelet integral transform for finding the smallest RBF network to accomplish any given input-output mapping. The performance of the network based scheme is generally comparable to that of the binary search approach.
Reference: [Murakami and Kumar 82] <author> H. Murakami and V. Kumar, </author> <title> "Efficient Calculation of Primary Images from a Set of Images," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 4, No. 5, </volume> <pages> pp. 511-515, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: We have summarized a few of the representative algorithms in [Murase and Nayar 95]. In our experiments we have used a fast implementation [Nene et al. 94] of the algorithm proposed by Murakami and Kumar <ref> [Murakami and Kumar 82] </ref>. On a Sun IPX workstation, for instance, 20 eigenvectors of a set of 100 images (each 128x128 in size) can be computed in about 3 minutes, and 20 eigenvectors of a 1000 image set in less than 4 hours. <p> Image manipulation includes image segmentation, scale and brightness normalization, image-vector conversions, and provides tools for maintaining large image databases. Subspace computation, the second module, computes eigenvectors and eigenvalues of large image sets using the method outlined in <ref> [Murakami and Kumar 82] </ref>. The manifold generation module can be used for projecting image (or feature) sets to subspaces, B-spline interpolation [Rogers 90] of subspace projections to produce multivariate manifolds, dense resampling of manifolds, and orthogonalization [Householder 64] of multiple subspaces.
Reference: [Murase and Nayar 93] <author> H. Murase and S. K. Nayar, </author> <title> "Learning and Recognition of 3D Objects from Appearance," </title> <booktitle> Proc. of IEEE Workshop on Qualitative Vision, </booktitle> <pages> pp. 39-50, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The exact number of task DOF is of course application dependent. The above representation is called the parametric eigenspace. It was initially introduced in <ref> [Murase and Nayar 93] </ref> [Murase and Nayar 95] for the real-time recognition and pose estimation of 3D objects and was later used in [Murase and Nayar 94] for finding optimal illumination for robust object recognition.
Reference: [Murase and Nayar 95] <author> H. Murase and S. K. Nayar, </author> <title> "Visual Learning and Recognition of 3D Objects from Appearance," </title> <journal> International Journal of Comp. Vision, </journal> <volume> Vol. 14, No. 1, </volume> <pages> pp. 5-24, </pages> <year> 1995. </year>
Reference-contexts: This observation has motivated us to take an extreme approach to visual representation. What we seek is not a representation of geometry but rather appearance <ref> [Murase and Nayar 95] </ref>, encoded in which are brightness variations caused by three-dimensional shape, surface reflectance properties, illumination conditions, and the parameters of the robot task. Given the number of factors at work, it is immediate that an appearance representation that captures all possible variations is simply impractical. <p> Simple normalizations can be applied to the input vector i to enhance the robustness of visual processing. For instance, in a recognition task an object region is segmented from the scene and scale normalized <ref> [Murase and Nayar 95] </ref> to fit a predetermined image size. This ensures that the recognition system is invariant to magnification, i.e. the distance of the object from the image sensor. <p> Fast algorithms for solving this problem have been a topic of active research in the area of image coding/compression and pattern recognition. We have summarized a few of the representative algorithms in <ref> [Murase and Nayar 95] </ref>. In our experiments we have used a fast implementation [Nene et al. 94] of the algorithm proposed by Murakami and Kumar [Murakami and Kumar 82]. <p> The exact number of task DOF is of course application dependent. The above representation is called the parametric eigenspace. It was initially introduced in [Murase and Nayar 93] <ref> [Murase and Nayar 95] </ref> for the real-time recognition and pose estimation of 3D objects and was later used in [Murase and Nayar 94] for finding optimal illumination for robust object recognition. <p> Consider two images ^ i r and ^ i s that belong to the image set used to compute an eigenspace. Let points f r and f s be eigenspace projections of the two images. It is well-known in pattern recognition theory [Oja 83] <ref> [Murase and Nayar 95] </ref> that the distance between the two points in eigenspace is an approximation to the correlation between the two images: jj ^ i r ^ i s jj 2 jj f r f s jj 2 (9) The closer the projections, the more similar are the images in <p> The algorithm we have developed for efficient binary search through multiple dimensions [Nene and Nayar 95] has kept the task of finding the closest manifold point within real-time performance. For instance, in <ref> [Murase and Nayar 95] </ref> a 20-object recognition system was presented that identifies complex objects and computes their poses in a few hundred milliseconds. 22 * Fine Tuning Positioning: In all of our experiments, we have sampled the visual workspace in a uniform manner. <p> However, for most applications that involve more that three variables, the process of sampling the visual workspace tends to be impractical since the samples increase exponentially with parameters. Furthermore, even if the workspace samples can be acquired, their storage and manipulation during eigenvector computation becomes more cumbersome and time-consuming <ref> [Murase and Nayar 95] </ref>. This said, it is well worth noting that machines are fast improving in performance and appearance matching for large vision problem may indeed prove practical in due course of time.
Reference: [Murase and Nayar 94] <author> H. Murase and S. K. Nayar, </author> <title> "Illumination Planning for Object Recognition in Structured Environments," </title> <booktitle> Proc. of IEEE Intl. Conf. on Robotics and and Automation, Seattle, </booktitle> <pages> pp. 31-38, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: The exact number of task DOF is of course application dependent. The above representation is called the parametric eigenspace. It was initially introduced in [Murase and Nayar 93] [Murase and Nayar 95] for the real-time recognition and pose estimation of 3D objects and was later used in <ref> [Murase and Nayar 94] </ref> for finding optimal illumination for robust object recognition. While almost all previous work in 3D vision relies on geometric (CAD) models of objects, the parametric eigenspace serves as the basis for developing new appearance based learning and recognition schemes.
Reference: [Nayar et al. 94] <author> S. K. Nayar, H. Murase, and S. A. Nene, </author> <title> "Learning, Positioning, and Tracking Visual Appearance," </title> <booktitle> Proc. of IEEE Intl. Conf. on Robotics and Automation, </booktitle> <address> San Diego, </address> <month> May </month> <year> 1994. </year>
Reference: [Nayar et al. 95] <author> S. K. Nayar, X. Fang, and T. E. Boult, </author> <title> "Separation of Reflection Components Using Color and Polarization," </title> <journal> International Journal of Computer Vision, </journal> <note> in press, 1995. Also Tech. Rep. CUCS-058-92. </note>
Reference-contexts: This causes the illumination of the scene to be linearly polarized. Since specular reflections tend to preserve the polarization characteristics of the incident light, they are blocked by the cross-polarized filter appended to the sensor <ref> [Nayar et al. 95] </ref>. Diffuse reflections, on the other hand, tends to be unpolarized even under polarized illumination and hence are allowed to pass through to the sensor. The result is an image that is more or less devoid of specularities.
Reference: [Nene et al. 94] <author> S. A. Nene, S. K. Nayar, H. Murase, </author> <title> "SLAM: A Software Library for Appearance Matching," </title> <booktitle> Proc. of ARPA IU Workshop, </booktitle> <address> Monterey, </address> <month> Nov. </month> <year> 1994. </year> <note> Also Tech. Rep. CUCS-019-94. </note>
Reference-contexts: Defects in the novel part are detected as large deviations from the model manifold. All of our results together demonstrate that the techniques underlying appearance modeling and matching are general. This has led to the development of a comprehensive software package <ref> [Nene et al. 94] </ref> for appearance matching that is briefly described. We conclude with a discussion on various issues related to appearance matching and its application to real-world problems. 2 Appearance Based Approach We begin with an overview of the subspace based approach. <p> Fast algorithms for solving this problem have been a topic of active research in the area of image coding/compression and pattern recognition. We have summarized a few of the representative algorithms in [Murase and Nayar 95]. In our experiments we have used a fast implementation <ref> [Nene et al. 94] </ref> of the algorithm proposed by Murakami and Kumar [Murakami and Kumar 82]. <p> In view of this, we have developed a software package named SLAM <ref> [Nene et al. 94] </ref> as a general tool for appearance modeling and recognition problems. The package is coded in C++ and uses advanced object-oriented programming techniques to achieve high space/time efficiency. It has four primary modules: image manipulation, subspace computation, manifold generation, and recognition.
Reference: [Nene and Nayar 95] <author> S. A. Nene and S. K. Nayar, </author> <title> "Binary Search Through Multiple Dimensions," </title> <type> Technical Report CUCS-018-94, </type> <institution> Dept. of Computer Science, Columbia Univ., </institution> <month> January, </month> <year> 1994, </year> <note> Revised February, </note> <year> 1995. </year>
Reference-contexts: The computational complexity is O ( K n ) where n is the number of manifold points and K is the dimensionality of the eigenspace. We have implemented two alternative schemes. The first is an efficient technique for binary search in multiple dimensions <ref> [Nene and Nayar 95] </ref>. This algorithm uses a carefully designed data structure to facilitate quick search through the multi-dimensional eigenspace in approximately O ( k log 2 n ). Figure 4 illustrates the use of the data structure. This data structure is created off-line as follows. <p> Next, using the forward and backward maps, the possible candidates are determined as those with all coordinates within * from those of the novel point. Exhaustive search on this short list of candidate points reveals the closest manifold point and the corresponding task parameters q c . In <ref> [Nene and Nayar 95] </ref>, code for the above algorithm is given and the algorithm is demonstrated to be easier to implement than most existing algorithms with similar complexity. <p> This high efficiency results from the simplicity of the matching procedure. The projection of visual inputs to eigenspace requires only dot products of the inputs with the dimensions of the eigenspace. The algorithm we have developed for efficient binary search through multiple dimensions <ref> [Nene and Nayar 95] </ref> has kept the task of finding the closest manifold point within real-time performance. <p> The manifold generation module can be used for projecting image (or feature) sets to subspaces, B-spline interpolation [Rogers 90] of subspace projections to produce multivariate manifolds, dense resampling of manifolds, and orthogonalization [Householder 64] of multiple subspaces. Finally, the recognition module includes efficient search implementations <ref> [Nene and Nayar 95] </ref> that find manifold points which lie closest to novel input projections. All four modules can be accessed via an intuitive graphical interface built on X/Motif.
Reference: [Oja 83] <author> E. Oja, </author> <title> Subspace methods of Pattern Recognition, </title> <publisher> Research Studies Press, </publisher> <address> Hert-fordshire, </address> <year> 1983. </year>
Reference-contexts: It is clear that vision systems of the future must be capable of acquiring object models without human assistance. It turns out that the appearance model proposed here is easier to acquire through an automatic learning phase than to create manually. Drawing on subspace methods <ref> [Oja 83] </ref>, we show that both acquisition of appearance models as well as recognition of novel appearances can be performed efficiently in a low-dimensional space. <p> Our appearance based approach to robot vision offers a solution to servoing that differs from previous work in two significant ways; (a) the method uses raw brightness images directly without the computation of image features, and (b) the learning algorithm is based on principal component analysis <ref> [Oja 83] </ref> [Fukunaga 90] rather than a large input/output mapping network. During the learning stage, a sizable image window is selected that represents the appearance of the object when the robot is in the desired position. <p> The obvious step is to take advantage of this redundancy and compress the large set to a low-dimensional representation that captures the key appearance characteristics of the visual workspace. A suitable compression technique is based on principal component analysis <ref> [Oja 83] </ref> [Fukunaga 90], where the eigenvectors of the image set are computed and used as orthogonal bases for representing individual images. <p> Note that each eigenvector is of size N , i.e. the size of an image. These K eigenvectors constitute our eigenspace; it is an approximation to a complete Hilbert space with N dimensions. Pattern recognition theory suggests several criteria for selecting K given the covariance matrix Q <ref> [Oja 83] </ref>. <p> The eigenspace representation possesses an important property. Consider two images ^ i r and ^ i s that belong to the image set used to compute an eigenspace. Let points f r and f s be eigenspace projections of the two images. It is well-known in pattern recognition theory <ref> [Oja 83] </ref> [Murase and Nayar 95] that the distance between the two points in eigenspace is an approximation to the correlation between the two images: jj ^ i r ^ i s jj 2 jj f r f s jj 2 (9) The closer the projections, the more similar are the
Reference: [Papanikolopoulos et al. 91] <author> N. Papanikolopoulos, P. Khosla, and T. Kanade, </author> <title> "Adaptive robotic visual tracking," </title> <booktitle> Proceedings of Automatic Control Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Image features used vary from geometric primitives such as edges, lines, vertices, and circles [Weiss et al. 87] 1 [Feddema et al. 91], [Koivo and Houshangi 91] [Hashimoto et al. 91] to optical flow esti-mates <ref> [Papanikolopoulos et al. 91] </ref> [Luo et al. 88] [Castano and Hutchinson 92] and object location estimates obtained using stereo [Allen et al. 92].
Reference: [Pentland et al. 94] <author> A. Pentland, B. Moghaddam, and T. Starner, </author> <title> "View-Based and Modular Eigenspaces for Face Recognition," </title> <booktitle> Proc. of IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Seattle, </address> <month> June </month> <year> 1994. </year> <month> 27 </month>
Reference-contexts: Principal component analysis has been previously used in computer vision for deriving basis functions for feature detection [Hummel 79] [Lenz 87], representing human face images [Sirovich and Kirby 87], and recognizing face images [Turk and Pentland 91] <ref> [Pentland et al. 94] </ref>. Though, in general, all the eigenvectors of an image set are needed for perfect reconstruction of any particular image, only a few are sufficient for visual recognition.
Reference: [Poggio and Girosi 90] <author> T. Poggio and F. Girosi, </author> <title> "Networks for Approximation and Learn--ing," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol. 78, No. 9, </volume> <pages> pp. 1481-1497, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: In [Nene and Nayar 95], code for the above algorithm is given and the algorithm is demonstrated to be easier to implement than most existing algorithms with similar complexity. The second approach [Mukherjee and Nayar 95] uses three-layered radial basis function (RBF) networks <ref> [Poggio and Girosi 90] </ref> to learn the mapping between input points and task parameters. The complexity of the network approach depends on the number of networks used and their sizes.
Reference: [Press et al. 88] <author> W. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling, </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference: [Rogers 90] <author> D. F. Rogers, </author> <title> Mathematical Elements for Computer Graphics, 2nd ed., </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Hence, the discrete points obtained by projecting all the samples of the workspace can be assumed to lie on a manifold that represents a continuous appearance function. The discrete points are interpolated to obtain this manifold. In our implementation, we have used a standard quadratic B-spline interpolation algorithm <ref> [Rogers 90] </ref>. <p> Subspace computation, the second module, computes eigenvectors and eigenvalues of large image sets using the method outlined in [Murakami and Kumar 82]. The manifold generation module can be used for projecting image (or feature) sets to subspaces, B-spline interpolation <ref> [Rogers 90] </ref> of subspace projections to produce multivariate manifolds, dense resampling of manifolds, and orthogonalization [Householder 64] of multiple subspaces. Finally, the recognition module includes efficient search implementations [Nene and Nayar 95] that find manifold points which lie closest to novel input projections.
Reference: [Sirovich and Kirby 87] <author> L. Sirovich and M. Kirby, </author> <title> "Low dimensional procedure for the characterization of human faces," </title> <journal> Journal of Optical Society of America, </journal> <volume> Vol. 4, No. 3, </volume> <pages> pp. 519-524, </pages> <year> 1987. </year>
Reference-contexts: Principal component analysis has been previously used in computer vision for deriving basis functions for feature detection [Hummel 79] [Lenz 87], representing human face images <ref> [Sirovich and Kirby 87] </ref>, and recognizing face images [Turk and Pentland 91] [Pentland et al. 94]. Though, in general, all the eigenvectors of an image set are needed for perfect reconstruction of any particular image, only a few are sufficient for visual recognition.
Reference: [Turk and Pentland 91] <author> M. A. Turk and A. P. Pentland, </author> <title> "Face Recognition Using Eigen-faces," </title> <booktitle> Proc. of IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 586-591, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Principal component analysis has been previously used in computer vision for deriving basis functions for feature detection [Hummel 79] [Lenz 87], representing human face images [Sirovich and Kirby 87], and recognizing face images <ref> [Turk and Pentland 91] </ref> [Pentland et al. 94]. Though, in general, all the eigenvectors of an image set are needed for perfect reconstruction of any particular image, only a few are sufficient for visual recognition.
Reference: [Walter et al. 90] <author> J. Walter, T. Martinez, and K. Schulten, </author> <title> "Industrial robot learns visuo-motor coordination by means of neural-gas network," </title> <booktitle> Proceedings of International Joint Conference of Neural Networks, </booktitle> <month> June, </month> <year> 1990. </year>
Reference-contexts: In addition, calibration of the vision sensor is not required as long as the sensor-robot configuration remains unaltered between learning and servoing. These methods differ from each other primarily in the type of learning algorithm used. The learning strategies vary from neural-like networks [Kuperstien 87] [Mel 87] [Miller 89] <ref> [Walter et al. 90] </ref> to table lookup mechanisms such as the cerebellar model articulation controller (CMAC) [Albus 75] [Miller 87].
Reference: [Weiss et al. 87] <author> L. Weiss, A. Sanderson, and C. Neuman, </author> <title> "Dynamic sensor-based control of robots with visual feedback," </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> Vol. RA-3, No. 5, </volume> <pages> pp. 404-417, </pages> <month> Oct. </month> <year> 1987. </year> <month> 28 </month>
Reference-contexts: The objective is to find the rotation and translation that must be applied to the end-effector to bring the features back to their desired positions in the image. Image features used vary from geometric primitives such as edges, lines, vertices, and circles <ref> [Weiss et al. 87] </ref> 1 [Feddema et al. 91], [Koivo and Houshangi 91] [Hashimoto et al. 91] to optical flow esti-mates [Papanikolopoulos et al. 91] [Luo et al. 88] [Castano and Hutchinson 92] and object location estimates obtained using stereo [Allen et al. 92].
References-found: 37

