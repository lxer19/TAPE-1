URL: ftp://ftp.cs.umass.edu/pub/mckinley/asplos.ps.gz
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Email: carr@cs.mtu.edu mckinley@cs.umass.edu tseng@cs.stanford.edu  
Title: Compiler Optimizations for Improving Data Locality  
Author: Steve Carr Kathryn S. M c Kinley Chau-Wen Tseng 
Affiliation: Department of Computer Science Department of Computer Science Computer Systems Laboratory Michigan Technological University University of Massachusetts Stanford University  
Abstract: In the past decade, processor speed has become significantly faster than memory speed. Small, fast cache memories are designed to overcome this discrepancy, but they are only effective when programs exhibit data locality. In this paper, we present compiler optimizations to improve data locality based on a simple yet accurate cost model. The model computes both temporal and spatial reuse of cache lines to find desirable loop organizations. The cost model drives the application of compound transformations consisting of loop permutation, loop fusion, loop distribution, and loop reversal. We demonstrate that these program transformations are useful for optimizing many programs. To validate our optimization strategy, we implemented our algorithms and ran experiments on a large collection of scientific programs and kernels. Experiments with kernels illustrate that our model and algorithm can select and achieve the best performance. For over thirty complete applications, we executed the original and transformed versions and simulated cache hit rates. We collected statistics about the inherent characteristics of these programs and our ability to improve their data locality. To our knowledge, these studies are the first of such breadth and depth. We found performance improvements were difficult to achieve because benchmark programs typically have high hit rates even for small data caches; however, our optimizations significantly improved several programs. 
Abstract-found: 1
Intro-found: 1
Reference: [AS79] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: Our algorithms never found an opportunity where loop reversal could improve locality. 2 Related Work Abu-Sufah first discussed applying compiler transformations based on data dependence (e.g., loop interchange, fusion, distribution, and tiling) to improve paging <ref> [AS79] </ref>. In this paper, we extend and validate recent research to integrate optimizations for parallelism and memory [KM92]. We extend their original cost model to capture more types of reuse. The only transformation they perform is loop permutation, whereas we integrate permutation, fusion, distribution, and reversal into a comprehensive approach.
Reference: [Car92] <author> S. Carr. </author> <title> Memory-Hierarchy Management. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: In this paper, we concentrate on the first step. Our algorithms are complementary to and in fact, improve the effectiveness of optimizations performed in the latter two steps <ref> [Car92] </ref>. However, the other steps and interactions between steps are beyond the scope of this paper. 1.2 Overview We present a compiler strategy based on an effective, yet simple, model for estimating the cost of executing a given loop nest in terms of the number of cache line references. <p> In this case, the improvements in cache performance far outweigh the potential loss in low-level parallelism when the recurrence is carried by the innermost loop. To regain any lost parallelism, unroll-and-jam can be applied to the outermost loop <ref> [CCK88, Car92] </ref>. Finally, it is important to note that the programmer was allowed to write the code in a form for one type of machine and still attain machine-independent performance through the use of compiler optimization.
Reference: [CCK88] <author> D. Callahan, J. Cocke, and K. Kennedy. </author> <title> Estimating interlock and improving balance for pipelined machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(4) </volume> <pages> 334-358, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In this case, the improvements in cache performance far outweigh the potential loss in low-level parallelism when the recurrence is carried by the innermost loop. To regain any lost parallelism, unroll-and-jam can be applied to the outermost loop <ref> [CCK88, Car92] </ref>. Finally, it is important to note that the programmer was allowed to write the code in a form for one type of machine and still attain machine-independent performance through the use of compiler optimization.
Reference: [CCK90] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIG-PLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Knowledge of the cache size, associativity, and replacement policy is essential. Higher degrees of tiling can be applied to exploit multi-level caches, the TLB, etc. 3. Promote register reuse through unroll-and-jam (also known as register tiling) and scalar replacement <ref> [CCK90] </ref>. The number and type of registers available are required to determine the degree of unroll-and-jam and the number of array references to replace with scalars. In this paper, we concentrate on the first step.
Reference: [CHH + 93] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKin-ley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference: [CHK93] <author> K. Cooper, M. W. Hall, and K. Kennedy. </author> <title> A methodology for procedure cloning. </title> <journal> Computer Languages, </journal> <volume> 19(2) </volume> <pages> 105-117, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Other coding styles may also inhibit optimization in our system. For example, Linpackd and Matrix300 are written in a modular style with singly nested loops enclosing function calls to routines which also contain singly nested loops. To improve programs written in this style requires interprocedural optimization <ref> [CHK93, HKM91] </ref>; these optimizations are not currently implemented in our translator. Many loop nests (69%) in the original programs are already in memory order, and even more (74%) have the loop carrying the most reuse in the innermost position.
Reference: [CK94] <author> S. Carr and K. Kennedy. </author> <title> Scalar replacement in the presence of conditional control flow. </title> <journal> SoftwarePractice and Experience, </journal> <volume> 24(1) </volume> <pages> 51-77, </pages> <month> January </month> <year> 1994. </year>
Reference: [CMT94] <author> S. Carr, K. S. M c Kinley, and C. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <type> Technical Report TR94-, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: The amount of group reuse is presented for each type of self reuse and their average (Avg). The LoopCost Ratio column estimates the potential improvement as an average (Avg) over all the nests and a weighted 4 The data access properties for all the programs are presented elsewhere <ref> [CMT94] </ref>.
Reference: [FST91] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: Our approach has several advantages over previous research. It is applicable to a wider range of programs because we do not require perfect nests or nests that can be made perfect with conditionals <ref> [FST91, GJG88, LP92, WL91] </ref>. It is quicker, both in the expected and worse case. Previous work generates all loop permutations [FST91, GJG88] or unimodular transformations (a combination of permutation, skewing, and reversal) [LP92, WL91], evaluates the locality of all legal permutations, and then picks the best. <p> It is applicable to a wider range of programs because we do not require perfect nests or nests that can be made perfect with conditionals [FST91, GJG88, LP92, WL91]. It is quicker, both in the expected and worse case. Previous work generates all loop permutations <ref> [FST91, GJG88] </ref> or unimodular transformations (a combination of permutation, skewing, and reversal) [LP92, WL91], evaluates the locality of all legal permutations, and then picks the best. This process requires the evaluation of up to n! loop permutations (though n is typically small). <p> We therefore chose not to include skewing, even though it is implemented in our system [KMT93] and our model can drive it. We did integrate reversal, but it did not help to improve locality. The exhaustive approach taken by previous researchers <ref> [FST91, GJG88, LP92, WL91] </ref> is not practical when including transformations which create and combine loop nests (e.g., fusion, distribution). Fusion for improving reuse is by itself NP-hard [KM93].
Reference: [GJG88] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Our approach has several advantages over previous research. It is applicable to a wider range of programs because we do not require perfect nests or nests that can be made perfect with conditionals <ref> [FST91, GJG88, LP92, WL91] </ref>. It is quicker, both in the expected and worse case. Previous work generates all loop permutations [FST91, GJG88] or unimodular transformations (a combination of permutation, skewing, and reversal) [LP92, WL91], evaluates the locality of all legal permutations, and then picks the best. <p> It is applicable to a wider range of programs because we do not require perfect nests or nests that can be made perfect with conditionals [FST91, GJG88, LP92, WL91]. It is quicker, both in the expected and worse case. Previous work generates all loop permutations <ref> [FST91, GJG88] </ref> or unimodular transformations (a combination of permutation, skewing, and reversal) [LP92, WL91], evaluates the locality of all legal permutations, and then picks the best. This process requires the evaluation of up to n! loop permutations (though n is typically small). <p> We therefore chose not to include skewing, even though it is implemented in our system [KMT93] and our model can drive it. We did integrate reversal, but it did not help to improve locality. The exhaustive approach taken by previous researchers <ref> [FST91, GJG88, LP92, WL91] </ref> is not practical when including transformations which create and combine loop nests (e.g., fusion, distribution). Fusion for improving reuse is by itself NP-hard [KM93]. <p> Two references are in the same reference group if they exhibit group-temporal or group-spatial reuse, i.e., they access the same cache line on the same or different iterations of an inner loop. This formulation is more general than previous work [KM92], but slightly more restrictive than uniformly generated references <ref> [GJG88] </ref>.
Reference: [GKT91] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: model, our algorithms are efficient and usually find the best transformations for data locality using permutation, fusion and distribution. 3 Background In this section, we characterize data reuse and present an effective data locality cost model. 3.1 Data Dependence We assume the reader is familiar with concept of data dependence <ref> [KKP + 81, GKT91] </ref>. ~ ffi = fffi 1 : : : ffi k g is a hybrid distance/direction vector with the most precise information derivable. It represents a data dependence between two array references, corresponding left to right from the outermost loop to innermost loop enclosing the references.
Reference: [HKM91] <author> M. W. Hall, K. Kennedy, and K. S. M c Kinley. </author> <title> Interprocedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Other coding styles may also inhibit optimization in our system. For example, Linpackd and Matrix300 are written in a modular style with singly nested loops enclosing function calls to routines which also contain singly nested loops. To improve programs written in this style requires interprocedural optimization <ref> [CHK93, HKM91] </ref>; these optimizations are not currently implemented in our translator. Many loop nests (69%) in the original programs are already in memory order, and even more (74%) have the loop carrying the most reuse in the innermost position.
Reference: [IT88] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: Improve order of memory accesses to exploit all levels of the memory hierarchy through loop permutation, fusion, distribution, skewing, and reversal. This process is mostly machine-independent, and requires knowledge only of the cache line size. 2. Fully utilize the cache through tiling, a combination of strip-mining and loop permutation <ref> [IT88] </ref>. Knowledge of the cache size, associativity, and replacement policy is essential. Higher degrees of tiling can be applied to exploit multi-level caches, the TLB, etc. 3. Promote register reuse through unroll-and-jam (also known as register tiling) and scalar replacement [CCK90]. <p> Assuming that the cache size is relatively large, the compiler can apply loop tiling, a combination of strip-mining and loop interchange, to capture long-term reuse at outer loops <ref> [IT88, LRW91, WL91, Wol87] </ref>. Tiling must be applied judiciously because it affects scalar optimizations, increases loop overhead, and may decrease spatial reuse at tile boundaries.
Reference: [KKP + 81] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: model, our algorithms are efficient and usually find the best transformations for data locality using permutation, fusion and distribution. 3 Background In this section, we characterize data reuse and present an effective data locality cost model. 3.1 Data Dependence We assume the reader is familiar with concept of data dependence <ref> [KKP + 81, GKT91] </ref>. ~ ffi = fffi 1 : : : ffi k g is a hybrid distance/direction vector with the most precise information derivable. It represents a data dependence between two array references, corresponding left to right from the outermost loop to innermost loop enclosing the references.
Reference: [KM92] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: This paper extends previous work <ref> [KM92] </ref> with a slightly more accurate memory model. We use the model to derive a loop structure which results in the fewest accesses to main memory. The loop structure is achieved through an algorithm that uses compound loop transformations. The compound loop transformations are permutation, fusion, distribution, and reversal. <p> In this paper, we extend and validate recent research to integrate optimizations for parallelism and memory <ref> [KM92] </ref>. We extend their original cost model to capture more types of reuse. The only transformation they perform is loop permutation, whereas we integrate permutation, fusion, distribution, and reversal into a comprehensive approach. Our extensive experimental results are unique to this paper. <p> Two references are in the same reference group if they exhibit group-temporal or group-spatial reuse, i.e., they access the same cache line on the same or different iterations of an inner loop. This formulation is more general than previous work <ref> [KM92] </ref>, but slightly more restrictive than uniformly generated references [GJG88]. <p> If it is not legal, Permute selects a legal permutation as close to memory order as possible, taking worst case n (n 1) time <ref> [KM92] </ref>. These steps are inexpensive; evaluating the locality of the nest is the most expensive step. Our algorithm computes the best permutation 2 In Section 4.5, we perform imperfect interchanges with distribution. with one evaluation step for each loop in the nest. <p> Reversal does not change the pattern of reuse, but it is an enabler; i.e., it may enable permutation to achieve better locality. We extend Permute to perform reversal as follows. If memory order is not legal, Permute places outer loops in position first, building up lexicographically positive dependence vectors <ref> [KM92] </ref>. If Permute cannot legally position a loop in a desired position, Permute tests if reversal is legal and enables the loop to be put in the position.
Reference: [KM93] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: We did integrate reversal, but it did not help to improve locality. The exhaustive approach taken by previous researchers [FST91, GJG88, LP92, WL91] is not practical when including transformations which create and combine loop nests (e.g., fusion, distribution). Fusion for improving reuse is by itself NP-hard <ref> [KM93] </ref>. <p> Previous research has shown that optimizing temporal locality for an adjacent set of n loops with compatible headers is NP-hard <ref> [KM93] </ref>; here all the headers are not necessarily compatible. We therefore apply a greedy strategy based on the depth of compatibility. We build a DAG from the candidate loops.
Reference: [KMT93] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in an interactive parallel programming tool. </title> <journal> Concur-rency: Practice & Experience, </journal> <volume> 5(7) </volume> <pages> 575-602, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: We degrade only one program by 2%, Applu from the NAS Benchmarks. In Wolf and Lam's experiments, skewing was never needed and reversal was seldom applied [Wol92]. We therefore chose not to include skewing, even though it is implemented in our system <ref> [KMT93] </ref> and our model can drive it. We did integrate reversal, but it did not help to improve locality. The exhaustive approach taken by previous researchers [FST91, GJG88, LP92, WL91] is not practical when including transformations which create and combine loop nests (e.g., fusion, distribution).
Reference: [LP92] <author> W. Li and K. Pingali. </author> <title> Access normalization: Loop restructuring for NUMA compilers. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Our approach has several advantages over previous research. It is applicable to a wider range of programs because we do not require perfect nests or nests that can be made perfect with conditionals <ref> [FST91, GJG88, LP92, WL91] </ref>. It is quicker, both in the expected and worse case. Previous work generates all loop permutations [FST91, GJG88] or unimodular transformations (a combination of permutation, skewing, and reversal) [LP92, WL91], evaluates the locality of all legal permutations, and then picks the best. <p> It is quicker, both in the expected and worse case. Previous work generates all loop permutations [FST91, GJG88] or unimodular transformations (a combination of permutation, skewing, and reversal) <ref> [LP92, WL91] </ref>, evaluates the locality of all legal permutations, and then picks the best. This process requires the evaluation of up to n! loop permutations (though n is typically small). In comparison, our approach only performs one evaluation step because it directly determines the best loop permutation. <p> We therefore chose not to include skewing, even though it is implemented in our system [KMT93] and our model can drive it. We did integrate reversal, but it did not help to improve locality. The exhaustive approach taken by previous researchers <ref> [FST91, GJG88, LP92, WL91] </ref> is not practical when including transformations which create and combine loop nests (e.g., fusion, distribution). Fusion for improving reuse is by itself NP-hard [KM93].
Reference: [LRW91] <author> M. Lam, E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: LoopCost then calculates the total number of cache lines accessed by all references when l is the innermost loop. It simply sums RefCost for all reference groups, then multiplies the result by the trip counts 1 Lam et al. confirm this assumption <ref> [LRW91] </ref>. <p> Unfortunately, fusion also lowered hit rates Track, Dnasa7, and Wave; the degradation may be due to added cache conflict and capacity misses after loop fusion. To recognize and avoid these situations requires cache capacity and interference analysis similar to that performed for evaluating loop tiling <ref> [LRW91] </ref>. Because our fusion algorithm only attempts to optimize reuse at the innermost loop level, it may sometimes merge array references that interfere or overflow cache. We intend to correct this deficiency in the future. <p> Assuming that the cache size is relatively large, the compiler can apply loop tiling, a combination of strip-mining and loop interchange, to capture long-term reuse at outer loops <ref> [IT88, LRW91, WL91, Wol87] </ref>. Tiling must be applied judiciously because it affects scalar optimizations, increases loop overhead, and may decrease spatial reuse at tile boundaries.
Reference: [McK92] <author> K. S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: RefCost and LoopCost appear in Figure 1. This method evaluates imperfectly nested loops, complicated subscript expressions and nests with symbolic bounds <ref> [McK92] </ref>. 4 Compound Loop Transformations In this section, we show how the cost model guides loop permutation, fusion, distribution, and reversal. Each subsection describes tests based on the cost model to determine when individual transformations are profitable.
Reference: [War84] <author> J. Warren. </author> <title> A hierachical basis for reordering transformations. </title> <booktitle> In Conference Record of the Eleventh Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Salt Lake City, UT, </address> <month> January </month> <year> 1984. </year>
Reference-contexts: Reversal did not improved locality in our experiments, therefore we will not discuss it further. 4.3 Loop Fusion Loop fusion takes multiple loop nests and combines their bodies into one loop nest. It is legal only if no data dependences are reversed <ref> [War84] </ref>. As an example of its effect, consider the scalar-ization into Fortran 77 in Figure 3 (b) of the Fortran 90 code fragment for performing ADI integration in Figure 3 (a).
Reference: [WL91] <author> M. E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Our approach has several advantages over previous research. It is applicable to a wider range of programs because we do not require perfect nests or nests that can be made perfect with conditionals <ref> [FST91, GJG88, LP92, WL91] </ref>. It is quicker, both in the expected and worse case. Previous work generates all loop permutations [FST91, GJG88] or unimodular transformations (a combination of permutation, skewing, and reversal) [LP92, WL91], evaluates the locality of all legal permutations, and then picks the best. <p> It is quicker, both in the expected and worse case. Previous work generates all loop permutations [FST91, GJG88] or unimodular transformations (a combination of permutation, skewing, and reversal) <ref> [LP92, WL91] </ref>, evaluates the locality of all legal permutations, and then picks the best. This process requires the evaluation of up to n! loop permutations (though n is typically small). In comparison, our approach only performs one evaluation step because it directly determines the best loop permutation. <p> In comparison, our approach only performs one evaluation step because it directly determines the best loop permutation. Wolf and Lam use unimodular transformations and tiling with estimates of temporal and spatial reuse to improve data locality <ref> [WL91] </ref>. Their memory model is potentially more precise than ours because it directly calculates reuse across outer loops; however, it may be less precise because it ignores loop bounds even when they are known constants. <p> We therefore chose not to include skewing, even though it is implemented in our system [KMT93] and our model can drive it. We did integrate reversal, but it did not help to improve locality. The exhaustive approach taken by previous researchers <ref> [FST91, GJG88, LP92, WL91] </ref> is not practical when including transformations which create and combine loop nests (e.g., fusion, distribution). Fusion for improving reuse is by itself NP-hard [KM93]. <p> Assuming that the cache size is relatively large, the compiler can apply loop tiling, a combination of strip-mining and loop interchange, to capture long-term reuse at outer loops <ref> [IT88, LRW91, WL91, Wol87] </ref>. Tiling must be applied judiciously because it affects scalar optimizations, increases loop overhead, and may decrease spatial reuse at tile boundaries.
Reference: [Wol87] <author> M. J. Wolfe. </author> <title> Iteration space tiling for memory hierarchies, </title> <note> De-cember 1987. Extended version of a paper which appeared in Proceedings of the Third SIAM Conference on Parallel Processing. </note>
Reference-contexts: Assuming that the cache size is relatively large, the compiler can apply loop tiling, a combination of strip-mining and loop interchange, to capture long-term reuse at outer loops <ref> [IT88, LRW91, WL91, Wol87] </ref>. Tiling must be applied judiciously because it affects scalar optimizations, increases loop overhead, and may decrease spatial reuse at tile boundaries.
Reference: [Wol91] <author> M. J. Wolfe. </author> <title> The Tiny loop restructuring research tool. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Note that our system handles the permutation of both triangular and rectangular nests. To gather performance results for Cholesky, we generated all possible loop permutations; they are all legal. For each permutation, we applied the minimal amount of loop distribution necessary. (Wolfe enumerates these loop organizations <ref> [Wol91] </ref>.) Compared to matrix multiply, there are more variations in observed and predicted behavior.
Reference: [Wol92] <author> M. E. Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Stanford University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: It is difficult to directly compare our experiments because their cache optimization results include tiling and scalar replacement and are executed on a different processor. We improve a few more programs/routines than they do, but their cache optimizations degrade 6 programs/routines, in one case by 20% <ref> [Wol92] </ref>. We degrade only one program by 2%, Applu from the NAS Benchmarks. In Wolf and Lam's experiments, skewing was never needed and reversal was seldom applied [Wol92]. We therefore chose not to include skewing, even though it is implemented in our system [KMT93] and our model can drive it. <p> We improve a few more programs/routines than they do, but their cache optimizations degrade 6 programs/routines, in one case by 20% <ref> [Wol92] </ref>. We degrade only one program by 2%, Applu from the NAS Benchmarks. In Wolf and Lam's experiments, skewing was never needed and reversal was seldom applied [Wol92]. We therefore chose not to include skewing, even though it is implemented in our system [KMT93] and our model can drive it. We did integrate reversal, but it did not help to improve locality. <p> We intend to correct this deficiency in the future. Our results are very favorable when compared to Wolf's results, though direct comparisons are difficult because he combines tiling with cache optimizations and reports improvements only relative to programs with scalar replacement <ref> [Wol92] </ref>. Wolf applied permutation, skewing, reversal and tiling to the Perfect Benchmarks and Dnasa7 on a DECstation 5000 with a 64KB direct-map cache. His results show performance degradations or no change in all but Adm, which showed a small (1%) improvement in execution time.
References-found: 25

