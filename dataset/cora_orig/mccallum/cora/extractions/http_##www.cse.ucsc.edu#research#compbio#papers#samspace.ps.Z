URL: http://www.cse.ucsc.edu/research/compbio/papers/samspace.ps.Z
Refering-URL: http://www.cse.ucsc.edu/research/compbio/sam.html
Root-URL: http://www.cse.ucsc.edu
Title: Reduced space hidden Markov model training  
Author: Christopher Tarnas and Richard Hughey 
Note: PREPRINT to appear in CABIOS, 1998  
Address: Santa Cruz, CA 95064  
Affiliation: Department of Computer Engineering Jack Baskin School of Engineering University of California  
Abstract: 1 Abstract 
Abstract-found: 1
Intro-found: 1
Reference: <author> Altschul, S. F. </author> <year> (1991). </year> <title> Amino acid substitution matrices from an information theoretic perspective. </title> <journal> J. Mol. Biol. </journal> <volume> 219, </volume> <pages> 555-565. </pages>
Reference-contexts: The logarithm of the ratio of the probability of the sequence being generated by the model and by the null model, the log-odds score, indicates how much better (or worse) the structured HMM models the sequence then the simple unstructured null model <ref> (Altschul, 1991) </ref>. For SAM, a sequence can make use of fully local alignment as follows. The initial segment of the sequence that does not correspond to any part of the HMM will align to the initial FIM.
Reference: <author> Barrett, C., Hughey, R., & Karplus, K. </author> <year> (1997). </year> <title> Scoring hidden Markov models. </title> <journal> CABIOS, </journal> <volume> 13 (2), </volume> <pages> 191-199. </pages>
Reference: <author> Bucher, P. & Bairoch, A. </author> <year> (1994). </year> <title> A generalized profile syntax for biomolecular sequence motifs and its function in automatic sequence interpretation. </title> <booktitle> In: Proc. Int. Conf. Intelligent Systems for Molecular Biology, </booktitle> <editor> (Altman, R. et al., eds) pp. </editor> <address> 53-61, Menlo Park, CA: </address> <publisher> AAAI/MIT Press. </publisher>
Reference-contexts: In the more general form, these costs would be position-dependent <ref> (Bucher & Bairoch, 1994) </ref>, and could also be trained given sufficient data. Analogous changes are made to the forward-backward calculation. Semi-local dynamic programming, which allows a complete sequence to match a subsection of the model, is simpler than fully local alignment.
Reference: <author> Eddy, S. </author> <year> (1996). </year> <title> Hidden Markov models. </title> <booktitle> Current Opinion in Structural Biology, </booktitle> <volume> 6 (3), </volume> <pages> 361-365. </pages>
Reference-contexts: The notation has been simplified; the reader is referred to the litera ture for a more detailed treatment (Rabiner, 1989; Krogh et al., 1994) and an HMM review <ref> (Eddy, 1996) </ref>. The simplest approach to computing these O (nm) dynamic programs is to create a large, nfim table in memory to store values. Unfortunately, this table will not fit entirely in a workstation's memory for large model and sequence lengths.
Reference: <author> Eddy, S., Mitchison, G., & Durbin, R. </author> <year> (1995). </year> <title> Maximum discrimination hidden Markov models of sequence consensus. </title> <journal> J. Comput. Biol. </journal> <volume> 2, </volume> <pages> 9-23. </pages>
Reference-contexts: This algorithm is well known in the computational biology community (Myers & Miller, 1988), and has, for example, been implemented in the HMMer package for sequence alignment to a trained HMM <ref> (Eddy et al., 1995) </ref>. The divide-and-conquer algorithm does not work with forward-backward training. The efficiency of the divide-and-conquer algorithm is in its partitioning into subproblems so that, while the entire dynamic programming matrix must be evaluated once, recursive calls consider smaller and smaller segments of the matrix.
Reference: <author> Gotoh, O. </author> <year> (1982). </year> <title> An improved algorithm for matching biological sequences. </title> <journal> J. Mol. Biol. </journal> <volume> 162 (3), </volume> <pages> 705-708. </pages>
Reference-contexts: The addition of affine gap costs g to the recurrence <ref> (Gotoh, 1982) </ref> produces three interleaved recurrences of similar form. Here, the comparison or alignment can be thought of as being in one of three states: in the midst of a sequence of matches, deletions, or insertions, where cost for changing states provides the constant term in the affine gap cost.
Reference: <author> Gribskov, M., Luthy, R., & Eisenberg, D. </author> <year> (1990). </year> <title> Profile analysis. </title> <booktitle> Methods in Enzymology, </booktitle> <volume> 183, </volume> <pages> 146-159. </pages>
Reference: <author> Grice, J. A., Hughey, R., & Speck, D. </author> <year> (1997). </year> <title> Reduced space sequence alignment. </title> <journal> CABIOS, </journal> <volume> 13 (1), </volume> <pages> 45-53. </pages>
Reference-contexts: With forward-backward training, all paths through the matrix are an important part of training the HMM, and thus this gain cannot be used. One answer is a recently introduced checkpoint-ing algorithm <ref> (Grice et al., 1997) </ref>. In the simplest case, diagonals, rows, or columns of the dynamic programming matrix are stored at regular intervals to reduce space use to O (m p n) while increasing runtime by a small constant factor. <p> We describe this variant in more detail below, and refer the reader to the original work for information on and analysis of the complete family of algorithms <ref> (Grice et al., 1997) </ref>. 4.1 2-Level checkpoints Simply stated, the idea behind 2-level checkpoints is to segment the backwards computation. For each of the segments to be computed, a single checkpoint of all values along a row, column, or diagonal is saved during a global forward calculation.
Reference: <author> Hennessy, J. L. & Patterson, D. A. </author> <year> (1996). </year> <title> Computer Architecture: A Quantitative Approach. </title> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: In a current machine, a cache may require one clock cycle to access, main memory 10 to 20 clock cycles, and disk over one million cycles <ref> (Hennessy & Patterson, 1996) </ref>. Thus, the cost of paging into virtual memory is high, and can make runs of a given size effectively uncomputable. The solution to this is to use a sequence alignment method that requires less space.
Reference: <author> Hirschberg, D. S. </author> <year> (1975). </year> <title> A linear space algorithm for computing maximal common subsequences. </title> <journal> Communications of the ACM, </journal> <volume> 18 (6), </volume> <pages> 341-343. </pages>
Reference-contexts: In the case of finding the single best path, there is an elegant divide-and-conquer algorithm that requires only O (n + m) space, where n is the sequence length and m is the model length <ref> (Hirschberg, 1975) </ref>. The approach of this algorithm is to find a midpoint of the best 3 path without saving all O (nm) dynamic program-ming entries, and then to solve two smaller problems, each of approximate size nm=4 using the same algorithm.
Reference: <author> Huang, X. </author> <year> (1989). </year> <title> A space-efficient parallel sequence comparison algorithm for a message-passing multiprocessor. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18 (3), </volume> <pages> 223-239. </pages>
Reference-contexts: When applying checkpointing to the Viterbi algorithm, diagonal checkpoints can be used to greatly reduce the amount of recalculation, making use of the same principle introduced in a parallelization of the divide-and-conquer algorithm <ref> (Huang, 1989) </ref>.
Reference: <author> Hughey, R. </author> <year> (1996). </year> <title> Parallel sequence comparison and alignment. </title> <journal> CABIOS, </journal> <volume> 12 (6), </volume> <pages> 473-479. </pages>
Reference: <author> Hughey, R. & Krogh, A. </author> <year> (1996). </year> <title> Hidden Markov models for sequence analysis: Extension and analysis of the basic method. </title> <journal> CABIOS, </journal> <volume> 12 (2), </volume> <pages> 95-107. </pages>
Reference: <author> Krogh, A., Brown, M., Mian, I. S., Sjolander, K., & Haussler, D. </author> <year> (1994). </year> <title> Hidden Markov models in computational biology: Applications to protein modeling. </title> <journal> J. Mol. Biol. </journal> <volume> 235, </volume> <pages> 1501-1531. </pages>
Reference: <author> Myers, E. W. & Miller, W. </author> <year> (1988). </year> <title> Optimal alignments in linear space. </title> <journal> CABIOS, </journal> <volume> 4 (1), </volume> <pages> 11-17. </pages>
Reference-contexts: The approach of this algorithm is to find a midpoint of the best 3 path without saving all O (nm) dynamic program-ming entries, and then to solve two smaller problems, each of approximate size nm=4 using the same algorithm. This algorithm is well known in the computational biology community <ref> (Myers & Miller, 1988) </ref>, and has, for example, been implemented in the HMMer package for sequence alignment to a trained HMM (Eddy et al., 1995). The divide-and-conquer algorithm does not work with forward-backward training.
Reference: <author> Needleman, S. B. & Wunsch, C. D. </author> <year> (1970). </year> <title> A general method applicable to the search for similarities in the amino acid sequences of two proteins. </title> <journal> J. Mol. Biol. </journal> <volume> 48, </volume> <pages> 443-453. </pages>
Reference: <author> Rabiner, L. R. </author> <year> (1989). </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <journal> Proc. IEEE, </journal> <volume> 77 (2), </volume> <pages> 257-286. </pages>
Reference: <author> Sellers, P. H. </author> <year> (1974). </year> <title> On the theory and computation of evolutionary distances. </title> <journal> SIAM J. Appl. Math. </journal> <volume> 26, </volume> <pages> 787-793. </pages>
Reference-contexts: Dynamic programming example to find least cost edit of "BACKTRACK" into "TRACE-BACK" using Sellers' evolutionary distance metric <ref> (Sellers, 1974) </ref>. Below the dynamic programming table are two possible alignments and an illustration of the data dependencies.
Reference: <author> Sjolander, K., Karplus, K., Brown, M. P., Hughey, R., Krogh, A., Mian, I. S., & Haussler, D. </author> <year> (1996). </year> <title> Dirichlet mixtures: A method for improving detection of weak but significant protein sequence homology. </title> <journal> CABIOS, </journal> <volume> 12 (4), </volume> <pages> 327-345. </pages> <note> 8 Smith, </note> <author> T. F. & Waterman, M. S. </author> <year> (1981). </year> <title> Identification of common molecular subsequences. </title> <journal> J. Mol. Biol. </journal> <volume> 147, </volume> <pages> 195-197. </pages>
Reference-contexts: i;j b i;j : This value, the probability that a certain character was generated by a certain state of the HMM, is then used to update the probabilities in the HMM, in association with the values for other sequences in the training set and a regularizer or Dirichlet mixture prior <ref> (Sjolander et al., 1996) </ref>. The notation has been simplified; the reader is referred to the litera ture for a more detailed treatment (Rabiner, 1989; Krogh et al., 1994) and an HMM review (Eddy, 1996).
Reference: <author> Wagner, R. A. & Fischer, M. J. </author> <year> (1974). </year> <title> The string-to-string correction problem. </title> <journal> J. ACM, </journal> <volume> 21 (1), </volume> <pages> 168-173. </pages>
References-found: 20

