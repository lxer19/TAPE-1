URL: http://drl.cs.uiuc.edu/pubs/splc96/latex/splc96.ps
Refering-URL: http://drl.cs.uiuc.edu/pubs/splc96.html
Root-URL: http://www.cs.uiuc.edu
Title: The Panda Library for Parallel I/O of Large Multidimensional Arrays  
Author: M. Winslett K. E. Seamons Y. Chen, Y. Cho, S. Kuo, M. Subramaniam 
Address: Street Urbana, IL 61801 USA Pittsburgh, PA 15219 USA  
Note: 707 Grant  
Affiliation: Department of Computer Science Transarc Corporation University of Illinois  
Abstract: Users of current parallel I/O facilities on distributed-memory platforms usually have to contend with complex, idiosyncratic user interfaces and unexpectedly poor performance. In this paper we discuss the reasons behind these problems: the difficulty in adding parallelism to a file system, the low-level nature of file system interfaces, and the need to be all things to all users. We also describe how we have avoided these problems in the Panda library for parallel I/O of large multidimensional arrays, by building on top of commodity file systems, targeting a specific class of applications, and providing a high-level interface for users. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. E. Bassow. </author> <title> IBM AIX Parallel I/O File System: Installation, Administration, and Use. </title> <institution> IBM, Kingston, </institution> <address> N.Y., </address> <month> May </month> <year> 1995. </year> <note> Document SH34-6065-00. </note>
Reference-contexts: If vendors can be convinced to adopt disk-directed I/O for their file systems, then users will be relieved of many performance problems (though they will probably still need a higher-level interface provided by a library layer). Several efforts are underway to construct scalable parallel file systems <ref> [9, 1, 20, 19, 14] </ref>. Many of the proposed designs are clearly influenced by the desire to provide efficient support for applications accessing persistent arrays. Some of these projects provide facilities that are motivated by the assumption that arrays must be stored on disk in traditional row-major or column-major order.
Reference: [2] <author> R. Bennett, K. Bryant, A. Sussman, R. Das, and J. Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proc. 1994 Scalable Parallel Libraries Conf., </booktitle> <pages> pp. 10-20, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: It assumes I/O operations are collective, and only a subset of the clients send requests to the I/O devices, to limit network traffic. Panda incorporates elements of this strategy for write operations as well. Jovian <ref> [2] </ref> is a runtime library for collective I/O of SPMD-style codes.
Reference: [3] <author> R. Bordawekar, A. Choudhary, K. Kennedy, C. Koelbel, and M. Paleczny. </author> <title> A model and compilation strategy for out-of-core data parallel programs. </title> <journal> TR CRPC-TR94507-S, </journal> <note> Center for Research on Parallel Computation, </note> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: An I/O system based on the two-phase approach could use Panda to output the data following the reorganization phase. The two-phase approach has also been adopted by some approaches for compiler-based I/O, especially for out-of-core applications <ref> [5, 6, 22, 24, 22, 3] </ref>. Disk-directed I/O for collective I/O operations involving arrays is proposed in [15] as a means of overcoming the performance problems described in Section 1 by integrating new facilities into a commodity file system.
Reference: [4] <author> R. Bordawekar, J. M. del Rosario, and A. Choud-hary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proc. Supercomputing '93, </booktitle> <pages> pp. 452-461, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Panda's high-level interface would permit the implementation to automatically map high-level array I/O semantics to MPI-IO primitives. However, for Panda to provide high performance, it is likely that Panda will have to map a given high-level I/O request to different MPI-IO calls on different platforms. <ref> [4] </ref> describes run-time primitives to support a two-phase access strategy [11, 10] for conducting parallel I/O of arrays.
Reference: [5] <author> P. Brezany, M. Gernt, P. Mehrotra, and H. Zima. </author> <title> Concurrent file operations in a High Performance Fortran. </title> <booktitle> In Proc. Supercomputing '92, </booktitle> <pages> pp. 230-237, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: An I/O system based on the two-phase approach could use Panda to output the data following the reorganization phase. The two-phase approach has also been adopted by some approaches for compiler-based I/O, especially for out-of-core applications <ref> [5, 6, 22, 24, 22, 3] </ref>. Disk-directed I/O for collective I/O operations involving arrays is proposed in [15] as a means of overcoming the performance problems described in Section 1 by integrating new facilities into a commodity file system.
Reference: [6] <author> P. Brezany, T. A. Mueck, and E. Schikuta. Lan--guage, </author> <title> compiler and parallel database support for I/O intensive applications. </title> <booktitle> In Proceedings of the High Performance Computing and Networking 1995 Europe. </booktitle> <publisher> Springer-Verlag, </publisher> <month> May </month> <year> 1995. </year>
Reference-contexts: An I/O system based on the two-phase approach could use Panda to output the data following the reorganization phase. The two-phase approach has also been adopted by some approaches for compiler-based I/O, especially for out-of-core applications <ref> [5, 6, 22, 24, 22, 3] </ref>. Disk-directed I/O for collective I/O operations involving arrays is proposed in [15] as a means of overcoming the performance problems described in Section 1 by integrating new facilities into a commodity file system.
Reference: [7] <author> A. Choudhary, R. Bordawekar, M. Harry, R. Krishnaiyer, R. Ponnusamy, T. Singh, and R. Thakur. </author> <title> PASSION: parallel and scalable software for input-output. </title> <type> TR SCCS-636, </type> <institution> ECE Dept., NPAC and CASE Center, Syracuse University, </institution> <month> Sep. </month> <year> 1994. </year>
Reference: [8] <author> P. Corbett, D. Feitelson, Y. Hsu, J.-P. Prost, M. Snir, S. Fineberg, B. Nitzberg, B. Traversat, and P. Wong. </author> <title> MPI-IO: A parallel file I/O interface for MPI. </title> <type> TR NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> Jan. </month> <year> 1995. </year> <note> Version 0.3. </note>
Reference-contexts: In this section we briefly cover the related work from the compilers and parallel I/O communities. With few exceptions [13], the interfaces proposed for parallel I/O are at a lower level than those used in Panda. For example, MPI-IO <ref> [8] </ref> is a proposal for a parallel file I/O interface, inspired by the problem of portable performance described in Section 1. The goal of the proposal is to provide a standard for describing parallel I/O operations in MPI applications.
Reference: [9] <author> P. F. Corbett and D. G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proc. of the Scalable High-Performance Computing Conference, </booktitle> <pages> pp. 63-70, </pages> <year> 1994. </year>
Reference-contexts: If vendors can be convinced to adopt disk-directed I/O for their file systems, then users will be relieved of many performance problems (though they will probably still need a higher-level interface provided by a library layer). Several efforts are underway to construct scalable parallel file systems <ref> [9, 1, 20, 19, 14] </ref>. Many of the proposed designs are clearly influenced by the desire to provide efficient support for applications accessing persistent arrays. Some of these projects provide facilities that are motivated by the assumption that arrays must be stored on disk in traditional row-major or column-major order.
Reference: [10] <author> J. M. del Rosario, R. Bordawekar, and A. Choud-hary. </author> <title> Improving parallel I/O performance using a two-phase access strategy. </title> <type> Technical Report SCCS-406, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <year> 1993. </year>
Reference-contexts: However, for Panda to provide high performance, it is likely that Panda will have to map a given high-level I/O request to different MPI-IO calls on different platforms. [4] describes run-time primitives to support a two-phase access strategy <ref> [11, 10] </ref> for conducting parallel I/O of arrays. In this approach, for read operations, the compute nodes cooperate to bring all the data into memory in a way that minimizes the total number of disk accesses by having the data layout in memory conform to the data layout on disk.
Reference: [11] <author> J. M. del Rosario and A. Choudhary. </author> <title> High performance I/O for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: However, for Panda to provide high performance, it is likely that Panda will have to map a given high-level I/O request to different MPI-IO calls on different platforms. [4] describes run-time primitives to support a two-phase access strategy <ref> [11, 10] </ref> for conducting parallel I/O of arrays. In this approach, for read operations, the compute nodes cooperate to bring all the data into memory in a way that minimizes the total number of disk accesses by having the data layout in memory conform to the data layout on disk.
Reference: [12] <author> J. M. del Rosario, M. Harry, and A. Choudhary. </author> <title> The design of VIP-FS: A virtual, parallel file system for high performance parallel and distributed computing. </title> <type> TR SCCS-628, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: We have experimented with layering Panda above some of these parallel file systems, providing the user with an easy-to-use interface for optimized array I/O. VIP-FS <ref> [12] </ref> is another user-level library designed to be layered above commodity file systems on clusters of workstations. VIP-FS supports a variety of access strategies; direct access, two-phase access, and assumed-requests access. Assumed-requests access is intended to improve the performance of read operations and has similarities to Panda's server-directed I/O.
Reference: [13] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <address> pp.pages 462-471, </address> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: In this section we briefly cover the related work from the compilers and parallel I/O communities. With few exceptions <ref> [13] </ref>, the interfaces proposed for parallel I/O are at a lower level than those used in Panda. For example, MPI-IO [8] is a proposal for a parallel file I/O interface, inspired by the problem of portable performance described in Section 1.
Reference: [14] <author> J. V. Huber Jr., C. L. Elford, D. A. Reed, A. A. Chien, and D. S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proc. of the Intl. Conf. on Supercomputing, </booktitle> <month> Jun. </month> <year> 1995. </year>
Reference-contexts: If vendors can be convinced to adopt disk-directed I/O for their file systems, then users will be relieved of many performance problems (though they will probably still need a higher-level interface provided by a library layer). Several efforts are underway to construct scalable parallel file systems <ref> [9, 1, 20, 19, 14] </ref>. Many of the proposed designs are clearly influenced by the desire to provide efficient support for applications accessing persistent arrays. Some of these projects provide facilities that are motivated by the assumption that arrays must be stored on disk in traditional row-major or column-major order.
Reference: [15] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <type> TR PCS-TR94-226, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year> <note> Revised Nov., </note> <year> 1994. </year>
Reference-contexts: This approach has been labeled the direct method [26] or traditional caching <ref> [15] </ref> and is known to perform poorly in many cases due to excess disk seeks and write-back errors. <p> The two-phase approach has also been adopted by some approaches for compiler-based I/O, especially for out-of-core applications [5, 6, 22, 24, 22, 3]. Disk-directed I/O for collective I/O operations involving arrays is proposed in <ref> [15] </ref> as a means of overcoming the performance problems described in Section 1 by integrating new facilities into a commodity file system. <p> Disk-directed I/O allows on-the-fly reorganization of data from one physical schema to another during I/O as data flows from the I/O processors to the compute processors, in contrast to the two-phase I/O approach just described. The simulation in <ref> [15] </ref> shows disk-directed I/O to have advantages over traditional caching. They argue that disk-directed I/O offers all the advantages of two-phase I/O and provides additional advantages as well, such as the ability to overlap the I/O and rearrangement process as well as exploit details of the low-level physical data layout.
Reference: [16] <author> D. Kotz. </author> <title> Disk-directed I/O for an out-of-core computation. </title> <type> TR PCS-TR95-251, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: Simulations show disk-directed I/O has advantages over traditional caching for an out-of-core LU decomposition problem <ref> [16] </ref>. Simulations also show disk directed I/O is effective for irregularly structured requests, data-dependent distributions, and data-dependent filtering [17]. There are some disadvantages in implementing disk-directed I/O within a file system, as intended by its creators.
Reference: [17] <author> D. Kotz. </author> <title> Expanding the potential for disk-directed I/O. </title> <type> TR PCS-TR95-254, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: Simulations show disk-directed I/O has advantages over traditional caching for an out-of-core LU decomposition problem [16]. Simulations also show disk directed I/O is effective for irregularly structured requests, data-dependent distributions, and data-dependent filtering <ref> [17] </ref>. There are some disadvantages in implementing disk-directed I/O within a file system, as intended by its creators.
Reference: [18] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: More detail on these experiments and those described below can be found at the Panda web site, http://bunny.cs.uiuc.edu/CDR/panda. Our experiments also show that with very fast disks on the SP2, Panda would usually achieve throughputs close to the MPI message passing peak bandwidth <ref> [18] </ref> on the SP2, which we measured as 34 MB/s. For example, Figure 2 shows Panda's performance for writes of large arrays block-distributed in memory and in traditional order on disk, when the file system calls on the I/O nodes are commented out to simulate an infinitely fast disk.
Reference: [19] <author> S. A. Moyer and V. S. Sunderam. </author> <title> A parallel I/O system for high-performance distributed computing. </title> <booktitle> In Proc. IFIP WG10.3 Working Conf. on Programming Environments for Massively Parallel Distributed Systems, </booktitle> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: If vendors can be convinced to adopt disk-directed I/O for their file systems, then users will be relieved of many performance problems (though they will probably still need a higher-level interface provided by a library layer). Several efforts are underway to construct scalable parallel file systems <ref> [9, 1, 20, 19, 14] </ref>. Many of the proposed designs are clearly influenced by the desire to provide efficient support for applications accessing persistent arrays. Some of these projects provide facilities that are motivated by the assumption that arrays must be stored on disk in traditional row-major or column-major order.
Reference: [20] <author> S. A. Moyer and V. S. Sunderam. </author> <title> PIOUS: A scalable parallel I/O system for distributed computing environments. </title> <booktitle> In Proc. Scalable High-Performance Computing Conference, </booktitle> <pages> pp. 71-78, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: If vendors can be convinced to adopt disk-directed I/O for their file systems, then users will be relieved of many performance problems (though they will probably still need a higher-level interface provided by a library layer). Several efforts are underway to construct scalable parallel file systems <ref> [9, 1, 20, 19, 14] </ref>. Many of the proposed designs are clearly influenced by the desire to provide efficient support for applications accessing persistent arrays. Some of these projects provide facilities that are motivated by the assumption that arrays must be stored on disk in traditional row-major or column-major order.
Reference: [21] <institution> National Center for Supercomputing Applications, University of Illinois. </institution> <note> NCSA HDF Reference Manual, Version 3.3, </note> <month> February </month> <year> 1994. </year>
Reference-contexts: This is the direction we are following for current research on Panda. Panda is a research project, not a development project, so Panda is not the solution to every scientist's parallel I/O woes. For example, Panda lacks the sort of down-to-earth facilities found in HDF <ref> [21] </ref> to, for example, translate all floating point numbers to a vendor-neutral standard format. Instead, our focus has been to support a small group of scientists directly (to understand what the real issues are), and effect technology transfer to widely-used libraries and file systems.
Reference: [22] <author> M. Paleczny, K. Kennedy, and C. Koelbel. </author> <title> Compiler support for out-of-core arrays on data parallel machines. </title> <booktitle> In Proc. Symposium on Frontiers of Massively Parallel Computation, </booktitle> <pages> pp. 110-118, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: An I/O system based on the two-phase approach could use Panda to output the data following the reorganization phase. The two-phase approach has also been adopted by some approaches for compiler-based I/O, especially for out-of-core applications <ref> [5, 6, 22, 24, 22, 3] </ref>. Disk-directed I/O for collective I/O operations involving arrays is proposed in [15] as a means of overcoming the performance problems described in Section 1 by integrating new facilities into a commodity file system.
Reference: [23] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server directed collective I/O in Panda. </title> <booktitle> In Proc. Supercomputing '95, </booktitle> <address> San Diego, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: In all cases the user would be provided with an interface at a level of abstraction that is easy for the user to understand, and with basic portability across platforms. To achieve high performance, Panda uses server-directed I/O <ref> [23] </ref> to organize the input or output of an array into long sequential reads and writes. The server-directed I/O architecture is targeted for SPMD applications in distributed-memory environments that are closely synchronized during I/O (i.e., collective I/O) and read and write multidimensional arrays.
Reference: [24] <author> R. Thakur, R. Bordawekar, and A. Choudhary. </author> <title> Compiler and runtime support for out-of-core HPF programs. </title> <booktitle> In Proc. of the Intl. Conf. on Supercomputing, </booktitle> <pages> pp. 382-391, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: An I/O system based on the two-phase approach could use Panda to output the data following the reorganization phase. The two-phase approach has also been adopted by some approaches for compiler-based I/O, especially for out-of-core applications <ref> [5, 6, 22, 24, 22, 3] </ref>. Disk-directed I/O for collective I/O operations involving arrays is proposed in [15] as a means of overcoming the performance problems described in Section 1 by integrating new facilities into a commodity file system.
Reference: [25] <author> R. Thakur, R. Bordawekar, A. Choudhary, R. Ponnusamy, and T. Singh. </author> <title> PASSION runtime library for parallel I/O. </title> <booktitle> In Proc. Scalable Parallel Libraries Conf., </booktitle> <pages> pp. 119-128, </pages> <month> Oct. </month> <year> 1994. </year>
Reference: [26] <author> R. Thakur and A. Choudhary. </author> <title> Accessing sections of out-of-core arrays using an extended two-phase method. </title> <type> TR SCCS-685, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: This approach has been labeled the direct method <ref> [26] </ref> or traditional caching [15] and is known to perform poorly in many cases due to excess disk seeks and write-back errors.
Reference: [27] <author> M. Subramaniam. </author> <title> High-performance implementation of server-directed I/O. </title> <type> MS Thesis, </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: To gather a subchunk, a server first sends data request messages 1 to all the overlapping clients for that subchunk, and then waits for the clients to send the requested data back. Read operations follow the reverse process. Details can be found in <ref> [27] </ref>. Panda's high-level interface is instrumental in its provision of server-directed I/O, because it allows an entire collective I/O request involving multiple arrays to be described in a single message from a Panda client to a Panda server, thus reducing message traffic.
References-found: 27

