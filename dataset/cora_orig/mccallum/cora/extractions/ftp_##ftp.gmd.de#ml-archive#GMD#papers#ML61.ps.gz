URL: ftp://ftp.gmd.de/ml-archive/GMD/papers/ML61.ps.gz
Refering-URL: http://nathan.gmd.de/persons/werner.emde/publications.html
Root-URL: 
Email: email: werner.emde@gmd.de  
Title: Inductive Learning of Characteristic Concept Descriptions  
Author: Werner Emde 
Address: Schlo Birlinghoven D-53754 St. Augustin Germany  
Affiliation: GMD, FIT.KI  
Abstract: This paper deals with the problem of learning characteristic concept descriptions from examples and describes a new generalization approach implemented in the system Cola-2. The approach tries to take advantage of the information which can be induced from descriptions of unclassified objects using a conceptual clustering algorithm. Experimental results in various real-world domains strongly support the hypothesis that the new approach delivers more correct (and possibly more comprehesible) concept descriptions than exisiting methods, if the induced concept descriptions are also used to classify objects which belong to concepts which were not present in the training data set. This paper describes the generalization approach implemented in Cola and presents experimental results obtained with a relational and a propositional real world data set. 
Abstract-found: 1
Intro-found: 1
Reference: [Bisson 91] <author> Gilles Bisson. </author> <title> Learning of rule systems in a first order representation. </title> <institution> Rapport de Recherche 628, LRI, University de Paris-Sud, </institution> <year> 1991. </year>
Reference-contexts: Therefore, we refer the reader to [Bisson 92a, Bisson 92b] for a detailed description of the similarity-measure-guided (polynomial) conceptual clustering approach used in Sprite. There are only three major difference between Kbg and Sprite. First, generalized class descriptions are not pruned in Sprite which is done in Kbg (see <ref> [Bisson 91] </ref>). Second, the ouput of Sprite is a set of rules which are independent from each other while Kbg constructs a system of rules, i.e., a sub-class is described in Kbg by a reference to the super-class and a list of additional literals describing the specialization.
Reference: [Bisson 92a] <author> Gilles Bisson. </author> <title> Conceptual Clustering in a First Order Logic Representation. </title> <booktitle> In Proc. of the 10th ECAI, </booktitle> <pages> pp. 558-462, </pages> <year> 1992. </year>
Reference-contexts: In this paper Sprite can be regarded as re-implementation of the conceptual clustering algorithm of the system Kbg-2 in Prolog. Therefore, we refer the reader to <ref> [Bisson 92a, Bisson 92b] </ref> for a detailed description of the similarity-measure-guided (polynomial) conceptual clustering approach used in Sprite. There are only three major difference between Kbg and Sprite. First, generalized class descriptions are not pruned in Sprite which is done in Kbg (see [Bisson 91]).
Reference: [Bisson 92b] <author> Gilles Bisson. </author> <title> Learning in FOL with a Similarity Measure. </title> <booktitle> In AAAI92, </booktitle> <pages> pp. 82-87. </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: Some conceptual clustering programs (e.g., Clus--ter [Michalski/Stepp 83] or Cobweb [Fisher 87]) construct a hierarchy of classes, other programs (e.g., Unimem [Lebowitz 87], Kbg <ref> [Bisson 92b] </ref>) construct a directed acyclic graph of classes, which means that the classes are not necessarily disjoint. Cola uses the conceptual clustering program Sprite to perform the conceptual clustering step. With respect to the topic of this paper Sprite can be regarded as a reimplementation of Kbg-2 [Bisson 92b]. <p> 87], Kbg <ref> [Bisson 92b] </ref>) construct a directed acyclic graph of classes, which means that the classes are not necessarily disjoint. Cola uses the conceptual clustering program Sprite to perform the conceptual clustering step. With respect to the topic of this paper Sprite can be regarded as a reimplementation of Kbg-2 [Bisson 92b]. The advantage of the conceptual clustering approach developed with Kbg-2 compared to systems like Unimem or Cobweb lies in the fact that Kbg's knowledge representation language is based on first-order logic (without negation and function symbols) with some extensions, e.g., for numerical values. <p> In this paper Sprite can be regarded as re-implementation of the conceptual clustering algorithm of the system Kbg-2 in Prolog. Therefore, we refer the reader to <ref> [Bisson 92a, Bisson 92b] </ref> for a detailed description of the similarity-measure-guided (polynomial) conceptual clustering approach used in Sprite. There are only three major difference between Kbg and Sprite. First, generalized class descriptions are not pruned in Sprite which is done in Kbg (see [Bisson 91]).
Reference: [Cameron-Jones/Quinlan 94] <author> R. Mike Cameron-Jones and J. Ross Quinlan. </author> <title> Efficient Top-down Induction of Logic Programs. </title> <journal> SIGART Bulletin, </journal> <volume> 5(1) </volume> <pages> 33-42, </pages> <year> 1994. </year>
Reference-contexts: We chose the learning program Foil-6.1 <ref> [Quinlan 90, Cameron-Jones/Quinlan 94] </ref> as a representative of systems which produce most discriminant generalizations. As a representative of systems performing most specific generalizations we used Cilgg [Kietz 94] which is based on the well-known least general generalization rule of Plotkin [71] .
Reference: [Dietterich/Michalski 81] <author> Thomas G. Dietterich and Ryszard S. Michalski. </author> <title> Inductive Learning of Structural Descriptions. </title> <journal> Artificial Intelligence, </journal> <volume> 16 </volume> <pages> 257-294, </pages> <year> 1981. </year> <month> 17 </month>
Reference-contexts: Some examples of well known systems which are able to learn from examples in relational domains are: Foil [Quinlan 90], Golem [Muggleton/Feng 90], Rdt [Kietz/Wrobel 92], Arch [Winston 75] and Ogust [Vrain 90]. In the literature two kinds of generalized descriptions are distinguished: characteristic descriptions and discriminant descriptions <ref> [Dietterich/Michalski 81] </ref>. A characteristic description of a class of objects describes the sufficient conditions for class membership and enables a system to identify all instances of the class and reject all instances of other (disjoint) classes.
Reference: [Dolsak/Muggelton 92] <author> Bojan Dolsak and Stephen Muggelton. </author> <title> The Application of Induc--tive Logic Programming to Finite Element Mesh Design. </title> <editor> In Stephen Muggleton (ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 453-472. </pages> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: The attributes of objects and relations among objects in the domain are described by facts. A set of facts related to an edge (named a2) in a finite element mesh model <ref> [Dolsak/Muggelton 92] </ref> of a cylinder is shown as an example in table 1. <p> The last fact shown in table 1 means that there is one element on the edge a2. The data set contained in the knowledge base is identical to the data sets used in experiments made by other authors (see, e.g., <ref> [Dolsak/Muggelton 92] </ref>, [Lavrac/Dzeroski 94]) except the fact that they usually represented the number of elements on an edge using a two-place predicate, e.g., the last fact is represented by the fact mesh (a2,1). <p> A description of the domain can be found in <ref> [Dolsak/Muggelton 92] </ref>. The target predicate is meshN (E), where E is a name of the edge and N is the number of finite elements 11 on that edge.
Reference: [Emde 94] <author> Werner Emde. </author> <title> Inductive Learning of Characteristic Concept Descriptions from Small Sets of Classified Examples. </title> <editor> In Francesco Bergadano and Luc De Raedt (eds.), </editor> <booktitle> Machine Learning: ECML-94, European Conference on Machine Learning, Catania, </booktitle> <address> Italy, </address> <month> April </month> <year> 1994, </year> <booktitle> Proceedings, volume 784 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pp. 103-121, </pages> <address> Berlin, </address> <year> 1994. </year> <note> Springer-Verlag. Also as Ar-beitspapiere der GMD No. 821. </note>
Reference-contexts: In this case it can happen that the induce concept description becomes too specific and covers only the examples, but only a few other instances of the goal concept. In order to deal with this problem we have developed a new generalization approach called conceptual-clustering-based generalization (CCG) <ref> [Emde 94] </ref>. This approach first implemented in the system Cola-1 tries to take advantage of the information contained in descriptions of unclassified objects (as a kind of additional background knowledge) using a conceptual-clustering algorithm. <p> In the last example, the definition of class 25 would be the most specific CCG and the description of class 66 would be the most general CCG. Some experiments described in <ref> [Emde 94] </ref> have shown that, in general, the induced description becomes to general if the most-general CCG is used. Therefore, we focus in this paper on the use of most specific CCG's. <p> the knowledge base (while all class descriptions are deleted) and the concept description becomes available to the overall system as well as to following learning processes. 4 Experimental Results Cola has been applied in several real-world domains in order to test if the CCG approach improves upon existing methods (cf. <ref> [Emde 94] </ref>) In the following we report only about experiments conducted in the relational mesh domain from Bojan Dolsak (also available from the ml-archive@gmd.de) and the propositional Cleveland heart disease data set (stored in the UCI Machine Learning data repository) containing numerical attributes. 4.1 Test Domains Ten different FE mesh models <p> In contrast to other programs which are based on the LGG approach, Cilgg allows the user to switch off the prunning of redundant literals. In contrast to the LGG program used in previous experiments <ref> [Emde 94] </ref>, Cilgg constructs a set of Horn clauses to cover the set of positive examples without covering negative examples. <p> The accuracy of the Cilgg results increased from 49% to 59%. Similar to experiments in other domains (see <ref> [Emde 94] </ref>), the accuracy of Cola's learning result improves with the number of descriptions of unclassified objects.
Reference: [Fisher 87] <author> Douglas H. Fisher. </author> <title> Knowledge Acquisition Via Incremental Conceptual Clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-172, </pages> <year> 1987. </year>
Reference-contexts: A class is regarded as meaningful if the organization of knowledge becomes more effective and/or more efficient, e.g., enables a system to infer missing information 2 for partially described objects. Some conceptual clustering programs (e.g., Clus--ter [Michalski/Stepp 83] or Cobweb <ref> [Fisher 87] </ref>) construct a hierarchy of classes, other programs (e.g., Unimem [Lebowitz 87], Kbg [Bisson 92b]) construct a directed acyclic graph of classes, which means that the classes are not necessarily disjoint. Cola uses the conceptual clustering program Sprite to perform the conceptual clustering step.
Reference: [Kietz 94] <author> Jorg-Uwe Kietz. </author> <title> Induktive Analyse von relationalen Daten. </title> <type> Technical report, </type> <institution> Universitat Dortmund, </institution> <year> 1994. </year> <note> to appear. </note>
Reference-contexts: If the maximum depth parameter is set to 0.5, Sprite receives a2: neighbour (a2,a1), fixed (a2), not important (a2), not loaded (a2) as a description of the edge a2. 2 The idea to describe this kind of depth restriction with fractionals has been borrowed from Kietz <ref> [Kietz 94] </ref>. 10 fixed (a2) not important (a2) not loaded (a2) neighbour (a2,a1) Table 4: Facts related to edge a2 up to depth 0.5 without identical sub-structures Such descriptions are constructed for all objects which are classified as positive and/or negative examples of a goal predicate (e.g., mesh1) and all available <p> We chose the learning program Foil-6.1 [Quinlan 90, Cameron-Jones/Quinlan 94] as a representative of systems which produce most discriminant generalizations. As a representative of systems performing most specific generalizations we used Cilgg <ref> [Kietz 94] </ref> which is based on the well-known least general generalization rule of Plotkin [71] . In contrast to other programs which are based on the LGG approach, Cilgg allows the user to switch off the prunning of redundant literals.
Reference: [Kietz/Lubbe 94] <author> Jorg-Uwe Kietz and Marcus Lubbe. </author> <title> An Efficient Subsumbtion Algorith for Inductive Logic Programming. </title> <booktitle> In Proc. 11th Int. Conference on Machine Learning, </booktitle> <year> 1994. </year> <note> to appear. </note>
Reference: [Kietz/Wrobel 92] <author> Jorg-Uwe Kietz and Stefan Wrobel. </author> <title> Controlling the Complexity of Learning through Syntactic and Task-Oriented Models. </title> <editor> In S. Muggleton (ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 107-126. </pages> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: The concept description is supposed to help to determine other instances (and non-instances) of the concept. Some examples of well known systems which are able to learn from examples in relational domains are: Foil [Quinlan 90], Golem [Muggleton/Feng 90], Rdt <ref> [Kietz/Wrobel 92] </ref>, Arch [Winston 75] and Ogust [Vrain 90]. In the literature two kinds of generalized descriptions are distinguished: characteristic descriptions and discriminant descriptions [Dietterich/Michalski 81].
Reference: [Lavrac/Dzeroski 94] <author> Nada Lavrac and Saso Dzeroski. </author> <title> Inductive Logic Programming - Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: If the same number of instances is covered, Cola selects the class whose class description contains the least number of disjuncts. The description is added to the knowledge base (while all class descriptions built by Sprite or Cola are deleted) <ref> [Lavrac/Dzeroski 94, p. 162-172] </ref>. 6 and the concept description becomes available to the overall system as well as to following learning processes. 3 Learning in relational domains Cola is an inductive learning tool in the knowledge acquisition and machine learning system Mobal [Morik et al. 93] and makes use of Mobal's <p> The last fact shown in table 1 means that there is one element on the edge a2. The data set contained in the knowledge base is identical to the data sets used in experiments made by other authors (see, e.g., [Dolsak/Muggelton 92], <ref> [Lavrac/Dzeroski 94] </ref>) except the fact that they usually represented the number of elements on an edge using a two-place predicate, e.g., the last fact is represented by the fact mesh (a2,1).
Reference: [Lebowitz 87] <author> Michael Lebowitz. </author> <title> Experiments with Incremental Concept Formation: </title> <journal> UNIMEM. Machine Learning, </journal> <volume> 2 </volume> <pages> 103-138, </pages> <year> 1987. </year>
Reference-contexts: Some conceptual clustering programs (e.g., Clus--ter [Michalski/Stepp 83] or Cobweb [Fisher 87]) construct a hierarchy of classes, other programs (e.g., Unimem <ref> [Lebowitz 87] </ref>, Kbg [Bisson 92b]) construct a directed acyclic graph of classes, which means that the classes are not necessarily disjoint. Cola uses the conceptual clustering program Sprite to perform the conceptual clustering step.
Reference: [Michalski/Stepp 83] <author> Ryszard S. Michalski and Robert E. Stepp. </author> <title> Learning from Observation: Conceptual Clustering. In R.S. </title> <editor> Michalski, J.G. Carbonell, and T.M. Mitchell (eds.), </editor> <booktitle> Machine Learning, </booktitle> <volume> volume I, </volume> <pages> pp. 331-363. </pages> <publisher> Tioga, </publisher> <address> Palo Alto, CA, </address> <year> 1983. </year>
Reference-contexts: A class is regarded as meaningful if the organization of knowledge becomes more effective and/or more efficient, e.g., enables a system to infer missing information 2 for partially described objects. Some conceptual clustering programs (e.g., Clus--ter <ref> [Michalski/Stepp 83] </ref> or Cobweb [Fisher 87]) construct a hierarchy of classes, other programs (e.g., Unimem [Lebowitz 87], Kbg [Bisson 92b]) construct a directed acyclic graph of classes, which means that the classes are not necessarily disjoint. Cola uses the conceptual clustering program Sprite to perform the conceptual clustering step. <p> Although it has been argued that conjunctive descriptions are one of the most common descriptive forms used by humans <ref> [Michalski/Stepp 83] </ref>, it is clear that disjunctive concepts can also be relevant. An obvious solution to this problem is to select a set of classes, which cover together all positive and exclude all negative examples, and to connect the corresponding class descriptions disjunctively.
Reference: [Morik et al. 93] <author> Katharina Morik, Stefan Wrobel, Jorg-Uwe Kietz, and Werner Emde. </author> <title> Knowledge Acquisition and Machine Learning: Theory Methods and Applications. </title> <publisher> Academic Press, </publisher> <address> London, New York, </address> <year> 1993. </year>
Reference-contexts: descriptions built by Sprite or Cola are deleted) [Lavrac/Dzeroski 94, p. 162-172]. 6 and the concept description becomes available to the overall system as well as to following learning processes. 3 Learning in relational domains Cola is an inductive learning tool in the knowledge acquisition and machine learning system Mobal <ref> [Morik et al. 93] </ref> and makes use of Mobal's knowledge representation environment. This means that Cola uses an extended function-free Horn-clause representation (paraconsistent with negation) [Morik et al. 93, p. 27ff]. The attributes of objects and relations among objects in the domain are described by facts. <p> This means that Cola uses an extended function-free Horn-clause representation (paraconsistent with negation) <ref> [Morik et al. 93, p. 27ff] </ref>. The attributes of objects and relations among objects in the domain are described by facts. A set of facts related to an edge (named a2) in a finite element mesh model [Dolsak/Muggelton 92] of a cylinder is shown as an example in table 1.
Reference: [Muggleton/Feng 90] <author> Stephen Muggleton and Cao Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the 1st Conference on Algorithmic Learning Theory, </booktitle> <year> 1990. </year>
Reference-contexts: The concept description is supposed to help to determine other instances (and non-instances) of the concept. Some examples of well known systems which are able to learn from examples in relational domains are: Foil [Quinlan 90], Golem <ref> [Muggleton/Feng 90] </ref>, Rdt [Kietz/Wrobel 92], Arch [Winston 75] and Ogust [Vrain 90]. In the literature two kinds of generalized descriptions are distinguished: characteristic descriptions and discriminant descriptions [Dietterich/Michalski 81]. <p> The system Cola uses currently a relatively strong language restriction to deal with this problem and to avoid a overly large search space. Using this language restriction Cola is able to learn non-recursive k-clause 1-1 (non)determinate clauses (see <ref> [Muggleton/Feng 90] </ref>) with conclusions described by one-place predicates. This language restriction is realized in Cola in the procedures which construct the input of Sprite. Cola-2 constructs fact chains of the objects to be clustered up to a maximum depth of one. Suppose one of the objects is the edge a2.
Reference: [Plotkin 71] <author> Gordon D. Plotkin. </author> <title> A further note on inductive generalization. </title> <editor> In B. Meltzer and D. Michie (eds.), </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 6, chapter 8, </volume> <pages> pp. 101-124. </pages> <publisher> American Elsevier, </publisher> <year> 1971. </year>
Reference: [Quinlan 90] <author> J. R. Quinlan. </author> <title> Learning Logical Definitions from Relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year> <month> 18 </month>
Reference-contexts: The concept description is supposed to help to determine other instances (and non-instances) of the concept. Some examples of well known systems which are able to learn from examples in relational domains are: Foil <ref> [Quinlan 90] </ref>, Golem [Muggleton/Feng 90], Rdt [Kietz/Wrobel 92], Arch [Winston 75] and Ogust [Vrain 90]. In the literature two kinds of generalized descriptions are distinguished: characteristic descriptions and discriminant descriptions [Dietterich/Michalski 81]. <p> We chose the learning program Foil-6.1 <ref> [Quinlan 90, Cameron-Jones/Quinlan 94] </ref> as a representative of systems which produce most discriminant generalizations. As a representative of systems performing most specific generalizations we used Cilgg [Kietz 94] which is based on the well-known least general generalization rule of Plotkin [71] .
Reference: [Smyth/Mellstrom 92] <author> Padhraic Smyth and Jeff Mellstrom. </author> <title> Detecting Novel Classes with Applications to Fault Diagnosis. </title> <editor> In Derek Sleeman and Peter Edwards (eds.), </editor> <booktitle> Machine Learning proceedings of the Ninth International Workshop (ML92), </booktitle> <pages> pp. 416-425, </pages> <address> Los Altos, CA, 1992. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Unfortunately, in some application domains (e.g., medical domains) it is very difficult to collect examples of all relevant classes, because the number of these classes can be quite large (see also <ref> [Smyth/Mellstrom 92] </ref>). For example, suppose the learning task is to find a description of a particular disease D 1 (e.g., a skin disease). <p> Their approach is also concerned with the problem of learning a classification function which rejects instances of concepts not seen in the learning phase. In <ref> [Smyth/Mellstrom 92] </ref> they discuss the pratical relevance of robust learning results for fault diagnosis and describe an algorithm based on ideas from statistical kernel density estimation and mixture models.
Reference: [Vrain 90] <author> Christel Vrain. OGUST: </author> <title> A system that learns using domain properties expressed as theorems. </title> <editor> In Yves Kodratoff and Ryszard Michalski (eds.), </editor> <booktitle> Machine Learning AnArtificial Intelligence Approach, volume III, chapter 13, </booktitle> <pages> pp. 360-382. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The concept description is supposed to help to determine other instances (and non-instances) of the concept. Some examples of well known systems which are able to learn from examples in relational domains are: Foil [Quinlan 90], Golem [Muggleton/Feng 90], Rdt [Kietz/Wrobel 92], Arch [Winston 75] and Ogust <ref> [Vrain 90] </ref>. In the literature two kinds of generalized descriptions are distinguished: characteristic descriptions and discriminant descriptions [Dietterich/Michalski 81].
Reference: [Winston 75] <author> P. H. Winston. </author> <title> Learning structural descriptions from examples. </title> <editor> In P.H. Winston (ed.), </editor> <booktitle> The Psychology of Computer Vision. </booktitle> <publisher> McGraw-Hill, </publisher> <year> 1975. </year> <month> 19 </month>
Reference-contexts: The concept description is supposed to help to determine other instances (and non-instances) of the concept. Some examples of well known systems which are able to learn from examples in relational domains are: Foil [Quinlan 90], Golem [Muggleton/Feng 90], Rdt [Kietz/Wrobel 92], Arch <ref> [Winston 75] </ref> and Ogust [Vrain 90]. In the literature two kinds of generalized descriptions are distinguished: characteristic descriptions and discriminant descriptions [Dietterich/Michalski 81].
References-found: 21

