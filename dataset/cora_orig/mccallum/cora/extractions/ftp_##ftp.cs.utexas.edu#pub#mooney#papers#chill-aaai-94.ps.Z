URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/chill-aaai-94.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: zelle@cs.utexas.edu, mooney@cs.utexas.edu  
Title: Inducing Deterministic Prolog Parsers from Treebanks: A Machine Learning Approach  
Author: John M. Zelle and Raymond J. Mooney 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas  
Date: 1994  
Note: Appears in Proceedings of the Twelfth National Conference on Artificial Intelligence, pp 748-753, AAI Press/MIT Press,  
Abstract: This paper presents a method for constructing deterministic Prolog parsers from corpora of parsed sentences. Our approach uses recent machine learning methods for inducing Prolog rules from examples (inductive logic programming). We discuss several advantages of this method compared to recent statistical methods and present results on learning complete parsers from portions of the ATIS corpus. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. R. </author> <year> (1977). </year> <title> Induction of augmented transition networks. </title> <journal> Cognitive Science, </journal> <volume> 1 </volume> <pages> 125-157. </pages>
Reference: <author> Berwick, B. </author> <year> (1985). </year> <title> The Acquisition of Syntactic Knowledge. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Black, E., Jelineck, F., Lafferty, J., Magerman, D., Mercer, R., and Roukos, S. </author> <year> (1993). </year> <title> Towards history-based grammars: Using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 31-37. </pages> <address> Columbus, Ohio. </address>
Reference-contexts: Introduction Recent approaches to constructing robust parsers from corpora primarily use statistical and probabilistic methods such as stochastic context-free grammars (Black et al., 1992; Pereira and Schabes, 1992). Although several current methods learn some symbolic structures such as decision trees <ref> (Black et al., 1993) </ref> and transformations (Brill, 1993), statistical methods still dominate. In this paper, we present a method that uses recent techniques in machine learning to construct symbolic, deterministic parsers from parsed corpora (treebanks).
Reference: <author> Black, E., Lafferty, J., and Roukaos, S. </author> <year> (1992). </year> <title> Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 185-192. </pages> <address> Newark, Delaware. </address>
Reference: <author> Black, E. et. al. </author> <year> (1991). </year> <title> A procedure for quantitatively comparing the syntactic coverage of English grammars. </title> <booktitle> In Proceedings of the Fourth DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 306-311. </pages>
Reference-contexts: Another accuracy measure, which has been used in evaluating systems that bracket the input sentence into unlabeled constituents, is the proportion of constituents in the parse that do not cross any constituent boundaries in the correct tree <ref> (Black, 1991) </ref>.
Reference: <author> Brill, E. </author> <year> (1993). </year> <title> Automatic grammar induction and parsing free text: A transformation-based approach. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 259-265. </pages> <address> Columbus, Ohio. </address>
Reference-contexts: Introduction Recent approaches to constructing robust parsers from corpora primarily use statistical and probabilistic methods such as stochastic context-free grammars (Black et al., 1992; Pereira and Schabes, 1992). Although several current methods learn some symbolic structures such as decision trees (Black et al., 1993) and transformations <ref> (Brill, 1993) </ref>, statistical methods still dominate. In this paper, we present a method that uses recent techniques in machine learning to construct symbolic, deterministic parsers from parsed corpora (treebanks). <p> Statistical approaches relying on n-grams or probabilistic context-free grammars would have difficulty due to the large number of terminal symbols (around 400) appearing in the modest-sized training corpus. The data for lexical selection would be too sparse to adequately train the pre-defined models. Likewise, the transformational approach of <ref> (Brill, 1993) </ref> is limited to bracketing strings of lexical classes, not words. A major advantage of our approach is the ability of the learning mechanism to automatically construct and attend to just those features of the input that are most useful in guiding parsing.
Reference: <author> Charniak, E. </author> <year> (1993). </year> <title> Statistical Language Learning. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Related Work As mentioned above, most recent work on automatically constructing parsers from corpora has focused on acquiring stochastic grammars rather than symbolic parsers. When learning in this framework, "one simply gathers statistics" to set the parameters of a predefined model <ref> (Charniak, 1993) </ref>. However, there is a long tradition of research in AI and Machine Learning suggesting the utility of techniques that extract underlying structural models from the data.
Reference: <author> Hindle, D. and Rooth, M. </author> <year> (1993). </year> <title> Structural ambiguity and lexical relations. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 103-120. </pages>
Reference-contexts: There is some existing work on learning lexical classes from corpora (Schutze, 1992); however, the classes are based on word co-occurrence rather than the specific needs of parsing. There are also meth-ods for learning to resolve attachments using lexical information <ref> (Hindle and Rooth, 1993) </ref>; however, they do not create new lexical classes. Chill uses a single learning algorithm to perform both of these tasks. Conclusion This paper has demonstrated that modern machine-learning methods are capable of inducing traditional shift-reduce parsers from corpora, complementing the results of recent statistical methods.
Reference: <author> Kijsirikul, B., Numao, M., and Shimura, M. </author> <year> (1992). </year> <title> Discrimination-based constructive induction of logic programs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 44-49. </pages> <address> San Jose, CA. </address>
Reference: <author> Marcus, M. </author> <year> (1980). </year> <title> A Theory of Syntactic Recognition for Natural Language. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Marcus, M., Santorini, B., and Marcinkiewicz, M. </author> <year> (1993). </year> <title> Building a large annotated corpus of english: The Penn treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330. </pages>
Reference-contexts: In section 2, we summarize our ILP method for learning deterministic parsers, and how this method was tailored to work with existing treebanks. In section 3, we present and discuss experimental results on learning parsers from the ATIS corpus of the Penn Treebank <ref> (Marcus et al., 1993) </ref>. Section 4 covers related work, and section 5 presents our conclusions. The Chill System Overview Our system, Chill, (Constructive Heuristics Induction for Language Learning) is an approach to parser acquisition which utilizes a general learning mechanism. <p> The parser is then specialized by introducing search-control heuristics. These control heuristics limit the contexts in 1 In an untagged treebank, parses are represented as phrase-level word groupings without lexical categories dominating the words (e.g. parsed text from Penn Treebank <ref> (Marcus et al., 1993) </ref>) which certain operations are performed, eliminating the spurious analyses. Constructing the Overly-General Parser The syntactic parse of a sentence is a labeled bracketing of the words in the sentence. <p> Parsing the Treebank Training a program to do accurate parsing requires large corpora of parsed text for training. Fortunately, such treebanks are being compiled and becoming available. For the current experiments, we have used parsed text from a preliminary version of the Penn Treebank <ref> (Marcus et al., 1993) </ref>. One complication in using this data is that sentences are parsed only to the "phrase level", leaving the internal structure of NPs unanalyzed and allowing arbitrary-arity constituents. Rather than forcing the parser to learn reductions for arbitrary length constituents, Chill was restricted to learning binary-branching structures.
Reference: <author> Muggleton, S. and Buntine, W. </author> <year> (1988). </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 339-352. </pages>
Reference-contexts: This control rule comprises a Horn-clause definition that covers the positive control examples for the operator but not the negative. There is a growing body of research in inductive logic programming which addresses this problem. Chill combines elements from bottom-up techniques found in systems such as Cigol <ref> (Muggleton and Buntine, 1988) </ref> and Golem (Muggle-ton and Feng, 1992) and top-down methods from systems like Foil (Quinlan, 1990), and is able to invent new predicates in a manner analogous to Champ (Ki-jsirikul et al., 1992).
Reference: <institution> Ann Arbor, MI. </institution>
Reference: <author> Muggleton, S. and Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. </title> <editor> In Muggleton, S., editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 281-297. </pages> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Muggleton, S. H., </author> <title> editor (1992). </title> <booktitle> Inductive Logic Programming. </booktitle> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference: <author> Pereira, F. and Schabes, Y. </author> <year> (1992). </year> <title> Inside-outside rees-timation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 128-135. </pages> <address> Newark, Delaware. </address>
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266. </pages>
Reference-contexts: There is a growing body of research in inductive logic programming which addresses this problem. Chill combines elements from bottom-up techniques found in systems such as Cigol (Muggleton and Buntine, 1988) and Golem (Muggle-ton and Feng, 1992) and top-down methods from systems like Foil <ref> (Quinlan, 1990) </ref>, and is able to invent new predicates in a manner analogous to Champ (Ki-jsirikul et al., 1992). Details of the Chill induction algorithm can be found in (Zelle and Mooney, 1993b; Zelle and Mooney, 1994).
Reference: <author> Schutze, H. </author> <year> (1992). </year> <title> Context space. </title> <booktitle> In Working Notes, AAAI Fall Symposium Series, </booktitle> <pages> pages 113-120. </pages> <month> AAAI-Press. </month>
Reference-contexts: Chill's ability to invent new classes of words and phrases specifically for resolving ambiguities such as prepositional phrase attachment makes it particularly interesting. There is some existing work on learning lexical classes from corpora <ref> (Schutze, 1992) </ref>; however, the classes are based on word co-occurrence rather than the specific needs of parsing. There are also meth-ods for learning to resolve attachments using lexical information (Hindle and Rooth, 1993); however, they do not create new lexical classes.
Reference: <author> Simmons, R. F. and Yu, Y. </author> <year> (1992). </year> <title> The acquisition and use of context dependent grammars for English. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 391-418. </pages>
Reference-contexts: Chill, which creates fully labeled parses, has a more general learning mechanism allowing it to make distinctions based on more subtle structural and lexical cues (e.g. creating semantic word classes for resolving attachment). Our framework for learning deterministic, context-dependent parsers is very similar to that of <ref> (Simmons and Yu, 1992) </ref>; however, there are two advantages of our ILP method compared to their exemplar matching method. First, ILP methods can handle unbounded, structured data so that the context does not need to be fixed to a limited window of the stack and the remaining sentence.
Reference: <author> Tomita, M. </author> <year> (1986). </year> <title> Efficient Parsing for Natural Language. </title> <address> Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: We believe our approach offers several potential advantages compared to current methods. First, it constructs deterministic shift-reduce parsers, which are very powerful and efficient <ref> (Tomita, 1986) </ref> and arguably more cognitively plausible (Marcus, 1980; Berwick, 1985).
Reference: <author> Zelle, J. M. and Mooney, R. J. </author> <year> (1993a). </year> <title> Combining FOIL and EBG to speed-up logic programs. </title> <booktitle> In Proceedings of the Thirteenth International Joint conference on Artificial intelligence, </booktitle> <pages> pages 1106-1111. </pages> <address> Chambery, France. </address>
Reference-contexts: The program must be specialized by including control heuristics that guide the application of operator clauses. This section outlines the basic approach used in Chill. More detail on incorporating clause selection information in Prolog programs can be found in <ref> (Zelle and Mooney, 1993a) </ref>. Program specialization occurs in three phases. First, the training examples are analyzed to construct positive and negative control examples for each operator clause.
Reference: <author> Zelle, J. M. and Mooney, R. J. </author> <year> (1993b). </year> <title> Learning semantic grammars with constructive inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 817-822. </pages> <address> Washington, D.C. </address>
Reference-contexts: In this paper, we present a method that uses recent techniques in machine learning to construct symbolic, deterministic parsers from parsed corpora (treebanks). Specifically, our approach is implemented in a program called Chill <ref> (Zelle and Mooney, 1993b) </ref> that uses inductive logic programming (ILP) (Muggle-ton, 1992) to learn heuristic rules for controlling a deterministic shift-reduce parser written in Prolog. We believe our approach offers several potential advantages compared to current methods. <p> It can also learn to produce case-role assignments instead of syntactic parse trees and can use learned lexical and semantic classes to resolve ambiguities such as prepositional phrase attachment and lexical ambiguity <ref> (Zelle and Mooney, 1993b) </ref>. Fourth, it uses a single, uniform parsing framework to perform all of these tasks and a single, general learning method that has also been used to induce a range of diverse logic programs from examples (Zelle and Mooney, 1994).
Reference: <author> Zelle, J. M. and Mooney, R. J. </author> <year> (1994). </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle> <address> New Brunswick, NJ. </address>
Reference-contexts: Fourth, it uses a single, uniform parsing framework to perform all of these tasks and a single, general learning method that has also been used to induce a range of diverse logic programs from examples <ref> (Zelle and Mooney, 1994) </ref>. The remainder of the paper is organized as follows. In section 2, we summarize our ILP method for learning deterministic parsers, and how this method was tailored to work with existing treebanks.
References-found: 23

