URL: http://www.cs.bu.edu/techreports/93-006-mermera3.ps.Z
Refering-URL: http://cs-www.bu.edu/techreports/Home.html
Root-URL: 
Title: An Implementation of Mermera: A Shared Memory System that Mixes Coherence with Non-coherence  
Author: Abdelsalam Heddaya Himanshu Sinha 
Keyword: Distributed Shared Memory, Weak consistency, Parallel Computing, Asynchronous Iterative Methods, Isis.  
Note: This research was supported in part by NSF under grants IRI-8910195, IRI-9041581, CCR-8901647 and CDA 8920936.  
Address: Boston, MA 02215  
Affiliation: Boston University Computer Science Department  
Pubnum: (Supersedes 92-013)  
Email: heddaya@cs.bu.edu  hss@cs.bu.edu  
Phone: Phone: 617 353-8919 Fax: 617 353-6457  
Date: June 4, 1993  
Abstract: Coherent shared memory is a convenient, but inefficient, method of inter-process communication for parallel programs. By contrast, message passing can be less convenient, but more efficient. To get the benefits of both models, several non-coherent memory behaviors have recently been proposed in the literature. We present an implementation of Mermera, a shared memory system that supports both coherent and non-coherent behaviors in a manner that enables programmers to mix multiple behaviors in the same program [HS93]. A programmer can debug a Mermera program using coherent memory, and then improve its performance by selectively reducing the level of coherence in the parts that are critical to performance. Mermera permits a trade-off of coherence for performance. We analyze this trade-off through measurements of our implementation, and by an example that illustrates the style of programming needed to exploit non-coherence. We find that, even on a small network of workstations, the performance advantage of non-coherence is compelling. Raw non-coherent memory operations perform 20-40 times better than non-coherent memory operations. An example aplication program is shown to run 5-11 times faster when permitted to exploit non-coherence. We conclude by commenting on our use of the Isis Toolkit of multicast protocols in implementing Mermera. 
Abstract-found: 1
Intro-found: 1
Reference: [AF92] <author> H. Attiya and R. Friedman. </author> <title> A correctness condition for high-performance multiprocessors. </title> <type> Technical Report 719, </type> <institution> Technion|Israel Institute of Technology, Department of Computer Science, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Our approach has been to provide the programmer with a choice of a wide variety of cleanly related memory operations satisfying different ordering constraints, thereby giving him an opportunity to trade off programming simplicity for performance. In <ref> [AF92] </ref>, a similar approach has been taken in the specification of hybrid consistency, which defines a hybrid memory behavior consisting of one coherent behavior and one non-coherent behavior.
Reference: [AH90] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering|a new definition. </title> <booktitle> In Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May 28-31 </month> <year> 1990. </year>
Reference-contexts: In response, several non-coherent memories have been proposed [LS88, HA90, WW90]. In [HS92] we develop a formalism to describe all these memories. The architecture community has also recognized the need to relax consistency constraints for shared memory multiprocessors, resulting in the proposals given in <ref> [AH90, GLL + 90, Dub90, SD88] </ref>. Systems implementing some of these weaker constraints include PLUS [BR90]. The DASH multiprocessor [LLJ + 93] and the Munin software shared memory system [CBZ91] allow sequential consistency to be broken, but only during the execution of code blocks explicitly marked by the programmer.
Reference: [AHJ90] <author> Mustaque Ahamad, Philip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal distributed shared memory. </title> <type> Technical Report GIT-CC-90-49, </type> <institution> Georgia Institute of Technology, School of Information and Computer Science, </institution> <year> 1990. </year>
Reference-contexts: This class of algorithms can be used to solve problems such as simultaneous linear equations (section 5 below) and finding shortest paths in graphs [Roo93]. Causal memory, which is not supported in Mermera, can run additional applications, including the travelling salesman problem, and the dictionary problem <ref> [AHJ90] </ref>. Mermera allows the programmer to mix different memory behaviors during the same computation, simply by invoking the appropriate operation to access different memory locations at different times. We provide four kinds of write operations to shared memory: CO Write, PRAM Write, Slow Write and Local Write.
Reference: [Bau78] <author> Gerard M. Baudet. </author> <title> Asynchronous iterative methods for multiprocessors. </title> <journal> J. ACM, </journal> <volume> 25(2) </volume> <pages> 226-244, </pages> <month> April </month> <year> 1978. </year>
Reference-contexts: A call to isis accept events () enables pending updates from other processes to be applied to the local copy of shared memory and also for events like timeouts to be handled. 14 The potential performance advantage of asynchronous iterative methods was first established by Baudet in <ref> [Bau78] </ref>.
Reference: [BDG + 91] <author> Adam Beguelin, Jack Dongarra, Al Geist, Robert Mancheck, and Vaidy Sunderam. </author> <title> A user's guide to PVM Parallel Virtual Machine. </title> <type> Technical Report ORNL/TM-11826, </type> <institution> Oak Ridge National Laboratory, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction In recent years several attempts <ref> [BDG + 91, LW89] </ref> have been made to harness today's high performance networked workstations for parallel computing. Most of the work in this area uses the message passing method of inter-process communication.
Reference: [BJ87] <author> K. Birman and T.A. Joseph. </author> <title> Exploiting virtual synchrony in distributed systems. </title> <booktitle> In Proc. 11th ACM Symp. on Operating System Principles, </booktitle> <address> Austin, Texas, </address> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: A read operation returns the value in the local copy. A write operation updates the local copy and propagates the value to other processes, using version 2.2.5 of the Isis toolkit of multicast protocols <ref> [BSS91, BJ87] </ref>. The exact manner of transmission depends on the type of the write operation. The specification of Mermera does not require that all writes be propagated to other processes. Only CO Writes and PRAM Writes are guaranteed to be propagated to all processes. <p> The weaker ordering constraints on non-coherent operations also allow us to buffer multiple writes and propagate them together. Writes are propagated by broadcasting the values to other processes. We use the Isis toolkit <ref> [BSS91, BJ87] </ref> for our implementation because it gives us a suite of broadcasts to groups of processes which satisfy different ordering properties. The broadcasts of interest to us are abcast (), f bcast () and mbcast ().
Reference: [BR90] <author> Roberto Bisiani and Mosur Ravishankar. </author> <title> Programming the PLUS distributed-memory system. </title> <booktitle> In Proc. 5th Distributed Memory Computing Conference, </booktitle> <month> April 8-12 </month> <year> 1990. </year>
Reference-contexts: In [HS92] we develop a formalism to describe all these memories. The architecture community has also recognized the need to relax consistency constraints for shared memory multiprocessors, resulting in the proposals given in [AH90, GLL + 90, Dub90, SD88]. Systems implementing some of these weaker constraints include PLUS <ref> [BR90] </ref>. The DASH multiprocessor [LLJ + 93] and the Munin software shared memory system [CBZ91] allow sequential consistency to be broken, but only during the execution of code blocks explicitly marked by the programmer.
Reference: [BSS91] <author> Kenneth Birman, Andre Schiper, and Pat Stephenson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <month> Aug. </month> <year> 1991. </year> <month> 19 </month>
Reference-contexts: A read operation returns the value in the local copy. A write operation updates the local copy and propagates the value to other processes, using version 2.2.5 of the Isis toolkit of multicast protocols <ref> [BSS91, BJ87] </ref>. The exact manner of transmission depends on the type of the write operation. The specification of Mermera does not require that all writes be propagated to other processes. Only CO Writes and PRAM Writes are guaranteed to be propagated to all processes. <p> The weaker ordering constraints on non-coherent operations also allow us to buffer multiple writes and propagate them together. Writes are propagated by broadcasting the values to other processes. We use the Isis toolkit <ref> [BSS91, BJ87] </ref> for our implementation because it gives us a suite of broadcasts to groups of processes which satisfy different ordering properties. The broadcasts of interest to us are abcast (), f bcast () and mbcast (). <p> More efficient implementations such as directory 13 In general, CO Write access times are consistent with the performance of Isis abcast for small packets <ref> [BSS91] </ref>. 11 based schemes (see [CKA91] for an example) exist. Our response to this argument is that there are some applications (e.g., the linear solver of section 5) which intrinsically generate a message traffic that is similar to the traffic generated by using full replication to implement shared memory.
Reference: [BT89] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: done on a best effort basis rather than in a reliable manner as is done in our current implementation. 5 Solving a System of Equations In this section we show how to use all the behaviors of Mermera to solve a system of linear equations using an asynchronous iterative method <ref> [BT89] </ref> 14 . Consider a system Ax + b = 0 of linear equations in n variables.
Reference: [CBZ91] <author> John B. Carter, John K. Bennett, and Willy Zwaenopoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proc. 13th ACM Symp. on Operating System Principles, Asilomar, California, </booktitle> <pages> pages 152-164, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Systems implementing some of these weaker constraints include PLUS [BR90]. The DASH multiprocessor [LLJ + 93] and the Munin software shared memory system <ref> [CBZ91] </ref> allow sequential consistency to be broken, but only during the execution of code blocks explicitly marked by the programmer. In [HS93] we summarize our formalism and describe the design and performance of a pilot implementation of our system, Mermera 2 , on a BBN TC2000 distributed memory parallel machine.
Reference: [CKA91] <author> David Chaiken, John Kubiatowics, and Anant Agarwal. </author> <title> LimitLESS directories: a scalable cache coherence scheme. </title> <booktitle> In Proc. 4th ACM Intl. Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, California, </address> <pages> pages 224-234, </pages> <month> Apr. </month> <year> 1991. </year> <note> Describes the Alewife machine. </note>
Reference-contexts: More efficient implementations such as directory 13 In general, CO Write access times are consistent with the performance of Isis abcast for small packets [BSS91]. 11 based schemes (see <ref> [CKA91] </ref> for an example) exist. Our response to this argument is that there are some applications (e.g., the linear solver of section 5) which intrinsically generate a message traffic that is similar to the traffic generated by using full replication to implement shared memory.
Reference: [DC90] <author> S. E. Deering and D. R. Cheriton. </author> <title> Multicast routing in datagram internetworks and extended LANs. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 8(2), </volume> <month> May </month> <year> 1990. </year>
Reference-contexts: Use of IP Multicast: The current version of Isis implements a multicast to a group by sending a point-to-point message to each member of the group. This causes the CPU overhead for sending a message to be paid for each member of a group. The Deering multicast protocol <ref> [DC90] </ref> can use the multicast facilities provided by the hardware. As a result, the CPU overhead for a multicast can be greatly reduced. An added benefit is that the elapsed time for a multicast is reduced.
Reference: [Dub90] <author> Michel Dubois. </author> <title> Delayed consistency protocols. </title> <type> Technical Report CENG 90-21, </type> <institution> Electrical Engineering-Systems Department, University of Southern California, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: In response, several non-coherent memories have been proposed [LS88, HA90, WW90]. In [HS92] we develop a formalism to describe all these memories. The architecture community has also recognized the need to relax consistency constraints for shared memory multiprocessors, resulting in the proposals given in <ref> [AH90, GLL + 90, Dub90, SD88] </ref>. Systems implementing some of these weaker constraints include PLUS [BR90]. The DASH multiprocessor [LLJ + 93] and the Munin software shared memory system [CBZ91] allow sequential consistency to be broken, but only during the execution of code blocks explicitly marked by the programmer.
Reference: [GLL + 90] <author> Kourosh Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proc. 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: In response, several non-coherent memories have been proposed [LS88, HA90, WW90]. In [HS92] we develop a formalism to describe all these memories. The architecture community has also recognized the need to relax consistency constraints for shared memory multiprocessors, resulting in the proposals given in <ref> [AH90, GLL + 90, Dub90, SD88] </ref>. Systems implementing some of these weaker constraints include PLUS [BR90]. The DASH multiprocessor [LLJ + 93] and the Munin software shared memory system [CBZ91] allow sequential consistency to be broken, but only during the execution of code blocks explicitly marked by the programmer.
Reference: [HA90] <author> P.W. Hutto and M. Ahamad. </author> <title> Slow memory: weakening consistency to enhance con-currency in distributed shared memories. </title> <booktitle> In Proc. 10th IEEE Intl. Conference on Distributed Computing Systems, </booktitle> <address> Paris, France, </address> <month> June </month> <year> 1990. </year> <note> Also available as Georgia Institute of Technology technical report GIT-ICS-89/39. </note>
Reference-contexts: Lipton and Sandberg [LS88] observed that it is impossible to have coherent shared memory whose access time is less than linear in the worst case delay of the communication network connecting the processors. In response, several non-coherent memories have been proposed <ref> [LS88, HA90, WW90] </ref>. In [HS92] we develop a formalism to describe all these memories. The architecture community has also recognized the need to relax consistency constraints for shared memory multiprocessors, resulting in the proposals given in [AH90, GLL + 90, Dub90, SD88]. <p> A complete formal description can be found in [Sin93]. Our system combines the behaviors of Coherent Memory [HS92], Pipelined RAM [LS88], Slow Memory <ref> [HA90] </ref> and Locally Consistent Memory [HS92]. Our model consists of processes sharing a region of their address space. These processes may be running on different processors.
Reference: [HS92] <author> Abdelsalam Heddaya and Himanshu S. Sinha. </author> <title> Coherence, non-coherence and Local Consistency in distributed shared memory for parallel computing. </title> <type> Technical Report BU-CS-92-004, </type> <institution> Boston University, Computer Sciene Dept., </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Lipton and Sandberg [LS88] observed that it is impossible to have coherent shared memory whose access time is less than linear in the worst case delay of the communication network connecting the processors. In response, several non-coherent memories have been proposed [LS88, HA90, WW90]. In <ref> [HS92] </ref> we develop a formalism to describe all these memories. The architecture community has also recognized the need to relax consistency constraints for shared memory multiprocessors, resulting in the proposals given in [AH90, GLL + 90, Dub90, SD88]. Systems implementing some of these weaker constraints include PLUS [BR90]. <p> Mermeros is an ancient Greek name meaning care laden. So far as we know, Mermera is a new word. 1 In section 2 we informally describe the different behaviors supported by Mermera and how these behaviors are combined. A complete formal description using partial orders can be found in <ref> [HS92] </ref>. Section 3 presents an implementation of Mermera on a network of workstations. Section 4 gives the raw performance of our prototype. <p> A complete formal description can be found in [Sin93]. Our system combines the behaviors of Coherent Memory <ref> [HS92] </ref>, Pipelined RAM [LS88], Slow Memory [HA90] and Locally Consistent Memory [HS92]. Our model consists of processes sharing a region of their address space. These processes may be running on different processors. <p> A complete formal description can be found in [Sin93]. Our system combines the behaviors of Coherent Memory <ref> [HS92] </ref>, Pipelined RAM [LS88], Slow Memory [HA90] and Locally Consistent Memory [HS92]. Our model consists of processes sharing a region of their address space. These processes may be running on different processors. <p> Consider a system Ax + b = 0 of linear equations in n variables. To solve it, we want to find the fixed point of the following iteration. x i = (b i + j=1 m X a ij fi x j )=a ii In <ref> [HS92] </ref> we proved that Slow memory is sufficient for the convergence of this asynchronous iteration provided A and b satisfy certain numerical conditions. But we need stronger behavior of the memory for detecting the convergence of the iteration. Our algorithm is shown in figure 4.
Reference: [HS93] <author> A. Heddaya and Himanshu Sinha. </author> <title> Computing with non-coherent shared memory. </title> <type> Technical Report TBA, </type> <institution> Boston University, Computer Sciene Dept., </institution> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Systems implementing some of these weaker constraints include PLUS [BR90]. The DASH multiprocessor [LLJ + 93] and the Munin software shared memory system [CBZ91] allow sequential consistency to be broken, but only during the execution of code blocks explicitly marked by the programmer. In <ref> [HS93] </ref> we summarize our formalism and describe the design and performance of a pilot implementation of our system, Mermera 2 , on a BBN TC2000 distributed memory parallel machine.
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes mul-tiprocess programs. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> Sep. </month> <year> 1979. </year>
Reference-contexts: x is said to be observed by process i if the value returned by a read by process i is the value written by x:w or is a value that may have been influenced by x:w. 5 Sinha [Sin93] shows that coherent memory is equivalent to Lamport's sequentially consistent memory <ref> [Lam79] </ref>. 2 cesses, i.e., if a process performs w 1 followed by w 2 , then no process can read them in the reverse order. Writes by different processes may be interleaved in different orders by different processes. <p> Locally Consistent Memory: It appears to each process that all the events it observes are executed on a single processor in an order consistent with its program. This is much weaker than sequentially consistent memory <ref> [Lam79] </ref> in which all events appear to be executed on a single processor in an order consistent with every process' program. Several classes of parallel programs have been shown to run correctly on different non-coherent memory behaviors, potentially benefitting from the superior performance of these memories.
Reference: [LH89] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> Nov. </month> <year> 1989. </year>
Reference-contexts: An alternative to this is the shared memory paradigm, which has the advantage that it hides the number and identities of processors from the programmer, thereby easing the programming task. Attempts at providing a shared memory interface <ref> [LH89, RAK88] </ref> have concentrated on keeping the memory coherent 1 . Lipton and Sandberg [LS88] observed that it is impossible to have coherent shared memory whose access time is less than linear in the worst case delay of the communication network connecting the processors.
Reference: [LLJ + 93] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH prototype: logic overhead and performance. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: The architecture community has also recognized the need to relax consistency constraints for shared memory multiprocessors, resulting in the proposals given in [AH90, GLL + 90, Dub90, SD88]. Systems implementing some of these weaker constraints include PLUS [BR90]. The DASH multiprocessor <ref> [LLJ + 93] </ref> and the Munin software shared memory system [CBZ91] allow sequential consistency to be broken, but only during the execution of code blocks explicitly marked by the programmer.
Reference: [LS88] <author> R.J. Lipton and J.S. Sandberg. </author> <title> PRAM: a scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton Univ., Dept. of Computer Science, </institution> <month> Sep. </month> <year> 1988. </year>
Reference-contexts: Attempts at providing a shared memory interface [LH89, RAK88] have concentrated on keeping the memory coherent 1 . Lipton and Sandberg <ref> [LS88] </ref> observed that it is impossible to have coherent shared memory whose access time is less than linear in the worst case delay of the communication network connecting the processors. In response, several non-coherent memories have been proposed [LS88, HA90, WW90]. <p> Lipton and Sandberg [LS88] observed that it is impossible to have coherent shared memory whose access time is less than linear in the worst case delay of the communication network connecting the processors. In response, several non-coherent memories have been proposed <ref> [LS88, HA90, WW90] </ref>. In [HS92] we develop a formalism to describe all these memories. The architecture community has also recognized the need to relax consistency constraints for shared memory multiprocessors, resulting in the proposals given in [AH90, GLL + 90, Dub90, SD88]. <p> A complete formal description can be found in [Sin93]. Our system combines the behaviors of Coherent Memory [HS92], Pipelined RAM <ref> [LS88] </ref>, Slow Memory [HA90] and Locally Consistent Memory [HS92]. Our model consists of processes sharing a region of their address space. These processes may be running on different processors. <p> Several classes of parallel programs have been shown to run correctly on different non-coherent memory behaviors, potentially benefitting from the superior performance of these memories. Lip-ton and Sandberg show in <ref> [LS88] </ref> that PRAM can be used to solve a large number of applications like FFT, matrix-vector product, matrix-matrix product, dynamic programming and other computations that are in the large class of oblivious computations 6 . One of the authors has shown, 6 According to [LS88] "A computation is oblivious if its <p> Lip-ton and Sandberg show in <ref> [LS88] </ref> that PRAM can be used to solve a large number of applications like FFT, matrix-vector product, matrix-matrix product, dynamic programming and other computations that are in the large class of oblivious computations 6 . One of the authors has shown, 6 According to [LS88] "A computation is oblivious if its data motion and the operations it executes at a given step are independent of the actual values of data." 3 in [Sin93], that the class of asynchronous iterative algorithms, tolerates Slow memory.
Reference: [LW89] <author> Jerrold S. Leichter and Robert A. Whiteside. </author> <title> Implementing Linda for distributed and parallel processing. </title> <type> Technical Report YALEU/DCS/TR-715, </type> <institution> Yale Univ., Dept. of Computer Science, </institution> <month> April </month> <year> 1989. </year> <month> 20 </month>
Reference-contexts: 1 Introduction In recent years several attempts <ref> [BDG + 91, LW89] </ref> have been made to harness today's high performance networked workstations for parallel computing. Most of the work in this area uses the message passing method of inter-process communication.
Reference: [RAK88] <author> Umakishore Ramachandran, Mustaque Ahamad, and M. Yousef A. Khalidi. </author> <title> Unifying synchronization and data transfer in mantaining coherence of distributed shared memory. </title> <type> Technical Report GIT-ICS-88/23, </type> <institution> School of Information and Computer Science, Georgia Institute of Technology, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: An alternative to this is the shared memory paradigm, which has the advantage that it hides the number and identities of processors from the programmer, thereby easing the programming task. Attempts at providing a shared memory interface <ref> [LH89, RAK88] </ref> have concentrated on keeping the memory coherent 1 . Lipton and Sandberg [LS88] observed that it is impossible to have coherent shared memory whose access time is less than linear in the worst case delay of the communication network connecting the processors.
Reference: [Roo93] <author> Nicholas M. Roosevelt. </author> <title> An implementation of mermera on a thinking machines cm-5. </title> <type> Master's thesis, </type> <institution> Boston University, Computer Sciene Dept., </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: This class of algorithms can be used to solve problems such as simultaneous linear equations (section 5 below) and finding shortest paths in graphs <ref> [Roo93] </ref>. Causal memory, which is not supported in Mermera, can run additional applications, including the travelling salesman problem, and the dictionary problem [AHJ90]. Mermera allows the programmer to mix different memory behaviors during the same computation, simply by invoking the appropriate operation to access different memory locations at different times.
Reference: [SD88] <author> C. Scheurich and M. Dubois. </author> <title> Concurrent miss resolution in multiprocessor caches. </title> <booktitle> In Proc. of the 1988 International Conference on Parallel Processing, </booktitle> <month> August 15-19 </month> <year> 1988. </year>
Reference-contexts: In response, several non-coherent memories have been proposed [LS88, HA90, WW90]. In [HS92] we develop a formalism to describe all these memories. The architecture community has also recognized the need to relax consistency constraints for shared memory multiprocessors, resulting in the proposals given in <ref> [AH90, GLL + 90, Dub90, SD88] </ref>. Systems implementing some of these weaker constraints include PLUS [BR90]. The DASH multiprocessor [LLJ + 93] and the Munin software shared memory system [CBZ91] allow sequential consistency to be broken, but only during the execution of code blocks explicitly marked by the programmer.
Reference: [Sin93] <author> Himanshu Sinha. Mermera: </author> <title> Non-coherent Distributed Shared Memory for Parallel Computing. </title> <type> PhD thesis, </type> <institution> Boston University, Computer Science Department, </institution> <address> 111 Cum-mington Street, Boston, MA 02215, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: We discuss how our implementation can be optimized in section 7. 2 Mermera's Memory Behavior In this section we briefly describe each of the memory behaviors supported by Mermera and explain how the behaviors are combined. A complete formal description can be found in <ref> [Sin93] </ref>. Our system combines the behaviors of Coherent Memory [HS92], Pipelined RAM [LS88], Slow Memory [HA90] and Locally Consistent Memory [HS92]. Our model consists of processes sharing a region of their address space. These processes may be running on different processors. <p> 3 Defined in section 4. 4 Informally, a write x:w to location x is said to be observed by process i if the value returned by a read by process i is the value written by x:w or is a value that may have been influenced by x:w. 5 Sinha <ref> [Sin93] </ref> shows that coherent memory is equivalent to Lamport's sequentially consistent memory [Lam79]. 2 cesses, i.e., if a process performs w 1 followed by w 2 , then no process can read them in the reverse order. Writes by different processes may be interleaved in different orders by different processes. <p> One of the authors has shown, 6 According to [LS88] "A computation is oblivious if its data motion and the operations it executes at a given step are independent of the actual values of data." 3 in <ref> [Sin93] </ref>, that the class of asynchronous iterative algorithms, tolerates Slow memory. This class of algorithms can be used to solve problems such as simultaneous linear equations (section 5 below) and finding shortest paths in graphs [Roo93].
Reference: [vRBC + 92] <author> Robbert van Renesse, Ken Birman, Rober Cooper, Bradford Glade, and Patrick Stephenson. </author> <title> Reliable multicast between microkernels. </title> <booktitle> In Proceedings of the USENIX workshop on Micro-Kernels and Other Kernel Architectures, </booktitle> <address> Seattle, Washington, </address> <pages> pages 269-283, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: All writes will benefit from this enhancement and this will also result in a better utilization of the network bandwidth because there will be fewer messages on the network for each write done by any process. The Horus project <ref> [vRBC + 92] </ref> addresses this issue. Our approach in this implementation was to use Isis multicast primitives that approximately satisfy the conditions required by the different kinds of write operations.
Reference: [WW90] <author> W. E. Weihl and Paul Wang. </author> <title> Multi-version memory: Software cache management for concurrent B-trees. </title> <booktitle> In Proc. IEEE Symposium on Parallel and Distributed Systems, </booktitle> <address> Dallas, Texas, </address> <month> Dec. </month> <year> 1990. </year> <month> 21 </month>
Reference-contexts: Lipton and Sandberg [LS88] observed that it is impossible to have coherent shared memory whose access time is less than linear in the worst case delay of the communication network connecting the processors. In response, several non-coherent memories have been proposed <ref> [LS88, HA90, WW90] </ref>. In [HS92] we develop a formalism to describe all these memories. The architecture community has also recognized the need to relax consistency constraints for shared memory multiprocessors, resulting in the proposals given in [AH90, GLL + 90, Dub90, SD88].
References-found: 28

