URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/window-of-vulnerability.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/window-of-vulnerability.html
Root-URL: 
Title: Closing the Window of Vulnerability in Multiphase Memory Transactions  
Author: John Kubiatowicz, David Chaiken, and Anant Agarwal 
Address: Cambridge, Massachusetts 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Multiprocessor architects have begun to explore several mechanisms such as prefetching, context-switching and software-assisted dynamic cache-coherence, which transform single-phase memory transactions in conventional memory systems into multiphase operations. Multiphase operations introduce a window of vulnerability in which data can be invalidated before it is used. Losing data due to invalidations introduces damaging livelock situations. This paper discusses the origins of the window of vulnerability and proposes an architectural framework that closes it. The framework is implemented in Alewife, a large-scale multiprocessor being built at MIT. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: This paper investigates such a unifying framework, and explores one consequence, the window of vulnerability. Although we have implemented the complete framework in the MIT Alewife machine <ref> [1] </ref>, mechanisms can be mixed and matched; other multiprocessor designers may choose to implement a subset of this framework that suits their own needs. Many of the mechanisms associated with shared memory attempt to address a central problem: access to global memory may require a large number of cycles.
Reference: [2] <author> David V. James, Anthony T. Laundrie, Stein Gjessing, and Gurindar S. Sohi. </author> <title> Distributed-Directory Scheme: Scalable Coherent Interface. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 74-77, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: That is, after a first-time fetch of data from a remote node, subsequent accesses of the data are satisfied entirely within the node. The resulting cache coherence problem can be solved using a variety of directory based schemes <ref> [2, 3, 4] </ref>. In a cache-based system, memory and processor resources are wasted if no processing is done while waiting for memory transactions to complete. Such transactions include first-time data fetches and invalidations required to enforce coherence.
Reference: [3] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: That is, after a first-time fetch of data from a remote node, subsequent accesses of the data are satisfied entirely within the node. The resulting cache coherence problem can be solved using a variety of directory based schemes <ref> [2, 3, 4] </ref>. In a cache-based system, memory and processor resources are wasted if no processing is done while waiting for memory transactions to complete. Such transactions include first-time data fetches and invalidations required to enforce coherence. <p> The use of a transaction buffer architecture has been presented in several milieux, such as lockup-free caching [13], victim caching [14], and the remote-access cache of the DASH multiprocessor <ref> [3] </ref>. The need for an associative match on the address stems from several factors. First, protocol traffic is tagged by address rather than by context number.
Reference: [4] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASP-LOS IV), </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: That is, after a first-time fetch of data from a remote node, subsequent accesses of the data are satisfied entirely within the node. The resulting cache coherence problem can be solved using a variety of directory based schemes <ref> [2, 3, 4] </ref>. In a cache-based system, memory and processor resources are wasted if no processing is done while waiting for memory transactions to complete. Such transactions include first-time data fetches and invalidations required to enforce coherence. <p> Such high-availability interrupts violate instruction atomicity by faulting loads or stores which are in progress. This class of interrupts allows migration of hardware functionality into software. In Alewife, for example, high-availability interrupts are used to implement the LimitLESS coherence protocol <ref> [4] </ref>, a fast user and system-level messaging facility, and network deadlock recovery. LimitLESS interrupts must be able to occur under most circumstances, because they can affect forward progress in the machine, both by deadlocking the protocol and by blocking the network. <p> The nodes communicate via messages through a cost-effective direct network with a mesh topology. A single-chip communication and memory management unit (CMMU) on each node holds the cache tags and transaction buffers (described below), implements a variant of the cache coherence protocol described in <ref> [4] </ref>, and provides a direct message-passing interface to the underlying network. The CMMU's transaction store is the heart of Alewife's implementation of associative thrashlock. The transaction store is a fully associative set of 16 transaction buffers.
Reference: [5] <author> David Callahan, Ken Kennedy, and Allan Porterfield. </author> <title> Software Prefetching. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 40-52. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: Such transactions include first-time data fetches and invalidations required to enforce coherence. Applying basic pipelining ideas, resource utilization can be improved by allowing a processor to transmit more than one memory request at a time. Multiple outstanding transactions can be supported using software prefetch <ref> [5, 6] </ref>, rapid context switching [7, 8], or weak ordering [9]. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed [10, 11].
Reference: [6] <author> Todd Mowry and Anoop Gupta. </author> <title> Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Such transactions include first-time data fetches and invalidations required to enforce coherence. Applying basic pipelining ideas, resource utilization can be improved by allowing a processor to transmit more than one memory request at a time. Multiple outstanding transactions can be supported using software prefetch <ref> [5, 6] </ref>, rapid context switching [7, 8], or weak ordering [9]. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed [10, 11].
Reference: [7] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Exploring the Benefits of Multiple Hardware Contexts in a Multiprocessor Architecture: Preliminary Results. </title> <booktitle> In Proceedings 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 273-280, </pages> <address> New York, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Such transactions include first-time data fetches and invalidations required to enforce coherence. Applying basic pipelining ideas, resource utilization can be improved by allowing a processor to transmit more than one memory request at a time. Multiple outstanding transactions can be supported using software prefetch [5, 6], rapid context switching <ref> [7, 8] </ref>, or weak ordering [9]. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed [10, 11].
Reference: [8] <author> Anant Agarwal, Beng-Hong Lim, David A. Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <address> Seattle, WA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Such transactions include first-time data fetches and invalidations required to enforce coherence. Applying basic pipelining ideas, resource utilization can be improved by allowing a processor to transmit more than one memory request at a time. Multiple outstanding transactions can be supported using software prefetch [5, 6], rapid context switching <ref> [7, 8] </ref>, or weak ordering [9]. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed [10, 11]. <p> An Alewife processing node consists of a 33 MHz Spar-cle processor, 64K bytes of direct-mapped cache, a 4Mbyte portion of globally-shared main memory, and a floating-point coprocessor. The Sparcle processor is a modified SPARC processor [16], utilizing register-windows for rapid context-switching <ref> [8] </ref>. Our current implementation provides four distinct hardware contexts. Both the cache and floating-point units are SPARC compatible. The nodes communicate via messages through a cost-effective direct network with a mesh topology.
Reference: [9] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Applying basic pipelining ideas, resource utilization can be improved by allowing a processor to transmit more than one memory request at a time. Multiple outstanding transactions can be supported using software prefetch [5, 6], rapid context switching [7, 8], or weak ordering <ref> [9] </ref>. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed [10, 11]. Allowing multiple outstanding transactions in a cache-based multiprocessor opens the window of vulnerability and leads to situations involving livelock.
Reference: [10] <author> Kiyoshi Kurihara, David Chaiken, and Anant Agarwal. </author> <title> Latency Tolerance through Multithreading in Large-Scale Multiprocessors. </title> <booktitle> In Proceedings International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Japan, April 1991. </address> <publisher> IPS Press. </publisher>
Reference-contexts: Multiple outstanding transactions can be supported using software prefetch [5, 6], rapid context switching [7, 8], or weak ordering [9]. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed <ref> [10, 11] </ref>. Allowing multiple outstanding transactions in a cache-based multiprocessor opens the window of vulnerability and leads to situations involving livelock.
Reference: [11] <author> Kirk Johnson. </author> <title> The impact of communication locality on large-scale multiprocessor performance. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 392-402, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Multiple outstanding transactions can be supported using software prefetch [5, 6], rapid context switching [7, 8], or weak ordering [9]. Studies have shown that the utilization of the network, processor, and memory systems can be improved almost in proportion to the number of outstanding transactions allowed <ref> [10, 11] </ref>. Allowing multiple outstanding transactions in a cache-based multiprocessor opens the window of vulnerability and leads to situations involving livelock.
Reference: [12] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concur-rency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: Touchwait eliminates the livelock scenarios of the previous section, because the cache retains data blocks until the requesting context returns to access them. Problems Unfortunately, the locking mechanism can lead to four distinct types of deadlock, illustrated in Figure 6. This figure contains four different waits-for graphs <ref> [12] </ref>, which represent dependencies between transactions. In these graphs, the large italic letters represent transactions: D for data transactions and I for instruction transactions. The superscripts either P or S represent primary or secondary transactions, respectively.
Reference: [13] <author> David Kroft. </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization. </title> <booktitle> In Proceedings of the 8th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: More general schemes might keep track of the context that owns each buffer to prevent premature lock release (see Section 4.1). The use of a transaction buffer architecture has been presented in several milieux, such as lockup-free caching <ref> [13] </ref>, victim caching [14], and the remote-access cache of the DASH multiprocessor [3]. The need for an associative match on the address stems from several factors. First, protocol traffic is tagged by address rather than by context number.
Reference: [14] <author> N.P. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proceedings, International Symposium on Computer Architecture '90, </booktitle> <pages> pages 364-373, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: More general schemes might keep track of the context that owns each buffer to prevent premature lock release (see Section 4.1). The use of a transaction buffer architecture has been presented in several milieux, such as lockup-free caching [13], victim caching <ref> [14] </ref>, and the remote-access cache of the DASH multiprocessor [3]. The need for an associative match on the address stems from several factors. First, protocol traffic is tagged by address rather than by context number. <p> Each of the 16 transaction buffers contains an address, state bits, and space for a complete memory-line. The transaction buffers record the state of all outstanding memory transactions. The transaction store is completely integrated with the cache 9 coherence protocol; indeed, it is much like a multiprocessor victim cache <ref> [14] </ref>. Data may be transferred between transaction buffers and the cache or the processor may access transaction buffers directly. In addition, special instructions permit the processor to initiate non-binding prefetches. The transaction store has independent data paths to the processor, to memory, and to the network.
Reference: [15] <author> John Kubiatowicz. </author> <title> User's Manual for the A-1000 Communications and Memory Management Unit. ALEWIFE Memo No. </title> <type> 19, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: The next section discusses our experiences building a system based on associative thrashlock. 5 Implementation of the Framework The Alewife machine employs the associative thrashlock framework to close the window of vulnerability. This section overviews some of the key parameters of this implementation. For additional details, see <ref> [15] </ref>. Alewife is a large-scale multiprocessor with distributed shared memory. An Alewife processing node consists of a 33 MHz Spar-cle processor, 64K bytes of direct-mapped cache, a 4Mbyte portion of globally-shared main memory, and a floating-point coprocessor.
Reference: [16] <author> Anant Agarwal, Johnathan Babb, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Gino Maa, Ken MacKenzie, Dan Nuss-baum, Mike Parkin, and Donald Yeung. Sparcle: </author> <title> Today's Micro for Tomorrow's Multiprocessor. </title> <booktitle> In HOTCHIPS, </booktitle> <month> August </month> <year> 1992. </year> <month> 11 </month>
Reference-contexts: For additional details, see [15]. Alewife is a large-scale multiprocessor with distributed shared memory. An Alewife processing node consists of a 33 MHz Spar-cle processor, 64K bytes of direct-mapped cache, a 4Mbyte portion of globally-shared main memory, and a floating-point coprocessor. The Sparcle processor is a modified SPARC processor <ref> [16] </ref>, utilizing register-windows for rapid context-switching [8]. Our current implementation provides four distinct hardware contexts. Both the cache and floating-point units are SPARC compatible. The nodes communicate via messages through a cost-effective direct network with a mesh topology.
References-found: 16

