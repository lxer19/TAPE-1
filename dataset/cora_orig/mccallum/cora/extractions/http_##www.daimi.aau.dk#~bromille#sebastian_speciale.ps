URL: http://www.daimi.aau.dk/~bromille/sebastian_speciale.ps
Refering-URL: http://www.daimi.aau.dk/~bromille/thesis.html
Root-URL: http://www.daimi.aau.dk
Title: The Complexity of Minimizing Disjunctive Normal Form Formulas  
Author: Sebastian Lukas Arne Czort 
Date: January 26, 1999  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Angluin, D., </author> <title> "Remarks on the Difficulty of Finding a Disjunctive Normal Form for Boolean Functions", </title> <type> unpublished manuscript. </type>
Reference-contexts: So whether the exponential sized instances arise from [16] or from Wegener's reproduction is not clear. To complete our overview of the literature it should also be mentioned, and here we quote 2 Pitt & Valiant [17] p. 970: "In <ref> [1] </ref> and [12] it is shown that finding a minimum simple disjunction (i.e., a 1-CNF formula) consistent with given data is NP-hard. Their proof is by reduction from the Set Covering problem [4]. It is also deduced in [1] that finding a minimum size DNF formula is NP-hard for various size <p> and here we quote 2 Pitt & Valiant [17] p. 970: "In <ref> [1] </ref> and [12] it is shown that finding a minimum simple disjunction (i.e., a 1-CNF formula) consistent with given data is NP-hard. Their proof is by reduction from the Set Covering problem [4]. It is also deduced in [1] that finding a minimum size DNF formula is NP-hard for various size measures (e.g., the number of literals in the formula)." [1] is an unpublished manuscript by Angluin! For decision trees, which can be viewed as a form of DNF formulas, only one result is known. <p> Their proof is by reduction from the Set Covering problem [4]. It is also deduced in <ref> [1] </ref> that finding a minimum size DNF formula is NP-hard for various size measures (e.g., the number of literals in the formula)." [1] is an unpublished manuscript by Angluin! For decision trees, which can be viewed as a form of DNF formulas, only one result is known. In [6] Hancock et al. show that the hA; Bi-version of minimum node decision tree is N P -complete.
Reference: [2] <author> Cook, S.A., </author> <title> "The Complexity of Theorem Proving-procedures", </title> <booktitle> in: Proc. 3rd Ann. ACM Symp. on Theory of Computing (1971) 151-158. </booktitle>
Reference-contexts: The reference in [4] for the Aversion is an article [5] by Gimpel published in 1965, thereby preceding the theory of N P -completeness. This theory was initiated six years later in the 1971-published article "The Complexity of Theorem-Proving Procedures" <ref> [2] </ref> by Cook. Gimpel's article does not contain a proof of the A-version being N P -complete, but a result from the article can be used as the basis for a reduction from EXACT COVER BY 3-SETS 1 (X3C) to establish N P - completeness.
Reference: [3] <author> Frandsen, G.S. </author> <year> (1996), </year> <title> "Combinatorial Complexity Theory", course notes, </title> <type> DAIMI, Arhus Universitet. </type>
Reference-contexts: Other notions used will be developed as we go along. The notation used is the standard notation used in the literature on boolean formulas, as in f.x. <ref> [3] </ref>. From now and on we will interchangeably write 0 for f alse and 1 for true. We will write + for logical or. will denote logical and. Usually we will omit the .
Reference: [4] <author> Garey, M.R., and Johnson, D.S. </author> <year> (1979), </year> <title> "Computers and Intractability: A Guide to the Theory of NP-completeness", </title> <publisher> Freeman, </publisher> <address> New York. </address>
Reference-contexts: This in an article [17] by Pitt & Valiant where the hA; Bi-version is proven N P -complete. In the "bible" on N P - completeness "Computers and Intractability: A Guide to the Theory of NP-completeness" <ref> [4] </ref> Garey & Johnson mention that the A-version and the full truth-table version are N P -complete too. The reference in [4] for the Aversion is an article [5] by Gimpel published in 1965, thereby preceding the theory of N P -completeness. <p> In the "bible" on N P - completeness "Computers and Intractability: A Guide to the Theory of NP-completeness" <ref> [4] </ref> Garey & Johnson mention that the A-version and the full truth-table version are N P -complete too. The reference in [4] for the Aversion is an article [5] by Gimpel published in 1965, thereby preceding the theory of N P -completeness. This theory was initiated six years later in the 1971-published article "The Complexity of Theorem-Proving Procedures" [2] by Cook. <p> Furthermore, Garey & Johnson mention that the reduction is from general SET COVERING. But as we point out later in the thesis, it seems that it will not work for arbitrary set cover instances. The reference in <ref> [4] </ref> for the full truth-table version is to an article [13] by Masek. In this 1 All the problems used in reductions in this thesis are "standard" N P -complete problems. Readers not familiar with these problems can find a description of them in [4]. 3 article Masek does prove that <p> The reference in <ref> [4] </ref> for the full truth-table version is to an article [13] by Masek. In this 1 All the problems used in reductions in this thesis are "standard" N P -complete problems. Readers not familiar with these problems can find a description of them in [4]. 3 article Masek does prove that the full truth-table version of minimum term DNF is N P -complete. Although this result is interesting, his article was not published. <p> Their proof is by reduction from the Set Covering problem <ref> [4] </ref>. <p> However, for the problems which turn out to be in P we actually solve the function problem, i.e. we compute a minimum DNF. When proving problems N P -hard we use reductions that can be carried out in polynomial time. I.e. our reductions are polynomial transformations as defined in <ref> [4] </ref>. The term Karp reduction is also used for this kind of reductions in the literature. Instead we could have used f.x. logarithmic-space (log-space) reductions as defined in [15]. A log-space reduction is also a polynomial time reduction. <p> Which means that a proof of one our problems being N P -complete is also a proof of it being strongly N P -complete. For the definition of a pseudo-polynomial algorithm and strong N P -completenees see <ref> [4] </ref> or [15]. Now 2. In time polynomial in the size of g we can verify that g is within the given measure; i.e. that l k, that P l i=1 jt i j k or that max l i=1 jt i j k. Finally 3. <p> Clearly we can compute the number l + k in time polynomial in the size of T . All in all we use time polynomial in the size of T . Hence is the reduction a polynomial time reduction. 2 As we noted in the introduction, Garey & Johnson <ref> [4] </ref> mention that Gimpel's theorem can be used in a reduction from general SET COVERING. But as argued above, the reduction will not carry through for general set cover instances. <p> A result by Garey & Johnson now gives that, under the assumption P 6= N P , a fully polynomial time approximation scheme does not exists for any of the 7 problems (the reader is referred to <ref> [4] </ref> for the result and a definition of a fully polynomial time approximation scheme). After this parenthesis, we return to inapproximability ratios. <p> If an internal node contains a test T i and T i (x) = 0 we go left, if T i (x) = 1 we go right. We say that IT separates X if IT (x) = x for all x 2 X. 48 In <ref> [4] </ref> Garey & Johnson mention that a problem DECISION TREE is N P -complete. The problem is as follows. <p> It was only known that finding a decision tree with the minimum external path length is NP-hard <ref> [9, 4] </ref>." Quinlan & Rivest look at a more general notion of decision trees than our notion in [20]. A set of objects each having a set of attributes (not necessarily two-valued) is given. Each internal node tests an attribute and leaves are labelled negative or positive.
Reference: [5] <author> Gimpel, J.F., </author> <title> "A Method of Producing a Boolean Function Having an Arbitrarily Prescribed Prime Implicant Table", in: </title> <journal> IEEE Trans. </journal> <note> Computers 14 (1965) 485-488. </note>
Reference-contexts: In the "bible" on N P - completeness "Computers and Intractability: A Guide to the Theory of NP-completeness" [4] Garey & Johnson mention that the A-version and the full truth-table version are N P -complete too. The reference in [4] for the Aversion is an article <ref> [5] </ref> by Gimpel published in 1965, thereby preceding the theory of N P -completeness. This theory was initiated six years later in the 1971-published article "The Complexity of Theorem-Proving Procedures" [2] by Cook. <p> His motivation was that if there was such a special structure it might be exploited in an algorithm for finding a minimum term DNF. He found a method for producing a boolean function with an arbitrarily prescribed prime implicant table <ref> [5] </ref>. I.e. he answered this question negatively. We might have to solve an arbitrary covering problem. We will use Gimpel's result to show that the A-version of minimum term DNF is N P -hard. We have rewritten his result to serve this purpose. <p> We might have to solve an arbitrary covering problem. We will use Gimpel's result to show that the A-version of minimum term DNF is N P -hard. We have rewritten his result to serve this purpose. Proofs are more detailed than in Gimpel's article. 3 <ref> [5] </ref> p. 485: "The standard methods for determining the simplest normal form ... reduce the problem to one of finding a solution to a covering problem [14], [18]." 13 Let us say that a table T is interesting if T is coverable (i.e., each column has a 1-entry), T cannot be
Reference: [6] <author> Hancock, T., Jiang, T., Li, M., and Tromp, J., </author> <title> "Lower Bounds on Learning Decision Lists and Trees", </title> <note> in: Information and Computation 126 (1996) 114-122. </note>
Reference-contexts: In <ref> [6] </ref> Hancock et al. show that the hA; Bi-version of minimum node decision tree is N P -complete. For general circuits the situation is even worse | no N P -completeness results are known. <p> Obviously, the above overview of the literature accounts for the survey part. With respect to presentation and extension we will mainly concentrate on DNF formulas. Hopefully, this should come as no surprise considering the heavily suggestive title of the thesis. Although we concentrate on DNF formulas, the result from <ref> [6] </ref> on the N P -completeness of the hA; Bi-version of minimum node decision tree is presented. <p> But its applicability does not stop here! Indeed the reduction is the closest we get to a panacea for our problems. In section 7.2 we use it to prove the hA; Bi-version of minimum depth DNF N P -hard. And in <ref> [6] </ref> it was used to show the only known result on decision trees. Furthermore, we observe that the reduction can also be used to establish N P -hardness of two other measures on decision trees. The reduction is a follows. <p> The result is from <ref> [6] </ref>. The proof is more detailed than the one in [6]. This enables us to easily observe that the proof also holds for the external path length measure and the depth measure. Theorem 34 (Hancock et al., 1996) The hA; Bi-version of minimum node decision tree is NP -hard. Proof. <p> The result is from <ref> [6] </ref>. The proof is more detailed than the one in [6]. This enables us to easily observe that the proof also holds for the external path length measure and the depth measure. Theorem 34 (Hancock et al., 1996) The hA; Bi-version of minimum node decision tree is NP -hard. Proof. For the third and last time we use Haussler's reduction. <p> At least they are different enough to require that taking one for the other should not be done without a proof of this being allowed. As the following will demonstrate, not everyone shares this view. In <ref> [6] </ref>, i.e. the article where the hA; Bi-version of minimum node decision tree (our notion!) was proven N P -hard, Hancock et al. write on p. 115: "Note that it was not even known whether finding the smallest decision tree is NP-hard. <p> As we saw in section 8, the (current) inapproximability ratio for GRAPH COLORING is n * whereas it is log n for SET COVERING. So proving hunch 20 would give us a better inapproximability ratio for the hA; Bi version of minimum length DNF. * In <ref> [6] </ref> Hancock et al. "amplify" decision trees. The inapproximability ratio on SET COVERING and theorem 34 then gives that decision 50 trees can not be approximated within a factor 2 log ffi n for any ffi &lt; 1, under the assumption N P * ~ P .
Reference: [7] <author> Haussler, D., </author> <title> "Quantifying Inductive Bias: AI Learning Algorithms and Valiant's Learning Framework", </title> <booktitle> in: Artificial Intelligence 36 No. </booktitle> <month> 2 </month> <year> (1988) </year> <month> 177-222. </month>
Reference-contexts: Therefore we give up on trying to reuse the reduction from lemma 10. Fortunately, we can do something else instead. Using a reduction by Haussler we will reduce SET COVERING to the hA; Bi-version of minimum length DNF. In <ref> [7] </ref> Haussler used the reduction to prove that finding a consistent monomial is N P -hard; in our terminology is a consistent monomial a DNF, consisting of a single term, that realizes hA; Bi.
Reference: [8] <editor> Hochbaum, D.S. (Ed.) </editor> <year> (1996), </year> <title> "Approximation Algorithms for NP-hard Problems", </title> <address> PWS, Boston. </address>
Reference-contexts: In [17] Pitt & Valiant translated inapprox-imability ratio 2 ffi, for any ffi &gt; 0, under the assumption P 6= N P to the hA; Bi-version of minimum term DNF. Since then considerable progress has been made in the theory on inapproximability. In <ref> [8] </ref> it is mentioned that the (current) inapproximability ratio for GRAPH COLORING is n * , for some fixed * &gt; 0, under the assumption that P 6= N P (n is the input size). We immediately get. <p> The reduction from SET COVERING to the hA; Bi-versions of minimum length and minimum depth DNF also preserves solution values. According to <ref> [8] </ref> the inapproximability ratio for SET COVERING is (1 ffi) ln n, for any ffi &gt; 0, under the assumption that N P * ~ P . The following theorem follows immediately.
Reference: [9] <author> Hyafil, L., and Rivest, </author> <title> R.L., "Constructing Optimal Binary Decision Trees is NP-complete", </title> <note> in: Information Processing Letters 5 (1976) 15-17. </note>
Reference-contexts: We ask if there exist an identification tree (over X and T ), with external path length k, that separates the elements in X. The reference for the N P -completeness proof is an article by Hyafil & Rivest <ref> [9] </ref>. Indeed DECISION TREE is proven NP -complete in [9]. I.e. Hyafil & Rivest show that finding a minimum external path length identification tree is NP -hard. This is done by reducing X3C to DECISION TREE. <p> We ask if there exist an identification tree (over X and T ), with external path length k, that separates the elements in X. The reference for the N P -completeness proof is an article by Hyafil & Rivest <ref> [9] </ref>. Indeed DECISION TREE is proven NP -complete in [9]. I.e. Hyafil & Rivest show that finding a minimum external path length identification tree is NP -hard. This is done by reducing X3C to DECISION TREE. <p> It was only known that finding a decision tree with the minimum external path length is NP-hard <ref> [9, 4] </ref>." Quinlan & Rivest look at a more general notion of decision trees than our notion in [20]. A set of objects each having a set of attributes (not necessarily two-valued) is given. Each internal node tests an attribute and leaves are labelled negative or positive. <p> The interested reader is referred to [20] 49 And, yes! Funnily enough is Rivest author on both <ref> [9] </ref> and [20]. As was pointed out, Hyafil and Rivest show that finding a minimum external path length identification tree is N P -hard.
Reference: [10] <author> Karnaugh, </author> <title> "The Map Method for Synthesis of Combinatorial Logic Circuits", in: </title> <journal> AIEE Trans. Comm. </journal> <volume> Elect. </volume> <month> 72 </month> <year> (1953) </year> <month> 593-598. </month>
Reference-contexts: Finally, we will not concern ourselves with encodings of the mathematical objects used in reductions. So when we write "the size of O", we actually mean the length of a reasonable encoding of O. 3 Computing a Minimum DNF Back in the 1950's Quine, among others <ref> [10, 14] </ref>, looked at the problem of simplifying truth functions. In [18] he developed a "mechanical procedure" that given a DNF formula g would reduce it to its simplest equivalent DNF, i.e. to the simplest DNF realizing the function that g realizes. Simplest meant the DNF with fewest terms.
Reference: [11] <author> Kearns, M., and Valiant, L., </author> <title> "Cryptographic Limitations on Learning Boolean Formulae and Finite Automata", </title> <booktitle> in: Proc. 21st Ann. ACM Symp. on Theory of Computing (1989) 433-444. </booktitle>
Reference-contexts: For general circuits the situation is even worse | no N P -completeness results are known. Although no N P -completeness results are known for general circuits, Kearns & Valiant have shown <ref> [11] </ref> that the hA; Bi-version of minimum size circuit is not in P , under some cryptographic assumptions. The main purpose of this thesis is to survey, present and extend hardness results on function representations. Obviously, the above overview of the literature accounts for the survey part.
Reference: [12] <author> Levin, L.A., </author> <title> "Universal Sorting Problems", </title> <note> in: Problemy Peredaci Informacii 9 (1973) 265-266. 52 </note>
Reference-contexts: So whether the exponential sized instances arise from [16] or from Wegener's reproduction is not clear. To complete our overview of the literature it should also be mentioned, and here we quote 2 Pitt & Valiant [17] p. 970: "In [1] and <ref> [12] </ref> it is shown that finding a minimum simple disjunction (i.e., a 1-CNF formula) consistent with given data is NP-hard. Their proof is by reduction from the Set Covering problem [4].
Reference: [13] <author> Masek, W.J. </author> <year> (1979), </year> <title> "Some NP-complete Set Covering Problems", </title> <type> unpublished manuscript. </type>
Reference-contexts: Furthermore, Garey & Johnson mention that the reduction is from general SET COVERING. But as we point out later in the thesis, it seems that it will not work for arbitrary set cover instances. The reference in [4] for the full truth-table version is to an article <ref> [13] </ref> by Masek. In this 1 All the problems used in reductions in this thesis are "standard" N P -complete problems. <p> In 1979 Masek identified such instances by proving the full truth-table version of minimum term DNF N P -hard. Even though this result is interesting, and the reduction is innovative, his article <ref> [13] </ref> was not published. The following reduction is his. But the presentation is different to the one in [13], and the correctness of the reduction is proven in greater detail. To facilitate a detailed proof some errors in Masek's article had to be corrected. <p> Even though this result is interesting, and the reduction is innovative, his article <ref> [13] </ref> was not published. The following reduction is his. But the presentation is different to the one in [13], and the correctness of the reduction is proven in greater detail. To facilitate a detailed proof some errors in Masek's article had to be corrected. In particular, the way I/O-points are connected (to be defined) had to be modified.
Reference: [14] <author> McCluskey, E.J., </author> <title> "Minimization of Boolean Functions", in: </title> <journal> Bell Sys. Tech. J. </journal> <pages> 35 1417-1444. </pages>
Reference-contexts: Finally, we will not concern ourselves with encodings of the mathematical objects used in reductions. So when we write "the size of O", we actually mean the length of a reasonable encoding of O. 3 Computing a Minimum DNF Back in the 1950's Quine, among others <ref> [10, 14] </ref>, looked at the problem of simplifying truth functions. In [18] he developed a "mechanical procedure" that given a DNF formula g would reduce it to its simplest equivalent DNF, i.e. to the simplest DNF realizing the function that g realizes. Simplest meant the DNF with fewest terms. <p> We have rewritten his result to serve this purpose. Proofs are more detailed than in Gimpel's article. 3 [5] p. 485: "The standard methods for determining the simplest normal form ... reduce the problem to one of finding a solution to a covering problem <ref> [14] </ref>, [18]." 13 Let us say that a table T is interesting if T is coverable (i.e., each column has a 1-entry), T cannot be covered using two rows or less (i.e., for any two rows, there is a column with no 1-entries in either), and T is row-dominance free, i.e.
Reference: [15] <author> Papadimitriou, C.H. </author> <year> (1994), </year> <title> "Computational Complexity", </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: I.e. our reductions are polynomial transformations as defined in [4]. The term Karp reduction is also used for this kind of reductions in the literature. Instead we could have used f.x. logarithmic-space (log-space) reductions as defined in <ref> [15] </ref>. A log-space reduction is also a polynomial time reduction. But it is an open question ([15] p. 177) whether any problem proven N P -hard using a polynomial time reduction, can be proved N P -hard using a log-space reduction. <p> Which means that a proof of one our problems being N P -complete is also a proof of it being strongly N P -complete. For the definition of a pseudo-polynomial algorithm and strong N P -completenees see [4] or <ref> [15] </ref>. Now 2. In time polynomial in the size of g we can verify that g is within the given measure; i.e. that l k, that P l i=1 jt i j k or that max l i=1 jt i j k. Finally 3.
Reference: [16] <author> Paul, W.J., </author> <note> "Boolesche Minimalpolynome und Uberdeckungsprobleme", in: Acta Informatica 4 (1975) 321-336. </note>
Reference-contexts: The reduction seems faulty since the instance produced by the reduction has size exponential in the size of the ground-set of the set cover instance. Wegener credits Paul for the reduction. The reference is <ref> [16] </ref>, an article in German. The author has not been able to properly understand the reduction in [16]. So whether the exponential sized instances arise from [16] or from Wegener's reproduction is not clear. <p> Wegener credits Paul for the reduction. The reference is <ref> [16] </ref>, an article in German. The author has not been able to properly understand the reduction in [16]. So whether the exponential sized instances arise from [16] or from Wegener's reproduction is not clear. <p> Wegener credits Paul for the reduction. The reference is <ref> [16] </ref>, an article in German. The author has not been able to properly understand the reduction in [16]. So whether the exponential sized instances arise from [16] or from Wegener's reproduction is not clear. <p> Then the measure formula length is looked at. Us looking at formula length is motivated by showing that minimizing with respect to formula length is different to minimizing with respect to terms. Apparently, similar work has been done by Paul in <ref> [16] </ref>. But the author has not had any luck in understanding the approach taken in Paul's article. The A-version, the hA; Bi-version and the full truth-table version of minimum length DNF are shown N P -complete. We also look at the measure depth.
Reference: [17] <author> Pitt, L., and Valiant, L., </author> <title> "Computational Limits on Learning from Examples", in: </title> <journal> Journal of the ACM 35 No. </journal> <month> 4 </month> <year> (1988) </year> <month> 965-984. </month>
Reference-contexts: Although N P -completeness of these different variants of minimum term DNF is mentioned in the literature, only one proof actually establishing N P - completeness has been published. This in an article <ref> [17] </ref> by Pitt & Valiant where the hA; Bi-version is proven N P -complete. In the "bible" on N P - completeness "Computers and Intractability: A Guide to the Theory of NP-completeness" [4] Garey & Johnson mention that the A-version and the full truth-table version are N P -complete too. <p> The author has not been able to properly understand the reduction in [16]. So whether the exponential sized instances arise from [16] or from Wegener's reproduction is not clear. To complete our overview of the literature it should also be mentioned, and here we quote 2 Pitt & Valiant <ref> [17] </ref> p. 970: "In [1] and [12] it is shown that finding a minimum simple disjunction (i.e., a 1-CNF formula) consistent with given data is NP-hard. Their proof is by reduction from the Set Covering problem [4]. <p> In essence; N P -hardness of finding a minimum candidate implies negative results on learnability. Therefore we turn to the literature on learning algorithms and look for hardness results on hA; Bi-versions. We find that Pitt & Valiant classified the hA; Bi-version NP-hard in <ref> [17] </ref>. This was done by reducing the N P -hard problem GRAPH COLORING to the hA; Bi-version of minimum term DNF. The reduction is as follows. Let G = (V; E) be a graph. The points in A and B will have length n = jV j. <p> We saw; G = (V; E) can be k-colored () a DNF, with k terms, realizes hA; Bi. This means that we can translate any known inapproximability ratio on GRAPH COLORING to the hA; Bi-version of minimum term DNF. In <ref> [17] </ref> Pitt & Valiant translated inapprox-imability ratio 2 ffi, for any ffi &gt; 0, under the assumption P 6= N P to the hA; Bi-version of minimum term DNF. Since then considerable progress has been made in the theory on inapproximability.
Reference: [18] <author> Quine, </author> <title> W.V., "The Problem of Simplifying Truth Functions", </title> <note> in: American Mathematical Monthly 59 (1952) 521-531. </note>
Reference-contexts: So when we write "the size of O", we actually mean the length of a reasonable encoding of O. 3 Computing a Minimum DNF Back in the 1950's Quine, among others [10, 14], looked at the problem of simplifying truth functions. In <ref> [18] </ref> he developed a "mechanical procedure" that given a DNF formula g would reduce it to its simplest equivalent DNF, i.e. to the simplest DNF realizing the function that g realizes. Simplest meant the DNF with fewest terms. <p> Essentially, the procedure we present for computing a minimum term DNF consists of the refined first part and the unrefined second part. 8 We need some definitions. These are definitions introduced by Quine in <ref> [18] </ref> and [19]. A term t subsumes term t 0 if t contains all literals that t 0 contains. Furthermore, if t is a point and t subsumes t 0 we will say that t 0 covers t. <p> We have rewritten his result to serve this purpose. Proofs are more detailed than in Gimpel's article. 3 [5] p. 485: "The standard methods for determining the simplest normal form ... reduce the problem to one of finding a solution to a covering problem [14], <ref> [18] </ref>." 13 Let us say that a table T is interesting if T is coverable (i.e., each column has a 1-entry), T cannot be covered using two rows or less (i.e., for any two rows, there is a column with no 1-entries in either), and T is row-dominance free, i.e. no
Reference: [19] <author> Quine, </author> <title> W.V., "A Way to Simplify Truth Functions", </title> <note> in: American Mathematical Monthly 62 (1955) 627-631. </note>
Reference-contexts: Simplest meant the DNF with fewest terms. His procedure consists of two parts; finding the so-called prime implicants of the DNF and covering the points of the DNF by prime implicants. In <ref> [19] </ref> he refined his procedure. The refinement meant that it was no longer necessary to expand the DNF into its points when finding the prime implicants. This refinement made the covering part more complicated. <p> Essentially, the procedure we present for computing a minimum term DNF consists of the refined first part and the unrefined second part. 8 We need some definitions. These are definitions introduced by Quine in [18] and <ref> [19] </ref>. A term t subsumes term t 0 if t contains all literals that t 0 contains. Furthermore, if t is a point and t subsumes t 0 we will say that t 0 covers t.
Reference: [20] <author> Quinlan, J.R., and Rivest, </author> <title> R.L., "Inferring Decision Trees Using the Minimum Description Length Principle", </title> <note> in: Information and Computation 80 (1989) 227-248. </note>
Reference-contexts: It was only known that finding a decision tree with the minimum external path length is NP-hard [9, 4]." Quinlan & Rivest look at a more general notion of decision trees than our notion in <ref> [20] </ref>. A set of objects each having a set of attributes (not necessarily two-valued) is given. Each internal node tests an attribute and leaves are labelled negative or positive. <p> Each internal node tests an attribute and leaves are labelled negative or positive. Reading objects as elements in f0; 1g n and attributes as variables, we see that our notion of decision trees falls under Quinlan & Rivest's more general notion. p. 242 in <ref> [20] </ref>: "It is probably difficult to compute the best decision tree under our measure 7 . <p> The interested reader is referred to <ref> [20] </ref> 49 And, yes! Funnily enough is Rivest author on both [9] and [20]. As was pointed out, Hyafil and Rivest show that finding a minimum external path length identification tree is N P -hard. <p> The interested reader is referred to <ref> [20] </ref> 49 And, yes! Funnily enough is Rivest author on both [9] and [20]. As was pointed out, Hyafil and Rivest show that finding a minimum external path length identification tree is N P -hard.
Reference: [21] <author> Wegener, I. </author> <year> (1987), </year> <title> "The Complexity of Boolean Functions", </title> <publisher> Wiley. </publisher> <pages> 53 </pages>
Reference-contexts: Although this result is interesting, his article was not published. It should also be noted that a reduction from general SET COVERING to what seems to be the hA; Bi-version of minium term DNF has been presented by Wegener in <ref> [21] </ref>. The reduction seems faulty since the instance produced by the reduction has size exponential in the size of the ground-set of the set cover instance. Wegener credits Paul for the reduction. The reference is [16], an article in German.
References-found: 21

