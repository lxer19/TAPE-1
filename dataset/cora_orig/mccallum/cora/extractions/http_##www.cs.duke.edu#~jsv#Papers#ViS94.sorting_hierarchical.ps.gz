URL: http://www.cs.duke.edu/~jsv/Papers/ViS94.sorting_hierarchical.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node5.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: CS-1993-02 Algorithms for Parallel Memory II: Hierarchical Multilevel Memories  
Author: Jeffrey Scott Vitter, Elizabeth A. M. Shriver 
Date: January 20, 1993  
Address: 27708-0129  
Affiliation: Department of Computer Science Duke University Durham, North Carolina  
Abstract-found: 0
Intro-found: 1
Reference: [AAC] <author> A. Aggarwal, B. Alpern, A. K. Chandra, and M. Snir, </author> <title> "A Model for Hierarchical Memory," </title> <booktitle> Proceedings of 19th Annual ACM Symposium on Theory of Computing (May 1987), </booktitle> <pages> 305-314. </pages>
Reference-contexts: 1 Introduction Large-scale computer systems contain many levels of memory|ranging from very small but very fast registers, to successively larger but slower memories, such as multiple levels of cache, primary memory, magnetic disks, and archival storage. An elegant hierarchical memory model was introduced by Aggarwal, Alpern, Chandra, and Snir <ref> [AAC] </ref> and further developed by Aggarwal, Chandra, and Snir [ACS] to take into account block transfer. All computation takes place in the central processing unit (CPU), one instruction per unit time. <p> All computation takes place in the central processing unit (CPU), one instruction per unit time. Access to memory takes a varying amount of time, depending on how low in the memory hierarchy the memory access is. Optimal bounds are obtained in <ref> [AAC, ACS] </ref> for several sorting and matrix-related problems. In this paper we investigate the capabilities of parallel memory hierarchies. In the next section, we define two uniform memory models, each consisting of P hierarchical memories connected together at their base levels. <p> In this paper we investigate the capabilities of parallel memory hierarchies. In the next section, we define two uniform memory models, each consisting of P hierarchical memories connected together at their base levels. The P hierarchical memories can be either of the two types <ref> [AAC, ACS] </ref> mentioned earlier. For each model, we develop matching upper and lower bounds for the problems of sorting, FFT, and matrix multiplication, which are defined in Section 3. <p> The algorithms that realize the optimal bounds for sorting are applications of the optimal disk sorting algorithm developed in the companion paper [ViS] for a two-level memory model with parallel block transfer. We apply the partitioning technique of [ViS] to the one-hierarchy sorting algorithms of <ref> [AAC, ACS] </ref>. Intuitively, the hierarchical algorithms are optimal because the internal processing in the corresponding two-level algorithms is efficient. The main results are given in Section 4 and are proved in Sections 5-10. <p> Conclusions and open problems are discussed in Section 11. 2 Parallel Hierarchical Memory Models A hierarchical memory model is a uniform model consisting of memory whose locations take different amounts of time to access. The basic unit of transfer in the hierarchical memory model HMM <ref> [AAC] </ref> is the record; access to location x takes time f (x). <p> The lower bounds for P-HMM and P-BT are based upon the approach used in <ref> [AAC] </ref> and [ACS]. Theorem 1 In the P-HMM model, the time for sorting and the FFT is fi N log N log log N !! fi N ff+1 N log N if f (x) = x ff , ff &gt; 0. <p> In Sections 5-10 we prove Theorems 1 and 2. In the process we also develop optimal algorithms in the P-HMM and P-BT models for matrix addition, and the so-called "simple" problems (like two-way merging) of <ref> [AAC, ACS] </ref>. Our techniques can be extended to get optimal algorithms for other problems of [AAC, ACS, ViS], such as searching, generating permutations, and permutation networks. <p> In the process we also develop optimal algorithms in the P-HMM and P-BT models for matrix addition, and the so-called "simple" problems (like two-way merging) of [AAC, ACS]. Our techniques can be extended to get optimal algorithms for other problems of <ref> [AAC, ACS, ViS] </ref>, such as searching, generating permutations, and permutation networks. Our optimal P-HMM and P-BT algorithms for sorting and FFT are applications of the optimal algorithms of [ViS] for the two-level model with parallel block transfer, applied to the HMM and BT algorithms given in [AAC, ACS]. <p> Our optimal P-HMM and P-BT algorithms for sorting and FFT are applications of the optimal algorithms of [ViS] for the two-level model with parallel block transfer, applied to the HMM and BT algorithms given in <ref> [AAC, ACS] </ref>. <p> We prove that the randomized distribution sort algorithm we develop is simultaneously optimal for all well-behaved access cost functions, as defined in Section 2. The sorting algorithm is a modified version of the optimal distribution sort algorithm for the one-hierarchy HMM <ref> [AAC] </ref>. For simplicity, we assume that records have distinct key 6 5 SORTING IN P-HMM values; this assumption is satisfied, for example, if we append to the key field of each record the original global memory location of the record. <p> Let us define the "sequential time" of A to be the sum of its time costs for each of the P hierarchies; the sequential time of A is at most P times its running time. Following the approach in <ref> [AAC] </ref>, we can imagine superimposing onto the P-HMM-type hierarchical memory a sequence of two-level memories. For each M in the range P M &lt; N, we superimpose on the P-HMM an internal memory of size M and one infinite-sized disk. <p> The second term follows from the N log N lower bound for sorting in the comparison model of computation. 2 The next theorem shows that the sorting algorithm given earlier in Section 5 is uniformly optimal (in the language of <ref> [AAC] </ref>) in that it is optimal for all well-behaved access costs f (x), as defined in Section 2. <p> The following theorem, when combined with Theorem 4, also gives an alternate proof of Theorem 3. Theorem 6 The algorithm given at the beginning of Section 5 is optimal for all well-behaved access costs f (x). Proof : We use a parallelized version of the approach of <ref> [AAC] </ref>, combined with the lower bound strategy of Theorems 4 and 5.
Reference: [ACS] <author> A. Aggarwal, A. Chandra, and M. Snir, </author> <title> "Hierarchical Memory with Block Transfer," </title> <booktitle> Proceedings of 28th Annual IEEE Symposium on Foundations of Computer Science (October 1987), </booktitle> <pages> 204-216. </pages>
Reference-contexts: An elegant hierarchical memory model was introduced by Aggarwal, Alpern, Chandra, and Snir [AAC] and further developed by Aggarwal, Chandra, and Snir <ref> [ACS] </ref> to take into account block transfer. All computation takes place in the central processing unit (CPU), one instruction per unit time. Access to memory takes a varying amount of time, depending on how low in the memory hierarchy the memory access is. <p> All computation takes place in the central processing unit (CPU), one instruction per unit time. Access to memory takes a varying amount of time, depending on how low in the memory hierarchy the memory access is. Optimal bounds are obtained in <ref> [AAC, ACS] </ref> for several sorting and matrix-related problems. In this paper we investigate the capabilities of parallel memory hierarchies. In the next section, we define two uniform memory models, each consisting of P hierarchical memories connected together at their base levels. <p> In this paper we investigate the capabilities of parallel memory hierarchies. In the next section, we define two uniform memory models, each consisting of P hierarchical memories connected together at their base levels. The P hierarchical memories can be either of the two types <ref> [AAC, ACS] </ref> mentioned earlier. For each model, we develop matching upper and lower bounds for the problems of sorting, FFT, and matrix multiplication, which are defined in Section 3. <p> The algorithms that realize the optimal bounds for sorting are applications of the optimal disk sorting algorithm developed in the companion paper [ViS] for a two-level memory model with parallel block transfer. We apply the partitioning technique of [ViS] to the one-hierarchy sorting algorithms of <ref> [AAC, ACS] </ref>. Intuitively, the hierarchical algorithms are optimal because the internal processing in the corresponding two-level algorithms is efficient. The main results are given in Section 4 and are proved in Sections 5-10. <p> The basic unit of transfer in the hierarchical memory model HMM [AAC] is the record; access to location x takes time f (x). The BT model <ref> [ACS] </ref> represents a notion of block transfer applied to HMM; in the BT model, access to the t + 1 records at locations x t, x t + 1, . . . , x takes time f (x) + t. <p> The lower bounds for P-HMM and P-BT are based upon the approach used in [AAC] and <ref> [ACS] </ref>. Theorem 1 In the P-HMM model, the time for sorting and the FFT is fi N log N log log N !! fi N ff+1 N log N if f (x) = x ff , ff &gt; 0. <p> In Sections 5-10 we prove Theorems 1 and 2. In the process we also develop optimal algorithms in the P-HMM and P-BT models for matrix addition, and the so-called "simple" problems (like two-way merging) of <ref> [AAC, ACS] </ref>. Our techniques can be extended to get optimal algorithms for other problems of [AAC, ACS, ViS], such as searching, generating permutations, and permutation networks. <p> In the process we also develop optimal algorithms in the P-HMM and P-BT models for matrix addition, and the so-called "simple" problems (like two-way merging) of [AAC, ACS]. Our techniques can be extended to get optimal algorithms for other problems of <ref> [AAC, ACS, ViS] </ref>, such as searching, generating permutations, and permutation networks. Our optimal P-HMM and P-BT algorithms for sorting and FFT are applications of the optimal algorithms of [ViS] for the two-level model with parallel block transfer, applied to the HMM and BT algorithms given in [AAC, ACS]. <p> Our optimal P-HMM and P-BT algorithms for sorting and FFT are applications of the optimal algorithms of [ViS] for the two-level model with parallel block transfer, applied to the HMM and BT algorithms given in <ref> [AAC, ACS] </ref>. <p> It follows that the sorting algorithm is optimal. 2 6 Sorting in P-BT In this section we show the matching upper and lower bounds quoted in Theorem 2 in Section 4 for sorting in the P-BT model. The following useful lemma is a parallel version of a theorem in <ref> [ACS] </ref>. <p> cost per operation of O log fl n O log log P if f (x) = x ff , 0 &lt; ff &lt; 1; n O n ff1 if f (x) = x ff , ff &gt; 1, 12 6 SORTING IN P-BT where n is the number of operations <ref> [ACS] </ref>. The cost of merging two lists of P elements in base memory is log P . <p> We can access the levels within our hierarchies optimally if we read and write (x=P ) ff elements at a time when we access global location x. Our optimal P-BT sorting algorithm is a modified version of the one-hierarchy algorithm presented in <ref> [ACS] </ref>. The key component of the algorithm is our use of the partitioning technique of [ViS] to spread the records in each bucket evenly among the P hierarchies. <p> In Step 5, we use the "touch" algorithm of <ref> [ACS] </ref> independently in each hierarchy to process records P at a time. <p> sort the subsets G i is bounded by t T N P + P = N 1ff P + O N The time for touching the G i subsets and accumulating the N= log N elements of set A in Step 3 is O ((N=P ) log log (N=P )) <ref> [ACS] </ref>. Using Lemma 1, we can show that the time for the two-way merge sort used in Step 3 to sort n elements is O ((n=P ) log n (log log n + log P )). <p> The data movement to reposition the buckets in Step 8 can be done by the same method used by the one-hierarchy algorithm <ref> [ACS] </ref>, that is, by computing the generalized matrix transposition for each hierarchy independently; the time needed is thus O ((N=P )(log log (N=P )) 4 ) with high probability. <p> The data movement to reposition the buckets in Step 8 can be done as noted earlier by the one-hierarchy algorithm <ref> [ACS] </ref>, that is, by computing the generalized matrix transposition for each hierarchy independently; the time needed is O ((N=P )(log 2 (N=P )) with high probability. The previous remarks given in the proof of Theorem 7 about the distribution properties of Phase 2 still apply. <p> for the case f (x) = x ff , ff = 1, the ((N=P ) ff + (N=P ) log N ) term in the lower bound for the case f (x) = x ff , ff &gt; 1, follow from a parallelization of the P = 1 bounds in <ref> [ACS] </ref>. 7 FFT in P-HMM The P-HMM lower bounds proved in Theorem 4 and 5 for sorting apply also to the FFT computation. This follows immediately by substituting the phrase "FFT" for "sorting" in the proofs of the theorems. <p> The lower bounds for sorting apply also to FFT. The FFT algorithm that meets these bounds is based on the FFT algorithm of Section 7. The shu*ing is done by touching the records in each group, using the touching algorithm of <ref> [ACS] </ref> applied to the hierarchies independently. <p> Two matrices can be added by touching the corresponding elements of the matrices simultaneously, using the touching algorithm of <ref> [ACS] </ref> applied to the hierarchies independently. Once two elements are in base memory level together, they can be added. <p> The upper bounds for the remaining cases of Theorem 2 follow by applying the other cases of Lemma 3. The lower bound for the access cost function f (x) = x ff , where ff = 3=2, for the BT model <ref> [ACS] </ref> can be modified for the P-BT model.
Reference: [AgV] <author> A. Aggarwal and J. S. Vitter, </author> <title> "The Input/Output Complexity of Sorting and Related Problems," </title> <journal> Communications of the ACM (September 1988), </journal> <pages> 1116-1127. </pages>
Reference-contexts: Following the approach in [AAC], we can imagine superimposing onto the P-HMM-type hierarchical memory a sequence of two-level memories. For each M in the range P M &lt; N, we superimpose on the P-HMM an internal memory of size M and one infinite-sized disk. By <ref> [AgV] </ref>, the I/O complexity of sorting N records with one disk, no blocking, and an internal memory of size M is T M (N ) = N log N M : (1) The " M " term permits M records to reside initially in the internal memory.
Reference: [ACF] <author> B. Alpern, L. Carter, E. Feig, and T. Selker, </author> <title> "The Uniform Memory Hierarchy Model of Computation," </title> <journal> Algorithmica, </journal> <note> this issue. </note>
Reference-contexts: Subsequently, Nodine and Vitter developed optimal sorting algorithms for P-HMM and P-BT based on distribution sort that are deterministic [NoVb, NoVc]. Another interesting type of hierarchical memory is introduced in <ref> [ACF] </ref>. Parallel hierarchies of this type are studied in [NoVa]. Acknowledgments. We thank Mark Nodine and Greg Plaxton for their helpful comments.
Reference: [LuP] <author> F. Luccio and L. Pagli, </author> <title> "A Model of Sequential Computation Based on a Pipelined Access to Memory," </title> <booktitle> Proceedings of the 27th Annual Allerton Conference on Communication, Control, and Computing (September 1989). </booktitle> <pages> 19 </pages>
Reference-contexts: Typical access cost functions are f (x) = log x and f (x) = x ff , for some ff &gt; 0. A model similar to the BT model that allows pipelined access to memory in O (log n) time was developed independently by Luccio and Pagli <ref> [LuP] </ref>. We can think of a memory hierarchy as being organized into discrete levels, as shown in Figure 1 for HMM; for each k 1, level k contains the 2 k1 locations at addresses 2 k1 , 2 k1 + 1, . . . , 2 k 1.
Reference: [NoVa] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Large-Scale Sorting in Uniform Memory Hierarchies," </title> <note> Journal of Parallel and Distributed Computing (January 1993), special issue. </note>
Reference-contexts: Subsequently, Nodine and Vitter developed optimal sorting algorithms for P-HMM and P-BT based on distribution sort that are deterministic [NoVb, NoVc]. Another interesting type of hierarchical memory is introduced in [ACF]. Parallel hierarchies of this type are studied in <ref> [NoVa] </ref>. Acknowledgments. We thank Mark Nodine and Greg Plaxton for their helpful comments.
Reference: [NoVb] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Optimal Deterministic Sorting on Parallel Memory Hierarchies," </title> <institution> Department of Computer Science, Duke University, </institution> <type> Technical Report, </type> <month> January </month> <year> 1993. </year>
Reference-contexts: The algorithm is based on merge sort and does not seem to provide optimal P-HMM and P-BT algorithms when applied to hierarchical memory. Subsequently, Nodine and Vitter developed optimal sorting algorithms for P-HMM and P-BT based on distribution sort that are deterministic <ref> [NoVb, NoVc] </ref>. Another interesting type of hierarchical memory is introduced in [ACF]. Parallel hierarchies of this type are studied in [NoVa]. Acknowledgments. We thank Mark Nodine and Greg Plaxton for their helpful comments.
Reference: [NoVc] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Optimal Deterministic Sorting on Parallel Disks," </title> <institution> Department of Computer Science, Duke University, </institution> <note> Technical Report , January 1993. </note>
Reference-contexts: The algorithm is based on merge sort and does not seem to provide optimal P-HMM and P-BT algorithms when applied to hierarchical memory. Subsequently, Nodine and Vitter developed optimal sorting algorithms for P-HMM and P-BT based on distribution sort that are deterministic <ref> [NoVb, NoVc] </ref>. Another interesting type of hierarchical memory is introduced in [ACF]. Parallel hierarchies of this type are studied in [NoVa]. Acknowledgments. We thank Mark Nodine and Greg Plaxton for their helpful comments.
Reference: [NoVd] <author> M. H. Nodine and J. S. Vitter, </author> <title> "Large-Scale Sorting in Parallel Memories," </title> <booktitle> Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures (July 1991). </booktitle>
Reference-contexts: Addendum. Recently an alternative two-level sorting algorithm that is both optimal and deterministic was developed by Nodine and Vitter <ref> [NoVd] </ref>. The algorithm is based on merge sort and does not seem to provide optimal P-HMM and P-BT algorithms when applied to hierarchical memory. Subsequently, Nodine and Vitter developed optimal sorting algorithms for P-HMM and P-BT based on distribution sort that are deterministic [NoVb, NoVc].
Reference: [ReV] <author> J. H. Reif and L. G. Valiant, </author> <title> "A Logarithmic Time Sort on Linear Size Networks," </title> <journal> Journal of the ACM 34 (January 1987), </journal> <pages> 60-76. </pages>
Reference-contexts: We assume that the P base memory level locations are interconnected via a network such as a hypercube or cube-connected cycles so that the P records in the base memory level can be sorted in O (log P ) time (perhaps via a randomized algorithm <ref> [ReV] </ref>) and so that two p p P=2 matrices stored in the base memory level can be multiplied in O ( p P ) time. We denote by P-HMM and P-BT the P -hierarchy variants of the hierarchical memory models HMM and BT, as described above.
Reference: [SaV] <author> J. Savage and J. S. Vitter, </author> <title> "Parallelism in Space-Time Tradeoffs," </title> <booktitle> in Advances in Computing Research, </booktitle> <volume> Volume 4 , F. </volume> <editor> P. Preparata, ed., </editor> <publisher> JAI Press, </publisher> <year> 1987, </year> <pages> 117-146. </pages>
Reference-contexts: This can be proved using the same approach as in Theorem 6. By <ref> [SaV] </ref>, the I/O complexity of multipling two k fi k matrices with one disk, no blocking, and an internal memory of size M is p M : (6) Let T M;P (N) be the average number of I/O steps done by the standard matrix multiplication algorithm with respect to an internal
Reference: [ViS] <author> J. S. Vitter and E. A. Shriver, </author> <title> "Algorithms for Parallel Memory I: Two-Level Memories," </title> <journal> Algorithmica, </journal> <note> this issue. </note>
Reference-contexts: For each model, we develop matching upper and lower bounds for the problems of sorting, FFT, and matrix multiplication, which are defined in Section 3. The algorithms that realize the optimal bounds for sorting are applications of the optimal disk sorting algorithm developed in the companion paper <ref> [ViS] </ref> for a two-level memory model with parallel block transfer. We apply the partitioning technique of [ViS] to the one-hierarchy sorting algorithms of [AAC, ACS]. Intuitively, the hierarchical algorithms are optimal because the internal processing in the corresponding two-level algorithms is efficient. <p> The algorithms that realize the optimal bounds for sorting are applications of the optimal disk sorting algorithm developed in the companion paper <ref> [ViS] </ref> for a two-level memory model with parallel block transfer. We apply the partitioning technique of [ViS] to the one-hierarchy sorting algorithms of [AAC, ACS]. Intuitively, the hierarchical algorithms are optimal because the internal processing in the corresponding two-level algorithms is efficient. The main results are given in Section 4 and are proved in Sections 5-10. <p> We shall refer to the P locations, one per hierarchy, at the same relative position in each of the P hierarchies as a track, by analogy with the two-level disk model <ref> [ViS] </ref>. The ith track, for i 1, consists of location i from each of the P hierarchies. In this terminology, the base memory level is the track at location 1. The global memory locations (which refer collectively to the P hierarchies combined) are numbered track by track. <p> CPUs can communicate among one another via the network; we assume, for example, that the P records stored in their accumulators can be sorted in O (log P ) time. 3 Problem Definitions The following definitions of sorting, FFT, and standard matrix multiplication are essentially those of the companion paper <ref> [ViS] </ref>. Sorting Problem Instance: The N records are stored in the first N global memory locations. Goal: The N records are stored in sorted non-decreasing order in the first N global memory locations. Fast Fourier Transform (FFT) Problem Instance: Let N be a power of 2. <p> memory locations. 4 Main Results The fundamental problem that arises in trying to take full advantage of parallel transfer in these models is how to distribute records among the P memory hierarchies so that each hierarchy is kept "busy." We shall show later how the randomized distribution sort algorithm of <ref> [ViS] </ref> for a two-level memory model with parallel block transfer can be used as a basic building block to get optimal sorting algorithms for the hierarchical models. The lower bounds for P-HMM and P-BT are based upon the approach used in [AAC] and [ACS]. <p> In the process we also develop optimal algorithms in the P-HMM and P-BT models for matrix addition, and the so-called "simple" problems (like two-way merging) of [AAC, ACS]. Our techniques can be extended to get optimal algorithms for other problems of <ref> [AAC, ACS, ViS] </ref>, such as searching, generating permutations, and permutation networks. Our optimal P-HMM and P-BT algorithms for sorting and FFT are applications of the optimal algorithms of [ViS] for the two-level model with parallel block transfer, applied to the HMM and BT algorithms given in [AAC, ACS]. <p> Our techniques can be extended to get optimal algorithms for other problems of [AAC, ACS, ViS], such as searching, generating permutations, and permutation networks. Our optimal P-HMM and P-BT algorithms for sorting and FFT are applications of the optimal algorithms of <ref> [ViS] </ref> for the two-level model with parallel block transfer, applied to the HMM and BT algorithms given in [AAC, ACS]. <p> Each bucket is then sorted recursively. The final sorted order is the concatenation of the S sorted buckets. The key component of our P-HMM algorithm is the partitioning technique of <ref> [ViS] </ref>, which we use to spread the records in each bucket evenly among the P memory hierarchies so that the next level of recursion can proceed optimally. The partitioning technique is actually two techniques|Phase 1 and Phase 2|each with its own range of applicability. <p> In this case, we use the Phase 2 partitioning technique, motivated by a different instance of the hashing problem. Both Phase 1 and Phase 2 work with overwhelming probability in their respective ranges of applicability. First we develop some useful notation like that of <ref> [ViS] </ref>, but simplified for our purpose: Hierarchy striping is a programming technique in which the P hierarchies are coordinated so that at any given time the memory locations accessed by the P hierarchies form a track. <p> The pointer last write j;k points to the last location in the kth hierarchy written to by the jth bucket. The pointer next write k points to the next unwritten location in the kth hierarchy. The final carryover we use from <ref> [ViS] </ref> is a simplified notion of diagonal, for use in Phase 2, when N &lt; P 3=2 = ln P . For simplicity of exposition, let us assume that N and P are powers of 2. <p> The value of x in Step 3, which determines the number of partitioning elements, is chosen so that the partitioning analysis from the companion paper <ref> [ViS] </ref> can be modified to show that the records with high probability are distributed evenly among the buckets. 5.1 Logarithmic Access Cost Let us first consider the case f (x) = log x of Theorem 1. <p> The method of choosing the partitioning elements guarantees that the size N j of the jth bucket is at most 2N=(S 1), for each 1 j S; the proof is along the lines of Lemmas 3 and 4 in the companion paper <ref> [ViS] </ref>. The time needed to partition the file in Steps 5, 6, and 7 is O ((N=P ) log (N=P ) + (N=P ) log P ) = O ((N=P ) log N ). The analysis in the companion paper [ViS] can be modified to show that with high probability the <p> lines of Lemmas 3 and 4 in the companion paper <ref> [ViS] </ref>. The time needed to partition the file in Steps 5, 6, and 7 is O ((N=P ) log (N=P ) + (N=P ) log P ) = O ((N=P ) log N ). The analysis in the companion paper [ViS] can be modified to show that with high probability the records in each bucket are distributed evenly over the P hierarchies, so that the time for sorting the buckets recursively in Step 8 is with high probability X T (N j ) + O N log P ; where P <p> This yields as desired the time bound T (N) = O N log N log log N !! with high probability. The probability bounds quoted in Theorem 3 follow from those in <ref> [ViS] </ref>. 2 The following lower bound matches the the algorithm's running time in Theorem 3, and thus the algorithm is optimal. <p> Our optimal P-BT sorting algorithm is a modified version of the one-hierarchy algorithm presented in [ACS]. The key component of the algorithm is our use of the partitioning technique of <ref> [ViS] </ref> to spread the records in each bucket evenly among the P hierarchies. For brevity we present only the portion of the P-BT sorting algorithm whose description differs from the P-HMM sorting algorithm of Section 5. 2. <p> The analysis in the companion paper <ref> [ViS] </ref> can be modified to show that with high probability the records in each bucket are distributed evenly over the P hierarchies, so that the time for sorting the buckets recursively in Step 8 is with high probability X T (N j ) + O N log log N ; where <p> This yields as desired the time bound T (N ) = O N log N with high probability. The probability bounds follow from those derived in <ref> [ViS] </ref>. 2 A lower bound of ((N=P ) log N) time for sorting with f (x) = x ff , for 1 2 ff &lt; 1, follows from the well-known lower bound for sorting in a RAM under the comparison model of computation. 14 7 FFT IN P-HMM 6.2 Other Access <p> We can perform the FFT when N P 2 using the following well-known technique that mimics somewhat the recursive decomposition used in Theorem 3. (The reader is referred to Section 5 in <ref> [ViS] </ref> for a discussion of FFT and the shu*e-merge technique.) 1. We start by computing p N FFTs. The ith FFT is computed on the ith contiguous group of p N =P tracks. 2. We shu*e-merge the records to form p N new contiguous groups of p N =P tracks. <p> Once two elements are in base memory level together, they can be added. 2 16 9 STANDARD MATRIX MULTIPLICATION IN P-HMM We use the following divide-and-conquer algorithm, as for two-level memories <ref> [ViS] </ref>: 1. If k M , we multiply the matrices internally. Otherwise we do the following steps: 2. We subdivide A and B into 8 k=2 fi k=2 submatrices: A 1 A 4 and B 1 -B 4 . <p> The sorting algorithm is a randomized version of distribution sort, using the partitioning technique of the companion paper <ref> [ViS] </ref>, which was developed for optimal sorting on two-level memories with parallel block transfer. Addendum. Recently an alternative two-level sorting algorithm that is both optimal and deterministic was developed by Nodine and Vitter [NoVd].
References-found: 12

