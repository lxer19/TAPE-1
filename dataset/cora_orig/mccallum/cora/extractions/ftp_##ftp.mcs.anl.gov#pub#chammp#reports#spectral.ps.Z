URL: ftp://ftp.mcs.anl.gov/pub/chammp/reports/spectral.ps.Z
Refering-URL: http://www.mcs.anl.gov/chammp/pstswm.html
Root-URL: http://www.mcs.anl.gov
Title: PARALLEL ALGORITHMS FOR THE SPECTRAL TRANSFORM METHOD  
Author: IAN T. FOSTER AND PATRICK H. WORLEY 
Keyword: Key words. Spectral transform method, parallel algorithms, performance analysis  
Abstract: The spectral transform method is a standard numerical technique for solving partial differential equations on a sphere and is widely used in atmospheric circulation models. Recent research has identified several promising algorithms for implementing this method on massively parallel computers; however, no detailed comparison of the different algorithms has previously been attempted. In this paper, we describe these different parallel algorithms and report on computational experiments that we have conducted to evaluate their efficiency on parallel computers. The experiments used a testbed code that solves the nonlinear shallow water equations on a sphere; considerable care was taken to ensure that the experiments provide a fair comparison of the different algorithms and that the results are relevant to global models. We focus on hypercube- and mesh-connected multicomputers with cut-through routing, such as the Intel iPSC/860, DELTA, and Paragon, and the nCUBE/2, but also indicate how the results extend to other parallel computer architectures. The results of this study are relevant not only to the spectral transform method but also to multidimensional FFTs and other parallel transforms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. H. Bailey, </author> <title> FFTs in external or hierarchical memory, </title> <editor> J. </editor> <booktitle> Supercomputing, 4 (1990), </booktitle> <pages> pp. 23-45. </pages>
Reference-contexts: In each vertical layer of the physical domain, fields are approximated on an I fi J longitude-latitude grid, where the I longitude grid lines are evenly spaced and the J latitude grid lines are placed at the Gaussian quadrature points f j g in <ref> [1; 1] </ref>. Transforming from physical coordinates to spectral coordinates involves first performing a Fourier transform for each line of constant latitude, generating the values f m ( j )g on an M fi J wavenumber-latitude grid that we will refer to as the Fourier grid. <p> See Table 5. 5.2. Transpose FFT. An alternative algorithm reorganizes the physical grid from (I X ; J Y ; K) to (I; J Y ; K X ) prior to the forward FFT so that each latitude row is stored within a single processor <ref> [1, 3, 6, 24, 28] </ref>. This eliminates the need for communication during the FFT, but requires communication within the transpose used for the reorganization. After the transform, the Fourier grid is decomposed as (M; J Y ; K X ).
Reference: [2] <author> M. Barnett, R. van de Geijn, S. Gupta, D. G. Payne, L. Shuler, and J. Watts, </author> <title> Inter-processor collective communication library (InterCom), </title> <booktitle> in Proc. Scalable High Performance Computing Conf., </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year> <note> (in press). </note>
Reference-contexts: Mesh-based algorithms. We have restricted ourselves to algorithms designed for one-dimensional processor meshes. In cases where nonsquare logical meshes were mapped to approximately square physical grids, it would be possible in principle to utilize specialized algorithms that exploit the extra connectivity <ref> [2, 32] </ref>. Because our experiments show that "optimal" processor grids are mostly close to square, we believe that these algorithms would not change our results. This issue will be addressed in further research. Physics computations.
Reference: [3] <author> S. Barros and T. Kauranne, </author> <title> On the parallelization of global spectral Eulerian shallow-water models, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science: Proceedings of the Fifth ECMWF Workshop on Use of Parallel Processors in Meteorology, </booktitle> <editor> G.-R. Hoffman and T. Kau-ranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 36-43. </pages>
Reference-contexts: See Table 5. 5.2. Transpose FFT. An alternative algorithm reorganizes the physical grid from (I X ; J Y ; K) to (I; J Y ; K X ) prior to the forward FFT so that each latitude row is stored within a single processor <ref> [1, 3, 6, 24, 28] </ref>. This eliminates the need for communication during the FFT, but requires communication within the transpose used for the reorganization. After the transform, the Fourier grid is decomposed as (M; J Y ; K X ). <p> Because the transpose FFT/transpose LT algorithm also partitions the wavenumber dimension, it also suffers from load imbalance. Since all equipartitions incur the same communication costs in the transpose algorithms, we minimize load imbalance by using the partitioning strategy described by Barros and Kauranne <ref> [3] </ref>. This pairs "short" transforms with "long" transforms in the assignment, and there is no load imbalance when P Y divides (M +1)=2 evenly.
Reference: [4] <author> W. Bourke, </author> <title> An efficient, one-level, primitive-equation spectral model, </title> <journal> Mon. Wea. Rev., </journal> <volume> 102 (1972), </volume> <pages> pp. 687-701. </pages>
Reference-contexts: in climate models comprises a Fourier transform phase, in which fast Fourier transforms (FFTs) are applied to each latitude of a latitude/longitude grid, and a Legendre transform phase, in which Gaussian quadrature is used to approximate the Legendre transform (LT) applied to each longitude (now wavenumber) of the same grid <ref> [4] </ref>. Efficient parallel FFT and LT algorithms have been the topic of intensive research (e.g., see [17, 27, 30, 31]).
Reference: [5] <author> G. L. Browning, J. J. Hack, and P. N. Swarztrauber, </author> <title> A comparison of three numerical methods for solving differential equations on the sphere, </title> <journal> Mon. Wea. Rev., </journal> <volume> 117 (1989), </volume> <pages> pp. 1058-1075. </pages>
Reference-contexts: These equations are frequently used to investigate and compare numerical methods because they present many of the difficulties found in simulating the horizontal dynamics in three-dimensional global atmospheric models <ref> [5] </ref>. The algorithms used to solve the shallow water equations via the spectral transform method are similar to those employed in the NCAR Community Climate Model to handle the horizontal dynamics component of the primitive equations [20].
Reference: [6] <author> D. Dent, </author> <title> The ECMWF model on the Cray Y-MP8, in The Dawn of Massively Parallel Processing in Meteorology, </title> <editor> G.-R. Hoffman and D. K. Maretis, eds., </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: Other researchers have examined a transpose approach, in which communication requirements are encapsulated in a matrix transpose operation. This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model <ref> [6] </ref> and in Loft and Sato's data parallel implementation of CCM2 [24]. It has also been explored by Kauranne and Barros [23], Pelz and Stern [28], and Gartel, Joppich, and Schuller [18]. <p> See Table 5. 5.2. Transpose FFT. An alternative algorithm reorganizes the physical grid from (I X ; J Y ; K) to (I; J Y ; K X ) prior to the forward FFT so that each latitude row is stored within a single processor <ref> [1, 3, 6, 24, 28] </ref>. This eliminates the need for communication during the FFT, but requires communication within the transpose used for the reorganization. After the transform, the Fourier grid is decomposed as (M; J Y ; K X ).
Reference: [7] <author> J. B. Drake, R. E. Flanery, I. T. Foster, J. J. Hack, J. G. Michalakes, R. L. Stevens, D. W. Walker, D. L. Williamson, and P. H. Worley, </author> <title> The message-passing version of the parallel community climate model, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science: Proceedings of the Fifth ECMWF Workshop on Use of Parallel Processors in Meteorology, PARALLEL SPECTRAL TRANSFORM ALGORITHMS 36 G.-R. </booktitle> <editor> Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 500-513. </pages>
Reference-contexts: Box 2008, Bldg. 6012, Oak Ridge, TN 37831-6367. 1 PARALLEL SPECTRAL TRANSFORM ALGORITHMS 2 developed a parallel transform approach based on parallel FFT and quadrature algorithms [14, 34, 37]; this work has been incorporated in a parallel implementation <ref> [7, 8] </ref> of the National Center for Atmospheric Research (NCAR)'s Community Climate Model (CCM2) [20]. Other researchers have examined a transpose approach, in which communication requirements are encapsulated in a matrix transpose operation. <p> This approach performs twice as many transposes as the transpose FFT/distributed LT algorithm, but can use 5 times more processors without load imbalance. It has been used successfully in the message-passing version of CCM2 <ref> [7] </ref>. 1b. We can avoid the double transpose at the cost of redundant work and some other additional communication by duplicating one field and decomposing over K levels and 3 sets of 3 fields.
Reference: [8] <author> J. B. Drake, I. T. Foster, J. J. Hack, J. G. Michalakes, B. D. Semeraro, B. Toonen, D. L. Williamson, and P. H. Worley, PCCM2: </author> <title> A GCM adapted for scalable parallel computer, </title> <booktitle> in Fifth Symposium on Global Change Studies, </booktitle> <publisher> American Meteorological Society, </publisher> <address> Boston, </address> <year> 1994, </year> <pages> pp. 91-98. </pages>
Reference-contexts: Box 2008, Bldg. 6012, Oak Ridge, TN 37831-6367. 1 PARALLEL SPECTRAL TRANSFORM ALGORITHMS 2 developed a parallel transform approach based on parallel FFT and quadrature algorithms [14, 34, 37]; this work has been incorporated in a parallel implementation <ref> [7, 8] </ref> of the National Center for Atmospheric Research (NCAR)'s Community Climate Model (CCM2) [20]. Other researchers have examined a transpose approach, in which communication requirements are encapsulated in a matrix transpose operation.
Reference: [9] <author> A. Dubey, M. Zubair, and C. E. Grosch, </author> <title> A general purpose subroutine for fast Fourier transform on a distributed memory parallel machine, Parallel Computing, </title> <note> (to appear). </note>
Reference-contexts: The distributed FFT can also be modified to use log 4 Q stages and can then exploit factors of 4 to reduce computation costs. And it is possible to apply a transpose-like algorithm within the FFT itself <ref> [9] </ref>. These hybrid algorithms can improve performance somewhat in regimes where message startup costs and data volume costs are comparable. However, they place additional requirements on problem size and processor counts. Mesh-based algorithms. We have restricted ourselves to algorithms designed for one-dimensional processor meshes.
Reference: [10] <author> T. H. Dunigan, </author> <title> Communication performance of the Intel Touchstone DELTA Mesh, </title> <type> Tech. Report ORNL/TM-11983, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> December </month> <year> 1991. </year> <title> [11] , Performance of the Intel iPSC/860 and the Ncube 6400 hypercubes, </title> <booktitle> Parallel Computing, 17 (1991), </booktitle> <pages> pp. 1285-1302. </pages>
Reference-contexts: Target Computers. We performed experiments on the five parallel computer systems listed in Table 7. These systems have similar architectures and programming models, but vary considerably in their communication and computational capabilities. Our values for t s and t w differ from those reported by most researchers <ref> [11, 10] </ref> because we measure the time required to swap floating-point values between two processors rather than the time to send bytes from a source to a destination.
Reference: [12] <author> A. Edelman, </author> <title> Optimal matrix transposition and bit reversal on hypercubes: all-to-all personalized communication, </title> <journal> J. Par. Dist. Comp., </journal> <volume> 11 (1991), </volume> <pages> pp. 328-331. </pages>
Reference-contexts: The two primary implementation approaches require fi (P X ) and fi (log P X ) communication steps, respectively. fi (Q) Transpose. The first algorithm proceeds in Q 1 steps on Q processors: at each step, each processor sends 1=Q of its data to another processor <ref> [12, 22, 30] </ref>. PARALLEL SPECTRAL TRANSFORM ALGORITHMS 13 Communication cost is as follows. T linear transpose = (Q 1) D ! Substituting appropriate values for D and Q and counting both the forward and inverse FFTs, we obtain the expression in Table 1. <p> Also, the transform must be divided into the same number of stages as the transpose algorithm to allow for interleaving. This restriction may diminish the computational rate. Bandwidth Limitations. Neither transpose algorithm suffers from significant bandwidth limitations on hypercubes <ref> [12, 17, 22] </ref>, but both do so on mesh architectures. In the fi (Q) transpose, a total of (Q 3 Q)=3 hops are traversed on 2 (Q 1) wires, requiring that the data volume be scaled by Q (Q + 1)=6 instead of Q 1.
Reference: [13] <author> J. O. Eklundh, </author> <title> A fast computer method for matrix transposing, </title> <journal> IEEE Trans. Comput., </journal> <volume> C-21 (1972), </volume> <pages> pp. 801-803. </pages>
Reference-contexts: The schedules used in our experiments send at most one message to each processor during a given step. fi (log Q) Transpose. The transpose can be performed in (log 2 Q) communication steps at the cost of increased communication volume <ref> [13, 29] </ref>. We first partition processors into two sets. Each processor sends to the corresponding processor in the other set a single message containing all the data that it possesses that is destined for processors in the other set.
Reference: [14] <author> I. Foster, W. Gropp, and R. Stevens, </author> <title> The parallel scalability of the spectral transform method, </title> <journal> Mon. Wea. Rev., </journal> <volume> 120 (1992), </volume> <pages> pp. 835-850. </pages>
Reference-contexts: Box 2008, Bldg. 6012, Oak Ridge, TN 37831-6367. 1 PARALLEL SPECTRAL TRANSFORM ALGORITHMS 2 developed a parallel transform approach based on parallel FFT and quadrature algorithms <ref> [14, 34, 37] </ref>; this work has been incorporated in a parallel implementation [7, 8] of the National Center for Atmospheric Research (NCAR)'s Community Climate Model (CCM2) [20]. Other researchers have examined a transpose approach, in which communication requirements are encapsulated in a matrix transpose operation. <p> In contrast, on a 1-D mesh of Q processors, each processor generates messages that must traverse 1, 2, ..., 2 q1 ; 1 hops distant in the q + 1 steps of the algorithm <ref> [14, 19] </ref>. The total number of hops traversed by these messages is Q (1 + P q1 i=0 2 i ) = Q 2 . This represents the number of wires to which a processor requires exclusive access during the FFT. <p> Empirical Studies. As indicated in preceding sections, the analytic models introduced in Tables 1-4 can provide insights into performance issues. The models can also be used to evaluate scalability and to make rough performance estimates <ref> [14, 16, 23] </ref>. For definitive algorithm comparisons, however, empirical studies are required to calibrate and validate the models. We expect constant factors to matter for a large range of multiprocessor sizes, and the relative efficiency of the implementations of the different algorithms can also play a crucial role.
Reference: [15] <author> I. Foster and B. Toonen, </author> <title> Load balancing in climate models, </title> <booktitle> Proc. 1994 Scalable High-Performance Computing Conf., </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year>
Reference-contexts: These load variations need to be accounted for when developing a parallel model, and PARALLEL SPECTRAL TRANSFORM ALGORITHMS 34 can influence our choice of spectral transform algorithm. As our experience suggests that load variation is very model dependent <ref> [26, 15] </ref>, we have not considered these effects in this paper. However, note that transpose FFT algorithms can combine the redistribution required for load balancing with the transpose.
Reference: [16] <author> I. T. Foster and P. H. Worley, </author> <title> Parallelizing the spectral transform method: A comparison of alternative parallel algorithms, in Parallel Processing for Scientific Computing, </title> <editor> R. F. Sincovec, D. E. Keyes, M. R. Leuze, L. R. Petzold, and D. A. Reed, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, PA, </address> <year> 1993, </year> <pages> pp. 100-107. </pages>
Reference-contexts: In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both. A comprehensive comparison of these algorithms has not previously been attempted. (Both <ref> [16] </ref> and [23] provide a qualitative analysis of some algorithms, but not detailed quantitative results or performance models.) Hence, it is difficult to evaluate the performance tradeoffs that arise when choosing a parallel algorithm for a particular application. <p> Empirical Studies. As indicated in preceding sections, the analytic models introduced in Tables 1-4 can provide insights into performance issues. The models can also be used to evaluate scalability and to make rough performance estimates <ref> [14, 16, 23] </ref>. For definitive algorithm comparisons, however, empirical studies are required to calibrate and validate the models. We expect constant factors to matter for a large range of multiprocessor sizes, and the relative efficiency of the implementations of the different algorithms can also play a crucial role.
Reference: [17] <author> G. C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker, </author> <title> Solving Problems on Concurrent Processors, </title> <journal> vol. </journal> <volume> 1, </volume> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Efficient parallel FFT and LT algorithms have been the topic of intensive research (e.g., see <ref> [17, 27, 30, 31] </ref>). <p> Each of the first q stages of the FFT involves a pairwise exchange of 2 dq1 data or intermediate results with another processor, while the last d q stages can proceed without further communication after a single additional exchange <ref> [17, 19, 27, 30, 31] </ref>. <p> Both the one- and two-block algorithms can be mapped to a hypercube without competition for bandwidth <ref> [17] </ref>. As noted in x4.3, they will suffer from bandwidth limitations on a 1-D mesh. Applying (16) to the shallow water code, we obtain the expression in Table 2. This omits the additional exchange required in the real FFT, but is still a reasonable lower bound. Algorithm Limitations. <p> Also, the transform must be divided into the same number of stages as the transpose algorithm to allow for interleaving. This restriction may diminish the computational rate. Bandwidth Limitations. Neither transpose algorithm suffers from significant bandwidth limitations on hypercubes <ref> [12, 17, 22] </ref>, but both do so on mesh architectures. In the fi (Q) transpose, a total of (Q 3 Q)=3 hops are traversed on 2 (Q 1) wires, requiring that the data volume be scaled by Q (Q + 1)=6 instead of Q 1. <p> Butterfly Sum. The butterfly sum algorithm is a hybrid of two algorithms [33]. For long vectors, we use a recursive halving algorithm <ref> [17] </ref> that utilizes a butterfly PARALLEL SPECTRAL TRANSFORM ALGORITHMS 16 Table 3 Communication Characteristics of Parallel LT Algorithms Algorithm Messages Data Volume Ring sum 2 (P Y 1) 8 (J + 1)(2J + 5)K (P Y 1) Butterfly sum 2 log 2 P Y 8 (J + 1)(2J + 5)K (P
Reference: [18] <author> U. G artel, W. Joppich, and A. Sch uller, </author> <title> Parallelizing the ECMWF's weather forecast program: The 2D case, </title> <booktitle> Parallel Computing, 19 (1993), </booktitle> <pages> pp. 1413-1426. </pages>
Reference-contexts: This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model [6] and in Loft and Sato's data parallel implementation of CCM2 [24]. It has also been explored by Kauranne and Barros [23], Pelz and Stern [28], and Gartel, Joppich, and Schuller <ref> [18] </ref>. In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both.
Reference: [19] <author> A. Gupta and V. Kumar, </author> <title> The scalability of FFT on parallel computers, </title> <journal> IEEE Trans. Par. Dist. Sys., </journal> <volume> 4 (1993), </volume> <pages> pp. 922-932. </pages>
Reference-contexts: Each of the first q stages of the FFT involves a pairwise exchange of 2 dq1 data or intermediate results with another processor, while the last d q stages can proceed without further communication after a single additional exchange <ref> [17, 19, 27, 30, 31] </ref>. <p> In contrast, on a 1-D mesh of Q processors, each processor generates messages that must traverse 1, 2, ..., 2 q1 ; 1 hops distant in the q + 1 steps of the algorithm <ref> [14, 19] </ref>. The total number of hops traversed by these messages is Q (1 + P q1 i=0 2 i ) = Q 2 . This represents the number of wires to which a processor requires exclusive access during the FFT.
Reference: [20] <author> J. J. Hack, B. A. Boville, B. P. Briegleb, J. T. Kiehl, P. J. Rasch, and D. L. Williamson, </author> <title> Description of the NCAR Community Climate Model (CCM2), </title> <type> NCAR Tech. </type> <institution> Note NCAR/TN-382+STR, National Center for Atmospheric Research, Boulder, Colo., </institution> <year> 1992. </year>
Reference-contexts: Bldg. 6012, Oak Ridge, TN 37831-6367. 1 PARALLEL SPECTRAL TRANSFORM ALGORITHMS 2 developed a parallel transform approach based on parallel FFT and quadrature algorithms [14, 34, 37]; this work has been incorporated in a parallel implementation [7, 8] of the National Center for Atmospheric Research (NCAR)'s Community Climate Model (CCM2) <ref> [20] </ref>. Other researchers have examined a transpose approach, in which communication requirements are encapsulated in a matrix transpose operation. This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model [6] and in Loft and Sato's data parallel implementation of CCM2 [24]. <p> The algorithms used to solve the shallow water equations via the spectral transform method are similar to those employed in the NCAR Community Climate Model to handle the horizontal dynamics component of the primitive equations <ref> [20] </ref>. Hence, a model that solves the shallow water equations on multiple (independent) levels during each timestep of the simulation provides a framework in which the performance of CCM2's horizontal dynamics can be studied in isolation from the other aspects of the full model.
Reference: [21] <author> J. J. Hack and R. Jakob, </author> <title> Description of a global shallow water model based on the spectral transform method, </title> <type> NCAR Tech Note NCAR/TN-343+STR, </type> <institution> National Center for Atmospheric Research, Boulder, CO, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Principal Data Structures in Spectral Transform In the shallow water equation code <ref> [21] </ref>, each timestep begins by calculating the nonlinear terms U j, V j, U , V , and + (U 2 + V 2 )=(2 (1 2 )) on the physical grid. Next, the nonlinear terms and the state variables j, ffi, and are Fourier transformed. <p> To permit a fair comparison of the suitability of the various algorithms for atmospheric models, we have incorporated the algorithms in a single testbed code called PSTSWM (for parallel spectral transform shallow water model). PSTSWM is a message-passing parallel implementation of the sequential Fortran code STSWM 2.0 <ref> [21] </ref>. STSWM uses the spectral transform method to solve the nonlinear shallow water equations on a rotating sphere; its data structures and implementation are based directly on equivalent structures and algorithms in CCM2.
Reference: [22] <author> S. L. Johnsson and C.-T. Ho, </author> <title> Algorithms for matrix transposition on Boolean N-cube configured ensemble architectures, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 9 (1988), </volume> <pages> pp. 419-454. </pages>
Reference-contexts: For example, the FFT described above can be organized to execute without competition for bandwidth on a hypercube <ref> [22] </ref>. In contrast, on a 1-D mesh of Q processors, each processor generates messages that must traverse 1, 2, ..., 2 q1 ; 1 hops distant in the q + 1 steps of the algorithm [14, 19]. <p> the two-dimensional logical processor mesh of size P X fi P Y = 2 q fi 2 r = 2 p is mapped into a hypercube of dimension p in such a way that each processor row and column is mapped to a subcube of dimension q and r, respectively <ref> [22] </ref>. Hence, performance analysis reduces to the problem of determining the cost of an FFT or LT in a hypercube. On a 2-D mesh computer, we assume that the P X fi P Y logical processor mesh is mapped to an equivalent physical mesh. <p> The two primary implementation approaches require fi (P X ) and fi (log P X ) communication steps, respectively. fi (Q) Transpose. The first algorithm proceeds in Q 1 steps on Q processors: at each step, each processor sends 1=Q of its data to another processor <ref> [12, 22, 30] </ref>. PARALLEL SPECTRAL TRANSFORM ALGORITHMS 13 Communication cost is as follows. T linear transpose = (Q 1) D ! Substituting appropriate values for D and Q and counting both the forward and inverse FFTs, we obtain the expression in Table 1. <p> Also, the transform must be divided into the same number of stages as the transpose algorithm to allow for interleaving. This restriction may diminish the computational rate. Bandwidth Limitations. Neither transpose algorithm suffers from significant bandwidth limitations on hypercubes <ref> [12, 17, 22] </ref>, but both do so on mesh architectures. In the fi (Q) transpose, a total of (Q 3 Q)=3 hops are traversed on 2 (Q 1) wires, requiring that the data volume be scaled by Q (Q + 1)=6 instead of Q 1.
Reference: [23] <author> T. Kauranne and S. Barros, </author> <title> Scalability estimates of parallel spectral atmospheric models, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science: Proceedings of the Fifth ECMWF Workshop on Use of Parallel Processors in Meteorology, </booktitle> <editor> G.-R. Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 312-328. </pages>
Reference-contexts: This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model [6] and in Loft and Sato's data parallel implementation of CCM2 [24]. It has also been explored by Kauranne and Barros <ref> [23] </ref>, Pelz and Stern [28], and Gartel, Joppich, and Schuller [18]. In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both. A comprehensive comparison of these algorithms has not previously been attempted. (Both [16] and [23] provide a qualitative analysis of <p> explored by Kauranne and Barros <ref> [23] </ref>, Pelz and Stern [28], and Gartel, Joppich, and Schuller [18]. In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both. A comprehensive comparison of these algorithms has not previously been attempted. (Both [16] and [23] provide a qualitative analysis of some algorithms, but not detailed quantitative results or performance models.) Hence, it is difficult to evaluate the performance tradeoffs that arise when choosing a parallel algorithm for a particular application. <p> As K can be significantly smaller than I, this restriction is limiting for the PARALLEL SPECTRAL TRANSFORM ALGORITHMS 14 transpose algorithms. One approach to mitigating this problem is to decompose also over the field "dimension" (8 for the forward FFT and 5 for the inverse) <ref> [23] </ref>. Many of these fields must be reunited for the LT phase, however, resulting in other performance problems. This generalization and the associated problems are discussed in x9. The fi (log Q) transpose algorithm requires that P X be a power of two. Load Balance. <p> Empirical Studies. As indicated in preceding sections, the analytic models introduced in Tables 1-4 can provide insights into performance issues. The models can also be used to evaluate scalability and to make rough performance estimates <ref> [14, 16, 23] </ref>. For definitive algorithm comparisons, however, empirical studies are required to calibrate and validate the models. We expect constant factors to matter for a large range of multiprocessor sizes, and the relative efficiency of the implementations of the different algorithms can also play a crucial role. <p> Nonpower-of-two problem dimensions also cause load imbalances, and the amount of performance degradation is strongly algorithm dependent. Real weather and climate models often use a number of vertical levels significantly smaller than the other dimensions of the problem. For example, T213L31 is used in some operational weather-forecast models <ref> [23] </ref>, corresponding to a 640fi320fi31 physical grid. The transpose FFT algorithms suffer because they must use a larger number of processors for the LT than an algorithm that uses a distributed FFT. Decomposing "field" dimension.
Reference: [24] <author> R. D. Loft and R. K. Sato, </author> <title> Implementation of the NCAR CCM2 on the Connection Machine, </title> <booktitle> in Parallel Supercomputing in Atmospheric Science: Proceedings of the Fifth ECMWF Workshop on Use of Parallel Processors in Meteorology, </booktitle> <editor> G.-R. Hoffman and T. Kauranne, eds., </editor> <publisher> World Scientific Publishing Co. Pte. Ltd., </publisher> <address> Singapore, </address> <year> 1993, </year> <pages> pp. 371-393. </pages>
Reference-contexts: Other researchers have examined a transpose approach, in which communication requirements are encapsulated in a matrix transpose operation. This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model [6] and in Loft and Sato's data parallel implementation of CCM2 <ref> [24] </ref>. It has also been explored by Kauranne and Barros [23], Pelz and Stern [28], and Gartel, Joppich, and Schuller [18]. In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both. <p> See Table 5. 5.2. Transpose FFT. An alternative algorithm reorganizes the physical grid from (I X ; J Y ; K) to (I; J Y ; K X ) prior to the forward FFT so that each latitude row is stored within a single processor <ref> [1, 3, 6, 24, 28] </ref>. This eliminates the need for communication during the FFT, but requires communication within the transpose used for the reorganization. After the transform, the Fourier grid is decomposed as (M; J Y ; K X ).
Reference: [25] <author> B. Machenhauer, </author> <title> The spectral method, in Numerical Methods Used in Atmospheric Models, vol. II of GARP Pub. </title> <journal> Ser. </journal> <volume> No. 17. </volume> <publisher> JOC, World Meteorological Organization, </publisher> <address> Geneva, Switzer-land, </address> <year> 1979, </year> <journal> ch. </journal> <volume> 3, </volume> <pages> pp. 121-275. </pages>
Reference-contexts: The spherical harmonic functions are the eigensolutions of the Laplacian operator in spherical coordinates and constitute a complete and orthogonal expansion basis for square integrable functions on the sphere. Additional properties of these functions can be found in <ref> [25] </ref>. In the truncated expansion, M is the highest Fourier mode and N (m) is the highest degree of the associated Legendre function in the north-south representation. Since the physical quantities are real, m n is the complex conjugate of m n .
Reference: [26] <author> J. Michalakes, </author> <title> Analysis of workload and load balancing issues in the NCAR Community Climate Model, </title> <type> Tech. report ANL/MCS-TM-144, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1991. </year>
Reference-contexts: These load variations need to be accounted for when developing a parallel model, and PARALLEL SPECTRAL TRANSFORM ALGORITHMS 34 can influence our choice of spectral transform algorithm. As our experience suggests that load variation is very model dependent <ref> [26, 15] </ref>, we have not considered these effects in this paper. However, note that transpose FFT algorithms can combine the redistribution required for load balancing with the transpose.
Reference: [27] <author> M. Pease, </author> <title> An adaptation of the fast Fourier transform for parallel processing, J. Assoc. Comput. PARALLEL SPECTRAL TRANSFORM ALGORITHMS 37 Mach., </title> <booktitle> 15 (1968), </booktitle> <pages> pp. 252-264. </pages>
Reference-contexts: Efficient parallel FFT and LT algorithms have been the topic of intensive research (e.g., see <ref> [17, 27, 30, 31] </ref>). <p> Each of the first q stages of the FFT involves a pairwise exchange of 2 dq1 data or intermediate results with another processor, while the last d q stages can proceed without further communication after a single additional exchange <ref> [17, 19, 27, 30, 31] </ref>.
Reference: [28] <author> R. B. Pelz and W. F. Stern, </author> <title> A balanced parallel algorithm for spectral global climate models, in Parallel Processing for Scientific Computing, </title> <editor> R. F. Sincovec, D. E. Keyes, M. R. Leuze, L. R. Petzold, and D. A. Reed, eds., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <address> Philadelphia, PA, </address> <year> 1993, </year> <pages> pp. 126-128. </pages>
Reference-contexts: This approach is used, for example, in the European Center for Medium-Range Weather Forecasts spectral weather model [6] and in Loft and Sato's data parallel implementation of CCM2 [24]. It has also been explored by Kauranne and Barros [23], Pelz and Stern <ref> [28] </ref>, and Gartel, Joppich, and Schuller [18]. In addition to the transform and transpose approaches, a variety of hybrid algorithms are possible that combine aspects of both. <p> See Table 5. 5.2. Transpose FFT. An alternative algorithm reorganizes the physical grid from (I X ; J Y ; K) to (I; J Y ; K X ) prior to the forward FFT so that each latitude row is stored within a single processor <ref> [1, 3, 6, 24, 28] </ref>. This eliminates the need for communication during the FFT, but requires communication within the transpose used for the reorganization. After the transform, the Fourier grid is decomposed as (M; J Y ; K X ).
Reference: [29] <author> H. S. Stone, </author> <title> Parallel processing with the perfect shu*e, </title> <journal> IEEE Trans. Comput., </journal> <volume> C-21 (1971), </volume> <pages> pp. 153-161. </pages>
Reference-contexts: The schedules used in our experiments send at most one message to each processor during a given step. fi (log Q) Transpose. The transpose can be performed in (log 2 Q) communication steps at the cost of increased communication volume <ref> [13, 29] </ref>. We first partition processors into two sets. Each processor sends to the corresponding processor in the other set a single message containing all the data that it possesses that is destined for processors in the other set.
Reference: [30] <author> P. N. Swarztrauber, </author> <title> Multiprocessor FFTs, </title> <booktitle> Parallel Computing, 5 (1987), </booktitle> <pages> pp. 197-210. </pages>
Reference-contexts: Efficient parallel FFT and LT algorithms have been the topic of intensive research (e.g., see <ref> [17, 27, 30, 31] </ref>). <p> Each of the first q stages of the FFT involves a pairwise exchange of 2 dq1 data or intermediate results with another processor, while the last d q stages can proceed without further communication after a single additional exchange <ref> [17, 19, 27, 30, 31] </ref>. <p> The two primary implementation approaches require fi (P X ) and fi (log P X ) communication steps, respectively. fi (Q) Transpose. The first algorithm proceeds in Q 1 steps on Q processors: at each step, each processor sends 1=Q of its data to another processor <ref> [12, 22, 30] </ref>. PARALLEL SPECTRAL TRANSFORM ALGORITHMS 13 Communication cost is as follows. T linear transpose = (Q 1) D ! Substituting appropriate values for D and Q and counting both the forward and inverse FFTs, we obtain the expression in Table 1.
Reference: [31] <author> P. N. Swarztrauber, W. L. Briggs, R. A. Sweet, V. E. Henson, and J. Otto, </author> <title> Bluestein's FFT for arbitrary n on the hypercube, </title> <booktitle> Parallel Computing, 17 (1991), </booktitle> <pages> pp. 607-618. </pages>
Reference-contexts: Efficient parallel FFT and LT algorithms have been the topic of intensive research (e.g., see <ref> [17, 27, 30, 31] </ref>). <p> Each of the first q stages of the FFT involves a pairwise exchange of 2 dq1 data or intermediate results with another processor, while the last d q stages can proceed without further communication after a single additional exchange <ref> [17, 19, 27, 30, 31] </ref>.
Reference: [32] <author> S. S. Takkella and S. R. Seidel, </author> <title> Complete exchange and broadcast algorithms for meshes, </title> <booktitle> in Proc. Scalable High Performance Computing Conf., </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year> <note> (in press). </note>
Reference-contexts: Mesh-based algorithms. We have restricted ourselves to algorithms designed for one-dimensional processor meshes. In cases where nonsquare logical meshes were mapped to approximately square physical grids, it would be possible in principle to utilize specialized algorithms that exploit the extra connectivity <ref> [2, 32] </ref>. Because our experiments show that "optimal" processor grids are mostly close to square, we believe that these algorithms would not change our results. This issue will be addressed in further research. Physics computations.
Reference: [33] <author> R. A. van de Geijn, </author> <title> Efficient global combine operations, </title> <booktitle> in The Sixth Distributed Memory Computing Conference Proceedings, </booktitle> <editor> Q. F. Stout and M. Wolfe, eds., </editor> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991, </year> <pages> pp. 291-294. </pages>
Reference-contexts: Butterfly Sum. The butterfly sum algorithm is a hybrid of two algorithms <ref> [33] </ref>.
Reference: [34] <author> D. W. Walker, P. H. Worley, and J. B. Drake, </author> <title> Parallelizing the spectral transform method. Part II, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4 (1992), </volume> <pages> pp. 509-531. </pages>
Reference-contexts: Box 2008, Bldg. 6012, Oak Ridge, TN 37831-6367. 1 PARALLEL SPECTRAL TRANSFORM ALGORITHMS 2 developed a parallel transform approach based on parallel FFT and quadrature algorithms <ref> [14, 34, 37] </ref>; this work has been incorporated in a parallel implementation [7, 8] of the National Center for Atmospheric Research (NCAR)'s Community Climate Model (CCM2) [20]. Other researchers have examined a transpose approach, in which communication requirements are encapsulated in a matrix transpose operation. <p> Computation/Communication Overlap. To exploit overlap, the single-block FFT can be divided into two, allowing one block's communication to be overlapped with the other's computation <ref> [34] </ref>. Only the first swap involving the first block is not overlapped with computation. This process requires twice as many messages, as indicated in Table 1, but has been shown to be cost effective on some multiprocessors. <p> As the Fourier transform is unordered, the distributed FFT algorithm assigns blocks of permuted Fourier coefficients to the P x processor columns. This assignment approximately balances the assignment of "short" Legendre transforms (large wavenumbers) and "long" Legendre transforms (small wavenumbers) <ref> [34] </ref>, but the load PARALLEL SPECTRAL TRANSFORM ALGORITHMS 18 Table 5 Relative Increase in Computation and Communication Costs in Fourier Transforms As a Result of Load Imbalances Algorithm Computation Cost Data Volume Dist. FFT/Dist. LT dJ=(2P Y )e dJ=(2P Y )e Trans. FFT/Dist.
Reference: [35] <author> W. Washington and C. Parkinson, </author> <title> An Introduction to Three-Dimensional Climate Modeling, </title> <publisher> University Science Books, </publisher> <address> Mill Valley, CA, </address> <year> 1986. </year>
Reference-contexts: Let i, j, and k denote unit vectors in spherical geometry, V denote the horizontal velocity, V = iu + jv, denote the geopotential, and f denote the Coriolis term. Then the horizontal momentum and mass continuity equations can be written as <ref> [35] </ref> DV = f k fi V r (1) Dt where the substantial derivative is given by D ( ) j @t The spectral transform method does not solve these equations directly; rather, it uses a streamfunction-vorticity formulation in order to work with scalar fields. <p> For a triangular truncation, exact, unaliased transforms of quadratic terms are obtained if I 3M + 1 and if I = 2J <ref> [35] </ref>. In this work we also use a fast Fourier transform (FFT) algorithm that requires I to be a power of two. As is commonly done, for a given M we choose I to be the minimum power of two satisfying I 3M + 1, and set J = I=2.
Reference: [36] <author> D. L. Williamson, J. B. Drake, J. J. Hack, R. Jakob, and P. N. Swarztrauber, </author> <title> A standard test set for numerical approximations to the shallow water equations on the sphere, </title> <journal> J. Computational Physics, </journal> <volume> 102 (1992), </volume> <pages> pp. 211-224. </pages>
Reference-contexts: Section 10 presents our conclusions. 2. The Shallow Water Equations. The nonlinear shallow water equations on a rotating sphere constitute a two-dimensional atmospheric-like fluid prediction model that exhibits many of the features of more complete models <ref> [36] </ref>. These equations are frequently used to investigate and compare numerical methods because they present many of the difficulties found in simulating the horizontal dynamics in three-dimensional global atmospheric models [5]. <p> All experiments used the performance benchmark described in <ref> [36] </ref>: global steady state nonlinear zonal geostrophic flow. Experiments were performed for problem sizes T21L8, T42L16, and T85L32. 8.4. Results: Algorithm Selection. In presenting the results of the algorithm selection experiments, we do not discuss the communication parameters studied (see [38] for details) but focus on the algorithms.
Reference: [37] <author> P. H. Worley and J. B. Drake, </author> <title> Parallelizing the spectral transform method, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 4 (1992), </volume> <pages> pp. 269-291. </pages>
Reference-contexts: Box 2008, Bldg. 6012, Oak Ridge, TN 37831-6367. 1 PARALLEL SPECTRAL TRANSFORM ALGORITHMS 2 developed a parallel transform approach based on parallel FFT and quadrature algorithms <ref> [14, 34, 37] </ref>; this work has been incorporated in a parallel implementation [7, 8] of the National Center for Atmospheric Research (NCAR)'s Community Climate Model (CCM2) [20]. Other researchers have examined a transpose approach, in which communication requirements are encapsulated in a matrix transpose operation. <p> When the interleaving is organized so that the communication of one stage of the algorithm is overlapped with the computation of the next stage, the ring sum is able to perform fi (J 4 =P Y ) computation while communicating fi (P Y J 3 ) data <ref> [37] </ref>. This overlapping can be highly effective for small P Y and/or large J , decreasing the cost of communication significantly. Overlap is less effective for the butterfly sum.
Reference: [38] <author> P. H. Worley and I. T. Foster, </author> <title> Parallel Spectral Transform Shallow Water Model: A runtime-tunable parallel benchmark code, </title> <booktitle> in Proc. Scalable High Performance Computing Conf., </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year> <note> (in press). </note>
Reference-contexts: In the analytic studies, we develop models that characterize the performance of the various spectral transform algorithms by relating communication requirements and load imbalances to problem size, processor count, and other parameters. The empirical studies utilize a parallel shallow water equation solver designed specifically for these experiments <ref> [38] </ref>. Considerable care has been taken to ensure that experiments are as fair as possible, that is, that one algorithm is not unduly favored through choice of data structures, greater optimization, etc. <p> In addition, the distributed FFT can use either the two-block algorithm that permits computation/communication overlap or the one-block algorithm, and the ring sum LT can use either the overlap or nonoverlap algorithms. Additional parameters select a range of variants of each of these major algorithms <ref> [38] </ref>. Note that all parallel algorithms were carefully implemented, eliminating unnecessary buffer copying and exploiting our knowledge of the context in which they are called. At the present time, this allows us to achieve better performance than can be achieved by calling available vendor-supplied routines. <p> All experiments used the performance benchmark described in [36]: global steady state nonlinear zonal geostrophic flow. Experiments were performed for problem sizes T21L8, T42L16, and T85L32. 8.4. Results: Algorithm Selection. In presenting the results of the algorithm selection experiments, we do not discuss the communication parameters studied (see <ref> [38] </ref> for details) but focus on the algorithms. Table 8 summarizes both the algorithms considered and those selected for further consideration on different machines. For the most part, the table is self-explanatory. We always selected at least one distributed algorithm and one transpose algorithm for both the LT and FFT.
References-found: 37

