URL: http://osl-www.cs.umass.edu/~stefanov/nips-sl2-final-rev.ps
Refering-URL: http://www.cs.umass.edu/~stefanov/
Root-URL: 
Title: Learning to Schedule Straight-Line Code  
Author: Eliot Moss, Paul Utgoff, John Cavazos Doina Precup, Darko Stefanovi c Carla Brodley, David Scheeff 
Address: Mass. Amherst, MA 01003  W. Lafayette, IN 47907  
Affiliation: Dept. of Comp. Sci., Univ. of  Sch. of Elec. and Comp. Eng. Purdue University  
Abstract: Program execution speed on modern computers is sensitive, by a factor of two or more, to the order in which instructions are presented to the processor. To realize potential execution efficiency, an optimizing compiler must employ a heuristic algorithm for instruction scheduling. Such algorithms are painstakingly hand-crafted, which is expensive and time-consuming. We show how to cast the instruction scheduling problem as a learning task, obtaining the heuristic scheduling algorithm automatically. Our focus is the narrower problem of scheduling straight-line code (also called basic blocks of instructions). Our empirical results show that just a few features are adequate for quite good performance at this task for a real modern processor, and that any of several supervised learning methods perform nearly opti mally with respect to the features used.
Abstract-found: 1
Intro-found: 1
Reference: <author> Beaty, S., Colcord, S., & Sweany, P. </author> <year> (1996). </year> <title> Using genetic algorithms to fine-tune instruction-scheduling heuristics. </title> <booktitle> In Proc. of the Int'l Conf. on Massively Parallel Computer Systems. </booktitle> <institution> Digital Equipment Corporation, </institution> <year> (1992). </year> <title> DECchip 21064-AA Microprocessor Hardware Reference Manual, </title> <address> Maynard, MA, first edition, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Being a patent, this work does not offer experimental results. The other related item is the application of genetic algorithms to tuning weights of heuristics used in a greedy scheduler <ref> (Beaty, S., Colcord, & Sweany, 1996) </ref>. The authors showed that different hardware targets resulted in different learned weights, but they did not offer experimental evaluation of the quality of the resulting schedulers.
Reference: <author> Haykin, S. </author> <year> (1994). </year> <title> Neural networks: A comprehensive foundation. </title> <address> New York, NY: </address> <publisher> Macmillan. </publisher>
Reference: <author> Moss, E., Cavazos, J., Stefanovic, D., Utgoff, P., Precup, D., Scheeff, D., & Brodley, C. </author> <year> (1997). </year> <title> Learning Policies for Local Instruction Scheduling. </title> <note> Submitted for publication. </note>
Reference-contexts: While we obtained good performance predictions, we did not report performance on a real processor. (More recently we obtained those results <ref> (Moss, et al., 1997) </ref>; ELF tied Orig for the best scheme.) This raises issues not only of faithfulness of the simulator to reality, but also of global instruction scheduling, i.e., across basic blocks, and of somewhat more general rewritings that allow more reorderings of instructions.
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R.J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart & McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Scheeff, D., Brodley, C., Moss, E., Cavazos, J., Stefanovic, D. </author> <year> (1997). </year> <title> Applying Reinforcement Learning to Instruction Scheduling within Basic Blocks. </title> <type> Technical report. </type>
Reference-contexts: Still, both measures of performance are quite good. What about reinforcement learning? We ran experiments with temporal difference (TD) learning, some of which are described in <ref> (Scheeff, et al., 1997) </ref> and the results are not as good. This problem appears to be tricky to cast in a form suitable for TD, because TD looks at candidate instructions in isolation, rather than in a preference setting.
Reference: <author> Sites, R. </author> <year> (1992). </year> <title> Alpha Architecture Reference Manual. </title> <institution> Digital Equip. Corp., Maynard, </institution> <address> MA. </address>
Reference-contexts: To proceed, we selected a computer architecture implementation and a standard suite of benchmark programs (SPEC95) compiled for that architecture. We extracted basic blocks from the compiled programs and used them for training, testing, and evaluation as described below. 3.1 Architecture and Benchmarks We chose the Digital Alpha <ref> (Sites, 1992) </ref> as our architecture for the instruction scheduling problem. When introduced it was the fastest scalar processor available, and from a dependence analysis and scheduling standpoint its instruction set is simple.
Reference: <author> Srivastava, A. & Eustace, A. </author> <year> (1994). </year> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proc. ACM SIGPLAN '94 Conf. on Prog. Lang. Design and Impl., </booktitle> <address> 196205. </address>
Reference-contexts: We do conclude, though, that the approach is promising enough to warrant these additional investigations. Acknowledgments: We thank various people of Digital Equipment Corporation, for the DEC scheduler and the ATOM program instrumentation tool <ref> (Srivastava & Eustace, 1994) </ref>, essential to this work. We also thank Sun Microsystems and Hewlett-Packard for their support.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Tarsy, G. & Woodard, M. </author> <year> (1994). </year> <title> Method and apparatus for optimizing cost-based heuristic instruction schedulers. </title> <type> US Patent #5,367,687. </type> <note> Filed 7/7/93, granted 11/22/94. </note>
Reference-contexts: Also, optimal instruction scheduling for today's complex processors is NP-complete. We found two pieces of more closely related work. One is a patent <ref> (Tarsy & Woodard, 1994) </ref>. From the patent's claims it appears that the inventors trained a simple perceptron by adjusting weights of some heuristics. They evaluate each weight setting by scheduling an entire benchmark suite, running the resulting programs, and using the resulting times to drive weight adjustments.
Reference: <author> Utgoff, P. E., Berkman, N. C., & Clouse, J. A. </author> <title> (in press). Decision tree induction based on efficient tree restructuring. </title> <booktitle> Machine Learning. </booktitle>
Reference: <author> Utgoff, P. E., & Precup, D. </author> <year> (1997). </year> <title> Constructive function approximation, </title> <type> (Technical Report 97-04), </type> <institution> Amherst, MA: University of Massachusetts, Department of Computer Science. </institution>
Reference-contexts: Thus, table lookup is unbiased and one would expect it to give the best predictions possible for the chosen features, assuming the statistics of the training and test sets are consistent. The third method is the ELF function approximator <ref> (Utgoff & Precup, 1997) </ref>, which constructs additional features (much like a hidden unit) as necessary while it updates its representation of the function that it is learning. The function is represented by two layers of mapping.
References-found: 11

