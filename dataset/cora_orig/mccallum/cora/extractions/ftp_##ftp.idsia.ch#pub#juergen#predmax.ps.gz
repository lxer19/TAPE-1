URL: ftp://ftp.idsia.ch/pub/juergen/predmax.ps.gz
Refering-URL: http://www.bmc.riken.go.jp/sensor/Allan/ICA/index.html
Root-URL: 
Title: DISCOVERING PREDICTABLE CLASSIFICATIONS (Neural Computation, 5(4):625-635, 1993)  
Author: Jurgen Schmidhuber Daniel Prelinger 
Address: Boulder, CO 80309, USA  Arcisstr. 21, 8000 Munchen 2, Germany  
Affiliation: Department of Computer Science University of Colorado  Institut fur Informatik Technische Universitat Munchen  
Abstract: Prediction problems are among the most common learning problems for neural networks (e.g. in the context of time series prediction, control, etc.). With many such problems, however, perfect prediction is inherently impossible. For such cases we present novel unsupervised systems that learn to classify patterns such that the classifications are predictable while still being as specific as possible. The approach can be related to the IMAX method of Hinton, Becker and Zemel (1989, 1991). Experiments include a binary stereo task proposed by Becker and Hinton, which can be solved more readily by our system.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barlow, H. B., Kaushal, T. P., and Mitchison, G. J. </author> <year> (1989). </year> <title> Finding minimum entropy codes. </title> <journal> Neural Computation, </journal> <volume> 1(3) </volume> <pages> 412-423. </pages>
Reference-contexts: This was called the principle of predictability minimization. This principle encourages each output unit of T l to represent environmental properties that are statistically independent from environmental properties represented by the remaining output units. The procedure aims at generating binary `factorial codes' <ref> (Barlow et al., 1989) </ref>. It is our preferred method, because (unlike the methods used by Linsker (1988), Becker and Hinton (1989), and Zemel and Hinton (1991) ) it has a potential for removing even non-linear statistical dependencies 2 among the output units of some classifier.
Reference: <author> Becker, S. and Hinton, G. E. </author> <year> (1989). </year> <title> Spatial coherence as an internal teacher for a neural network. </title> <type> Technical Report CRG-TR-89-7, </type> <institution> Department of Computer Science, University of Toronto, </institution> <address> Ontario. </address>
Reference-contexts: An input pattern is generated by concatenating a strip from the right image with the corresponding strip from the left image. "So the input can be interpreted as a fronto-parallel surface at an integer depth. The only local property that is invariant across space is the depth (i.e. shift)." <ref> (Becker and Hinton, 1989) </ref>. With a given pair of different input patterns, the task is to extract a non-trivial classification of whatever is common to both patterns which happens to be the stereoscopic shift. <p> T 2 always learned to emit different localized representations in response to members of predictable classes, while superfluous output units remained switched off. 4.2 STEREO TASK The binary stereo experiment described in <ref> (Becker and Hinton, 1989) </ref> (see also example 2 in section 1) served to compare IMAX to our approach.
Reference: <author> Bridle, J. S. and MacKay, D. J. C. </author> <year> (1992). </year> <title> Unsupervised classifiers, mutual information and `phantom' targets. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, to appear. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> LeCun, Y. </author> <year> (1985). </year> <title> Une procedure d'apprentissage pour reseau a seuil asymetrique. </title> <booktitle> Proceedings of Cognitiva 85, Paris, </booktitle> <pages> pages 599-604. </pages>
Reference: <author> Linsker, R. </author> <year> (1988). </year> <title> Self-organization in a perceptual network. </title> <journal> IEEE Computer, </journal> <volume> 21 </volume> <pages> 105-117. </pages>
Reference-contexts: D l is defined as 1 X kh p;l x p;l k 2 : (7) 2.3 INFOMAX Following Linsker's Infomax approach <ref> (Linsker, 1988) </ref>, we might think of defining D l explicitly as the mutual information between the inputs and the outputs of T l .
Reference: <author> Nowlan, S. J. </author> <year> (1988). </year> <title> Auto-encoding with entropy constraints. </title> <booktitle> In Proceedings of INNS First Annual Meeting, </booktitle> <address> Boston, MA. </address> <note> Also published in special supplement to Neural Networks. </note>
Reference-contexts: This goal is achieved by simply training S i l to predict y p;l i from fy p;l k ; k 6= ig. See figure 1. 2 Steve Nowlan has described an alternative non-predictor based approach for finding non-redundant codes <ref> (Nowlan, 1988) </ref>.
Reference: <author> Parker, D. B. </author> <year> (1985). </year> <title> Learning-logic. </title> <type> Technical Report TR-47, </type> <institution> Center for Comp. Research in Economics and Management Sci., MIT. </institution>
Reference: <author> Prelinger, D. </author> <year> (1992). </year> <type> Diploma thesis. </type> <institution> Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference-contexts: to show that the first term on the right hand side of (5) is maximized subject to (6) if each input pattern is locally represented (just like with winner-take-all networks) by exactly one corner of the q-dimensional hypercube spanned by the possible output vectors, if there are sufficient output units <ref> (Prelinger, 1992) </ref> 1 . <p> With each experiment, positive training examples were randomly drawn from the set of legal pairs of input patterns. Details can be found in <ref> (Schmidhuber and Prelinger, 1992) </ref>. 4.1 FINDING PREDICTABLE LOCAL CLASS REPRESENTATIONS This experiment was motivated by example 1 (see section 1).
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <booktitle> In Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Schmidhuber, J. H. </author> <year> (1992). </year> <title> Learning factorial codes by predictability minimization. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 863-879. </pages>
Reference-contexts: D 2 can be defined in more than one reasonable way. The next section will list four alternative possibilities with mutual advantages and disadvantages. These alternatives include (a) a novel method for constrained variance maximization, (b) auto-encoders, and (c) a recent technique called "predictability minimization" <ref> (Schmidhuber, 1992) </ref>. The total error to be minimized by T 2 is *M + (1 *)D 2 ; (2) where 0 &lt; * &lt; 1 determines the relative weighting of the opposing error terms. <p> Section 2.3 mentions the Infomax approach for defining D l and explains why we do not pursue this approach. Section 2.4 finally defines D l by the recent method for predictability minimization <ref> (Schmidhuber, 1992) </ref>. <p> With each experiment, positive training examples were randomly drawn from the set of legal pairs of input patterns. Details can be found in <ref> (Schmidhuber and Prelinger, 1992) </ref>. 4.1 FINDING PREDICTABLE LOCAL CLASS REPRESENTATIONS This experiment was motivated by example 1 (see section 1).
Reference: <author> Schmidhuber, J. H. and Prelinger, D. </author> <year> (1992). </year> <title> Discovering predictable classifications. </title> <type> Technical Report CU-CS-626-92, </type> <institution> Dept. of Comp. Sci., University of Colorado at Boulder. </institution>
Reference-contexts: D 2 can be defined in more than one reasonable way. The next section will list four alternative possibilities with mutual advantages and disadvantages. These alternatives include (a) a novel method for constrained variance maximization, (b) auto-encoders, and (c) a recent technique called "predictability minimization" <ref> (Schmidhuber, 1992) </ref>. The total error to be minimized by T 2 is *M + (1 *)D 2 ; (2) where 0 &lt; * &lt; 1 determines the relative weighting of the opposing error terms. <p> Section 2.3 mentions the Infomax approach for defining D l and explains why we do not pursue this approach. Section 2.4 finally defines D l by the recent method for predictability minimization <ref> (Schmidhuber, 1992) </ref>. <p> With each experiment, positive training examples were randomly drawn from the set of legal pairs of input patterns. Details can be found in <ref> (Schmidhuber and Prelinger, 1992) </ref>. 4.1 FINDING PREDICTABLE LOCAL CLASS REPRESENTATIONS This experiment was motivated by example 1 (see section 1).
Reference: <author> Shannon, C. E. </author> <year> (1948). </year> <title> A mathematical theory of communication (parts I and II). </title> <journal> Bell System Technical Journal, XXVII:379-423. </journal>
Reference-contexts: This holds for the simplifying Gaussian noise models studied by Linsker, but it does not hold for the general case. (c) Even under appropriate Gaussian assumptions, with more than one-dimensional representations, Infomax implies maximization of functions of the determinant DET of the covariance matrix of the output activations <ref> (Shannon, 1948) </ref>. In a small application, Linsker explicitly calculated DET 's derivatives. <p> In the case of vector-valued output representations, Zemel and Hinton (1991) again make simplifying Gaussian assumptions and maximize functions of the determinant D of the q fi q-covariance matrices (DET MAX) of the output activations <ref> (Shannon, 1948) </ref> (see again section 2.3). DET MAX can remove only linear redundancy among the output units. (It should be mentioned, however, that with Zemel's and Hinton's approach the outputs may be non-linear functions of the inputs).
Reference: <author> Werbos, P. J. </author> <year> (1974). </year> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University. </institution>
Reference-contexts: In its simplest form, our basic approach to unsupervised discovery of predictable classifications is based on two neural networks called T 1 and T 2 . Both can be implemented as standard back-prop networks <ref> (Werbos, 1974) </ref>(LeCun, 1985)(Parker, 1985)(Rumelhart et al., 1986). With a given pair of input patterns, T 1 sees the first pattern, T 2 sees the second pattern. Let us first focus on the asymmetric case.
Reference: <author> Zemel, R. S. and Hinton, G. E. </author> <year> (1991). </year> <title> Discovering viewpoint-invariant relationships that characterize objects. </title> <editor> In Lippman, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 299-305. </pages> <address> San Mateo, </address> <publisher> CA: </publisher> <editor> Morgan Kaufmann. </editor> <title> network is encouraged to tell something about its input by means of the recent technique for `predictability minimization'. This technique requires additional intra-representational predictors (8 of them shown above) for detecting redundancies among the output units of the networks. Alternatives are provided in the text. </title>
References-found: 14

