URL: http://www.cs.ubc.ca/spider/price/ICML98-1.ps.gz
Refering-URL: http://www.cs.ubc.ca/spider/price/
Root-URL: 
Email: price,cebly@cs.ubc.ca  
Title: Reinforcement Learning with Imitation in Heterogeneous Multi-Agent Systems  
Author: Bob Price and Craig Boutilier 
Keyword: reinforcement learning, multi-agent systems, imitation  
Note: Contact: Bob Price, price@cs.ubc.ca, 604-822-6625  
Address: Vancouver, BC, Canada, V6T 1Z4  
Affiliation: Department of Computer Science University of British Columbia  
Abstract: The application of decision making and learning algorithms to multi-agent systems presents many interestingresearch challenges and opportunities. Among these is the ability for agents to learn how to act by observing or imitating other agents. We describe an algorithm, the IQ-algorithm, that integrates imitation with Q-learning. Roughly, a Q-learner uses the observations it has made of an expert agent to bias its exploration in promising directions. This algorithm goes beyond previous work in this direction by relaxing the oft-made assumptions that the learner (observer) and the expert (observed agent) share the same objectives and abilities. Our preliminary experiments demonstrate significant transfer between agents using the IQ-model and in many cases reductions in training time. 
Abstract-found: 1
Intro-found: 1
Reference: [AS97] <author> Christopher G. Atkeson and Stegan Schaal. </author> <title> Robot learning from demonstration. </title> <booktitle> International Conference on Machine Learning, </booktitle> <year> 1997. </year>
Reference-contexts: This form of knowledge transfer has been explored in RL before, typically in the guise of teachers providing some information to learners, or learning by example <ref> [Whi91, Lin92, AS97] </ref>. However, this transfer can be implicit as well. In particular, one agent can learn by observing or imitating another agent, without the consent, or even the awareness, of the observed agent. In this paper, we formulate a model of imitation learning in multi-agent systems based on Q-learning. <p> We also do not address the interesting questions of how to identify mentors and determining when to observe them. Our model is well-suited to cases where agents may have no incentive to share what they've learned with another agent, where agents cannot communicate the 1 Atkeson and Schaal <ref> [AS97] </ref> also consider the case where the apprentice may not control the system in the same way as the mentor. 2 specific actions they've taken, or where actions cannot be observed directly. <p> Finally, the requirement that the domain contain invertible actions is limiting. Imitation has been used as a form of robot programming in recent years by a number of researchers. To give just one example, Atkeson and Schaal <ref> [AS97] </ref> develop a system that learns to perform the complex non-linear task of balancing an inverted pendulum using examples of trajectories from humans.
Reference: [BK96] <author> Paul Bakker and Yasuo Kuniyoshi. </author> <title> Robot see, robot do : An overview of robot imitation. </title> <booktitle> AISB96 Workshop on Learning in Robots and Animals, </booktitle> <pages> pages 3-11, </pages> <year> 1996. </year>
Reference-contexts: vis explicit communication, imitation allows knowledge transfer between agents who cannot communicate, who do not share communication protocols, or for whom communication is risk-laden (e.g., in competitive environments such as financial or military domains). 4 Our main interest in imitation lies in the fact, pointed out by Bakker and Ku--niyoshi <ref> [BK96] </ref>, that imitation learning allows one to learn unobtrusively, without assistance from the mentor or even the mentor's knowledge. This allows learning to take place without need of complex teaching protocols or explicit agreement between the apprentice and mentor; knowledge transfer can even takeplace between noncooperative agents.
Reference: [Kae93] <author> Leslie Pack Kaelbling. </author> <title> Learning in Embedded Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: Of course, if we hope to enhance overall performance during learning, it makes sense (at least intuitively) to bias selection toward actions with higher estimated values. A number of such exploitive exploration strategies have been proposed [KLM96, MB98], but below we will use a variation of Kaelbling's <ref> [Kae93] </ref> interval estimation technique. Roughly, we assume that the Q-estimates r + fi max a 0 fQ (t; a 0 )g are samples from a distribution over Q fl (s; a) (taken to be normal). <p> At any point in time, when the apprentice finds itself in state s, it will choose an action according to some exploitive strategy. In our experiments, we have used a variant of Kaelbling's <ref> [Kae93] </ref> interval estimation strategy, with simple, heuristic backpropagation of uncertainty over future value used to influence confidence levels (see [MB98] for a discussion). We defer details to a longer version of the paper.
Reference: [KLM96] <author> Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: 1 Introduction Reinforcement learning (RL) has proven to be quite successful as an approach to dealing with sequential decision problems in which an agent does not have full knowledge of system dynamics, its capabilities or the relative value of different states <ref> [KLM96] </ref>. A number of researchers have applied these techniques to multi-agent systems [Wei93, SSH94, Lit94] with reasonable success. <p> Of course, if we hope to enhance overall performance during learning, it makes sense (at least intuitively) to bias selection toward actions with higher estimated values. A number of such exploitive exploration strategies have been proposed <ref> [KLM96, MB98] </ref>, but below we will use a variation of Kaelbling's [Kae93] interval estimation technique. Roughly, we assume that the Q-estimates r + fi max a 0 fQ (t; a 0 )g are samples from a distribution over Q fl (s; a) (taken to be normal).
Reference: [Lin92] <author> Long-Ji Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <booktitle> Machine Learning, </booktitle> <address> ?:293, </address> <year> 1992. </year> <month> 15 </month>
Reference-contexts: This form of knowledge transfer has been explored in RL before, typically in the guise of teachers providing some information to learners, or learning by example <ref> [Whi91, Lin92, AS97] </ref>. However, this transfer can be implicit as well. In particular, one agent can learn by observing or imitating another agent, without the consent, or even the awareness, of the observed agent. In this paper, we formulate a model of imitation learning in multi-agent systems based on Q-learning. <p> In particular, one agent can learn by observing or imitating another agent, without the consent, or even the awareness, of the observed agent. In this paper, we formulate a model of imitation learning in multi-agent systems based on Q-learning. Following the work of Whitehead [Whi91] and Lin <ref> [Lin92] </ref>, we wish to integrate imitation into precise models of RL, while relaxing some of the restrictive assumptions made in that work. Specifically, we assume that there is no direct communication between the mentor (agent being imitated) and the apprentice (agent imitating the other). <p> Furthermore, a mentor need not be an expert: by observing an agent who consistently chooses actions that it deems poor, an apprentice can learn valuable information. There has been considerable interest in formalizing imitative processes so that they can be applied to robotics and multi-state decision problems. Lin <ref> [Lin92] </ref> was among the first to try to combine imitation learning with a formal reinforcement learning model. An apprentice agent is given the ability to see through an expert agent's eyes. The apprentice can therefore benefit from the experiences of the expert.
Reference: [Lit94] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforce-ment learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157-163, </pages> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: A number of researchers have applied these techniques to multi-agent systems <ref> [Wei93, SSH94, Lit94] </ref> with reasonable success. However, attention seems to have focused primarily on difficulties that arise when RL is used in multi-agent systems, such as lack of coordination; relatively little effort has been made to exploit the opportunities that emerge to make learning and acting easier.
Reference: [MB98] <editor> Nicolas Meuleau and Paul Bourgine. </editor> <title> Exploration of multi-state environments: Local measures and back-propagation of uncertainty. </title> <booktitle> Machine Learning, </booktitle> <year> 1998. </year> <note> to appear. </note>
Reference-contexts: Of course, if we hope to enhance overall performance during learning, it makes sense (at least intuitively) to bias selection toward actions with higher estimated values. A number of such exploitive exploration strategies have been proposed <ref> [KLM96, MB98] </ref>, but below we will use a variation of Kaelbling's [Kae93] interval estimation technique. Roughly, we assume that the Q-estimates r + fi max a 0 fQ (t; a 0 )g are samples from a distribution over Q fl (s; a) (taken to be normal). <p> We then randomly select a sample value from the distribution for each action and take the action with the maximum sample. We also backup variances across states to improve exploration. The strategy is similar to that proposed in <ref> [MB98] </ref>. 2.2 Imitation: Concepts and Techniques Imitation is a form of learning in which one agent learns by observing the behavior of another and attempting (in one form or another) to duplicate it. <p> In our experiments, we have used a variant of Kaelbling's [Kae93] interval estimation strategy, with simple, heuristic backpropagation of uncertainty over future value used to influence confidence levels (see <ref> [MB98] </ref> for a discussion). We defer details to a longer version of the paper. With each transition hs; a; r; ti it takes, the apprentice updates Q (s; a) according to Equation 1. In a certain sense, the apprentice is a typical Q-learner.
Reference: [SSH94] <author> Sandip Sen, Mahendra Sekaran, and John Hale. </author> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 426-431, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: A number of researchers have applied these techniques to multi-agent systems <ref> [Wei93, SSH94, Lit94] </ref> with reasonable success. However, attention seems to have focused primarily on difficulties that arise when RL is used in multi-agent systems, such as lack of coordination; relatively little effort has been made to exploit the opportunities that emerge to make learning and acting easier.
Reference: [Sut88] <author> Richard S. Sutton. </author> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Our initial empirical results with IQ-learning are promising, suggesting that significant speed up can be realized under realistic assumptions. Future directions include extending the current IQ-learning with traces in the style of TD () <ref> [Sut88] </ref>. A number of other issues remain to be explored as well, such as that of identifying mentors in a large collection of agents, deciding when to make observations, dealing with partial observability, and learning and generalizing reward functions.
Reference: [Tsi94] <author> John H. Tsitsiklis. </author> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16 </volume> <pages> 185-202, </pages> <year> 1994. </year>
Reference-contexts: If ff is decreased appropriately during learning and all actions are sampled infinitely, Q-learning ensures that the estimates Q (s; a) converge to the true values Q fl (s; a) for all actions <ref> [WD92, Tsi94] </ref>. Convergence of Q-learning does not depend on the exploration strategy used. An agent can try its actions at any timethere is no requirement to perform actions that are currently estimated to be best.
Reference: [WD92] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: We discuss these results, and some limititations of the IQ-model in Section 5, and conclude in Section 6 with a summary and thoughts on future research directions. 2 Reinforcement Learning and Imitation 2.1 Q-Learning A simple, well-understood algorithm for reinforcement learning in a single agent setting is Q-learning <ref> [WD92] </ref>. We assume the agent wants to control a stochastic process modeled as a Markov decision process, with a finite set of states S, a finite set of actions A and a reward function R : S ! R, associating rewards with with states s 2 S. <p> If ff is decreased appropriately during learning and all actions are sampled infinitely, Q-learning ensures that the estimates Q (s; a) converge to the true values Q fl (s; a) for all actions <ref> [WD92, Tsi94] </ref>. Convergence of Q-learning does not depend on the exploration strategy used. An agent can try its actions at any timethere is no requirement to perform actions that are currently estimated to be best.
Reference: [Wei93] <author> Gerhard Wei. </author> <title> Learning to coordinate actions in multi-agent systems. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 311-316, </pages> <address> Chambery, FR, </address> <year> 1993. </year>
Reference-contexts: A number of researchers have applied these techniques to multi-agent systems <ref> [Wei93, SSH94, Lit94] </ref> with reasonable success. However, attention seems to have focused primarily on difficulties that arise when RL is used in multi-agent systems, such as lack of coordination; relatively little effort has been made to exploit the opportunities that emerge to make learning and acting easier.
Reference: [Whi91] <author> Steven D. Whitehead. </author> <title> Complexity and cooperation in q-learning. </title> <booktitle> In Machine Learning. Proceedings of the Eighth International Workshop (ML91), </booktitle> <pages> pages 363-367, </pages> <year> 1991. </year>
Reference-contexts: This form of knowledge transfer has been explored in RL before, typically in the guise of teachers providing some information to learners, or learning by example <ref> [Whi91, Lin92, AS97] </ref>. However, this transfer can be implicit as well. In particular, one agent can learn by observing or imitating another agent, without the consent, or even the awareness, of the observed agent. In this paper, we formulate a model of imitation learning in multi-agent systems based on Q-learning. <p> In particular, one agent can learn by observing or imitating another agent, without the consent, or even the awareness, of the observed agent. In this paper, we formulate a model of imitation learning in multi-agent systems based on Q-learning. Following the work of Whitehead <ref> [Whi91] </ref> and Lin [Lin92], we wish to integrate imitation into precise models of RL, while relaxing some of the restrictive assumptions made in that work. Specifically, we assume that there is no direct communication between the mentor (agent being imitated) and the apprentice (agent imitating the other). <p> The apprentice can therefore benefit from the experiences of the expert. The model is fairly exacting, however, as a complete trace of the expert's behavior must be saved and replayed. The expert and apprentice must have identical sensor and effector protocols and both must agree to share information. Whitehead <ref> [Whi91] </ref> demonstrated a simple form of imitation that reduced exploration costs exponentially. In his model, an apprentice agent observes an expert mentor and attempts one step of a solution.
Reference: [Wyw96] <author> Wanda Wywicka. </author> <title> Imitation in Human and Animal Behaviour. </title> <publisher> Transaction Publishers, </publisher> <address> New Brunswick, New Jersey, </address> <year> 1996. </year> <month> 16 </month>
Reference-contexts: Long recognized as an important process in both human and animal development (see, e.g., Wyrwicka <ref> [Wyw96] </ref> who traces the study of imitation in the psychological literature back more than a century), it has been gaining increasing attention in machine learning circles as a possible means of speeding up learning and interacting with humans in realistic environments.
References-found: 14

