URL: http://www-anw.cs.umass.edu/cgi-bin/getfile/pub/anw/pub/singh/singh-AAAI94.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/Publications/recent.html
Root-URL: 
Email: singh@psyche.mit.edu  
Title: Reinforcement Learning Algorithms for Average-Payoff Markovian Decision Processes  
Author: Satinder P. Singh 
Address: Cambridge, MA 02139  
Affiliation: Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  
Abstract: Reinforcement learning (RL) has become a central paradigm for solving learning-control problems in robotics and artificial intelligence. RL researchers have focussed almost exclusively on problems where the controller has to maximize the discounted sum of payoffs. However, as emphasized by Schwartz (1993), in many problems, e.g., those for which the optimal behavior is a limit cycle, it is more natural and com-putationally advantageous to formulate tasks so that the controller's objective is to maximize the average payoff received per time step. In this paper I derive new average-payoff RL algorithms as stochastic approximation methods for solving the system of equations associated with the policy evaluation and optimal control questions in average-payoff RL tasks. These algorithms are analogous to the popular TD and Q-learning algorithms already developed for the discounted-payoff case. One of the algorithms derived here is a significant variation of Schwartz's R-learning algorithm. Preliminary empirical results are presented to validate these new algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A.G.; Sutton, R.S.; and Anderson, C.W. </author> <year> 1983. </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <booktitle> IEEE SMC 13 </booktitle> <pages> 835-846. </pages>
Reference-contexts: Policy Evaluation Policy evaluation involves determining the average payoff and the relative values for a fixed policy . Strictly speaking, policy evaluation is a prediction problem and not a RL problem. However, because many RL architectures are based on policy evaluation <ref> (e.g., Barto, Sutton, & Anderson 1983) </ref>, I will first develop average-case policy evaluation algorithms.
Reference: <author> Barto, A.G.; Bradtke, S.J.; and Singh, </author> <title> S.P. to appear. Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence. </journal>
Reference: <author> Bellman, R.E. </author> <year> 1957. </year> <title> Dynamic Programming. </title> <publisher> Prince-ton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: <author> Bertsekas, D.P. </author> <year> 1982. </year> <title> Distributed dynamic programming. </title> <journal> IEEE Transactions on Automatic Control 27 </journal> <pages> 610-616. </pages>
Reference: <author> Bertsekas, D.P. </author> <year> 1987. </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <publisher> Prentice-Hall, </publisher> <address> Engle-wood Cliffs, NJ. </address>
Reference-contexts: Using the Markov assumption it can be shown that and V are solutions to the following systems of linear equations: Policy evaluation equations for the average payoff case <ref> (e.g., Bertsekas 1987) </ref> + V (x) = R (x; (x)) + y2S where to get a unique solution, we set V (r) = 0, for some arbitrarily chosen reference state r 2 S. This is needed because there are jSj + 1 unknowns and only jSj equations.
Reference: <author> Jaakkola, T.; Jordan, M.I.; and Singh, </author> <title> S.P. to appear. Stochastic convergence of iterative DP algorithms. </title> <booktitle> Neural Computation. </booktitle>
Reference: <author> Jalali, A. and Ferguson, M. </author> <year> 1990. </year> <title> Adaptive control of markov chains with local updates. </title> <journal> Systems & Control Letters 14 </journal> <pages> 209-218. </pages>
Reference: <author> Schwartz, A. </author> <year> 1993. </year> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth Machine Learning Conference. </booktitle>
Reference-contexts: A natural measure of performance in such undiscounted applications is the average payoff per time step received by the agent (e.g., Bert-sekas 1987). For problems where either the discounted-payoff or the average-payoff formulations can be used, often there are strong computational reasons to prefer the average-payoff formulation <ref> (see Schwartz 1993, for a recent discussion) </ref>. Recently, Jaakkola, Jordan, & Singh (to appear) have developed a fairly complete mathematical understanding of discounted-payoff RL algorithms as stochastic approximation methods for solving the system of Bellman (1957) equations associated with discounted-payoff Markovian decision processes (MDPs) (also see Tsitsiklis 1993).
Reference: <author> Singh, S. P. </author> <year> 1993. </year> <title> Learning to Solve Markovian Decision Processes. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Massachusetts. </institution> <note> also, CMPSCI Technical Report 93-77. </note>
Reference-contexts: The relationship to stochastic approximation is in the following fact: EfBg = B <ref> (see Singh 1993 for an explanation) </ref>. Policy Evaluation Policy evaluation involves determining the average payoff and the relative values for a fixed policy . Strictly speaking, policy evaluation is a prediction problem and not a RL problem.
Reference: <author> Sutton, R.S. </author> <year> 1988. </year> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44. </pages>
Reference: <author> Tsitsiklis, J. </author> <year> 1993. </year> <title> Asynchronous stochastic approximation and Q-learning. </title> <note> Submitted. </note>
Reference-contexts: Recently, Jaakkola, Jordan, & Singh (to appear) have developed a fairly complete mathematical understanding of discounted-payoff RL algorithms as stochastic approximation methods for solving the system of Bellman (1957) equations associated with discounted-payoff Markovian decision processes (MDPs) <ref> (also see Tsitsiklis 1993) </ref>. In this paper, I develop average-payoff RL algorithms by deriving stochastic approximation methods for solving the analogous Bellman equations for MDPs in which the measure to be optimized is the average payoff per time step.
Reference: <author> Watkins, C.J.C.H. and Dayan, P. </author> <year> 1992. </year> <title> Q-learning. </title> <booktitle> Machine Learning </booktitle> 8(3/4):279-292.

References-found: 12

