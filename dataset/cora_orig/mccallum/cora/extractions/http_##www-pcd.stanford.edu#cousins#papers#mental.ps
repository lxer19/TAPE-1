URL: http://www-pcd.stanford.edu/cousins/papers/mental.ps
Refering-URL: http://flamingo.stanford.edu/groups/nobotics/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Logics of Mental Attitudes in AI a very preliminary survey  
Author: Yoav Shoham and Steve B. Cousins 
Address: Stanford, CA USA  
Affiliation: Computer Science Department Stanford University  
Abstract: There has been a growing interest in AI in formal, qualitative models of various mental attitudes, starting with the well-researched concepts of knowledge and belief, and continuing with more exotic notions such as goals and intentions. In this initial survey we take stock of the work carried out so far in this field, including the motivation for this line of research, the mental attitudes considered, those not, the (primarily logical) tools used, and some of the outstanding problems.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> C. E. Alchourron, P. Gardenfors, and D. Makinson. </author> <title> On the logic of theory change: Partial meet contraction and revision functions. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 50 </volume> <pages> 510-530, </pages> <year> 1985. </year>
Reference-contexts: Alchourron, Gardenfors, and Makinson (AGM) first formalized the notion of belief revision, and argued that any reasonable revision function would satisfy eight postulates <ref> [28, 1] </ref>.
Reference: 2. <author> J. F. Allen and C. R. Perrault. </author> <title> Analyzing intention in utterances. </title> <journal> Artificial Intelligence, </journal> <volume> 15 </volume> <pages> 143-178, </pages> <year> 1980. </year>
Reference-contexts: In fact it was early work by Allen, Cohen and Perrault on intelligent dialog systems <ref> [52, 19, 2] </ref> that gave impetus to this line of research, first at the University of Toronto and then at SRI International and the University of Rochester. <p> For example, agents may induce information about the plans and beliefs of other agents by observing their behavior; thus, work on plan-recognition in AI is as applicable to multi-agent systems (cf. [12]) as it is to natural-language understanding (cf. <ref> [2, 37, 44] </ref>). Agents may also communicate this information directly to one another. This example in particular requires that all agents be built of the "stuff" of plans, beliefs, and their ilk; the assumption is that an agent with no beliefs or plans would find it hard to articulate any.
Reference: 3. <author> L. Aqvist. </author> <title> Introduction to deontic logic and the theory of normative systems. </title> <institution> Bibliopo--lis, Napoli, </institution> <year> 1987. </year>
Reference-contexts: Work in this area relates to the motivations of the previous section, but gives social, moral, and/or rational reasons for behaving in a certain way. Some pointers to the philosophical literature include <ref> [3, 13, 35] </ref>.
Reference: 4. <author> N. D. Belnap and M. Perloff. </author> <title> Seeing to it that: A canonical form of agentives. </title> <journal> Theoria, </journal> <volume> 54 </volume> <pages> 175-199, </pages> <year> 1989. </year>
Reference-contexts: Some work in philosophy has attempted to capture this causal connection, defining formal operators such as DO, REFRAIN (or OMIT), STIT (see to it that), and others; references include <ref> [4, 14, 34] </ref>. Most of this work has yet to impact AI.
Reference: 5. <editor> A. H. Bond and L. Gasser. </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Ablex Publishing Corporation, </publisher> <year> 1988. </year>
Reference-contexts: Much work on achieving coordination in distributed AI relies on some mental structure common to all agents (cf. <ref> [5, 21, 22] </ref>). Halpern and Moses used an S5 formulation of knowledge in distributed systems to allow system designers to reason about the knowledge ascribed to processors, and in particular about knowledge that processors had about the knowledge of other processors, etc. [32].
Reference: 6. <author> C. Boutilier. </author> <title> Conditional Logics for Default Reasoning and Belief Revision. </title> <type> Technical report 92-1, </type> <institution> University of British Columbia, Department of Computer Science Rm 333 - 6356 Agricultural Road Vancouver, B.C. </institution> <address> Canada V6T 1Z2, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: This connection has been reinforced by other work. For example, Boutilier <ref> [6] </ref> has developed a conditional logic framework which handles both nonmonotonic logic and belief revision, and Friedman and Halpern [27] have defined a notion of belief change which is closely related to a subclass of nonmonotonic logics called `preference logics' [60]. 5.3 Belief, Desire, and Intention Compared to the informational attitudes, <p> They have proposed a logic that defines intentions in terms of goals and beliefs. Their proposal has generated some discussion and debate, some of which can be found in [18]. 5.4 Obligation The notion of obligation has scarcely been studied in AI. See work by McCarty [45] and Boutilier <ref> [6] </ref> for early examples of work formalizing "ought". Quite recently, Pearl has proposed a formalization of conditional obligations [51]. 6 Final comments In this final section, we will briefly mention three aspects of agency which have not yet come up in this survey: perception, indexicality, and quantitative measures.
Reference: 7. <author> R. I. Brafman, J.-C. Latombe, Y. Moses, and Y. Shoham. </author> <title> Knowledge as a tool in motion planning under uncertainty. </title> <booktitle> In Theoretical Aspects of Reasoning about Knowledge: Proceedings of the 1994 Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Finally, knowledge can be a high-level design analysis tool. This has been most clearly demonstrated in the multi-agent case (see below), but has also found utility in the single agent case (cf. <ref> [7] </ref>). The potential applications of mental notions to multi-agent systems are perhaps the most intriguing of all. In a setting in which multiple agents are designed and programmed by different people, notions of mental state can be useful to achieve coherent interaction between the various agents.
Reference: 8. <author> M. E. Bratman. </author> <title> Intention, Plans, and Practical Reason. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: It is interesting to note that in another field that is concerned with action selection economics- most of these distinctions are of no interest whatsoever. It appears that AI's emphasis on computing lends more urgency to these distinctions. This is in line with Bratman's argument in philosophy <ref> [8] </ref> that, when one considers agents with resource limitations, the notions of desire and intention should be kept quite distinct.
Reference: 9. <author> M. E. Bratman, D. J. Israel, and M. E. Pollack. </author> <title> Plans and resource-bounded practical reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 4 </volume> <pages> 349-355, </pages> <year> 1988. </year>
Reference-contexts: The classic "BDI" architecture, which attempted to model complete rational agents, is illustrated in the work of Bratman, Israel, and Pollack <ref> [9] </ref>. Agents built with this architecture create plans and maintain them in a changing environment. Konolige and Pollack use a BDI architecture to recognize the plans of other agents [39]. Rao and Georgeff have also defined a logic with beliefs, goals, and intentions [54].
Reference: 10. <author> M. Brown. </author> <title> On the logic of ability. </title> <journal> Journal of Philosophical Logic, </journal> <volume> 17 </volume> <pages> 1-26, </pages> <year> 1988. </year>
Reference-contexts: Singh uses a related term, "knowhow" [61]. Within philosophy some recent references include work by Elgesem [24] and by Brown <ref> [10, 11] </ref>; that work has so far not had an impact on AI. 4.4 Doing, bringing about An essential element of agency, perhaps its defining ingredient, is the causal relation between an agent and occurrences in the world.
Reference: 11. <author> M. Brown. </author> <title> Action and ability. </title> <journal> Journal of Philosophical Logic, </journal> <volume> 19 </volume> <pages> 95-114, </pages> <year> 1990. </year>
Reference-contexts: Singh uses a related term, "knowhow" [61]. Within philosophy some recent references include work by Elgesem [24] and by Brown <ref> [10, 11] </ref>; that work has so far not had an impact on AI. 4.4 Doing, bringing about An essential element of agency, perhaps its defining ingredient, is the causal relation between an agent and occurrences in the world.
Reference: 12. <author> N. F. Carver, V. R. Lesser, and D. L. McCue. </author> <title> Focusing on plan recognition. </title> <booktitle> In Proc. National Conference on Artificial Intelligence (AAAI-84), </booktitle> <year> 1984. </year>
Reference-contexts: For example, agents may induce information about the plans and beliefs of other agents by observing their behavior; thus, work on plan-recognition in AI is as applicable to multi-agent systems (cf. <ref> [12] </ref>) as it is to natural-language understanding (cf. [2, 37, 44]). Agents may also communicate this information directly to one another.
Reference: 13. <author> H. N. Castaneda. </author> <title> Thinking and doing: </title> <booktitle> The philosophical foundations of institutions. </booktitle> <address> D. </address> <publisher> Reidel Pub. Co., Dordrecht, Holland, </publisher> <year> 1975. </year>
Reference-contexts: Work in this area relates to the motivations of the previous section, but gives social, moral, and/or rational reasons for behaving in a certain way. Some pointers to the philosophical literature include <ref> [3, 13, 35] </ref>.
Reference: 14. <author> B. F. Chellas. </author> <title> Modal logic. </title> <publisher> Cambridge University Press, </publisher> <year> 1980. </year>
Reference-contexts: Some work in philosophy has attempted to capture this causal connection, defining formal operators such as DO, REFRAIN (or OMIT), STIT (see to it that), and others; references include <ref> [4, 14, 34] </ref>. Most of this work has yet to impact AI.
Reference: 15. <author> P. R. Cohen and H. Levesque. </author> <title> Rational interaction as the basis for communication. </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Ma., </address> <year> 1990. </year>
Reference-contexts: Konolige and Pollack use a BDI architecture to recognize the plans of other agents [39]. Rao and Georgeff have also defined a logic with beliefs, goals, and intentions [54]. Among the more influential work is that Cohen and Levesque <ref> [16, 15, 17] </ref>. They have proposed a logic that defines intentions in terms of goals and beliefs. Their proposal has generated some discussion and debate, some of which can be found in [18]. 5.4 Obligation The notion of obligation has scarcely been studied in AI.
Reference: 16. <author> P. R. Cohen and H. J. Levesque. </author> <title> Intention is choice with commitment. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 213-261, </pages> <year> 1990. </year>
Reference-contexts: Konolige and Pollack use a BDI architecture to recognize the plans of other agents [39]. Rao and Georgeff have also defined a logic with beliefs, goals, and intentions [54]. Among the more influential work is that Cohen and Levesque <ref> [16, 15, 17] </ref>. They have proposed a logic that defines intentions in terms of goals and beliefs. Their proposal has generated some discussion and debate, some of which can be found in [18]. 5.4 Obligation The notion of obligation has scarcely been studied in AI.
Reference: 17. <author> P. R. Cohen and H. J. Levesque. </author> <title> Teamwork. </title> <journal> Nous, </journal> <volume> 25 </volume> <pages> 11-24, </pages> <year> 1991. </year>
Reference-contexts: Konolige and Pollack use a BDI architecture to recognize the plans of other agents [39]. Rao and Georgeff have also defined a logic with beliefs, goals, and intentions [54]. Among the more influential work is that Cohen and Levesque <ref> [16, 15, 17] </ref>. They have proposed a logic that defines intentions in terms of goals and beliefs. Their proposal has generated some discussion and debate, some of which can be found in [18]. 5.4 Obligation The notion of obligation has scarcely been studied in AI.
Reference: 18. <author> P. R. Cohen, J. Morgan, and M. E. Pollack, </author> <title> editors. Intentions in Communication. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Among the more influential work is that Cohen and Levesque [16, 15, 17]. They have proposed a logic that defines intentions in terms of goals and beliefs. Their proposal has generated some discussion and debate, some of which can be found in <ref> [18] </ref>. 5.4 Obligation The notion of obligation has scarcely been studied in AI. See work by McCarty [45] and Boutilier [6] for early examples of work formalizing "ought".
Reference: 19. <author> P.R. Cohen and C.R. Perrault. </author> <title> Elements of a plan-based theory of speech acts. </title> <journal> Cognitive Science, </journal> <volume> 3 </volume> <pages> 177-212, </pages> <year> 1979. </year>
Reference-contexts: In fact it was early work by Allen, Cohen and Perrault on intelligent dialog systems <ref> [52, 19, 2] </ref> that gave impetus to this line of research, first at the University of Toronto and then at SRI International and the University of Rochester.
Reference: 20. <author> E. Davis. </author> <title> Representation of Comonsense Knowledge. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Whatever work does exist tends to concentrate on the `physical' variety. Capabilities of agents have been formalized in two ways, as operations on actions ("can-do") or as operations on propositions ("can-achieve"); these terms are used, for example, by Davis <ref> [20] </ref> and Thomas [62]. Singh uses a related term, "knowhow" [61].
Reference: 21. <author> R. Davis and R. G. Smith. </author> <title> Negotiation as a metaphor for distributed problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 20(1) </volume> <pages> 63-109, </pages> <year> 1983. </year>
Reference-contexts: Much work on achieving coordination in distributed AI relies on some mental structure common to all agents (cf. <ref> [5, 21, 22] </ref>). Halpern and Moses used an S5 formulation of knowledge in distributed systems to allow system designers to reason about the knowledge ascribed to processors, and in particular about knowledge that processors had about the knowledge of other processors, etc. [32].
Reference: 22. <author> E. H. Durfee and T. A. Montgomery. </author> <title> A hierarchical protocol for coordinating multi-agent behaviors. </title> <booktitle> In Proc. National Conference on Artificial Intelligence (AAAI-90). AAAI, </booktitle> <year> 1990. </year>
Reference-contexts: Much work on achieving coordination in distributed AI relies on some mental structure common to all agents (cf. <ref> [5, 21, 22] </ref>). Halpern and Moses used an S5 formulation of knowledge in distributed systems to allow system designers to reason about the knowledge ascribed to processors, and in particular about knowledge that processors had about the knowledge of other processors, etc. [32].
Reference: 23. <editor> P. Edwards, editor. </editor> <booktitle> Encyclopedia of Philosophy. </booktitle> <publisher> Macmillan, </publisher> <address> New York, </address> <year> 1967. </year> <note> Section on Epistemology. </note>
Reference-contexts: None of the work mentioned so far addresses the connection between knowledge and belief. Indeed, there is relatively little work on this in AI. A common philosophical slogan about knowledge being "justified true belief," which can be traced back to Plato <ref> [23] </ref>, has not been backed up with a formal theory, and at least for a while seemed to fall into disrepute within philosophy (cf. [30]).
Reference: 24. <author> D. Elgesem. </author> <title> He would have done it anyway: The logic of agency, ability and opportunity. </title> <institution> Stanford University, </institution> <type> CSLI, manuscript, </type> <year> 1990. </year>
Reference-contexts: Capabilities of agents have been formalized in two ways, as operations on actions ("can-do") or as operations on propositions ("can-achieve"); these terms are used, for example, by Davis [20] and Thomas [62]. Singh uses a related term, "knowhow" [61]. Within philosophy some recent references include work by Elgesem <ref> [24] </ref> and by Brown [10, 11]; that work has so far not had an impact on AI. 4.4 Doing, bringing about An essential element of agency, perhaps its defining ingredient, is the causal relation between an agent and occurrences in the world.
Reference: 25. <author> R. Fagin, J. Halpern, Y. Moses, and M. Vardi. </author> <title> Reasoning about Knowledge. </title> <publisher> MIT Press, </publisher> <year> 1994. </year> <note> (to appear). </note>
Reference-contexts: Moore was an early importer of his ideas into AI [47]; the most comprehensive and up-to-date discussion of knowledge and belief in computer science appears in <ref> [25] </ref>. Most formalization of knowledge and belief (and, for that matter, of other mental attitudes) is couched in modal logic. Typically, to capture a mental attitude one invents a new modal operator representing the attitude.
Reference: 26. <author> R. Fagin and J. Y. Halpern. </author> <title> Belief, awareness, and limited reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 34 </volume> <pages> 39-76, </pages> <year> 1988. </year>
Reference-contexts: Several attempts were made to avoid this property: Kono-lige adopts a syntactic solution [38], Moses defines the notion of `resource-bounded knowledge' [48], and Levesque [43] as well as Fagin and Halpern <ref> [26] </ref> talk of explicit versus implicit belief. Halpern and Fagin rest their proposal in part on the notion of `awareness'; an agent cannot explicitly believe a formula of which he is not aware. None of the work mentioned so far addresses the connection between knowledge and belief.
Reference: 27. <author> N. Friedman and J.. Y. Halpern. </author> <title> A knowledge-based framework for belief change. Part I: Foundations. </title> <editor> In R. Fagin, editor, </editor> <booktitle> Theoretical Aspects of Reasoning about Knowledge: Proc. Fifth Conference, </booktitle> <address> San Mateo, CA, </address> <note> 1994 (to appear). Morgan Kaufmann. </note>
Reference-contexts: This connection has been reinforced by other work. For example, Boutilier [6] has developed a conditional logic framework which handles both nonmonotonic logic and belief revision, and Friedman and Halpern <ref> [27] </ref> have defined a notion of belief change which is closely related to a subclass of nonmonotonic logics called `preference logics' [60]. 5.3 Belief, Desire, and Intention Compared to the informational attitudes, relatively little work has been done to formalize what we have called motivational attitudes.
Reference: 28. <author> P. Gardenfors. </author> <title> Knowledge in Flux. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1988. </year>
Reference-contexts: Alchourron, Gardenfors, and Makinson (AGM) first formalized the notion of belief revision, and argued that any reasonable revision function would satisfy eight postulates <ref> [28, 1] </ref>.
Reference: 29. <author> Michael Gelfond, Vladimir Lifschitz, and Arkady Rabinov. </author> <title> What are the limitations of the situation calculus? In Robert Boyer, editor, Automated Reasoning: </title> <booktitle> Essays in Honor of Woody Bledsoe, </booktitle> <pages> pages 167-179. </pages> <publisher> Kluwer Academic, </publisher> <address> Dordrecht, </address> <year> 1991. </year>
Reference-contexts: What in our view is still substantially unsettled is the integration of time and action in a single framework. Several years ago arguments were made that action can be defined in a temporal framework [57], or the other way around <ref> [29] </ref>. Pinto and Reiter's [53] is a more symmetric integration of time and action, and is to our knowledge the latest work in AI on the topic. 4.3 Capabilities Presumably agents cannot commit to, intend, or plan actions of which they do not believe themselves to be capable.
Reference: 30. <author> E. L. Gettier. </author> <title> Is justified true belief knowledge? Analysis, </title> <booktitle> 23 </booktitle> <pages> 121-123, </pages> <year> 1963. </year>
Reference-contexts: A common philosophical slogan about knowledge being "justified true belief," which can be traced back to Plato [23], has not been backed up with a formal theory, and at least for a while seemed to fall into disrepute within philosophy (cf. <ref> [30] </ref>). Within philosophy Lenzen's work [40, 41] is an exception, and within AI published material on the connection between the knowledge and belief includes Moses and Shoham's [48]. Recent but as yet unpublished work by a number of researchers, all related to conditional logic, promises new insights.
Reference: 31. <author> A. J. Grove. </author> <title> Topics in Multi-Agent Epistemic Logic. </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Science Department, </institution> <year> 1992. </year>
Reference-contexts: Lesperance has argued that in-dexicality may be more than expedient: it may be necessary in order to communicate certain ideas between agents [42], and Grove has made simliar arguments <ref> [31] </ref>. Although we have concentrated in this paper on qualitative methods, it should be mentioned that formal quantitative frameworks such as probability theory and decision theory have a lot to say about how agents behave.
Reference: 32. <author> J. Y. Halpern and Y. Moses. </author> <title> A guide to completeness and complexity for modal logics of knowledge and belief. </title> <journal> Artificial Intelligence, </journal> <volume> 54 </volume> <pages> 319-379, </pages> <year> 1992. </year>
Reference-contexts: Halpern and Moses used an S5 formulation of knowledge in distributed systems to allow system designers to reason about the knowledge ascribed to processors, and in particular about knowledge that processors had about the knowledge of other processors, etc. <ref> [32] </ref>. Perhaps the most radical embracing of the notion of mental state in a computational setting is Agent Oriented Programming, a programming paradigm in which the state of individual modules (`agents') consists entirely of mental attitudes such as beliefs and obligations [58].
Reference: 33. <author> J. Hintikka. </author> <title> Knowledge and Belief. </title> <publisher> Cornell University Press, </publisher> <year> 1962. </year>
Reference-contexts: Most work in AI (and some work in distributed systems) on knowledge and belief has its origins in the particular philosophical work of Hintikka <ref> [33] </ref>. Moore was an early importer of his ideas into AI [47]; the most comprehensive and up-to-date discussion of knowledge and belief in computer science appears in [25]. Most formalization of knowledge and belief (and, for that matter, of other mental attitudes) is couched in modal logic.
Reference: 34. <author> J. F. Horty and N. Belnap. </author> <title> The deliberative stit: A study of action, omission, ability, and obligation. </title> <type> Manuscript, </type> <institution> Philosophy Dept., Univ. of Maryland, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Some work in philosophy has attempted to capture this causal connection, defining formal operators such as DO, REFRAIN (or OMIT), STIT (see to it that), and others; references include <ref> [4, 14, 34] </ref>. Most of this work has yet to impact AI.
Reference: 35. <author> A. Jones and I. Porn. </author> <title> 'ought' and 'must'. </title> <journal> Synthese, </journal> <volume> 66 </volume> <pages> 89-93, </pages> <year> 1986. </year>
Reference-contexts: Work in this area relates to the motivations of the previous section, but gives social, moral, and/or rational reasons for behaving in a certain way. Some pointers to the philosophical literature include <ref> [3, 13, 35] </ref>.
Reference: 36. <author> H. Katsuno and A. O. Mendelzon. </author> <title> On the difference between updating a knowledge base and revising it. </title> <booktitle> In Proc. Second Conference on Knowledge Representation and Reasoning, </booktitle> <address> Boston, MA, </address> <year> 1991. </year>
Reference-contexts: Alchourron, Gardenfors, and Makinson (AGM) first formalized the notion of belief revision, and argued that any reasonable revision function would satisfy eight postulates [28, 1]. Katsuno and Mendelzon give a simplified version in six axioms, where ffi denotes the revision operation <ref> [36] </ref>: R1 ffi implies R2 If ^ is satisfiable then ffi ^ R3 If is satisfiable then ffi is also satisfiable R4 If 1 2 and 1 2 then 1 ffi 1 2 ffi 2 R5 ( ffi ) ^ implies ffi ( ^ ) R6 If ( ffi ) ^
Reference: 37. <author> H. A. Kautz. </author> <title> A Formal Theory of Plan Recognition. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <year> 1987. </year>
Reference-contexts: For example, agents may induce information about the plans and beliefs of other agents by observing their behavior; thus, work on plan-recognition in AI is as applicable to multi-agent systems (cf. [12]) as it is to natural-language understanding (cf. <ref> [2, 37, 44] </ref>). Agents may also communicate this information directly to one another. This example in particular requires that all agents be built of the "stuff" of plans, beliefs, and their ilk; the assumption is that an agent with no beliefs or plans would find it hard to articulate any.
Reference: 38. <author> K. Konolige. </author> <title> A Deduction Model of Belief. </title> <publisher> Pitman / Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: Several attempts were made to avoid this property: Kono-lige adopts a syntactic solution <ref> [38] </ref>, Moses defines the notion of `resource-bounded knowledge' [48], and Levesque [43] as well as Fagin and Halpern [26] talk of explicit versus implicit belief.
Reference: 39. <author> K. Konolige and M. E. Pollack. </author> <title> Ascribing plans to agents: Preliminary report. </title> <booktitle> In Proc. 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 924-930, </pages> <year> 1989. </year>
Reference-contexts: Agents built with this architecture create plans and maintain them in a changing environment. Konolige and Pollack use a BDI architecture to recognize the plans of other agents <ref> [39] </ref>. Rao and Georgeff have also defined a logic with beliefs, goals, and intentions [54]. Among the more influential work is that Cohen and Levesque [16, 15, 17]. They have proposed a logic that defines intentions in terms of goals and beliefs.
Reference: 40. <author> W. Lenzen. </author> <title> Recent work in epistemic logic. </title> <journal> ACTA Phil Fennica, </journal> <volume> 30 </volume> <pages> 1-137, </pages> <year> 1978. </year>
Reference-contexts: A common philosophical slogan about knowledge being "justified true belief," which can be traced back to Plato [23], has not been backed up with a formal theory, and at least for a while seemed to fall into disrepute within philosophy (cf. [30]). Within philosophy Lenzen's work <ref> [40, 41] </ref> is an exception, and within AI published material on the connection between the knowledge and belief includes Moses and Shoham's [48]. Recent but as yet unpublished work by a number of researchers, all related to conditional logic, promises new insights.
Reference: 41. <editor> W. Lenzen. Epistemologische betrachtungen zu (S4, S5). Erkenntnis, </editor> <volume> 14 </volume> <pages> 33-56, </pages> <year> 1979. </year>
Reference-contexts: A common philosophical slogan about knowledge being "justified true belief," which can be traced back to Plato [23], has not been backed up with a formal theory, and at least for a while seemed to fall into disrepute within philosophy (cf. [30]). Within philosophy Lenzen's work <ref> [40, 41] </ref> is an exception, and within AI published material on the connection between the knowledge and belief includes Moses and Shoham's [48]. Recent but as yet unpublished work by a number of researchers, all related to conditional logic, promises new insights.
Reference: 42. <author> Y. Lesperance. </author> <title> A Formal Theory of Indexical Knowledge and Action. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <year> 1991. </year> <note> Technical Report CSRI-248. </note>
Reference-contexts: Lesperance has argued that in-dexicality may be more than expedient: it may be necessary in order to communicate certain ideas between agents <ref> [42] </ref>, and Grove has made simliar arguments [31]. Although we have concentrated in this paper on qualitative methods, it should be mentioned that formal quantitative frameworks such as probability theory and decision theory have a lot to say about how agents behave.
Reference: 43. <author> H. J. Levesque and R. J. Brachman. </author> <title> Expressiveness and tractability in knowledge representation and reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 3(2) </volume> <pages> 78-93, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Several attempts were made to avoid this property: Kono-lige adopts a syntactic solution [38], Moses defines the notion of `resource-bounded knowledge' [48], and Levesque <ref> [43] </ref> as well as Fagin and Halpern [26] talk of explicit versus implicit belief. Halpern and Fagin rest their proposal in part on the notion of `awareness'; an agent cannot explicitly believe a formula of which he is not aware.
Reference: 44. <author> D. J. Litman and J. F. Allen. </author> <title> A plan recognition model for subdialogues in conversations. </title> <journal> Cognitive Science, </journal> <volume> 11 </volume> <pages> 163-200, </pages> <year> 1987. </year>
Reference-contexts: For example, agents may induce information about the plans and beliefs of other agents by observing their behavior; thus, work on plan-recognition in AI is as applicable to multi-agent systems (cf. [12]) as it is to natural-language understanding (cf. <ref> [2, 37, 44] </ref>). Agents may also communicate this information directly to one another. This example in particular requires that all agents be built of the "stuff" of plans, beliefs, and their ilk; the assumption is that an agent with no beliefs or plans would find it hard to articulate any.
Reference: 45. <author> L. T. McCarty. </author> <title> Defeasible deontic reasoning. </title> <booktitle> In Fourth International Workshop on Nonmonotonic Reasoning, </booktitle> <pages> pages 139-147, </pages> <year> 1992. </year>
Reference-contexts: They have proposed a logic that defines intentions in terms of goals and beliefs. Their proposal has generated some discussion and debate, some of which can be found in [18]. 5.4 Obligation The notion of obligation has scarcely been studied in AI. See work by McCarty <ref> [45] </ref> and Boutilier [6] for early examples of work formalizing "ought".
Reference: 46. <author> D. V. McDermott. </author> <title> Tarskian semantics, or no notation without denotation! Cognitive Science, </title> <booktitle> 2(3) </booktitle> <pages> 277-282, </pages> <year> 1978. </year>
Reference-contexts: this work? When are we justified in ascribing a particular mental quality such as belief to a particular component of a artifact? In [58], the following elements are suggested as justification: A precise theory regarding the particular mental category; the theory must have clear semantics (or, to quote McDermott in <ref> [46] </ref>, "No Notation without Denota tion"), and should correspond to the commonsense use of the term. A demonstration that the component of the artifact obeys the theory.
Reference: 47. <author> R. C. Moore. </author> <title> Reasoning about Knowledge and Action. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1980. </year>
Reference-contexts: Most work in AI (and some work in distributed systems) on knowledge and belief has its origins in the particular philosophical work of Hintikka [33]. Moore was an early importer of his ideas into AI <ref> [47] </ref>; the most comprehensive and up-to-date discussion of knowledge and belief in computer science appears in [25]. Most formalization of knowledge and belief (and, for that matter, of other mental attitudes) is couched in modal logic.
Reference: 48. <author> Y. Moses and Y. Shoham. </author> <title> Belief as defeasible knowledge. </title> <journal> Artificial Intelligence, </journal> <volume> 64, </volume> <year> 1993. </year>
Reference-contexts: Several attempts were made to avoid this property: Kono-lige adopts a syntactic solution [38], Moses defines the notion of `resource-bounded knowledge' <ref> [48] </ref>, and Levesque [43] as well as Fagin and Halpern [26] talk of explicit versus implicit belief. Halpern and Fagin rest their proposal in part on the notion of `awareness'; an agent cannot explicitly believe a formula of which he is not aware. <p> Within philosophy Lenzen's work [40, 41] is an exception, and within AI published material on the connection between the knowledge and belief includes Moses and Shoham's <ref> [48] </ref>. Recent but as yet unpublished work by a number of researchers, all related to conditional logic, promises new insights.
Reference: 49. <author> C. Nass and J. Steuer. </author> <title> Voices, boxes, and sources of messages: Computers and social actors. </title> <journal> Human Communication Research, </journal> <volume> 19(4) </volume> <pages> 504-527, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: There is evidence that people in general relate to technology as they relate to other people, even with full knowledge that the artifact is not human <ref> [49] </ref>. It is also clear that there is a serious danger of stretching such anthropomorphic metaphors to their breaking points. Explicit mental attitudes can potentially be used to increase the safety margin of the illusion.
Reference: 50. <author> A. Ortony, G. L. Clore, and A. Collins. </author> <title> The cognitive structure of emotions. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: However, to date there has been little work in AI on these examples include the work of Ortony, Clore and Collins <ref> [50] </ref>, and the work of Bates [55] and none formal, and we will have no more to say about this category in the sequel. 3 Statics vs. Dynamics Before we discuss technical aspects of formalizing mental attitudes, we should point to an important distinction.
Reference: 51. <author> J. Pearl. </author> <title> From conditional oughts to qualitative decision theory. </title> <editor> In D. Heckerman and A. Mamdani, editors, </editor> <booktitle> Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 12-20. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: See work by McCarty [45] and Boutilier [6] for early examples of work formalizing "ought". Quite recently, Pearl has proposed a formalization of conditional obligations <ref> [51] </ref>. 6 Final comments In this final section, we will briefly mention three aspects of agency which have not yet come up in this survey: perception, indexicality, and quantitative measures.
Reference: 52. <author> C.R. Perrault, J.F. Allen, and P.R. Cohen. </author> <title> Speech acts as a basis for understanding dialogue coherence. </title> <booktitle> In Proceedings of the Second Workshop on Theoretical Issues in Natural Language Processing (TINLAP-2), </booktitle> <pages> pages 125-132, </pages> <year> 1978. </year>
Reference-contexts: In fact it was early work by Allen, Cohen and Perrault on intelligent dialog systems <ref> [52, 19, 2] </ref> that gave impetus to this line of research, first at the University of Toronto and then at SRI International and the University of Rochester.
Reference: 53. <author> J. Pinto and R. Reiter. </author> <title> A temporal situation calculus. </title> <booktitle> In Proc. Second AAAI Workshop on Formalizing Common Sense, </booktitle> <year> 1993. </year>
Reference-contexts: What in our view is still substantially unsettled is the integration of time and action in a single framework. Several years ago arguments were made that action can be defined in a temporal framework [57], or the other way around [29]. Pinto and Reiter's <ref> [53] </ref> is a more symmetric integration of time and action, and is to our knowledge the latest work in AI on the topic. 4.3 Capabilities Presumably agents cannot commit to, intend, or plan actions of which they do not believe themselves to be capable. Capability is a complex notion.
Reference: 54. <author> A. S. Rao and M. P. Georgeff. </author> <title> Modeling rational agents within a BDI-architecture. </title> <booktitle> In Proc. Second Conference on Knowledge Representation and Reasoning, </booktitle> <pages> pages 473-484, </pages> <year> 1991. </year>
Reference-contexts: Agents built with this architecture create plans and maintain them in a changing environment. Konolige and Pollack use a BDI architecture to recognize the plans of other agents [39]. Rao and Georgeff have also defined a logic with beliefs, goals, and intentions <ref> [54] </ref>. Among the more influential work is that Cohen and Levesque [16, 15, 17]. They have proposed a logic that defines intentions in terms of goals and beliefs.
Reference: 55. <author> W. S. Reilly and J. Bates. </author> <title> Building emotional agents. </title> <type> Technical Report CMU-CS-92--143, </type> <institution> Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: However, to date there has been little work in AI on these examples include the work of Ortony, Clore and Collins [50], and the work of Bates <ref> [55] </ref> and none formal, and we will have no more to say about this category in the sequel. 3 Statics vs. Dynamics Before we discuss technical aspects of formalizing mental attitudes, we should point to an important distinction. Consider belief, for example (but the following applies to any attitude).
Reference: 56. <author> R. Reiter. </author> <title> Nonmonotonic reasoning. </title> <booktitle> Annual Revew of Computer Science, </booktitle> <volume> 2 </volume> <pages> 147-186, </pages> <year> 1987. </year>
Reference-contexts: Any further discussion of nonmonotonic logics lies beyond the scope of this sur-vey; our main point was to alert the reader to this importance connection. For those not familiar with nonmonotonic logics, the most up-to-date survey is Reiter's <ref> [56] </ref>. 4.2 Time and action Time and action are important notions in connection with mental attitudes.
Reference: 57. <author> Y. Shoham. </author> <title> Time for action. </title> <booktitle> In Proc. 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 954-959, </pages> <year> 1989. </year>
Reference-contexts: What in our view is still substantially unsettled is the integration of time and action in a single framework. Several years ago arguments were made that action can be defined in a temporal framework <ref> [57] </ref>, or the other way around [29].
Reference: 58. <author> Y. Shoham. </author> <title> Agent-oriented programming. </title> <journal> Artificial Intelligence, </journal> <volume> 60(1) </volume> <pages> 51-92, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Perhaps the most radical embracing of the notion of mental state in a computational setting is Agent Oriented Programming, a programming paradigm in which the state of individual modules (`agents') consists entirely of mental attitudes such as beliefs and obligations <ref> [58] </ref>. We have so far explained and motivated the `mental attitudes' part of "formal, qualitative, explicit models of mental attitudes"; let us discuss the other parts now. By `formal' we mean theories that are mathematically sound. <p> What then is the criterion by which we evaluate this work? When are we justified in ascribing a particular mental quality such as belief to a particular component of a artifact? In <ref> [58] </ref>, the following elements are suggested as justification: A precise theory regarding the particular mental category; the theory must have clear semantics (or, to quote McDermott in [46], "No Notation without Denota tion"), and should correspond to the commonsense use of the term.
Reference: 59. <author> Y. Shoham and N. Goyal. </author> <title> Temporal reasoning. </title> <editor> In H. Shrobe, editor, </editor> <booktitle> Exploring Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: In general terms, the two major tools used are temporal logics (usually explicit-time logics, not tense logics), and the situation calculus. An short overview of temporal representation in AI can be found in Shoham and Goyal's <ref> [59] </ref>, although that survey is somewhat outdated. Let us mention only one important issue in this area, which we consider not yet settled.
Reference: 60. <author> Yoav Shoham. </author> <title> Reasoning about Change. </title> <publisher> The MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: For example, Boutilier [6] has developed a conditional logic framework which handles both nonmonotonic logic and belief revision, and Friedman and Halpern [27] have defined a notion of belief change which is closely related to a subclass of nonmonotonic logics called `preference logics' <ref> [60] </ref>. 5.3 Belief, Desire, and Intention Compared to the informational attitudes, relatively little work has been done to formalize what we have called motivational attitudes. The classic "BDI" architecture, which attempted to model complete rational agents, is illustrated in the work of Bratman, Israel, and Pollack [9].
Reference: 61. <author> M. P. Singh. </author> <title> A logic of situated know-how. </title> <booktitle> In Proc. National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> pages 343-348, </pages> <year> 1991. </year>
Reference-contexts: Whatever work does exist tends to concentrate on the `physical' variety. Capabilities of agents have been formalized in two ways, as operations on actions ("can-do") or as operations on propositions ("can-achieve"); these terms are used, for example, by Davis [20] and Thomas [62]. Singh uses a related term, "knowhow" <ref> [61] </ref>.
Reference: 62. <author> S. R. Thomas. </author> <title> PLACA, An agent oriented programming language. </title> <type> PhD thesis, </type> <institution> Stan-ford University, </institution> <year> 1993. </year>
Reference-contexts: Whatever work does exist tends to concentrate on the `physical' variety. Capabilities of agents have been formalized in two ways, as operations on actions ("can-do") or as operations on propositions ("can-achieve"); these terms are used, for example, by Davis [20] and Thomas <ref> [62] </ref>. Singh uses a related term, "knowhow" [61].
Reference: 63. <author> J. Van Benthem. </author> <title> The Logic of Time. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Mas-sachusetts, </address> <year> 1983. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Let us mention only one important issue in this area, which we consider not yet settled. Most issues in representing time are well understood; this is largely due to work in philosophical logic (see van Benthem's <ref> [63] </ref> for a comprehensive survey of temporal logics). Action is also well studied, though there is still no universal agreement on some issues such as the modelling of concurrent or overlapping actions.
References-found: 63

