URL: http://www.cogs.susx.ac.uk/users/hilanb/ciga.ps
Refering-URL: http://www.cs.bham.ac.uk/~wbl/biblio/gp-bibliography.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email:fhilanb, ibrahimg@cogs.susx.ac.uk  
Title: Constructive Induction using Genetic Programming  
Author: Hilan Bensusan and Ibrahim Kuscu 
Address: Falmer, Brighton BN1 9QH  
Affiliation: School of Computing and Cognitive Sciences University of Sussex  
Abstract: A constructive induction model using genetic programming is presented. The model evolves new attributes starting from a random population of possible attributes constructed as functions of the original attributes. The model is tested on hard supervised learning problems and its performance is compared with backpropagation and C4.5. The performance of the system on learning incomplete 4-bit parity is reported to be bet ter.
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA, USA, </address> <year> 1984. </year>
Reference-contexts: In general, CI is used to deal with problems that are hard for the most used learning algorithms, such as feed forward neural networks trained by backpropagation ( [ 14 ] ) and top down decision tree building by a CART-like algorithm ( <ref> [ 1 ] </ref> , [ 12 ] ) 1 . Such learning algorithms are often put in a common class called the class of selective learners.
Reference: [ 2 ] <author> S. E. Fahlman. </author> <title> An empirical study of learning speed in back-propagation networks. </title> <type> Technical Report CMU-CS-88-162, </type> <institution> Computer Science Department - Carnegie-Mellon, </institution> <address> Pittsburgh, PA - USA, </address> <year> 1988. </year>
Reference-contexts: From the original 12 training instances, 8 have been randomly selected for the train-train set and the remaining 4 were used as a train-test set. The test set was composed by unseen cases only (the other 4 instances). We have used a 6-2-1 feed forward network and QuickProp ( <ref> [ 2 ] </ref> ) as the implementation of the selective learner. Given the training set, the genetic constructive induction system (GCI) detects the need for a better attribute vector and calls the genetic search to find a suitable one.
Reference: [ 3 ] <author> R. French and A. Messinger. </author> <title> Genes, phenes and the baldwin effect. </title> <booktitle> In Artificial Life IV, </booktitle> <address> Cam-bridge MA, USA, 1994. </address> <publisher> MIT Pres. </publisher>
Reference-contexts: One can also think of the Baldwin Effect, where the course of evolutionary change is determined by the ability of the individuals to acquire a fit behaviour by learning (see [ 10 ] , <ref> [ 3 ] </ref> ). A system like GCI shows that there is a tradeoff between the generality of the operators for construction of new attributes in CI and the complexity of the search for new attributes.
Reference: [ 4 ] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> MA, USA, </address> <year> 1989. </year>
Reference-contexts: AND @R @ AND @R *I2* *I1* The Schema Theorem developed by Holland ( [ 5 ] ) is based on genetic search and has proven to be useful in many applications involving large, complex and deceptive search spaces ( <ref> [ 4 ] </ref> ). Genetic search is thus most likely to allow fast, robust evolution of genotypes encoding for potential attributes as Boolean expressions. Using Genetic Algorithms (GA) the model is imple mented in LISP. The top level structure of the system executes the following routine: 1.
Reference: [ 5 ] <author> J. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI, USA, </address> <year> 1975. </year>
Reference-contexts: A random point in the selected expression (tree) is chosen as a crossover or mutation point. The typical structure of an expression (AND ((*I1*)(OR (AND (*I2 I1*)(*I2*))))) is shown in figure 1. AND @R @ AND @R *I2* *I1* The Schema Theorem developed by Holland ( <ref> [ 5 ] </ref> ) is based on genetic search and has proven to be useful in many applications involving large, complex and deceptive search spaces ( [ 4 ] ). Genetic search is thus most likely to allow fast, robust evolution of genotypes encoding for potential attributes as Boolean expressions.
Reference: [ 6 ] <author> J. Koza. </author> <title> Genetic Programming:On the programming of computers by means of natural selection. </title> <publisher> MIT press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Learning problems that are hard for selective learners such as learning parity functions, monk2 ( [ 19 ] )or conditional approach ( [ 17 ] ) are therefore often referred simply as hard learning problems. In the genetic programming (GP) paradigm ( <ref> [ 6 ] </ref> [ 7 ] ), problems of artificial intelligence (AI) are viewed as the discovery of computer programs which produce desired outputs for particular inputs. A computer program could be an expression, formula, plan, control strategy, decision tree or a model depending on the sort of AI problem. <p> The system is therefore neither data-driven nor hypothesis-driven (see [ 21 ] for a classification of CI systems) for it performs a genetic based search oriented by the performance of the selective learner. 3 THE GENETIC SEARCH MODEL Koza <ref> [ 6 ] </ref> claims that solving AI problems requires searching the space of possible computer programs for the better fitting individual computer program. GP is a method of searching for this better fitting individual computer program based on Darwinian selection and genetic operations.
Reference: [ 7 ] <editor> J. Koza. </editor> <booktitle> Genetic Programming II. </booktitle> <publisher> MIT press, </publisher> <year> 1994. </year>
Reference-contexts: Learning problems that are hard for selective learners such as learning parity functions, monk2 ( [ 19 ] )or conditional approach ( [ 17 ] ) are therefore often referred simply as hard learning problems. In the genetic programming (GP) paradigm ( [ 6 ] <ref> [ 7 ] </ref> ), problems of artificial intelligence (AI) are viewed as the discovery of computer programs which produce desired outputs for particular inputs. A computer program could be an expression, formula, plan, control strategy, decision tree or a model depending on the sort of AI problem.
Reference: [ 8 ] <author> I. Kuscu. </author> <title> Evolution of learning rules for hard learning problems. </title> <booktitle> In the Proceedings of The Fifth Annual Conference on Evolutionary Programming, </booktitle> <year> 1996. </year>
Reference-contexts: This paper reports initial results obtained by using genetic programming as part of a constructive induction system to learn supervised tasks. GP techniques have been used to tackle hard learning problems ( <ref> [ 8 ] </ref> ). Since such learning problems involve functions on the attribute vector and genetic operators can be used to evolve a solution for these problems starting from a random initial population of functions. In this case, the genetic operators do the learning themselves.
Reference: [ 9 ] <author> C Matheus. </author> <title> Feature construction: an analytic framework and an application to decision trees. </title> <type> Technical Report CSR-89-1559, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL,USA, </institution> <year> 1989. </year>
Reference-contexts: 1 INTRODUCTION Constructive induction (CI) is an effort to improve the attribute vector of a learning problem in order to make the problem more easily learned for a particular learning algorithm (see <ref> [ 9 ] </ref> , [ 11 ] , [ 21 ] ).
Reference: [ 10 ] <author> J. Maynard-Smith. </author> <title> When learning guides evolution. </title> <journal> Nature, </journal> <volume> 329 </volume> <pages> 761-762, </pages> <year> 1987. </year>
Reference-contexts: One can also think of the Baldwin Effect, where the course of evolutionary change is determined by the ability of the individuals to acquire a fit behaviour by learning (see <ref> [ 10 ] </ref> , [ 3 ] ). A system like GCI shows that there is a tradeoff between the generality of the operators for construction of new attributes in CI and the complexity of the search for new attributes.
Reference: [ 11 ] <author> G. Pagallo and D. Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 71-100, </pages> <year> 1990. </year>
Reference-contexts: 1 INTRODUCTION Constructive induction (CI) is an effort to improve the attribute vector of a learning problem in order to make the problem more easily learned for a particular learning algorithm (see [ 9 ] , <ref> [ 11 ] </ref> , [ 21 ] ). <p> They have important similarities concerning their soft biases since 1 The BP-HCI system of Sazonov and Wnek ( [ 15 ] ) is an example of applying CI to improve backpropagation's performance while Pagallo's ( <ref> [ 11 ] </ref> ) FRINGE exemplifies the application of CI to ID3. they are based on the exploitation of statistical regularities between the attributes and the output values (see [ 18 ] ).
Reference: [ 12 ] <author> R Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, USA, </address> <year> 1993. </year>
Reference-contexts: In general, CI is used to deal with problems that are hard for the most used learning algorithms, such as feed forward neural networks trained by backpropagation ( [ 14 ] ) and top down decision tree building by a CART-like algorithm ( [ 1 ] , <ref> [ 12 ] </ref> ) 1 . Such learning algorithms are often put in a common class called the class of selective learners.
Reference: [ 13 ] <author> L Rendell and R. Seshu. </author> <title> Learning hard concepts through constructive induction: Framework and rationale. </title> <journal> Computational Intelligence, </journal> <volume> 9 </volume> <pages> 247-270, </pages> <year> 1990. </year>
Reference-contexts: CI is often used to tackle hard problems for a given learning algorithm L problems are hard for L if the training set contains all the relevant information for the induction of the target function but this information cannot be extracted by L (see <ref> [ 13 ] </ref> ).
Reference: [ 14 ] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. Rumelhart, J. McClelland, </editor> <title> and the PDP Research Group, editors, Parallel Distributed Processing: Explorations in the Micro-structures of Cognition. Vols I and II. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986. </year>
Reference-contexts: In general, CI is used to deal with problems that are hard for the most used learning algorithms, such as feed forward neural networks trained by backpropagation ( <ref> [ 14 ] </ref> ) and top down decision tree building by a CART-like algorithm ( [ 1 ] , [ 12 ] ) 1 . Such learning algorithms are often put in a common class called the class of selective learners.
Reference: [ 15 ] <author> V. Sazonov and J. Wnek. </author> <title> A hypothesis-driven constructive induction approach to expanding neural networks. </title> <booktitle> In ML-COLT '94 Workshop on Constructive Induction and Change of Representation, </booktitle> <year> 1994. </year>
Reference-contexts: Such learning algorithms are often put in a common class called the class of selective learners. They have important similarities concerning their soft biases since 1 The BP-HCI system of Sazonov and Wnek ( <ref> [ 15 ] </ref> ) is an example of applying CI to improve backpropagation's performance while Pagallo's ( [ 11 ] ) FRINGE exemplifies the application of CI to ID3. they are based on the exploitation of statistical regularities between the attributes and the output values (see [ 18 ] ).
Reference: [ 16 ] <author> R. Seshu. </author> <title> Solving the parity problem. </title> <type> Technical report, </type> <institution> Inductive Learning Group Beck-man Institute University of Illinois at Urbana-Champaign, </institution> <year> 1989. </year>
Reference-contexts: The result is a poor performance in the unseen cases of the test set. It is known that the addition of a suitable parity attribute in the attribute vector can weaken the soft bias of selective learners against any Boolean function ( <ref> [ 16 ] </ref> ). For this reason we have used a 2-bit parity function (XOR) as the only operator of the initial population of potential new attributes. The idea was to find constructed attributes expressed in terms of 2-parity operators that could easen the job of backpropagation.
Reference: [ 17 ] <author> C. Thornton. </author> <title> Supervised learning of conditional approach: a case study. </title> <type> Technical Report 291, </type> <institution> COGS, University of Sussex, </institution> <year> 1993. </year>
Reference-contexts: Learning problems that are hard for selective learners such as learning parity functions, monk2 ( [ 19 ] )or conditional approach ( <ref> [ 17 ] </ref> ) are therefore often referred simply as hard learning problems. In the genetic programming (GP) paradigm ( [ 6 ] [ 7 ] ), problems of artificial intelligence (AI) are viewed as the discovery of computer programs which produce desired outputs for particular inputs.
Reference: [ 18 ] <author> C. Thornton. </author> <title> Measuring the difficulty of specific learning problems. </title> <journal> Connection Science, </journal> <volume> 7(1), </volume> <year> 1995. </year>
Reference-contexts: Sazonov and Wnek ( [ 15 ] ) is an example of applying CI to improve backpropagation's performance while Pagallo's ( [ 11 ] ) FRINGE exemplifies the application of CI to ID3. they are based on the exploitation of statistical regularities between the attributes and the output values (see <ref> [ 18 ] </ref> ). Learning problems that are hard for selective learners such as learning parity functions, monk2 ( [ 19 ] )or conditional approach ( [ 17 ] ) are therefore often referred simply as hard learning problems.
Reference: [ 19 ] <author> S. B. Thrun, J. Bala, E. Bloendorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. D^zeroski, S. E. Fahlman, D. Fisher, R. Hamann, K. Kauf-man, S. Keller, I. Kononenko, J. Kreuziger, R. S. Michalski, T. Mitchell, P. Pachovicz, Y. Re-ich, H. Vafaie, W. Van de Welde, W. Wentzel, J. Wnek, and J. Zhang. </author> <title> The monk's problems a performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> School of Computer Science, Carnegie-Mellon University., </institution> <address> Pittsburgh, PA - USA, </address> <year> 1991. </year>
Reference-contexts: Learning problems that are hard for selective learners such as learning parity functions, monk2 ( <ref> [ 19 ] </ref> )or conditional approach ( [ 17 ] ) are therefore often referred simply as hard learning problems.
Reference: [ 20 ] <author> D. Whitley and T. Hanson. </author> <title> Optimizing neural networks using faster, more accurate genetic search. </title> <editor> In J.D. Schaffer, editor, </editor> <booktitle> Proceedings of Third iInternational Conference on Genetic Algorithms, </booktitle> <year> 1989. </year>
Reference-contexts: Apply genetic operators to create a new popula tion 5. If the solution is found or sufficient number of generations are created then stop; if not go to 2. In the model, parent selection technique for reproduction is normalised by using an exponential function taken from Whitley's ( <ref> [ 20 ] </ref> ) rank-based selection technique. The function generates integer numbers from 1 to the size of the population The generation of numbers exhibits characteristics of a non-linear function where there is a greater tendency to produce smaller numbers (since higher scoring expressions are on top of the rank).
Reference: [ 21 ] <author> J. Wnek and R. S. Michalski. </author> <title> Hypothesis- driven constructive induction in aq17: A method and experiments. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 139-169, </pages> <year> 1994. </year>
Reference-contexts: 1 INTRODUCTION Constructive induction (CI) is an effort to improve the attribute vector of a learning problem in order to make the problem more easily learned for a particular learning algorithm (see [ 9 ] , [ 11 ] , <ref> [ 21 ] </ref> ). <p> Each pair of new attributes compose, along with the original attribute, an input vector that is measured for fitness. The system is therefore neither data-driven nor hypothesis-driven (see <ref> [ 21 ] </ref> for a classification of CI systems) for it performs a genetic based search oriented by the performance of the selective learner. 3 THE GENETIC SEARCH MODEL Koza [ 6 ] claims that solving AI problems requires searching the space of possible computer programs for the better fitting individual
References-found: 21

