URL: http://www.ai.sri.com/~blei/blei.ps
Refering-URL: http://www.ai.sri.com/~blei/
Root-URL: 
Title: Optimal Navigation in a Probibalistic World  
Author: David Blei 
Date: June 5, 1997  
Abstract: In this paper, we define and examine two versions of the bridge problem. The first variant of the bridge problem is a determistic model where the agent knows a superset of the transitions and a priori probabilities that those transitions are intact. In the second variant, transitions can break or be fixed with some probability at each time step. These problems are applicable to planning in uncertain domains as well as packet routing in a computer network. We show how an agent can act optimally in these models by reduction to Markov decision processes. We describe methods of solving them but note that these methods are intractable for reasonably sized problems. Finally, we suggest neuro-dynamic programming as a method of value function approximation for these types of models.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bertsekas, Dimitri P. and John N. Tsitsiklis. </author> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific; Belmont, </publisher> <year> 1996. </year>
Reference-contexts: world, how can the agent maximize its reward? Formally, a finite MDP is represented by a tuple, (S; A; T; R) where * S is a finite set of states in the world. * A is a finite set of actions. * T : S fi A fi S ! <ref> [0; 1] </ref> is the state-transition function. For each state and action, T gives a probability distribution over next states. T (s; a; s 0 ) represents the probability of reaching state s 0 given that the agent began in state s and took action a. <p> Neuro-dynamic programming suggests choosing some parametrization of the value function and then trying to learn those parameters with a combination of reinforcement learning and neural networks. Figure 7 illustrates this model for learning the value function. Bertsekas and Tsitsiklis have written a comprehensive source book on this field <ref> [1] </ref>. Due to time constraints, we haven't implemented these methods yet. Results on this kind of solution vary from problem to problem and rely heavily on a clever choice of parametrization of the value function.
Reference: [2] <author> Dean, Bayse, and Shewchuck. </author> <title> Reinforcement Learning for Planning and Control,in Machine Learning Methods for Planning and Scheduling, </title> <editor> ed. Steve Minton. </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Stochastic value iteration updates the value function with the least mean square rule. It uses model simulation to approximate the value of the next state so a large state space or ignorance of the model is no obstacle <ref> [2] </ref>. However, we are trying to solve a continuous state MDP. As we said earlier, this means that the value function is continuous. So neither value iteration nor stochastic value iteration will work since they both assume discrete models.
Reference: [3] <author> Kaelbling, Littman, and Cassandra. </author> <title> Planning and Acting in Partially Observable Stochastic Domains. </title> <month> November 1, </month> <year> 1995. </year>
Reference-contexts: The policy computes the next action based on the current belief state given by the state estimator. Figure 3 nicely depicts how these parts work together <ref> [3] </ref>. We have several choices for exactly how to define the belief state. For instance, it could simply be the state that we are most likely in given our past history of observations and actions. In general however this is not enough information. <p> For instance, it could simply be the state that we are most likely in given our past history of observations and actions. In general however this is not enough information. The best choice of belief state is a probability distribution over states <ref> [3] </ref>. If is a belief state, (s) is the probability that we are in state s 2 S. Note that since is a probability distribution over states, P With this choice of belief state, the state estimator (SE) is straightforward. <p> Therefore, V k is piecewise-linear and convex. In the infinite horizon, it is no longer necessarily linear since there may be infinitely many pieces. However, it can be shown that the optimal infinite horizon value function can be approximated by computing the k-step value function for sufficiently large k <ref> [3] </ref>. Therefore, the optimal value function is effectively p.l.c. Of course, the number of policy trees is too large to effectively search and compute the optimal value function. However, efficient algorithms exist to exactly solve POMDPs. <p> Of course, the number of policy trees is too large to effectively search and compute the optimal value function. However, efficient algorithms exist to exactly solve POMDPs. They are beyond the scope of this paper but Kaelbling, Littman, and Cassandra provide a good overview of the Witness algorithm <ref> [3] </ref>. Because the optimal value function is p.l.c., the optimal policy is constructible by dividing the state space into regions of the various linear pieces. These regions are each associated with an action. The agent takes the action associated with the region in which its belief state falls. <p> In the belief state MDP of our first conversion, we showed that the optimal value function is piecewise-linear and convex. These properties allowed us to find an optimal policy <ref> [3] </ref>. In this MDP however, the value function is no longer piecewise-linear.
Reference: [4] <author> Littman, Dean, and Kaelbling. </author> <title> On the Complexity of Solving Markov Decision Problems. </title> <booktitle> Proceedings of the Eleventh International Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: Value iteration runs in time polynomial in jSj, jAj, the greatest possible reward on any given step, and 1 1fl . The proof of this is beyond the scope of this paper however Littman, Dean, and Kaelbling provide an excellent explanation <ref> [4] </ref>. 5 Examples To find a solution to the problem in Figure 1, we need to solve an MDP with 972 states. The resulting optimal policy first tries bridge b0. If it works, it attempts b1 to reach the goal island.
Reference: [5] <author> Puterman, Martin L. </author> <title> Markov Decision Processes. </title> <publisher> John Wiley and Sons; New York City, </publisher> <year> 1994 </year>
Reference-contexts: In 1875, Cayley proposed a sequential lottery problem which, in 1962, was solved 2 by Karlin using an MDP. Martin Puterman's Markov Decision Processes <ref> [5] </ref> is an excellent, comprehensive source on this topic. The second variant of the bridge problem is equivalent to the first problem except on each time step, there is some chance that a bridge will break or be fixed. <p> Maximizing the entire value function for a stationary policy means finding the unique solution to the set of equations V (s) = max [R (s; a) + fl s 0 2S for all s. This system of equations is known as the Bellman equations <ref> [5] </ref>. Our optimal stationary policy is a mapping from each s to that a for which the above equation gives a maximum. That is, (s) = arg max [R (s; a) + fl s 0 2S We are primarily concerned with infinite horizon models and stationary policies for two reasons. <p> We continue computing V t until the maximum difference in V t (s) and V t1 (s) is sufficiently small. The policy derived from this value function is provably optimal <ref> [5] </ref>. 4 Building an MDP from a Bridge Problem 4.1 A Formal Definition of the Bridge Problem Before we describe how to convert the bridge problem into an MDP, we need some formal notation to represent the bridge problem (BP).
References-found: 5

