URL: http://www.cs.indiana.edu/pub/dswise/ATABLE92.ps.Z
Refering-URL: http://www.cs.indiana.edu/pub/dswise/
Root-URL: http://www.cs.indiana.edu
Email: dswise@cs.indiana.edu  
Title: Matrix Algorithms using Quadtrees Invited Talk,  
Author: David S. Wise 
Date: June, 1992  
Address: Bloomington, Indiana 47405-4101, USA  
Affiliation: Computer Science Department Indiana University  
Pubnum: ATABLE-92 Technical Report 357  
Abstract: Several years ago functional style drew me to treatment of d-dimensional arrays as 2 d -ary trees; in particular, matrices become quaternary trees or quadtrees. This convention yields efficient recopying-cum-update of any array; recursive, algebraic decomposition of conventional arithmetic algorithms; and uniform representations and algorithms for both dense and sparse matrices. For instance, any nonsingular subtree is a candidate as the pivot block for Gaussian elimination; the restriction actually helps identification of pivot blocks, because searching a tree is easy. A new block-decomposition formulation for LU decomposition, called (L + U ); D 0 decomposition, is particularly well suited to quadtrees. It provides efficient representation of intermediate matrices when pivoting on blocks of various sizes, i.e. during "undulant-block" elimination. A given matrix, A is decomposed into two matrices, plus two permutations. The fl To appear in Proc. Intl. Workshop on Arrays, Functional Lanaugages, and Parallel Systems, Montreal (1992). y Research reported herein was sponsored, in part, by the National Science Foundation under Grant Number CCR 90-02797. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. K. Abdali & D. S. Wise. </author> <title> Experiments with quadtree representation of matrices Proceedings 1988 Intl. </title> <booktitle> Symp. on Symbolic and Algebraic Computation, Lecture Notes in Computer Science 358 Berlin, Springer (1989), </booktitle> <pages> 96-108. </pages>
Reference-contexts: quads ) where colExchange [nw,ne,sw,se] = [ne,nw,se,sw] prmDiagSqsh [nw,ne,sw,se] = [nw,se,nw,se] offDiagSqsh [nw,ne,sw,se] = [sw,ne,sw,ne] abs _ = error "abs not implemented yet on Matrces." signum _ = error "signum not implemented yet on Matrces." 6 In fact, experiments in Reduce that applied these algorithms to integer and symbolic matrices <ref> [1] </ref> of sizes 4 fi 4 up to 100 fi 100 demonstrated the accelerations with sparsity that one would expect from Table 1. The improvement depends only on the axioms on ZeroM for identity and annihilation.
Reference: [2] <author> P. Beckman. </author> <title> Static measures of quadtree prepresentation of the Harwell-Boeing sparse matrix collection. </title> <type> Tech. </type> <institution> Rept. 324, Computer Science Dept., Indiana University (Jan. </institution> <year> 1991). </year>
Reference-contexts: Importantly, Table 1 indicates that this single representation responds well to sparse pathologies, so that we can write uniform algorithms and run them on both sparse and non-sparse problems with confidence that they will perform with reasonable efficiency in special cases. Related results by Beckman <ref> [2] </ref> show another use of quadtree representation: to configure sparse matrices for input/output. The Harwell-Boeing test suite [6] is formatted on magnetic media to suit Fortran transput conventions; it contains both floating-pont entries and location descriptors about each one's location in the matrix.
Reference: [3] <author> P. Beckman. </author> <title> Parallel LU Decomposition for Sparse Matrices using Quadtrees on a Shared-Heap Multiprocessor. </title> <type> Ph.D. dissertation, </type> <note> Indiana University (in preparation). </note>
Reference-contexts: In this way, a quadtree decorated only for scalar elimination might, nevertheless, experience elimination of 2 fi 2 blocks. 5 Experience In order to test this family of algorithms, Beckman <ref> [3] </ref> has implemented them in C on the BBN Butterfly 1000 computer and tested them on twenty different matrices from the Harwell-Boeing collection [6]. Typically these were unsym-metrical, real, assembled matrices of order 1000 or so.
Reference: [4] <author> I. S. Duff, A. M. Erisman, & J. K. Reid. </author> <title> Direct Methods for Sparse Matrices. </title> <publisher> Clarendon Press, </publisher> <address> Oxford (1989). </address>
Reference-contexts: Fill-in is predicted using Markovitz vectors <ref> [4] </ref> accumulated during a former elimination. Size is isomorphic to the depth of the block within the tree; a semidecision procedure [20] is used to identify which blocks in A are non-singular. <p> That is, the ScalarM elements of the quadtree would better require more than one flop in each step: e.g. symbolic arithmetic, exact arithmetic, or elements of hypermatrices <ref> [4] </ref>. The problem is that there is significant overhead in building nodes near the leaves of the tree that can only be amortized by an advantage in parallelism. 12 Experience shows that these algorithms do perform well on problems that have large subproblems.
Reference: [5] <author> J. W. Demmel & N. J. Higham. </author> <title> Stability of block algorithms with fast Level 3 BLAS. </title> <note> LAPACK Working Note 22, </note> <institution> Computer Science Dept., The Univ. of Tennessee (revised: </institution> <month> July </month> <year> 1991). </year> <journal> ACM Trans. Math. </journal> <note> Software (to appear). </note>
Reference-contexts: This formulation supersedes earlier attempts at Gauss-Jordan elimination on quadtrees [17, 18]. It is distinguished from pivoting on blocks of fixed size [9], that can be unstable when there is no decent pivot block of the fixed size <ref> [5] </ref>. Under undulant-block pivoting, however, one can always eliminate a troublesome, large entry as a 1 fi 1 block before resuming elimination of larger blocks. Definition. A matrix A is proper lower (upper) triangular if a i;j = 0 for i j (respectively, i j). Notation. <p> These values can be computed as follows: first D 0 mc is solved recursively (with good accuracy); then the pivot row and pivot column are completed using BLAS Level 3 operations <ref> [5] </ref>, ( A 0 me ) = D 0 nc sc = A nc mc ; finally, t ( P w P e ) ; Q n nw A 0 A 0 se ; D 0 ne sw D 0 results from recursive pivoting on the (n k) fi (n k)
Reference: [6] <author> I. S. Duff & R. G. Grimes & J. G. Lewis. </author> <title> Sparse matrix test problems. </title> <journal> ACM Trans. Math. </journal> <volume> Software 15, </volume> <month> 1 (March, </month> <year> 1989), </year> <pages> 1-14. </pages>
Reference-contexts: Related results by Beckman [2] show another use of quadtree representation: to configure sparse matrices for input/output. The Harwell-Boeing test suite <ref> [6] </ref> is formatted on magnetic media to suit Fortran transput conventions; it contains both floating-pont entries and location descriptors about each one's location in the matrix. <p> a quadtree decorated only for scalar elimination might, nevertheless, experience elimination of 2 fi 2 blocks. 5 Experience In order to test this family of algorithms, Beckman [3] has implemented them in C on the BBN Butterfly 1000 computer and tested them on twenty different matrices from the Harwell-Boeing collection <ref> [6] </ref>. Typically these were unsym-metrical, real, assembled matrices of order 1000 or so. Absolute performance must be interpreted because the system is layered over an expensive foundation: a global heap (managed by reference counting) on an architecture where memory latency is sensitive to non-local references (NUMA).
Reference: [7] <author> V. N. </author> <title> Faddeeva Computational Methods of Linear Algebra. </title> <publisher> Dover Publications, </publisher> <address> New York (1959). </address> <note> Translated from Russian, originally published Moscow (1950). </note>
Reference-contexts: Notation. I denotes the identity matrix of any order. Similarly, 0 denotes the zero matrix of any order. Definition. Two matrices, A and B, are said to be disjoint if 8i; j (a i;j = 0 _ b i;j = 0): Definition. A square matrix A is quasi-diagonal <ref> [7] </ref> if it has square submatrices (cells) along its main diagonal with its remaining elements equal to zero. This more obscure term is used instead of block-diagonal to emphasize that the blocks along the diagonal can differ in size, as Faddeeva illustrates.
Reference: [8] <author> P. C. Fischer & R. L. Probert. </author> <title> Storage reorganization techniques for matrix computation in a paging environment. </title> <journal> Comm. ACM 22, </journal> <month> 7 (July </month> <year> 1979), </year> <pages> 405-415. </pages>
Reference-contexts: Certainly the algorithm is intended for block decomposition, but it does not require any particular one. Blocks may be sized according to the size of pages under virtual memory, as long ago recommended <ref> [8] </ref>, or be determined by the geography of a block within a distributed system. Of course, a block is here intended to be a quadrant, or a quadrant of a quadrant, etc.: some subtree of a quadtree. <p> Not only is this critical to absorbing logarithmic access times (Table 1), but also it provides locality where, for instance, subblocks are stored locally|either on the same distributed processor or adjacently in cached or paged <ref> [8] </ref> memory. My presumption that memory is a heap creates a large storage-management problem, which a separate project at Indiana is addressing. We are already building our second edition of a self-managing heap designed for general use on multiprocessors like the Butterfly [16].
Reference: [9] <author> G. H. Golub & C. F. Van Loan. </author> <title> Matrix Computations 2nd edition. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore (1989). </address>
Reference-contexts: Haskell can infer the definition of from negate and +, or it can also be specified: one tree traversal rather than two. In the Gaussian elimination algorithm, there is no syntactic distinction between scalar and block multiplication|nor is there here. <ref> [9, x1.3] </ref> 5 Readers familiar with Strassen's algorithm [14] will recognize the quadtree as ideally suited to implement it, and Haskell as the perfect language in which to code it. <p> Therefore it is called "undulant" block pivoting. It is presented in the general case of full pivoting, but partial pivoting is surely available. This formulation supersedes earlier attempts at Gauss-Jordan elimination on quadtrees [17, 18]. It is distinguished from pivoting on blocks of fixed size <ref> [9] </ref>, that can be unstable when there is no decent pivot block of the fixed size [5]. Under undulant-block pivoting, however, one can always eliminate a troublesome, large entry as a 1 fi 1 block before resuming elimination of larger blocks. Definition.
Reference: [10] <author> P. Hudak. </author> <title> Arrays, non-determinism, side-effects, and parallelism: a functional perspective (extended abstract). </title> <editor> In J. Fasel and R. Keller (eds.) </editor> <title> Graph Reduction. </title> <booktitle> Lecture Notes in Computer Science 279, </booktitle> <address> New York, </address> <publisher> Springer (1987), </publisher> <pages> 312-327. </pages>
Reference: [11] <editor> P. Hudak & P. Wadler (eds.) </editor> <title> Report on the Programming Language Haskell, a Non-strict, Purely Functional Language, Version 1.2. </title> <journal> ACM SIG-PLAN Notices 27, </journal> <month> 5 (May, </month> <year> 1992). </year> <month> 15 </month>
Reference-contexts: APL maps across arrays, which is insufficient; Haskell <ref> [11] </ref> maps (or hcringei zipWith3s) only across lists; this is also too weak. The most important target for mapping is homogeneous tuples of relatively small size, so to control the growth of parallelism-and the scheduling problem. Much is done here with fourples. <p> Although this compression was done for our I/O convenience, the results might, nevertheless, be useful on any computer without room for raw representation of matrices. 2 Matrix rings in Haskell The code below is an introduction to Haskell <ref> [11] </ref> and to quadtree programming. Important to understand is the data declaration. It declares "constructors" to be used later in pattern-matches to identify special cases. ZeroM is a nullary constructor, easily recognized and treated as an additive identity and multiplicative annihilator.
Reference: [12] <author> H. Samet. </author> <title> The quadtree and related hierarchical data structures. </title> <journal> Comput. Surveys 16, </journal> <month> 2 (June, </month> <year> 1984), </year> <pages> 187-260. </pages>
Reference-contexts: Thesis. Any d-dimensional array is decomposed by blocks and represented as a 2 d -ary tree. Only vectors and matrices are considered below: where d = 1 yields binary trees and d = 2 suggests quaternary trees|or quadtrees <ref> [12] </ref>. Binary Vector Representation.
Reference: [13] <author> G. W. </author> <title> Stewart Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York (1973), </address> <month> 154. </month>
Reference-contexts: Usually a search for the "midcentral block" occurs between applications of elimination, between recursive invocations of Algorithm 1. Here follows an argument that quadtrees distribute that pivot search across Algorithm 1 so that no extra traversal (bandwidth) is incurred for "pivoting." (The idea is hardly new <ref> [13] </ref>, but it is entertaining that its loss is blamed on Fortran.) Consider, for example, the case of scalar pivoting on the element of largest magnitude. Full pivoting seems to require traversal of the entire matrix.
Reference: [14] <author> V. Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numer. Math. </journal> <volume> 13, </volume> <month> 4 (Aug. </month> <year> 1969), </year> <pages> 354-356. </pages>
Reference-contexts: Haskell can infer the definition of from negate and +, or it can also be specified: one tree traversal rather than two. In the Gaussian elimination algorithm, there is no syntactic distinction between scalar and block multiplication|nor is there here. [9, x1.3] 5 Readers familiar with Strassen's algorithm <ref> [14] </ref> will recognize the quadtree as ideally suited to implement it, and Haskell as the perfect language in which to code it.
Reference: [15] <author> D. S. Wise. </author> <title> Representing matrices as quadtrees for parallel processors (extended abstract). </title> <journal> ACM SIGSAM Bulletin 18, </journal> <month> 3 (August </month> <year> 1984), </year> <pages> 24-25. </pages>
Reference-contexts: In fact, as was pointed out early on <ref> [15] </ref>, this code can even beat Strassen's because it knows the algebra of ZeroM, and uses it to great advantage on sparse matrices. module UndecoratedMatrices (..) where type Quadrants a = [Matrx a] --list of *four* submatrices. data Matrx a = ZeroM | ScalarM a | Mtx (Quadrants a) | IdentM
Reference: [16] <author> D. S. Wise. </author> <title> Design for a multiprocessing heap with on-board reference counting. </title> <editor> In J.-P. Jouannaud (ed.), </editor> <booktitle> Functional Programming Languages and Computer Architecture, Lecture Notes in Computer Science 201, </booktitle> <address> Berlin, </address> <publisher> Springer (1985), </publisher> <pages> 289-304. </pages>
Reference-contexts: My presumption that memory is a heap creates a large storage-management problem, which a separate project at Indiana is addressing. We are already building our second edition of a self-managing heap designed for general use on multiprocessors like the Butterfly <ref> [16] </ref>. It would accelerate the performance reported in the last section more than twice. Significant work remains to be done. For instance, there has been no effort to order the matrix before applying this decomposition.
Reference: [17] <author> D. S. Wise. </author> <title> Parallel decomposition of matrix inversion using quadtrees. </title> <booktitle> Proc. 1986 International Conference on Parallel Processing (IEEE Cat. </booktitle> <volume> No. 86CH2355-6), </volume> <pages> 92-99. </pages>
Reference-contexts: The algorithm is Gaussian elimination of blocks of varying sizes. Therefore it is called "undulant" block pivoting. It is presented in the general case of full pivoting, but partial pivoting is surely available. This formulation supersedes earlier attempts at Gauss-Jordan elimination on quadtrees <ref> [17, 18] </ref>. It is distinguished from pivoting on blocks of fixed size [9], that can be unstable when there is no decent pivot block of the fixed size [5].
Reference: [18] <author> D. S. Wise. </author> <title> Matrix algebra and applicative programming. </title> <editor> In Kahn, G. (Ed.), </editor> <booktitle> Functional Programming Languages and Computer Architecture, Lecture Notes in Computer Science 274, </booktitle> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1987, </year> <pages> 134-153. </pages>
Reference-contexts: The algorithm is Gaussian elimination of blocks of varying sizes. Therefore it is called "undulant" block pivoting. It is presented in the general case of full pivoting, but partial pivoting is surely available. This formulation supersedes earlier attempts at Gauss-Jordan elimination on quadtrees <ref> [17, 18] </ref>. It is distinguished from pivoting on blocks of fixed size [9], that can be unstable when there is no decent pivot block of the fixed size [5].
Reference: [19] <author> D. S. Wise & J. Franco. </author> <title> Costs of quadtree representation of non-dense matrices. </title> <journal> J. Parallel Distrib. Comput. </journal> <volume> 9, </volume> <month> 3 (July </month> <year> 1990), </year> <pages> 282-296. </pages>
Reference-contexts: These subtrees are respectively identified as northwest, northeast, southwest , and southeast, isomorphic to the four quadrants of a block decomposition of the conventional tableau, in the order that those names suggest. Table 1 is borrowed <ref> [19] </ref> to display analytical results of the number of nodes, and the average path length in quadtree matrices of various patterns. The number of nodes is a measure of space; for an n fi n matrix it ranges from 4=3n 2 (33% above serial representation) down to zero.
Reference: [20] <author> D. S. Wise. </author> <title> Undulant-block pivoting and (L + U ); D 0 decomposition. </title> <type> Tech. </type> <institution> Rept. 328, Computer Science Dept., Indiana University (Sept. </institution> <year> 1991). </year> <month> 16 </month>
Reference-contexts: Block algorithms provide a supply, but they raise other problems, like efficient representation and global permutations, that are also addressed here. This terse section is abstracted from Wise <ref> [20] </ref>. The algorithm is Gaussian elimination of blocks of varying sizes. Therefore it is called "undulant" block pivoting. It is presented in the general case of full pivoting, but partial pivoting is surely available. This formulation supersedes earlier attempts at Gauss-Jordan elimination on quadtrees [17, 18]. <p> Fill-in is predicted using Markovitz vectors [4] accumulated during a former elimination. Size is isomorphic to the depth of the block within the tree; a semidecision procedure <ref> [20] </ref> is used to identify which blocks in A are non-singular. Finally (and new here) is the observation that the pivot block can be expanded as Algorithm 1 backs up the tree that is A 0 .
References-found: 20

