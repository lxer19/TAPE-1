URL: http://www.cs.pitt.edu/~gupta/research/Comp/lcpc96.ps
Refering-URL: http://www.cs.pitt.edu/~gupta/research/scheduling.html
Root-URL: 
Phone: 2  
Title: Integrating Program Optimizations and Transformations with the Scheduling of Instruction Level Parallelism  
Author: David A. Berson Pohua Chang Rajiv Gupta Mary Lou Soffa 
Address: Santa Clara, CA 95052  Pittsburgh, Pittsburgh, PA 15260  
Affiliation: 1 Intel Corporation,  University of  
Abstract: Code optimizations and restructuring transformations are typically applied before scheduling to improve the quality of generated code. However, in some cases, the optimizations and transformations do not lead to a better schedule or may even adversely affect the schedule. In particular, optimizations for redundancy elimination and restructuring transformations for increasing parallelism are often accompanied with an increase in register pressure. Therefore their application in situations where register pressure is already too high may result in the generation of additional spill code. In this paper we present an integrated approach to scheduling that enables the selective application of optimizations and restructuring transformations by the scheduler when it determines their application to be beneficial. The integration is necessary because information that is used to determine the effects of optimizations and transformations on the schedule is only available during instruction scheduling. Our integrated scheduling approach is applicable to various types of global scheduling techniques; in this paper we present an integrated algorithm for scheduling superblocks.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Berson, R. Gupta and M.L. Soffa, </author> <title> "Resource Spackling: A framework for integrating register allocation in local and global schedulers," </title> <booktitle> In Proc. of Intl. Conf. on Parallel Architectures and Compilation Techniques, </booktitle> <pages> pages 135-146, </pages> <year> 1994. </year>
Reference-contexts: The importance of integrating these latter phases is growing with the recognition that the quality of code produced for parallel systems can be greatly improved through the sharing of information. Integration of register allocation and instruction scheduling has been recently studied by various researchers <ref> [1, 14, 13] </ref>. However, the integration of optimizations and transformations with schedulers has not been fully addressed. In the optimization phase, optimizations are applied to improve the quality of code while restructuring transformations are applied to increase the instruction level parallelism (ILP). <p> In the scheduling phase, both local and global techniques are used to ex-ploit ILP. Global scheduling techniques uncover and exploit ILP across basic blocks <ref> [5, 8, 6, 1] </ref>. The program is divided into scheduling units (such as traces, superblocks, and control dependence regions) composed of a collection of basic blocks. <p> The technique first measures the requirements of resources at each program point. This information is then used by the scheduler to determine where code motion should occur, taking into account the resource requirements of both resources simultaneously <ref> [1] </ref>. To measure the resource requirements, we first create a special type of DAG for each resource that reflects the resource requirements for the code in a block.
Reference: 2. <author> D. Berson, R. Gupta and M.L. Soffa, "GURRR: </author> <title> A global unified resource requirements representation," </title> <booktitle> In ACM SIGPLAN Workshop on Intermediate Representations, Sigplan Notices, </booktitle> <volume> vol. 30, </volume> <pages> pages 23-34, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: For each resource, the precedence and usage information in a block is used to construct a partial ordering of the operations indicating which pairs of operations temporally share a single instance of the resource under any allowable schedule. Details of computing usage information for registers is described elsewhere <ref> [2] </ref>. For each resource R, a Reuse R DAG is constructed to represent the partial order. Sets of operations in the Reuse R DAG that are fully ordered are called allocation chains, since by definition of the partial order, they can all safely be allocated a single instance of R. <p> Excessive sets are computed by finding the sections of the schedulable block where there are portions of an excessive number of allocation chains, which is performed in graph linear time <ref> [2] </ref>. Critical sets: A critical set of length L is the minimal set of operations that must be propagated out of a schedulable block to reduce that block's critical path length by L cycles.
Reference: 3. <author> P.P. Chang, S.A. Mahlke, and W-M. Hwu, </author> <title> "Using profile information to assist classic code optimizations," </title> <journal> Software-Practice and Experience, </journal> <volume> 21(12) </volume> <pages> 1301-1321, </pages> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: Mutation scheduling selects the appropriate alternative based upon availability of resources. DAG transformations which exploit constant operands are also used. In other related work Ebcioglu et al. and Chang et al. consider the removal of partially dead code during instruction scheduling <ref> [4, 3] </ref>.
Reference: 4. <author> K. Ebcioglu, R.D. Groves, K-C. Kim, G. Silberman, and I. Ziv, </author> <title> "VLIW compilation techniques in a superscalar environment," </title> <booktitle> In Proc. of Sigplan Conf. on Prog. Language Design and Implementation, </booktitle> <pages> pages 36-48, </pages> <year> 1994. </year>
Reference-contexts: Mutation scheduling selects the appropriate alternative based upon availability of resources. DAG transformations which exploit constant operands are also used. In other related work Ebcioglu et al. and Chang et al. consider the removal of partially dead code during instruction scheduling <ref> [4, 3] </ref>.
Reference: 5. <author> J.A. Fisher, </author> <title> "Trace scheduling: a technique for global microcode compaction," </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-30(7):478-490, </volume> <year> 1981. </year>
Reference-contexts: In the scheduling phase, both local and global techniques are used to ex-ploit ILP. Global scheduling techniques uncover and exploit ILP across basic blocks <ref> [5, 8, 6, 1] </ref>. The program is divided into scheduling units (such as traces, superblocks, and control dependence regions) composed of a collection of basic blocks.
Reference: 6. <author> R. Gupta and M.L. Soffa, </author> <title> "Region scheduling: an approach for detecting and redistributing parallelism," </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 16(4) </volume> <pages> 421-431, </pages> <year> 1990. </year>
Reference-contexts: In the scheduling phase, both local and global techniques are used to ex-ploit ILP. Global scheduling techniques uncover and exploit ILP across basic blocks <ref> [5, 8, 6, 1] </ref>. The program is divided into scheduling units (such as traces, superblocks, and control dependence regions) composed of a collection of basic blocks.
Reference: 7. <author> P. Hsu and E. Davidson, </author> <title> "Highly concurrent scalar processing," </title> <booktitle> In Proc. of 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 386-395, </pages> <year> 1986. </year>
Reference-contexts: ILP is uncovered by restructuring the code within a scheduling unit. If enough parallelism is not found within the unit, code is propagated from adjacent scheduling units. Code is moved across units using code duplication, speculative execution [9], and predicated execution <ref> [7] </ref>. The separation of the optimization and scheduling phases is problematic in that the applications of optimizations and transformations are performed without knowing if the scheduler will be able to utilize their effects.
Reference: 8. <author> W-M. Hwu et al., </author> <title> "The superblock: an effective technique for VLIW and superscalar compilation," </title> <booktitle> In The Journal of Supercomputing vol. A, </booktitle> <pages> pages 229-248, </pages> <year> 1993. </year>
Reference-contexts: In the scheduling phase, both local and global techniques are used to ex-ploit ILP. Global scheduling techniques uncover and exploit ILP across basic blocks <ref> [5, 8, 6, 1] </ref>. The program is divided into scheduling units (such as traces, superblocks, and control dependence regions) composed of a collection of basic blocks. <p> In section 2 we identify the optimizations that should be integrated with instruction scheduling. In section 3 we briefly outline the relationship between resource requirements and global code motion. In section 4 we present an integrated schedule driven algorithm using superblocks <ref> [8] </ref>. 2 Optimizations and Instruction Scheduling While the integration of optimizations with instruction scheduling may improve their effectiveness, it also increases the complexity of the instruction scheduling algorithm.
Reference: 9. <author> W-M. Hwu and Y. Patt, </author> <title> "Checkpoint repair for out-of-order execution machines," </title> <booktitle> In Proc. of 14th Annual Intl. Symp. on Comp. Architecture, </booktitle> <pages> pages 18-26, </pages> <year> 1987. </year>
Reference-contexts: ILP is uncovered by restructuring the code within a scheduling unit. If enough parallelism is not found within the unit, code is propagated from adjacent scheduling units. Code is moved across units using code duplication, speculative execution <ref> [9] </ref>, and predicated execution [7]. The separation of the optimization and scheduling phases is problematic in that the applications of optimizations and transformations are performed without knowing if the scheduler will be able to utilize their effects.
Reference: 10. <author> J. Knoop, O. Ruthing and B. Steffen, </author> <title> "Optimal code motion: </title> <journal> theory and practice," In ACM Trans. on Programming Languages and Systems, </journal> <volume> 16(4) </volume> <pages> 1117-1155, </pages> <year> 1994. </year>
Reference-contexts: Also, our approach allows the exploitation of new opportunities for redundancy elimination that arise due to code motion per formed by the instruction scheduler. In our discussion so far we have not considered the elimination of partial redundancy elimination <ref> [10] </ref>. This is because the application of tail duplication that is performed prior to superblock construction, converts partial redundancy to full redundancy.
Reference: 11. <author> J. Knoop, O. Ruthing, B. Steffen, </author> <title> "Partial dead code elimination," </title> <booktitle> In Proc. of Sigplan Conf. on Prog. Language Design and Implementation, </booktitle> <pages> pages 147-158, </pages> <year> 1994. </year>
Reference: 12. <author> S. Novack and A. Nicolau, </author> <title> "Mutation scheduling: a unified approach to compiling for fine-grain parallelism," </title> <booktitle> Proc. Languages and Compilers for Parallel Computing, LNCS 892, </booktitle> <year> 1994. </year>
Reference-contexts: We presented an algorithm using the integrated approach for scheduling su-perblocks. However, this technique is also applicable to other global scheduling techniques such as those based upon control dependence regions. The research most closely related to our work is the mutation scheduling technique proposed by Novack and Nicolau <ref> [12] </ref> which integrates a number of DAG transformations into the instruction scheduler. In most architectures certain functions can be performed through a number of alternative instruction sequences. Mutation scheduling selects the appropriate alternative based upon availability of resources. DAG transformations which exploit constant operands are also used.
Reference: 13. <author> C. Norris and L. Pollock, </author> <title> "A scheduler-sensitive global register allocator," </title> <booktitle> In Proc. of Supercomputing, </booktitle> <pages> pages 804-813, </pages> <year> 1993. </year>
Reference-contexts: The importance of integrating these latter phases is growing with the recognition that the quality of code produced for parallel systems can be greatly improved through the sharing of information. Integration of register allocation and instruction scheduling has been recently studied by various researchers <ref> [1, 14, 13] </ref>. However, the integration of optimizations and transformations with schedulers has not been fully addressed. In the optimization phase, optimizations are applied to improve the quality of code while restructuring transformations are applied to increase the instruction level parallelism (ILP).
Reference: 14. <author> S. Pinter, </author> <title> "Register allocation with instruction scheduling: a new approach," </title> <booktitle> In Proc. of Sigplan Conf. on Prog. Lang. Design and Impl., </booktitle> <pages> pages 248-257, </pages> <year> 1993. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: The importance of integrating these latter phases is growing with the recognition that the quality of code produced for parallel systems can be greatly improved through the sharing of information. Integration of register allocation and instruction scheduling has been recently studied by various researchers <ref> [1, 14, 13] </ref>. However, the integration of optimizations and transformations with schedulers has not been fully addressed. In the optimization phase, optimizations are applied to improve the quality of code while restructuring transformations are applied to increase the instruction level parallelism (ILP).
References-found: 14

