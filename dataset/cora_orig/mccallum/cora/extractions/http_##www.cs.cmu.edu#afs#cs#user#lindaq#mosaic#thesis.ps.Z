URL: http://www.cs.cmu.edu/afs/cs/user/lindaq/mosaic/thesis.ps.Z
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/lindaq/mosaic/my-home-page.html
Root-URL: 
Title: Error-Responsive Feedback Mechanisms for Speech Recognizers  
Author: Lin Lawrance Chase 
Date: April 11, 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> F. Alleva, X. Huang, and M. Hwang. </author> <title> An Improved Search Algorithm for Continuous Speech Recognition. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: A detailed description of the averaging mechanism used for this purpose can be found in <ref> [1] </ref>. In general the system performs better in terms of WER when it is permitted to choose from among this greatly expanded set of word segmentations, even though the acoustic scores used were not computed directly from the SCHMMs. <p> Thus the language model scores used to generate the results of the first two passes can be described as "trigram scores computed on a bigram topology" of word links. The third pass of the Sphinx-II system (also described in <ref> [1] </ref>), is an A* search that starts at the beginning of the utterance and generates an N-best list of paths, which are reported as word sequences ordered by likelihood under the acoustic and language models in question. <p> A generalization of this model is available which includes two main changes: 1. Categorical response variables are possible. In our case two levels, the classes correct and incorrect, are modeled, with y 2 <ref> [0; 1] </ref>. 2. The variance of the response y is modeled to change with the mean of y. 5 These have been described in detail previously in this chapter. The exact list is lmscore + numphones + nbest + avgWF + delWF + numShort + acNormS + duration. <p> The second set, referred to as "Current Best", includes all predictors reported here. 239 Binary response values, such as our response variable class, can be mod-eled with the assumption that they have a mean 2 <ref> [0; 1] </ref> and a variance (1 ) that changes with the mean. Generalized linear models extend the original linear regression model with two changes: 1. A link function is used to describes how the mean varies with the predictors : g () = fi T x, and 2. <p> If we use the link function called "logit", = 1 + e we are guaranteed that the mean will be in the interval <ref> [0; 1] </ref>, which is appropriate for modeling a probability that varies between two possible outcomes.
Reference: [2] <author> A. O. Asadi. </author> <title> Automatic Detection and Modeling of New Words in a Large-Vocabulary Continuous Speech Recognition System. </title> <type> PhD thesis, </type> <institution> Electrical and Computer Engineering, Northeastern University, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: do not compare favorably with 165 of error labels in classes fcorrect, incorrect/other, incorrect/oovg. 166 of error labels in classes fcorrect, incorrect/other, incorrect/oovg. 167 for the best word-level predictor of error labels in classes incorrect=oov; not incorrect=oov. 168 previous best attempts at the task, such as in Asadi's Ph.D. thesis <ref> [2] </ref>. It should be noted that the task in which we are attempting to identify OOVs is a much harder recognition problem overall than was Asadi's task. Also, in his domain the rate of OOVs was generally larger than the rate of errors due to other reasons. <p> The best OOV annotator achieves only 13.8% reduction in cross-entropy for this class, as opposed to the almost 21% for the main problem. These results in predicting OOVs do not compare favorably with previous best attempts at the task, such as in Asadi's Ph.D. thesis <ref> [2] </ref>. However, the approach reported here is extremely general and does not require any system-specific tuning of search parameters or models, which was the central technique in Asadi's work.
Reference: [3] <author> J. Baker and etc.. </author> <title> Large Vocabulary Recognition of Wall Street Journal Sentences at Dragon Systems. </title> <booktitle> In DARPA Speech and Language Workshop, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: However, a few popular approaches have dominated activity in this area, so it is worth stating that a majority of the word pronunciations used in these experiments were drawn from Dragon System's large vocabulary dictionary <ref> [3] </ref>. Exact details are given in Section 2.3 below. Language model training: As with acoustic training, a standard approach to language modeling has been developed for the purpose of controlled comparisons of system performance. A complete description of this process can be found in [54].
Reference: [4] <author> L. E. Baum. </author> <title> An Inequality and Associated Maximization Technique in Statistical Estimation of Probabilistic Functions of Markov Processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: The semi-continuous modeling relies on 256 codebook entries (or mixture components). (For the NAB experiments reported here no phone-class dependent codebooks were used, although this functionality is supported in Sphinx-II.) Training of the mixture components (or codebooks) is initialized using k-means clustering of the cepstral vectors. The Baum-Welch algorithm <ref> [4] </ref> is used to estimate both output and transition probabilities from this point, which is initially specified with uniform output distributions. For the experiment reported here the acoustic training data set was divided into two parts one for male speakers and the other for female.
Reference: [5] <author> F. Beaufays and M. Weintraub. </author> <title> Neural-network Based Measures of Confidence. </title> <booktitle> LVCSR Hub5 Workshop Presentation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Most predictor variables are clearly not independent of at least some others. 94 5.1 What Level of Abstraction? (- ) Because all previous related work in confidence annotation [12] [6] [51] [20] <ref> [5] </ref> [41] [8] [10] has been done at the word level, and because it is very simple to implement, the baseline metric is defined and tested on a word-by-word basis. All previous related work modeled only one error class, incorrect. <p> This predictor variable is directly related to strong predictors reported by [51], [41], [20], and <ref> [5] </ref>. It captures information about how many distinct possibilities 5 the recognizer was considering over the frames included in the hypothesized word's segmentation. In Figure 6.3 we see the distribution of values of N-best homogeneity for words that were actually in classes correct; incorrect=other; incorrect=oov. <p> In this section I compare the decision tree approach with three other methods, each of which share some but not all of the character of decision trees. This effort was motivated in part by some interesting but incomplete results reported by other researchers: * The group at SRI <ref> [5] </ref> reported results using neural networks, which indicated tentatively that they outperformed decision trees in terms of reduction in cross-entropy of confidence annotations. * Several groups [51] [20] have tried generalized linear regression techniques.
Reference: [6] <author> William Byrne. </author> <type> Personal Communication. unpublished, </type> <year> 1996. </year>
Reference-contexts: Most predictor variables are clearly not independent of at least some others. 94 5.1 What Level of Abstraction? (- ) Because all previous related work in confidence annotation [12] <ref> [6] </ref> [51] [20] [5] [41] [8] [10] has been done at the word level, and because it is very simple to implement, the baseline metric is defined and tested on a word-by-word basis. All previous related work modeled only one error class, incorrect.
Reference: [7] <author> J.M. Chambers and T.J. Hastie. </author> <title> Statistical Models in S. </title> <publisher> Wadsworth and Brooks, </publisher> <address> Pacific Grove, CA, </address> <year> 1992. </year>
Reference-contexts: Trees are grown using a binary recursive partitioning algorithm. The algorithm (explained in detail in <ref> [7] </ref>), tries to partition the training examples it works with into homogeneous groups, which correspond to regions in the space of variable values. <p> Switchboard data. Each learning method is described below, followed by some concluding remarks. Generalized Linear Models [37] [59] <ref> [7] </ref> Generalized linear models are an extension of standard linear regression techniques. <p> the result is not quite as useful as the "reading" of decision trees. 6 In all of the experiments reported here in which a binomial link function is used (including the generalized additive models), the logit link was compared with two other link functions, commonly called "probit" and "complementary log-log" <ref> [7] </ref> [59]. In all cases the probit link function performed identically with the logit function and the complementary log-log function performed slightly worse than logit and probit. Thus only logit-based results are presented. 240 Generalized Additive Models [59] [7] We can extend the generalized linear model to capture nonlinearities by replacing <p> compared with two other link functions, commonly called "probit" and "complementary log-log" <ref> [7] </ref> [59]. In all cases the probit link function performed identically with the logit function and the complementary log-log function performed slightly worse than logit and probit. Thus only logit-based results are presented. 240 Generalized Additive Models [59] [7] We can extend the generalized linear model to capture nonlinearities by replacing the linear predictors with a parameterized set of additive predictors.
Reference: [8] <author> L. Chase and R. Rosenfeld. </author> <title> Error-Responsive Feedback to Our Speech Recognizers. </title> <booktitle> LVCSR Hub5 Workshop Presentation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Most predictor variables are clearly not independent of at least some others. 94 5.1 What Level of Abstraction? (- ) Because all previous related work in confidence annotation [12] [6] [51] [20] [5] [41] <ref> [8] </ref> [10] has been done at the word level, and because it is very simple to implement, the baseline metric is defined and tested on a word-by-word basis. All previous related work modeled only one error class, incorrect.
Reference: [9] <author> J. Cohen. </author> <type> Personal Communication. unpublished, </type> <year> 1996. </year> <month> 282 </month>
Reference-contexts: This has the unfortunate effect of reducing the amount of training data available for training, typically by a factor of two. While various efforts are under way to eliminate this splitting problem <ref> [9] </ref>, this simple approach was still the best available at the time the NAB experiments were run.
Reference: [10] <author> Stephen Cox and Richard Rose. </author> <title> Confidence Measures for the Switch--board Database. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 511-514, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Most predictor variables are clearly not independent of at least some others. 94 5.1 What Level of Abstraction? (- ) Because all previous related work in confidence annotation [12] [6] [51] [20] [5] [41] [8] <ref> [10] </ref> has been done at the word level, and because it is very simple to implement, the baseline metric is defined and tested on a word-by-word basis. All previous related work modeled only one error class, incorrect. <p> But for the purposes of determining the relationship between the value P (Ajw) for a word w and the correctness of w, the use of some P (A) as a normalizing factor for our word acoustic score P (Ajw) can work well. (This approach is similar to that found in <ref> [10] </ref>.) In related experiments involving (among other things) the automatic identification of errors in conversational speech recognition, Young and Ward [65] used a similar approach.
Reference: [11] <author> S.B. Davis and P. Mermelstein. </author> <title> Comparison of Parametric Representations of Monosyllabic Word Recognition in Continuously Spoken Sentences,. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-28(4):357-366, </volume> <month> August </month> <year> 1980. </year>
Reference-contexts: Each sample is in linear format with 16 bits per sample. The sampled waveform is then partitioned into 25.6 msec frames, which are spaced at 10 msec intervals. A pre-emphasis filter is applied, and 12 mel-scale frequency coefficients are computed for each frame <ref> [11] </ref> [56]. Four parallel sets of features are then computed from the MFC values. 1.
Reference: [12] <author> Ellen Eide, Herbert Gish, Philippe Jeanrenaud, and Angela Mielke. </author> <title> Understanding and Improving Speech Recognition performance Through the Use of Diagnostic Tools. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 221-224, </pages> <year> 1995. </year>
Reference-contexts: Most predictor variables are clearly not independent of at least some others. 94 5.1 What Level of Abstraction? (- ) Because all previous related work in confidence annotation <ref> [12] </ref> [6] [51] [20] [5] [41] [8] [10] has been done at the word level, and because it is very simple to implement, the baseline metric is defined and tested on a word-by-word basis. All previous related work modeled only one error class, incorrect. <p> words that were not seen in the acoustic training data somewhat more frequently for decoding incorrect=oov words than for other classes. 2 By default, throughout this chapter "word" refers to a single pronunciation variant of a word. 3 The use of this variable was prompted by its use at BBN <ref> [12] </ref> in a different but related context. When diagnosing errors using known transcripts of spontaneous telephone conversations, it was found that in general words seen in acoustic training were more reliably decoded than unseen words. Here we're not using this predictor for diagnosis, but prediction. <p> This is especially true for comparisons of hypotheses against the best basephone sequence, acoustic score normalizations, and various measures of confusion in the N-best list. Our analysis also shows that predictors formerly believed to be major contributors to error <ref> [12] </ref> are not as important as previously thought. In particular, the number of phones in a word, the number of minimally short phones in a word (3 frames), and the number of times a triphone was seen in the training data are not good predictors of error.
Reference: [13] <author> M. Finke, P. Geutner, H. Hild, T. Kemp, K. Ries, and M. Westphal. </author> <title> The Karlsruhe-Verbmobil Speech Recognition Engine. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1997. </year>
Reference-contexts: In Janus all of these features are treated in a single stream. Cepstral mean subtraction is used, as is the vocal tract length normalization scheme described in <ref> [13] </ref>.
Reference: [14] <author> M. Finke and T. Zeppenfeld. </author> <title> LVCSR Switchboard April 1996 Evaluation Report. </title> <booktitle> In The LVCSR Hub 5 Workshop, </booktitle> <month> April 29 May 1 </month> <year> 1996. </year>
Reference-contexts: In this chapter I will give details on both the forced alignment and the decoding strategies employed by Sphinx-II [50] [27] and Janus [66] <ref> [14] </ref>, as well as some additional side computations that are useful in building error feedback mechanisms.
Reference: [15] <author> Michael Finke. </author> <type> Personal Communication. unpublished, </type> <year> 1997. </year>
Reference-contexts: Finding the optimal LW=IP pair for a speaker: Initial analysis indicates that simply trying to optimize the LW=IP pair for a single speaker within the Switchboard task could improve base word error rate substantially <ref> [15] </ref>. This could potentially be accomplished via repeated rescoring of lattices under a variety of LW=IP pairs and selecting the values that yield the best confidence rating over a complete conversation side.
Reference: [16] <author> P. Finke, M. Geutner, H. Hild, T. Kemp, K. Ries, and M. Westphal. </author> <title> The Karlsruhe-Verbmobil Speech Recognition Engine. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1997. </year>
Reference-contexts: Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] [19] [36] [39] [40] [42] [43] [44] [48] [64] [63] [62] <ref> [16] </ref>. All of the experimental techniques described in this thesis have been designed to be portable to all such systems. During the development of these techniques tests and experiments were performed on a total of four systems: Sphinx-II, Sphinx-III [50], Janus, and the commercial version of HTK 1 .
Reference: [17] <author> Eric Fosler. </author> <type> Personal Communication. unpublished, </type> <year> 1996. </year>
Reference-contexts: Distance in W/C Phone Space In [61], Withgott and Chen describe a 24-element vector of phoneme attributes that create a space in which our basephones can be distinguished from each other <ref> [17] </ref>. The attributes in this space include features that help separate vowels from consonants, front vowels from back vowels, and liquids and glides from fricatives, etc.
Reference: [18] <author> J.L. Gauvain, L. Lamel, G. Adda, and M. Adda-Decker. </author> <title> The LIMSI Nov93 WSJ System. </title> <booktitle> In ARPA Speech and Natural Language Workshop, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: In this chapter we describe the design and functionality of the major components of these systems, breaking out into parallel discussions where major differences occur. Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today <ref> [18] </ref> [50] [19] [36] [39] [40] [42] [43] [44] [48] [64] [63] [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems.
Reference: [19] <author> J.L. Gauvain, L. Lamel, and M. Adda-Decker. </author> <title> Developments in Large Vocabulary Dictation: The LIMSI Nov94 NAB System. </title> <booktitle> In ARPA Spoken Language Systems Technology Workshop, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: In this chapter we describe the design and functionality of the major components of these systems, breaking out into parallel discussions where major differences occur. Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] <ref> [19] </ref> [36] [39] [40] [42] [43] [44] [48] [64] [63] [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems.
Reference: [20] <author> L. Gillick and Y. Ito. </author> <title> Confidence Estimation and Evaluation. </title> <booktitle> LVCSR Hub5 Workshop Presentation, </booktitle> <month> October </month> <year> 1996. </year> <month> 283 </month>
Reference-contexts: Most predictor variables are clearly not independent of at least some others. 94 5.1 What Level of Abstraction? (- ) Because all previous related work in confidence annotation [12] [6] [51] <ref> [20] </ref> [5] [41] [8] [10] has been done at the word level, and because it is very simple to implement, the baseline metric is defined and tested on a word-by-word basis. All previous related work modeled only one error class, incorrect. <p> This predictor variable is directly related to strong predictors reported by [51], [41], <ref> [20] </ref>, and [5]. It captures information about how many distinct possibilities 5 the recognizer was considering over the frames included in the hypothesized word's segmentation. In Figure 6.3 we see the distribution of values of N-best homogeneity for words that were actually in classes correct; incorrect=other; incorrect=oov. <p> This effort was motivated in part by some interesting but incomplete results reported by other researchers: * The group at SRI [5] reported results using neural networks, which indicated tentatively that they outperformed decision trees in terms of reduction in cross-entropy of confidence annotations. * Several groups [51] <ref> [20] </ref> have tried generalized linear regression techniques. This learning mechanism seemed to produce similar-quality results to the decision trees in terms of reduction in cross-entropy. 234 SWB. BASEline system. 235 SWB.
Reference: [21] <author> A. Hauptmann and H. Wactlar. </author> <title> Indexing and Search of Multimodal Information. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: When producing speech-to-text transcriptions, having a confidence mea sure reduces the amount of hand error checking necessary to produce a final text. 2. When automatically processing or classifying recorded data, as in multimedia storage and retrieval applications <ref> [21] </ref>, having a measure with which to weight likely "hits" based on recognizer performance reduces error. 3. In human/computer dialogue and automatic translation systems re gions of low confidence can be made the focus of clarification actions. 4. <p> Applications that rely on speech recognizer output usually integrate this output directly, without benefit of additional information such as confidence annotations. For example, the word sequences output by a speech recognizer are commonly used directly to tag the voice components of multi-media databases for retrieval <ref> [21] </ref>. In another example, B. Suhm [58] [57] has found in his work on dictation error correction that the unannotated use of recognizer output places the burden of detecting errors in the recognizer output squarely on the shoulders of the user of the dictation system. <p> Poor quality transcripts or time-marked tags on the speech elements of a database can impede retrieval capabilities substantially <ref> [21] </ref>. The use of confidence measures can increase the reliability of the tags applied to such database segments, at the cost of applying fewer labels per segment. Table 6.6 shows this tradeoff between quality of tags and number of tags, demonstrated on the NAB data described earlier. <p> Incorporating these differences into our models may improve confidence annotation performance, especially in topic-driven ap plications. * Application of confidence annotators: Tagging multi-media databases: In collaboration with a multi-media database project, we are currently investigating the application of confidence annotation to improving document retrieval. <ref> [21] </ref> [22]. Finding the optimal LW=IP pair for a speaker: Initial analysis indicates that simply trying to optimize the LW=IP pair for a single speaker within the Switchboard task could improve base word error rate substantially [15].
Reference: [22] <author> A.G. Hauptmann, M.J. Witbrock, A.I. Rudnicky, and S. Reed. </author> <title> Speech for Multimedia Information Retrieval. </title> <booktitle> In The User Interface Software Technology Conference, </booktitle> <month> November </month> <year> 1995 1997. </year>
Reference-contexts: Incorporating these differences into our models may improve confidence annotation performance, especially in topic-driven ap plications. * Application of confidence annotators: Tagging multi-media databases: In collaboration with a multi-media database project, we are currently investigating the application of confidence annotation to improving document retrieval. [21] <ref> [22] </ref>. Finding the optimal LW=IP pair for a speaker: Initial analysis indicates that simply trying to optimize the LW=IP pair for a single speaker within the Switchboard task could improve base word error rate substantially [15].
Reference: [23] <author> X.D. Huang, F. Alleva, H.W. Hon, M.Y. Hwang, K.F. Lee, and R. Rosenfeld. </author> <title> The SPHINX-II Speech Recognition System: An Overview. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 2 </volume> <pages> 137-148, </pages> <year> 1993. </year>
Reference-contexts: above. 36 where IP is a constant "insertion penalty" which is applied to each word in the final path sequence of length n. 4 The log base used varies according to the recognizer's implementation. 2.5.1 Sphinx-II's Three Pass Search The Sphinx-II system used in this thesis work, described thoroughly in <ref> [23] </ref>, [24], and [26], is a three-pass system. The first two passes use Semi-Continuous Hidden Markov Models (SCHMMs) based on 10,000 senones and a trigram language model to produce a very large set of possible word segmentations.
Reference: [24] <author> X.D. Huang, F. Alleva, M.Y. Hwang, and R. Rosenfeld. </author> <title> An Overview of the SPHINX-II Speech Recognition System. </title> <booktitle> In ARPA Human Language Technology Workshop, </booktitle> <month> March </month> <year> 1993. </year>
Reference-contexts: 36 where IP is a constant "insertion penalty" which is applied to each word in the final path sequence of length n. 4 The log base used varies according to the recognizer's implementation. 2.5.1 Sphinx-II's Three Pass Search The Sphinx-II system used in this thesis work, described thoroughly in [23], <ref> [24] </ref>, and [26], is a three-pass system. The first two passes use Semi-Continuous Hidden Markov Models (SCHMMs) based on 10,000 senones and a trigram language model to produce a very large set of possible word segmentations.
Reference: [25] <author> Don R. Hush and Bill G. </author> <title> Horne. Progress in Supervised Neural Networks. </title> <journal> IEEE Signal Processing Magazine, </journal> <pages> pages 8 - 39, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Interactions between predictor variables must be modeled explicitly before regression is performed. "Reading" generalized additive models is possible (by examining learned regression coefficients, etc.), but not quite as straightforward as looking at decision trees. Neural Nets For the neural net model, I used a standard multi-layer perceptron <ref> [25] </ref> with one input node per predictor variable, twice as many hidden nodes as predictors, and one output node. Sigmoid activation functions were used at both hidden and output nodes. Inputs were not normalized in any way.
Reference: [26] <author> M. Hwang, R. Rosenfeld, E. Thayer, R. Mosur, L. Chase, R. Weide, X. Huang, and F. Alleva. </author> <title> Improving Speech Recognition Performance via Phone-Dependent VQ Codebooks and Adaptive Language Models in Sphinx-II. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: IP is a constant "insertion penalty" which is applied to each word in the final path sequence of length n. 4 The log base used varies according to the recognizer's implementation. 2.5.1 Sphinx-II's Three Pass Search The Sphinx-II system used in this thesis work, described thoroughly in [23], [24], and <ref> [26] </ref>, is a three-pass system. The first two passes use Semi-Continuous Hidden Markov Models (SCHMMs) based on 10,000 senones and a trigram language model to produce a very large set of possible word segmentations.
Reference: [27] <author> Mei-Yuh Hwang. </author> <title> Subphonetic Acoustic Modeling for Speaker-Independent Continuous Speech Recognition. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: In this chapter I will give details on both the forced alignment and the decoding strategies employed by Sphinx-II [50] <ref> [27] </ref> and Janus [66] [14], as well as some additional side computations that are useful in building error feedback mechanisms.
Reference: [28] <author> M.Y. Hwang and X.D. Huang. </author> <title> Shared-Distribution Hidden Markov Models for Speech Recognition. </title> <type> Technical report cmu-cs-91-124, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: The set of triphones modeled is made up of all of the within-word and crossword triphones possible given the dictionary in question. For the NAB experiments reported here the total number of triphones modeled is 84,580. Each triphone is modeled with a set of five senones <ref> [28] </ref>. Triphones are clustered to enable sharing of distributions (and thus training data) at the senone level. The clustering is done in two phases. The first is a top-down splitting algorithm that creates a decision tree using linguistically motivated questions to create splits in conjunction with an entropy measure.
Reference: [29] <author> P. Jeanrenaud, M. Siu, and H. Gish. </author> <title> Large Vocabulary Word Scoring as a Basis for Transcription Generation. </title> <booktitle> In Proceedings of Eurospeech, </booktitle> <pages> pages 2149-2152, </pages> <year> 1995. </year>
Reference-contexts: The ratio of matches found to the total number of entries in the N-best list is calculated, with each found instance weighted by the total path score of the N-best list entry in which is was found. 4 The implementation of this predictor variable follows that reported by BBN in <ref> [29] </ref>. This predictor variable is directly related to strong predictors reported by [51], [41], [20], and [5]. It captures information about how many distinct possibilities 5 the recognizer was considering over the frames included in the hypothesized word's segmentation.
Reference: [30] <author> F. Jelinek. </author> <title> Self-Organized Language Modeling for Speech Recognition. </title> <editor> In A. Waibel and K-F. Lee, editors, </editor> <booktitle> Readings in Speech Recognition. </booktitle> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year> <month> 284 </month>
Reference-contexts: See <ref> [30] </ref> for a good example of the correct use of the distinct types of data. This paper originates from the IBM speech research group, which is generally credited with convincing the rest of the community to adopt these standards in the use of data. 53 1. <p> This splitting algorithm produces vastly overtrained trees which need to be pruned back and smoothed. This is accomplished under a ten-way "jacknifing" <ref> [30] </ref> cross-validation (rotating through 9 parts train,1 part test) 103 the BASE2:Combined+Duration baseline confidence annotator. The lower x-axis is tree size counted by number of nodes. The upper x-axis is value of k, the pruning parameter. The y-axis is the average tree deviance under a 10-way cross-validation. <p> sections we examine several distance measures between base-phones and their relative performance in predicting word class membership when applied to our two "best basephone guessing" methods. 131 6.5.1 Best Senone Score Normalization (-) The first acoustic normalization scheme is suggested by a portion of the fundamental equation of speech recognition <ref> [30] </ref>: P (wjA) = P (A) In practice we typically ignore the term P (A) in the denominator, since for all of the difference possible decodings for a particular utterance this value is constant.
Reference: [31] <author> S.M. Katz. </author> <title> Estimation of Probabilities from Sparse Data for the Lan--guage Model Component of a Speech Recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-35:400-401, </volume> <month> March </month> <year> 1987. </year>
Reference-contexts: Different smoothing and pruning approaches, are used, however. Statistics on both are presented next. 2.4.1 The NAB Language Model The smoothing algorithm for the construction of the NAB trigram language model is widely referred to as the "Katz" approach, and is described in <ref> [31] </ref>. The NAB language model was constructed using the toolkit described in [54], which is a widely adopted standard package for building Katz N-gram language models. The run-time calculation of language model probabilities from a Katz trigram language model can be expressed as one of five cases.
Reference: [32] <author> Reinhard Kneser and Hermann Ney. </author> <title> Improved backing-off for m-gram language modeling. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Detroit, Michigan, 1995. </address> <publisher> IEEE. </publisher>
Reference-contexts: The smoothing algorithm used in the Janus system for language models is the Kneser-Ney backoff scheme <ref> [32] </ref>. The language model used contained a total of 14,150 unigrams, 401,402 bigrams, and 1,313,567 trigrams. No cutoffs were used in its construction. 2.5 Search Both Sphinx-II and Janus rely fundamentally on left-to-right time-synchronized Viterbi decoding embedded in a beam search as their main method of search control.
Reference: [33] <author> F. Kubala. </author> <title> Fill Me In Description of NAB DATA. </title> <booktitle> In DARPA Speech and Language Workshop, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: Specific attention is paid to issues that might relate to extending these techniques to other tasks and types of data and recognizers. The first section describes the portions of the widely used North American Business News corpus [49] [46] [45] [47] <ref> [33] </ref> that were used for acoustic training, error feedback training, and independent test materials. This section describes the subset of the data used, and includes baseline statistics for the performance of the tools described in the previous chapter. <p> It is described fully in corpus reports generated during those years [49] <ref> [33] </ref>. As described in Section 2.6.2, the corpus consists mainly of native speakers of North American English reading stories from the Wall Street Journal one sentence at a time. The database was collected in support of a wide variety of experiments done at many sites.
Reference: [34] <author> K.F. Lee and S. Mahajan. </author> <title> Corrective and Reinforcement Learning for Speaker-Independent Continuous Speech Recognition. </title> <type> Technical Report CMU-CS-89-100, </type> <institution> Carnegie Mellon University, </institution> <month> January </month> <year> 1989. </year>
Reference-contexts: In the case of confusions between acoustic models, however, corrective training of the models is probably in order, as in <ref> [34] </ref>.
Reference: [35] <author> C. Leggetter and P. Woodland. </author> <title> Speaker Adaptation of HMMs Using Linear Regression. </title> <type> Technical Report TR 181, </type> <institution> Cambridge University, </institution> <address> Cambridge, UK, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: See Section 6.9 for details on how this is accomplished. In the Switchboard/Janus experiments, MLLR acoustic adaptation can be used <ref> [35] </ref>. This makes it possible to avoid the use of (and thus avoid the negative data-splitting effects of) gender-specific models. The Switchboard experiments reported here were conducted using gender-independent acoustic models. 3.4 Summary This chapter described the training data and test sets used in the experiments reported in later chapters. <p> Switchboard data. Using Confidence Annotators to Improve Acoustic Adaptation Another use of confidence annotators is in the improvement of unsupervised acoustic adaptation. Like many state-of-the-art speech recognizers, the Janus system is configured to optionally use MLLR adaptation for its acoustic models <ref> [35] </ref>. This adaptation approach requires a transcript against which the acoustic models are aligned before a linear regression is run to effect the adaptation. For supervised adaptation the transcript can simply be the reference transcript, but for unsupervised adaptation a recognition hypothesis is required.
Reference: [36] <author> A. Ljolje, M. Riley, D. Hindle, and F. Pereira. </author> <title> The ATT 60,000 Word Speech-to-Text System. </title> <booktitle> In ARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pages 162-165, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: In this chapter we describe the design and functionality of the major components of these systems, breaking out into parallel discussions where major differences occur. Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] [19] <ref> [36] </ref> [39] [40] [42] [43] [44] [48] [64] [63] [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems.
Reference: [37] <author> P. McCullagh and J.A. Nelder. </author> <title> Generalized Linear Models. </title> <publisher> Chapman and Hall, 2-6 Boundary Row, </publisher> <address> London SE1 8HN, UK, </address> <year> 1989. </year>
Reference-contexts: Switchboard data. Each learning method is described below, followed by some concluding remarks. Generalized Linear Models <ref> [37] </ref> [59] [7] Generalized linear models are an extension of standard linear regression techniques.
Reference: [38] <author> Ravishankar Mosur and Maxine Eskanzi. </author> <type> Personal communication, </type> <month> February </month> <year> 1997. </year>
Reference-contexts: model approaches, we may be able to focus the more sophisticated language models on regions which are highly error-prone, thereby reducing the problem of fixing things that aren't broken. * Using the blame assignment algorithm: Detecting dictionary problems: Similar work involving the use of phone-only decodings and reference forced alignments <ref> [38] </ref> to detect dictionary pronunciation problems is under way. Initial results indicate that the slightly more sophisticated approach to detecting transcript/dictionary problems in the blame assignment algorithm is reliable.
Reference: [39] <author> H. Murveit, J. Butzberger, V. Digalakis, and M. Weintraub. </author> <title> Large-Vocabulary Dictation Using SRI's Decipher Speech Recognition System: Progressive Search Techniques. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 319-322, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] [19] [36] <ref> [39] </ref> [40] [42] [43] [44] [48] [64] [63] [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems.
Reference: [40] <author> H. Murveit, P. Monaco, V. Digalakis, and J. Butzberger. </author> <title> Techniques to Achieve an Accurate Real-Time Large-Vocabulary Speech Recognition System. </title> <booktitle> In ARPA Human Language Technology Workshop, </booktitle> <pages> pages 368-373, </pages> <month> March </month> <year> 1994. </year> <month> 285 </month>
Reference-contexts: Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] [19] [36] [39] <ref> [40] </ref> [42] [43] [44] [48] [64] [63] [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems.
Reference: [41] <author> C. Neti, S. Roukos, and E. Eide. </author> <title> Confidence Measures Based Search Strategy for Continuous Speech Recognition. </title> <booktitle> LVCSR Hub5 Workshop Presentation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: That is, what is needed in these cases is not necessarily a change in the language model score produced, but rather an adjustment to the relative contribution of the acoustic and language models, possibly through dynamic adjustment of the language weight LW , as in <ref> [41] </ref>. For more details on this type of behavior, see Chapter 9. 84 segmentations and scores included. 85 model. <p> Confidence annotation can be used to dynamically adjust the local contribution of acoustic and language models <ref> [41] </ref>. 6. Unsupervised acoustic adaptation can be improved by using a confi dence measure to filter adaptation data. (See Section 8.3.3.) 7. <p> Most predictor variables are clearly not independent of at least some others. 94 5.1 What Level of Abstraction? (- ) Because all previous related work in confidence annotation [12] [6] [51] [20] [5] <ref> [41] </ref> [8] [10] has been done at the word level, and because it is very simple to implement, the baseline metric is defined and tested on a word-by-word basis. All previous related work modeled only one error class, incorrect. <p> This predictor variable is directly related to strong predictors reported by [51], <ref> [41] </ref>, [20], and [5]. It captures information about how many distinct possibilities 5 the recognizer was considering over the frames included in the hypothesized word's segmentation. In Figure 6.3 we see the distribution of values of N-best homogeneity for words that were actually in classes correct; incorrect=other; incorrect=oov. <p> This could potentially be accomplished via repeated rescoring of lattices under a variety of LW=IP pairs and selecting the values that yield the best confidence rating over a complete conversation side. Dynamic adaptation of LW : Initial results from IBM <ref> [41] </ref> using a rudimentary confidence annotator indicate that dynamic adaptation of the contributions of the acoustic and language models may be possible. 276 Focusing language models on error-prone regions: Syntax- and semantic--based language models have not yet been able to outperform the simple trigram language model in speech recognizers.
Reference: [42] <author> L. Nguyen, T. Anastasakos, F. Kubala, C. LaPre, J. Makhoul, R. Schwartz, N. Yuan, G. Zavaliagkos, and Y. Zhao. </author> <title> The 1994 BBN/BYBLOS Speech Recognition System. </title> <booktitle> In ARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pages 77-81, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] [19] [36] [39] [40] <ref> [42] </ref> [43] [44] [48] [64] [63] [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems.
Reference: [43] <author> Y. Normandin, D. Bowness, R. Cardin, C. Drouin, R. Lacouture, and A. Lazarides. </author> <title> CRIM's November94 Continous Speech Recognition System. </title> <booktitle> In ARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 153-155, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] [19] [36] [39] [40] [42] <ref> [43] </ref> [44] [48] [64] [63] [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems.
Reference: [44] <author> J. Odell, V. Valtchev, P. Woodland, and S. Young. </author> <title> One Pass Decoder Design for Large Vocabulary Recognition. </title> <booktitle> In ARPA Human Language Technology Workshop, </booktitle> <year> 1994. </year>
Reference-contexts: Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] [19] [36] [39] [40] [42] [43] <ref> [44] </ref> [48] [64] [63] [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems.
Reference: [45] <author> D. Pallett, J. Fiscus, W. Fisher, J. Garofolo, B. Lund, A. Martin, and M. Przybocki. </author> <title> 1994 Benchmark Tests for the ARPA Spoken Language Program. </title> <booktitle> In ARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pages 5-38, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Specific attention is paid to issues that might relate to extending these techniques to other tasks and types of data and recognizers. The first section describes the portions of the widely used North American Business News corpus [49] [46] <ref> [45] </ref> [47] [33] that were used for acoustic training, error feedback training, and independent test materials. This section describes the subset of the data used, and includes baseline statistics for the performance of the tools described in the previous chapter.
Reference: [46] <author> D. Pallett, J. Fiscus, W. Fisher, J. Garofolo, B. Lund, and M. Przybocki. </author> <title> 1993 Benchmark Tests for the ARPA Spoken Language Program. </title> <booktitle> In ARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 15-40, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Specific attention is paid to issues that might relate to extending these techniques to other tasks and types of data and recognizers. The first section describes the portions of the widely used North American Business News corpus [49] <ref> [46] </ref> [45] [47] [33] that were used for acoustic training, error feedback training, and independent test materials. This section describes the subset of the data used, and includes baseline statistics for the performance of the tools described in the previous chapter.
Reference: [47] <author> D. Pallett, J. Fiscus, W. Fisher, J. Garofolo, A. Martin, and M. Przy-bocki. </author> <title> 1995 Hub-3 NIST Multiple Microphone Corpus Benchmark Tests. </title> <booktitle> In ARPA Speech Recognition Workshop, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Specific attention is paid to issues that might relate to extending these techniques to other tasks and types of data and recognizers. The first section describes the portions of the widely used North American Business News corpus [49] [46] [45] <ref> [47] </ref> [33] that were used for acoustic training, error feedback training, and independent test materials. This section describes the subset of the data used, and includes baseline statistics for the performance of the tools described in the previous chapter.
Reference: [48] <author> D. Paul. </author> <title> An Efficient A* Stack Decoder Algorithm for Continuous Speech Recognition with a Stochastic Language Model. </title> <type> Technical Report 930, </type> <institution> Lincoln Laboratory, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] [19] [36] [39] [40] [42] [43] [44] <ref> [48] </ref> [64] [63] [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems.
Reference: [49] <author> D.B. Paul and J.M. Baker. </author> <title> The Design for the Wall Street Journal-based CSR Corpus. </title> <booktitle> In DARPA Speech and Language Workshop, </booktitle> <address> San Mateo, CA, Feb 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher> <pages> 286 </pages>
Reference-contexts: Specific attention is paid to issues that might relate to extending these techniques to other tasks and types of data and recognizers. The first section describes the portions of the widely used North American Business News corpus <ref> [49] </ref> [46] [45] [47] [33] that were used for acoustic training, error feedback training, and independent test materials. This section describes the subset of the data used, and includes baseline statistics for the performance of the tools described in the previous chapter. <p> It is described fully in corpus reports generated during those years <ref> [49] </ref> [33]. As described in Section 2.6.2, the corpus consists mainly of native speakers of North American English reading stories from the Wall Street Journal one sentence at a time. The database was collected in support of a wide variety of experiments done at many sites. <p> There is no overlap between the content of these acoustic training utterances and the development data (see below) used for other training purposes in the system. See <ref> [49] </ref> for complete details. Section 3.1.1 below covers the exact use of this data and further characteristics of this data as they pertain to this work.
Reference: [50] <author> Mosur K. Ravishankar. </author> <title> Efficient Algorithms for Speech Recognition. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: In this chapter we describe the design and functionality of the major components of these systems, breaking out into parallel discussions where major differences occur. Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] <ref> [50] </ref> [19] [36] [39] [40] [42] [43] [44] [48] [64] [63] [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems. <p> All of the experimental techniques described in this thesis have been designed to be portable to all such systems. During the development of these techniques tests and experiments were performed on a total of four systems: Sphinx-II, Sphinx-III <ref> [50] </ref>, Janus, and the commercial version of HTK 1 . The two systems finally reported here were the most straightforward to use for engineering and other logistic reasons. The availability and stability of source code for the recognizers is currently of critical importance for this type of work. <p> In this chapter I will give details on both the forced alignment and the decoding strategies employed by Sphinx-II <ref> [50] </ref> [27] and Janus [66] [14], as well as some additional side computations that are useful in building error feedback mechanisms. <p> In total four recognition systems were used: Sphinx-II, Sphinx-III <ref> [50] </ref>, Janus, and the commercial version of HTK. Not all of these experiments are reported in this document.
Reference: [51] <author> F. Richardson, M. Siu, and H. Gish. </author> <title> Confidence Scoring for the 1996 Spanish CallHome Evaluation. </title> <booktitle> LVCSR Hub5 Workshop Presentation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Most predictor variables are clearly not independent of at least some others. 94 5.1 What Level of Abstraction? (- ) Because all previous related work in confidence annotation [12] [6] <ref> [51] </ref> [20] [5] [41] [8] [10] has been done at the word level, and because it is very simple to implement, the baseline metric is defined and tested on a word-by-word basis. All previous related work modeled only one error class, incorrect. <p> This predictor variable is directly related to strong predictors reported by <ref> [51] </ref>, [41], [20], and [5]. It captures information about how many distinct possibilities 5 the recognizer was considering over the frames included in the hypothesized word's segmentation. In Figure 6.3 we see the distribution of values of N-best homogeneity for words that were actually in classes correct; incorrect=other; incorrect=oov. <p> This effort was motivated in part by some interesting but incomplete results reported by other researchers: * The group at SRI [5] reported results using neural networks, which indicated tentatively that they outperformed decision trees in terms of reduction in cross-entropy of confidence annotations. * Several groups <ref> [51] </ref> [20] have tried generalized linear regression techniques. This learning mechanism seemed to produce similar-quality results to the decision trees in terms of reduction in cross-entropy. 234 SWB. BASEline system. 235 SWB.
Reference: [52] <author> Ze'ev Rivlin, Michael Cohen, Victor Abrash, and Thomas Chung. </author> <title> A Phone-dependent Confidence Measure for Utterance Rejection. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 515-517, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Seeking these special response relationships and incorporating them into our predictors may improve confidence annotation performance. Some promising results along these lines are reported in <ref> [52] </ref>. Topic-dependent words: Work in topic-dependent language modeling [55] indicates that error distributions among words considered to be topic-dependent are different than among those considered to be topic-independent.
Reference: [53] <author> Ronald Rosenfeld. </author> <title> Adaptive Statistical Language Modeling: A Maximum Entropy Approach. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1994. </year> <type> Ph.D. </type> <note> thesis also published as Technical Report CMU-CS-94-138. </note>
Reference-contexts: In contrast to short-term models such as the bigram and trigram, however, they can cause effects that are much harder to model. A trigger or cache language model <ref> [53] </ref>, for instance, might by extension cause entire sequences of utterances to be considered as an error region, even though very few segments would have been considered as errorful under the word error rate (WER) metric. <p> Cross-entropy is a measure that compares two distributions. In our case, the cross-entropy is measured between some set of confidence annotations and the actual class labels of the stream. (For a detailed discussion of cross-entropy and the related concept of perplexity, see <ref> [53] </ref>.) The cross-entropy of the non-annotated output of the speech recognizer can also be called the "self-entropy" of the recognizer output stream.
Reference: [54] <author> Ronald Rosenfeld. </author> <title> The CMU Statistical Language Modeling Toolkit, and its use in the 1994 ARPA CSR Evaluation. </title> <booktitle> In Proc. ARPA Spoken Language Technology Workshop, </booktitle> <month> Jan </month> <year> 1995. </year>
Reference-contexts: Statistics on both are presented next. 2.4.1 The NAB Language Model The smoothing algorithm for the construction of the NAB trigram language model is widely referred to as the "Katz" approach, and is described in [31]. The NAB language model was constructed using the toolkit described in <ref> [54] </ref>, which is a widely adopted standard package for building Katz N-gram language models. The run-time calculation of language model probabilities from a Katz trigram language model can be expressed as one of five cases. <p> Exact details are given in Section 2.3 below. Language model training: As with acoustic training, a standard approach to language modeling has been developed for the purpose of controlled comparisons of system performance. A complete description of this process can be found in <ref> [54] </ref>. In 1994 [54] a standard trigram language model was constructed for shared use. It was derived from a standard body of language model training data and was based on a fixed 20,000 word vocabulary that was also derived from the setaside language model training data. <p> Exact details are given in Section 2.3 below. Language model training: As with acoustic training, a standard approach to language modeling has been developed for the purpose of controlled comparisons of system performance. A complete description of this process can be found in <ref> [54] </ref>. In 1994 [54] a standard trigram language model was constructed for shared use. It was derived from a standard body of language model training data and was based on a fixed 20,000 word vocabulary that was also derived from the setaside language model training data.
Reference: [55] <author> Kristie Seymore and Roni Rosenfeld. </author> <type> Personal Communication. unpublished, </type> <year> 1997. </year>
Reference-contexts: Seeking these special response relationships and incorporating them into our predictors may improve confidence annotation performance. Some promising results along these lines are reported in [52]. Topic-dependent words: Work in topic-dependent language modeling <ref> [55] </ref> indicates that error distributions among words considered to be topic-dependent are different than among those considered to be topic-independent.
Reference: [56] <author> S. S. Stevens and J. Volkmann. </author> <title> The Relation of Pitch to Frequency:a Revised Scale. </title> <journal> The American Journal of Psychology, </journal> <volume> 53 </volume> <pages> 329-353, </pages> <year> 1940. </year>
Reference-contexts: Each sample is in linear format with 16 bits per sample. The sampled waveform is then partitioned into 25.6 msec frames, which are spaced at 10 msec intervals. A pre-emphasis filter is applied, and 12 mel-scale frequency coefficients are computed for each frame [11] <ref> [56] </ref>. Four parallel sets of features are then computed from the MFC values. 1.
Reference: [57] <author> B. Suhm, B. Myers, and A. Waibel. </author> <title> Interactive Recovery from Speech Recognition Errors in Speech User Interfaces. </title> <booktitle> In The International Conference on Spoken Language Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 861-864, </pages> <month> Oc-tober </month> <year> 1996. </year>
Reference-contexts: For example, the word sequences output by a speech recognizer are commonly used directly to tag the voice components of multi-media databases for retrieval [21]. In another example, B. Suhm [58] <ref> [57] </ref> has found in his work on dictation error correction that the unannotated use of recognizer output places the burden of detecting errors in the recognizer output squarely on the shoulders of the user of the dictation system.
Reference: [58] <author> Bernhard Suhm. </author> <type> Personal communication, </type> <month> January </month> <year> 1997. </year>
Reference-contexts: For example, the word sequences output by a speech recognizer are commonly used directly to tag the voice components of multi-media databases for retrieval [21]. In another example, B. Suhm <ref> [58] </ref> [57] has found in his work on dictation error correction that the unannotated use of recognizer output places the burden of detecting errors in the recognizer output squarely on the shoulders of the user of the dictation system.
Reference: [59] <author> W.N. Venables and B.D. Ripley. </author> <title> Modern Applied Statistiscs with S-Plus. </title> <publisher> Springer-Verlag, </publisher> <address> New York, USA, </address> <year> 1994. </year>
Reference-contexts: Switchboard data. Each learning method is described below, followed by some concluding remarks. Generalized Linear Models [37] <ref> [59] </ref> [7] Generalized linear models are an extension of standard linear regression techniques. <p> result is not quite as useful as the "reading" of decision trees. 6 In all of the experiments reported here in which a binomial link function is used (including the generalized additive models), the logit link was compared with two other link functions, commonly called "probit" and "complementary log-log" [7] <ref> [59] </ref>. In all cases the probit link function performed identically with the logit function and the complementary log-log function performed slightly worse than logit and probit. Thus only logit-based results are presented. 240 Generalized Additive Models [59] [7] We can extend the generalized linear model to capture nonlinearities by replacing the <p> was compared with two other link functions, commonly called "probit" and "complementary log-log" [7] <ref> [59] </ref>. In all cases the probit link function performed identically with the logit function and the complementary log-log function performed slightly worse than logit and probit. Thus only logit-based results are presented. 240 Generalized Additive Models [59] [7] We can extend the generalized linear model to capture nonlinearities by replacing the linear predictors with a parameterized set of additive predictors. <p> Sigmoid activation functions were used at both hidden and output nodes. Inputs were not normalized in any way. The standard back-propagation algorithm was used, but the optimization function was entropy-based, not the standard least-squares method <ref> [59] </ref>. This approach allows each data point to influence the final model directly, captures nonlinearities directly, and models interactions directly. It is not usually possible to "read" a neural net after it has been trained, however.
Reference: [60] <author> Wayne Ward. </author> <type> Personal Communication. unpublished, </type> <year> 1997. </year>
Reference-contexts: Identifying training examples for corrective acoustic training: Similar work involving the use of the best path in a search lattice as a reference point to identify acoustic errors that should be corrected for decreased confusability is in progress <ref> [60] </ref>.
Reference: [61] <author> Withgott and Chen. </author> <title> Computational Models of American Speech. </title> <booktitle> 1995. </booktitle> <pages> 287 </pages>
Reference-contexts: The first is based solely on a phonological model of the phonemes represented by the basephone set. The other is an empirical model of basephone confusability that was developed from direct observation of the decoding systems used. Distance in W/C Phone Space In <ref> [61] </ref>, Withgott and Chen describe a 24-element vector of phoneme attributes that create a space in which our basephones can be distinguished from each other [17].
Reference: [62] <author> P. Woodland, M. Gales, D. Pye, and V. Valtchev. </author> <title> The HTK Large Vocabulary Recognition System for the 1995 ARPA H3 Task. </title> <booktitle> In ARPA Speech Recognition Workshop, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] [19] [36] [39] [40] [42] [43] [44] [48] [64] [63] <ref> [62] </ref> [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems. During the development of these techniques tests and experiments were performed on a total of four systems: Sphinx-II, Sphinx-III [50], Janus, and the commercial version of HTK 1 .
Reference: [63] <author> P. Woodland, C. Leggetter, J. Odell, V. Valtchev, and S. Young. </author> <title> The Development of the 1994 HTK Large Vocabulary Speech Recognition System. </title> <booktitle> In ARPA Spoken Language Systems Technology Workshop, </booktitle> <pages> pages 104-109, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] [19] [36] [39] [40] [42] [43] [44] [48] [64] <ref> [63] </ref> [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems.
Reference: [64] <author> P. Woodland, J. Odell, V. Valtchev, and S. Young. </author> <title> The HTK Large Vocabulary Continuous Speech Recognition System: An Overview. </title> <booktitle> In ARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 98-101, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Together these two systems cover most of the major design features of the best large vocabulary speech recognizers in use today [18] [50] [19] [36] [39] [40] [42] [43] [44] [48] <ref> [64] </ref> [63] [62] [16]. All of the experimental techniques described in this thesis have been designed to be portable to all such systems.
Reference: [65] <author> Sheryl R. Young. </author> <title> Recognition Confidence Measures: Detection of Mis-recognitions and Out-of-Vocabulary Words. </title> <type> Technical Report No. </type> <institution> CMU-CS-94-157, Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, 15232, USA, </address> <year> 1994. </year>
Reference-contexts: of w, the use of some P (A) as a normalizing factor for our word acoustic score P (Ajw) can work well. (This approach is similar to that found in [10].) In related experiments involving (among other things) the automatic identification of errors in conversational speech recognition, Young and Ward <ref> [65] </ref> used a similar approach.
Reference: [66] <author> T. Zeppenfeld, M. Finke, K. Ries, M. Westphal, and A. Waibel. </author> <title> Recognition of Conversational Telephone Speech using the Janus Speech Engine. </title> <booktitle> In IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <year> 1997. </year> <month> 288 </month>
Reference-contexts: In this chapter I will give details on both the forced alignment and the decoding strategies employed by Sphinx-II [50] [27] and Janus <ref> [66] </ref> [14], as well as some additional side computations that are useful in building error feedback mechanisms.
References-found: 66

