URL: ftp://psyche.mit.edu/pub/jordan/hierarchies.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/jordan.html
Root-URL: 
Title: Hierarchical mixtures of experts and the EM algorithm  
Author: Michael I. Jordan Robert A. Jacobs 
Affiliation: Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  Department of Psychology University of Rochester  
Date: 6, 181-214, 1994.  
Note: Published in: Neural Computation,  
Abstract: We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain. *We want to thank Geoffrey Hinton, Tony Robinson, Mitsuo Kawato and Daniel Wolpert for helpful comments on the manuscript. This project was supported in part by a grant from the McDonnell-Pew Foundation, by a grant from ATR Human Information Processing Research Laboratories, by a grant from Siemens Corporation, by by grant IRI-9013991 from the National Science Foundation, and by grant N00014-90-J-1942 from the Office of Naval Research. The project was also supported by NSF grant ASC-9217041 in support of the Center for Biological and Computational Learning at MIT, including funds provided by DARPA under the HPCC program, and NSF grant ECS-9216531 to support an Initiative in Intelligent Control at MIT. Michael I. Jordan is a NSF Presidential Young Investigator. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bourlard, H., & Kamp, Y. </author> <year> (1988). </year> <title> Auto-association by multilayer perceptrons and singular value decomposition. </title> <journal> Biological Cybernetics, </journal> <volume> 59, </volume> <pages> 291-294. </pages>
Reference-contexts: That option is still available, although we lose the EM proof of convergence (cf. Jordan & Xu, 1993) and we lose the ability to fit the sub-networks efficiently with IRLS. One interesting example of such an application is the case where the experts are auto-associators <ref> (Bourlard & Kamp, 1988) </ref>, in which case the architecture fits hierarchically-nested local principal component decompositions. Another area in unsupervised learning worth exploring is the non-associative version of the hierarchical architecture.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Bridle, J. </author> <year> (1989). </year> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. </title> <editor> In F. Fogelman-Soulie & J. Herault (Eds.), Neuro-computing: </editor> <booktitle> Algorithms, Architectures, and Applications. </booktitle> <address> New York: </address> <publisher> Springer-Verlag. 27 Buntine, W. </publisher> <year> (1991). </year> <title> Learning classification trees. </title> <institution> NASA Ames Technical Report FIA-90-12-19-01, Moffett Field, </institution> <address> CA. </address>
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. </author> <year> (1988). </year> <title> Autoclass: A Bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <address> Ann Arbor, MI. </address>
Reference-contexts: It also opens the door to the Bayesian approaches that have been found to be useful in the context of unsupervised mixture model estimation <ref> (Cheeseman, et al., 1988) </ref>. Although we have not emphasized theoretical issues in this paper, there are 26 a number of points that are worth mentioning.
Reference: <author> Cox, D. R. </author> <year> (1970). </year> <title> The Analysis of Binary Data. </title> <address> London: Chapman-Hall. </address>
Reference-contexts: i X g jji e 2 2 (y ij ) T (y ij ) Example (binary classification) In binary classification problems the output y is a discrete random variable having possible outcomes of "failure" and "success." The probabilistic component of the model is generally assumed to be the Bernoulli distribution <ref> (Cox, 1970) </ref>.
Reference: <author> Cox, D. R., & Hinkley, D. V. </author> <year> (1974). </year> <institution> Theoretical Statistics. </institution> <address> London: Chapman-Hall. </address>
Reference-contexts: The maximum likelihood framework allows standard tools from statistical theory to be brought to bear in developing inference procedures and measures of uncertainty for the architecture <ref> (Cox & Hinkley, 1974) </ref>. It also opens the door to the Bayesian approaches that have been found to be useful in the context of unsupervised mixture model estimation (Cheeseman, et al., 1988).
Reference: <author> Dempster, A. P., Laird, N. M., & Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, B, </journal> <volume> 39, </volume> <pages> 1-38. </pages>
Reference-contexts: learning literature, in which EM has appeared in the context of clustering (Cheeseman, et al. 1988; Nowlan, 1990) and density estimation (Specht, 1991), as well as the statistics literature, in which applications include missing data problems (Little & Rubin, 1987), mixture density estimation (Redner & Walker, 1984), and factor analysis <ref> (Dempster, Laird, & Rubin, 1977) </ref>. Another unsupervised learning application is the learning problem for Hidden Markov Models, for which the Baum-Welch reestimation formulas are a special case of EM.
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference: <author> Finney, D. J. </author> <year> (1973). </year> <title> Statistical Methods in Biological Assay. </title> <address> New York: Hafner. </address>
Reference: <author> Friedman, J. H. </author> <year> (1991). </year> <title> Multivariate adaptive regression splines. </title> <journal> The Annals of Statistics, </journal> <volume> 19, </volume> <pages> 1-141. </pages>
Reference: <author> Fun, W. & Jordan, M. I. </author> <year> (1993). </year> <title> The moving basin: Effective action search in forward models. </title> <institution> MIT Computational Cognitive Science Tech Report 9205, </institution> <address> Cam-bridge, MA. </address>
Reference-contexts: Simulation results We tested Algorithm 1 and Algorithm 2 on a nonlinear system identification problem. The data were obtained from a simulation of a four-joint robot arm moving in three-dimensional space <ref> (Fun & Jordan, 1993) </ref>. The network must learn the forward dynamics of the arm; a state-dependent mapping from joint torques to joint accelerations. The state of the arm is encoded by eight real-valued variables: four positions (rad) and four angular velocities (rad/sec).
Reference: <author> Geman, S., Bienenstock, E., & Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 1-52. </pages>
Reference: <author> Golub, G. H., & Van Loan, G. F. </author> <year> (1989). </year> <title> Matrix Computations. </title> <address> Baltimore, MD: </address> <publisher> The Johns Hopkins University Press. </publisher>
Reference-contexts: A least-squares algorithm In the case of regression, in which a Gaussian probability model and an identity link function are used, the IRLS loop for the expert networks reduces to weighted least squares, which can be solved (in one pass) by any of the standard least-squares algorithms <ref> (Golub & van Loan, 1989) </ref>. The gating networks still require iterative processing. Suppose, however, that we fit the parameters of the gating networks using least squares rather than maximum likelihood. <p> The backpropagation network had 60 hidden units, which yields approximately the same number of parameters in the network as in the hierarchy. The HME architecture was trained by Algorithms 1 and 2, utilizing Cholesky decomposition to solve the weighted least-squares problems <ref> (Golub & van Loan, 1989) </ref>. Note that the HME algorithms have no free parameters.
Reference: <author> Hastie, T. J., & Tibshirani, R. J. </author> <year> (1990). </year> <title> Generalized Additive Models. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> Haykin, S. </author> <year> (1991). </year> <title> Adaptive Filter Theory. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Finally, it is also of interest to note that the recursive least squares algorithm that we utilized in obtaining an on-line variant of Algorithm 2 is not the only possible on-line approach. Any of the fast filter algorithms <ref> (Haykin, 1991) </ref> could also be utilized, giving rise to a family of on-line algorithms. Also, it is worth studying the application of the recursive algorithms to PRESS-like cross-validation calculations to efficiently compute the changes in likelihood that arise from adding or deleting parameters or data points.
Reference: <author> Hinton, G. E. & Sejnowski, T. J. </author> <year> (1986). </year> <title> Learning and relearning in Boltzmann machines. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: </booktitle> <volume> Volume 1, </volume> <pages> 282-317. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: If the algorithm simply increases the function during the M step, rather than maximizing the function, then the algorithm is referred to as a Generalized EM (GEM) algorithm. The Boltzmann learning algorithm <ref> (Hinton & Sejnowski, 1986) </ref> is a neural network example of a GEM algorithm. GEM algorithms are often significantly slower to converge than EM algorithms.
Reference: <author> Hornik, K., Stinchcombe, M., & White, H. </author> <year> (1989). </year> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2, </volume> <pages> 359-366. </pages>
Reference: <author> Jacobs, R. A, Jordan, M. I., Nowlan, S. J., & Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 79-87. </pages> <note> 28 Jordan, </note> <author> M. I., & Jacobs, R. A. </author> <year> (1992). </year> <title> Hierarchies of adaptive experts. </title> <editor> In J. Moody, S. Hanson, & R. Lippmann (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> pp. 985-993. </pages>
Reference-contexts: The work that we describe here makes contact with a number of branches of statistical theory. First, as in our earlier work <ref> (Jacobs, Jordan, Nowlan, & Hin-ton, 1991) </ref>, we formulate the learning problem as a mixture estimation problem (cf. Cheeseman, et al, 1988; Duda & Hart, 1973; Nowlan, 1991; Redner & Walker, 1984; Titterington, Smith, & Makov, 1985).
Reference: <author> Jordan, M. I., & Xu, L. </author> <year> (1993). </year> <title> Convergence properties of the EM approach to learning in mixture-of-experts architectures. </title> <journal> Computational Cognitive Science Tech. Rep. </journal> <volume> 9301, </volume> <publisher> MIT, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Simulation results We tested Algorithm 1 and Algorithm 2 on a nonlinear system identification problem. The data were obtained from a simulation of a four-joint robot arm moving in three-dimensional space <ref> (Fun & Jordan, 1993) </ref>. The network must learn the forward dynamics of the arm; a state-dependent mapping from joint torques to joint accelerations. The state of the arm is encoded by eight real-valued variables: four positions (rad) and four angular velocities (rad/sec). <p> Second, convergence results are available for the architecture. We have shown that the convergence rate of the algorithm is linear in the condition number of a matrix that is the product of an inverse covariance matrix and the Hessian of the log likelihood for the architecture <ref> (Jordan & Xu, 1993) </ref>. Finally, it is worth noting a number of possible extensions of the work reported here. Our earlier work on hierarchical mixtures of experts utilized the multilayer perceptron as the primitive function for the expert networks and gating networks (Jordan & Jacobs, 1992). <p> Our earlier work on hierarchical mixtures of experts utilized the multilayer perceptron as the primitive function for the expert networks and gating networks (Jordan & Jacobs, 1992). That option is still available, although we lose the EM proof of convergence <ref> (cf. Jordan & Xu, 1993) </ref> and we lose the ability to fit the sub-networks efficiently with IRLS. One interesting example of such an application is the case where the experts are auto-associators (Bourlard & Kamp, 1988), in which case the architecture fits hierarchically-nested local principal component decompositions.
Reference: <author> Little, R. J. A., & Rubin, D. B. </author> <year> (1987). </year> <title> Statistical Analysis with Missing Data. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: This is true of the neural network literature and machine learning literature, in which EM has appeared in the context of clustering (Cheeseman, et al. 1988; Nowlan, 1990) and density estimation (Specht, 1991), as well as the statistics literature, in which applications include missing data problems <ref> (Little & Rubin, 1987) </ref>, mixture density estimation (Redner & Walker, 1984), and factor analysis (Dempster, Laird, & Rubin, 1977). Another unsupervised learning application is the learning problem for Hidden Markov Models, for which the Baum-Welch reestimation formulas are a special case of EM.
Reference: <author> Ljung, L. & Soderstrom, T. </author> <year> (1986). </year> <title> Theory and practice of recursive identification. </title> <publisher> Cambridge: MIT Press. </publisher>
Reference-contexts: An on-line algorithm The batch least-squares algorithm that we have described (Algorithm 2) can be converted into an on-line algorithm by noting that linear least squares and weighted linear least squares problems can be solved by recursive procedures that update the parameter estimates with each successive data point <ref> (Ljung & Soderstrom, 1986) </ref>. Our application of these recursive algorithms is straightforward; however, care must be taken to handle the observation weights (the posterior probabilities) correctly. These weights change as a function of the changing parameter values. <p> Equation 14). These updates are essentially the same, except that the scalar is replaced by the matrix R (t) It can be shown, however, that R (t) ij is an estimate of the inverse Hessian of the least-squares cost function <ref> (Ljung & Soderstrom, 1986) </ref>, thus Equation 32 is in fact a stochastic approximation to a Newton-Raphson method rather than a gradient method. 7 Similar equations apply for the updates of the gating networks.
Reference: <author> McCullagh, P. & Nelder, J.A. </author> <year> (1983). </year> <title> Generalized Linear Models. </title> <publisher> London: Chap-man and Hall. </publisher>
Reference-contexts: We show that the algorithm that is generally employed for the unsupervised learning of mixture parameters|the Expectation-Maximization (EM) algorithm of Dempster, Laird and Rubin (1977)|can also be exploited for supervised learning. Second, we utilize generalized linear model (GLIM) theory <ref> (McCullagh & Nelder, 1983) </ref> to provide the basic statistical structure for the components of the architecture. In particular, the "soft splits" referred to above are modeled as multinomial logit models|a specific form of GLIM. <p> See Jordan and Xu (1993) for a recursive formalism that handles arbitrary hierarchies. 4 All of the expert networks in the tree are linear with a single output nonlinearity. We will refer to such a network as "generalized linear," borrowing the terminology from statistics <ref> (McCullagh & Nelder, 1983) </ref>. Expert network (i; j) produces its output ij as a generalized linear function of the input x: ij = f (U ij x); (1) where U ij is a weight matrix and f is a fixed continuous nonlinearity. <p> Other models (e.g., multiway classification, counting, rate estimation and survival estimation) are handled by making other choices for f (). These models are smoothed piecewise analogs of the corresponding GLIM models <ref> (cf. McCullagh & Nelder, 1983) </ref>. The gating networks are also generalized linear. Define intermediate variables ~ i as follows: ~ i = v T where v i is a weight vector. <p> Equations 2, 4, 3 and 5) corresponds to a multinomial logit probability model at each nonterminal of the tree (see Appendix 2). A multinomial logit model is a special case of a GLIM that is commonly used for "soft" multiway classification <ref> (McCullagh & Nelder, 1983) </ref>. Under the multinomial logit model, we interpret the gating networks as modeling the input-dependent, multinomial probabilities associated with decisions at particular levels of resolution in a tree-structured model of the data. <p> Given our parameterization of P ij , the log likelihood in Equation 27 is a weighted log likelihood for a GLIM. An efficient algorithm known as iteratively reweighted least-squares (IRLS) is available to solve the maximum likelihood problem for such models <ref> (McCullagh & Nelder, 1983) </ref>. We discuss IRLS in Appendix A. Equation 28 involves maximizing the cross-entropy between the posterior probabilities h (t) k and the prior probabilities g (t) k .
Reference: <author> Moody, J. </author> <year> (1989). </year> <title> Fast learning in multi-resolution hierarchies. </title> <editor> In D.S. Touretzky (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Murthy, S. K., Kasif, S., & Salzberg, S. </author> <year> (1993). </year> <title> OC1: A randomized algorithm for building oblique decision trees. </title> <type> Technical Report, </type> <institution> Department of Computer Science, Johns Hopkins University. </institution>
Reference: <author> Nowlan, S.J. </author> <year> (1990). </year> <title> Maximum likelihood competitive learning. </title> <editor> In D.S. Touretzky (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Nowlan, S.J. </author> <year> (1991). </year> <title> Soft competitive adaptation: Neural network learning algorithms based on fitting statistical mixtures. </title> <type> Tech. Rep. </type> <address> CMU-CS-91-126, CMU, Pittsburgh, PA. </address>
Reference-contexts: The work that we describe here makes contact with a number of branches of statistical theory. First, as in our earlier work <ref> (Jacobs, Jordan, Nowlan, & Hin-ton, 1991) </ref>, we formulate the learning problem as a mixture estimation problem (cf. Cheeseman, et al, 1988; Duda & Hart, 1973; Nowlan, 1991; Redner & Walker, 1984; Titterington, Smith, & Makov, 1985). <p> The "neural tree" algorithm (Stromberg, Zrida, & Isaksson, 1991) is a decision tree with multi-layer perceptions (MLP's) at the non-terminals. This architecture can form oblique (or curvilinear) splits, however the MLP's are trained by a heuristic that has no clear relationship to overall classification performance. Finally, Hinton and Nowlan <ref> (see Nowlan, 1991) </ref> have independently proposed extending the Jacobs et al. (1991) modular architecture to a tree-structured system. They did not develop a likelihood approach to the problem, however, proposing instead a heuristic splitting scheme. Conclusions We have presented a tree-structured architecture for supervised learning.
Reference: <author> Quandt, R.E., & Ramsey, J.B. </author> <year> (1972). </year> <title> A new approach to estimating switching regressions. </title> <journal> Journal of the American Statistical Society, </journal> <volume> 67, </volume> <pages> 306-310. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: Similar comments can be made with respect to the decision tree methodology in the machine learning literature. Algorithms such as ID3 build trees that have axis-parallel splits and use heuristic splitting algorithms <ref> (Quinlan, 1986) </ref>. More recent research has studied decision trees with oblique splits (Murthy, Kasif & Salzberg, 1993; Utgoff & Brodley, 1990). None of these papers, however, have treated the problem of splitting data as a statistical problem, nor have they provided a global goodness-of-fit measure for their trees.
Reference: <author> Quinlan, J. R., & Rivest, R. L. </author> <year> (1989). </year> <title> Inferring decision trees using the Minimum Description Length Principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <pages> 227-248. </pages>
Reference: <author> Redner, R. A., & Walker, H. F. </author> <year> (1984). </year> <title> Mixture densities, maximum likelihood and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26, </volume> <pages> 195-239. </pages>
Reference-contexts: of the neural network literature and machine learning literature, in which EM has appeared in the context of clustering (Cheeseman, et al. 1988; Nowlan, 1990) and density estimation (Specht, 1991), as well as the statistics literature, in which applications include missing data problems (Little & Rubin, 1987), mixture density estimation <ref> (Redner & Walker, 1984) </ref>, and factor analysis (Dempster, Laird, & Rubin, 1977). Another unsupervised learning application is the learning problem for Hidden Markov Models, for which the Baum-Welch reestimation formulas are a special case of EM.
Reference: <author> Sanger, T. D. </author> <year> (1991). </year> <title> A tree-structured adaptive network for function approximation in high dimensional spaces. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2, </volume> <pages> 285-293. </pages>
Reference: <author> Scott, D. W. </author> <year> (1992). </year> <title> Multivariate Density Estimation. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference-contexts: The foregoing considerations suggest that divide-and-conquer algorithms generally tend to be variance-increasing algorithms. This is indeed the case and is particularly problematic in high-dimensional spaces where data become exceedingly sparse <ref> (Scott, 1992) </ref>. One response to this dilemma|that adopted by CART, MARS, and ID3, and also adopted here|is to utilize piecewise constant or piecewise linear functions. These functions minimize variance at a cost of increased bias.
Reference: <author> Specht, D. F. </author> <year> (1991). </year> <title> A general regression neural network. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2, </volume> <pages> 568-576. </pages>
Reference-contexts: In practice EM has been applied almost exclusively to unsupervised learning problems. This is true of the neural network literature and machine learning literature, in which EM has appeared in the context of clustering (Cheeseman, et al. 1988; Nowlan, 1990) and density estimation <ref> (Specht, 1991) </ref>, as well as the statistics literature, in which applications include missing data problems (Little & Rubin, 1987), mixture density estimation (Redner & Walker, 1984), and factor analysis (Dempster, Laird, & Rubin, 1977).
Reference: <author> Stone, C. J. </author> <year> (1977). </year> <title> Consistent nonparametric regression. </title> <journal> The Annals of Statistics, </journal> <volume> 5, </volume> <pages> 595-645. </pages>
Reference: <author> Stromberg, J. E., Zrida, J., & Isaksson, A. </author> <year> (1991). </year> <title> Neural trees|using neural nets in a tree classifier structure. </title> <booktitle> IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pp. 137-140. </pages>
Reference-contexts: There are a variety of neural network architectures that are related to the HME architecture. The multi-resolution aspect of HME is reminiscent of Moody's (1989) multi-resolution CMAC hierarchy, differing in that Moody's levels of resolution are handled explicitly by separate networks. The "neural tree" algorithm <ref> (Stromberg, Zrida, & Isaksson, 1991) </ref> is a decision tree with multi-layer perceptions (MLP's) at the non-terminals. This architecture can form oblique (or curvilinear) splits, however the MLP's are trained by a heuristic that has no clear relationship to overall classification performance.
Reference: <author> Titterington, D. M., Smith, A. F. M., & Makov, U. E. </author> <year> (1985). </year> <title> Statistical Analysis of Finite Mixture Distributions. </title> <address> New York: </address> <publisher> John Wiley. </publisher>
Reference: <author> Utgoff, P. E., & Brodley, C. E. </author> <year> (1990). </year> <title> An incremental method for finding mul-tivariate splits for decision trees. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> Los Altos, CA. </address>
Reference: <author> Wahba, G., Gu, C., Wang, Y., & Chappell, R. </author> <year> (1993). </year> <title> Soft classification, a.k.a. risk estimation, via penalized log likelihood and smoothing spline analysis of variance. </title> <type> Tech. Rep. 899, </type> <institution> Department of Statistics, University of Wisconsin, Madison. </institution>
Reference: <author> Wu, C. F. J. </author> <year> (1983). </year> <title> On the convergence properties of the EM algorithm. </title> <journal> The Annals of Statistics, </journal> <volume> 11, </volume> <pages> 95-103. </pages>
Reference-contexts: What is the effect of such a step on the incomplete likelihood? Dempster, et al. proved that an increase in Q implies an increase in the incomplete likelihood: l ( (p+1) ; X ) l ( (p) ; X ): Equality obtains only at the stationary points of l <ref> (Wu, 1983) </ref>. Thus the likelihood l increases monotonically along the sequence of parameter estimates generated by an EM algorithm. In practice this implies convergence to a local maximum.
References-found: 39

