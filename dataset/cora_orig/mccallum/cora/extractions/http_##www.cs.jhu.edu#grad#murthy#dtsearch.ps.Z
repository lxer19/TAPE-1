URL: http://www.cs.jhu.edu/grad/murthy/dtsearch.ps.Z
Refering-URL: http://www.cs.jhu.edu/grad/murthy/home.html
Root-URL: http://www.cs.jhu.edu
Email: murthy@scr.siemens.com  salzberg@cs.jhu.edu  
Title: Investigations of the greedy heuristic for classification tree induction  
Author: Sreerama K. Murthy Steven L. Salzberg 
Address: 755 College Road East, Princeton, NJ 08540, USA  Baltimore, MD 21218  
Affiliation: Siemens Corporate Research,  Department of Computer Science, Johns Hopkins University,  
Abstract: Most existing methods for automatic construction of classification trees utilize the greedy heuristic: trees are constructed one node at a time with no looking ahead or backtracking, choosing locally optimal splits to divide the data. This heuristic is commonly believed to work well in practice and is widely used, even though it is known to produce necessarily suboptimal trees. In this paper, we attempt to systematically study the effectiveness of the greedy heuristic. In a large-scale experimental study, we compare greedily induced trees with the corresponding optimal trees while varying several control variables. We also contrast the greedy heuristic with a seemingly superior alternative: limited lookahead search. We design tens of thousands of synthetic data sets for our experiments and build trees using two popular goodness measures. We also use real-world data. Two main observations from the experiments are: (1) the greedy heuristic produces trees that are consistently close to the optimal in terms of expected depth, and (2) augmenting with one-level lookahead not only does not improve upon the greedy heuristic, but often produces worse results. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkinson, J. B. </author> <year> (1994). </year> <title> A greedy look-ahead heuristic for combinatorial optimization: An application to vehicle scheduling with time windows. </title> <journal> Journal of the Operational Research Society, </journal> <volume> 45 (6), </volume> <pages> 673-684. </pages>
Reference-contexts: This paper presents few analyses of the effects of lookahead on tree induction. Lookahead obviously is not the only way of inducing globally optimal decision trees. Three approaches mentioned in the combinatorial optimization literature to temper the crudeness of the greedy approach are "insert/delete", randomization and, of course, lookahead <ref> (Atkinson, 1994) </ref>. The insert/delete method allows that steps may be added to or removed from the partial solution built thus far. An example is the problem of constructing school timetables (De Gans, 1981), where classes may be added or deleted based on the conflicts in schedule that arrive.
Reference: <author> Baer, J., & Schwab, B. </author> <year> (1977). </year> <title> A comparison of tree-balancing algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 20 (5), </volume> <pages> 322-330. </pages>
Reference: <author> Bennett, K. P. </author> <year> (1994). </year> <title> Global tree optimization: A non-greedy decision tree algorithm. </title> <booktitle> In Proceedings of Interface 94: The 26th Symposium on the Interface Research Triangle, </booktitle> <publisher> North Carolina. </publisher>
Reference-contexts: In the second stage, the tree is refined to be as close to optimal as possible. Refinement techniques attempted include dynamic programming (Meisel & Michalopoulos, 1973), fuzzy logic search (Wang & Suen, 1987) and multi-linear programming <ref> (Bennett, 1994) </ref>. The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: A few details about our overall experimental procedure. A large number of goodness measures have been explored for test selection in tree induction (Murthy, 1997). We use in this paper two popular ones, namely, information gain (Quinlan, 1986) and gini index <ref> (Breiman, Friedman, Olshen, & Stone, 1984) </ref>. In the experiments in which the training data is noise-free, no pruning was used. <p> This uses the same set of candidate splits as Greedy. However, the goodness of a candidate split T is computed by examining all splits one level down from T . As we use two goodness measures, namely, gini index <ref> (Breiman et al., 1984) </ref> and information gain (Quinlan, 1986), this gives four algorithms, which we name Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm (Breiman et al., 1984) and Greedy-Info to the ID3 algorithm (Quinlan, 1986). <p> As we use two goodness measures, namely, gini index <ref> (Breiman et al., 1984) </ref> and information gain (Quinlan, 1986), this gives four algorithms, which we name Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm (Breiman et al., 1984) and Greedy-Info to the ID3 algorithm (Quinlan, 1986). For the experiments in this section, we use only three of the six tree quality measures described in Section 1, namely, accuracy, size, maximum depth. <p> We use a large number of carefully designed synthetic data sets as well as some difficult real-world data sets for our experiments. The greedy tree induction methods used are very similar to CART <ref> (Breiman et al., 1984) </ref> and ID3 (Quinlan, 1986). <p> Our observation that these measures consistently produced identical trees, in terms of six tree quality measures, in a large experiment involving more than a hundred thousand synthetic data sets strengthens the existing results. 2 * Cost complexity pruning <ref> (Breiman et al., 1984) </ref> dealt effectively with both attribute and class noise. However, the accuracies on the training set were overly optimistic in the presence of attribute noise. Most of our experiments in this paper are based on synthetic data.
Reference: <author> Breiman, L. </author> <year> (1994). </year> <title> Bagging predictors. </title> <type> Tech. rep., </type> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA. </address>
Reference: <author> Bucy, R., & Diesposti, R. </author> <year> (1993). </year> <title> Decision tree design by simulated annealing. </title> <journal> Mathematical Modieling and Numerical Analysis, </journal> <volume> 27 (5), </volume> <pages> 515-534. </pages> <note> A RAIRO Journal. </note>
Reference: <author> Buntine, W., & Niblett, T. </author> <year> (1992). </year> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 75-85. </pages>
Reference-contexts: Tree construction using partial or exhaustive lookahead has been considered in statistics (Fielding, 1977; Elder, 1995), in pattern recognition (Hartmann, Varshney, Mehrotra, & Gerberich, 1982), for tree structured vector quantizers (Riskin & Gray, 1991), for Bayesian class probability trees <ref> (Buntine, 1992) </ref>, for neural trees and in machine learning (Hunt, Marin, & Stone, 1966; Nor-ton, 1989; Ragavan & Rendell, 1993; Murthy & Salzberg, 1995b). Hartmann et al.(1982) describe a tree induction algorithm based on an information theoretic criterion between branching levels in a tree.
Reference: <author> Buntine, W. </author> <year> (1992). </year> <title> Learning classification trees. </title> <journal> Statistics and Computing, </journal> <volume> 2, </volume> <pages> 63-73. </pages>
Reference-contexts: Tree construction using partial or exhaustive lookahead has been considered in statistics (Fielding, 1977; Elder, 1995), in pattern recognition (Hartmann, Varshney, Mehrotra, & Gerberich, 1982), for tree structured vector quantizers (Riskin & Gray, 1991), for Bayesian class probability trees <ref> (Buntine, 1992) </ref>, for neural trees and in machine learning (Hunt, Marin, & Stone, 1966; Nor-ton, 1989; Ragavan & Rendell, 1993; Murthy & Salzberg, 1995b). Hartmann et al.(1982) describe a tree induction algorithm based on an information theoretic criterion between branching levels in a tree.
Reference: <author> Chang, H., & Iyengar, S. </author> <year> (1984). </year> <title> Efficient algorithms to globally balance a binary search tree. </title> <journal> Communications of the ACM, </journal> <volume> 27 (7), </volume> <pages> 695-702. </pages>
Reference: <author> Chou, P. A., & Gray, R. M. </author> <year> (1986). </year> <title> On decision trees for pattern recognition. </title> <booktitle> In Proceedings of the IEEE Symposium on Information Theory, </booktitle> <address> p. 69 Ann Arbor, MI. </address>
Reference: <author> Cormen, T. H., Leiserson, C. E., & Rivest, R. L. </author> <year> (1990). </year> <title> Introduction to Algorithms. </title> <publisher> The MIT Press and McGraw-Hill Book Company. </publisher>
Reference: <author> Cox, L. A., Qiu, Y., & Kuehner, W. </author> <year> (1989). </year> <title> Heuristic least-cost computation of discrete classification functions with uncertain argument values. </title> <journal> Annals of Operations Research, </journal> <volume> 21 (1), </volume> <pages> 1-30. </pages>
Reference-contexts: Previous studies that attempted comparisons to optimal trees (e.g., <ref> (Cox, Qiu, & Kuehner, 1989) </ref>) used approaches like dynamic programming to generate the optimal trees. Because it is slow, this option is impractical for our study, in which we use hundreds of thousands of artificial data sets.
Reference: <author> De Gans, O. B. </author> <year> (1981). </year> <title> A computer timetabling system for secondary schools in the netherlands. </title> <journal> European Journal of Operations Research, </journal> <volume> 7, </volume> <pages> 175-182. </pages>
Reference-contexts: The insert/delete method allows that steps may be added to or removed from the partial solution built thus far. An example is the problem of constructing school timetables <ref> (De Gans, 1981) </ref>, where classes may be added or deleted based on the conflicts in schedule that arrive. Randomization of specific steps in a greedy method clearly produces a potentially more powerful heuristic.
Reference: <author> Dietterich, T. G., & Kong, E. B. </author> <year> (1995). </year> <title> Machine learning bias, statistical bias and statistical variance of decision tree algorithms. </title> <editor> In Schlimmer, J. (Ed.), MLC-95: </editor> <booktitle> Machine Learning. Proceedings of the Twelfth International Conference Tahoe City, </booktitle> <address> CA. </address> <publisher> Morgan Kaufmann Publishers Inc. </publisher>
Reference: <author> Elder, J. F. </author> <year> (1994). </year> <title> Growing decision trees less greedily. </title> <editor> In Sall, J., & Lehmann, A. (Eds.), </editor> <title> Computation-ally intensive statistical methods: </title> <booktitle> Proceedings of the 26th symposium on the Interface, </booktitle> <pages> pp. 161-166. </pages> <publisher> Interface Foundation, North America. </publisher>
Reference-contexts: Ragavan and Rendell describe experiments that show that LFC outperforms more straightforward approaches to feature construction and lookahead, in symbolic domains. The effectiveness of the greedy heuristic in the context of (stepwise) regression is discussed in <ref> (Elder, 1994) </ref>, and is used to motivate a simple one-level lookahead alternative to CART called Texas Two-step. This paper presents few analyses of the effects of lookahead on tree induction. Lookahead obviously is not the only way of inducing globally optimal decision trees.
Reference: <author> Elder, J. F. </author> <year> (1995). </year> <title> Heuristic search for model structure.. </title> <editor> In Fisher (Fisher, </editor> <year> 1995), </year> <pages> pp. 199-210. </pages>
Reference: <author> Fielding, A. </author> <year> (1977). </year> <title> Binary segmentation: the automatic interaction detector and related techniques for exploring data structure. </title> <editor> In O'Muircheartaigh, C. A., & Payne, C. (Eds.), </editor> <booktitle> The analysis of survey data, </booktitle> <volume> Vol. I, </volume> <pages> pp. 221-257. </pages> <publisher> John Wiley & Sons, </publisher> <address> Chichester, UK. </address>
Reference: <author> Fisher, D. (Ed.). </author> <year> (1995). </year> <title> AI&Stats-95: </title> <booktitle> Preliminary Papers of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL. </address> <institution> Society for AI and Statistics. </institution> <note> 30 Fukanaga, </note> <author> K., & Hayes, R. A. </author> <year> (1989). </year> <title> Effect of sample size in classifier design. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11, </volume> <pages> 873-885. </pages>
Reference: <author> Garey, M. R., & Graham, R. L. </author> <year> (1974). </year> <title> Performance bounds on the splitting algorithm for binary testing. </title> <journal> Acta Informatica, </journal> <volume> 3 (Fasc. 4), </volume> <pages> 347-355. </pages>
Reference: <author> Goodman, R. M., & Smyth, P. J. </author> <year> (1988). </year> <title> Decision tree design from a communication theory standpoint. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 34 (5), </volume> <pages> 979-994. </pages>
Reference-contexts: These intractability results imply that polynomial greedy algorithms, such as those used in practice, produce suboptimal trees assuming P 6= N P . Moreover, greedy induction using mutual information-based goodness measures has 1 been proven <ref> (Goodman & Smyth, 1988) </ref> to be equivalent to a form of Shannon Fano prefix coding. As Shannon Fano codes are suboptimal, so are trees induced using measures like information gain, in terms of expected depth and prediction accuracy. <p> Our experiments indicate that earlier theoretical results <ref> (Goodman & Smyth, 1988) </ref> may be applicable to the grow and prune tree algorithms used today. 28 * The maximum depth of greedily induced trees was worse than the optimal. The only significant benefit of lookahead search also was in producing trees with slightly shallower worst-case depth.
Reference: <author> Graham, R., & Hell, P. </author> <year> (1985). </year> <title> On the history of minimum spanning tree problem. </title> <journal> Annals of History of Computing, </journal> <volume> 7. </volume>
Reference-contexts: The concept of pathology in the context of decision trees is described and illustrated. Section 5 concludes the paper. 2. Existing Work Greedy algorithms do not always yield optimal solutions, but for some problems they do. Examples of the latter variety include methods for constructing minimum spanning trees <ref> (Graham & Hell, 1985) </ref> and methods for producing optimal Huffman codes for data compression (Lelewer & Hirschberg, 1987). Greedy search is used as a heuristic for a number of well-known NP-hard problems.
Reference: <author> Hartmann, C. R. P., Varshney, P. K., Mehrotra, K. G., & Gerberich, C. L. </author> <year> (1982). </year> <title> Application of information theory to the construction of efficient decision trees. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-28 (4), </volume> <pages> 565-577. </pages>
Reference-contexts: Tree construction using partial or exhaustive lookahead has been considered in statistics (Fielding, 1977; Elder, 1995), in pattern recognition <ref> (Hartmann, Varshney, Mehrotra, & Gerberich, 1982) </ref>, for tree structured vector quantizers (Riskin & Gray, 1991), for Bayesian class probability trees (Buntine, 1992), for neural trees and in machine learning (Hunt, Marin, & Stone, 1966; Nor-ton, 1989; Ragavan & Rendell, 1993; Murthy & Salzberg, 1995b).
Reference: <author> Heath, D., Kasif, S., & Salzberg, S. </author> <year> (1993). </year> <title> k-DT: A multi-tree learning method. </title> <booktitle> In Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <pages> pp. </pages> <address> 138-149 Harpers Ferry, WV. </address> <institution> George Mason University. </institution>
Reference: <author> Holte, R. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 (1), </volume> <pages> 63-90. </pages>
Reference-contexts: It has been observed that most of the data sets in the UCI repository can be described by very simple classification rules <ref> (Holte, 1993) </ref>. So, we needed to be careful in our choice of real world domains. We used a survey of results on several UCI data sets provided by Holte (Holte, 1993) to choose six "difficult" domains domains for which the best known accuracy 25 is at most 90%. <p> has been observed that most of the data sets in the UCI repository can be described by very simple classification rules <ref> (Holte, 1993) </ref>. So, we needed to be careful in our choice of real world domains. We used a survey of results on several UCI data sets provided by Holte (Holte, 1993) to choose six "difficult" domains domains for which the best known accuracy 25 is at most 90%.
Reference: <author> Horowitz, E., & Sahni, S. </author> <year> (1984). </year> <title> Fundamentals of Computer Algorithms. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD. </address>
Reference-contexts: Greedy search is used as a heuristic for a number of well-known NP-hard problems. Examples are the 0/1 knapsack problem (Sahni, 1975), multiprocessor scheduling <ref> (Horowitz & Sahni, 1984) </ref> and, of course, decision tree induction. There exist a few theoretical analyses of the adequacy of greedy search for decision tree induction (Chou & Gray, 1986; Goodman & Smyth, 1988).
Reference: <author> Hunt, E. B., Marin, J., & Stone, P. J. </author> <year> (1966). </year> <title> Experiments in Induction. </title> <publisher> Academic Press, </publisher> <address> New York and London. </address>
Reference: <author> Hyafil, L., & Rivest, R. L. </author> <year> (1976). </year> <title> Constructing optimal binary decision trees is NP-complete. </title> <journal> Information Processing Letters, </journal> <volume> 5 (1), </volume> <pages> 15-17. </pages>
Reference-contexts: They do not address the issue of what happens if search strategies superior to greedy search (e.g., limited lookahead) are adopted. There are many NP-completeness results related to decision tree construction. Hyafil and Rivest <ref> (Hyafil & Rivest, 1976) </ref> proved that the problem of building optimal decision trees from 3 decision tables, minimizing the expected number of tests required to classify an unknown sample, is NP-Complete.
Reference: <author> Kira, K., & Rendell, L. A. </author> <year> (1992). </year> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In AAAI-92: Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 129-134 San Jose, CA. </address> <booktitle> American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press / The MIT Press. </publisher>
Reference-contexts: They suggest that this problem can be alleviated by using a more elaborate attribute weighting criterian <ref> (Kira & Rendell, 1992) </ref> in conjunction with the regular greedy algorithm. Kroger (Kroger, 1996) distributes a software program for optimizing decision trees in the medical domain. Quinlan and Jones (Quinlan & Cameroon-Jones, 1995) recently coined the term "oversearch-ing" to describe the perils of excessive searching in classifier induction.
Reference: <author> Kononenko, I., Simec, E., & Robnik-Sikonja, M. </author> <year> (1997). </year> <title> Overcoming the myopia of inductive learning algorithms with RELIEFF. </title> <journal> Applied Intelligence: The International Journal of Artificial Intelligence, Neural Networks and Complex Problem-solving Technologies, </journal> <volume> 7 (1), </volume> <pages> 39-55. </pages>
Reference: <author> Koza, J. R. </author> <year> (1991). </year> <title> Concept formation and decision tree induction using the genetic programming paradigm. </title>
Reference-contexts: The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree. In order to escape local minima in the search space, randomized search techniques such as genetic programming <ref> (Koza, 1991) </ref> and simulated annealing (Bucy & Diesposti, 1993; Lutsko & Kuijpers, 1993) have been attempted. These methods search the space of all decision trees using random perturbations, additions and deletions of the splits.
Reference: <editor> In Schwefel, H. P., & Manner, R. (Eds.), </editor> <booktitle> Parallel Problem Solving from Nature Proceedings of 1st Workshop, PPSN 1, Vol. 496 of Lecture Notes in Computer Science, </booktitle> <pages> pp. </pages> <address> 124-128 Dortmund, Germany. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany. </address>
Reference: <author> Kroger, M. </author> <year> (1996). </year> <title> Optimization of classification trees: strategy and algorithm improvement. </title> <journal> Computer Physics Communications, </journal> <volume> 95, </volume> <pages> 98. </pages>
Reference-contexts: They suggest that this problem can be alleviated by using a more elaborate attribute weighting criterian (Kira & Rendell, 1992) in conjunction with the regular greedy algorithm. Kroger <ref> (Kroger, 1996) </ref> distributes a software program for optimizing decision trees in the medical domain. Quinlan and Jones (Quinlan & Cameroon-Jones, 1995) recently coined the term "oversearch-ing" to describe the perils of excessive searching in classifier induction.
Reference: <author> Kwok, S., & Carter, C. </author> <year> (1990). </year> <title> Multiple decision trees. </title> <editor> In Schachter, R., Levitt, T., Kanal, L., & Lemmer, J. (Eds.), </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <volume> Vol. 4, </volume> <pages> pp. 327-335. </pages> <publisher> Elsevier Science, Amsterdam. </publisher>
Reference: <author> Lazarides, A., Normandin, Y., & Kuhn, R. </author> <year> (1996). </year> <title> Improving decision trees for acoustic modeling. </title> <editor> In Bunnel, H., & Idsardi, T. (Eds.), </editor> <booktitle> Proceedings of ICSLP 96: Fourth International Conference on Spoken Language Processing, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 1053-1056. </pages> <publisher> IEEE. </publisher>
Reference-contexts: The fact that we only used binary splits in real-valued domains may be one reason why information gain and gini index behaved similarly. 29 To determine how often pathology occurs in real-world, more detailed analyses will be necessary. (See <ref> (Lazarides, Normandin, & Kuhn, 1996) </ref> for an example of pathology in a real-world problem.) Other than lookahead, we have not explored in this paper any techniques to extend greedy search.
Reference: <author> Lelewer, D. A., & Hirschberg, D. S. </author> <year> (1987). </year> <title> Data compression. </title> <journal> ACM Computing Surveys, </journal> <volume> 19 (3), </volume> <pages> 261-296. </pages>
Reference-contexts: Section 5 concludes the paper. 2. Existing Work Greedy algorithms do not always yield optimal solutions, but for some problems they do. Examples of the latter variety include methods for constructing minimum spanning trees (Graham & Hell, 1985) and methods for producing optimal Huffman codes for data compression <ref> (Lelewer & Hirschberg, 1987) </ref>. Greedy search is used as a heuristic for a number of well-known NP-hard problems. Examples are the 0/1 knapsack problem (Sahni, 1975), multiprocessor scheduling (Horowitz & Sahni, 1984) and, of course, decision tree induction.
Reference: <author> Lutsko, J. F., & Kuijpers, B. </author> <year> (1993). </year> <title> Simulated annealing in the construction of near-optimal decision trees. </title> <booktitle> In AI&Stats-93: Preliminary Papers of the Fourth International Workshop on Artificial Intelligence and Statistics Ft. </booktitle> <address> Lauderdale, FL. </address> <institution> Society for AI and Statistics. </institution>
Reference: <author> Meisel, W. S., & Michalopoulos, D. A. </author> <year> (1973). </year> <title> A partitioning algorithm with application in pattern classification and the optimization of decision trees. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-22 (1), </volume> <pages> 93-103. </pages>
Reference-contexts: In the first stage, a sufficient 4 partitioning is induced using any reasonably good (greedy) method. In the second stage, the tree is refined to be as close to optimal as possible. Refinement techniques attempted include dynamic programming <ref> (Meisel & Michalopoulos, 1973) </ref>, fuzzy logic search (Wang & Suen, 1987) and multi-linear programming (Bennett, 1994). The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree.
Reference: <editor> Mellish, C. (Ed.). </editor> <booktitle> (1995). IJCAI-95: Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada. </address> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA. </address>
Reference: <author> Miller, D., & Rose, K. </author> <year> (1996). </year> <title> Hierarchical, unsupervised learning with growing via phase transitions. </title> <journal> Neural Computation, </journal> <volume> 8 (2), </volume> <pages> 427-452. </pages> <note> 31 Mingers, J. </note> <year> (1989). </year> <title> An empirical comparison of selection measures for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 319-342. </pages>
Reference-contexts: In the unsupervised learning literature, it has been shown recently that greedy splitting and growing of trees, even using pruning, can be significantly suboptimal <ref> (Miller & Rose, 1996) </ref>. For problems where greedy search is guaranteed not to produce optimal results, it is natural to look for techniques that systematically bridge the gap between the approximate solutions provided by greedy search and the optimal solutions.
Reference: <author> Moret, B. M. </author> <year> (1982). </year> <title> Decision trees and diagrams. </title> <journal> Computing Surveys, </journal> <volume> 14 (4), </volume> <pages> 593-623. </pages>
Reference-contexts: Randomization of specific steps in a greedy method clearly produces a potentially more powerful heuristic. For early work using dynamic programming and branch-and-bound techniques to convert decision tables to optimal trees, see <ref> (Moret, 1982) </ref>. Constructing optimal or near-optimal decision trees using a two-stage approach has been attempted by many authors. In the first stage, a sufficient 4 partitioning is induced using any reasonably good (greedy) method. In the second stage, the tree is refined to be as close to optimal as possible.
Reference: <author> Murphy, P. M., & Aha, D. </author> <year> (1994). </year> <title> UCI repository of machine learning databases a machine-readable data repository. </title> <institution> Maintained at the Department of Information and Computer Science, University of California, Irvine. </institution> <note> Anonymous FTP from ics.uci.edu in the directory pub/machine-learning-databases. </note>
Reference-contexts: We build trees on the entire classes with and without lookahead, and quantitatively measure the improvement caused by one-level lookahead over the greedy heuristic. We also use several real-world data sets taken from the UCI repository <ref> (Murphy & Aha, 1994) </ref>. A few details about our overall experimental procedure. A large number of goodness measures have been explored for test selection in tree induction (Murthy, 1997). <p> Saia suggests a Monte Carlo empirical technique to estimate priors more effectively in the context of rule learning. The style of empirical investigation used in this paper needs a mention. It has been made possible by the existence of extremely fast, inexpensive workstations. Murphy and Pazzani <ref> (Murphy & Pazzani, 1994) </ref> first used this style of experimentation to evaluate the Occam's Razor principle by constructing all decision trees consistent with a fixed concept. Portions of this paper have been reported at two conferences in the past (Murthy & Salzberg, 1995a, 1995b). 3. <p> As in Section 3, our experiments are based on large numbers of synthetic data sets. In addition, we can also use real world data here, as we do not need to know the optimal trees. We experiment with seven real-world domains from the UCI machine learning repository <ref> (Murphy & Aha, 1994) </ref>, as described in Section 4.5. It is not appropriate to use here the same synthetic data sets as in Section 3. * We would like to use data for which one-level lookahead is close to exhaustive search and thus is likely to improve over greedy search. <p> For class C S , average accuracy dropped from 99.5 to 99.0 when stop-splitting was used. 4.5 Experiment 9 In the final experiment, we evaluate the effects of one-level lookahead on seven data sets taken from the University of California at Irvine repository of machine learning databases <ref> (Murphy & Aha, 1994) </ref>. If a greedy method can induce a highly accurate, concise classifier for a domain (the well-known Iris data is one such example), it is unlikely that we can observe significant benefits of lookahead on that domain.
Reference: <author> Murphy, P. M., & Pazzani, M. J. </author> <year> (1994). </year> <title> Exploring the decision forest: An empirical investigation of Occam's Razor in decision tree induction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 257-275. </pages>
Reference-contexts: We build trees on the entire classes with and without lookahead, and quantitatively measure the improvement caused by one-level lookahead over the greedy heuristic. We also use several real-world data sets taken from the UCI repository <ref> (Murphy & Aha, 1994) </ref>. A few details about our overall experimental procedure. A large number of goodness measures have been explored for test selection in tree induction (Murthy, 1997). <p> Saia suggests a Monte Carlo empirical technique to estimate priors more effectively in the context of rule learning. The style of empirical investigation used in this paper needs a mention. It has been made possible by the existence of extremely fast, inexpensive workstations. Murphy and Pazzani <ref> (Murphy & Pazzani, 1994) </ref> first used this style of experimentation to evaluate the Occam's Razor principle by constructing all decision trees consistent with a fixed concept. Portions of this paper have been reported at two conferences in the past (Murthy & Salzberg, 1995a, 1995b). 3. <p> As in Section 3, our experiments are based on large numbers of synthetic data sets. In addition, we can also use real world data here, as we do not need to know the optimal trees. We experiment with seven real-world domains from the UCI machine learning repository <ref> (Murphy & Aha, 1994) </ref>, as described in Section 4.5. It is not appropriate to use here the same synthetic data sets as in Section 3. * We would like to use data for which one-level lookahead is close to exhaustive search and thus is likely to improve over greedy search. <p> For class C S , average accuracy dropped from 99.5 to 99.0 when stop-splitting was used. 4.5 Experiment 9 In the final experiment, we evaluate the effects of one-level lookahead on seven data sets taken from the University of California at Irvine repository of machine learning databases <ref> (Murphy & Aha, 1994) </ref>. If a greedy method can induce a highly accurate, concise classifier for a domain (the well-known Iris data is one such example), it is unlikely that we can observe significant benefits of lookahead on that domain.
Reference: <author> Murthy, S. K. </author> <year> (1997). </year> <title> Automatic construction of decision trees from data: </title> <note> A multi-disciplinary survey.. Under submission to Data Mining and Knowledge Discovery journal. Manuscript in http://www.cs.jhu.edu/murthy. </note>
Reference-contexts: We also use several real-world data sets taken from the UCI repository (Murphy & Aha, 1994). A few details about our overall experimental procedure. A large number of goodness measures have been explored for test selection in tree induction <ref> (Murthy, 1997) </ref>. We use in this paper two popular ones, namely, information gain (Quinlan, 1986) and gini index (Breiman, Friedman, Olshen, & Stone, 1984). In the experiments in which the training data is noise-free, no pruning was used. <p> We used two such steps, stopsplitting and rebalancing, in our experiments. We describe these steps and quantify their effects in this section. Stopsplitting was the most popular way of stopping tree growth prior to adopting pruning methods <ref> (Murthy, 1997) </ref>. The idea is very simple, to stop splitting nodes when a criterian is met. In our case, the criterian was simply a threshold on the number of instances at the node.
Reference: <author> Murthy, S. K., Kasif, S., & Salzberg, S. </author> <year> (1994). </year> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 1-33. </pages>
Reference-contexts: In the experiments using noisy training sets, we used Breiman et al.'s cost complexity pruning ((Breiman et al., 1984), Chapter 3) with the one standard error rule, reserving 10% of the training data as the pruning set. All experiments were done using the OC1 software <ref> (Murthy, Kasif, & Salzberg, 1994) </ref>.
Reference: <author> Murthy, S. K., & Salzberg, S. </author> <year> (1995a). </year> <title> Decision tree induction: How effective is the greedy heuristic?. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Discovery in Databases Montreal, </booktitle> <address> Canada. </address>
Reference-contexts: Murphy and Pazzani (Murphy & Pazzani, 1994) first used this style of experimentation to evaluate the Occam's Razor principle by constructing all decision trees consistent with a fixed concept. Portions of this paper have been reported at two conferences in the past <ref> (Murthy & Salzberg, 1995a, 1995b) </ref>. 3.
Reference: <author> Murthy, S. K., & Salzberg, S. </author> <year> (1995b). </year> <title> Lookahead and pathology in decision tree induction.. </title> <editor> In Mellish (Mellish, </editor> <year> 1995), </year> <pages> pp. 1025-1031. </pages>
Reference: <author> Mutchler, D. </author> <year> (1993). </year> <title> The multi-player version of minimax displays game pathology. </title> <journal> Artificial Intelligence, </journal> <volume> 64 (2), </volume> <pages> 323-336. </pages>
Reference-contexts: Greedy-Info considers a total of 1545 splits while inducing its tree, whereas Look-Info considers a total of 1,455,901 splits. actually produce an inferior program, both with two players (Nau, 1983) and with multiple players <ref> (Mutchler, 1993) </ref>. Decision trees, one can argue, are analogous to a one-player game tree. We illustrate the concept of pathology using a training set belonging to class C S .
Reference: <author> Nakamura, Y., Abe, S., Ohsawa, Y., & Sakauchi, M. </author> <year> (1993). </year> <title> A balanced hierarchical data structure for multidimensional data with highly efficient dynamic characteristics. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5 (4), </volume> <pages> 682-694. </pages>
Reference: <author> Nau, D. S. </author> <year> (1983). </year> <title> Decision quality as a function of search depth on game trees. </title> <journal> Journal of the Association of Computing Machinery, </journal> <volume> 30 (4), </volume> <pages> 687-708. </pages>
Reference-contexts: On the other hand, Look-Info induces a tree with size 10, maximum depth 4 and accuracy of 99.10%. Greedy-Info considers a total of 1545 splits while inducing its tree, whereas Look-Info considers a total of 1,455,901 splits. actually produce an inferior program, both with two players <ref> (Nau, 1983) </ref> and with multiple players (Mutchler, 1993). Decision trees, one can argue, are analogous to a one-player game tree. We illustrate the concept of pathology using a training set belonging to class C S .
Reference: <author> Norton, S. W. </author> <year> (1989). </year> <title> Generating better decision trees. </title> <editor> In Sridharan, N. S. (Ed.), </editor> <booktitle> IJCAI-89: Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 800-805. </pages> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA. </address>
Reference-contexts: However, Hartmann et al. do not demonstrate that lookahead yields any improvements over greedy search. The ideas in GOTA motivated Norton's IDX system <ref> (Norton, 1989) </ref>, which is a variant of ID3 (Quinlan, 1986) that performs lookahead. Norton conducted experiments on a voting records database (see Section 4.5) using ID3, IDX and GOTA, and found that lookahead reduced the average decision tree depth. <p> In addition to these six "difficult" domains, we also experimented with two variants of the congressional voting records data used by Norton <ref> (Norton, 1989) </ref> for his lookahead experiments. Brief descriptions of each of the real data sets we used are given below. BC Breast cancer recurrence data (UCI ML repository, a). Contains 286 instances, each described by 9 attributes and one class label. <p> LY Lymphography domain (UCI ML repository, c). Contains 148 instances, each described using 19 at tributes, including the class attribute. VO 1984 United States congressional voting records database. This data is used by Norton <ref> (Norton, 1989) </ref> in his experiments. The data contains 435 instances, each described by 16 nominal attributes and one class label. The task is to classify democrats from republicans on the basis of their voting records.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: A few details about our overall experimental procedure. A large number of goodness measures have been explored for test selection in tree induction (Murthy, 1997). We use in this paper two popular ones, namely, information gain <ref> (Quinlan, 1986) </ref> and gini index (Breiman, Friedman, Olshen, & Stone, 1984). In the experiments in which the training data is noise-free, no pruning was used. <p> However, Hartmann et al. do not demonstrate that lookahead yields any improvements over greedy search. The ideas in GOTA motivated Norton's IDX system (Norton, 1989), which is a variant of ID3 <ref> (Quinlan, 1986) </ref> that performs lookahead. Norton conducted experiments on a voting records database (see Section 4.5) using ID3, IDX and GOTA, and found that lookahead reduced the average decision tree depth. With a few exceptions, however, the advantages of lookahead were very small in Norton's experiments. <p> This uses the same set of candidate splits as Greedy. However, the goodness of a candidate split T is computed by examining all splits one level down from T . As we use two goodness measures, namely, gini index (Breiman et al., 1984) and information gain <ref> (Quinlan, 1986) </ref>, this gives four algorithms, which we name Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm (Breiman et al., 1984) and Greedy-Info to the ID3 algorithm (Quinlan, 1986). <p> As we use two goodness measures, namely, gini index (Breiman et al., 1984) and information gain <ref> (Quinlan, 1986) </ref>, this gives four algorithms, which we name Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm (Breiman et al., 1984) and Greedy-Info to the ID3 algorithm (Quinlan, 1986). For the experiments in this section, we use only three of the six tree quality measures described in Section 1, namely, accuracy, size, maximum depth. <p> We use a large number of carefully designed synthetic data sets as well as some difficult real-world data sets for our experiments. The greedy tree induction methods used are very similar to CART (Breiman et al., 1984) and ID3 <ref> (Quinlan, 1986) </ref>.
Reference: <author> Quinlan, J. R., & Cameroon-Jones, R. M. </author> <year> (1995). </year> <note> Oversearching and layered-search in empirical learning.. </note>
Reference-contexts: They suggest that this problem can be alleviated by using a more elaborate attribute weighting criterian (Kira & Rendell, 1992) in conjunction with the regular greedy algorithm. Kroger (Kroger, 1996) distributes a software program for optimizing decision trees in the medical domain. Quinlan and Jones <ref> (Quinlan & Cameroon-Jones, 1995) </ref> recently coined the term "oversearch-ing" to describe the perils of excessive searching in classifier induction. Through a series of experiments in real and synthetic data sets, they show that excessive searching can lead to inferior results in the context of rule induction. <p> We only considered one-level lookahead in this paper. Experiments with deeper levels of lookahead may be interesting, but we suspect the findings (e.g., pathology) would be similar. Recent work analyzing oversearch in the context of rule induction <ref> (Quinlan & Cameroon-Jones, 1995) </ref> shows that even at search depths close to exhaustive search, one can find solutions inferior than what greedy search finds. It is intriguing that limited lookahead search can produce inferior decision trees as compared to greedy search.
Reference: <editor> In Mellish (Mellish, </editor> <year> 1995), </year> <pages> pp. 1019-1024. </pages>
Reference-contexts: They suggest that this problem can be alleviated by using a more elaborate attribute weighting criterian (Kira & Rendell, 1992) in conjunction with the regular greedy algorithm. Kroger (Kroger, 1996) distributes a software program for optimizing decision trees in the medical domain. Quinlan and Jones <ref> (Quinlan & Cameroon-Jones, 1995) </ref> recently coined the term "oversearch-ing" to describe the perils of excessive searching in classifier induction. Through a series of experiments in real and synthetic data sets, they show that excessive searching can lead to inferior results in the context of rule induction. <p> We only considered one-level lookahead in this paper. Experiments with deeper levels of lookahead may be interesting, but we suspect the findings (e.g., pathology) would be similar. Recent work analyzing oversearch in the context of rule induction <ref> (Quinlan & Cameroon-Jones, 1995) </ref> shows that even at search depths close to exhaustive search, one can find solutions inferior than what greedy search finds. It is intriguing that limited lookahead search can produce inferior decision trees as compared to greedy search.
Reference: <author> Ragavan, H., & Rendell, L. </author> <year> (1993). </year> <title> Lookahead feature construction for learning hard concepts. </title> <editor> In Utgoff, P. E. (Ed.), MLC-93: </editor> <booktitle> Machine Learning. Proceedings of the Tenth International Conference, </booktitle> <pages> pp. </pages> <institution> 252-259 University of Massachusetts, </institution> <address> Amherst, MA. </address> <publisher> Morgan Kaufmann Publishers Inc. </publisher>
Reference-contexts: With a few exceptions, however, the advantages of lookahead were very small in Norton's experiments. Moreover, since this study only considered a single data set, it is not clear how these results generalize to other domains. Ragavan and Rendell's Lookahead Feature Construction (LFC) algorithm <ref> (Ragavan & Rendell, 1993) </ref> uses lookahead to construct composite Boolean features and uses the constructed features to induce decision trees. This method is more efficient than methods like IDX because it caches the features found while looking ahead.
Reference: <author> Rendell, L., & Ragavan, H. </author> <year> (1993). </year> <title> Improving the design of induction methods by analyzing algorithm functionality and data-based concept complexity. </title> <editor> In Bajcsy, R. (Ed.), </editor> <booktitle> IJCAI-93: Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <volume> Vol. 2, </volume> <pages> pp. </pages> <address> 952-958 Chambery, France. </address> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, CA. </address>
Reference-contexts: With a few exceptions, however, the advantages of lookahead were very small in Norton's experiments. Moreover, since this study only considered a single data set, it is not clear how these results generalize to other domains. Ragavan and Rendell's Lookahead Feature Construction (LFC) algorithm <ref> (Ragavan & Rendell, 1993) </ref> uses lookahead to construct composite Boolean features and uses the constructed features to induce decision trees. This method is more efficient than methods like IDX because it caches the features found while looking ahead.
Reference: <author> Risannen, J. </author> <year> (1989). </year> <title> Stochastic Complexity in Statistica Enquiry. </title> <publisher> World Scientific. </publisher>
Reference-contexts: According to (Saia, 1997), oversearching occurs if and only if there is a certain type of anamoly in the scoring function used for hypothesis selection, caused by inaccurate or oversimplified priors. Techniques to avoid overfitting, such as pruning or Minimum Description Length <ref> (Risannen, 1989) </ref>, do not reduce oversearching because they do not give priors which are accurate enough. Saia suggests a Monte Carlo empirical technique to estimate priors more effectively in the context of rule learning. The style of empirical investigation used in this paper needs a mention.
Reference: <author> Riskin, E. A., & Gray, R. M. </author> <year> (1991). </year> <title> Lookahead in growing tree-structured vector quantizers. </title> <booktitle> In ICASSP 91: International Conference on Accoustics, Speech and Signal Processing, </booktitle> <volume> Vol. 4, </volume> <pages> pp. </pages> <address> 2289-2292 Toronto, Ontario. </address> <publisher> IEEE. </publisher>
Reference-contexts: Tree construction using partial or exhaustive lookahead has been considered in statistics (Fielding, 1977; Elder, 1995), in pattern recognition (Hartmann, Varshney, Mehrotra, & Gerberich, 1982), for tree structured vector quantizers <ref> (Riskin & Gray, 1991) </ref>, for Bayesian class probability trees (Buntine, 1992), for neural trees and in machine learning (Hunt, Marin, & Stone, 1966; Nor-ton, 1989; Ragavan & Rendell, 1993; Murthy & Salzberg, 1995b).
Reference: <author> Rose, K., Miller, D., & Gersho, A. </author> <year> (1994). </year> <title> Entropy-constrained tree-structured vector quantizer design by the minimum cross entropy principle. </title> <booktitle> In Proceedings of the Data Compression Conference. </booktitle> <address> Snowbird, UT, USA. </address>
Reference-contexts: These methods search the space of all decision trees using random perturbations, additions and deletions of the splits. Deterministic procedures for searching for optimal trees have been suggested in sequential fault diagnosis (Sun, Qiu, & Cox, 1995) and vector quantization <ref> (Rose, Miller, & Gersho, 1994) </ref>. Recently, Kononenko et al.(Kononenko, Simec, & Robnik-Sikonja, 1997) pointed out that commonly used goodness measures assume that one attribute's interactions with the class label are independent of another's, which results in greedy induction producing suboptimal trees.
Reference: <author> Sahni, S. </author> <year> (1975). </year> <title> Approximate algorithms for the 0/1 knapsack problem. </title> <journal> Journal of the ACM, </journal> <volume> 22, </volume> <pages> 115-124. </pages>
Reference-contexts: Greedy search is used as a heuristic for a number of well-known NP-hard problems. Examples are the 0/1 knapsack problem <ref> (Sahni, 1975) </ref>, multiprocessor scheduling (Horowitz & Sahni, 1984) and, of course, decision tree induction. There exist a few theoretical analyses of the adequacy of greedy search for decision tree induction (Chou & Gray, 1986; Goodman & Smyth, 1988).
Reference: <author> Saia, J. </author> <year> (1997). </year> <type> Personal communication. </type> <note> See http://www.cs.washington.edu/homes/saia/aiPres/index.htm. </note>
Reference-contexts: Segal (Segal, 1996), however, claims that the oversearching effect is caused by overfitting and by a poor match between the evaluation function used for training and how rule performance is judged. According to <ref> (Saia, 1997) </ref>, oversearching occurs if and only if there is a certain type of anamoly in the scoring function used for hypothesis selection, caused by inaccurate or oversimplified priors.
Reference: <author> Sarkar, U. K., Chakrabarti, P. P., Ghose, S., & DeSarkar, S. C. </author> <year> (1994). </year> <title> Improving greedy algorithms by lookahead-search. </title> <journal> Journal of Algorithms, </journal> <volume> 16 (1), </volume> <pages> 1-23. </pages> <note> 32 Segal, </note> <author> R. </author> <year> (1996). </year> <title> An analysis of oversearch. Unplublished. email of the author: </title> <publisher> rsegal@watson.ibm.com. </publisher>
Reference-contexts: Can we consistently produce better trees by using extensions to the greedy heuristic that are not computationally prohibitive? Fixed depth lookahead is a standard way of improving upon greedy search <ref> (Sarkar, Chakrabarti, Ghose, & DeSarkar, 1994) </ref>. So we ask how trees induced using one-level lookahead search compare to greedily induced trees. To address the first question, we induce decision trees on thousands of synthetic data sets and compare them to the corresponding optimal trees. <p> Fixed-depth lookahead search is a standard technique for improving greedy algorithms <ref> (Sarkar et al., 1994) </ref>. Though scattered uses of lookahead for tree induction exist in the literature (Section 2), the advantages, or lack thereof, of lookahead search have yet not been systematically quantified. We attempt to quantify the effect of lookahead on decision tree induction in this section.
Reference: <author> Shlien, S. </author> <year> (1990). </year> <title> Multiple binary decision tree classifiers. </title> <journal> Pattern Recognition, </journal> <volume> 23 (7), </volume> <pages> 757-763. </pages>
Reference: <author> Shlien, S. </author> <year> (1992). </year> <title> Nonparametric classification using matched binary decision trees. </title> <journal> Pattern Recognition Letters, </journal> <volume> 13 (2), </volume> <pages> 83-88. </pages>
Reference: <author> Stout, Q. F., & Warren, B. L. </author> <year> (1986). </year> <title> Tree rebalancing in optimal time and space. </title> <journal> Communications of the ACM, </journal> <volume> 29 (9), </volume> <pages> 902-908. </pages>
Reference: <author> Sun, X., Qiu, Y., & Cox, L. A. </author> <year> (1995). </year> <title> A hill-climbing approach to construct near-optimal decision trees.. </title>
Reference-contexts: These methods search the space of all decision trees using random perturbations, additions and deletions of the splits. Deterministic procedures for searching for optimal trees have been suggested in sequential fault diagnosis <ref> (Sun, Qiu, & Cox, 1995) </ref> and vector quantization (Rose, Miller, & Gersho, 1994). Recently, Kononenko et al.(Kononenko, Simec, & Robnik-Sikonja, 1997) pointed out that commonly used goodness measures assume that one attribute's interactions with the class label are independent of another's, which results in greedy induction producing suboptimal trees.
Reference: <author> In Fisher (Fisher, </author> <year> 1995), </year> <pages> pp. 513-519. </pages>
Reference-contexts: They suggest that this problem can be alleviated by using a more elaborate attribute weighting criterian (Kira & Rendell, 1992) in conjunction with the regular greedy algorithm. Kroger (Kroger, 1996) distributes a software program for optimizing decision trees in the medical domain. Quinlan and Jones <ref> (Quinlan & Cameroon-Jones, 1995) </ref> recently coined the term "oversearch-ing" to describe the perils of excessive searching in classifier induction. Through a series of experiments in real and synthetic data sets, they show that excessive searching can lead to inferior results in the context of rule induction. <p> We only considered one-level lookahead in this paper. Experiments with deeper levels of lookahead may be interesting, but we suspect the findings (e.g., pathology) would be similar. Recent work analyzing oversearch in the context of rule induction <ref> (Quinlan & Cameroon-Jones, 1995) </ref> shows that even at search depths close to exhaustive search, one can find solutions inferior than what greedy search finds. It is intriguing that limited lookahead search can produce inferior decision trees as compared to greedy search.
Reference: <author> Tu, P.-L., & Chung, J.-Y. </author> <year> (1992). </year> <title> A new decision-tree classification algorithm for machine learning. </title> <booktitle> In Proceedings of the IEEE International Conference on Tools with AI, </booktitle> <pages> pp. </pages> <address> 370-377 Arlington, </address> <institution> Virginia. UCI ML repository. Breast cancer data.. Obtained from the University Medical Centre, Institute of Oncology, Ljubljana, </institution> <month> Yugoslavia. </month> <title> Data provided by Matjaz Zwitter and Milan Soklic. UCI ML repository. Cleveland heart disease database.. Collected by Roberto Detrano, M.D., </title> <type> Ph.D., </type> <institution> V.A. Medical center, Long Beach and Cleveland Clinic Foundation. UCI ML repository. Lymphography data.. Obtained from the University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia. Data provided by Matjaz Zwitter and Milan Soklic. </institution>
Reference-contexts: The problem of constructing the smallest decision tree which best distinguishes characteristics of multiple distinct groups is shown to be NP-complete in <ref> (Tu & Chung, 1992) </ref>. The fact that optimal decision tree construction is intractable implies, among other things, that greedy polynomial algorithms produce necessarily suboptimal solutions, assuming P 6= N P .
Reference: <author> Wang, Q. R., & Suen, C. Y. </author> <year> (1987). </year> <title> Large tree classifier with heuristic search and global training. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-9 (1), </volume> <pages> 91-102. </pages>
Reference-contexts: In the first stage, a sufficient 4 partitioning is induced using any reasonably good (greedy) method. In the second stage, the tree is refined to be as close to optimal as possible. Refinement techniques attempted include dynamic programming (Meisel & Michalopoulos, 1973), fuzzy logic search <ref> (Wang & Suen, 1987) </ref> and multi-linear programming (Bennett, 1994). The build-and-refine strategy can be seen as a search through the space of all possible decision trees, starting at the greedily built suboptimal tree.
Reference: <author> Zanakis, S. H., Evans, J. R., & Vazacopoulos, A. </author> <year> (1989). </year> <title> Heuristic methods and applications: a categorised survey. </title> <journal> European journal of operations research, </journal> <volume> 43, </volume> <pages> 88-110. 33 </pages>
Reference-contexts: test; after a full tree is grown, prune it back to avoid overfitting This kind of approach, in which solution to a problem is built up stage by stage through a sequence of progressively more complex partial solutions, is referred to as the construction heuristic in the combinatorial optimization literature <ref> (Zanakis, Evans, & Vazacopoulos, 1989) </ref>. Many construction heuristics use a greedy approach. A greedy algorithm makes the choice that looks best at the moment, and does not return later to reconsider that choice.
References-found: 69

