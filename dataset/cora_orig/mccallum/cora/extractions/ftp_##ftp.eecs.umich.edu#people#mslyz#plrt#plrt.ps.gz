URL: ftp://ftp.eecs.umich.edu/people/mslyz/plrt/plrt.ps.gz
Refering-URL: http://www.eecs.umich.edu/~mslyz/
Root-URL: http://www.cs.umich.edu
Email: mslyz@eecs.umich.edu  
Title: Piecewise Linear Tree-Structured Models This is a revision of an article in Proc. Data Compression
Author: Conference, J. Storer and M. Cohen eds., Marko J. Slyz David L. Neuhoff 
Address: Ann Arbor, MI 48109-2122 USA  
Date: April 1996.  February 16, 1996  
Note: IEEE Computer Society Press,  
Affiliation: Snowbird, Utah,  Department of Electrical Engineering and Computer Science University of Michigan,  
Abstract: This paper presents a probabilistic model for use in lossless image compressors. For each pixel the model provides a conditional distribution, which in the most simple case is a discretized gaussian. The mean and variance of this gaussian are determined by using that pixel's neighbors to search a tree to find an autoregressive model, which is then applied to those same neighbors. Finally, an arithmetic coder transmits the pixel. This paper also shows how to design this tree and find the distribution parameters associated with each leaf. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, </author> <title> "Hinging Hyperplanes for Regression, Classification, and Function Approximation", </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39:3, </volume> <pages> pp. 999-1013, </pages> <year> 1993. </year>
Reference-contexts: These are then used in an AR model in the usual way to generate the distribution of X k . Such models have been studied for various S's by Tong in [14]. Similar models are also discussed by Breiman in <ref> [1] </ref> and Michel and Hero in [7]. This paper proposes a class of S's, and a method for searching this class for the S that best fits a given set of training data.
Reference: [2] <author> D. Clark and R. Gonzalez, </author> <title> "Optimal Solution Of Linear Inequalities With Applications To Pattern Recognition", </title> <journal> IEEE Transactions on Pattern Analysis And Machine Intelligence, </journal> <volume> 3:6, </volume> <pages> pp. 643-655, </pages> <year> 1981. </year>
Reference-contexts: Appendix: Solving the LMIW Problem Clark and Gonzalez (CG) <ref> [2] </ref> solve the LMIW problem (10) for the special case of h i;1 = 1 and h i;2 = 0 8i. Almost no changes are needed to generalize [2]'s Lemmas 1 and 2 so that they describe the location of w's that are optimal in the sense of (10); simply replace <p> Of the two further speed-ups proposed in <ref> [2] </ref>, the "replacement and deletion rule" works well while the "ordering rule" breaks for non-Haar data. The implementation of the CG algorithm used here is based on the linear algebra routines in [13]. It incorporates two approximations that greatly speed up the algorithm.
Reference: [3] <author> T. Cover and J. Thomas, </author> <title> Elements of Information Theory, </title> <address> New York, </address> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: and the parameters associated with the tree's leaves. 3 The Model Fitting Algorithm Given some conditional pmf, fi (z j+1 jz j 1 ), an arithmetic coder can represent an image (X 1 ; : : : ; X N ) using approximately N i=1 j bits per pixel (bpp) <ref> [3, Section 5.10] </ref>. fi is not known a priori. To choose it the following optimality criterion will be used: pick fi so that (5) is small on some training set of data.
Reference: [4] <author> A. Gersho and R. Gray, </author> <title> Vector Quantization and Signal Compression, </title> <address> Boston, </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Parametric models for sampled data are often chosen from the class of autoregressive (AR) random processes <ref> [4, Section 2.8] </ref>: X k = a X k1 where * X 1 ; X 2 ; : : :, is a finite sequence of discrete random variables representing the pixel values, * X k1 km = (X k1 ; : : : ; X km ) is the prediction context <p> This paper proposes a class of S's, and a method for searching this class for the S that best fits a given set of training data. Specifically, the S's used here are based on a generalized version of a Tree Structured Vector Quantizer (TSVQ) <ref> [4, Section 12.4] </ref> [10] as follows: S is stored as a binary tree with a q 1 dimensional hyperplane in each internal node, and an AR parameter vector in each leaf. To evaluate S, start by comparing X k1 kq with the root's hyperplane.
Reference: [5] <author> J. Goldschneider, </author> <month> May </month> <year> 1994, </year> <month> ftp://isdl.ee.washington.edu/pub/VQ/code/stdvq/voronoi_fs.c </month>
Reference-contexts: This was drawn using the algorithm from <ref> [5] </ref>. The trees produced by the training program are read by a coder program, which also reads an input image in raster scan order. For each pixel it extracts the context vector and finds the leaf corresponding to it.
Reference: [6] <author> P. Howard, J. Vitter, </author> <title> "New Methods for Lossless Image Compression Using Arithmetic Coding", </title> <journal> Inform. Proc. & Management, </journal> <volume> vol. 28, no. 5, </volume> <pages> pp. 765-79, </pages> <year> 1992. </year>
Reference-contexts: Let the AR parameters in this leaf be denoted by (a i ; i ). Next i is scalar quantized by a function Q ( i ) : &lt; ! f1; 2; : : : ; 37g to one of the standard deviations derived in <ref> [6] </ref>.
Reference: [7] <author> O. Michel and A. O. Hero, </author> <title> "Tree Structured Non-linear Signal Modeling and Prediction", </title> <booktitle> Proc. of the IEEE 1995 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> Detroit, </address> <pages> pp. 1689-1692. </pages>
Reference-contexts: These are then used in an AR model in the usual way to generate the distribution of X k . Such models have been studied for various S's by Tong in [14]. Similar models are also discussed by Breiman in [1] and Michel and Hero in <ref> [7] </ref>. This paper proposes a class of S's, and a method for searching this class for the S that best fits a given set of training data.
Reference: [8] <author> A. Miyake, </author> <title> "On the Haar Condition in Algorithms for the Optimal Solution of Linear Inequalities", </title> <editor> J. Kittler, K.S. Fu, and L.F. Pau eds., </editor> <booktitle> Pattern Recognition Theory and Applications, </booktitle> <address> D. </address> <publisher> Reidel, Dordrecht, </publisher> <pages> pp. 35-42. </pages>
Reference-contexts: One problem is that fC (v 1 ); : : : ; C (v N c )g does not generally satisfy the Haar condition, which is a prerequisite for their algorithm. <ref> [8] </ref> circumvents this, but it's possible to verify with exhaustive search that for this data the basic CG algorithm gives very nearly the optimal w's even though the data is not Haar.
Reference: [9] <author> A. Moffat, R. Neal, I.H. Witten, </author> <title> "Arithmetic Coding Revisited", </title> <booktitle> Proc. Data Compression Conference, </booktitle> <editor> J. Storer and M. Cohen, eds., </editor> <address> Snowbird, Utah, </address> <publisher> IEEE Computer Society Press, </publisher> <month> March </month> <year> 1995, </year> <pages> pp. 202-211. 10 </pages>
Reference-contexts: The arithmetic coder from <ref> [9] </ref> was used. It was slightly modified to assign nonzero counts to only the 256 values of (X k round (a i X k1 km )) that are possible when round (a i X k1 km ) takes on a given value.
Reference: [10] <author> A. Rao, D. Miller, K. Rose, A. Gersho, </author> <title> "Generalized Vector Quantization: Jointly Optimal Quantization and Estimation", </title> <booktitle> Proc. 1995 IEEE International Symposium on Information Theory, </booktitle> <address> Whistler, B.C., </address> <month> Sept. </month> <year> 1995, </year> <note> p. 432. </note>
Reference-contexts: This paper proposes a class of S's, and a method for searching this class for the S that best fits a given set of training data. Specifically, the S's used here are based on a generalized version of a Tree Structured Vector Quantizer (TSVQ) [4, Section 12.4] <ref> [10] </ref> as follows: S is stored as a binary tree with a q 1 dimensional hyperplane in each internal node, and an AR parameter vector in each leaf. To evaluate S, start by comparing X k1 kq with the root's hyperplane.
Reference: [11] <author> M. Rabani and P. Jones, </author> <title> Digital Image Compression Techniques, </title> <type> Bellingham, Wash., </type> <institution> SPIE Optical Engineering Press, </institution> <year> 1991. </year>
Reference-contexts: Also, to simplify the discussion, all formulas will assume one-dimensional data. The extension to two dimensions only requires fancier subscripts. When using this model for statistical image coding, the parameters (a; ) are either fixed in advance or adapted to the image being encoded <ref> [11, Chapter 9] </ref>. For example, a and might be fitted with least squares to some training region (X km1 ; : : : ; X kmM ) where M m. AR models are practical and effective.
Reference: [12] <author> J. Robinson, </author> <title> "Binary Tree Predictive Coding", </title> <type> Version 3, </type> <address> http://monet.uwaterloo.ca/~john/btpc.html, Sept. </address> <year> 1995. </year>
Reference-contexts: The fitting took about 3.5 hours, which is acceptable since it is a one-time cost after which the tree can be used for many pictures. All times are for a Sparcstation 20. The last two columns of Table 1 give some results from Robinson's BTPC algorithm <ref> [12] </ref> for comparison. Please note that BTPC is optimized for doing both lossy and lossless compression. It is clear that the current PLRT-based coder achieves lower rate at the cost of higher complexity.
Reference: [13] <author> D.E. Stewart and Z. Leyk, Meschach: </author> <title> Matrix Computations in C, Centre for Mathematics and its Applications, </title> <booktitle> Australian National University, Canberra, Proceedings of the CMA, </booktitle> <volume> #32, </volume> <year> 1994. </year>
Reference-contexts: Of the two further speed-ups proposed in [2], the "replacement and deletion rule" works well while the "ordering rule" breaks for non-Haar data. The implementation of the CG algorithm used here is based on the linear algebra routines in <ref> [13] </ref>. It incorporates two approximations that greatly speed up the algorithm. The first arose from the observation that, for the standard CG algorithm, the decrease in (10) is exponential in time.
Reference: [14] <author> H. Tong, </author> <title> Non-linear Time Series: a Dynamical System Approach, </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: These are then used in an AR model in the usual way to generate the distribution of X k . Such models have been studied for various S's by Tong in <ref> [14] </ref>. Similar models are also discussed by Breiman in [1] and Michel and Hero in [7]. This paper proposes a class of S's, and a method for searching this class for the S that best fits a given set of training data.
Reference: [15] <institution> University of East Anglia Signal and Image Processing Group, "Standard Image Page", </institution> <note> http://www.sys.uea.ac.uk/Research/ResGroups/SIP/images_ftp/index.html 11 </note>
Reference-contexts: 0.0015 4.39 Table 2: This shows the effect on rate of varying L (really only the stopping threshold is varied since there is no direct control over L). m is fixed at 13, while q is fixed at 4. to about 1.8 million vectors extracted from the following pictures in <ref> [15] </ref>: goldhill-720x576, woman2-512x512, bridge512x512, crowd512x512, teapot340x340, harbour-512x512, and woman1-512x512. The fitting took about 3.5 hours, which is acceptable since it is a one-time cost after which the tree can be used for many pictures. All times are for a Sparcstation 20.
References-found: 15

