URL: ftp://ftp.cs.pitt.edu/panos/PMDBnet/ic3n_95.ps.gz
Refering-URL: http://www.cs.pitt.edu/admt/Publications/conf._and_workshops.html
Root-URL: 
Title: Data Sharing and Recovery in Gigabit-Networked Databases  
Author: Sujata Banerjee Panos K. Chrysanthis 
Address: Pittsburgh Pittsburgh, PA 15260 Pittsburgh, PA 15260  
Affiliation: Telecommunications Program Computer Science Department University of Pittsburgh University of  
Abstract: Major advances in optical fiber transmission and switching technology have enabled the development of very high speed networks with data rates of the order of gigabits per second. It is anticipated that in the future, wide area gigabit networks will interconnect database servers around the globe creating extremely powerful distributed information systems. In this paper, we examine the implications of such a high speed network on data access and sharing techniques and propose a lock-based concurrency control protocol and a log-based recovery protocol that ensures data consistency in gigabit-networked databases. Both protocols exploit the characteristics of a gigabit network to enhance the performance of the database system and, in particular, the fact that the size of the message is less of a concern than the number of sequential phases of message passing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Ramaswami, </author> <title> "Multiwavelength Lightwave Net works for Computer Communication," </title> <journal> IEEE Communications, </journal> <volume> vol. 31, </volume> <pages> pp. 78-88, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: 1 Introduction The evolution of very high speed networks is prompting research in many areas, including that of distributed database systems of the future. These networks will have speeds of the order of Gigabits per second, and may even increase to Terabits per second someday <ref> [1] </ref>. It is anticipated that in the future, wide area gigabit networks will interconnect database servers to clients around the globe creating extremely powerful distributed information systems. We refer to these as gigabit-networked databases (GNDB).
Reference: [2] <author> L. Kleinrock, </author> <title> "The Latency/Bandwidth Tradeoff in Gigabit Networks," </title> <journal> IEEE Communications, </journal> <volume> vol. 30, </volume> <pages> pp. 36-40, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: At gigabit rates, the propagation latency is the dominant component of the overall delay <ref> [2] </ref>. For example, the propagation delay across the United States (at the speed of light) is 20 milliseconds. At a network speed of 1 Mbps, the transmission delay for a 1 Mb file is 1 second, and the ratio of the propagation delay to the transmission delay is 0.02.
Reference: [3] <author> C. Partridge, </author> <title> "Protocols for High Speed Networks: Some questions and a few answers," </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> vol. 25, </volume> <pages> pp. 1019-1028, </pages> <month> April </month> <year> 1993. </year>
Reference: [4] <author> C. Partridge, </author> <title> Gigabit Networking. Professional Com puting, </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference: [5] <author> J. D. Touch and D. J. Farber, </author> <title> "The Effect of Latency on Protocols," </title> <booktitle> Proceedings of the ACM SIGCOMM, </booktitle> <year> 1994. </year>
Reference: [6] <author> S. Banerjee, V.O.K. Li, and C. Wang, </author> <title> "Distributed Database Systems in High-Speed Wide-Area Networks," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, </volume> <pages> pp. 617-630, </pages> <year> 1993. </year>
Reference-contexts: This scheme has a similar flavor to the send-on-demand scheme proposed in <ref> [6] </ref>. However, in that scheme, the data items were migrated from site to site according to the demand generated. With the location change, the ownership of the data item is transferred to the new site, thus making the recovery mechanism difficult (owing to the distribution of the log records).
Reference: [7] <author> S. Banerjee and V.O.K. Li, </author> <title> "Application Protocols for Wide Area Gigabit Networks," </title> <booktitle> Proccedings of the Gigabit Networking Workshop, </booktitle> <year> 1994. </year>
Reference: [8] <author> M. Carey, M. Franklin, M. Livny, and E. Shekita., </author> <title> "Data Caching Tradeoffs in Client-Server DBMS Architectures," </title> <booktitle> Proceedings of the ACM SIGMOD, </booktitle> <pages> pp. 357-366, </pages> <year> 1991. </year>
Reference: [9] <author> Y. Wang and L. Rowe., </author> <title> "Cache Consistency and Con currency Control in a Client/server DBMS Architecture," </title> <booktitle> Proceedings of the ACM SIGMOD, </booktitle> <pages> pp. 367-376, </pages> <year> 1991. </year>
Reference: [10] <author> K. P. Eswaran, J. Gray, R. Lorie, and I. Traiger, </author> <title> "The Notion of Consistency and Predicate Locks in a Database System," </title> <journal> Communications of the ACM, </journal> <volume> vol. 19, </volume> <pages> pp. 624-633, </pages> <month> November </month> <year> 1976. </year>
Reference-contexts: This means that clients and servers must handle in a coordinated manner the effects of concurrency and failures which are the two basic sources of data inconsistencies. In this paper, we propose a lock-based concurrency control protocol, a variant of the strict two-phase locking <ref> [10] </ref>, and a log-based recovery protocol [11] that ensures reliability in such a client-server database environment. Distributed concurrency control and recovery algorithms typically require sites to engage in conversations (sequential message transfers involving round-trip propagation delays). <p> Let us also assume that a client executes one transaction at a time. In the presence of concurrent requests from different clients, the DB server preserves data consistency by following the strict two-phase locking protocol (2PL) <ref> [10] </ref>, the most commonly used concurrency control mechanism. The 2PL protocol ensures data consistency as defined by serializability which requires the concurrent, interleaved, execution of requests to be equivalent to some serial, non-interleaved, execution of the same requests [11]. The 2PL protocol executes each transaction in two phases.
Reference: [11] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman, </author> <title> Concurrency Control and Recovery in Database Systems. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: In this paper, we propose a lock-based concurrency control protocol, a variant of the strict two-phase locking [10], and a log-based recovery protocol <ref> [11] </ref> that ensures reliability in such a client-server database environment. Distributed concurrency control and recovery algorithms typically require sites to engage in conversations (sequential message transfers involving round-trip propagation delays). <p> The 2PL protocol ensures data consistency as defined by serializability which requires the concurrent, interleaved, execution of requests to be equivalent to some serial, non-interleaved, execution of the same requests <ref> [11] </ref>. The 2PL protocol executes each transaction in two phases. A transaction can access a data item only if no other transaction has a lock on it 2 . <p> Although this enables C i to execute concurrently with the reading clients, C i cannot release its updates until it receives a release message from all the reading clients. Here is interesting to point out that the protocol just described behaves similar to the two-copy version 2PL protocol <ref> [11] </ref> which allows more concurrency than the standard 2PL protocol. As before, if there are no waiting transactions that need exclusive access, the release messages are returned to the server. If there are n clients reading a single data item, 3n messages in 3 rounds will be required. <p> In the next section, we will discuss how careful construction of the forward list potentially reduces the number of deadlocks. In general, we assume that the server detects deadlocks by maintaining a wait-for graph and checking for cycles in the graph <ref> [11] </ref>. 3.2 Creating the Forward List For each data item, in each window, a forward list is created during the time period that the requests from the previous window are being served. This is basically performed in two steps. <p> The main problem stems from the need to ensure that every successor site of a transaction comes to the same decision regarding the transaction, viz., the transaction is committed or aborted. In the following, we propose an atomic commit protocol <ref> [11] </ref> that allows the set of successors to reach a consistent decision, and gain access to the correct data. When a predecessor transaction of an unreliable site commits, it sends the data item to its successor site as well as to the successor of its successor for that data item.
Reference: [12] <author> W. </author> <title> Stallings, </title> <journal> Data and Computer Communications. </journal> <volume> Macmilan, </volume> <year> 1991. </year>
Reference-contexts: At 1 Gbps, the same ratio is computed to be 20, a 1000 fold increase over the previous value. Most existing protocols do not exhibit scalable performance over such a wide range of variation of this ratio <ref> [12] </ref>. Thus, the problem of propagation latencies actually gets worse as the data rate increases. This basic characteristic of high speed networks (also referred to as a high bandwidth-delay product) has significant implications on distributed applications.
Reference: [13] <author> M. Stonebraker, P. M. Aoki, R. Devine, W. Litwin, and M. Olson, "Mariposa: </author> <title> A New Architecture for Distributed Data," </title> <booktitle> Proceedings of International Conference on Data Engineering, </booktitle> <pages> pp. 54-65, </pages> <year> 1994. </year>
Reference-contexts: The scheme proposed in this paper differs in that each data item is owned by a specific DB server. In the Mariposa database system <ref> [13] </ref>, one of the performance-enhancing criteria proposed was that the ownership of data items should not be fixed, and it was acknowledged that in doing so, the recovery operation would become very difficult. <p> In the sequel, we consider such a recovery scheme that attempts to reduce the overall cost of recovery by distinguishing between reliable and unreliable sites. 4.1 An Adaptive Recovery Scheme Recovery is very difficult in a situation where data items may migrate from site to site <ref> [13, 14] </ref>. The future high speed networking environment will provide quality of service (QoS) guarantees, including high network reliability. Thus, the probability of network partitioning and link failures will be relatively low, and only site failures need to be considered.
Reference: [14] <author> S. Banerjee, </author> <title> Distributed Database Systems in High Speed Networks. </title> <type> PhD thesis, </type> <institution> University of Southern California, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: In the sequel, we consider such a recovery scheme that attempts to reduce the overall cost of recovery by distinguishing between reliable and unreliable sites. 4.1 An Adaptive Recovery Scheme Recovery is very difficult in a situation where data items may migrate from site to site <ref> [13, 14] </ref>. The future high speed networking environment will provide quality of service (QoS) guarantees, including high network reliability. Thus, the probability of network partitioning and link failures will be relatively low, and only site failures need to be considered.
Reference: [15] <author> C. Mohan, D. Haderle, B. Lindsay, H. Pirahesh, and P. Schwarz, </author> <title> "ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging," </title> <journal> ACM Transactions on Database Systems, </journal> <volume> vol. 17, </volume> <pages> pp. 94-162, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: A. Reliable Sites: Since reliable sites are expected to be able to recover from failures relatively quickly, reliable sites are assumed to support stable storage, combined with an efficient write-ahead-logging (WAL) scheme, e.g., Aries <ref> [15] </ref>. When a reliable site receives a data item, the site force-writes the data item along with its forward list to the stable storage, and then sends an acknowledgement message to its predecessor.
References-found: 15

