URL: http://robotics.stanford.edu/users/ronnyk/wrappers.ps.Z
Refering-URL: http://robotics.stanford.edu/users/ronnyk/
Root-URL: 
Email: ronnyk@sgi.com  gjohn@CS.Stanford.EDU  
Title: AIJ special issue on relevance Wrappers for Feature Subset Selection  
Author: Ron Kohavi George H. John 
Date: May 20, 1997  
Web: http://robotics.stanford.edu/~fronnyk,gjohng  
Address: 2011 N. Shoreline Boulevard Mountain View, CA 94043  650 Harry Rd. San Jose, CA 95120  
Affiliation: Data Mining and Visualization Silicon Graphics, Inc.  Global Business Intelligence Solutions IBM Almaden Research Center  
Abstract: In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1992), </year> <title> "Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms", </title> <journal> International Journal of Man-Machine Studies 36(1), </journal> <pages> pp. 267-287. </pages>
Reference: <author> Aha, D. W. & Bankert, R. L. </author> <year> (1994), </year> <title> Feature selection for case-based classification of cloud types: An empirical comparison, </title> <booktitle> in Working Notes of the AAAI-94 Workshop on Case-Based Reasoning, </booktitle> <pages> pp. 106-112. </pages>
Reference: <author> Aha, D. W. & Bankert, R. L. </author> <year> (1995), </year> <title> A comparative evaluation of sequential feature selection algorithms, </title> <editor> in D. Fisher & H. Lenz, eds, </editor> <booktitle> Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, </address> <pages> pp. 1-7. </pages>
Reference: <author> Aha, D. W., Kibler, D. & Albert, M. K. </author> <year> (1991), </year> <title> "Instance-based learning algorithms", </title> <booktitle> Machine Learning 6(1), </booktitle> <pages> pp. 37-66. </pages>
Reference: <author> Almuallim, H. & Dietterich, T. G. </author> <year> (1991), </year> <title> Learning with many irrelevant features, </title> <booktitle> in Ninth National Conference on Artificial Intelligence, </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 547-552. </pages>
Reference-contexts: The main disadvantage of the filter approach is that it totally ignores the effects of the selected feature subset on the performance of the induction algorithm. We now review some existing algorithms that fall into the filter approach. 2.4.1 The FOCUS Algorithm The FOCUS algorithm <ref> (Almuallim & Dietterich 1991, Almuallim & Dietterich 1994) </ref>, originally defined for noise-free Boolean domains, exhaustively examines all subsets of features, selecting the minimal subset of 6 features that is sufficient to determine the label value for all instances in the training set.
Reference: <author> Almuallim, H. & Dietterich, T. G. </author> <year> (1994), </year> <title> "Learning boolean concepts in the presence of many irrelevant features", </title> <booktitle> Artificial Intelligence 69(1-2), </booktitle> <pages> pp. 279-306. </pages>
Reference: <author> Anderson, J. R. & Matessa, M. </author> <year> (1992), </year> <title> "Explorations of an incremental, Bayesian algorithm for categorization", </title> <booktitle> Machine Learning 9 pp. </booktitle> <pages> 275-308. </pages>
Reference: <author> Atkeson, C. G. </author> <year> (1991), </year> <title> Using locally weighted regression for robot learning, </title> <booktitle> in Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pp. 958-963. </pages>
Reference: <author> Bala, J., Jong, K. A. D., Haung, J., Vafaie, H. & Wechsler, H. </author> <year> (1995), </year> <title> Hybrid learning using genetic algorithms and decision trees for pattern classification, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 719-724. </pages>
Reference: <author> Ben-Bassat, M. </author> <year> (1982), </year> <title> Use of distance measures, information measures and error bounds in feature evaluation, </title> <editor> in P. R. Krishnaiah & L. N. Kanal, eds, </editor> <booktitle> Handbook of Statistics, </booktitle> <volume> Vol. 2, </volume> <publisher> North-Holland Publishing Company, </publisher> <pages> pp. 773-791. </pages>
Reference: <author> Berliner, H. </author> <year> (1981), </year> <title> The B fl tree search algorithm: A best-first proof procedure, </title> <editor> in B. Webber & N. Nilsson, eds, </editor> <booktitle> Readings in Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 79-87. </pages>
Reference-contexts: Models drop out of the race when the confidence interval of the accuracy does not overlap with the confidence interval of the accuracy of the best model (this is analogous to imposing a higher and lower bound on the estimation function in the B fl algorithm <ref> (Berliner 1981) </ref>). The race ends when there is a winner, or when all n steps in the leave-one-out cross-validation have been executed.
Reference: <author> Blum, A. L. & Rivest, R. L. </author> <year> (1992), </year> <title> "Training a 3-node neural network is NP-complete", </title> <booktitle> Neural Networks 5 pp. </booktitle> <pages> 117-127. </pages>
Reference: <author> Boddy, M. & Dean, T. </author> <year> (1989), </year> <title> Solving time-dependent planning problems, </title> <editor> in N. S. Sridharan, ed., </editor> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <volume> Vol. 2, </volume> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 979-984. </pages>
Reference-contexts: Best-first search usually terminates upon reaching the goal. Our problem is an optimization problem, so the search can be stopped at any point and the best solution found so far can be returned (theoretically improving over time), thus making it an anytime algorithm <ref> (Boddy & Dean 1989) </ref>. In practice, we must stop the run at some stage, and we use what we call a stale search: if we have not found an improved node in the last k expansions, we terminate the search.
Reference: <author> Brazdil, P., Gama, J. & Henery, B. </author> <year> (1994), </year> <title> Characterizing the applicability of classification algorithms using meta-level learning, </title> <editor> in F. Bergadano & L. D. Raedt, eds, </editor> <booktitle> Proceedings of the European Conference on Machine Learning. </booktitle>
Reference: <author> Breiman, L. </author> <year> (1996), </year> <title> "Bagging predictors", </title> <booktitle> Machine Learning 24 pp. </booktitle> <pages> 123-140. </pages>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: We examine two general approaches to feature subset selection: the filter approach and the wrapper approach, and we then investigate each in detail. 2.1 The Problem Practical machine learning algorithms, including top-down induction of decision tree algorithms such as ID3 (Quinlan 1986), C4.5 (Quinlan 1993), and CART <ref> (Breiman, Friedman, Olshen & Stone 1984) </ref>, and instance-based algorithms, such as IBL (Dasarathy 1990, Aha, Kibler & Albert 1991), are known to degrade in performance (prediction accuracy) when faced with many features that are not necessary for predicting the desired output. <p> After the test is chosen, the instances are split according to the test, and the subproblems are solved recursively. C4.5 uses gain ratio, a variant of mutual information, as the feature selection measure; other measures have been proposed, such as the Gini index <ref> (Breiman et al. 1984) </ref>, C-separators (Fayyad & Irani 1992), distance-based measures (De Mantaras 1991), and Relief (Kononenko 1995).
Reference: <author> Buntine, W. </author> <year> (1992), </year> <title> "Learning classification trees", </title> <journal> Statistics and Computing 2(2), </journal> <month> June, </month> <pages> pp. 63-73. </pages>
Reference: <author> Cardie, C. </author> <year> (1993), </year> <title> Using decision trees to improve case-based learning, </title> <booktitle> in Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 25-32. </pages>
Reference: <author> Caruana, R. & Freitag, D. </author> <year> (1994), </year> <title> Greedy attribute selection, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 28-36. </pages>
Reference: <author> Cestnik, B. </author> <year> (1990), </year> <title> Estimating probabilities: A crucial task in machine learning, </title> <editor> in L. C. Aiello, ed., </editor> <booktitle> Proceedings of the ninth European Conference on Artificial Intelligence, </booktitle> <pages> pp. 147-149. </pages> <note> 38 Cover, </note> <author> T. M. & Campenhout, J. M. V. </author> <year> (1977), </year> <title> "On the possible orderings in the measurement selection problem", </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics SMC-7(9), </journal> <pages> pp. 657-661. </pages>
Reference: <author> Dasarathy, B. V. </author> <year> (1990), </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California. </address>
Reference-contexts: the filter approach and the wrapper approach, and we then investigate each in detail. 2.1 The Problem Practical machine learning algorithms, including top-down induction of decision tree algorithms such as ID3 (Quinlan 1986), C4.5 (Quinlan 1993), and CART (Breiman, Friedman, Olshen & Stone 1984), and instance-based algorithms, such as IBL <ref> (Dasarathy 1990, Aha, Kibler & Albert 1991) </ref>, are known to degrade in performance (prediction accuracy) when faced with many features that are not necessary for predicting the desired output.
Reference: <author> De Mantaras, R. L. </author> <year> (1991), </year> <title> "A distance-based attribute selection measure for decision tree induction", </title> <booktitle> Machine Learning 6 pp. </booktitle> <pages> 81-92. </pages>
Reference-contexts: C4.5 uses gain ratio, a variant of mutual information, as the feature selection measure; other measures have been proposed, such as the Gini index (Breiman et al. 1984), C-separators (Fayyad & Irani 1992), distance-based measures <ref> (De Mantaras 1991) </ref>, and Relief (Kononenko 1995).
Reference: <author> Devijver, P. A. & Kittler, J. </author> <year> (1982), </year> <title> Pattern Recognition: A Statistical Approach, </title> <booktitle> Prentice-Hall International. </booktitle>
Reference-contexts: The term forward selection refers to a search that begins at the empty set of features; the term backward elimination refers to a search that begins at the full set of features <ref> (Devijver & Kittler 1982, 9 feature deleted or added. Miller 1990) </ref>. The initial state we use in most of our experiments is the empty set of features, hence we are using a forward selection approach. <p> Future work on the abstract problem presented above might improve the applicability of the wrapper method to even larger state spaces. 8 Related Work The pattern recognition literature <ref> (Devijver & Kittler 1982, Kittler 1986, Ben-Bassat 1982) </ref>, statistics literature (Draper & Smith 1981, Miller 1984, Miller 1990, Neter et al. 1990), and recent machine learning papers (Almuallim & Dietterich 1991, Almuallim & Dietterich 1994, Kira & Rendell 1992a, Kira & Rendell 1992b, Kononenko 1994) consist of many measures for feature
Reference: <author> Doak, J. </author> <year> (1992), </year> <title> An evaluation of feature selection methods and their application to computer security, </title> <type> Technical Report CSE-92-18, </type> <institution> University of California at Davis. </institution>
Reference: <author> Domingos, P. & Pazzani, M. </author> <year> (1996), </year> <title> Beyond independence: conditions for the optimality of the simple Bayesian classifier, </title> <editor> in L. Saitta, ed., </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 105-112. </pages>
Reference: <author> Dougherty, J., Kohavi, R. & Sahami, M. </author> <year> (1995), </year> <title> Supervised and unsupervised discretization of continuous features, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 194-202. </pages>
Reference: <author> Draper, N. R. & Smith, H. </author> <year> (1981), </year> <title> Applied Regression Analysis, 2nd edition, </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973), </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley. </publisher>
Reference: <author> Fayyad, U. M. </author> <year> (1991), </year> <title> On the induction of decision trees for multiple concept learning, </title> <type> PhD thesis, </type> <institution> EECS Dept, Michigan University. </institution>
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1992), </year> <title> The attribute selection problem in decision tree generation, </title> <booktitle> in Tenth National Conference on Artificial Intelligence, </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 104-110. </pages>
Reference-contexts: After the test is chosen, the instances are split according to the test, and the subproblems are solved recursively. C4.5 uses gain ratio, a variant of mutual information, as the feature selection measure; other measures have been proposed, such as the Gini index (Breiman et al. 1984), C-separators <ref> (Fayyad & Irani 1992) </ref>, distance-based measures (De Mantaras 1991), and Relief (Kononenko 1995).
Reference: <author> Fong, P. W. L. </author> <year> (1995), </year> <title> A quantitative study of hypothesis selection, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 226-234. </pages>
Reference: <author> Freund, Y. </author> <year> (1990), </year> <title> Boosting a weak learning algorithm by majority, </title> <booktitle> in Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pp. 202-216. </pages> <note> To appear in Information and Computation. </note>
Reference: <author> Freund, Y. & Schapire, R. E. </author> <year> (1995), </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting, </title> <booktitle> in Proceedings of the Second European Conference on Computational Learning Theory, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 23-37. </pages>
Reference: <author> Furnival, G. M. & Wilson, R. W. </author> <year> (1974), </year> <title> "Regression by leaps and bounds", </title> <type> Technometrics 16(4), </type> <pages> pp. 499-511. </pages>
Reference: <author> Geman, S., Bienenstock, E. & Doursat, R. </author> <year> (1992), </year> <title> "Neural networks and the bias/variance dilemma", </title> <booktitle> Neural Computation 4 pp. </booktitle> <pages> 1-48. </pages>
Reference-contexts: In practical learning scenarios, however, we are face with two problems: the learning algorithms are not given access to the underlying distribution, and most practical algorithms attempt to find a hypothesis by approximating NP-hard optimization problems. The first problem is closely related to the bias-variance tradeoff <ref> (Geman, Bienenstock & Doursat 1992, Kohavi & Wolpert 1996) </ref>: one must tradeoff estimation of more parameters (bias reduction) with accurately estimating these parameters (variance reduction). This problem is independent of the computational power available to the learner. <p> The following observations can be made: * For the real datasets and ID3, this simple version of feature subset selection provides a regularization mechanism, which reduces the variance of the algorithm <ref> (Geman et al. 1992, Kohavi & Wolpert 1996) </ref>. By hiding features from ID3, a smaller tree is grown. This type of regularization is different than pruning, which is another regularization method, because it is global: a feature is either present or absent, whereas pruning is a local operation. <p> In the following experiments, k was set to five and epsilon was 0.1%. While best-first search is a more thorough search technique, it is not obvious that it is better for feature subset selection. Because of the bias-variance tradeoff <ref> (Geman et al. 1992, Kohavi & Wolpert 1996) </ref>, it is possible that a more thorough search will increase variance and thus reduce accuracy. Quinlan (1995) and Murthy & Salzberg (1995) showed examples where increasing the search effort degraded the overall performance.
Reference: <author> Gennari, J. H., Langley, P. & Fisher, D. </author> <year> (1989), </year> <title> "Models of incremental concept formation", </title> <journal> Artificial Intelligence 40 pp. </journal> <pages> 11-61. </pages>
Reference: <author> Ginsberg, M. L. </author> <year> (1993), </year> <booktitle> Essentials of Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The wrapper approach conducts a search in the space of possible parameters. A search requires a state space, an initial state, a termination condition, and a search engine <ref> (Ginsberg 1993, Russell & Norvig 1995) </ref>. The next section focuses on comparing search engines: hill-climbing and best-first search. The search space organization that we chose is such that each state represents a feature subset.
Reference: <author> Goldberg, D. E. </author> <year> (1989), </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning, </title> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference: <author> Good, I. J. </author> <year> (1965), </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods, </title> <publisher> M.I.T. Press. </publisher>
Reference-contexts: In case of zero occurrences for a label value and a feature value, we use the :5=m as the probability, where m is the number of instances. Other approaches are possible, such as using Laplace's law of succession or using a beta prior <ref> (Good 1965, Cestnik 1990) </ref>. In these approaches, the probability for n successes after N trials is estimated at (n + a)=(N + a + b), where a and b are the parameters of the beta function.
Reference: <author> Greiner, R. </author> <year> (1992), </year> <title> Probabilistic hill climbing : Theory and applications, </title> <editor> in J. Glasgow & R. Hadley, eds, </editor> <booktitle> Proceedings of the Ninth Canadian Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 60-67. </pages>
Reference: <author> Hancock, T. R. </author> <year> (1989), </year> <title> On the difficulty of finding small consistent decision trees, </title> <type> Unpublished Manuscript, </type> <institution> Harvard University. </institution>
Reference: <author> Hoeffding, W. </author> <year> (1963), </year> <title> "Probability inequalities for sums of bounded random variables", </title> <journal> Journal of the American Statistical Association 58 pp. </journal> <pages> 13-30. </pages> <publisher> 39 Holland, </publisher> <editor> J. H. </editor> <year> (1992), </year> <title> Adaptation in natural and artificial systems : an introductory analysis with applications to biology, control, </title> <booktitle> and artificial intelligence, </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: The race ends when there is a winner, or when all n steps in the leave-one-out cross-validation have been executed. The confidence interval is defined according to Hoeffding's formula <ref> (Hoeffding 1963) </ref>: p fi fi fi &gt; * &lt; 2e 2m* 2 =B 2 where b f (s) is the average of m evaluations and B bounds the possible spread of point values.
Reference: <author> Hyafil, L. & Rivest, R. L. </author> <year> (1976), </year> <title> "Constructing optimal binary decision trees is NP-complete", </title> <journal> Information Processing Letters 5(1), </journal> <pages> pp. 15-17. </pages>
Reference-contexts: The second problem, that of finding a "best" (or approximately best) hypothesis, is usually intractable and thus poses an added computational burden. For example, decision tree induction algorithms usually attempt to find a small tree that fits the data well, yet finding the optimal binary decision tree is NP-hard <ref> (Hyafil & Rivest 1976, Hancock 1989) </ref>. For neural-networks, the problem is even harder; the problem of loading a three-node neural network with a training set is NP-hard if the nodes compute linear threshold functions (Judd 1988, Blum & Rivest 1992).
Reference: <author> John, G. H. </author> <year> (1997), </year> <title> Enhancements to the Data Mining Process, </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Science department. </institution>
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages>
Reference-contexts: In the feature subset selection problem, a learning algorithm is faced with the problem of selecting some subset of features upon which to focus its attention, while ignoring the rest. In the wrapper approach <ref> (John, Kohavi & Pfleger 1994) </ref>, the feature subset selection algorithm exists as a wrapper around the induction algorithm. The feature subset selection algorithm conducts a search for a good subset using the induction algorithm itself as part of the function evaluating feature subsets. <p> These measures and the relevance measure assigned by Relief would not be appropriate as feature subset selectors for algorithms such as Naive-Bayes because in some cases the performance of Naive-Bayes improves with the removal of relevant features. The Corral dataset, which is an artificial dataset from <ref> (John et al. 1994) </ref> gives a possible scenario where filter approaches fail miserably. There are 32 instances in this Boolean domain. <p> Contextual features are harder to find because they involve interactions. These definitions are orthogonal to ours: a feature may be primary and either strongly or weakly relevant, or contextual and either strongly or weakly relevant. Since the introduction of the wrapper approach <ref> (John et al. 1994) </ref>, we have seen it used in a few papers. Langley & Sage (1994a) used the wrapper approach to select features for Naive-Bayes (but without dis-cretization) and Langley & Sage (1994b) used it to select features for a nearest-neighbor algorithm.
Reference: <author> Judd, S. </author> <year> (1988), </year> <title> "On the complexity of loading shallow neural networks", </title> <journal> Journal of Complexity 4 pp. </journal> <pages> 177-192. </pages>
Reference-contexts: For neural-networks, the problem is even harder; the problem of loading a three-node neural network with a training set is NP-hard if the nodes compute linear threshold functions <ref> (Judd 1988, Blum & Rivest 1992) </ref>. Because of the above problems, we define an optimal feature subset with respect to a particular induction algorithm, taking into account its heuristics, biases, and tradeoffs. The problem of feature subset selection is then reduced to the problem of finding an optimal subset.
Reference: <author> Kaelbling, L. P. </author> <year> (1993), </year> <title> Learning in Embedded Systems, </title> <publisher> MIT Press. </publisher>
Reference-contexts: Increasing the number of runs shrinks the confidence interval for the mean, but requires more time. The tradeoff between more accurate estimates and more extensive exploration of the search space is referred to as the exploration versus exploitation problem <ref> (Kaelbling 1993) </ref>. We can either exploit our knowledge and shrink the confidence intervals of the explored nodes to make sure we select the right one, or we can explore new nodes in the hope of finding better nodes. The tradeoff leads to the following abstract search problem. <p> Fong (1995) gives bounds for the sample complexity (the number of samples one needs to collect before termination) in the k-armed bandit problem. His fl-IE approach allows trading off exploitation and exploration, thus generalizing Kaelbling's interval estimation strategy <ref> (Kaelbling 1993) </ref>. However, in all cases the worst-case bound remains the same and the optimal tradeoff between exploration and exploitation was empirically determined to be domain dependent. When using the wrapper method, it is important to explore a sufficient portion of the search space.
Reference: <author> Kira, K. & Rendell, L. A. </author> <year> (1992a), </year> <title> The feature selection problem: Traditional methods and a new algorithm, </title> <booktitle> in Tenth National Conference on Artificial Intelligence, </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 129-134. </pages>
Reference-contexts: Given only the SSN, any induction algorithm is expected to generalize very poorly. 2.4.2 The Relief Algorithm The Relief algorithm <ref> (Kira & Rendell 1992a, Kira & Rendell 1992b, Kononenko 1994) </ref> assigns a "relevance" weight to each feature, which is meant to denote the relevance of the feature to the target concept. Relief is a randomized algorithm. <p> The Relief algorithm attempts to find all relevant features: Relief does not help with redundant features. If most of the given features are relevant to the concept, it would select most of them even though only a fraction are necessary for concept description <ref> (Kira & Rendell 1992a, page 133) </ref>. In real domains, many features have high correlations with the label, and thus many are weakly relevant, and will not be removed by Relief. <p> In real domains, many features have high correlations with the label, and thus many are weakly relevant, and will not be removed by Relief. In the simple parity example used in <ref> (Kira & Rendell 1992a, Kira & Rendell 1992b) </ref>, there were only strongly relevant and irrelevant features, so Relief found the strongly relevant features most of the time. The Relief algorithm was motivated by nearest-neighbors and it is good specifically for similar types of induction algorithms.
Reference: <author> Kira, K. & Rendell, L. A. </author> <year> (1992b), </year> <title> A practical approach to feature selection, </title> <booktitle> in Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kittler, J. </author> <year> (1978), </year> <title> Une generalisation de quelques algorithms sous-optimaux de recherche d'ensembles d'attributs, </title> <booktitle> in Proc. </booktitle> <institution> Congres Reconnaissance des Formes et Traitement des Images. </institution>
Reference: <author> Kittler, J. </author> <year> (1986), </year> <title> Feature Selection and Extraction, </title> <publisher> Academic Press, Inc, </publisher> <pages> chapter 3, pp. 59-83. </pages>
Reference-contexts: Note that feature subset selection chooses a set of features from existing features, and does not construct new ones; there is no feature extraction or construction <ref> (Kittler 1986, Rendell & Seshu 1990) </ref>. From a purely theoretical standpoint, the question of which features to use is not of much interest.
Reference: <author> Kohavi, R. </author> <year> (1994), </year> <title> Feature subset selection as search with probabilistic estimates, </title> <booktitle> in AAAI Fall Symposium on Relevance, </booktitle> <pages> pp. 122-126. </pages>
Reference-contexts: In the feature subset selection problem, a learning algorithm is faced with the problem of selecting some subset of features upon which to focus its attention, while ignoring the rest. In the wrapper approach <ref> (John, Kohavi & Pfleger 1994) </ref>, the feature subset selection algorithm exists as a wrapper around the induction algorithm. The feature subset selection algorithm conducts a search for a good subset using the induction algorithm itself as part of the function evaluating feature subsets.
Reference: <author> Kohavi, R. </author> <year> (1995a), </year> <title> The power of decision tables, </title> <editor> in N. Lavrac & S. Wrobel, eds, </editor> <booktitle> Proceedings of the European Conference on Machine Learning, Lecture Notes in Artificial Intelligence 914, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <pages> pp. 174-189. </pages>
Reference: <author> Kohavi, R. </author> <year> (1995b), </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 1137-1143. </pages>
Reference-contexts: The accuracy of the induced classifiers is estimated using accuracy estimation techniques <ref> (Kohavi 1995b) </ref>. The problem we are investigating is that of state space search, and different search engines will be investigated in the next sections. The wrapper approach conducts a search in the space of possible parameters.
Reference: <author> Kohavi, R. </author> <year> (1995c), </year> <title> Wrappers for Performance Enhancement and Oblivious Decision Graphs, </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Science department. STAN-CS-TR-95-1560, ftp://starry.stanford.edu/pub/ronnyk/teza.ps. </institution>
Reference: <author> Kohavi, R. & Frasca, B. </author> <year> (1994), </year> <title> Useful feature subsets and rough set reducts, </title> <booktitle> in Third International Workshop on Rough Sets and Soft Computing, </booktitle> <pages> pp. 310-317. </pages> <note> Also appeared in Soft Computing by Lin and Wildberger. </note>
Reference-contexts: In the feature subset selection problem, a learning algorithm is faced with the problem of selecting some subset of features upon which to focus its attention, while ignoring the rest. In the wrapper approach <ref> (John, Kohavi & Pfleger 1994) </ref>, the feature subset selection algorithm exists as a wrapper around the induction algorithm. The feature subset selection algorithm conducts a search for a good subset using the induction algorithm itself as part of the function evaluating feature subsets.
Reference: <author> Kohavi, R. & John, G. </author> <year> (1995), </year> <title> Automatic parameter selection by minimizing estimated error, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 304-312. </pages>
Reference: <author> Kohavi, R. & Sommerfield, D. </author> <year> (1995), </year> <title> Feature subset selection using the wrapper model: Overfitting and dynamic search space topology, </title> <booktitle> in The First International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pp. 192-197. </pages>
Reference: <author> Kohavi, R. & Wolpert, D. H. </author> <year> (1996), </year> <title> Bias plus variance decomposition for zero-one loss functions, </title> <editor> in L. Saitta, ed., </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 275-283. </pages> <note> Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference-contexts: = y) p (Y = y) p ( ~ X = ~x) is same for all label values. = i=1 p (X i = x i j Y = y) p (Y = y) by independence The version of Naive-Bayes we use in our experiments was implemented in MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1996) </ref>. The probabilities for nominal features are estimated from data using maximum likelihood estimation. Continuous features are discretized using a minimum-description length procedure described in Dougherty, Kohavi & Sahami (1995), and were thereafter treated as multi-valued nominals. <p> We would like thank our anonymous reviewers. One reviewer formulated Example 3, which is much better than our original example. Pat Langley, Nick Littlestone, Nils Nilsson, and Peter Turney gave helpful feedback on the ideas and presentation. Dan Sommerfield implemented large parts of the wrapper in MLC ++ <ref> (Kohavi et al. 1996) </ref>, and all of the experiments were done using MLC ++ . George John's work was supported under a National Science Foundation Graduate Research Fellowship. Most of the research for this paper was completed while the authors were at Stanford University. 37
Reference: <author> Kohavi, R., Sommerfield, D. & Dougherty, J. </author> <year> (1996), </year> <title> Data mining using MLC ++ : A machine learning library in C ++ , in Tools with Artificial Intelligence, </title> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 234-245. </pages> <note> Received the best paper award. http://www.sgi.com/Technology/mlc. </note>
Reference-contexts: = y) p (Y = y) p ( ~ X = ~x) is same for all label values. = i=1 p (X i = x i j Y = y) p (Y = y) by independence The version of Naive-Bayes we use in our experiments was implemented in MLC ++ <ref> (Kohavi, Sommerfield & Dougherty 1996) </ref>. The probabilities for nominal features are estimated from data using maximum likelihood estimation. Continuous features are discretized using a minimum-description length procedure described in Dougherty, Kohavi & Sahami (1995), and were thereafter treated as multi-valued nominals. <p> We would like thank our anonymous reviewers. One reviewer formulated Example 3, which is much better than our original example. Pat Langley, Nick Littlestone, Nils Nilsson, and Peter Turney gave helpful feedback on the ideas and presentation. Dan Sommerfield implemented large parts of the wrapper in MLC ++ <ref> (Kohavi et al. 1996) </ref>, and all of the experiments were done using MLC ++ . George John's work was supported under a National Science Foundation Graduate Research Fellowship. Most of the research for this paper was completed while the authors were at Stanford University. 37
Reference: <author> Kononenko, I. </author> <year> (1994), </year> <title> Estimating attributes: Analysis and extensions of Relief, </title> <editor> in F. Bergadano & L. D. Raedt, eds, </editor> <booktitle> Proceedings of the European Conference on Machine Learning. 40 Kononenko, I. </booktitle> <year> (1995), </year> <title> On biases in estimating multi-valued attributes, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 1034-1040. </pages>
Reference: <author> Koza, J. </author> <year> (1992), </year> <title> Genetic Programming : On the Programming of Computers by Means of Natural Selection, </title> <publisher> MIT Press. </publisher>
Reference-contexts: Looking at the search, we have seen that one general area of the search space is explored heavily when it is found to be good. It might be worthwhile to introduce some diversity into the search, following the genetic algorithm and genetic programming approaches <ref> (Holland 1992, Goldberg 1989, Koza 1992) </ref>. The problem has been abstracted as search with probabilistic estimates (Section 7), but we have not done experiments in an attempt to understand the tradeoff between the quality of the estimates and the search size, i.e., exploration versus exploitation experiments.
Reference: <author> Krogh, A. & Vedelsby, J. </author> <year> (1995), </year> <title> Neural network ensembles, cross validation, and active learning, </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 7, </volume> <publisher> MIT Press. </publisher>
Reference: <author> Kwok, S. W. & Carter, C. </author> <year> (1990), </year> <title> Multiple decision trees, </title> <editor> in R. D. Schachter, T. S. Levitt, L. N. Kanal & J. F. Lemmer, eds, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <publisher> Elsevier Science Publishers, </publisher> <pages> pp. 327-335. </pages>
Reference: <author> Laarhoven, P. & Aarts, E. </author> <year> (1987), </year> <title> Simulated annealing : Theory and Applications, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: We have examined hill-climbing and best-first search engines. Other approaches could be examined, such as simulated annealing approaches that evaluate the better nodes more times <ref> (Laarhoven & Aarts 1987) </ref>. Looking at the search, we have seen that one general area of the search space is explored heavily when it is found to be good.
Reference: <author> Langley, P. </author> <year> (1994), </year> <title> Selection of relevant features in machine learning, </title> <booktitle> in AAAI Fall Symposium on Relevance, </booktitle> <pages> pp. 140-144. </pages>
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994a), </year> <title> Induction of selective bayesian classifiers, </title> <booktitle> in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Seattle, WA, </address> <pages> pp. 399-406. </pages>
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994b), </year> <title> Oblivious decision trees and abstract cases, </title> <booktitle> in Working Notes of the AAAI-94 Workshop on Case-Based Reasoning, </booktitle> <publisher> AAAI Press, </publisher> <address> Seattle, </address> <pages> pp. 113-117. </pages>
Reference: <author> Langley, P., Iba, W. & Thompson, K. </author> <year> (1992), </year> <title> An analysis of Bayesian classifiers, </title> <booktitle> in Proceedings of the tenth national conference on artificial intelligence, </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 223-228. </pages>
Reference-contexts: Algorithms such as Naive-Bayes <ref> (Langley, Iba & Thompson 1992, Duda & Hart 1973, Good 1965) </ref> are robust with respect to irrelevant features (i.e., their performance degrades very slowly as more irrelevant features are added) but their performance may degrade quickly if correlated (even if relevant) features are added.
Reference: <author> Linhart, H. & Zucchini, W. </author> <year> (1986), </year> <title> Model Selection, </title> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: In theory, every possible feature subset identifies a different model, so the problem can be viewed as that of model selection <ref> (Linhart & Zucchini 1986) </ref> in Statistics.
Reference: <author> Littlestone, N. & Warmuth, M. K. </author> <year> (1994), </year> <title> "The weighted majority algorithm", </title> <booktitle> Information and Computation 108(2), </booktitle> <pages> pp. 212-261. </pages>
Reference-contexts: While we concentrated on selection of relevant features in this paper, an alternative method is to weigh features, giving each one a degree of relevance. Theoretical results have been shown for multiplicative learning algorithms, which work well for linear combinations of features (e.g., Perceptrons) <ref> (Littlestone & Warmuth 1994) </ref>. Skalak (1994) uses the wrapper approach for feature subset selection and for decreasing the number of prototypes stored in instance-based methods. He shows that very few prototypes sometimes suffice. This is an example of choosing relevant training instances as opposed to relevant features.
Reference: <author> Mallows, C. L. </author> <year> (1973), </year> <title> "Some comments on c p ", Technometrics 15 pp. </title> <type> 661-675. </type>
Reference-contexts: In some cases, measures can be devised that are algorithm specific, and these may be computed efficiently. For example, measures such as Mallow's C p <ref> (Mallows 1973) </ref> and PRESS (Prediction sum of squares) (Neter, Wasserman & Kutner 1990) have been devised specifically for linear regression. <p> Notable selection measures that satisfy the monotonicity assumption are residual sum of squares (RSS), adjusted R-square, minimum mean residual, Mallow's C p <ref> (Mallows 1973) </ref>, discriminant functions, and distance measures, such as the Bhattacharyya distance and divergence. The PRESS measure (Prediction sum of squares), however, does not obey monotonicity. For monotonic functions, branch and bound techniques can be used to prune the search space.
Reference: <author> Marill, T. & Green, D. M. </author> <year> (1963), </year> <title> "On the effectiveness of receptors in recognition systems", </title> <journal> IEEE Transactions on Information Theory 9 pp. </journal> <pages> 11-17. </pages>
Reference: <author> Maron, O. & Moore, A. W. </author> <year> (1994), </year> <title> Hoeffding races: Accelerating model selection search for classification and function approximation, </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 6, </volume> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Given a confidence level, one can determine *, and hence a confidence interval for f fl (s), from the above formula. The paper <ref> (Maron & Moore 1994) </ref>, however, does not discuss any search heuristic, and assumes that a fixed set of models is given by some external source.
Reference: <author> Merz, C. J. & Murphy, P. M. </author> <year> (1996), </year> <note> UCI repository of machine learning databases, http://www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: All datasets except for Corral were obtained from the University of California at Irvine repository <ref> (Merz & Murphy 1996) </ref>, from which full documentation for all datasets can be obtained. Corral was introduced in John et al. (1994) and was defined above.
Reference: <author> Miller, A. J. </author> <year> (1984), </year> <title> "Selection of subsets of regression variables", </title> <journal> Royal Statistical Society A 147 pp. </journal> <pages> 389-425. </pages>
Reference: <author> Miller, A. J. </author> <year> (1990), </year> <title> Subset Selection in Regression, </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Minsky, M. L. & Papert, S. </author> <year> (1988), </year> <title> Perceptrons : an Introduction to Computational Geometry, </title> <publisher> MIT Press. Expanded ed. </publisher>
Reference: <author> Mladenic, D. </author> <year> (1995), </year> <title> Automated model selection, </title> <booktitle> in ECML workshop on Knowledge Level Modeling and Machine Learning. </booktitle>
Reference: <author> Modrzejewski, M. </author> <year> (1993), </year> <title> Feature selection using rough sets theory, </title> <editor> in P. B. Brazdil, ed., </editor> <booktitle> Proceedings of the European Conference on Machine Learning, </booktitle> <publisher> Springer, </publisher> <pages> pp. 213-226. </pages>
Reference: <author> Moore, A. W. & Lee, M. S. </author> <year> (1994), </year> <title> Efficient algorithms for minimizing cross validation error, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: Given a confidence level, one can determine *, and hence a confidence interval for f fl (s), from the above formula. The paper <ref> (Maron & Moore 1994) </ref>, however, does not discuss any search heuristic, and assumes that a fixed set of models is given by some external source. <p> The algorithm does a forward selection and backward elimination, but instead of estimating the accuracy of 33 each added (deleted) feature using leave-one-out cross-validation, all the features that can be added (deleted) are raced in parallel until there is a clear winner. Schemata search <ref> (Moore & Lee 1994) </ref> is another search variant that allows taking into account interactions between features.
Reference: <author> Moret, B. M. E. </author> <year> (1982), </year> <title> "Decision trees and diagrams", </title> <journal> ACM Computing Surveys 14(4), </journal> <pages> pp. 593-623. </pages>
Reference: <author> Murthy, S. & Salzberg, S. </author> <year> (1995), </year> <title> Lookahead and pathology in decision tree induction, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 1025-1031. </pages>
Reference: <author> Narendra, M. P. & Fukunaga, K. </author> <year> (1977), </year> <title> "A branch and bound algorithm for feature subset selection", </title> <journal> IEEE Transactions on Computers C-26(9), </journal> <month> September, </month> <pages> pp. 917-922. </pages>
Reference: <author> Neter, J., Wasserman, W. & Kutner, M. H. </author> <year> (1990), </year> <title> Applied Linear Statistical Models, 3rd edition, </title> <type> Irwin: Homewood, </type> <institution> IL. </institution>
Reference-contexts: In some cases, measures can be devised that are algorithm specific, and these may be computed efficiently. For example, measures such as Mallow's C p (Mallows 1973) and PRESS (Prediction sum of squares) <ref> (Neter, Wasserman & Kutner 1990) </ref> have been devised specifically for linear regression. These measures and the relevance measure assigned by Relief would not be appropriate as feature subset selectors for algorithms such as Naive-Bayes because in some cases the performance of Naive-Bayes improves with the removal of relevant features.
Reference: <author> Pawlak, Z. </author> <year> (1991), </year> <title> Rough Sets, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Atkeson (1991) used leave-one-out cross-validation to search a multidimensional real-valued space which includes feature weights in addition to other parameters for local learning. The theory of rough sets defines notions of relevance that are closely related to the ones defined here <ref> (Pawlak 1991) </ref>. The set of strongly relevant features form the core and any set of features that allow a Bayes classifier to achieve the highest possible accuracy forms a reduct. A reduct can only contain strongly relevant and weakly relevant features.
Reference: <author> Pawlak, Z. </author> <year> (1993), </year> <title> "Rough sets: present state and the future", </title> <booktitle> Foundations of Computing and Decision Sciences 18(3-4), </booktitle> <pages> pp. 157-166. </pages>
Reference: <author> Pazzani, M. J. </author> <year> (1995), </year> <title> Searching for dependencies in Bayesian classifiers, </title> <editor> in D. Fisher & H. Lenz, eds, </editor> <booktitle> Proceedings of the fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL. </address>
Reference: <author> Perrone, M. </author> <year> (1993), </year> <title> Improving regression estimation: averaging methods for variance reduction with extensions to general convex measure optimization, </title> <type> PhD thesis, </type> <institution> Brown University, Physics Dept. </institution>
Reference: <author> Provan, G. M. & Singh, M. </author> <year> (1995), </year> <title> Learning Bayesian networks using feature selection, </title> <editor> in D. Fisher & H. Lenz, eds, </editor> <booktitle> Proceedings of the fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Ft. Lauderdale, FL, </address> <pages> pp. 450-456. </pages>
Reference: <author> Provost, F. J. </author> <year> (1992), </year> <title> Policies for the Selection of Bias in Inductive Machine Learning, </title> <type> PhD thesis, </type> <institution> University of Pittsburgh, Computer Science Department. </institution> <note> Report No. 92-34. </note>
Reference: <author> Provost, F. J. & Buchanan, B. G. </author> <year> (1995), </year> <title> "Inductive policy: The pragmatics of bias selection", </title> <booktitle> Machine Learning 20 pp. </booktitle> <pages> 35-61. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986), </year> <title> "Induction of decision trees", </title> <note> Machine Learning 1 pp. 81-106. Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference-contexts: We examine two general approaches to feature subset selection: the filter approach and the wrapper approach, and we then investigate each in detail. 2.1 The Problem Practical machine learning algorithms, including top-down induction of decision tree algorithms such as ID3 <ref> (Quinlan 1986) </ref>, C4.5 (Quinlan 1993), and CART (Breiman, Friedman, Olshen & Stone 1984), and instance-based algorithms, such as IBL (Dasarathy 1990, Aha, Kibler & Albert 1991), are known to degrade in performance (prediction accuracy) when faced with many features that are not necessary for predicting the desired output. <p> The Naive-Bayes algorithm is explained below. The specific details are not essential for the rest of the paper. The C4.5 algorithm (Quinlan 1993) is a descendent of ID3 <ref> (Quinlan 1986) </ref>, which builds decision trees top-down and prunes them. In our experiments we used release 7 of C4.5. The tree is constructed by finding the best single-feature test to conduct at the root node of the tree.
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: We examine two general approaches to feature subset selection: the filter approach and the wrapper approach, and we then investigate each in detail. 2.1 The Problem Practical machine learning algorithms, including top-down induction of decision tree algorithms such as ID3 (Quinlan 1986), C4.5 <ref> (Quinlan 1993) </ref>, and CART (Breiman, Friedman, Olshen & Stone 1984), and instance-based algorithms, such as IBL (Dasarathy 1990, Aha, Kibler & Albert 1991), are known to degrade in performance (prediction accuracy) when faced with many features that are not necessary for predicting the desired output. <p> Decision trees have been well documented in Quinlan (1993), Breiman et al. (1984), Fayyad (1991), Buntine (1992), and Moret (1982); hence we will describe them briefly. The Naive-Bayes algorithm is explained below. The specific details are not essential for the rest of the paper. The C4.5 algorithm <ref> (Quinlan 1993) </ref> is a descendent of ID3 (Quinlan 1986), which builds decision trees top-down and prunes them. In our experiments we used release 7 of C4.5. The tree is constructed by finding the best single-feature test to conduct at the root node of the tree. <p> A relatively unknown post processing step in C4.5 replaces a node by one of its children if the accuracy of the child is considered better <ref> (Quinlan 1993, page 39) </ref>. In one case (the Corral database described below), this had a significant impact on the resulting tree: although the root split was incorrect, it was replaced by one of the children.
Reference: <author> Quinlan, J. R. </author> <year> (1995), </year> <title> Oversearching and layered search in empirical learning, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 1019-1024. </pages>
Reference: <author> Rendell, L. & Seshu, R. </author> <year> (1990), </year> <title> "Learning hard concepts through constructive induction: framework and rationale", </title> <booktitle> Computational Intelligence 6(4), </booktitle> <month> November, </month> <pages> pp. 247-270. </pages>
Reference: <author> Rosenblatt, F. </author> <year> (1958), </year> <title> "The perceptron: A probabilistic model for information storage and organization in the brain", </title> <journal> Psychological Review 65 pp. </journal> <pages> 386-408. </pages>
Reference-contexts: The following example shows that this may be true. Example 3 (Optimality does not imply relevance) Assume there exists a feature that always takes the value one. Under all the definitions of relevance described above, this feature is irrelevant. Now consider a limited Perceptron classifier <ref> (Rosenblatt 1958, Minsky & Papert 1988) </ref> that has an associated weight with each feature and then classifies instances based upon whether the linear combination is greater than zero (the threshold is fixed at zero). (Contrast this with a regular Perceptron that classifies instances depending on whether the linear combination is greater
Reference: <author> Russell, S. J. & Norvig, P. </author> <year> (1995), </year> <title> Artificial Intelligence: A Modern Approach, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey 07632. </address>
Reference-contexts: The results, especially on the artificial datasets where we know what the relevant features are, indicate that the feature subset selection is getting stuck at local maxima too often. The next section deals with improving the search engine. 3.3 A best-first Search Engine Best-first search <ref> (Russell & Norvig 1995, Ginsberg 1993) </ref> is a more robust method than hill-climbing. The idea is to select the most promising node we have generated so far that has not already been expanded.
Reference: <author> Schaffer, C. </author> <year> (1993), </year> <title> "Selecting a classification method by cross-validation", </title> <booktitle> Machine Learning 13(1), </booktitle> <pages> pp. 135-143. </pages>
Reference-contexts: If there are only a few models, as is the case when one chooses between three induction algorithms, one can estimate the accuracy of each one and select the one with the highest accuracy <ref> (Schaffer 1993) </ref> or perhaps even find some underlying theory to help predict the best one for a given dataset (Brazdil, Gama & Henery (1994)).
Reference: <author> Schapire, R. E. </author> <year> (1990), </year> <title> "The strength of weak learnability", </title> <booktitle> Machine Learning 5(2), </booktitle> <pages> pp. 197-227. </pages>
Reference: <author> Siedlecki, W. & Sklansky, J. </author> <year> (1988), </year> <title> "On automatic feature selection", </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence 2(2), </journal> <pages> pp. 197-220. </pages>
Reference-contexts: More recent papers attempt to use AI techniques, such as beam search and bidirectional search <ref> (Siedlecki & Sklansky 1988) </ref>, best-first search (Xu, Yan & Chang 1989), and genetic algorithms (Vafai & De Jong 1992, Vafai & De Jong 1993).
Reference: <author> Singh, M. & Provan, G. M. </author> <year> (1995), </year> <title> A comparison of induction algorithms for selective and non-selective Bayesian classifiers, </title> <booktitle> in Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pp. 497-505. </pages>
Reference: <author> Skalak, D. B. </author> <year> (1994), </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> Street, W. N., Mangasarian, O. L. & Wolberg, W. H. </author> <year> (1995), </year> <title> An inductive learning approach to prognostic prediction, </title> <booktitle> in Machine Learning: Proceedings of the Twelfth International Conference. </booktitle>
Reference: <author> Taylor, C., Michie, D. & Spiegalhalter, D. </author> <year> (1994), </year> <title> Machine Learning, Neural and Statistical Classification, </title> <publisher> Paramount Publishing International. </publisher> <address> 42 Thrun et al. </address> <year> (1991), </year> <title> The Monk's problems: A performance comparison of different learning algorithms, </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: A detailed description of the datasets and these considerations is given in Kohavi (1995c). Small datasets were tested using ten-fold cross-validation; artificial datasets and large datasets were split into training and testing sets (the artificial datasets have a well-defined training set, as does the DNA dataset from StatLog <ref> (Taylor, Michie & Spiegalhalter 1994) </ref>). The baseline accuracy is the accuracy (on the whole dataset) when predicting the majority class. 11 3.1.2 Algorithms We use two families of induction algorithms as a basis for comparisons. These are the decision-tree and the Naive-Bayes induction algorithms.
Reference: <author> Turney, P. D. </author> <year> (1993), </year> <title> Exploiting context when learning to classify, </title> <editor> in P. B. Brazdil, ed., </editor> <booktitle> Proceedings of the European Conference on Machine Learning (ECML), </booktitle> <pages> pp. 402-407. </pages>
Reference: <author> Turney, P. D. </author> <year> (1996), </year> <title> The identification of context-sensitive features, a formal definition of context for concept learning, </title> <editor> in M. Kubat & G. Widmer, eds, </editor> <booktitle> Proceedings of the Workshop on Learning in Context-Sensitive Domains, </booktitle> <pages> pp. 53-59. </pages> <note> Also available as National Research Council of Canada Technical Report #39222. </note>
Reference: <author> Utgoff, P. E. </author> <year> (1994), </year> <title> An improved algorithm for incremental induction of decision trees, </title> <booktitle> in Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 318-325. </pages>
Reference: <author> Utgoff, P. E. </author> <year> (1995), </year> <title> Decision tree induction based on efficient tree restructuring, </title> <type> Technical Report 05-18, </type> <institution> University of Massachusetts, Amherst. </institution>
Reference-contexts: Although C4.5 does not support incremental operations, Utgoff (1994) has shown that this is possible and has implemented a fast version of leave-one-out for decision trees <ref> (Utgoff 1995) </ref>. The wrapper approach is also very easy to parallelize. In a node expansion, all children can be evaluated in parallel, which will cut the running time by a factor equal to the number of attributes assuming enough processors are available (e.g., 180 for DNA).
Reference: <author> Vafai, H. & De Jong, K. </author> <year> (1992), </year> <title> Genetic algorithms as a tool for feature selection in machine learning, </title> <booktitle> in Fourth International Conference on Tools with Artificial Intelligence, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 200-203. </pages>
Reference-contexts: More recent papers attempt to use AI techniques, such as beam search and bidirectional search (Siedlecki & Sklansky 1988), best-first search (Xu, Yan & Chang 1989), and genetic algorithms <ref> (Vafai & De Jong 1992, Vafai & De Jong 1993) </ref>. All the algorithms described above use a deterministic evaluation function, although in some cases they can easily be extended to probabilistic estimates, such as cross-validation that we use.
Reference: <author> Vafai, H. & De Jong, K. </author> <year> (1993), </year> <title> Robust feature selection algorithms, </title> <booktitle> in Fifth International Conference on Tools with Artificial Intelligence, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 356-363. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1992a), </year> <title> "On the connection between in-sample testing and generalization error", </title> <journal> Complex Systems 6 pp. </journal> <pages> 47-94. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1992b), </year> <title> "Stacked generalization", </title> <booktitle> Neural Networks 5 pp. </booktitle> <pages> 241-259. </pages>
Reference: <author> Xu, L., Yan, P. & Chang, T. </author> <year> (1989), </year> <title> Best first strategy for feature selection, </title> <booktitle> in Ninth International Conference on Pattern Recognition, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 706-708. </pages>
Reference-contexts: More recent papers attempt to use AI techniques, such as beam search and bidirectional search (Siedlecki & Sklansky 1988), best-first search <ref> (Xu, Yan & Chang 1989) </ref>, and genetic algorithms (Vafai & De Jong 1992, Vafai & De Jong 1993). All the algorithms described above use a deterministic evaluation function, although in some cases they can easily be extended to probabilistic estimates, such as cross-validation that we use.
Reference: <author> Yan, D. & Mukai, H. </author> <year> (1992), </year> <title> "Stochastic discrete optimization", </title> <journal> Siam J. Control and Optimization 30(3), </journal> <pages> pp. 594-612. </pages>
Reference: <author> Yu, B. & Yuan, B. </author> <year> (1993), </year> <title> "A more efficient branch and bound algorithm for feature selection", </title> <booktitle> Pattern Recognition 26(6), </booktitle> <pages> pp. 883-889. </pages>
Reference: <author> Ziarko, W. </author> <year> (1991), </year> <title> The discovery, analysis, and representation of data dependencies in databases, </title> <editor> in G. Piatetsky-Shapiro & W. Frawley, eds, </editor> <title> Knowledge Discovery in Databases, </title> <publisher> MIT Press. </publisher> <pages> 43 </pages>
References-found: 117

