URL: http://bugle.cs.uiuc.edu/Papers/DirkSchedule.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/Papers/papers.html
Root-URL: http://www.cs.uiuc.edu
Title: Dynamic Task Placement on Multicomputers  
Author: Dirk C. Grunwald Bobby A. A. Nazief zx Daniel A. Reed 
Address: Boulder, Colorado 80309  Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Colorado  Department of Computer Science University of Illinois  
Abstract: fl An abridged version of this paper appeared in the Proceedings of the Fifth Distributed Memory Computing Conference, April 8-12, 1990, Charleston, NC. y Supported in part by the National Science Foundation under NSF Grant CCR-9010624. z Supported in part by the National Science Foundation under grants NSF CCR86-57696, NSF CCR87-06653 and NSF CDA87-22836, by the National Aeronautics and Space Administration under NASA Contract Number NAG-1-613, and by a grant from the Digital Equipment Corporation External Research Program. x Author's present address: Department of Computer Science, University of Indonesia, Jakarta, Indonesia 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> AMETEK. </author> <title> The Ametek System 14 Users Guide. </title> <institution> Ametek Computer Research Division, </institution> <month> May </month> <year> 1986. </year> <note> Document No. V12970. </note>
Reference-contexts: Each computing node minimally contains a processor, a local memory, and a communication controller. Multicomputers have evolved from machines with modest performance computation nodes and low-speed communication networks to current generation systems with high-performance computation nodes and high-speed communication networks. Early multicomputers (e.g., the Ametek System/14 <ref> [1] </ref>, the Intel iPSC/1 [26], and the Ncube/ten [15]) were little more than research curiosities.
Reference: [2] <author> ATHAS, W., AND SEITZ, C. </author> <title> Cantor User Report, Version 2.0. </title> <type> Tech. Rep. </type> <institution> 5232:TR:86, California Institute of Technology, Department of Computer Science, </institution> <month> January </month> <year> 1987. </year>
Reference-contexts: Several research groups continue to explore techniques that support the construction of dynamic, parallel programs without forcing programmers to resort to low-level techniques; examples include Cantor <ref> [2] </ref>, Linda [4], the Chare Kernel [19], and Mentat [10]. In general, these tools provide an abstraction of a task that can be dynamically created or destroyed, and they support a programming abstraction where the intertask communication pattern is independent of task location.
Reference: [3] <author> ATHAS, W. C., AND SEITZ, C. L. </author> <title> Multicomputers: Message-Passing Concurrent Computers. </title> <booktitle> IEEE Computer (August 1988), </booktitle> <pages> 9-24. </pages>
Reference-contexts: Load placement strategies assign tasks to processors when the tasks are created, whereas load redistribution strategies rearrange extant tasks to balance loads. Several groups have proposed load placement strategies <ref> [21, 29, 23, 18, 28, 3, 32, 31] </ref>, yet few comparative studies exist. Fewer studies exist for modern, low-latency network architectures. <p> To distinguish the two, they refer to the prior CWN strategy as Naive CWN (NCWN). The ACWN strategy is similar to NCWN; however, it adapts to varying loads by dynamically altering the minimum and maximum distance a task invocation message travels. Athas and Seitz <ref> [3] </ref> proposed global, random task placement; a random node is selected for each invocation request. Random allocation is appealing both because of its implementation simplicity and its theoretical underpinnings for large dynamic parallel systems, random placement statistically balances the load.
Reference: [4] <author> CARRIERO, N., AND GELERNTER, D. </author> <title> Linda in Context. </title> <journal> Communications of the ACM 32, </journal> <month> 4 (April </month> <year> 1989), </year> <pages> 444-458. </pages>
Reference-contexts: Several research groups continue to explore techniques that support the construction of dynamic, parallel programs without forcing programmers to resort to low-level techniques; examples include Cantor [2], Linda <ref> [4] </ref>, the Chare Kernel [19], and Mentat [10]. In general, these tools provide an abstraction of a task that can be dynamically created or destroyed, and they support a programming abstraction where the intertask communication pattern is independent of task location.
Reference: [5] <author> CARROLL, M. </author> <title> A Comparison of Two Dynamic Load Balancing Schemes for Multiprocessor Systems. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1987. </year> <month> 35 </month>
Reference-contexts: Others have studied strategies based on the idea of limited diffusion or gradient planes. Kale [17] proposed a strategy called Contracting Within A Neighborhood (CWN) for placement of individual clauses in a parallel Prolog system. Kale and Carroll <ref> [18, 5] </ref> determined that the gradient strategy 7 did not distribute tasks rapidly enough in an idle system. Thus, CWN imposes minimum and maximum forwarding thresholds, forwarding a task at least once, but no more than a predetermined limit.
Reference: [6] <author> CHIEN, A. A., AND DALLY, W. J. </author> <title> Experience with Concurrent Aggregate (CA): </title> <booktitle> Implementation and Programming. In Proceedings of the 5 th Distributed Memory Computing Conference (April 1990), Association For Computing Machinery, </booktitle> <pages> pp. 1040-1049. </pages>
Reference-contexts: Moreover, most are experimental, with a limited user base and few large application codes. To study a variety of strategies within a consistent context, we captured task activity traces from two dynamic task creation systems, the Chare Kernel [19] and Concurrent Aggregates <ref> [6] </ref>, and then augmented these with synthesized traces from abstract descriptions of program behavior. These synthetic traces provide a range of task behavior absent in the captured workloads. <p> Computation time was measured using a microsecond timer and scaled to simulate a processor executing approximately ten million instructions per second. A similar process was used to trace 15 the execution of a multigrid algorithm implemented using Concurrent Aggregates (CA) <ref> [6] </ref>, a concurrent programming language developed by Dally and Chien. Below, we briefly describe each of the measured workloads. Queens: Solves the ten-queens problem for a chess board ten squares on each edge using a divide-and-conquer algorithm. Communications occur only between a task and its children.
Reference: [7] <author> CHOW, E., MADEN, H., PETERSON, J., GRUNWALD, D., AND REED, D. </author> <title> Hyperswitch Network for the Hypercube Computer. </title> <booktitle> In Proceedings of the 15 th International Symposium on Computer Architecture (June 1988), </booktitle> <pages> pp. 90-99. </pages> <note> to appear in IEEE Transactions on Computers. </note>
Reference-contexts: We simulated a binary hypercube with performance comparable to the CalTech/JPL Mark-IIIe HyperSwitch network <ref> [7] </ref>. The HyperSwitch network is a point-to-point, circuit-switched network. Thus, only the source and destination nodes for each message directly interact with those messages. Messages use a crossbar switch to cut-through intermediate nodes, reducing the overhead for nonlocal communication.
Reference: [8] <author> EAGER, D. L., LAZOWSKA, E. D., AND ZAHORJAN, J. </author> <title> A Comparison of Receiver-Initiated and Sender-Initiated Adaptive Load Sharing. Performance Evaluation 6, </title> <month> 1 (March </month> <year> 1986), </year> <pages> 53-68. </pages>
Reference-contexts: First, however, we offer a few observations, illustrated using an analysis of the Concurrent Aggregates multigrid code. 6.1 Task Forwarding and Information Staleness The strategies of Table 2 differ in the number of times a task invocation request can be transferred. All our task placement strategies are server initiated <ref> [8] </ref> (i.e., it is the responsibility of overly busy nodes to disperse tasks); however, the node selected during task distribution can delegate the new task by itself transferring the task to another node.
Reference: [9] <author> EAGER, D. L., LAZOWSKA, E. D., AND ZAHORJAN, J. </author> <title> Adaptive Load Sharing in Homogenous Distributed Systems. </title> <journal> IEEE Tranactions on Software Enginerring SE-12, </journal> <month> 5 (May </month> <year> 1986), </year> <pages> 662-675. </pages>
Reference-contexts: Because global random placement ignores communication locality, it must be possible to move tasks to distant nodes. In store-and-forward networks, this greatly increases communication overhead. In circuit-switched or wormhole networks, communication locality is less important, making random placement feasible. Eager et al. <ref> [9] </ref> have examined load redistribution, a problem similar to load placement. An interesting conclusion from their study was that any redistribution strategy was better than none, and that simple policies were almost as effective as more complex ones.
Reference: [10] <author> GRIMSHAW, A. </author> <title> The Mentat Run-Time System: Support for Medium Grain Parallel Computation. </title> <booktitle> In Proceedings of the 5 th Distributed Memory Computing Conference (April 1990), Association For Computing Machinery, </booktitle> <pages> pp. 1064-1073. </pages>
Reference-contexts: Several research groups continue to explore techniques that support the construction of dynamic, parallel programs without forcing programmers to resort to low-level techniques; examples include Cantor [2], Linda [4], the Chare Kernel [19], and Mentat <ref> [10] </ref>. In general, these tools provide an abstraction of a task that can be dynamically created or destroyed, and they support a programming abstraction where the intertask communication pattern is independent of task location.
Reference: [11] <author> GRUNWALD, D., NAZIEF, B., AND REED, D. </author> <title> Dynamic Load Distribution on Point-to-Point Multicomputer Metworks. </title> <type> Tech. Rep. </type> <institution> CU-CS-542-91, Dept. of Computer Science, Univ. of Colorado, Boulder, CO, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: When an originating node selects a bid from the set of respondents, it broadcasts a cancellation message to all nodes with losing bids. Although diffusion scheduling successfully locates idle nodes, broadcast messages are costly; newer task placement methods can locate the an idle node with fewer messages <ref> [11] </ref>. The Rediflow multiprocessor [20] was based on a mesh-connected network of nodes. For this system, Lin [22, 23] proposed the gradient strategy, a local, demand driven, adaptive load sharing strategy for task scheduling. <p> Taken together, the range of task activities (e.g., total computation time, total number of messages sent and average message size) spans two orders of magnitude. Given space constraints, in this paper we concentrate on the captured execution traces; see <ref> [11, 12] </ref> for a more extended comparison. <p> For additional data buttressing our observations, see <ref> [11] </ref>. Although static performance metrics (e.g., speedup) demonstrate the performance of different load placement strategies, they do not illuminate the reasons for that performance. <p> Because it considers only neighboring nodes, tasks are more likely to become ensnared in local load minima than when globally placed; this reduces the likelihood of task forwarding. For programs with even shorter task lifetimes <ref> [11] </ref>, both the local and global variants, (DLL and DGL), of the Drift policy spent far too much time balancing the load. Clearly, the time constant for information propagation must be commensurate with the time between application 21 program task state changes. <p> Each is described briefly below, followed by a case study of the multigrid program; see <ref> [11] </ref> for additional details. <p> Thus, certain nodes are erroneously, and repeatedly selected. transfers per placement for Multigrid problem. Here, we show the information one 256 processors; the information for different network sizes is illuminating, but does not change our conclusions - see <ref> [12, 11] </ref> for more information. Figure 6 shows that task placement occurs in three distinctive stages. Initially, nodes have little load information about remote nodes, and assume those nodes are idle (i.e., the estimate of remote load is zero).
Reference: [12] <author> GRUNWALD, D. C. </author> <title> Heuristic Load Distribution in Circuit Switched Multicomputer Systems. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> August </month> <year> 1989. </year> <month> 36 </month>
Reference-contexts: The first time an invocation request is transferred, it is sent to the candidate node with minimum status value; on subsequent transfers, it is sent to the candidate node with the minimum load value. In <ref> [12, 24] </ref>, we examined a plethora of possible task placement strategies (i.e., many instances of each policy family with different sources of load and status information). Table 2 summarizes the parameters used in a small subset of these studies. <p> Taken together, the range of task activities (e.g., total computation time, total number of messages sent and average message size) spans two orders of magnitude. Given space constraints, in this paper we concentrate on the captured execution traces; see <ref> [11, 12] </ref> for a more extended comparison. <p> We simulated two networks The first network assumed a 32MB/s bandwidth with 10 microsecond processor communication latency and 1 microsecond switch latency. We also simulated a slower network, but this did not result in significant differences see <ref> [12] </ref> for more information. We implemented a simulacrum operating system; it has enough substance to schedule tasks, orchestrate message delivery and implement a system scheduling task. Application program processes are represented by tasks in the simulated system. <p> Thus, we have distilled the data to infer general trends and highlights. 1 Although our results and observations are based on the workloads in Table 3 and <ref> [12] </ref>, we believe the results are generally applicable to almost all dynamic task task placement policies. For additional data buttressing our observations, see [11]. Although static performance metrics (e.g., speedup) demonstrate the performance of different load placement strategies, they do not illuminate the reasons for that performance. <p> The performance increases with increasing hypercube dimension because there are more neighboring nodes. Other methods distributing tasks only to local nodes suffer from similar problems <ref> [12] </ref>; with these strategies tasks can be placed up to two hops away, using only a small fraction of the available nodes. Because the multigrid tasks are long lived, the limited task dispersion of the local strategies is debilitating. <p> Thus, certain nodes are erroneously, and repeatedly selected. transfers per placement for Multigrid problem. Here, we show the information one 256 processors; the information for different network sizes is illuminating, but does not change our conclusions - see <ref> [12, 11] </ref> for more information. Figure 6 shows that task placement occurs in three distinctive stages. Initially, nodes have little load information about remote nodes, and assume those nodes are idle (i.e., the estimate of remote load is zero).
Reference: [13] <author> GRUNWALD, D. C., AND REED, D. A. </author> <title> Analysis of Backtracking Routing in Binary Hypercube Computers. </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-89-1486, University of Illinois at Urbana-Champaign, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: Although, strictly speaking, communication processor utilization ignores the utilization of the actual circuits or wires used, other studies <ref> [13, 14] </ref> have shown that the communication processor usually saturates before the network transport medium. Following its creation, a task is quiescent during the search for a node that satisfies the task placement strategy's criteria.
Reference: [14] <author> GRUNWALD, D. C., AND REED, D. A. </author> <title> Benchmarking Hypercube Hardware and Software. In Hypercube Multiprocessors (1987), </title> <editor> M. Heath, Ed., </editor> <booktitle> Society for Industrial and Applied Mathematics, </booktitle> <pages> pp. 169-177. </pages>
Reference-contexts: Although, strictly speaking, communication processor utilization ignores the utilization of the actual circuits or wires used, other studies <ref> [13, 14] </ref> have shown that the communication processor usually saturates before the network transport medium. Following its creation, a task is quiescent during the search for a node that satisfies the task placement strategy's criteria.
Reference: [15] <author> HAYES, J. P., MUDGE, T., STOUT, Q. F., COLLEY, S., AND PALMER, J. </author> <title> A Microprocessor-based Hypercube Supercomputer. </title> <booktitle> IEEE Micro 6, </booktitle> <month> 5 (October </month> <year> 1986), </year> <pages> 6-17. </pages>
Reference-contexts: Multicomputers have evolved from machines with modest performance computation nodes and low-speed communication networks to current generation systems with high-performance computation nodes and high-speed communication networks. Early multicomputers (e.g., the Ametek System/14 [1], the Intel iPSC/1 [26], and the Ncube/ten <ref> [15] </ref>) were little more than research curiosities. Not only was the peak performance of their processors comparable to early personal computers, their communication network performance was poor store-and-forward packet switching created communication latency proportional to the distance between the source and destination nodes.
Reference: [16] <author> HPFF. </author> <title> High-Performance Fortran Language Specfication, version 1.0. </title> <type> Tech. rep., </type> <institution> High Performance Fortran Forum, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: By comparison, multicomputer software has evolved slowly. Most systems are programmed using either a data parallel language like CM-Fortran [30] or HPF <ref> [16] </ref>, or with a sequential programming language augmented with a message passing library. The popularity of data parallelism and the rapidity of its adoption reflect its effectiveness in succinctly expressing operations with 3 great spatial and temporal regularity (e.g., matrix operations).
Reference: [17] <author> KAL E, L. V. </author> <title> Parallel Architectures for Problem Solving. </title> <type> PhD thesis, </type> <institution> State University of New York at Stony Brook, </institution> <month> December </month> <year> 1985. </year>
Reference-contexts: Others have studied strategies based on the idea of limited diffusion or gradient planes. Kale <ref> [17] </ref> proposed a strategy called Contracting Within A Neighborhood (CWN) for placement of individual clauses in a parallel Prolog system. Kale and Carroll [18, 5] determined that the gradient strategy 7 did not distribute tasks rapidly enough in an idle system.
Reference: [18] <author> KAL E, L. V. </author> <title> Comparing the Performance of Two Dynamic Load Distribution Methods. </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-87-1776, University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: Load placement strategies assign tasks to processors when the tasks are created, whereas load redistribution strategies rearrange extant tasks to balance loads. Several groups have proposed load placement strategies <ref> [21, 29, 23, 18, 28, 3, 32, 31] </ref>, yet few comparative studies exist. Fewer studies exist for modern, low-latency network architectures. <p> Others have studied strategies based on the idea of limited diffusion or gradient planes. Kale [17] proposed a strategy called Contracting Within A Neighborhood (CWN) for placement of individual clauses in a parallel Prolog system. Kale and Carroll <ref> [18, 5] </ref> determined that the gradient strategy 7 did not distribute tasks rapidly enough in an idle system. Thus, CWN imposes minimum and maximum forwarding thresholds, forwarding a task at least once, but no more than a predetermined limit. <p> Once placed, tasks cannot be transferred to other nodes. Drift: This policy is similar to Min; however, invocation requests can be forwarded after they arrive at a node. The NCWN algorithm, an implementation of the Naive Contracting Within a Neighborhood strategy <ref> [18, 28] </ref>, is a member of the Drift policy that uses the load values of adjacent nodes to select locations for new tasks. Region: This strategy is similar to Drift, but differs from all of the previous methods by explicitly using load and status information.
Reference: [19] <author> KAL E, L. V., AND SHU, W. </author> <title> The Chare-Kernel Language for Parallel Programming: A Perspective. </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-89-1451, University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> May </month> <year> 1989. </year> <month> 37 </month>
Reference-contexts: Several research groups continue to explore techniques that support the construction of dynamic, parallel programs without forcing programmers to resort to low-level techniques; examples include Cantor [2], Linda [4], the Chare Kernel <ref> [19] </ref>, and Mentat [10]. In general, these tools provide an abstraction of a task that can be dynamically created or destroyed, and they support a programming abstraction where the intertask communication pattern is independent of task location. <p> Moreover, most are experimental, with a limited user base and few large application codes. To study a variety of strategies within a consistent context, we captured task activity traces from two dynamic task creation systems, the Chare Kernel <ref> [19] </ref> and Concurrent Aggregates [6], and then augmented these with synthesized traces from abstract descriptions of program behavior. These synthetic traces provide a range of task behavior absent in the captured workloads. <p> Given space constraints, in this paper we concentrate on the captured execution traces; see [11, 12] for a more extended comparison. The Chare Kernel (CK) <ref> [19] </ref> is a portable environment for distributed computation using chares, 14 Minmax IDA*-15 Queens Cubes MultiGrid Environment CK CK CK CK CA Number of Tasks 2,801 5,676 3,874 1,933 949 Task Depth 4 23 15 3 17 Total Messages 2,800 5,646 8,189 5,313 28,843 Total Message Size (bytes) 11,200 90,336 393,072
Reference: [20] <author> KELLER, R., LIN, F., AND TANAKA, J. </author> <title> Rediflow Multiprocessing. </title> <booktitle> In CompCon '84 (1984), </booktitle> <pages> pp. 410-417. </pages>
Reference-contexts: Although diffusion scheduling successfully locates idle nodes, broadcast messages are costly; newer task placement methods can locate the an idle node with fewer messages [11]. The Rediflow multiprocessor <ref> [20] </ref> was based on a mesh-connected network of nodes. For this system, Lin [22, 23] proposed the gradient strategy, a local, demand driven, adaptive load sharing strategy for task scheduling.
Reference: [21] <author> KRATZER, A., AND HAMMERSTROM, D. </author> <title> A Study of Load Leveling. </title> <booktitle> In Compcon '79 (Fall 1980), IEEE, </booktitle> <pages> pp. 647-654. </pages>
Reference-contexts: Load placement strategies assign tasks to processors when the tasks are created, whereas load redistribution strategies rearrange extant tasks to balance loads. Several groups have proposed load placement strategies <ref> [21, 29, 23, 18, 28, 3, 32, 31] </ref>, yet few comparative studies exist. Fewer studies exist for modern, low-latency network architectures.
Reference: [22] <author> LIN, F. C. H. </author> <title> Load Balancing and Fault Tolerance in Applicative Systems. </title> <type> PhD thesis, </type> <institution> University of Utah, </institution> <month> June </month> <year> 1985. </year>
Reference-contexts: Although diffusion scheduling successfully locates idle nodes, broadcast messages are costly; newer task placement methods can locate the an idle node with fewer messages [11]. The Rediflow multiprocessor [20] was based on a mesh-connected network of nodes. For this system, Lin <ref> [22, 23] </ref> proposed the gradient strategy, a local, demand driven, adaptive load sharing strategy for task scheduling. This and other, similar techniques are limited diffusion strategies because, unlike CHoPP, they depend only on local communication; no broadcasts are required.
Reference: [23] <author> LIN, F. C. H., AND KELLER, R. M. </author> <title> The Gradient Model Load Balancing Method. </title> <journal> IEEE Software SE-13, </journal> <month> 1 (January </month> <year> 1987), </year> <pages> 32-38. </pages>
Reference-contexts: Load placement strategies assign tasks to processors when the tasks are created, whereas load redistribution strategies rearrange extant tasks to balance loads. Several groups have proposed load placement strategies <ref> [21, 29, 23, 18, 28, 3, 32, 31] </ref>, yet few comparative studies exist. Fewer studies exist for modern, low-latency network architectures. <p> Although diffusion scheduling successfully locates idle nodes, broadcast messages are costly; newer task placement methods can locate the an idle node with fewer messages [11]. The Rediflow multiprocessor [20] was based on a mesh-connected network of nodes. For this system, Lin <ref> [22, 23] </ref> proposed the gradient strategy, a local, demand driven, adaptive load sharing strategy for task scheduling. This and other, similar techniques are limited diffusion strategies because, unlike CHoPP, they depend only on local communication; no broadcasts are required. <p> However, the selected node can elect to forward the task if it believes other nodes are less busy. This process continues until a forwarding limit is reached or a node believes it is the least loaded node in the network. 11 Gradient: This is Lin's gradient scheduling algorithm <ref> [23] </ref>, where each node is classified as idle, abundant, or neutral. If a task is distributed, it can only be sent to an adjacent node; that node may elect to continue transferring the task to reach an idle node.
Reference: [24] <author> NAZIEF, B. A. A. </author> <title> Load Distribution in Multicomputer Systems. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: The first time an invocation request is transferred, it is sent to the candidate node with minimum status value; on subsequent transfers, it is sent to the candidate node with the minimum load value. In <ref> [12, 24] </ref>, we examined a plethora of possible task placement strategies (i.e., many instances of each policy family with different sources of load and status information). Table 2 summarizes the parameters used in a small subset of these studies. <p> As part of our ongoing study, we are examining task migration strategies that respond to a variety of resource constraints, including processor utilization and message transmission delay <ref> [24] </ref>. We hope such strategies can refine a coarse task distribution, produced by an initial task placement strategy, then use information gleaned from task dynamics to move tasks after they have begun execution.
Reference: [25] <author> RAMKUMAR, B., AND KAL E, L. V. </author> <title> Compiled Execution of the Reduce-Or Process Model on Multiprocessors. </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-89-1513, University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: The total number of tasks and the execution time of each task are unknown at compile time. We used four traces from Chare Kernel programs; two were C programs and two were compiled ROPM Prolog <ref> [25] </ref> programs. The externally observable behavior of each Chare Kernel task was recorded in a timestamped trace. Computation time was measured using a microsecond timer and scaled to simulate a processor executing approximately ten million instructions per second.
Reference: [26] <author> RATTNER, J. </author> <title> Concurrent Processing: A New Direction in Scientific Computing. </title> <booktitle> In Proceedings of the 1985 National Computer Conference (1985), </booktitle> <publisher> AFIPS Press, </publisher> <pages> pp. 157-166. </pages>
Reference-contexts: Multicomputers have evolved from machines with modest performance computation nodes and low-speed communication networks to current generation systems with high-performance computation nodes and high-speed communication networks. Early multicomputers (e.g., the Ametek System/14 [1], the Intel iPSC/1 <ref> [26] </ref>, and the Ncube/ten [15]) were little more than research curiosities. Not only was the peak performance of their processors comparable to early personal computers, their communication network performance was poor store-and-forward packet switching created communication latency proportional to the distance between the source and destination nodes.
Reference: [27] <author> REED, D. A., AND FUJIMOTO, R. M. </author> <title> Multicomputer Networks: Message-Based Parallel Processing. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1987. </year> <month> 38 </month>
Reference-contexts: 1 Introduction Parallel computation offers surcease for computational limitations when solving larger or more complex problems than is possible on single processor systems. One promising parallel architecture is the multicomputer <ref> [27] </ref>. Multicomputers consist of a large number of interconnected computing nodes that asynchronously cooperate via a message passing network to execute the tasks of parallel programs. Each computing node minimally contains a processor, a local memory, and a communication controller.
Reference: [28] <author> SHU, W., AND KAL E, L. V. </author> <title> Dynamic Scheduling of Medium Grained Processes on Multi--processors. </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-89-1528, University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: Load placement strategies assign tasks to processors when the tasks are created, whereas load redistribution strategies rearrange extant tasks to balance loads. Several groups have proposed load placement strategies <ref> [21, 29, 23, 18, 28, 3, 32, 31] </ref>, yet few comparative studies exist. Fewer studies exist for modern, low-latency network architectures. <p> Kale and Carroll [18, 5] determined that the gradient strategy 7 did not distribute tasks rapidly enough in an idle system. Thus, CWN imposes minimum and maximum forwarding thresholds, forwarding a task at least once, but no more than a predetermined limit. Shu and Kale <ref> [28] </ref> have recently developed the Adaptive Contracting Within a Neighborhood (ACWN) distribution strategy. To distinguish the two, they refer to the prior CWN strategy as Naive CWN (NCWN). <p> One can classify task distribution strategies based on the sources of information used to make distribution decisions. Table 1, an extension of the classification scheme proposed by Shu and Kale <ref> [28] </ref>, summarizes six possible categories of task distribution strategies, ranging from information-less to global status. Note, however, that this classification does not distinguish among policies based on possible task destinations. <p> Once placed, tasks cannot be transferred to other nodes. Drift: This policy is similar to Min; however, invocation requests can be forwarded after they arrive at a node. The NCWN algorithm, an implementation of the Naive Contracting Within a Neighborhood strategy <ref> [18, 28] </ref>, is a member of the Drift policy that uses the load values of adjacent nodes to select locations for new tasks. Region: This strategy is similar to Drift, but differs from all of the previous methods by explicitly using load and status information.
Reference: [29] <author> SULLIVAN, H., AND BRASHKOW, T. </author> <title> A Large Scale Homogeneous Machine I & II. </title> <booktitle> In Proc. 4th Annual Symposium on Computer Architecture (1977), </booktitle> <pages> pp. 105-124. </pages>
Reference-contexts: Load placement strategies assign tasks to processors when the tasks are created, whereas load redistribution strategies rearrange extant tasks to balance loads. Several groups have proposed load placement strategies <ref> [21, 29, 23, 18, 28, 3, 32, 31] </ref>, yet few comparative studies exist. Fewer studies exist for modern, low-latency network architectures. <p> Instead, we present empirical data that can be used to construct and verify such models. In one of the earliest proposals, Sullivan suggested diffusion scheduling for workload distribution in the CHoPP system <ref> [29] </ref>, a binary hypercube of processors. At the time of task creation, a task 6 invocation message is broadcast to a subcube of the nodes. Idle nodes bid on the request, and one such bid is chosen by the originating node.
Reference: [30] <institution> THINKING MACHINES CORPORTATION. </institution> <address> CM-Fortran Manual. Cambridge, MA, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: In contrast, current multicomputers (e.g., the Thinking Machines CM-5 and Intel Paragon XP/S) have multi-gigaflop peak performance and high-bandwidth networks based on either circuit switching or wormhole routing. By comparison, multicomputer software has evolved slowly. Most systems are programmed using either a data parallel language like CM-Fortran <ref> [30] </ref> or HPF [16], or with a sequential programming language augmented with a message passing library. The popularity of data parallelism and the rapidity of its adoption reflect its effectiveness in succinctly expressing operations with 3 great spatial and temporal regularity (e.g., matrix operations).
Reference: [31] <author> TILBORG, A. V., AND WITTIE, L. D. </author> <title> Wave Scheduling Decentralized Scheduling of Task Forces in Multicomputers. </title> <journal> IEEE Transactions on Computers C-33, </journal> <month> 9 (September </month> <year> 1984), </year> <pages> 835-864. </pages>
Reference-contexts: Load placement strategies assign tasks to processors when the tasks are created, whereas load redistribution strategies rearrange extant tasks to balance loads. Several groups have proposed load placement strategies <ref> [21, 29, 23, 18, 28, 3, 32, 31] </ref>, yet few comparative studies exist. Fewer studies exist for modern, low-latency network architectures.
Reference: [32] <author> VAN TILBORG, A., AND WITTIE, L. D. </author> <title> Distributed Task Force Scheduling in Multi-Microcomputer Networks. </title> <booktitle> In Proceedings of the 1981 National Computer Conference (1981), AFIPS, </booktitle> <pages> pp. 283-289. </pages>
Reference-contexts: Load placement strategies assign tasks to processors when the tasks are created, whereas load redistribution strategies rearrange extant tasks to balance loads. Several groups have proposed load placement strategies <ref> [21, 29, 23, 18, 28, 3, 32, 31] </ref>, yet few comparative studies exist. Fewer studies exist for modern, low-latency network architectures.
Reference: [33] <author> XU, J., AND HWANG, K. </author> <title> Heuristic methods for dynamic load balancing in a message-passing multicomputer. </title> <journal> Journal of Parallel and Distributed Computing 18, </journal> <month> 1 (May </month> <year> 1993), </year> <pages> 1-13. 39 </pages>
Reference-contexts: An interesting conclusion from their study was that any redistribution strategy was better than none, and that simple policies were almost as effective as more complex ones. Although the problems differ, this offers encouragement for simple strategies such as random placement. The model by Xu and Hwang <ref> [33] </ref> is the most similar to our study. They examined a centrally mediated, sender-initiated load balancing hueristic using a parameteric model driven by statistical parameters. They found that a single local scheduling decsision was effective until processor 8 utilization was greater than 60%. <p> We concluded that distribution strategies should limit task forwarding by using threshold policies. Ideally, the forwarding limit should adapt to system communication overhead and expected task computation time. Xu et al <ref> [33] </ref> also found this worked well in practice. 6.2 Information Distribution Techniques and Task Graph Structure Load information can be acquired passively, by piggybacking information of application program messages, or actively, by explicit load messages. Both provide an approximate view of the system state.
References-found: 33

