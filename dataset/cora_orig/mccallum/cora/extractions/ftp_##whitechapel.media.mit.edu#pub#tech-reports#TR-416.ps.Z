URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-416.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: contact:trevor@interval.com  
Title: active methods for direct control  
Author: T. Darrell(*), S. Basu(+), C. Wren(+), A. Pentland(+) 
Date: January 14, 1997  
Affiliation: MIT Media Lab(+) Interval Research Corp.(*)  
Note: Perceptually-driven Avatars and Interfaces:  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 416 Abstract We show how machine perception techniques can allow people to use their own bodies to control complex virtual representations in computer graphic worlds. In contrast to existing solutions for motion capture, tracking people for virtual avatars or intelligent interfaces requires processing at multiple levels of resolution. We apply active perception techniques and use visual attention to track a user's pose or gesture at several scales simultaneously. We also develop an active speech interface that leverages this visual tracking ability; by electronically focusing a microphone array towards a particular user, speech recognition in acoustically cluttered environments is possible. Together, these methods allow virtual representations of people to be based on their actual expression, tracking their body and face gestures and speech utterances as they freely move about a room without attached wires, microphones, or other sensors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Akita, K., </author> <title> Analysis of body motion image sequences, </title> <booktitle> Proceedings of the 6th International Conference on Pattern Recognition, </booktitle> <pages> pp. 320-327, </pages> <month> October </month> <year> 1982. </year>
Reference-contexts: of images and sound between the user and the virtual world. 2 Perception-driven user models The idea of using machine vision to track people has been previously developed in the computer graphics literature for interactive games and experiences [20, 35, 12] and for the problem of motion or expression capture <ref> [1, 4, 15, 36, 34] </ref>. These techniques can capture a single, specific motion performed by a human user for later use in creating a special effect or animating a virtual character.
Reference: [2] <author> Ballard, D., and Brown, C., </author> <title> (1982) Computer Vision, </title> <publisher> Prentice-Hall, Engle-wood </publisher>
Reference-contexts: Establishing the calibration of a camera is a well-studied problem, and several classical techniques are available to solve it in certain broad cases <ref> [2, 16] </ref>. Typically these methods model the camera optics as a pinhole perspective optical system, and establish its parameters by matching known 3-D points with their 2-D projection.
Reference: [3] <author> BBN HARK Systems, </author> <title> HARK Users Guide, </title> <address> Cambridge MA, </address> <year> 1994. </year>
Reference-contexts: An application of this result was built into the ALIVE system in 1995. We combined this active acoustic sensor with the HARK recognition system from BBN <ref> [3] </ref>, constructed a restricted grammar for the recognizer, and then trained a virtual dog character [6] to respond to the recognition results. At ICCV95, we demonstrated unencumbered speech recognition in an open environment using this technique.
Reference: [4] <author> Bergeron, P., Lachapelle, P., </author> <title> Techniques for Animating Characters, Advanced Computer Graphics and Animation, </title> <booktitle> ACM SIGGRAPH 85 Tutorial Notes, </booktitle> <pages> pp. 6179, </pages> <year> 1985. </year>
Reference-contexts: of images and sound between the user and the virtual world. 2 Perception-driven user models The idea of using machine vision to track people has been previously developed in the computer graphics literature for interactive games and experiences [20, 35, 12] and for the problem of motion or expression capture <ref> [1, 4, 15, 36, 34] </ref>. These techniques can capture a single, specific motion performed by a human user for later use in creating a special effect or animating a virtual character.
Reference: [5] <author> Beymer, </author> <title> D.J., Face recognition under varying pose, </title> <booktitle> Proc. IEEE Conf. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 756-761, </pages> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: Other similarity measures are certainly possible; a metric which included geometric distortion as well as intensity distortion would likely offer increased robustness (for example see [24] or <ref> [5] </ref>, we are currently investigating the integration of these methods into our system.) With a smooth similarity function the similarity score of a particular view model as the object undergoes non-linear transformations will be a roughly convex function.
Reference: [6] <author> Blumberg, B and Tinsley Galyean. </author> <title> Multi-level Direction of Autonomous Creatures for Real-Time Virtual Environ ments. </title> <booktitle> Proceedings of SIG-GRAPH 95 , 1995. </booktitle>
Reference-contexts: Computer vision routines analyze the image of the person to allow him to effect the virtual world through direct manipulation and/or gestural commands. (b) Image presented on video wall. With this system, user can interact with virtual agents, such as the dog character in our ALIVE system <ref> [12, 6] </ref> (c) Results of feature tracking routine; head, hands, and feet are marked with color-coded balls. Our experimental interaction domain is a smart room or office with an interactive graphics display. The user faces a wall-size screen which contains cameras and other sensors that observe the user. <p> An application of this result was built into the ALIVE system in 1995. We combined this active acoustic sensor with the HARK recognition system from BBN [3], constructed a restricted grammar for the recognizer, and then trained a virtual dog character <ref> [6] </ref> to respond to the recognition results. At ICCV95, we demonstrated unencumbered speech recognition in an open environment using this technique. The system filtered out noise from other observers/sound sources in the room, and users were able to successfully command the dog character using both speech and gesture.
Reference: [7] <author> Casey, M., Gardner, W., and Basu, S., </author> <title> Vision Steered Beam-forming and Transaural Rendering for the Artificial Life Interactive Virtual Environment (ALIVE). </title> <journal> In Proc. Audio Eng. Soc. </journal> <volume> Conv., </volume> <year> 1995. </year> <month> 21 </month>
Reference-contexts: The polar response patterns for this arrangement are shown in Figure 10 (b). A detailed examination of the response patterns with different array geometries and element responses is developed in <ref> [7] </ref>. With this beamforming implementation, we were able to sufficiently in 16 crease the signal to noise ratio to successfully feed the output into a commercial speech recognition system. This is particularly impressive in that these systems are typically designed to work only with noise-cancelling headset mics.
Reference: [8] <author> Cox, H., </author> <title> Robust Adaptive Beamforming IEEE Transactions on Acoustics, </title> <booktitle> Speech and Signal Processing, </booktitle> <volume> 35(10) </volume> <pages> 1365-1376, </pages> <year> 1987. </year>
Reference-contexts: As a result, we have the advantages of a static beamforming solution that is adaptive through the use of vision information. Beamforming is a relatively old technique; it was developed in the 1950's for radar applications [22]. In addition, its use in microphone arrays has been widely studied <ref> [8, 19, 31, 33] </ref>. In our implementation, four cardioid microphones were placed 0.5 meters apart from one another in a broadside configuration due to space constraints 10 (a).
Reference: [9] <author> Darrell, T., and Pentland, A., </author> <title> Space-Time Gestures. </title> <booktitle> Proceedings IEEE CVPR-93, </booktitle> <address> New York, </address> <publisher> IEEE Comp. Soc. Press, </publisher> <year> 1993. </year>
Reference-contexts: 30 to +30 degrees of gaze angle with two reported saccades. (a) Three spatial views of an eyeball at +30, 0, and 30 of gaze angle. (b) Similarity scores of these three view models (c) Interpolated gaze angle showing these saccades, using RBF method described in text. ages into classes <ref> [9] </ref>, and then construct the class density function. For reasons of computational efficiency, when searching for the spatial location which has maximal similarity across all models, we instead use the normalized correlation of the new image with the mean (zero-th order eigenvector) of the class as our similarity measure.
Reference: [10] <author> Darrell, T., Essa, I., and Pentland, A., </author> <title> Correlation and Interpolation Networks for Real-Time Expression Analysis/Synthesis, </title> <booktitle> Advances in Neural Information Processing Systems-7, </booktitle> <editor> Tesauro, Touretzky, and Leen, eds., </editor> <publisher> MIT Press. Conference, </publisher> <year> 1994. </year>
Reference-contexts: The truncated eigenvector sequence allows rapid computation of the likelihood a new image is in a particular pose class, and thus efficient computation of our similarity score vector. see <ref> [10] </ref> for details. The similarity scores were mapped to a computer graphics face model (described in [14]) with physiologically-valid muscle parameters corresponding to different facial muscle groups; the large number of parameters makes this a difficult model to animate in real time via conventional control.
Reference: [11] <author> Darrell, T., Moghaddam, B., and Pentland, A., </author> <title> Active Face Tracking and Pose Estimation in an Interactive Room, </title> <booktitle> Proc. Computer Vision and Pattern Recognition, </booktitle> <address> CVPR-96, San Francisco. </address> <year> 1996. </year>
Reference-contexts: In the former case, we take the set of example images for a particular expression or pose class and form an model of the class probability density function (e.g., a function which returns the probability that a new image is a member of the class) using an eigenspace technique (see <ref> [11] </ref> and [23]). Our similarity function is the likelihood that a new image is the member of the given class. <p> There is considerable additional training time, but no additional run-time cost, and the system becomes much more robust to global illumination and other effects. In <ref> [11] </ref> we evaluated the recognition performance of our system using just the similarity scores, and found recognition rates in excess of 84% for the task of discriminating amonst the three head gaze poses were possible. (See video for demonstration.) 5 Active Speech Recognition Speech is an essential element of a complete
Reference: [12] <author> Darrell, T., Maes, P., Blumberg, B., Pentland, A. P., </author> <title> A Novel Environment for Situated Vision and Behavior, </title> <booktitle> Proc. IEEE Workshop for Visual Behaviors, IEEE Comp. </booktitle> <publisher> Soc. Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994 </year>
Reference-contexts: longer deaf and blind to their users, interfaces that balance the flow of images and sound between the user and the virtual world. 2 Perception-driven user models The idea of using machine vision to track people has been previously developed in the computer graphics literature for interactive games and experiences <ref> [20, 35, 12] </ref> and for the problem of motion or expression capture [1, 4, 15, 36, 34]. These techniques can capture a single, specific motion performed by a human user for later use in creating a special effect or animating a virtual character. <p> Computer vision routines analyze the image of the person to allow him to effect the virtual world through direct manipulation and/or gestural commands. (b) Image presented on video wall. With this system, user can interact with virtual agents, such as the dog character in our ALIVE system <ref> [12, 6] </ref> (c) Results of feature tracking routine; head, hands, and feet are marked with color-coded balls. Our experimental interaction domain is a smart room or office with an interactive graphics display. The user faces a wall-size screen which contains cameras and other sensors that observe the user. <p> While this is insufficient for complex sentence recognition, it is sufficient for single word interfaces. 6 Perceptive Interfaces and Avatars Many applications are possible with a perception-based user interface. Together with other colleagues at the MIT Media Lab, we have developed systems for interacting with autonomous virtual agents <ref> [12] </ref>, browsing a multimedia database using arm pointing gestures [32], playing interactive 3-D game environments using a full body interface for navigation [30], and a system for multi-user interaction in a shared virtual world [13] (Figure 11).
Reference: [13] <author> Darrell, T., Blumberg, B., Maes, P., and Pentland, A., </author> <title> ALIVE: dreams and illusions, </title> <booktitle> Visual Proceedings of SIGGRAPH 95, </booktitle> <publisher> ACM Press, </publisher> <year> 1995. </year>
Reference-contexts: colleagues at the MIT Media Lab, we have developed systems for interacting with autonomous virtual agents [12], browsing a multimedia database using arm pointing gestures [32], playing interactive 3-D game environments using a full body interface for navigation [30], and a system for multi-user interaction in a shared virtual world <ref> [13] </ref> (Figure 11). As first implemented these systems used only the fixed camera person tracking system and could thus only detect coarse body gestures. With active visual tracking, information about detailed hand or face expression of a user can now be used in the interface.
Reference: [14] <author> Essa, I., and Pentland, A. P., </author> <title> A vision system for observing and extracting facial action parameters, </title> <booktitle> In Proc. IEEE Conf. Computer Vision and Pattern Recognition, </booktitle> <year> 1994. </year>
Reference-contexts: The truncated eigenvector sequence allows rapid computation of the likelihood a new image is in a particular pose class, and thus efficient computation of our similarity score vector. see [10] for details. The similarity scores were mapped to a computer graphics face model (described in <ref> [14] </ref>) with physiologically-valid muscle parameters corresponding to different facial muscle groups; the large number of parameters makes this a difficult model to animate in real time via conventional control.
Reference: [15] <author> Ginsberg, C., and Maxwell, D., </author> <title> Graphical Marionette, </title> <booktitle> In Proc. SIG-GRAPH / SIGART Motion Workshop, </booktitle> <pages> pp. 172-179, </pages> <address> Toronto, </address> <year> 1983. </year>
Reference-contexts: of images and sound between the user and the virtual world. 2 Perception-driven user models The idea of using machine vision to track people has been previously developed in the computer graphics literature for interactive games and experiences [20, 35, 12] and for the problem of motion or expression capture <ref> [1, 4, 15, 36, 34] </ref>. These techniques can capture a single, specific motion performed by a human user for later use in creating a special effect or animating a virtual character.
Reference: [16] <author> Horn, B.K.P., </author> <title> Robot Vision, </title> <publisher> M.I.T. Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: Establishing the calibration of a camera is a well-studied problem, and several classical techniques are available to solve it in certain broad cases <ref> [2, 16] </ref>. Typically these methods model the camera optics as a pinhole perspective optical system, and establish its parameters by matching known 3-D points with their 2-D projection.
Reference: [17] <author> Kakadiaris, I. and Metaxas, D. and Bajcsy, R., </author> <title> Active Part-Decomposition, Shape and Motion Estimation of Articulated Objects: A Physics-based Approach, </title> <booktitle> Proc. </booktitle> <volume> CVPR '94, </volume> <pages> pp. 980-984, </pages> <year> 1994. </year>
Reference-contexts: Color segmentation is but one method that can be used to track people; range estimation, motion-based segmentation, and thermal imaging are other methods that have been explored in the computer vision literature <ref> [28, 29, 25, 17, 18] </ref>. We choose to use color segmentation because accurate estimates can be obtained in real-time using an implementation with no special hardware other than a video camera and pentium-class PC processor.
Reference: [18] <author> Kanade, T., Yoshida, A., Oda, K., Kano, H., and Tanaka, M., </author> <title> A Stereo Engine for Video-rate Dense Depth Mapping and Its New Applications, </title> <booktitle> Proceedings of Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 196-202, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Color segmentation is but one method that can be used to track people; range estimation, motion-based segmentation, and thermal imaging are other methods that have been explored in the computer vision literature <ref> [28, 29, 25, 17, 18] </ref>. We choose to use color segmentation because accurate estimates can be obtained in real-time using an implementation with no special hardware other than a video camera and pentium-class PC processor.
Reference: [19] <author> Khalil, F., Jullien, J.P., and Gilloire, A., </author> <title> Microphone Array for Sound Pickup in Teleconference Systems. </title> <journal> Journal of the Audio Engineering Society, </journal> <volume> 42(9) </volume> <pages> 691-699, </pages> <year> 1994. </year>
Reference-contexts: As a result, we have the advantages of a static beamforming solution that is adaptive through the use of vision information. Beamforming is a relatively old technique; it was developed in the 1950's for radar applications [22]. In addition, its use in microphone arrays has been widely studied <ref> [8, 19, 31, 33] </ref>. In our implementation, four cardioid microphones were placed 0.5 meters apart from one another in a broadside configuration due to space constraints 10 (a).
Reference: [20] <author> Krueger, M.W., </author> <title> Artificial Reality II, </title> <publisher> Addison Wesley, </publisher> <year> 1990. </year>
Reference-contexts: longer deaf and blind to their users, interfaces that balance the flow of images and sound between the user and the virtual world. 2 Perception-driven user models The idea of using machine vision to track people has been previously developed in the computer graphics literature for interactive games and experiences <ref> [20, 35, 12] </ref> and for the problem of motion or expression capture [1, 4, 15, 36, 34]. These techniques can capture a single, specific motion performed by a human user for later use in creating a special effect or animating a virtual character.
Reference: [21] <author> Litwinowitcz, P., Williams, L., </author> <title> Animating Images with Drawings, </title> <booktitle> Proc. ACM SIGGRAPH, </booktitle> <pages> pp. 409412, </pages> <year> 1994. </year> <month> 22 </month>
Reference-contexts: head/hand configuration nearest to goal state indicated by crosses. (b) Result after warping arm to goal state. (c) Avatar rendered with user (slightly smaller) to show real-time tracking ability. show silhouette from coarse-scale figure/ground segmentation and fine scale face pose analysis on active camera imagery. 19 polated keyframe technique (see <ref> [21] </ref> for a related system).
Reference: [22] <author> Mailloux, </author> <title> R.J., Phased Array Antenna Handbook. </title> <publisher> Artech House, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: This algorithm is then updated periodically (5 Hz) with the vision information. As a result, we have the advantages of a static beamforming solution that is adaptive through the use of vision information. Beamforming is a relatively old technique; it was developed in the 1950's for radar applications <ref> [22] </ref>. In addition, its use in microphone arrays has been widely studied [8, 19, 31, 33]. In our implementation, four cardioid microphones were placed 0.5 meters apart from one another in a broadside configuration due to space constraints 10 (a).
Reference: [23] <author> Moghaddam, B. and Pentland, A., </author> <title> Probabilistic Visual Learning for Object Detection, </title> <booktitle> Proc. of Int'l Conf. on Comp. Vision, Camb., </booktitle> <address> MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: former case, we take the set of example images for a particular expression or pose class and form an model of the class probability density function (e.g., a function which returns the probability that a new image is a member of the class) using an eigenspace technique (see [11] and <ref> [23] </ref>). Our similarity function is the likelihood that a new image is the member of the given class.
Reference: [24] <author> Moghaddam, B., Nastar, C., and Pentland, A., </author> <title> A Bayesian Similarity Metric for Direct Image Matching, </title> <booktitle> Proc. of Int'l Conf. on Pattern Recognition, </booktitle> <address> Zurich, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: Other similarity measures are certainly possible; a metric which included geometric distortion as well as intensity distortion would likely offer increased robustness (for example see <ref> [24] </ref> or [5], we are currently investigating the integration of these methods into our system.) With a smooth similarity function the similarity score of a particular view model as the object undergoes non-linear transformations will be a roughly convex function.
Reference: [25] <author> Ohya, J., Ebihara, K., Kurumisawa, J., and Nakatsu, R., </author> <title> Virtual Kabuki Theater: Towards the realization of human metamorphosis system, </title> <booktitle> Proc. of 5th IEEE International Workshop on Robot and Human Communication, </booktitle> <address> pp.416-421, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: Color segmentation is but one method that can be used to track people; range estimation, motion-based segmentation, and thermal imaging are other methods that have been explored in the computer vision literature <ref> [28, 29, 25, 17, 18] </ref>. We choose to use color segmentation because accurate estimates can be obtained in real-time using an implementation with no special hardware other than a video camera and pentium-class PC processor.
Reference: [26] <author> Poggio, T., and Girosi, F., </author> <title> A theory of networks for approximation and learning, </title> <institution> MIT AI Lab TR-1140, </institution> <year> 1989. </year>
Reference-contexts: View models are acquired using unsupervised clustering while the interpolation is trained using supervised learning. Using the Radial Basis Function (RBF) method presented in <ref> [26] </ref>, we compute a result vector ^y to be a weighted sum of radial functions centered at an exemplar value: ^y (g) = i=1 where c = F 1 y; (F) ij = F (jjg (i) g (j) jj); y = [y (1) ; :::; y (n) ] T ; (2)
Reference: [27] <author> Poggio, T., and Edelman, S., </author> <title> A Network that Learns to Recognize Three Dimensional Objects, </title> <journal> Nature, </journal> <volume> Vol. 343, No. 6255, </volume> <pages> pp. 263-266, </pages> <year> 1990. </year>
Reference-contexts: This approach is related to the idea of view-based representation, as advocated by Ullman [38] and Poggio <ref> [27] </ref>, for representing 3-D objects by interpolating between a small set of 2-D views. Appearance or view-based representation achieves robust real-time performance in gesture analysis by exploiting the principle of using only as much representation as needed.
Reference: [28] <author> Rehg, J.M. and Kanade, T., </author> <title> Visual Tracking of High DoF Articulated Structures: An Application to Human Hand Tracking, </title> <booktitle> Proc. </booktitle> <volume> ECCV '94, </volume> <pages> pp. </pages> <address> B:35-46, </address> <year> 1994. </year>
Reference-contexts: Color segmentation is but one method that can be used to track people; range estimation, motion-based segmentation, and thermal imaging are other methods that have been explored in the computer vision literature <ref> [28, 29, 25, 17, 18] </ref>. We choose to use color segmentation because accurate estimates can be obtained in real-time using an implementation with no special hardware other than a video camera and pentium-class PC processor.
Reference: [29] <author> Rohr, K., </author> <title> Towards Model-Based Recognition of Human Movements in Image Sequences, Comp. Vision, Graphics, </title> <booktitle> and Image Processing: Image Understanding, </booktitle> <volume> Vol. 59, no. 1, </volume> <pages> pp. 94-115, </pages> <month> Jan </month> <year> 1994. </year>
Reference-contexts: Color segmentation is but one method that can be used to track people; range estimation, motion-based segmentation, and thermal imaging are other methods that have been explored in the computer vision literature <ref> [28, 29, 25, 17, 18] </ref>. We choose to use color segmentation because accurate estimates can be obtained in real-time using an implementation with no special hardware other than a video camera and pentium-class PC processor.
Reference: [30] <author> Russell, K., Starner, T., and Pentland, A., </author> <title> Unencumbered Virtual Environments. MIT Media Laboratory Perceptual Computing Group Technical Report 305. </title> <booktitle> Appears in IJCAI '95 Workshop on Entertainment and AI/Alife. </booktitle> <address> Montreal, Canada, </address> <month> August 20-25, </month> <year> 1995. </year>
Reference-contexts: Together with other colleagues at the MIT Media Lab, we have developed systems for interacting with autonomous virtual agents [12], browsing a multimedia database using arm pointing gestures [32], playing interactive 3-D game environments using a full body interface for navigation <ref> [30] </ref>, and a system for multi-user interaction in a shared virtual world [13] (Figure 11). As first implemented these systems used only the fixed camera person tracking system and could thus only detect coarse body gestures.
Reference: [31] <author> Soede, W., Berkhout, A., and Bilsen, F. </author> <title> Development of a Directional Hearing Instrument Based on Array Technology. </title> <journal> Journal of the Aous-tical Society of America, </journal> <volume> 94(2) </volume> <pages> 785-798, </pages> <year> 1993. </year>
Reference-contexts: As a result, we have the advantages of a static beamforming solution that is adaptive through the use of vision information. Beamforming is a relatively old technique; it was developed in the 1950's for radar applications [22]. In addition, its use in microphone arrays has been widely studied <ref> [8, 19, 31, 33] </ref>. In our implementation, four cardioid microphones were placed 0.5 meters apart from one another in a broadside configuration due to space constraints 10 (a).
Reference: [32] <author> Sparacino, F., Wren, C., Pentland, A., Davenport G., HyperPlex: </author> <title> a World of 3D Interactive Digital Movies, </title> <booktitle> Appears in IJCAI '95 Workshop on Entertainment and AI/Alife. </booktitle> <address> Montreal, Canada, </address> <month> August 20-25, </month> <year> 1995. </year>
Reference-contexts: Together with other colleagues at the MIT Media Lab, we have developed systems for interacting with autonomous virtual agents [12], browsing a multimedia database using arm pointing gestures <ref> [32] </ref>, playing interactive 3-D game environments using a full body interface for navigation [30], and a system for multi-user interaction in a shared virtual world [13] (Figure 11). As first implemented these systems used only the fixed camera person tracking system and could thus only detect coarse body gestures.
Reference: [33] <author> Stadler, R., and Rabinowitz, W., </author> <title> On the Potential of Fixed Arrays for Hearing Aids. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 94(3) </volume> <pages> 1332-1342, </pages> <year> 1993. </year>
Reference-contexts: As a result, we have the advantages of a static beamforming solution that is adaptive through the use of vision information. Beamforming is a relatively old technique; it was developed in the 1950's for radar applications [22]. In addition, its use in microphone arrays has been widely studied <ref> [8, 19, 31, 33] </ref>. In our implementation, four cardioid microphones were placed 0.5 meters apart from one another in a broadside configuration due to space constraints 10 (a).
Reference: [34] <author> Terzopoulos, D., and Waters, K., </author> <title> Physically-based Facial Modeling, Analysis, and Animation, </title> <journal> The Journal of Visualization and Computer Animation, </journal> <volume> vol. 1, </volume> <pages> pp. 7380, </pages> <year> 1990. </year> <month> 23 </month>
Reference-contexts: of images and sound between the user and the virtual world. 2 Perception-driven user models The idea of using machine vision to track people has been previously developed in the computer graphics literature for interactive games and experiences [20, 35, 12] and for the problem of motion or expression capture <ref> [1, 4, 15, 36, 34] </ref>. These techniques can capture a single, specific motion performed by a human user for later use in creating a special effect or animating a virtual character.
Reference: [35] <author> Vincent, V., Mandala: </author> <title> Virtual Village, </title> <booktitle> SIGGRAPH-93 Visual Proceedings, Tomorrow's Realities, ACM SIGGRAPH 1993, </booktitle> <pages> pp. 207, </pages> <year> 1993. </year>
Reference-contexts: longer deaf and blind to their users, interfaces that balance the flow of images and sound between the user and the virtual world. 2 Perception-driven user models The idea of using machine vision to track people has been previously developed in the computer graphics literature for interactive games and experiences <ref> [20, 35, 12] </ref> and for the problem of motion or expression capture [1, 4, 15, 36, 34]. These techniques can capture a single, specific motion performed by a human user for later use in creating a special effect or animating a virtual character.
Reference: [36] <author> Williams, L., </author> <title> Performance-driven facial animation, </title> <journal> Proc. ACM SIG-GRAPH, </journal> <volume> Vol. 24, No. 4, </volume> <pages> pp. 235-242, </pages> <year> 1990. </year>
Reference-contexts: of images and sound between the user and the virtual world. 2 Perception-driven user models The idea of using machine vision to track people has been previously developed in the computer graphics literature for interactive games and experiences [20, 35, 12] and for the problem of motion or expression capture <ref> [1, 4, 15, 36, 34] </ref>. These techniques can capture a single, specific motion performed by a human user for later use in creating a special effect or animating a virtual character.
Reference: [37] <author> Wren, C., Azarbayejani, A., Darrell, T., and Pentland, A. pfinder: </author> <title> Real-Time Tracking of the Human Body, </title> <booktitle> Second Int'l Conf. on Automatic Face and Gesture Recognition, </booktitle> <address> Killington, VT, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: We use a multi-class color classification test to compute figure/ground segmentation, using a single Gaussian model of background pixel color and an adaptive mixture model of foreground (person) colors. The color classification takes care to identify possible shadow regions, and to normalize these region's brightness before the figure/ground classification <ref> [37] </ref>. Once the set of pixels most likely belonging to the user has been found, we use connected components and morphological analysis to delineate the foreground region. <p> Likely hand and face locations are set to strong flesh-colored color priors; others are initialized to cover clothing regions. A competitive method reallocates support in the foreground image among the mixture model elements, creating or deleting new elements as needed <ref> [37] </ref>. With this framework hands can be tracked in front of the body; when one reappears after being occluded or shadowed only a few frames of video are needed to regain tracking.
Reference: [38] <author> Ullman, S., and Basri, R., </author> <title> Recognition by Linear Combinations of Models, </title> <journal> IEEE PAMI, </journal> <volume> Vol. 13, No. 10, </volume> <pages> pp. 992-1007, </pages> <year> 1991. </year> <month> 24 </month>
Reference-contexts: This approach is related to the idea of view-based representation, as advocated by Ullman <ref> [38] </ref> and Poggio [27], for representing 3-D objects by interpolating between a small set of 2-D views. Appearance or view-based representation achieves robust real-time performance in gesture analysis by exploiting the principle of using only as much representation as needed.
References-found: 38

