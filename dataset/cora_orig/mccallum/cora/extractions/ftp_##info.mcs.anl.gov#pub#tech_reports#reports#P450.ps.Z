URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P450.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: On Orthogonal Block Elimination  and the canonical kernel S (A  
Author: Christian Bischof and Xiaobai Sun 
Keyword: Key words. Orthogonal Matrices, Block Elimination, Basis-Kernel Representation, Canonical Basis and Kernel, Householder Matrices, Sparse Matrices, Orthogonal Factorization.  
Note: Argonne Preprint MCS-P450-0794 A 2 C  A 2  1 C) C T which can be  
Address: Argonne, IL 60439  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Email: fbischof,xiaobaig@mcs.anl.gov  
Web: where,  
Abstract: We consider the block elimination problem Q given a matrix A 2 R mfik , A 11 2 R kfik , we try to find a matrix C with C T C = A T A and an orthogonal matrix Q that eliminates A 2 . Sun and Bischof recently showed that any orthogonal matrix can be represented in the so-called basis-kernel representation Q = Q(Y; S) = I Y ST T . Applying this framework to the block elimination problem, we show that there is considerable freedom in solving the block elimination problem and that, depending on A and C, we can find Y 2 R mfir , S 2 R rfir , where r is between rank(A 2 ) and k, to solve the block elimination problem. We then introduce the canonical determined easily once C has been computed, and relate this view to previously suggested approaches for computing block orthogonal matrices. We also show that the condition of S has a fundamental impact on the numerical stability with which Q can be computed and prove that the well-known compact WY representation approach, employed, for example, in LAPACK, leads to a well-conditioned kernel. Lastly, we discuss the computational promise of the canonical basis and kernel, in particular in the sparse setting, and suggest pre- and postconditioning strategies to ensure that S can be computed reliably and is wellconditioned. fl This work was supported by the Applied and Computational Mathematics Program, Advanced Research Projects Agency, under contract DM28E04120, and by the Office of Scientific Computing, U.S. Department of Energy, under Contract W-31-109-Eng-38. This paper is PRISM Working Note #21, available via anonymous ftp to ftp.super.org in the directory pub/prism. basis Y =
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: This reduction in mem-ory requirement may be significant, since typically m AE k. Block orthogonal transformations based on the compact WY form have, for example, been incorporated into the LAPACK library <ref> [1] </ref>. Such blocking approaches are particular solutions to the orthogonal block elimination problem: Given a matrix A = A 2 , A 2 R mfik , A 1 2 R kfik , m &gt; k.
Reference: [2] <author> Christian H. Bischof and Per Christian Hansen. </author> <title> Structure-preserving and rank-revealing QR factorizations. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(6) </volume> <pages> 1332-1350, </pages> <month> November </month> <year> 1991. </year> <month> 25 </month>
Reference: [3] <author> Christian H. Bischof and Per Christian Hansen. </author> <title> A block algorithm for computing rank-revealing QR factorizations. Numerical Algorithms, </title> <address> 2(3-4):371-392, </address> <year> 1992. </year>
Reference: [4] <author> Christian H. Bischof and Charles F. Van Loan. </author> <title> The WY representation for products of Householder matrices. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 8:s2-s13, </volume> <year> 1987. </year>
Reference-contexts: An example is the WY representation <ref> [4] </ref> Q = I + W Y T ; (2) where W and Y are m fi k matrices and Y is composed of the Householder vectors of H i .
Reference: [5] <author> Tony F. Chan. </author> <title> Rank revealing QR factorizations. </title> <journal> Linear Algebra and Its Applications, </journal> 88/89:67-82, 1987. 
Reference: [6] <author> G. Dietrich. </author> <title> A new formulation of the hypermatrix Householder-QR decomposition. </title> <booktitle> Computer Methods in Applied Mechanical Engineering., </booktitle> <volume> 9 </volume> <pages> 273-280, </pages> <year> 1976. </year>
Reference-contexts: For example, we proved in Lemma 5 that there is always a factor U such that the canonical kernel S is nonsingular, and hence Y is of full rank. Dietrich <ref> [6] </ref> used such a choice in his work to avoid the case of a rank-deficient kernel. Kaufman [11] used essentially diagonal orthogonal factors. Her algorithm assumed that A 1 , the top kfik submatrix of A, is upper triangular, or applied an initial QR factorization to A 1 first.
Reference: [7] <author> Jack Dongarra and Sven Hammarling. </author> <title> Evolution of Numerical Software for Dense Linear Algebra, </title> <address> pages 297-327. </address> <publisher> Oxford University Press, Oxford, </publisher> <address> UK, </address> <year> 1989. </year>
Reference-contexts: Computationally, the application of H (or H T ) to a matrix B involves a matrix-vector product and a rank-one update. On modern machines with memory hierarchies and, in particular, parallel machines with distributed memories, matrix-matrix operations, especially matrix-matrix multiply <ref> [7, 8] </ref>, significantly outperform matrix-vector operations.
Reference: [8] <author> Jack J. Dongarra, Iain S. Duff, Danny C. Sorensen, and Henk A. Van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared-Memory Computers. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: Computationally, the application of H (or H T ) to a matrix B involves a matrix-vector product and a rank-one update. On modern machines with memory hierarchies and, in particular, parallel machines with distributed memories, matrix-matrix operations, especially matrix-matrix multiply <ref> [7, 8] </ref>, significantly outperform matrix-vector operations.
Reference: [9] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Orthogonal transformations are a well-known tool in numerical linear algebra and are used extensively in decompositions such as the QR factorization, tridiagonalization, bidiagonalization, Hessenberg reduction, or the eigenvalue or singular value decomposition of a matrix (see, for example, <ref> [9, 15] </ref>). <p> In the former case, C is triangular; in the latter case, it is symmetric. The usual block Householder approach (see, for example, <ref> [9, pp. 211-213] </ref>), employed, for example, in LAPACK, to solve the block elimination problem (4) essentially consists of two parts. First, compute an unblocked QR factorization of A to generate k Householder transformations. Second, accumulate a compact WY representation [9, pp. 211-213] for block updates. <p> The usual block Householder approach (see, for example, <ref> [9, pp. 211-213] </ref>), employed, for example, in LAPACK, to solve the block elimination problem (4) essentially consists of two parts. First, compute an unblocked QR factorization of A to generate k Householder transformations. Second, accumulate a compact WY representation [9, pp. 211-213] for block updates. This approach results in a triangular image C. <p> Thus, G 1 V s = U V s S I ! The CS decomposition of G (see, for example, <ref> [9] </ref>) then implies that the last k r columns of G 2 V s are zeros exactly when the last k r columns of (G 1 + U )V s are zero and its first r columns are of full rank. <p> Example 8 (Block Reflectors). Suppose G 1 = V l V T r is a SVD of G 1 . r for some (real) diagonal matrix D such that jDj = I. Then 1. G 1 = U M is a polar decomposition of G 1 <ref> [9, p. 148] </ref>, where M = V r DV T r is symmetric. 2. S = (G 1 + U ) y U = (I + M ) y V = V r (I + D)V T r is symmetric. 3. <p> We also note that the computation of the WY representation for k Householder vectors of length m takes 2k 2 (m k=3) flops for the WY basis and mk (k + 1) for the WY kernel <ref> [9, p. 212] </ref>. In this section we discuss other, more block-oriented, ways of achieving reasonably conditioned basis-kernel representations, and also consider the issue of sparsity.
Reference: [10] <author> Nicholas J. Higham. </author> <title> Computing the polar decomposition with applications. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 7 </volume> <pages> 1160-1174, </pages> <year> 1986. </year>
Reference-contexts: Parlett [12] showed, however, that the alternate can be computed in an equally stable fashion. In their computational procedures for block reflectors, Schreiber and Par-lett [13] use Higham's algorithm <ref> [10] </ref> to compute the polar decomposition of G 1 , and hence implicitly chose D = I. In this case, S = V r (I + ) 1 V T r is positive definite, and S is always extremely wellconditioned, as 2 (S) 2.
Reference: [11] <author> Linda Kaufman. </author> <title> The generalized Householder transformation and sparse matrices. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 90 </volume> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: For example, we proved in Lemma 5 that there is always a factor U such that the canonical kernel S is nonsingular, and hence Y is of full rank. Dietrich [6] used such a choice in his work to avoid the case of a rank-deficient kernel. Kaufman <ref> [11] </ref> used essentially diagonal orthogonal factors. Her algorithm assumed that A 1 , the top kfik submatrix of A, is upper triangular, or applied an initial QR factorization to A 1 first.
Reference: [12] <author> Beresford N. Parlett. </author> <title> Analysis of algorithms for reflections in bisectors. </title> <journal> SIAM Review, </journal> <volume> 13 </volume> <pages> 197-208, </pages> <year> 1971. </year>
Reference-contexts: The LAPACK selection d = sign (g 1 ) (see (18)) for a nontrivial Householder matrix results in the smaller scaling factor S among the two choices. Parlett <ref> [12] </ref> showed, however, that the alternate can be computed in an equally stable fashion. In their computational procedures for block reflectors, Schreiber and Par-lett [13] use Higham's algorithm [10] to compute the polar decomposition of G 1 , and hence implicitly chose D = I.
Reference: [13] <author> Robert Schreiber and Beresford Parlett. </author> <title> Block reflectors: </title> <journal> Theory and computation. SIAM Journal on Numerical Analysis, </journal> <volume> 25(1) </volume> <pages> 189-205, </pages> <year> 1988. </year>
Reference-contexts: We call the matrix C the image of A under Q. The orthogonality of Q implies that C must satisfy what Schreiber and Parlett <ref> [13] </ref> called the isometry property, C T C = A T A: (5) The Cholesky factor of A T A or the square root of A T A, for example, satisfies this condition. In the former case, C is triangular; in the latter case, it is symmetric. <p> That is, depending on the choice of the image of A, the degree of the orthogonal matrix for the block elimination problem for a full rank matrix A with k columns may be anywhere between rank (A 2 ) and k. 4.2 Block Reflectors Schreiber and Parlett <ref> [13] </ref> developed a theory on block reflectors, which are symmetric orthogonal matrices Q = I Y T Y T ; Q T Q = I; T = T T ; Y 2 R mfik : (19) A particular example is the situation where Y has orthonormal columns and T = 2I. <p> Parlett [12] showed, however, that the alternate can be computed in an equally stable fashion. In their computational procedures for block reflectors, Schreiber and Par-lett <ref> [13] </ref> use Higham's algorithm [10] to compute the polar decomposition of G 1 , and hence implicitly chose D = I. In this case, S = V r (I + ) 1 V T r is positive definite, and S is always extremely wellconditioned, as 2 (S) 2. <p> In this case, S = V r (I + ) 1 V T r is positive definite, and S is always extremely wellconditioned, as 2 (S) 2. The case that the actual block size could be smaller than the number of columns of A was first mentioned in <ref> [13] </ref>, although the link to the rank of A 2 (or G 2 ) was not recognized. 4.3 The Compact WY Representation The WY approach for generating orthogonal transformations for the block elimination problem does not require an orthonormal basis of R (A) or of R (Y ). <p> Kaufman was also the first one to implicitly exploit the the sparsity of A preserved in the canonical basis and to observe the stability problems arising from ill-conditioned kernels. Except for Schreiber and Parlett <ref> [13] </ref>, all previous approaches tried to avoid producing a singular kernel. Our theory shows that, instead of being a problem, the singularity of a kernel can be taken advantage of, as it allows the generation of an orthogonal transformation of lower degree. <p> The Cholesky factor of A T A is an obvious choice. If A has orthonormal columns, the Cholesky factor is I, the identity. Therefore, the first step of the algorithms in <ref> [13] </ref> is to orthonormalize A's columns, for example, with the conventional Householder QR factorization. The conventional Householder QR procedure is also considered a reliable procedure to compute Cholesky factors.
Reference: [14] <author> Robert Schreiber and Charles Van Loan. </author> <title> A storage efficient WY repre-sentation for products of Householder transformations. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 10(1) </volume> <pages> 53-57, </pages> <year> 1989. </year>
Reference-contexts: An example is the WY representation [4] Q = I + W Y T ; (2) where W and Y are m fi k matrices and Y is composed of the Householder vectors of H i . Mathematically equivalent is the compact WY representation <ref> [14] </ref> Q = I Y SY T ; (3) where S is a k-by-k triangular matrix. We see that the compact WY rep resentation requires only O (k 2 ) workspace for the matrix S, compared with 2 the WY representation's O (mk) workspace for W .
Reference: [15] <author> G. W. Stewart. </author> <title> Introduction to Matrix Computation. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: 1 Introduction Orthogonal transformations are a well-known tool in numerical linear algebra and are used extensively in decompositions such as the QR factorization, tridiagonalization, bidiagonalization, Hessenberg reduction, or the eigenvalue or singular value decomposition of a matrix (see, for example, <ref> [9, 15] </ref>). <p> If S is illconditioned with respect to inversion, then S could be quite big <ref> [15] </ref>. The simplest and best-conditioned kernel is a multiple of the identity matrix. We know from the orthogonality condition (22) that a nonsingular symmetric kernel S is 2I if and only if the associated Householder basis Y is orthonormal.
Reference: [16] <author> Xiaobai Sun and Christian Bischof. </author> <title> A basis-kernel representation of orthogonal matrices. </title> <type> Preprint MCS-P431-0594, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year> <month> 27 </month>
Reference-contexts: First, compute an unblocked QR factorization of A to generate k Householder transformations. Second, accumulate a compact WY representation [9, pp. 211-213] for block updates. This approach results in a triangular image C. Recently, Sun and Bischof <ref> [16] </ref> showed that, far from being just a convenient way of formulating products of Householder matrices, any orthogonal matrix can be expressed in the form (3), where S need not necessarily be triangular. <p> Among other results, the paper <ref> [16] </ref> showed the following: 3 * Given an arbitrary basis-kernel representation of Q, one can construc-tively derive a regular basis-kernel representation, namely, one in which both Y and S have full rank. <p> Partitioning Y = [ Y 1 ; Y 2 ], where Y 1 is m fi r, we have Y T 2 A = 0. The proof of Theorem 5 in <ref> [16] </ref> showed that there exist a lower triangular matrix L and an upper triangular matrix R such that S = LRL T . Thus, Q ( Y ; S) = Q ( ~ Y ; R) with ~ Y = Y L. <p> The claim holds if G 1 + U is nonsingular. We consider the case that G 1 + U is singular. From the proof of Lemma 2 in <ref> [16] </ref>, we have U T (G 1 + U ) = V s S ! s for some orthogonal matrix V s and a nonsingular matrix S of order, say, r. <p> Applying the conventional WY approach to the transformed block elimination problem (16), we get a diagonal image U as a result of the orthogonality among G's columns. The diagonal elements of U are determined one by one by the rule (18). On the other hand, Corollary 6 in <ref> [16] </ref> showed that, given an arbitrary k fi k diagonal matrix D, jDj = I, one can determine a sequence of Householder matrices with corresponding Householder vectors 14 y 1 ; ; y k so that WY (y 1 ; ; y k )G = D ! As it turns out, <p> Proof. If the Householder matrices determining WY (y 1 ; ; y k ) are all equal to the identity, then W Y (y 1 ; ; y k ) = I is already of the lowest degree. Otherwise, it was shown in <ref> [16] </ref> that W Y (y 1 ; ; y k ) = Q ( ^ Y ; ^ S) for a regular basis ^ Y and a nonsingular kernel ^ S, where ^ Y consists of the nonzero Householder vectors. <p> Therefore, achieving a kernel with unity condition number is computationally too expensive under normal circumstances. We also note that Example 8 shows that there exist representations for symmetric orthogonal matrices (or equivalently, block reflectors), with ill-conditioned kernels. Theorem 5 in <ref> [16] </ref> showed that any orthogonal matrix can be represented with a triangular kernel. As it turns out, the condition of a triangular kernel is greatly influenced by the scaling of its corresponding basis. <p> So now assume that we have determined a picture C of A that results in an ill-conditioned kernel, and hence an numerically rank-deficient basis. As was shown in the proof of Lemma 2 in <ref> [16] </ref>, we can derive a factorization S = F ^ SF T of S so that ^ S is wellconditioned, and then use the conditioned basis-kernel representation Q (Y; S) = Q (Y F; ^ S).
References-found: 16

