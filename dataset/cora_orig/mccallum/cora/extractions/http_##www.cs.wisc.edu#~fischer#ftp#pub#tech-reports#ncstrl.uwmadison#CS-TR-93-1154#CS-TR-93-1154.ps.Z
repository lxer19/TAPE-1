URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1154/CS-TR-93-1154.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-93-1154/
Root-URL: http://www.cs.wisc.edu
Title: Memory-Adaptive External Sorting  
Author: HweeHwa Pang Michael J. Carey Miron Livny 
Note: This work was partially supported by a scholarship from the Institute of Systems Science, National University of Singapore, and by an IBM Research Initiation Grant.  
Address: Wisconsin Madison  
Affiliation: Computer Sciences Department University of  
Abstract: In real-time and goal-oriented database systems, the amount of memory assigned to queries that sort or join large relations may fluctuate due to contention from other higher-priority transactions. This study focuses on techniques that enable external sorts both to reduce their buffer usage when they lose memory, and to effectively utilize any additional buffers that are given to them. We also show how these techniques can be extended to work with sort-merge joins. A series of experiments confirms that our proposed techniques are useful for sorting and joining large relations in the face of memory fluctuations. An abridged version of this paper appears in the proceedings of the 19th International Conference on Very Large Data Bases, August 1993. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
Abstract-found: 1
Intro-found: 1
Reference: [Abbo88] <author> R. Abbott, H. Garcia-Molina, </author> <title> "Scheduling Real-Time Transactions: A Performance Evaluation", </title> <booktitle> Proc. of the 14th Int. Conf. on Very Large Data Bases, </booktitle> <month> August </month> <year> 1988. </year>
Reference: [Bitt88] <author> D. Bitton, J. Gray, </author> <title> "Disk Shadowing", </title> <booktitle> Proc. of the 14th Int. Conf. on Very Large Data Bases, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: 40,000 Copy a tuple to output buffer 64 Terminate a sort 10,000 Compare two keys 50 Start an I/O operation 1000 iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c Table 4: Number of CPU Instructions Per Operation - 13 - As in <ref> [Bitt88] </ref>, the time required to seek across n tracks is given by: Seek Time (n) = SeekFactor dd n Finally, the system has a total memory size of M MBytes. A memory reservation mechanism is provided to allow operators, including sorts and joins, to reserve buffers.
Reference: [Blas77] <author> M.W. Blasgen, </author> <title> K.P. Eswaran, "Storage and Access in Relational Databases", </title> <journal> IBM Systems Journal, </journal> <volume> Vol. 16,4, </volume> <year> 1977. </year>
Reference-contexts: This study focuses on the same problem for large external sorts, i.e. sorts that involve relations that cannot fit entirely in the available memory, and for sort-merge joins. Sorting is frequently used in database systems to produce ordered query results. It is also the basis of sort-merge join <ref> [Blas77] </ref>, a join algorithm employed by many existing DBMSs, and it is used in some systems for processing group-by queries.
Reference: [Brat84] <author> K. Bratbergsengen, </author> <title> "Hashing Methods and Relational Algebra Operations", </title> <booktitle> Proc. of the 10th Int. Conf. on Very Large Data Bases, </booktitle> <month> August </month> <year> 1984. </year>
Reference-contexts: We therefore did not pursue this option. - 29 - 6. Sort-Merge Joins Sort-merge join is a join algorithm employed by many existing database systems. Although recent work has shown hash join to often be superior to sort-merge join in performing ad-hoc join operations <ref> [Brat84, DeWi84, Shap86] </ref>, sort-merge join is still useful under certain conditions, e.g. when significant data skew is present, or when the results need to be presented in sorted order [Grae93]. Hence sort-merge join is likely to continue to be offered as one of the alternative join algorithms in future DBMSs.
Reference: [Brow93] <author> K.P. Brown, M.J. Carey, M. Livny, </author> <title> "Managing Memory to Meet Multiclass Workload Response Time Goals", </title> <booktitle> Proc. of the 19th Int. Conf. on Very Large Data Bases, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: 1. Introduction Database management systems (DBMS) are faced with increasingly demanding performance objectives. These objectives include time constraints, as in real-time database systems [SIGM88, RTS92], and administratively-defined performance goals, as in goal-oriented database systems <ref> [Ferg93, Brow93] </ref>. Traditional DBMS scheduling policies are no longer adequate to meet such objectives; a DBMS has to prioritize transactions that are competing for system resources according to the system-wide objectives and the resource requirements of the transactions.
Reference: [DeWi84] <author> D.J. DeWitt, R.H. Katz, F. Olken, L.D. Shapiro, M. Stonebraker, D. Wood, </author> <title> "Implementation Techniques for Main Memory Database Systems", </title> <booktitle> Proc. of the ACM 1984 SIGMOD Conf., </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: We therefore did not pursue this option. - 29 - 6. Sort-Merge Joins Sort-merge join is a join algorithm employed by many existing database systems. Although recent work has shown hash join to often be superior to sort-merge join in performing ad-hoc join operations <ref> [Brat84, DeWi84, Shap86] </ref>, sort-merge join is still useful under certain conditions, e.g. when significant data skew is present, or when the results need to be presented in sorted order [Grae93]. Hence sort-merge join is likely to continue to be offered as one of the alternative join algorithms in future DBMSs.
Reference: [DeWi90] <author> D.J. DeWitt, S. Ghandeharizadeh, D.A. Schneider, A. Bricker, H.-I Hsiao, R. Rasmussen, </author> <title> "The Gamma Database Machine Project", </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> Vol. 2,1, </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: The MIPS rating of the CPU is given by CPUSpeed. Table 4 gives the cost of various CPU operations that are involved in the execution of external sorts and sort-merge joins. These CPU costs are based on instruction counts taken from the Gamma database machine <ref> [DeWi90] </ref>. Turning to the disk model parameters in Table 3, NumDisks specifies the number of disks attached to the system. Each disk has its own queue and disk requests are serviced according to the elevator algorithm. The characteristics of the disks are also given in Table 3.
Reference: [DeWi91] <author> D.J. DeWitt, J.F. Naughton, D.A. Schneider, </author> <title> "Parallel Sorting on a Shared-Nothing Architecture using Probabilistic Splitting", </title> <booktitle> Proc. of the Int. Conf. on Parallel and Distributed Information Systems, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: A nice discussion of the details involved in implementing replacement selection can be found in [Salz90]. - 3 - Although using replacement selection instead of Quicksort can shorten the merge phase, replacement selection is not always the preferred choice because it can also lead to a longer split phase <ref> [Grae90, DeWi91] </ref>. With Quicksort, there is a cycle of reading several pages from the source relation, sorting them, and then writing them to disk. In contrast, replacement selection alternates between reading a page from the source relation and writing a page to the current run.
Reference: [Ferg93] <author> D. Ferguson, C. Nikolaou, L. Georgiadis, </author> <title> "Goal Oriented, Adaptive Transaction Routing for High Performance Transaction Processing Systems", </title> <booktitle> Proc. of the 2nd Int. Conf. on Parallel and Distributed Information Systems, </booktitle> <month> January </month> <year> 1989. </year>
Reference-contexts: 1. Introduction Database management systems (DBMS) are faced with increasingly demanding performance objectives. These objectives include time constraints, as in real-time database systems [SIGM88, RTS92], and administratively-defined performance goals, as in goal-oriented database systems <ref> [Ferg93, Brow93] </ref>. Traditional DBMS scheduling policies are no longer adequate to meet such objectives; a DBMS has to prioritize transactions that are competing for system resources according to the system-wide objectives and the resource requirements of the transactions.
Reference: [Grae90] <author> G. Graefe, </author> <title> "Parallel External Sorting in Volcano", </title> <type> Technical Report CU-CS-459-90, </type> <institution> University of Colorado, Boulder, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: A nice discussion of the details involved in implementing replacement selection can be found in [Salz90]. - 3 - Although using replacement selection instead of Quicksort can shorten the merge phase, replacement selection is not always the preferred choice because it can also lead to a longer split phase <ref> [Grae90, DeWi91] </ref>. With Quicksort, there is a cycle of reading several pages from the source relation, sorting them, and then writing them to disk. In contrast, replacement selection alternates between reading a page from the source relation and writing a page to the current run. <p> In contrast, replacement selection alternates between reading a page from the source relation and writing a page to the current run. When the source relation and the run reside on the same disk, this results in many more disk seeks than in the case of Quicksort <ref> [Grae90] </ref>. <p> An alternate strategy is to merge just enough runs in the first step so that each of the subsequent steps merges m - 1 runs. Figure 1 (b) illustrates the second strategy. The first merging strategy is called "naive" merging, and the second strategy is called "optimized" merging <ref> [Grae90] </ref>. From Figures 1 (a) and 1 (b), it should be apparent that "naive" merging is more expensive than "optimized" merging, as the final step has to process all of the tuples in the relation in both strategies.
Reference: [Grae93] <author> G. Graefe, A. Linville, L.D. Shapiro, </author> <title> "Sort versus Hash Revisited", </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <note> to appear, </note> <year> 1993. </year>
Reference-contexts: By merging more runs, "naive" merging increases the cost of the preliminary steps unnecessarily. Thus, the general rule is to adopt "optimized" merging <ref> [Grae93] </ref>. Another important aspect of the merging strategy concerns the choice of input runs. All of the merge steps, other than the final merge, have a choice of input runs and should thus merge the shortest possible runs. <p> The decision of naive to include more runs in the first preliminary step thus leads to an increase in the cost of each of these affected steps <ref> [Grae93] </ref>. The more merge steps there are, the larger the number of affected steps becomes, and consequently the higher the penalty of naive gets. <p> Although recent work has shown hash join to often be superior to sort-merge join in performing ad-hoc join operations [Brat84, DeWi84, Shap86], sort-merge join is still useful under certain conditions, e.g. when significant data skew is present, or when the results need to be presented in sorted order <ref> [Grae93] </ref>. Hence sort-merge join is likely to continue to be offered as one of the alternative join algorithms in future DBMSs. In this section, we address the issue of extending the techniques that we have explored earlier to handle sort-merge joins, thus making them memory-adaptive. 6.1.
Reference: [Hari90] <author> J. Haritsa, M. Carey, M. Livny, </author> <title> "On Being Optimistic about Real-Time Constraints", </title> <booktitle> Proc. of the 1990 ACM PODS Symposium, </booktitle> <month> April </month> <year> 1990. </year>
Reference: [Huan89] <author> J. Huang, J.A. Stankovic, D. Towsley, K. Ramamritham, </author> <title> "Experimental Evaluation of Real-Time Transaction Processing", </title> <booktitle> Proc. of the 1989 IEEE 10th Real-Time Systems Symposium (RTSS). </booktitle>
Reference: [Kim91] <author> W. Kim, J. Srivastava, </author> <title> "Enhancing Real-Time DBMS Performance with Multiversion Data and Priority Based Disk Scheduling", </title> <booktitle> Proc. of the 1991 IEEE 12th Real-Time Systems Symposium (RTSS). </booktitle>
Reference: [Knut73] <author> D. Knuth, </author> <title> The Art of Computer Programming, Vol. III: Sorting and Searching, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA., </address> <year> 1973. </year>
Reference-contexts: When none of the tuples in the heap satisfy this condition, the current run ends and a new run is started. On the average, the length of the runs produced by replacement selection is twice the memory allocated for the split phase <ref> [Knut73] </ref>, i.e. twice as long as the runs generated with Quicksort. Hence, replacement selection creates only half as many runs as Quicksort. This could significantly shorten the merge phase that follows.
Reference: [Kort90] <author> H.F. Korth, N. Soparkar, A. Silberschatz, </author> <title> "Triggered Real-Time Databases with Consistency Constraints", </title> <booktitle> Proc. of the 16th Int. Conf. on Very Large Data Bases, </booktitle> <month> August </month> <year> 1990. </year>
Reference: [Livn87] <author> M. Livny, S. Khoshafian, H. Boral, </author> <title> "Multi-Disk Management Algorithms", </title> <booktitle> Proc. of the ACM 1987 SIG-METRICS Conf., </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: To facilitate this, the database consists of NumRel relations. Each relation i (1 i NumRel), in turn, has a size of RelSize i MBytes and occupies con tiguous pages on disk. If there are multiple disks, all relations are declustered (horizontally partitioned) <ref> [Ries78, Livn87] </ref> across all of the disks. To minimize disk head movement, the relations are allotted the middle cylinders of the disks; temporary files occupy either the inner cylinders or the outer cylinders.
Reference: [Livn90] <author> M. Livny, </author> <note> "DeNet User's Guide, Version 1.5", </note> <institution> Computer Sciences Department, University of Wisconsin, Madison, </institution> <year> 1990. </year>
Reference-contexts: The simula- tor is written in the DeNet simulation language <ref> [Livn90] </ref>. 4.1. Database and Workload Model Table 2 summarizes the database and workload model parameters that are relevant to this study. Our objective is to simulate a stream of external sorts or sort-merge joins on different source relations. To facilitate this, the database consists of NumRel relations.
Reference: [Pang93] <author> H. Pang, M.J. Carey, M. Livny, </author> <title> "Partially Preemptible Hash Joins", </title> <booktitle> Proc. of the ACM 1993 SIGMOD Conf., </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: This seriously reduces the effectiveness of priority scheduling. Moreover, this practice does not allow a query to take advantage of excess memory that may become available. There is therefore a need for large queries to be adaptive when memory availability varies. In a recent paper <ref> [Pang93] </ref>, we presented and evaluated techniques that allow hash joins to adapt to changes in their allocated memory. This study focuses on the same problem for large external sorts, i.e. sorts that involve relations that cannot fit entirely in the available memory, and for sort-merge joins.
Reference: [Ries78] <author> D. Ries, R. Epstein, </author> <title> "Evaluation of Distribution Criteria for Distributed Database Systems", </title> <type> UCB/ERL Technical Report M78/22, </type> <institution> UC Berkeley, </institution> <month> May </month> <year> 1978. </year> <month> - 34 </month> - 
Reference-contexts: To facilitate this, the database consists of NumRel relations. Each relation i (1 i NumRel), in turn, has a size of RelSize i MBytes and occupies con tiguous pages on disk. If there are multiple disks, all relations are declustered (horizontally partitioned) <ref> [Ries78, Livn87] </ref> across all of the disks. To minimize disk head movement, the relations are allotted the middle cylinders of the disks; temporary files occupy either the inner cylinders or the outer cylinders.
Reference: [RTS92] <institution> Real-Time Systems, </institution> <note> 4(3), Special Issue on Real-Time Databases, </note> <month> September </month> <year> 1992. </year>
Reference-contexts: 1. Introduction Database management systems (DBMS) are faced with increasingly demanding performance objectives. These objectives include time constraints, as in real-time database systems <ref> [SIGM88, RTS92] </ref>, and administratively-defined performance goals, as in goal-oriented database systems [Ferg93, Brow93]. Traditional DBMS scheduling policies are no longer adequate to meet such objectives; a DBMS has to prioritize transactions that are competing for system resources according to the system-wide objectives and the resource requirements of the transactions.
Reference: [Salz90] <author> B. Salzberg, A. Tsukerman, J. Gray, M. Stewart, S. Uren, B. Vaughan, "FastSort: </author> <title> A Distributed Single-Input Single-Output External Sort", </title> <booktitle> Proc. of the ACM 1990 SIGMOD Conf., </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Hence, replacement selection creates only half as many runs as Quicksort. This could significantly shorten the merge phase that follows. A nice discussion of the details involved in implementing replacement selection can be found in <ref> [Salz90] </ref>. - 3 - Although using replacement selection instead of Quicksort can shorten the merge phase, replacement selection is not always the preferred choice because it can also lead to a longer split phase [Grae90, DeWi91].
Reference: [Sarg76] <author> R. Sargent, </author> <title> "Statistical Analysis of Simulation Output Data", </title> <booktitle> Proc. of the 4th Annual Symposium on the Simulation of Computer Systems, </booktitle> <month> August </month> <year> 1976. </year>
Reference-contexts: The performance metric of interest here is the average sort response time. To ensure the statistical validity of our results, we verified that the 90% confidence intervals for response times (computed using the batch means approach <ref> [Sarg76] </ref>) were sufficiently tight. The size of these confidence intervals was within a few percent of the mean in almost all cases, which is more than sufficient for our purposes. Throughout the paper we discuss only statistically significant performance differences. - 14 - 5.1.
Reference: [Shap86] <author> L.D. Shapiro, </author> <title> "Join Processing in Database Systems with Large Main Memories", </title> <journal> ACM Transactions on Database Systems, </journal> <volume> Vol. 11,3, </volume> <month> September </month> <year> 1986. </year>
Reference-contexts: We therefore did not pursue this option. - 29 - 6. Sort-Merge Joins Sort-merge join is a join algorithm employed by many existing database systems. Although recent work has shown hash join to often be superior to sort-merge join in performing ad-hoc join operations <ref> [Brat84, DeWi84, Shap86] </ref>, sort-merge join is still useful under certain conditions, e.g. when significant data skew is present, or when the results need to be presented in sorted order [Grae93]. Hence sort-merge join is likely to continue to be offered as one of the alternative join algorithms in future DBMSs.
Reference: [SIGM88] <editor> SIGMOD Record, </editor> <volume> Vol. </volume> <month> 17,1, </month> <title> Special Issue on Real-Time Data Base Systems, </title> <editor> S. Son, editor, </editor> <month> March </month> <year> 1988. </year>
Reference-contexts: 1. Introduction Database management systems (DBMS) are faced with increasingly demanding performance objectives. These objectives include time constraints, as in real-time database systems <ref> [SIGM88, RTS92] </ref>, and administratively-defined performance goals, as in goal-oriented database systems [Ferg93, Brow93]. Traditional DBMS scheduling policies are no longer adequate to meet such objectives; a DBMS has to prioritize transactions that are competing for system resources according to the system-wide objectives and the resource requirements of the transactions.
Reference: [Teng84] <author> J. Teng, R.A. Gumaer, </author> <title> "Managing IBM Database 2 Buffers to Maximize Performance", </title> <journal> IBM Systems Journal, </journal> <volume> Vol. 23,2, </volume> <year> 1984. </year>
Reference-contexts: This lengthens the time that is needed to satisfy the waiting buffer request, and should be avoided if possible. For this reason, an asynchronous memory write process is provided to flush dirty pages to disk periodically <ref> [Teng84] </ref>. The write process is activated every SleepTime seconds. Upon activation, the process flushes all of the dirty pages that are older than FlushThreshold. The reason for flushing only the "old" dirty pages is to prevent unnecessary writes of pages that are frequently updated. 5.
Reference: [Zell90] <author> H. Zeller, J. Gray, </author> <title> "An Adaptive Hash Join Algorithm for Multiuser Environments", </title> <booktitle> Proc. of the 16th Int. Conf. on Very Large Data Bases, </booktitle> <month> August </month> <year> 1990. </year> <month> - 35 </month> -
References-found: 27

