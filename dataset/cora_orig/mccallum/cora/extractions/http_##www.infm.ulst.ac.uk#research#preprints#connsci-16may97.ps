URL: http://www.infm.ulst.ac.uk/research/preprints/connsci-16may97.ps
Refering-URL: http://www.infm.ulst.ac.uk/research/preprints.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Combining Neural Network Forecasts on Wavelet-Transformed Time Series  
Author: Alex Aussemy and Fionn Murtaghz yUniversite Rene Descartes, 
Keyword: Dynamical Recurrent Neural Networks, time series prediction, wavelet transform.  
Note: Running Head Neuro-Wavelet Forecasts Send Correspondence to Alex Aussem (alex@math-info.univ-paris5.fr)  Preprint INFM-97-005,  
Address: 45, rue des Saints-Peres, 75006 Paris France  Londonderry BT48 7JL Northern Ireland  Magee College  
Affiliation: UFR de Mathematiques et Informatique,  Faculty of Informatics Magee College, University of Ulster  University of Ulster, Faculty of Informatics,  
Email: Email: alex@math-info.univ-paris5.fr  Email: fd.murtagh@ulst.ac.uk  
Phone: Tel (33 1) 44 55 35 40  
Date: 1997 May 16  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aussem, A., Murtagh, F. & Sarazin, M. </author> <title> (1995) Dynamical recurrent neural networks towards environmental time series prediction. </title> <journal> International Journal on Neural Systems, </journal> <volume> 6. </volume>
Reference-contexts: The model 5 is endowed with a trainable internal memory and hence can directly implement dynamical systems <ref> (Aussem, 1995) </ref>. <p> stability of the parameter adaptation procedure, it has been shown that the norm of the the gradient error vector backpropagated through the adjoint network is bounded by a series decaying at exponential rate with respect to the retrograde time step in the adjoint under reasonable conditions on the weight matrix <ref> (Aussem et al., 1995) </ref>. Consequently, for the weight changes computed at each iteration to follow as closely as desired the true gradient, it is sufficient to backpropagate a finite number of time steps through the adjoint. <p> Consequently, for the weight changes computed at each iteration to follow as closely as desired the true gradient, it is sufficient to backpropagate a finite number of time steps through the adjoint. In view of this rapid gradient decay, an implementation-friendly algorithm called Temporal Recurrent Back-Propagation (TRBP) <ref> (Aussem et al., 1995) </ref> has been proposed to adjust the DRNN. TRBP is local, on-line and preserves the symmetry between the forward propagation of unit outputs and the backward propagation of error terms through the adjoint at each time step. <p> Although not pursued here, readers interested in learning more about the DRNN model and the TRBP algorithm are encouraged to consult <ref> (Aussem, 1995) </ref> and the references therein. 6 4 Wavelet Decomposition A wavelet transform provides a decomposition in terms of time and frequency, or of scale and position.
Reference: <author> Aussem, A. </author> <title> (1995) Theory and Applications of Dynamical and Recurrent Neural Networks towards Prediction, Modeling and Adaptive Control of Dynamical Processes. </title> <type> Ph.D. </type> <note> Thesis (in French), Universite Rene Descartes, Available at &lt;http://www.eso.org/meteo-seeing/aa-thesis/aa.html&gt;. </note>
Reference-contexts: The model 5 is endowed with a trainable internal memory and hence can directly implement dynamical systems <ref> (Aussem, 1995) </ref>. <p> stability of the parameter adaptation procedure, it has been shown that the norm of the the gradient error vector backpropagated through the adjoint network is bounded by a series decaying at exponential rate with respect to the retrograde time step in the adjoint under reasonable conditions on the weight matrix <ref> (Aussem et al., 1995) </ref>. Consequently, for the weight changes computed at each iteration to follow as closely as desired the true gradient, it is sufficient to backpropagate a finite number of time steps through the adjoint. <p> Consequently, for the weight changes computed at each iteration to follow as closely as desired the true gradient, it is sufficient to backpropagate a finite number of time steps through the adjoint. In view of this rapid gradient decay, an implementation-friendly algorithm called Temporal Recurrent Back-Propagation (TRBP) <ref> (Aussem et al., 1995) </ref> has been proposed to adjust the DRNN. TRBP is local, on-line and preserves the symmetry between the forward propagation of unit outputs and the backward propagation of error terms through the adjoint at each time step. <p> Although not pursued here, readers interested in learning more about the DRNN model and the TRBP algorithm are encouraged to consult <ref> (Aussem, 1995) </ref> and the references therein. 6 4 Wavelet Decomposition A wavelet transform provides a decomposition in terms of time and frequency, or of scale and position.
Reference: <author> Aussem, A., Murtagh, F. & Sarazin, M. </author> <title> (1996) Fuzzy astronomical seeing nowcasts with a dynamical and recurrent connectionist network. </title> <journal> Neurocomputing, </journal> <volume> 12, No. </volume> <pages> 4, </pages> <note> in press. 11 Aussem, </note> <author> A. </author> <title> (1996) Exponential gradient decay in dynamical necurrent neural networks. </title> <note> Submitted to Neural Computation. </note>
Reference-contexts: The versatility of the DRNN model is not to the detriment of ease of training. Indeed it has been demonstrated that backward error propagation of the unfolded adjoint decays at exponential speed <ref> (Aussem, 1996) </ref>. Therefore it suffices to back-propagate the gradient a limited number of times to approximate the true gradient as closely as desired, thus limiting drastically the computational burden.
Reference: <author> Baldi, P. </author> <title> (1995) Gradient descent learning algorithm overview: a general dynamical systems perspective. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 6. </volume>
Reference: <author> Bruce, A. & Gao, H.-Y. </author> <note> (1994) S+Wavelets User's Manual, Version 1.0. </note> <institution> StatSci Division, MathSoft Inc., </institution> <address> Seattle, WA. </address>
Reference: <author> Daubechies, I. </author> <title> (1992) Ten Lectures on Wavelets. </title> <journal> Phildelphia: Society for Industrial and Applied Mathematics (SIAM). </journal>
Reference: <author> Hertz, J., Krogh, A. & Palmer, R. </author> <title> (1991) An Introduction to the Theory of Neural Computation. </title> <address> Redwood City, CA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Holschneider, M. & Tchamitchian, P. </author> <note> (1990) In Lemarie, </note> <editor> P.G., Ed. </editor> <title> Les Ondelettes en 1989. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: <author> Meyer, Y. </author> <title> (1993) Wavelets: Algorithms and Applications. </title> <journal> Phildelphia: Society for Industrial and Applied Mathematics (SIAM). </journal>
Reference: <author> Murtagh, F., Aussem, A. & Kardaun, O. </author> <title> (1996) The wavelet transform in multivariate data analysis. </title> <editor> COMPSTAT'96. Prat, A., Ed., Wurzburg: </editor> <publisher> Physica-Verlag. </publisher> <address> Barcelona, </address> <month> August, </month> <note> in press. </note>
Reference: <author> Murtagh, F., Starck, J.-L. & Bijaoui, A. </author> <title> (1995) Multiresolution in astronomical image processing: a general framework. </title> <journal> International Journal of Imaging Systems and Technology, </journal> <volume> 6. </volume>
Reference: <author> Nason, </author> <title> G.P. & Silverman, G.P. (1995) The stationary wavelet transform and some statistical applications. </title> <type> Preprint, </type> <institution> University of Bath. </institution>
Reference-contexts: A wavelet transform for discrete data is provided by the particular version known as the a trous (with holes) algorithm (Holschneider, 1990; Shensa, 1992). This is a "stationary" <ref> (Nason, 1995) </ref> or redundant transform, i.e. decimation is not carried out. Take c 0 (t) = x (t), the input data.
Reference: <author> Piche, </author> <title> S.W. (1994) Steepest descent algorithms for neural network controllers and filters. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5. </volume>
Reference: <author> Pineda, F.J. </author> <title> (1987) Generalization of back-propagation to recurrent neural networks. </title> <journal> Physical Review Letters 59. </journal>
Reference-contexts: This yields a generalized @ rule, which is essentially a backsweep or backward recurrent propagation procedure in that it puts recurrent-backpropagation <ref> (Pineda, 1987) </ref> and backpropagation-through-time (Rumelhart, 1986) into a single framework. Its implementation makes necessary the artifact of unfolding the adjoint network in a reverse way in time. This is achieved by replicating the units and the connections backward through time until all delays have been removed.
Reference: <author> Press, W.H., Teukolsky, S.A., Vetterling, W.T. & Flannery, </author> <title> B.P. (1992) Numerical Recipes, </title> <booktitle> 2nd Ed. Chapter 13, </booktitle> <address> New York: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Priestley, </author> <title> M.B. (1981) Spectral Analysis and Time Series. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Rumelhart, D.E., Hinton, G.E. & Williams, </author> <title> R.J. (1986) Learning internal representations by error 12 propagation. </title> <booktitle> In Parallel Distributed Processing Explorations in the Microstructure of Cognition, </booktitle> <volume> Vol. 1, </volume> <editor> D.E. Rumelhart and J.L. McClelland, Eds., </editor> <address> Cambridge, MA: </address> <publisher> MIT Press, Bradfords Books. </publisher>
Reference-contexts: This yields a generalized @ rule, which is essentially a backsweep or backward recurrent propagation procedure in that it puts recurrent-backpropagation (Pineda, 1987) and backpropagation-through-time <ref> (Rumelhart, 1986) </ref> into a single framework. Its implementation makes necessary the artifact of unfolding the adjoint network in a reverse way in time. This is achieved by replicating the units and the connections backward through time until all delays have been removed.
Reference: <author> Shensa, M.J. </author> <title> (1992) Discrete wavelet transforms: wedding the a trous and Mallat algorithms. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 40. </volume>
Reference: <author> Starck, J.-L. & Bijaoui, A. </author> <title> (1994) Filtering and deconvolution by the wavelet transform. </title> <booktitle> Signal Processing, </booktitle> <pages> 35. </pages>
Reference: <author> Starck, J.L., Murtagh, F. & Bijaoui, A. </author> <title> (1995) Multiresolution support applied to image filtering and deconvolution. Graphical Models and Image Processing, </title> <type> 57. </type>
Reference: <author> Tong, H. </author> <title> (1990) Non Linear Time Series. </title> <publisher> Oxford: Clarendon Press. </publisher>
Reference: <author> Wan, </author> <title> E.A. (1993) Finite Impusle Response Neural Networks with Applications in Time Series Prediction. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University. </institution>
Reference-contexts: These models, called Dynamical Recurrent Neural Networks (DRNN), have an advantage over static recurrent networks in much the same way that feedforward FIR networks <ref> (Wan, 1993) </ref> have an advantage other static feedforward networks. The versatility of the DRNN model is not to the detriment of ease of training. Indeed it has been demonstrated that backward error propagation of the unfolded adjoint decays at exponential speed (Aussem, 1996).
Reference: <author> Weigend, A.S., Rumelhart, D.E. & Huberman, </author> <title> B.A. (1990) Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1. </volume>
Reference: <author> Werbos, P., </author> <title> (1974) Beyond Regression: New Tools for Prediction and Analysis in Behavioral Sciences. </title> <type> Ph.D. thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, MA. </address>

References-found: 24

