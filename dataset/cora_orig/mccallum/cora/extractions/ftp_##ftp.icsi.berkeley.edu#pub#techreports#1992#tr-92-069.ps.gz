URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1992/tr-92-069.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1992.html
Root-URL: http://www.icsi.berkeley.edu
Title: 0 Inductive learning of compact rule sets by using efficient hypotheses reduction  
Author: Thomas Koch 
Date: October 1992  
Pubnum: TR 92-069  
Abstract: A method is described which reduces the hypotheses space with an efficient and easily interpretable reduction criteria called a - reduction. A learning algorithm is described based on a - reduction and analyzed by using probability approximate correct learning results. The results are obtained by reducing a rule set to an equivalent set of kDNF formulas. The goal of the learning algorithm is to induce a compact rule set describing the basic dependencies within a set of data. The reduction is based on criterion which is very exible and gives a semantic interpretation of the rules which fulfill the criteria. Comparison with syntactical hypotheses reduction show that the a reduction improves search and has a smaller probability of missclassification. 
Abstract-found: 1
Intro-found: 1
Reference: [Bloendorn Michalski 91] <author> Bloendorn E. and Michalski R. S.: </author> <title> Data-driven Constructive Induction in AQ17-DCI: A Method and Experiments, Reports of Machine Learning and Inference Laboratory, </title> <institution> Center for Artificial Intelligence, George Mason University, </institution> <year> 1991 </year>
Reference: [Clark Niblett 89] <author> Clark P. and Niblett T. </author> : <title> The CN2 induction algorithm, </title> <journal> Machine Learning. </journal> <volume> 3 </volume> <pages> 261-283, </pages> <year> 1989 </year>
Reference: [Erdoes Rado] <author> Erdoes, P. and Rado R.: </author> <title> Intersection theorems for systems of sets, </title> <journal> Journal of the London Math. Society, vol.35 pp. </journal> <pages> 85-90, </pages> <year> 1960 </year>
Reference: [Fehsenfeld et al 91] <author> Fehsenfeld B., Harris S., Koch T. and Kirchheiner R. </author> : <title> Automatic learning with RULEARN in the example of corrosion tests, </title> <note> in Technische Mitteilungen Krupp 1/1991 </note>
Reference: [Kearns 89] <author> Kearns M. J.: </author> <title> The Computational Complexity of Machine Learning, </title> <publisher> MIT press, </publisher> <year> 1989 </year>
Reference-contexts: In one sense the VC dimension is a measure of the number of degrees of freedom possessed by C. The VC -dimension was originally introduced by Vapnic and Chervonenkis [Vapnic and Chervonenkis 71]. Kearns <ref> [Kearns 89] </ref> derived a lower bound for the sample space complexity in the noise free case. If , and then C is PAC - identifiable if where is the Vapnic-Chervonenkis dimension.
Reference: [Kirchheiner Koch 92] <author> Kirchheiner R. and Koch T. </author> : <title> Computer aided learning of corrosion rules b 1 1 2 X n m - 19 - from factual data bases, </title> <booktitle> Proceedings of the NACE-CORRSION 92, </booktitle> <address> Nashville 1992 </address>
Reference: [Koch 88] <author> Koch T.: </author> <title> Effizientes Lernen und Bewerten von Regeln, </title> <booktitle> Kuenstliche Intelligenz Informatik Fachberichte 181, </booktitle> <pages> 186-195, </pages> <address> Springer Berlin 1988 </address>
Reference-contexts: Systems that perform this task are for example ID3 family [Quinlan 86] and [Utgoff 89], AQ15 [Michalski 83] or CN2 [Clark, Niblett 89] and RULEARN <ref> [Koch 88] </ref>. The questions which arises after finding the rules can be formulated as: With what level of confidence do the generated rules describe the true dependencies in the data set? This question is solved by showing that the task of finding rules is PAC (probability approximately correct) identifiable.
Reference: [Michalski 83] <author> Michalski R. S.: </author> <title> A Theory and Methodology of Inductive Learning in Machine Learning: An Artificial Intelligence Approach, </title> <editor> Michalski R.S.,Carbonell J. and Mitchell T. </editor> <booktitle> (Eds.) </booktitle> <pages> pp. 83-134, </pages> <publisher> Morgan Kaufmann Publishing Co., </publisher> <address> Mountain View, CA, </address> <year> 1983 </year>
Reference-contexts: The scenario can be described as given a data set (i.e. set of examples) from a database, construct a set of rules which describe the dependencies within the data set. Systems that perform this task are for example ID3 family [Quinlan 86] and [Utgoff 89], AQ15 <ref> [Michalski 83] </ref> or CN2 [Clark, Niblett 89] and RULEARN [Koch 88].
Reference: [Natrajan 91] <author> Natrajan B. K. </author> : <title> Machine learning: A theoretical approach, </title> <publisher> Morgan Kaufmann Publishers 1991 </publisher>
Reference-contexts: Kearns [Kearns 89] derived a lower bound for the sample space complexity in the noise free case. If , and then C is PAC - identifiable if where is the Vapnic-Chervonenkis dimension. If we dont specify bounds on e and d Natrajan <ref> [Natrajan 91] </ref> showed that if then C is PAC - identifiable (without noise).
Reference: [Pfeiffer et al 88] <author> Pfeiffer J., Siepmann T. and Teichmann W.: </author> <title> Expert system for metal machining practise, </title> <note> in Technische Mitteilungen Krupp 46 (1988) pp 113 - 124 </note>
Reference-contexts: RULEARN is used as a data evaluation method but also as a knowledge acquisition tool for building expert systems [Kirchheiner and Koch 92]. For reference of expert system development see for example <ref> [Pfeiffer et al 88] </ref>. - 2 - In section 2 we describe the ideas of the RULEARN system, which uses three user defined evaluation criteria to distinguish between good and bad rules.
Reference: [Shackelford, Volper 88] <author> Shackelford G. and Volper D. </author> : <title> Learning k-DNF with noise in thee Attributes. </title> <booktitle> Proceedings of the first Annual ACM Workshop on Computational Learning Theory 1988, </booktitle> <pages> 97-103 </pages>
Reference-contexts: In this noise model, each rule component a i,j (literal) is affected by a known noise rate b so that a i,j = a i,j with probability 1 - b a i,j = 0 with probability 0.5 b a i,j = 1 with probability 0.5 b Shackelford and Volper <ref> [Shackelford, Volper 88] </ref> showed that the class of kDNF is PAC - identifiable which means there exists as algorithm A, so that for , , and for all distributions D using as Oracle function E (f) outputs a Boolean function c such that: (1) where h is the true concept to
Reference: [Quinlan 86] <author> Quinlan J.R.: </author> <title> Induction of decision trees, </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986 </year>
Reference-contexts: The scenario can be described as given a data set (i.e. set of examples) from a database, construct a set of rules which describe the dependencies within the data set. Systems that perform this task are for example ID3 family <ref> [Quinlan 86] </ref> and [Utgoff 89], AQ15 [Michalski 83] or CN2 [Clark, Niblett 89] and RULEARN [Koch 88].
Reference: [Utgoff 89] <author> Utgoff P.E.: </author> <title> Incremental Learning of Decision trees., </title> <booktitle> Machine Learning 4 </booktitle> <pages> 161-186, </pages> <year> 1989 </year>
Reference-contexts: The scenario can be described as given a data set (i.e. set of examples) from a database, construct a set of rules which describe the dependencies within the data set. Systems that perform this task are for example ID3 family [Quinlan 86] and <ref> [Utgoff 89] </ref>, AQ15 [Michalski 83] or CN2 [Clark, Niblett 89] and RULEARN [Koch 88].
Reference: [[Valiant 84] <author> Valiant L.G. </author> : <title> A Theory of the learnable, </title> <journal> Communications of the ACM, </journal> <volume> 27:11, </volume> <pages> 1134-1142, </pages> <year> 1984 </year>
Reference: [Valiant 85] <author> Valiant L.G. </author> : <title> Learning disjunctions of conjunctions,Proc. </title> <booktitle> of the 9 th IJCAI, </booktitle> <address> Los Angeles California, </address> <publisher> Morgan Kaufman 1985, </publisher> <pages> pp 560-566 </pages>
Reference-contexts: The other attributes arent affected by noise. This conditions have been analyzed by Valiant <ref> [Valiant 85] </ref> Let be the number of possible rule premises (terms), then , where l is the maxi mal number of possible values for each attribute.

References-found: 15

