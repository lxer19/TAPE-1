URL: http://ftp.eecs.umich.edu/people/neuhoff/simplistic.it.ps
Refering-URL: http://ftp.eecs.umich.edu/people/neuhoff/
Root-URL: http://www.eecs.umich.edu
Title: Simplistic universal coding  
Author: David L. Neuhoff and Paul C. Shields Paul C. Shields 
Keyword: Key words and phrases: universal coding  
Address: Ann Arbor, MI 48109, USA.  OH 43606, USA.  
Affiliation: Department of Electrical Engineering and Computer Science, University of Michigan,  Department of Mathematics, University of Toledo, Toledo  
Note: David L. Neuhoff is with the  is with the  
Abstract: A conceptually simple coding method may be described as follows. The source sequence is parsed into fixed length blocks and a list of these blocks is placed in a dictionary. In the lossless case, the dictionary is transmitted and each successive block is encoded by giving its dictionary location. In the lossy case, the smallest collection of blocks such that every member of the dictionary is within distortion ffi of the collection is determined, this codebook is transmitted, and each successive block is encoded by giving its codebook location. We show that by optimizing on the block length, this method is universal, that is, for any ergodic process it achieves entropy in the limit in the lossless case and the rate-distortion function R(ffi) in the lossy case. Versions of this paper were presented at the IEEE Information Theory Workshop in Rydzyna, Poland, June, 1995, and the IEEE Information Theory Symposium in Ulm, Germany, June, 1997. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. D. Davisson, </author> <title> "Comments on `Sequence time coding for data compression'," </title> <booktitle> Proc. </booktitle> <volume> 54(1966), </volume> <year> 2010. </year>
Reference-contexts: In our case no special ordering is needed and blocks are coded with a fixed-length code. Note also that our code does not exploit frequencies of any kind, unlike Lynch-Davisson type codes <ref> [6, 1, 2] </ref> or the code of [9].
Reference: [2] <author> L. D. Davisson, </author> <title> "Universal noiseless coding," </title> <journal> IEEE Trans. Inform. Th., </journal> <volume> vol. IT-19, </volume> <pages> pp. 783-795, </pages> <month> Nov. </month> <year> 1973. </year>
Reference-contexts: In our case no special ordering is needed and blocks are coded with a fixed-length code. Note also that our code does not exploit frequencies of any kind, unlike Lynch-Davisson type codes <ref> [6, 1, 2] </ref> or the code of [9].
Reference: [3] <author> P. Elias, </author> <title> "Interval and recency rank source coding: Two on-line adaptive variable-length schemes," </title> <journal> IEEE Trans. Inform. Th., </journal> <volume> vol. IT-33, </volume> <pages> pp. 3-10, </pages> <month> Jan. </month> <year> 1987. </year> <month> 10 </month>
Reference-contexts: A related, but somewhat more complicated, coding procedure is described in <ref> [3] </ref>. Our lossy code is also based on trying all k-block codes and choosing the best. The k-block lossy code begins as in the lossless case by parsing the source sequence into k-blocks and forming a dictionary of the distinct words that appear.
Reference: [4] <author> R. Gallager, </author> <title> Information theory and reliable communication. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1968. </year>
Reference-contexts: Since C k is the smallest such set it follows that jC k j jJ k j = jBj + jD k [B] ffi j: The complement bound (7) shows that EjD k [B] ffi j k ffi ); which implies (10). The lossy source coding theorem, see <ref> [4, Section 9.8] </ref>, provides for each k a set U k ae A k of cardinality at most 2 k (R+*=2) such that P ([U k ] ffi ) ! 0, as k ! 1.
Reference: [5] <author> R. L. Gray, D. L. Neuhoff, and P. C. Shields, </author> <title> "A generalization of Ornstein's d distance with applications to information theory," </title> <journal> Ann. Probab., </journal> <volume> 3(1975), </volume> <pages> 315-328. </pages>
Reference-contexts: The first was Ziv's fixed-rate method, [12], which is like our method except that a target rate R is specified and one searches for the codebook of size 2 kR that yields least average distortion for the given sequence. Another fixed-rate method is described in <ref> [5, 8] </ref>. A lossy code similar to ours is described in [9], but it orders the codebook in a special way and uses a variable-length code to specify locations.
Reference: [6] <author> T. J. Lynch, </author> <title> "Sequence time coding for data compression," </title> <booktitle> Proc. IEEE, </booktitle> <pages> 54(1966) 1490-1491. </pages>
Reference-contexts: In our case no special ordering is needed and blocks are coded with a fixed-length code. Note also that our code does not exploit frequencies of any kind, unlike Lynch-Davisson type codes <ref> [6, 1, 2] </ref> or the code of [9].
Reference: [7] <author> J. C. Kieffer, </author> <title> "A survey of the theory of source coding," </title> <journal> IEEE Trans. Inform. Th., </journal> <volume> IT-39(1993), </volume> <pages> 1473-1490. </pages>
Reference-contexts: 1 Introduction Since universal codes were discovered, a number of coding algorithms have been proposed and shown to be universal, that is, for ergodic sources, code rate approaches entropy-rate in the lossless case (or the rate-distortion function in the lossy case) as the length of the source sequence increases. (See <ref> [7] </ref> for a survey.) The goals of such work include finding universal codes that minimize quantities such as redundancy, complexity, and block length. On the other hand, it is also of interest to learn how conceptually simple a code can be and still have the universality property.
Reference: [8] <author> D. L. Neuhoff, R. L. Gray, and L. D. Davisson, </author> <title> "Fixed-rate universal block source coding with a fidelity criterion," </title> <journal> IEEE Trans. Inform. Th., </journal> <volume> vol. IT-21, </volume> <pages> pp. 511-523, </pages> <year> 1975. </year>
Reference-contexts: The first was Ziv's fixed-rate method, [12], which is like our method except that a target rate R is specified and one searches for the codebook of size 2 kR that yields least average distortion for the given sequence. Another fixed-rate method is described in <ref> [5, 8] </ref>. A lossy code similar to ours is described in [9], but it orders the codebook in a special way and uses a variable-length code to specify locations.
Reference: [9] <author> D. Ornstein and P. Shields, </author> <title> "Universal almost sure data compression," </title> <journal> Ann. Probab., </journal> <volume> 18(1990), </volume> <pages> 441-452. </pages>
Reference-contexts: On the other hand, for n 2 k (H+2*) the index rate is close to H. The details of this argument are given in Section 2. Remark 1 The dictionary coding scheme given in <ref> [9] </ref> orders the dictionary in terms of frequency of occurrence and uses a variable-length code to specify locations, obtaining universality by letting k grow slowly with source sequence length. In our case no special ordering is needed and blocks are coded with a fixed-length code. <p> In our case no special ordering is needed and blocks are coded with a fixed-length code. Note also that our code does not exploit frequencies of any kind, unlike Lynch-Davisson type codes [6, 1, 2] or the code of <ref> [9] </ref>. The "price" paid for the simplicity of our code is that k must be large enough that the AEP applies, whereas in the Lynch-Davisson type codes, for example, k need only be large enough that (log k)=k is small and k-th order entropy is close to entropy-rate. <p> Another fixed-rate method is described in [5, 8]. A lossy code similar to ours is described in <ref> [9] </ref>, but it orders the codebook in a special way and uses a variable-length code to specify locations. We should note, however, that while our code is conceptually simple, our block length needs to be much larger than it does in [9]. 3 2 Proof of the lossless theorem. <p> A lossy code similar to ours is described in <ref> [9] </ref>, but it orders the codebook in a special way and uses a variable-length code to specify locations. We should note, however, that while our code is conceptually simple, our block length needs to be much larger than it does in [9]. 3 2 Proof of the lossless theorem. Throughout the remainder of the paper, A denotes a finite source alphabet and, for s t, x t s denotes the sequence x s ; x s+1 ; : : : ; x t , where each x j 2 A. <p> For the almost-sure lossy result we note that Lemma 2 (ii) in <ref> [9] </ref>, implies the following. <p> With this lemma used in place of Lemma 1 the lossless argument yields the almost-sure lossy result. Remark 4 Lemma 2 (ii) in <ref> [9] </ref> was established only for the usual Hamming distortion, but that proof is easily extended to the case of a general per-symbol distortion measure.
Reference: [10] <author> D. Ornstein and B. Weiss, </author> <title> "How sampling reveals a process," </title> <journal> Ann. Probab., </journal> <volume> 18(1990), </volume> <pages> 905-930. </pages>
Reference-contexts: Almost-sure asymptotic negligibility of the dictionary rate follows from the fact that an asymptotic equipartition property holds eventually 6 almost surely for the empirical distribution of k-blocks in n-blocks, so long as n 2 kH , and k ! 1; a result established by Ornstein and Weiss, <ref> [10] </ref>. Their Lemma 1 immediately implies the following. <p> In the usual AEP, the set T k consists of entropy-typical k-sequences, that is, T k (*=2) = fx k 1 )j *=2g; while in Lemma 1, the set T k consists of k-sequences that are mostly built-up from shorter entropy-typical blocks, see <ref> [10, Lemma 1] </ref>. 7 3 Proof of the lossy theorem.
Reference: [11] <author> F. M. J. Willems, </author> <title> "Universal data compression and repetition times," </title> <journal> IEEE Trans. Inform. Th., </journal> <volume> vol. IT-35, </volume> <pages> pp. 54-58, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: Like our method, Lempel-Ziv coding does not exploit 2 frequencies; however, it should not be considered as simplistic. On the other hand, Willems <ref> [11] </ref> describes a universal code that merely encodes the time until the most recent occurrence of the present k-tuple, up to some limit, which, though it leaves unspecified how k is to grow, is certainly in the same simple-minded spirit as the code presented here.
Reference: [12] <author> J. Ziv, </author> <title> "Coding of sources with unknown statistics-Part II: Distortion relative to a fidelity criterion", </title> <journal> IEEE Trans. Inform. Th., </journal> <volume> vol. IT-18, </volume> <pages> pp. 389-394, </pages> <month> May </month> <year> 1972. </year> <month> 11 </month>
Reference-contexts: Remark 2 Several lossy codes have been shown to be universal. The first was Ziv's fixed-rate method, <ref> [12] </ref>, which is like our method except that a target rate R is specified and one searches for the codebook of size 2 kR that yields least average distortion for the given sequence. Another fixed-rate method is described in [5, 8].
References-found: 12

