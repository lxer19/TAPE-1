URL: ftp://ftp.cs.virginia.edu/pub/techreports/IPC-90-05.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: The Nondeterministic Divide  
Author: Arthur Charlesworth and 
Note: This research was supported by the National Aeronautics and Space Administration, under grant number NAG-1-774, by the Jet Propulsion Laboratory, under grant number 95722, and by sabbatical funding from the University of Richmond.  
Address: Charlottesville, VA 22903  Richmond, VA 23173  
Affiliation: Institute for Parallel Computation School of Engineering and Applied Science University of Virginia  Department of Mathematics and Computer Science University of Richmond  
Date: November 30, 1990  
Pubnum: IPC-TR-90-005  
Abstract-found: 0
Intro-found: 1
Reference: [] 
Reference: [BST89] <author> Bal, H. E., Steiner, J. G., and Tanenbaum, A. S. </author> <title> Programming languages for distributed computing systems. </title> <journal> Computing Surveys. </journal> <volume> 21, </volume> <month> 3 (Sept. </month> <year> 1989), </year> <pages> 261-322. </pages>
Reference-contexts: Even a wide-area network of heterogeneous workstations may be viewed as a single distributed computing system <ref> [BST89] </ref>. A paramount question is how such diverse computers, and computers resulting from additional evolution, should be programmed. As always with software, program correctness is a key issue.
Reference: [Bar84] <author> Barnes, J. G. P. </author> <title> Programming in Ada. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1984. </year>
Reference-contexts: In addition, most readers either will be familiar with Ada or have ready access to an Ada reference, such as <ref> [Bar84] </ref> and [Uni83]. In view of Ada's existing complexity, the author is not recommending that Ada be extended with syntax to support diva algorithms.
Reference: [Ben86] <author> Bentley, J., </author> <title> Programming Pearls. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1986. </year>
Reference-contexts: This problem and its history are the focus of the chapter on algorithm design techniques in <ref> [Ben86] </ref> 9 . The feasibility of using a diva procedure to solve this problem was suggested by Dana Richards. EXAMPLE 4: Find a component value of one vector corresponding to the maximum of another vector. <p> cheaply, it is necessary to find a node in the current partial tour occurring at the minimum distance from this chosen node.) Another example of the use of FIND_MIN_CORR is in programming the Prim-Dijkstra algorithm for finding 9 Actually empty slices are included in the statement of the problem in <ref> [Ben86] </ref> so the desired answer when all component values of the vector are negative is zero rather than the largest value of the vector.
Reference: [Bur85] <author> Burns, A. </author> <title> Efficient initialisation routines for multiprocessor systems programmed in Ada. </title> <journal> Ada Letters, </journal> <volume> 5, </volume> <month> 1 (July Aug </month> <year> 1985), </year> <pages> 55-60. </pages>
Reference-contexts: Place the declaration of the array MY_INDEX_TO_CALL and the local ... declarations of the source's P here. ... Place the declaration of procedure P_1 of Figure 7 here. begin -- WORKER_TYPE ... Assign appropriate value to task index MY_INDEX. (See <ref> [Bur85] </ref>.) ... Determine the values of MY_SECTION_START, MY_GRAIN_SIZE, ... MY_NUM_ITERATIONS, MY_NUM_TO_CALL, and MY_INDEX_TO_CALL. -- Perform computations independent of the other workers: P_1 (MY_SECTION_START, MY_SECTION_START + MY_GRAIN_SIZE - 1, INITIAL_PARM); -- Combine local results with the results from other workers: if NUM_OF_WORKERS &gt; 1 then for I in 1 .. <p> Assign appropriate value to task index MY_INDEX. (See <ref> [Bur85] </ref>.) ... Determine the values of MY_SECTION_START, MY_GRAIN_SIZE, ... MY_NUM_ITERATIONS, MY_NUM_TO_CALL, and MY_INDEX_TO_CALL. -- Perform computations independent of the other workers: FIND_SUM_1 (MY_SECTION_START, MY_SECTION_START + MY_GRAIN_SIZE - 1, INITIAL_SUM); -- Combine local results with the results from other workers: if NUM_OF_WORKERS &gt; 1 then for I in 1 ..
Reference: [CK88] <author> Callahan, D. and Kennedy, K. </author> <title> Compiling programs for distributed memory multiprocessors. </title> <journal> J. of Supercomputing, </journal> <volume> 2, </volume> <year> (1988), </year> <pages> 151-169. </pages>
Reference-contexts: Permitting distributed memory computers to be programmed at a level supporting the illusion of shared memory is a related but separate research problem that is being investigated by numerous researchers; for example, see <ref> [CK88, KMR90, Li86, RSW88, SCMB90] </ref>. Thus the focus of this section is on activities within the diva call itself, rather than on distributing vectors and other data among the local memories. The target code given in Figure 9 can be modified for distributed memory as follows.
Reference: [Cha86] <author> Charlesworth, A. </author> <title> The design and implementation of a preprocessor for Ada to provide support for the multiway accept, </title> <institution> NASA Langley Research Center grant proposal NAG-1-774, </institution> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: The development of diva procedures arose naturally from a project to implement multiway rendezvous activities in log time whenever possible on suitable parallel computers <ref> [Cha86, Cha87, Cha89] </ref>. Often when a diva call appears within the sequential code of a multiway rendezvous, it is possible to use the processes participating in the multiway rendezvous to compute the results of the diva call, rather than activating new processes.
Reference: [Cha87] <author> Charlesworth, A. </author> <title> The multiway rendezvous. </title> <journal> ACM Trans. Program. Lang. Syst., </journal> <volume> 9, 2, </volume> <month> (July </month> <year> 1987), </year> <pages> 350-366. </pages>
Reference-contexts: The development of diva procedures arose naturally from a project to implement multiway rendezvous activities in log time whenever possible on suitable parallel computers <ref> [Cha86, Cha87, Cha89] </ref>. Often when a diva call appears within the sequential code of a multiway rendezvous, it is possible to use the processes participating in the multiway rendezvous to compute the results of the diva call, rather than activating new processes.
Reference: [Cha89] <author> Charlesworth, A. </author> <title> On a class of divide and conquer algorithms. Preliminary report. </title> <journal> Abstracts of the Amer. Math. Soc., </journal> <volume> 10, </volume> <month> 6 (Nov. </month> <year> 1989), </year> <month> 492. </month>
Reference-contexts: The development of diva procedures arose naturally from a project to implement multiway rendezvous activities in log time whenever possible on suitable parallel computers <ref> [Cha86, Cha87, Cha89] </ref>. Often when a diva call appears within the sequential code of a multiway rendezvous, it is possible to use the processes participating in the multiway rendezvous to compute the results of the diva call, rather than activating new processes. <p> Diva calls will be defined as restricted calls on diva procedures <ref> [Cha89] </ref>. DEFINITION 1.
Reference: [Cha90a] <author> Charlesworth, A. </author> <title> Adam Language Reference Manual, </title> <note> preliminary version 7/30/90, </note> <institution> Dept. Math. and Comp. Sci., U. of Richmond, Va., </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: This idea has been encorpo-rated into a translator for a language, Adam, supporting the use of the multiway rendezvous <ref> [Cha90a] </ref>. The organization of the rest of the paper is as follows: Compiler-enforceable syntax for a diva algorithm, in terms of diva calls, is given in Section 3. Examples of diva calls are given in Section 4 and informal verification of diva calls is illustrated in Section 5.
Reference: [Cha90b] <author> Charlesworth, A. </author> <title> A characterization of associativity, </title> <type> Tech. Rep. </type> <institution> IPC-TR-90-007, Inst. for Parallel Computation, U. of Virginia, Charlottesville, </institution> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: The results stated in this paragraph as well as the next two theorems are proven in <ref> [Cha90b] </ref>. RELATIVE ASSOCIATIVITY CHARACTERIZATION THEOREM. <p> Another approach to proving associativity is to rewrite the requirements for the definition of associativity to hold, perhaps using conditional expressions, before performing a case analysis. Such a proof of relative associativity for the combining function f for the false branch of FIND_LENGTH appears in the appendix to <ref> [Cha90b] </ref> and is also significantly more complex than the entire proof of the diva procedure FIND_LENGTH given in Section 5. <p> It is unnatural to include the requirement of associativity in the specification of a function, since in the context of such a specification the function operates on just two values. 4. As discussed in <ref> [Cha90b] </ref>, a proof technique based on strong induction can be used to prove the associativity of f using the same number of cases as 44 Define a type ANSWER_TYPE as type ANSWER_TYPE is record COUNT, VALUE: INTEGER; end record; initialize V so that for all I V (I).COUNT := 0; V <p> Thus typically two sequences must be referred to within the proof of associativity. Additional applications are possible if the associativity requirement is generalized to a requirement that the function simply be associative relative to the sequences of interest <ref> [Cha90b] </ref>. Such a requirement involving a sequence is even less natural to include in the specification of the function. 5. Strong induction can provide a simple approach to verifying program correctness for the kind of applications considered in this paper, as illustrated in Section 5.
Reference: [Cha] <author> Charlesworth, A. </author> <title> Programming a class of divide-and-conquer algorithms on the Intel iPSC/2. </title> <note> Paper in progress. </note>
Reference-contexts: Apply the general reduction operator to reduce V using f. 13 In addition, the iPSC/2 software requires that f be commutative, but this requirement can be overcome <ref> [Cha] </ref>. 43 A comparison of diva algorithms with general reduction operators can be summarized as follows: 1. The use of recursion is well-understood by the sequential programmer.
Reference: [Dal86] <author> Dally, W. J. </author> <title> A VLSI Architecture for Concurrent Data Structures. </title> <type> Ph.D. Dissertation, </type> <institution> Dept. of Computer Science, Calif. Instit. of Tech., </institution> <type> Tech. Rep. 5209, </type> <year> 1986. </year> <month> 51 </month>
Reference-contexts: Ideally, shared memory computers are programmed without considering the actual physical location of memory; however, contention for nonlocal memory can lead to serious degradation when a large number of processors attempt to access the same memory location simultaneously [YTL86, RT86]. Similarly, whereas the use of worm-hole routing <ref> [Dal86] </ref> permits distributed multicomputers to be programmed without considering the distance (in communication hops) between processors, contention for channels can lead to system degradation when many processors are involved in simultaneous communication.
Reference: [Dij59] <author> Dijkstra, E. </author> <title> A note on two problems in connexion with graphs. </title> <booktitle> Nu--mersiche Mathematik 1 (1959), </booktitle> <pages> 269-271. </pages>
Reference-contexts: and right slices begin if A'LENGTH = 1 then CHOSEN.MAX := A (A'FIRST); CHOSEN.CORR := B (A'FIRST); else FIND_MAX_CORR (A'INITIAL, B'INITIAL, L); FIND_MAX_CORR (A'FINAL, B'FINAL, R); if L.MAX &gt;= R.MAX then CHOSEN := L; else CHOSEN := R; end if; end FIND_MAX_CORR; of a Vector 15 a minimum spanning tree <ref> [Pri57, Dij59] </ref>. (A node occurring at the minimum distance from the nodes of a partial spanning tree is iteratively added to the partial spanning tree.) A straightforward modification of the diva procedure FIND_MAX_CORR yields a single generic diva procedure supporting both FIND_MAX_CORR and FIND_MIN_CORR: A functional parameter FUNCT would be used
Reference: [Dun90] <author> Duncan, R. </author> <title> A survey of parallel computer architectures. </title> <booktitle> Computer 23, </booktitle> <month> 2 (Feb. </month> <year> 1990), </year> <pages> 5-16. </pages>
Reference-contexts: have from the low hundreds to a thousand processors, 3 while the number of fine-grained processors in current single instruction stream, multiple data stream (SIMD) machines is in the tens of thousands. 4 Hybrids of approaches are also appearing, such as architectures that combine both the MIMD and SIMD approach <ref> [Dun90] </ref> and the use of a pool of processors by a system of workstations to perform tasks requiring multiple processors [MT86]. Even a wide-area network of heterogeneous workstations may be viewed as a single distributed computing system [BST89].
Reference: [Gri81] <editor> Gries, D. </editor> <booktitle> The Science of Programming. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Such a diva procedure is related to finding the number of alternating runs [Knu73] in a sequence. The next example also illustrates the use of overlap information. This problem is found in <ref> [Gri81] </ref>. 12 EXAMPLE 3: Find the length of the longest plateau of a nondecreasing sequence. A "plateau" is a slice all of whose components have equal values.
Reference: [HS86] <author> Hillis, W. D. and Steele, G. L., Jr. </author> <title> Data parallel algorithms. </title> <journal> Comm. ACM, </journal> <volume> 29, 12, </volume> <month> (Dec. </month> <year> 1986), </year> <pages> 1170-1183. </pages>
Reference-contexts: use such a parameter to get around the restriction on accessing dynamic parameters mentioned at the beginning of this paragraph. 8.3 Translation for SIMD Computers A modification of the target code in Figure 9 produces code for SIMD computers, such as the ICL DAP [ICL80, Per87] and the Connection Machine <ref> [HS86] </ref>, that permit data to be shifted from processors with higher processor numbers to processors with lower numbers by shifts that are powers of 2. This modification is straightforward since the message passing in Figure 9 can be replaced by shifting. <p> are combined; combining of results from different processors can be obtained through the use of a processor tree with communication via access to shared memory (Section 8.1) or via message-passing (Section 8.2), through the use of shifts (Section 8.3), or through the use of another technique (such as pointer doubling <ref> [HS86] </ref>). The allocation of processors for the execution of the diva call can be either static or dynamic and tolerance for faulty channels can be programmed into a parallel implementation, since there are n 1 possible choices for division point for vectors of length n.
Reference: [Hoe62] <author> Hoel, P. G. </author> <title> Introduction to Mathematical Statistics, </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1962. </year>
Reference: [Hoa78] <author> Hoare, C. A. R. </author> <title> Communicating sequential processes. </title> <journal> Comm. ACM, </journal> <volume> 21, 8, </volume> <month> (Aug. </month> <year> 1978), </year> <pages> 666-677. </pages>
Reference-contexts: Thus diva calls provide one approach to simplifying the programming of parallel computers. 49 In deciding whether to include a notation or construct supporting the non-deterministic divide within a programming language, one is reminded of the advice in Hoare's CSP article <ref> [Hoa78] </ref>: Where a more elaborate construction ... is frequently useful, has properties which are more simply provable, and can also be implemented more efficiently than the general case, there is a strong reason for including in a programming language a special notation for that construction.
Reference: [ICL80] <author> International Computers Ltd. </author> <title> DAP Fortran language. </title> <type> ICL Technical Pub. 6755, </type> <year> 1980. </year>
Reference-contexts: it could be quite costly to use such a parameter to get around the restriction on accessing dynamic parameters mentioned at the beginning of this paragraph. 8.3 Translation for SIMD Computers A modification of the target code in Figure 9 produces code for SIMD computers, such as the ICL DAP <ref> [ICL80, Per87] </ref> and the Connection Machine [HS86], that permit data to be shifted from processors with higher processor numbers to processors with lower numbers by shifts that are powers of 2. This modification is straightforward since the message passing in Figure 9 can be replaced by shifting.
Reference: [Int89] <author> Intel Corporation, </author> <title> iPSC/2 Programmer's Reference Manual. </title> <address> Beaver-ton, Or., </address> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: using standard associative functions, such as +, *, and, or, max, and min, is commonly provided within languages for parallel computing. 5 Support for using less trivial programmer-defined associative functions in computing reductions of vectors is also provided in several languages for parallel computing, such as iPSC/2 Fortran and C <ref> [Int89] </ref> and the innovative and less conventional languages Connection Machine Lisp [SH86] and Paralation Lisp [Sab88]. Within conventional semantic models, the level of abstraction provided by such general reduction operators is less than ideal when nontrivial programmer-defined functions are used. <p> Note that, like the use of diva procedures, such a computation can be programmed without requiring that the programmer consider parallelism. Other approaches, such as that of iPSC/2 Fortran and C <ref> [Int89] </ref> are only permitted within parallel code and assume a particular target architecture. 13 To illustrate this technique, consider computing the number of times a term appears in a finite sequence S that is greater than the next term of the sequence. (Recall that this was accomplished in Example 2 using
Reference: [Ive62] <author> Iverson, K. E., </author> <title> A Programming Language. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: combining of function values, the difficulty of showing that a nontrivial function is associative in the 1 e.g., Sequent Symmetry, Encore Multimax 2 e.g., BBN Butterfly, IBM RP3 3 e.g., Intel iPSC/2, NCube/10 4 e.g., Connection Machine, MassPar MP-1 5 Language support for reductions was provided much earlier by APL <ref> [Ive62] </ref>. 1 absence of a suitable conceptualization, the fact that the language form for ex-pressing a general reduction operator does not make underlying assumptions clear to the programmer, the fact that information known at the programmer's level of abstraction must be withheld sometimes from compilers and syntax-directed editors even though such
Reference: [KMR90] <author> Koebel, C., Mehrotra, P., and Rosendale, J. V. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <journal> SIGPLAN NOTICES, </journal> <volume> 25, </volume> <month> 3 (Mar. </month> <year> 1990), </year> <pages> 177-186. </pages> <booktitle> (Proc. Second ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming) </booktitle>
Reference-contexts: Permitting distributed memory computers to be programmed at a level supporting the illusion of shared memory is a related but separate research problem that is being investigated by numerous researchers; for example, see <ref> [CK88, KMR90, Li86, RSW88, SCMB90] </ref>. Thus the focus of this section is on activities within the diva call itself, rather than on distributing vectors and other data among the local memories. The target code given in Figure 9 can be modified for distributed memory as follows.
Reference: [Knu73] <author> Knuth, D. E. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol. </volume> <month> 3: </month> <title> Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1973. </year>
Reference-contexts: Information about the 4-element overlap between adjacent slices of the sequence needs to be preserved in order to examine each consecutive triple of terms in the sequence. Such a diva procedure is related to finding the number of alternating runs <ref> [Knu73] </ref> in a sequence. The next example also illustrates the use of overlap information. This problem is found in [Gri81]. 12 EXAMPLE 3: Find the length of the longest plateau of a nondecreasing sequence. A "plateau" is a slice all of whose components have equal values.
Reference: [Li86] <author> Li, Kai. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> Ph. D. thesis, </type> <institution> Yale University, </institution> <address> New Haven, Conn., </address> <month> Sept. </month> <year> 1986. </year>
Reference-contexts: Permitting distributed memory computers to be programmed at a level supporting the illusion of shared memory is a related but separate research problem that is being investigated by numerous researchers; for example, see <ref> [CK88, KMR90, Li86, RSW88, SCMB90] </ref>. Thus the focus of this section is on activities within the diva call itself, rather than on distributing vectors and other data among the local memories. The target code given in Figure 9 can be modified for distributed memory as follows.
Reference: [MT86] <author> Mullender, S. J. and Tanenbaum, A. S. </author> <title> The design of a capability-based distributed operating system. </title> <journal> Computer J., </journal> <volume> 29, 4 (1986), </volume> <pages> 289-300. </pages>
Reference-contexts: stream, multiple data stream (SIMD) machines is in the tens of thousands. 4 Hybrids of approaches are also appearing, such as architectures that combine both the MIMD and SIMD approach [Dun90] and the use of a pool of processors by a system of workstations to perform tasks requiring multiple processors <ref> [MT86] </ref>. Even a wide-area network of heterogeneous workstations may be viewed as a single distributed computing system [BST89]. A paramount question is how such diverse computers, and computers resulting from additional evolution, should be programmed. As always with software, program correctness is a key issue.
Reference: [Per87] <author> Perrott, R. H. </author> <title> Parallel Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1987. </year> <month> 52 </month>
Reference-contexts: it could be quite costly to use such a parameter to get around the restriction on accessing dynamic parameters mentioned at the beginning of this paragraph. 8.3 Translation for SIMD Computers A modification of the target code in Figure 9 produces code for SIMD computers, such as the ICL DAP <ref> [ICL80, Per87] </ref> and the Connection Machine [HS86], that permit data to be shifted from processors with higher processor numbers to processors with lower numbers by shifts that are powers of 2. This modification is straightforward since the message passing in Figure 9 can be replaced by shifting.
Reference: [Pri57] <author> Prim, R. C. </author> <title> Shortest connection networks and some generalizations. </title> <journal> Bell System Tech. J. </journal> <volume> 36 (1957), </volume> <pages> 1389-1401. </pages>
Reference-contexts: and right slices begin if A'LENGTH = 1 then CHOSEN.MAX := A (A'FIRST); CHOSEN.CORR := B (A'FIRST); else FIND_MAX_CORR (A'INITIAL, B'INITIAL, L); FIND_MAX_CORR (A'FINAL, B'FINAL, R); if L.MAX &gt;= R.MAX then CHOSEN := L; else CHOSEN := R; end if; end FIND_MAX_CORR; of a Vector 15 a minimum spanning tree <ref> [Pri57, Dij59] </ref>. (A node occurring at the minimum distance from the nodes of a partial spanning tree is iteratively added to the partial spanning tree.) A straightforward modification of the diva procedure FIND_MAX_CORR yields a single generic diva procedure supporting both FIND_MAX_CORR and FIND_MIN_CORR: A functional parameter FUNCT would be used
Reference: [RT86] <author> Rettberg, R. and Thomas, R. </author> <title> Contention is no obstacle to shared-memory multiprocessing. </title> <journal> Comm. ACM, </journal> <volume> 29, </volume> <month> 12 (Dec. </month> <year> 1986), </year> <pages> 1202-1212. </pages>
Reference-contexts: Ideally, shared memory computers are programmed without considering the actual physical location of memory; however, contention for nonlocal memory can lead to serious degradation when a large number of processors attempt to access the same memory location simultaneously <ref> [YTL86, RT86] </ref>. Similarly, whereas the use of worm-hole routing [Dal86] permits distributed multicomputers to be programmed without considering the distance (in communication hops) between processors, contention for channels can lead to system degradation when many processors are involved in simultaneous communication.
Reference: [RSW88] <author> Rosing, M., Schnabel, R. B., and Weaver, R. P. Dino: </author> <title> Summary and examples. </title> <booktitle> Proc. Third Conf. on Hypercube Concurrent Computers and Appl. </booktitle> <year> (1988), </year> <pages> 472-481. </pages>
Reference-contexts: Permitting distributed memory computers to be programmed at a level supporting the illusion of shared memory is a related but separate research problem that is being investigated by numerous researchers; for example, see <ref> [CK88, KMR90, Li86, RSW88, SCMB90] </ref>. Thus the focus of this section is on activities within the diva call itself, rather than on distributing vectors and other data among the local memories. The target code given in Figure 9 can be modified for distributed memory as follows.
Reference: [RSL74] <author> Rosenkrantz, D., Stearns, R., and Lewis, P. </author> <title> Approximate algorithms for the traveling salesperson problem. </title> <booktitle> Proc. 15th Annual IEEE Symp. on Switching and Automata Theory. </booktitle> <year> 1974, </year> <pages> 33-42. </pages>
Reference-contexts: Both FIND_MAX_CORR and FIND_MIN_CORR support programming the farthest insertion heuristic algorithm for the Euclidean traveling salesman problem <ref> [RSL74] </ref>. (A node occurring at the maximum distance from the nodes of the current partial tour is the next node chosen to add to the partial tour, and to add this node most cheaply, it is necessary to find a node in the current partial tour occurring at the minimum distance
Reference: [Sab88] <author> Sabot, G. </author> <title> The Paralation Model: Architecture-Independent Parallel Programming. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: provided within languages for parallel computing. 5 Support for using less trivial programmer-defined associative functions in computing reductions of vectors is also provided in several languages for parallel computing, such as iPSC/2 Fortran and C [Int89] and the innovative and less conventional languages Connection Machine Lisp [SH86] and Paralation Lisp <ref> [Sab88] </ref>. Within conventional semantic models, the level of abstraction provided by such general reduction operators is less than ideal when nontrivial programmer-defined functions are used. <p> This storage advantage is also provided by the "fields" of a paralation <ref> [Sab88] </ref>: such "fields" can be added and deleted according to the needs of different parts of the program. <p> Such implementation details could help obtain more efficient use of the target computer, yet the source code would retain the existing advantages of diva procedures relating to logical simplicity and portability. 42 9 COMPARISON WITH REDUCTION OP- ERATORS Connection Machine Lisp [SH86] and Paralation Lisp <ref> [Sab88] </ref>, recent innovative and unconventional languages for parallel computing, permit the programmer to reduce a vector V using a programmer-defined function f . Such a reduction can be implemented in log time on a suitable parallel computer, assuming that each call on the function takes constant time.
Reference: [SCMB90] <author> Saltz, J., Crowley, K., Mirchandaney, R., and Berryman, H. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> J. of Parallel and Distributed Computing, </journal> <note> to appear. </note>
Reference-contexts: Permitting distributed memory computers to be programmed at a level supporting the illusion of shared memory is a related but separate research problem that is being investigated by numerous researchers; for example, see <ref> [CK88, KMR90, Li86, RSW88, SCMB90] </ref>. Thus the focus of this section is on activities within the diva call itself, rather than on distributing vectors and other data among the local memories. The target code given in Figure 9 can be modified for distributed memory as follows.
Reference: [SH86] <author> Steele, G. L., Jr. and Hillis, W. D. </author> <title> Connection Machine Lisp: Fine-grained parallel symbolic processing. </title> <booktitle> Proc. 1986 ACM Conference on Lisp and Functional Programming. </booktitle> <month> Aug. </month> <year> 1986, </year> <pages> 279-297. </pages>
Reference-contexts: and min, is commonly provided within languages for parallel computing. 5 Support for using less trivial programmer-defined associative functions in computing reductions of vectors is also provided in several languages for parallel computing, such as iPSC/2 Fortran and C [Int89] and the innovative and less conventional languages Connection Machine Lisp <ref> [SH86] </ref> and Paralation Lisp [Sab88]. Within conventional semantic models, the level of abstraction provided by such general reduction operators is less than ideal when nontrivial programmer-defined functions are used. <p> Such implementation details could help obtain more efficient use of the target computer, yet the source code would retain the existing advantages of diva procedures relating to logical simplicity and portability. 42 9 COMPARISON WITH REDUCTION OP- ERATORS Connection Machine Lisp <ref> [SH86] </ref> and Paralation Lisp [Sab88], recent innovative and unconventional languages for parallel computing, permit the programmer to reduce a vector V using a programmer-defined function f .
Reference: [Qui83] <author> Quinn, M. J. </author> <title> The Design and Analysis of Algorithms and Data Structures for the Efficient Solution of Graph Theoretic Problems on MIMD Computers. </title> <publisher> Ph. </publisher> <address> D. </address> <institution> Dissertation, Dept. of Computer Science, Wash. State Univ., </institution> <address> Pullman, Wash., </address> <year> 1983. </year>
Reference: [Uni83] <institution> United States Department of Defense. </institution> <note> Reference Manual for the Ada Programming Language. </note> <institution> ANSI/MIL-STD-1815A-1983, American National Standards Institute, </institution> <year> 1983. </year>
Reference-contexts: The nondeterministic divide can be implemented as a divide just before the last component of a vector (or just after the first component) thereby 6 A slice of a vector is a sequence of consecutive components of the vector <ref> [Uni83] </ref>. 3 yielding a sequential loop. Such a loop is often more efficient than the use of recursion to implement a diva algorithm on a sequential computer. <p> In addition, most readers either will be familiar with Ada or have ready access to an Ada reference, such as [Bar84] and <ref> [Uni83] </ref>. In view of Ada's existing complexity, the author is not recommending that Ada be extended with syntax to support diva algorithms. <p> Diva calls are most useful when the range of such vectors is large. Since parameter passing for array parameters may be implemented by copy <ref> [Uni83, 6.2.7] </ref>, the Ada programmer will realize that it could be quite costly to use Z as an actual argument when not all fields are needed. 10 An alternative in this situation would be to define several different vectors of records for different portions of the program, but since some of <p> It is no violation of the syntax rules for IN_PARM_TYPE to be a named array type with unconstrained range. However, since parameter passing for an array may be implemented by copy <ref> [Uni83, 6.2.7] </ref>, the Ada programmer will realize that it could be quite costly to use such a parameter to get around the restriction on accessing dynamic parameters mentioned at the beginning of this paragraph. 8.3 Translation for SIMD Computers A modification of the target code in Figure 9 produces code for
Reference: [YTL86] <author> Yew, P. C., Tzeng, N. S., and Lawrie, D. H. </author> <title> Distributing hot spot addressing in large-scale multiprocessors. </title> <booktitle> Proc. of the 1986 Intl. Conf. on Parallel Processing IEEE Press, </booktitle> <address> New York, </address> <year> 1986, </year> <pages> 51-58. 53 </pages>
Reference-contexts: Ideally, shared memory computers are programmed without considering the actual physical location of memory; however, contention for nonlocal memory can lead to serious degradation when a large number of processors attempt to access the same memory location simultaneously <ref> [YTL86, RT86] </ref>. Similarly, whereas the use of worm-hole routing [Dal86] permits distributed multicomputers to be programmed without considering the distance (in communication hops) between processors, contention for channels can lead to system degradation when many processors are involved in simultaneous communication.
References-found: 37

