URL: http://www.iscs.nus.sg/~plong/papers/distdrift2.ps
Refering-URL: 
Root-URL: 
Email: plong@iscs.nus.edu.sg  
Title: The Complexity of Learning According to Two Models of a Drifting Environment sufficient condition for
Author: Philip M. Long 
Note: VCdim(F)  ffi  
Address: Singapore 119260, Republic of Singapore  
Affiliation: ISCS Department National University of Singapore  
Abstract: We show that a c* 3 VCdim(F) bound on the rate of drift of the distribution generating the examples is sufficient for agnostic learning to relative accuracy *, where c &gt; 0 is a constant; this matches a known necessary condition to within a constant factor. We establish a c* 2 condition to within a constant factor. We provide a relatively simple proof of a bound of O * 2 VCdim(F ) + log 1 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Anthony, N. Biggs, and J. </author> <title> Shawe-Taylor. </title> <booktitle> The learn-ability of formal concepts. Proceedings of the 1990 Workshop on Computational Learning Theory, </booktitle> <pages> pages 246257, </pages> <year> 1990. </year>
Reference-contexts: Lemma 1 (see [27]) Choose countable sets Z 1 and Z 2 , a function f : Z 1 fi Z 2 ! <ref> [0; 1] </ref> and probability distributions D 1 over Z 1 and D 2 over Z 2 . <p> Then if m (ff ) 2 then ( i=1 ( fi fi ^ E ~z (g) Z fi fi ) Next, we record a slight variant of a well-known lemma for converting tail bounds to expectation bounds. Lemma 14 For any <ref> [0; 1] </ref>-valued random variable Y , if ' : [0; 1] ! [0; 1] is such that for all fi, Pr (Y &gt; fi) '(fi), then for all 0 = a 0 a 1 :::a k a k+1 = 1, E (Y ) i=0 Proof: The distribution on Y that maximizes <p> Lemma 14 For any <ref> [0; 1] </ref>-valued random variable Y , if ' : [0; 1] ! [0; 1] is such that for all fi, Pr (Y &gt; fi) '(fi), then for all 0 = a 0 a 1 :::a k a k+1 = 1, E (Y ) i=0 Proof: The distribution on Y that maximizes its expectation subject to 8i; Pr (Y &gt; a <p> Lemma 14 For any <ref> [0; 1] </ref>-valued random variable Y , if ' : [0; 1] ! [0; 1] is such that for all fi, Pr (Y &gt; fi) '(fi), then for all 0 = a 0 a 1 :::a k a k+1 = 1, E (Y ) i=0 Proof: The distribution on Y that maximizes its expectation subject to 8i; Pr (Y &gt; a i ) '(a <p> (F ) e (d + 1) k VCdim (F ) VCdim (F ) : Taking logs, we get d ln 2 k ln e (d + 1) + VCdim (F ) ln VCdim (F ) Since for all x; &gt; 0, 1 + ln x x + ln (1=) (see <ref> [1] </ref>), we have that for all &gt; 0, d ln 2 (2d + 1) + (VCdim (F ) + k) ln (1=): Solving for d and substituting = 1=10 completes the proof. Lemma 19 Choose m 2 N and F f0; 1g m .
Reference: [2] <author> P. Auer and M. K. Warmuth. </author> <title> Tracking the best disjunction. </title> <booktitle> Proceedings of the 36th Annual Symposium on the Foundations of Computer Science, </booktitle> <year> 1995. </year> <title> 3 The behavior of A 0 F for small m is immaterial. </title>
Reference-contexts: Models of a changing environment that are more dissimilar to that studied here were considered in <ref> [22, 20, 21, 7, 10, 15, 2, 19, 32, 16] </ref>. must output a hypothesis h t using only the first t 1 examples.
Reference: [3] <author> P. L. Bartlett. </author> <title> Learning with a slowly changing distribution. </title> <booktitle> Proceedings of the 1992 Workshop on Computational Learning Theory, </booktitle> <pages> pages 243252, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Learning often takes place in a gradually changing environment. This phenomenon has been studied theoretically by assuming that the function to be learned, the distribution generating the examples, or both, change at most a certain amount between examples (see <ref> [14, 3, 5, 6] </ref>). 1 In this paper, we study the problem of learning functions from some set X to f0; 1g (concepts) using two models of a drifting environment. <p> In the first <ref> [3] </ref>, it is assumed that examples (x 1 ; y 1 ); (x 2 ; y 2 ); ::: are generated independently at random from a sequence of joint distributions over X fi f0; 1g, and the only constraint is that consecutive pairs of distributions have small total variation distance. <p> The second model of learning in a drifting environment <ref> [14, 3, 5] </ref> is obtained from the above by adding the requirement that each distribution P t has some f t 2 F such that the probability that the pair (x t ; y t ) drawn according to P t has f t (x t ) = y t is <p> This work continues an existing line of research <ref> [14, 3, 5, 6] </ref>, and matches known necessary conditions for both the agnostic [6] and realizable [3] cases to within a constant factor, closing log-factor gaps. <p> This work continues an existing line of research [14, 3, 5, 6], and matches known necessary conditions for both the agnostic [6] and realizable <ref> [3] </ref> cases to within a constant factor, closing log-factor gaps. Note that both models allow for variation both in the target and in the marginal distribution on the domain elements; some previous work addressed these two types of changes separately. <p> In the realizable case, as in <ref> [14, 3, 5] </ref>, we consider an algorithm based on the one-inclusion graph algorithm [13], which was originally designed for learning concepts in a fixed environment.
Reference: [4] <author> P. L. Bartlett, S. Ben-David, and S. R. Kulkarni. </author> <title> Learning changing concepts by exploiting the structure of change. </title> <booktitle> Proceedings of the 1996 Conference on Computational Learning Theory, </booktitle> <pages> pages 131139, </pages> <year> 1996. </year>
Reference-contexts: If this distance is always at most , then the sequence of distribu tions is called -gradual. For each t, the learning algorithm 1 Recently, other constraints on the drift have been examined (e.g., <ref> [4, 9] </ref>). In this paper we restrict our attention to the simplest drift models, but direct application of a slight variant of Lemma 13 of this paper leads to a small improvement in the analysis of [9].
Reference: [5] <author> P. L. Bartlett and D. P. Helmbold, </author> <year> 1995. </year> <type> Manuscript. </type>
Reference-contexts: 1 Introduction Learning often takes place in a gradually changing environment. This phenomenon has been studied theoretically by assuming that the function to be learned, the distribution generating the examples, or both, change at most a certain amount between examples (see <ref> [14, 3, 5, 6] </ref>). 1 In this paper, we study the problem of learning functions from some set X to f0; 1g (concepts) using two models of a drifting environment. <p> The second model of learning in a drifting environment <ref> [14, 3, 5] </ref> is obtained from the above by adding the requirement that each distribution P t has some f t 2 F such that the probability that the pair (x t ; y t ) drawn according to P t has f t (x t ) = y t is <p> This work continues an existing line of research <ref> [14, 3, 5, 6] </ref>, and matches known necessary conditions for both the agnostic [6] and realizable [3] cases to within a constant factor, closing log-factor gaps. <p> In the realizable case, as in <ref> [14, 3, 5] </ref>, we consider an algorithm based on the one-inclusion graph algorithm [13], which was originally designed for learning concepts in a fixed environment. <p> Since any one-inclusion graph for F can be shown to be sparse relative to VCdim (F ), the edges can be directed so that the out-degree of any vertex is at most VCdim (F ) [13]. In <ref> [14, 5] </ref>, the vertex set was expanded to include elements of f0; 1g m that are within some Hamming distance of elements of f (f (x 1 ); :::; f (x m )) : f 2 F g; these graphs also can be shown to be sparse.
Reference: [6] <author> R. D. Barve and P. M. </author> <title> Long. On the complexity of learning from drifting distributions. Information and Computation, </title> <address> 138(2):101123, </address> <year> 1997. </year>
Reference-contexts: 1 Introduction Learning often takes place in a gradually changing environment. This phenomenon has been studied theoretically by assuming that the function to be learned, the distribution generating the examples, or both, change at most a certain amount between examples (see <ref> [14, 3, 5, 6] </ref>). 1 In this paper, we study the problem of learning functions from some set X to f0; 1g (concepts) using two models of a drifting environment. <p> This work continues an existing line of research <ref> [14, 3, 5, 6] </ref>, and matches known necessary conditions for both the agnostic [6] and realizable [3] cases to within a constant factor, closing log-factor gaps. <p> This work continues an existing line of research [14, 3, 5, 6], and matches known necessary conditions for both the agnostic <ref> [6] </ref> and realizable [3] cases to within a constant factor, closing log-factor gaps. Note that both models allow for variation both in the target and in the marginal distribution on the domain elements; some previous work addressed these two types of changes separately.
Reference: [7] <author> A. Blum and P. Chalasani. </author> <title> Learning switching concepts. </title> <booktitle> Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages pages 231 242, </pages> <year> 1992. </year>
Reference-contexts: Models of a changing environment that are more dissimilar to that studied here were considered in <ref> [22, 20, 21, 7, 10, 15, 2, 19, 32, 16] </ref>. must output a hypothesis h t using only the first t 1 examples.
Reference: [8] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> JACM, </journal> <volume> 36(4):929965, </volume> <year> 1989. </year>
Reference-contexts: said to (*; ffi)-agnostically learn F from m examples if for all distributions P on X fi f0; 1g, P m ~z : er P (A (~z)) &gt; * + inf er P (f ) ffi: To set the context, we briefly review the work that our anal-ysis builds on <ref> [33, 24, 8, 11] </ref>. For each f 2 F , define L f : X fi f0; 1g ! f0; 1g by L f (x; y) = jf (x) yj. Define L F = fL f : f 2 F g. <p> The following will also be useful. Lemma 4 (see <ref> [8] </ref>) VCdim (L F ) VCdim (F ). So now we can concentrate on determining distribution-free bounds, in terms on the VC-dimension, on the number of examples required to obtain uniformly good estimates of the expectations of random variables in some set.
Reference: [9] <author> Y. Freund and Y. Mansour. </author> <title> Learning under persistent drift. </title> <booktitle> Proceedings of the 1997 European Conference on Computational Learning Theory, </booktitle> <year> 1997. </year>
Reference-contexts: If this distance is always at most , then the sequence of distribu tions is called -gradual. For each t, the learning algorithm 1 Recently, other constraints on the drift have been examined (e.g., <ref> [4, 9] </ref>). In this paper we restrict our attention to the simplest drift models, but direct application of a slight variant of Lemma 13 of this paper leads to a small improvement in the analysis of [9]. <p> In this paper we restrict our attention to the simplest drift models, but direct application of a slight variant of Lemma 13 of this paper leads to a small improvement in the analysis of <ref> [9] </ref>. Models of a changing environment that are more dissimilar to that studied here were considered in [22, 20, 21, 7, 10, 15, 2, 19, 32, 16]. must output a hypothesis h t using only the first t 1 examples.
Reference: [10] <author> Y. Freund and D. Ron. </author> <title> Learning to model sequences generated by switching distributions. </title> <booktitle> Proceedings of the 1995 Conference on Computational Learning Theory, </booktitle> <pages> pages 4150, </pages> <year> 1995. </year>
Reference-contexts: Models of a changing environment that are more dissimilar to that studied here were considered in <ref> [22, 20, 21, 7, 10, 15, 2, 19, 32, 16] </ref>. must output a hypothesis h t using only the first t 1 examples.
Reference: [11] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. Information and Computation, </title> <address> 100(1):78150, </address> <year> 1992. </year>
Reference-contexts: vertex is bounded appropriately in terms of its distance to the closest element of f (f (x 1 ); :::; f (x m )) : f 2 F g as well as the VC-dimension of F . 1.1 Agnostic learning in a fixed environment In the standard agnostic learning model <ref> [11, 17] </ref>, random examples (x 1 ; y 1 ); :::; (x m ; y m ) are drawn from an arbitrary joint distribution P , and the learner's goal is to output a function h such that probability that h (x) 6= y for another pair (x; y) drawn according <p> This bound, which also follows from previous work of Talagrand [31], improves on the bound of O 1 VCdim (F ) log 1 * + log 1 that follows from Vap-nik and Chervonenkis' results (see <ref> [11] </ref>), and matches Si-mon's general lower bound [30] to within a constant factor for each concept class F . Our constants are greater than Ta lagrand's, but our proof is simpler and more elementary. 2 Preliminaries Fix a countable set X. <p> said to (*; ffi)-agnostically learn F from m examples if for all distributions P on X fi f0; 1g, P m ~z : er P (A (~z)) &gt; * + inf er P (f ) ffi: To set the context, we briefly review the work that our anal-ysis builds on <ref> [33, 24, 8, 11] </ref>. For each f 2 F , define L f : X fi f0; 1g ! f0; 1g by L f (x; y) = jf (x) yj. Define L F = fL f : f 2 F g. <p> m=2 By combining Lemmas 3, 4, 5, 6, and 7, and applying a bound on jGj in terms of VCdim (G) [28, 29, 33] in Lemma 7, one gets a bound of O 1 VCdim (F ) log 1 + log ffi on the sample complexity of agnostically learning F <ref> [11] </ref>. Our argument will take advantage of the following refinement of Lemma 7, which also follows directly from Lemma 2. Lemma 8 Choose m; k 2 N, and suppose that H R m has the property that each h 2 H has P m i k.
Reference: [12] <author> D. Haussler. </author> <title> Sphere packing numbers for subsets of the boolean n-cube with bounded Vapnik-Chervonenkis dimension. </title> <journal> Journal of Combinatorial Theory, Series A, </journal> <volume> 69(2):217232, </volume> <year> 1995. </year>
Reference: [13] <author> D. Haussler, N. Littlestone, and M. K. Warmuth. </author> <title> Predicting f0; 1g-functions on randomly drawn points. Information and Computation, </title> <address> 115(2):129161, </address> <year> 1994. </year>
Reference-contexts: In the realizable case, as in [14, 3, 5], we consider an algorithm based on the one-inclusion graph algorithm <ref> [13] </ref>, which was originally designed for learning concepts in a fixed environment. <p> Since any one-inclusion graph for F can be shown to be sparse relative to VCdim (F ), the edges can be directed so that the out-degree of any vertex is at most VCdim (F ) <ref> [13] </ref>. In [14, 5], the vertex set was expanded to include elements of f0; 1g m that are within some Hamming distance of elements of f (f (x 1 ); :::; f (x m )) : f 2 F g; these graphs also can be shown to be sparse. <p> Recall that VCdim (L F ) VCdim (F ) (Lemma 4). Choose a -gradual sequence P 1 ; P 2 ; ::: of probability distributions, f fl 2 F and t &gt; m. Applying Lemma 1 as in <ref> [13] </ref>, the probability that (x 1 ; y 1 ); :::; (x t ; y t ) drawn ac cording to Q t i=1 P i causes a mistake for A is equal to the expectation, with respect to the first t 1 examples, of P t f (x t ;
Reference: [14] <author> D. P. Helmbold and P. M. </author> <title> Long. Tracking drifting concepts by minimizing disagreements. </title> <booktitle> Machine Learning, </booktitle> <address> 14(1):2746, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Learning often takes place in a gradually changing environment. This phenomenon has been studied theoretically by assuming that the function to be learned, the distribution generating the examples, or both, change at most a certain amount between examples (see <ref> [14, 3, 5, 6] </ref>). 1 In this paper, we study the problem of learning functions from some set X to f0; 1g (concepts) using two models of a drifting environment. <p> The second model of learning in a drifting environment <ref> [14, 3, 5] </ref> is obtained from the above by adding the requirement that each distribution P t has some f t 2 F such that the probability that the pair (x t ; y t ) drawn according to P t has f t (x t ) = y t is <p> This work continues an existing line of research <ref> [14, 3, 5, 6] </ref>, and matches known necessary conditions for both the agnostic [6] and realizable [3] cases to within a constant factor, closing log-factor gaps. <p> In the realizable case, as in <ref> [14, 3, 5] </ref>, we consider an algorithm based on the one-inclusion graph algorithm [13], which was originally designed for learning concepts in a fixed environment. <p> Since any one-inclusion graph for F can be shown to be sparse relative to VCdim (F ), the edges can be directed so that the out-degree of any vertex is at most VCdim (F ) [13]. In <ref> [14, 5] </ref>, the vertex set was expanded to include elements of f0; 1g m that are within some Hamming distance of elements of f (f (x 1 ); :::; f (x m )) : f 2 F g; these graphs also can be shown to be sparse.
Reference: [15] <author> M. Herbster and M. K. Warmuth. </author> <title> Tracking the best expert. </title> <booktitle> Proceedings of of the Twelvth International Conference on Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: Models of a changing environment that are more dissimilar to that studied here were considered in <ref> [22, 20, 21, 7, 10, 15, 2, 19, 32, 16] </ref>. must output a hypothesis h t using only the first t 1 examples.
Reference: [16] <author> M. Herbster and M. K. Warmuth. </author> <title> Tracking the best regressor. </title> <booktitle> Proceedings of the 1998 Conference on Computational Learning Theory, </booktitle> <year> 1998. </year>
Reference-contexts: Models of a changing environment that are more dissimilar to that studied here were considered in <ref> [22, 20, 21, 7, 10, 15, 2, 19, 32, 16] </ref>. must output a hypothesis h t using only the first t 1 examples.
Reference: [17] <author> M. J. Kearns, R. E. Schapire, and L. M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <journal> Machine Learning, </journal> <volume> 17:115 141, </volume> <year> 1994. </year>
Reference-contexts: vertex is bounded appropriately in terms of its distance to the closest element of f (f (x 1 ); :::; f (x m )) : f 2 F g as well as the VC-dimension of F . 1.1 Agnostic learning in a fixed environment In the standard agnostic learning model <ref> [11, 17] </ref>, random examples (x 1 ; y 1 ); :::; (x m ; y m ) are drawn from an arbitrary joint distribution P , and the learner's goal is to output a function h such that probability that h (x) 6= y for another pair (x; y) drawn according
Reference: [18] <author> A. N. Kolmogorov and V. M. Tihomirov. </author> <title> *-entropy and *-capacity of sets in functional spaces. </title> <journal> American Mathematical Society Translations (Ser. </journal> <volume> 2), 17:277 364, </volume> <year> 1961. </year>
Reference-contexts: We will use the following result due to Haussler, which bounds the number of significantly different elements of a set G in terms of its VC-dimension. This can be used to bound the size of an approximation to G <ref> [18] </ref>. Lemma 10 ([12]) For all m 2 N, for all k m, if each pair g; h of elements of G f0; 1g m has (g; h) &gt; k, then jGj 41m VCdim (G) : Proof (of Lemma 9): Let n = 1 + blog 2 mc.
Reference: [19] <author> A. Kuh. </author> <title> Comparison of tracking algorithms for single layer threshold networks in the presence of random drift. </title> <journal> IEEE Trans. on Signal Processing, </journal> <volume> 45(3):640 650, </volume> <year> 1997. </year>
Reference-contexts: Models of a changing environment that are more dissimilar to that studied here were considered in <ref> [22, 20, 21, 7, 10, 15, 2, 19, 32, 16] </ref>. must output a hypothesis h t using only the first t 1 examples.
Reference: [20] <author> A. Kuh, T. Petsche, and R. Rivest. </author> <title> Learning time varying concepts. </title> <booktitle> In NIPS 3. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Models of a changing environment that are more dissimilar to that studied here were considered in <ref> [22, 20, 21, 7, 10, 15, 2, 19, 32, 16] </ref>. must output a hypothesis h t using only the first t 1 examples.
Reference: [21] <author> A. Kuh, T. Petsche, and R. Rivest. </author> <title> Mistake bounds of incremental learners when concepts drift with appli cations to feedforward networks. </title> <booktitle> In NIPS 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Models of a changing environment that are more dissimilar to that studied here were considered in <ref> [22, 20, 21, 7, 10, 15, 2, 19, 32, 16] </ref>. must output a hypothesis h t using only the first t 1 examples.
Reference: [22] <author> N. Littlestone and M. K. Warmuth. </author> <title> The weighted majority algorithm. Information and Computation, </title> <address> 108:212261, </address> <year> 1994. </year>
Reference-contexts: Models of a changing environment that are more dissimilar to that studied here were considered in <ref> [22, 20, 21, 7, 10, 15, 2, 19, 32, 16] </ref>. must output a hypothesis h t using only the first t 1 examples.
Reference: [23] <author> P. M. </author> <title> Long. On the sample complexity of learning functions with bounded variation. </title> <booktitle> Proceedings of the 1998 Conference on Computational Learning Theory, </booktitle> <year> 1998. </year>
Reference-contexts: Note that both models allow for variation both in the target and in the marginal distribution on the domain elements; some previous work addressed these two types of changes separately. The agnostic drift analysis uses a technique called Chaining from Empirical Process Theory (see [25]). (In a companion paper <ref> [23] </ref>, we apply this technique to bound the sample complexity of agnostically learning functions with bounded variation.) We defer a high-level description of this tech nique until later in the paper when appropriate context is available.
Reference: [24] <author> D. Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Lemma 2 (see <ref> [24] </ref>) Let Y 1 ; :::; Y m be independent random variables taking values in [a 1 ; b 1 ]; :::; [a m ; b m ] respectively. <p> said to (*; ffi)-agnostically learn F from m examples if for all distributions P on X fi f0; 1g, P m ~z : er P (A (~z)) &gt; * + inf er P (f ) ffi: To set the context, we briefly review the work that our anal-ysis builds on <ref> [33, 24, 8, 11] </ref>. For each f 2 F , define L f : X fi f0; 1g ! f0; 1g by L f (x; y) = jf (x) yj. Define L F = fL f : f 2 F g.
Reference: [25] <author> D. Pollard. </author> <title> Empirical Processes : Theory and Applications, </title> <booktitle> volume 2 of NSF-CBMS Regional Conference Series in Probability and Statistics. </booktitle> <institution> Institute of Math. Stat. and Am. Stat. Assoc., </institution> <year> 1990. </year>
Reference-contexts: Note that both models allow for variation both in the target and in the marginal distribution on the domain elements; some previous work addressed these two types of changes separately. The agnostic drift analysis uses a technique called Chaining from Empirical Process Theory (see <ref> [25] </ref>). (In a companion paper [23], we apply this technique to bound the sample complexity of agnostically learning functions with bounded variation.) We defer a high-level description of this tech nique until later in the paper when appropriate context is available. <p> Then if U is the uniform distribution over f1; 1g m , for any &gt; 0, ( fi fi fi m i=1 fi fi fi ) The proof is a chaining argument. See Pollard's book <ref> [25] </ref> for another, and for references to others. The idea is as follows. First, we form a sequence G 0 ; :::; G n of approximations to G. The approximations get successively finer until G n = G.
Reference: [26] <author> S. Roy. </author> <title> Semantic complexity of relational queries and data independent data partitioning. </title> <booktitle> Proceedings of the ACM SIGACT-SIGART-SIGMOD Annual Symposium on Principles of Database Systems, </booktitle> <year> 1991. </year>
Reference-contexts: Lemma 17 ([29, 28, 8]) For m 2 N, F f0; 1g m , jF j (em=VCdim (F )) VCdim (F ) : The proof of our next lemma is similar to that of a related result of Roy <ref> [26] </ref>. Lemma 18 For any m 2 N, for any F f0; 1g m , for any k 2 f1; :::; mg, VCdim ( k1 (F ) [ k (F )) 5 (VCdim (F ) + k): Proof: Assume without loss of generality that jF j &gt; 1.
Reference: [27] <author> H. L. Royden. </author> <title> Real Analysis. </title> <publisher> Macmillan, </publisher> <year> 1963. </year>
Reference-contexts: Lemma 1 (see <ref> [27] </ref>) Choose countable sets Z 1 and Z 2 , a function f : Z 1 fi Z 2 ! [0; 1] and probability distributions D 1 over Z 1 and D 2 over Z 2 .
Reference: [28] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> J. Combinatorial Theory (A), </journal> <volume> 13:145147, </volume> <year> 1972. </year>
Reference-contexts: U is the uniform distribution over f1; 1g m , U ~ : 9g 2 G; fi fi m X i g i fi fi &gt; m 2jGje 2 m=2 By combining Lemmas 3, 4, 5, 6, and 7, and applying a bound on jGj in terms of VCdim (G) <ref> [28, 29, 33] </ref> in Lemma 7, one gets a bound of O 1 VCdim (F ) log 1 + log ffi on the sample complexity of agnostically learning F [11]. Our argument will take advantage of the following refinement of Lemma 7, which also follows directly from Lemma 2.
Reference: [29] <author> S. Shelah. </author> <title> A combinatorial problem; stability and order for models and theories in infinitary languages. </title> <journal> Pacific J. Math., </journal> <volume> 41:247261, </volume> <year> 1972. </year>
Reference-contexts: U is the uniform distribution over f1; 1g m , U ~ : 9g 2 G; fi fi m X i g i fi fi &gt; m 2jGje 2 m=2 By combining Lemmas 3, 4, 5, 6, and 7, and applying a bound on jGj in terms of VCdim (G) <ref> [28, 29, 33] </ref> in Lemma 7, one gets a bound of O 1 VCdim (F ) log 1 + log ffi on the sample complexity of agnostically learning F [11]. Our argument will take advantage of the following refinement of Lemma 7, which also follows directly from Lemma 2.
Reference: [30] <author> H. U. Simon. </author> <title> General lower bounds on the number of examples needed for learning probabilistic concepts. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52(2):239 254, </volume> <year> 1996. </year>
Reference-contexts: This bound, which also follows from previous work of Talagrand [31], improves on the bound of O 1 VCdim (F ) log 1 * + log 1 that follows from Vap-nik and Chervonenkis' results (see [11]), and matches Si-mon's general lower bound <ref> [30] </ref> to within a constant factor for each concept class F . Our constants are greater than Ta lagrand's, but our proof is simpler and more elementary. 2 Preliminaries Fix a countable set X. Denote the reals by R, and the natural numbers by N.
Reference: [31] <author> M. Talagrand. </author> <title> Sharper bounds for Gaussian and empirical processes. </title> <journal> Annals of Probability, </journal> <volume> 22:2876, </volume> <year> 1994. </year>
Reference-contexts: This bound, which also follows from previous work of Talagrand <ref> [31] </ref>, improves on the bound of O 1 VCdim (F ) log 1 * + log 1 that follows from Vap-nik and Chervonenkis' results (see [11]), and matches Si-mon's general lower bound [30] to within a constant factor for each concept class F .
Reference: [32] <author> X. Tian and A. Kuh. </author> <title> Performance bounds for single layer threshold networks when tracking a drifting adversary. Neural Networks, </title> <address> 10(5):897906, </address> <year> 1997. </year>
Reference-contexts: Models of a changing environment that are more dissimilar to that studied here were considered in <ref> [22, 20, 21, 7, 10, 15, 2, 19, 32, 16] </ref>. must output a hypothesis h t using only the first t 1 examples.
Reference: [33] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, </title> <address> 16(2):264280, </address> <year> 1971. </year>
Reference-contexts: said to (*; ffi)-agnostically learn F from m examples if for all distributions P on X fi f0; 1g, P m ~z : er P (A (~z)) &gt; * + inf er P (f ) ffi: To set the context, we briefly review the work that our anal-ysis builds on <ref> [33, 24, 8, 11] </ref>. For each f 2 F , define L f : X fi f0; 1g ! f0; 1g by L f (x; y) = jf (x) yj. Define L F = fL f : f 2 F g. <p> U is the uniform distribution over f1; 1g m , U ~ : 9g 2 G; fi fi m X i g i fi fi &gt; m 2jGje 2 m=2 By combining Lemmas 3, 4, 5, 6, and 7, and applying a bound on jGj in terms of VCdim (G) <ref> [28, 29, 33] </ref> in Lemma 7, one gets a bound of O 1 VCdim (F ) log 1 + log ffi on the sample complexity of agnostically learning F [11]. Our argument will take advantage of the following refinement of Lemma 7, which also follows directly from Lemma 2.
References-found: 33

