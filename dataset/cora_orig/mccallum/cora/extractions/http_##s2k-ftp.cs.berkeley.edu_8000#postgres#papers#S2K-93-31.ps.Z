URL: http://s2k-ftp.cs.berkeley.edu:8000/postgres/papers/S2K-93-31.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/mariposa/papers.html
Root-URL: 
Title: Mariposa: A New Architecture for Distributed Data  
Author: Michael Stonebraker, Paul M. Aoki, Robert Devine, Witold Litwin and Michael Olson 
Address: Berkeley, California 94720  
Affiliation: Computer Science Div., Dept. of EECS University of California  
Abstract: We describe the design of Mariposa, an experimental distributed data management system that provides high performance in an environment of high data mobility and heterogeneous host capabilities. The Mariposa design unifies the approaches taken by distributed file systems and distributed databases. In addition, Mariposa provides a general, exible platform for the development of new algorithms for distributed query optimization, storage management, and scalable data storage structures. This exibility is primarily due to a unique rule-based design that permits autonomous, local-knowledge decisions to be made regarding data placement, query execution location, and storage management. 
Abstract-found: 1
Intro-found: 1
Reference: [BERN81] <author> Bernstein, P. A., Goodman, N., Wong, E., Reeve, C . L. and Rothnie, J. </author> <title> Query Processing in a System for Distributed Databases (SDD-1), </title> <journal> ACM Trans. Database Sys., </journal> <volume> 6(4), </volume> <month> Dec. </month> <year> 1981. </year>
Reference-contexts: We first consider the approach taken by distributed database management systems. Several distributed DBMS prototypes were developed during the early 1980s, such as R* [WILL81], SDD-1 <ref> [BERN81] </ref>, SIRIUS-DELTA [LITW82], and Distributed INGRES [STON86a]. All extended single-site DBMSs to manage relations that were spread over the sites in a computer network in a seamless manner. <p> All extended single-site DBMSs to manage relations that were spread over the sites in a computer network in a seamless manner. In order to do so, researchers developed techniques for handling distributed query optimization <ref> [EPST78, SELI80, BERN81] </ref>, distributed transactions [SKEE81, LIND83, BERN81], and (sometimes) multiple copies of data [STON79]. Commercial systems based on these techniques are now available from several relational DBMS vendors. Architecturally, the distributed database approach looks quite different from the other approaches. <p> All extended single-site DBMSs to manage relations that were spread over the sites in a computer network in a seamless manner. In order to do so, researchers developed techniques for handling distributed query optimization [EPST78, SELI80, BERN81], distributed transactions <ref> [SKEE81, LIND83, BERN81] </ref>, and (sometimes) multiple copies of data [STON79]. Commercial systems based on these techniques are now available from several relational DBMS vendors. Architecturally, the distributed database approach looks quite different from the other approaches.
Reference: [EPOC92] <institution> Epoch Systems, </institution> <note> EpochServ Software Release Notes (Rel. 5.1), Doc. 64-001614 Rev 01, </note> <institution> Epoch Systems, Inc., </institution> <address> Westborough, MA, </address> <month> Sep. </month> <year> 1992. </year>
Reference-contexts: Most file systems implement a hard-coded cache manager, t ypically utilizing a least-recently-used (LRU) eviction strategy. A third approach to distribution is that of tertiary memory, or deep store, file systems. Systems in this category include Inversion [OLSO93], Highlight [KOHL92], and commercial offerings such as Epoch <ref> [EPOC92] </ref> and UniTree [GENE91]. Such systems typically offer a seamless caching service for a tertiary memory system. Some products require that the disk cache and the deep store be on the same physical machine. However, most allow t he tertiary memory device to be remote from the disk cache.
Reference: [EPST78] <author> Epstein, R. S., Stonebraker, M. and Wong, E., </author> <title> Distributed Query Processing in Relational Database Systems, </title> <booktitle> Proc. 1978 ACM-SIGMOD Conf. on Management of Data, </booktitle> <address> Austin, TX, </address> <month> May </month> <year> 1978. </year>
Reference-contexts: All extended single-site DBMSs to manage relations that were spread over the sites in a computer network in a seamless manner. In order to do so, researchers developed techniques for handling distributed query optimization <ref> [EPST78, SELI80, BERN81] </ref>, distributed transactions [SKEE81, LIND83, BERN81], and (sometimes) multiple copies of data [STON79]. Commercial systems based on these techniques are now available from several relational DBMS vendors. Architecturally, the distributed database approach looks quite different from the other approaches.
Reference: [GANG92] <author> Ganguly, S., Hasan, W. and Krishnamurthy, R., </author> <title> Query Optimization for Parallel Execution, </title> <booktitle> Proc. 1992 ACM-SIGMOD Conf. on Management of Data, </booktitle> <address> San Diego, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Recent research demonstrates that bushy query plan trees permit substantial gains in performance in both serial (e.g, [IOAN91]) and parallel (e.g., <ref> [GANG92] </ref>) database systems. We intend to experiment with a number of different optimization strategies (tree topology families) to determine which method provides the the best results in the Mariposa environment. (2) Parallelization: This phase takes a binary query operator tree as input and produces a non-binary query plan tree.
Reference: [GENE91] <author> General Atomics (DISCOS Division), </author> <title> The UniTree Virtual Disk System: An Overview, </title> <type> DISCOS Technical Report, </type> <institution> General Atomics Corp., </institution> <address> San Diego, CA, </address> <year> 1991. </year>
Reference-contexts: Most file systems implement a hard-coded cache manager, t ypically utilizing a least-recently-used (LRU) eviction strategy. A third approach to distribution is that of tertiary memory, or deep store, file systems. Systems in this category include Inversion [OLSO93], Highlight [KOHL92], and commercial offerings such as Epoch [EPOC92] and UniTree <ref> [GENE91] </ref>. Such systems typically offer a seamless caching service for a tertiary memory system. Some products require that the disk cache and the deep store be on the same physical machine. However, most allow t he tertiary memory device to be remote from the disk cache.
Reference: [HONG91] <author> Hong, W. and Stonebraker, </author> <title> M ., Optimization of Parallel Query Execution Plans in XPRS, </title> <booktitle> Proc. 1st Int. Conf. on Parallel and Distributed Info. Sys., </booktitle> <address> Miami Beach, FL, </address> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: Other strategies require an arbitrarily large amount of current global knowledge regarding fragment composition and location. This use of standard optimizer technology as a basis for subsequent refinement is similar to that proposed in <ref> [HONG91] </ref>. In the discussion that follows, the word site generally indicates the Mariposa query processing engine running on a particular computer. The words query plan and query tree are used interchangably.
Reference: [HOWA88] <author> Howard, J. H., Kazar, M. L. Menees, S. G., Nichols, D. A., Satyanarayanan, M., Sidebotham, R. N. and West, M. J., </author> <title> Scale and Performance in a Distributed File System, </title> <journal> ACM Trans. Comp. Sys., </journal> <volume> 6(1), </volume> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: To the best of our knowledge, no system actually implemented fragments; however, in theory, it is the unit of storage allocation. A second approach to distribution is that taken by client-server file systems such as Andrew <ref> [HOWA88] </ref> and the NFS-oriented commercial offerings. In such systems, a file (or collection of files) is the unit of storage allocation and has a unique home on some server.
Reference: [IOAN91] <author> Ioannidis, Y. E. a nd Kang, Y. C., </author> <title> Left-Deep vs. Bushy Trees: An Analysis of Strategy Spaces and its Implications for Query Optimization, </title> <booktitle> Proc. 1991 ACM-SIGMOD Conf. on Management of Data, </booktitle> <address> Denver, CO, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Note that this first phase can consist of any optimization method proposed for single-site database management systems that produces (or can be modified to produce) a binary query operator tree. Recent research demonstrates that bushy query plan trees permit substantial gains in performance in both serial (e.g, <ref> [IOAN91] </ref>) and parallel (e.g., [GANG92]) database systems.
Reference: [KOHL92] <author> Kohl, J., Staelin, C. and Stonebraker, M., Highlight: </author> <title> Using a Log-structured File System for Tertiary Storage Management, </title> <type> Sequoia 2000 Technical Report 92/16, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Most file systems implement a hard-coded cache manager, t ypically utilizing a least-recently-used (LRU) eviction strategy. A third approach to distribution is that of tertiary memory, or deep store, file systems. Systems in this category include Inversion [OLSO93], Highlight <ref> [KOHL92] </ref>, and commercial offerings such as Epoch [EPOC92] and UniTree [GENE91]. Such systems typically offer a seamless caching service for a tertiary memory system. Some products require that the disk cache and the deep store be on the same physical machine.
Reference: [LIND83] <author> Lindsay, B. G., Haas, L. M., Mohan, C., Wilms, P. F. and Yost, R. A., </author> <title> Computation and Communication in R*: A Distributed Database Manager, </title> <journal> ACM Trans. Comp. Sys. </journal> <volume> 2(1), </volume> <month> Feb. </month> <year> 1984. </year> <month> 16 </month>
Reference-contexts: All extended single-site DBMSs to manage relations that were spread over the sites in a computer network in a seamless manner. In order to do so, researchers developed techniques for handling distributed query optimization [EPST78, SELI80, BERN81], distributed transactions <ref> [SKEE81, LIND83, BERN81] </ref>, and (sometimes) multiple copies of data [STON79]. Commercial systems based on these techniques are now available from several relational DBMS vendors. Architecturally, the distributed database approach looks quite different from the other approaches.
Reference: [LITW82] <author> Litwin, W. et al., </author> <title> SIRIUS System for Distributed Data Management, in Distributed Databases, </title> <editor> H. J. Schneider (ed.), </editor> <publisher> North-Holland, </publisher> <year> 1982. </year>
Reference-contexts: We first consider the approach taken by distributed database management systems. Several distributed DBMS prototypes were developed during the early 1980s, such as R* [WILL81], SDD-1 [BERN81], SIRIUS-DELTA <ref> [LITW82] </ref>, and Distributed INGRES [STON86a]. All extended single-site DBMSs to manage relations that were spread over the sites in a computer network in a seamless manner.
Reference: [MACK86] <author> Mackert, L. F. and Lohman, G. M., </author> <title> R* Optimizer Validation and Performance Evaluation for Local Queries, </title> <booktitle> Proc. 1986 ACM-SIGMOD Conf. on Management of Data, </booktitle> <address> Washington, DC, </address> <month> May </month> <year> 1986. </year>
Reference-contexts: Traditional cost-based query optimizers, including nearly all optimizers found in commercial products, have been based on resource-usage models and exponential-complexity dynamic-programming search algorithms similar to those developed in System R [SELI79]. Extension of these traditional optimizers to handle distributed database systems, as in R* <ref> [SELI80, MACK86] </ref>, is straightforward and produces optimal plans. However, this approach has two key disadvantages. First, the exponential complexity of the search space makes the use of such optimizers impractical in very large distributed systems. <p> Although the initial use of single-site optimization may often generate suboptimal plans, as pointed out in <ref> [MACK86] </ref>, we believe that other strategies are impractical in our environment. Other strategies require an arbitrarily large amount of current global knowledge regarding fragment composition and location. This use of standard optimizer technology as a basis for subsequent refinement is similar to that proposed in [HONG91]. <p> Mariposa assumes that all network connections have equivalent bandwidth and latency. This is not to say that all hosts must be on the same local area network or a set of completely homogeneous networks, but rather that the networks are all fast in the sense discussed in <ref> [MACK86] </ref>, such that network bandwidth does not overwhelmingly dominate query processing costs. If we can make the simplifying assumption that hosts are essentially equidistant, then importing a fragment from a given remote host costs the same as importing the fragment from any other remote host.
Reference: [OLSO93] <author> Olson, M., </author> <title> The Design and Implementation of the Inversion File System, </title> <booktitle> Proc. Winter 1993 USENIX Conf., </booktitle> <address> San Diego, CA, </address> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Most file systems implement a hard-coded cache manager, t ypically utilizing a least-recently-used (LRU) eviction strategy. A third approach to distribution is that of tertiary memory, or deep store, file systems. Systems in this category include Inversion <ref> [OLSO93] </ref>, Highlight [KOHL92], and commercial offerings such as Epoch [EPOC92] and UniTree [GENE91]. Such systems typically offer a seamless caching service for a tertiary memory system. Some products require that the disk cache and the deep store be on the same physical machine.
Reference: [OUST90] <author> Ousterhout, J. K., </author> <title> Tcl: An Embeddable Command Language, </title> <booktitle> Proc. Winter 1990 USENIX Conf., </booktitle> <address> Washington, DC, </address> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: Mariposa implements a collection of built-in types of events to support query processing, storage allocation and name service. The built-in events are described below. The action portion of a rule is written in a general purpose language such as Tcl <ref> [OUST90] </ref>, a scripting language with escapes to C subroutines. In addition, there is a collection of built-in primitive actions. In this section, we describe the events and actions that Mariposa requires as built-ins; additional user-defined actions and events can be defined for each site. 3.1.
Reference: [SELI79] <author> Selinger, P. G., Astrahan, M. M., Chamberlin, D. D., Lorie, R. A. and Price, T. G., </author> <title> Access Path Selection in a Relational Database Management System, </title> <booktitle> Proc. 1979 ACM-SIGMOD Conf. on Management of Data, </booktitle> <address> Boston, MA, </address> <month> June </month> <year> 1979. </year>
Reference-contexts: Traditional cost-based query optimizers, including nearly all optimizers found in commercial products, have been based on resource-usage models and exponential-complexity dynamic-programming search algorithms similar to those developed in System R <ref> [SELI79] </ref>. Extension of these traditional optimizers to handle distributed database systems, as in R* [SELI80, MACK86], is straightforward and produces optimal plans. However, this approach has two key disadvantages. First, the exponential complexity of the search space makes the use of such optimizers impractical in very large distributed systems.
Reference: [SELI80] <author> Selinger, P. G., and Adiba, M. E., </author> <title> Access Path Selection in Distributed Data Base Management Systems, </title> <booktitle> Proc. 1980 Int. Conf. on Data Bases, </booktitle> <address> Aberdeen, Scotland, </address> <month> July </month> <year> 1980. </year>
Reference-contexts: All extended single-site DBMSs to manage relations that were spread over the sites in a computer network in a seamless manner. In order to do so, researchers developed techniques for handling distributed query optimization <ref> [EPST78, SELI80, BERN81] </ref>, distributed transactions [SKEE81, LIND83, BERN81], and (sometimes) multiple copies of data [STON79]. Commercial systems based on these techniques are now available from several relational DBMS vendors. Architecturally, the distributed database approach looks quite different from the other approaches. <p> Traditional cost-based query optimizers, including nearly all optimizers found in commercial products, have been based on resource-usage models and exponential-complexity dynamic-programming search algorithms similar to those developed in System R [SELI79]. Extension of these traditional optimizers to handle distributed database systems, as in R* <ref> [SELI80, MACK86] </ref>, is straightforward and produces optimal plans. However, this approach has two key disadvantages. First, the exponential complexity of the search space makes the use of such optimizers impractical in very large distributed systems.
Reference: [SKEE81] <author> Skeen, D., </author> <title> Crash Recovery in a Distributed Database System, </title> <type> Ph.D. thesis, </type> <institution> Dept. of EECS, Univ. of California, Berkeley, </institution> <address> CA, </address> <year> 1981. </year>
Reference-contexts: All extended single-site DBMSs to manage relations that were spread over the sites in a computer network in a seamless manner. In order to do so, researchers developed techniques for handling distributed query optimization [EPST78, SELI80, BERN81], distributed transactions <ref> [SKEE81, LIND83, BERN81] </ref>, and (sometimes) multiple copies of data [STON79]. Commercial systems based on these techniques are now available from several relational DBMS vendors. Architecturally, the distributed database approach looks quite different from the other approaches.
Reference: [STON79] <author> Stonebraker, M., </author> <title> Concurrency Control and Consistency of Multiple Copies of Data in Distributed INGRES, </title> <journal> IEEE Trans. Software Eng. </journal> <volume> TSE-5(3), </volume> <month> May </month> <year> 1979. </year>
Reference-contexts: All extended single-site DBMSs to manage relations that were spread over the sites in a computer network in a seamless manner. In order to do so, researchers developed techniques for handling distributed query optimization [EPST78, SELI80, BERN81], distributed transactions [SKEE81, LIND83, BERN81], and (sometimes) multiple copies of data <ref> [STON79] </ref>. Commercial systems based on these techniques are now available from several relational DBMS vendors. Architecturally, the distributed database approach looks quite different from the other approaches. Each distributed DBMS assumes a shared nothing architecture [STON86b] and has the basic processing philosophy of move the query to the data. <p> Specifically, the update must be sent to each site with a copy, and each can either accept the update and modify its copy or discard its copy. 15 It can be noted that this scheme is basically a primary copy algorithm <ref> [STON79] </ref> extended to allow the num-ber of copies to be (perhaps rapidly) varying. Again, the meta data must include the location of each copy, and it may be arbitrarily out of date.
Reference: [STON86a] <author> Stonebraker, M., </author> <title> The Design and Implementation of Distributed INGRES, in The INGRES Papers, </title> <editor> M. Stonebraker (ed.), </editor> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: We first consider the approach taken by distributed database management systems. Several distributed DBMS prototypes were developed during the early 1980s, such as R* [WILL81], SDD-1 [BERN81], SIRIUS-DELTA [LITW82], and Distributed INGRES <ref> [STON86a] </ref>. All extended single-site DBMSs to manage relations that were spread over the sites in a computer network in a seamless manner. In order to do so, researchers developed techniques for handling distributed query optimization [EPST78, SELI80, BERN81], distributed transactions [SKEE81, LIND83, BERN81], and (sometimes) multiple copies of data [STON79].
Reference: [STON86b] <author> Stonebraker, M., </author> <title> The Case for Shared Nothing, </title> <booktitle> IEEE Database Engineering 9(1), </booktitle> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: Commercial systems based on these techniques are now available from several relational DBMS vendors. Architecturally, the distributed database approach looks quite different from the other approaches. Each distributed DBMS assumes a shared nothing architecture <ref> [STON86b] </ref> and has the basic processing philosophy of move the query to the data. As a result, these systems allocate data to sites in a computer network, and this allocation remains in place until it is subsequently changed by a database administrator.
Reference: [STON86c] <author> Stonebraker, M. R. and Rowe, L. A., </author> <title> The Design of POSTGRES, </title> <booktitle> Proc. 1986 ACM-SIGMOD Conf. on Management of Data, </booktitle> <address> Washington, DC, </address> <month> Jun. </month> <year> 1986. </year>
Reference-contexts: Distributed query optimization has been identified as an area that will receive a strong emphasis and we will also examine how to build a system that has a rule system at its core. We hav e begun implementation of Mariposa as an extension of the POSTGRES next-generation DBMS <ref> [STON86c] </ref> and expect to be able to present results from a working prototype in approximately one year. Future work remains in the areas of system robustness, distributed failure recovery, and performance assessment. The analysis of these important large system issues will be addressed during the construction of Mariposa.
Reference: [WILL81] <author> Williams, R., et al., </author> <title> R*: An Overview of the Architecture, </title> <institution> IBM Research Report RJ3325, IBM Research Laboratory, </institution> <address> San Jose, CA, </address> <month> Dec. </month> <year> 1981. </year> <month> 17 </month>
Reference-contexts: We first consider the approach taken by distributed database management systems. Several distributed DBMS prototypes were developed during the early 1980s, such as R* <ref> [WILL81] </ref>, SDD-1 [BERN81], SIRIUS-DELTA [LITW82], and Distributed INGRES [STON86a]. All extended single-site DBMSs to manage relations that were spread over the sites in a computer network in a seamless manner.
References-found: 22

