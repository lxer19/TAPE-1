URL: http://robotics.stanford.edu/~ronnyk/mineset.ps.gz
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: fbrunk,jkelly,ronnykg@engr.sgi.com  
Title: MineSet: An Integrated System for Data Mining  
Author: Cliff Brunk James Kelly Ron Kohavi 
Address: 2011 N. Shoreline Blvd Mountain View, CA 94043-1389  
Affiliation: Data Mining and Visualization Silicon Graphics, Inc.  
Note: Appears in KDD-97  
Abstract: MineSet TM , Silicon Graphics' interactive system for data mining, integrates three powerful technologies: database access, analytical data mining, and data visualization. It supports the knowledge discovery process from data access and preparation through iterative analysis and visualization to deployment. Mine-Set is based on a client-server architecture that scales to large databases. The database access component provides a rich set of operators that can be used to preprocess and transform the stored data into forms appropriate for visualization and analytical mining. The 3D visualization capabilities allow direct data visualization for exploratory analysis, including tools for displaying high-dimensional data containing geographical and hierarchical information. The analytical mining algorithms help identify potentially interesting models of the data, which can be viewed using visualization tools specialized for the learned models. Third party vendors can interface to the MineSet tools for model deployment and for integration with other packages. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Domingos, P., and Pazzani, M. </author> <year> 1996. </year> <title> Beyond independence: conditions for the optimality of the simple Bayesian classifier. </title> <editor> In Saitta, L., ed., </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> 105-112. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Duda, R., and Hart, P. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley. </publisher>
Reference: <author> Fayyad, U. M.; Piatetsky-Shapiro, G.; and Smyth, P. </author> <year> 1996. </year> <title> From data mining to knowledge discovery: An overview. In Advances in Knowledge Discovery and Data Mining. </title> <publisher> AAAI Press and the MIT Press. </publisher> <pages> chapter 1, 1-34. </pages>
Reference-contexts: The disks represent the probability of the right side alone. The colors represent the frequency of both the left hand side and the right hand side occuring together. Managing the KDD Process In addition to visualization and analytical mining, Mi-neSet supports the broader knowledge discovery process <ref> (Fayyad, Piatetsky-Shapiro, & Smyth 1996) </ref>. This section focuses on the role that Tool Manager and Data Mover play in helping execute and manage this process. Tool Manager provides a consistent graphical interface to all of the tools.
Reference: <author> Kohavi, R., and John, G. H. </author> <title> (to appear). Wrappers for feature subset selection. </title> <journal> Artificial Intelligence. </journal>
Reference: <author> Kohavi, R., and Kunz, C. </author> <year> 1997. </year> <title> Option decision trees with majority votes. </title> <editor> In Fisher, D., ed., </editor> <booktitle> Machine Learning: Proceedings of the Fourteenth International Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: The classifiers include decision trees similar to C4.5 (Quinlan 1993), Simple/Naive-Bayes (Duda & Hart 1973; Domingos & Pazzani 1996) with wrapper-style feature selection (Kohavi & John to appear), and option decision trees <ref> (Kohavi & Kunz 1997) </ref>. Holdout and cross-validation can be used to estimate the future prediction accuracy of classifiers. Mi-neSet provides reasonable default settings for all mining algorithms making them easy to use, while allowing experienced users to optimize the standard defaults.
Reference: <author> Kohavi, R., and Li, C.-H. </author> <year> 1995. </year> <title> Oblivious decision trees, graphs, and top-down pruning. </title> <editor> In Mel-lish, C. S., ed., </editor> <booktitle> Proceedings of the 14th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1071-1077. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Note how bin ranges are unequal, with four small intervals between the ages 20 and 30, and bigger ranges above 30. Automatic attribute selection aids users in finding the relevant attributes for their task. MineSet identifies the important attributes using conditional entropy minimization based on oblivious decision trees <ref> (Kohavi & Li 1995) </ref>. Thus, when a user needs to select the axes for a scat-terplot in order to understand a given factor, he or she can ask the system to recommend some attributes that are important for explaining that factor.
Reference: <author> Kohavi, R.; Sommerfield, D.; and Dougherty, J. </author> <year> 1996. </year> <title> Data mining using MLC ++ : A machine learning library in C ++ . In Tools with Artificial Intelligence, </title> <address> 234-245. </address> <publisher> IEEE Computer Society Press. </publisher> <address> http://www.sgi.com/Technology/mlc. </address>
Reference-contexts: Holdout and cross-validation can be used to estimate the future prediction accuracy of classifiers. Mi-neSet provides reasonable default settings for all mining algorithms making them easy to use, while allowing experienced users to optimize the standard defaults. MineSet's analytical mining components are based on MLC ++ <ref> (Kohavi, Sommerfield, & Dougherty 1996) </ref>, the Maching Learning library in C ++ . MLC ++ source code is freely available for research purposes, providing openness and promoting data mining research.
Reference: <author> Mitchell, T. M. </author> <year> 1997. </year> <title> Machine Learning. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: The next section describes the analytical mining tools, and the subsequent section describes the specialized visual tools that help users understand the results of those algorithms. Analytical Data Mining Engines MineSet provides a suite of analytical mining tools based on proven algorithms, such as classifier inducers <ref> (Mitchell 1997) </ref> and association generators (Srikand & Agrawal 1995). The classifiers include decision trees similar to C4.5 (Quinlan 1993), Simple/Naive-Bayes (Duda & Hart 1973; Domingos & Pazzani 1996) with wrapper-style feature selection (Kohavi & John to appear), and option decision trees (Kohavi & Kunz 1997).
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Analytical Data Mining Engines MineSet provides a suite of analytical mining tools based on proven algorithms, such as classifier inducers (Mitchell 1997) and association generators (Srikand & Agrawal 1995). The classifiers include decision trees similar to C4.5 <ref> (Quinlan 1993) </ref>, Simple/Naive-Bayes (Duda & Hart 1973; Domingos & Pazzani 1996) with wrapper-style feature selection (Kohavi & John to appear), and option decision trees (Kohavi & Kunz 1997). Holdout and cross-validation can be used to estimate the future prediction accuracy of classifiers.
Reference: <author> Srikand, R., and Agrawal, R. </author> <year> 1995. </year> <title> Mining generalized association rules. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Databases. </booktitle>
Reference-contexts: Analytical Data Mining Engines MineSet provides a suite of analytical mining tools based on proven algorithms, such as classifier inducers (Mitchell 1997) and association generators <ref> (Srikand & Agrawal 1995) </ref>. The classifiers include decision trees similar to C4.5 (Quinlan 1993), Simple/Naive-Bayes (Duda & Hart 1973; Domingos & Pazzani 1996) with wrapper-style feature selection (Kohavi & John to appear), and option decision trees (Kohavi & Kunz 1997).
References-found: 10

