URL: ftp://theory.cs.uni-bonn.de/pub/reports/cs-reports/1991/8561-cs.ps.gz
Refering-URL: http://cs.uni-bonn.de/info5/publications/CS-1991-en.html
Root-URL: http://cs.uni-bonn.de
Title: Some Computational Problems in Linear Algebra as Hard as Matrix Multiplication  
Author: Peter Burgisser Marek Karpinski Thomas Lickteig 
Note: Supported in part by the Leibniz Center for Research in Computer Science, by the DFG Grant KA 673/2-1 and by the SERC Grant GR-E 68297  
Address: Berkeley, California  Berkeley, California  Berkeley, California  
Affiliation: International Computer Science Institute  Deptartment of Computer Science University of Bonn and International Computer Science Institute  International Computer Science Institute  
Abstract: We define the complexity of a computational problem given by a relation using the model of a computation tree with Ostrowski complexity measure. To a sequence of problems we assign an exponent similar as for matrix multiplication. For the complexity of the following computational problems in linear algebra * KER n : Compute a basis of the kernel for a given n fi n-matrix. * OGB n : Find an invertible matrix that transforms a given symmetric n fi n matrix to diagonal form. * SP R n : Find a sparse representation of a given n fi n-matrix. we prove relative lower bounds of the form aM n b and absolute lower bounds dn 2 , where M n denotes the complexity of matrix multiplication and a; b; d are suitably chosen constants. We show that the exponent of the problem sequences KER; OGB; SP R is the same as the exponent ! of matrix multiplication. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.V. Aho and J.E. Hopcroft and J.D. Ullman, </author> <title> The design and analysis of computer algorithms, </title> <address> Reading MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1974. </year>
Reference-contexts: Theorem 3 The exponent for any of the sequences of problems tCP R; KER; OGB; SP R; SP T M is less or equal the exponent ! of matrix multiplication. The proof is based on ideas from [4, 11, 12]. See also <ref> [1, pages 233-240] </ref>. The proceeding is to subdivide the occuring matrices into blocks, to perform a sort of Gaussian Elimination blockwise using a fast hypothetical matrix multiplication algorithm, and then to continue recursively. We leave the details to the reader.
Reference: [2] <author> A. Alder and V. Strassen, </author> <title> On the algorithmic complexity of associative algebras, </title> <booktitle> Theor. Computer Science 15(1981), </booktitle> <pages> 201-211. </pages>
Reference: [3] <author> W. Baur and V. Strassen, </author> <title> The complexity of partial derivatives, </title> <booktitle> Theor. Computer Science 22(1982), </booktitle> <pages> 317-330. </pages>
Reference-contexts: Problems like matrix inversion, computation of the determinant or of all coefficients of the characteristic polynomial, LR-decomposition and over the complex numbers also QR-decomposition and unitary transformation to Hessenberg form are all known to be as hard as matrix multiplication.(See <ref> [3, 4, 6, 7, 11, 12, 15] </ref>.) In this paper we study some computational problems in linear algebra that are not specified by a function but by a relation. Let F denote a field of characteristic zero that may be endowed with an ordering . <p> The main goal of this paper is to prove lower bounds on the complexity of the problems cited above in terms of the complexity of matrix multiplication. The proofs rest on ideas from [15] and the important Derivation Theorem (see <ref> [3] </ref> ). In the last section we employ the notion of dimension for an affine variety. <p> : : : ; x m ]; A 2 F mfim ; and put Then L F [x] (g 1 ; : : : ; g n ) L F [x] (f 1 ; : : : ; f n ): Finally we cite the important Theorem 2 ("Derivation Theorem" <ref> [3] </ref>) Let f 2 F (x 1 ; : : : ; x m ) be a rational func tion.
Reference: [4] <author> J. Bunch and J. Hopcroft, </author> <title> Triangular factorizationand inversion by fast matrix multiplication, </title> <journal> Math. Comp. </journal> <volume> 28(1974), </volume> <pages> 231-236. </pages>
Reference-contexts: Problems like matrix inversion, computation of the determinant or of all coefficients of the characteristic polynomial, LR-decomposition and over the complex numbers also QR-decomposition and unitary transformation to Hessenberg form are all known to be as hard as matrix multiplication.(See <ref> [3, 4, 6, 7, 11, 12, 15] </ref>.) In this paper we study some computational problems in linear algebra that are not specified by a function but by a relation. Let F denote a field of characteristic zero that may be endowed with an ordering . <p> Theorem 3 The exponent for any of the sequences of problems tCP R; KER; OGB; SP R; SP T M is less or equal the exponent ! of matrix multiplication. The proof is based on ideas from <ref> [4, 11, 12] </ref>. See also [1, pages 233-240]. The proceeding is to subdivide the occuring matrices into blocks, to perform a sort of Gaussian Elimination blockwise using a fast hypothetical matrix multiplication algorithm, and then to continue recursively. We leave the details to the reader.
Reference: [5] <author> D. Coppersmith and S. Winograd, </author> <title> Matrix multiplication via arithmetic progressions, </title> <booktitle> Proc. 19th ACM STOC, </booktitle> <address> New York(1987) , 1-6. </address>
Reference: [6] <author> K. Kalorkoti, </author> <title> The trace invariant and matrix inversion, </title> <booktitle> Theor. Computer Science 59(1988), </booktitle> <pages> 277-286. </pages>
Reference-contexts: Problems like matrix inversion, computation of the determinant or of all coefficients of the characteristic polynomial, LR-decomposition and over the complex numbers also QR-decomposition and unitary transformation to Hessenberg form are all known to be as hard as matrix multiplication.(See <ref> [3, 4, 6, 7, 11, 12, 15] </ref>.) In this paper we study some computational problems in linear algebra that are not specified by a function but by a relation. Let F denote a field of characteristic zero that may be endowed with an ordering . <p> We have T r (A 1 ) = T r (S T (D 1 S)): Therefore L K (T r (A 1 ) L K (S) + 4n 2 + n: We proceed now similar as in <ref> [6] </ref>. Let V 2 F nfin be symmetric and * be an indeterminate over K.
Reference: [7] <author> W. Keller-Gehrig, </author> <title> Fast algorithms for the characteristic polynomial, </title> <booktitle> Theor. Computer Science 36(1985), </booktitle> <pages> 309-317. </pages>
Reference-contexts: Problems like matrix inversion, computation of the determinant or of all coefficients of the characteristic polynomial, LR-decomposition and over the complex numbers also QR-decomposition and unitary transformation to Hessenberg form are all known to be as hard as matrix multiplication.(See <ref> [3, 4, 6, 7, 11, 12, 15] </ref>.) In this paper we study some computational problems in linear algebra that are not specified by a function but by a relation. Let F denote a field of characteristic zero that may be endowed with an ordering .
Reference: [8] <author> H. Kraft, </author> <title> Geometric methods in representation theory, in: Representations of Algebras, </title> <booktitle> Workshop Proc., Puebla, </booktitle> <address> Mexico 1980, LNM 944, Berlin-Heidelberg-New York 1982. </address>
Reference-contexts: determined, namely d 0 (n 1 ; : : : ; n t ) = n 2 X (n 0 where (n 0 1 ; : : : ; n 0 t 0 ) denotes the partition dual to (n 1 ; : : : ; n t ). (See <ref> [8, page 192] </ref>.) Using this fact we conclude dim ( n ) = maxfn 2 (a + b + c) 2 (a + b) 2 a 2 : n = 3a + 2b + c; a; b; c 2 Ng 2n 2 =3; 16 with equality when n is a multiple
Reference: [9] <author> J.C. Lafon and W. Winograd, </author> <title> A lower bound for the multiplicative complexity of the product of two matrices, </title> <type> (unpublished) manuscript, </type> <year> 1978. </year>
Reference: [10] <author> T. Lickteig, </author> <title> On semialgebraic decision complexity, </title> <journal> Habilitationsschrift, </journal> <note> to appear. </note>
Reference-contexts: We define the cost of a computation tree T as the maximal number of multiplications and divisions that may be performed in a computation of the tree T .(Compare <ref> [10, 13, 14, 16] </ref>.) The complexity C (f ) of a function is then defined as the minimal cost of a tree computing f, and finally we put C (P ) := minfC (f ) : f function solving P g for the complexity of the problem P . <p> Let us shortly describe this notion, a detailed discussion can be found in <ref> [10, 13, 14, 16] </ref>. As the set of operational symbols and the set R of relational symbols (together with arity functions) we take = F t f0; 1; +; ; fl; =g R = f=g (or R = fg when we consider an ordered field (F; )).
Reference: [11] <author> A. </author> <title> Schonhage, </title> <journal> Unitare Transformationen grosser Matrizen, Num. Math. </journal> <volume> 20(1973), </volume> <pages> 409-417. </pages>
Reference-contexts: Problems like matrix inversion, computation of the determinant or of all coefficients of the characteristic polynomial, LR-decomposition and over the complex numbers also QR-decomposition and unitary transformation to Hessenberg form are all known to be as hard as matrix multiplication.(See <ref> [3, 4, 6, 7, 11, 12, 15] </ref>.) In this paper we study some computational problems in linear algebra that are not specified by a function but by a relation. Let F denote a field of characteristic zero that may be endowed with an ordering . <p> Theorem 3 The exponent for any of the sequences of problems tCP R; KER; OGB; SP R; SP T M is less or equal the exponent ! of matrix multiplication. The proof is based on ideas from <ref> [4, 11, 12] </ref>. See also [1, pages 233-240]. The proceeding is to subdivide the occuring matrices into blocks, to perform a sort of Gaussian Elimination blockwise using a fast hypothetical matrix multiplication algorithm, and then to continue recursively. We leave the details to the reader.
Reference: [12] <author> V. Strassen, </author> <title> Gaussian elimination is not optimal, </title> <journal> Numer. </journal> <volume> Mathematik 13(1969), </volume> <pages> 354-356. </pages>
Reference-contexts: Problems like matrix inversion, computation of the determinant or of all coefficients of the characteristic polynomial, LR-decomposition and over the complex numbers also QR-decomposition and unitary transformation to Hessenberg form are all known to be as hard as matrix multiplication.(See <ref> [3, 4, 6, 7, 11, 12, 15] </ref>.) In this paper we study some computational problems in linear algebra that are not specified by a function but by a relation. Let F denote a field of characteristic zero that may be endowed with an ordering . <p> Theorem 3 The exponent for any of the sequences of problems tCP R; KER; OGB; SP R; SP T M is less or equal the exponent ! of matrix multiplication. The proof is based on ideas from <ref> [4, 11, 12] </ref>. See also [1, pages 233-240]. The proceeding is to subdivide the occuring matrices into blocks, to perform a sort of Gaussian Elimination blockwise using a fast hypothetical matrix multiplication algorithm, and then to continue recursively. We leave the details to the reader.
Reference: [13] <author> V. Strassen, </author> <title> Berechnung und Programm I, </title> <journal> Acta Informatica 1(1973), </journal> <pages> 320-335. </pages>
Reference-contexts: We define the cost of a computation tree T as the maximal number of multiplications and divisions that may be performed in a computation of the tree T .(Compare <ref> [10, 13, 14, 16] </ref>.) The complexity C (f ) of a function is then defined as the minimal cost of a tree computing f, and finally we put C (P ) := minfC (f ) : f function solving P g for the complexity of the problem P . <p> Let us shortly describe this notion, a detailed discussion can be found in <ref> [10, 13, 14, 16] </ref>. As the set of operational symbols and the set R of relational symbols (together with arity functions) we take = F t f0; 1; +; ; fl; =g R = f=g (or R = fg when we consider an ordered field (F; )).
Reference: [14] <author> V. Strassen, </author> <title> Berechnung und Programm II, </title> <journal> Acta Informatica 2(1973), </journal> <pages> 64-79. </pages>
Reference-contexts: We define the cost of a computation tree T as the maximal number of multiplications and divisions that may be performed in a computation of the tree T .(Compare <ref> [10, 13, 14, 16] </ref>.) The complexity C (f ) of a function is then defined as the minimal cost of a tree computing f, and finally we put C (P ) := minfC (f ) : f function solving P g for the complexity of the problem P . <p> Let us shortly describe this notion, a detailed discussion can be found in <ref> [10, 13, 14, 16] </ref>. As the set of operational symbols and the set R of relational symbols (together with arity functions) we take = F t f0; 1; +; ; fl; =g R = f=g (or R = fg when we consider an ordered field (F; )).
Reference: [15] <author> V.Strassen, </author> <title> Vermeidung von Divisionen, </title> <journal> Crelles Journal fur die reine und angewandte Mathematik 264(1973), </journal> <pages> 184-202. </pages>
Reference-contexts: Problems like matrix inversion, computation of the determinant or of all coefficients of the characteristic polynomial, LR-decomposition and over the complex numbers also QR-decomposition and unitary transformation to Hessenberg form are all known to be as hard as matrix multiplication.(See <ref> [3, 4, 6, 7, 11, 12, 15] </ref>.) In this paper we study some computational problems in linear algebra that are not specified by a function but by a relation. Let F denote a field of characteristic zero that may be endowed with an ordering . <p> The main goal of this paper is to prove lower bounds on the complexity of the problems cited above in terms of the complexity of matrix multiplication. The proofs rest on ideas from <ref> [15] </ref> and the important Derivation Theorem (see [3] ). In the last section we employ the notion of dimension for an affine variety. <p> For dealing with the nonscalar complexity of rational functions we will use some known techniques that are listed in the next section. 3 Properties of the nonscalar complexity Let us recall some ideas from <ref> [15] </ref>. <p> We omit the trivial proof. The next theorem will be used throughout in the paper. Theorem 1 ("Vermeidung von Divisionen" <ref> [15] </ref>) Let 2 F m , f 1 ; : : : ; f n 2 O , d 2 N. <p> A well known consequence of the statement above is M n = C (M AM U (n;n;n) ) = L F [X;Y ] (f l=1 The proof of Theorem 1 (see <ref> [15] </ref>) immediately leads to 7 Corollary 1 Let F be algebraically closed. Let 2 F m ; f 1 ; : : : ; f n 2 O ; d 2 N.
Reference: [16] <author> V. Strassen, </author> <title> The complexity of continued fraction, </title> <journal> SIAM J. Comp. </journal> <volume> 12/1(1983), </volume> <pages> 1-27. </pages>
Reference-contexts: We define the cost of a computation tree T as the maximal number of multiplications and divisions that may be performed in a computation of the tree T .(Compare <ref> [10, 13, 14, 16] </ref>.) The complexity C (f ) of a function is then defined as the minimal cost of a tree computing f, and finally we put C (P ) := minfC (f ) : f function solving P g for the complexity of the problem P . <p> Let us shortly describe this notion, a detailed discussion can be found in <ref> [10, 13, 14, 16] </ref>. As the set of operational symbols and the set R of relational symbols (together with arity functions) we take = F t f0; 1; +; ; fl; =g R = f=g (or R = fg when we consider an ordered field (F; )).
Reference: [17] <author> V. Strassen, </author> <title> Relative bilinear complexity and matrix multiplication, </title> <journal> J. fur die reine und angewandte Mathematik 375/376(1987), </journal> <pages> 406-443. </pages>
Reference: [18] <author> I. Wegener, </author> <title> The complexity of Boolean functions, </title> <address> Wiley-Teubner, </address> <year> 1987. </year> <month> 19 </month>
Reference-contexts: The lower bound we are going to prove shows that an analogue of the carry save adders for matrix multiplication does not exist.(Compare <ref> [18] </ref>.) (2) KERNEL: KER n := f (A; B) 2 F nfin fi n G F nfii : B 2 F nfi (nR (A)) ; R (A)+R (B) = n; AB = 0g: This is of course the problem of computing a basis of the kernel for a given matrix. (3)
References-found: 18

