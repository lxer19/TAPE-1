URL: http://www.cs.huji.ac.il/labs/learning/Papers/clustew.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/MLT_list.html
Root-URL: http://www.cs.huji.ac.il
Title: Distributional Clustering of English Words  
Author: Fernando Pereira Naftali Tishby Lillian Lee 
Date: April 25, 1993  
Affiliation: AT&T Bell Laboratories Hebrew University Cornell University  
Abstract: We describe and experimentally evaluate a method for automatically clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy is used to measure the dissimilarity of those distributions. Clusters are represented by "typical" context distributions averaged from the given words according to their probabilities of cluster membership, and in many cases can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters. As the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical "soft" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.
Abstract-found: 1
Intro-found: 1
Reference: [BDPd + 90] <author> Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. </author> <title> Class-based n-gram models of natural language. </title> <booktitle> In Proceedings of the IBM Natural Language ITL, </booktitle> <pages> pages 283-298, </pages> <address> Paris, France, </address> <month> March </month> <year> 1990. </year> <month> 14 </month>
Reference-contexts: More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p (cjw) for each word w. Most other class-based modeling techniques for natural language rely 1 instead on "hard" Boolean classes <ref> [BDPd + 90] </ref>. Class construction is then combinatorially very demanding and depends on frequency counts for joint events involving particular words, a potentially unreliable source of information as we noted above.
Reference: [CG91] <author> Kenneth W. Church and William A. Gale. </author> <title> A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 19-54, </pages> <year> 1991. </year>
Reference-contexts: One technical difficulty is that D (f k f 0 ) is not defined (infinite) when f 0 (x) = 0 but f (x) &gt; 0. We could sidestep this problem (as we did initially) by smoothing zero frequencies appropriately <ref> [CG91] </ref>. However, this is not very satisfactory because one of the goals of our work is precisely to avoid the problems of data sparseness by grouping words into classes.
Reference: [Chu88] <author> Kenneth W. Church. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143, </pages> <address> Austin, Texas, </address> <year> 1988. </year> <institution> Association for Computational Linguistics, Morristown, </institution> <address> New Jersey. </address>
Reference-contexts: Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by Hindle's parser Fidditch [Hin93]. More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger <ref> [Chu88] </ref> and of tools for regular expression pattern matching on tagged corpora [Yar92]. <p> This corpus had previously been automatically tagged with parts of speech <ref> [Chu88] </ref>. With the help of concordancing and pattern-matching tools [Yar92], we extracted from the corpus a set of verb-noun pairs likely to correspond to the main verb and to the head of the direct object of the verb.
Reference: [DH73] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, New York, </address> <year> 1973. </year>
Reference-contexts: Here we proceed with the asymmetric model. 1 As usual in clustering models <ref> [DH73] </ref>, we assume that the model distribution and the empirical distribution are interchangeable at the solution of the parameter estimation equations, since the model is assumed to be able to represent correctly the data at that solution point.
Reference: [DLR77] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: We shall show that, with the maximum entropy (ME) membership distribution, ML estimation is equivalent to the minimization of the average distortion of the data. The combined entropy maximization entropy and distortion minimization is carried out by a two-stage iterative process similar to the standard EM method <ref> [DLR77] </ref>. The first stage of an iteration is a maximum likelihood, or minimum distortion, estimation of the cluster centroids given fixed membership probabilities. In the second iteration stage, the entropy of the membership distribution is maximized with a fixed average distortion.
Reference: [DMM92] <author> Ido Dagan, Shaul Markus, and Shaul Markovitch. </author> <title> Contextual word similarity and the estimation of sparse lexical relations. </title> <note> Submitted for publication, </note> <year> 1992. </year>
Reference-contexts: The data for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. <ref> [DMM92] </ref>. A small number (104) of (v; n) pairs with a fairly frequent verb (between 500 and 5000 occurrences) was randomly picked, and all occurrences of each pair in the training set were deleted. The resulting training set was used to build a sequence of cluster models as before.
Reference: [Hin90] <author> Donald Hindle. </author> <title> Noun classification from predicate-argument structures. </title> <booktitle> In 28th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 268-275, </pages> <address> Pittsburgh, Pennsylvania, </address> <year> 1990. </year> <institution> Association for Computational Linguistics, Morristown, </institution> <address> New Jersey. </address>
Reference-contexts: The problem is that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities. Hindle <ref> [Hin90] </ref> proposed dealing with the sparseness problem by estimating the likelihood of unseen events from that of "similar" events that have been seen. For instance, one may estimate the likelihood of a particular direct object for a verb from the likelihoods of that direct object for similar verbs.
Reference: [Hin93] <author> Donald Hindle. </author> <title> A parser for text corpora. </title> <editor> In B.T.S. Atkins and A. Zampoli, editors, </editor> <title> Computational Approaches to the Lexicon. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> England, </address> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by Hindle's parser Fidditch <ref> [Hin93] </ref>. More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger [Chu88] and of tools for regular expression pattern matching on tagged corpora [Yar92].
Reference: [Res92] <author> Philip Resnik. </author> <title> WordNet and distributional analysis: A class-based approach to lexical discovery. </title> <booktitle> In AAAI Workshop on Statistically-Based Natural-Language-Processing Techniques, </booktitle> <address> San Jose, California, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: While it may be worthwhile to base such a model on preexisting sense classes <ref> [Res92] </ref>, in the work described here we look at how to derive the classes directly from distributional data. More specifically, we model senses as probabilistic concepts or clusters c with corresponding cluster membership probabilities p (cjw) for each word w.
Reference: [RGF90] <author> Kenneth Rose, Eitan Gurewitz, and Geoffrey C. Fox. </author> <title> Statistical mechanics and phase transitions in clustering. </title> <journal> Physical Review Letters, </journal> <volume> 65(8) </volume> <pages> 945-948, </pages> <year> 1990. </year>
Reference-contexts: The analogy with statistical mechanics suggest a deterministic annealing procedure for the clustering <ref> [RGF90] </ref>, in which the number of clusters is determined through a sequence of phase transitions by contin uously increasing the parameter fi (decreasing the temperature T ) as we explain in the next section. 4.2 Hierarchical Clustering The statistics of natural languages is inherently ill defined. <p> To observe these transition for clustering it is necessary to follow the free energy of two solutions of consecutive number of clusters as functions of the decreasing temperature. An effective way of doing this is through deterministic annealing, as proposed by Rose et al. <ref> [RGF90] </ref>.
Reference: [Sch92] <author> Yves Schabes. </author> <title> Stochastic lexicalized tree-adjoining grammars. </title> <booktitle> In Proceeedings of the 14th International Conference on Computational Linguistics, </booktitle> <address> Nantes, France, </address> <year> 1992. </year>
Reference-contexts: Moving further in the direction of class-based language models, we plan to consider additional distributional relations (for instance, adjective-noun) and apply the results of clustering to the grouping of lexical associations in lexicalized grammar frameworks such as stochastic lexicalized tree-adjoining grammars <ref> [Sch92] </ref>. 8 Acknowledgments We would like to thank Don Hindle for making available the 1988 Associated Press verb-object data set, the Fidditch parser and a verb-object structure filter, Mats Rooth for selecting the objects of "fire" data set and many discussions, David Yarowsky for help with his stemming and concordancing tools,
Reference: [Yar92] <author> David Yarowsky. CONC: </author> <title> Tools for text corpora. </title> <type> Technical Memorandum 11222-921222-29, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1992. </year>
Reference-contexts: The corpus used in our first experiment was derived from newswire text automatically parsed by Hindle's parser Fidditch [Hin93]. More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger [Chu88] and of tools for regular expression pattern matching on tagged corpora <ref> [Yar92] </ref>. <p> This corpus had previously been automatically tagged with parts of speech [Chu88]. With the help of concordancing and pattern-matching tools <ref> [Yar92] </ref>, we extracted from the corpus a set of verb-noun pairs likely to correspond to the main verb and to the head of the direct object of the verb.
References-found: 12

