URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/nonoverlapping.ps.gz
Refering-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Root-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Email: mschmitt@lmi.ruhr-uni-bochum.de  
Phone: Phone: ++49 234 700-3209 Fax: ++49 234 7094-465  
Title: On the Sample Complexity for Nonoverlapping Neural Networks  
Author: Michael Schmitt 
Keyword: Neural networks, read-once formulas, threshold gates, sigmoidal gates, PAC learning, Vapnik-Chervonenkis dimension  
Note: Work supported in part by the ESPRIT Working Group in Neural and Computational Learning II, NeuroCOLT2, No. 27150. Part of this work was done while the author was with the Institute for Theoretical  
Address: Ruhr-Universitat Bochum D-44780 Bochum Germany  Graz, A-8010 Graz, Austria.  
Affiliation: Lehrstuhl Mathematik und Informatik Fakultat fur Mathematik  Computer Science at the Technische Universitat  
Abstract: A neural network is said to be nonoverlapping if there is at most one edge outgoing from each node. We investigate the number of examples that a learning algorithm needs when using nonoverlapping neural networks as hypotheses. We derive bounds for this sample complexity in terms of the Vapnik-Chervonenkis dimension. In particular, we consider networks consisting of threshold, sigmoidal and linear gates. We show that the class of nonoverlapping threshold networks and the class of nonoverlapping sig-moidal networks on n inputs both have Vapnik-Chervonenkis dimension (n log n). This bound is asymptotically tight for the class of nonoverlapping threshold networks. We also present an upper bound for this class where the constants involved are considerably smaller than in a previous calculation. Finally, we argue that the Vapnik-Chervonenkis dimension of nonoverlapping threshold or sigmoidal networks cannot become larger by allowing the nodes to compute linear functions. This sheds some light on a recent result that exhibited neural networks with quadratic VC dimension. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D., Hellerstein, L., and Karpinski, M. </author> <year> (1993). </year> <title> Learning read-once formulas with queries. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 40 </volume> <pages> 185-210. </pages>
Reference-contexts: Since such networks have less degrees of freedom, learning using these hypotheses is expected to be more efficient in terms of sample size and computing time than when using unrestricted neural networks. The computational complexity of learning using nonoverlapping networks has been extensively studied in the literature. <ref> (Angluin et al., 1993) </ref>, for instance, investigated the existence of efficient algorithms that use queries to learn networks that are known as Boolean trees or read-once formulas. In particular, they showed that read-once formulas can be exactly identified in polynomial time using both equivalence and membership queries.
Reference: <author> Anthony, M. and Biggs, N. </author> <year> (1992). </year> <title> Computational Learning Theory. </title> <booktitle> Cambridge Tracts in Theoretical Computer Science. </booktitle> <publisher> Cambridge University Press, Cam-bridge. </publisher>
Reference-contexts: The VC dimension of a sigmoidal neural network with w weights is O (w 4 ). This has been shown by (Karpinski and Macintyre, 1997). By Sauer's Lemma (see, e.g., <ref> (Anthony and Biggs, 1992) </ref>) the number of dichotomies induced by a class of functions with VC dimension d 2 on a set of cardinality m 2 can be bounded from above by m d .
Reference: <author> Barkai, E., Hansel, D., and Kanter, I. </author> <year> (1990). </year> <title> Statistical mechanics of a multi-layered neural network. </title> <journal> Physical Review Letters, </journal> <volume> 65(18) </volume> <pages> 2312-2315. </pages>
Reference-contexts: In other words, the connectivity or architecture of the network is a tree. 2 The notion of "nonoverlapping" can be traced back to the work of <ref> (Barkai et al., 1990) </ref> and has been introduced to model a type of biological neural network where the receptive fields of the neurons do not overlap, i.e., are pairwise disjoint. In a nonoverlapping neural network there is exactly one node, the output node, that has no edge outgoing.
Reference: <author> Baum, E. B. and Haussler, D. </author> <year> (1989). </year> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1 </booktitle> <pages> 151-160. </pages>
Reference-contexts: We briefly mention the most relevant ones for this article. Concerning upper bounds, a feedforward network of threshold gates is known to have VC dimension at most O (w log w) where w is the number of weights <ref> (Baum and Haussler, 1989) </ref>. Networks using piecewise polynomial functions for their gates have VC dimension O (w 2 ) (Goldberg and Jerrum, 1995) whereas for sigmoidal networks the bound O (w 4 ) is known (Karpinski and Macintyre, 1997). <p> Taking logarithms on both sides we obtain m (2n 1) log (mn) + 1 : We weaken this to m 2n log (mn) : (1) 3 We do not make use of the equivalence relations involved in this result but of the improve ment that it achieves compared to <ref> (Baum and Haussler, 1989) </ref>. 8 Assume without loss of generality that m log n. Then it is easy to see that for each such m there is a real number r 1 such that m can be written as m = r log (rn). <p> On the other hand, networks consisting of threshold gates only have VC dimension O (w log w). This follows from (Cover, 1968) and has also been shown by <ref> (Baum and Haussler, 1989) </ref>. Results that this bound is tight for threshold networks are due to (Sakurai, 1993) and (Maass, 1994).
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. </author> <year> (1989). </year> <title> Learn-ability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36 </volume> <pages> 929-965. </pages>
Reference: <author> Cover, T. M. </author> <year> (1965). </year> <title> Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 14 </volume> <pages> 326-334. </pages>
Reference-contexts: for this lower bound since a threshold gate and a sigmoidal gate both have VC dimension n + 1: This follows for the threshold gate from a bound on the number of different regions arising from sets of hyperplanes in IR n which is due to (Schlafli, 1901) (see also <ref> (Cover, 1965) </ref>). For the sigmoidal gate this follows from the fact that its pseudo dimension is n + 1 (Haussler, 1992). Together with the upper bound O (n log n) due to (Hancock et al., 1994) we obtain asymptotically tight bounds for the class of nonoverlapping threshold networks.
Reference: <author> Cover, T. M. </author> <year> (1968). </year> <title> Capacity problems for linear machines. </title> <editor> In Kanal, L. N., editor, </editor> <booktitle> Pattern Recognition, </booktitle> <pages> pages 283-289, </pages> <publisher> Thompson Book Co., Washing-ton. </publisher>
Reference-contexts: This result was somewhat unexpected since networks consisting of linear gates only compute linear functions and therefore have VC dimension O (w). On the other hand, networks consisting of threshold gates only have VC dimension O (w log w). This follows from <ref> (Cover, 1968) </ref> and has also been shown by (Baum and Haussler, 1989). Results that this bound is tight for threshold networks are due to (Sakurai, 1993) and (Maass, 1994).
Reference: <author> Goldberg, P. W. and Jerrum, M. R. </author> <year> (1995). </year> <title> Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. </title> <journal> Machine Learning, </journal> <volume> 18 </volume> <pages> 131-148. </pages>
Reference-contexts: Concerning upper bounds, a feedforward network of threshold gates is known to have VC dimension at most O (w log w) where w is the number of weights (Baum and Haussler, 1989). Networks using piecewise polynomial functions for their gates have VC dimension O (w 2 ) <ref> (Goldberg and Jerrum, 1995) </ref> whereas for sigmoidal networks the bound O (w 4 ) is known (Karpinski and Macintyre, 1997). With respect to lower bounds threshold networks with VC dimension (w log w) have been constructed (Sakurai, 1993; Maass, 1994). <p> Combining this with the bound (4n) n1 employed in the proof of Theorem 5 and using similar arguments we obtain the bound as claimed. 6 Nonoverlapping Neural Networks Containing Linear Gates From <ref> (Goldberg and Jerrum, 1995) </ref> it has been known that neural networks employing piecewise polynomial activation functions have VC dimension O (w 2 ), where w is the number of weights. The question whether this bound is tight for such networks has been settled by (Koiran and Sontag, 1997).
Reference: <author> Golea, M. and Marchand, M. </author> <year> (1990). </year> <title> A growth algorithm for neural network decision trees. </title> <journal> Europhysics Letters, </journal> <volume> 12(3) </volume> <pages> 205-210. </pages>
Reference: <author> Golea, M., Marchand, M., and Hancock, T. R. </author> <year> (1993). </year> <title> On learning -Perceptron networks with binary weights. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 591-598. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: They also proved a negative result stating that neither equivalence nor membership queries alone are sufficient to exactly identify all read-once formulas in polynomial time. Research on the learnability of nonoverlapping networks employing neural gates has been initiated by <ref> (Golea et al., 1993) </ref>. They studied the probably approximately correct (PAC) learnability of so-called -Perceptron networks with binary weights: A -Perceptron network is a disjunction of threshold gates where each input node is connected to exactly one threshold gate.
Reference: <author> Golea, M., Marchand, M., and Hancock, T. R. </author> <year> (1996). </year> <title> On learning -Perceptron networks on the uniform distribution. </title> <booktitle> Neural Networks, </booktitle> <volume> 9 </volume> <pages> 67-82. </pages> <note> 12 Hancock, </note> <author> T. R., Golea, M., and Marchand, M. </author> <year> (1994). </year> <title> Learning nonoverlap-ping Perceptron networks from examples and membership queries. </title> <journal> Machine Learning, </journal> 16:161-183. 
Reference-contexts: In particular, they designed algorithms that PAC learn these networks in polynomial time from examples only when these examples are randomly drawn under the uniform distribution. <ref> (Golea et al., 1996) </ref> generalized these results to tree structures in the form of -Perceptron decision lists. General nonoverlapping architectures that employ threshold gates as network nodes were considered by (Hancock et al., 1994).
Reference: <author> Haussler, D. </author> <year> (1992). </year> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100 </volume> <pages> 78-150. </pages>
Reference-contexts: Moreover, these estimates of the sample complexity in terms of the VC dimension hold even for agnostic PAC learning, that is, in the case when the training examples are generated by some arbitrary probability distribution <ref> (Haussler, 1992) </ref>. Furthermore, the VC dimension is known to yield bounds for the complexity of learning in various on-line learning models (Littlestone, 1988; Maass and Turan, 1992). 2 Results on the VC dimension for neural networks abound; see, for instance, the survey by (Maass, 1995). <p> For the sigmoidal gate this follows from the fact that its pseudo dimension is n + 1 <ref> (Haussler, 1992) </ref>. Together with the upper bound O (n log n) due to (Hancock et al., 1994) we obtain asymptotically tight bounds for the class of nonoverlapping threshold networks. Corollary 4 The VC dimension of the class of nonoverlapping threshold networks on n inputs is fi (n log n).
Reference: <author> Karpinski, M. and Macintyre, A. </author> <year> (1997). </year> <title> Polynomial bounds for VC dimension of sigmoidal and general pfaffian neural networks. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 54 </volume> <pages> 169-176. </pages>
Reference-contexts: Networks using piecewise polynomial functions for their gates have VC dimension O (w 2 ) (Goldberg and Jerrum, 1995) whereas for sigmoidal networks the bound O (w 4 ) is known <ref> (Karpinski and Macintyre, 1997) </ref>. With respect to lower bounds threshold networks with VC dimension (w log w) have been constructed (Sakurai, 1993; Maass, 1994). Furthermore, (Koiran and Sontag, 1997) have shown that there are neural networks with VC dimension (w 2 ). <p> We give a brief account. Proposition 7 The class of nonoverlapping sigmoidal networks on n inputs has VC dimension O (n 4 ). 9 Proof. The VC dimension of a sigmoidal neural network with w weights is O (w 4 ). This has been shown by <ref> (Karpinski and Macintyre, 1997) </ref>. By Sauer's Lemma (see, e.g., (Anthony and Biggs, 1992)) the number of dichotomies induced by a class of functions with VC dimension d 2 on a set of cardinality m 2 can be bounded from above by m d .
Reference: <author> Koiran, P. and Sontag, E. D. </author> <year> (1997). </year> <title> Neural networks with quadratic VC dimension. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 54 </volume> <pages> 190-198. </pages>
Reference-contexts: With respect to lower bounds threshold networks with VC dimension (w log w) have been constructed (Sakurai, 1993; Maass, 1994). Furthermore, <ref> (Koiran and Sontag, 1997) </ref> have shown that there are neural networks with VC dimension (w 2 ). Among these are networks that consist of both threshold and linear gates, and sigmoidal networks. <p> Finally, in Section 6 we show that adding linear gates to nonoverlapping threshold or sigmoidal networks cannot increase their VC dimension. Interestingly, it was this use of linear gates that lead to a quadratic lower bound for sigmoidal neural networks in the work of <ref> (Koiran and Sontag, 1997) </ref>. <p> The question whether this bound is tight for such networks has been settled by <ref> (Koiran and Sontag, 1997) </ref>. They have shown that networks consisting of threshold and linear gates can have VC dimension (w 2 ). This result was somewhat unexpected since networks consisting of linear gates only compute linear functions and therefore have VC dimension O (w).
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318. </pages>
Reference: <author> Maass, W. </author> <year> (1994). </year> <title> Neural nets with superlinear VC-dimension. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 877-884. </pages>
Reference-contexts: It is well known that in a network that computes a Boolean function a threshold gate can be replaced by a sigmoidal gate without changing the function of the network. (If necessary, the weights have to be scaled appropriately. See, for instance, <ref> (Maass et al., 1994) </ref> for a treatment in the context of circuit complexity). Thus, the lower bound (n log n) also holds for nonoverlapping depth-two networks that may consist of threshold or sigmoidal gates. <p> On the other hand, networks consisting of threshold gates only have VC dimension O (w log w). This follows from (Cover, 1968) and has also been shown by (Baum and Haussler, 1989). Results that this bound is tight for threshold networks are due to (Sakurai, 1993) and <ref> (Maass, 1994) </ref>. Therefore, the question arises whether a similar increase of the VC dimension is possible for nonoverlapping threshold or sigmoidal networks by allowing some of the nodes to compute linear functions. We show now that this cannot happen.
Reference: <author> Maass, W. </author> <year> (1995). </year> <title> Vapnik-Chervonenkis dimension of neural nets. </title> <editor> In Arbib, M. A., editor, </editor> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 1000-1003. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Furthermore, the VC dimension is known to yield bounds for the complexity of learning in various on-line learning models (Littlestone, 1988; Maass and Turan, 1992). 2 Results on the VC dimension for neural networks abound; see, for instance, the survey by <ref> (Maass, 1995) </ref>. We briefly mention the most relevant ones for this article. Concerning upper bounds, a feedforward network of threshold gates is known to have VC dimension at most O (w log w) where w is the number of weights (Baum and Haussler, 1989).
Reference: <author> Maass, W., Schnitger, G., and Sontag, E. D. </author> <year> (1994). </year> <title> A comparison of the computational power of sigmoid and Boolean threshold circuits. </title> <editor> In Roychowdhury, V., Siu, K.-Y., and Orlitsky, A., editors, </editor> <booktitle> Theoretical Advances in Neural Computation and Learning, </booktitle> <pages> pages 127-151. </pages> <publisher> Kluwer, </publisher> <address> Boston. </address>
Reference-contexts: It is well known that in a network that computes a Boolean function a threshold gate can be replaced by a sigmoidal gate without changing the function of the network. (If necessary, the weights have to be scaled appropriately. See, for instance, <ref> (Maass et al., 1994) </ref> for a treatment in the context of circuit complexity). Thus, the lower bound (n log n) also holds for nonoverlapping depth-two networks that may consist of threshold or sigmoidal gates. <p> On the other hand, networks consisting of threshold gates only have VC dimension O (w log w). This follows from (Cover, 1968) and has also been shown by (Baum and Haussler, 1989). Results that this bound is tight for threshold networks are due to (Sakurai, 1993) and <ref> (Maass, 1994) </ref>. Therefore, the question arises whether a similar increase of the VC dimension is possible for nonoverlapping threshold or sigmoidal networks by allowing some of the nodes to compute linear functions. We show now that this cannot happen.
Reference: <author> Maass, W. and Turan, G. </author> <year> (1992). </year> <title> Lower bound methods and separation results for on-line learning models. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 107-145. </pages>
Reference: <author> Sakurai, A. </author> <year> (1993). </year> <title> Tighter bounds of the VC-dimension of three-layer networks. </title> <booktitle> In Proceedings of the World Congress on Neural Networks WCNN'93, </booktitle> <volume> volume 3, </volume> <pages> pages 540-543. </pages>
Reference-contexts: On the other hand, networks consisting of threshold gates only have VC dimension O (w log w). This follows from (Cover, 1968) and has also been shown by (Baum and Haussler, 1989). Results that this bound is tight for threshold networks are due to <ref> (Sakurai, 1993) </ref> and (Maass, 1994). Therefore, the question arises whether a similar increase of the VC dimension is possible for nonoverlapping threshold or sigmoidal networks by allowing some of the nodes to compute linear functions. We show now that this cannot happen.
Reference: <author> Schlafli, L. </author> <year> (1901). </year> <editor> Theorie der vielfachen Kontinuitat. Zurcher & Furrer, </editor> <address> Zurich. </address>
Reference-contexts: depth two is minimal for this lower bound since a threshold gate and a sigmoidal gate both have VC dimension n + 1: This follows for the threshold gate from a bound on the number of different regions arising from sets of hyperplanes in IR n which is due to <ref> (Schlafli, 1901) </ref> (see also (Cover, 1965)). For the sigmoidal gate this follows from the fact that its pseudo dimension is n + 1 (Haussler, 1992).
Reference: <editor> Reprinted in: Schlafli, L. </editor> <year> (1950). </year> <note> Gesammelte Mathematische Abhandlungen. </note>
Reference: <editor> Band I. </editor> <publisher> Birkhauser, Basel. </publisher>
Reference: <author> Shawe-Taylor, J. </author> <year> (1995). </year> <title> Sample sizes for threshold networks with equivalences. </title> <journal> Information and Computation, </journal> <volume> 118 </volume> <pages> 65-72. </pages>
Reference: <author> Sirat, J. A. and Nadal, J.-P. </author> <year> (1990). </year> <title> Neural trees: a new tool for classification. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 1 </volume> <pages> 423-438. </pages> <note> 13 Valiant, </note> <author> L. G. </author> <year> (1984). </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> 27:1134-1142.
References-found: 25

