URL: http://www.robotics.stanford.edu/~parr/sara98.ps.Z
Refering-URL: http://www.robotics.stanford.edu/~parr/
Root-URL: http://www.robotics.stanford.edu
Email: parr@cs.berkeley.edu  
Title: A Unifying Framework for Temporal Abstraction in Stochastic Processes  
Author: Ronald Parr 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California  
Abstract: This paper presents a framework for unifying the large and growing body of literature that deals with what broadly can be defined as temporal abstraction in Markov Decision Processes (MDPs). MDPs provide an appealing formal framework for modeling a large variety of stochastic problems. The main drawback of this approach is that a requirement of the formal model, i.e., the Markov property, typically forces very fine granularity models which, in many cases, can be intractably large. Numerous approaches have been developed to help manage large state spaces by introducing actions at varying levels of granularity, including hierarchical decomposition, multi-layer control, and macro-actions. This paper extracts and generalizes a common thread that runs through all of these methods: the transformation of a policy defined over a region of the state space into a temporally abstract action in a new semi-Markov decision process (SMDP). Convergence theorems are presented for SMDP reinforcement learning and an asynchronous form of policy iteration that exploits this transformation. The existing methods for temporal abstraction are divided into three categories which are analyzed within this framework. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bellman, R. E. </author> <year> 1957. </year> <title> Dynamic Programming. </title> <publisher> Princeton, </publisher> <address> New Jersey: </address> <publisher> Princeton University Press. </publisher>
Reference: <author> Bertsekas, D. C., and Tsitsiklis, J. N. </author> <year> 1996. </year> <title> Neuro-Dynamic Programming. </title> <address> Belmont, Massachusetts: </address> <publisher> Athena Scientific. </publisher>
Reference: <author> Boutilier, C., and Dearden, R. </author> <year> 1994. </year> <title> Using abstractions for decison theoretic planning with time constraints. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> 1016-1022. </pages> <address> Seattle, Washington: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Bradtke, S. J., and Duff, M. O. </author> <year> 1995. </year> <title> Reinforcement learning methods for continuous-time Markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7: Proceedings of the 1994 Conference. </booktitle> <address> Denver, Col-orado: </address> <publisher> MIT Press. </publisher>
Reference: <author> Dantzig, G., and Wolfe, P. </author> <year> 1960. </year> <title> Decomposition principle for dynamic programs. </title> <journal> Operations Research 8(1) </journal> <pages> 101-111. </pages>
Reference-contexts: Dean and Lin (1995) use a very similar approach to the iterative refinement of abstract actions based upon the Dantzig-Wolfe decomposition of linear programs <ref> (Dantzig & Wolfe 1960) </ref>. The Dantzig-Wolfe decomposition guarantees convergence for a specific type of decomposition, but the relationship between abstract actions and SMDPs permits a much more general approach to decomposition. This insight arises from the relationship between the generation of a new abstract actions and asynchronous policy iteration.
Reference: <author> Dayan, P., and Hinton, G. E. </author> <year> 1993. </year> <title> Feudal reinforcement learning. </title> <editor> In Hanson, S. J.; Cowan, J. D.; and Giles, C. L., eds., </editor> <booktitle> Neural Information Processing Systems 5, </booktitle> <pages> 361-368. </pages> <address> San Mateo, California: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Dean, T., and Givan, R. </author> <year> 1997. </year> <title> Model minimization in Markov decision processes. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Aritificial Intelligence., </booktitle> <pages> 106-111. </pages> <address> Providence, Rhode Island: </address> <publisher> MIT Press. </publisher>
Reference: <author> Dean, T., and Lin, S.-H. </author> <year> 1995. </year> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> 1121-1127. </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dietterich, T. G. </author> <year> 1997. </year> <title> Hierarchical reinforcement learning with the MAXQ value function decomposition. </title> <type> Tech nical report, </type> <institution> Department of Computer Science, Oregon State University, Corvallis, Oregon. </institution>
Reference: <author> Forestier, J.-P., and Varaiya, P. </author> <year> 1978. </year> <title> Multilayer control of large Markov chains. </title> <journal> IEEE Transactions on Automatic Control AC-23:298-304. </journal>
Reference: <author> Hauskrecht, M.; Meuleau, N.; Boutilier, C.; Pack Kael-bling, L.; and Dean, T. </author> <year> 1998. </year> <title> Hierarchical solution of Markov decision processes using macro-actions. </title> <booktitle> In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI-98). to appear. </booktitle>
Reference: <author> Jaakkola, T.; Jordan, M.; and Singh, S. P. </author> <year> 1994. </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <booktitle> Neural Computation 6(6) </booktitle> <pages> 1185-1201. </pages>
Reference: <author> Lin, L.-J. </author> <year> 1993. </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> Ph.D. Dissertation, </type> <institution> Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania. </institution>
Reference: <author> Mahadevan, S.; Khaleeli, N.; and Marchalleck, N. </author> <year> 1997. </year> <title> Designing agent controllers using discrete-event Markov models. </title> <booktitle> In Proceedings of the AAAI 97 Fall Symposium on Model-Directed Autonomous Systems. </booktitle>
Reference: <author> McGovern, A.; Sutton, R. S.; and Fagg, A. H. </author> <year> 1997. </year> <title> Roles of macro-actions in accelerating reinforcement learning. </title> <booktitle> In 1997 Grace Hopper Celebration of Women in Computing. </booktitle>
Reference: <author> Parr, R., and Russell, S. </author> <year> 1998. </year> <title> Reinforcement learning with hierarchies of machines. </title> <booktitle> In Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Conference, to appear. </booktitle>
Reference-contexts: The determination of subtasks and task completion values are fairly domain-dependent and ad-hoc. The use of policy fragments, particularly HAMs <ref> (Parr & Russell 1998) </ref>, has the advantage that the abstractions can be defined in a programmatic way in terms of the agent's sensors.
Reference: <author> Parr, R. </author> <year> 1996. </year> <title> Policy based clustering for markov decision processes. </title> <booktitle> In Proceedings of the AAAI 96 Fall Symposium on Learning Complex Behaviors. </booktitle>
Reference: <author> Parr, R. </author> <year> 1998a. </year> <title> Flexible decomposition agorithms for weakly coupled Markov decision processes. </title> <booktitle> In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI-98). to appear. </booktitle>
Reference: <author> Parr, R. </author> <year> 1998b. </year> <title> Hierarchical Control and Learning for Markov Decision Processes. </title> <type> Ph.D. Dissertation, </type> <institution> Computer Science Division, University of California, Berkeley, California. </institution>
Reference: <author> Precup, D., and Sutton, R. S. </author> <year> 1998. </year> <title> Multi-time models for temporally abstract planning. </title> <booktitle> In Advances in Neural Information Processing Systems 10: Proceedings of the 1997 Conference, to appear. </booktitle>
Reference-contexts: For example, an agent executing a policy corresponding to an abstract action is also executing concrete actions along the way. The agent is free to update the Q-values for each state-action pair that occurs, learning Q tables for several different subtasks simultaneously <ref> (Sutton, Precup, & Singh 1998) </ref>. Concurrent methods are discussed in Precup, Sutton and Singh (1997), where convergence results are established for value iteration and policy iteration in MDPs with the addition of abstract actions.
Reference: <author> Precup, D.; Sutton, R. S.; and Singh, S. P. </author> <year> 1997. </year> <title> Planning with closed-loop macro actions. </title> <booktitle> In Proceedings of the AAAI Fall Symposium. </booktitle>
Reference: <author> Puterman, M. L. </author> <year> 1994. </year> <title> Markov Decision Processes. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Singh, S. P., and Gullapalli, V. </author> <year> 1993. </year> <title> Asynchronous modified policy iteration with single-side updates. </title> <type> unpublished manuscript. </type>
Reference: <author> Singh, S. P. </author> <year> 1992a. </year> <title> Scaling reinforcement learning algorithms by learning variable temporal resolution models. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning. </booktitle> <address> Aberdeen: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Singh, S. P. </author> <year> 1992b. </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <booktitle> Machine Learning 8(3) </booktitle> <pages> 323-340. </pages>
Reference-contexts: A useful trick that concurrent methods can exploit in RL is the fact that any state transition can be treated as an experience in several different, concurrent learning tasks, <ref> (Singh 1992b) </ref>. For example, an agent executing a policy corresponding to an abstract action is also executing concrete actions along the way. The agent is free to update the Q-values for each state-action pair that occurs, learning Q tables for several different subtasks simultaneously (Sutton, Precup, & Singh 1998).
Reference: <author> Sutton, R. S.; Precup, D.; and Singh, S. P. </author> <year> 1998. </year> <title> Between MDPs and semi-MDPs: Learning, planning, and representing knowledge at multiple temporal scales. </title> <booktitle> in prep. </booktitle>
Reference-contexts: For example, an agent executing a policy corresponding to an abstract action is also executing concrete actions along the way. The agent is free to update the Q-values for each state-action pair that occurs, learning Q tables for several different subtasks simultaneously <ref> (Sutton, Precup, & Singh 1998) </ref>. Concurrent methods are discussed in Precup, Sutton and Singh (1997), where convergence results are established for value iteration and policy iteration in MDPs with the addition of abstract actions.
Reference: <author> Sutton, R. S. </author> <year> 1995. </year> <title> Temporal abstraction in reinforcement learning. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 531-539. </pages> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tate, A. </author> <year> 1977. </year> <title> Generating project networks. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence (IJCAI-77), </booktitle> <pages> 888-893. </pages> <address> Cambridge, Mas-sachusetts: IJCAII. </address>
References-found: 28

