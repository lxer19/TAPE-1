URL: http://www.cs.rice.edu:80/~achauhan/work/fall95/comp425/aprop.ps
Refering-URL: http://www.cs.rice.edu:80/~achauhan/work/courses.html
Root-URL: 
Email: (achauhan@cs.rice.edu)  (henrik@cs.rice.edu)  (shilpa@cs.rice.edu)  
Title: Quantitative Comparison of Set-Associative and Victim Caches Proposal  
Author: Arun Chauhan Henrik Weimer Shilpa Lawande 
Date: October 9, 1995  
Abstract: Victim caches are small fully associative caches placed between a cache and its refill path. This results in the misses served by the victim caches, having only a very small miss penalty, typically one cycle, as opposed to several cycles for main memory. Small victim caches from one to five line sizes are sufficient to significantly improve the effective cache hit rate. This improvement however is sub-linear in the size of the victim cache and can be poorer for set associative caches. We propose to do a quantitative comparison of set-associative caches and direct-mapped caches together with a victim cache. In particular we aim to find the "associativity" of a direct-mapped cache with a victim cache. This would give us a quantitative measure of the size of a system with victim cache needed to match the performance of one with set-associative cache. 
Abstract-found: 1
Intro-found: 1
Reference: [AP93] <author> A Agarwal and D Pudar. </author> <title> Column-associative caches: A technique for reducing the miss rate of direct-mapped caches. In Computer Architecture News, </title> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year> <booktitle> 20th Annual International Symposium on Computer Architecture ISCA '20. </booktitle>
Reference-contexts: At the same time it increases miss penalty. Increasing the cache associativity can reduce conflict misses, but can lead to higher hit times. Targeting higher clock rates favors a simple cache design with low hit times. Other techniques include column-associative caches <ref> [AP93] </ref>, hardware pre-fetching of instruction and data, and compiler-controlled prefetching and optimizations. Victim caches (figure 1) are used to store the data that is discarded from the cache because of a conflict, and hence, reduce the miss penalty due to conflict misses.
Reference: [Jou90] <author> Norman P Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Conference Proceedings Annual Symposium on Computer Architecture, </booktitle> <address> Los Alamitos, California, Aug 1990. </address> <publisher> IEEE, Computer Society Press. </publisher>
Reference-contexts: A cache miss might result in a heavy penalty in terms of number of instructions which may be even higher for superscalar architectures that issue multiple instructions per cycle. Table 1 lists some cache miss times and the effects of the misses on machine performance <ref> [Jou90] </ref>. All this points to a greater incentive for reducing the cache miss rates. <p> Table 1: The increasing cost of cache misses Machine cycle per cycle time mem time miss cost miss cost instruction (ns) (ns) (cycles) (instr) VAX11/780 10.0 200 1200 6 4 WRL Titan 1.4 45 540 12 8.5 ? 0.5 4 280 70 140.0 A victim cache <ref> [Jou90] </ref> is a small fully associative cache placed between a cache and the object it caches (main memory, in our case). It is used to hold the "victims" that are chosen for replacement from the cache.
Reference: [PH96] <author> David A Patterson and John L Hennessey. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Fran-cisco, California, </address> <note> second edition, </note> <year> 1996. </year>
Reference-contexts: This margin could, in fact, be used to increase the size of the direct-mapped cache. 2 Previous Work There are several techniques for improving cache hit rate, cache miss penalty, and cache hit times <ref> [PH96] </ref>. We focus on techniques of improving cache hit rates 2 in this project. All cache misses can be categorized into the following types. Compulsory These are first reference misses since a block must be brought into the cache the first time it is accessed.
Reference: [Sti94] <author> Dimitrios Stiliadis. </author> <title> Selective victim caching : A method to improve the performance of direct-mapped caches. </title> <booktitle> In Proceedings of the 27th Hawaii International Conference on System Sciences, </booktitle> <address> Los Alamitos, California, 1994. </address> <publisher> IEEE, Computer Society Press. </publisher> <pages> 5 </pages>
Reference-contexts: The major advantages of this approach are that it does not incur any overhead per cycle and the primary cache can be kept simple by making it direct-mapped. Selective victim caching was proposed by <ref> [Sti94] </ref> to improve the performance of victim caches. The incoming blocks are selectively placed into the victim or 3 the main cache based on their past history of use. The blocks are also exchanged between the two caches selectively.
References-found: 4

