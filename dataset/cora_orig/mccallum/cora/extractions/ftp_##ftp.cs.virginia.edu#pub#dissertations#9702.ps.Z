URL: ftp://ftp.cs.virginia.edu/pub/dissertations/9702.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/dissertations/README.html
Root-URL: http://www.cs.virginia.edu
Title: SOR as a Preconditioner  
Author: Michael A. DeLong 
Degree: A Dissertation Presented to The Faculty of the  In Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy (Computer Science) by  
Date: May 1997  
Affiliation: School of Engineering and Applied Science University of Virginia  
Abstract-found: 0
Intro-found: 1
Reference: [Ada83] <author> L. Adams. </author> <title> Iterative Algorithms for Large Sparse Linear Systems on Parallel Computers. </title> <type> PhD thesis, </type> <institution> Department of Applied Mathematics and Computer Science, University of Virginia, </institution> <year> 1983. </year>
Reference-contexts: Dubois, Greenbaum, and Rodrigue [DGR79] used a Jacobi iteration as a preconditioner. Johnson, Micchelli, and Paul [JMP83] use least-squares and min-max polynomials in conjunction with the Jacobi iteration, and give a framework for more general subsidiary iteration preconditioning with their discussion of inner and outer iterations. Adams <ref> [Ada83] </ref>, [Ada85] used m-step SSOR preconditioning and also discussed the more general case of a polynomial SSOR preconditioner. Adams [Ada83] briefly discusses preconditioning CG with SOR, and gives an example showing that the SOR iteration is not a valid preconditioner for CG: the loss of symmetry of the preconditioned system leads <p> Adams <ref> [Ada83] </ref>, [Ada85] used m-step SSOR preconditioning and also discussed the more general case of a polynomial SSOR preconditioner. Adams [Ada83] briefly discusses preconditioning CG with SOR, and gives an example showing that the SOR iteration is not a valid preconditioner for CG: the loss of symmetry of the preconditioned system leads to a loss of convergence. <p> CHAPTER 1. INTRODUCTION 5 1.2 Nonsymmetric Problems If A is nonsymmetric (A T 6= A) the CG algorithm is not valid: its minimization property is lost, and its finite termination property may be lost as well (see e.g. <ref> [Ada83] </ref>, p. 127). Instead, several methods have been developed that are CG-like in that * They are optimal in some sense, or * Their iterates or residuals are related by short recurrences. We will discuss several of these methods in Chapter 2.
Reference: [Ada85] <author> L. Adams. </author> <title> M -step preconditioned conjugate gradient methods. </title> <journal> SIAM J. Sci Stat. Comput., </journal> <volume> 6(2) </volume> <pages> 452-463, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: Dubois, Greenbaum, and Rodrigue [DGR79] used a Jacobi iteration as a preconditioner. Johnson, Micchelli, and Paul [JMP83] use least-squares and min-max polynomials in conjunction with the Jacobi iteration, and give a framework for more general subsidiary iteration preconditioning with their discussion of inner and outer iterations. Adams [Ada83], <ref> [Ada85] </ref> used m-step SSOR preconditioning and also discussed the more general case of a polynomial SSOR preconditioner. <p> We denote GMRES restarted every m iterations right-preconditioned with k steps of SOR by SOR (k)-GMRES (m), and the algorithm is shown in Figure 16. When an iterative method is used in this way to precondition another iterative method, it is refered to as a subsidiary iteration. Adams <ref> [Ada85] </ref> introduced the use of a symmetric form of SOR (SSOR) as a preconditioner for CG in this way. 2.9 The Bi-CGSTAB algorithm There are many iterative methods for solving nonsymmetric linear systems that, like GMRES, are guaranteed to converge in at most n iterations if A is n fi n.
Reference: [AJ85] <author> L. Adams and H. Jordan. </author> <note> Is SOR color-blind? SIAM J. </note> <institution> Sci. Stat. Comput., </institution> <month> 7 </month> <pages> 490-506, </pages> <year> 1985. </year>
Reference-contexts: There are many structured problems whose graphs can be colored using some number of colors that can be determined a priori. Adams and Jordan <ref> [AJ85] </ref> show that for a certain class of stencils, the asymptotic convergence of SOR in multicolor orderings is identical to its asymptotic convergence in the natural ordering. This property suggests that for those problems in those orderings, SOR may also be an effective parallel preconditioner.
Reference: [Amd67] <author> G. </author> <title> Amdahl. Validity of the single-processor approach to achieving large-scale computer capabilities. </title> <booktitle> In AFIPS Conf. Proc., </booktitle> <volume> volume 30, </volume> <pages> pages 483-485, </pages> <year> 1967. </year>
Reference-contexts: of the speedup of a parallel program is (see e.g. [GO93]): S P = Execution time on a single processor Execution time using P processors : (151) Efficiency is closely related: E P = P The classical theoretical limit to speedup using the definition (151) is described by Amdahl's law <ref> [Amd67] </ref>.
Reference: [AG88] <author> C. Ashcraft and R. Grimes. </author> <title> On Vectorizing Incomplete Factorizations and SSOR Pre-conditioners. </title> <journal> SIAM J. Sci Stat. Comput., </journal> <volume> 9 </volume> <pages> 122-151, </pages> <year> 1988. </year>
Reference: [Axe74] <author> O. Axelsson. </author> <title> On preconditioning and convergence acceleration in sparse matrix problems. </title> <type> Technical report, </type> <institution> CERN, </institution> <year> 1974. </year>
Reference-contexts: The term preconditioning is generally credited to Turing [Tur48], and while the idea of preconditioning the CG algorithm was first presented by Hestenes [Hes56], the term and the idea were not brought together until the papers by Evans [Eva73] and Axelsson <ref> [Axe74] </ref>. It is not necessary to form S or SAS T explicitly; instead, the preconditioning is usually incorporated into the CG algorithm. This is shown in Figure 2, where M = (S T S) 1 .
Reference: [BBC + 94] <author> R. Barrett, M. Berry, T. F. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. Van der Vorst. </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, 2nd Edition. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1994. </year>
Reference-contexts: There are other parallel preconditioners for the CG method that are not IC, SSOR, nor any of the polynomial preconditioners mentioned earlier. A survey of preconditioners for the CG method may be found in <ref> [BBC + 94] </ref> and [Ort88]. CHAPTER 1. INTRODUCTION 5 1.2 Nonsymmetric Problems If A is nonsymmetric (A T 6= A) the CG algorithm is not valid: its minimization property is lost, and its finite termination property may be lost as well (see e.g. [Ada83], p. 127). <p> It is difficult to know a priori what will make a good preconditioner for using a given solver to solve a particular problem or class of problems. As a result, finding the "best" preconditioner-solver combination for a given problem is often a process of trial and error <ref> [BBC + 94] </ref>. One popular preconditoner for nonsymmetric systems uses Incomplete LU (ILU) factorization [Mv77]. Like IC, it is a variant of Gaussian Elimination, or LU factorization, in which A is factored as where U is upper triangular, L is lower triangular, and R 6= 0 is the residual matrix. <p> However, in <ref> [BBC + 94] </ref>, the authors claim that for nonsymmetric problems "[SOR and Gauss-Seidel] are never used as preconditioners, for a rather technical reason." They explain that preconditioning with SOR maps the eigenvalues of the preconditioned matrix onto a circle in the complex plane, and that polynomial acceleration of SOR yields no <p> In fact, Saad [Saa94] briefly considers multiple Gauss-Seidel steps, but does not use a over-relaxation factor. 1.3 Summary of our results In this dissertation we will show that * For an important class of problems, SOR is an effective preconditioner, despite the statement to the contrary in <ref> [BBC + 94] </ref>. * Unlike SSOR, SOR does not suffer serious degradation in the rate of convergence in the red black ordering. * It is critical to use multiple steps of Gauss-Seidel. This is why the use of a single Gauss-Seidel step in [ST94] was ineffective. CHAPTER 1. <p> Thus, the matrices may be viewed as small perturbations of the Poisson matrix. We will discuss the effect of larger in Section 3.4. 3.3 The effect of ! As mentioned in Chapter 1, Saad [Saa94] reports results only for the Gauss-Seidel iteration and in <ref> [BBC + 94] </ref> there is a footnote to the effect that SOR is never used as a preconditioner because CHAPTER 3. <p> The SOR-GMRES times are always better than SOR although for its optimal ! the time for SOR is very close to the best time for SOR-GMRES, corroborating the statement in <ref> [BBC + 94] </ref> that polynomial acceleration of SOR with the optimal ! is not effective. 2. For values of ! smaller than optimal, SOR-GMRES is considerably better than SOR. CHAPTER 3. <p> It is an open question under what conditions this is true. * Despite the fact that SSOR as a stand-alone iteration often converges more slowly than SOR (see [You71], p. 462, <ref> [BBC + 94] </ref>, p.12), it is sometimes considered as a preconditioner for nonsymmetric matrices ([Ton92], [BBC + 94]). The merit of SOR as a preconditioner relative to SSOR as a preconditioner needs to be explored. <p> It is an open question under what conditions this is true. * Despite the fact that SSOR as a stand-alone iteration often converges more slowly than SOR (see [You71], p. 462, <ref> [BBC + 94] </ref>, p.12), it is sometimes considered as a preconditioner for nonsymmetric matrices ([Ton92], [BBC + 94]). The merit of SOR as a preconditioner relative to SSOR as a preconditioner needs to be explored.
Reference: [Bok95] <author> S. Bokhari. </author> <title> Communication Overhead on the Intel Paragon, IBM SP-2, & Meiko CS-2. </title> <type> Technical Report Int28, </type> <institution> ICASE, </institution> <year> 1995. </year>
Reference-contexts: PARALLEL IMPLEMENTATION ISSUES 63 coprocessor or as an application processor. The i860 has a clock speed of 50 MHz, a theoretical peak computational rate of 75 Mflops <ref> [Bok95] </ref>, and has been measured at 10 Mflops on the LINPACK Benchmark [Don95]. Each processor runs OSF/1, a complete UNIX standards-compliant operating system. Of the 32 MBytes of memory on each node, about 24 Mbytes is available to the user. <p> The network behaves as though the nodes were fully connected all processors are meant to be equidistant, so a messages should take no longer to reach a node that is "far away" than one that is "nearby." However, Bokhari <ref> [Bok95] </ref> refutes this claim with random pair matching experiments, which show variation in excess of 100% in the times required for message exchanges across a busy 8 fi 8 submesh. Like the Paragon the SP2 uses wormhole routing, but with some byte buffering at each switch element for blocked packets.
Reference: [Bro91] <author> P. Brown. </author> <title> A theoretical comparison of the Arnoldi and GMRES algorithms. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 12(1) </volume> <pages> 58-78, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: We do not use Joubert's strategy. 2.5.4 Breakdown and stagnation Many iterative methods for solving nonsymmetric linear systems can break down, and fail to produce a new iterate. Brown <ref> [Bro91] </ref> contrasts the breakdown behavior of GMRES with that of an earlier method developed by Saad [Saa81], the Full Orthogonalization Method (FOM). <p> Then AV k = V k H k , the vectors v 1 ; : : :; v k span K m (A; r 0 ), so the current iterate x k solves Ax = b exactly. This is called lucky breakdown [SS86] or happy breakdown <ref> [Bro91] </ref>. It rarely occurs. * H k is singular. When H k is singular FOM cannot generate the new iterate x m . In the first case both FOM and GMRES generate the exact solution and stop. Brown [Bro91] shows that when FOM generates a singular H k , GMRES will <p> This is called lucky breakdown [SS86] or happy breakdown <ref> [Bro91] </ref>. It rarely occurs. * H k is singular. When H k is singular FOM cannot generate the new iterate x m . In the first case both FOM and GMRES generate the exact solution and stop. Brown [Bro91] shows that when FOM generates a singular H k , GMRES will under the same circumstances generate the same iterate again: x k = x k1 ; but does not break down. This behavior can persist for several iterations, and it is called stagnation. <p> See e.g. [EG90], [EG91]. CHAPTER 4. THEORY 54 That SOR preconditioning tends to distribute the eigenvalues of the preconditioned matrix was already noted in [Saa93]. Circular spectra can lead to pathological convergence behavior for iterative methods, particularly if the eigenvalues are evenly spaced (see for example [van89], <ref> [Bro91] </ref>).
Reference: [Cha78] <author> R. Chandra. </author> <title> Conjugate gradient methods for partial differential equations. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Yale University, </institution> <year> 1978. </year> <note> 95 BIBLIOGRAPHY 96 </note>
Reference-contexts: It is a special case of Theorem 5 of [SS86], which was taken from <ref> [Cha78] </ref>. The proof given here is simpler than the one given there.
Reference: [CS96] <author> A. Chapman and Y. Saad. </author> <title> Deflated and augmented Krylov subspace techniques. Numerical Linear Algebra with Applications, </title> <note> 1996. To appear. </note>
Reference-contexts: The properties of the Krylov subspace are determined by the "mix" of eigenvectors in the initial residual, a topic that has recently given rise to the so-called "Deflated GMRES" algorithm ([Mor95], <ref> [CS96] </ref>), which we will not discuss further here. 4.3 Form of the preconditioned matrix ^ A Given the splitting A = D L U , the SOR iteration matrix may be written H = P 1 Q, where P = 1 ! D + U .
Reference: [CSW96] <author> A. Chapman, Y. Saad, and L. Wigton. </author> <title> High order ILU preconditioners for CFD problems. </title> <type> Technical Report 96/14, </type> <institution> University of Minnesota, </institution> <year> 1996. </year>
Reference-contexts: A discussion of fill techniques can be found in [Saa95], Chapter 10. A more concise discussion can be found in the tech report <ref> [CSW96] </ref>. It is also possible to use wavefront and multicolor orderings to parallelize ILU preconditioning, and, like IC, multicolor orderings can lead to a severe degradation in the overall rate of convergence of the iterative method. We will give a few experimental results to this effect in Chapter 3. <p> This is not the case for the ILU preconditioners, for which fill elements must be introduced to improve the preconditioner. One unfortunate effect of introducing fill is that ILU preconditioners can be "unstable" ([Elm86], <ref> [CSW96] </ref>), in that there are cases for which CHAPTER 7. SUMMARY 94 introducing additional fill can actually cause the preconditioner to become less accurate. This leads to two questions: 1.
Reference: [Cha93] <author> F. Chatelin. </author> <title> Eigenvalues of Matrices. </title> <publisher> Wiley, </publisher> <year> 1993. </year>
Reference-contexts: See e.g. the discussion of Chebyshev polynomials over arbitrary domains in [Saa92] or <ref> [Cha93] </ref>. 4.6 Summary In this chapter we reviewed the well-known convergence results for GMRES and discussed the distribution of the spectrum of an SOR-preconditioned matrix for a particular class of matrices. We CHAPTER 4.
Reference: [CGO76] <author> P. Concus, G. Golub, and D. O'Leary. </author> <title> A generalized conjugate gradient method for the numerical solution of elliptic partial differential equations. </title> <editor> In J. Bunch and D. Rose, editors, </editor> <booktitle> Sparse Matrix Computations, </booktitle> <pages> pages 309-332. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: The form given in improves the quality of the preconditioner. CHAPTER 1. INTRODUCTION 4 Another approach is to use several steps of another iterative method as a subsidiary iteration. This basic idea is implicit in the paper by Concus, Golub, and O'Leary <ref> [CGO76] </ref>, where they consider CG and another iterative method, Symmetric Successive Over-Relaxation (SSOR), working together as a "generalized CG method." They cite three previous papers ([Axe74], [YHS75], [Ehr75]), all of which consider CG as an accelerator for some other iterative method, in particular, the SSOR method. <p> If A has m n distinct eigenvalues, then the CG iterates will converge in at most m iterations. Thus, a good preconditioner will reduce the condition number or will cluster the eigenvalues, thus effectively reducing the number that are distinct. In <ref> [CGO76] </ref> the authors state that as CG progresses, the effect of the extremal eigenvalue-eigenvector pairs is removed, so that the the condition number (A) is effectively reduced, giving superlinear convergence, where the rate of convergence increases with each iteration. This has been analyzed in [vv86].
Reference: [Dan91] <author> J. Dancis. </author> <title> The optimal ! is not best for the SOR method. </title> <journal> Lin. Alg. Appl., </journal> <volume> 154-156:819-845, </volume> <year> 1991. </year>
Reference-contexts: Riemann's mapping theorem states that any simple closed curve can be mapped onto the unit circle. In principle, it is thus possible to enclose the spectrum of an SOR-preconditioned matrix in an airfoil-shaped curve, with the airfoil becoming very close to the "banjo" <ref> [Dan91] </ref> or "tennis-racket" shape we actually see for ( ^ A). However, Riemann's mapping theorem does not give us the mapping, but only proves that it exists.
Reference: [DFT90] <author> E. D'Azevedo, P. Forsyth, and W. Tang. </author> <title> Ordering methods for preconditioned conjugate gradient methods applied to unstructured grids. </title> <type> Technical Report CS-90-04, </type> <institution> Computer Science Department, University of Waterloo, Waterloo, </institution> <address> Ontario, Canada, </address> <year> 1990. </year>
Reference-contexts: An important classical reordering for systems arising from partial differential equations is the red-black ordering [You71]. It decouples the underlying equations into two groups, and effectively replaces the triangular solve with two half-sized matrix-vector multiplies. However, it has been shown ([AG88], [DM89], <ref> [DFT90] </ref>, [HO90], [Ort91]) for both IC and SSOR preconditionings that the red-black ordering may cause severe degradation in the rate of convergence of the Preconditioned Conjugate Gradient algorithm as compared with the row-wise natural ordering.
Reference: [DO95] <author> M. DeLong and J. Ortega. </author> <title> SOR as a Preconditioner. </title> <journal> Applied Numerical Mathematics, </journal> <volume> 18 </volume> <pages> 431-440, </pages> <year> 1995. </year>
Reference-contexts: We also give results for Problems 2 and 3 for different values of ! and the GMRES restart parameter. We conclude by giving some results for Gauss-Seidel- and SOR-preconditioned Bi-CGSTAB for Problems 1 and 2. Many of the results in this chapter were also included in <ref> [DO95] </ref>. 3.1 Implementation All the results in this chapter are from an IBM RS/6000 Model 250. The matrix A was stored in the Compressed Sparse Row (CSR) format (see [Saa95], Section 11.5.2), and the red-black ordering was imposed explicitly.
Reference: [DO96] <author> M. DeLong and J. Ortega. </author> <title> SOR as a parallel preconditioner. In Linear and Nonlinear Conjugate Gradient-Related Methods. </title> <publisher> SIAM, </publisher> <year> 1996. </year>
Reference-contexts: All of the cases in Table 11 were compiled with the vectorizer turned on, with the exception of the CSR code: its access patterns are essentially random, and as a result the vectorized code gives 1 This is the storage scheme we used for the results in our paper <ref> [DO96] </ref>. CHAPTER 5. PARALLEL IMPLEMENTATION ISSUES 72 poorer performance than the un-vectorized code.
Reference: [Don95] <author> J. Dongarra. </author> <title> Performance of Various Computers Using Standard Linear Equations Software. </title> <type> Technical Report CS-89-95, </type> <institution> Computer Science Department, University of Tennessee, </institution> <year> 1995. </year>
Reference-contexts: PARALLEL IMPLEMENTATION ISSUES 63 coprocessor or as an application processor. The i860 has a clock speed of 50 MHz, a theoretical peak computational rate of 75 Mflops [Bok95], and has been measured at 10 Mflops on the LINPACK Benchmark <ref> [Don95] </ref>. Each processor runs OSF/1, a complete UNIX standards-compliant operating system. Of the 32 MBytes of memory on each node, about 24 Mbytes is available to the user. <p> The Model 590 is a POWER2-based [WD95] system, with a clock speed of 66.5 Mhz and a theoretical peak computation rate of 264 Mflops. It has been CHAPTER 5. PARALLEL IMPLEMENTATION ISSUES 64 measured at 130 Mflops on the LINPACK Benchmark <ref> [Don95] </ref>. Each processor runs AIX, IBM's implementation of UNIX. The nodes are connected by a multistage bidirectional interconnect (an omega network) made up of 4 fi 4 crossbar switch elements. Each node has special routing hardware.
Reference: [DGR79] <author> P. Dubois, A. Greenbaum, and G. Rodrigue. </author> <title> Approximating the inverse of a matrix for use on iterative algorithms on vector processors. </title> <journal> Computing, </journal> <volume> 22 </volume> <pages> 257-268, </pages> <year> 1979. </year>
Reference-contexts: Dubois, Greenbaum, and Rodrigue <ref> [DGR79] </ref> used a Jacobi iteration as a preconditioner. Johnson, Micchelli, and Paul [JMP83] use least-squares and min-max polynomials in conjunction with the Jacobi iteration, and give a framework for more general subsidiary iteration preconditioning with their discussion of inner and outer iterations.
Reference: [DM89] <author> I. Duff and G. Meurant. </author> <title> The Effects of Ordering on Preconditioned Conjugate Gradients. </title> <journal> BIT, </journal> <volume> 29 </volume> <pages> 635-657, </pages> <year> 1989. </year>
Reference-contexts: A different approach is to reorder the equations. An important classical reordering for systems arising from partial differential equations is the red-black ordering [You71]. It decouples the underlying equations into two groups, and effectively replaces the triangular solve with two half-sized matrix-vector multiplies. However, it has been shown ([AG88], <ref> [DM89] </ref>, [DFT90], [HO90], [Ort91]) for both IC and SSOR preconditionings that the red-black ordering may cause severe degradation in the rate of convergence of the Preconditioned Conjugate Gradient algorithm as compared with the row-wise natural ordering.
Reference: [Ehr75] <author> L. Ehrlich. </author> <title> On some experience using matrix splitting and conjugate gradient. </title> <booktitle> In SIAM Fall Meeting, </booktitle> <year> 1975. </year>
Reference-contexts: This basic idea is implicit in the paper by Concus, Golub, and O'Leary [CGO76], where they consider CG and another iterative method, Symmetric Successive Over-Relaxation (SSOR), working together as a "generalized CG method." They cite three previous papers ([Axe74], [YHS75], <ref> [Ehr75] </ref>), all of which consider CG as an accelerator for some other iterative method, in particular, the SSOR method. Their formulation is equivalent to preconditioning CG with a single SSOR step.
Reference: [Eis81] <author> S. Eisenstat. </author> <title> Efficient Implementation of a Class of Conjugate Gradient Methods. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 2 </volume> <pages> 1-4, </pages> <year> 1981. </year>
Reference-contexts: For ILU (0) preconditioning, if A = D L U , the preconditioned matrix is ~ A = (D L) 1 A = I (D L) 1 U: (107) Eisenstat <ref> [Eis81] </ref> notes that by working directly with ~ A, the amount of work required to carry out the matrix-vector multiply ~ Ar is barely more than for Ar itself; that is, the preconditioning is essentially free. <p> Unfortunately, for additional steps the savings does not persist: each successive step requires both a forward solve and a multiplication by U . For this reason, we have not implemented the special one-step savings for the results of Table 1, nor have we used the Eisenstat modification <ref> [Eis81] </ref> for ILU in Table 1, since our intent is simply to show its behavior in the two orderings. Finally, Table 1 shows that improvements can be expected for larger numbers of Gauss-Seidel iterations and Arnoldi vectors. This is consistent with Saad's observations [Saa94] for the number of Gauss-Seidel iterations.
Reference: [Elm82] <author> H. Elman. </author> <title> Iterative Methods for large, sparse, nonsymmetric systems of linear equations. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University, </institution> <year> 1982. </year> <note> BIBLIOGRAPHY 97 </note>
Reference-contexts: See [NRT92] for examples comparing GMRES and CGS. Van der Vorst originally chose the constants ! i in (106) so that ~ i is equivalent to the polynomial generated by the Minimal Residual (MR) <ref> [Elm82] </ref> (or equivalently, GMRES (1)) method. Incorporating (l1) additional Arnoldi vectors into the method used to generate ~ i gives the Bi-CGSTAB (l) methods ([SF93], [SvF94]), which for moderate values of l can be more accurate than Bi-CGSTAB and give smoother convergence behavior. <p> We will not discuss their results here, but we will discuss analogous results for nonsymmetric methods. 47 CHAPTER 4. THEORY 48 4.2 Convergence of GMRES When GMRES is used to solve nonsymmetric problems, (109) does not hold. Instead we have three results, two due to Elman <ref> [Elm82] </ref> and one due to Trefethen [Tre90]. <p> Elman <ref> [Elm82] </ref> originally gave these two bounds for the computed residuals for another iterative method, the Generalized Conjugate Residual (GCR) method, but they can be applied to GMRES as well because of its optimality property [SS86]. <p> For a proof, see <ref> [Elm82] </ref>, page 40. This result is not very useful in practice: in general it is difficult to know whether A S is positive-definite. Moreover, this estimate can be very pessimistic. <p> M k = min max jq k ()j (114) If A is normal (AA T = A T A), then kr k k 2 M k kr 0 k 2 . CHAPTER 4. THEORY 49 For a proof, see <ref> [Elm82] </ref>, page 42. In general Theorem 2 gives a tighter bound for the residual norm than does Theorem 1, and it applies for a larger class of problems. We will return to this bound later in the chapter, with a more precise expression for M k for SOR-preconditioned matrices. <p> symmetric part of ^ A, ^ A S 1 2 ( ^ A+ ^ A T ), is positive-definite, then Theorem 1 gives kr k k 2 1 min ( ^ A S ) ! k kr 0 k 2 : (129) Following the proof of Theorem 1 given in <ref> [Elm82] </ref> we can also give a related bound for the reduction of the residual norm for a given step k: kr k k 2 u t 1 min ( ^ A S ) ! or kr k1 k 2 v u 2 max ( ^ A T ^ A) : (131)
Reference: [Elm86] <author> H. Elman. </author> <title> A stability analysis of incomplete LU factorizations. </title> <journal> Math. Comp., </journal> <volume> 47(175) </volume> <pages> 191-217, </pages> <year> 1986. </year>
Reference: [EG90] <author> H. Elman and G. Golub. </author> <title> Iterative methods for cyclically reduced non-self-adjoint linear systems. </title> <journal> Math. Comp., </journal> <volume> 54(190) </volume> <pages> 671-700, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: We found ! opt : = 1:5062 using Matlab 3 . 3 We do not know the value of the optimal ! analytically. See e.g. <ref> [EG90] </ref>, [EG91]. CHAPTER 4. THEORY 54 That SOR preconditioning tends to distribute the eigenvalues of the preconditioned matrix was already noted in [Saa93]. Circular spectra can lead to pathological convergence behavior for iterative methods, particularly if the eigenvalues are evenly spaced (see for example [van89], [Bro91]).
Reference: [EG91] <author> H. Elman and G. Golub. </author> <title> Iterative methods for cyclically reduced non-self-adjoint linear systems, II. </title> <journal> Math. Comp., </journal> <volume> 56(193) </volume> <pages> 215-242, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: We found ! opt : = 1:5062 using Matlab 3 . 3 We do not know the value of the optimal ! analytically. See e.g. [EG90], <ref> [EG91] </ref>. CHAPTER 4. THEORY 54 That SOR preconditioning tends to distribute the eigenvalues of the preconditioned matrix was already noted in [Saa93]. Circular spectra can lead to pathological convergence behavior for iterative methods, particularly if the eigenvalues are evenly spaced (see for example [van89], [Bro91]).
Reference: [Eva73] <author> D. Evans. </author> <title> The analysis and application of spare matrix algorithms in the finite element method. </title> <booktitle> In The Mathematics of Finite Elements and Applications, </booktitle> <pages> pages 427-447. </pages> <publisher> Academic Press, </publisher> <year> 1973. </year>
Reference-contexts: The term preconditioning is generally credited to Turing [Tur48], and while the idea of preconditioning the CG algorithm was first presented by Hestenes [Hes56], the term and the idea were not brought together until the papers by Evans <ref> [Eva73] </ref> and Axelsson [Axe74]. It is not necessary to form S or SAS T explicitly; instead, the preconditioning is usually incorporated into the CG algorithm. This is shown in Figure 2, where M = (S T S) 1 .
Reference: [FF91] <author> B. Fischer and R. Freund. </author> <title> Chebyshev polynomials are not always optimal. </title> <journal> J. Approx. Theory, </journal> <volume> 65 </volume> <pages> 261-272, </pages> <year> 1991. </year>
Reference-contexts: For ellipses, Chebyshev polynomials are not guaranteed to be optimal <ref> [FF91] </ref>, but they do give an upper bound. Saad [Saa95] shows that they are asymptotically optimal. Riemann's mapping theorem states that any simple closed curve can be mapped onto the unit circle.
Reference: [Fle76] <author> R. Fletcher. </author> <title> Conjugate gradient methods for indefinite systems. </title> <editor> In G. A. Watson, editor, </editor> <booktitle> Lecture Notes in Math 506: Proceedings of the Dundee Biennial Conference on Numerical Analysis, </booktitle> <pages> pages 73-89. </pages> <publisher> Springer-Verlag, </publisher> <year> 1976. </year>
Reference-contexts: End. revived by Fletcher <ref> [Fle76] </ref> to solve symmetric indefinite systems. Saad later considered the method for solving nonsymmetric systems [Saa82], and discussed conditions under which it was feasible.
Reference: [Fra94] <author> H. Franke. </author> <title> MPI-F: An MPI Implementation for IBM SP-1/SP-2. </title> <type> Technical report, </type> <institution> IBM T. J. Watson Research Center, </institution> <year> 1994. </year>
Reference-contexts: The Paragons and SP2s at all three sites (CACR, LaRC, and NAS) have vendor-supported implementations of MPI. IBM recently integrated MPI into their Parallel Environment for AIX [IBM95]. Their first implementation of MPI, called MPI-F <ref> [Fra94] </ref>, used two internal protocols: eager and rendezvous. In the eager protocol, messages are forced through the network. Messages that arrive before their matching receive is posted have to be buffered by the receiver. <p> We used a blocking send-receive pair to estimate the bandwidth delivered by the SP2 under MPI. Our messages varied in length from 128 bytes to 8096 bytes in increments of 128 bytes, in an experiment closely modeled after the one used by Franke <ref> [Fra94] </ref> and reported in his Figure 3. The results from three different runs of our experiment are shown in Figure 31, showing behavior identical to that of MPI-F reported in [Fra94], and leading us to conclude that the protocols used in [IBM95] are the same used in [Fra94]. <p> bytes to 8096 bytes in increments of 128 bytes, in an experiment closely modeled after the one used by Franke <ref> [Fra94] </ref> and reported in his Figure 3. The results from three different runs of our experiment are shown in Figure 31, showing behavior identical to that of MPI-F reported in [Fra94], and leading us to conclude that the protocols used in [IBM95] are the same used in [Fra94]. The change of protocol only comes into play for our larger CHAPTER 5. PARALLEL IMPLEMENTATION ISSUES 67 problems, where the number of gridpoints per line exceeds 1024. <p> used by Franke <ref> [Fra94] </ref> and reported in his Figure 3. The results from three different runs of our experiment are shown in Figure 31, showing behavior identical to that of MPI-F reported in [Fra94], and leading us to conclude that the protocols used in [IBM95] are the same used in [Fra94]. The change of protocol only comes into play for our larger CHAPTER 5. PARALLEL IMPLEMENTATION ISSUES 67 problems, where the number of gridpoints per line exceeds 1024.
Reference: [Gd90] <author> G. Golub and J. de Pillis. </author> <title> Toward an effective two-parameter SOR method. </title> <editor> In D. Kin-caid and L. Hayes, editors, </editor> <title> Iterative Methods for Large Linear Systems. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: In addition, there are unstructured problems, such as may arise from adaptive mesh refinement, whose graphs can be colored sub-optimally using a greedy algorithm [Saa94], but it is unknown whether SOR preconditioning would exhibit good convergence properties as a preconditioner for problems like these. * Golub and de Pillis <ref> [Gd90] </ref> have also shown that in the red-black ordering, the transient convergence behavior of SOR, which would be more important for the use of SOR as a preconditioner, can be improved by using two different ! parameters: one for the red points, another for the black points.
Reference: [GO93] <author> G. Golub and J. Ortega. </author> <title> Scientific Computing: An Introduction with Parallel Computing. </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1993. </year>
Reference-contexts: It is well-known (see e.g. <ref> [GO93] </ref>) that the CG algorithm has three desirable properties: * It is guaranteed to converge in at most n iterations. <p> It can be shown (see e.g. <ref> [GO93] </ref>, Chapter 9) that the residuals r i generated by the CG method are orthogonal, while the direction vectors p i are A-conjugate. 2.1.3 Norms A norm is a real-valued function over R n denoted by k k, having these three properties: CHAPTER 2. <p> This makes it an attractive candidate as a parallel preconditioner for a nonsymmetric method like GMRES. 2.7.1 The Gauss-Seidel and SOR iterations The Gauss-Seidel (GS) method (see e.g. <ref> [GO93] </ref>) for solving Ax = b is x k+1 a ii (b i j&lt;i a ij x k+1 P j ) (67) Given the splitting A = D L U (68) where D is the diagonal part of A, L is the strictly lower-diagonal part of A, and U is the <p> Consider the splitting A = P Q such that P is nonsingular and H = P 1 Q has (H) &lt; 1. Then (I P 1 Q) 1 = k=0 Since P 1 A = P 1 (P Q) = I P 1 Q this gives Theorem 9.2.1 of <ref> [GO93] </ref>: A 1 = (I P 1 Q) 1 P 1 = k=0 ! Truncating this series to k terms gives an approximation to A 1 : A 1 : k = (I + H + + H k1 )P 1 : (90) This suggests that M k can be used <p> section we give the traditional definition of speedup, follow with a brief discussion of scaled speedup, and conclude by introducing the the scaled speedup we will use to evaluate our experimental results in Chapter 6. 5.7.1 Speedup The classical definition of the speedup of a parallel program is (see e.g. <ref> [GO93] </ref>): S P = Execution time on a single processor Execution time using P processors : (151) Efficiency is closely related: E P = P The classical theoretical limit to speedup using the definition (151) is described by Amdahl's law [Amd67]. Another definition of speedup is [GO93]: S 0 Execution time <p> program is (see e.g. <ref> [GO93] </ref>): S P = Execution time on a single processor Execution time using P processors : (151) Efficiency is closely related: E P = P The classical theoretical limit to speedup using the definition (151) is described by Amdahl's law [Amd67]. Another definition of speedup is [GO93]: S 0 Execution time of the best serial algorithm on a single processor Execution time of the parallel algorithm using P processors (153) We will focus our discussion on a scaled variant of (151). 5.7.2 Scaled Speedup Measuring parallel performance by solving the same problem using an increasing number of
Reference: [Gre86] <author> A. Greenbaum. </author> <title> Solving triangular linear systems using FORTRAN with parallel extensions on the NYU Ultracomputer prototype. </title> <type> Technical Report 99, </type> <institution> Courant Institute, </institution> <address> New York University, New York, </address> <year> 1986. </year>
Reference-contexts: One problem that arises when using IC or SSOR on a parallel computer is that they require the solution of large sparse triangular systems of equations, making them difficult to carry out in parallel. It is possible to extract some parallelism in the triangular solve using wavefront orderings ([AG88], <ref> [Gre86] </ref>, [Sal90]), and this approach is somewhat effective for vector computers. But Stotland [Sto93] has shown that the wavefront algorithm computation is completely swamped by communication on distributed-memory multicomputers such as the Intel iPSC/860. A different approach is to reorder the equations.
Reference: [GPS96] <author> A. Greenbaum, V. Ptak, and Z. Strakos. </author> <title> Any nonincreasing convergence curve is possible for GMRES. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 17(3) </volume> <pages> 465-469, </pages> <year> 1996. </year>
Reference-contexts: The crucial properties of preconditioners for nonsymmetric CG-type methods are not as well-understood as they are for the CG method: there is no need to maintain symmetry; furthermore, eigenvalue information alone for the preconditioned system may not be sufficient (see e.g. [GS94a], <ref> [GPS96] </ref>). In addition, characteristics of a problem that may be good for one nonsymmetric CG-type method may be bad for another [NRT92]. It is difficult to know a priori what will make a good preconditioner for using a given solver to solve a particular problem or class of problems. <p> CHAPTER 4. THEORY 50 and from the optimality of the GMRES polynomial the result (115) follows. This bound removes the Jordan condition number from the residual norm estimate. For cases where A is highly non-normal, eigenvalue and initial residual information are not enough to describe convergence behavior <ref> [GPS96] </ref>. We will not be discussing cases where the A is highly non-normal. 4.2.2 "Superlinear" convergence The above convergence results do not capture the so-called superlinear convergence behavior of GMRES. At step k GMRES produces a k fi k upper-Hessenberg matrix H k .
Reference: [GS94a] <author> A. Greenbaum and Z. Strakos. </author> <title> Matrices that generate the same Krylov residual spaces. </title> <editor> In G. Golub, M. Luskin, and A. Greenbaum, editors, </editor> <booktitle> Recent Advances in Iterative Methods, </booktitle> <pages> pages 95-118. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: The crucial properties of preconditioners for nonsymmetric CG-type methods are not as well-understood as they are for the CG method: there is no need to maintain symmetry; furthermore, eigenvalue information alone for the preconditioned system may not be sufficient (see e.g. <ref> [GS94a] </ref>, [GPS96]). In addition, characteristics of a problem that may be good for one nonsymmetric CG-type method may be bad for another [NRT92]. It is difficult to know a priori what will make a good preconditioner for using a given solver to solve a particular problem or class of problems.
Reference: [GL] <author> W. Gropp and E. Lusk. </author> <title> User's Guide for mpich, a Portable Implementation of MPI. </title> <institution> Argonne National Laboratory Mathematics and Computer Science Division. </institution> <address> BIBLIOGRAPHY 98 </address>
Reference-contexts: It can be used with either Fortran or C. MPI is implemented in one of two ways: by the vendor, or by using the portable implementation MPICH <ref> [GL] </ref>. MPICH has been implemented on several different platforms using Abstract Device Interfaces (ADIs) [GL94], which serve as an interface to the native message-passing libraries. Vendor implementations of MPI do not use ADIs, and the implementation details are usually not made public.
Reference: [GL94] <author> W. Gropp and E. Lusk. </author> <title> An abstract device definition to support the implementation of a high-level point-to-point message-passing interface. </title> <type> Technical Report MCS-P342-119, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year> <type> preprint. </type>
Reference-contexts: It can be used with either Fortran or C. MPI is implemented in one of two ways: by the vendor, or by using the portable implementation MPICH [GL]. MPICH has been implemented on several different platforms using Abstract Device Interfaces (ADIs) <ref> [GL94] </ref>, which serve as an interface to the native message-passing libraries. Vendor implementations of MPI do not use ADIs, and the implementation details are usually not made public. The Paragons and SP2s at all three sites (CACR, LaRC, and NAS) have vendor-supported implementations of MPI.
Reference: [Gus] <author> J. Gustafson. </author> <title> Fixed time, tiered memory, and superlinear speedup. </title> <address> Webpage. http://www.scl.ameslab.gov/Publications/Superlinear/Superlinear.html. </address>
Reference-contexts: These definitions have grown more complex as the authors have chosen different definitions of the hypothetical uniprocessor machine capable of solving the problem being solved a given parallel machine. An overview of these approaches may be found in <ref> [Gus] </ref>.
Reference: [Gus78] <author> I. Gustafsson. </author> <title> A class of first order factorization methods. </title> <journal> BIT, </journal> <volume> 18 </volume> <pages> 142-156, </pages> <year> 1978. </year>
Reference-contexts: No-fill, or ILU (0), preconditioning (Figure 4) retains the sparsity pattern of A. As is the case with IC preconditioning, it is possible to improve the quality of the preconditioner by adding some kind of diagonal compensation (the so-called modified factorizations) <ref> [Gus78] </ref>, or by allowing fill elements, according to their level, profile, or magnitude. A discussion of fill techniques can be found in [Saa95], Chapter 10. A more concise discussion can be found in the tech report [CSW96].
Reference: [Gus88] <author> J. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Comm. ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <year> 1988. </year>
Reference-contexts: Gustafson <ref> [Gus88] </ref> and others ([GMB88], [SG91], [SN90], [Hoc87], [GS94b]) have developed an increasingly complex definition of speedup that stems from this basic idea of scaling the amount of work with the number of processors, while keeping the amount of work per processor fixed.
Reference: [GMB88] <author> J. Gustafson, G. Monty, and R. Benner. </author> <title> Development of Parallel Methods for a 1024-Processor Hypercube. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9(4) </volume> <pages> 609-638, </pages> <year> 1988. </year>
Reference: [GS94b] <author> J. Gustafson and Q. Snell. Hint: </author> <title> A new way to measure computer performance. </title> <type> Technical report, </type> <institution> Ames Laboratory, </institution> <year> 1994. </year>
Reference-contexts: Gustafson [Gus88] and others ([GMB88], [SG91], [SN90], [Hoc87], <ref> [GS94b] </ref>) have developed an increasingly complex definition of speedup that stems from this basic idea of scaling the amount of work with the number of processors, while keeping the amount of work per processor fixed.
Reference: [HO90] <author> D. Harrar and J. Ortega. </author> <title> Solution of three-dimensional generalized Poisson equations on vector computers. </title> <editor> In D. Kincaid and L. Hayes, editors, </editor> <title> Iterative Methods for Large Linear Systems. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: An important classical reordering for systems arising from partial differential equations is the red-black ordering [You71]. It decouples the underlying equations into two groups, and effectively replaces the triangular solve with two half-sized matrix-vector multiplies. However, it has been shown ([AG88], [DM89], [DFT90], <ref> [HO90] </ref>, [Ort91]) for both IC and SSOR preconditionings that the red-black ordering may cause severe degradation in the rate of convergence of the Preconditioned Conjugate Gradient algorithm as compared with the row-wise natural ordering.
Reference: [HM89] <author> D. Helmbold and C. McDowell. </author> <title> Modeling speedup(n) greater than n. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <volume> volume 3, </volume> <pages> pages 219-225, </pages> <year> 1989. </year>
Reference-contexts: We measure time on each processor using the MPI function mpi wtime (), which returns the wall-clock time, by calling it before and after each timed section and taking the difference. To find 1 More precisely, this is a scaled variant of what Helmbold and McDowell <ref> [HM89] </ref> would describe as linear super-unitary speedup, although not in the strictest sense, as the efficiency here is not always strictly greater than one for the matrix-vector multiply or the preconditioner. CHAPTER 6.
Reference: [Hes56] <author> M. Hestenes. </author> <title> The conjugate gradient method for solving linear systems. </title> <booktitle> In Proc. 6th Symp. Appl. Math., </booktitle> <pages> pages 83-102. </pages> <publisher> McGraw-Hill, </publisher> <year> 1956. </year>
Reference-contexts: The term preconditioning is generally credited to Turing [Tur48], and while the idea of preconditioning the CG algorithm was first presented by Hestenes <ref> [Hes56] </ref>, the term and the idea were not brought together until the papers by Evans [Eva73] and Axelsson [Axe74]. It is not necessary to form S or SAS T explicitly; instead, the preconditioning is usually incorporated into the CG algorithm.
Reference: [HS52] <author> M. Hestenes and E. </author> <title> Stiefel. Method of conjugate gradients for solving linear systems. </title> <journal> J. Res. Natl. Bur. Standards, </journal> <volume> 49(6) </volume> <pages> 409-436, </pages> <month> December </month> <year> 1952. </year>
Reference-contexts: One such iterative method is the Conjugate Gradient (CG) method. 1.1 The Conjugate Gradient (CG) Method If A is real and symmetric (A T = A) and positive definite (x T Ax &gt; 0; for any nonzero vector x), then (1) can be solved using the Conjugate Gradient (CG) method <ref> [HS52] </ref>, shown in Figure 1. In this figure, and henceforth, we use Greek letters for scalars (e.g. ff and fi), upper-case letters for 1 CHAPTER 1. INTRODUCTION 2 1. Choose x 0 . Set p 0 = r 0 = b Ax 0 . 2.
Reference: [Hoc87] <author> R. Hockney. </author> <title> Characterizing computers and optimizing the FACR(1) Poisson-solver on parallel unicomputers. </title> <journal> Parallel Comput., </journal> <volume> 5 </volume> <pages> 97-103, </pages> <year> 1987. </year>
Reference-contexts: Gustafson [Gus88] and others ([GMB88], [SG91], [SN90], <ref> [Hoc87] </ref>, [GS94b]) have developed an increasingly complex definition of speedup that stems from this basic idea of scaling the amount of work with the number of processors, while keeping the amount of work per processor fixed.
Reference: [HJ91] <author> R. Horn and C. Johnson. </author> <title> Topics in Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: the residual norm for a given step k: kr k k 2 u t 1 min ( ^ A S ) ! or kr k1 k 2 v u 2 max ( ^ A T ^ A) : (131) If ^ A were normal, then according to Horn and Johnson <ref> [HJ91] </ref> min ( ^ A S ) is just the real part of min ( ^ A) = 1 and max ( ^ A T ^ A) = 1.
Reference: [HST95] <author> S. Hutchinson, J. Shadid, and R. Tuminaro. </author> <note> Aztec User's Guide, Version 1.0. </note> <institution> Sandia National Laboratories, </institution> <year> 1995. </year>
Reference-contexts: This behavior is magnified in the case shown in Figure 19 (b), where the problem is the same, except that = t = 32. Here 1 These results, as well as the BiCG-STAB results given in the next section, are from runs using Hutchinson, Shadid, and Tuminaro's AZTEC code <ref> [HST95] </ref>. CHAPTER 2. <p> NAMELISTs are not part of the ANSI standard for FORTRAN 77, but are a common and useful extension for changing program parameters without recompiling. It would normally be preferable to use command-line arguments: the AZTEC <ref> [HST95] </ref> code, for example, uses them instead. However, in our case we used a large number of parameters that changed very little from run to run, and using NAMELISTs allowed us to keep track of a large number of parameters without having to parse them. <p> Are there any problems for which an ILU preconditioner will become unstable, but an SOR preconditioner does not? * Experimental results not included here suggested that SOR preconditioning with a nominal ! can be quite competitive to the Neumann and least-squares polynomial preconditioners included in the AZTEC package <ref> [HST95] </ref>. It is an open question under what conditions this is true. * Despite the fact that SSOR as a stand-alone iteration often converges more slowly than SOR (see [You71], p. 462, [BBC + 94], p.12), it is sometimes considered as a preconditioner for nonsymmetric matrices ([Ton92], [BBC + 94]).
Reference: [IBM95] <author> IBM. </author> <title> IBM Parallel Environment for AIX: MPI Programming and Subroutine Reference Version 2, </title> <type> Release 1, </type> <year> 1995. </year>
Reference-contexts: Vendor implementations of MPI do not use ADIs, and the implementation details are usually not made public. The Paragons and SP2s at all three sites (CACR, LaRC, and NAS) have vendor-supported implementations of MPI. IBM recently integrated MPI into their Parallel Environment for AIX <ref> [IBM95] </ref>. Their first implementation of MPI, called MPI-F [Fra94], used two internal protocols: eager and rendezvous. In the eager protocol, messages are forced through the network. Messages that arrive before their matching receive is posted have to be buffered by the receiver. <p> When the ack is received the message is delivered. This means that no intermediate buffering is required for early arrivals, but at the cost of an extra round trip for the request-to-send message. In the documentation for the new implementation of MPI <ref> [IBM95] </ref> there is no mention of these two protocols. We used a blocking send-receive pair to estimate the bandwidth delivered by the SP2 under MPI. <p> The results from three different runs of our experiment are shown in Figure 31, showing behavior identical to that of MPI-F reported in [Fra94], and leading us to conclude that the protocols used in <ref> [IBM95] </ref> are the same used in [Fra94]. The change of protocol only comes into play for our larger CHAPTER 5. PARALLEL IMPLEMENTATION ISSUES 67 problems, where the number of gridpoints per line exceeds 1024.
Reference: [Int92] <author> Intel Corporation. </author> <title> i860 Microprocessor Family Programmer's Reference Manual, 1992. BIBLIOGRAPHY 99 </title>
Reference-contexts: The Paragon has no vector units, but the compiler uses three code transformation processes nested loop transformations, streaming and stripmining, and vector idiom recognition, to allow the programmer to "use the data cache as if it were a large block of vector registers" <ref> [Int92] </ref>. To do this the compiler replaces blocks of user code with calls to hand-coded routines provided by Intel. These optimizations can have a substantial impact on performance. The SP2 compiler xlf/xlf90 implements IBM AIX XL FORTRAN, which includes ANSI standard FORTRAN 90 plus some extensions.
Reference: [Int93] <author> Intel Corporation. </author> <title> Minimizing Virtual Memory Paging, </title> <year> 1993. </year>
Reference-contexts: The Paragon parallel execution facility pexec allows a user to lock pages in memory using its -plk flag, and in <ref> [Int93] </ref> the authors suggest using it to force the operating system to swap its own pages rather than those of the user application. This flag locks the application's data segment and any ALLOCATABLE variables that may be on the stack or heap. <p> Initially we had considerable trouble getting repeatable timings on the Paragon, but by running the timed portion of the code twice, timing the second run, and locking pages, we were able to get timing results that were repeatable. In <ref> [Int93] </ref> the authors also recommend using dynamic storage allocation for large arrays to minimize paging, claiming that doing so can reduce execution time by a factor of ten on one node, and up to a factor of fifteen on eight nodes. <p> For the Miscellaneous code, using dynamic storage allocation gives an improvement that is on the order of the factor of ten promised in <ref> [Int93] </ref>, and as a result we use dynamic storage allocation for the large arrays for all of the results given in the next chapter. 5.5 Data structures For our parallel experiments we wanted a data structure that would give good locality of reference and that would be easy to partition and
Reference: [Int95a] <author> Intel. </author> <title> Intel Paragon T M Supercomputer Product Brochure. Intel Corporation web page, </title> <note> 1995. http://www.ssd.intel.com/paragon.html. </note>
Reference-contexts: Fortunately there are several ways to minimize virtual memory paging, which we will discuss in Section 5.4. OSF/1 also provides message-passing primitives for both synchronous and asynchronous communication. Shirley, et. al. [SRS95] discuss these in detail. While the interconnection network is touted as "appearing fully connected," <ref> [Int95a] </ref>, it is actually a two-dimensional mesh, with "row-column" or "X-Y" routing, in which messages are sent first along the source node's row and then along the destination node's column. The Paragon uses wormhole routing: cut-through routing in which messages are not buffered at intermediate nodes.
Reference: [Int95b] <author> Intel Corporation. </author> <title> Paragon System Fortran Compiler User's Guide, </title> <month> April </month> <year> 1995. </year>
Reference-contexts: The Paragon Fortran compiler, if77, implements an enhanced version of Fortran 77 that conforms to the ANSI standard for FORTRAN 77, but includes extensions from VAX/VMS Fortran, IBM/VS Fortran, and MIL-STD-1753 [Int95c]. The optimizations it provides includes what the designers CHAPTER 5. PARALLEL IMPLEMENTATION ISSUES 65 call "vectorization" <ref> [Int95b] </ref>. The Paragon has no vector units, but the compiler uses three code transformation processes nested loop transformations, streaming and stripmining, and vector idiom recognition, to allow the programmer to "use the data cache as if it were a large block of vector registers" [Int92]. <p> This is also true for the Paragon. Table 11 shows results taken from runs on a single node. The CSR 2D-NAT 2D-RB 2D-S1 GS .0678 .0604 .0306 .0281 Table 11: Average times on the Paragon streaming supported by the Paragon FORTRAN compiler <ref> [Int95b] </ref> makes a substantial difference in the results: the two-dimensional storage scheme is considerably better in the red-black ordering than it is in the natural ordering.
Reference: [Int95c] <author> Intel Corporation. </author> <title> Paragon System Fortran Language Reference Manual, </title> <month> May </month> <year> 1995. </year>
Reference-contexts: In the next section we will discuss the message-passing library. The Paragon Fortran compiler, if77, implements an enhanced version of Fortran 77 that conforms to the ANSI standard for FORTRAN 77, but includes extensions from VAX/VMS Fortran, IBM/VS Fortran, and MIL-STD-1753 <ref> [Int95c] </ref>. The optimizations it provides includes what the designers CHAPTER 5. PARALLEL IMPLEMENTATION ISSUES 65 call "vectorization" [Int95b].
Reference: [JMP83] <author> O. Johnson, C. Micchelli, and G. Paul. </author> <title> Polynomial preconditioners for conjugate gradient calculations. </title> <journal> SIAM J. Numer. Anal, </journal> <volume> 20 </volume> <pages> 362-376, </pages> <year> 1983. </year>
Reference-contexts: Dubois, Greenbaum, and Rodrigue [DGR79] used a Jacobi iteration as a preconditioner. Johnson, Micchelli, and Paul <ref> [JMP83] </ref> use least-squares and min-max polynomials in conjunction with the Jacobi iteration, and give a framework for more general subsidiary iteration preconditioning with their discussion of inner and outer iterations. Adams [Ada83], [Ada85] used m-step SSOR preconditioning and also discussed the more general case of a polynomial SSOR preconditioner.
Reference: [Jou94] <author> W. Joubert. </author> <title> On the Convergence Behavior of the Restarted GMRES Algorithm for Solving Nonsymmetric Linear Systems. </title> <booktitle> Numerical Linear Algebra with Applications, </booktitle> <pages> pages 427-439, </pages> <year> 1994. </year>
Reference-contexts: GMRES (m) is not guaranteed to converge in n steps, but will still converge if m is chosen large enough. Saad and Schultz [SS86] give conditions under which GMRES (m) will converge. Joubert <ref> [Jou94] </ref> has introduced a strategy for choosing the restart frequency adaptively. At each iteration the method produces two estimates of the residual norm for the next iteration: one with restarting, one without, and chooses to restart if restarting is expected to be more efficient.
Reference: [Kat76] <author> T. Kato. </author> <title> Perturbation theory for linear operators. </title> <publisher> Springer-Verlag, </publisher> <year> 1976. </year>
Reference-contexts: Then f (A) is defined by the Dunford-Taylor integral (see e.g. <ref> [Kat76] </ref> p. 44): 1 Z f (z)(zI A) 1 dz (116) which is the analogue of the Cauchy integral formula in function theory. Let p k (z) be some polynomial in P k . It is analytic in D, where fl * D.
Reference: [KRYG82] <author> D. Kincaid, J. Respess, D. Young, and R. Grimes. </author> <title> ITPACK 2C: A Fortran package for solving large sparse linear systems by adaptive accelerated iterative methods. </title> <journal> ACM Trans. Math. Soft, </journal> <volume> 8 </volume> <pages> 302-322, </pages> <year> 1982. </year> <title> Algorithm 586. </title>
Reference-contexts: The analysis done in Chapter 4 also suggests a simple relationship between ! and the residual norm ratios. It may be possible to exploit this relationship, and use it to chose either the GMRES restart parameter or the relaxation parameter adaptively. The scheme included in ITPACK 2C <ref> [KRYG82] </ref> uses a similar approach to estimate ! opt for SOR as a stand-alone iteration. * We have given results for a structured problem whose graph can be colored using only two colors.
Reference: [Lan52] <author> C. </author> <title> Lanczos. Solution of systems of linear equations by minimized iterations. </title> <journal> J. Res. Natl. Bur. Standards, </journal> <volume> 49(1) </volume> <pages> 33-53, </pages> <month> July </month> <year> 1952. </year>
Reference-contexts: As a result the CG method is generally not applicable for matrices that are nonsymmetric or not positive definite. An attempt to overcome this problem leads to the BCG algorithm. 2.9.2 The Bi-Conjugate Gradient (BCG) algorithm The Bi-Conjugate Gradient algorithm (Figure 17) was originally introduced by Lanczos <ref> [Lan52] </ref> and 1. x 0 is an initial guess; r 0 = b Ax 0 2. ^ r 0 is an arbitrary vector such that ( ^ r 0 ; r 0 ) 6= 0 e.g. ^ r 0 = r 0 3. 0 = 1; ^ p 0 = p
Reference: [MMRW94] <author> A. Maccabe, K. McCurley, R. Riesen, and S. Wheat. </author> <title> SUNMOS for the Intel Paragon: </title>
Reference-contexts: Each GP node consists of two Intel i860 processors, one dedicated to computation, the other dedicated to communication. The two processors share a single memory, but detailed explanations of exactly how the two processors work together have not been made public <ref> [MMRW94] </ref>. Some Paragons also have Multiprocessor (MP) nodes, which have three i860s two to execute application code, and a third that can be used as either a communication 62 CHAPTER 5. PARALLEL IMPLEMENTATION ISSUES 63 coprocessor or as an application processor.
References-found: 62

