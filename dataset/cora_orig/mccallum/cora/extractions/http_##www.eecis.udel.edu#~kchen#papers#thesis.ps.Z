URL: http://www.eecis.udel.edu/~kchen/papers/thesis.ps.Z
Refering-URL: http://www.eecis.udel.edu/~kchen/
Root-URL: http://www.eecis.udel.edu
Title: 3D FACIAL MOTION ANALYSIS AND TRANSMISSION THROUGH INTERNET  
Author: by Kai Chen Kai Chen 
Degree: A thesis submitted to the Faculty of the University of Delaware in partial fulfillment of the requirements for the degree of Master of Science in Computer Science  All Rights Reserved  
Note: c 1998  
Date: Spring 1998  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Pete Doenges and Ganesh Rajan, </author> <title> SNHC Frequently Asked Questions (version 1.0), </title> <address> ISO/IEC JTC1/SC29/WG11/N1669m3, </address> <year> 1997. </year>
Reference-contexts: To our knowledge, this is the first reported work on extraction of FAPs using 3D facial data. Recent research on face model is coupled with MPEG-4 SNHC. SNHC stands 3 for Synthetic/Natural Hybrid Coding. MPEG-4 is the next generation coding stan-dard of time-varying digital media consisting of audio-visual objects <ref> [1] </ref>. SNHC deals with the representation and coding of synthetically and naturally generated audiovisual information. <p> points based on 3D geometric hints. 14 Chapter 3 FACIAL ANIMATION PARAMETER EXTRACTION In this chapter we focus on the extraction of Facial Animation Parameters (FAPs) from facial feature points motion. 3.1 Introduction 3.1.1 MPEG-4 SHNC FAP MPEG-4 is a new standard for coding digital media consisting of audiovisual objects <ref> [1] </ref>. SNHC stands for Synthetic/Natural Hybrid Coding. It deals with representation and coding of synthetically and naturally generated audio-visual information [1]. This reflects an aspect of MPEG-4 that combines synthetic and natural media types. An important issue in SNHC is defining a synthetic virtual human. <p> extraction of Facial Animation Parameters (FAPs) from facial feature points motion. 3.1 Introduction 3.1.1 MPEG-4 SHNC FAP MPEG-4 is a new standard for coding digital media consisting of audiovisual objects <ref> [1] </ref>. SNHC stands for Synthetic/Natural Hybrid Coding. It deals with representation and coding of synthetically and naturally generated audio-visual information [1]. This reflects an aspect of MPEG-4 that combines synthetic and natural media types. An important issue in SNHC is defining a synthetic virtual human. <p> An important issue in SNHC is defining a synthetic virtual human. To this end, SNHC standardize Face & Body Animation (FBA) in which there are two sets of parameters, the Facial Animation Parameters (FAPs) and the Facial Definition Parameters (FDPs) <ref> [1] </ref>. FDPs are used to customize the initial or baseline face model of the rendering software to a particular face or to download a face model along with the information about how to animate it [1]. FAPs are used to describe the animation of a face. <p> two sets of parameters, the Facial Animation Parameters (FAPs) and the Facial Definition Parameters (FDPs) <ref> [1] </ref>. FDPs are used to customize the initial or baseline face model of the rendering software to a particular face or to download a face model along with the information about how to animate it [1]. FAPs are used to describe the animation of a face. In this study, we only consider extracting FAPs. FAPs are defined based on the study of facial actions and muscle actions [1]. It contains a complete set of basic facial actions. <p> a particular face or to download a face model along with the information about how to animate it <ref> [1] </ref>. FAPs are used to describe the animation of a face. In this study, we only consider extracting FAPs. FAPs are defined based on the study of facial actions and muscle actions [1]. It contains a complete set of basic facial actions. So, we can represent most natural facial expressions with a set of FAPs. Exaggerated value of the parameters will result in some exaggerated expressions for cartoon-like characters [1]. 15 Currently the FAP standard has 68 parameters. <p> FAPs are defined based on the study of facial actions and muscle actions <ref> [1] </ref>. It contains a complete set of basic facial actions. So, we can represent most natural facial expressions with a set of FAPs. Exaggerated value of the parameters will result in some exaggerated expressions for cartoon-like characters [1]. 15 Currently the FAP standard has 68 parameters. Two parameters are high--level parameters, and the other 66 are low-level parameters [1]. The two high-level parameters are viseme and expression. A viseme is a visual correlate to a phoneme [1]. <p> So, we can represent most natural facial expressions with a set of FAPs. Exaggerated value of the parameters will result in some exaggerated expressions for cartoon-like characters <ref> [1] </ref>. 15 Currently the FAP standard has 68 parameters. Two parameters are high--level parameters, and the other 66 are low-level parameters [1]. The two high-level parameters are viseme and expression. A viseme is a visual correlate to a phoneme [1]. A number of standardized visemes are defined for a set of phonemes. For example, viseme value 1 corresponds to phonemes p, b, and m [2]. <p> the parameters will result in some exaggerated expressions for cartoon-like characters <ref> [1] </ref>. 15 Currently the FAP standard has 68 parameters. Two parameters are high--level parameters, and the other 66 are low-level parameters [1]. The two high-level parameters are viseme and expression. A viseme is a visual correlate to a phoneme [1]. A number of standardized visemes are defined for a set of phonemes. For example, viseme value 1 corresponds to phonemes p, b, and m [2]. The expression parameter represents a specific expression [1]. A number of standardized expressions are defined. For example, expression 1 means joy. <p> A viseme is a visual correlate to a phoneme <ref> [1] </ref>. A number of standardized visemes are defined for a set of phonemes. For example, viseme value 1 corresponds to phonemes p, b, and m [2]. The expression parameter represents a specific expression [1]. A number of standardized expressions are defined. For example, expression 1 means joy. These two high-level parameters can control the speaking and emotion of a virtual face, provided that the FAP player is able to render these two parameters correctly. <p> Other parameters are low-level parameters that define the movement of a specific region of a face. For example, FAP#3 (open jaw) is defined as vertical jaw displacement <ref> [1] </ref>. FAPs are organized into 10 groups based on the face region and functionality [1]. <p> Other parameters are low-level parameters that define the movement of a specific region of a face. For example, FAP#3 (open jaw) is defined as vertical jaw displacement <ref> [1] </ref>. FAPs are organized into 10 groups based on the face region and functionality [1]. The FAP groups are [2]: 1) high-level viseme and expression; 2) jaw, chin, inner low-erlip, cornerlip, midlip; 3) eyeballs, pupils, eyelids; 4) eyebrow; 5) cheeks; 6) tongue; 7) head rotation; 8)outer lip positions; 9) nose; 10) ears. Our approach is to convert the facial point movement into low-level FAPs.
Reference: [2] <author> Eric Petajan, </author> <title> Facial Animation Coding Unofficial Derivative of MPEG-4 Standardization, ISO/IEC JTC1/SC29/WG11 Face and Body Animation Ad Hoc Group, </title> <month> September </month> <year> 1997. </year>
Reference-contexts: The two high-level parameters are viseme and expression. A viseme is a visual correlate to a phoneme [1]. A number of standardized visemes are defined for a set of phonemes. For example, viseme value 1 corresponds to phonemes p, b, and m <ref> [2] </ref>. The expression parameter represents a specific expression [1]. A number of standardized expressions are defined. For example, expression 1 means joy. These two high-level parameters can control the speaking and emotion of a virtual face, provided that the FAP player is able to render these two parameters correctly. <p> Other parameters are low-level parameters that define the movement of a specific region of a face. For example, FAP#3 (open jaw) is defined as vertical jaw displacement [1]. FAPs are organized into 10 groups based on the face region and functionality [1]. The FAP groups are <ref> [2] </ref>: 1) high-level viseme and expression; 2) jaw, chin, inner low-erlip, cornerlip, midlip; 3) eyeballs, pupils, eyelids; 4) eyebrow; 5) cheeks; 6) tongue; 7) head rotation; 8)outer lip positions; 9) nose; 10) ears. Our approach is to convert the facial point movement into low-level FAPs. <p> Third, we calculate the x, y, or z displacement of the feature point between frames, and normalize the displacement by Facial Animation Parameter Units (FAPUs). Different FAPs are normalized by different FAPUs as defined in the FAP document <ref> [2] </ref>. 3.2.2 Facial Animation Parameter Units As described in the last section, FAPUs are used to normalize the displacement of some feature points. These units are defined in order to allow interpretation of the FAPs on any facial model in a consistent way [2]. <p> FAPUs as defined in the FAP document <ref> [2] </ref>. 3.2.2 Facial Animation Parameter Units As described in the last section, FAPUs are used to normalize the displacement of some feature points. These units are defined in order to allow interpretation of the FAPs on any facial model in a consistent way [2]. The FAPUs are illustrated in Figure 3.2 [2]. This figure is taken from [2]. It corresponds to the distances between some key facial feature points. In our study, we only consider the FAPUs as in Table 3.2 [2]. The fractional units are to allow enough precision [2]. <p> These units are defined in order to allow interpretation of the FAPs on any facial model in a consistent way <ref> [2] </ref>. The FAPUs are illustrated in Figure 3.2 [2]. This figure is taken from [2]. It corresponds to the distances between some key facial feature points. In our study, we only consider the FAPUs as in Table 3.2 [2]. The fractional units are to allow enough precision [2]. <p> These units are defined in order to allow interpretation of the FAPs on any facial model in a consistent way <ref> [2] </ref>. The FAPUs are illustrated in Figure 3.2 [2]. This figure is taken from [2]. It corresponds to the distances between some key facial feature points. In our study, we only consider the FAPUs as in Table 3.2 [2]. The fractional units are to allow enough precision [2]. <p> interpretation of the FAPs on any facial model in a consistent way <ref> [2] </ref>. The FAPUs are illustrated in Figure 3.2 [2]. This figure is taken from [2]. It corresponds to the distances between some key facial feature points. In our study, we only consider the FAPUs as in Table 3.2 [2]. The fractional units are to allow enough precision [2]. <p> a consistent way <ref> [2] </ref>. The FAPUs are illustrated in Figure 3.2 [2]. This figure is taken from [2]. It corresponds to the distances between some key facial feature points. In our study, we only consider the FAPUs as in Table 3.2 [2]. The fractional units are to allow enough precision [2].
Reference: [3] <author> Frederick I. Parke, </author> <title> A Parametric Model of Human Faces, </title> <type> Ph.D. Dissertation, </type> <institution> Computer Science Department, University of Utah, </institution> <year> 1974. </year>
Reference-contexts: Most facial models describe facial actions using either FACS (Facial Action Coding System) [7] or muscle movements [8]. Research in parametric face models began in 1974 with Parke's Ph.D. thesis <ref> [3] </ref> in which he presented a 3D parametric human face model. The face model 2 is constructed by a number of polygonal surfaces, and is manipulated by a set of parameters which control the movement of various facial features [3]. <p> in parametric face models began in 1974 with Parke's Ph.D. thesis <ref> [3] </ref> in which he presented a 3D parametric human face model. The face model 2 is constructed by a number of polygonal surfaces, and is manipulated by a set of parameters which control the movement of various facial features [3]. Figure 1.1 is an example image of Parke's face model [4]. On the other hand, Waters' face model [8] considers the development of a muscle process, controlled by a limited number of parameters.
Reference: [4] <author> Frederic I. Parke and Keith Waters, </author> <title> Computer Facial Animation, A K Peters, </title> <publisher> Ltd., </publisher> <address> Wellesley, MA, </address> <year> 1996. </year>
Reference-contexts: The face model 2 is constructed by a number of polygonal surfaces, and is manipulated by a set of parameters which control the movement of various facial features [3]. Figure 1.1 is an example image of Parke's face model <ref> [4] </ref>. On the other hand, Waters' face model [8] considers the development of a muscle process, controlled by a limited number of parameters. The deformation of skin is described as an elastic mesh where unit actions are simulated by forces [8]. This allows for more subtle facial actions. <p> The deformation of skin is described as an elastic mesh where unit actions are simulated by forces [8]. This allows for more subtle facial actions. Figure 1.2 is an example image of Waters' face model <ref> [4] </ref>. Our work is based on MPEG-4 specification of Facial Animation Parameters. To our knowledge, this is the first reported work on extraction of FAPs using 3D facial data. Recent research on face model is coupled with MPEG-4 SNHC. SNHC stands 3 for Synthetic/Natural Hybrid Coding.
Reference: [5] <author> Catherine Pelachaud, Norman Badler and Marie-Luce Viaud, </author> <title> Final Report to NSF of the Standards for Facial Animation Workshop, </title> <institution> Computer and Information Science Department, University of Pennsylvania, </institution> <year> 1994. </year>
Reference-contexts: FAPs are used to animate the face. 1.2.2 Facial Data Acquisition We describe two methods of 3D facial data acquisition. The first one is rangefinding. Rangefinding measures the relative positions of a series of points of a static surface from the range finder <ref> [5] </ref>. An example is range laser scanner by Cyberware Inc.. It can move around a fixed object in order to get complete 3D information. One of our experiment on 3D face data set is produced by this method. Another 3D facial data acquisition technique is stereo vision. <p> For a complete discussion and comparison of these approaches, please refer to the Final Report of the Standards for Facial Animation Workshop, sponsored by NSF <ref> [5] </ref>. Our approach in this study takes a different direction in that we detect the motion of feature points in 3D face data. We use non-rigid motion analysis algorithms for this purpose. The motion of objects can generally be classified according to the degree of nonrigidity [16].
Reference: [6] <author> P. Ekman and W. Friesen, </author> <title> Facial Action Coding System, </title> <publisher> Consulting Psychologists Press Inc., </publisher> <address> Palo Alto, CA, </address> <year> 1978. </year>
Reference: [7] <author> P. Ekman and W. Friesen, </author> <title> Manual for the Facial Action Coding System, </title> <publisher> Press Palo Alto California, </publisher> <year> 1977. </year>
Reference-contexts: Most facial models describe facial actions using either FACS (Facial Action Coding System) <ref> [7] </ref> or muscle movements [8]. Research in parametric face models began in 1974 with Parke's Ph.D. thesis [3] in which he presented a 3D parametric human face model.
Reference: [8] <author> D. Terzopoulos and K. Waters, </author> <title> Analysis and Synthesis of Facial Image Sequences using Physical and Anatomical Models, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence (volume 15, </journal> <pages> pages 569-579), </pages> <year> 1993 </year>
Reference-contexts: Most facial models describe facial actions using either FACS (Facial Action Coding System) [7] or muscle movements <ref> [8] </ref>. Research in parametric face models began in 1974 with Parke's Ph.D. thesis [3] in which he presented a 3D parametric human face model. <p> The face model 2 is constructed by a number of polygonal surfaces, and is manipulated by a set of parameters which control the movement of various facial features [3]. Figure 1.1 is an example image of Parke's face model [4]. On the other hand, Waters' face model <ref> [8] </ref> considers the development of a muscle process, controlled by a limited number of parameters. The deformation of skin is described as an elastic mesh where unit actions are simulated by forces [8]. This allows for more subtle facial actions. <p> On the other hand, Waters' face model <ref> [8] </ref> considers the development of a muscle process, controlled by a limited number of parameters. The deformation of skin is described as an elastic mesh where unit actions are simulated by forces [8]. This allows for more subtle facial actions. Figure 1.2 is an example image of Waters' face model [4]. Our work is based on MPEG-4 specification of Facial Animation Parameters. To our knowledge, this is the first reported work on extraction of FAPs using 3D facial data.
Reference: [9] <author> Keith Waters, </author> <title> A muscle model for animating three-dimensional facial expressions, </title> <journal> Computer Graphics, </journal> <volume> 21(4): </volume> <pages> 17-24, </pages> <month> July </month> <year> 1987. </year>
Reference: [10] <author> Keith Waters and Demetri Terzopoulos, </author> <title> A physical model of facial tissue and muscle articulation, </title> <booktitle> First Conference on Visualization in Biomedical Computing, </booktitle> <address> Atlanta, GA, </address> <month> May </month> <year> 1990. </year>
Reference: [11] <author> Chandra Kambhamettu, </author> <title> Nonrigid Motion Analysis Under Small Deformations, </title> <type> Ph.D. Dissertation, </type> <institution> Computer Science and Engineer Department, University of South Florida, </institution> <year> 1994. </year> <month> 45 </month>
Reference-contexts: In this study, we use a simple and fast algorithm based on discriminant of first fundamental form in order to estimate small deformation and perform facial feature point tracking. 1.2.4 3D Non-rigid Motion Detection The motion of objects can generally be classified according to the degree of nonrigidity <ref> [11] </ref>. Rigid motion preserves the 3D distances between any two points in an object. Non-rigid motion can be divided into three categories, namely, articulated, elastic and fluids. For a complete discussion of these motions, please refer to [16]. We consider facial motion as a locally continuous small deformation. <p> Non-rigid motion can be divided into three categories, namely, articulated, elastic and fluids. For a complete discussion of these motions, please refer to [16]. We consider facial motion as a locally continuous small deformation. Kamb--hamettu and Goldgof <ref> [16, 14, 15, 11] </ref> proposed various approaches for point correspondence and nonrigid motion estimation. The key idea in [11] is, by hypothesizing point correspondence, one can detect the correct correspondence through minimizing the deviation from the assumed small deformation of surfaces. <p> For a complete discussion of these motions, please refer to [16]. We consider facial motion as a locally continuous small deformation. Kamb--hamettu and Goldgof [16, 14, 15, 11] proposed various approaches for point correspondence and nonrigid motion estimation. The key idea in <ref> [11] </ref> is, by hypothesizing point correspondence, one can detect the correct correspondence through minimizing the deviation from the assumed small deformation of surfaces. <p> The dilation is given by [16]: (u; v) = D (u; v) Dilation is negative for surface contractions, and positive for surface expansions <ref> [11] </ref>. In our experiments, we estimate facial motion to be locally small deformation. Correspondence hypotheses are then selected in a neighborhood around the point of interest. In our facial experiment, it is defined as a small window around the position of the original point. <p> Thomas S. Hang of the University of Illinois. The surface points of face are obtained in a cylindrical fashion by Cyberware scanner. We have used the front view of the face, sampled into 100 by 100 points <ref> [11] </ref>. There are total three frames.
Reference: [12] <author> H. Schulzrinne, S. Casner, R. Frederick and V. Jacobson, RTP: </author> <title> A Transport Protocol for Real-Time Applications, </title> <type> RFC 1889, </type> <year> 1996. </year>
Reference-contexts: On top of that, there are many upper layer protocols, such as TELNET, FTP, etc., which perform specific tasks. Our goals is to perform real-time delivery of facial parameters. There are some real time protocols available. One of them is RTP (Real Time Protocol) <ref> [12] </ref>. RTP is a generic real time transferring protocol. With RTP, user can put arbitrary formated data inside each RTP packet. The key idea of RTP is that in each RTP data packet, there is a time stamp field which indicates the starting time of this data packet. <p> Second, delivering each frame in a timely fashion. This helps to achieve real time effect. Third, overhead of the RTP header is small. This overhead will not account for too much bandwidth. A general description of RTP packet format can be found in <ref> [12] </ref>. An RTP packet contains two parts, header and data. Some important header fields are: Payload Type (PT), Time Stamp (TS), and Sequence Number. In RTP protocol, PT indicates the media type of the data. TS indicates the start time of this frame. <p> Some important header fields are: Payload Type (PT), Time Stamp (TS), and Sequence Number. In RTP protocol, PT indicates the media type of the data. TS indicates the start time of this frame. It increases with a frequency that equals the maximum frame rate <ref> [12] </ref>. By explicitly keeping TS in each RTP packet, receiver can reconstruct the timing sequence of the frames. Sequence number indicates the sending sequence of the frame. RTP data comes after the header. It contains two parts of data, the FAP mask and the individual FAPs. <p> In our experiments, most FAPs turn out to be from -500 to 500. Sender and receiver should agree on the same RTP format. RTCP stands for Real Time Control Protocol. It comes with RTP. RTCP has a number of functionalities <ref> [12] </ref>. In our unicast scheme, receiver periodically sends RTCP packets to the sender to report the current lost rate. The format of our RTCP packet is shown in Figure 4.4. The lost rate occupies 2 bytes.
Reference: [13] <author> Z. Tang and J. Zhou, </author> <title> Introduction to CAD, </title> <institution> China Science and Technology Press, Beijing, China, </institution> <year> 1988. </year>
Reference-contexts: It is interpolated using a cubic spline, as shown in Figure 4.7. P1 and P2 are two old parameters, while P3 is the new one. P4 is the parameter to be interpolated. A cubic spline with free ends is <ref> [13] </ref>: 39 time_stamp parameter_value Y3 Y4 X1 X2 X4 X3 P3 P1 d1 d2 2 6 6 4 1 4 1 3 7 7 5 6 6 6 P 1 P 2 P 3 3 7 7 5 2 6 6 4 3 (P 3 P 1 ) 3 7 7
Reference: [14] <author> Chandra Kambhamettu and Dmitry B. Goldgof, </author> <title> Towards finding point correspondences in nonrigid motion, </title> <booktitle> The 7th Scandinavian Conference on Image Analysis, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: Non-rigid motion can be divided into three categories, namely, articulated, elastic and fluids. For a complete discussion of these motions, please refer to [16]. We consider facial motion as a locally continuous small deformation. Kamb--hamettu and Goldgof <ref> [16, 14, 15, 11] </ref> proposed various approaches for point correspondence and nonrigid motion estimation. The key idea in [11] is, by hypothesizing point correspondence, one can detect the correct correspondence through minimizing the deviation from the assumed small deformation of surfaces.
Reference: [15] <author> Chandra Kambhamettu and Dmitry B. Goldgof, </author> <title> Point correspondence recovery in nonrigid motion, </title> <booktitle> Proceedings of IEEE conference on Computer Vision and Pattern Recognition, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: Non-rigid motion can be divided into three categories, namely, articulated, elastic and fluids. For a complete discussion of these motions, please refer to [16]. We consider facial motion as a locally continuous small deformation. Kamb--hamettu and Goldgof <ref> [16, 14, 15, 11] </ref> proposed various approaches for point correspondence and nonrigid motion estimation. The key idea in [11] is, by hypothesizing point correspondence, one can detect the correct correspondence through minimizing the deviation from the assumed small deformation of surfaces.
Reference: [16] <author> Chandra Kambhamettu and Dmitry B. Goldgof and Demetri Terzopoulos and Thomas S. Huang, </author> <title> Nonrigid Motion Analysis, </title> <editor> in Tzay Young, editor, </editor> <booktitle> Handbook of PRIP: Computer vision, </booktitle> <volume> volume II, </volume> <pages> pages 405-430, </pages> <publisher> Academic Press, </publisher> <address> San Diego, California, </address> <year> 1994. </year>
Reference-contexts: Our approach in this study takes a different direction in that we detect the motion of feature points in 3D face data. We use non-rigid motion analysis algorithms for this purpose. The motion of objects can generally be classified according to the degree of nonrigidity <ref> [16] </ref>. We approximate facial motion as a local small deformation, which is a subclass of elastic motion. The key idea is that by hypothesizing point correspondence, one can estimate the correct correspondence through minimizing the deviation from the assumed local small deformation [18]. <p> Rigid motion preserves the 3D distances between any two points in an object. Non-rigid motion can be divided into three categories, namely, articulated, elastic and fluids. For a complete discussion of these motions, please refer to <ref> [16] </ref>. We consider facial motion as a locally continuous small deformation. Kamb--hamettu and Goldgof [16, 14, 15, 11] proposed various approaches for point correspondence and nonrigid motion estimation. <p> Non-rigid motion can be divided into three categories, namely, articulated, elastic and fluids. For a complete discussion of these motions, please refer to [16]. We consider facial motion as a locally continuous small deformation. Kamb--hamettu and Goldgof <ref> [16, 14, 15, 11] </ref> proposed various approaches for point correspondence and nonrigid motion estimation. The key idea in [11] is, by hypothesizing point correspondence, one can detect the correct correspondence through minimizing the deviation from the assumed small deformation of surfaces. <p> The dilation is given by <ref> [16] </ref>: (u; v) = D (u; v) Dilation is negative for surface contractions, and positive for surface expansions [11]. In our experiments, we estimate facial motion to be locally small deformation. Correspondence hypotheses are then selected in a neighborhood around the point of interest. <p> At each point of the search window, we consider a local patch. A least-square error is thus defined for each hypothesis [18]. Correct correspondence is then estimated by minimizing this error. From equation 2.2, we can define an error function ER <ref> [16] </ref>: ER = i2j i D i D i ) 2 where j represents the neighborhood area for searching correct correspondence. The correct correspondence should get a zero ER, or a minimum value among a hypoth esis area. By minimizing ER, we get [16]: = i2j (D i D 0 i <p> we can define an error function ER <ref> [16] </ref>: ER = i2j i D i D i ) 2 where j represents the neighborhood area for searching correct correspondence. The correct correspondence should get a zero ER, or a minimum value among a hypoth esis area. By minimizing ER, we get [16]: = i2j (D i D 0 i ) i2j (D 2 ER is then calculated for each hypothesis point in the neighborhood area.
Reference: [17] <author> Lynne L. Grewe and Avinash C. Kak, </author> <title> Stereo Vision, </title> <editor> in Tzay Young, editor, </editor> <booktitle> Handbook of PRIP: Computer vision, </booktitle> <volume> volume II, </volume> <pages> pages 240-315, </pages> <publisher> Academic Press, </publisher> <address> San Diego, California, </address> <year> 1994. </year>
Reference-contexts: It uses a pair of cameras separated by a small baseline that captures two images from slightly different perspectives. Structure light is typically used to solve the stereo registration problem. A host computer gets the image information and produces 3D space based on stereo vision technique <ref> [17] </ref>. An example of this method is CyberSight 3D motion imaging system from Lawrence Livermore National Laboratory [23]. The advantage of this method is that it can record a moving object.
Reference: [18] <author> Chandra Kambhamettu and Dmitry B. Goldgof and Matthew He, </author> <title> Determination of Motion parameters and Estimation of Point Correspondences in small Nonrigid deformations, </title> <booktitle> Proceedings of IEEE conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 943-946, </pages> <year> 1994. </year>
Reference-contexts: We approximate facial motion as a local small deformation, which is a subclass of elastic motion. The key idea is that by hypothesizing point correspondence, one can estimate the correct correspondence through minimizing the deviation from the assumed local small deformation <ref> [18] </ref>. In this study, we use a simple and fast algorithm based on discriminant of first fundamental form in order to estimate small deformation and perform facial feature point tracking. 1.2.4 3D Non-rigid Motion Detection The motion of objects can generally be classified according to the degree of nonrigidity [11]. <p> Similar to unit normal tracking, it searches the neighborhood area for a correct correspondence. The only difference is that the discriminant-based method compensates for a small deformation while tracking point correspondence <ref> [18] </ref>. We implemented this method in our study in order to track facial feature points. 2.3.1 Discriminant Consider a curve C: u = u (t), v = v (t) on surface S. <p> In our facial experiment, it is defined as a small window around the position of the original point. At each point of the search window, we consider a local patch. A least-square error is thus defined for each hypothesis <ref> [18] </ref>. Correct correspondence is then estimated by minimizing this error. From equation 2.2, we can define an error function ER [16]: ER = i2j i D i D i ) 2 where j represents the neighborhood area for searching correct correspondence.
Reference: [19] <author> C. E. Weatherburn, </author> <title> Differential Geometry in Three Dimensions, volume II, </title> <publisher> Cambridge University Press, </publisher> <year> 1930. </year>
Reference-contexts: This expression is a homogeneous function of second degree in du, dv. It is known as the first fundamental differential quadratic form of S <ref> [19] </ref>. The discriminant of the surface is then given by, D 2 = EG F 2 where E, F, G are the coefficients of the first fundamental form.
Reference: [20] <author> Alan L. Yuille, David S. Cohen and Peter W. Hallinan, </author> <title> Feature extraction from faces using deformable templates, </title> <booktitle> Proceedings of IEEE conference on Computer Vision and Pattern Recognition, </booktitle> <year> 1989. </year>
Reference-contexts: This kind of data set is different from other existing facial recognition studies, where 2D intensity data is used <ref> [20, 27, 28] </ref>. There are some advantages of using 3D range data: i) more geometric hints for facial feature points detection; ii) facial parameters are 3D oriented; iii) 3D movements constitute a large part of facial gestures. <p> Most of them are based on 2D face intensity image. Among them there are optical flow [29, 30], deformable templates <ref> [20, 22, 21] </ref>, and contour models [31, 32, 33]. 20 A preliminary proposal for automatically locating feature points is by 3D deformable templates with prior knowledge. Yuille et al. proposed a parameterized deformable templates to extract face features in 2D intensity images [20]. <p> Yuille et al. proposed a parameterized deformable templates to extract face features in 2D intensity images <ref> [20] </ref>. It defines an energy function which links edges, peaks and valleys in the image intensity to corresponding properties of the templates [20]. Similarly, we can define a 3D template to fit the whole face, and deform itself to fit the 3D geometric facial features. <p> Yuille et al. proposed a parameterized deformable templates to extract face features in 2D intensity images <ref> [20] </ref>. It defines an energy function which links edges, peaks and valleys in the image intensity to corresponding properties of the templates [20]. Similarly, we can define a 3D template to fit the whole face, and deform itself to fit the 3D geometric facial features.
Reference: [21] <author> A. Yuille, D. Cohen, and P. Hallinan, </author> <title> Feature extraction from faces using de-formable templates, </title> <journal> International Journal of Computer Vision, </journal> <volume> 8(2) </volume> <pages> 99-111, </pages> <year> 1992. </year>
Reference-contexts: Most of them are based on 2D face intensity image. Among them there are optical flow [29, 30], deformable templates <ref> [20, 22, 21] </ref>, and contour models [31, 32, 33]. 20 A preliminary proposal for automatically locating feature points is by 3D deformable templates with prior knowledge. Yuille et al. proposed a parameterized deformable templates to extract face features in 2D intensity images [20].
Reference: [22] <author> A. L. Yuille, </author> <title> Deformable templates for face recognition, </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 59-70, </pages> <year> 1991. </year>
Reference-contexts: Most of them are based on 2D face intensity image. Among them there are optical flow [29, 30], deformable templates <ref> [20, 22, 21] </ref>, and contour models [31, 32, 33]. 20 A preliminary proposal for automatically locating feature points is by 3D deformable templates with prior knowledge. Yuille et al. proposed a parameterized deformable templates to extract face features in 2D intensity images [20].
Reference: [23] <author> George Pavel and Shin-Yee Lu, </author> <title> CyberSight True 3D Motion Imaging System, </title> <institution> http://www.llnl.gov/automation-robotics/cyber.html, Lawrence Liver-more National Laboratory, </institution> <month> October </month> <year> 1997. </year> <month> 46 </month>
Reference-contexts: Structure light is typically used to solve the stereo registration problem. A host computer gets the image information and produces 3D space based on stereo vision technique [17]. An example of this method is CyberSight 3D motion imaging system from Lawrence Livermore National Laboratory <ref> [23] </ref>. The advantage of this method is that it can record a moving object. Our second set of 3D facial data is obtained by this method. 1.2.3 Facial Data Analysis Extensive research has been done in the area of identification and recognition of face motion. <p> A light projector projects a set of line patterns as landmarks for stereo registration. A host computer gets the image information and produces 3D space based on stereo computer vision technique <ref> [23] </ref>. Figure 3.6 is an example of the CyberSight face. It is a snapshot from SGI movie-player playing a quicktime movie created by CyberSight team at Lawrence Livermore National Laboratory. We have 130 frames which is a talking sequence saying "Come, journey with us!".
Reference: [24] <author> J. Bolot and T. Turletti, </author> <title> A Rate Control Mechanism for Packet Video in the Internet, </title> <institution> INRIA B.P.93, </institution> <year> 1993. </year>
Reference-contexts: One way to support packet video is to use feedback mechanisms that adapt the output rate of senders based on the state of the network <ref> [24] </ref>. The idea of this mechanism is 35 that when the network is congested, it is sensible to slow down the sending rate to prevent unacceptable services to all users of the network. The feedback scheme in [24] adjusts the maximum output rate of the sender so that the median loss <p> adapt the output rate of senders based on the state of the network <ref> [24] </ref>. The idea of this mechanism is 35 that when the network is congested, it is sensible to slow down the sending rate to prevent unacceptable services to all users of the network. The feedback scheme in [24] adjusts the maximum output rate of the sender so that the median loss rate stays below a tolerable loss rate. That is [24]: if med_loss &gt; tol_loss /* tol_loss is set to be 10% */ send_rate = max (send_rate / 2, min_rate); else send_rate = gain * send_rate; /* gain <p> The feedback scheme in <ref> [24] </ref> adjusts the maximum output rate of the sender so that the median loss rate stays below a tolerable loss rate. That is [24]: if med_loss &gt; tol_loss /* tol_loss is set to be 10% */ send_rate = max (send_rate / 2, min_rate); else send_rate = gain * send_rate; /* gain is set to be 1.5 */ We take a similar approach but have two thresholds for tolerable loss rate. lost high denotes a
Reference: [25] <author> S. McCanne and V. Jacobson and M. Vetterli, </author> <title> Receiver-driven Layered Multi-cast, </title> <booktitle> ACM SIGCOMM '96, </booktitle> <year> 1996. </year>
Reference-contexts: Feedback scheme is not appropriate because there are too many receivers; thus it will lead to feedback explosion. Another reason is that there is no target rate for different receivers <ref> [25, 26] </ref> since different receivers have different loss rate. Therefore, we fit our framework into Receiver-driven Layered Multicast [25]. In that scheme, there is no feedback information. 37 Sender sends data into multiple multicast groups. <p> Feedback scheme is not appropriate because there are too many receivers; thus it will lead to feedback explosion. Another reason is that there is no target rate for different receivers [25, 26] since different receivers have different loss rate. Therefore, we fit our framework into Receiver-driven Layered Multicast <ref> [25] </ref>. In that scheme, there is no feedback information. 37 Sender sends data into multiple multicast groups. And receivers are responsible for choosing a subset of those groups according to the current network bandwidth. FAP is a hierarchical parameter set. The 68 FAPs are divided into multiple groups.
Reference: [26] <author> J. Bolot and T. Turletti and I. Wakeman, </author> <title> Scalable Feedback Control for Mul-ticast Video Distribution in the Internet, </title> <institution> INRIA, </institution> <year> 1993 </year>
Reference-contexts: Feedback scheme is not appropriate because there are too many receivers; thus it will lead to feedback explosion. Another reason is that there is no target rate for different receivers <ref> [25, 26] </ref> since different receivers have different loss rate. Therefore, we fit our framework into Receiver-driven Layered Multicast [25]. In that scheme, there is no feedback information. 37 Sender sends data into multiple multicast groups.
Reference: [27] <author> Y. Yacoob and L. Davis, </author> <title> Computing spatio-temporal representations of human faces, </title> <booktitle> Computer Vision and Pattern Recognition Conference, </booktitle> <year> 1994 </year>
Reference-contexts: This kind of data set is different from other existing facial recognition studies, where 2D intensity data is used <ref> [20, 27, 28] </ref>. There are some advantages of using 3D range data: i) more geometric hints for facial feature points detection; ii) facial parameters are 3D oriented; iii) 3D movements constitute a large part of facial gestures.
Reference: [28] <author> A. Pentland, B. Moghaddam, and T. Starner, </author> <title> View-based and modular eigenspaces for face recognition, </title> <journal> Computer Vision and Pattern Recognition, </journal> <year> 1994 </year>
Reference-contexts: This kind of data set is different from other existing facial recognition studies, where 2D intensity data is used <ref> [20, 27, 28] </ref>. There are some advantages of using 3D range data: i) more geometric hints for facial feature points detection; ii) facial parameters are 3D oriented; iii) 3D movements constitute a large part of facial gestures.
Reference: [29] <author> I. A. Essa, </author> <title> Analysis, Interpretation, and Synthesis of Facial Expressions, </title> <type> PhD Thesis, </type> <institution> MIT, Media Laboratory, </institution> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: Most of them are based on 2D face intensity image. Among them there are optical flow <ref> [29, 30] </ref>, deformable templates [20, 22, 21], and contour models [31, 32, 33]. 20 A preliminary proposal for automatically locating feature points is by 3D deformable templates with prior knowledge. Yuille et al. proposed a parameterized deformable templates to extract face features in 2D intensity images [20].
Reference: [30] <author> I. A. Essa and A. Pentland, </author> <title> A vision system for observing and extracting facial action parameters, </title> <booktitle> Proceedings of Computer Vision and Pattern Recognition (CVPR 94), </booktitle> <pages> pages 76-83, </pages> <year> 1994. </year>
Reference-contexts: Most of them are based on 2D face intensity image. Among them there are optical flow <ref> [29, 30] </ref>, deformable templates [20, 22, 21], and contour models [31, 32, 33]. 20 A preliminary proposal for automatically locating feature points is by 3D deformable templates with prior knowledge. Yuille et al. proposed a parameterized deformable templates to extract face features in 2D intensity images [20].
Reference: [31] <author> D. Terzopoulos and K. Waters, </author> <title> Physically-based facial modeling, analysis, and animation, </title> <journal> Journal of Visualization and Computer Animation, </journal> <volume> 1(2) </volume> <pages> 73-78, </pages> <year> 1990. </year>
Reference-contexts: Most of them are based on 2D face intensity image. Among them there are optical flow [29, 30], deformable templates [20, 22, 21], and contour models <ref> [31, 32, 33] </ref>. 20 A preliminary proposal for automatically locating feature points is by 3D deformable templates with prior knowledge. Yuille et al. proposed a parameterized deformable templates to extract face features in 2D intensity images [20].
Reference: [32] <author> D. Terzopoulos and K. Waters, </author> <title> Analysis and synthesis of facial image sequences using physical and anatomical models, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <pages> pages 569-579, </pages> <year> 1993. </year>
Reference-contexts: Most of them are based on 2D face intensity image. Among them there are optical flow [29, 30], deformable templates [20, 22, 21], and contour models <ref> [31, 32, 33] </ref>. 20 A preliminary proposal for automatically locating feature points is by 3D deformable templates with prior knowledge. Yuille et al. proposed a parameterized deformable templates to extract face features in 2D intensity images [20].
Reference: [33] <author> C. Bregler and Y. Konig, </author> <title> Eigenlips for robust speech recognition, </title> <booktitle> Proceedings of the Int. Conf. on Acoustics Speech and Signal Processing (IEEE-ICASSP), </booktitle> <address> Adelaide, Australia, </address> <year> 1994. </year>
Reference-contexts: Most of them are based on 2D face intensity image. Among them there are optical flow [29, 30], deformable templates [20, 22, 21], and contour models <ref> [31, 32, 33] </ref>. 20 A preliminary proposal for automatically locating feature points is by 3D deformable templates with prior knowledge. Yuille et al. proposed a parameterized deformable templates to extract face features in 2D intensity images [20].
Reference: [34] <author> Andrew S. Tanenbaum, </author> <title> Computer Networks (3rd Edition), </title> <publisher> Prentice Hall, </publisher> <address> Upper Saddle River, New Jersey, </address> <year> 1996 </year> <month> 47 </month>
Reference-contexts: There are two kinds of IP address, unicast IP and multicast IP. Unicast IP denotes a specific host, while multicast IP represents a group of hosts. Each host can express interests in receiving a particular multicast IP group. The routing protocol for multicast and unicast packets are different <ref> [34] </ref>. QoS stands for Quality of Service. It includes a variety of parameters such as peak rate, minimum rate, lose ratio, transfer delay, delay variation, etc.. QoS is an important issue in network services especially for real-time traffic.
References-found: 34

