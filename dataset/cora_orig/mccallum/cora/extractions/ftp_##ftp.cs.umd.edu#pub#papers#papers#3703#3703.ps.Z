URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3703/3703.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: hollings,mike-@cs.umd.edu  
Title: Grindstone: A Test Suite for Parallel Performance Tools UU  
Author: Jeffrey K. Hollingsworth Michael Steele 
Note: U This work was supported in part by NIST CRA award 70-NANB-5H0055, DOE Grant DE-FG02-93ER25176, and  
Address: College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies Computer Science Department and Computer Science Department University of Maryland  the State of Maryland.  
Pubnum: CS-TR-3703 UMIACS-TR-96-73  
Abstract: We describe Grindstone, a suite of programs for testing and calibrating parallel pe r-formance measurement tools. The suite consists of nine simple SPMD style PVM pr o-grams that demonstrate common communication and computational bottlenecks that o c-cur in parallel programs. In addition, we provide a short case study that demonstrates the use of the test suite on three performance tools for PVM. The results of the case study showed that we were able to uncover bugs or other anomalies in all three tools. The p a per also describes how to acquire, compile, and use the test suite. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Berry, et al. </author> , <title> The PERFECT Club benchmarks: Effective performance evaluation of supe r-computers, </title> <journal> The International Journal of Supercomputing Applications, 1989. </journal> <volume> 3 (3), </volume> <pages> pp. 5-40. </pages>
Reference-contexts: We have created a validation suite of programs, called Grindstone 1 , that demonstrate simple but common problems which parallel programs commonly suffer from. Test Program suites such as SPLASH [5] and PERFECT Club <ref> [1] </ref> have proved useful to evaluate parallel compiles. We intend Grin d-stone as the starting point for a standardized validation suite which authors of performance analysis tools will be able to use to verify that their software gives correct prognoses for common problems.
Reference: 2. <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam, </author> <title> PVM : Parallel Virtual Machine. </title> <booktitle> 1994, </booktitle> <address> Cambridge, Mass: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Eventually, the PVM daemons exhaust the available virtual memory and exit. It would appear that this is really two bugs: PVaniM can't keep up with a high volume of traffic, and PVM does not gracefully handle large backlogs of messages. 3.2 XPVM XPVM <ref> [2] </ref> is a performance visualization and configuration management tool for PVM that was developed by Oak Ridge National Labs. For our study we used version 1.1. The tool is implemented in TCL/TK and provides several visualizations to display the communication behavior of PVM programs.
Reference: 3. <author> B. P. Miller, M. D. Callaghan, J. M. Cargille, J. K. Hollingsworth, R. B. Irvin, K. L. Karavanic, K. Kunchithapadam, and T. Newhall, </author> <booktitle> The Paradyn Parallel Performance Measurement Tools , IEEE Computer, </booktitle> <month> Nov. </month> <year> 1995. </year> <pages> 28 (11), pp. 37-46. </pages>
Reference-contexts: We are not sure if this problem was due to a version miss-match between Pablo and XPVM or if they never worked together. 3.3 Paradyn The third program tested is Paradyn <ref> [3] </ref>, developed at the University of Wisconsin. To test the automated search features of Paradyn, when we increased the number of iterations to one million for the small-messages program. However, Paradyn couldn't handle it: some of the Paradyn daemon processes died before the small-messages processes were done running.
Reference: 4. <author> D. A. Reed, R. A. Aydt, R. J. Noe, P. C. Roth, K. A. Shields, B. W. Schwartz, and L. F. Tavera, </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment , in Scalable Parallel Libraries Conference, </title> <editor> A. Skjellum, Editor. </editor> <booktitle> 1993, </booktitle> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: Finally, the documentation claims that the trace output files are in SDDF format, ready for Pablo to use for analysis, but we found that this wasn't completely the case. XPVM outputs data types and trace ou t-put interleaved, but Pablo <ref> [4] </ref> insists that all the data types be defined before any of the trace output lines. Also, XPVM's SDDF files don't begin with the appropriate magic string.
Reference: 5. <author> J. P. Singh, W.-D. Weber, and A. Gupta, </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory, Computer Architecture News, </title> <journal> March 1992. </journal> <volume> 20 (1), </volume> <pages> pp. 5-44. </pages>
Reference-contexts: We feel that a similar suite of parallel programs would be useful to help develop parallel performance tools. We have created a validation suite of programs, called Grindstone 1 , that demonstrate simple but common problems which parallel programs commonly suffer from. Test Program suites such as SPLASH <ref> [5] </ref> and PERFECT Club [1] have proved useful to evaluate parallel compiles. We intend Grin d-stone as the starting point for a standardized validation suite which authors of performance analysis tools will be able to use to verify that their software gives correct prognoses for common problems.
Reference: 6. <author> B. Topol, J. T. Stasko, and V. S. Sunderam, </author> <title> Monitoring and Visualization in Cluster Enviro n-ments,GIT-CC-96-10,Georgia Institute of Technology, </title> <month> March </month> <year> 1996. </year>
Reference-contexts: We had tried several other tools, but were never able to get them to work in our environment. 3.1 PVaniM The Georgia Tech Graphics, Visualization, and Usability Center's PVaniM <ref> [6] </ref> visualization tool was the first program tested. Using PVaniM (version 2.0) required a few minor modifications to the test suite source code (using the PVaniM include file, and calling a function to register the process with the PVaniM monitor program).
References-found: 6

