URL: http://www.cs.ubc.ca/spider/cebly/Papers/_download_/pruning.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: cebly@cs.ubc.ca, dearden@cs.ubc.ca  
Title: Approximating Value Trees in Structured Dynamic Programming  
Author: Craig Boutilier and Richard Dearden 
Address: Vancouver, BC V6T 1Z4, CANADA  
Affiliation: Department of Computer Science University of British Columbia  
Abstract: We propose and examine a method of approximate dynamic programming for Markov decision processes based on structured problem representations. We assume an MDP is represented using a dynamic Bayesian network, and construct value functions using decision trees as our function representation. The size of the representation is kept within acceptable limits by pruning these value trees so that leaves represent possible ranges of values, thus approximating the value functions produced during optimization. We propose a method for detecting convergence, prove errors bounds on the resulting approximately optimal value functions and policies, and describe some preliminary experi mental results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Richard E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1957. </year>
Reference-contexts: With a known action model and rewards, optimization methods based on dynamic programming can be used to produce an optimal policy <ref> [1, 13, 20] </ref>. But a serious problem for dynamic programming is the curse of dimensionality: the time (and space) required grows polynomially with the size of the state space, which itself grows exponentially with the number domain features. <p> We conclude with a discussion of the applicability of these ideas to reinforcement learning. 2 MDPs and Structured Representations We assume that the system to be controlled can be described as a fully-observable, discrete state Markov decision process <ref> [1, 13, 19] </ref>, with a finite set of system states S. The controlling agent has available a finite set of actions A which cause stochastic state transitions: we write Pr (s; a; t) to denote the probability action a causes a transition to state t when executed in state s. <p> A similar representation can be used to represent the reward function R, as shown in Figure 1 (b). We call this the (immediate) reward tree, Tree (R). 3 Structured Policy Construction A very simple algorithm for optimal policy construction is value iteration <ref> [1] </ref>. We produce a sequence of n-step optimal value functions V n by setting V 0 = R, and defining V i+1 (s) = max fR (s) + fi t2S The sequence of functions V i converges linearly to V fl in the limit.
Reference: [2] <author> D. P. Bertsekas and D. A. Castanon. </author> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 34 </volume> <pages> 589-598, </pages> <year> 1989. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [22, 2, 16, 4, 5, 11, 12, 9, 17] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric; but most methods generally assume that the states so grouped have the same value.
Reference: [3] <author> Marko Bohanic and Ivan Bratko. </author> <title> Trading accuracy for sim-plicty in decision trees. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 223-250, </pages> <year> 1994. </year>
Reference-contexts: We then focus on issues arising due to approximation of these value trees. We first describe, in Section 4, an algorithm for pruning (and ordering) a single value tree, using methods adapted from those in the literature on classification by decision trees <ref> [3, 25] </ref>. In Section 5, we describe a structured version of value iteration that approximates the n-step optimal value functions it produces using the pruning method. These approximate value trees are labeled with value ranges that are guaranteed to contain the true values of the states to which they refer. <p> This problem, of course, is strongly related to work on pruning decision trees in classification. Given a fixed decision tree (assuming training has been completed), Bohanec and Bratko <ref> [3] </ref> present an algorithm for producing the sequence of pruned trees of decreasing size such that each tree in the sequence is the most accurate among all trees of that size.
Reference: [4] <author> Craig Boutilier and Richard Dearden. </author> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1016-1022, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [22, 2, 16, 4, 5, 11, 12, 9, 17] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric; but most methods generally assume that the states so grouped have the same value. <p> One difficulty with the general presentation of MDPs given above is its failure to exploit natural problem structure. Most systems are characterized by a set of random variables or propositions that describe relevant features, and actions and rewards are specified in terms of these features <ref> [15, 4, 24] </ref>. In addition, since the state space grows exponentially with the number of features, explicit specification and computation over the state space can be problematic.
Reference: [5] <author> Craig Boutilier, Richard Dearden, and Moises Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1104-1111, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [22, 2, 16, 4, 5, 11, 12, 9, 17] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric; but most methods generally assume that the states so grouped have the same value. <p> In previous work, we described a method for optimal policy construction that exploited the problem structure laid bare by the Bayes net representation <ref> [5] </ref>. Our algorithm built aggregations in a nonuniform and adaptive way, representing value functions (and policies) using decision trees, and performed structured dynamic programming using this representation. <p> This method thus exploits prior problem structure in a way that leads to very informed approximation. In Section 2, we describe MDPs and their structured representation using dynamic Bayes nets, followed in Section 3 by a brief description of the SPI algorithm of <ref> [5] </ref> that performs optimization using a decision tree representation of value functions. We then focus on issues arising due to approximation of these value trees. <p> We assume that a set of atomic propositions P describes our system, inducing a state space of size 2 jPj , and use two-stage temporal or dynamic Bayesian networks to describe our actions <ref> [10, 5] </ref>. For each action, we have a Bayes net with one set of nodes representing the system state prior to the action (one node for each variable), another set representing the world after the action has been performed, and directed arcs representing causal influences between the these sets. <p> Each post-action node has an associated conditional probability table (CPT) quantifying the influence of the action on the corresponding variable, given the value of its influences (see <ref> [5, 6] </ref> for a more detailed discussion of this representation). 3 Figure 1 (a) illustrates this representation for a single action. 4 The lack of an arc from a pre-action variable X to a post-action variable Y in the network for action a reflects the independence of a's effect on Y <p> in which a robot is supposed to get coffee from a coffee shop across the street, can get wet if it is raining unless it has an umbrella, and is rewarded if it brings coffee when the user requests it, and penalized (to a lesser extent) if it gets wet <ref> [5, 7] </ref>. This network describes the action of fetching coffee. ues to (conditional) probabilities. For instance, the trees in is true and W is false (left arrows are assumed to be labeled true and right arrows false). <p> In an effort to mitigate the curse of dimensionality, researchers have sought to use aggregation or generalization to group states. One possible approach uses action models to form regions in the state space that have identical value and performs dynamic programming steps in this way <ref> [5, 12] </ref>. <p> Intuitively, we perform a stochastic generalization of goal regression [12]. Rather than provide details, we illustrate the intuitions with a simple example (see <ref> [5] </ref> for details). Consider the initial value tree V in Figure 2 (again, left arrows denote true, right arrows false) and suppose action a (Figure 1 (a)) is to be performed. <p> The conditions (prior to the ac 5 This algorithm is a minor variant of the SPI algorithm <ref> [5] </ref>, which is based on modified policy iteration [20]. The basic operations are the same, though intermediate policies are not produced in SVI. tion) that influence WC becoming true (after the action) can be read from the network for a, giving rise to the first partial tree (Step 1). <p> We could instead label 6 For instance, one can easily construct examples of small (polynomial) Bayes net descriptions of MDPs that have value functions with many (exponential) distinct values <ref> [5] </ref>. the new region with a range encompassing all replaced values (as shown in the figure).
Reference: [6] <author> Craig Boutilier and Moises Goldszmidt. </author> <title> The frame problem and bayesian network action representations. </title> <booktitle> In Proceedings of the Eleventh Biennial Canadian Conference on Artificial Intelligence, </booktitle> <address> Toronto, </address> <year> 1996. </year> <note> to appear. </note>
Reference-contexts: Each post-action node has an associated conditional probability table (CPT) quantifying the influence of the action on the corresponding variable, given the value of its influences (see <ref> [5, 6] </ref> for a more detailed discussion of this representation). 3 Figure 1 (a) illustrates this representation for a single action. 4 The lack of an arc from a pre-action variable X to a post-action variable Y in the network for action a reflects the independence of a's effect on Y
Reference: [7] <author> Craig Boutilier and David Poole. </author> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> Portland, OR, </address> <year> 1996. </year> <note> to appear. </note>
Reference-contexts: in which a robot is supposed to get coffee from a coffee shop across the street, can get wet if it is raining unless it has an umbrella, and is rewarded if it brings coffee when the user requests it, and penalized (to a lesser extent) if it gets wet <ref> [5, 7] </ref>. This network describes the action of fetching coffee. ues to (conditional) probabilities. For instance, the trees in is true and W is false (left arrows are assumed to be labeled true and right arrows false).
Reference: [8] <author> Justin A. Boyan and Andrew W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1995. </year>
Reference-contexts: This allows local error bounds to be maintained during computation with minimal effort. These will typically be much tighter than possible global bounds. Moreover, while approximation of value functions can sometimes lead to arbitrarily bad results <ref> [8] </ref>, maintaining accurate value ranges allows us to circumvent convergence problems. We show convergence, describe error bounds, and report on some preliminary experimental results. <p> But this is irrelevant to the construction of the value function. 9 For further discussion of convergence problems that arise due to approximation, see <ref> [8] </ref>. Table 1: Results in the 400 state domain for both fixed and sliding tolerance pruning. Fixed Tolerance Sliding Tolerance Pruning Iterations Time (s) Max. Error Pruning Iterations Time (s) Max.
Reference: [9] <author> David Chapman and Leslie Pack Kaelbling. </author> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the Twelfth International Joint Conferenceon Artificial Intelligence, </booktitle> <pages> pages 726-731, </pages> <address> Sydney, </address> <year> 1991. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [22, 2, 16, 4, 5, 11, 12, 9, 17] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric; but most methods generally assume that the states so grouped have the same value.
Reference: [10] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: In this paper, we consider the problem of constructing an approximately optimal policy when the action-model and reward function are known. 1 In addition, we assume that the action model is specified using a compact and natural specification language, namely dynamic Bayesian networks <ref> [18, 10] </ref>. In previous work, we described a method for optimal policy construction that exploited the problem structure laid bare by the Bayes net representation [5]. <p> We assume that a set of atomic propositions P describes our system, inducing a state space of size 2 jPj , and use two-stage temporal or dynamic Bayesian networks to describe our actions <ref> [10, 5] </ref>. For each action, we have a Bayes net with one set of nodes representing the system state prior to the action (one node for each variable), another set representing the world after the action has been performed, and directed arcs representing causal influences between the these sets.
Reference: [11] <author> Thomas Dean and Shieu-Hong Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1121-1127, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [22, 2, 16, 4, 5, 11, 12, 9, 17] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric; but most methods generally assume that the states so grouped have the same value.
Reference: [12] <author> Thomas G. Dietterich and Nicholas S. Flann. </author> <title> Explanation-based learning and reinforcement learning: A unified approach. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 176-184, </pages> <address> Lake Tahoe, </address> <year> 1995. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [22, 2, 16, 4, 5, 11, 12, 9, 17] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric; but most methods generally assume that the states so grouped have the same value. <p> In an effort to mitigate the curse of dimensionality, researchers have sought to use aggregation or generalization to group states. One possible approach uses action models to form regions in the state space that have identical value and performs dynamic programming steps in this way <ref> [5, 12] </ref>. <p> The Bayes net for a allows us to easily determine the conditions that influence the probability of reaching any such region when a is performed: we simply read from the network the variables that influence the variables in V . Intuitively, we perform a stochastic generalization of goal regression <ref> [12] </ref>. Rather than provide details, we illustrate the intuitions with a simple example (see [5] for details). Consider the initial value tree V in Figure 2 (again, left arrows denote true, right arrows false) and suppose action a (Figure 1 (a)) is to be performed. <p> We are also interested in approximation methods based on algorithms other than value iteration. Finally, in the full paper we describe the application of our pruning method to RL. Roughly, following Diet-terich and Flann <ref> [12] </ref>, we can use action descriptions to perform (a stochastic, non-goal-based) generalization of goal-regression along explored trajectories. Pruning produces generalizations of the state space that correspond to regions with approximately the same (currently estimated) value.
Reference: [13] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1960. </year>
Reference-contexts: With a known action model and rewards, optimization methods based on dynamic programming can be used to produce an optimal policy <ref> [1, 13, 20] </ref>. But a serious problem for dynamic programming is the curse of dimensionality: the time (and space) required grows polynomially with the size of the state space, which itself grows exponentially with the number domain features. <p> We conclude with a discussion of the applicability of these ideas to reinforcement learning. 2 MDPs and Structured Representations We assume that the system to be controlled can be described as a fully-observable, discrete state Markov decision process <ref> [1, 13, 19] </ref>, with a finite set of system states S. The controlling agent has available a finite set of actions A which cause stochastic state transitions: we write Pr (s; a; t) to denote the probability action a causes a transition to state t when executed in state s. <p> In order to compare policies, we adopt expected total discounted reward as our optimality criterion; future rewards are discounted by rate 0 fi &lt; 1. The value of a policy can be shown to satisfy <ref> [13] </ref>: V (s) = R (s) + fi t2S The value of at any initial state s can be computed by solving this system of linear equations. A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 .
Reference: [14] <author> L. Hyafil and R. L. Rivest. </author> <title> Constructing optimal binary decision trees is NP-complete. </title> <journal> Information Processing Letters, </journal> <volume> 5 </volume> <pages> 15-17, </pages> <year> 1976. </year>
Reference-contexts: Again, this issue arises in research on classification [21, 25]. Finding the smallest decision tree representing a given function is NP-hard <ref> [14] </ref>, but there are feasible heuristics one can use in our setting to reorder the tree to make it smaller and/or more amenable to pruning.
Reference: [15] <author> Nicholas Kushmerick, Steve Hanks, and Daniel Weld. </author> <title> An algorithm for probabilistic least-commitment planning. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1073-1078, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: One difficulty with the general presentation of MDPs given above is its failure to exploit natural problem structure. Most systems are characterized by a set of random variables or propositions that describe relevant features, and actions and rewards are specified in terms of these features <ref> [15, 4, 24] </ref>. In addition, since the state space grows exponentially with the number of features, explicit specification and computation over the state space can be problematic.
Reference: [16] <author> Andrew W. Moore. </author> <title> Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state-spaces. </title> <booktitle> In Proceedings of the Eighth International Conference on Machine Learning, </booktitle> <pages> pages 333-337, </pages> <address> Evanston, IL, </address> <year> 1991. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [22, 2, 16, 4, 5, 11, 12, 9, 17] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric; but most methods generally assume that the states so grouped have the same value.
Reference: [17] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [22, 2, 16, 4, 5, 11, 12, 9, 17] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric; but most methods generally assume that the states so grouped have the same value.
Reference: [18] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1988. </year>
Reference-contexts: In this paper, we consider the problem of constructing an approximately optimal policy when the action-model and reward function are known. 1 In addition, we assume that the action model is specified using a compact and natural specification language, namely dynamic Bayesian networks <ref> [18, 10] </ref>. In previous work, we described a method for optimal policy construction that exploited the problem structure laid bare by the Bayes net representation [5].
Reference: [19] <author> Martin L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: We conclude with a discussion of the applicability of these ideas to reinforcement learning. 2 MDPs and Structured Representations We assume that the system to be controlled can be described as a fully-observable, discrete state Markov decision process <ref> [1, 13, 19] </ref>, with a finite set of system states S. The controlling agent has available a finite set of actions A which cause stochastic state transitions: we write Pr (s; a; t) to denote the probability action a causes a transition to state t when executed in state s. <p> This ensures the resulting value function V i+1 is within " 2 of the optimal function V fl at any state, and that the induced policy is "-optimal (i.e., its value is within " of V fl ) <ref> [19] </ref>. In an effort to mitigate the curse of dimensionality, researchers have sought to use aggregation or generalization to group states. One possible approach uses action models to form regions in the state space that have identical value and performs dynamic programming steps in this way [5, 12].
Reference: [20] <author> Martin L. Puterman and M.C. Shin. </author> <title> Modified policy iteration algorithms for discounted Markov decision problems. </title> <journal> Management Science, </journal> <volume> 24 </volume> <pages> 1127-1137, </pages> <year> 1978. </year>
Reference-contexts: With a known action model and rewards, optimization methods based on dynamic programming can be used to produce an optimal policy <ref> [1, 13, 20] </ref>. But a serious problem for dynamic programming is the curse of dimensionality: the time (and space) required grows polynomially with the size of the state space, which itself grows exponentially with the number domain features. <p> The conditions (prior to the ac 5 This algorithm is a minor variant of the SPI algorithm [5], which is based on modified policy iteration <ref> [20] </ref>. The basic operations are the same, though intermediate policies are not produced in SVI. tion) that influence WC becoming true (after the action) can be read from the network for a, giving rise to the first partial tree (Step 1).
Reference: [21] <author> J. Ross Quinlan. C45: </author> <title> Programs for Machince Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1993. </year>
Reference-contexts: The amount of pruning that one can do by removing sub-trees, within acceptable tolerancesindeed the size of the tree before pruningmay be strongly influenced by the node ordering used in the value tree. Again, this issue arises in research on classification <ref> [21, 25] </ref>. Finding the smallest decision tree representing a given function is NP-hard [14], but there are feasible heuristics one can use in our setting to reorder the tree to make it smaller and/or more amenable to pruning. <p> Among these, one appears rather promising and is strongly related to the information gain heuristic <ref> [21] </ref>. 7 5 Policy Construction with Approximate Value Functions 5.1 The ASVI Algorithm and its Properties Armed with a method for pruning an r-tree, we now examine how this can be applied to a policy construction technique like SVI. Our basic strategy can be described as follows.
Reference: [22] <author> Paul L. Schweitzer, Martin L. Puterman, and Kyle W. Kindle. </author> <title> Iterative aggregation-disaggregation procedures for discounted semi-Markov reward processes. </title> <journal> Operations Research, </journal> <volume> 33 </volume> <pages> 589-605, </pages> <year> 1985. </year>
Reference-contexts: These aggregates are treated as a single state in dynamic programming algorithms for the solution of MDPs or the related methods used in RL <ref> [22, 2, 16, 4, 5, 11, 12, 9, 17] </ref>. Such aggregations can be based on a number of different problem features, such as similarity of states according to some domain metric; but most methods generally assume that the states so grouped have the same value.
Reference: [23] <author> Satinder P. Singh and Richard C. Yee. </author> <title> An upper bound on the loss from approximate optimal-value functions. </title> <journal> Machine Learning, </journal> <volume> 16 </volume> <pages> 227-233, </pages> <year> 1994. </year>
Reference-contexts: Suppose we have a desired percentage tolerance t for error (e.g., all approximate values should lie within t = 0:1, or 10%, of true value). There are two ways to implement such a tolerance: a) a fixed tolerance set at t 1 fi 10 See <ref> [23] </ref> for discussion of policy error given an approximate value function. or b) a sliding tolerance, where the tree for the n-stage to go function V n is pruned using a tolerance of t i=0 i max min (Here R max and R min are the maximum and minimum immediate rewards;
Reference: [24] <author> Joseph A. Tatman and Ross D. Shachter. </author> <title> Dynamic programming and influence diagrams. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 20(2) </volume> <pages> 365-379, </pages> <year> 1990. </year>
Reference-contexts: One difficulty with the general presentation of MDPs given above is its failure to exploit natural problem structure. Most systems are characterized by a set of random variables or propositions that describe relevant features, and actions and rewards are specified in terms of these features <ref> [15, 4, 24] </ref>. In addition, since the state space grows exponentially with the number of features, explicit specification and computation over the state space can be problematic.
Reference: [25] <author> Paul E. Utgoff. </author> <title> Decision tree induction based on efficient tree restructuring. </title> <type> Technical Report 95-18, </type> <institution> University of Massachusetts, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: We then focus on issues arising due to approximation of these value trees. We first describe, in Section 4, an algorithm for pruning (and ordering) a single value tree, using methods adapted from those in the literature on classification by decision trees <ref> [3, 25] </ref>. In Section 5, we describe a structured version of value iteration that approximates the n-step optimal value functions it produces using the pruning method. These approximate value trees are labeled with value ranges that are guaranteed to contain the true values of the states to which they refer. <p> The amount of pruning that one can do by removing sub-trees, within acceptable tolerancesindeed the size of the tree before pruningmay be strongly influenced by the node ordering used in the value tree. Again, this issue arises in research on classification <ref> [21, 25] </ref>. Finding the smallest decision tree representing a given function is NP-hard [14], but there are feasible heuristics one can use in our setting to reorder the tree to make it smaller and/or more amenable to pruning. <p> The variable with the smallest ranges is installed at the root of the tree (e.g., see <ref> [25] </ref>). This is repeated with the variables remaining in the new subtrees. We defer details to the full paper. (Thanks to Will Evans for his help with these ideas.) Input: ranged value tree T Output: Labels SEQ-LABEL indicating order in which to replace subtrees rooted at labeled node 1.
Reference: [26] <author> Christopher J. C. H. Watkins and Peter Dayan. </author> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: Unfortunately, with many problems, even structured representations may not help greatly with optimal policy construction, for the optimal value function may take on a large number of distinct values, precluding compact rep 1 The close relationship between RL methods such as Q-learning <ref> [26] </ref> and the solution of MDPs with known models suggests that our ideas should be applicable to the unknown-model setting (see Section 6). resentation. However, often the distinctions made are of minor importanceif states with roughly the same value can be grouped, good (though possibly suboptimal) policies should result. <p> In addition, given any such structured value tree V , we can use the Bayes net action description to produce a Q-tree Q a for any action a. This tree describes the value of performing action a assuming terminal value is given by V (i.e., a Q-function <ref> [26] </ref>). Roughly, each branch of V determines a region of the state space with a unique value.
References-found: 26

