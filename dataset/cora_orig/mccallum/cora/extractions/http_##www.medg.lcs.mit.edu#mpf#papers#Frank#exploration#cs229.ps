URL: http://www.medg.lcs.mit.edu/mpf/papers/Frank/exploration/cs229.ps
Refering-URL: http://www.medg.lcs.mit.edu/mpf/papers/Frank/Frank-90d.html
Root-URL: 
Title: Exploration in Machine Learning  
Author: Michael P. Frank 
Date: December 2, 1990  
Address: Stanford, California 94305  
Affiliation: Stanford University  
Note: Program in Symbolic Systems  
Abstract: Most researchers in machine learning have built their learning systems under the assumption that some external entity would do all the work of furnishing the learning experiences. Recently, however, investigators in several subfields of machine learning have designed systems that play an active role in choosing the situations from which they will learn. Such activity is generally called exploration. This paper describes a few of these exploratory learning projects, as reported in the literature, and attempts to extract a general account of the issues involved in exploration.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Peter Cheeseman. </author> <title> In defense of probability. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1002-1009, </pages> <address> Los Angeles, CA, </address> <month> 18-23 August </month> <year> 1985. </year> <pages> IJCAI. </pages>
Reference-contexts: Instead of choosing the action whose outcome is most uncertain, Moore's system chooses the action whose outcome, whatever it is, will most likely achieve the goal. 2 See Leslie Kaelbling's Ph.D. thesis [7] for information on "Learning in Embedded Systems". 3 Peter Cheeseman <ref> [1] </ref> has made a good case for the use of probability in machine learning. 3 At first it may be unclear how this method accomplishes exploration; won't it just choose whatever previous action failed the least, and not generate new actions? The answer is no, because the correct probabilistic formulas automatically
Reference: [2] <author> Peter Cheeseman. </author> <title> On finding the most probable model. In Jeff Shrager and Pat Langley, editors, Computational Models of Scientific Discovery and Theory Formation, </title> <booktitle> chapter 3, </booktitle> <pages> pages 73-95. </pages> <publisher> Unknown Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Even then, the analysis is not simple. This work seems like a case study in how probabilistic methods seem correct, but can often be hard to apply. However, probabilistic methods have had some interesting successes in machine learning, such as in Moore's system and in Cheeseman's work on Autoclass <ref> [3, 2] </ref>.
Reference: [3] <author> Peter Cheeseman, James Kelley, Matthew Self, John Stutz, Will Taylor, and Don Freeman. </author> <title> Autoclass: A bayesian classification system. </title> <booktitle> In Shavlik and Dietterich [17], </booktitle> <pages> pages 296-306. </pages>
Reference-contexts: Even then, the analysis is not simple. This work seems like a case study in how probabilistic methods seem correct, but can often be hard to apply. However, probabilistic methods have had some interesting successes in machine learning, such as in Moore's system and in Cheeseman's work on Autoclass <ref> [3, 2] </ref>.
Reference: [4] <author> Thomas Dean, Kenneth Bayse, Robert Chekaluk, Seungseok Hyun, Moises Lejter, and Margaret Randazza. </author> <title> Coping with uncertainty in a control system for navigation and exploration. </title> <booktitle> In Proceedings Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1010-1015, </pages> <address> Boston, MA, </address> <month> 29 July - 3 August </month> <year> 1990. </year> <booktitle> American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press / The MIT Press. </publisher>
Reference: [5] <author> Michael R. Genesereth. </author> <title> A comparative analysis of some simple architectures for autonomous agents. Other publication data unknown, </title> <year> 1988. </year>
Reference-contexts: There must also be an environment that exececutes actions and provides resulting percepts, and there must be a given goal state of the environment. These are all parts of standard task definitions for agents doing problem-solving, such as in Genesereth's work <ref> [5] </ref>. The difference is in their algorithm; their procedure alternates between generating an "exploration plan," learning from the results of the exploration, and attempting straight problem-solving.
Reference: [6] <editor> IJCAI. </editor> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <year> 1989. </year>
Reference: [7] <author> Leslie Pack Kaelbling. </author> <title> Learning in Embedded Systems. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Instead of choosing the action whose outcome is most uncertain, Moore's system chooses the action whose outcome, whatever it is, will most likely achieve the goal. 2 See Leslie Kaelbling's Ph.D. thesis <ref> [7] </ref> for information on "Learning in Embedded Systems". 3 Peter Cheeseman [1] has made a good case for the use of probability in machine learning. 3 At first it may be unclear how this method accomplishes exploration; won't it just choose whatever previous action failed the least, and not generate new
Reference: [8] <author> Pat Langley, Herbert A. Simon, and Gary L. Bradshaw. </author> <title> Heuristics for empirical discovery. </title> <booktitle> In Shavlik and Dietterich [17], </booktitle> <pages> pages 356-372. </pages>
Reference-contexts: An example is the "learning by discovery" research, in which the learning systems often conduct experiments to test their hypotheses. The discovery-learning viewpoint, however, often focuses on abstract realms of applicability such as mathematics or science (e.g. the AM [9] and BACON <ref> [8] </ref> programs). There is less work on what an active role in learning 1 See Shavlik and Dietterich [16] for a good overview of classification learning. 1 would be like for general agents, operating in an everyday domain, instead of a mathematical or scientific one.
Reference: [9] <author> Douglas B. Lenat. </author> <title> The ubiquity of discovery. </title> <booktitle> In Shavlik and Dietterich [17], </booktitle> <pages> pages 341-354. </pages>
Reference-contexts: An example is the "learning by discovery" research, in which the learning systems often conduct experiments to test their hypotheses. The discovery-learning viewpoint, however, often focuses on abstract realms of applicability such as mathematics or science (e.g. the AM <ref> [9] </ref> and BACON [8] programs). There is less work on what an active role in learning 1 See Shavlik and Dietterich [16] for a good overview of classification learning. 1 would be like for general agents, operating in an everyday domain, instead of a mathematical or scientific one.
Reference: [10] <author> Tom M. Mitchaell. </author> <title> Generalization as search. </title> <booktitle> In Shavlik and Dietterich [17], </booktitle> <pages> pages 96-107. </pages>
Reference-contexts: The difference is in their algorithm; their procedure alternates between generating an "exploration plan," learning from the results of the exploration, and attempting straight problem-solving. The learning phase involves a fairly straightforward generation and specialization process, and can be thought of in terms of Mitchell's version spaces <ref> [10] </ref> and related work. The problem-solving phase is simple STRIPS-style planning. The novelty lies in the creation of the exploration plans, which are intended to provide "suprising" experiences to serve as the basis for new rules.
Reference: [11] <author> Tom M. Mitchaell and Paul E. Utgoff. </author> <title> Learning by experimentation: Acquiring and refining problem-solving heuristics. </title> <booktitle> In Shavlik and Dietterich [17], </booktitle> <pages> pages 510-522. </pages>
Reference-contexts: The problem-solving phase is simple STRIPS-style planning. The novelty lies in the creation of the exploration plans, which are intended to provide "suprising" experiences to serve as the basis for new rules. This process is reminiscent of the creation of "practice problems" in Mitchell's heuristic problem solver, LEX <ref> [11] </ref>, except that Shen and Simon's work is focused on the creation of new pieces of the domain theory and not just heuristics for guiding planning. The design of Shen and Simon's algorithm of course depends on their definition of the exploration problem.
Reference: [12] <author> Tom M. Mitchell. </author> <title> The need for biases in learning generalizations. </title> <booktitle> In Shavlik and Dietterich [17], </booktitle> <pages> pages 184-191. 6 </pages>
Reference-contexts: These programs all have a kind of built-in bias, a bias towards dealing with a particular goal-space. We can call this the goal-space bias, and think of it as a specialized kind of bias similar to the inductive bias introduced by Mitchell <ref> [12] </ref>.
Reference: [13] <author> Andrew W. Moore. </author> <title> Acquisition of dynamic control knowledge for a robotic manipulator. </title> <editor> In Bruce W. Porter and Ray J. Mooney, editors, </editor> <booktitle> Proceedings of the Seventh International Workshop on Machine Learning, </booktitle> <pages> pages 244-252, </pages> <institution> University of Texas, Austin, </institution> <address> TX, June 21-23 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: using probabilty to represent learned knowledge when uncertainty is involved. 3 The vague goal of "exploration" can be quantified nicely in terms of reducing uncertainty about the environment, and Scott and Markovitch do so. 4 Robotic Manipulators Andrew Moore's recent paper, "Acquisition of Dynamic Control Knowledge for a Robotic Manipulator," <ref> [13] </ref> resembles the previous paper in two ways: it treats the environment as providing a fixed-size vector of attributes, instead of a variable-size database of propositions, and it uses probabilities in choosing candidate actions for exploring.
Reference: [14] <author> Barney Pell. </author> <title> Exploratory learning in the game of go: Initial results. </title> <booktitle> In Proceedings of the London Computer Game Playing Conference, </booktitle> <address> London, </address> <year> 1990. </year> <title> Unknown organization. </title>
Reference-contexts: Thus, Moore's system searches new areas of the environment, until the correct action is found. 4 Barney Pell uses a similar "chance of success" criterion for his explorative learning program for the game of go <ref> [14] </ref>. Moreover, in both of these programs, the exploration is not really kept separate from the learning; the single strategy of choosing "most likely to succeed" actions seems to accomplish both the selecting of unexplored states and the achieving of the current goal based on the past history. <p> Still, the basic ideas are the same. 5 Pell notes this too, in <ref> [14] </ref>. 4 known at the time of exploration, and whether more goals are expected to arise after the current one is achieved.
Reference: [15] <author> Paul D. Scott and Shaul Markovitch. </author> <title> Learning novel domains through curiosity and conjecture. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence [6], </booktitle> <pages> pages 669-674. </pages>
Reference-contexts: Their exploration algorithm makes sense only given this state of affairs. Such background assumptions about the nature of goals turn out to be a central theme in exploring, which I will discuss later. 2 3 Classification Learning In constast, the next paper, "Learning Novel Domains Through Curiosity and Conjecture," <ref> [15] </ref> by Scott and Markovitch, assumes that all the exploration takes place before any specific goals are known. This training-testing distinction reveals the classification/concept-learning perspective that Scott and Markovitch take.
Reference: [16] <author> Jude W. Shavlik and Thomas G. Dietterich. </author> <title> Inductive learning from preclassified training examples. </title> <booktitle> In Readings in Machine Learning [17], chapter 2.1, </booktitle> <pages> pages 45-56. </pages>
Reference-contexts: The discovery-learning viewpoint, however, often focuses on abstract realms of applicability such as mathematics or science (e.g. the AM [9] and BACON [8] programs). There is less work on what an active role in learning 1 See Shavlik and Dietterich <ref> [16] </ref> for a good overview of classification learning. 1 would be like for general agents, operating in an everyday domain, instead of a mathematical or scientific one.
Reference: [17] <author> Jude W. Shavlik and Thomas G. Dietterich, </author> <title> editors. </title> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [18] <author> Wei-Min Shen and Herbert A. Simon. </author> <title> Rule creation and rule learning through environmental exploration. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence [6], </booktitle> <pages> pages 675-680. 7 </pages>
Reference-contexts: Section 6 of this paper will attempt to generalize the four approaches to present a broader view of exploration in terms of which the particular approaches can be described. 2 Problem Solving In the paper, "Rule Creation and Rule Learning through Environmental Exploration," <ref> [18] </ref>, Shen and Simon describe a system for "learning from environment," which integrates exploration, learning, and problem-solving in a single framework. As would be expected from Simon, the work is totally within the classical AI tradition of logic-oriented problem-solving and planning; their program is based on STRIPS and GPS.
References-found: 18

