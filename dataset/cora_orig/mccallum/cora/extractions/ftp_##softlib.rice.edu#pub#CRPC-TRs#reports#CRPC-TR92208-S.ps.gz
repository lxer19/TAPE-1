URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92208-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Compiler Blockability of Numerical Algorithms  
Author: Steve Carr Ken Kennedy 
Address: Houston TX 77251-1892  
Affiliation: Department of Computer Science Rice University  
Abstract: Over the past decade, microprocessor design strategies have focused on increasing the computational power on a single chip. Unfortunately, memory speeds have not kept pace. The result is an imbalance between computation speed and memory speed. This imbalance is leading machine designers to use more complicated memory hierarchies. In turn, programmers are explicitly restructuring codes to perform well on particular memory systems, leading to machine-specific programs. This paper describes our investigation into compiler technology designed to obviate the need for machine-specific programming. Our results reveal that through the use of compiler optimizations many numerical algorithms can be expressed in a natural form while retaining good memory performance. 
Abstract-found: 1
Intro-found: 1
Reference: [AK87] <author> J.R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: A dependence is carried by a loop if the references at the source and sink of the dependence are on different iterations of the loop and the dependence is not carried by an outer loop <ref> [AK87] </ref>. To enhance the dependence information, section analysis can be used to describe the portion of an array that is accessed by a particular reference or set of references [CK87, HK91]. <p> However, this would result in a performance degradation due to a decrease in loop-level parallelism and an increase in instructions executed. Instead, a combination of IF-conversion and sparse-matrix techniques, called IF-inspection, can be used to keep the guard out of the innermost loop and still allow unroll-and-jam <ref> [AK87] </ref>. The idea is to inspect at run-time the values of an outer-loop induction variable for which the guard is true and the inner loop is executed. Then, the inner-loop nest is executed only for those values.
Reference: [Car92] <author> S. Carr. </author> <title> Memory-Hierarchy Management. </title> <type> PhD thesis, </type> <institution> Rice University, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: DO 10 I = 1,N,IS DO 10 II = I,MIN ((J-fi)/ff,I+IS-1) 10 loop body This formula can be trivially extended to handle the cases where ff &lt; 0 and where a linear function of I appears in the upper bound instead of the lower bound <ref> [Car92] </ref>. Triangular strip-mine-and-interchange can be extended to triangular unroll-and-jam as follows. Since the iteration space defined by the two inner loops is a trapezoidal region, the number of iterations of the innermost loop vary with J, making unrolling more difficult. <p> Ad ditionally, triangular unroll-and-jam can be extended to handle other common triangles <ref> [Car92] </ref>. 3.2 Trapezoidal iteration spaces While the previous method applies to many of the common non-rectangular-shaped iteration spaces, Page 3 there are still some important loops that it will not handle. In linear algebra, seismic and partial differential equation codes, loops with trapezoidal-shaped iteration spaces occur. <p> To handle this loop, blocking can be extended to rhomboidal regions using index-set splitting similar to the case for triangular regions <ref> [Car92] </ref>. As another more complex example, consider the following loop which computes the convolution of two time series. DO 10 I = 0,N3 10 F3 (I) = F3 (I)+DT*F1 (K)*F2 (I-K) The MAX function in the lower bound can be index-set split similar to a MIN function [Car92]. <p> for triangular regions <ref> [Car92] </ref>. As another more complex example, consider the following loop which computes the convolution of two time series. DO 10 I = 0,N3 10 F3 (I) = F3 (I)+DT*F1 (K)*F2 (I-K) The MAX function in the lower bound can be index-set split similar to a MIN function [Car92]. In this example, complete splitting to remove the functions from the loop bounds would result in four separate loops that can each be blocked. The previous two loops come from an oil exploration program and constitute 20% of the program's execution time.
Reference: [CCK90] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: In addition, the temporal reuse of JS values of B out of cache occurs for every iteration of the J-loop if JS is less than the size of the cache and there is no interference [LRW91]. A transformation analogous to strip-mine-and-interchange is unroll-and-jam <ref> [CCK90] </ref>. Unroll-and-jam is used for register blocking instead of cache blocking and can be seen as an application of strip mining, loop interchange and loop unrolling. Essentially, the inner loop is completely unrolled after strip-mine-and-interchange to effect unroll-and-jam. <p> The previous two loops come from an oil exploration program and constitute 20% of the program's execution time. After performing index-set splitting, unroll-and-jam and a transformation called scalar replacement on both loops, we ran them on arrays of double-precision REALS <ref> [CCK90] </ref>. Below is a table of the results on an IBM RS/6000 540. For timing measurements, we iterated over each kernel 1000 times with 75% of the execution in the triangular regions.
Reference: [CK87] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <year> 1987. </year>
Reference-contexts: To enhance the dependence information, section analysis can be used to describe the portion of an array that is accessed by a particular reference or set of references <ref> [CK87, HK91] </ref>. Sections describe common substructures of arrays such as elements, rows, columns and diagonals. 2.2 Cache reuse When applied to memory-hierarchy management, a dependence can be thought of as an opportunity for reuse. There are two types of reuse: temporal and spatial.
Reference: [CK89] <author> S. Carr and K. Kennedy. </author> <title> Blocking linear algebra codes for memory hierarchies. </title> <booktitle> In Proceedings of the Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Chicago, IL, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: To assist this research, Dongarra and Sorensen have contributed point and block versions of several algorithms used in LAPACK. This paper extends some preliminary results of our efforts to define the compiler algorithms that would be needed to generate the block algorithms from the point algorithms automatically <ref> [CK89] </ref>.
Reference: [DDSvdV91] <author> J.J. Dongarra, I.S. Duff, D.C. Sorensen, and H.A. van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared-Memory Computers. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991. </year>
Reference-contexts: To investigate the viability of this approach, we embarked on an experiment to determine if a compiler could automatically generate the block algorithms in LAPACK from the corresponding point algorithms expressed in Fortran 77 <ref> [DDSvdV91] </ref>. To assist this research, Dongarra and Sorensen have contributed point and block versions of several algorithms used in LAPACK. This paper extends some preliminary results of our efforts to define the compiler algorithms that would be needed to generate the block algorithms from the point algorithms automatically [CK89]. <p> It may be the case that an inner loop is guarded by an IF-statement to prevent unnecessary computation. Consider the following matrix multiply code that is take from the BLAS routine SGEMM <ref> [DDSvdV91] </ref>. DO 20 J = 1,N IF (B (K,J) .EQ. 0.0) GOTO 20 DO 10 I = 1,N 20 CONTINUE If the IF-statement were ignored and unroll-and-jam were performed on the K-loop, references to B would be introduced that would never be checked by the guard. <p> To improve its cache performance, scientists have developed a block algorithm that essentially groups a number of updates to the matrix A together and applies them all at once to a block portion of the array <ref> [DDSvdV91] </ref>. To attain the best block version, strip-mine-and-interchange is completed for the K-loop on only a portion of the inner loop nest. We show how to attain the block algorithm using IndexSetSplit. <p> This would seem to preclude the existence of a block analogue similar to the non-pivoting case. However, a block algorithm that ignores the preventing recurrence and distributes the KK-loop can still be mathematically derived (see Figure 8) <ref> [DDSvdV91] </ref>. This block algorithm also exhibits the increased loop-level parallelism found in the algorithm in Figure 6. In the point version, each row interchange is followed by a whole-column update in which each row element is updated independently. <p> Although pivoting is not necessary for QR decomposition, the best block algorithm is not an aggrega tion of the original algorithm. The block application Page 8 of a number of elementary reflectors involves both computation and storage that does not exist in the original algorithm <ref> [DDSvdV91] </ref>.
Reference: [HK91] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: To enhance the dependence information, section analysis can be used to describe the portion of an array that is accessed by a particular reference or set of references <ref> [CK87, HK91] </ref>. Sections describe common substructures of arrays such as elements, rows, columns and diagonals. 2.2 Cache reuse When applied to memory-hierarchy management, a dependence can be thought of as an opportunity for reuse. There are two types of reuse: temporal and spatial. <p> The effectiveness of IndexSetSplit depends upon the representation of sections. The precision must be enough to relate the locations in the array to index variable values. The representation that we have chosen is equivalent to Fortran 90 array notation <ref> [HK91] </ref>. In Section 5, we show that this representation allows IndexSetSplit to greatly enhance the performance of solving systems of linear equations. 4 IF-inspection In addition to iteration-space shapes and dependence patterns, the effects of control flow on blocking must also be considered.
Reference: [IT88] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Conference Record of the Fifteenth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 319-328, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: We take this a step further by developing a general technique that handles more cases. Irigoin and Triolet describe a general technique for blocking iteration spaces for memory that uses a dependence abstraction called a dependence cone <ref> [IT88] </ref>.
Reference: [KKP + 81] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eight ACM Symposium on the Principles of Programming Languages, </booktitle> <year> 1981. </year>
Reference: [Kuc78] <author> D. Kuck. </author> <title> The Structure of Computers and Computations Volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: A dependence exists between two statements if there exists a control flow path from the first statement to the second, and both statements reference the same memory location <ref> [Kuc78] </ref>. * If the first statement writes to the location and the second reads from it, there is a true depen dence, also called a flow dependence. * If the first statement reads from the location and the second writes to it, there is an antidepen dence. * If both statements
Reference: [LRW91] <author> M.S. Lam, E.E. Rothberg, and M.E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architecural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: In addition, the temporal reuse of JS values of B out of cache occurs for every iteration of the J-loop if JS is less than the size of the cache and there is no interference <ref> [LRW91] </ref>. A transformation analogous to strip-mine-and-interchange is unroll-and-jam [CCK90]. Unroll-and-jam is used for register blocking instead of cache blocking and can be seen as an application of strip mining, loop interchange and loop unrolling. Essentially, the inner loop is completely unrolled after strip-mine-and-interchange to effect unroll-and-jam.
Reference: [Por89] <author> A.K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Strip-mine-and-interchange is a transformation that achieves this result <ref> [Wol87, Por89, WL91] </ref>. It shortens the distance between the source and sink of a dependence so that it is more likely for the datum to reside in cache when the reuse occurs. Consider the following loop nest.
Reference: [Sew90] <author> G Sewell. </author> <title> Computational Methods of Linear Algebra. </title> <publisher> Ellis Horwood, </publisher> <address> England, </address> <year> 1990. </year>
Reference-contexts: However, the expressibility of a language can be enhanced to allow block algorithms to be stated in a machine-independent form. In Section 6, we address this issue. 5.4 Givens QR Another form of orthogonal matrix that can be used in QR decomposition is the Givens rotation matrix <ref> [Sew90] </ref>. We currently know of no best block algorithm to derive from the point algorithm, so instead we show that IndexSetSplit and IF-inspection have wider applicability. Consider the Fortran code for Givens QR shown in Figure 9 [Sew90]. <p> matrix that can be used in QR decomposition is the Givens rotation matrix <ref> [Sew90] </ref>. We currently know of no best block algorithm to derive from the point algorithm, so instead we show that IndexSetSplit and IF-inspection have wider applicability. Consider the Fortran code for Givens QR shown in Figure 9 [Sew90]. The references to A in the inner K-loop have a long stride between successive accesses, resulting in poor cache performance.
Reference: [Ste73] <author> G.W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: This decomposition can be obtained by multiplying the matrix A by a series of elementary lower triangular matrices, where L = M 1 1 , as follows <ref> [Ste73] </ref>. U = M k : : : M 1 A Using this equation, an algorithm for LU decomposition without pivoting using Gaussian elimination is derived. The point algorithm, where statement 20 computes M k and statement 10 applies M k to A, is shown below after strip mining. <p> Any class of matrices that have this property can be used to solve a system of linear equations. One such class, having orthonormal columns, is used in QR decomposition <ref> [Ste73] </ref>. If A has linearly independent columns, then A can be written uniquely in the form A = QR, where Q has orthonormal columns, QQ T = I and R is upper triangular with positive diagonal elements. <p> Each V k eliminates the values below the diagonal in the kth column. For a more detailed discussion of the QR algorithm and the computation of V k , see Stewart <ref> [Ste73] </ref>. Although pivoting is not necessary for QR decomposition, the best block algorithm is not an aggrega tion of the original algorithm. The block application Page 8 of a number of elementary reflectors involves both computation and storage that does not exist in the original algorithm [DDSvdV91].
Reference: [WL91] <author> M.E. Wolf and M.S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIG-PLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Strip-mine-and-interchange is a transformation that achieves this result <ref> [Wol87, Por89, WL91] </ref>. It shortens the distance between the source and sink of a dependence so that it is more likely for the datum to reside in cache when the reuse occurs. Consider the following loop nest. <p> Wolf and Lam present a framework for applying blocking transformations and ordering a loop nest for memory performance and parallelism <ref> [WL91] </ref>. However, their framework does not include the application of index-set splitting nor is it applicable to non-perfectly nested loops. 8 Summary We have set out to determine whether a compiler can automatically restructure computations well enough to avoid the need for hand blocking.
Reference: [Wol82] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois, </institution> <month> Oc-tober </month> <year> 1982. </year>
Reference-contexts: Unfortunately, there is a recurrence between the definition of A (K) and the use of A (II) carried by the II-loop, preventing interchange with distribution. Standard dependence abstractions, such as distance or direction vectors, report that the recurrence exists for every value defined by A (K) <ref> [Wol82] </ref>. This means blocking is prevented. However, analyzing the sections of the arrays that are accessed at the source and sink of the backward true dependence reveals that there is potential to apply blocking. Consider written by A (K) goes from I to N.
Reference: [Wol86] <author> M. Wolfe. </author> <title> Advanced loop interchange. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: Interchanging loops that iterate over a triangular regions requires the modification of the loop bounds to preserve the semantics of the loop <ref> [Wol86, Wol87] </ref>. Therefore, blocking triangular regions also requires loop bound modification. Below, we derive the formula for determining loop bounds when blocking is performed on triangular iteration spaces. We begin with the derivation for strip-mine-and-interchange and then extend it to unroll-and-jam. <p> By doing so, the machine-dependency problem of LA-PACK would be removed, making LAPACK readily accessible to new architectures. 7 Previous work Wolfe has done a significant amount of work on cache blocking <ref> [Wol86, Wol87, Wol89] </ref>. In particular, he discusses strip-mine-and-interchange for triangular-shaped iteration spaces, but he does not present general compiler algorithms nor extend the work to unroll-and-jam. He also shows by example how to use index-set splitting to handle a trapezoidal region that arises from triangular iteration-space blocking.
Reference: [Wol87] <author> M. Wolfe. </author> <title> Iteration space tiling for memory hierarchies. </title> <booktitle> In Proceedings of the Third SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> December </month> <year> 1987. </year>
Reference-contexts: Strip-mine-and-interchange is a transformation that achieves this result <ref> [Wol87, Por89, WL91] </ref>. It shortens the distance between the source and sink of a dependence so that it is more likely for the datum to reside in cache when the reuse occurs. Consider the following loop nest. <p> Interchanging loops that iterate over a triangular regions requires the modification of the loop bounds to preserve the semantics of the loop <ref> [Wol86, Wol87] </ref>. Therefore, blocking triangular regions also requires loop bound modification. Below, we derive the formula for determining loop bounds when blocking is performed on triangular iteration spaces. We begin with the derivation for strip-mine-and-interchange and then extend it to unroll-and-jam. <p> By doing so, the machine-dependency problem of LA-PACK would be removed, making LAPACK readily accessible to new architectures. 7 Previous work Wolfe has done a significant amount of work on cache blocking <ref> [Wol86, Wol87, Wol89] </ref>. In particular, he discusses strip-mine-and-interchange for triangular-shaped iteration spaces, but he does not present general compiler algorithms nor extend the work to unroll-and-jam. He also shows by example how to use index-set splitting to handle a trapezoidal region that arises from triangular iteration-space blocking.
Reference: [Wol89] <author> M. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proceedings of the Supercomputing '89 Conference, </booktitle> <year> 1989. </year> <pages> Page 11 </pages>
Reference-contexts: By doing so, the machine-dependency problem of LA-PACK would be removed, making LAPACK readily accessible to new architectures. 7 Previous work Wolfe has done a significant amount of work on cache blocking <ref> [Wol86, Wol87, Wol89] </ref>. In particular, he discusses strip-mine-and-interchange for triangular-shaped iteration spaces, but he does not present general compiler algorithms nor extend the work to unroll-and-jam. He also shows by example how to use index-set splitting to handle a trapezoidal region that arises from triangular iteration-space blocking.
References-found: 19

