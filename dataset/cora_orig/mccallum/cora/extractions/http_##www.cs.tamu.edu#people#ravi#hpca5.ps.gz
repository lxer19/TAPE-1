URL: http://www.cs.tamu.edu/people/ravi/hpca5.ps.gz
Refering-URL: http://www.cs.tamu.edu/people/ravi/
Root-URL: http://www.cs.tamu.edu
Email: E-mail: fravi,bhuyang@cs.tamu.edu  
Title: Switch Cache: A Framework for Improving the Remote Memory Access Latency of CC-NUMA Multiprocessors  
Author: Ravi Iyer and Laxmi Bhuyan 
Keyword: cache architectures, crossbar switches, interconnection network, wormhole routing, shared memory multiprocessors, execution-driven simulation.  
Note: This research has been supported by NSF grant MIP 9622740.  
Address: College Station, TX 77843-3112, USA.  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: Cache coherent non-uniform memory access (CC-NUMA) multiprocessors provide users with large computational power by employing state-of-the-art superscalar processors. However, they continue to suffer from remote memory access latencies due to comparatively slow memory technology and data transfer latencies in the interconnection network. In this paper, we propose a novel hardware caching technique, called switch cache, to improve the remote memory access performance of shared memory multiprocessors. The main idea is to implement small fast caches in crossbar switches of the interconnect medium to capture and store shared data as they flow from the memory module to the requesting processor. This stored data acts as a cache for subsequent requests from other processors and thus reduces the latency of remote memory accesses tremendously. The implementation of a cache in a crossbar switch needs to be efficient and robust, yet, flexible for changes in the caching protocol. The design and implementation details of a CAche Embedded Switch ARchitecture, CAESAR, using wormhole routing with virtual channels is presented. We perform extensive simulations by modeling CAESAR in a detailed execution-driven simulator and analyze the performance benefits of switch caches. We find that switch caching is capable of improving the performance of CC-NUMA multiprocessors by reducing the number of reads served at distant remote memories by up to 45%. By serving these remote reads at a much lower latency, we observe improvements in application execution time as high as 20%. We conclude that the switch caches provide a cost-effective solution by improving the remote access performance of CC-NUMA multiprocessors to a high extent. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Anderson and T. Shanley, </author> <title> "Pentium Processor System Architecture," </title> <publisher> MindShare Inc., Addison-Wesley Publishing Company, </publisher> <address> Second Edition, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: The tag array is dual ported to allow two independent requests to access the tag at the same time, similar to the on-chip cache employed by the Pentium processor <ref> [1] </ref>. We now describe the switch cache access operations and their associated access delays. When the incoming flits are processed, we observe that requests to the switch cache can be broken into two types of requests: snoop requests and regular requests.
Reference: [2] <author> G. Astfalk and T. Brewer., </author> <title> "An Overview of the HP/Convex Exemplar Hardware.," </title> <address> http://www.convex.com/tech cache/ps/hw ov.ps. </address>
Reference-contexts: However the contention on the bus and system scalability issues heavily constrain the number of processors that can be connected to the bus. To build high performance systems that are highly scalable, several current systems <ref> [2, 12, 14, 15] </ref> employ the CC-NUMA architecture. The CC-NUMA architecture is based on connecting small bus-based processor-memory clusters using scalable interconnects. In such a system, the shared memory is distributed among all the nodes in the system to provide a closer local memory and several remote memories. <p> Network caches reduce the remote access penalty by serving the capacity misses of L2 caches and providing an additional shared cache layer to processors within the cluster. Current systems have implemented the network cache in different ways. The HP Exemplar <ref> [2] </ref> implements the network cache as a configurable partition of the local memory. Sequent's NUMA-Q [15] dedicates a 32MB DRAM memory for the network cache. The DASH multiprocessor [14] has provision for a network cache called the remote access cache.
Reference: [3] <author> BBN Laboratories Inc., </author> <title> "Butterfly Parallel Processor Overview, </title> <type> version 1," </type> <month> Dec. </month> <year> 1985. </year>
Reference-contexts: While offering an inherent tree-structure, the MIN is also highly scalable and provides a bisection bandwidth that scales linearly with the number of nodes in the system. These features of the MIN make it very attractive as high performance networks for commercial systems. Existing systems such as Butterfly <ref> [3] </ref>, CM-5 [13] and IBM SP2 [23] employ a bidirectional MIN. The Illinois Cedar multiprocessor [24] employs two separate uni-directional MINs (one for requests and one for replies). In this paper, the switch cache interconnect is a bidirectional MIN to take advantage of the inherent tree structure.
Reference: [4] <author> L. Bhuyan, R. Iyer et al., </author> <title> "Performance of the Multistage Bus Networks for a Distributed Shared Memory Multiprocessor," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 8(1) </volume> <pages> 82-95, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: Tree-based networks like the fat tree [13] , the hierarchical bus network <ref> [25, 4] </ref> and the multistage interconnection network (MIN) [17] provide hierarchical topologies suitable for global caching. While offering an inherent tree-structure, the MIN is also highly scalable and provides a bisection bandwidth that scales linearly with the number of nodes in the system. <p> In this paper, the switch cache interconnect is a bidirectional MIN to take advantage of the inherent tree structure. Note, however, that logical trees can be embedded on most other direct networks. The baseline topology of the 16-node bi-directional MIN (BMIN) <ref> [4] </ref> is shown in Figure 7a. In general, an N -node system using a BMIN comprises of N=k switching elements (a 2k fi 2k crossbar) in each of the log k N stages connected by bidirectional links.
Reference: [5] <author> L. Bhuyan, et al., </author> <title> "The Impact of Switch Design on the Application Performance of Shared Memory Multiprocessors," </title> <booktitle> International Parallel Processing Symposium, </booktitle> <month> Mar </month> <year> 1998. </year>
Reference-contexts: We found a high degree of read sharing among many applications to benefit from such an idea. The result is that such memory read requests are satisfied in the switch and the need to access the slow shared memory is avoided. Our recent study <ref> [5] </ref> indicates that increasing the buffer size beyond a certain value in a switch does not have much impact on the application performance for a shared memory multiprocessor. Thus we think that the large amount of buffers in current switches, such as SPIDER [10], is an overkill. <p> Queueing delays (denoted by Qs) at the network interface and memory module comprise a significant portion of the latency when the resource contention is high (i.e. when transactions arrive in bulk) <ref> [5] </ref>. 2.2 Ideal Solution: A Global Cache To reduce the impact of the remote read transactions, we would like to exploit the sharing pattern of the processors. Figure 3 plots the read sharing pattern for each application. <p> FFT is unaffected by the use of switch caches. Queuing delays in the network interface account for a significant portion of the remote access latency for two of the applications since read requests to a memory module arrive in bulk <ref> [5] </ref>. the six applications, the use of switch caching reduces the stall time by as high as 35% over the base system. We observe that a small cache size of 512 bytes is sufficient to provide a reasonable performance benefit.
Reference: [6] <author> J. Carbonaro and F. Verhoorn, "Cavallino: </author> <title> The Teraflops Router and NIC," </title> <booktitle> Proc. Symp. High Performance Interconnects (Hot Interconnects 4), </booktitle> <month> Aug. </month> <pages> 19 96. </pages>
Reference-contexts: Recall that our network is wormhole-routed. Thus messages are made up of flow control digits or flits. Each flit is 8 bytes as in Spider [10] and Cavallino <ref> [6] </ref>. The header flit of the message is the only one that contains the routing information, while tail flits follow the path created by the header. The header of the message follows the format shown in Figure 9. <p> Crossbar switches mainly differ in two design issues: switching technique and buffer management. As mentioned earlier, we use wormhole routing as the switching technique and input buffering with virtual channels [8] as the buffer management scheme since these are prevalent in current commercial crossbar switches <ref> [6, 10] </ref>. 3.3.1 Base Crossbar Switch Our base bi-directional crossbar switch has four inputs and four outputs as shown in Figure 10. Each input link in the crossbar switch has two virtual channels thus providing 8 possible input candidates for arbitration. <p> At each arbitration cycle, a maximum of 4 highest age flits are selected from 8 possible arbitration candidates. The internal switch core and transmission over the link operate at 200MHz (like Cavallino <ref> [6] </ref>). The wire width at the link is w = 16 bits. The 8 fi 4 crossbar switch takes 4 cycles to arbitrate and move flits from the input of the switch to the link transmitter at the each output. <p> Thus, it takes four cycles for the link to transmit a flit from one switch to another. The resultant throughput of the switch is 1 flit served per cycle. Note that the wire width and flit size parameters are the same as in the Cavallino switch <ref> [6] </ref>. Buffering in the crossbar switch is 14 provided in the input block at each link. The input block is organized as a fixed size FIFO buffer for each virtual channel to store flits belonging to the same message. <p> The IN employs 4 fi 4 switches organized in 4 stages as shown earlier in Figure 7. Virtual channels were added to the switching elements of the IN to simulate the behavior of commercial switches like Cavallino <ref> [6] </ref> and Spider [10]. The crossbar switch operation is similar to the description in Section 3.3.1. Each input link to the switch is provided with 2 virtual channel buffers capable of storing a maximum of 4 flits from a single message.
Reference: [7] <author> L. M. Censier and P. Feautrier, </author> <title> "A New Solution to Coherence Problems in Multicache Systems," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-27, no. 12, </volume> <pages> pp. 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: The base system employs 16 200MHz superscalar processors with a 16KB L1 cache and a 128KB L2 cache. Coherence is maintained using an invalidation-based three-state cache protocol and a full-map directory protocol <ref> [7] </ref>. The system enforces release consistency employing a write buffer for each processor. A wormhole routed bidirectional MIN is fully simulated as the interconnect between processor-memory nodes. <p> The effect of the organization of the cache on performance will be studied in Section 5. 3.2 The Caching Protocol The introduction of processor caches in a multiprocessor introduces the cache coherence problem. Most hardware cache-coherent systems employ a full-map directory scheme <ref> [7] </ref>, In this scheme, each node maintains a full-map bit vector of length N to keep track of the sharers of each block in its local shared memory space. <p> The raw memory access time is 40 cycles, but it takes more than 50 cycles to submit the request to the memory subsystem and read the data over the memory bus. The system employs the full-map three-state directory protocol <ref> [7] </ref> and the MSI cache protocol to maintain cache coherence. The system uses release consistency model. We modified RSIM to employ a wormhole routed bidirectional MIN (instead of a 2D mesh) as the interconnection network (IN).
Reference: [8] <author> W. J. Dally, </author> <title> "Virtual-Channel Flow Control," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 194-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The contribution of this paper is the detailed design and performance evaluation of a switch cache interconnect employing CAESAR, a CAche Embedded Switch ARchitecture. The CAESAR switch cache is a dual-ported SRAM cache operating at the same speed as a wormhole routed crossbar switch with virtual channels <ref> [8] </ref>. The switch design is optimized to maintain crossbar bandwidth and throughput, while at the same time providing sufficient switch cache throughput and improved remote access performance. Cache design optimizations that employ interleaved banks or wider data arrays are studied to further improve the cache throughput. <p> Similar to Spider [10], virtual channels are 9 also employed to improve the performance of the system when bulky arrival of messages congest the system and worms get blocked. They also help in avoiding deadlocks in the network <ref> [8] </ref>. In a shared memory system, communication between nodes is accomplished via read/write transactions and coherence requests/acknowledgments. The read/write requests and coherence acknowledgments from the processor to the memory use the forward links to traverse through the switches. <p> Crossbar switches mainly differ in two design issues: switching technique and buffer management. As mentioned earlier, we use wormhole routing as the switching technique and input buffering with virtual channels <ref> [8] </ref> as the buffer management scheme since these are prevalent in current commercial crossbar switches [6, 10]. 3.3.1 Base Crossbar Switch Our base bi-directional crossbar switch has four inputs and four outputs as shown in Figure 10.
Reference: [9] <author> J. Edmondson, </author> <title> "Internal Organization of the Alpha 21164 a 300-MHz 64-bit Quad-Issue CMOS RISC Microprocessor," </title> <journal> Digital Technical Journal, Special 10th Anniversary Issue, </journal> <volume> vol. 7, no. 1, </volume> <pages> pp. 119-135, </pages> <year> 1995. </year>
Reference-contexts: By employing such an interleaving, the cache can potentially serve two requests at a time. Another rather expensive method used to provide multiple ports is to employ duplicate copies of the same cache as in Alpha 21164 <ref> [9] </ref>. In this paper, we consider single ported caches as well as banked caches to improve the switch cache throughput. We evaluate and compare the performance of both the switch architectures. While designing the cache subsystem, our aim is to maximize cache bandwidth/throughput.
Reference: [10] <author> M. Galles, </author> <title> "Scalable Pipelined Interconnect for Distributed Endpoint Routing: The SGI SPIDER Chip," </title> <booktitle> Proc. Symp. High Performance Interconnect s (Hot Interconnects 4), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Our recent study [5] indicates that increasing the buffer size beyond a certain value in a switch does not have much impact on the application performance for a shared memory multiprocessor. Thus we think that the large amount of buffers in current switches, such as SPIDER <ref> [10] </ref>, is an overkill. A better utilization of these buffers can be accomplished by organizing them as a switch cache. There are several issues to be considered while designing such a caching technique. <p> We chose wormhole routing as the switching technique because it is prevalent in current systems [12, 23]. Furthermore, several studies have shown that it outperforms packet switching to a great extent. Similar to Spider <ref> [10] </ref>, virtual channels are 9 also employed to improve the performance of the system when bulky arrival of messages congest the system and worms get blocked. They also help in avoiding deadlocks in the network [8]. <p> The caching technique is implemented at the send module of the network interface where messages are prepared to be sent over the network. Recall that our network is wormhole-routed. Thus messages are made up of flow control digits or flits. Each flit is 8 bytes as in Spider <ref> [10] </ref> and Cavallino [6]. The header flit of the message is the only one that contains the routing information, while tail flits follow the path created by the header. The header of the message follows the format shown in Figure 9. <p> Crossbar switches mainly differ in two design issues: switching technique and buffer management. As mentioned earlier, we use wormhole routing as the switching technique and input buffering with virtual channels [8] as the buffer management scheme since these are prevalent in current commercial crossbar switches <ref> [6, 10] </ref>. 3.3.1 Base Crossbar Switch Our base bi-directional crossbar switch has four inputs and four outputs as shown in Figure 10. Each input link in the crossbar switch has two virtual channels thus providing 8 possible input candidates for arbitration. <p> Each input link in the crossbar switch has two virtual channels thus providing 8 possible input candidates for arbitration. The arbitration process in the crossbar switch is the age technique, similar to the one employed in the SGI Spider Switch <ref> [10] </ref>. At each arbitration cycle, a maximum of 4 highest age flits are selected from 8 possible arbitration candidates. The internal switch core and transmission over the link operate at 200MHz (like Cavallino [6]). The wire width at the link is w = 16 bits. <p> The IN employs 4 fi 4 switches organized in 4 stages as shown earlier in Figure 7. Virtual channels were added to the switching elements of the IN to simulate the behavior of commercial switches like Cavallino [6] and Spider <ref> [10] </ref>. The crossbar switch operation is similar to the description in Section 3.3.1. Each input link to the switch is provided with 2 virtual channel buffers capable of storing a maximum of 4 flits from a single message. A detailed list of simulation parameters is also shown in Table 2.
Reference: [11] <author> M. Horowitz, </author> <title> "High Frequency Clock Distribution," </title> <booktitle> 1996 Symposium on VLSI Circuits, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: Such an organization changes cache changes the aspect ratio of the cache [27] and may affect the cycle time of the cache. Wilson et al.[26] showed that the increase in cycle time measured using the fan-out-of-four (FO4) <ref> [11] </ref> for banked or interleaved caches over single ported caches was minimal. Moreover, current processors such as Pentium Pro [21] and MIPS R10000 [28] use interleaved caches for improved performance. In a 2-way interleaved implementation, two regular requests directed to different banks can be simultaneously processed.
Reference: [12] <author> J. Laudon and D. Lenoski, </author> <title> "The SGI Origin: A ccNUMA Highly Scalable Server," </title> <booktitle> Proceedings of 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 241-251, </pages> <year> 1997. </year>
Reference-contexts: However the contention on the bus and system scalability issues heavily constrain the number of processors that can be connected to the bus. To build high performance systems that are highly scalable, several current systems <ref> [2, 12, 14, 15] </ref> employ the CC-NUMA architecture. The CC-NUMA architecture is based on connecting small bus-based processor-memory clusters using scalable interconnects. In such a system, the shared memory is distributed among all the nodes in the system to provide a closer local memory and several remote memories. <p> In general, an N -node system using a BMIN comprises of N=k switching elements (a 2k fi 2k crossbar) in each of the log k N stages connected by bidirectional links. We chose wormhole routing as the switching technique because it is prevalent in current systems <ref> [12, 23] </ref>. Furthermore, several studies have shown that it outperforms packet switching to a great extent. Similar to Spider [10], virtual channels are 9 also employed to improve the performance of the system when bulky arrival of messages congest the system and worms get blocked.
Reference: [13] <author> C. E. Leiserson etal, </author> <title> "The Network Architecture of the Connection Machine CM-5," </title> <booktitle> Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 272-285, </pages> <year> 1992. </year> <month> 31 </month>
Reference-contexts: Tree-based networks like the fat tree <ref> [13] </ref> , the hierarchical bus network [25, 4] and the multistage interconnection network (MIN) [17] provide hierarchical topologies suitable for global caching. While offering an inherent tree-structure, the MIN is also highly scalable and provides a bisection bandwidth that scales linearly with the number of nodes in the system. <p> These features of the MIN make it very attractive as high performance networks for commercial systems. Existing systems such as Butterfly [3], CM-5 <ref> [13] </ref> and IBM SP2 [23] employ a bidirectional MIN. The Illinois Cedar multiprocessor [24] employs two separate uni-directional MINs (one for requests and one for replies). In this paper, the switch cache interconnect is a bidirectional MIN to take advantage of the inherent tree structure.
Reference: [14] <author> D. Lenoski et al., </author> <title> "The Stanford DASH Multiprocessor," </title> <journal> IEEE Computer, </journal> <volume> 25(3), </volume> <pages> pages 63-79, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: However the contention on the bus and system scalability issues heavily constrain the number of processors that can be connected to the bus. To build high performance systems that are highly scalable, several current systems <ref> [2, 12, 14, 15] </ref> employ the CC-NUMA architecture. The CC-NUMA architecture is based on connecting small bus-based processor-memory clusters using scalable interconnects. In such a system, the shared memory is distributed among all the nodes in the system to provide a closer local memory and several remote memories. <p> Current systems have implemented the network cache in different ways. The HP Exemplar [2] implements the network cache as a configurable partition of the local memory. Sequent's NUMA-Q [15] dedicates a 32MB DRAM memory for the network cache. The DASH multiprocessor <ref> [14] </ref> has provision for a network cache called the remote access cache. A recent proposal by Moga et al.[16] explores the use of small SRAM (instead of DRAM) network caches integrated with a page cache employed in S-COMA multiprocessors [20].
Reference: [15] <author> T. Lovett and R. Clapp., "STiNG: </author> <title> A CC-NUMA Computer System for the Commercial Marketplace.," </title> <booktitle> Proceedings of the 23rd Annual International Symposium on Computer Architecture., </booktitle> <pages> pages 308-317. </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: However the contention on the bus and system scalability issues heavily constrain the number of processors that can be connected to the bus. To build high performance systems that are highly scalable, several current systems <ref> [2, 12, 14, 15] </ref> employ the CC-NUMA architecture. The CC-NUMA architecture is based on connecting small bus-based processor-memory clusters using scalable interconnects. In such a system, the shared memory is distributed among all the nodes in the system to provide a closer local memory and several remote memories. <p> Current systems have implemented the network cache in different ways. The HP Exemplar [2] implements the network cache as a configurable partition of the local memory. Sequent's NUMA-Q <ref> [15] </ref> dedicates a 32MB DRAM memory for the network cache. The DASH multiprocessor [14] has provision for a network cache called the remote access cache.
Reference: [16] <author> A. Moga and M. Dubois., </author> <title> "The Effectiveness of SRAM Network Caches on Clustered DSMs," </title> <booktitle> Proceesings of the 4th International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 103-112, </pages> <month> Feb. </month> <year> 1998. </year>
Reference-contexts: To reduce the impact of remote memory access latencies, researchers have proposed improved caching strategies <ref> [16, 18, 20, 29] </ref> within each cluster of the multiprocessor. These caching techniques are primarily based on data sharing among multiple processors within the same cluster. Nayfeh et al. [18] explore the use of shared L2 caches instead of individual ones to reduce access latency. <p> The shared L2 cache benefits from shared working set effects and a reduction in communication misses between processors within the cluster. Another alternative to provide an additional shared cache within each cluster is the use of network caches or remote data caches <ref> [16, 29] </ref>. Network caches reduce the remote access penalty by serving the capacity misses of L2 caches and providing an additional shared cache layer to processors within the cluster. Current systems have implemented the network cache in different ways.
Reference: [17] <author> A. Nanda and L. Bhuyan, </author> <title> "Design and Analysis of Cache Coherent Multistage Interconnection Networks," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 42, no. 4, </volume> <month> April </month> <year> 1993. </year>
Reference-contexts: Tree-based networks like the fat tree [13] , the hierarchical bus network [25, 4] and the multistage interconnection network (MIN) <ref> [17] </ref> provide hierarchical topologies suitable for global caching. While offering an inherent tree-structure, the MIN is also highly scalable and provides a bisection bandwidth that scales linearly with the number of nodes in the system.
Reference: [18] <author> B. Nayfeh, et al., </author> <title> "The Impact of Shared-Cache Clustering in Small-Scale Shared-Memory Multiprocessors," </title> <booktitle> Proceesings of the 2nd International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 74-84, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: To reduce the impact of remote memory access latencies, researchers have proposed improved caching strategies <ref> [16, 18, 20, 29] </ref> within each cluster of the multiprocessor. These caching techniques are primarily based on data sharing among multiple processors within the same cluster. Nayfeh et al. [18] explore the use of shared L2 caches instead of individual ones to reduce access latency. <p> To reduce the impact of remote memory access latencies, researchers have proposed improved caching strategies [16, 18, 20, 29] within each cluster of the multiprocessor. These caching techniques are primarily based on data sharing among multiple processors within the same cluster. Nayfeh et al. <ref> [18] </ref> explore the use of shared L2 caches instead of individual ones to reduce access latency. The shared L2 cache benefits from shared working set effects and a reduction in communication misses between processors within the cluster.
Reference: [19] <author> V. Pai et al., </author> <note> "RSIM Reference Manual. Version 1.0," </note> <institution> Department of Electrical and Computer Engineering, Rice University. </institution> <type> Technical Report 9705. </type> <month> July </month> <year> 1997. </year>
Reference-contexts: We chose six applications (FWA, GS, GE, MM, SOR, FFT) to study the remote access performance for a wide variety of access patterns. The applications were run on our base system with 16 processors simulated using a modified version of RSIM <ref> [19] </ref>. The base system employs 16 200MHz superscalar processors with a 16KB L1 cache and a 128KB L2 cache. Coherence is maintained using an invalidation-based three-state cache protocol and a full-map directory protocol [7]. The system enforces release consistency employing a write buffer for each processor. <p> the benefits of employing switch caches in the interconnect when compared to a base system and one with network caches. 5.1 Simulation Methodology To evaluate the performance impact of switch caches on the application performance of CC-NUMA multiprocessors, we use a modified version of RSIM (Rice Simulator for ILP Multiprocessors) <ref> [19] </ref>. RSIM is an execution driven simulator for shared memory multiprocessors with accurate models of current processors that exploit instruction-level parallelism. In this section, we describe the various system configurations (and corresponding modifications to RSIM) used for conducting simulation runs. The base system configuration consists of 16 nodes.
Reference: [20] <author> A. Saulsbury, et al., </author> <title> "An Argument for Simple COMA," </title> <booktitle> Proceedings of the 1st International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 276-285, </pages> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: To reduce the impact of remote memory access latencies, researchers have proposed improved caching strategies <ref> [16, 18, 20, 29] </ref> within each cluster of the multiprocessor. These caching techniques are primarily based on data sharing among multiple processors within the same cluster. Nayfeh et al. [18] explore the use of shared L2 caches instead of individual ones to reduce access latency. <p> The DASH multiprocessor [14] has provision for a network cache called the remote access cache. A recent proposal by Moga et al.[16] explores the use of small SRAM (instead of DRAM) network caches integrated with a page cache employed in S-COMA multiprocessors <ref> [20] </ref>. The use of 32KB SRAM chips reduces the access latency of network caches tremendously. Our goal is to reduce remote memory access latencies by implementing a global shared cache abstraction central to all processors in the CC-NUMA system.
Reference: [21] <author> T. Shanley, </author> <title> "Pentium Pro Processor System Architecture," </title> <publisher> MindShare Inc., Addison-Wesley Publishing Company, </publisher> <month> April </month> <year> 1997. </year>
Reference-contexts: For example, a cache with 32-byte blocks and a width of 64 bits will provide 64 of 256 bits in each cache cycle decreasing the cache throughput to one read in four cycles. Current processors such as the Pentium Pro <ref> [21] </ref> employs a L1 data cache with a 64-bit output width. Besides increasing the width of the data array, employing multiple ports provide a significant performance advantage since multiple requests can be satisfied in parallel. To implement multiple ports, two methods are currently used. <p> To implement multiple ports, two methods are currently used. One method splits the cache into different interleaved banks, each mapping a different set of addresses. The R10000 processor [28] and the Pentium Pro <ref> [21] </ref> employ a 2-way banked L1 cache, where odd address blocks are stored in one bank and even address blocks in the other. By employing such an interleaving, the cache can potentially serve two requests at a time. <p> Wilson et al.[26] showed that the increase in cycle time measured using the fan-out-of-four (FO4) [11] for banked or interleaved caches over single ported caches was minimal. Moreover, current processors such as Pentium Pro <ref> [21] </ref> and MIPS R10000 [28] use interleaved caches for improved performance. In a 2-way interleaved implementation, two regular requests directed to different banks can be simultaneously processed. This doubles the cache throughput when the cache access pattern is ideal.
Reference: [22] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> "SPLASH: Stanford Parallel Applications for Shared-Memory," </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: These applications are Floyd-Warshall's all-pair-shortest-path algorithm, Gaussian elimination (GE), QR factorization using the Gram-Schmidt Algorithm (GS) and the multiplication of 2D matrices (MM), successive over-relaxation of a grid (SOR) and the six-step 1D fast fourier transform (FFT) from SPLASH <ref> [22] </ref>. The input data sizes for each of these applications are shown in Table 2.
Reference: [23] <author> C. B. Stunkel et al., </author> <title> "The SP2 High Performance Switch," </title> <journal> IBM Systems Journal, </journal> <volume> vol. 34, no. 2, </volume> <pages> pp. 185-204, </pages> <year> 1995 </year>
Reference-contexts: These features of the MIN make it very attractive as high performance networks for commercial systems. Existing systems such as Butterfly [3], CM-5 [13] and IBM SP2 <ref> [23] </ref> employ a bidirectional MIN. The Illinois Cedar multiprocessor [24] employs two separate uni-directional MINs (one for requests and one for replies). In this paper, the switch cache interconnect is a bidirectional MIN to take advantage of the inherent tree structure. <p> In general, an N -node system using a BMIN comprises of N=k switching elements (a 2k fi 2k crossbar) in each of the log k N stages connected by bidirectional links. We chose wormhole routing as the switching technique because it is prevalent in current systems <ref> [12, 23] </ref>. Furthermore, several studies have shown that it outperforms packet switching to a great extent. Similar to Spider [10], virtual channels are 9 also employed to improve the performance of the system when bulky arrival of messages congest the system and worms get blocked.
Reference: [24] <author> J. Torrellas and Z. Zheng, </author> <title> "The Performance of the Cedar Multistage Switching Network," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 8, no. 4, </volume> <pages> pp. 321-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These features of the MIN make it very attractive as high performance networks for commercial systems. Existing systems such as Butterfly [3], CM-5 [13] and IBM SP2 [23] employ a bidirectional MIN. The Illinois Cedar multiprocessor <ref> [24] </ref> employs two separate uni-directional MINs (one for requests and one for replies). In this paper, the switch cache interconnect is a bidirectional MIN to take advantage of the inherent tree structure. Note, however, that logical trees can be embedded on most other direct networks.
Reference: [25] <author> A.W. Wilson, </author> <title> "Hierarchical cache/bus architecture for shared memory multiprocessors," </title> <booktitle> Proc.14th Ann. Int'l. Symp. on Comp. Arch., </booktitle> <pages> pp. 244-252, </pages> <year> 1987. </year>
Reference-contexts: Tree-based networks like the fat tree [13] , the hierarchical bus network <ref> [25, 4] </ref> and the multistage interconnection network (MIN) [17] provide hierarchical topologies suitable for global caching. While offering an inherent tree-structure, the MIN is also highly scalable and provides a bisection bandwidth that scales linearly with the number of nodes in the system.
Reference: [26] <author> K. Wilson and K. Olukotun, </author> <title> "Designing High Bandwidth On-Chip Caches," </title> <booktitle> Proceedings of the 23th International Symposium on Computer Achitecture, </booktitle> <pages> pp. 121-132, </pages> <year> 1997. </year>
Reference-contexts: The cycle time and access time of an SRAM cache depends heavily on several factors such as associativity, cache output width, number of wordlines, number of bitlines and cache size <ref> [26, 27] </ref>. Associativity affects cache performance in two different ways. Direct mapped caches (an associativity of 1) have low cycle times since a direct indexing method is used to locate the line in the tag and data arrays, but have poor hit ratios due to mapping conflicts in the cache.
Reference: [27] <author> S. Wilton and N. Jouppi, </author> <title> "An Enhanced Access and Cycle Time Model for On-Chip Caches," </title> <type> Technical Report, </type> <institution> DEC-WRL, </institution> <year> 1994. </year>
Reference-contexts: The cycle time and access time of an SRAM cache depends heavily on several factors such as associativity, cache output width, number of wordlines, number of bitlines and cache size <ref> [26, 27] </ref>. Associativity affects cache performance in two different ways. Direct mapped caches (an associativity of 1) have low cycle times since a direct indexing method is used to locate the line in the tag and data arrays, but have poor hit ratios due to mapping conflicts in the cache. <p> Currently, most processors employ multi-ported two-way set associative L1 caches operating within a single processor cycle. Cache output width is also an important issue that primarily affects the cache access time. As studied by Wilson et al. <ref> [27] </ref> , the increase in data array width increases the number of sense amplifiers required. Note also that the organization of the cache can also make a significant difference in terms of chip area. <p> Increasing the data array width, however, does not 21 come cheap. The increase in width requires a higher number of sense amplifiers and chip area is also affected <ref> [27] </ref>. While increasing the cache output width is one alternative, providing independent banks or interleaved caches (CAESAR + ) is another solution. block address determines which bank the request is directed to. Such an organization changes cache changes the aspect ratio of the cache [27] and may affect the cycle time <p> and chip area is also affected <ref> [27] </ref>. While increasing the cache output width is one alternative, providing independent banks or interleaved caches (CAESAR + ) is another solution. block address determines which bank the request is directed to. Such an organization changes cache changes the aspect ratio of the cache [27] and may affect the cycle time of the cache. Wilson et al.[26] showed that the increase in cycle time measured using the fan-out-of-four (FO4) [11] for banked or interleaved caches over single ported caches was minimal.
Reference: [28] <author> K. Yeager. </author> <title> "The MIPS R10000 Superscalar Microprocessor," </title> <journal> IEEE Micro, </journal> <volume> 16(2) </volume> <pages> 28-40, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: To implement multiple ports, two methods are currently used. One method splits the cache into different interleaved banks, each mapping a different set of addresses. The R10000 processor <ref> [28] </ref> and the Pentium Pro [21] employ a 2-way banked L1 cache, where odd address blocks are stored in one bank and even address blocks in the other. By employing such an interleaving, the cache can potentially serve two requests at a time. <p> Wilson et al.[26] showed that the increase in cycle time measured using the fan-out-of-four (FO4) [11] for banked or interleaved caches over single ported caches was minimal. Moreover, current processors such as Pentium Pro [21] and MIPS R10000 <ref> [28] </ref> use interleaved caches for improved performance. In a 2-way interleaved implementation, two regular requests directed to different banks can be simultaneously processed. This doubles the cache throughput when the cache access pattern is ideal.
Reference: [29] <author> Z. Zhang and J. Torellas., </author> <title> "Reducing Remote Conflict Misses: NUMA with Remote Cache versus COMA," </title> <booktitle> Proceedings of the 3rd International Symposium on High Performance Computer Architecture, </booktitle> <month> Jan. </month> <year> 1997. </year> <month> 32 </month>
Reference-contexts: To reduce the impact of remote memory access latencies, researchers have proposed improved caching strategies <ref> [16, 18, 20, 29] </ref> within each cluster of the multiprocessor. These caching techniques are primarily based on data sharing among multiple processors within the same cluster. Nayfeh et al. [18] explore the use of shared L2 caches instead of individual ones to reduce access latency. <p> The shared L2 cache benefits from shared working set effects and a reduction in communication misses between processors within the cluster. Another alternative to provide an additional shared cache within each cluster is the use of network caches or remote data caches <ref> [16, 29] </ref>. Network caches reduce the remote access penalty by serving the capacity misses of L2 caches and providing an additional shared cache layer to processors within the cluster. Current systems have implemented the network cache in different ways.
References-found: 29

