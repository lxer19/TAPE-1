URL: http://www.cs.columbia.edu/dcc/classes/E6998-025/References/cmpjrc.ps
Refering-URL: http://www.cs.columbia.edu/dcc/classes/E6998-025/projects.html
Root-URL: http://www.cs.columbia.edu
Title: Comparison of Techniques for Diagnosing Performance Problems in Information Systems: Case Study and Analytic Models  
Author: Joseph L. Hellerstein 
Address: Yorktown Heights, NY 10598  Yorktown Heights, New York San Jose, California Zurich, Switzerland  
Affiliation: IBM Research Division T.J. Watson Research Center  IBM Research Division  
Note: A  
Abstract: Research Report LIMITED DISTRIBUTION NOTICE This report has been submitted for publication outside of IBM and will probably be copyrighted if accepted for publication. It has been issued as a Research Report for early dissemination of its contents. In view of the transfer of copyright to the outside publisher, its distribution outside of IBM prior to publication should be limited to peer communications and specific requests. After outside publication, requests should be filled only by reprints or legally obtained copies of the article (e.g., payment of royalties). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Adelman: </author> <title> "Experiments, Quasi-Experiments, and Case Studies: A Review of Empirical Methods for Evaluating Decision Support Systems," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 21, </journal> <pages> 293-301, </pages> <year> 1991. </year>
Reference-contexts: A second approach to assessment is having human experts review the results produced by a system incorporating the algorithm under study (e.g., <ref> [1] </ref>, [8], and [24]). Unfortunately, the cost of such experiments necessitates that their scope be limited. Also, this approach provides little insight into the behavior of the algorithms since the entire system is being assessed. Still another deficiency in the current literature is that algorithms are assessed in isolation. <p> Can this difficulty be eliminated? One approach is to employ a finer quantification of weights by using distribution quantiles obtained from the reference data instead of binary values that indicate whether or not a particular quantile is exceeded (e.g., [8]). Thus, the weights would be real values in <ref> [0; 1] </ref>. However, there is still a chance of encountering a y j that exceeds the largest value in the reference data. For such measurement sample paths, threshold analysis assigns a 1 to w j (T ), and so once again this technique is not first moment consistent.
Reference: [2] <author> B. Arinze, M. Igbaria, </author> <title> and L.F. Young: "A Knowledge Based Decision Support System for Computer Performance Management," </title> <booktitle> Decision Support Systems 8, </booktitle> <pages> 501-515, </pages> <year> 1992. </year>
Reference-contexts: Effectiveness is the ratio of the causal variable's weight to the largest weight of the non-causal variables. Numerous applications have been constructed for diagnosing performance problems in information systems (e.g., <ref> [2] </ref>, [6], [5], [9], [17]). More broadly, applications have been developed to diagnose performance problems in domains such as: quality of service problems in commercial T1-lines [16], asset management problems in power plants [22], corporate financial performance [18], and resource utilizations in hospitals [8]. <p> Threshold Analysis Threshold analysis is probably the most widely used technique for computing arc weights. It has been employed in numerous expert systems for performance tuning (e.g., <ref> [2] </ref>, [9], and [17]) as well as applications for assessing quality of service in commercial T1-lines [16] and diagnosing problems in power plants [10]. Threshold analysis requires specifying an acceptable range for values of each measurement variable; variables that lie outside their range are considered causal.
Reference: [3] <author> R. A. Backman: </author> <title> "Easy to Use Performance Tools With a Consistent User Interface Across HP Operating Systems," </title> <journal> Hewlett Packard Journal 42, </journal> <pages> 65-70, </pages> <year> 1991. </year>
Reference: [4] <author> Robert F. Berry and Joseph L. Hellerstein: </author> <title> "A Unified Approach to Interpreting Measurement Data in Performance Management Applications," </title> <booktitle> First IEEE Conference on Systems Management, </booktitle> <institution> University of California, Los Ange-les, </institution> <month> May, </month> <year> 1993. </year>
Reference-contexts: For this reason, a hierarchical approach is usually employed that exploits part-of relationships between measurement variables (e.g., [5], [6], and [9]). A convenient way to express such relationships is with a measurement navigation graph (MNG) <ref> [4] </ref> in which measurement variables are represented as nodes and the relationships between variables are indicated by directed arcs. Fig. 1 displays a portion of a MNG for solving response time problems in a mainframe computer system.
Reference: [5] <author> Bernard Domanski: </author> <title> "A PROLOG-based Expert System for Tuning MVS/XA," </title> <booktitle> Proceedings of the Computer Measurement Group, </booktitle> <pages> 160-166, </pages> <year> 1987. </year>
Reference-contexts: Unfortunately, such an unstructured approach can result in a very large search space since complex systems have thousands (if not hundreds of thousands) of measurement variables. For this reason, a hierarchical approach is usually employed that exploits part-of relationships between measurement variables (e.g., <ref> [5] </ref>, [6], and [9]). A convenient way to express such relationships is with a measurement navigation graph (MNG) [4] in which measurement variables are represented as nodes and the relationships between variables are indicated by directed arcs. <p> Effectiveness is the ratio of the causal variable's weight to the largest weight of the non-causal variables. Numerous applications have been constructed for diagnosing performance problems in information systems (e.g., [2], [6], <ref> [5] </ref>, [9], [17]). More broadly, applications have been developed to diagnose performance problems in domains such as: quality of service problems in commercial T1-lines [16], asset management problems in power plants [22], corporate financial performance [18], and resource utilizations in hospitals [8].
Reference: [6] <author> Boole & Babbage: </author> <type> DASD ADVISOR Technical Backgrounder, </type> <institution> Boole & Bab-bage, Inc., 10BAB34.BGR, </institution> <month> December, </month> <year> 1987. </year> <month> 27 </month>
Reference-contexts: Unfortunately, such an unstructured approach can result in a very large search space since complex systems have thousands (if not hundreds of thousands) of measurement variables. For this reason, a hierarchical approach is usually employed that exploits part-of relationships between measurement variables (e.g., [5], <ref> [6] </ref>, and [9]). A convenient way to express such relationships is with a measurement navigation graph (MNG) [4] in which measurement variables are represented as nodes and the relationships between variables are indicated by directed arcs. <p> Effectiveness is the ratio of the causal variable's weight to the largest weight of the non-causal variables. Numerous applications have been constructed for diagnosing performance problems in information systems (e.g., [2], <ref> [6] </ref>, [5], [9], [17]). More broadly, applications have been developed to diagnose performance problems in domains such as: quality of service problems in commercial T1-lines [16], asset management problems in power plants [22], corporate financial performance [18], and resource utilizations in hospitals [8]. <p> Bottleneck Analysis This technique has been used in many contexts, such as expert systems for performance tuning (e.g., <ref> [6] </ref> and [11]), general methodologies for performance analysis (e.g., [23]), and automating assistance with linear programming models [12].
Reference: [7] <author> A. Bouloutas, S. Calo, and A. </author> <title> Finkel "Alarm Correlation and Fault Iden--tification in Communication Networks," </title> <type> Research Report, RC 17967, </type> <institution> IBM Corporation, </institution> <year> 1992. </year> <note> (To appear in the IEEE Transactions on Communications.) </note>
Reference-contexts: Indeed, to the best of our knowledge, only two kinds of assessments have been reported. The first is to prove a sufficient condition under which an algorithm is assured of identifying the causal variables (e.g., <ref> [7] </ref>). While this provides formal insights, the sufficient conditions are often difficult to satisfy; so designers of diagnostic applications are left without insights as to how algorithms behave in practice.
Reference: [8] <author> Tom Bowen and Liz Payling: </author> <title> "Expert Systems for Performance Review," </title> <journal> Journal of Operations Research Society, </journal> <volume> 38 929-934, </volume> <year> 1987. </year>
Reference-contexts: More broadly, applications have been developed to diagnose performance problems in domains such as: quality of service problems in commercial T1-lines [16], asset management problems in power plants [22], corporate financial performance [18], and resource utilizations in hospitals <ref> [8] </ref>. Even though many applications have been developed, little has been done to assess the effectiveness of the algorithms employed. Indeed, to the best of our knowledge, only two kinds of assessments have been reported. <p> A second approach to assessment is having human experts review the results produced by a system incorporating the algorithm under study (e.g., [1], <ref> [8] </ref>, and [24]). Unfortunately, the cost of such experiments necessitates that their scope be limited. Also, this approach provides little insight into the behavior of the algorithms since the entire system is being assessed. Still another deficiency in the current literature is that algorithms are assessed in isolation. <p> In some diagnostic applications (e.g., the analysis of power plants in [22]), the t k are derived from physical tolerances. However, such tolerances are rare in information systems. Instead, a common approach is to use quantiles obtained from separate reference data (as in <ref> [8] </ref>). Ideally, these should be quantiles of mean values averaged over M samples. <p> Hence, threshold analysis is not first moment consistent. Can this difficulty be eliminated? One approach is to employ a finer quantification of weights by using distribution quantiles obtained from the reference data instead of binary values that indicate whether or not a particular quantile is exceeded (e.g., <ref> [8] </ref>). Thus, the weights would be real values in [0; 1]. However, there is still a chance of encountering a y j that exceeds the largest value in the reference data.
Reference: [9] <institution> Computer Associates: "CA-MINDOVER," Computer Associates, </institution> <address> 711 Stew-art Avenue, Garden City, New York, </address> <year> 1993. </year>
Reference-contexts: Unfortunately, such an unstructured approach can result in a very large search space since complex systems have thousands (if not hundreds of thousands) of measurement variables. For this reason, a hierarchical approach is usually employed that exploits part-of relationships between measurement variables (e.g., [5], [6], and <ref> [9] </ref>). A convenient way to express such relationships is with a measurement navigation graph (MNG) [4] in which measurement variables are represented as nodes and the relationships between variables are indicated by directed arcs. <p> Effectiveness is the ratio of the causal variable's weight to the largest weight of the non-causal variables. Numerous applications have been constructed for diagnosing performance problems in information systems (e.g., [2], [6], [5], <ref> [9] </ref>, [17]). More broadly, applications have been developed to diagnose performance problems in domains such as: quality of service problems in commercial T1-lines [16], asset management problems in power plants [22], corporate financial performance [18], and resource utilizations in hospitals [8]. <p> Threshold Analysis Threshold analysis is probably the most widely used technique for computing arc weights. It has been employed in numerous expert systems for performance tuning (e.g., [2], <ref> [9] </ref>, and [17]) as well as applications for assessing quality of service in commercial T1-lines [16] and diagnosing problems in power plants [10]. Threshold analysis requires specifying an acceptable range for values of each measurement variable; variables that lie outside their range are considered causal.
Reference: [10] <author> A. D'Ambrosio, M. Oriati, and A. Serventi: </author> <title> "AI in the Power Plants: PERF-EXS, a PERFormance diagnostics EXpert System," </title> <booktitle> IEEE Conference on AI Applications, </booktitle> <pages> 11-17, </pages> <year> 1988. </year>
Reference-contexts: It has been employed in numerous expert systems for performance tuning (e.g., [2], [9], and [17]) as well as applications for assessing quality of service in commercial T1-lines [16] and diagnosing problems in power plants <ref> [10] </ref>. Threshold analysis requires specifying an acceptable range for values of each measurement variable; variables that lie outside their range are considered causal. Typically, threshold analysis is applied to mean values; that is, x 1 ; ; x N .
Reference: [11] <institution> Siegmund Gralla "An Expert System for VM Performance Analysis," Research Report, RC 12784, IBM Corporation, </institution> <year> 1987. </year>
Reference-contexts: Bottleneck Analysis This technique has been used in many contexts, such as expert systems for performance tuning (e.g., [6] and <ref> [11] </ref>), general methodologies for performance analysis (e.g., [23]), and automating assistance with linear programming models [12].
Reference: [12] <author> Harvey J. Greenberg: </author> <title> "Rule-based Intelligence to Support Linear Program Analysis," </title> <booktitle> Decision Support Systems, </booktitle> <volume> 9, </volume> <pages> 425-447, </pages> <year> 1993. </year>
Reference-contexts: Bottleneck Analysis This technique has been used in many contexts, such as expert systems for performance tuning (e.g., [6] and [11]), general methodologies for performance analysis (e.g., [23]), and automating assistance with linear programming models <ref> [12] </ref>. <p> Change is quantified by using mean values from a separate set of reference data. Applications incorporating this approach include expert systems for performance tuning [13], tools that assist with linear programming models <ref> [12] </ref>, and knowledge-based systems that analyze corporate financial statements [18]. What's-different analysis can be applied to functions whose algebraic form is unknown if the signs of the first partial derivatives are known.
Reference: [13] <author> J.L. Hellerstein, David A. Klein, and Keith R. Milliken: </author> <booktitle> Expert Systems in Data Processing Addison-Wesley, </booktitle> <year> 1989. </year>
Reference-contexts: What's-Different Analysis What's-different analysis approaches arc weight computations by determining how changes in each child affect the parent. Change is quantified by using mean values from a separate set of reference data. Applications incorporating this approach include expert systems for performance tuning <ref> [13] </ref>, tools that assist with linear programming models [12], and knowledge-based systems that analyze corporate financial statements [18]. What's-different analysis can be applied to functions whose algebraic form is unknown if the signs of the first partial derivatives are known.
Reference: [14] <author> IBM: </author> <title> VMPAF User's Guide and Reference Manual, </title> <institution> (SC23-0564-00) IBM Corporation, </institution> <year> 1993. </year>
Reference-contexts: Instead, this technique looks at changes in the level of the performance problem within the target data. Correlation analysis has been incorporated into several tools for performance analysis, including NaviGraph [19] and the Performance Analysis Facility/VM <ref> [14] </ref>. Correlation analysis requires that fine-grain measurements be available from which the (lag 0) sample cross correlation is computed.
Reference: [15] <author> IBM: </author> <title> Analyzing Resource Measurement Facility Version 4 Monitor III Reports, </title> <institution> (LY28-1008-2) IBM Corporation, </institution> <year> 1990. </year>
Reference: [16] <author> D. R. Irwin: </author> <title> "Monitoring the Performance of Commercial T1-rate Transmission Service," </title> <journal> IBM Journal of Research and Development, </journal> <pages> 805-814, </pages> <year> 1991. </year>
Reference-contexts: Numerous applications have been constructed for diagnosing performance problems in information systems (e.g., [2], [6], [5], [9], [17]). More broadly, applications have been developed to diagnose performance problems in domains such as: quality of service problems in commercial T1-lines <ref> [16] </ref>, asset management problems in power plants [22], corporate financial performance [18], and resource utilizations in hospitals [8]. Even though many applications have been developed, little has been done to assess the effectiveness of the algorithms employed. <p> Threshold Analysis Threshold analysis is probably the most widely used technique for computing arc weights. It has been employed in numerous expert systems for performance tuning (e.g., [2], [9], and [17]) as well as applications for assessing quality of service in commercial T1-lines <ref> [16] </ref> and diagnosing problems in power plants [10]. Threshold analysis requires specifying an acceptable range for values of each measurement variable; variables that lie outside their range are considered causal. Typically, threshold analysis is applied to mean values; that is, x 1 ; ; x N .
Reference: [17] <author> Kiyoshi Itoh, and Takaaki Konno: </author> <title> "An Integrated Method for Parameter Tuning on Synchronized Queueing Network Bottlenecks by Qualitative and Quantitative Reasoning," </title> <journal> IEICE Transactions on Information and Systems 5, </journal> <pages> 635-647, </pages> <year> 1992. </year>
Reference-contexts: Effectiveness is the ratio of the causal variable's weight to the largest weight of the non-causal variables. Numerous applications have been constructed for diagnosing performance problems in information systems (e.g., [2], [6], [5], [9], <ref> [17] </ref>). More broadly, applications have been developed to diagnose performance problems in domains such as: quality of service problems in commercial T1-lines [16], asset management problems in power plants [22], corporate financial performance [18], and resource utilizations in hospitals [8]. <p> Threshold Analysis Threshold analysis is probably the most widely used technique for computing arc weights. It has been employed in numerous expert systems for performance tuning (e.g., [2], [9], and <ref> [17] </ref>) as well as applications for assessing quality of service in commercial T1-lines [16] and diagnosing problems in power plants [10]. Threshold analysis requires specifying an acceptable range for values of each measurement variable; variables that lie outside their range are considered causal.
Reference: [18] <author> Donald W. Kosy and Ben P. Wise: </author> <title> "Self-Explanatory Financial Planning Models," </title> <booktitle> Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 176-181, </pages> <year> 1984. </year>
Reference-contexts: More broadly, applications have been developed to diagnose performance problems in domains such as: quality of service problems in commercial T1-lines [16], asset management problems in power plants [22], corporate financial performance <ref> [18] </ref>, and resource utilizations in hospitals [8]. Even though many applications have been developed, little has been done to assess the effectiveness of the algorithms employed. Indeed, to the best of our knowledge, only two kinds of assessments have been reported. <p> Change is quantified by using mean values from a separate set of reference data. Applications incorporating this approach include expert systems for performance tuning [13], tools that assist with linear programming models [12], and knowledge-based systems that analyze corporate financial statements <ref> [18] </ref>. What's-different analysis can be applied to functions whose algebraic form is unknown if the signs of the first partial derivatives are known.
Reference: [19] <institution> Landmark Systems Corporation: "NaviGraph," Landmark Systems Cor--poration, </institution> <address> 8000 Towers Crescent Drive, Vienna, Virginia, </address> <year> 1993. </year>
Reference-contexts: However, unlike what's-different analysis, correlation analysis does not require a separate set of reference data. Instead, this technique looks at changes in the level of the performance problem within the target data. Correlation analysis has been incorporated into several tools for performance analysis, including NaviGraph <ref> [19] </ref> and the Performance Analysis Facility/VM [14]. Correlation analysis requires that fine-grain measurements be available from which the (lag 0) sample cross correlation is computed.
Reference: [20] <author> Rich Olcott: </author> <title> "Useful Computer System Metrics. Part 1. Towards a General Theory of Useful Metrics," </title> <booktitle> Computer Measurement Group Transactions, Summer, </booktitle> <pages> 45-55, </pages> <year> 1993. </year>
Reference: [21] <institution> Unix International Performance Management Working Group: "Performance Management Activities Within Unix International," Computer Measurement Group Transactions, </institution> <month> Summer, </month> <pages> 37-44, </pages> <year> 1993. </year>
Reference: [22] <author> Douglas Smith: </author> <title> "Monitoring/diagnostic Systems Enhance Plant Asset Management," </title> <booktitle> Power Engineering, </booktitle> <pages> 23-28, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Numerous applications have been constructed for diagnosing performance problems in information systems (e.g., [2], [6], [5], [9], [17]). More broadly, applications have been developed to diagnose performance problems in domains such as: quality of service problems in commercial T1-lines [16], asset management problems in power plants <ref> [22] </ref>, corporate financial performance [18], and resource utilizations in hospitals [8]. Even though many applications have been developed, little has been done to assess the effectiveness of the algorithms employed. Indeed, to the best of our knowledge, only two kinds of assessments have been reported. <p> One difficulty with applying threshold analysis in practice is selecting appropriate values for the t k . In some diagnostic applications (e.g., the analysis of power plants in <ref> [22] </ref>), the t k are derived from physical tolerances. However, such tolerances are rare in information systems. Instead, a common approach is to use quantiles obtained from separate reference data (as in [8]). Ideally, these should be quantiles of mean values averaged over M samples.
Reference: [23] <author> William Tetzlaff: </author> <title> "A New Approach to VM Performance Analysis," </title> <booktitle> Proceedings of the Computer Measurement Group, </booktitle> <pages> 339-350, </pages> <year> 1982. </year>
Reference-contexts: Bottleneck Analysis This technique has been used in many contexts, such as expert systems for performance tuning (e.g., [6] and [11]), general methodologies for performance analysis (e.g., <ref> [23] </ref>), and automating assistance with linear programming models [12].
Reference: [24] <author> Sholom M. Weiss, Casimir A. Kulikowski, Saul Amarel, and Aran Safir: </author> <title> "A Model-Based Method for Computer-Aided Medical Decision Making," </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 11 145-172, </pages> <year> 1978. </year> <month> 29 </month>
Reference-contexts: A second approach to assessment is having human experts review the results produced by a system incorporating the algorithm under study (e.g., [1], [8], and <ref> [24] </ref>). Unfortunately, the cost of such experiments necessitates that their scope be limited. Also, this approach provides little insight into the behavior of the algorithms since the entire system is being assessed. Still another deficiency in the current literature is that algorithms are assessed in isolation.
References-found: 24

