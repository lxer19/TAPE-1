URL: ftp://ftp.cs.brown.edu/pub/techreports/96/cs96-37.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-96-37.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., Lai, J. C. and Mercer, R. L. </author> <title> An estimate of an upper bound for the entropy of english. </title> <note> Computational Linguistics 181 (1991). </note>
Reference-contexts: In deleted interpolation one estimates a conditional probability by linearly interpolating several probabilities, each one deleting some of the conditioning events of the probability to be estimated. The canonical example of this occurs in trigram models of language (e.g., <ref> [1] </ref> ), where the basic probability is the probability of a word given the two previous words in the text, p (w i j w i2 ; w i1 ), where w i is the ith word in the text. <p> A good example of frequency interpolation can be found in <ref> [1] </ref>. An example of deleted interpolation used for tagging models can be found in [6], which also discusses the use of frequency interpolation.
Reference: 2. <author> Charniak, E. </author> <title> Statistical parsing with a context-free grammar and lexical information. </title> <publisher> (Forthcoming, </publisher> <year> 1996). </year>
Reference-contexts: Section three outlines our solution to this problem, while section four gives some empirical results. 2 Frequency Interpolation in Parsing In <ref> [2] </ref> we describe a program that parses using a language model. The program assigns every sentence a probability that is the sum of the probabilities of all of the sentence's possible parses (or at least all that the model constructs). <p> The second probability, p (s j t) also has a large component that is obtained from counting what happens in the training corpus, but includes as well a mechanism to assign probabilities to word-tag combinations that have not appeared in the training data, (Actually, the smoothing equation used in <ref> [2] </ref> is somewhat more complicated than Equation 4, but these complications are not relevant to this paper, and we ignore them.) Next, we need to discuss how values are assigned to 1 (h; t).
Reference: 3. <author> Collins, M. J. </author> <title> A new statistical parser based on bigram lexical dependencies. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL. </booktitle> <year> 1996. </year>
Reference-contexts: In the following table we give the results of our two runs, along with the best previous results on this exact test, those of Collins <ref> [3] </ref>. Labeled Labeled System Precision Recall Frequency Interpolation 86.7% 86.4% Expected-Frequency Interpolation 87.1% 86.8% Collins 86.3% 85.8% 3 Actually, to allow comparison to previous work, we use a minor variation of this described in [2,3].
Reference: 4. <author> Hindle, D. and Rooth, M. </author> <title> Structural ambiguity and lexical relations. </title> <booktitle> In Proceedings of the Association for Computational Linguistics. ACL, 1991, </booktitle> <volume> 229 - 236. </volume>
Reference-contexts: The reader might note that these probabilities are the same ones used by Hindle and Rooth <ref> [4] </ref> in their early work on pp attachment.
Reference: 5. <author> Marcus, M. P., Santorini, B. and Marcinkiewicz, M. A. </author> <title> Building a large annotated corpus of English: the Penn treebank. </title> <booktitle> Computational Linguistics 19 (1993), </booktitle> <pages> 313-330. </pages>
Reference-contexts: other hand, if it is headed by "%" then there are some integers, like "31," but there are also a lot of very low probability items like "31.65," things we typically do not find modifying "January." 10 slightly under one million words of the Penn tree-bank Wall Street Journal corpus <ref> [5] </ref>. It was tested on another 50,000 words of held out testing data. These are all of the sentences in one sub-file of the corpus restricting consideration to those of length less than or equal to 40 words and punctuation.
Reference: 6. <author> Merialdo, B. </author> <title> Tagging english text with a probabilistic model. </title> <booktitle> Computational Linguistics 202 (1994), </booktitle> <pages> 155-172. 13 </pages>
Reference-contexts: A good example of frequency interpolation can be found in [1]. An example of deleted interpolation used for tagging models can be found in <ref> [6] </ref>, which also discusses the use of frequency interpolation. While deleted interpolation, and more specifically, frequency interpolation works reasonably well for trigram language modeling, we suspect that this is due in part to the particularly simple structure of the problem.
References-found: 6

