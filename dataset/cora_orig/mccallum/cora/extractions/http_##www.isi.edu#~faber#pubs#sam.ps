URL: http://www.isi.edu/~faber/pubs/sam.ps
Refering-URL: http://www.isi.edu/~faber/
Root-URL: http://www.isi.edu
Title: Optimizing Throughput in a Workstation-based Network File System over a High Bandwidth Local Area Network  
Author: Theodore Faber 
Keyword: Distributed Systems, Network File Systems, High Speed LANs  
Address: 4676 Admiralty Way Marina del Rey, CA 90292  
Affiliation: University of Southern California/Information Sciences Institute  
Note: This appeared in SIGOPS Operating Systems Review, vol. 32, no. 1, pp.  
Email: faber@isi.edu  
Phone: Phone: 310-821-5080 x190 FAX: 310-823-6714  
Date: 29-40, January 1998  
Abstract: This paper describes methods of optimizing a client/server network file system to take advantage of high bandwidth local area networks in a conventional distributed computing environment. The environment contains hardware that removes network and disk bandwidth bottlenecks. The remaining bottlenecks at clients include excessive context switching, inefficient data translation, and cumbersome data encapsulation methods. When these are removed, the null-write performance of a current implementation of Sun's Network File System improves by 30%. A prototype system including a high speed RAM disk demonstrates an 18% improvement in overall write throughput. The prototype system fully utilizes the available peripheral bandwidth of the server. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Russel Sandberg, David Goldberg, Steve Kleiman, Dan Walsh, and Bob Lyon, </author> <title> Design and Implementation of the Sun Network File System, </title> <booktitle> Proceedings of the USENIX Conference, </booktitle> <pages> pp. 119-130, </pages> <month> USENIX (June </month> <year> 1985). </year>
Reference: 2. <author> Sun Microsystems, Inc., </author> <title> Network Filesystem Specification, </title> <address> RFC-1094 (March 1, </address> <year> 1989). </year>
Reference-contexts: Such systems are optimized for the demands of supercomputing applications that require access to large data files rather than the small file accesses of workstation users. The ATOMIC file server is based on an implementation of the NFS Version 2 specification <ref> [2] </ref>, which has been superseded by the NFS Version 3 specification [5]. The most important change with respect to file transfer bandwidth is that asynchronous writes are explicitly supported by Version 3. This is a prerequisite for any high-bandwidth system based on NFS.
Reference: 3. <author> Robert Felderman, Annette DeSchon, Danny Cohen, and Gregory Finn, </author> <title> Atomic: A High Speed Local Communication Architecture, </title> <journal> Journal of High Speed Networks, </journal> <volume> vol. 3, </volume> <pages> pp. </pages> <month> 1-29 </month> <year> (1994). </year>
Reference-contexts: It concentrates on improving the bulk data transfer capability of the file system, by removing unnecessary data translations and reducing context switches at the clients. Application of these techniques results in a 30% increase in Sun Network File System Version 2 (NFS)[1,2] write bandwidth using the ATOMIC LAN <ref> [3] </ref> when files are not written to disk. The cost of implementation of data encapsulation methods, specifically Sun XDR [4], is also assessed. <p> The ATOMIC-2 research group at the University of Southern California's Information Sciences Institute (USC/ISI) is exploring the issues associated with introducing the ATOMIC LAN, a 640 Mbps LAN invented at ISI and the California Institute of Technology <ref> [3] </ref>, into a distributed computing environment. 1 This work is supported by the Defense Advanced Research Projects Agency through Ft. Huachuca contract #DABT63-93-C-0062 entitled Netstation Architecture and Advanced Atomic Network. <p> Our testbed, which is used for all the experiments described in Section 3, uses Sun SPARCStation 20/71 workstations for both clients and servers. The hosts are connected by Myricom's implementation of the ATOMIC LAN <ref> [3] </ref>, Myrinet [8], which exhibits full duplex link rates of 640 Mb/sec, and a measured process-to-process TCP throughput in excess of 88 Mb/sec. NFS is not network-bound in this testbed. Null disk writes ensure that disk bottlenecks were avoided; however, without disk writes the system is impractical.
Reference: 4. <author> R. Srinivasan, XDR: </author> <title> External Data Representation Standard, </title> <month> RFC-1832 (August </month> <year> 1995). </year>
Reference-contexts: Application of these techniques results in a 30% increase in Sun Network File System Version 2 (NFS)[1,2] write bandwidth using the ATOMIC LAN [3] when files are not written to disk. The cost of implementation of data encapsulation methods, specifically Sun XDR <ref> [4] </ref>, is also assessed. A prototype system was built using a Texas Memory Systems SAM-300 RAM disk; the system exhibits an 18% improvement in overall NFS write throughput, and fully utilizes the server peripheral bandwidth. <p> Data Encapsulation and Portability Macklem has shown that XDR implementations in the kernel are often a significant source of overhead [6]. XDR is a standard data encapsulation method used to communicate between servers and clients running on different hardware <ref> [4] </ref>. He observed significant increases in throughput of an NFS system running under the 4.3BSD Reno system after replacing the subroutine calls that implemented XDR conversion with macros.
Reference: 5. <author> Sun Microsystems, Inc., NFS: </author> <title> Network File System Version 3 Protocol Specification, Sun Microsys-tems, </title> <publisher> Inc., </publisher> <address> Mountain View, CA (February 16, </address> <year> 1994). </year>
Reference-contexts: Each bar in Figure 1 corresponds to the removal of a system bottleneck: Synch Ethernet: NFS using synchronous disk writes which underuses both the network and disk bandwidth. NFS version 3 relaxes the synchronous write constraints <ref> [5] </ref>. The bottleneck in this configuration is the use of synchronous disk writes at the server. Asynch Ethernet: NFS using asynchronous disk writes at the server on a 10 Mb/sec Ethernet. The network bottlenecks this system. Asynch ATOMIC: NFS using asynchronous disk writes and the ATOMIC LAN. <p> The prototype system consists of the code described in Section 3 and a server equipped with a SAM-300. Note that in this section all writes at the server are asynchronous including those of the SunOS NFS. Unless asynchronous writes are implemented, as in NFS Version 3 <ref> [5] </ref>, protocol performance issues are moot, because the system performance will be limited by the disk's synchronous write performance, as seen in Figure 1. <p> The ATOMIC file server is based on an implementation of the NFS Version 2 specification [2], which has been superseded by the NFS Version 3 specification <ref> [5] </ref>. The most important change with respect to file transfer bandwidth is that asynchronous writes are explicitly supported by Version 3. This is a prerequisite for any high-bandwidth system based on NFS.
Reference: 6. <author> R. Macklem, </author> <title> Lessons Learned from Tuning the 4.3BSD Reno Implementation of the NFS Protocol, </title> <booktitle> Proceedings of the Winter USENIX Conference, </booktitle> <pages> pp. 53-64, </pages> <publisher> USENIX, </publisher> <address> Dallas, TX (January 1991). </address>
Reference-contexts: The baseline system for these improvements was the NFS system using null writes across the ATOMIC LAN (the rightmost bar in Figure 1). These improvements resulted in a 30% performance increase. We also estimated the performance benefits of rewriting the XDR interface in the style of Macklem <ref> [6] </ref>, i.e., inlining all function calls. Such If XDR overhead was completely removed by such inlin-ing, throughput could approach TCP levels. We did not implement Macklem's improvements, because we focused our efforts on evaluating new system performance enhancements. <p> Our measurements indicated that clients were fully utilizing their CPU, so our improvement efforts concentrated on improving the useful work done by that CPU. Specifically we set out to avoid unnecessary data structure translation overheads and context switch overheads. We also show that the XDR improvements suggested by Macklem <ref> [6] </ref> could improve NFS throughput to nearly TCP throughput, if they remove all XDR overhead. This is certainly an optimistic upper bound. Although we believe these optimizations to be effective, we did not implement them throughout our system because we focused our efforts on new improvements. <p> This could allow the ATOMIC file server to react dynamically to changing network state more simply than a multiprocess NFS implementation, but this is not implemented yet. 3.3. Data Encapsulation and Portability Macklem has shown that XDR implementations in the kernel are often a significant source of overhead <ref> [6] </ref>. XDR is a standard data encapsulation method used to communicate between servers and clients running on different hardware [4]. He observed significant increases in throughput of an NFS system running under the 4.3BSD Reno system after replacing the subroutine calls that implemented XDR conversion with macros. <p> The single-process model also offers new opportunities for dynamic control of file system bandwidth. We also showed that there is promise in inlining data encapsulations in the ATOMIC environment, as proposed by Macklem <ref> [6] </ref>. Streamlined data encapsulation improved data transfer speed to that of TCP. The protocol enhancements were then tested in a functional system including a high-bandwidth RAM disk, and the system was found to use the full bandwidth available.
Reference: 7. <author> Gregory R. Ganger and M. Franz Kaashoek, </author> <title> Embedded Inodes and Explicit Grouping: Exploiting Disk Bandwidth for Small Files, </title> <booktitle> USENIX 1997 Annual Technical Conference, </booktitle> <pages> pp. 1-18, </pages> <publisher> USENIX, </publisher> <address> Anaheim, CA (January 6-10, </address> <year> 1997). </year>
Reference-contexts: For example a client using the modified NFS presented here would benefit from using an NFS server that implemented the embedded inode and grouping optimizations described by Ganger and Kaashoek <ref> [7] </ref>. Also, in order to have real users for our prototype system there were considerable benefits to maintaining the NFS semantics that are understood by our users. As our experiment (in Figure 1) demonstrated, the protocol implementation bottlenecks of NFS are only visible when high-bandwidth hardware is in place.
Reference: 8. <author> Myricom, Inc., Nannette J. Boden, Danny Cohen, Robert E. Felderman, Alan E Kulawik, Charles L. Seitz, Jakov N. Selovic, and Wen-King Su, Myrinet: </author> <title> A Gigabit-per-second Local Area Network, </title> <booktitle> IEEE Micro, </booktitle> <pages> pp. 29-36, </pages> <note> IEEE (February 1995). </note>
Reference-contexts: Our testbed, which is used for all the experiments described in Section 3, uses Sun SPARCStation 20/71 workstations for both clients and servers. The hosts are connected by Myricom's implementation of the ATOMIC LAN [3], Myrinet <ref> [8] </ref>, which exhibits full duplex link rates of 640 Mb/sec, and a measured process-to-process TCP throughput in excess of 88 Mb/sec. NFS is not network-bound in this testbed. Null disk writes ensure that disk bottlenecks were avoided; however, without disk writes the system is impractical.
Reference: 9. <author> Sun Microsystems, Inc., </author> <title> Remote Procedure Call Specification, </title> <address> RFC-1057 (June 1, </address> <year> 1988). </year>
Reference-contexts: Using an alternate implementation of the data transfer protocol reduced context switches by more than 40%. Optimizing for throughput requires having multiple data requests outstanding to fill the bandwidth-delay pipeline between the server and client. In NFS, data transfers are effected via Sun's RPC protocol <ref> [9] </ref>, which, in SunOS 4.1.3, allows only one request per process to be pending. SunOS achieves request paral-lelism by using multiple processes, called biod processes, each running an instantiation of the RPC protocol. This is a common technique for file systems, especially NFS systems, and other transaction-based services. <p> Creating such a data transfer mechanism in SunOS was straightforward, because Sun RPC is designed to allow such an implementation <ref> [9] </ref>. The replacement protocol sends and receives standard Sun RPC messages, using the transaction ID to associate requests with responses. NFS was modified to use our single-process RPC implementation instead of biod processes.
Reference: 10. <author> Richard P. Draves, Brian N. Bershad, Richard F. Rashid, and Randall W. Dean, </author> <title> Using Continuations to Implement Thread Management and Communication in Operating Systems, </title> <booktitle> Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <pages> pp. 122-136, </pages> <note> ACM (October 1991). </note>
Reference-contexts: Converting multiple processes to a single process is similar in spirit and implementation to the use of continuations in Mach, and related systems <ref> [10] </ref>. The state of each incomplete NFS transaction is kept in the single process explicitly as a continuation rather than implicitly in the process stack. As Draves et al. point out, this reduces the code size of our implementation in addition to reducing the number of context switches.
Reference: 11. <author> David A. Patterson, Garth Gibson, and Randy H. Katz, </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proceedings of the ACM SIGMOD, </booktitle> <pages> pp. </pages> <month> 109-116 (June </month> <year> 1988). </year>
Reference-contexts: Related Work The ATOMIC project is working to scale a traditional distributed computing environment along the bandwidth axis, in which small file accesses are common. RAID systems are another way to achieve high throughput by striping data across clusters of disks <ref> [11] </ref>. They are inapplicable to our environment because they perform best under the sustained load facilitated by large files, rather than the bursty load of workstation users. Furthermore, accessing a RAID across a network entails all the software problems addressed by the ATOMIC file server. <p> The AT OMIC file server optimizes such a communication model for high bandwidth access. Scaling for throughput has been the goal of some file systems, but not in the conventional workstation environment. The Extensible File System (ELFS)[16], NFS/bds [12], and numerous RAID-based systems <ref> [11] </ref> are examples of high bandwidth systems. These systems are in use in supercomputer environments, which have less bursty workloads, and hosts with more powerful CPUs.
Reference: 12. <author> Larry McVoy, </author> <note> NFS/bds - NFS goes to the gym (December 1995), available electronically at &lt;http://reality.sgi.com/lm/talks/bds.ps&gt;. </note>
Reference-contexts: Rather than the transaction-based approach to data transfer that the ATOMIC file server uses, some systems advocate a streaming data protocol such as TCP to transfer files. This technique, used by NFS/bds, has shown promise, especially on machines using very powerful CPUs <ref> [12] </ref>. However, the added overhead of converting the data from an undelimited byte stream to operating system structures and realigning data in the absence of specialized hardware is CPU intensive at the client. <p> The AT OMIC file server optimizes such a communication model for high bandwidth access. Scaling for throughput has been the goal of some file systems, but not in the conventional workstation environment. The Extensible File System (ELFS)[16], NFS/bds <ref> [12] </ref>, and numerous RAID-based systems [11] are examples of high bandwidth systems. These systems are in use in supercomputer environments, which have less bursty workloads, and hosts with more powerful CPUs.
Reference: 13. <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout, </author> <title> Caching in the Sprite Network File System, </title> <journal> ACM Transactions on Operating Systems, </journal> <volume> vol. 6, no. 1, </volume> <pages> pp. </pages> <month> 134-154 (February </month> <year> 1988). </year>
Reference-contexts: The ATOMIC file server addresses the problem of utilizing network bandwidth. Other systems that use the same request/response communication model concentrated their efforts on adding functionality to the file system, and optimizing other parameters, for example improving caching or availability. Such systems include NFS, AFS, the Sprite file system <ref> [13] </ref>, the Spring file system [14], and Ficus [15]. The AT OMIC file server optimizes such a communication model for high bandwidth access. Scaling for throughput has been the goal of some file systems, but not in the conventional workstation environment.
Reference: 14. <author> Michael N. Nelson and Yousef A. Khalidi, </author> <title> Extensible File Systems in Spring, </title> <booktitle> Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 1-14, </pages> <note> ACM, Asheville, NC (December 1993). </note>
Reference-contexts: Other systems that use the same request/response communication model concentrated their efforts on adding functionality to the file system, and optimizing other parameters, for example improving caching or availability. Such systems include NFS, AFS, the Sprite file system [13], the Spring file system <ref> [14] </ref>, and Ficus [15]. The AT OMIC file server optimizes such a communication model for high bandwidth access. Scaling for throughput has been the goal of some file systems, but not in the conventional workstation environment.
Reference: 15. <author> Richard G. Guy, John S. Heidemann, Wai Mak, Thomas W. Page, Jr., Gerald J. Popek, and Dieter Rothmeier, </author> <title> Implementation of the Ficus Replicated File System, </title> <booktitle> USENIX Conference Proceeding, </booktitle> <pages> pp. 63-71, </pages> <month> USENIX (June </month> <year> 1990). </year>
Reference-contexts: Other systems that use the same request/response communication model concentrated their efforts on adding functionality to the file system, and optimizing other parameters, for example improving caching or availability. Such systems include NFS, AFS, the Sprite file system [13], the Spring file system [14], and Ficus <ref> [15] </ref>. The AT OMIC file server optimizes such a communication model for high bandwidth access. Scaling for throughput has been the goal of some file systems, but not in the conventional workstation environment.
Reference: 16. <author> John F. Karpovich, Andrew S. Grimshaw, and James C. </author> <title> French, Extensible File Systems (ELFS): An Object-Oriented Approach to High Performance File I/O, </title> <booktitle> Proceedings of the Ninth Annual Conference on Object-Oriented Programming Systems, Langauges and Applications (October 1994). </booktitle>
Reference: 17. <author> J. Hartman and J. Osterhout, </author> <title> The Zebra Striped Network File System, </title> <booktitle> Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 29-36, </pages> <publisher> ACM, </publisher> <address> Asheville, NC (Decem-ber 1993). </address>
Reference-contexts: These systems are in use in supercomputer environments, which have less bursty workloads, and hosts with more powerful CPUs. Because most scientific visualization and network of workstations environments exhibit architectures and workloads that are very similar to supercomputing environments, they are equivalent for the purpose of this discussion. Zebra <ref> [17] </ref> and the xFS serverless file system [18] have addressed the issue of high throughput in a cluster of workstations environment. Such systems are optimized for the demands of supercomputing applications that require access to large data files rather than the small file accesses of workstation users.
Reference: 18. <author> Thomas E. Anderson, Michael D. Dahlin, Jeanna M. Neefe, David A. Patterson, Drew S. Roselli, and Randolph Y. Wang, </author> <title> Serverless Network File Systems, </title> <booktitle> Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pp. 109-126, </pages> <publisher> ACM, </publisher> <address> Copper Mountain Resort, </address> <month> Colorado (December </month> <year> 1995). </year>
Reference-contexts: Because most scientific visualization and network of workstations environments exhibit architectures and workloads that are very similar to supercomputing environments, they are equivalent for the purpose of this discussion. Zebra [17] and the xFS serverless file system <ref> [18] </ref> have addressed the issue of high throughput in a cluster of workstations environment. Such systems are optimized for the demands of supercomputing applications that require access to large data files rather than the small file accesses of workstation users.
References-found: 18

