URL: ftp://ftp.cse.ucsc.edu/pub/hsnlab/lampros-dissertation.ps.gz
Refering-URL: http://www.cse.ucsc.edu/~lampros/papers.html
Root-URL: http://www.cse.ucsc.edu
Title: Congestion Management in High Speed Networks  
Author: Anujan Varma K.K. Ramakrishnan J.J. Garcia-Luna-Aceves Dean 
Degree: A dissertation submitted in partial satisfaction of the requirements for the degree of DOCTOR OF PHILOSOPHY in COMPUTER ENGINEERING by Lampros Kalampoukas  The dissertation of Lampros Kalampoukas is approved:  
Date: September 1997  
Affiliation: UNIVERSITY OF CALIFORNIA SANTA CRUZ  of Graduate Studies and Research  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Y. Afek, Y. Mansour, and Z. Ostfeld, Phantom: </author> <title> A simple and effective flow control scheme, </title> <booktitle> in Proc. of SIGCOMM'96, </booktitle> <volume> vol. 26, </volume> <pages> pp. 169182, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: As a result, 22 RTT seconds later the link utilization will get close to one. Several enhancements and extensions to the basic ERICA algorithm are also presented in [50, 51]. Many examples of state-less explicit rate allocation algorithms can be found in <ref> [1, 34, 72, 79, 90] </ref>. The schemes proposed in [61, 103] use control theoretic approaches with emphasis on system stability issues. A comparison of explicit rate allocation schemes can be found in [3]. 1.5 Packet Discard Approaches During congestion the switches may have to drop packets.
Reference: [2] <author> T. E. Anderson, S. S. Owicki, J. B. Saxe, and C. P. Thacker, </author> <title> High-speed switch scheduling for local-area networks, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 11, no. 4, 319352, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing [18, 26, 36, 52, 76, 101], and scheduling mechanisms <ref> [2, 23, 35, 37, 82, 93, 99] </ref>. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [3] <author> A. Arulambalam, X. Chen, and N. Ansari, </author> <title> Allocating fair rates for Available Bit Rate service in ATM Networks, </title> <journal> IEEE Communications Magazine, </journal> <volume> vol. 34, no. 11, 92100, </volume> <month> November </month> <year> 1996. </year>
Reference-contexts: Many examples of state-less explicit rate allocation algorithms can be found in [1, 34, 72, 79, 90]. The schemes proposed in [61, 103] use control theoretic approaches with emphasis on system stability issues. A comparison of explicit rate allocation schemes can be found in <ref> [3] </ref>. 1.5 Packet Discard Approaches During congestion the switches may have to drop packets. The decision of when and which packets to drop has implications on the performance of end-to-end protocols. TCP uses packet losses as congestion indication signals. <p> Several rate allocation algorithms that operate in the explicit-rate marking mode have been proposed [13, 46, 55] (for a survey, see <ref> [3] </ref>). These approaches differ in terms of their execution time, implementation complexity, level of fairness in the allocation of the available bandwidth, responsiveness to network changes, convergence time, and stability properties.
Reference: [4] <author> H. Balakrishnan, V. N. Padmanabhan, S. Seshan, and R. H. Katz, </author> <title> A comparison of mechanisms for improving TCP performance over wireless links, </title> <booktitle> in Proc. of SIGCOMM'96, </booktitle> <volume> vol. 26, </volume> <pages> pp. 25669, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Several techniques has been developed with the objective of improving TCP performance in such environments. A discussion of such techniques can be found in <ref> [4, 19] </ref>. 1.2.2 Link-by-Link Window-Based Approaches Most of the schemes in this category have been proposed in the context of ATM networks [41, 64]. Window-based control that is applied on a connection and link-by-link basis is also referred to as credit-based control.
Reference: [5] <author> A. W. Barnhart, </author> <title> Evaluation of Source Behavior Specification, ATM Forum, Traffic Management Working Group, </title> <month> April </month> <year> 1995, </year> <title> ATM Forum/95-0267R1. </title>
Reference-contexts: We first start with the simple case of D = 0, and use a method similar to that of Barnhart <ref> [5] </ref> to derive a closed-form expression for the instantaneous rate of the source. In the actual source policy, rate increases occur only when an RM cell is received, and the increase occurs in steps of N rm AIR.
Reference: [6] <author> D. Bertsekas and R. Gallager, </author> <title> Data networks, </title> <publisher> Prentice Hall, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: 1. Introduction Congestion refers to the situation where network resources are persistently overloaded or equivalently, the total demand of a resource exceeds its capacity. Congestion is the result of the statistical multiplexing principle <ref> [6, 58] </ref> upon which the design of packet-switched networks is based, with the objective of maximizing resource utilization. Congestion is a problem that cannot by solved by current technological advances alone. <p> Connections 3 whose requests are not fully satisfied are allocated the same amount under a max-min fair allocation. A max-min allocation maximizes the utilization of the bottlenecked resource (usually link or buffer). A formal definition of max-min fairness can be found in <ref> [6, 58] </ref>. <p> An iterative procedure that performs such a computation in 21 centralized environment can be found in <ref> [6] </ref>. Although the centralized version of the problem is easy to solve, the distributed version is much harder. Charny [13, 14] proposed an iterative distributed method that can be used to perform explicit rate allocation. The proposed method maintains per-connection state information. <p> This introduces the need for an algorithm, that will be executed by network elements, to perform the bandwidth allocation. The goal of the allocation is to arrive at an efficient allocation that is also max-min fair <ref> [6] </ref>. Each switch on the path of the RM cell may modify the request based on the bandwidth it is able to allocate to the connection on its outbound link. <p> These approaches differ in terms of their execution time, implementation complexity, level of fairness in the allocation of the available bandwidth, responsiveness to network changes, convergence time, and stability properties. In this chapter we describe and algorithm that can arrive at a max-min fair alloca 31 tion <ref> [6] </ref> or variations of max-min fairness defined in [89]. We study the dynamics and the performance of the rate allocation algorithm under both ATM-layer-generated ABR traffic and TCP-controlled ABR traffic.
Reference: [7] <author> F. Bonomi and K. W. Fendick, </author> <title> The rate-based flow Control framework for the Available Bit Rate ATM service, </title> <journal> IEEE Network, </journal> <volume> vol. 9, no. 2, 2539, </volume> <month> March/April </month> <year> 1995. </year>
Reference-contexts: The Available-Bit-Rate (ABR) service class, defined to support delay tolerant best-effort applications, uses rate-based feedback mechanisms to allow them to adjust their transmission rates to make full utilization of the available bandwidth <ref> [7] </ref>. Compliant connections are also assured of a low loss rate, and if needed, a minimum bandwidth allocation. This class of service admits to the possibility of congestion, where the aggregate demand of the sources exceeds the capacity of the network resources. <p> Keshav et. al. [59] propose extensions to the packet-pair scheme to handle random packet losses better. 1.3.1.2 Rate-Based Congestion Control Framework for ABR Service in ATM Networks More recently, the ATM Forum specified a rate-based framework for the support of the ABR service in ATM networks <ref> [7] </ref>. The defined framework uses explicit feedback information to adjust the transmission rates of individual connections at the source end system. This framework defines the behavior of the source (SES) and destination (DES) end-systems, but leaves the switch behavior unspecified in order to provide the switch vendors with more flexibility. <p> Our interest in this chapter is on TCP operating over a rate-controlled channel, such as that offered by the Available Bit Rate (ABR) service in an Asynchronous Transfer Mode (ATM) network <ref> [7] </ref>. It is expected that the burstiness of the traffic seen at the network nodes (switches or routers) would be less in a rate-controlled network as compared to one without rate control. <p> Several earlier studies have been reported on the behavior of TCP in ATM networks [21, 33, 53], but none of them consider the effects of two-way traffic on TCP behavior. The Available-Bit-Rate (ABR) service class <ref> [7] </ref> was defined to support delay-tolerant best-effort applications and employ rate-based feedback mechanisms to allow the sources to adjust their transmission rates to make full utilization of the available network capacity [7]. <p> The Available-Bit-Rate (ABR) service class <ref> [7] </ref> was defined to support delay-tolerant best-effort applications and employ rate-based feedback mechanisms to allow the sources to adjust their transmission rates to make full utilization of the available network capacity [7]. The rate-control framework developed by the ATM Forum allows a number of options for the switches to signal their congestion state to the source. The most promising of these, the explicit-rate marking option, is the focus of our work.
Reference: [8] <author> R. Braden and J. Postel, </author> <title> Transmission Control Protocol, Request for Comments: </title> <type> 793, </type> <month> September </month> <year> 1981. </year>
Reference-contexts: TCP uses the mean and the deviation of the RTT measurements to determine the retransmission time-out (RTO) value. Jacobson [42] has shown that using both the mean and the deviation of RTT 10 improves TCP performance compared to the case where only the mean is considered <ref> [8] </ref>. TCP controls congestion using window-based control on an end-to-end basis. Although there are many similarities in the window adjustment process between TCP and DECbit, their main difference is that TCP uses implicit methods to detect congestion and makes no assumptions about the underlying network technology. <p> Jacobson et. al. [45] introduced a window scaling option that allows the TCP window to grow beyond the 64 KBytes limit specified in <ref> [8] </ref>. Such a modification will improve TCP performance in networks with large bandwidth-delay product. To adapt faster to the network state, it is necessary for TCP to maintain a very accurate RTT estimation.
Reference: [9] <author> L. S. Brakmo and L. L. Peterson, </author> <title> Performance problems in BSD4.4 TCP, </title> <journal> Computer Communication Review, </journal> <volume> vol. 25, no. 5, 6984, </volume> <month> October </month> <year> 1995. </year>
Reference-contexts: A detailed description of TCP, the algorithms it employs, and its implementation can be found in [91, 92]. Several studies report on the efficiency of the mechanisms employed by TCP and its performance <ref> [9, 27, 30, 40, 98, 100] </ref>. An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in [53, 87]. Finally, performance issues of TCP over ATM for a specific TCP implementation are discussed in [21] and [78].
Reference: [10] <author> L. S. Brakmo and L. L. Peterson, </author> <title> TCP Vegas: End to end congestion avoidance on a global Internet, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 13, no. 8, 146580, </volume> <month> October </month> <year> 1995. </year>
Reference-contexts: To adapt faster to the network state, it is necessary for TCP to maintain a very accurate RTT estimation. Jacobson et. al. [45] proposed to perform an RTT measurement for each data packet transmitted rather than only once for every window worth of data. The TCP-Vegas <ref> [10] </ref> and Tri-S [96] approaches both recognized that once the TCP window is large enough to fill the round-trip network pipe, further window increases contribute only to increased queueing delays in the network without improving its performance. <p> Depending on the relative difference between the expected and the measured throughputs the source may have to reduce the size of the window. Both simulation and experimental studies report on the dynamics and the performance of TCP-Vegas <ref> [10, 69] </ref>. Floyd proposed extensions to TCP so that it will become responsive to explicit congestion notification (ECN) messages [28]. <p> Even though several extensions to TCP have been proposed with the objective of adapting the window size to the actual bandwidth-delay product of the underlying network <ref> [10, 96] </ref>, current implementations require a packet loss to reduce the congestion window size. In 193 the absence of such losses the congestion window increases up to the maximum socket buffer advertised by the receiver.
Reference: [11] <author> CCITT, </author> <title> Draft Recommendation I.363, CCITT Study Group XVIII, </title> <address> Geneva, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: The peak rate for all VCs (link rate). For convenience, we have used the same set of source parameters for all configurations, although the round-trip delays of connections vary over a wide range. When simulating TCP over ATM, we use the ATM Adaptation Layer Type 5 (AAL 5) <ref> [11] </ref>. AAL 5 performs segmentation and re-assembly between IP packets and ATM cells. Each IP packet is extended by eight bytes to accommodate the AAL header. Thus, the number of ATM cells produced by the original IP datagram is given by ~ IP packet size+8 48 .
Reference: [12] <author> C.-H. Chang, D. Flower, J. Forecase, H. Gray, B. Hawe, A. Nadkarni, K. K. Ra-makrishnan, U. Shikarpur, and K. Wilde, </author> <title> High-performance TCP/IP and UDP/IP networking in DEC OSF/1 for Alpha AXP, </title> <booktitle> in Proc. of the Third IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pp. 3542, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: For example, the transmission time of a 9-Kbyte packet at a link rate of 10 Mbits/second is approximately 8 msecs, while the TCP protocol processing time in real systems typically does not exceed a few hundred microseconds <ref> [12, 83, 102] </ref>. The effect of non-zero TCP processing time on throughput is considered in the simulation results presented in Section 4.4. The functionality assumed for the IP layer is simple. <p> For example, the transmission time of a 1500 byte packet over a 100 Kbits/sec link is about 120 msecs, while the TCP protocol processing time in modern workstations does not exceed a few hundred microseconds <ref> [12, 83, 102] </ref>. Thus, we can safely assume that t pr is zero. 5.2 Demonstrating Effect of Ack Compression over Asymmetric Links We use both simulation and experimentation to illustrate that link asymmetry may cause severe performance degradation to TCP. TCP connections.
Reference: [13] <author> A. Charny, </author> <title> An algorithm for rate allocation in a packet-switching network with feedback, </title> <type> Master's thesis, </type> <institution> Massachusets Institute of Technology, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: An iterative procedure that performs such a computation in 21 centralized environment can be found in [6]. Although the centralized version of the problem is easy to solve, the distributed version is much harder. Charny <ref> [13, 14] </ref> proposed an iterative distributed method that can be used to perform explicit rate allocation. The proposed method maintains per-connection state information. The rate allocation procedure is executed whenever an RM cell is received. <p> On reaching its destination, the RM cell is returned to the source, which now sets its rate based on that allocated on the bottleneck link in the path of the connection. Several rate allocation algorithms that operate in the explicit-rate marking mode have been proposed <ref> [13, 46, 55] </ref> (for a survey, see [3]). These approaches differ in terms of their execution time, implementation complexity, level of fairness in the allocation of the available bandwidth, responsiveness to network changes, convergence time, and stability properties. <p> Such algorithms for rate allocation in packet-switched networks have been described by Charny <ref> [13] </ref>, Jain [46], Kalampoukas, et al. [55], and Siu, et al. [90]. In this section we describe the operation of the proposed rate allocation algorithm and discuss its properties. <p> The analysis assumes the source rate increase algorithm of equation 3.1.1. In addition, we also assume that the rate allocation algorithm in the switches provides an allocation at least equal to the current transmission rate throughout the increase period. The rate allocation algorithms described in <ref> [13] </ref> and [55] can meet this requirement. 3.2.1 Approximation for Zero Round-Trip Delay Let D be the round-trip delay along the path of the connection. We assume D to be constant in this analysis. <p> The assumption of uncongested network is reasonable in the context of ATM networks that support explicit rate allocation. Under this assumption, since each connection is guaranteed a minimum rate, the performance of a TCP connection may be analyzed independent of the other connections. The rate allocation algorithms of <ref> [13, 55] </ref> meet this requirement. Assuming that the idle-source detection option is not used, the rate of the ATM source will monotonically increase during the TCP slow-start phase as the window-size is increased. However, the evolution of the source can be different from that seen in Section 3.
Reference: [14] <author> A. Charny, D. D. Clark, and R. Jain, </author> <title> Congestion control with explicit rate indication, </title> <booktitle> in Proc. of IEEE ICC'95, </booktitle> <volume> vol. 3, </volume> <pages> pp. </pages> <address> 195463, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: An iterative procedure that performs such a computation in 21 centralized environment can be found in [6]. Although the centralized version of the problem is easy to solve, the distributed version is much harder. Charny <ref> [13, 14] </ref> proposed an iterative distributed method that can be used to perform explicit rate allocation. The proposed method maintains per-connection state information. The rate allocation procedure is executed whenever an RM cell is received.
Reference: [15] <author> A. Charny, K. K. Ramakrishnan, and A. Lauck, </author> <title> Time scale analysis and scalability issues for explicit rate allocation in ATM networks, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> vol. 4, no. 4, 569581, </volume> <year> 1996. </year>
Reference-contexts: Thus, it does not scale well with increasing number of connections. Scaling is a critical issue in ATM networks. The convergence and dynamic properties of this scheme as well as possible enhancements are discussed in <ref> [15] </ref>.
Reference: [16] <author> D.-M. Chiu and R. Jain, </author> <title> Analysis of increase and decrease algorithms for congestion avoidance in computer networks, </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> vol. 17, 1 14, </volume> <year> 1989. </year> <month> 242 </month>
Reference-contexts: The combination of additive increase and multiplicative decrease mechanisms with selective marking has been shown to lead to a stable and fair system <ref> [16] </ref>. 1.2.1.2 Congestion Control in TCP TCP is the most commonly used transport-layer protocol today, due to the popularity of the Internet. TCP has a two-fold task: (i) to provide reliable data transmissions and (ii) to control congestion.
Reference: [17] <author> A. K. Choudhury and E. L. Hahne, </author> <title> Dynamic queue length thresholds in a shared memory ATM switch, </title> <booktitle> in Proc. of IEEE INFOCOM'96, </booktitle> <volume> vol. 2, </volume> <pages> pp. 67987, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: The computed feedback is a function of the free buffer space at the AAP. In that sense it is similar to the idea proposed by Choudhury and Hahne <ref> [17] </ref> for controlling dynamic buffer thresholds in a shared-memory switch. In our 204 case, however, the feedback computed is used to adapt the TCP window maintained at the sources in order to limit packet losses in the AAP's buffer.
Reference: [18] <author> D. D. Clark, S. Shenker, and L. Zhang, </author> <title> Supporting real-time applications in an integrated services packet network: Architecture and mechanism, </title> <booktitle> in Proc. of ACM SIGCOMM'92, </booktitle> <pages> pp. 1426, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing <ref> [18, 26, 36, 52, 76, 101] </ref>, and scheduling mechanisms [2, 23, 35, 37, 82, 93, 99]. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [19] <author> J. A. Cobb and P. Agrawal, </author> <title> Congestion of corruption? A strategy for efficient wireless TCP sessions, </title> <booktitle> in Proc. of IEEE Symposium on Computers and Communications, </booktitle> <pages> pp. 262268, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Several techniques has been developed with the objective of improving TCP performance in such environments. A discussion of such techniques can be found in <ref> [4, 19] </ref>. 1.2.2 Link-by-Link Window-Based Approaches Most of the schemes in this category have been proposed in the context of ATM networks [41, 64]. Window-based control that is applied on a connection and link-by-link basis is also referred to as credit-based control.
Reference: [20] <author> R. Cole, D. Shur, and C. Villamizar, </author> <title> IP over ATM: A framework document, Request for Comments (RFC): </title> <year> 1932, </year> <month> April </month> <year> 1996. </year>
Reference-contexts: The approach currently being undertaken is to map all TCP connections set up between the end-systems (in general, subnetworks containing the end-systems) to a single ATM connection <ref> [20] </ref>. Synchronization among TCP connections that are transported within a common ATM virtual channel can give rise to the effects discussed above. 4.4.7 Simulation Results for a Client-Server Topology We close this section with some simulation results from a final network configuration, one that is encountered frequently. <p> We will refer to the routers at the boundary between the two networks as ATM Access Points (AAPs) and to the LAN segments as IP networks. For IP over ATM we assume the framework described in <ref> [20] </ref>. According to this framework, a single VC is set up for carrying ABR traffic between a source and a destination system. TCP connections destined to different IP networks will be carried over separate VCs.
Reference: [21] <author> D. E. Comer and J. C. Lin, </author> <title> TCP buffering and performance over an ATM network, Internetworking: </title> <journal> Research and Experience, </journal> <volume> vol. 6, 113, </volume> <year> 1995. </year>
Reference-contexts: An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in [53, 87]. Finally, performance issues of TCP over ATM for a specific TCP implementation are discussed in <ref> [21] </ref> and [78]. The assumption that a packet loss is a reliable indication of network congestion may cause performance degradation in environments with high bit error rates, such as wireless networks. Several techniques has been developed with the objective of improving TCP performance in such environments. <p> Although our analysis is focussed on TCP over ABR, the results apply equally well to TCP operating over other networks providing a steady rate, predictable delay, and in-order delivery. Several earlier studies have been reported on the behavior of TCP in ATM networks <ref> [21, 33, 53] </ref>, but none of them consider the effects of two-way traffic on TCP behavior.
Reference: [22] <author> C. Dalton, G. Watson, D. Banks, C. Calamvokis, and A. Edwards, </author> <title> Afterburner (network-independent card for protocols), </title> <journal> IEEE Network, </journal> <volume> vol. 7, no. 4, 3643, </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: Therefore, we anticipate that the TCP/IP processing overhead in a 150 SPECmark machine will be significantly lower, of the order of 50 seconds. This excludes copy and checksum overhead, which are beginning to become insignificant with hardware support in zero-copy interfaces <ref> [22] </ref>. For our specific example, the segment transmission time at a link rate of 155 Mbits/second 146 1L (data packets). cessing overhead. is t x = 0:56 msecs. Assuming a TCP processing time of 50 secs, the value of L pr is 0:09 seg ments.
Reference: [23] <author> A. Demers, S. Keshav, and S. Shenker, </author> <title> Analysis and simulation of a fair queueing algorithm, Internetworking: </title> <journal> Research and Experience, </journal> <volume> vol. 1, no. 1, 326, </volume> <month> September </month> <year> 1990. </year>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing [18, 26, 36, 52, 76, 101], and scheduling mechanisms <ref> [2, 23, 35, 37, 82, 93, 99] </ref>. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [24] <author> B. J. Ewy, J. B. Evans, V. S. Frost, and G. J. Minden, </author> <title> TCP/ATM experiences in the MAGIC testbed, </title> <booktitle> in Proc. of the Fourth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pp. 8793, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: The performance of RED is explored in [32] where it has been shown to improve both the throughput and the fairness of TCP compared to the Drop-Tail scheme. Experimental studies on RED can be found in <ref> [24, 95] </ref>. Finally, Floyd et. al. [29] address some of the shortcomings of RED and discuss possible enhancements.
Reference: [25] <author> K. W. Fendick, </author> <title> Evolution of controls for the Available Bit Rate sercice, </title> <journal> IEEE Communications Magazine, </journal> <volume> vol. 34, no. 11, 3039, </volume> <month> November </month> <year> 1996. </year>
Reference-contexts: The evolution of the congestion control framework and the trade-offs made during the process of its specification are discussed in <ref> [25, 80, 86] </ref>. A detailed discussion of the source system behavior for the rate-based framework can be found in [49, 68]. 1.3.2 Hop-by-Hop Rate-Based Approaches Rate-based control can be applied on a link-by-link (also known as hop-by-hop) basis as well. Mishra et. al. [74] have developed such a scheme.
Reference: [26] <author> D. Ferrari and D. Verma, </author> <title> A scheme for real-time channel establishment in wide-area networks, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 8, no. 3, 368379, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing <ref> [18, 26, 36, 52, 76, 101] </ref>, and scheduling mechanisms [2, 23, 35, 37, 82, 93, 99]. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [27] <author> S. Floyd, </author> <title> Connections with multiple congested gateways in packet-switched networks, Part I: One-way traffic, </title> <journal> Computer Communication Review, </journal> <volume> vol. 21, no. 5, 3047, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: A detailed description of TCP, the algorithms it employs, and its implementation can be found in [91, 92]. Several studies report on the efficiency of the mechanisms employed by TCP and its performance <ref> [9, 27, 30, 40, 98, 100] </ref>. An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in [53, 87]. Finally, performance issues of TCP over ATM for a specific TCP implementation are discussed in [21] and [78]. <p> The decision of when and which packets to drop has implications on the performance of end-to-end protocols. TCP uses packet losses as congestion indication signals. As shown in <ref> [27, 30] </ref>, its performance depends not only on the number of losses but their pattern as well. Because of its popularity, significant research effort has focused on devising packet discard methods that will improve not only the performance, but also the fairness properties of TCP. <p> TCP will detect the loss either from a timeout or from returning acks and will retransmit the lost packet. The advantage of Drop-Tail switches is their simplicity. However, they do not provide any isolation between different TCP flows. As pointed out in <ref> [27, 30] </ref>, in switches with Drop-Tail packet discard policy, losses tend to occur in bursts. That is, a connection is likely to lose multiple packets within the same window. Furthermore, during congestion, multiple connections are likely to face similar loss patterns. <p> to 5 msecs, ATM backbone delay = 5 msecs). the ATM link at IP-ATM Router 1 with EWA measured in intervals of 250 msecs (IP Network 1 delays vary from 1 to 5 msecs, ATM back bone delay = 5 msecs). effect of TCP favoring connections with smaller round-trip times <ref> [27] </ref>. Achieving equal throughputs under asymmetric RTTs requires bandwidth allocation and scheduling at the level of individual TCP flows. The buffer occupancy in AAP-1 is shown in Figure 6.27.
Reference: [28] <author> S. Floyd, </author> <title> TCP and explicit congestion notification, </title> <journal> Computer Communication Review, </journal> <volume> vol. 24, no. 5, 823, </volume> <month> October </month> <year> 1994. </year>
Reference-contexts: Both simulation and experimental studies report on the dynamics and the performance of TCP-Vegas [10, 69]. Floyd proposed extensions to TCP so that it will become responsive to explicit congestion notification (ECN) messages <ref> [28] </ref>. With ECN, sources can be informed of incipient 13 congestion quickly and unambiguously, without having to wait for a timeout event or for three duplicate acks, potentially reducing the number of packet losses. <p> Floyd suggests that TCP's response to explicit congestion indication should be similar to its response to the receipt of three duplicate acks. Simulation results presented in <ref> [28] </ref> showed improved throughput and fairness and lower response time for delay-sensitive applications (e.g. telnet). A detailed description of TCP, the algorithms it employs, and its implementation can be found in [91, 92].
Reference: [29] <author> S. Floyd and K. </author> <month> Fall, </month> <title> Router mechanisms to support end-to-end congestion control, </title> <institution> Technical Report http://ftp.ee.lbl.gov/floyd/papers.html, Lawrence Berkeley Laboratories, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: The performance of RED is explored in [32] where it has been shown to improve both the throughput and the fairness of TCP compared to the Drop-Tail scheme. Experimental studies on RED can be found in [24, 95]. Finally, Floyd et. al. <ref> [29] </ref> address some of the shortcomings of RED and discuss possible enhancements.
Reference: [30] <author> S. Floyd and V. Jacobson, </author> <title> On traffic phase effects in packet-switched gateways, Internetworking: </title> <journal> Research and Experience, </journal> <volume> vol. 3, no. 3, 115156, </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: A detailed description of TCP, the algorithms it employs, and its implementation can be found in [91, 92]. Several studies report on the efficiency of the mechanisms employed by TCP and its performance <ref> [9, 27, 30, 40, 98, 100] </ref>. An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in [53, 87]. Finally, performance issues of TCP over ATM for a specific TCP implementation are discussed in [21] and [78]. <p> The decision of when and which packets to drop has implications on the performance of end-to-end protocols. TCP uses packet losses as congestion indication signals. As shown in <ref> [27, 30] </ref>, its performance depends not only on the number of losses but their pattern as well. Because of its popularity, significant research effort has focused on devising packet discard methods that will improve not only the performance, but also the fairness properties of TCP. <p> TCP will detect the loss either from a timeout or from returning acks and will retransmit the lost packet. The advantage of Drop-Tail switches is their simplicity. However, they do not provide any isolation between different TCP flows. As pointed out in <ref> [27, 30] </ref>, in switches with Drop-Tail packet discard policy, losses tend to occur in bursts. That is, a connection is likely to lose multiple packets within the same window. Furthermore, during congestion, multiple connections are likely to face similar loss patterns. <p> These synchronized losses result in a corresponding synchronized reaction by multiple connections simultaneously reducing their windows. The potential over-correction may eventually lead to reduced throughput <ref> [30] </ref>. Furthermore, if multiple losses occur from a single connection within one RTT, the connection may be forced into the slow-start phase. We will now consider two scenarios to illustrate the effects of packet losses occurring at the AAPs on TCP performance.
Reference: [31] <author> S. Floyd and V. Jacobson, </author> <title> On traffic phase effects in packet-switched gateways, Internetworking: </title> <journal> Research and Experience, </journal> <pages> pp. 115156, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: This is considerably different from the behavior of TCP in a datagram-based network, where it has been shown to favor short connections <ref> [31] </ref>. Once steady-state has been reached, the congestion windows remain at their maximum set value of 150 Kbytes until 0.5 second, when a packet loss is simulated by discarding a single ATM cell from connection 1.
Reference: [32] <author> S. Floyd and V. Jacobson, </author> <title> Random Early Detection gateways for congestion avoidance, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> vol. 1, no. 4, 397413, </volume> <month> August </month> <year> 1993. </year>
Reference-contexts: With Drop-Tail, Drop-from-Front,and Random-Drop, the switches operate in a reactive mode: a packet is dropped only when the buffer overflows. The early random drop [38] and random early detection (RED) <ref> [32] </ref> approaches propose to drop packets when incipient congestion is detected without waiting for the buffer to overflow. That is, they operate 24 pro-actively. In the early random drop strategy, packets are dropped with a fixed probability when the queue length exceeds a certain threshold. <p> Alternately, RED can be used to mark packets instead of dropping them. This is useful when the traffic is controlled by a DECbit-like source, which is responsive to explicit congestion notification messages. The performance of RED is explored in <ref> [32] </ref> where it has been shown to improve both the throughput and the fairness of TCP compared to the Drop-Tail scheme. Experimental studies on RED can be found in [24, 95]. Finally, Floyd et. al. [29] address some of the shortcomings of RED and discuss possible enhancements. <p> larger portion of the available bandwidth. 198 6.1.1 TCP Performance with Drop-from-Front AAPs To reduce the tendency of Drop-Tail routers to synchronize losses of multiple connections, and the possibility of multiple losses to a connection in a round-trip, two alternative policies have been suggested: Drop-from-Front [67] and Random Early Detection <ref> [32] </ref>. With Drop-from-Front, when a packet arrives at a full queue, the packet stored at the head of the queue is dropped. If service for the packet at the head of the queue has already started, the following packet in the queue is dropped instead. <p> In contrast to the Drop-Tail case, the progress of the two active TCP connections is fair. 6.1.2 TCP performance with Random Early Detection (RED) AAPs The Random Early Detection (RED) scheme also has the same objective of improving TCP throughput and fairness <ref> [32] </ref>. Congestion is determined by comparing the average queue size to a predetermined threshold. Congestion can be detected even before the buffer is full, thus controlling the average queue size. In periods of congestion RED marks incoming packets in order to indicate congestion to TCP sources. <p> However, RED succeeds in achieving fairness. To study the sensitivity of the scheme to the maximum marking probability, we increased max p from 0.02 to 0.05, while keeping the rest of the parameters the same. This value is within the range suggested for RED <ref> [32] </ref>. The sequence number plots for this case are given in Figure 6.11, and the measured link utilization in Figure 6.12. We observe that the connection progress is again fair but the overall performance is further degraded, and the results are now comparable to those from Drop-from-Front.
Reference: [33] <author> S. Floyd and A. Romanow, </author> <title> Dynamics of TCP traffic over ATM networks, </title> <booktitle> in Proc. of ACM SIGCOMM'94, </booktitle> <pages> pp. 7988, </pages> <month> September </month> <year> 1994. </year> <month> 243 </month>
Reference-contexts: Furthermore, we study the behavior of the rate allocation algorithm under TCP-controlled ABR traffic. Several studies have shown that in certain configurations, in the absence of an ATM-layer congestion control mechanism, TCP performance degrades when operating over ATM networks <ref> [33, 53] </ref>. In this work we study the interaction of TCP built-in mechanisms for protection against congestion with our proposed explicit rate allocation scheme in conjunction with the basic rate increase/decrease algorithm of ATM Forum's source policy. <p> It is also our objective to characterize the effect of this option on TCP behavior. 72 Several studies have been reported on the performance of TCP in ATM networks. While previous studies have focused on the effect of fragmentation of TCP segments at the ATM layer <ref> [33] </ref>, and the fairness behavior with multiple TCP connections [53], our interest in this chapter is in investigating how the ATM source policy in the rate-based congestion control algorithm affects the congestion control and avoidance mechanisms currently employed in TCP. <p> Although our analysis is focussed on TCP over ABR, the results apply equally well to TCP operating over other networks providing a steady rate, predictable delay, and in-order delivery. Several earlier studies have been reported on the behavior of TCP in ATM networks <ref> [21, 33, 53] </ref>, but none of them consider the effects of two-way traffic on TCP behavior.
Reference: [34] <author> C. Fulton, S.-Q. Li, and C. S. Lim, </author> <title> An ABR feedback control scheme with tracking, </title> <booktitle> in Proc. of IEEE INFOCOM'97, </booktitle> <pages> pp. 806813, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: As a result, 22 RTT seconds later the link utilization will get close to one. Several enhancements and extensions to the basic ERICA algorithm are also presented in [50, 51]. Many examples of state-less explicit rate allocation algorithms can be found in <ref> [1, 34, 72, 79, 90] </ref>. The schemes proposed in [61, 103] use control theoretic approaches with emphasis on system stability issues. A comparison of explicit rate allocation schemes can be found in [3]. 1.5 Packet Discard Approaches During congestion the switches may have to drop packets.
Reference: [35] <author> S. Golestani, </author> <title> A self-clocked fair queueing scheme for broadband applications, </title> <booktitle> in Proc. of INFOCOM'94, </booktitle> <pages> pp. 636646. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing [18, 26, 36, 52, 76, 101], and scheduling mechanisms <ref> [2, 23, 35, 37, 82, 93, 99] </ref>. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [36] <author> M. Grossglauser, S. Keshav, and D. Tse, RCBR: </author> <title> A simple and efficient service for multiple time-scale traffic, </title> <booktitle> in Proc. of ACM SIGCOMM'95, </booktitle> <volume> vol. 25, </volume> <pages> pp. 21930, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing <ref> [18, 26, 36, 52, 76, 101] </ref>, and scheduling mechanisms [2, 23, 35, 37, 82, 93, 99]. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [37] <author> E. Hahne, </author> <title> Round-robin scheduling for max-min fairness in data networks, </title> <journal> IEEE Journal on Selected Areas of Communications, </journal> <volume> vol. 9, no. 7, 10241039, </volume> <month> September </month> <year> 1986. </year>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing [18, 26, 36, 52, 76, 101], and scheduling mechanisms <ref> [2, 23, 35, 37, 82, 93, 99] </ref>. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified. <p> The downstream node sends a credit to the upstream node for every data packet it forwards. To reserve bandwidth, these credits may be aggregated and a single credit update message may be sent. Fairness can be achieved by serving individual connection in a round-robin fashion <ref> [37] </ref>. It is demonstrated in [53, 65] that credit-based control with strict buffer partitioning and round-robin scheduling provides loss-free and fair operation while maximizing the network performance. The application of credit-based control with strict buffer partitioning may be limited by its memory requirements in wide-area networks (WAN).
Reference: [38] <author> E. Hashem, </author> <title> Analysis of random drop for gateway congestion control, </title> <type> Technical Report LCS/TR-465, </type> <institution> Laboratory for Computer Science, MIT, </institution> <year> 1989. </year>
Reference-contexts: With Drop-Tail, Drop-from-Front,and Random-Drop, the switches operate in a reactive mode: a packet is dropped only when the buffer overflows. The early random drop <ref> [38] </ref> and random early detection (RED) [32] approaches propose to drop packets when incipient congestion is detected without waiting for the buffer to overflow. That is, they operate 24 pro-actively.
Reference: [39] <author> D. P. Heyman, A. Tabatabai, and T. V. Lakshman, </author> <title> Statistical analysis and simulation study of video teleconference traffic in ATM networks, </title> <journal> IEEE Transactions on Circuits and Systems for Video Technology, </journal> <volume> vol. 2, no. 1, 4959, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: All the ABR connections request peak bandwidth. The VBR connections have an allocated bandwidth, which is based on the average rate for the video data generated by the application. The VBR traffic is based on the model described by Heyman et. al. <ref> [39] </ref>.
Reference: [40] <author> J. C. Hoe, </author> <title> Start-up dynamics of TCP's congestion control and avoidance schemes, </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: A detailed description of TCP, the algorithms it employs, and its implementation can be found in [91, 92]. Several studies report on the efficiency of the mechanisms employed by TCP and its performance <ref> [9, 27, 30, 40, 98, 100] </ref>. An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in [53, 87]. Finally, performance issues of TCP over ATM for a specific TCP implementation are discussed in [21] and [78].
Reference: [41] <author> I. Iliadis, </author> <title> A new feedback congestion control policy for long propagation delays, </title> <journal> IEEE Journal on Selected Areas of Communications, </journal> <volume> vol. 13, no. 7, 12841295, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: Several techniques has been developed with the objective of improving TCP performance in such environments. A discussion of such techniques can be found in [4, 19]. 1.2.2 Link-by-Link Window-Based Approaches Most of the schemes in this category have been proposed in the context of ATM networks <ref> [41, 64] </ref>. Window-based control that is applied on a connection and link-by-link basis is also referred to as credit-based control. With credit-based control and with reference to a single link, the upstream and the downstream nodes of a connection exchange explicit information regarding buffer availability in the downstream node.
Reference: [42] <author> V. Jacobson, </author> <title> Congestion avoidance and control, </title> <booktitle> in Proc. of ACM SIGCOMM'88, </booktitle> <pages> pp. 314329, </pages> <year> 1988. </year>
Reference-contexts: In current TCP implementations (Tahoe and Reno), only one packet is timed per RTT. TCP uses the mean and the deviation of the RTT measurements to determine the retransmission time-out (RTO) value. Jacobson <ref> [42] </ref> has shown that using both the mean and the deviation of RTT 10 improves TCP performance compared to the case where only the mean is considered [8]. TCP controls congestion using window-based control on an end-to-end basis. <p> Thus, the number of ATM cells produced by the original IP datagram is given by ~ IP packet size+8 48 . The model for TCP used in the simulations is based on the TCP-Reno version. It supports the congestion control mechanism described by Jacobson <ref> [42] </ref>, exponential back-off, enhanced round-trip (RTT) estimation based on both the mean and the variance of the measured RTT, and the fast retransmit and fast recovery mechanisms. <p> Before proceeding further, it is important to mention how ack compression originates at the end systems in the two-way environment and how it causes throughput degradation. The genesis of ack compression can be traced to the slow-start phase of a TCP connection that increases the window progressively at startup <ref> [42] </ref>. The slow-start algorithm sets the initial window size to one and increases it by one with every acknowledgement received. This effectively doubles the window every round-trip time. Thus, during slow start, the receipt of every ack causes the end system to add two segments to its outgoing queue. <p> The model of TCP used in the simulations is based on the TCP-Reno version. It supports the 168 congestion control mechanism described by Jacobson <ref> [42] </ref>, exponential back-off, enhanced round-trip (RTT) estimation based on both the mean and the variance of the measured RTT, and the fast retransmit and fast recovery mechanisms. <p> We used the OPNET modeling tool for all the simulations. The TCP model is based on the Reno version. It supports the congestion control mechanism described by Jacob-son <ref> [42] </ref>, exponential back-off, enhanced round-trip time (RTT) estimation based on both the mean and the variance of the measured RTT, and the fast retransmit and fast recovery mechanisms. <p> Even though these rapid increases in the buffer occupancy do not cause actual packet drops, detailed analysis of the simulation results has revealed that packet retransmissions are triggered for two connections: TCP treats significant increase to RTT as and indication of packet loss <ref> [42] </ref> 1 . The effect of these retransmissions on the throughput of the corresponding connections can be seen in Figure 6.36: at times t = 10 and t = 30 secs.
Reference: [43] <author> V. Jacobson, </author> <title> Compressing TCP/IP headers for low-speed serial links, Request for Comments: </title> <type> 1144, </type> <month> February </month> <year> 1990. </year>
Reference-contexts: With link-layer data compression, the effective bandwidth of the upstream channel was found to be approximately 50 Kbits/sec. With IP header compression <ref> [43] </ref> enabled, we observed an ack size of 9 bytes on the serial link. We used ttcp to measure throughput of TCP connections between the workstations. Through separate measurements, we ensured that the workstations were not a bottleneck. <p> The size of acknowledgments was set as 28 bytes after IP header compression <ref> [43] </ref>, and including link layer overhead. The backpressure threshold is set to one data packet. For the first experiment, the size of data packets for both connections was set to 1500 bytes. Thus, n j = 43 packets, fi = 54, and ff = 50.
Reference: [44] <author> V. Jacobson, </author> <title> Modified TCP congestion avoidance algorithm, message to end2end-interest mailing list, </title> <month> April </month> <year> 1990. </year>
Reference-contexts: Recovering from a packet loss without having to wait for a timer to expire is known as fast retransmit. Also, in TCP-Reno, recovering from a packet loss indicated from three duplicate acks by only halving the congestion window is known as fast recovery <ref> [44, 91] </ref>. 12 The justification behind the two phases in the congestion window increase process is that during the slow start phase a connection can grow its window rapidly thus claiming very quickly any available bandwidth.
Reference: [45] <author> V. Jacobson, R. Braden, and D. </author> <title> Borman, TCP extensions for high performance, Request for Comments: </title> <type> 1323, </type> <month> May </month> <year> 1992. </year>
Reference-contexts: Because of its popularity, significant effort has gone into devising new techniques or enhancing existing ones in order to improve TCP performance, responsiveness, and tolerance to congestion, especially in high-speed networks. Jacobson et. al. <ref> [45] </ref> introduced a window scaling option that allows the TCP window to grow beyond the 64 KBytes limit specified in [8]. Such a modification will improve TCP performance in networks with large bandwidth-delay product. <p> Such a modification will improve TCP performance in networks with large bandwidth-delay product. To adapt faster to the network state, it is necessary for TCP to maintain a very accurate RTT estimation. Jacobson et. al. <ref> [45] </ref> proposed to perform an RTT measurement for each data packet transmitted rather than only once for every window worth of data.
Reference: [46] <author> R. Jain, </author> <title> Congestion control and traffic management in ATM networks: Recent advances and a survey, </title> <note> submitted to Computer Networks and ISDN Systems. </note>
Reference-contexts: On reaching its destination, the RM cell is returned to the source, which now sets its rate based on that allocated on the bottleneck link in the path of the connection. Several rate allocation algorithms that operate in the explicit-rate marking mode have been proposed <ref> [13, 46, 55] </ref> (for a survey, see [3]). These approaches differ in terms of their execution time, implementation complexity, level of fairness in the allocation of the available bandwidth, responsiveness to network changes, convergence time, and stability properties. <p> Such algorithms for rate allocation in packet-switched networks have been described by Charny [13], Jain <ref> [46] </ref>, Kalampoukas, et al. [55], and Siu, et al. [90]. In this section we describe the operation of the proposed rate allocation algorithm and discuss its properties. The following definitions and notations are used in the description of our algorithm: Consider any switch in the path of a connection.
Reference: [47] <author> R. Jain, </author> <title> Congestion control in computer networks: Issues and trends, </title> <journal> IEEE Network Magazine, </journal> <volume> vol. 4, no. 3, 2430, </volume> <month> May </month> <year> 1990. </year>
Reference-contexts: Congestion is the result of the statistical multiplexing principle [6, 58] upon which the design of packet-switched networks is based, with the objective of maximizing resource utilization. Congestion is a problem that cannot by solved by current technological advances alone. As was pointed out in <ref> [47, 48] </ref>, neither higher buffer capacity, nor faster processors and links offer a complete solution to the problem. On the contrary, these improvements may aggravate the problem.
Reference: [48] <author> R. Jain, </author> <title> Myths about congestion management in high-speed networks, </title> <journal> Internet-working: Research and Experience, </journal> <volume> vol. 3, no. 3, 101113, </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: Congestion is the result of the statistical multiplexing principle [6, 58] upon which the design of packet-switched networks is based, with the objective of maximizing resource utilization. Congestion is a problem that cannot by solved by current technological advances alone. As was pointed out in <ref> [47, 48] </ref>, neither higher buffer capacity, nor faster processors and links offer a complete solution to the problem. On the contrary, these improvements may aggravate the problem. <p> Data link mechanisms usually operate on a link basis and are referred to as hop-by-hop or link-by-link schemes. Longer-term congestion should be handled by the transport layer protocols. Congestion control mechanisms in different layers serve different objectives. As was pointed out in <ref> [48] </ref>, the longer the congestion persists, the higher the layer where control should be exercised. Even though we used the term congestion to describe long-term over-subscription of transmission links, it is applicable to other network resources as well. <p> The advantage of rate-based control is that it can potentially reduce the amount of buffering needed in the network switches if the transmission rate at the source matches that available in the network. A comparison of window-based versus rate-based control can be found in <ref> [48] </ref>. For window-based or rate-based control to be effective the size of the window or the transmission rate must adapt to the load of the network. <p> When TCP traffic is carried over an ATM network, the window-based congestion control mechanisms of TCP can interact with the rate-based control mechanisms in the ATM network in undesirable ways. These interactions are a result of the mismatch in the dynamics introduced by rate-based and window-based control <ref> [48] </ref>. TCP controls the total amount of data injected into the network, but does not control its burstiness. However, data traffic entering the ATM network is shaped based on the available bandwidth.
Reference: [49] <author> R. Jain, S. Kalyanaraman, S. Fahmy, and R. Goyal, </author> <title> Source behavior for the ATM ABR traffic management: An explanation, </title> <journal> IEEE Communications Magazine, </journal> <volume> vol. 34, no. 11, 5057, </volume> <month> November </month> <year> 1996. </year>
Reference-contexts: The evolution of the congestion control framework and the trade-offs made during the process of its specification are discussed in [25, 80, 86]. A detailed discussion of the source system behavior for the rate-based framework can be found in <ref> [49, 68] </ref>. 1.3.2 Hop-by-Hop Rate-Based Approaches Rate-based control can be applied on a link-by-link (also known as hop-by-hop) basis as well. Mishra et. al. [74] have developed such a scheme. The equations used for the predictive control in [74] are similar to those in packet-pair.
Reference: [50] <author> R. Jain, S. Kalyanaraman, R. Goyal, S. Fahmy, and R. Viswanathan, </author> <title> The ERICA switch algorithm for ABR traffic management in ATM networks, Part I: Description,, </title> <note> available from http://www.cis.ohio-state.edu/ jain/papers/erica-to.htm. </note>
Reference-contexts: The most recently computed feedback will become effective one round-trip time later and at that time the switch will correct, if necessary, its estimate. An example of such an approach is the ERICA algorithm <ref> [50, 51] </ref>. ERICA maintains an estimate of the link load z which is periodically computed as the ratio of arrival rate to the available rate for ABR traffic. Every active connection is allocated at least a fair share of the available bandwidth. <p> As a result, 22 RTT seconds later the link utilization will get close to one. Several enhancements and extensions to the basic ERICA algorithm are also presented in <ref> [50, 51] </ref>. Many examples of state-less explicit rate allocation algorithms can be found in [1, 34, 72, 79, 90]. The schemes proposed in [61, 103] use control theoretic approaches with emphasis on system stability issues.
Reference: [51] <author> R. Jain, S. Kalyanaraman, R. Goyal, S. Fahmy, and R. Viswanathan, </author> <title> The ER-ICA switch algorithm for ABR traffic management in ATM networks, Part II: Requirements and performance evaluation, </title> <note> available from http://www.cis.ohio-state.edu/ jain/papers/erica-t2.htm. 244 </note>
Reference-contexts: The most recently computed feedback will become effective one round-trip time later and at that time the switch will correct, if necessary, its estimate. An example of such an approach is the ERICA algorithm <ref> [50, 51] </ref>. ERICA maintains an estimate of the link load z which is periodically computed as the ratio of arrival rate to the available rate for ABR traffic. Every active connection is allocated at least a fair share of the available bandwidth. <p> As a result, 22 RTT seconds later the link utilization will get close to one. Several enhancements and extensions to the basic ERICA algorithm are also presented in <ref> [50, 51] </ref>. Many examples of state-less explicit rate allocation algorithms can be found in [1, 34, 72, 79, 90]. The schemes proposed in [61, 103] use control theoretic approaches with emphasis on system stability issues.
Reference: [52] <author> S. Jamin, P. B. Danzig, S. J. Shenker, and L. Zhang, </author> <title> A measurement-based admission control algorithm for integrated service packet networks, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> vol. 5, no. 1, 5670, </volume> <month> Ffebruary </month> <year> 1997. </year>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing <ref> [18, 26, 36, 52, 76, 101] </ref>, and scheduling mechanisms [2, 23, 35, 37, 82, 93, 99]. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [53] <author> L. Kalampoukas and A. Varma, </author> <title> Performance of TCP over multi-hop ATM networks: A comparative study of ATM layer congestion control schemes, </title> <booktitle> in Proc. of ICC'95, </booktitle> <pages> pp. 14721477, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Several studies report on the efficiency of the mechanisms employed by TCP and its performance [9, 27, 30, 40, 98, 100]. An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in <ref> [53, 87] </ref>. Finally, performance issues of TCP over ATM for a specific TCP implementation are discussed in [21] and [78]. The assumption that a packet loss is a reliable indication of network congestion may cause performance degradation in environments with high bit error rates, such as wireless networks. <p> To reserve bandwidth, these credits may be aggregated and a single credit update message may be sent. Fairness can be achieved by serving individual connection in a round-robin fashion [37]. It is demonstrated in <ref> [53, 65] </ref> that credit-based control with strict buffer partitioning and round-robin scheduling provides loss-free and fair operation while maximizing the network performance. The application of credit-based control with strict buffer partitioning may be limited by its memory requirements in wide-area networks (WAN). <p> The dynamics and the performance of TCP over ATM networks using PPD or EPD policies are studied in <ref> [53, 87] </ref>. <p> Furthermore, we study the behavior of the rate allocation algorithm under TCP-controlled ABR traffic. Several studies have shown that in certain configurations, in the absence of an ATM-layer congestion control mechanism, TCP performance degrades when operating over ATM networks <ref> [33, 53] </ref>. In this work we study the interaction of TCP built-in mechanisms for protection against congestion with our proposed explicit rate allocation scheme in conjunction with the basic rate increase/decrease algorithm of ATM Forum's source policy. <p> While previous studies have focused on the effect of fragmentation of TCP segments at the ATM layer [33], and the fairness behavior with multiple TCP connections <ref> [53] </ref>, our interest in this chapter is in investigating how the ATM source policy in the rate-based congestion control algorithm affects the congestion control and avoidance mechanisms currently employed in TCP. <p> Although our analysis is focussed on TCP over ABR, the results apply equally well to TCP operating over other networks providing a steady rate, predictable delay, and in-order delivery. Several earlier studies have been reported on the behavior of TCP in ATM networks <ref> [21, 33, 53] </ref>, but none of them consider the effects of two-way traffic on TCP behavior.
Reference: [54] <author> L. Kalampoukas and A. Varma, </author> <title> Analysis of source policy in rate-controlled ATM networks, </title> <booktitle> in Proc. of ICC'96. IEEE, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: Note that the rate 142 increase process is not smooth but consists of a sequence of steps. This is because of the gaps in the data flow during the TCP slow-start process, and has been analyzed in <ref> [54] </ref>. The progress for the left-to-right TCP connection is shown in Figure 4.15, in terms of the increase in TCP sequence numbers transmitted by the connection.
Reference: [55] <author> L. Kalampoukas, A. Varma, and K. K. Ramakrishnan, </author> <title> An efficient rate allocation algorithm for ATM networks providing max-min fairness, </title> <booktitle> in Proc. of 6th IFIP International Conference on High Performance Networking, HPN'95, </booktitle> <pages> pp. 143154, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Thus, it does not scale well with increasing number of connections. Scaling is a critical issue in ATM networks. The convergence and dynamic properties of this scheme as well as possible enhancements are discussed in [15]. Kalampoukas et. al. <ref> [55, 56] </ref> proposed an explicit rate allocation algorithm that has similar convergence and performance properties with Charny's scheme but with significantly better scaling properties: on the arrival of an RM cell only the state of the connection it belongs to needs to be extracted and possibly updated. <p> On reaching its destination, the RM cell is returned to the source, which now sets its rate based on that allocated on the bottleneck link in the path of the connection. Several rate allocation algorithms that operate in the explicit-rate marking mode have been proposed <ref> [13, 46, 55] </ref> (for a survey, see [3]). These approaches differ in terms of their execution time, implementation complexity, level of fairness in the allocation of the available bandwidth, responsiveness to network changes, convergence time, and stability properties. <p> Such algorithms for rate allocation in packet-switched networks have been described by Charny [13], Jain [46], Kalampoukas, et al. <ref> [55] </ref>, and Siu, et al. [90]. In this section we describe the operation of the proposed rate allocation algorithm and discuss its properties. The following definitions and notations are used in the description of our algorithm: Consider any switch in the path of a connection. <p> and CCR fields is taken as the bandwidth request for connection 1, then switch 1 will allocate 66.67 Mbits/second to connection 2, thus preventing the link from being under-utilized. 2.1.1 Providing Minimum Rate Guarantees The algorithm described in the preceding section has been modified from its original version presented in <ref> [55, 56] </ref> in order to support minimum rate requirements (MCR) for ABR connections. The way minimum rate is guaranteed depends on the definition of fairness adopted. The ATM Forum has specified several definitions for fairness criteria [89]. <p> The analysis assumes the source rate increase algorithm of equation 3.1.1. In addition, we also assume that the rate allocation algorithm in the switches provides an allocation at least equal to the current transmission rate throughout the increase period. The rate allocation algorithms described in [13] and <ref> [55] </ref> can meet this requirement. 3.2.1 Approximation for Zero Round-Trip Delay Let D be the round-trip delay along the path of the connection. We assume D to be constant in this analysis. <p> The assumption of uncongested network is reasonable in the context of ATM networks that support explicit rate allocation. Under this assumption, since each connection is guaranteed a minimum rate, the performance of a TCP connection may be analyzed independent of the other connections. The rate allocation algorithms of <ref> [13, 55] </ref> meet this requirement. Assuming that the idle-source detection option is not used, the rate of the ATM source will monotonically increase during the TCP slow-start phase as the window-size is increased. However, the evolution of the source can be different from that seen in Section 3.
Reference: [56] <author> L. Kalampoukas, A. Varma, and K. K. Ramakrishnan, </author> <title> Dynamics of an explicit rate allocation algorithm for ATM networks, </title> <booktitle> in Proc. of International Broadband Communications Conference'96. </booktitle> <address> IFIP-IEEE, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Thus, it does not scale well with increasing number of connections. Scaling is a critical issue in ATM networks. The convergence and dynamic properties of this scheme as well as possible enhancements are discussed in [15]. Kalampoukas et. al. <ref> [55, 56] </ref> proposed an explicit rate allocation algorithm that has similar convergence and performance properties with Charny's scheme but with significantly better scaling properties: on the arrival of an RM cell only the state of the connection it belongs to needs to be extracted and possibly updated. <p> The pseudocode in Figure 2.1 summarizes the algorithm that is invoked on the receipt of each RM cell traveling in the forward direction. The pseudocode given in Figure 2.1 is based on that presented in <ref> [56] </ref> but extended in order to support minimum rate guarantees. The issue of providing minimum rate guarantees is discussed in more details in the following section. <p> and CCR fields is taken as the bandwidth request for connection 1, then switch 1 will allocate 66.67 Mbits/second to connection 2, thus preventing the link from being under-utilized. 2.1.1 Providing Minimum Rate Guarantees The algorithm described in the preceding section has been modified from its original version presented in <ref> [55, 56] </ref> in order to support minimum rate requirements (MCR) for ABR connections. The way minimum rate is guaranteed depends on the definition of fairness adopted. The ATM Forum has specified several definitions for fairness criteria [89]. <p> It has been shown that, by the use of a rate-allocation algorithm that maintains state, unfairness problems that occur in a datagram network due to the differences in round-trip delay of TCP connections can be avoided in an ATM network <ref> [56] </ref>. Before proceeding further, it is important to mention how ack compression originates at the end systems in the two-way environment and how it causes throughput degradation. <p> Furthermore, in the analysis of Section 4.2, we assume that the network provides a constant-delay path between the end nodes. Such an assumption is realistic in the ATM ABR environment when the switches employ explicit rate allocation, since the queue sizes in the switches can be maintained small <ref> [56] </ref>. The queueing delays in network switches are taken into account in the simulation results of Section 4.4. The data segments transmitted by the TCP connection in one direction share a common outgoing ATM virtual channel with the acks transmitted by the connection in the other direction. <p> While the rate allocation algorithm converges to a steady state with a small queue (typically under 150 cells) in the unidirectional case <ref> [56] </ref>, there is virtually no queueing in the switches with bidirectional TCP traffic. This is because of the idle periods on the link caused by TCP, allowing the switches to clear their queues. <p> We also assume use of the rate allocation algorithm described in Chapter 2 in the ATM network for support of ABR service, providing the three VCs with a fair and loss-free environment <ref> [56] </ref>. We consider this as a best case scenario. Each of the links in this configuration has a capacity of 155 Mbits/sec. The one-way propagation delays of connections 1 and 2b in IP Network 1 are set to 3 milliseconds and that of connection 3b to 0.8 sec.
Reference: [57] <author> S. Keshav, </author> <title> A control-theoretic approach to flow control, </title> <booktitle> in Proc. of SIGCOMM'91, </booktitle> <pages> pp. 315, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: We review in this section some representative approaches that belong in this class. 1.3.1 End-to-End Rate-Based Approaches 1.3.1.1 Packet-Pair In the packet-pair scheme, proposed by Keshav <ref> [57] </ref>, the source adjusts its transmission rate by estimating the service rate at the bottleneck link. This scheme operates on an end-to-end basis and does not require any explicit feedback from the network. However, it requires each network element to schedule traffic from different flows using a round-robin-like discipline. <p> Keshav <ref> [57] </ref> also discusses how to handle special cases, such as the start-up of a connection and cases where the source does not have two data packets to transmit.
Reference: [58] <author> S. Keshav, </author> <title> An engineering approach to computer networking: ATM networks, the Internet, and the telephone network, </title> <publisher> Addison-Wesley, </publisher> <address> 1st edition, </address> <year> 1997. </year>
Reference-contexts: 1. Introduction Congestion refers to the situation where network resources are persistently overloaded or equivalently, the total demand of a resource exceeds its capacity. Congestion is the result of the statistical multiplexing principle <ref> [6, 58] </ref> upon which the design of packet-switched networks is based, with the objective of maximizing resource utilization. Congestion is a problem that cannot by solved by current technological advances alone. <p> Connections 3 whose requests are not fully satisfied are allocated the same amount under a max-min fair allocation. A max-min allocation maximizes the utilization of the bottlenecked resource (usually link or buffer). A formal definition of max-min fairness can be found in <ref> [6, 58] </ref>.
Reference: [59] <author> S. Keshav and S. P. Morgan, </author> <title> SMART: Performance with overload and random losses, </title> <booktitle> in Proc. of INFOCOM'97, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: Keshav [57] also discusses how to handle special cases, such as the start-up of a connection and cases where the source does not have two data packets to transmit. Keshav et. al. <ref> [59] </ref> propose extensions to the packet-pair scheme to handle random packet losses better. 1.3.1.2 Rate-Based Congestion Control Framework for ABR Service in ATM Networks More recently, the ATM Forum specified a rate-based framework for the support of the ABR service in ATM networks [7].
Reference: [60] <author> L. Kleinrock, </author> <title> Power and deterministic rules of thumb for probabilistic problems in computer communications, </title> <booktitle> in Proc. of the International Symposium on Flow Control in Computer Networks, </booktitle> <pages> pp. </pages> <address> 43.1.143.1.10, </address> <month> June </month> <year> 1979. </year>
Reference-contexts: We will refer to these four desirable properties of a congestion control mechanism as efficiency, fairness, responsiveness, and stability, respectively. Resource efficiency can be expressed in terms of power <ref> [60] </ref>. The power of a resource is defined as the ratio of its throughput to its response time. Maximizing the power is the objective in data networks.
Reference: [61] <author> A. Kolarov and G. Ramamurthy, </author> <title> A control theoretic approach to the design of close loop rate based flow control for high speed ATM networks, </title> <booktitle> in Proc. of IEEE INFOCOM'97, </booktitle> <pages> pp. 293300, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Several enhancements and extensions to the basic ERICA algorithm are also presented in [50, 51]. Many examples of state-less explicit rate allocation algorithms can be found in [1, 34, 72, 79, 90]. The schemes proposed in <ref> [61, 103] </ref> use control theoretic approaches with emphasis on system stability issues. A comparison of explicit rate allocation schemes can be found in [3]. 1.5 Packet Discard Approaches During congestion the switches may have to drop packets.
Reference: [62] <author> H. T. Kung, T. Blackwell, and A. Chapman, </author> <title> Credit-Based flow control for ATM networks: Credit update protocol, adaptive credit allocation, and statistical multiplexing, </title> <booktitle> in Proc. of ACM SIGCOMM'94, </booktitle> <pages> pp. 101114, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: The application of credit-based control with strict buffer partitioning may be limited by its memory requirements in wide-area networks (WAN). Methods that reduce the amount of buffering required for credit-based control are presented in <ref> [62, 63, 81] </ref>. Kung et. al. [62] investigate the effectiveness of a statistical multiplexing approach where the available buffer can be shared by competing flows, and they show that the performance of the system is still acceptable but it can no longer guarantee loss-free operation. <p> The application of credit-based control with strict buffer partitioning may be limited by its memory requirements in wide-area networks (WAN). Methods that reduce the amount of buffering required for credit-based control are presented in [62, 63, 81]. Kung et. al. <ref> [62] </ref> investigate the effectiveness of a statistical multiplexing approach where the available buffer can be shared by competing flows, and they show that the performance of the system is still acceptable but it can no longer guarantee loss-free operation. <p> This logarithmic feedback function allows us to allocate the free buffer among the TCP connections, incrementally, on encountering an ack from each connection. This does not require maintaining any connection-level state. Although this scheme has similarities with buffer allocation in credit-based flow control <ref> [62, 81] </ref>, our goal is not to achieve loss-free operation, but to reduce losses dramatically, while achieving fairness. Gross over-allocation is avoided because of the incremental allocation of the buffer upon each ack received, so that the feedback returned is a function of the current free buffer.
Reference: [63] <author> H. T. Kung and K. Chang, </author> <title> Receiver-oriented adaptive buffer allocation in credit-ased flow control for ATM networks, </title> <booktitle> in Proc. of INFOCOM'95, </booktitle> <pages> pp. 239252. </pages> <publisher> IEEE, </publisher> <month> April </month> <year> 1995. </year>
Reference-contexts: The application of credit-based control with strict buffer partitioning may be limited by its memory requirements in wide-area networks (WAN). Methods that reduce the amount of buffering required for credit-based control are presented in <ref> [62, 63, 81] </ref>. Kung et. al. [62] investigate the effectiveness of a statistical multiplexing approach where the available buffer can be shared by competing flows, and they show that the performance of the system is still acceptable but it can no longer guarantee loss-free operation. <p> Kung et. al. [62] investigate the effectiveness of a statistical multiplexing approach where the available buffer can be shared by competing flows, and they show that the performance of the system is still acceptable but it can no longer guarantee loss-free operation. In <ref> [63] </ref>, Kung et. al. propose to allocate the buffer available in the downstream node proportional to the bandwidth demand of individual connections.
Reference: [64] <author> H. T. Kung and A. Chapman, </author> <title> The FCVC (Flow-Controlled Virtual Channels) proposal for ATM networks, </title> <booktitle> in Proc. of the 1993 International Conference on Network Protocols, </booktitle> <pages> pp. 116127, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Several techniques has been developed with the objective of improving TCP performance in such environments. A discussion of such techniques can be found in [4, 19]. 1.2.2 Link-by-Link Window-Based Approaches Most of the schemes in this category have been proposed in the context of ATM networks <ref> [41, 64] </ref>. Window-based control that is applied on a connection and link-by-link basis is also referred to as credit-based control. With credit-based control and with reference to a single link, the upstream and the downstream nodes of a connection exchange explicit information regarding buffer availability in the downstream node.
Reference: [65] <author> H. T. Kung, R. Morris, T. Charuhas, and D. Lin, </author> <title> Use of link-by-link flow control in maximizing ATM network performance: Simulation results, </title> <booktitle> in Proc. of IEEE Hot Interconnects Symposium, </booktitle> <year> 1993. </year>
Reference-contexts: To reserve bandwidth, these credits may be aggregated and a single credit update message may be sent. Fairness can be achieved by serving individual connection in a round-robin fashion [37]. It is demonstrated in <ref> [53, 65] </ref> that credit-based control with strict buffer partitioning and round-robin scheduling provides loss-free and fair operation while maximizing the network performance. The application of credit-based control with strict buffer partitioning may be limited by its memory requirements in wide-area networks (WAN).
Reference: [66] <author> T. V. Lakshman and U. Madhow, </author> <title> Performance analysis of window-base flow control using TCP/IP: The effect of high bandwidth-delay products and random loss, </title> <booktitle> in Proc. of High Performance Networking, V. IFIP TC6/WG6.4 Fifth International Conference, </booktitle> <volume> vol. C, </volume> <pages> pp. 135149, </pages> <month> June </month> <year> 1994. </year> <month> 245 </month>
Reference-contexts: A detailed description of TCP, the algorithms it employs, and its implementation can be found in [91, 92]. Several studies report on the efficiency of the mechanisms employed by TCP and its performance [9, 27, 30, 40, 98, 100]. An analytical approach to TCP performance can be found in <ref> [66] </ref>. Studies on the performance of TCP over ATM networks can be found in [53, 87]. Finally, performance issues of TCP over ATM for a specific TCP implementation are discussed in [21] and [78]. <p> This behavior tends to be periodic, and may result in loss of throughout and overall performance degradation. The effect of such periodic losses on TCP performance was analyzed by Lakshman, et. al. <ref> [66] </ref>. Although the above problem could occur in more general environments, we focus our attention in this chapter on TCP/IP internetworks where the end systems are connected to legacy LANs (such as Ethernet or Token Ring), with the LANs interconnected through a rate-controlled ATM virtual circuit. <p> The fairness and efficiency of packet discard schemes such as Drop-Tail, Drop-from-Front, and RED degrade dramatically when the amount of buffer at the AAP is less than the round-trip bandwidth-delay product. Lakshman and Madhow <ref> [66] </ref> showed that the amount of buffer in Drop-Tail switches should be at least two to three times the bandwidth-delay product of the network in order for TCP to achieve decent performance and to avoid losses when operating in the slow-start phase.
Reference: [67] <author> T. V. Lakshman, A. Neidhardt, and T. J. Ott, </author> <title> The drop from front strategy in TCP and in TCP over ATM, </title> <booktitle> in Proc. of IEEE INFOCOM'96, </booktitle> <volume> vol. 3, </volume> <pages> pp. 124250, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: These connections will occupy a large portion of the buffer space, thus increasing the loss rate for well-behaved connections. Instead of dropping from the tail, Lakshman et. al. <ref> [67] </ref> propose to drop the packet at the head of the queue when the buffer becomes full. With Drop-from-Front, TCP will detect network congestion approximately a buffer draining time earlier, since it does not have to wait for the loss to be propagated through a full buffer. <p> In addition, the Drop-from-Front scheme improves both throughput and fairness by avoiding bursty losses and by breaking synchronization between connections <ref> [67] </ref>. Mankin [70] proposed the Random-Drop strategy. In this scheme, when the buffer overflows, a packet already in the queue is randomly selected and dropped. The objective is to match the loss-rate of a connection to its throughput share. <p> Another discard policy that has been shown to work well with TCP is Drop-from-Front, which drops the packets at the head of the FIFO queue in the router when the buffer becomes full <ref> [67] </ref>. This approach, however, may still cause back-to-back losses and unfairness. Both RED and Drop-from-Front are generic solutions with the potential of improving the efficiency and fairness of networks with TCP-controlled traffic, without being tied to a specific network 192 technology. <p> much faster, thus obtaining a larger portion of the available bandwidth. 198 6.1.1 TCP Performance with Drop-from-Front AAPs To reduce the tendency of Drop-Tail routers to synchronize losses of multiple connections, and the possibility of multiple losses to a connection in a round-trip, two alternative policies have been suggested: Drop-from-Front <ref> [67] </ref> and Random Early Detection [32]. With Drop-from-Front, when a packet arrives at a full queue, the packet stored at the head of the queue is dropped. If service for the packet at the head of the queue has already started, the following packet in the queue is dropped instead. <p> Lakshman and Madhow [66] showed that the amount of buffer in Drop-Tail switches should be at least two to three times the bandwidth-delay product of the network in order for TCP to achieve decent performance and to avoid losses when operating in the slow-start phase. Also, Lakshman, et. al. <ref> [67] </ref> demonstrated that the buffering needed by the Drop-from-Front scheme must be of the order of the bandwidth-delay product for the network to achieve good performance. For this experiment we use the same network topology and traffic scenario from the previous experiment.
Reference: [68] <author> D. Lee, K. K. Ramakrishnan, W. M. Moh, and A. U. Shankar, </author> <title> Performance and correcteness of the ATM ABR rate control ccheme, </title> <booktitle> in Proc. of IEEE INFOCOM'97, </booktitle> <pages> pp. 786793, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: The evolution of the congestion control framework and the trade-offs made during the process of its specification are discussed in [25, 80, 86]. A detailed discussion of the source system behavior for the rate-based framework can be found in <ref> [49, 68] </ref>. 1.3.2 Hop-by-Hop Rate-Based Approaches Rate-based control can be applied on a link-by-link (also known as hop-by-hop) basis as well. Mishra et. al. [74] have developed such a scheme. The equations used for the predictive control in [74] are similar to those in packet-pair.
Reference: [69] <author> Z. Liu, E. Yan, P. Danzig, and J. Ahn, </author> <title> An evaluation of TCP Vegas: Emulation and experiment, </title> <booktitle> in Proc. of SIGCOMM'95, </booktitle> <pages> pp. 185195, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Depending on the relative difference between the expected and the measured throughputs the source may have to reduce the size of the window. Both simulation and experimental studies report on the dynamics and the performance of TCP-Vegas <ref> [10, 69] </ref>. Floyd proposed extensions to TCP so that it will become responsive to explicit congestion notification (ECN) messages [28].
Reference: [70] <author> A. Mankin, </author> <title> Random drop congestion control, </title> <booktitle> in Proc. of ACM SIGCOMM'90, </booktitle> <pages> pp. 17, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: In addition, the Drop-from-Front scheme improves both throughput and fairness by avoiding bursty losses and by breaking synchronization between connections [67]. Mankin <ref> [70] </ref> proposed the Random-Drop strategy. In this scheme, when the buffer overflows, a packet already in the queue is randomly selected and dropped. The objective is to match the loss-rate of a connection to its throughput share. <p> Thus, Random-Drop discards packets from a connection in proportion to its buffer share. However, when traffic is controlled by a high-level protocol, such as TCP, the buffer share of the connection (especially during congestion), does not necessarily reflect the long-term bandwidth share. Mankin <ref> [70] </ref> points out that the Random-Drop policy is ineffective when used for congestion recovery: connections with lower demands but with longer paths incur more packet loss with Random-Drop than without it.
Reference: [71] <author> A. Mankin and K. K. Ramakrishnan, </author> <title> Gateway congestion control survey, Request for Comments: </title> <type> 1254, </type> <month> August </month> <year> 1991. </year>
Reference: [72] <author> S. Mascolo, D. Cavendish, and M. Gerla, </author> <title> ATM rate based congestion control using a Smith predictor: An EPRCA implementation, </title> <booktitle> in Proc. of IEEE INFOCOM'96, </booktitle> <volume> vol. 2, </volume> <pages> pp. 56976, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: As a result, 22 RTT seconds later the link utilization will get close to one. Several enhancements and extensions to the basic ERICA algorithm are also presented in [50, 51]. Many examples of state-less explicit rate allocation algorithms can be found in <ref> [1, 34, 72, 79, 90] </ref>. The schemes proposed in [61, 103] use control theoretic approaches with emphasis on system stability issues. A comparison of explicit rate allocation schemes can be found in [3]. 1.5 Packet Discard Approaches During congestion the switches may have to drop packets.
Reference: [73] <author> K. Maxwell, </author> <title> Asymmetric digital subscribe line: Interim technology for the next forty years, </title> <journal> IEEE Communications Magazine, </journal> <volume> vol. 34, no. 10, 100106, </volume> <month> October </month> <year> 1996. </year>
Reference-contexts: This chapter extends the analysis presented in Chapter 4 by considering the effect of asymmetric access links on the performance of two-way TCP traffic. Asymmetric link speeds are likely to be common in the future, with the widespread deployment of cable and high-speed DSL (Digital Subscriber Line) <ref> [73] </ref> access networks to homes and businesses. These access networks have substantially higher speed in one direction than the other. We demonstrate that the asymmetry in link bandwidth has a dramatic effect on the performance of the connection going through the faster down-link.
Reference: [74] <author> P. P. Mishra and H. Kanakia, </author> <title> A hop by hop rate-based congestion control scheme, </title> <booktitle> in Proc. of ACM SIGCOMM'92, </booktitle> <pages> pp. 112123, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: A detailed discussion of the source system behavior for the rate-based framework can be found in [49, 68]. 1.3.2 Hop-by-Hop Rate-Based Approaches Rate-based control can be applied on a link-by-link (also known as hop-by-hop) basis as well. Mishra et. al. <ref> [74] </ref> have developed such a scheme. The equations used for the predictive control in [74] are similar to those in packet-pair. The difference, however, is 20 that the downstream node sends explicit information to the upstream one regarding per-connection service rate and buffer occupancy. <p> Mishra et. al. <ref> [74] </ref> have developed such a scheme. The equations used for the predictive control in [74] are similar to those in packet-pair. The difference, however, is 20 that the downstream node sends explicit information to the upstream one regarding per-connection service rate and buffer occupancy. <p> If desired, the queue sizes can be maintained close to a set target by an adaptive scheme that varies the ER values signaled to the sources based on the target queue size and the control delays, using an approach similar to that proposed by Mishra and Kanakia <ref> [74] </ref>. The utilization of the bottleneck link achieves its maximum value somewhat quicker when the number of connections is increased. As shown in Figure 2.13, the utilization of the bottleneck link achieves its maximum in about 15 msecs, compared to 25 msecs 50 for the same configuration with three connections.
Reference: [75] <author> D. Mitra, </author> <title> Asymptotically optimal design of congestion control for high speed data networks, </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. 40, no. 2, 301311, </volume> <month> February </month> <year> 1992. </year>
Reference-contexts: Any further increase of the window size contributes only to increased queueing delay, not improved throughput. Optimal setting of the window size, however, requires knowledge of the RTT and the bandwidth-delay product of the network <ref> [75] </ref>. Such information is usually not available at network elements. Instead, EWA determines when the network pipe is full by monitoring the occupancy of the buffer serving the outgoing ATM virtual circuit at the AAP: non-empty buffer is either an indication of a full pipe or bursty traffic.
Reference: [76] <author> D. J. Mitzel, D. Estrin, S. Shenker, and L. Zhang, </author> <title> A study of reservation dynamics in integrated services packet networks, </title> <booktitle> in Proc. of IEEE INFOCOM'96, </booktitle> <volume> vol. 2, </volume> <pages> pp. 871879, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing <ref> [18, 26, 36, 52, 76, 101] </ref>, and scheduling mechanisms [2, 23, 35, 37, 82, 93, 99]. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [77] <author> J. C. Mogul, </author> <title> Observing TCP dynamics in real networks, </title> <booktitle> in Proc. of ACM SIG-COMM'92, </booktitle> <pages> pp. 305317, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Alternatively, queueing outgoing acks at a higher priority eliminates ack compression, but requires support for message priorities in the protocol stack. This idea of processing acks at a higher priority was suggested by Mogul <ref> [77] </ref> in the 158 context of IP routers. In Chapter 5 we investigate these and other solutions to improve the performance of two-way TCP over ATM. 159 5.
Reference: [78] <author> K. Moldeklev and P. Gunningberg, </author> <title> How a large ATM MTU causes deadlocks in TCP data transfers, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> vol. 3, no. 4, 409422, </volume> <month> August </month> <year> 1995. </year>
Reference-contexts: An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in [53, 87]. Finally, performance issues of TCP over ATM for a specific TCP implementation are discussed in [21] and <ref> [78] </ref>. The assumption that a packet loss is a reliable indication of network congestion may cause performance degradation in environments with high bit error rates, such as wireless networks. Several techniques has been developed with the objective of improving TCP performance in such environments.
Reference: [79] <author> S. Muddu, F. M. Chiussi, C. Tryfonas, and V. P. Kumar, </author> <title> Max-min rate control algorithm for available bit rate service in ATM networks, </title> <booktitle> in Proc. of ICC'96, </booktitle> <volume> vol. 1, </volume> <pages> pp. 412418, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: As a result, 22 RTT seconds later the link utilization will get close to one. Several enhancements and extensions to the basic ERICA algorithm are also presented in [50, 51]. Many examples of state-less explicit rate allocation algorithms can be found in <ref> [1, 34, 72, 79, 90] </ref>. The schemes proposed in [61, 103] use control theoretic approaches with emphasis on system stability issues. A comparison of explicit rate allocation schemes can be found in [3]. 1.5 Packet Discard Approaches During congestion the switches may have to drop packets.
Reference: [80] <author> P. Newman, </author> <title> Traffic management for ATM local area networks, </title> <journal> IEEE Communications Magazine, </journal> <volume> vol. 32, no. 8, 4450, </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: The evolution of the congestion control framework and the trade-offs made during the process of its specification are discussed in <ref> [25, 80, 86] </ref>. A detailed discussion of the source system behavior for the rate-based framework can be found in [49, 68]. 1.3.2 Hop-by-Hop Rate-Based Approaches Rate-based control can be applied on a link-by-link (also known as hop-by-hop) basis as well. Mishra et. al. [74] have developed such a scheme.
Reference: [81] <author> C. M. Ozveren, R. Simcoe, and G. Varghese, </author> <title> Reliable and efficient hop-by-hop flow control, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 13, no. 4, 642650, </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: The application of credit-based control with strict buffer partitioning may be limited by its memory requirements in wide-area networks (WAN). Methods that reduce the amount of buffering required for credit-based control are presented in <ref> [62, 63, 81] </ref>. Kung et. al. [62] investigate the effectiveness of a statistical multiplexing approach where the available buffer can be shared by competing flows, and they show that the performance of the system is still acceptable but it can no longer guarantee loss-free operation. <p> They show that with rather modest memory requirements (of the order of four times the bandwidth-delay product of the link) they can provide loss-free operation while the throughput of individual connection can ramp up to the fair share exponentially. Ozveren et. el. <ref> [81] </ref> propose another buffer sharing approach that has been incorporated in the DEC AN2 switch. The buffer is divided into two areas, called pools: a common pool that can be used by any packet and a private pool allocated to each connection. <p> This logarithmic feedback function allows us to allocate the free buffer among the TCP connections, incrementally, on encountering an ack from each connection. This does not require maintaining any connection-level state. Although this scheme has similarities with buffer allocation in credit-based flow control <ref> [62, 81] </ref>, our goal is not to achieve loss-free operation, but to reduce losses dramatically, while achieving fairness. Gross over-allocation is avoided because of the incremental allocation of the buffer upon each ack received, so that the feedback returned is a function of the current free buffer.
Reference: [82] <author> A. K. Parekh and R. G. Gallager, </author> <title> A generalized processor sharing approach to flow control in integrated services networks: The single-node case, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> vol. 1, no. 3, 344357, </volume> <month> June </month> <year> 1993. </year> <month> 246 </month>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing [18, 26, 36, 52, 76, 101], and scheduling mechanisms <ref> [2, 23, 35, 37, 82, 93, 99] </ref>. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [83] <author> K. K. Ramakrishnan, </author> <title> Performance considerations in designing network interfaces, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, 203219, </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: For example, the transmission time of a 9-Kbyte packet at a link rate of 10 Mbits/second is approximately 8 msecs, while the TCP protocol processing time in real systems typically does not exceed a few hundred microseconds <ref> [12, 83, 102] </ref>. The effect of non-zero TCP processing time on throughput is considered in the simulation results presented in Section 4.4. The functionality assumed for the IP layer is simple. <p> Then, the TCP processing time expressed in terms of segment transmission time will be L pr = t pr =t s . We can obtain an estimate of t pr by following the guidelines in <ref> [83] </ref>, where it was shown that the fixed part of the TCP, UDP and IP processing times scale well with the processor instruction execution times (or SPECmarks) of the underlying host. For a 25 SPECmark machine, the combined TCP/IP processing time for a packet is approximately 150 seconds [83]. <p> guidelines in <ref> [83] </ref>, where it was shown that the fixed part of the TCP, UDP and IP processing times scale well with the processor instruction execution times (or SPECmarks) of the underlying host. For a 25 SPECmark machine, the combined TCP/IP processing time for a packet is approximately 150 seconds [83]. Therefore, we anticipate that the TCP/IP processing overhead in a 150 SPECmark machine will be significantly lower, of the order of 50 seconds. This excludes copy and checksum overhead, which are beginning to become insignificant with hardware support in zero-copy interfaces [22]. <p> For example, the transmission time of a 1500 byte packet over a 100 Kbits/sec link is about 120 msecs, while the TCP protocol processing time in modern workstations does not exceed a few hundred microseconds <ref> [12, 83, 102] </ref>. Thus, we can safely assume that t pr is zero. 5.2 Demonstrating Effect of Ack Compression over Asymmetric Links We use both simulation and experimentation to illustrate that link asymmetry may cause severe performance degradation to TCP. TCP connections.
Reference: [84] <author> K. K. Ramakrishnan and R. Jain, </author> <title> A binary feedback scheme for congestion avoidance in computer networks, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 8, no. 2, 158181, </volume> <month> May </month> <year> 1990. </year>
Reference-contexts: we discuss link-by-link control approaches. 1.2.1 End-to-End Window-Based Approaches We first describe the DECbit scheme because it chronologically precedes TCP and because many of the mechanisms used by TCP in the window adjustment process are similar to those used by DECbit. 1.2.1.1 Congestion Control in DECbit In the DECbit scheme <ref> [84, 85] </ref> the sources adjust the window size based on explicit feedback received from the network. All data packets carry a bit, called congestion indication bit. Congested switches mark this bit to indicate their state to the end systems. <p> An example of a regeneration cycle is graphically illustrated in Figure 1.1. Notice that it is important to take into account the current busy period since it is possible for the last busy period to be outdated <ref> [84] </ref>. The DECbit scheme has been shown to achieve max-min fairness when all the connections share the same resources in the network. However, when different connections have unequal delays, it shows bias against connections with long delays.
Reference: [85] <author> K. K. Ramakrishnan, R. Jain, and D.-M. Chiu, </author> <title> Congestion avoidance in computer networks with a connectionless network layer: Part IV: A selective binary feedback scheme for general topologies methodology, </title> <type> Technical Report DEC-TR-510, </type> <institution> Digital Equipment Corporation, </institution> <year> 1987. </year>
Reference-contexts: we discuss link-by-link control approaches. 1.2.1 End-to-End Window-Based Approaches We first describe the DECbit scheme because it chronologically precedes TCP and because many of the mechanisms used by TCP in the window adjustment process are similar to those used by DECbit. 1.2.1.1 Congestion Control in DECbit In the DECbit scheme <ref> [84, 85] </ref> the sources adjust the window size based on explicit feedback received from the network. All data packets carry a bit, called congestion indication bit. Congested switches mark this bit to indicate their state to the end systems. <p> However, when different connections have unequal delays, it shows bias against connections with long delays. Furthermore, in periods of congestion it signals congestion not only to overloading connections but also to those whose demand is less than the max-min fair share. In <ref> [85] </ref> a selective marking mechanism was added to DECbit; the decision to set the congestion bit in a packet is based not only on the average queue length, but on the traffic load of individual connections as well. That is, the bandwidth usage of individual connections is constantly monitored.
Reference: [86] <author> K. K. Ramakrishnan and P. Newman, </author> <title> Integration of rate and credit schemes for ATM flow control, </title> <journal> IEEE Network, </journal> <volume> vol. 9, no. 2, 4956, </volume> <month> March/April </month> <year> 1995. </year>
Reference-contexts: The evolution of the congestion control framework and the trade-offs made during the process of its specification are discussed in <ref> [25, 80, 86] </ref>. A detailed discussion of the source system behavior for the rate-based framework can be found in [49, 68]. 1.3.2 Hop-by-Hop Rate-Based Approaches Rate-based control can be applied on a link-by-link (also known as hop-by-hop) basis as well. Mishra et. al. [74] have developed such a scheme.
Reference: [87] <author> A. Romanow and S. Floyd, </author> <title> Dynamics of TCP traffic over ATM networks, </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 633-41, no. 4, 13, </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: Several studies report on the efficiency of the mechanisms employed by TCP and its performance [9, 27, 30, 40, 98, 100]. An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in <ref> [53, 87] </ref>. Finally, performance issues of TCP over ATM for a specific TCP implementation are discussed in [21] and [78]. The assumption that a packet loss is a reliable indication of network congestion may cause performance degradation in environments with high bit error rates, such as wireless networks. <p> The dynamics and the performance of TCP over ATM networks using PPD or EPD policies are studied in <ref> [53, 87] </ref>.
Reference: [88] <author> S. S. Sathaye, </author> <title> ATM Forum traffic management specification, Version 4.0, ATM Forum, Traffic Management Working Group, </title> <month> June </month> <year> 1995, </year> <title> ATM Forum/95-0013R4. </title>
Reference-contexts: A more detailed description of this framework can be found in Section 1.3.1 or in [89]. Table 3.1 summarizes the notations for parameters used in our analysis of the source behavior, along with the corresponding terminology used in the ATM Forum Traffic Management Specification <ref> [88] </ref>. The simulations described in the chapter are based on the use of the rate-allocation algorithm presented in Chapter 2. The source policy is responsible for adjusting the current transmission rate of the source based on feedback received via the returned RM cells. <p> The switches support the explicit rate allocation algorithm described in Chapter 2. Each of the nodes implements the ABR source policy defined by ATM Forum <ref> [88] </ref>.
Reference: [89] <author> S. S. Sathaye, </author> <title> Traffic management specification, version 4.0, Traffic Management Working Group, </title> <month> April </month> <year> 1996. </year>
Reference-contexts: The ATM Forum Traffic Management Committee recently completed work on a rate-based congestion control framework to meet this objective <ref> [89] </ref>. Even though the rate-control framework that has been defined for the ABR service can successfully control congestion in ATM networks, it will not be as effective if the ATM network does not extend to the end systems. <p> The goal of the allocation is to arrive at an efficient allocation that is also fair. Possible fairness criteria are discussed in <ref> [89] </ref>. <p> This proportional transmission of RM cells is to ensure that the amount of overhead for RM cells is a constant, independent of the number of connections in the network or their rates. The RM cell contains several fields <ref> [89] </ref>. Of interest to us are the following: 1. The virtual circuit identifier to identify the connection it belongs to. 2. The amount of bandwidth requested, called explicit rate (ER). 3. The current cell rate (CCR) of the connection. <p> This algorithm maximizes the utilization of the bottleneck link in the network while at the same time allocating the available bandwidth according to the max-min fairness criterion. We also show how the algorithm can be easily modified to support other definitions of fairness such as those recommended in <ref> [89] </ref>. The algorithm is insensitive to cell losses and to the source behavior (as long as the source does not exceed the indicated rate). It is executed only on the arrival of RM cells and its execution requires the extraction/update of the state of a single connection. <p> Design and Evaluation of an Explicit Rate Allocation Algorithm for Available Bit-Rate (ABR) Service in ATM Networks The ATM Forum has developed a rate-control framework for the ABR service in ATM networks <ref> [89] </ref>. This framework consists of two principal components: (i) the behavior of the source and the destination end systems, and (ii) the behavior of the network elements (switches). <p> A brief discussion of the source and destination behavior was given in Section 1.3.1. A detailed specification of the control loop operation can be found in <ref> [89] </ref>. In summary, the operation of the control loop is as follows: A source specifies the bandwidth demand and the current transmission rate of the connection in each transmitted RM cell. <p> In this chapter we describe and algorithm that can arrive at a max-min fair alloca 31 tion [6] or variations of max-min fairness defined in <ref> [89] </ref>. We study the dynamics and the performance of the rate allocation algorithm under both ATM-layer-generated ABR traffic and TCP-controlled ABR traffic. <p> The way minimum rate is guaranteed depends on the definition of fairness adopted. The ATM Forum has specified several definitions for fairness criteria <ref> [89] </ref>. With the MCR plus equal share criterion, each connection is allocated the negotiated 42 MCR plus a fair portion of the remaining bandwidth. Supporting this definition in our algorithm is straightforward: Every connection is allocated its MCR by default. <p> A more detailed description of this framework can be found in Section 1.3.1 or in <ref> [89] </ref>. Table 3.1 summarizes the notations for parameters used in our analysis of the source behavior, along with the corresponding terminology used in the ATM Forum Traffic Management Specification [88]. The simulations described in the chapter are based on the use of the rate-allocation algorithm presented in Chapter 2.
Reference: [90] <author> K.-Y. Siu and H.-Y. Tzeng, </author> <title> Intelligent congestion control for ABR service in ATM networks, </title> <journal> Computer Communication Review, </journal> <volume> vol. 24, no. 5, 81106, </volume> <month> October </month> <year> 1994. </year>
Reference-contexts: As a result, 22 RTT seconds later the link utilization will get close to one. Several enhancements and extensions to the basic ERICA algorithm are also presented in [50, 51]. Many examples of state-less explicit rate allocation algorithms can be found in <ref> [1, 34, 72, 79, 90] </ref>. The schemes proposed in [61, 103] use control theoretic approaches with emphasis on system stability issues. A comparison of explicit rate allocation schemes can be found in [3]. 1.5 Packet Discard Approaches During congestion the switches may have to drop packets. <p> Such algorithms for rate allocation in packet-switched networks have been described by Charny [13], Jain [46], Kalampoukas, et al. [55], and Siu, et al. <ref> [90] </ref>. In this section we describe the operation of the proposed rate allocation algorithm and discuss its properties. The following definitions and notations are used in the description of our algorithm: Consider any switch in the path of a connection.
Reference: [91] <author> W. R. </author> <title> Stevens, </title> <journal> TCP/IP Illustrated, </journal> <volume> vol. 1, </volume> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1994. </year>
Reference-contexts: Recovering from a packet loss without having to wait for a timer to expire is known as fast retransmit. Also, in TCP-Reno, recovering from a packet loss indicated from three duplicate acks by only halving the congestion window is known as fast recovery <ref> [44, 91] </ref>. 12 The justification behind the two phases in the congestion window increase process is that during the slow start phase a connection can grow its window rapidly thus claiming very quickly any available bandwidth. <p> Simulation results presented in [28] showed improved throughput and fairness and lower response time for delay-sensitive applications (e.g. telnet). A detailed description of TCP, the algorithms it employs, and its implementation can be found in <ref> [91, 92] </ref>. Several studies report on the efficiency of the mechanisms employed by TCP and its performance [9, 27, 30, 40, 98, 100]. An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in [53, 87]. <p> The delay estimation algorithm attempts to maintain a good estimate of the round-trip delay which is used as a basis to set the retransmission timers. The TCP Reno Version <ref> [91] </ref>, introduced in 1990, added the fast retransmit and fast recovery algorithm to avoid performing slow-start when the level of congestion in the network is not severe to warrant congestion recovery by slow-start.
Reference: [92] <author> W. R. Stevens and G. R. </author> <title> Wright, </title> <journal> TCP/IP Illustrated, </journal> <volume> vol. 2, </volume> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1995. </year>
Reference-contexts: Simulation results presented in [28] showed improved throughput and fairness and lower response time for delay-sensitive applications (e.g. telnet). A detailed description of TCP, the algorithms it employs, and its implementation can be found in <ref> [91, 92] </ref>. Several studies report on the efficiency of the mechanisms employed by TCP and its performance [9, 27, 30, 40, 98, 100]. An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in [53, 87]. <p> Therefore, to avoid packet losses at the source node, we assume that the IP queue has a size equal to the maximum window size of the sending TCP. The interaction between TCP and IP described above is consistent with the 4.4 BSD-Lite Unix Release <ref> [92] </ref>. 4.1.2 The Onset of Ack Compression We briefly discuss now how the window-growth process during the TCP slow-start phase leads to ack compression in a two-way configuration.
Reference: [93] <author> D. Stiliadis and A. Varma, </author> <title> Design and analysis of frame-based fair queueing: A new traffic scheduling algorithm for packet-switched networks, </title> <booktitle> in Proc. of ACM SIGMETRICS '96, </booktitle> <address> http://www.cse.ucsc.edu/research/hsnlab/publications/, May 1996. </address>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing [18, 26, 36, 52, 76, 101], and scheduling mechanisms <ref> [2, 23, 35, 37, 82, 93, 99] </ref>. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [94] <author> J. S. Turner, </author> <title> Maintaining high throughput during overload in ATM switches, </title> <booktitle> in Proc. of IEEE INFOCOM'96, </booktitle> <volume> vol. 1, </volume> <pages> pp. 287295, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: The dynamics and the performance of TCP over ATM networks using PPD or EPD policies are studied in [53, 87]. An analysis on the effect of the EPD buffer threshold on TCP performance is presented in <ref> [94] </ref>. 1.6 Contributions of this Dissertation The rate-based congestion control framework developed by the ATM Forum to support the ABR service defines the behavior of the source and destination end systems of the underlying ATM network.
Reference: [95] <author> C. Villamizar and C. Song, </author> <title> High performance TCP in ANSNET, </title> <journal> Computer Communications Review, </journal> <volume> vol. 24, no. 5, 4560, </volume> <month> October </month> <year> 1994. </year>
Reference-contexts: The performance of RED is explored in [32] where it has been shown to improve both the throughput and the fairness of TCP compared to the Drop-Tail scheme. Experimental studies on RED can be found in <ref> [24, 95] </ref>. Finally, Floyd et. al. [29] address some of the shortcomings of RED and discuss possible enhancements.
Reference: [96] <author> Z. Wang and J. Crowcroft, </author> <title> A new congestion control scheme: Slow start and search (Tri-S), </title> <journal> Computer Communication Review, </journal> <volume> vol. 21, no. 1, 3243, </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: To adapt faster to the network state, it is necessary for TCP to maintain a very accurate RTT estimation. Jacobson et. al. [45] proposed to perform an RTT measurement for each data packet transmitted rather than only once for every window worth of data. The TCP-Vegas [10] and Tri-S <ref> [96] </ref> approaches both recognized that once the TCP window is large enough to fill the round-trip network pipe, further window increases contribute only to increased queueing delays in the network without improving its performance. <p> Even though several extensions to TCP have been proposed with the objective of adapting the window size to the actual bandwidth-delay product of the underlying network <ref> [10, 96] </ref>, current implementations require a packet loss to reduce the congestion window size. In 193 the absence of such losses the congestion window increases up to the maximum socket buffer advertised by the receiver.
Reference: [97] <author> R. Wilder, K. K. Ramakrishnan, and A. Mankin, </author> <title> Dynamics of congestion control and avoidance of two-way traffic in an OSI testbed, </title> <journal> ACM Computer Communication Review, </journal> <volume> vol. 21, no. 2, 4358, </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: In the case of two way traffic, the idle interval is due to the batching of the acknowledgments which do not serve any more as a reliable clocking mechanism <ref> [97, 102] </ref>. Even worse, unlike the case presented in Figure 3.10 where cells transmitted in the previous cycle cause the rate increase in the current one, in this case it is mostly likely that cells will be returning while the channel is still idle. <p> These packets and acknowledgements may share a common buffer in the end systems as well as network switches/routers. This sharing has been shown to result in an effect called ack compression, where acks of a connection arrive at the source bunched together <ref> [97, 102] </ref>. The result of ack-compression is a marked unfairness in the throughput received with competing connections, and reduced overall throughput compared to what could be expected without this effect [97]. Ack compression may occur either at the end system or in a switch/router. <p> The result of ack-compression is a marked unfairness in the throughput received with competing connections, and reduced overall throughput compared to what could be expected without this effect <ref> [97] </ref>. Ack compression may occur either at the end system or in a switch/router. In either case, the smooth flow of acknowl-edgements to the source is disturbed, potentially resulting in reduction of throughput for the TCP connections involved. <p> The effect of ack compression and the resulting dynamics of transport protocols under two-way traffic have been studied previously by Zhang, et al. [102], and by Wilder, et al. <ref> [97] </ref>. Zhang, et al. [102] studied TCP dynamics under two-way traffic in a datagram network by simulation, and observed that the queues in the routers exhibit periodic behavior. Wilder, et al. [97] observed a similar effect in OSI-based networks under two-way 114 traffic causing unfairness and an overall reduction in throughput. <p> transport protocols under two-way traffic have been studied previously by Zhang, et al. [102], and by Wilder, et al. <ref> [97] </ref>. Zhang, et al. [102] studied TCP dynamics under two-way traffic in a datagram network by simulation, and observed that the queues in the routers exhibit periodic behavior. Wilder, et al. [97] observed a similar effect in OSI-based networks under two-way 114 traffic causing unfairness and an overall reduction in throughput.
Reference: [98] <author> L. Zhang, </author> <title> Why TCP timers don't work well, </title> <booktitle> in Proc. of ACM SIGCOMM'86, </booktitle> <pages> pp. 397405, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: A detailed description of TCP, the algorithms it employs, and its implementation can be found in [91, 92]. Several studies report on the efficiency of the mechanisms employed by TCP and its performance <ref> [9, 27, 30, 40, 98, 100] </ref>. An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in [53, 87]. Finally, performance issues of TCP over ATM for a specific TCP implementation are discussed in [21] and [78].
Reference: [99] <author> L. Zhang, VirtualClock: </author> <title> A new traffic control algorithm for packet switching networks, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 9, no. 2, 101124, </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing [18, 26, 36, 52, 76, 101], and scheduling mechanisms <ref> [2, 23, 35, 37, 82, 93, 99] </ref>. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [100] <author> L. Zhang and D. D. Clark, </author> <title> Oscillating behavior of network traffic: A case study simulation, </title> <journal> Intenetworking: Research and Experience, </journal> <volume> vol. 1, no. 2, 101112, </volume> <month> December </month> <year> 1990. </year> <month> 247 </month>
Reference-contexts: A detailed description of TCP, the algorithms it employs, and its implementation can be found in [91, 92]. Several studies report on the efficiency of the mechanisms employed by TCP and its performance <ref> [9, 27, 30, 40, 98, 100] </ref>. An analytical approach to TCP performance can be found in [66]. Studies on the performance of TCP over ATM networks can be found in [53, 87]. Finally, performance issues of TCP over ATM for a specific TCP implementation are discussed in [21] and [78].
Reference: [101] <author> L. Zhang, S. Deering, D. Estrin, S. Shenker, and D. Zappala, RSVP: </author> <title> A new resource reservation protocol, </title> <journal> IEEE Network, </journal> <volume> vol. 7, no. 5, 818, </volume> <month> September </month> <year> 1993. </year>
Reference-contexts: Connections subject to open-loop control will specify their requirements to the network elements which will reserve, if available, the necessary resources. With open-loop, congestion is handled by a combination of admission control, policing <ref> [18, 26, 36, 52, 76, 101] </ref>, and scheduling mechanisms [2, 23, 35, 37, 82, 93, 99]. Closed-loop control is more appropriate for networks where resource reservation is not possible or when the traffic behavior cannot be formally specified.
Reference: [102] <author> L. Zhang, S. Shenker, and D. D. Clark, </author> <title> Observations on the dynamics of a congestion control algorithm: The effects of two-way traffic, </title> <booktitle> in Proc. of ACM SIGCOMM'91, </booktitle> <pages> pp. 133147, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: In the case of two way traffic, the idle interval is due to the batching of the acknowledgments which do not serve any more as a reliable clocking mechanism <ref> [97, 102] </ref>. Even worse, unlike the case presented in Figure 3.10 where cells transmitted in the previous cycle cause the rate increase in the current one, in this case it is mostly likely that cells will be returning while the channel is still idle. <p> These packets and acknowledgements may share a common buffer in the end systems as well as network switches/routers. This sharing has been shown to result in an effect called ack compression, where acks of a connection arrive at the source bunched together <ref> [97, 102] </ref>. The result of ack-compression is a marked unfairness in the throughput received with competing connections, and reduced overall throughput compared to what could be expected without this effect [97]. Ack compression may occur either at the end system or in a switch/router. <p> In either case, the smooth flow of acknowl-edgements to the source is disturbed, potentially resulting in reduction of throughput for the TCP connections involved. The effect of ack compression and the resulting dynamics of transport protocols under two-way traffic have been studied previously by Zhang, et al. <ref> [102] </ref>, and by Wilder, et al. [97]. Zhang, et al. [102] studied TCP dynamics under two-way traffic in a datagram network by simulation, and observed that the queues in the routers exhibit periodic behavior. <p> The effect of ack compression and the resulting dynamics of transport protocols under two-way traffic have been studied previously by Zhang, et al. <ref> [102] </ref>, and by Wilder, et al. [97]. Zhang, et al. [102] studied TCP dynamics under two-way traffic in a datagram network by simulation, and observed that the queues in the routers exhibit periodic behavior. Wilder, et al. [97] observed a similar effect in OSI-based networks under two-way 114 traffic causing unfairness and an overall reduction in throughput. <p> For example, the transmission time of a 9-Kbyte packet at a link rate of 10 Mbits/second is approximately 8 msecs, while the TCP protocol processing time in real systems typically does not exceed a few hundred microseconds <ref> [12, 83, 102] </ref>. The effect of non-zero TCP processing time on throughput is considered in the simulation results presented in Section 4.4. The functionality assumed for the IP layer is simple. <p> For example, the transmission time of a 1500 byte packet over a 100 Kbits/sec link is about 120 msecs, while the TCP protocol processing time in modern workstations does not exceed a few hundred microseconds <ref> [12, 83, 102] </ref>. Thus, we can safely assume that t pr is zero. 5.2 Demonstrating Effect of Ack Compression over Asymmetric Links We use both simulation and experimentation to illustrate that link asymmetry may cause severe performance degradation to TCP. TCP connections.
Reference: [103] <author> Y. Zhao, S.-Q. Li, and S. Sigarto, </author> <title> A linear dynamic model for design of stable explicit-rate ABR control schemes, </title> <booktitle> in Proc. of IEEE INFOCOM'97, </booktitle> <pages> pp. 283290, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: Several enhancements and extensions to the basic ERICA algorithm are also presented in [50, 51]. Many examples of state-less explicit rate allocation algorithms can be found in [1, 34, 72, 79, 90]. The schemes proposed in <ref> [61, 103] </ref> use control theoretic approaches with emphasis on system stability issues. A comparison of explicit rate allocation schemes can be found in [3]. 1.5 Packet Discard Approaches During congestion the switches may have to drop packets.
References-found: 103

