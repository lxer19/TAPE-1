URL: http://www.cs.Helsinki.fi/research/cosco/Articles/kdd96.ps.gz
Refering-URL: 
Root-URL: 
Email: Email: Firstname.Lastname@cs.Helsinki.FI  
Phone: 26,  
Title: Predictive Data Mining with Finite Mixtures  
Author: Petri Kontkanen Petri Myllymaki Henry Tirri 
Date: August 1996).  
Note: Pp. 176-182 in Proceedings of The Second International Conference on Knowledge Discovery and Data Mining (Portland, OR,  
Web: URL: http://www.cs.Helsinki.FI/research/cosco/  
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Abstract: In data mining the goal is to develop methods for discovering previously unknown regularities from databases. The resulting models are interpreted and evaluated by domain experts, but some model evaluation criterion is needed also for the model construction process. The optimal choice would be to use the same criterion as the human experts, but this is usually impossible as the experts are not capable of expressing their evaluation criteria formally. On the other hand, it seems reasonable to assume that any model possessing the capability of making good predictions also captures some structure of the reality. For this reason, in predictive data mining the search for good models is guided by the expected predictive error of the models. In this paper we describe the Bayesian approach to predictive data mining in the finite mixture modeling framework. The finite mixture model family is a natural choice for domains where the data exhibits a clustering structure. In many real world domains this seems to be the case, as is demonstrated by our experimental results on a set of public domain databases. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agrawal, R.; Mannila, H.; Srikant, R.; Toivonen, H.; and Verkamo, A. </author> <year> 1996. </year> <title> Fast discovery of association rules. </title> <editor> In Fayyad, U.; Piatetsky-Shapiro, G.; Smyth, P.; and Uthurusamy, R., eds., </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Typically the pattern extraction phase is performed by a structure searching program, and the interpretation phase by a human expert. The various proposed approaches differ in the representation language for the structure to be discovered (association rules <ref> (Agrawal et al. 1996) </ref>, Bayesian networks (Spirtes, Glymour, & Scheines 1993), functional dependencies (Mannila & Raiha 1991), prototypes (Hu & Cercone 1995) etc.), and in the search methodology used for discovering such structures.
Reference: <author> Basilevsky, A. </author> <year> 1994. </year> <title> Statistical Factor Analysis and Related Methods. </title> <booktitle> Theory and Applicatioms. </booktitle> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: A large body of the data mining research is exploratory in nature, i.e., search for any kind of structure in the database in order to understand the domain better. Akin to the practice of multivariate exploratory analysis in social sciences <ref> (Basilevsky 1994) </ref>, much of the work in the data mining area relies on a task-specific expert assessment of the model goodness. We depart from this tradition, and assume that the discovery process is performed with the expected prediction capability in mind.
Reference: <author> Bernardo, J., and Smith, A. </author> <year> 1994. </year> <title> Bayesian theory. </title> <publisher> John Wiley. </publisher>
Reference-contexts: Given a finite mixture model fi that models the cluster structure of the database, predictive inference can be performed in a computationally efficient manner. The Bayesian approach to predictive inference (see e.g., <ref> (Bernardo & Smith 1994) </ref>) aims at predicting unobserved future quantities by means of already observed quantities.
Reference: <author> Cheeseman, P., and Stutz, J. </author> <year> 1996. </year> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In Fayyad, U.; Piatetsky-Shapiro, G.; Smyth, P.; and Uthurusamy, R., eds., </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <address> Menlo Park: </address> <publisher> AAAI Press. </publisher> <address> chapter 6. </address>
Reference-contexts: Our approach is akin to the AutoClass system (Cheeseman et al. 1988), which has been successfully used for data mining problems, such as Land-Sat data clustering <ref> (Cheeseman & Stutz 1996) </ref>. In the case of finite mixtures, the model search problem can be seen as searching for the missing values of the unobserved latent clustering variable in the dataset. The model construction process consists of two phases: model class selection and model class parameter selection.
Reference: <author> Cheeseman, P.; Kelly, J.; Self, M.; Stutz, J.; Taylor, W.; and Freeman, D. </author> <year> 1988. </year> <title> Autoclass: A Bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> 54-64. </pages>
Reference-contexts: Bayesian approach also makes a clear separation between the search component and the model measure, and allows therefore modular combinations of different search algorithms and model evaluation criteria. Our approach is akin to the AutoClass system <ref> (Cheeseman et al. 1988) </ref>, which has been successfully used for data mining problems, such as Land-Sat data clustering (Cheeseman & Stutz 1996). In the case of finite mixtures, the model search problem can be seen as searching for the missing values of the unobserved latent clustering variable in the dataset.
Reference: <author> Cheeseman, P. </author> <year> 1995. </year> <title> On Bayesian model selection. </title>
Reference-contexts: This is clearly not feasible for data mining considerations, since such a model can hardly be given any useful semantic interpretation. We therefore use only a single, maximum a posteriori probability (MAP) model for making predictions. The feasibility of this approach is discussed in <ref> (Cheeseman 1995) </ref>. Bayesian inference by finite mixture models In our predictive data mining framework the problem domain is modeled by m discrete random variables X 1 ; : : : ; X m .
Reference: <editor> In Wolpert, D., ed., </editor> <title> The Mathematics of Generalization, volume XX of SFI Studies in the Sciences of Complexity. </title> <publisher> Addison-Wesley. </publisher> <pages> 315-330. </pages>
Reference: <author> DeGroot, M. </author> <year> 1970. </year> <title> Optimal statistical decisions. </title> <publisher> McGraw-Hill. </publisher>
Reference: <author> Dempster, A.; Laird, N.; and Rubin, D. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> Series B 39(1) </volume> <pages> 1-38. </pages>
Reference-contexts: Finding the exact MAP estimate of fi is, however, computationally infeasible task, thus we are forced to use numerical approximation methods. We used here a variant of the Expectation-Maximization (EM) algorithm <ref> (Dempster, Laird, & Rubin 1977) </ref> for this purpose, since the method is easily applicable in this domain and produces good solutions quite rapidly, as can be seen in (Kontkanen, Myllymaki, & Tirri 1996b).
Reference: <author> Everitt, B., and Hand, D. </author> <year> 1981. </year> <title> Finite Mixture Distributions. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <editor> Fayyad, U.; Piatetsky-Shapiro, G.; Smyth, P.; and Uthurusamy, R., eds. </editor> <booktitle> 1996. Advances in Knowledge Discovery and Data Mining. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Introduction Data mining aims at extracting useful information from databases by discovering previously unknown regularities from data <ref> (Fayyad et al. 1996) </ref>. In the most general context, finding such interesting regularities is a process (often called knowledge discovery in databases) which includes the interpretation of the extracted patterns based on the domain knowledge available.
Reference: <author> Geisser, S. </author> <year> 1975. </year> <title> The predictive sample reuse method with applications. </title> <journal> Journal of the American Statistical Association 70(350) </journal> <pages> 320-328. </pages>
Reference: <author> Gelman, A.; Carlin, J.; Stern, H.; and Rubin, D. </author> <year> 1995. </year> <title> Bayesian Data Analysis. </title> <publisher> Chapman & Hall. </publisher>
Reference: <author> Heckerman, D.; Geiger, D.; and Chickering, D. </author> <year> 1995. </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <booktitle> Machine Learning 20(3) </booktitle> <pages> 197-243. </pages>
Reference: <author> Hu, X., and Cercone, N. </author> <year> 1995. </year> <title> Rough sets similarity-based learning from databases. </title> <editor> In Fayyad, U., and Uthurusamy, R., eds., </editor> <booktitle> Proceedings of the First International Conference on Knowledge Discovery & Data Mining, </booktitle> <pages> 162-167. </pages>
Reference-contexts: The various proposed approaches differ in the representation language for the structure to be discovered (association rules (Agrawal et al. 1996), Bayesian networks (Spirtes, Glymour, & Scheines 1993), functional dependencies (Mannila & Raiha 1991), prototypes <ref> (Hu & Cercone 1995) </ref> etc.), and in the search methodology used for discovering such structures. A large body of the data mining research is exploratory in nature, i.e., search for any kind of structure in the database in order to understand the domain better.
Reference: <author> Kontkanen, P.; Myllymaki, P.; and Tirri, H. </author> <year> 1996a. </year> <title> Comparing Bayesian model class selection criteria by discrete finite mixtures. </title> <booktitle> In Proceedings of the ISIS (Information, Statistics and Induction in Science) Conference. (To appear.). </booktitle>
Reference-contexts: In the case of large databases several approximations to these criteria could be used, but many of them are inaccurate with small databases as pointed out in <ref> (Kontkanen, Myllymaki, & Tirri 1996a) </ref>. Alternatively we can choose some prediction problem, and evaluate prediction error empirically by using the available database. An example of such a prediction task would be to predict an unknown attribute value of a data item, given a set of some 176 other instantiated attributes.
Reference: <author> Kontkanen, P.; Myllymaki, P.; and Tirri, H. </author> <year> 1996b. </year> <title> Constructing Bayesian finite mixture models by the EM algorithm. </title> <type> Technical Report C-1996-9, </type> <institution> University of Helsinki, Department of Computer Science. </institution>
Reference-contexts: We used here a variant of the Expectation-Maximization (EM) algorithm (Dempster, Laird, & Rubin 1977) for this purpose, since the method is easily applicable in this domain and produces good solutions quite rapidly, as can be seen in <ref> (Kontkanen, Myllymaki, & Tirri 1996b) </ref>. Empirical results The finite mixture based approach for predictive data mining described above has been implemented as part of a more general software environment for probabilistic modeling.
Reference: <author> Mannila, H., and Raiha, K.-J. </author> <year> 1991. </year> <title> The design of relational databases. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The various proposed approaches differ in the representation language for the structure to be discovered (association rules (Agrawal et al. 1996), Bayesian networks (Spirtes, Glymour, & Scheines 1993), functional dependencies <ref> (Mannila & Raiha 1991) </ref>, prototypes (Hu & Cercone 1995) etc.), and in the search methodology used for discovering such structures. A large body of the data mining research is exploratory in nature, i.e., search for any kind of structure in the database in order to understand the domain better.
Reference: <editor> Michie, D.; Spiegelhalter, D.; and Taylor, C., eds. </editor> <year> 1994. </year> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> London: Ellis Horwood. </publisher>
Reference-contexts: The selection of databases was done on the basis of their reported use, i.e., we have preferred databases that have been used for testing many different methods over databases with only isolated results. Many of the databases used are from the StatLog project <ref> (Michie, Spiegelhalter, & Taylor 1994) </ref>. The experimental setups and the best success rates obtained are shown in Table 1. All our results are crossvalidated, and when possible (for the StatLog datasets) we have used the same crossvalidation schemes (the same number of folds) as in (Michie, Spiegelhalter, & Taylor 1994). <p> are from the StatLog project <ref> (Michie, Spiegelhalter, & Taylor 1994) </ref>. The experimental setups and the best success rates obtained are shown in Table 1. All our results are crossvalidated, and when possible (for the StatLog datasets) we have used the same crossvalidation schemes (the same number of folds) as in (Michie, Spiegelhalter, & Taylor 1994). The results on each of these datasets are shown in Figures 1-3. In each case, the maximum, minimum and the average success rate on 30 independent crossvalidation runs are given.
Reference: <author> Myllymaki, P., and Tirri, H. </author> <year> 1994. </year> <title> Massively parallel case-based reasoning with probabilistic similarity metrics. </title> <editor> In Wess, S.; Althoff, K.-D.; and Richter, M., eds., </editor> <booktitle> Topics in Case-Based Reasoning, volume 837 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag. </publisher> <pages> 144-154. </pages>
Reference-contexts: Observe that K is usually small compared to the sample size N , and thus the prediction computation can be performed very efficiently <ref> (Myllymaki & Tirri 1994) </ref>. The predictive distributions can be used for classification and regression tasks. In classification problems, we have a special class variable X c which is used for classifying data.
Reference: <author> Raftery, A. </author> <year> 1993. </year> <title> Approximate Bayes factors and accounting for model uncertainty in generalized linear models. </title> <type> Technical Report 255, </type> <institution> Department of Statistics, University of Washington. </institution>
Reference: <author> Rissanen, J. </author> <year> 1989. </year> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company. </publisher>
Reference-contexts: Assuming equal priors for the model classes, they can be ranked by evaluating the evidence P (DjM K ) (or equivalently the stochastic complexity <ref> (Rissanen 1989) </ref>) for each model class. This term is defined as a multidimensional integral and it is usually very hard to evaluate, although with certain assumptions, the evidence can in some cases be determined analytically (Heck-erman, Geiger, & Chickering 1995; Kontkanen, Myl lymaki, & Tirri 1996a).
Reference: <author> Shrager, J., and Langley, P., eds. </author> <year> 1990. </year> <title> Computational Models of Scientific Discovery and Theory Formation. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: In this work we adopt the empirical approach and use the crossvalidation method (Stone 1974; Geisser 1975) for model selection on a set of public domain databases. In the work presented below we have adopted the basic concepts from the general framework of exploring computational models of scientific discovery <ref> (Shrager & Langley 1990) </ref>. Given a database, we do not attempt to discover arbitrary structures, but restrict the possible patterns (models) to be members of a predefined set, which we call the model space.
Reference: <author> Spirtes, P.; Glymour, C.; and Scheines, R., eds. </author> <year> 1993. </year> <title> Causation, Prediction and Search. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Typically the pattern extraction phase is performed by a structure searching program, and the interpretation phase by a human expert. The various proposed approaches differ in the representation language for the structure to be discovered (association rules (Agrawal et al. 1996), Bayesian networks <ref> (Spirtes, Glymour, & Scheines 1993) </ref>, functional dependencies (Mannila & Raiha 1991), prototypes (Hu & Cercone 1995) etc.), and in the search methodology used for discovering such structures.
Reference: <author> Stone, M. </author> <year> 1974. </year> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Journal of the Royal Statistical Society (Series B) 36 </journal> <pages> 111-147. </pages>
Reference: <author> Tirri, H.; Kontkanen, P.; and Myllymaki, P. </author> <year> 1996. </year> <title> Probabilistic instance-based learning. </title> <editor> In Saitta, L., ed., </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference (to appear). </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Finite mixture models can also be seen to offer a Bayesian solution to the case matching and case adaptation problems in instance-based reasoning (see the discussion in <ref> (Tirri, Kontkanen, & Myl-lymaki 1996) </ref>), i.e., they can also be viewed as a theoretically sound representation language for a "prototype" model space.
Reference: <author> Titterington, D.; Smith, A.; and Makov, U. </author> <year> 1985. </year> <title> Statistical Analysis of Finite Mixture Distributions. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Wallace, C., and Freeman, P. </author> <year> 1987. </year> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society 49(3) </journal> <pages> 240-265. 182 </pages>
References-found: 28

