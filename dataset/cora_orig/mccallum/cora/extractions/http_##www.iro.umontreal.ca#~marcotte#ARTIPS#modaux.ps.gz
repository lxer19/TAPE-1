URL: http://www.iro.umontreal.ca/~marcotte/ARTIPS/modaux.ps.gz
Refering-URL: http://www.iro.umontreal.ca/~marcotte/publi.html
Root-URL: http://www.iro.umontreal.ca
Title: COUPLING THE AUXILIARY PROBLEM PRINCIPLE WITH DESCENT METHODS OF PSEUDOCONVEX PROGRAMMING  
Author: Daoli ZHU Patrice MARCOTTE 
Date: November 25, 1998  
Address: Montreal, Canada H3C 3J7  Montreal, Canada H3C 3J7  
Affiliation: CRT, Universite de Montreal  DIRO, Universite de Montreal  
Abstract: The descent auxiliary problem method allows one to find the solution of minimization problems by solving a sequence of auxiliary problems which incorporate a linesearch strategy. We derive the basic algorithm and study its convergence properties within the framework of infinite dimensional pseudoconvex minimization. We also introduce a partial descent type auxiliary problem method which partially linearizes the objective functional and includes the auxiliary term only for of subset of variables. Numerous examples of this very general scheme are provided. fl Research supported by NSERC grant A5789
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Avriel, M., Diwert, W.E., Shaible, S. and Zang, I., </author> <title> Generalized convexity and applications, </title> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, Holland (1988). </address>
Reference-contexts: Without (much) loss of generality, the reader can assume that the space of interest is the Euclidian space R n . 2 Definition 1 The functional F is convex on V if for all u 1 ; u 2 in V and for all ff 2 <ref> [0; 1] </ref> we have F (ffu 1 + (1 ff)u 2 ) ffF (u 1 ) + (1 ff)F (u 2 ): (1) The functional F is strongly convex with modulus b on V if there exists a positive constant b such that F (ffu 1 + (1 ff)u 2 ) <p> Fractional programming. One advantage of our method is that we only require the objective J + J 1 to be pseudocon vex. Therefore it can be applied in the realm of generalized fractional programming considered by Avriel, Diwert, Schaible and Zang <ref> [1] </ref>: min f 1 (x) + h (x) where f 1 (x) is convex and f 1 (x) + g (x)=h (x) is pseudoconvex.
Reference: [2] <author> Bertsekas, </author> <title> D.P., "On the Goldstein-Levitin-Polyak gradient projection method", </title> <journal> IEEE Transactions on Automatic Control 21, </journal> <month> 174-184 </month> <year> (1976). </year>
Reference-contexts: If furthermore r 2 f (x k ) is positive definite and Lipschitz continuous on V , then the convergence is quadratic (see Pschenichny and Danilin [17]). An application of Newton-like methods can be found in Bertsekas <ref> [2] </ref>, Dembo and Tulowitzki [6], Pang and Yu [16], etc. 5 A partial auxiliary problem principle In this section we address the decomposable mathematical program DMP: min (J + J 1 )(u 1 ; u 2 ) (39) where u = (u 1 ; u 2 ) T and V ae
Reference: [3] <author> Cohen, G., </author> <title> "Optimization by decomposition and coordination: a unified approach", </title> <journal> IEEE Transactions on Automatic Control 23, </journal> <month> 222-232 </month> <year> (1978). </year> <month> 18 </month>
Reference-contexts: By adequatly selecting the auxiliary function, one may achieve the decomposition of a nonseparable problem into a sequence of separable subproblems. The main advantage of the descent approach over the standard auxiliary problem principle approach presented in <ref> [3] </ref> is that convergence is guaranteed under weaker conditions. The price to pay is the additional burden of a linesearch. Recently however, the authors [21] [22] have shown how to somewhat overcome this difficulty.
Reference: [4] <author> Cohen, G., </author> <title> "Auxiliary problem principle and decomposition of optimization problems", </title> <journal> Journal of Optimization Theory and Applications 32, </journal> <month> 277-305 </month> <year> (1980). </year>
Reference: [5] <author> Cohen, G., and Zhu, </author> <title> D.L., "Decomposition coordination methods in large scale problems: the nondifferentiable case and the use of augmented Lagrangians", </title> <booktitle> in Advances in large-scale systems 1, Chapter 8, </booktitle> <editor> J.B. Cruz editor, </editor> <publisher> JAI Press, </publisher> <address> Greenwich, USA , (1984). </address>
Reference-contexts: 1 Introduction For classical optimization problems, Cohen [3][4] and Cohen and Zhu <ref> [5] </ref> introduced the auxiliary problem principle as a general framework to describe and analyze computational algorithms ranging from gradient (or subgradient) algorithms to decomposition/coordination algorithms. In this paper we combine the auxiliary problem method with a descent scheme for solving smooth pseudoconvex problems.
Reference: [6] <author> Dembo, R.S. and Tulowitzki, U., </author> <title> "Computing equilibria on large multicommodity networks: an application of truncated quadratic programming algorithms", </title> <booktitle> Networks 18, </booktitle> <month> 273-284 </month> <year> (1988). </year>
Reference-contexts: If furthermore r 2 f (x k ) is positive definite and Lipschitz continuous on V , then the convergence is quadratic (see Pschenichny and Danilin [17]). An application of Newton-like methods can be found in Bertsekas [2], Dembo and Tulowitzki <ref> [6] </ref>, Pang and Yu [16], etc. 5 A partial auxiliary problem principle In this section we address the decomposable mathematical program DMP: min (J + J 1 )(u 1 ; u 2 ) (39) where u = (u 1 ; u 2 ) T and V ae U = U 1
Reference: [7] <author> Ekeland, I. and Temam, R., </author> <title> Convex analysis and variational problems, </title> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, Holland (1970). </address>
Reference-contexts: The equivalence of (i)-(v) is shown in see Ekeland and Temam <ref> [7] </ref>. (ii) ) (vi) Since F 1 is convex and u fl 2 V , we have hF 0 for all u in V .
Reference: [8] <author> Guler, </author> <title> "On the convergence of the proximal point algorithm for convex minimization", </title> <journal> SIAM J. Control and Optimization 29, </journal> <month> 403-419 </month> <year> (1991). </year>
Reference-contexts: ff)F (u 2 ) b=2ku 1 u 2 k 2 ff (1 ff); (2) A G^ateaux-differentiable functional F is pseudoconvex on V if hF 0 (u 1 ); u 2 u 1 i 0 ) F (u 2 ) F (u 1 ); (3) It has been shown (see Guler <ref> [8] </ref>) that if F is G-differentiable on V , then the following statements are equivalent: F is strongly convex on V with modulus b (4) F (u 2 ) F (u 1 ) hF 0 (u 1 ); u 2 u 1 i + b=2ku 2 u 1 k 2 (5)
Reference: [9] <author> Ha. Cu.D., </author> <title> "A generalization of the proximal point algorithm for convex minimization", </title> <journal> SIAM J. Control and Optimization 28, </journal> <month> 503-512 </month> <year> (1990). </year>
Reference-contexts: We consider problem MP with J j 0 and K k (u 2 ) = 1=2ku 2 u k 2 k 2 . The auxiliary problem becomes min J 1 (u) + 2* k ku 2 u k which constitutes a generalization of the proximal point algorithm of Ha <ref> [9] </ref>. Fractional programming. Consider the fractional programming problem min F (x; y) = j=1 f 1j (x j ) + g (y) ; (52) where the functions f 1j 's are strongly convex positive for all j, g is convex nonnegative and h is a positive, concave function.
Reference: [10] <author> Lasdon, </author> <title> L.S., "Optimization theory for large systems", </title> <publisher> Macmillan, </publisher> <address> New York, </address> <year> (1970). </year>
Reference-contexts: F j (u j ) + G (u): (29) If F j is strongly convex for all j, take K k = P n P n The auxiliary problem becomes min F j (u j ) + hG 0 and we recover the partial linearization technique of Larsson and Migdalas <ref> [10] </ref> as a special case. However, through the incorporation of a regularizing term, our method can deal with F j 's that are only convex (not strongly convex). <p> The latter can be solved efficiently even if the network is large. Nonlinear programming with coupling variables (Lasdon <ref> [10] </ref>) The partial auxiliary problem principle yields a practical method for solving nonlinear programming with coupling variables and separable constraint sets.
Reference: [11] <author> Larsson, T. and Migdalas, A., </author> <title> "An algorithm for nonlinear programs over Cartesian product sets", </title> <booktitle> Optimization 21, </booktitle> <month> 535-542 </month> <year> (1990). </year>
Reference: [12] <author> Marcotte, P., </author> <title> "Network design problem with congestion effects: a case of bilevel programming", </title> <booktitle> Mathematical Programming 34, </booktitle> <month> 142-162 </month> <year> (1986). </year>
Reference-contexts: A continuous network design problem (Marcotte <ref> [12] </ref>) Consider the problem of determining optimal improvements to an urban road network. Let z a denote the improvement on arc a of the network, v a the vehicle flow on arc a, S a the delay (congestion) function and OE a the investment in upgrading arc a.
Reference: [13] <author> Martinet, B., </author> <title> "Regularisation d'inequations variationnelles par approximations succes-sives", </title> <institution> Revue Francaise d'Informatique et de Recherche Operationnelle 4, </institution> <month> 154-159 </month> <year> (1970). </year>
Reference-contexts: Indeed, after 500 iterations of the Frank-Wolfe algorithm, the distance ku u fl k is still equal to .02. 4 Realizations of the descent auxiliary problem principle. In this section we consider instances of algorithms that fit our general framework. 9 The Proximal Point algorithm. (Martinet <ref> [13] </ref>, Rockafellar [19]) Set J j 0; and K k (u) = 1 2 ku u k k 2 in AP k . The auxiliary problem becomes min J 1 (u) + 2* k ku u k k 2 ; (28) which corresponds to the proximal iteration.
Reference: [14] <author> Martos, B., </author> <title> "Nonlinear Programming", </title> <publisher> North-Holland (1975). </publisher>
Reference-contexts: However, J + J 1 needs to be strongly monotone on V , and other conditions are required as well. Furthermore, a drawback of this method is its slow (typically sublinear) rate of convergence. As a numerical example consider the pseudoconvex mathematical programming problem, taken from Martos <ref> [14] </ref>: min J (u 1 ; u 2 ) = u 3 1 u 2 + 4u 3 1 19u 1 u 2 6u 2 s.t. u 1 + u 2 6 u 1 ; u 2 0 We have: J 0 (u) = 3u 2 1 19u 2 9 1
Reference: [15] <author> Ortega, J.M. and Rheinboldt, </author> <title> Iterative solution of nonlinear equations in several Variables, </title> <publisher> Academic Press (1970). </publisher>
Reference: [16] <author> Pang, J-S. and Yu, C.-S., </author> <title> "Linearized simplicial decomposition methods for computing traffic equilibria on networks", </title> <booktitle> Networks 14, </booktitle> <month> 427-438 </month> <year> (1982). </year>
Reference-contexts: If furthermore r 2 f (x k ) is positive definite and Lipschitz continuous on V , then the convergence is quadratic (see Pschenichny and Danilin [17]). An application of Newton-like methods can be found in Bertsekas [2], Dembo and Tulowitzki [6], Pang and Yu <ref> [16] </ref>, etc. 5 A partial auxiliary problem principle In this section we address the decomposable mathematical program DMP: min (J + J 1 )(u 1 ; u 2 ) (39) where u = (u 1 ; u 2 ) T and V ae U = U 1 fi U 2 .
Reference: [17] <author> Pschenichny, B.N. and Danilin, Y., </author> <title> Numerical methods in extremal problems, </title> <publisher> MIR Publishers, </publisher> <address> Moscow (1978). </address>
Reference-contexts: If the Hessian matrix r 2 f (x k ) is positive semidefinite on V , the convergence of Newton's method is assured. If furthermore r 2 f (x k ) is positive definite and Lipschitz continuous on V , then the convergence is quadratic (see Pschenichny and Danilin <ref> [17] </ref>).
Reference: [18] <author> Powell, W.B. and Sheffi, Y., </author> <title> "The convergence of equilibrium algorithms with predetermined step sizes", </title> <journal> Trans. Res. </journal> <volume> 15B, </volume> <month> 45-55 </month> <year> (1982). </year>
Reference-contexts: In some instances the objective function is costly to evaluate, and even an approxi mate linesearch could be hard to realize. In this case it is still possible to use a predetermined sequence of stepsizes fff k g. For instance, Powell and Sheffi <ref> [18] </ref> set u k+1 = u k + ff k (^u k u k ) with ff k ! 0 and P ff k = 1. This method does not require the derivative of the mapping J + J 1 to be Lipschitz continuous.
Reference: [19] <author> Rockafellar, </author> <title> R.T., "Augmented Lagrangians and applications of the proximal point algorithm in convex programming", </title> <note> Mathematics of Operations Research 1, 97-116 (1976). 19 </note>
Reference-contexts: Indeed, after 500 iterations of the Frank-Wolfe algorithm, the distance ku u fl k is still equal to .02. 4 Realizations of the descent auxiliary problem principle. In this section we consider instances of algorithms that fit our general framework. 9 The Proximal Point algorithm. (Martinet [13], Rockafellar <ref> [19] </ref>) Set J j 0; and K k (u) = 1 2 ku u k k 2 in AP k . The auxiliary problem becomes min J 1 (u) + 2* k ku u k k 2 ; (28) which corresponds to the proximal iteration. A partial linearization technique.
Reference: [20] <author> Zhu, </author> <title> D.L., Optimisation sous-differentiable et methodes de decomposition, </title> <type> Ph.D. Thesis, </type> <institution> Centre d'Automatique et Informatique, Ecole des Mines de Paris , Fontainebleau, </institution> <address> France (1982). </address>
Reference-contexts: For instance, consider the following example of Zhu <ref> [20] </ref>: U = R, V = [ 1 2 ; 1 2 ], F 1 (u) = u 3 (pseudoconvex) and F 2 = u 2 . We have u fl = 0.
Reference: [21] <author> Zhu. D.L. and Marcotte. P., </author> <title> "An extended descent framework for monotone variational inequalities", </title> <address> JOTA 80 (1994) 349-366. </address>
Reference-contexts: The main advantage of the descent approach over the standard auxiliary problem principle approach presented in [3] is that convergence is guaranteed under weaker conditions. The price to pay is the additional burden of a linesearch. Recently however, the authors <ref> [21] </ref> [22] have shown how to somewhat overcome this difficulty.
Reference: [22] <author> Zhu. D.L. and Marcotte, P., </author> <title> "Global convergence of descent processes for solving non strictly monotone variational inequalities", </title> <note> preprint (1992). 20 </note>
Reference-contexts: The main advantage of the descent approach over the standard auxiliary problem principle approach presented in [3] is that convergence is guaranteed under weaker conditions. The price to pay is the additional burden of a linesearch. Recently however, the authors [21] <ref> [22] </ref> have shown how to somewhat overcome this difficulty.
References-found: 22

