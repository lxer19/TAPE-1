URL: http://louis.hunter.cuny.edu/faculty/epstein/papers/CogSci.ps
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: 
Title: right reasons: The FORR architecture for learning in a skill domain  
Author: SUSAN L. EPSTEIN 
Address: New York  
Affiliation: Department of Computer Science Hunter College and The Graduate School of The City University of  
Note: For the  
Abstract: This paper appeared in 1994 in Cognitive Science, 18 (3): 479-511. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anantharaman, T., Campbell, M. & Hsu, F. </author> <year> (1988). </year> <title> Singular Extensions: Adding Selectivity to Brute Force Searching. </title> <booktitle> In Proceedings of the AAAI Symposium on Computer Game Playing. AAAI 1988 Spring Symposium Series. </booktitle> <pages> 8-13. </pages>
Reference: <author> Anderson, J. R. </author> <year> (1983). </year> <title> Acquisition of Proof Skills in Geometry. </title> <editor> In R. S. Michalski, J. </editor> <publisher> G. </publisher>
Reference: <editor> Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (pp. </booktitle> <pages> 191-219). </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
Reference: <author> Bell, R. C. </author> <year> (1969). </year> <title> Board and Table Games from Many Civilizations . London: </title> <publisher> Oxford University Press. </publisher>
Reference: <author> Berliner, H. & Ebeling, C. </author> <year> (1989). </year> <title> Pattern Knowledge and Search: The SUPREM Architecture. </title> <journal> Artificial Intelligence, </journal> <volume> 38(2), </volume> <pages> 161-198. </pages>
Reference: <author> Brooks, R. A. </author> <year> (1991). </year> <title> Intelligence without Representation. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 47(1-3), 139-160. </pages>
Reference-contexts: At least ten tests were run with each game; many of the games have been learned hundreds of times under varying conditions. Multiple expertise: The Advisors constitute multiple, if imperfect, experts. Their organization into what Brooks calls a subsumption hierarchy accounts for FORRs modularity <ref> (Brooks, 1991) </ref>. One can specify a new Advisor with very minimal consideration of its impact on the control structure. A new Advisor need only be established in an appropriate tier. <p> FORRs most radical feature is its reactivity. For the premise that true intelligent behavior arises out of a unified reasoning system (Carbonell, et al., 1991), it substitutes the idea that the responses from many small individual agents can be coordinated reflexively to simulate intelligent decision making <ref> (Brooks, 1991) </ref>. FORR differs from the classical reactive system, however, in its insistence on explicit concept representation (Epstein, 1992c). Most general problem solving and learning architectures like PRODIGY, SOAR, and Theo are designed to learn how to specialize themselves to solve problems.
Reference: <author> Carbonell, J. G., Knoblock, C. A. & Minton, S. N. </author> <year> (1991). </year> <title> PRODIGY: An Integrated Architecture for Planning and Learning. </title> <editor> In K. VanLehn (Ed.), </editor> <booktitle> Architectures for Intelligence (pp. </booktitle> <pages> 241-278). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: Finally, FORR does not just respond to the current state of the world; it relies on accumulated useful knowledge to comment and to decide. FORRs most radical feature is its reactivity. For the premise that true intelligent behavior arises out of a unified reasoning system <ref> (Carbonell, et al., 1991) </ref>, it substitutes the idea that the responses from many small individual agents can be coordinated reflexively to simulate intelligent decision making (Brooks, 1991). FORR differs from the classical reactive system, however, in its insistence on explicit concept representation (Epstein, 1992c). <p> SOAR does forward chaining in multiple problem spaces and then incrementally compiles search control decisions from traces of problem solving as production rules called chunks (Laird, Rosenbloom, & Newell, 1987). PRODIGY uses means ends analysis in a search space and then learns explanations that select, reject, or prefer rules <ref> (Carbonell, et al., 1991) </ref>. Theo tries an ordered list of inference methods in sequence to learn slot values (Mitchell, et al., 1991).
Reference: <author> Charness, N. </author> <year> (1991). </year> <title> Expertise in Chess: The Balance between Knowledge and Search. </title> <editor> In K. </editor> <publisher> A. </publisher>
Reference-contexts: Instead, an instantiated weak theory is precisely those processes and knowledge specific to the particular domain that human experts have been observed to have (Ericsson & Smith, 1991). The deliberate omission of extensive search during decision making is consistent with the general problemsolving literature on human behavior <ref> (Charness, 1991) </ref>. Both induction and the instantiation of the useful knowledge frame are also supported by the literature (Holyoak, 1991). Data from recent studies support the hypothesis that experts, unlike novices, process multiple solution constraints in parallel rather than test and reject hypotheses serially (Novick & Cot, 1992). <p> Hoyles significant states, and all the Advisors that reference them, associate a solution method as part of the immediate comprehension of the task, as human experts do (Ericsson, et al., 1991). The Hoyle Advisor on openings simulates observed human behavior too <ref> (Charness, 1991) </ref>. What is clearly lacking from Hoyle is knowledge representations relevant to perceptual chunks, for both storage and analysis. That is now being addressed in current research.
Reference: <author> Ericsson, & J. Smith (Ed.), </author> <title> Toward a General Theory of Expertise - Prospects and Limits (pp. </title> <address> 39-63). Cambridge: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Clancey, W. J. </author> <year> (1993). </year> <title> Situated Action: A Neuropsychological Interpretation (Response to Vera and Simon). </title> <journal> Cognitive Science, </journal> <volume> 17(1), </volume> <pages> 87-116. </pages>
Reference-contexts: This is intended to produce a broader expertise, not for a particular task, but for the problem class as a whole. Although it has Advisors instead of sensors, FORRs control approximates what Clancey has termed a dialectic coupling of sensorimotor systems in an ongoing sequence of coordination <ref> (Clancey, 1993) </ref>. The FORR Architecture - Epstein Page 14 DISCUSSION FORRs principal strengths are its smooth integration of multiple expertise, its ability to learn many ways, its tolerance for human and machine error, its graceful degradation, its transparency, and its support for the developmental paradigm.
Reference: <author> Collins, G., Birnbaum, L. & Krulwich, B. </author> <year> (1989). </year> <title> An Adaptive Model of Decision-Making in Planning. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> 511-516. </pages>
Reference-contexts: Unlike Hoyle, which attempts to learn from every contest, Tadepallis program learns only when one of its plans fails, and cannot use symmetry. Collins and others have explored adaptive decision making in tic-tac-toe and chess to learn Advisors, part of the weak theory <ref> (Collins, Birnbaum, & Krulwich, 1989) </ref>. Flann has described a knowledge-compilation method to learn problem-instance/best-action pairs that cover much of a game subtree with abstractions (Flann, 1992).
Reference: <author> Conry, S. E., MacIntosh, D. J. & Meyer, R. A. </author> <year> (1990). </year> <title> DARES: A Distributed Automated REasoning System. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press, </publisher> <pages> 78-85. </pages>
Reference-contexts: The plan, however, is partially predetermined and partially learned, and agents take only minimal directions, in the form of restricted options on the blackboard, from each other. Each Hoyle agent spends most of its time in computation rather than communication, as do those in DARES <ref> (Conry, MacIntosh, & Meyer, 1990) </ref>. Unlike DARES, however, lack of direct communication frees Hoyles individual agents to use powerful, even idiosyncratic, knowledge representations that support efficient reasoning from a particular viewpoint.
Reference: <author> Corkill, D. D. & Lesser, V. R. </author> <year> (1983). </year> <title> The Use of MetaLevel Control for Coordination in a Distributed Problem Solving Network. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Kaufmann, </publisher> <pages> 748-756. </pages>
Reference-contexts: This coordination relies upon a high-level strategic plan for advice sharing, similar in spirit to that of Corkill and Lesser <ref> (Corkill & Lesser, 1983) </ref>. The plan, however, is partially predetermined and partially learned, and agents take only minimal directions, in the form of restricted options on the blackboard, from each other.
Reference: <author> DAndrade, R. G. </author> <year> (1991). </year> <title> Culturally Based Reasoning. </title> <editor> In A. Gellatly, & D. Rogers (Ed.), </editor> <title> Cognition and Social Worlds Oxford: </title> <publisher> Clarendon Press. </publisher>
Reference-contexts: When absolute expertise is computationally intractable, as it often is in interesting problem classes, the human heuristic alternative is to be an expert, i.e., to perform better than most people. Better than most people means both faster and with a more highly-valued final outcome <ref> (DAndrade, 1991) </ref>. Expert programs, however, have traditionally had another constraint imposed upon them: that they make explicit the knowledge that supports their expert performance. A FORR-based program is intended to become an expert, not necessarily to develop absolute expertise.
Reference: <author> Dennett, D. C. & Kinsbourne, M. </author> <year> (1992). </year> <title> Time and the Observer: The Where and When of Consciousness in the Brain. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 15, </volume> <pages> 183-247. </pages> <note> The FORR Architecture - Epstein Page 32 Durfee, </note> <author> E. H. & Montgomery, T. A. </author> <year> (1990). </year> <title> A Hierarchical Protocol for Coordinating Multiagent Behaviors. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press, </publisher> <pages> 86-93. </pages>
Reference-contexts: Data from recent studies support the hypothesis that experts, unlike novices, process multiple solution constraints in parallel rather than test and reject hypotheses serially (Novick & Cot, 1992). Recent evidence for a parallel, rather than a serial, stream of consciousness, supports this approach too <ref> (Dennett & Kinsbourne, 1992) </ref>. Although Hoyles second tier is implemented on a serial machine, it effectively parallelizes this segment of the decision process. Empirical studies of expertise also support the cognitive plausibility of the choice of games as a test domain and portions of Hoyles weak method.
Reference: <author> Epstein, S. L. </author> <year> (1990). </year> <title> Learning Plans for Competitive Domains. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <editor> Ed. B. W. Porter, & R. J. Mooney. </editor> <publisher> Morgan Kaufmann, </publisher> <pages> 190-197. </pages>
Reference-contexts: and only if one side occupies 8 and 17 when 7, 9, 12, 13, 16, and 18 are empty. (The contents of the other positions are irrelevant.) The plan is known as a depth 3 fork, because it simultaneously threatens more than one fork, not just more than one mill <ref> (Epstein, 1990) </ref>. <p> Table 3 shows a hypothetical instantiation of the useful knowledge frame for tic-tac-toe. A significant state is a game state from which there is a certain winner, even though the contest is not yet over; a fork is a meta-plan for multiple ways to win <ref> (Epstein, 1990) </ref>.
Reference: <author> Epstein, S. L. </author> <year> (1992a). </year> <title> Learning Expertise from the Opposition - The Role of the Trainer in a Competitive Environment. </title> <booktitle> In Proceedings of the Ninth Canadian Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufman, </publisher> <pages> 236-243. </pages>
Reference-contexts: Reliance on experience driven by an external expert can be limiting. Recent work has demonstrated the substantial impact that the nature of the Expert Model can have on what is learned, how quickly it is learned, and how useful that learned knowledge is <ref> (Epstein, 1992a) </ref>. As it acquires useful knowledge, for example, Hoyles play may become somewhat routine and unadventurous. It may appear to have learned to play expertly, but an opponent who makes different, perhaps less skilled, moves could still readily defeat it.
Reference: <author> Epstein, S. L. </author> <year> (1992b). </year> <title> Prior Knowledge Strengthens Learning to Control Search in Weak Theory Domains. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 7, </volume> <pages> 547-586. </pages>
Reference-contexts: This Appendix describes Hoyles weak theory for two-person, perfect information, finite board games. Additional information and the learning algorithms are available in <ref> (Epstein, 1992b) </ref>. Hoyles Problem-Class Definition Hoyle defines a game as any instantiation of its problem class definition. In Table 1 the definition has been instantiated for tic-tac-toe. The definition is the slot names; the instantiation is the slot values and their association with those slots.
Reference: <author> Epstein, S. L. </author> <year> (1992c). </year> <title> The Role of Memory and Concepts in Learning. </title> <journal> Minds and Machines, </journal> <volume> 2, </volume> <pages> 239-265. </pages>
Reference-contexts: Four kinds of human expert knowledge form the core of a weak theory. (For the philosophical underpinnings, see <ref> (Epstein, 1992c) </ref>.) Problem solving knowledge: good reasons for making decisions in the domain and a behavioral pattern for how to proceed when working on any task there. <p> FORR differs from the classical reactive system, however, in its insistence on explicit concept representation <ref> (Epstein, 1992c) </ref>. Most general problem solving and learning architectures like PRODIGY, SOAR, and Theo are designed to learn how to specialize themselves to solve problems.
Reference: <author> Epstein, S. L. </author> <year> (1993). </year> <title> The Hoyle Training Experiments (CS-TR-93-01). </title> <institution> Department of Computer Science, Hunter College of The City University of New York. </institution>
Reference-contexts: The discussion below is drawn from several years of laboratory experiments with Hoyle, during which it has learned to play 18 games extremely well. (See, for example, <ref> (Epstein, 1993) </ref> and (Epstein, 1994).) These experiments have been directed to a variety of issues in machine learning. At least ten tests were run with each game; many of the games have been learned hundreds of times under varying conditions. Multiple expertise: The Advisors constitute multiple, if imperfect, experts. <p> for tic-tac-toe; more complex games may also have values stored for a list of predrawn lines on the game board, an adjacency graph of positions, and functions to map back and forth between a list representation of the game board and a two-dimensional plot of the positions and their contents <ref> (Epstein, Gelfand, Lesniak, & Abadie, 1993) </ref>. <p> Only the nonempty slots are shown for tic-tac-toe; more complex games may also have values stored for dangerous states (not proved significant within the time limit but seen in the context of a loss), an associative pattern database, and some distance measurements on the two-dimensional version of the game board <ref> (Epstein, et al., 1993) </ref>.
Reference: <author> Epstein, S. L. </author> <year> (1994). </year> <title> Toward an Ideal Trainer. </title> <journal> Machine Learning, </journal> <volume> 15(3). </volume>
Reference-contexts: The discussion below is drawn from several years of laboratory experiments with Hoyle, during which it has learned to play 18 games extremely well. (See, for example, (Epstein, 1993) and <ref> (Epstein, 1994) </ref>.) These experiments have been directed to a variety of issues in machine learning. At least ten tests were run with each game; many of the games have been learned hundreds of times under varying conditions. Multiple expertise: The Advisors constitute multiple, if imperfect, experts.
Reference: <author> Epstein, S. L., Gelfand, J., Lesniak, J. & Abadie, P. </author> <year> (1993). </year> <title> The Integration of Visual Cues into a Multiple-Advisor Game-Learning Program. </title> <booktitle> In Proceedings of the AAAI Fall Symposium on Games. </booktitle> <pages> 92-100. </pages>
Reference-contexts: The discussion below is drawn from several years of laboratory experiments with Hoyle, during which it has learned to play 18 games extremely well. (See, for example, <ref> (Epstein, 1993) </ref> and (Epstein, 1994).) These experiments have been directed to a variety of issues in machine learning. At least ten tests were run with each game; many of the games have been learned hundreds of times under varying conditions. Multiple expertise: The Advisors constitute multiple, if imperfect, experts. <p> for tic-tac-toe; more complex games may also have values stored for a list of predrawn lines on the game board, an adjacency graph of positions, and functions to map back and forth between a list representation of the game board and a two-dimensional plot of the positions and their contents <ref> (Epstein, Gelfand, Lesniak, & Abadie, 1993) </ref>. <p> Only the nonempty slots are shown for tic-tac-toe; more complex games may also have values stored for dangerous states (not proved significant within the time limit but seen in the context of a loss), an associative pattern database, and some distance measurements on the two-dimensional version of the game board <ref> (Epstein, et al., 1993) </ref>.
Reference: <author> Ericsson, K. A. & Smith, J. </author> <year> (1991). </year> <title> Prospects and Limits of the Empirical Study of Expertise: An Introduction. </title> <editor> In K. A. Ericsson, & J. Smith (Ed.), </editor> <title> Toward a General Theory of Expertise - Prospects and Limits (pp. </title> <address> 1-38). Cambridge: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: There is also evidence against a single, correct human way to be an expert (Holyoak, 1991). Instead, an instantiated weak theory is precisely those processes and knowledge specific to the particular domain that human experts have been observed to have <ref> (Ericsson & Smith, 1991) </ref>. The deliberate omission of extensive search during decision making is consistent with the general problemsolving literature on human behavior (Charness, 1991). Both induction and the instantiation of the useful knowledge frame are also supported by the literature (Holyoak, 1991). <p> Hoyles significant states, and all the Advisors that reference them, associate a solution method as part of the immediate comprehension of the task, as human experts do <ref> (Ericsson, et al., 1991) </ref>. The Hoyle Advisor on openings simulates observed human behavior too (Charness, 1991). What is clearly lacking from Hoyle is knowledge representations relevant to perceptual chunks, for both storage and analysis. That is now being addressed in current research.
Reference: <author> Esfahany, K. </author> <title> A Pattern Classifier that Learns to Play Games. </title> <note> In preparation. </note>
Reference: <author> Flann, N. </author> <year> (1990). </year> <title> Applying Abstraction and Simplification to Learn in Intractable Domains. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> 277-285. </pages>
Reference-contexts: The resultant incomplete theory is different from a weak theory in that it does not purport to cover the entire search space of a problem class, as a weak theory does, and so will find itself at a loss about 20% of the time <ref> (Flann, 1990) </ref>. Overall, Hoyle certainly begins with more prior general knowledge than these four game-learning systems.
Reference: <author> Flann, N. S. </author> <year> (1992). </year> <title> Correct Abstraction in Counter-Planning: A Knowledge Compilation Approach. </title> <type> Ph.D., </type> <institution> Oregon State University, </institution> <note> Flax, </note> <author> M. G., Gelfand, J. J., Lane, S. H. & Handelman, D. A. </author> <year> (1992). </year> <title> Integrating Neural Network and Tree Search Approaches to Produce an AutoSupervised System that Learns to Play Games. </title> <booktitle> In Proceedings of the 1992 International Joint Conference on Neural Networks. </booktitle>
Reference-contexts: Collins and others have explored adaptive decision making in tic-tac-toe and chess to learn Advisors, part of the weak theory (Collins, Birnbaum, & Krulwich, 1989). Flann has described a knowledge-compilation method to learn problem-instance/best-action pairs that cover much of a game subtree with abstractions <ref> (Flann, 1992) </ref>. The resultant incomplete theory is different from a weak theory in that it does not purport to cover the entire search space of a problem class, as a weak theory does, and so will find itself at a loss about 20% of the time (Flann, 1990).
Reference: <author> Holyoak, K. J. </author> <year> (1991). </year> <title> Symbolic Connectionism: Toward Third-Generation Theories. </title> <editor> In K. </editor> <publisher> A. </publisher>
Reference-contexts: The FORR Architecture - Epstein Page 17 COGNITIVE PLAUSIBILITY Empirical studies of expertise support the cognitive plausibility of the FORR architecture. Absolute expertise is not an issue: an expert is someone capable of doing the right thing at the right time <ref> (Holyoak, 1991) </ref>. There is also evidence against a single, correct human way to be an expert (Holyoak, 1991). Instead, an instantiated weak theory is precisely those processes and knowledge specific to the particular domain that human experts have been observed to have (Ericsson & Smith, 1991). <p> Absolute expertise is not an issue: an expert is someone capable of doing the right thing at the right time <ref> (Holyoak, 1991) </ref>. There is also evidence against a single, correct human way to be an expert (Holyoak, 1991). Instead, an instantiated weak theory is precisely those processes and knowledge specific to the particular domain that human experts have been observed to have (Ericsson & Smith, 1991). <p> The deliberate omission of extensive search during decision making is consistent with the general problemsolving literature on human behavior (Charness, 1991). Both induction and the instantiation of the useful knowledge frame are also supported by the literature <ref> (Holyoak, 1991) </ref>. Data from recent studies support the hypothesis that experts, unlike novices, process multiple solution constraints in parallel rather than test and reject hypotheses serially (Novick & Cot, 1992). Recent evidence for a parallel, rather than a serial, stream of consciousness, supports this approach too (Dennett & Kinsbourne, 1992).
Reference: <author> Ericsson, & J. Smith (Ed.), </author> <title> Toward a General Theory of Expertise - Prospects and Limits (pp. </title> <address> 301-336). Cambridge: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Laird, J. E., Rosenbloom, P. S. & Newell, A. </author> <year> (1987). </year> <title> SOAR: An Architecture for General Intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33(1), </volume> <pages> 1-64. </pages> <note> The FORR Architecture - Epstein Page 33 Lenat, </note> <author> D. B. </author> <year> (1976). </year> <title> AM: An Artificial Intelligence Approach to Discovery in Mathematics. </title> <type> Ph.D., </type> <institution> Department of Computer Science, Stanford University, </institution> <note> Lenat, </note> <author> D. B. </author> <year> (1983). </year> <title> EURISKO: A Program That Learns New Heuristics and Domain Concepts. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 26(1-2), 61-98. </pages>
Reference-contexts: Most general problem solving and learning architectures like PRODIGY, SOAR, and Theo are designed to learn how to specialize themselves to solve problems. SOAR does forward chaining in multiple problem spaces and then incrementally compiles search control decisions from traces of problem solving as production rules called chunks <ref> (Laird, Rosenbloom, & Newell, 1987) </ref>. PRODIGY uses means ends analysis in a search space and then learns explanations that select, reject, or prefer rules (Carbonell, et al., 1991). Theo tries an ordered list of inference methods in sequence to learn slot values (Mitchell, et al., 1991).
Reference: <author> Levesque, H. J., Cohen, P. R. & Nunes, J. H. T. </author> <year> (1990). </year> <title> On Acting Together. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press, </publisher> <pages> 94-99. </pages>
Reference-contexts: Hoyles agents do not negotiate; they act together because the centralized blackboard control strategy forces their collaboration when it combines their advice into a single choice recommendation <ref> (Levesque, Cohen, & Nunes, 1990) </ref>. This coordination relies upon a high-level strategic plan for advice sharing, similar in spirit to that of Corkill and Lesser (Corkill & Lesser, 1983).
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A Theory and Methodology of Inductive Learning. </title> <editor> In R. S. Michalski, </editor> <publisher> J. </publisher>
Reference-contexts: The definition of a skill domain incorporates Michalskis description of a process learned by repeated practice and correction of deviations from some desired behavior, but does not restrict it to nonsymbolic information, subconscious processes, or motor skills <ref> (Michalski, 1983) </ref>. Weak theory Problem class TaskTask Problem class TaskTask Weak theory Problem class TaskTask Problem class TaskTask Weak theory Problem class TaskTask Problem class TaskTask Commonsense A weak theory is broad general knowledge theoretically applicable to each problem class in a skill domain.
Reference: <editor> G. Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (pp. </booktitle> <pages> 83-134). </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
Reference: <author> Minton, S. </author> <year> (1984). </year> <title> Constraint-Based Generalization - Learning Game-Playing Plans from Single Examples. </title> <booktitle> In Proceedings of the Fourth National Conference on Artificial Intelligence. </booktitle> <publisher> William Kaufmann, </publisher> <pages> 251-254. </pages>
Reference-contexts: There has been much work in general methods for learning about how to play games. Minton used constraint-based generalization to deduce winning combinations from a single example in tic-tactoe, go-moku, and chess <ref> (Minton, 1984) </ref>. His program learned segments of Hoyles Advisor Pitchfork. Tadepallis program naively constructs king-pawn endgame move sequences in chess that are overly general, and then refines them when they fail (Tadepalli, 1989). These plans are more general than Hoyles highly restricted ones.
Reference: <author> Mitchell, T., Allen, J., Chalasani, P., Cheng, J., Etzioni, O., Ringuette, M. N. & Schlimmer, </author> <note> J. </note>
Reference: <author> C. </author> <year> (1991). </year> <title> Theo: A Framework for Self-Improving Systems. </title> <editor> In K. VanLehn (Ed.), </editor> <booktitle> Architectures for Intelligence (pp. </booktitle> <pages> 323-356). </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Nii, H. P. </author> <year> (1986). </year> <title> Blackboard Systems: The Blackboard Model of Problem Solving and the Evolution of Blackboard Architectures. </title> <journal> AI Magazine, </journal> <volume> 7(2), </volume> <pages> 38-53. </pages>
Reference-contexts: Decision making in FORR, shown in Figure 3, models the features identified for skill domain decisions. The behavioral script activates the problem class definition to post all the current permissible actions in a temporary knowledge repository or blackboard <ref> (Nii, 1986) </ref>. The behavioral script then forwards the current problem state to the decision-making algorithm which posts the current problem state and the useful knowledge for the current problem class on the blackboard.
Reference: <author> Novick, L. R. & Cot, N. </author> <year> (1992). </year> <title> The Nature of Expertise in Anagram Solution. </title> <booktitle> In Proceedings of the 14th Annual Conference of the Cognitive Science Society. </booktitle> <publisher> Lawrence Erlbaum, </publisher> <pages> 450-455. </pages>
Reference-contexts: Both induction and the instantiation of the useful knowledge frame are also supported by the literature (Holyoak, 1991). Data from recent studies support the hypothesis that experts, unlike novices, process multiple solution constraints in parallel rather than test and reject hypotheses serially <ref> (Novick & Cot, 1992) </ref>. Recent evidence for a parallel, rather than a serial, stream of consciousness, supports this approach too (Dennett & Kinsbourne, 1992). Although Hoyles second tier is implemented on a serial machine, it effectively parallelizes this segment of the decision process.
Reference: <author> Painter, J. </author> <year> (1993). </year> <title> Pattern Recognition for Decision Making in a Competitive Environment. </title> <type> Masters, </type> <institution> Hunter College of the City University of New York, </institution> <note> Rosenbloom, </note> <author> P. S., Newell, A. & Laird, J. E. </author> <year> (1991). </year> <title> Toward the Knowledge Level in Soar: The Role of the Architecture in the Use of Knowledge. </title> <editor> In K. VanLehn (Ed.), </editor> <booktitle> Architectures for Intelligence (pp. </booktitle> <pages> 75-112). </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Samuel, A. L. </author> <year> (1963). </year> <title> Some Studies in Machine Learning Using the Game of Checkers. </title> <editor> In E. </editor> <publisher> A. </publisher>
Reference: <editor> Feigenbaum, & J. Feldman (Ed.), </editor> <booktitle> Computers and Thought (pp. </booktitle> <pages> 71-105). </pages> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Schraagen, J. M. </author> <year> (1993). </year> <title> How Experts Solve a Novel Problem in Experimental Design. </title> <journal> Cognitive Science, </journal> <volume> 17(2), </volume> <pages> 285-309. </pages>
Reference-contexts: This also closely correlates with the types of knowledge (domain knowledge, heuristic strategies, The FORR Architecture - Epstein Page 11 control strategies, and learning strategies) Schraagen describes as required for expertise <ref> (Schraagen, 1993) </ref>. CONTROL STRATEGY FOR CONFLICT RESOLUTION To explain control in depth, it helpful to consider an implementation. Hoyle is a FORR-based program whose skill domain is two-person, perfect information, finite board games. The boards need not be square, or grid-like, or even two-dimensional.
Reference: <author> Soloway, E. </author> <year> (1978). </year> <title> Learning = Interpretation + Generalization: A Case Study in Knowledge-Directed Learning. </title> <type> Ph.D., </type> <institution> University of Massachusetts, </institution> <note> Sussman, </note> <author> G. J. </author> <year> (1975). </year> <title> A Computer Model of Skill Acquisition . New York: </title> <publisher> American Elsevier. </publisher>
Reference-contexts: BASEBALL used general knowledge, such as how to focus on traces and the nature of competition and cooperation, The FORR Architecture - Epstein Page 23 to extract and abstract general concepts, like hit and out, from the training instances <ref> (Soloway, 1978) </ref>. The program, applied only to baseball, embodied a declaratively represented weak theory about certain kinds of games in its analysis, while Hoyles is both declarative and procedural.
Reference: <author> Tadepalli, P. </author> <year> (1989). </year> <title> Lazy Explanation-Based Learning: A Solution to the Intractable Theory Problem. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> 694-700. </pages> <note> The FORR Architecture - Epstein Page 34 Tesauro, </note> <author> G. </author> <year> (1992). </year> <title> Practical Issues in Temporal Difference Learning. </title> <journal> Machine Learning, </journal> <volume> 8(3/4), </volume> <pages> 257-277. </pages>
Reference-contexts: Minton used constraint-based generalization to deduce winning combinations from a single example in tic-tactoe, go-moku, and chess (Minton, 1984). His program learned segments of Hoyles Advisor Pitchfork. Tadepallis program naively constructs king-pawn endgame move sequences in chess that are overly general, and then refines them when they fail <ref> (Tadepalli, 1989) </ref>. These plans are more general than Hoyles highly restricted ones. Unlike Hoyle, which attempts to learn from every contest, Tadepallis program learns only when one of its plans fails, and cannot use symmetry.
Reference: <author> Zaslavsky, C. </author> <year> (1982). </year> <title> Tic Tac Toe and Other Three-in-a-Row Games, from Ancient Egypt to the Modern Computer . New York: </title> <publisher> Crowell. </publisher>
References-found: 44

