URL: http://www.pdos.lcs.mit.edu/~engler/filaments-93.ps
Refering-URL: http://www.pdos.lcs.mit.edu/~engler/
Root-URL: 
Title: The Design and Implementation of Efficient Thread-Based Parallelism  
Author: Dawson R. Engler 
Date: Febuary 8 1994  
Affiliation: M.I.T. Laboratory of Computer Science  
Abstract: Efficient thread management allows operating systems and applications to effectively exploit parallelism. To aid designers in implementing an efficient thread package we record the evolution of a traditional stack-based thread package to a highly optimized "stateless" threads package. The effects of each optimization are discussed and quantified. Compared to the initial package, thread creation is made an order of magnitude more efficient, and thread execution 12-40 times faster, depending on the type of application. The final result is a new thread package, called Filaments. The cost of running a filament taking three arguments is approximately 6 instructions; creation is approximately 12. In addition, by exploiting the semantics of stateless threads, software based multithreading can be implemented with an efficiency competitive to a hardware implementation. Finally, Filaments is very portable; unlike other thread packages, it is completely written in a high-level language.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Agarwal et al. </author> <title> The MIT Alewife machine: A large-scale distributed-memory multiprocessor. </title> <type> Technical Report MIT/LCS TM-454, </type> <institution> M.I.T., </institution> <year> 1991. </year>
Reference-contexts: The need for efficient multithreading is great enough that several machines provide hardware support. For example, the MIT J-machine can switch between a restricted thread class in 2-5 cycles [13]. The MIT Alewife machine, also supports very efficient context-switching but, again, only within a limited number of threads <ref> [1] </ref>. In this section we present a software solution to the multithreading problem that exploits the semantics of stateless threads to multithread in time comparable to a hardware solution, eliminating the need to go to an exotic machine.
Reference: [2] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Scheduler activations: effective kernel support for the user-level management of parallelism. </title> <booktitle> In Proceedings of the ThirteenthACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 95-109, 91. </pages>
Reference-contexts: Two important results of stateless threads are the ability to completely implement a thread package in a high level language and to have the cost of software based multithreading be comparable to that of a hardware implementation. Many papers have discussed how to make threads efficient (e.g., [3], <ref> [2] </ref> and [30]) Our work synthesizes previous work on making threads efficient and extends it in several novel ways. Optimizations occur within a unified framework that allows their relative merits to be judged. <p> Several researchers have proposed ways to reduce the inefficiencies of standard thread packages. Anderson et al. [3] discusses the gain from using local ready queues, and <ref> [2] </ref> shows how to do user-level scheduling [2]. Markatos et al. [25] presents a thorough study of the tradeoffs between load balancing and locality in shared memory machines. <p> Several researchers have proposed ways to reduce the inefficiencies of standard thread packages. Anderson et al. [3] discusses the gain from using local ready queues, and <ref> [2] </ref> shows how to do user-level scheduling [2]. Markatos et al. [25] presents a thorough study of the tradeoffs between load balancing and locality in shared memory machines. Schoenberg and Hummel [19] explain how to avoid allocating a stack per thread and context switching in nested parallel for loops, which are a form of run-to-completion threads.
Reference: [3] <author> T. E. Anderson, E. D. Lazowska, and H. M. Levy. </author> <title> The performance implications of thread management alternatives for shared memory multiprocessors. </title> <journal> IEEE Transactions on Computers 38, </journal> <volume> 12 </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year> <month> 19 </month>
Reference-contexts: Two important results of stateless threads are the ability to completely implement a thread package in a high level language and to have the cost of software based multithreading be comparable to that of a hardware implementation. Many papers have discussed how to make threads efficient (e.g., <ref> [3] </ref>, [2] and [30]) Our work synthesizes previous work on making threads efficient and extends it in several novel ways. Optimizations occur within a unified framework that allows their relative merits to be judged. <p> The disadvantage is that two linked list traversals are now necessary instead of one: the first to obtain threads, the second to run them. The Ultracomputer project used a similar mechanism [17]. Local queue: Central queues are a sequentializing bottleneck for the creation and execution of threads <ref> [3] </ref>. Furthermore, on machines that have a gross difference in speed between the CPU and memory, locality must be explicit for reasonable performance [25]. <p> The effect of the added padding increases with the number of servers because of reduced contention. This optimization is simple, but without it a subtle performance bug is incurred when the size of frequently indexed queue structures is not a power of two. Lazy stack is an optimization from <ref> [3] </ref>. At thread creation there is no need to immediately associate a thread descriptor with a stack. This association is necessary only at thread execution. <p> Several researchers have proposed ways to reduce the inefficiencies of standard thread packages. Anderson et al. <ref> [3] </ref> discusses the gain from using local ready queues, and [2] shows how to do user-level scheduling [2]. Markatos et al. [25] presents a thorough study of the tradeoffs between load balancing and locality in shared memory machines.
Reference: [4] <author> T.E. Anderson, H.M. Levy, B.N. Bershad, and E.D. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Proc. Fourth International Conference on ASPLOS, </booktitle> <year> 1991. </year>
Reference-contexts: As RISC machines evolve context-switching is becoming more costly. The processor state is increasing as processors have more registers. Additionally, storing and loading values from main memory is getting 7 more expensive relative to CPU cycle time [27], <ref> [4] </ref>. * High Code complexity. To create, switch between, and terminate threads the thread package must explicitly manipulate thread stacks. While the code necessary to perform these tasks is short, it is notoriously difficult to get correct, as it has to deal with the idiosyncrasies of the underlying architecture. <p> An important part of this transition is the use of stateless threads, the advantages of which are: * No context-switching. Context-switching performance, because of larger register sets and slow memory speeds, is not scaling with integer CPU performance [27], <ref> [4] </ref>. By using stateless threads, this bottleneck is avoided. * Low memory consumption. By using one system-managed stack, stateless threads consume far less memory than a traditional thread package. This results in better caching and TLB performance. * Portability.
Reference: [5] <author> G. R. Andrews et al. </author> <title> An overview of the SR language and implementation. </title> <journal> ACM Transactions Programming Language System, </journal> <volume> 10 </volume> <pages> 51-86, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Trading ease of programming for performance is a decision that is often applied; for example, a weakly consistent memory [15], [7]. A compiler is sometimes used to implement the contract. We applied the optimizations to a traditional stack-based thread package that was extracted from the SR language's runtime system <ref> [5] </ref>. The result is a novel thread package: Filaments. Filaments is a stateless thread package, only allowing blocking primitives that do not require a context-switch. Although Filaments is less general than traditional packages, in practice almost all parallel programs can be easily written using these more restrictive threads.
Reference: [6] <author> B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> PRESTO: a system for object-oriented parallel programming. </title> <journal> Software|Practice and Experience, </journal> <volume> 18 </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The first step towards efficient parallelism was lightweight thread packages such as 16 Threads [14], Presto <ref> [6] </ref>, uSystem [9], C Threads [11], uC++ [8] and Sun Lightweight Processes [28]. We will call these standard packages to distinguish them from thread packages that do not have a stack for each thread. <p> Chores requires preprocessor support and the use of task generators in order to cluster fine-grained tasks into coarse-grain units. Two other drawbacks of the Chores approach are that extreme irregularity may make the coarse-grained tasks perform poorly, and the use of traditional threads (specifically Presto <ref> [6] </ref>) to implement a given chore makes fine-grained chores inefficient. Finally, the concept of a thread barrier requiring time proportional to the number of servers and not the number of threads was developed independently David Keppel [22].
Reference: [7] <author> B.N. Bershad and M.J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, CMU, </type> <year> 1991. </year>
Reference-contexts: The latter optimization greatly improves performance, but can only be applied when the programmer follows certain rules of threads usage. Trading ease of programming for performance is a decision that is often applied; for example, a weakly consistent memory [15], <ref> [7] </ref>. A compiler is sometimes used to implement the contract. We applied the optimizations to a traditional stack-based thread package that was extracted from the SR language's runtime system [5]. The result is a novel thread package: Filaments.
Reference: [8] <author> Peter A. Buhr, Glen Ditchfield, R. A. Stroobosscher, and B. M. Younger. </author> <title> uC++: concur-rency in the object oriented language C++. </title> <journal> Software|Practice and Experience, </journal> <volume> 22 </volume> <pages> 137-172, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: The first step towards efficient parallelism was lightweight thread packages such as 16 Threads [14], Presto [6], uSystem [9], C Threads [11], uC++ <ref> [8] </ref> and Sun Lightweight Processes [28]. We will call these standard packages to distinguish them from thread packages that do not have a stack for each thread.
Reference: [9] <author> Peter A. Buhr and R. A. Stroobosscher. </author> <title> The uSystem: providing light-weight concurrency on shared memory multiprocessor computers running unix. </title> <journal> Software|Practice and Experience, </journal> <volume> 20 </volume> <pages> 929-964, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The first step towards efficient parallelism was lightweight thread packages such as 16 Threads [14], Presto [6], uSystem <ref> [9] </ref>, C Threads [11], uC++ [8] and Sun Lightweight Processes [28]. We will call these standard packages to distinguish them from thread packages that do not have a stack for each thread.
Reference: [10] <author> Fred C. Chow. </author> <title> Minimizing register usage penalty at procedure calls. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 85-94, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The upshot is that there can be 4-8 unnecessary loads and stores at every function entrance and exit. These memory references can be eliminated using "shrink-wrapping" <ref> [10] </ref>. Load-balance: The final optimization is to reintroduce load-balancing for RTC and barrier threads.
Reference: [11] <author> Eric C. Cooper and Richard P. Draves. </author> <title> C threads. Internal note of the Mach research project, </title> <month> September 11 </month> <year> 1990. </year>
Reference-contexts: The first step towards efficient parallelism was lightweight thread packages such as 16 Threads [14], Presto [6], uSystem [9], C Threads <ref> [11] </ref>, uC++ [8] and Sun Lightweight Processes [28]. We will call these standard packages to distinguish them from thread packages that do not have a stack for each thread.
Reference: [12] <author> David E. Culler and et al. </author> <title> TAM|a compiler controlled threaded abstract machine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 347-370, </pages> <year> 1993. </year>
Reference-contexts: WorkCrews [30] supports fork/join parallelism on small-scale, shared-memory multiprocessors. The package is implemented in Modula2+. WorkCrews introduced the concept of pruning and of ordering queues to favor larger threads. Filaments has borrowed these ideas in its implementation of fork/join threads. TAM <ref> [12] </ref> is a compiler-controlled threaded abstract machine. It evolved from graph-based ex 17 ecution models for dataflow languages and provides a bridge between such models and the control flow models typically employed by standard multiprocessors. <p> We believe that removing the need for sophisticated compilers is an important advantage of Filaments, in that it allows Filaments to be easily ported to new machines with minimal effort. A TAM thread is a sequence of instructions which executes from beginning to end without suspension or branching <ref> [12] </ref>. In contrast, Filaments can include any number of control flow changes and blocking primitives as long as none necessitate a context-switch. This weaker constraint allows Filaments to have larger tasks and thus the potential for more efficiency. <p> For example, an implementation of quick-sort has 60.3 percent of its instructions devoted to "control" <ref> [12] </ref>. To be fair, this is a result of the TAM model being more ambitious than Filaments in that it is intended for use in message-based parallel computers and allows explicit management of multiphase transactions by the runtime system.
Reference: [13] <author> William J. Dally et al. </author> <title> The J-Machine: A fine-grain concurrent computer. </title> <editor> In G.X. Ritter, editor, </editor> <booktitle> Proceedings of the IFIP Congress, </booktitle> <pages> pages 1147-1153, </pages> <year> 1989. </year>
Reference-contexts: Total cost is two context-switches and three queue manipulations. This overhead significantly reduces the situations in which multithreading is profitable. The need for efficient multithreading is great enough that several machines provide hardware support. For example, the MIT J-machine can switch between a restricted thread class in 2-5 cycles <ref> [13] </ref>. The MIT Alewife machine, also supports very efficient context-switching but, again, only within a limited number of threads [1].
Reference: [14] <author> Thomas W. Doeppner. </author> <title> Threads: a system for the support of concurrent programming. </title> <type> Technical Report TR CS-87-11, </type> <institution> Brown, </institution> <year> 1987. </year>
Reference-contexts: This abstraction and the elimination of all machine language are the most fundamental contributions of Filaments. 6 Related Work There is a wealth of related research on threads packages, some of which support fine-grained parallelism. The first step towards efficient parallelism was lightweight thread packages such as 16 Threads <ref> [14] </ref>, Presto [6], uSystem [9], C Threads [11], uC++ [8] and Sun Lightweight Processes [28]. We will call these standard packages to distinguish them from thread packages that do not have a stack for each thread.
Reference: [15] <author> D. Dwarkadas, P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In The 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 144-155, </pages> <year> 1993. </year>
Reference-contexts: The latter optimization greatly improves performance, but can only be applied when the programmer follows certain rules of threads usage. Trading ease of programming for performance is a decision that is often applied; for example, a weakly consistent memory <ref> [15] </ref>, [7]. A compiler is sometimes used to implement the contract. We applied the optimizations to a traditional stack-based thread package that was extracted from the SR language's runtime system [5]. The result is a novel thread package: Filaments.
Reference: [16] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> enhanced run-time support for shared-memory parallel computing. </title> <journal> Trans. on Computer Systems, </journal> <volume> 11 </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: For example, Lin and Snyder [23] found a fine-grained implementation of Jacobi iteration using the Sequent Symmetry's parallel programming library to be 8 to 23 times slower than a coarse-grain one. More recent work, such as Chores <ref> [16] </ref>, has been able to get efficient performance by automatically clustering fine-grained tasks into larger units, but executing fine-grained tasks independently is still 10 to 20 times slower, again for Jacobi iteration on a Sequent Symmetry. <p> Jacobi iteration, which consists of very few operations, is within 11% of a coarse-grained implementation on a Sequent Symmetry, and within 40% on an SGI Iris. Considering that thread-based models have been found to be over twenty times slower than a coarse-grained application for this particular problem [23], <ref> [16] </ref>, our performance is good. Fibonacci, which consists of a conditional and at most a single addition and two function calls is within a factor of two of a purely sequential implementation. For more reasonable fork/join programs (e.g. quicksort, adaptive quadrature, FFT) there is no difference at all. <p> To be fair, this is a result of the TAM model being more ambitious than Filaments in that it is intended for use in message-based parallel computers and allows explicit management of multiphase transactions by the runtime system. Chores <ref> [16] </ref>, which runs on top of Presto on a Sequent Symmetry, is another threads package that is similar to Filaments. Chores, extends the Uniform System model by coalescing iterations into large tasks (in essence deriving coarse-grained parallelism from a fine-grained specification). Additionally, Chores provides support for fork/join parallelism.
Reference: [17] <author> J. Edler, A. Gottlieb, C.P. Kruskal, K.P. McAuliffe, L. Rudolph, M. Snir, P.J. Teller, and J. Wilson. </author> <title> Issues related to MIMD shared-memory computers: the NYU Ultracomputer approach. </title> <booktitle> In Proc. Twelfth Annual International Symposium on Computer Architecture, </booktitle> <year> 1985. </year>
Reference-contexts: The disadvantage is that two linked list traversals are now necessary instead of one: the first to obtain threads, the second to run them. The Ultracomputer project used a similar mechanism <ref> [17] </ref>. Local queue: Central queues are a sequentializing bottleneck for the creation and execution of threads [3]. Furthermore, on machines that have a gross difference in speed between the CPU and memory, locality must be explicit for reasonable performance [25].
Reference: [18] <author> Dawson R. Engler, Greg R. Andrews, and David K. Lowenthal. Filaments: </author> <title> Efficient support for fine-grain parallelism. </title> <note> In preparation. </note>
Reference-contexts: Creation overhead has been reduced by approximately a factor of three, as has null execution cost. Jacobi has improved by approximately 25% and Fibonacci by 25 times. 4.5 Implementation and Performance of Filaments The performance of Filaments in relation to coarse-grained approaches is discussed in [24] and <ref> [18] </ref>. To test the overhead of Filaments relative to a coarse-grained implementation, pathological cases were examined. Jacobi iteration, which consists of very few operations, is within 11% of a coarse-grained implementation on a Sequent Symmetry, and within 40% on an SGI Iris.
Reference: [19] <author> S. F. Hummel and E. Schonberg. </author> <title> Low-overhead scheduling of nested parallelism. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 35 </volume> <pages> 743-765, </pages> <month> September </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: Anderson et al. [3] discusses the gain from using local ready queues, and [2] shows how to do user-level scheduling [2]. Markatos et al. [25] presents a thorough study of the tradeoffs between load balancing and locality in shared memory machines. Schoenberg and Hummel <ref> [19] </ref> explain how to avoid allocating a stack per thread and context switching in nested parallel for loops, which are a form of run-to-completion threads. Thread packages that support finer-grained parallelism include the Uniform System, WorkCrews, TAM and Chores.
Reference: [20] <author> SPARC International. </author> <title> The SPARC Architecture Manual. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey 07632, </address> <year> 1992. </year>
Reference-contexts: For example, on the SPARC, the code must ensure that there is always a valid user stackpointer on which hardware interrupts can be executed <ref> [20] </ref>. * Limited Portability. Because thread packages explicitly manipulate stacks and registers, they are architecture dependent. To port a thread package typically requires rewriting the most difficult part of the package using assembly instructions and requires knowing the conventions of the target machine.
Reference: [21] <author> Anna R. Karlin, Kai Li, Mark S. Manasse, and Susan Owicki. </author> <title> Empirical studies of competitive spinning for a shared-memory multiprocessor. </title> <booktitle> In Proceedings of the ThirteenthACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 41-55, 91. </pages>
Reference-contexts: For example, floating-point constants in Jacobi iteration must be loaded for every filament, rather than 14 being allocated on an inter-filament basis. 5 Exploiting Stateless Semantics Multithreading has traditionally been a slow operation. For example, <ref> [21] </ref> cites times in the hundreds of instructions. This should not be surprising since the operations to switch between two threads T and T' and back (the sequence of operations required for multithreading) involves at a minimum the following operations: 1. Enqueue T on blocked queue. 2.
Reference: [22] <author> David Keppel. </author> <title> Tools and techniques for building fast portable threads packages. </title> <type> Technical Report UWCSE93-05-06, </type> <institution> The Univ. of Washington, </institution> <year> 1993. </year>
Reference-contexts: Finally, the concept of a thread barrier requiring time proportional to the number of servers and not the number of threads was developed independently David Keppel <ref> [22] </ref>. The main difference is that there is no notion of sequential code. 18 7 Conclusion We have discussed, in detail, a series of optimizations that can be done to increase creation and execution efficiencies of three thread classes.
Reference: [23] <author> Calvin Lin and Lawrence Snyder. </author> <title> A comparison of programming models for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 163-170, </pages> <year> 1990. </year>
Reference-contexts: Fine-grained parallelism is desirable for several reasons: architecture independence, ease of programming, ease of code generation and load-balancing potential. Unfortunately, while fine-grained parallelism has attractive attributes as a programming model and a target for code generation, conventional thread packages have been too inefficient. For example, Lin and Snyder <ref> [23] </ref> found a fine-grained implementation of Jacobi iteration using the Sequent Symmetry's parallel programming library to be 8 to 23 times slower than a coarse-grain one. <p> Jacobi iteration, which consists of very few operations, is within 11% of a coarse-grained implementation on a Sequent Symmetry, and within 40% on an SGI Iris. Considering that thread-based models have been found to be over twenty times slower than a coarse-grained application for this particular problem <ref> [23] </ref>, [16], our performance is good. Fibonacci, which consists of a conditional and at most a single addition and two function calls is within a factor of two of a purely sequential implementation. For more reasonable fork/join programs (e.g. quicksort, adaptive quadrature, FFT) there is no difference at all. <p> The Uniform System also employs task generators (a related collection of tasks)|and hence has essentially a coarse-grained programming model|whereas Filaments directly supports a fine-grained model. Because of these differences, the Uniform System cannot efficiently support thread-per-point decompositions for problems like Jacobi, as Lin <ref> [23] </ref> and others have noted. Furthermore, the model of parallelism is one of loop-based, not thread-based, parallelism. Specifying function-level parallelism in such a setting is cumbersome and inefficient. WorkCrews [30] supports fork/join parallelism on small-scale, shared-memory multiprocessors. The package is implemented in Modula2+.
Reference: [24] <author> David K. Lowenthal and Dawson R. Engler. </author> <title> Performance experiments for the filaments package. </title> <type> Technical Report TR 93-26, </type> <institution> The Univ. of Arizona, </institution> <year> 1993. </year>
Reference-contexts: Creation overhead has been reduced by approximately a factor of three, as has null execution cost. Jacobi has improved by approximately 25% and Fibonacci by 25 times. 4.5 Implementation and Performance of Filaments The performance of Filaments in relation to coarse-grained approaches is discussed in <ref> [24] </ref> and [18]. To test the overhead of Filaments relative to a coarse-grained implementation, pathological cases were examined. Jacobi iteration, which consists of very few operations, is within 11% of a coarse-grained implementation on a Sequent Symmetry, and within 40% on an SGI Iris.
Reference: [25] <author> E. P. Markatos and T. J. LeBlanc. </author> <title> Load balancing vs. locality management in shared-memory multiprocessors. </title> <booktitle> In Proc. 1992 International Conference on Parallel Processing, </booktitle> <pages> pages 258-267, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The Ultracomputer project used a similar mechanism [17]. Local queue: Central queues are a sequentializing bottleneck for the creation and execution of threads [3]. Furthermore, on machines that have a gross difference in speed between the CPU and memory, locality must be explicit for reasonable performance <ref> [25] </ref>. Local ready queues (each server 5 owns a queue) eliminates contention for a central lock and allows thread placement to be based on locality (i.e., threads working on the same data can be explicitly placed on the same queue). <p> Several researchers have proposed ways to reduce the inefficiencies of standard thread packages. Anderson et al. [3] discusses the gain from using local ready queues, and [2] shows how to do user-level scheduling [2]. Markatos et al. <ref> [25] </ref> presents a thorough study of the tradeoffs between load balancing and locality in shared memory machines. Schoenberg and Hummel [19] explain how to avoid allocating a stack per thread and context switching in nested parallel for loops, which are a form of run-to-completion threads.
Reference: [26] <author> F. Mueller. </author> <title> A library implementation of Posix threads under UNIX. </title> <booktitle> In Proc. USENIX Conference Winter'93, </booktitle> <pages> pages 29-41, </pages> <year> 1993. </year>
Reference-contexts: Obviously if thread primitives have a high overhead, achieving this goal will be difficult. Since generality is expensive, general purpose thread packages have high overhead, limiting their applicability. For example, thread creation in Pthreads <ref> [26] </ref> (an implementation of the general purpose POSIX thread specification), is over three orders of magnitude more costly than in the thread package described in this paper, Filaments. To reduce the expense of thread primitives, implementors concerned with efficiency often design packages optimized for their specific domain. <p> Given the increasing complexity of pipeline interactions and register saving conventions, this may be a time-consuming job; for example, the public domain Pthreads package that implements a draft of the POSIX standard contains 800 lines of SPARC assembly code <ref> [26] </ref>. 4 Stateless threads The problems discussed in the previous section can be solved using stateless threads, which are threads that do not support any blocking primitive requiring a context switch.
Reference: [27] <author> J. K. Ousterhout. </author> <title> Why aren't operating systems getting faster as fast as hardware? In Proc. </title> <booktitle> Summer Usenix, </booktitle> <pages> pages 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: As RISC machines evolve context-switching is becoming more costly. The processor state is increasing as processors have more registers. Additionally, storing and loading values from main memory is getting 7 more expensive relative to CPU cycle time <ref> [27] </ref>, [4]. * High Code complexity. To create, switch between, and terminate threads the thread package must explicitly manipulate thread stacks. While the code necessary to perform these tasks is short, it is notoriously difficult to get correct, as it has to deal with the idiosyncrasies of the underlying architecture. <p> An important part of this transition is the use of stateless threads, the advantages of which are: * No context-switching. Context-switching performance, because of larger register sets and slow memory speeds, is not scaling with integer CPU performance <ref> [27] </ref>, [4]. By using stateless threads, this bottleneck is avoided. * Low memory consumption. By using one system-managed stack, stateless threads consume far less memory than a traditional thread package. This results in better caching and TLB performance. * Portability.
Reference: [28] <author> D. Stein and D. Shah. </author> <title> Implementing lightweight threads. </title> <booktitle> In USENIX, </booktitle> <year> 1992. </year>
Reference-contexts: The first step towards efficient parallelism was lightweight thread packages such as 16 Threads [14], Presto [6], uSystem [9], C Threads [11], uC++ [8] and Sun Lightweight Processes <ref> [28] </ref>. We will call these standard packages to distinguish them from thread packages that do not have a stack for each thread. The goal of standard packages is to provide the user with a natural thread abstraction, and many of the usual concurrent programming primitives; different packages provide different primitives.
Reference: [29] <author> Robert H. Thomas and Will Crowther. </author> <title> The Uniform System: an approach to runtime support for large scale shared memory parallel processors. </title> <booktitle> In Proceedings of the 1988 Conference on Parallel Processing, </booktitle> <pages> pages 245-254, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Schoenberg and Hummel [19] explain how to avoid allocating a stack per thread and context switching in nested parallel for loops, which are a form of run-to-completion threads. Thread packages that support finer-grained parallelism include the Uniform System, WorkCrews, TAM and Chores. The Uniform System <ref> [29] </ref>, built for the BBN Butterfly, has several things in common with Filaments: there is no private stack per thread, no context switches and threads are not preemptable. The Uniform System's synchronous mode supports a simple form of barrier threads, and their finalization code is equivalent to our sequential code.
Reference: [30] <author> M. Vandevoorde and E. Roberts. WorkCrews: </author> <title> an abstraction for controlling parallelism. </title> <journal> Int. Journal of Parallel Programming, </journal> <volume> 17 </volume> <pages> 347-366, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Many papers have discussed how to make threads efficient (e.g., [3], [2] and <ref> [30] </ref>) Our work synthesizes previous work on making threads efficient and extends it in several novel ways. Optimizations occur within a unified framework that allows their relative merits to be judged. <p> This gives an order of magnitude improvement in performance of fork/join threads. WorkCrews is the first recorded thread package to use this optimization <ref> [30] </ref>. There are two subtleties to this otherwise straightforward optimization: the first is that in divide-and-conquer algorithms, the earliest threads will have the most work and so should be immediately accessible. Without the use of queue semantics (i.e., FIFO) little speedup will occur. This was first noted by [30]. <p> this optimization <ref> [30] </ref>. There are two subtleties to this otherwise straightforward optimization: the first is that in divide-and-conquer algorithms, the earliest threads will have the most work and so should be immediately accessible. Without the use of queue semantics (i.e., FIFO) little speedup will occur. This was first noted by [30]. The second is that, for simplicity, if a thread decides to prune a child, it prunes all of that child's progeny as well. <p> Because of these differences, the Uniform System cannot efficiently support thread-per-point decompositions for problems like Jacobi, as Lin [23] and others have noted. Furthermore, the model of parallelism is one of loop-based, not thread-based, parallelism. Specifying function-level parallelism in such a setting is cumbersome and inefficient. WorkCrews <ref> [30] </ref> supports fork/join parallelism on small-scale, shared-memory multiprocessors. The package is implemented in Modula2+. WorkCrews introduced the concept of pruning and of ordering queues to favor larger threads. Filaments has borrowed these ideas in its implementation of fork/join threads. TAM [12] is a compiler-controlled threaded abstract machine.
References-found: 30

