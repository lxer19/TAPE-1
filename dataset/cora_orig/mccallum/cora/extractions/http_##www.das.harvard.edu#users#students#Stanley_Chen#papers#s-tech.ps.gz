URL: http://www.das.harvard.edu/users/students/Stanley_Chen/papers/s-tech.ps.gz
Refering-URL: http://www.das.harvard.edu/users/students/Stanley_Chen/Stanley_Chen.html
Root-URL: 
Title: Building Probabilistic Models for Natural Language  
Author: Stanley F. Chen 
Degree: A thesis presented by  to The Division of Applied Sciences in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the subject of  
Date: May 1996  
Address: Cambridge, Massachusetts  
Affiliation: Computer Science Harvard University  
Abstract-found: 0
Intro-found: 1
Reference: <author> D. Angluin and C.H. Smith. </author> <year> 1983. </year> <title> Inductive inference: theory and methods. </title> <journal> ACM Computing Surveys, </journal> <volume> 15 </volume> <pages> 237-269. </pages>
Reference-contexts: The algorithm does not require the training data to be manually annotated in any way. 6 3.2 Grammar Induction as Search Grammar induction can be framed as a search problem, and has been framed as such almost without exception in past research <ref> (Angluin and Smith, 1983) </ref>. The search space is taken to be some class of grammars; for example, in our work we search within the space of probabilistic context-free grammars. We search for a grammar that optimizes some quantity, referred to as the objective function.
Reference: <author> Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. </author> <year> 1983. </year> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5(2):179-190, </volume> <month> March. </month>
Reference: <author> James K. Baker. </author> <year> 1979. </year> <title> Trainable grammars for speech recognition. </title> <booktitle> In Proceedings of the Spring Conference of the Acoustical Society of America, </booktitle> <pages> pages 547-550, </pages> <address> Boston, MA, </address> <month> June. </month>
Reference: <author> L.E. Baum. </author> <year> 1972. </year> <title> An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8. </pages>
Reference-contexts: likelihood distribution, or we can take the smoothed 0th-order model to be the uniform distribution p unif (w i ) = 1 17 Given fixed p ML , it is possible to search efficiently for the w i1 in+1 that maximize the probability of some data using the Baum-Welch algorithm <ref> (Baum, 1972) </ref>. To yield meaningful results, the data used to estimate the w i1 in+1 need to be disjoint from the data used to calculate the p ML . 7 In held-out interpolation, one reserves a section of the training data for this purpose.
Reference: <author> Timothy C. Bell, John G. Cleary, and Ian H. Witten. </author> <year> 1990. </year> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J. </address>
Reference: <author> Richard Bellman. </author> <year> 1957. </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton N.J. </address>
Reference: <author> Richard P. Brent. </author> <year> 1973. </year> <title> Algorithms for Minimization without Derivatives. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. DellaPietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. </author> <year> 1990. </year> <title> A statistical approach to machine translation. </title> <journal> Computational Linguistics, </journal> <volume> 16(2) </volume> <pages> 79-85, </pages> <month> June. </month>
Reference: <author> Peter F. Brown, Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. </author> <year> 1991a. </year> <title> Word sense disambiguation using statistical methods. </title> <booktitle> In Proceedings 29th Annual Meeting of the ACL, </booktitle> <pages> pages 265-270, </pages> <address> Berkeley, CA, </address> <month> June. </month>
Reference: <author> Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer. </author> <year> 1991b. </year> <title> Aligning sentences in parallel corpora. </title> <booktitle> In Proceedings 29th Annual Meeting of the ACL, </booktitle> <pages> pages 169-176, </pages> <address> Berkeley, CA, </address> <month> June. </month>
Reference: <author> Peter F. Brown, Stephen A. DellaPietra, Vincent J. DellaPietra, Jennifer C. Lai, and Robert L. Mercer. </author> <year> 1992a. </year> <title> An estimate of an upper bound for the entropy of English. </title> <journal> Computational Linguistics, </journal> <volume> 18(1) </volume> <pages> 31-40, </pages> <month> March. </month>
Reference: <author> Peter F. Brown, Vincent J. DellaPietra, Peter V. deSouza, Jennifer C. Lai, </author> <title> and Robert L. </title>
Reference: <author> Mercer. </author> <year> 1992b. </year> <title> Class-based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 467-479, </pages> <month> December. </month>
Reference: <author> Peter F. Brown, Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. </author> <year> 1993. </year> <title> The mathematics of statistical machine translation: Parameter estimation. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 263-312. </pages>
Reference: <author> Glenn Carroll. </author> <year> 1995. </year> <title> Learning Probabilistic Grammars for Language Modeling. </title> <type> Ph.D. thesis, </type> <institution> Brown University, </institution> <month> May. </month> <note> 146 Roberta Catizone, </note> <author> Graham Russell, and Susan Warwick. </author> <year> 1989. </year> <title> Deriving translation data from bilingual texts. </title> <booktitle> In Proceedings of the First International Acquisition Workshop, </booktitle> <address> Detroit, Michigan, </address> <month> August. </month>
Reference: <author> Stanley F. Chen and Joshua T. Goodman. </author> <year> 1996. </year> <title> An empirical study of smoothing techniques for language modeling. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL, </booktitle> <address> Santa Cruz, California, </address> <month> June. </month> <note> To appear. </note>
Reference: <author> Stanley F. Chen, Andrew S. Kehler, and Stuart M. Shieber. </author> <year> 1993. </year> <title> Experiments in stochastic grammar inference with simulated annealing and the inside-outside algorithm. </title> <type> Unpublished report. </type>
Reference: <author> Stanley F. Chen. </author> <year> 1993. </year> <title> Aligning sentences in bilingual corpora using lexical information. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the ACL, </booktitle> <pages> pages 9-16, </pages> <address> Columbus, Ohio, </address> <month> June. </month>
Reference: <author> Stanley F. Chen. </author> <year> 1995. </year> <title> Bayesian grammar induction for language modeling. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the ACL, </booktitle> <pages> pages 228-235, </pages> <address> Cambridge, Massachusetts, </address> <month> June. </month>
Reference: <author> Noam Chomsky. </author> <year> 1964. </year> <title> Syntactic Structures. </title> <publisher> Mouton. </publisher>
Reference: <author> Kenneth W. Church and William A. Gale. </author> <year> 1991. </year> <title> A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 19-54. </pages>
Reference: <author> Kenneth Church. </author> <year> 1988. </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143. </pages>
Reference: <author> Michael Collins and James Brooks. </author> <year> 1995. </year> <title> Prepositional phrase attachment through a backed-off model. </title> <editor> In David Yarowsky and Kenneth Church, editors, </editor> <booktitle> Proceedings of the Third Workshop on Very Large Corpora, </booktitle> <pages> pages 27-38, </pages> <address> Cambridge, MA, </address> <month> June. </month>
Reference: <author> Craig M. Cook, Azriel Rosenfeld, and Alan R. Aronson. </author> <year> 1976. </year> <title> Grammatical inference by hill climbing. </title> <journal> Information Sciences, </journal> <volume> 10 </volume> <pages> 59-80. </pages>
Reference: <author> T.M. Cover and R.C. King. </author> <year> 1978. </year> <title> A convergent gambling estimate of the entropy of English. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 24(4) </volume> <pages> 413-421. </pages>
Reference: <author> Ido Dagan, Alon Itai, and Ulrike Schwall. </author> <year> 1991. </year> <title> Two languages are more informative than one. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the ACL, </booktitle> <pages> pages 130-137. </pages>
Reference: <author> Carl de Marcken. </author> <year> 1995. </year> <title> Lexical heads, phrase structure, and the induction of grammar. </title> <booktitle> In Proceedings of the Third Workshop on Very Large Corpora, </booktitle> <address> Cambridge, MA, </address> <month> June. </month>
Reference: <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, 39(B):1-38. </journal>
Reference-contexts: not present any results on data sets approaching the sizes that we used, and like Cook et al., they do not present any language modeling results. 3.4.2 Other Approaches The most widely-used tool in probabilistic grammar induction is the Inside-Outside algorithm (Baker, 1979), a special case of the Expectation-Maximization algorithm <ref> (Dempster et al., 1977) </ref>. The Inside-Outside algorithm takes a probabilistic context-free grammar and adjusts its probabilities iteratively to attempt to maximize the probability the grammar assigns to some training data. <p> For the Hansard corpus, we have found that one hundred sentence pairs are sufficient to bootstrap the alignment model. This method can be considered to be a variation of the Viterbi version of the expectation-maximization (EM) algorithm <ref> (Dempster et al., 1977) </ref>. In the EM algorithm, an expectation phase, where counts on the corpus are taken using the current estimates of the parameters, is alternated with a maximization phase, where parameters are re-estimated based on the counts just taken.
Reference: <author> William A. Gale and Kenneth W. Church. </author> <year> 1990. </year> <title> Estimation procedures for language context: poor estimates are worse than none. </title> <booktitle> In COMPSTAT, Proceedings in Computational Statistics, 9th Symposium, </booktitle> <pages> pages 69-74, </pages> <address> Dubrovnik, Yugoslavia, </address> <note> September. </note> <author> 147 William A. Gale and Kenneth W. Church. </author> <year> 1991. </year> <title> A program for aligning sentences in bilingual corpora. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the ACL, </booktitle> <address> Berkeley, California, </address> <month> June. </month>
Reference: <author> William A. Gale and Kenneth W. Church. </author> <year> 1993. </year> <title> A program for aligning sentences in bilingual corpora. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 75-102. </pages>
Reference: <author> William A. Gale and Kenneth W. Church. </author> <year> 1994. </year> <title> What's wrong with adding one? In N. </title> <editor> Oostdijk and P. de Haan, editors, </editor> <booktitle> Corpus-Based Research into Language. </booktitle> <address> Rodolpi, Amsterdam. </address>
Reference: <author> William A. Gale and Geoffrey Sampson. </author> <year> 1995. </year> <title> Good-Turing frequency estimation without tears. </title> <journal> Journal of Quantitative Linguistics, </journal> <note> 2(3). To appear. </note>
Reference: <author> William A. Gale, Kenneth W. Church, and David Yarowsky. </author> <year> 1992. </year> <title> Using bilingual materials to develop word sense disambiguation methods. </title> <booktitle> In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, </booktitle> <pages> pages 101-112, </pages> <address> Montreal, Canada, </address> <month> June. </month>
Reference: <author> I.J. Good. </author> <year> 1953. </year> <title> The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40(3 and </volume> 4):237-264. 
Reference-contexts: Gale and Church (1990; 1994) have argued that this method generally performs poorly. 2.2.2 Good-Turing Estimate The Good-Turing estimate <ref> (Good, 1953) </ref> is central to many smoothing techniques.
Reference: <author> D.A. Huffman. </author> <year> 1952. </year> <title> A method for the construction of minimum redundancy codes. </title> <booktitle> Proceedings of the IRE, </booktitle> <volume> 40 </volume> <pages> 1098-1101. </pages>
Reference-contexts: Intuitively, we want to assign codeword lengths that are appropriate for probabilities that are negative powers of two that are near to the actual probabilities of each outcome. In fact, there is an algorithm for performing this assignment in an optimal way, namely Huffman coding <ref> (Huffman, 1952) </ref>. In this case, Huffman coding yields the codewords 0, 10, and 11.
Reference: <author> Jonathon Hull. </author> <year> 1992. </year> <title> Combining syntactic knowledge and visual text recognition: A hidden Markov model for part of speech tagging in a word recognition algorithm. </title> <booktitle> In AAAI Symposium: Probabilistic Approaches to Natural Language, </booktitle> <pages> pages 77-83. </pages>
Reference: <author> R. Isotani and S. Matsunaga. </author> <year> 1994. </year> <title> Speech recognition using a stochastic language model integrating local and global constraints. </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <pages> pages 88-93, </pages> <month> March. </month>
Reference: <author> R. Iyer, M. Ostendorf, and J.R. Rohlicek. </author> <year> 1994. </year> <title> Language modeling with sentence-level mixtures. </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <pages> pages 82-87, </pages> <month> March. </month>
Reference: <author> H. Jeffreys. </author> <year> 1948. </year> <title> Theory of Probability. </title> <publisher> Clarendon Press, </publisher> <address> Oxford, </address> <note> second edition. </note>
Reference: <author> Frederick Jelinek and Robert L. Mercer. </author> <year> 1980. </year> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceedings of the Workshop on Pattern Recognition in Practice, </booktitle> <address> Amsterdam, The Netherlands: </address> <publisher> North-Holland, </publisher> <month> May. </month>
Reference: <author> Frederick Jelinek, John D. Lafferty, and Robert L. Mercer. </author> <year> 1992. </year> <title> Basic methods of probabilistic context-free grammars. In Speech Recognition and Understanding: </title> <booktitle> Recent Advances, Trends, and Applications. Proceedings of the NATO Advanced Study Institute, </booktitle> <pages> pages 345-360, </pages> <address> Cetraro, Italy. </address>
Reference: <author> W.E. Johnson. </author> <year> 1932. </year> <title> Probability: deductive and inductive problems. </title> <journal> Mind, </journal> <volume> 41 </volume> <pages> 421-423. </pages>
Reference: <author> Slava M. Katz. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <note> ASSP-35(3):400-401, </note> <author> March. 148 Martin Kay and Martin Roscheisen. </author> <year> 1993. </year> <title> Text-translation alignment. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 121-142. </pages>
Reference: <author> M.D. Kernighan, K.W. Church, and W.A. Gale. </author> <year> 1990. </year> <title> A spelling correction program based on a noisy channel model. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Computational Linguistics, </booktitle> <pages> pages 205-210. </pages>
Reference-contexts: In optical character recognition and handwriting recognition (Hull, 1992; Srihari and Baltus, 1992), the channel can be interpreted as converting from text to image data instead of from text to speech, yielding the equation T = arg max p (T )p (imagejT ): In spelling correction <ref> (Kernighan et al., 1990) </ref>, the channel can be interpreted as an imper fect typist that converts perfect text T to noisy text T n with spelling mistakes, yielding T = arg max p (T )p (T n jT ): In machine translation (Brown et al., 1990), the channel can be interpreted
Reference: <author> Judith Klavans and Evelyne Tzoukermann. </author> <year> 1990. </year> <title> The bicord system. </title> <booktitle> In COLING-90, </booktitle> <pages> pages 174-179, </pages> <address> Helsinki, Finland, </address> <month> August. </month>
Reference: <author> A.N. Kolmogorov. </author> <year> 1965. </year> <title> Three approaches to the quantitative definition of information. Problems in Information Transmission, </title> <booktitle> 1(1) </booktitle> <pages> 1-7. </pages>
Reference: <author> R. Kuhn. </author> <year> 1988. </year> <title> Speech recognition and the frequency of recently used words: A modified markov model for natural language. </title> <booktitle> In 12th International Conference on Computational Linguistics, </booktitle> <pages> pages 348-350, </pages> <address> Budapest, </address> <month> August. </month>
Reference: <author> K. Lari and S.J. Young. </author> <year> 1990. </year> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56. </pages>
Reference: <author> K. Lari and S.J. Young. </author> <year> 1991. </year> <title> Applications of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 237-257. </pages>
Reference: <author> Ming Li and Paul Vitanyi. </author> <year> 1993. </year> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> G.J. Lidstone. </author> <year> 1920. </year> <title> Note on the general case of the Bayes-Laplace formula for inductive or a posteriori probabilities. </title> <journal> Transactions of the Faculty of Actuaries, </journal> <volume> 8 </volume> <pages> 182-192. </pages>
Reference: <author> David J. C. MacKay and Linda C. Peto. </author> <year> 1995. </year> <title> A hierarchical Dirichlet language model. </title> <booktitle> Natural Language Engineering, </booktitle> <volume> 1(3) </volume> <pages> 1-19. </pages>
Reference: <author> David M. Magerman and Mitchell P. Marcus. </author> <year> 1990. </year> <title> Parsing a natural language using mutual information statistics. </title> <booktitle> In Proceedings of the AAAI, </booktitle> <address> Boston, MA. </address>
Reference: <author> David M. Magerman. </author> <year> 1994. </year> <title> Natural Language Parsing as Statistical Pattern Recognition. </title> <type> Ph.D. thesis, </type> <institution> Stanford University, </institution> <month> February. </month>
Reference: <author> Michael K. McCandless and James R. Glass. </author> <year> 1993. </year> <title> Empirical acquisition of word and phrase classes in the ATIS domain. </title> <booktitle> In Third European Conference on Speech Communication and Technology, </booktitle> <address> Berlin, Germany, </address> <month> September. </month>
Reference: <author> Arthur Nadas. </author> <year> 1984. </year> <title> Estimation of probabilities in the language model of the IBM speech recognition system. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-32(4):859-861, </volume> <month> August. </month>
Reference: <author> A. Newell. </author> <year> 1973. </year> <title> Speech Understanding Systems: Final Report of a Study Group. </title> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> R. Pasco. </author> <year> 1976. </year> <title> Source coding algorithms for fast data compression. </title> <type> Ph.D. thesis, </type> <institution> Stanford University. 149 Fernando Pereira and Yves Schabes. </institution> <year> 1992. </year> <title> Inside-outside reestimation from partially brack-eted corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the ACL, </booktitle> <pages> pages 128-135, </pages> <address> Newark, Delaware. </address>
Reference: <author> W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling. </author> <year> 1988. </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference-contexts: In our implementation, we include a general multidimensional search engine for automatically searching for optimal parameter values for each smoothing technique. We use the implementation of Powell's search algorithm (Brent, 1973) given in Numerical Recipes in C <ref> (Press et al., 1988, pp. 309-317) </ref>. Powell's algorithm does not require the calculation of the gradient. It involves successive searches along vectors in the multidimensional search space. 2.4.3 Data We used the Penn treebank and TIPSTER corpora distributed by the Linguistic Data Consortium.
Reference: <author> Philip Resnik. </author> <year> 1992. </year> <title> Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics. </booktitle>
Reference: <author> J. Rissanen. </author> <year> 1976. </year> <title> Generalized Kraft inequality and arithmetic coding. </title> <journal> IBM Journal of Research and Development, 20:198. </journal>
Reference: <author> J. Rissanen. </author> <year> 1978. </year> <title> Modeling by the shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference-contexts: Though yet to perform as well as word-based models, grammatical models offer the best hope for significantly improving language modeling accuracy. We introduce a novel grammar induction algorithm based on the minimum description length principle <ref> (Rissanen, 1978) </ref> that surpasses the performance of existing algorithms. The third problem deals with the task of bilingual sentence alignment. There exist many corpora that contain equivalent text in multiple languages. For example, the Hansard corpus contains the Canadian parliament proceedings in both English and French. <p> In addition, this is related to why the maximum likelihood approach prefers overlarge, overfitting grammars, as the uniform prior assigns far too much probability to large grammars. Instead, we argue that taking a minimum description length (MDL) principle <ref> (Rissanen, 1978) </ref> prior is desirable. The minimum description length principle states that one should select a grammar G that minimizes the sum of l (G), the length of the description of the grammar, and l (OjG), the length of the description of the data given the grammar.
Reference: <author> J. Rissanen. </author> <year> 1989. </year> <title> Stochastical Complexity and Statistical Inquiry. </title> <publisher> World Scientific Publishing Company. </publisher>
Reference-contexts: In particular, the probability we assign to the symbol A expanding to exactly n B's is p (A ) fl B n ) = p MDL (n) = 2 n [log 2 (n + 1)] 2 where p MDL is the universal MDL prior over the natural numbers <ref> (Rissanen, 1989) </ref>. <p> However, it is inelegant to set a bound, and this encoding is inefficient for small integers. Instead, we use the encoding associated with the universal MDL prior over the natural numbers p MDL (n) <ref> (Rissanen, 1989) </ref> mentioned in Section 3.5.1, where p MDL (n) = 2 n [log 2 (n + 1)] 2 We take the length l (n) of an integer n to be log 2 1 p MDL (n) .
Reference: <author> Eric Sven Ristad. </author> <year> 1995. </year> <title> A natural law of succession. </title> <type> Technical Report CS-TR-495-95, </type> <institution> Princeton University. </institution>
Reference: <author> R. Rosenfeld and X.D. Huang. </author> <year> 1992. </year> <title> Improvements in stochastic language modeling. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <month> February. </month>
Reference: <author> Ronald Rosenfeld. </author> <year> 1994a. </year> <title> Adaptive Statistical Language Modeling: a Maximum Entropy Approach. </title> <type> Ph.D. thesis, </type> <institution> Carnegie Mellon University, </institution> <month> April. </month>
Reference: <author> Ronald Rosenfeld. </author> <year> 1994b. </year> <title> A hybrid approach to adaptive statistical language modeling. </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <pages> pages 76-81, </pages> <month> March. </month>
Reference: <author> V. Sadler. </author> <year> 1989. </year> <title> The Bilingual Knowledge Bank ANew Conceptual Basis for MT. </title> <address> BSO/Research, Utrecht. </address>
Reference: <author> Yves Schabes. </author> <year> 1992. </year> <title> Stochastic lexicalized tree-adjoining grammars. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics. </booktitle>
Reference: <author> C.E. Shannon. </author> <year> 1948. </year> <title> A mathematical theory of communication. </title> <journal> Bell Systems Technical Journal, </journal> <volume> 27 </volume> <month> 379-423,623-656. </month>
Reference-contexts: These and other applications can be placed in a single common framework (Bahl et al., 1983), the source-channel model used in information theory <ref> (Shannon, 1948) </ref>. In this section, we explain how speech recognition can be placed in this framework, and then explain how other applications are just variations on this theme.
Reference: <author> C.E. Shannon. </author> <year> 1951. </year> <title> Prediction and entropy of printed English. </title> <journal> Bell Systems Technical Journal, </journal> <volume> 30 </volume> <pages> 50-64, </pages> <month> January. </month>
Reference-contexts: First, we consider the problem of smoothing n-gram language models <ref> (Shannon, 1951) </ref>. Such models are dominant in language modeling, yielding the best current performance. In such models, the probability of a sentence is expressed through the probability of each word in the sentence; such models are word-based models.
Reference: <author> Stuart M. Shieber. </author> <year> 1996. </year> <type> Personal communication. </type>
Reference: <author> M. Simard, G. Foster, and P. Isabelle. </author> <year> 1992. </year> <title> Using cognates to align sentences in bilingual corpora. </title> <booktitle> In Fourth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-92), </booktitle> <address> Montreal, Canada. </address>
Reference-contexts: For example, punctuation, numbers, and proper names generally have the same spellings in English and French. Such words are members of a class called cognates <ref> (Simard et al., 1992) </ref>. Because identically spelled words can be recognized automatically and are frequently translations of each other, it is sensible to use this a priori information in initializing word bead frequencies.
Reference: <author> R.J. Solomonoff. </author> <year> 1959. </year> <title> A progress report on machines to learn to translate languages and retrieve information. </title> <booktitle> In Advances in Documnetation and Library Sciences, volume III, </booktitle> <pages> pages 941-953. </pages> <publisher> Interscience, </publisher> <address> New York. 150 R.J. Solomonoff. </address> <year> 1960. </year> <title> A preliminary report on a general theory of inductive inference. </title> <type> Technical Report ZTB-138, </type> <institution> Zator Company, </institution> <address> Cambridge, MA, </address> <month> November. </month>
Reference-contexts: In this work, we always name the sentential symbol S and it is always meant to correspond to a sentence (as opposed to a lower- or higher-level linguistic structure). A probabilistic context-free grammar <ref> (Solomonoff, 1959) </ref> is a context-free grammar that not just describes a set of strings, but also assigns probabilities to these strings. 2 A probability is associated with each rule in the grammar, such that the sum of the probabilities 1 The following abbreviations are used in this work: S = sentence
Reference: <author> R.J. Solomonoff. </author> <year> 1964. </year> <title> A formal theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22, 224-254, </pages> <month> March, June. </month>
Reference: <author> Rohini Srihari and Charlotte Baltus. </author> <year> 1992. </year> <title> Combining statistical and syntactic methods in recognizing handwritten sentences. </title> <booktitle> In AAAI Symposium: Probabilistic Approaches to Natural Language, </booktitle> <pages> pages 121-127. </pages>
Reference: <author> Andreas Stolcke and Stephen Omohundro. </author> <year> 1994. </year> <title> Best-first model merging for hidden Markov model induction. </title> <type> Technical Report TR-94-003, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA. </address>
Reference: <author> Susan Warwick and Graham Russell. </author> <year> 1990. </year> <title> Bilingual concordancing and bilingual lexicography. </title> <booktitle> In EURALEX 4th International Congress, Malaga, </booktitle> <address> Spain. </address>
Reference: <author> W. Woods, M. Bates, G. Brown, B. Bruce, C. Cook, J. Klovstad, J. Makhoul, B. Nash-Webber, R. Schwartz, J. Wolf, and V. Zue. </author> <year> 1976. </year> <title> Speech understanding systems: </title> <type> final report, </type> <month> November </month> <year> 1974-October 1976. </year> <institution> Bolt Beranek and Newman Inc., </institution> <address> Boston. </address>
Reference: <author> D.H. Younger. </author> <year> 1967. </year> <title> Recognition and parsing of context free languages in time n 3 . Information and Control, </title> <booktitle> 10 </booktitle> <pages> 198-208. 151 </pages>
References-found: 80

