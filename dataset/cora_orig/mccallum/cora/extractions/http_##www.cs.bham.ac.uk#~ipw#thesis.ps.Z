URL: http://www.cs.bham.ac.uk/~ipw/thesis.ps.Z
Refering-URL: http://www.cs.bham.ac.uk/~ipw/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Emotional Agents  
Author: Ian Paul Wright 
Degree: A thesis submitted to the Faculty of Science of the  for the degree of DOCTOR OF PHILOSOPHY  
Date: February 1997  
Address: Birmingham, B15 2TT England  
Affiliation: University of Birmingham  School of Computer Science Cognitive Science Research Centre University of Birmingham  
Abstract-found: 0
Intro-found: 1
Reference: <author> 215 Agre, P. E. & Chapman, D. </author> <year> (1987). </year> <title> Pengi: an implementation of a theory of activity. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 268-272, </pages> <address> Seattle. </address> <publisher> AAAI. </publisher>
Reference: <author> Aube, M. & Senteni, A. </author> <year> (1996a). </year> <title> Emotions as commitments operators: a foundation for control structure in multi-agent systems. </title> <booktitle> In Proceedings of the Seventh Eu-ropean Workshop on Modelling Autonomous Agents in a Multi-Agents World, MAAMAW '96, Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Aube, M. & Senteni, A. </author> <year> (1996b). </year> <title> What are emotions for? commitments management and regulation within animals/animats encounters. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., & Wilson, S. W. (Eds.), </editor> <booktitle> From Animals to Ani-mats IV, Proceedings of the Fourth International Conference on the Simulation of Adaptive Behavior, </booktitle> <pages> pages 264-271. </pages> <publisher> The MIT Press. </publisher>
Reference: <author> Axelrod, R. & Hamilton, W. D. </author> <year> (1981). </year> <title> The evolution of cooperation. </title> <journal> Science, </journal> <volume> 211 </volume> <pages> 1390-1396. </pages>
Reference-contexts: Minimally economic agents also require motivations to cooperate. A number of theories of the evolution of cooperative, altruistic behaviour in the natural world have been 161 proposed, notably how selfish behaviour leads to widespread TIT-FOR-TAT strate-gies <ref> (Axelrod & Hamilton, 1981) </ref>, or how genuinely altruistic behaviour can be inculcated in docile (those that are receptive to social influence) agents with bounded rationality (those unable to fully evaluate how acquired behaviours affect personal fitness) (Simon, 1990).
Reference: <author> Baldwin, J. M. </author> <title> (1896). A new factor in evolution. </title> <journal> American Naturalist, </journal> <volume> 30 </volume> <pages> 441-451, </pages> <month> 536-553. </month> <title> Reprinted in Adaptive Individuals in Evolving Populations: Models and Algorithms, edited by R. </title> <editor> K. Belew and M. </editor> <booktitle> Mitchell (SFI Studies in the Sciences of Complexity, Proc. </booktitle> <volume> Vol. XXVI, </volume> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1996). </year>
Reference: <author> Balkenius, C. </author> <year> (1995). </year> <booktitle> Natural intelligence in artificial creatures. </booktitle> <institution> Lund University Cognitive Studies 37. </institution>
Reference: <author> Bates, J. </author> <year> (1994). </year> <title> The role of emotion in believable agents. </title> <journal> Communications of the ACM, </journal> <volume> 37(7) </volume> <pages> 122-125. </pages>
Reference: <author> Bates, J., Loyall, A. B., & Reilly, W. S. </author> <year> (1991). </year> <title> Broad agents. </title> <note> In Paper presented at AAAI spring symposium on integrated intelligent architectures. (Available in SIGART BULLETIN, 2(4), </note> <month> Aug. </month> <year> 1991, </year> <pages> pp. 38-40). </pages>
Reference-contexts: Early work, still exploring general principles, need not make any commitment to the implementation details of mechanisms; for example, a neutral stance is taken towards symbolic or connectionist engines. Progress can be made by starting with `broad but shallow' <ref> (Bates, Loyall & Reilly, 1991) </ref> architectures that combine many sorts of capabilities (such as perception, planning, goal management, and action). Each capability is initially implemented in a simplified fashion. Subsequent work gradually refines and deepens the implementations.
Reference: <author> Baum, E. B. </author> <year> (1996). </year> <title> Toward a model of mind as a laissez-faire economy of idiots. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning. </booktitle>
Reference: <author> Beaudoin, L. P. </author> <year> (1994). </year> <title> Goal processing in autonomous agents. </title> <type> PhD thesis, </type> <institution> School of Computer Science, The University of Birmingham. </institution>
Reference-contexts: A goal-terminating mechanism [goal executor] ... 2. An interruption mechanism, that is, emotion, allows the processor to respond to urgent needs in real time (Simon, 1967). The theory implies that `organisms' have two kinds of processing that operate in parallel: a goal executor that generates actions, and vigilational <ref> (Beaudoin, 1994) </ref> processes that continuously check for contingencies that require urgent attention. The former, being resource limited, is interruptable by the latter. Jealousy, rage, triumph, excited anticipation, grief, obsessive love, despair, fear, joy, and so forth, all normally involve beliefs about states-of-affairs that are convergent or divergent with important goals. <p> This chapter briefly examines requirements for autonomous agency, summarised from a number of sources but mainly from <ref> (Beaudoin, 1994) </ref>, and provides a critical overview of some examples of existing agent architectures. The examples are representative but not exhaustive. A more detailed review can be found in (Wooldridge & Jennings, 1995; Wright, 1994; Beaudoin, 1994). <p> planning systems (e.g., STRIPS, NOAH, NONLIN etc., see chapter 9 of (Charniak & McDer-mott, 1985)) or models of cognition, such as GPS (section 5.7 of (Charniak & McDermott, 1985)), the SOAR architecture (Laird, Newell & Rosenbloom, 1987; Newell, 1990), Anderson's ACT-R (Webmaster, 1996) and blackboard architectures (see section 2.2.2 of <ref> (Beaudoin, 1994) </ref>). Such systems can be usefully conceived as agents when they attempt to control parts of a real or simulated world. For example, Nilsson's SHAKEY the robot (Nilsson, 1984) used a STRIPS-style planner to plan its behaviour. <p> However, the theory is speculative in that empirical checking has not yet been attempted and may be very difficult. The majority of the design work is due to Aaron Sloman and Luc Beaudoin. It has been described elsewhere, notably in <ref> (Beaudoin, 1994) </ref>, summarised in (Beau-doin & Sloman, 1993; Sloman, Beaudoin & Wright, 1994; Wright, Sloman & Beau-doin, 1996), and first elaborated in (Sloman, 1978; Sloman, 1985). <p> However, insistence measures based on fallible heuristics can sometimes cause `bad' decisions about what should and should not divert attention. Pre-attentive and attentive motive generactivators <ref> (Beaudoin, 1994) </ref>. These express agent `concerns' (Frijda, 1986; Moffat & Frijda, 1995). They operate asynchronously in parallel, triggered by internal and external events. They generate and activate or reactivate motivators, and set or reset their insistence level. <p> Often a motivator cannot be assessed until it has been partially expanded, or cannot be decided until it has been assessed, or cannot be assessed until partially executed, and so forth <ref> (Beaudoin, 1994) </ref>. In other words, motive management systems that have purely predefined and inflexible motive state transitions are unlikely to meet the full requirements (compare (Sloman, 1978)). MINDER1 has an extremely simple and shallow deciding process yet exhibits complicated interactions between deciding, scheduling, expanding and motivator states. <p> In this case the number of surfaced motives exceeds the maximum and the filter threshold is raised on each time step. The filter rises until the least insistent surfaced motive `dives' and the processing load returns to a manageable level (see figure 5.7). The original agent design <ref> (Beaudoin, 1994) </ref> did not specify a state transition of motive diving. Instead, suspended motives not attended to for some time decayed and were eventually removed. However, this is not entirely satisfactory as suspended motives, whether they be postponed for execution or deciding, will impose extra computational load on management processes. <p> Also, existing psychologial theories of motivation describe mechanisms that assess the importance of goals before adoption (Heckhausen & Kuhl, 1985) and self-regulatory processes that control motive adoption (Kuhl, 1992; Kuhl & Kraska, 1989) (this is discussed more thoroughly in <ref> (Beaudoin, 1994) </ref>). However, deciding on the precise form of representation of beliefs and other intentional structures was largely a matter of convenience. Accordingly, the implementation should be viewed as an exemplary illustration of the C&AP's design theory, but the details of the implementation are of secondary importance. <p> The metamanagement layer should include mechanisms for `self control', which are ways for motive management to be controlled, in particular to handle problematic emergent processing states (see next section). The filter mechanism could be extended to include a facility for `exception handlers' (discussed in <ref> (Beaudoin, 1994) </ref>), which would allow management processes to selectively prevent classes of motives surfacing regardless of their insistence. Finally, one of the difficulties of developing a motive processing architecture is the lack of a comprehensive theory of motive management. <p> Both these related types of irrationality are examples of the control precedence of emotions, in which mental life may be `out of control', externally determined. `Emotions do not act upon us, but involve something else which acts upon us' (R. M. Gordon, paraphrased in <ref> (Beaudoin, 1994) </ref>). Grief is an extreme example of this. Normal cases of mourning are characterised by highly insistent perturbant states that disrupt attention, adversely affecting day-to-day functioning, perhaps for many months and years. Attention is partially and episodically out of control. <p> E.g. detecting shortage of fluid and sending a signal to the brain may be done by a primitive mechanism that simply can't find out if the corresponding goal has previously been considered and rejected or adopted. (A. Sloman, quoted in <ref> (Beaudoin, 1994) </ref>). However, a person who desperately needs to urinate, or needs to eat, or needs warmth, is in a perturbant state. Needing to remove waste products, needing to get energy and needing to maintain body temperature are, from an evolutionary perspective, highly important concerns that should have control precedence. <p> Beaudoin has highlighted the absence of learning in his agent design <ref> (Beaudoin, 1994) </ref>, although the C&AP has understood the need for learning (for example, theses written by Tim Read (Read, 1995) and Edmund Shing (Shing, 1996) examined the role of learning in agent architectures). <p> As goals, commitments could result from many goal generactivators <ref> (Beaudoin, 1994) </ref>, some very primitive, and some more deliberative. Joint commitments are possible (e.g., `We will both move house') and are preconditions for cooperative action. <p> The relations between niche-space and design-space (Sloman, 1995a) are complex, involving many trade-offs and compromises, for example sacrificing speed of processing for depth of reasoning. The relations between requirements and design are important for understanding all types of systems, from software systems, agent architectures <ref> (Sloman, Beaudoin & Wright, 1994) </ref>, robots, and organisms in the natural world. MAS are no different in this respect: there will be many types of 146 MAS that satisfy different sets of requirements. <p> a loss of control of attention (due to important motives pertaining to the loved one surfacing into management) and the monitoring of a loss of CUE (libidinal generactivators lose credit because they now fail to meet conditions of 3 `Intensity' was briefly mentioned in section 5.5.3.3 and is discussed in <ref> (Beaudoin, 1994) </ref>. Intensity, a technical term used in our group, is the dispositional ability of a motive to hold onto management resources once adopted, and should not be confused with the intensity of valency, or non-technical uses of `intensity'. 182 satisfaction due to the loss the of object of attachment).
Reference: <author> Beaudoin, L. P. </author> <year> (1995). </year> <type> Personal communication. </type> <note> 216 Beaudoin, </note> <author> L. P. & Sloman, A. </author> <year> (1993). </year> <title> A study of motive processing and attention. </title>
Reference-contexts: for there may be good design reasons why such causal powers are hard to engineer; for example, recognising matches between exception fields and motives may be computationally expensive, or constructing efficacious exception fields may require knowledge of all the potential perturbing motives, which may be difficult to predict in advance <ref> (Beaudoin, 1995) </ref>. Other attempts have been made to explain longer term, disruptive perturbances.
Reference: <editor> In A.Sloman, D.Hogg, G.Humphreys, Partridge, D., & Ramsay, A. (Eds.), </editor> <booktitle> Prospects for Artificial Intelligence, </booktitle> <pages> pages 229-238. </pages> <address> Amsterdam: </address> <publisher> IOS Press. </publisher>
Reference: <author> Beer, R. D., Chiel, H. J., & Sterling, L. S. </author> <year> (1990). </year> <title> A biological perspective on autonomous agent design. </title> <editor> In Maes, P. (Ed.), </editor> <booktitle> Designing autonomous agents: Theory and practice from biology to engineering and back, </booktitle> <pages> pages 169-186. </pages> <address> Am-sterdam: </address> <publisher> Elsevier Science Publishers. </publisher>
Reference-contexts: An agent architecture is required to produce coherent, effective and robust behaviour <ref> (Beer, Chiel & Sterling, 1990) </ref> in a complex and unpredictable domain such that its goals are achieved subject to some evaluatory criteria. These requirements for successful operation in dynamic, unpredictable, real time environments pose certain design problems.
Reference: <author> Benson, S. & Nilsson, N. J. </author> <year> (1995). </year> <title> Reacting, planning, and learning in an autonomous agent. </title> <editor> In Furukawa, K., Michie, D., & Muggleton, S. (Eds.), </editor> <booktitle> Machine Intelligence 14. </booktitle> <publisher> Oxford: The Clarendon Press. </publisher>
Reference-contexts: will the agent exploit opportunities: ... if one of the agent's lesser goals is to deliver a certain library book to a far-away office, it will not realize that it can save time by picking up the book while it happens to be in the library for some other reason <ref> (Benson & Nilsson, 1995) </ref>. These problems are avoided by introducing the concept of a stable node. A stable node is a condition that continues to hold relative to a set of actions (TR programs). <p> The agent can explore its environment trying out different combinations of actions in different situations. Exploration can lead to the construction of new TR programs. The technical details of the learning algorithm are not reproduced here (but see <ref> (Benson & Nilsson, 1995) </ref>). Benson and Nilsson have successfully managed to integrate a number of desirable capabilities within a single agent architecture.
Reference: <author> Bhaskar, R. </author> <year> (1978). </year> <institution> A Realist Theory of Science. </institution> <address> Hassocks, Sussex: </address> <publisher> The Harvester Press Ltd. </publisher>
Reference-contexts: Objections to this approach are often based on a naive philosophy of science, or misplaced `physics envy'. (For a broader view see (Lakatos, 1970), chapter 2 of (Sloman, 1978), and <ref> (Bhaskar, 1978) </ref>). The design-based approach draws its inspiration from software engineering and conceptual analysis in philosophy (see chapter 4 of (Sloman, 1978)). <p> Further work is required to develop the theory into a mature unified theory of cognition. That would require the collaboration of many people over many years. 11.2 The exploration of design-space This thesis can be classified as belonging to the `creative modelling' phase of science <ref> (Bhaskar, 1978) </ref>, where competing models of generative mechanisms of surface phenomena are explored. Until we have a good `design-space' of broad and increasingly deep agent architectures it will be difficult to move from hypothetical explanation to justified selection between hypotheses.
Reference: <author> Boardman, J., Griffin, J., & Murray, O. </author> <year> (1993). </year> <title> The Oxford History of the Classical World. </title> <publisher> Oxford: Oxford University Press. </publisher>
Reference-contexts: Money - the representation of the, as yet to be defined, value of a thing arose at a certain point in human history to solve certain problems of production, consumption and 148 exchange. Pure gold was first coined as money in 625 BC in Greece <ref> (Boardman, Griffin & Murray, 1993) </ref>. In a matter of fifty years trade had burgeoned, and there were banks, merchants, and money-lenders. A numerical representation of value had a revolutionising effect on the capabilities of human society.
Reference: <author> Boddy, M. & Dean, T. </author> <year> (1989). </year> <title> Solving time-dependent planning problems. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <volume> vol. </volume> <pages> 2. </pages>
Reference-contexts: Other work in decision-theoretic control (rational deliberation under resource limitations), such as anytime algorithms <ref> (Boddy & Dean, 1989) </ref>, amended utility theory (Horvitz, Gregory & Heckerman, 1989; Miranda, 1996), algorithms that trade quality of solution with speed of response (Lesser, Pavlin & Durfee, 1989), or rational self-government (Doyle, 1989), is also of relevance for developing a theory of motive management.
Reference: <author> Boden, M. A. </author> <year> (1972). </year> <title> Purposive Explanation in Psychology. </title> <address> Hassocks Sussex Eng-land: </address> <publisher> The Harvester Press. </publisher>
Reference: <author> Bond, A. H. </author> <year> (1990). </year> <title> A computational model for organization of cooperating intelligent agents. </title> <booktitle> In Proc. of the Conference on Office information Systems, </booktitle> <pages> pages 21-30. </pages> <address> Cambridge, MA. </address>
Reference: <author> Botelho, L. M. & Coelho, H. </author> <year> (1996). </year> <title> Emotion-based attention shift in autonomous agents. </title> <editor> In Muller, J. P., Wooldridge, M. J., & Jennings, N. R. (Eds.), </editor> <booktitle> Intelligent Agents III, Proceedings of the Third International Workshop on Agent Theories, Architectures, and Languages, </booktitle> <address> Heidelberg. </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Bowlby, J. </author> <year> (1979). </year> <title> The Making and Breaking of Affectional Bonds. </title> <publisher> Tavistock Publications. </publisher>
Reference: <author> Bowlby, J. </author> <year> (1988). </year> <title> A Secure Base. </title> <publisher> Routledge. </publisher>
Reference: <author> Bowlby, J. </author> <year> (1991a). </year> <title> Attachment and Loss: Volume I. Penguin Books. </title> <note> First published in 1969. 217 Bowlby, J. </note> <year> (1991b). </year> <title> Attachment and Loss: Volume III. Penguin Books. </title> <note> First published in 1980. </note>
Reference-contexts: It must be stressed that attachment in humans involves much more than reinforcement learning. For example, it appears to include, at the very least, imprinting-like mechanisms and the construction of predictive models of the attachment figure <ref> (Bowlby, 1991a) </ref>. However, the libidinal selective system is specifically concerned with reinforcement learning. Its relations to other kinds of learning is an open question. Evolution, therefore, is the ultimate source of attachment motivation. <p> intensity of an emotion and the `intensity' of any associated learning was noted: `... the more strongly an appraised process is felt, and the more keenly, therefore, the consequences of some behaviour are experienced as pleasurable or painful, the quicker and more persistent is the ensuing learning likely to be' <ref> (Bowlby, 1991a) </ref>. Specification (p), stating that va-lency is the monitoring of credit assignment, partially explains this correlation. The greater the assignment of credit the greater the change in causal powers of substates and the greater the monitored intensity of the valenced state. <p> Bowlby was highly sympathetic to a control system approach to psychology, although he did not attempt to design and build them <ref> (Bowlby, 1991a) </ref>. The VAFP theory can extend his work and show how a distributed multi-component structure of attachment to an individual may develop within an agent architecture and influence subsequent processing.
Reference: <author> Braitenburg, V. </author> <year> (1984). </year> <title> Vehicles, experiments in synthetic psychology. </title> <publisher> MIT Press. </publisher>
Reference: <author> Bratman, M. E., Israel, D. J., & Pollack, M. E. </author> <year> (1988). </year> <title> Plans and resource-bounded practical reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 4 </volume> <pages> 349-355. </pages>
Reference: <author> Brooks, R. A. </author> <year> (1990). </year> <title> Elephants don't play chess. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 6 </volume> <pages> 3-15. </pages>
Reference: <author> Brooks, R. A. </author> <year> (1991a). </year> <title> Integrated systems based on behaviours. </title> <journal> SIGART Bulletin, </journal> <volume> 2(4) </volume> <pages> 46-50. </pages>
Reference-contexts: In other words, the agent should be composed of behaviour-producing modules. The subsumption architecture <ref> (Brooks, 1991a) </ref> is an example of this approach. Each behaviour module is a connected to others via wires that form a layered architecture. There is no explicit representation of goals or other symbolic structures.
Reference: <author> Brooks, R. A. </author> <year> (1991b). </year> <title> Intelligence without representation. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 139-159. </pages>
Reference-contexts: Such dedicated circuits do not store much enduring state as they are required to react very quickly to new environmental contingencies. At this level of control `it is better to use the world as its own model' <ref> (Brooks, 1991b) </ref>. Control conflicts between reactive systems are resolved via predefined hierarchical relations, such as subsump 52 tion (Brooks, 1991b), summed weights, simple `voting', gating, or inhibitory and excitatory links. <p> At this level of control `it is better to use the world as its own model' <ref> (Brooks, 1991b) </ref>. Control conflicts between reactive systems are resolved via predefined hierarchical relations, such as subsump 52 tion (Brooks, 1991b), summed weights, simple `voting', gating, or inhibitory and excitatory links. Reactive control evolved to respond to aspects of niches that are invariant over long time scales, such as the requirements for locomotion, including aspects that require speedy responses, such as the requirements for avoidance of looming objects.
Reference: <author> Calhoun, C. & Solomon, R. C. </author> <year> (1984). </year> <note> What is an Emotion? Oxford University Press. </note>
Reference-contexts: Phenomena-based theories assume that emotions are a well-specified category and attempt to correlate contemporaneous and measurable phenomena with the occurrence of an emotion, such as physiological changes or the firing of neural circuits. An early example is William James' peripheric theory (see <ref> (Calhoun & Solomon, 1984) </ref>); for a comprehensive review of many phenomena-based theories, see (Strongman, 1987). In contrast, Sloman's design-based approach, a rational reconstruction of the practice of AI, takes the stance of an engineer attempting to build a system that exhibits the phenomena to be explained.
Reference: <author> Chalmers, D. </author> <year> (1996). </year> <title> The Conscious Mind: In Search of a Fundamental Theory. </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: The study of `consciousness' is currently very fashionable, and there are many competing theories. An unfortunate number of those theories reject the possibility that `consciousness' could ever be fully explained in information processing terms, for example <ref> (Chalmers, 1996) </ref> and (Nagel, 1974). The rejection of the possibility of a mechanical explanation of `consciousness' has a long philosophical history. Those rejections often rest on what Ryle (1949) has called a `category mistake'. <p> Just as smells cannot be heard, introspections cannot be seen. Perspectival errors, in addition to category mistakes, often lead to dualist positions, for example hypothesising that `consciousness' is separate from `matter' and reconnecting the two ontological domains with superfluous `bridging laws' <ref> (Chalmers, 1996) </ref>. In summary, these very brief comments maintain that the so-called problem of conscious experience does not impact upon theories of emotion, and that arguments against the possibility of information processing theories of `consciousness' rest on basic philosophical errors.
Reference: <author> Chapman, D. </author> <year> (1989). </year> <title> Penguins can make cake. </title> <journal> AI Magazine, </journal> <volume> 10(4) </volume> <pages> 45-50. </pages>
Reference-contexts: A routine is a frequently used pattern of interaction between an agent and its world. In addition to routines, a simple way of representing the world is introduced, variously called indexical-functional aspects, indexical representation <ref> (Chapman, 1989) </ref> or deictic 40 representation. A deictic representation represents only what is necessary, immedi-ate and functionally important to the agent in its current situation.
Reference: <author> Chapman, D. </author> <year> (1990). </year> <title> Vision, instruction and action. </title> <type> Technical Report 1204, </type> <institution> Mas-sachusetts Institute of Technology, Artificial Intelligence Laboratory. </institution>
Reference-contexts: For example, instead of representing the location of a block using absolute coordinates, such as (AT BLOCK-213 427 991), Pengi will employ unitary entities such as the-block-I'm-pushing that gain semantic significance within a particular action context. The Sonja system <ref> (Chapman, 1990) </ref> is another example of a reactive architecture. Sonja is an agent operating in the Amazon domain (a 2-d world of walls, treasures, and mobile opponents). She has various tasks to perform and a human instructor can help and guide Sonja via natural language commands.
Reference: <author> Charniak, E. & McDermott, D. </author> <year> (1985). </year> <title> Introduction to Artificial Intelligence. </title> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference-contexts: But this is not so: the study of emotional states no more requires an explanation of `consciousness' than does the study of deliberative thought processes (see section 4.2.1). For example, models of cognition, such as GPS (section 5.7 of <ref> (Charniak & McDermott, 1985) </ref>) and the SOAR architecture (Laird, Newell & Rosenbloom, 1987; Newell, 1990), do not attempt to explain `consciousness', even though deliberative thought processes are also, unless communicated, private phenomena. If cognition can be modelled and replicated without reference to theories of `consciousness' then so can the emotions. <p> The following sections critically review a small subset of those designs, concentrating on reactive-deliberative architectures. 4.2.1 Deliberative architectures Very briefly, deliberative agent architectures are programs that perform something like deductive reasoning in order to solve problems, such as classical planning systems (e.g., STRIPS, NOAH, NONLIN etc., see chapter 9 of <ref> (Charniak & McDer-mott, 1985) </ref>) or models of cognition, such as GPS (section 5.7 of (Charniak & McDermott, 1985)), the SOAR architecture (Laird, Newell & Rosenbloom, 1987; Newell, 1990), Anderson's ACT-R (Webmaster, 1996) and blackboard architectures (see section 2.2.2 of (Beaudoin, 1994)). <p> architectures. 4.2.1 Deliberative architectures Very briefly, deliberative agent architectures are programs that perform something like deductive reasoning in order to solve problems, such as classical planning systems (e.g., STRIPS, NOAH, NONLIN etc., see chapter 9 of (Charniak & McDer-mott, 1985)) or models of cognition, such as GPS (section 5.7 of <ref> (Charniak & McDermott, 1985) </ref>), the SOAR architecture (Laird, Newell & Rosenbloom, 1987; Newell, 1990), Anderson's ACT-R (Webmaster, 1996) and blackboard architectures (see section 2.2.2 of (Beaudoin, 1994)). Such systems can be usefully conceived as agents when they attempt to control parts of a real or simulated world.
Reference: <author> Cichosz, P. </author> <year> (1994). </year> <title> Reinforcement learning algorithms based on the methods of temporal differences. </title> <type> Master's thesis, </type> <institution> Institute of Computer Science, Warsaw University of Technology. </institution>
Reference-contexts: The main design problem to be solved in RL is the credit assignment problem, which is the problem of `properly assigning credit or blame for overall outcomes to each of the learning system's internal decisions that contributed to those outcomes' (R. S. Sutton, quoted in <ref> (Cichosz, 1994) </ref>).
Reference: <author> Cohen, J. & Stewart, I. </author> <year> (1994). </year> <title> The collapse of chaos, discovering simplicity in a complex world. </title> <publisher> Viking. </publisher>
Reference-contexts: Designing conventions amounts to designing a set of rules that can interact to produce coherent and useful emergent behaviour. Even simple rules, such as those that define Langton's ant, can produce emergent behaviours that are very difficult to deduce from the rules themselves <ref> (Cohen & Stewart, 1994) </ref>. All coordination mechanisms may be reducible to, or expressible as, collections of commitments and conventions, but discovering useful coordination mechanisms is no easy task.
Reference: <author> Cziko, G. </author> <year> (1995). </year> <title> Without Miracles, universal selection and the second Darwinian revolution. </title> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: This is the corresponding movement in design-space. The design elements of an adaptive thermostat performing trial and error learning correspond to a selective system (Pepper, 1958), which is the abstract schema for an adaptive system <ref> (Cziko, 1995) </ref>.
Reference: <author> Davis, D. N. </author> <year> (1996). </year> <title> Reactive and motivational agents: towards a collective min-der. In Agent Theories, Architectures, </title> <booktitle> and Languages, the Third International Workshop, </booktitle> <address> Budapest, Hungary. ECAI-96. </address> <note> 218 Davis, </note> <author> D. N., Sloman, A., & Poli, R. </author> <year> (1995). </year> <title> Simulating agents and their environ-ments. </title> <journal> AISB Quarterly. </journal>
Reference: <author> Dawkins, R. </author> <year> (1986). </year> <title> The blind watchmaker. </title> <publisher> Harlow: Longman. </publisher>
Reference: <author> Dawkins, R. </author> <year> (1990). </year> <title> The selfish gene. </title> <address> New York: </address> <publisher> Oxford Unviversity Press. </publisher>
Reference: <author> Dawson, M. R. W. & Shamanski, K. S. </author> <year> (1994). </year> <title> Connectionism, confusion, </title> <journal> and cognitive science. The Journal of Intelligent Systems, </journal> <volume> 4 </volume> <pages> 215-262. </pages>
Reference-contexts: Also, connectionist learning systems normally use numerical representations and approximated continuous functions. Continuous functions have a derivative and, due to this property, calculus can be used to determine rules that manipulate weights to perform a gradient descent in an error space <ref> (Dawson & Shamanski, 1994) </ref>. A deeper analysis would provide reasons why real numbers are important for inductive learning algorithms.
Reference: <author> Dennett, D. C. </author> <year> (1978). </year> <title> Brainstorms: </title> <booktitle> Philosophical Essays on Mind and Psychology. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Dennett, D. C. </author> <year> (1991). </year> <title> Consciousness Explained. </title> <editor> Allen Lane: </editor> <publisher> Penguin Press. </publisher>
Reference-contexts: This thesis is concerned with explanations at what Sloman (1994b) has called the information processing level of abstraction, or `information level', a level concerned with both agent designs (i.e., part of Dennett's `design stance' <ref> (Dennett, 1991) </ref>) and the semantic content of information that is acquired, created, manipulated, stored and used by those agents (Sloman, 1995g). Information level explanations operate at a level of abstraction `higher' than the physical level but `lower' than Newell's `knowledge level' (Newell, 1990) and Dennett's `intentional stance' (Dennett, 1991). <p> Dennett's `design stance' <ref> (Dennett, 1991) </ref>) and the semantic content of information that is acquired, created, manipulated, stored and used by those agents (Sloman, 1995g). Information level explanations operate at a level of abstraction `higher' than the physical level but `lower' than Newell's `knowledge level' (Newell, 1990) and Dennett's `intentional stance' (Dennett, 1991). Knowledge level and intentional stance explanations pre 21 suppose the rationality of agents (i.e., the agent's actions are reliably determined by a rational relation between its knowledge and goals), whereas the information level does not.
Reference: <author> Dennett, D. C. </author> <year> (1996). </year> <title> Do animals have beliefs? In Roitblat, </title> <editor> H. L. & Meyer, J.- A. (Eds.), </editor> <booktitle> Comparative approaches to cognitive science, </booktitle> <pages> pages 111-118. </pages> <address> Cam-bridge, Massachusetts: </address> <publisher> "A Bradford Book" The MIT Press. </publisher> <editor> d'Inverno, M. & Luck, M. </editor> <year> (1996). </year> <title> Formalizing the contract net protocol as a goal-directed system. </title> <editor> In de Velde, W. V. & Perram, J. W. (Eds.), </editor> <booktitle> Agents Breaking Away, Proceedings of the 7th European Workshop on MAAMAW, Lecture Notes on Artificial Intelligence, </booktitle> <volume> No. 1308, </volume> <pages> pages 72-85. </pages> <address> Berlin: </address> <publisher> Springer. </publisher>
Reference: <author> Donnart, J. Y. & Meyer, J. A. </author> <year> (1994). </year> <title> A hierarchical classifier system implementing a motivationally autonomous animat. </title> <booktitle> In From Animals to Animats III, Proceedings of the Third International Conference on the Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Classifier systems have been extensively used in the Artificial Life (ALife) community (Steels, 1994), for example in developing control programs for robots through supervised learning (Dorigo & Colombetti, 1993) and learning paths through mazes <ref> (Donnart & Meyer, 1994) </ref>. However, a classifier system is limited in many ways. It does not have an explicit memory store. It tends to be an entirely reactive system with no representation of goals. It does not anticipate, or perform prior search within a world model before acting.
Reference: <author> Dorigo, M. & Bersini, H. </author> <year> (1994). </year> <title> A comparison of q-learning and classifier systems. </title> <booktitle> In From Animals to Animats III: Proceedings of the Third Conference on Simulation of Adaptive Behaviour. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: No complementary guidance is provided for helping the exploration/exploitation of the problem space, and therefore the agent can rely only on a trial-and-error strategy. <ref> (Dorigo & Bersini, 1994) </ref> Kaelbling, Littman & Moore (1995) provide a standard description of the RL situation. ... an agent is connected to its environment via perception and action ...
Reference: <author> Dorigo, M. & Colombetti, M. </author> <year> (1993). </year> <title> Robot shaping: developing situated agents through learning. </title> <type> Technical Report TR-92-040, </type> <institution> International Computer Science Institute, Berkeley, CA. </institution> <note> Revised version. </note>
Reference-contexts: Normally, the GA is applied less frequently than the bb 131 otherwise new classifiers will not have had sufficient time to be evaluated. Classifier systems have been extensively used in the Artificial Life (ALife) community (Steels, 1994), for example in developing control programs for robots through supervised learning <ref> (Dorigo & Colombetti, 1993) </ref> and learning paths through mazes (Donnart & Meyer, 1994). However, a classifier system is limited in many ways. It does not have an explicit memory store. It tends to be an entirely reactive system with no representation of goals.
Reference: <author> Dorigo, M., Maniezzo, V., & Colorni, A. </author> <year> (1996). </year> <title> The ant system: optimization by a colony of cooperating agents. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 26(1) </volume> <pages> 1-13. </pages>
Reference: <author> Doyle, J. </author> <year> (1989). </year> <title> Reasoning, representation, and rational self-government. </title> <editor> In Ras, Z. W. (Ed.), </editor> <booktitle> Methodologies for intelligent systems, </booktitle> <pages> pages 367-380. </pages> <address> New York: </address> <publisher> Elsevier Science Publishing. </publisher> <address> 219 Doyle, J. </address> <year> (1994). </year> <title> A reasoning economy for planning and replanning. </title> <booktitle> In Technical papers of the ARPA Planning Initiative Workshop. </booktitle>
Reference-contexts: Other work in decision-theoretic control (rational deliberation under resource limitations), such as anytime algorithms (Boddy & Dean, 1989), amended utility theory (Horvitz, Gregory & Heckerman, 1989; Miranda, 1996), algorithms that trade quality of solution with speed of response (Lesser, Pavlin & Durfee, 1989), or rational self-government <ref> (Doyle, 1989) </ref>, is also of relevance for developing a theory of motive management. Armed with such a theory MINDER1's primitive motive management mechanisms could be improved. 5.5 Emergent processing states There is much philosophical wringing of hands over the meaning of the term `emergence' and what it might really mean.
Reference: <author> Dyer, M. G. </author> <year> (1987). </year> <title> Emotions and their computations: Three computer models. </title> <journal> Cognition and Emotion, </journal> <volume> 1(3) </volume> <pages> 323-347. </pages>
Reference: <author> Etzioni, O. </author> <year> (1993). </year> <title> Intelligence without robots (a reply to brooks). </title> <journal> Artificial Intelligence Magazine. </journal>
Reference: <author> Fehling, M. R., Altman, A. M., & Wilber, B. M. </author> <year> (1989). </year> <title> The heuristic control virtual machine: an implementation of the schemer computational model of reflective, real-time problem-solving. In Blackboard Architectures and Applications. </title> <publisher> Academic Press, Inc. </publisher>
Reference-contexts: Examples of architectures that begin to com-bine deliberative control and reactive control are Firby's RAP system, Ferguson's TouringMachines, Pryor's PARETO, the Oz Project's HAP (Loyall & Bates, 1991; Reilly, 1993), Hayes-Roth's Intelligent Adaptive Systems (Hayes-Roth, 1990; Hayes-Roth, 1991a; Hayes-Roth, 1991b; Hayes-Roth, 1993b; Hayes-Roth, 1993a), Fehling's heuristic control virtual machine <ref> (Fehling, Altman & Wilber, 1989) </ref>, Georgeff's PRS and Benson and Nilsson's TRP architecture. architecture that subsumes a lower level reactive architecture. An attention filter protects deliberative resources from demands that may not be very urgent or important.
Reference: <author> Ferguson, I. A. </author> <year> (1992). </year> <title> TouringMachines: An architecture for dunamic, rational, mobile agents. </title> <type> PhD thesis, </type> <institution> University of Cambridge Computer Laboratory, University of Cambridge. </institution> <note> Technical report No. 273. </note>
Reference-contexts: Their layered architecture is still in development and they provide few details of the deliberation system. 4.2.3.2 TouringMachines: Ferguson Another example of a reactive-deliberative architecture is Ferguson's TouringMa-chine <ref> (Ferguson, 1992) </ref>. The TouringMachine is an integrated software control architecture designed for controlling the actions of autonomous agents operating in complex environments; in particular, the TouringWorld, a multi-agent traffic domain. The design consists of separate activity producing behaviours in a layered control framework, which resembles the behaviour-based approach of Brooks.
Reference: <author> Firby, R. J. </author> <year> (1987). </year> <title> An investigation into reactive planning in complex domains. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 202-206, </pages> <address> Seattle. </address> <publisher> AAAI. </publisher>
Reference: <author> Firby, R. J. </author> <year> (1989). </year> <title> Adaptive execution in complex dynamic worlds. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Yale University. </institution>
Reference: <author> Franklin, S. & Graesser, A. </author> <year> (1996). </year> <title> Is it an agent, or just a program?: A taxonomy for autonomous agents. </title> <booktitle> In Proceedings of the Third International Workshop on Agent Theories, Architectures, and Languages. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: is content to use Franklin's definition of an agent: An autonomous agent is a system situated within and a part of an environment that senses that environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future. <ref> (Franklin & Graesser, 1996) </ref> On this definition a simple thermostat, a negative feedback control system, is an agent. However, most AI agents are much more complex than a thermostat.
Reference: <author> Freud, S. </author> <year> (1987). </year> <title> On sexuality: three essays on the theory of sexuality and other works, volume VII of The Pelican Freud Library. </title> <publisher> Penguin Books. </publisher>
Reference-contexts: Freud writes - We have defined the concept of `libido' as a quantitatively variable force which could serve as a measure of processes and transformations occur ring in the field of sexual excitation. S. Freud, Three Essays on the Theory of Sexuality (1905), p. 138 of <ref> (Freud, 1987) </ref>. Therefore, libido is also quantitative; it becomes attached to objects in definite amounts.
Reference: <author> Freud, S. </author> <year> (1991). </year> <title> On metapychology: the theory of psychoanalysis, volume III of The Penguin Freud Library. </title> <publisher> Penguin Books. </publisher>
Reference-contexts: It corresponds to the instinct in so far as the latter has become detached from the idea and finds expression, proportionate in its quantity, in processes which are sensed as affects. 187 S. Freud, Repression (1915), p. 152 of <ref> (Freud, 1991) </ref>. It is unclear what is the precise meaning of affect is in this context. <p> Freud writes - ... the mechanisms of repression [conscious or ego-based suppression of motives] have at least this one thing in common: a withdrawal of the cathexis of energy (or of libido, where we are dealing with sexual instincts). S. Freud, Repression (1915), p. 154-5 of <ref> (Freud, 1991) </ref>. This quotation (and others) are often ambiguous as to what causes change. <p> Freud, Beyond the Pleasure Principle (1920), p. 337 of <ref> (Freud, 1991) </ref>. 188 This quotation conflates a number of phenomenological issues. The huge diver-sity of phenomenological phenomena that can be classified as either `pleasurable' or `unpleasurable' are treated together, without an attempt to distinguish cases.
Reference: <author> Frijda, N. H. </author> <year> (1986). </year> <title> The Emotions. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: These emotional states are all different in important ways. However, what they all have in common is control precedence <ref> (Frijda, 1986) </ref>, that is attention tends to be grabbed. Both the griever and excited anticipator find it difficult to turn their thoughts to other matters. This is not to imply that people can never learn to control their emotions. <p> The source of motives in MINDER1 is a suite of generactivators that express the agent's concerns <ref> (Frijda, 1986) </ref>. <p> If ... running during fear is rewarded, it tends to increase. The same holds for aggression. Success in fighting, in rats, produces animals that are more aggressive, whereas a history of defeats rather tends to induce subsequent submissiveness <ref> (Frijda, 1986) </ref>. Despite these remarks the CR theory does not integrate designs for learning mechanisms with the concern realisation system. The same is true of the other interrupt theories. A related point is the correlation between learning and the intensity of an emotion. <p> Frijda stresses the importance of the hedonic tone of emotional states, stating that `it is pure silliness to essay explanations of such experiences as mere organic sensations or cognitive assessments or some combination of both' <ref> (Frijda, 1986) </ref>. He agrees that both physiological reduction and intentional emergence provide inadequate explanations. The CR theory posits relevance signals of pleasure, pain, wonder or desire that occur when an event is compared to the satisfaction conditions of various concerns. <p> These motives may attempt to meet conditions of satisfaction of reinforcers, or may detect threats or opportunities relevant to current attachment plans. They are examples of Frijda's concerns <ref> (Frijda, 1986) </ref>. For example, particular libidinal generactivators may detect threats to attachment plans, such as interest from other sexually active males or females towards the loved one. 169 9.2.3 A conative universal equivalent (g) A scalar quantity form of value, the conative universal equivalent (CUE). <p> The CLE in its current form does not account for shorter-term control signals involved in initiating, preserving or terminating action tendencies (section 6.3.1 and <ref> (Frijda, 1986) </ref>), for example the pleasure of the warm sun on one's brow, the pain of a headache, or the sting of tired muscles. 180 9.4.2 The emotional learning problem As stated, interrupt theories do not provide mechanisms that integrate emotional states and learning. <p> This explanation links the `strength' of a motivational concern to the intensity of the valency experienced when the concern is met or violated (see <ref> (Frijda, 1986) </ref>, pages 340-342). A concern, implemented as a libidinal generactivator (see section 5.3.2.4), has motivational `strength' to the extent that it can buy processing power and dispositionally determine behaviour. Its causal power depends on its accumulated CUE.
Reference: <author> Frijda, N. H. </author> <year> (1995). </year> <title> The irrationality of emotions. Paper read at the conference on Philosophy and Psychology, </title> <institution> University of Oxford, </institution> <year> 1995. </year>
Reference-contexts: In the abstract, the requirements for MINDER1 are the same as those for human-like autonomy; hence, perturbant states have greater claim to model actual aspects of human information processing. Other models have difficulties making such claims. An exception is Moffat and Frijda's WILL architecture <ref> (Moffat & Frijda, 1995) </ref>, partly similar to MINDER1, and a design for a concern realisation system (see section 6.3.1). However, WILL does not exhibit protoemotional states. Also, Botelho & Coelho (1996) describe an agent architecture that can interrupt its attention in response to new environmental contingencies. <p> Neither is the CR theory specified in as much design detail as the AFP theory. The CR and AFP theories both share the view that the word `emotion' does not refer to a `natural class' <ref> (Frijda, 1995) </ref>, that it fails to refer to a well-defined class of phenomena clearly distinguishable from other mental and behavioural events. <p> I assert that they form a major part of what the word "emotion" refers to, and this of course is an object (the only object) of possible dispute' <ref> (Frijda, 1995) </ref>. There are three main differences between the CR and AFP theories. <p> This point will be discussed in more detail later. To date there are two partial computational realisations of the CR theory: the ACRES system (Frijda & Swagerman, 1987) and the agent architecture Will <ref> (Moffat & Frijda, 1995) </ref>. These systems were discussed in section 5.6. 99 6.3.2 Cognitive broadcasts: Oatley and Johnson-Laird (Oatley & Johnson-Laird, 1985; Oatley, 1992; Johnson-Laird, 1988) presents a communicative theory of emotions (COM). <p> This section outlines a number of related criticisms of interrupt theories, providing examples from the reviewed theories where appropriate. 6.4.1 Disruption, interruption and control precedence Emotions are sometimes irrational <ref> (Frijda, 1995) </ref>: they are unpremeditated, the agent has a passive relation to their occurrence or non-occurrence, unlike thoughts and actions that can be rationally dependent on a desire to have them; and often there are better alternatives than reacting in an emotional manner, from both a moral and a selfish standpoint,
Reference: <author> Frijda, N. H. & Swagerman, J. </author> <year> (1987). </year> <title> Can computers feel? theory and design of an emotional system. </title> <journal> Cognition and Emotion, </journal> <volume> 1 </volume> <pages> 235-257. </pages> <note> 220 Gasser, </note> <author> L. </author> <year> (1991). </year> <title> Social conceptions of knowledge and action: Dai foundations and open systems semantics. </title> <journal> Artificial Intelligence, </journal> 47:107-138. 
Reference-contexts: Frijda and Swagerman's ACRES system <ref> (Frijda & Swagerman, 1987) </ref> is a computer program that stores facts about emotions and reasons about those facts, but, in addition, has various goals or concerns (see section 6.3.1) that it attempts to meet, such as a concern to have correctly typed input. <p> major phenomena that constitute emotions are the existence of feelings of pleasure and pain, processes of appraisal based on concerns, the presence of innate, pre-programmed behaviours in addition to complex constructed plans for achieving emotion goals, the occurrence of behavioural interruption and disturbance, and the control precedence of emotion goals <ref> (Frijda & Swagerman, 1987) </ref>. An emotion can be generated by a process of appraisal that analyses a situation to detect whether there is a match or mismatch between events and concerns. <p> Pleasure and pain can be understood as the signals for match and mismatch. ... They imply action: `Pleasurable' indicates that the thing is welcomed, painful that the event is not as events should be. They are penetrant; they persist; they are compelling with respect to action control <ref> (Frijda & Swagerman, 1987) </ref>. The appraisal process, therefore, generates internal signals, such as pleasure and pain relevance signals, and also a control precedence signal. <p> The general action system can ward off being dominated by the concern realisation system, but necessarily at a cost <ref> (Frijda & Swagerman, 1987) </ref>. It is not clear whether Frijda would agree that an emotion could be present 98 without itself manifesting in attention or behaviour, or whether some degree of dis-traction or `nervousness' is required. <p> Finally, Frijda emphasises the pleasure and pain dimensions of emotional states, whereas the AFP theory does not. This point will be discussed in more detail later. To date there are two partial computational realisations of the CR theory: the ACRES system <ref> (Frijda & Swagerman, 1987) </ref> and the agent architecture Will (Moffat & Frijda, 1995). These systems were discussed in section 5.6. 99 6.3.2 Cognitive broadcasts: Oatley and Johnson-Laird (Oatley & Johnson-Laird, 1985; Oatley, 1992; Johnson-Laird, 1988) presents a communicative theory of emotions (COM).
Reference: <author> Georgeff, M. P. & Ingrand, F. F. </author> <year> (1989). </year> <title> Decision-making in an embedded reasoning system. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 972-978, </pages> <address> Detroit, MI. IJCAI. </address>
Reference-contexts: goals is based on a heuristic of high reward sooner rather than later; therefore, situations could occur where the agent pursues a rewarding goal at the expense of a less rewarding deadline goal, resulting in, for example, a missed train. 4.2.3.5 Procedural reasoning systems: Georgeff Georgeff's Procedural Reasoning System (PRS) <ref> (Georgeff & Ingrand, 1989) </ref> aims to achieve a balance between acting and decision making. It has many features in common with Firby's RAP system but is designed to provide more powerful mechanisms for balancing decision making requirements against the constraints on time and information that are typical of complex domains. <p> Such knowledge is encoded in metalevel KAs and can be invoked when needed; however, these processes are themselves interruptible; therefore, reactivity is maintained. The PRS architecture has been implemented within a physical robot concerned with navigation and emergency tasks <ref> (Georgeff & Lansky, 1989) </ref> and has also been used for fault isolation and diagnosis in the Space Shuttle.
Reference: <author> Georgeff, M. P. & Lansky, A. L. </author> <year> (1989). </year> <title> Reactive reasoning and planning. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 677-682, </pages> <address> Seattle. </address> <publisher> AAAI. </publisher>
Reference-contexts: goals is based on a heuristic of high reward sooner rather than later; therefore, situations could occur where the agent pursues a rewarding goal at the expense of a less rewarding deadline goal, resulting in, for example, a missed train. 4.2.3.5 Procedural reasoning systems: Georgeff Georgeff's Procedural Reasoning System (PRS) <ref> (Georgeff & Ingrand, 1989) </ref> aims to achieve a balance between acting and decision making. It has many features in common with Firby's RAP system but is designed to provide more powerful mechanisms for balancing decision making requirements against the constraints on time and information that are typical of complex domains. <p> Such knowledge is encoded in metalevel KAs and can be invoked when needed; however, these processes are themselves interruptible; therefore, reactivity is maintained. The PRS architecture has been implemented within a physical robot concerned with navigation and emergency tasks <ref> (Georgeff & Lansky, 1989) </ref> and has also been used for fault isolation and diagnosis in the Space Shuttle.
Reference: <author> Gerson, E. M. </author> <year> (1976). </year> <title> On `quality of life'. </title> <journal> American Sociological Review, </journal> <volume> 41 </volume> <pages> 793-806. </pages>
Reference-contexts: solving that necessitate the imposition of limits on local problem solvers: Hobbes chairs the Socratic dialogue. `Participation in any situation, therefore, is simultaneously constraining, in that people must make contributions to it, and be bound by its limitations, and yet enriching, in that participation provides resources and opportunities otherwise unavailable' <ref> (Gerson, 1976) </ref>. Social agents commit to a social convention of money that simultaneously constrains and enriches possible local outcomes. 8.4.6 Dynamic reallocation of labour As stated, an AMAS may need to frequently reallocate agents to different tasks in order to meet global goals and maintain coherent behaviour.
Reference: <author> Goldfarb, L. & Nigam, S. </author> <year> (1994). </year> <title> The unified learning paradigm: a foundation for ai. </title> <editor> In Hanovar, V. & Uhr, L. (Eds.), </editor> <title> Artificial Intelligence and Neural Networks: Steps towards Principled Integration. </title> <address> Boston, MA: </address> <publisher> Academic Press. </publisher>
Reference-contexts: There may be deeper mathematical reasons why a scalar quantity of value is useful in inductive learning systems, but a full analysis is beyond this author's competence. Clues can be found in <ref> (Goldfarb & Nigam, 1994) </ref>, which argues that real number metrics, in addition to symbolic representations, are required in general for inductive learning. Also, connectionist learning systems normally use numerical representations and approximated continuous functions.
Reference: <author> Gouldner, A. W. </author> <year> (1960). </year> <title> The norm of reciprocity: a preliminary statement. </title> <journal> American Sociological Review, </journal> <volume> 25(2) </volume> <pages> 161-178. </pages>
Reference-contexts: The Baldwin Effect (Baldwin, 1896) predicts that adaptive, acquired traits tend to be genetically assimilated over time. Assimilated TIT-FOR-TAT may form the foundation for certain social emotions, such as gratitude and guilt, which facilitate cooperative behaviour in natural MAS. The existence of a universal `norm of reciprocity' <ref> (Gouldner, 1960) </ref> in social behaviour has been known for some time in sociological theory.
Reference: <author> Green, O. H. </author> <year> (1992). </year> <title> The Emotions, a philosophical theory. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: The obverse of the control precedence of emotions is that a person is normally in a passive relation to them. For example, unlike actions that are causally and rationally dependent on a person's desires, emotions cannot be so instigated <ref> (Green, 1992) </ref>. Ignoring dissimulation and the abilities of professional actors, one does not normally choose to be angry, sad or disappointed. It is appropriate to command someone to perform an action, but inappropriate to command someone to have an emotion (Green, 1992). <p> dependent on a person's desires, emotions cannot be so instigated <ref> (Green, 1992) </ref>. Ignoring dissimulation and the abilities of professional actors, one does not normally choose to be angry, sad or disappointed. It is appropriate to command someone to perform an action, but inappropriate to command someone to have an emotion (Green, 1992). It is these kinds of facts that fit well with `interrupt of attention' theories of emotion. Two are reviewed here as a preliminary to a partial computational implementation of Sloman's attention filter penetration theory (AFP) in chapter 5. <p> A number of design-based theories of emotion are reviewed, followed by extensive critical comments. The criticisms of existing design-based theories motivate part III of the thesis. 6.1 Green's classification of emotion theories The philosopher O. H. Green provides a useful classification of emotion theories <ref> (Green, 1992) </ref>. Green classifies emotion theories according to the account they give of emotional intentionality and the relation of emotions to non-intentional phenomena. In philosophical language, `intentionality' means that a mental content, such as a belief, is `about' something or represents something, such as a belief about today's sunny weather. <p> The distinction made by philosophers and psychologists between beliefs (cognition) and desires (conation), where `the success of cognitive representations depends on the way the world is, that of conative representations on the way the world is made to be' <ref> (Green, 1992) </ref>, is directly mirrored in control system architecture from the very simplest negative feedback loop to the more complex agent 93 architectures developed by AI researchers. <p> Phenomena-based and semantic-based theories of emotion are ignored in the literature review for the reasons given in section 2.3.1 and section 5.6. 6.2 A philosophical theory: Green Green's belief-desire theory views emotions as relations between beliefs and desires: they are `intentional structures of beliefs and desires' <ref> (Green, 1992) </ref>. For an emotion to occur a set of beliefs and a set of desires must be about a common topic and semantically interrelated.
Reference: <author> Hanks, S. & Firby, R. J. </author> <year> (1990). </year> <title> Issues and architectures for planning and execution. In Proceedings of a Workshop on Innovative Approaches to Planning, Scheduling and Control, </title> <address> San Diego, CA. </address> <publisher> DARPA. </publisher>
Reference-contexts: Also, the RAP system cannot think ahead. Both these limitations point to the need for an extra layer of control that places constraints on RAP behaviour prior to execution; in other words, neither urgency or uncertainty obviate the need for more deliberative decision making. Hence, in <ref> (Hanks & Firby, 1990) </ref> the RAP system is extended by considering the extra, deliberative layer of control needed in an autonomous agent. The addition of planning ahead and reasoning abilities generates two new design problems: how to deliberate, and how to coordinate deliberation and reactive execution.
Reference: <author> Hayes-Roth, B. </author> <year> (1990). </year> <title> Architectural foundations for real-time performance in intelligent agents. </title> <journal> Journal of Real-Time Systems, </journal> <volume> 2 </volume> <pages> 99-125. </pages>
Reference: <author> Hayes-Roth, B. </author> <year> (1991a). </year> <title> Evaluation of integrated agent architectures. </title> <journal> SIGART Bulletin, </journal> <volume> 2(4) </volume> <pages> 82-84. </pages>
Reference: <author> Hayes-Roth, B. </author> <year> (1991b). </year> <title> An integrated architecture for intelligent agents. </title> <journal> SIGART Bulletin, </journal> <volume> 2(4) </volume> <pages> 79-81. </pages>
Reference: <author> Hayes-Roth, B. </author> <year> (1993a). </year> <title> An architecture for adaptive intelligent systems. </title> <type> Technical Report 93-19, </type> <institution> Knowledge Systems Laboratory, Department of Computer Science, Stanford University. </institution>
Reference: <author> Hayes-Roth, B. </author> <year> (1993b). </year> <title> Intelligent control. </title> <journal> Artificial Intelligence, </journal> <volume> 59 </volume> <pages> 213-220. </pages> <note> 221 Heckhausen, </note> <author> H. & Kuhl, J. </author> <year> (1985). </year> <title> From wishes to action: the dead ends and short cuts on the long way to action. </title> <editor> In Frese, M. & Sabini, J. (Eds.), </editor> <title> Goal Directed Behavior: The Concept of Action in Psychology. </title> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> Hofstadter, D. & Dennett, D. C. </author> <year> (1981). </year> <title> The Mind's I: Fantasies and Reflections on Self and Soul. </title> <address> Brighton: </address> <publisher> The Harvester Press Ltd. </publisher>
Reference: <author> Hofstadter, D. R. </author> <year> (1979). </year> <title> Godel, Escher, Bach: an Eternal Golden Braid. </title> <publisher> Penguin Books. </publisher>
Reference: <author> Holland, J. H. </author> <year> (1975). </year> <booktitle> Adaption in natural and artificial systems. </booktitle> <publisher> The MIT Press. </publisher>
Reference-contexts: The system, therefore, needs an additional learning algorithm that can discover new classifiers. One can envisage many possible types of discovery algorithm. Holland chose the genetic algorithm <ref> (Holland, 1975) </ref>, which is based on Darwinian natural selection. Genetic algorithms are widely used and well-known, so their operation will be only briefly summarised here. First, a subset of the classifier list is selected to act as parents.
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: the possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine learning, an artificial intelligence approach. </booktitle> <address> Los Altos, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A rule may fire if its conditions hold in the workspace. Information processing occurs through the continual firing of applicable rules. Rule-based systems normally do not alter their rules at run-time. J. Holland, dissatisfied with the `brittleness' of existing rule-based systems <ref> (Holland, 1986) </ref>, developed the classifier system in the early 1970's. It specifies a class of rule-based systems that adaptively alter their rules at run-time through interaction with a domain. <p> Accuracy allows XCS to evolve maximally general classifiers, those that could not have any more #'s without becoming inaccurate. A number of other researchers have incorporated accuracy information in their classifier systems (for an overview see (Wilson, 1995)). Holland, in the original paper on classifier systems <ref> (Holland, 1986) </ref>, suggested that accuracy information be incorporated in calculating classifier fitness, but did not develop the idea. In XCS the traditional bucket-brigade has been replaced with a learning algorithm similar to Q-learning. <p> Behaviour-producing components with high reward will be more likely to dispositionally determine the be-haviour of the system in the future than those components with low reward. For example, the bucket-brigade algorithm used in early classifier systems was inspired by an economic metaphor <ref> (Holland, Holyoak, Nisbett & Thagard, 1986) </ref>, in which system rules are agents consuming and producing internal messages (commodities) who each possess a certain amount of strength (money) which they exchange for messages at a global blackboard (the market).
Reference: <author> Holland, J. H. </author> <year> (1995). </year> <title> Hidden Order, how adaptation builds complexity. </title> <publisher> Helix Books. </publisher>
Reference-contexts: Internal messages, less directly linked to sensing or acting, will have more complex representational roles within the system. The semantics of messages depends on the dynamic relationship between message 2 The artificial frog is taken from <ref> (Holland, 1995) </ref>. 174 and environment. For example, the sensory message may match a classifier that posts an action message that results in simfrog throwing its sticky tongue in the direction of the detected fly.
Reference: <author> Holland, J. H., Holyoak, K. J., Nisbett, R. E., & Thagard, P. R. </author> <year> (1986). </year> <title> Induction: processes of inference, learning and discovery. </title> <publisher> The MIT Press. </publisher>
Reference-contexts: A rule may fire if its conditions hold in the workspace. Information processing occurs through the continual firing of applicable rules. Rule-based systems normally do not alter their rules at run-time. J. Holland, dissatisfied with the `brittleness' of existing rule-based systems <ref> (Holland, 1986) </ref>, developed the classifier system in the early 1970's. It specifies a class of rule-based systems that adaptively alter their rules at run-time through interaction with a domain. <p> Accuracy allows XCS to evolve maximally general classifiers, those that could not have any more #'s without becoming inaccurate. A number of other researchers have incorporated accuracy information in their classifier systems (for an overview see (Wilson, 1995)). Holland, in the original paper on classifier systems <ref> (Holland, 1986) </ref>, suggested that accuracy information be incorporated in calculating classifier fitness, but did not develop the idea. In XCS the traditional bucket-brigade has been replaced with a learning algorithm similar to Q-learning. <p> Behaviour-producing components with high reward will be more likely to dispositionally determine the be-haviour of the system in the future than those components with low reward. For example, the bucket-brigade algorithm used in early classifier systems was inspired by an economic metaphor <ref> (Holland, Holyoak, Nisbett & Thagard, 1986) </ref>, in which system rules are agents consuming and producing internal messages (commodities) who each possess a certain amount of strength (money) which they exchange for messages at a global blackboard (the market).
Reference: <author> Horvitz, E. J., Gregory, F. C., & Heckerman, D. E. </author> <year> (1989). </year> <title> Reflection and action under scarce resources: theoretical principles and empirical study. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <volume> vol. </volume> <pages> 2. </pages>
Reference: <author> Humphreys, M. </author> <year> (1996). </year> <title> Action selection methods using reinforcement learning. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., & Wilson, S. W. (Eds.), </editor> <booktitle> From Animals to Animats IV, Proceedings of the Fourth International Conference on the Simulation of Adaptive Behavior. </booktitle> <publisher> The MIT Press. </publisher>
Reference-contexts: Examples of RL algorithms are Q-learning ((Watkins & Dayan, 1992) and section 7.4.2), classifier systems ((Holland, 1975; Holland, Holyoak, Nisbett & Thagard, 1986; Wilson, 1995) and section 7.4.3), and W-learning <ref> (Humphreys, 1996) </ref>. Marvin Minsky's Snarc machine was an early reinforcement learner that encountered the credit-assignment problem (see section 7.6 of (Minsky, 1987)). <p> The work of Wellman (1995) and Doyle (1994) use economic markets for resource allocation problems. Baum's Hayek machine, Weiss' Dissolution and Formation of Groups algorithm, the Contract Net protocol (section 8.4.4), and Humphrey's W-learning <ref> (Humphreys, 1996) </ref> use computational `agents', price mechanisms and currency flow to learn intelligent behaviour in various task domains. The multi-agent system research community is actively exploring negotiation algorithms for competing and cooperating agents.
Reference: <author> Jennings, N. </author> <year> (1996). </year> <title> Coordination techniques for distributed artificial intelligence. </title>
Reference-contexts: For example, when a conflict is encountered the agents involved may generate proposals for joint commitments with associated explanations. The mooted proposals may then be evaluated, and various counter-proposals or compromises suggested. The Socratic dialogue continues until agreement is reached <ref> (Parsons & Jennings, 1996) </ref>. In order that local negotiations can meet global requirements there is need for local information, referring to those requirements, that can form a basis for controlling the negotiations. <p> One possible solution is a global controller that has a wider picture of the whole system and directs the activities of others; however, keeping the agent informed could entail high communication costs, create a communication bottleneck, and render the other agents unusable if the controller failed <ref> (Jennings, 1996) </ref>. The alternative is to distribute data and control, and economic systems suggest at least two possible mechanisms. On one side, a system composed of adaptive agents that attempt to maximise personal utility will exhibit distributed reorganisation of labour.
Reference: <editor> In O'Hare, G. & Jennings, N. (Eds.), </editor> <booktitle> Foundations of distributed artificial intelligence. </booktitle> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Johnson-Laird, P. N. </author> <year> (1988). </year> <title> The Computer and the Mind, an introduction to cognitive science. </title> <publisher> Fontana Press. </publisher>
Reference: <author> Kaelbling, L. P., Littman, M. L., & Moore, A. W. </author> <year> (1995). </year> <title> Reinforcement learning: a survey. </title> <booktitle> In Practice and Future of Autonomous Agents, </booktitle> <volume> volume 1. 222 Kagan, </volume> <editor> J. </editor> <year> (1978). </year> <title> On emotion and its development: a working paper. </title> <editor> In Lewis, M. & Rosenblum, L. A. (Eds.), </editor> <booktitle> The Development of Affect. </booktitle> <address> New York and London: </address> <publisher> Plenum Press. </publisher>
Reference-contexts: They are all examples of selective systems, and all exhibit is better than relations that order internal components with respect to norms. 7.4.1 Reinforcement learning algorithms The study of reinforcement learning (RL) algorithms is an active area of research <ref> (Kaelbling, Littman & Moore, 1995) </ref>. RL algorithms are selective systems as defined above. RL is a type of trial and error learning, and holds out the promise of 125 programming control programs for agents by reward and punishment without the need to specify how a task is to be achieved. <p> The agent's behaviour, B, should choose actions that tend to increase the long-run sum of values of the reinforcement signal. It can learn to do this over time by systematic trial and error, guided by a wide variety of algorithms ... <ref> (Kaelbling, Littman & Moore, 1995) </ref> 1995) that provides an intuitive way to understand the relation between the agent and its environment. <p> Normally, Dyna's RL component is a Q-learner. Typically, it requires more computation than Q-learning per cycle (depending on the value of K), but requires less interaction with the environment than Q-learning <ref> (Kaelbling, Littman & Moore, 1995) </ref>. Dyna incrementally deduces an optimal policy through hypothetical exploration of a world model while actually exploring the `real' world. 3 Normally planning means the production of an ordered list of primitive actions that transform an initial state into a goal state. <p> For example, they normally rely on the Markov assumption, which states that the environment state is fully observable and environment state transitions are independent of any previous states or agent actions <ref> (Kaelbling, Littman & Moore, 1995) </ref>.
Reference: <author> Kiss, G. </author> <year> (1992). </year> <title> Variable coupling of agents to their environment: combining situated and symbolic automata. </title> <editor> In Werner, E. & Demazeau, Y. (Eds.), Decentralized A.I.-3. </editor> <publisher> Elsevier Science Publishers. </publisher>
Reference: <author> Kittock, J. E. </author> <year> (1995). </year> <title> Emergent conventions and the structure of multiagent systems. </title> <editor> In Nadel, L. & Stein, D. (Eds.), </editor> <booktitle> 1993 Lectures in Complex Systems: the proceedings of the 1993 Complex Systems Summer School, Santa Fe Institute Studies in the Sciences of Complexity Lecture Volume VI. </booktitle> <address> Santa Fe Institute, </address> <publisher> Addison-Wesley Publishing Co. </publisher>
Reference: <author> Kovacs, T. </author> <year> (1996). </year> <title> Evolving optimal populations with xcs classifier systems. </title> <type> Technical Report CSR-96-17, </type> <institution> School of Computer Science, University of Birmingham. </institution>
Reference-contexts: Traditional classifier systems base classifier fitness on strength, which is used as a predictor of the expected payoff when selected. The change from using strength to using accuracy was motivated by a number of reasons and leads to a number of improvements in classifier system performance (see <ref> (Kovacs, 1996) </ref> for an extended review and analysis of the performance of XCS classifier systems). The first reason for using accuracy based fitness is that differing, yet useful, behaviours may have different payoff levels, both in terms of absolute magnitude and frequency of reward.
Reference: <author> Kraemer, G. W. </author> <year> (1992). </year> <title> A psychological theory of attachment. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 15 </volume> <pages> 493-541. </pages>
Reference-contexts: children grow their helplessness diminishes, along with the need to cry. (The picture becomes complicated later on, when the possibility of emotional deception is discovered | faking crying to manipulate others.) Also, the notion of `basic plans' as control mechanisms of last resort bears important similarities to Kraemer's `cascade hypothesis' <ref> (Kraemer, 1992) </ref>, which states that control will `cascade' down 207 to `genetically programmed neurobiological adaptive behaviours' if the organism is faced with `disasters' (problematic situations) that its `acquired behaviours' cannot deal with.
Reference: <author> Kuhl, J. </author> <year> (1992). </year> <title> A theory of self-regulation: action versus state orientation, self-discrimination, and some applications. </title> <journal> Applied Psychology: An International Review, </journal> <volume> 41(2) </volume> <pages> 97-129. </pages>
Reference: <author> Kuhl, J. & Kraska, K. </author> <year> (1989). </year> <editor> Self-regulation and metamotivation. In Kanfer, R., Ackerman, P. L., & Cudek, R. (Eds.), </editor> <title> Abilities, motivation, and methodology: </title> <booktitle> The Minnesota Symposium on Individual Differences, </booktitle> <pages> pages 343-374. </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates Inc. </publisher>
Reference: <author> Laird, J. E., Newell, A., & Rosenbloom, P. S. </author> <year> (1987). </year> <title> Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 1-64. </pages>
Reference-contexts: It is a forward-chaining production system interpreter, but provides a collection of unusual facilities, including a smooth interface to neural net or other `sub-symbolic' mechanisms, mechanisms for control to be transferred between collections of rules as the context changes (allowing SOAR-like <ref> (Laird, Newell & Rosenbloom, 1987) </ref> pushing and popping of contexts), and facilities for altering the allocation of processing resources to different rulesets (Sloman, 1995d).
Reference: <author> Lakatos, I. </author> <year> (1970). </year> <title> Falsification and the methodology of scientific research pro-grammes. </title> <editor> In Lakatos, I. & Musgrave, A. (Eds.), </editor> <booktitle> Criticism and the Growth of Knowledge, </booktitle> <pages> pages 91-196. </pages> <publisher> Cambridge University Press. </publisher>
Reference-contexts: There may never be a total ordering of merit among such theories, and the ordering may change over time as new phenomena are discovered. Objections to this approach are often based on a naive philosophy of science, or misplaced `physics envy'. (For a broader view see <ref> (Lakatos, 1970) </ref>, chapter 2 of (Sloman, 1978), and (Bhaskar, 1978)). The design-based approach draws its inspiration from software engineering and conceptual analysis in philosophy (see chapter 4 of (Sloman, 1978)).
Reference: <author> LeDoux, J. E. </author> <year> (1994). </year> <title> Emotion, memory and the brain. </title> <publisher> Scientific American, </publisher> <pages> pages 32-39. </pages>
Reference-contexts: Insistence heuristics can be similarly justified, in particular from the requirement for reactivity in dangerous domains; for example, there is strong evidence of `quick and dirty, emotional' processing pathways in the brain <ref> (LeDoux, 1994) </ref>). Also, existing psychologial theories of motivation describe mechanisms that assess the importance of goals before adoption (Heckhausen & Kuhl, 1985) and self-regulatory processes that control motive adoption (Kuhl, 1992; Kuhl & Kraska, 1989) (this is discussed more thoroughly in (Beaudoin, 1994)).
Reference: <author> Lesser, V. R., Pavlin, J., & Durfee, E. H. </author> <year> (1989). </year> <title> Approximate processing in real-time problem solving. </title> <editor> In Clearwater, S. (Ed.), </editor> <booktitle> Blackboard Architectures and 223 Applications. </booktitle> <publisher> Academic Press, </publisher> <address> Inc. </address> <booktitle> Reprinted with permission from AI Maga--zine, </booktitle> <volume> vol. 9, no. 1, </volume> <month> Spring </month> <year> 1988. </year>
Reference-contexts: Other work in decision-theoretic control (rational deliberation under resource limitations), such as anytime algorithms (Boddy & Dean, 1989), amended utility theory (Horvitz, Gregory & Heckerman, 1989; Miranda, 1996), algorithms that trade quality of solution with speed of response <ref> (Lesser, Pavlin & Durfee, 1989) </ref>, or rational self-government (Doyle, 1989), is also of relevance for developing a theory of motive management.
Reference: <author> Lichtenstein, P. M. </author> <year> (1983). </year> <title> An introduction to post-Keynesian and Marxian theories of value and price. </title> <publisher> London and Basingstoke: Macmillan Press. </publisher>
Reference-contexts: In contrast, the Marxist and to some extent the Keynesian tradition holds that value is an objective, social measure linked to the share of social labour expended in the production of the commodity <ref> (Lichtenstein, 1983) </ref>. The highly technical debates between these traditions continue to this day and have yet to be decisively resolved.
Reference: <author> Liebniz, G. W. </author> <year> (1991). </year> <institution> Monadology. Pittsburgh: 1991, University of Pittsburgh Press. </institution>
Reference-contexts: That being so, we should, on examining its interior, find only parts which work one upon another, and never anything by which to explain a perception. G. W. Leibniz (1646 - 1716) <ref> (Liebniz, 1991) </ref>. A person may tour the grounds of a university, and see buildings, faculties, schools, offices, lecturers and students, and yet still ask `Where is the university?' (Ryle, 1949).
Reference: <author> Loyall, A. B. & Bates, J. </author> <year> (1991). </year> <title> Hap a reactive, adaptive architecture for agents. </title> <type> Technical Report CMU-CS-91-147, </type> <institution> School of Computer Science, Carnegie Mel-lon University. </institution>
Reference-contexts: Early work, still exploring general principles, need not make any commitment to the implementation details of mechanisms; for example, a neutral stance is taken towards symbolic or connectionist engines. Progress can be made by starting with `broad but shallow' <ref> (Bates, Loyall & Reilly, 1991) </ref> architectures that combine many sorts of capabilities (such as perception, planning, goal management, and action). Each capability is initially implemented in a simplified fashion. Subsequent work gradually refines and deepens the implementations.
Reference: <author> Lyons, D. M. & Hendriks, A. J. </author> <year> (1992). </year> <title> Reactive planning. </title> <editor> In Shapiro, S. (Ed.), </editor> <booktitle> Encyclopedia of Artificial Intelligence (second ed.). </booktitle> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: For example, if an agent cannot be 39 certain of the effects of its actions, cannot make the assumption that the world will remain static while it thinks, and cannot assume it knows everything about the world, then a purely deliberative architecture will fail <ref> (Lyons & Hendriks, 1992) </ref>. For example, a planning agent may construct a plan based on out-of-date assumptions about the state of the world. Execution of such a plan is unlikely to achieve the agent's goal. <p> rightly abstracted from the problems of real-time behaviour and concentrated their efforts on the problems of intelligent deduction. 4.2.2 Reactive architectures In reaction to the unrealistic assumptions of classical planning, such as the assumption of an unchanging, static world, a new design paradigm arose in the mid-eighties, the behaviour-based approach <ref> (Lyons & Hendriks, 1992) </ref>, which sought to situate and test agents within unpredictable, changing and preferably real world domains. <p> Reactive architectures are characterised by relatively inflexible links from perception to action that immediately activate on the satisfaction of predefined conditions. Normally there is continual vigilance for the satisfaction of those conditions <ref> (Lyons & Hendriks, 1992) </ref>. Different behaviours are mediated by different dedicated coexisting circuits normally operating in parallel. Such dedicated circuits do not store much enduring state as they are required to react very quickly to new environmental contingencies.
Reference: <author> Mackintosh, N. J. </author> <year> (1983). </year> <title> Conditioning and associative learning. </title> <journal> Oxford Psychology Series No. </journal> <volume> 3. </volume> <publisher> Oxford: Oxford University Press. </publisher>
Reference-contexts: Reinforcement theory the theory that those behaviours that lead to reinforcing consequences are maintained is a narrow theory of learning arising from laboratory experimentation with animals, particularly rats (for a full review of its methods, theories and results see <ref> (Mackintosh, 1983) </ref>). It is a behaviouristic theory; that is, it avoids assuming internal mechanisms or states that mediate behaviour. However, simplified models, based on operationally defined variables, have been proposed. A number of its concepts, first formulated by B. F.
Reference: <author> MacLean, A., Young, R. M., Bellotti, V. M. E., & Morgan, T. P. </author> <year> (1991). </year> <title> Questions, options, and criteria: Elements of design space analysis. </title> <journal> Human-Computer Interaction, </journal> <volume> 6 </volume> <pages> 201-250. </pages>
Reference-contexts: In addition, there are theoretical reasons for such limits, such as limited physical 5 Existing work in software engineering attempts to formalise the relationships between requirements and design decisions, for example <ref> (MacLean, Young, Bellotti & Morgan, 1991) </ref>.
Reference: <author> Maes, P. </author> <year> (1989). </year> <title> The dynamics of action selection. </title> <booktitle> In Proceedings of the IJCAI-89 Conference, </booktitle> <address> Detroit. </address>
Reference: <author> Maes, P. </author> <year> (1990). </year> <title> Guest editorial: designing autonomous agents. </title> <editor> In Maes, P. (Ed.), </editor> <booktitle> Designing Autonomous Agents: theory and practice from biology to engineering and back, </booktitle> <pages> pages 1-2. </pages> <address> Amsterdam: </address> <publisher> Elsevier Science Publishers. </publisher>
Reference: <author> Maes, P. </author> <year> (1991). </year> <title> The agent network architecture. </title> <journal> SIGART Bulletin, </journal> <volume> 2(4) </volume> <pages> 115-120. </pages>
Reference: <author> Marimon, R., McGrattan, E., & Sargent, T. J. </author> <year> (1990). </year> <title> Money as a medium of exchange in an economy with artificially intelligent agents. </title> <journal> Journal of Economic Dyanmics and Control, </journal> <volume> 14 </volume> <pages> 329-373. </pages>
Reference: <author> Marjanovic, M., Scassellati, B., & Williamson, M. </author> <year> (1996). </year> <title> Self-taught visually-guided pointing for a humanoid robot. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., & Wilson, S. W. (Eds.), </editor> <booktitle> From Animals to Animats IV, Proceedings of the Fourth International Conference on the Simulation of Adaptive Behavior, </booktitle> <pages> pages 35-44. </pages> <publisher> The MIT Press. 224 Marx, </publisher> <editor> K. </editor> <title> (1887). Capital, a critical analysis of capitalist production, volume 1. </title> <publisher> Lawrence and Wishart. Lawrence and Wishart edition, </publisher> <year> 1970. </year>
Reference: <author> Marx, K. & Engels, F. </author> <year> (1968). </year> <title> Karl Marx and Friedrich Engels selected works. </title> <publisher> Lawrence and Wishart. </publisher>
Reference-contexts: To confuse matters, the neoclassical tradition is essentially concerned with the determination of price, whereas the Marxist tradition views value as a precondition for price; that is, an underlying `law of value' generates the surface appearance of prices within a system of commodity production (see Wages, Price and Profit in <ref> (Marx & Engels, 1968) </ref>). This is pursued further in a short appendix to the chapter. 8.3 The currency flow design solution Money, therefore, is just like any other commodity except for a social convention that ensures it is the means of exchange in all transactions.
Reference: <author> McCallum, A. R. </author> <year> (1996). </year> <title> Learning to use selective attention and short-term memory in sequential tasks. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., & Wilson, S. W. (Eds.), </editor> <booktitle> From Animals to Animats IV, Proceedings of the Fourth International Conference on the Simulation of Adaptive Behavior. </booktitle> <publisher> The MIT Press. </publisher>
Reference-contexts: Solutions to the problem of hidden state involve memory-based methods, for example <ref> (McCallum, 1996) </ref>, which describes a RL algorithm that can reason over an interaction history to determine which previous states affect possible 135 future states (imagine deducing the connection between a prior button press and changed state transitions in state s 1 ).
Reference: <author> McCarthy, J. </author> <year> (1979). </year> <title> Ascribing mental qualities to machines. </title> <editor> In Ringle, M. (Ed.), </editor> <booktitle> Philosophical Perspectives in Artificial Intelligence. </booktitle> <address> Sussex: </address> <publisher> Harvester Press. </publisher>
Reference: <author> McCarthy, J. </author> <year> (1995). </year> <title> Making robots conscious of their mental states. In AAAI Spring Symposium on Representing Mental States and Mechanisms. </title> <note> Accessible via http://www-formal.stanford.edu/jmc/. </note>
Reference: <author> Meuller, R. E. </author> <year> (1990). </year> <title> Daydreaming in humans and machines. </title> <address> New Jersey: </address> <publisher> Ablex Publishing Corporation. </publisher>
Reference: <author> Miceli, M. & Castelfranchi, C. </author> <year> (1989). </year> <title> A cognitive approach to values. </title> <journal> Journal for the Theory of Social Behaviour, </journal> <volume> 19 </volume> <pages> 169-193. </pages>
Reference-contexts: J. Dewey, 1939, Theory of Valuation. Chicago: The University of Chicago Press. Quoted in <ref> (Miceli & Castelfranchi, 1989) </ref>. The woodcutter prizes the chainsaw because it is a means to felling trees, and, by comparing it to the axe, he appraises or rates it more highly than the axe.
Reference: <author> Miller, G. A., Galanter, E., & Pribram, K. H. </author> <year> (1970). </year> <title> Plans and the structure of behaviour. </title> <publisher> Holt International Edition. </publisher>
Reference: <author> Minsky, M. L. </author> <year> (1987). </year> <title> The Society of Mind. </title> <publisher> London: William Heinemann Ltd. </publisher>
Reference-contexts: M. Minsky, The Society of Mind, `magnitude and marketplace', page 284. Marvin Minksy's The Society of Mind <ref> (Minsky, 1987) </ref> is the best example of the social metaphor applied to the understanding and design of minds. It outlines a computational society of heterogenous agents that compete and cooperate to produce mental capabilities. <p> Marvin Minsky's Snarc machine was an early reinforcement learner that encountered the credit-assignment problem (see section 7.6 of <ref> (Minsky, 1987) </ref>). RL algorithms use a scalar quantity representation of value, the reinforcement signal, to select those behaviour-producing components that satisfy conditions of reward over and above those components that do not.
Reference: <author> Miranda, L. P. </author> <year> (1996). </year> <title> Deciding, planning and practical reasoning: elements towards a cognitive architecture. </title> <type> Technical Report CSRP-96-6, </type> <institution> School of Computer Science and Cognitive Science Research Centre. </institution>
Reference: <author> Moffat, D. & Frijda, N. H. </author> <year> (1995). </year> <title> Where there's a will there's an agent. </title> <editor> In Wooldridge, M. & Jennings, N. (Eds.), </editor> <booktitle> Intelligent Agents. </booktitle> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In the abstract, the requirements for MINDER1 are the same as those for human-like autonomy; hence, perturbant states have greater claim to model actual aspects of human information processing. Other models have difficulties making such claims. An exception is Moffat and Frijda's WILL architecture <ref> (Moffat & Frijda, 1995) </ref>, partly similar to MINDER1, and a design for a concern realisation system (see section 6.3.1). However, WILL does not exhibit protoemotional states. Also, Botelho & Coelho (1996) describe an agent architecture that can interrupt its attention in response to new environmental contingencies. <p> This point will be discussed in more detail later. To date there are two partial computational realisations of the CR theory: the ACRES system (Frijda & Swagerman, 1987) and the agent architecture Will <ref> (Moffat & Frijda, 1995) </ref>. These systems were discussed in section 5.6. 99 6.3.2 Cognitive broadcasts: Oatley and Johnson-Laird (Oatley & Johnson-Laird, 1985; Oatley, 1992; Johnson-Laird, 1988) presents a communicative theory of emotions (COM).
Reference: <author> Mook, D. G. </author> <year> (1987). </year> <title> Motivation, the organization of action. </title> <publisher> Edinburgh: Edinburgh University Press. </publisher>
Reference-contexts: problem-solving capabilities of humans, and the kinds of deductive reasoning exhibited by artificial intelligence `classical' planning systems, such as STRIPS, or the problem solving 1 This sentence paraphrases Karl Popper's assertion that trial and error learning within a world model `permits our hypotheses to die in our stead' (quoted in <ref> (Mook, 1987) </ref>). 54 capabilities of the SOAR architecture. <p> Operants, however, are responses not elicited by stimuli but by the animal itself, voluntary rather than reflexive. Skinner's law of conditioning states `if the occurrence of an operant is followed by a presentation of a [positively] reinforcing stimulus, the strength of the operant is increased' <ref> (Mook, 1987) </ref>. <p> As an example, it is difficult to sustain a distinction between voluntary and reflexive behaviour by observation alone. Experiments with rats in the early fifties demonstrated the existence of reinforcement `centres' in the brain (including such areas as the lateral hypothalamus, brainstem and forebrain) <ref> (Mook, 1987) </ref>. An electrode can be directly inserted into a rat's brain. When the rat presses a lever a small current is passed through the electrode. If the electrode is in the correct place the rat will press the lever again and again, for hours at a time. <p> This brief description has ignored important complications such as satiation, habituation, and the relationship between reinforcement learning and other types of learning. 7.3.1 Reinforcement as a selective system Staddon and Simmelhag (1971; discussed in <ref> (Mook, 1987) </ref>) postulate principles of variation, which determine what subset of action responses from an animal's repertoire may occur in certain contexts. The initial repertoire of actions can be genetically determined and/or acquired through previous learning experiences.
Reference: <author> Morris, H. C. </author> <year> (1991). </year> <title> On the feasibility of computational artificial life a reply to critics. </title> <editor> In Meyer, J. A. & Wilson, S. W. (Eds.), </editor> <booktitle> From Animals to Animats, 225 Proceedings of the First International Conference on the Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: The relevance of MINDER1 to theories of emotion constitutes its primary scientific content and is discussed in section 5.6. MINDER1, like many computer simulations of mental phenomena, is both an artificial agent in its own right and a model of motive processing in natural agents <ref> (Morris, 1991) </ref>. 5.4.2 Some limitations MINDER1 could be improved in many ways, both from the standpoint of developing agents that handle multiple motives in more intelligent ways, and from the standpoint of developing a richer cognitive model of motive processing. All the mechanisms described could be `deepened'.
Reference: <author> Muggleton, S. </author> <year> (1994). </year> <title> Logic and learning: Turing's legacy. </title> <editor> In Furukawa, K., Michie, D., & Muggleton, S. (Eds.), </editor> <booktitle> Machine Intelligence 13. </booktitle> <publisher> Oxford: Oxford University Press. </publisher>
Reference-contexts: of the human child depends largely on a system of rewards and punishments, and this suggests that it ought to be possible to carry through the organising with only two interfering inputs, one for "pleasure" or "reward" (R) and the other for "pain" or "punishment" (P).' Alan Turing, quoted in <ref> (Muggleton, 1994) </ref>. `Rewards and punishments are the lowest form of education.' Chuang-Tzu This chapter examines adaptive agent architectures and how they support concepts of `value'.
Reference: <author> Nagel, E. & Newman, J. R. </author> <year> (1958). </year> <title> Godel's proof. </title> <publisher> London: Routledge and Kegan Paul. </publisher>
Reference-contexts: For example, the strength of a classifier rule is a number that is compared with other numbers. The strength quantity is never decomposed or decoded to a more complex structure, unlike, for example, binary representations of networks, Godel numbering of statements in elementary number theory <ref> (Nagel & Newman, 1958) </ref>, or numerical representations of n-dimensional vectors. Strength is only ever added to, subtracted from, or compared with. Scalar representations of value do not represent anything within or external to the RL system; rather, value specifies an internal is better than relation between substates.
Reference: <author> Nagel, T. </author> <year> (1974). </year> <title> What is it like to be a bat? The Philosophical Review, </title> <publisher> LXXXIII(4):435-50. </publisher>
Reference-contexts: The study of `consciousness' is currently very fashionable, and there are many competing theories. An unfortunate number of those theories reject the possibility that `consciousness' could ever be fully explained in information processing terms, for example (Chalmers, 1996) and <ref> (Nagel, 1974) </ref>. The rejection of the possibility of a mechanical explanation of `consciousness' has a long philosophical history. Those rejections often rest on what Ryle (1949) has called a `category mistake'. The following quotation, taken from Leib-niz's `Monadology', provides a good example of the error (for `perception' read 26 `consciousness').
Reference: <author> Newell, A. </author> <year> (1990). </year> <title> Unified Theories of Cognition. </title> <address> Cambridge, MA: </address> <publisher> Harvard University Press. </publisher>
Reference-contexts: Information level explanations operate at a level of abstraction `higher' than the physical level but `lower' than Newell's `knowledge level' <ref> (Newell, 1990) </ref> and Dennett's `intentional stance' (Dennett, 1991). Knowledge level and intentional stance explanations pre 21 suppose the rationality of agents (i.e., the agent's actions are reliably determined by a rational relation between its knowledge and goals), whereas the information level does not. <p> In addition, social requirements need to be considered to explain emotional expression and the function of emotions in human society (Aube & Senteni, 1996a; Aube & Senteni, 1996b). 212 * The VAFP theory (Sloman and Beaudoin's agent design plus valenced per--turbances) is a candidate `unified theory of cognition' <ref> (Newell, 1990) </ref> albeit in disguise. In terms of implementation and empirical validation it is not as developed as SOAR or ACT-R, but it has similar theoretical scope. However, the VAFP theory addresses different requirements to, say, SOAR, which is reflected in their very different architectures.
Reference: <author> Nietzsche, F. (1896). Thus Spoke Zarathustra: </author> <title> a book for everyone and no one. Har-mondsworth: Penguin. Penguin Edition 1969. Translated with an introduction by R.J. </title> <publisher> Hollingdale. </publisher>
Reference: <author> Nilsson, N. J. </author> <year> (1984). </year> <title> Shakey the robot. </title> <type> Technical Report Technical note 323, </type> <institution> SRI International, Menlo Park, California. </institution>
Reference-contexts: Such systems can be usefully conceived as agents when they attempt to control parts of a real or simulated world. For example, Nilsson's SHAKEY the robot <ref> (Nilsson, 1984) </ref> used a STRIPS-style planner to plan its behaviour. Deliberative architectures tend to work on a single task at a time and do not interleave deliberation with action. Normally they manipulate declarative representations of possible actions, including representations of states of affairs, such as action preconditions and consequences.
Reference: <author> Nilsson, N. J. </author> <year> (1994). </year> <title> Teleo-reactive programs for agent control. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 139-158. </pages>
Reference-contexts: It operates in a simulated domain called Botworld, which consists of robots, obstacles, and moveable bars. Benson and Nilsson identify four main requirements for their agent. First, the agent needs to react rapidly to commonly occurring situations that require stereotypical responses. Nilsson's teleo-reactive program (TRP) formalism <ref> (Nilsson, 1994) </ref> addresses this requirement by implementing the agent's plan steps as small reactive `packages' that direct the agent toward a subgoal in a manner that continuously takes into account changing environmental circumstances. The formalism is also similar to Firby's RAPs and is described in more detail in section 5.3.2. <p> Plans that can alter themselves to achieve their goals via continuous feedback from the current situation avoid this difficulty. An approach that partially meets this requirement is Nilsson's teleo-reactive (TR) program formalism <ref> (Nilsson, 1994) </ref>. A TR program is an ordered set of production rules that directs the agent toward a goal in a manner that takes into account changing environmental circumstances (Nilsson, 1994): K 1 ! a 1 K 2 ! a 2 K n ! a n K i are conditions on <p> An approach that partially meets this requirement is Nilsson's teleo-reactive (TR) program formalism <ref> (Nilsson, 1994) </ref>. A TR program is an ordered set of production rules that directs the agent toward a goal in a manner that takes into account changing environmental circumstances (Nilsson, 1994): K 1 ! a 1 K 2 ! a 2 K n ! a n K i are conditions on agent knowledge (including information items such as beliefs, new sense datum, representations of current motives etc.) and a i are actions on the world or on beliefs. <p> Each TR program is implemented as a set of SIM AGENT production rules satisfying the regression property. As TR programs are fully evaluated each time step it is helpful to think of their semantics in terms of dedicated circuits that continuously evaluate feedback from actions <ref> (Nilsson, 1994) </ref> (and section 4.2.3.4). 5.3.2.4 Generactivation of motivators For MINDER1 to use its TR programs it requires goals to achieve. The source of motives in MINDER1 is a suite of generactivators that express the agent's concerns (Frijda, 1986).
Reference: <author> Norman, T. J. </author> <year> (1994). </year> <title> Position paper: motivated goal and action selection. Presented at AISB workshop, Models or Behaviours, which way forward for robotics? Leeds, </title> <month> April </month> <year> 1994, </year> <institution> and University College London research note RN/94/18. </institution>
Reference: <author> Norman, T. J. </author> <year> (1996). </year> <title> Motivation-based direction of planning attention in agents with goal-autonomy. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University College London. </institution>
Reference: <author> Norman, T. J. & Long, D. P. </author> <year> (1995). </year> <title> Goal creation in motivated agents. </title> <editor> In Wooldridge, M. J. & Jennings, N. R. (Eds.), </editor> <booktitle> Intelligent Agents: Proceedings of the ECAI-94 Workshop on Agent Theories, Architectures, and Languages, </booktitle> <pages> pages 277-290. </pages> <editor> Springer-Verlag. </editor> <booktitle> Volume 890 of Lecture Notes in Artificial Intelligence. </booktitle>
Reference: <author> Oatley, K. </author> <year> (1992). </year> <title> Best Laid Schemes. Studies in Emotion and Social Interaction. </title> <address> Cambridge: Cambridge University Press. </address> <note> 226 Oatley, </note> <author> K. & Johnson-Laird, P. N. </author> <year> (1985). </year> <title> Sketch for a cognitive theory of emotions. </title> <type> Technical Report CSRP 045, </type> <institution> School of Cognitive Science, University of Sussex. </institution>
Reference-contexts: When a substantial change of probability occurs of achieving an important goal or subgoal, the monitoring mechanism broadcasts to the whole cognitive system a signal that can set it into readiness to respond to this change. Humans experience these signals and the states of readiness they induce as emotions <ref> (Oatley, 1992) </ref> They assume that mammalian cognitive architecture is modular, exhibiting a high degree of asynchrony, and is hierarchically organised with a top level processor that functions like the scheduling component of an `operating system', capable of invoking lower level processes in serial sequences. <p> Control signals are thought to be generated by phylogenetically older machinery and, due to their simplicity, need not be parsed or interpreted. Emotion control signals propagate globally among 1 Oatley & Johnson-Laird originally drew a distinction between propositional and control signalling; however, in Best Laid Schemes <ref> (Oatley, 1992) </ref> Oatley states that Sloman's suggestion of `semantic' as opposed to `propositional' (Sloman, 1992) better captures their intentions. 2 The following analogy may help capture the distinction: Imagine trains travelling on a complex network of tracks. Postal trains contain mail (semantic content) with destination addresses on the envelopes. <p> Oatley, however, suggests that the AFP theory has difficulty accounting for moods and longer term emotional states <ref> (Oatley, 1992) </ref>. The COM theory accounts for such phenomena by proposing a dissociation between semantic and control signals. For example, dissociative effects can occur when an emotion signal is transmitted but some other goal prohibits the expression of the semantic content to ourselves or others. <p> This criticism is more developed than but in accordance with Oatley's view <ref> (Oatley, 1992) </ref> that the AFP theory has difficulty accounting for longer term emotional states, states that do not simply interrupt but disrupt and coexist with other processing states over time. <p> The mental pain is the result of attachment structure substates losing value and hence their causal powers to dispositionally determine the contents of attention and behaviour. 1 Compare Oatley's treatment of grief: `a whole repertoire of subplans and knowledge becomes useless' <ref> (Oatley, 1992) </ref>. 208 5 The futility of other plans may trigger regression to a basic plan of crying. 6 Various self-control strategies may be instigated to overcome the perturbant states; however, the phenomenology of grief suggests that the causal powers of self-control mechanisms are limited. 7 Detachment takes time due to
Reference: <author> Ortony, A., Clore, G. L., & Collins, A. </author> <year> (1988). </year> <title> The Cognitive Structure of the Emotions. </title> <address> New York: </address> <publisher> Cambridge University Press. </publisher>
Reference: <author> Palmer, S. E. & Kimchi, R. </author> <year> (1984). </year> <title> The information processing approach to cognition. </title> <editor> In Knapp, T. J. & Robertson, L. C. (Eds.), </editor> <title> Approaches to Cognition: Contrasts and Controversies. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: of evolved, naturally occurring architectures, despite differences in low level implementation details. 22 Claim (a) underlies much contemporary Cognitive Science and has been ar-gued for or presupposed by many theorists (e.g., see (Miller, Galanter & Pribram, 1970; Johnson-Laird, 1988; Simon, 1967; Simon, 1981b; Simon, 1981a; Simon, 1995; Newell, 1990) and <ref> (Palmer & Kimchi, 1984) </ref> for different sub-theses). Claim (b) is more contentious and depends on finding appropriate levels of abstraction. There are other examples of congruity at high levels despite low level differences.
Reference: <author> Panskepp, J. </author> <year> (1980). </year> <title> Les circuits des emotions. </title> <journal> Science et Vie, Horsserie, </journal> <volume> 168 </volume> <pages> 58-67. </pages>
Reference-contexts: This would explain the fact that the loss of a close being remains a painful experience and also that the substances which are active in relieving pain, such as morphine, have a surprisingly powerful effect in reducing psychological distress due to social isolation' <ref> (Panskepp, 1980) </ref>. 9.5 A reappraisal of libido theory: computational psychodynamics Freud was concerned with motivational and dynamic aspects of cognition. In this section, the theory outlined here is compared to aspects of Freudian metapsychology, in particular, his much criticised concept of libidinal energy.
Reference: <institution> Translated by Michel Aube. </institution>
Reference: <author> Parsons, S. D. & Jennings, N. R. </author> <year> (1996). </year> <title> Negotiations through argumentation a preliminary report. </title> <booktitle> In Proceedings of the Second International Conference on Multi-Agent Systems. </booktitle>
Reference-contexts: For example, when a conflict is encountered the agents involved may generate proposals for joint commitments with associated explanations. The mooted proposals may then be evaluated, and various counter-proposals or compromises suggested. The Socratic dialogue continues until agreement is reached <ref> (Parsons & Jennings, 1996) </ref>. In order that local negotiations can meet global requirements there is need for local information, referring to those requirements, that can form a basis for controlling the negotiations.
Reference: <author> Penrose, R. </author> <year> (1989). </year> <title> The Emperor's New Mind: Concerning Computers, Minds and the Laws of Physics. </title> <publisher> Oxford: Oxford University Press. </publisher>
Reference: <author> Pepper, S. C. </author> <year> (1958). </year> <title> The Sources of Value. </title> <publisher> University of California Press. </publisher>
Reference-contexts: This is the corresponding movement in design-space. The design elements of an adaptive thermostat performing trial and error learning correspond to a selective system <ref> (Pepper, 1958) </ref>, which is the abstract schema for an adaptive system (Cziko, 1995). <p> Value appears in design-space when a requirement for adaptivity is imposed. It orders internal components with reference to normative criteria within a selective system. 4 4 The analysis of `value' presented here is influenced by S. C. Pepper's The Sources of Value (1958) <ref> (Pepper, 1958) </ref>. Pepper illustrates how all kinds of selective system support the concept ((Wright, 1995) contains a brief summary). <p> A natural norm is also a fact another sort of value fact, a normative fact in that it performs a normative function ... S. C. Pepper, 1958, The Sources of Value, p667 - 668 <ref> (Pepper, 1958) </ref>. 139 7.5.3 Value as a control signal In sections 6.3.2 and 6.4.3.5 Oatley and Johnson-Laird's distinction between semantic and control signalling was introduced. Semantic signals are intentional and refer, whereas control signals are non-intentional and perform a control function, such as changing the `control flow' of the system.
Reference: <author> Pfeifer, R. </author> <year> (1994). </year> <title> The `fungus eater approach' to emotion: a view from artificial intelligence. </title> <journal> Cognitive Studies: Bulletin of the Japanese Cognitive Science Society, </journal> <volume> 1(2) </volume> <pages> 42-57. </pages> <note> Extended and revised version of an invited talk at AISB-91, Leeds, UK. Also available as a technical report from AI Lab, </note> <institution> Institute for Informatics, University of Zurich-Irchel. </institution>
Reference-contexts: Such descriptions make no mention of the physical implementation of the program. In summary, the approach adopted in this thesis is to attempt to explain emotional phenomena at the information level of description. 2.3.1 The design-based approach The study of emotions is divided into differing `schools of thought' <ref> (Pfeifer, 1994) </ref>. Approaches to the study of emotions can be very broadly categorised as semantics-based, phenomena-based and design-based (Sloman, 1992; Sloman, 1993d). Semantics-based theories analyse the use of language to uncover implicit assumptions underlying emotion words, for example (Wierzbicka, 1992). <p> However, ACRES is not an implementation of an architecture that can support a distinction between attentive and pre-attentive processing, and does not exhibit emergent processing states. Pfeifer's FEELER system, reviewed in <ref> (Pfeifer, 1994) </ref>, also predicts appropriate emotional states given story scenarios.
Reference: <author> Popper, K. R. </author> <year> (1963). </year> <title> Conjectures and Refutations: the growth of scientific knowledge. </title> <publisher> London: Routledge & Kegan Paul. </publisher>
Reference-contexts: P. Johnson-Laird, The Computer and the Mind, (Johnson-Laird, 1988), p. 219. However, an inductive step can be validated, not by logical deduction, but by empirical checking. Validation may show that the inductive step is of notional utility, but validation is partial and always open to refutation (e.g., see <ref> (Popper, 1963) </ref>). Trial and error learning, therefore, can provide new knowledge about an environment, allowing run-time adaptation to actual conditions rather than design-time preadaption to predicted conditions.
Reference: <author> Powers, W. T. </author> <year> (1988). </year> <title> Living Control Systems, selected papers of William T. Powers. Kentucky: The Control Systems Group. </title>
Reference: <author> Pryor, L. </author> <year> (1994). </year> <title> Opportunities and planning in an unpredictable world. </title> <type> PhD thesis, </type> <institution> Northwestern University. </institution>
Reference-contexts: It is inconceivable how any behaviour-based architecture could cope with novel global task constraints, such as deadlines. Behaviour-based architectures make use of world regularities at design-time (by pre-compilation of action selection in the connectivity of wires) but make no provision for discovering such regularities at execution-time <ref> (Pryor, 1994) </ref>. (Nor-man, 1994; Norman & Long, 1995) illustrates how a behaviour-based system will become inefficient when a multiple-goals requirement is introduced and argues that a symbol manipulating mechanism is necessary to overcome this drawback. <p> In other words, there is no disciplined way of controlling attention in this architecture. For example, unlike Sloman and Beaudoin's agent architecture (see section 4.4), no explicit distinction is made between the urgency and importance of goals. 4.2.3.3 PARETO: Pryor PARETO <ref> (Pryor, 1994) </ref> is a plan execution system that operates in an uncertain and dynamic environment. The design of PARETO is motivated by the view that information gathering and opportunity taking are essential aspects to planning in an unpredictable domain.
Reference: <author> Pryor, L. & Collins, G. </author> <year> (1992). </year> <title> Reference features as guides to reasoning about opportunities. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society. </booktitle> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Such decisions require opportunity taking, which involves detecting when an unforeseen situation is helpful for achieving a goal (an opportunity) or harmful to a goal (a threat). PARETO recognises opportunities (and threats) by using a filtering process based on reference features <ref> (Pryor & Collins, 1992) </ref>. Reference features label functional stability, that is, they mark the functionally important aspects, relative to the agent's goals, of the elements that comprise a situation. For example, an agent may have an interview for a job and notice that a thread is showing on their jacket.
Reference: <author> Pryor, L. & Collins, G. </author> <year> (1993). </year> <title> Cassandra: planning for contingencies. </title> <type> Technical Report 41, </type> <institution> The Institute for Learning Sciences, Northwestern University. </institution> <note> 227 Rao, </note> <author> A. S. & Georgeff, M. P. </author> <year> (1991a). </year> <title> An abstract architecture for rational agents. </title> <booktitle> In Proceedings of the Third International Conference on Knowledge Representation and Reasoning, </booktitle> <address> Boston. </address>
Reference-contexts: Deferred decisions arise due to the unavailability of required information during plan formation. Deferred decisions, therefore, will require information gathering at execution time. The Cassandra system <ref> (Pryor & Collins, 1993) </ref> was designed to solve this problem in a traditional planning system by including explicit decision and information gathering steps within plan representations. PARETO, however, was designed to address the problem of unforeseen decisions, which are due to the unpredictability of the environment.
Reference: <author> Rao, A. S. & Georgeff, M. P. </author> <year> (1991b). </year> <title> Modeling rational agents within a BDI-architecture. </title> <editor> In Allen, J., Fikes, R., & Sandewall, E. (Eds.), </editor> <booktitle> Proceedings of the 2nd International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 473-484, </pages> <address> Cambridge, MA, USA. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Read, T. & Sloman, A. </author> <year> (1993). </year> <title> The terminological pitfalls of studying emotion. In Workshop on Architectures Underlying Motivation and Emotion WAUME93, </title> <institution> Birmingham, UK. </institution>
Reference: <author> Read, T. R. </author> <year> (1995). </year> <title> The use of systemic design to analyse Gray's theory of emotion. </title> <type> PhD thesis, </type> <institution> School of Computer Science, The University of Birmingham. </institution>
Reference-contexts: Beaudoin has highlighted the absence of learning in his agent design (Beaudoin, 1994), although the C&AP has understood the need for learning (for example, theses written by Tim Read <ref> (Read, 1995) </ref> and Edmund Shing (Shing, 1996) examined the role of learning in agent architectures). The lack is particularly acute when longer-term emotional episodes are analysed: without a theory of useful change it is difficult to account for both the persistence and decay of emotional episodes over time. <p> The full articulation between substates that possess CUE within the libidinal system and other cognitive processes, such as the management layer, has yet to be described. Understanding such causal relations would constitute a theory of `interaction between so called cognitive belief systems and the phylogenetically older reward systems' <ref> (Read, 1995) </ref>. Also, there is no theory of the full variety of substates that could mediate be-haviour within the CLE, nor is there a theory of the types of semantic signalling that can occur (i.e., the forms of representation that substates use and exchange).
Reference: <author> Reilly, W. S. </author> <year> (1993). </year> <title> Emotions as part of a broad agent architecture. </title> <booktitle> In Proceedings of the Workshop on Architectures Underlying Motivation and Emotion, </booktitle> <address> WAUME93, Birmingham, UK. </address>
Reference: <author> Rey, G. </author> <year> (1997). </year> <title> Contemporary philosophy of mind: a contentiously classical approach. </title> <address> Cambridge, Mass; Oxford: </address> <publisher> Blackwell. </publisher>
Reference: <author> Riolo, R. L. </author> <year> (1988). </year> <title> A package of domain independent subroutines for implementing classifier systems in arbitrary, user-defined environments. Logic of Computers Group, </title> <institution> Division of Computer Science and Engineering, University of Michigan. </institution>
Reference: <author> Rubin, I. I. </author> <year> (1988). </year> <title> Essays on Marx's Theory of Value. Montreal: Black Rose Books. </title> <note> Originally published 1928. </note>
Reference-contexts: Value is the transmission belt which transfers the working processes from one part of society to another, making that society a functioning whole' <ref> (Rubin, 1988) </ref>. Currency flow reinforces social cooperation: for example, a particular agent will not be able to acquire a commodity without first expending labour that has sufficient value to other agents.
Reference: <author> Ryle, G. </author> <year> (1949). </year> <title> The Concept of Mind. </title> <publisher> Hutchinson. </publisher>
Reference-contexts: G. W. Leibniz (1646 - 1716) (Liebniz, 1991). A person may tour the grounds of a university, and see buildings, faculties, schools, offices, lecturers and students, and yet still ask `Where is the university?' <ref> (Ryle, 1949) </ref>. The visitor's mistake is to think that the term `university' refers to one particular thing, rather than referring to a collection of things with mutual functional relationships. <p> It is heuristic as it needs to be computed inexpensively without diverting the very resources the filter mechanism is designed to protect. The insistence of a motivator, therefore, is a dispositional state (see <ref> (Ryle, 1949) </ref> that analyses mental terms as disposition words). It has a tendency, or potential, to disturb and divert attention but need not actually surface through the filter or disturb ongoing processing. <p> Ryle has argued <ref> (Ryle, 1949) </ref> that it does not make sense to assert that a man taking pleasure in fishing is performing two acts, one of fishing in a certain manner and another of taking pleasure. `Taking pleasure' need not refer to an occurrent, private mental episode, but rather serve as shorthand for a <p> Examples of control states referred to in folk psychology are beliefs, images, suppositions, desires, preferences, intentions, moods, learnt associations, innate or trained reflexes, personality traits, and emotional states. The dispositions may be of a high order (i.e. dispositions to invoke dispositions to invoke dispositions. . . <ref> (Ryle, 1949) </ref>.) Example substates discussed so far are untaught and learnt conditions of satisfaction and means of satisfaction. Every control state within the agent architecture has structure, aetiology, powers, transformation capabilities, liabilities, and in some cases semantics (Wright, Sloman & Beaudoin, 1996).
Reference: <author> Sartre, J. P. </author> <year> (1934). </year> <title> Esquisse d'une theorie phenomenlogique des emotions (The Emotions). Philosophical Library. </title> <note> Paris: Hermann. Translated 1948. </note>
Reference-contexts: Crying is the plan of last resort, and can be triggered by negatively valenced perturbant states. This is similar to Sartre's view of `emotions' as an attempt to `magically' transform the world <ref> (Sartre, 1934) </ref>. From an infant's standpoint, crying achieves things by magic.
Reference: <author> Schaerf, A., Shoham, Y., & Tennenholtz, M. </author> <year> (1995). </year> <title> Adaptive load balancing: a study in multi-agent learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 475-500. </pages>
Reference: <author> Scherer, K. R. </author> <year> (1993). </year> <title> Studying the emotion-antecedent appraisal process: an expert system approach. </title> <journal> Cognition and Emotion, </journal> <volume> 7 </volume> <pages> 325-355. </pages> <note> 228 Searle, </note> <author> J. R. </author> <year> (1980). </year> <title> Minds brains and programs. </title> <booktitle> The Behavioral and Brain Sci--ences, </booktitle> <pages> 3(3). </pages>
Reference: <author> Shing, E. </author> <year> (1996). </year> <title> Computational mechanisms for constraining learning. </title> <type> PhD thesis, </type> <institution> School of Computer Science, The University of Birmingham. </institution>
Reference-contexts: Beaudoin has highlighted the absence of learning in his agent design (Beaudoin, 1994), although the C&AP has understood the need for learning (for example, theses written by Tim Read (Read, 1995) and Edmund Shing <ref> (Shing, 1996) </ref> examined the role of learning in agent architectures). The lack is particularly acute when longer-term emotional episodes are analysed: without a theory of useful change it is difficult to account for both the persistence and decay of emotional episodes over time.
Reference: <author> Shoham, Y. & Tennenholtz, M. </author> <year> (1994). </year> <title> Co-learning and the evolution of social activity. </title> <type> Technical Report CS-TR-94-1511, </type> <institution> Robotics Laboratory, Department of Computer Science, Stanford University. </institution>
Reference: <author> Simon, H. A. </author> <year> (1967). </year> <title> Motivational and emotional controls of cognition. Reprinted in Models of Thought, </title> <publisher> Yale University Press, </publisher> <pages> 29-38, </pages> <year> 1979. </year>
Reference-contexts: Similarly, it is often taken for granted that general principles of feedback control apply both to natural and artificial systems. What needs to be added to this, following much work in Artificial Intelligence, and the ideas in <ref> (Simon, 1967) </ref>, is a level of explanation that involves richer and more profound forms of control of both external and internal behaviour using richer semantic structures and new sorts of control architectures to support various kinds of motivational processes (e.g., see (Simon, 1967; Sloman & Croucher, 1981; Sloman, 1987; Beaudoin & <p> The explanation is built on two central mechanisms: 1. A goal-terminating mechanism [goal executor] ... 2. An interruption mechanism, that is, emotion, allows the processor to respond to urgent needs in real time <ref> (Simon, 1967) </ref>. The theory implies that `organisms' have two kinds of processing that operate in parallel: a goal executor that generates actions, and vigilational (Beaudoin, 1994) processes that continuously check for contingencies that require urgent attention. The former, being resource limited, is interruptable by the latter. <p> The requirement for timely scheduling of actions and the constraint of limited resources, both computational and physical, require that current processing be inter-ruptable (see <ref> (Simon, 1967) </ref>). For example, to react to new, motivationally relevant events in the environment the agent will need to interrupt its ongoing processing and switch its `attention' to new contingencies (Sloman, 1987). <p> The question of why `sadness' control signals persist and are hard to control remains. A full answer to these questions is well beyond the scope of this thesis. However, section 9.4.4 will provide some suggestions. 6.4.2 Learning Simon's original paper <ref> (Simon, 1967) </ref> distinguished two types of learning pertinent to attention interruption: modification of the efficacy of certain `stimuli' to interrupt a central processing system, and the acquisition of new response programs to interrupting `stimuli'. <p> This is an example of the modification of the efficacy of certain `stimuli' to interrupt a central processing system, one type of learning that Simon identified as important in his original interrupt theory <ref> (Simon, 1967) </ref>. For example, a generactivator may gain CUE by constructing motives that, by their adoption by management and satisfaction by planning and execution, meet conditions of satisfaction. A gain in CUE is an increase in causal power: the generactivator, for example, increases its ability to commandeer management resources.
Reference: <author> Simon, H. A. </author> <year> (1981a). </year> <booktitle> Cognitive science: The newest science of the artificial. </booktitle> <editor> In Norman, D. A. (Ed.), </editor> <booktitle> Perspectives on Cognitive Science, </booktitle> <pages> pages 13-26. </pages> <address> Hillsdale, New Jersey: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Simon, H. A. </author> <year> (1981b). </year> <booktitle> The Sciences of the Artificial (second ed.). </booktitle> <publisher> The MIT Press. </publisher>
Reference: <author> Simon, H. A. </author> <year> (1982). </year> <title> Comments. </title> <editor> In Clark, M. S. & Fiske, S. T. (Eds.), Affect and cognition. </editor> <address> Hillsdale, New Jersey: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Neither is it to deny that many everyday feeling terms are also used as disposition terms in Ryle's sense. The intensity dimension of `affects' has also been noted by Simon <ref> (Simon, 1982) </ref>: `Affect is susceptible to continuous gradation in degree, like something that can be scaled by real numbers or modeled by an analogue device. <p> Again, ordinary language includes these phenomena in the orbit of affect and emotion, although when the state is not acute and interruptive, the terms `mood' and `feeling' will often be applied instead of `emotion'. Typical examples are hap-pinnes and sadness' <ref> (Simon, 1982) </ref>. The theory presented here concurs with Simon: valency, in itself, is not interrupting, although it may occur contemporaneously with interruption of attention.
Reference: <author> Simon, H. A. </author> <year> (1990). </year> <title> A mechanism for social selection and successful altruism. </title> <journal> Science, </journal> <volume> 250 </volume> <pages> 1665-1668. </pages>
Reference-contexts: world have been 161 proposed, notably how selfish behaviour leads to widespread TIT-FOR-TAT strate-gies (Axelrod & Hamilton, 1981), or how genuinely altruistic behaviour can be inculcated in docile (those that are receptive to social influence) agents with bounded rationality (those unable to fully evaluate how acquired behaviours affect personal fitness) <ref> (Simon, 1990) </ref>. The Baldwin Effect (Baldwin, 1896) predicts that adaptive, acquired traits tend to be genetically assimilated over time. Assimilated TIT-FOR-TAT may form the foundation for certain social emotions, such as gratitude and guilt, which facilitate cooperative behaviour in natural MAS.
Reference: <author> Simon, H. A. </author> <year> (1995). </year> <title> Artificial intelligence: an empirical science. </title> <journal> Artificial Intelligence, </journal> <volume> 77 </volume> <pages> 95-127. </pages>
Reference: <author> Sloman, A. </author> <year> (1969). </year> <title> How to derive "better" from "is". </title> <journal> American Philosophical Quarterly, </journal> <volume> 6(1) </volume> <pages> 43-52. </pages>
Reference-contexts: of `value' derive from relations of `better' and that trial and error learners, in addition to supporting concepts of `belief' and `desire' also support concepts of `value'. 7.5.1 `Better' and `value' Stating that P is better than Q involves evaluating their qualities with respect to a basis of comparison, B <ref> (Sloman, 1969) </ref>. For example, a chainsaw is better than an axe for cutting down trees. In other words, `better' is logically related to a goal or norm. Comparison of things with respect to a norm can be qualified.
Reference: <author> Sloman, A. </author> <year> (1978). </year> <title> The Computer Revolution in Philosophy: </title> <booktitle> Philosophy, Science and Models of Mind. </booktitle> <address> Hassocks, Sussex: </address> <publisher> Harvester Press (and Humanities Press). </publisher>
Reference-contexts: Objections to this approach are often based on a naive philosophy of science, or misplaced `physics envy'. (For a broader view see (Lakatos, 1970), chapter 2 of <ref> (Sloman, 1978) </ref>, and (Bhaskar, 1978)). The design-based approach draws its inspiration from software engineering and conceptual analysis in philosophy (see chapter 4 of (Sloman, 1978)). <p> Objections to this approach are often based on a naive philosophy of science, or misplaced `physics envy'. (For a broader view see (Lakatos, 1970), chapter 2 of <ref> (Sloman, 1978) </ref>, and (Bhaskar, 1978)). The design-based approach draws its inspiration from software engineering and conceptual analysis in philosophy (see chapter 4 of (Sloman, 1978)). It construes AI as a methodology for exploring an abstract space of possible requirements for functioning agents (niche space) and the space of possible designs for such agents (design space) and the mappings between them (Sloman, 1994a; Sloman, 1995a). <p> A mental architecture might allow a number of different sets of mutually consistent high level dispositions that co-exist, though only one set is active at a time (as sketched in chapter 10 of <ref> (Sloman, 1978) </ref>). Detection of incongruent states requires some sort of self-monitoring of the global `picture' and an explicit evaluation of it as fitting or not fitting the agent's ideals, long term objectives, or previous decisions regarding (for example) what to think about, or which desires are unacceptable. <p> In other words, motive management systems that have purely predefined and inflexible motive state transitions are unlikely to meet the full requirements (compare <ref> (Sloman, 1978) </ref>). MINDER1 has an extremely simple and shallow deciding process yet exhibits complicated interactions between deciding, scheduling, expanding and motivator states. The relations between these management processes are complex and have been distinguished for the sake of exposition. <p> MINDER1 is an engineering solution to a control problem, and could be used as a command and control system or put to use in computer games, but this was not the main reason for building it; rather, it is intended as a `toy designed to stretch our minds' <ref> (Sloman, 1978) </ref>, in particular to help us think about the full complexity 81 of emotional states. The relevance of MINDER1 to theories of emotion constitutes its primary scientific content and is discussed in section 5.6.
Reference: <author> Sloman, A. </author> <year> (1985). </year> <title> Real time multiple-motive expert systems. </title> <editor> In Merry, M. (Ed.), </editor> <booktitle> Expert Systems 85, </booktitle> <pages> pages 1-13. </pages> <address> Cambridge: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: It will have many diverse tasks to perform. In addition, goals will have associated temporal constraints, such as a goal to catch a train at a certain time. Therefore, an autonomous agent needs to schedule its goal processing and actions. This requires the ability to select between multiple motives <ref> (Sloman, 1985) </ref>, prioritise goals, decide 37 on a level of commitment towards current intentions, and notice opportunities for actions that satisfy more than one motive.
Reference: <author> Sloman, A. </author> <year> (1987). </year> <title> Motives mechanisms and emotions. </title> <journal> Cognition and Emotion, </journal> <volume> 1(3) </volume> <pages> 217-234. </pages> <editor> Reprinted in M.A.Boden (ed), </editor> <booktitle> The Philosophy of Artificial Intelligence, </booktitle> <address> OUP, </address> <year> 1990. </year> <note> 229 Sloman, </note> <author> A. </author> <year> (1990). </year> <title> Notes on consciousness. </title> <journal> AISB Quarterly, </journal> (72):8-14. Also pre-sented at Rockefeller foundation workshop on consciousness, Villa Serbelloni, Bellagio March 1990, organiser D.C.Dennett. 
Reference-contexts: For example, to react to new, motivationally relevant events in the environment the agent will need to interrupt its ongoing processing and switch its `attention' to new contingencies <ref> (Sloman, 1987) </ref>. For example, at one moment the agent may have very little to do and have the luxury of deliberation while at the next moment the agent may need to perform many complex tasks very quickly. The unpredictability of the environment renders complete planning prior to action impossible. <p> It typically expresses a motivational attitude (`make false', `keep true', etc.) towards a possible state of affairs (`short of food', `warm', `in danger', etc.), which may be expressed in propositional or non-propositional from. Motivators have various associated information items, including urgency, importance and an insistence value <ref> (Sloman, 1987) </ref> 2 . A new motivator's insistence level determines its ability to penetrate a (variable threshold) filter in order to be considered by management processes. Importance helps to determine whether it is adopted as something to be achieved, if it is considered.
Reference: <author> Sloman, A. </author> <year> (1992). </year> <title> Prolegomena to a theory of communication and affect. </title> <editor> In Ortony, A., Slack, J., & Stock, O. (Eds.), </editor> <booktitle> Communication from an Artificial Intelligence Perspective: Theoretical and Applied Issues, </booktitle> <pages> pages 229-260. </pages> <address> Hei-delberg, Germany: </address> <publisher> Springer. </publisher>
Reference-contexts: It is sufficient for current purposes that the prototype implementation demonstrates that it is possible that the original design meets its requirements. A related point is that many of the detailed design decisions taken during implementation were arbitrary. In <ref> (Sloman, Shing, Read & Beaudoin, 1992) </ref> six types of design decision were identified: (i) design decisions linked to initial requirements, (ii) decisions linked to empirical data, (iii) decisions linked indirectly to requirements via higher level design decisions, (iv) decisions made in order to test a theory, (v) arbitrary decisions where previous <p> Emotion control signals propagate globally among 1 Oatley & Johnson-Laird originally drew a distinction between propositional and control signalling; however, in Best Laid Schemes (Oatley, 1992) Oatley states that Sloman's suggestion of `semantic' as opposed to `propositional' <ref> (Sloman, 1992) </ref> better captures their intentions. 2 The following analogy may help capture the distinction: Imagine trains travelling on a complex network of tracks. Postal trains contain mail (semantic content) with destination addresses on the envelopes. These trains travel to the destinations and deposit the mail (the information). <p> muscular tension, sweating, etc.) In the VAFP theory these phenomena are deliberately ignored and regarded as only marginally relevant, since, in principle, they could occur without these other accompaniments, for instance, in beings from another planet whose mental functioning and social life were much like ours despite considerable bodily differences <ref> (Sloman, 1992) </ref>. 196 10.2 Attachment structure A raised surface can leave an impression on human skin; similarly, interaction with another person will leave an `impression' on mentality. Before the advent of information processing architectures this metaphor could not be unpacked.
Reference: <author> Sloman, A. </author> <year> (1993a). </year> <title> Definition of metamanagement. Internal Cognition and Affect project document. </title>
Reference: <author> Sloman, A. </author> <year> (1993b). </year> <title> The mind as a control system. </title> <editor> In Hookway, C. & Peterson, D. (Eds.), </editor> <booktitle> Philosophy and the Cognitive Sciences, </booktitle> <pages> pages 69-110. </pages> <publisher> Cambridge University Press. </publisher>
Reference-contexts: Negative feedback ensures that the temperature of the room stabilises around the control knob setting: the thermostat `acts' in the world to achieve its `desire'. Of course, a thermostat does not have sufficient architectural complexity required to support human beliefs and desires, but it is an illustrative `limiting case' <ref> (Sloman, 1993b) </ref> of a system that controls its environment with reference to an internal norm. A new feature and a new requirement can be imposed on the simple thermostat.
Reference: <author> Sloman, A. </author> <year> (1993c). </year> <title> Prospects for ai as the general science of intelligence. </title>
Reference: <editor> In A.Sloman, D.Hogg, G.Humphreys, Partridge, D., & Ramsay, A. (Eds.), </editor> <booktitle> Prospects for Artificial Intelligence, </booktitle> <pages> pages 1-10. </pages> <address> Amsterdam: </address> <publisher> IOS Press. </publisher>
Reference: <author> Sloman, A. </author> <year> (1993d). </year> <title> Why we need to study architectures. In Workshop on Architectures Underlying Motivation and Emotion WAUME93, </title> <institution> Birmingham UK. </institution>
Reference-contexts: I suspect we must wait for deeper theories about the underlying mechanisms before we can hope to define precisely what kinds of phenomena we are talking about, just as people had to wait for modern physics and chemistry before they could have good definitions for terms like `water' and `salt'. <ref> (Sloman, 1993d) </ref> A further problem of providing a definition of emotion is that different emotion researchers (e.g., psychologists, biologists, cognitive scientists etc.) often use different vocabulary for the same phenomena or use the same vocabulary for differing phenomena (Read & Sloman, 1993; Kagan, 1978). <p> When fully specified the architecture will be used as the basis for a host of new definitions of classes of mental states and processes (like basing the descriptions of types of physical stuff on a theory of the architecture of matter <ref> (Sloman, 1993d) </ref>). It is hard to think about the multifarious states and processes that can occur in such a complex paper design. A working implementation can aid analytical thinking, by exposing consequences of the design.
Reference: <author> Sloman, A. </author> <year> (1994a). </year> <title> Explorations in design space. </title> <booktitle> In Proceedings 11th European Conference on AI, </booktitle> <address> Amsterdam. </address>
Reference: <author> Sloman, A. </author> <year> (1994b). </year> <title> Semantics in an intelligent control system. </title> <journal> Philosophical Transactions of the Royal Society: Physical Sciences and Engineering, </journal> <volume> 349(1689) </volume> <pages> 43-58. </pages>
Reference-contexts: Sloman has argued that this can include semantics for the machine (e.g. <ref> (Sloman, 1994b) </ref>). In the case of human brains we do not know what the layers are. Yet causal relations between abstract structures clearly occur when a person's seeing something causes him to get angry, which in turn may cause him to strike out. <p> Every control state within the agent architecture has structure, aetiology, powers, transformation capabilities, liabilities, and in some cases semantics (Wright, Sloman & Beaudoin, 1996). These ideas include analogues of the linguistic notions of syntax, semantics, pragmatics, and inference <ref> (Sloman, 1994b) </ref>. For example, a motive in the MINDER1 implementation may have a complex internal structure (syntax), a content (based on that structure) referring to certain states of affairs (semantics), and a functional role, that is, dispositional powers to determine internal and external actions (pragmatics).
Reference: <author> Sloman, A. </author> <year> (1995a). </year> <title> Exploring design space & niche space. </title> <booktitle> In Proc. 5th Scandina-vian Conf. on AI, </booktitle> <address> Trondheim, Amsterdam. </address> <publisher> IOS Press. </publisher>
Reference-contexts: Different requirements entail different designs that meet those requirements. The relations between niche-space and design-space <ref> (Sloman, 1995a) </ref> are complex, involving many trade-offs and compromises, for example sacrificing speed of processing for depth of reasoning. The relations between requirements and design are important for understanding all types of systems, from software systems, agent architectures (Sloman, Beaudoin & Wright, 1994), robots, and organisms in the natural world.
Reference: <author> Sloman, A. </author> <year> (1995b). </year> <title> Musings on the roles of logical and non-logical representations in intelligence. In Glasgow, </title> <editor> J. & Hari Narayanan, C. (Eds.), </editor> <booktitle> Diagrammatic Reasoning: Computational and Cognitive Perspectives. </booktitle> <publisher> AAAI Press. </publisher>
Reference-contexts: Also, the agent will be physically constrained. It will only be able to move at a certain pace, manipulate a finite number of objects and so forth. Good design solutions will manage an agent's finite resources as efficiently as possible (although efficiency is a difficult notion to define <ref> (Sloman, 1995b) </ref>). The agent will need to pursue multiple goals, with perhaps conflicting objectives. It will have many diverse tasks to perform. In addition, goals will have associated temporal constraints, such as a goal to catch a train at a certain time.
Reference: <author> Sloman, A. </author> <year> (1995c). </year> <note> Poprulebase help file. Available at URL ftp://ftp.cs.bham.ac.uk/pub/dist/poplog/prb/help/poprulebase. </note>
Reference-contexts: The toolkit has two main components: Poprulebase <ref> (Sloman, 1995c) </ref> and the SIM AGENT library (Sloman, 1995e; Sloman, 1995f). Poprulebase is a sophisticated and very general interpreter for condition-action rules, written in Pop-11.
Reference: <author> Sloman, A. </author> <year> (1995d). </year> <note> Rulesystems help file. Available at URL ftp://ftp.cs.bham.ac.uk/pub/dist/poplog/prb/help/rulesystems. 230 Sloman, </note> <author> A. </author> <year> (1995e). </year> <title> Sim agent help file. </title> <note> Available at URL ftp://ftp.cs.bham.ac.uk/pub/dist/poplog/sim/help/sim agent. </note>
Reference-contexts: facilities, including a smooth interface to neural net or other `sub-symbolic' mechanisms, mechanisms for control to be transferred between collections of rules as the context changes (allowing SOAR-like (Laird, Newell & Rosenbloom, 1987) pushing and popping of contexts), and facilities for altering the allocation of processing resources to different rulesets <ref> (Sloman, 1995d) </ref>. The SIM AGENT library provides a set of base classes and scheduling mechanisms for running simulations involving a number of objects and agents whose internal mechanisms are implemented using Poprulebase. The scheduler simulates parallelism between agents and between subcomponents of agents.
Reference: <author> Sloman, A. </author> <year> (1995f). </year> <note> Sim agent web-page. Available at URL http://www.cs.bham.ac.uk/ axs/cog affect/sim agent.html. </note>
Reference: <author> Sloman, A. </author> <year> (1995g). </year> <title> What sort of control system is able to have a personality. </title> <note> Available at URL ftp://ftp.cs.bham.ac.uk/pub/groups/cog affect/ Aaron.Sloman.vienna.ps.Z, (Presented at Workshop on Designing personalities for synthetic actors, Vienna, </note> <month> June </month> <year> 1995). </year>
Reference-contexts: concerned with explanations at what Sloman (1994b) has called the information processing level of abstraction, or `information level', a level concerned with both agent designs (i.e., part of Dennett's `design stance' (Dennett, 1991)) and the semantic content of information that is acquired, created, manipulated, stored and used by those agents <ref> (Sloman, 1995g) </ref>. Information level explanations operate at a level of abstraction `higher' than the physical level but `lower' than Newell's `knowledge level' (Newell, 1990) and Dennett's `intentional stance' (Dennett, 1991).
Reference: <author> Sloman, A. </author> <year> (1996a). </year> <type> Personal communication. </type>
Reference-contexts: Hedonic tone can be either positive or negative corresponding to its `preserve/terminate' causal role. A theory of pleasure and pain is required independently of any theory of emotions as events may be pleasurable or painful without involving emotions <ref> (Sloman, 1996a) </ref>, for example enjoying one's food or sitting uncomfortably. Pleasure and pain in emotional states are a special case of a more general phenomenon. <p> this chapter are as follows: (i) the various agents of the system are relatively autonomous, that is they have their own goals and cannot be directly coerced into making choices that contradict their own goals, a condition that does not hold for many hierarchical multi-agent systems with strong top-down control <ref> (Sloman, 1996a) </ref> or most commercial, procedure-based computer programs; (ii) it is possible for agents to take conflicting and contradictory decisions and act on them, a condition that cannot hold for systems that need to be synchronously controlled such as, for example, motor systems; (iii) there is redundancy such that many agents
Reference: <author> Sloman, A. </author> <year> (1996b). </year> <title> Road crossing example. </title> <type> Personal communication. </type>
Reference: <author> Sloman, A. </author> <year> (1996c). </year> <title> A systems approach to consciousness. </title> <journal> Lecture presented at the Royal Society of Arts on 26th Feb 1996. </journal> <note> The slides used for the lecture available at http://www.cs.bham.ac.uk/ axs/misc/consciousness.lecture.ps and a summary of the talk at http://www.cs.bham.ac.uk/ axs/misc/consciousness.rsa.text. </note>
Reference-contexts: The capabilities are implemented on neural components with mutual functional relationships. They may be present in different combinations and different forms in animals, humans and machines <ref> (Sloman, 1996c) </ref>. `Consciousness' is a collection of things that brains do, not a thing that brains do or do not possess. The reification of consciousness normally leads to panpsy-chism (everything is conscious) or solipsism (only I am conscious). <p> It is not clear that a rat ever has control of thought processes. In that case it cannot lose control. If that is so, the sorts of processes being discussed cannot occur in a rat <ref> (Sloman, 1996c) </ref>. Having or losing control requires a mental architecture that can monitor, evaluate and modify thought processes. The ordinary notion of `self-control' is not a unitary concept admitting of a unique analysis. <p> Examples of reactive control are Brooksian behaviour-based architectures, Maes' spreading activation networks (Maes, 1989; Maes, 1991), the routines of Agre and Chapman, insect flight, and automatic and reflex behaviours in humans, such as low level motor control processes. Figure 4.1 (taken from <ref> (Sloman, 1996c) </ref>) is a rough sketch of a reactive architecture with dedicated links from perception to action. Engineering control system theory is primarily concerned with reactive forms of control, such as open and closed loop systems describable by differential equations. <p> There are no convincing examples of artificial architectures exhibiting meta-deliberative control, though Beaudoin (1994) analyses several of the requirements for such an architecture. Figure 4.3 (taken from <ref> (Sloman, 1996c) </ref>) is a rough sketch of a deliberative architecture with a meta-deliberative controller (labelled `meta-management'). The diagram is fully explained in the next section.
Reference: <author> Sloman, A. </author> <year> (1996d). </year> <title> Towards a general theory of representations. </title> <editor> In Peterson, D. (Ed.), </editor> <title> Forms of Representation. </title> <publisher> Intellect Books. </publisher>
Reference-contexts: Cognition is digital in 3 A control sub-state is a general term for a subcomponent of an architecture bearing information <ref> (Sloman, 1996d) </ref> that has relations of control to other subcomponents or parts of an environment. For example, the curvature of a thermostat's bi-metallic strip is a substate of the thermostat architecture that contains information about the ambient temperature of the room.
Reference: <author> Sloman, A. </author> <year> (1996e). </year> <title> What kinds of machine can have emotions? Paper presented at the British Association Annual Festival. </title> <note> Summary of talk available at http://www.cs.bham.ac.uk/ axs/misc/baas.text. </note>
Reference-contexts: Sloman claims that there are some aspects of emotion that we share with many other animals that depend on reactive subsystems of the brain. Examples include being startled, terrified, thrilled, distressed by extreme pain, disgusted by horrible tasting food, sexually aroused, and so forth <ref> (Sloman, 1996e) </ref>. 4.3.2 Deliberative control Deliberative control, unlike reactive control, can explicitly construct representations of alternative possible actions, and evaluate and choose between them prior to performance, allowing much more flexible behaviour (Sloman, 1996f).
Reference: <author> Sloman, A. </author> <year> (1996f). </year> <title> What sort of architecture is required for a human-like agent? Technical Report CSRP-96-12, </title> <institution> School of Computer Science and Cognitive Science Research Centre. </institution> <note> Invited talk for Cognitive Modelling Workshop at AAAI 96. </note>
Reference-contexts: of extra control knowledge in metalevel KAs. 4.3 Three forms of control By abstracting from the design details of the agent architectures reviewed and including design considerations from the natural world it is possible to identify three major types of control in information processing control systems: reactive, deliberative and meta-deliberative <ref> (Sloman, 1996f) </ref>. This taxonomy provides a high level common framework for the agents reviewed in this chapter. The three different types of control represent different degrees of causal coupling between agent information states and environmental states. Reactive control, when compared to deliberative control, is highly coupled to the environment. <p> being startled, terrified, thrilled, distressed by extreme pain, disgusted by horrible tasting food, sexually aroused, and so forth (Sloman, 1996e). 4.3.2 Deliberative control Deliberative control, unlike reactive control, can explicitly construct representations of alternative possible actions, and evaluate and choose between them prior to performance, allowing much more flexible behaviour <ref> (Sloman, 1996f) </ref>. Such rep 53 resentations may have complex syntactic forms and refer to things not present to the senses. Deliberative control can respond to novel environmental contingencies; that is, construct responses to events that may not activate or match predefined conditions or may not have predefined responses. <p> It is based on many facts about human capabilities, considerations regarding the evolution of intelligence, engineering design considerations inspired by reflection on the limitations of current AI systems <ref> (Sloman, 1996f) </ref>, and a requirements analysis for an intelligent system able to handle multiple motives in a complex and continually changing environment. It exhibits all three forms of control identified in the previous section. Much of the specification is still provisional in that work on implementations may reveal serious problems.
Reference: <author> Sloman, A., Beaudoin, L. P., & Wright, I. P. </author> <year> (1994). </year> <title> Computational modeling of motive-management processes. </title> <editor> In Frijda, N. (Ed.), </editor> <booktitle> Proceedings of the Conference of the International Society for Research in Emotions, </booktitle> <address> Cambridge. </address> <publisher> ISRE Publications. </publisher>
Reference-contexts: The relations between niche-space and design-space (Sloman, 1995a) are complex, involving many trade-offs and compromises, for example sacrificing speed of processing for depth of reasoning. The relations between requirements and design are important for understanding all types of systems, from software systems, agent architectures <ref> (Sloman, Beaudoin & Wright, 1994) </ref>, robots, and organisms in the natural world. MAS are no different in this respect: there will be many types of 146 MAS that satisfy different sets of requirements.
Reference: <author> Sloman, A. & Croucher, M. </author> <year> (1981). </year> <title> Why robots will have emotions. </title> <booktitle> In Proceedings of the Seventh International Joint Conference on Aritificial Intelligence, </booktitle> <address> Vancouver. </address> <note> 231 Sloman, </note> <author> A. & Poli, R. </author> <year> (1995). </year> <title> Sim agent: a toolkit for exploring agent designs. </title> <editor> In Wooldridge, M., Mueller, J., & Tambe, M. (Eds.), </editor> <booktitle> Intelligent Agents Vol II, </booktitle> <pages> pages 392-407. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The possibility of agent architectures that pursue multiple goals in complex, dynamic and uncertain domains, possessing both reactive, deliberative and meta-deliberative forms of control, which do not exhibit perturbant states, qualifies Slo 104 man's position that perturbances are unavoidable consequences of resource limited agency (e.g., see <ref> (Sloman & Croucher, 1981) </ref>). This thesis agrees that interruption is unavoidable but that disruption (i.e., loss of control) may not be. The AFP theory explains how perturbances can occur in an information processing architecture but does not explain why they occur.
Reference: <author> Sloman, A., Shing, E., Read, T., & Beaudoin, L. </author> <year> (1992). </year> <title> Six types of design decision. Notes from a Cognition and Affect project meeting. </title> <institution> Department of Computer Science, University of Birmingham. </institution>
Reference-contexts: It is sufficient for current purposes that the prototype implementation demonstrates that it is possible that the original design meets its requirements. A related point is that many of the detailed design decisions taken during implementation were arbitrary. In <ref> (Sloman, Shing, Read & Beaudoin, 1992) </ref> six types of design decision were identified: (i) design decisions linked to initial requirements, (ii) decisions linked to empirical data, (iii) decisions linked indirectly to requirements via higher level design decisions, (iv) decisions made in order to test a theory, (v) arbitrary decisions where previous <p> Emotion control signals propagate globally among 1 Oatley & Johnson-Laird originally drew a distinction between propositional and control signalling; however, in Best Laid Schemes (Oatley, 1992) Oatley states that Sloman's suggestion of `semantic' as opposed to `propositional' <ref> (Sloman, 1992) </ref> better captures their intentions. 2 The following analogy may help capture the distinction: Imagine trains travelling on a complex network of tracks. Postal trains contain mail (semantic content) with destination addresses on the envelopes. These trains travel to the destinations and deposit the mail (the information). <p> muscular tension, sweating, etc.) In the VAFP theory these phenomena are deliberately ignored and regarded as only marginally relevant, since, in principle, they could occur without these other accompaniments, for instance, in beings from another planet whose mental functioning and social life were much like ours despite considerable bodily differences <ref> (Sloman, 1992) </ref>. 196 10.2 Attachment structure A raised surface can leave an impression on human skin; similarly, interaction with another person will leave an `impression' on mentality. Before the advent of information processing architectures this metaphor could not be unpacked.
Reference: <author> Smith, P. K. & Cowie, H. </author> <year> (1991). </year> <title> Understanding children's development. </title> <publisher> Oxford: Blackwell Publishers. </publisher>
Reference-contexts: Although criticised in recent times (particularly the emphasis on maternal deprivation in childhood to explain subsequent problems in adulthood; see chapter three in <ref> (Smith & Cowie, 1991) </ref>) attachment theory is still used to account for both childhood and adult mourning in clinical psychology. Bowlby was highly sympathetic to a control system approach to psychology, although he did not attempt to design and build them (Bowlby, 1991a).
Reference: <author> Smith, R. G. </author> <year> (1980). </year> <title> The contract net protocol: high-level communication and control in a distributed problem solver. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 29(12) </volume> <pages> 1104-1113. </pages>
Reference: <author> Smith, R. G. & Davis, R. </author> <year> (1981). </year> <title> Frameworks for cooperation in distributed problem solving. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 11(1) </volume> <pages> 61-70. </pages>
Reference: <author> Sonnemans, J. & Frijda, N. H. </author> <year> (1994). </year> <title> The structure of subjective emotional intensity. </title> <journal> Cognition and Emotion, </journal> <volume> 8(4) </volume> <pages> 329-350. </pages>
Reference: <author> Steels, L. </author> <year> (1994). </year> <journal> The artificial life roots of artificial intelligence. Artificial Life Journal, </journal> <volume> 1(1). </volume>
Reference-contexts: Hence, new classifiers can be tried without losing the expertise accumulated in their parents. Normally, the GA is applied less frequently than the bb 131 otherwise new classifiers will not have had sufficient time to be evaluated. Classifier systems have been extensively used in the Artificial Life (ALife) community <ref> (Steels, 1994) </ref>, for example in developing control programs for robots through supervised learning (Dorigo & Colombetti, 1993) and learning paths through mazes (Donnart & Meyer, 1994). However, a classifier system is limited in many ways. It does not have an explicit memory store.
Reference: <author> Strongman, K. T. </author> <year> (1987). </year> <title> The Psychology of Emotion. </title> <publisher> John Wiley and Sons Ltd. </publisher>
Reference-contexts: An early example is William James' peripheric theory (see (Calhoun & Solomon, 1984)); for a comprehensive review of many phenomena-based theories, see <ref> (Strongman, 1987) </ref>. In contrast, Sloman's design-based approach, a rational reconstruction of the practice of AI, takes the stance of an engineer attempting to build a system that exhibits the phenomena to be explained.
Reference: <author> Sutton, R. S. </author> <year> (1991). </year> <title> Dyna, an integrated architecure for learning, planning, and reacting. </title> <booktitle> In Working Notes of the 1991 AAAI Spring Symposium, </booktitle> <pages> pages 151-155. </pages> <note> Also in SIGART Bulletin, </note> <month> 2, </month> <pages> pp. 160-163, </pages> <year> 1991. </year>
Reference-contexts: Therefore, in XCS the is better than relation is a composite of accuracy and predicted pay-off. XCS selects those actions that have a higher certainty of maximising payoff. 133 7.4.4 Dyna: Sutton The Dyna architecture <ref> (Sutton, 1991) </ref> is an agent architecture that attempts to integrate trial and error learning, planning and reactive execution. Dyna uses reinforcement learning to induce an optimal reactive policy (the mapping from states to actions). <p> Finally, all these processes occur incrementally and in parallel: Dyna can react, be reinforced, build an action model, and plan 3 all at the same time. Figure 7.2 (taken from <ref> (Sutton, 1991) </ref>) is pseudo-code for the generic algorithm driving Dyna. Dyna makes no commitments to the implementation details of the action model. Neither does it specify how hypothetical world states and actions are to be chosen for model-based, rather than action-based, reinforcement learning.
Reference: <author> Toates, F. </author> <year> (1986). </year> <title> Motivational systems. </title> <publisher> Cambrdige University Press. </publisher>
Reference-contexts: It is an open question whether circulation of value can be generalised to other motivational systems. For example, homeostatic and simpler feedback motivators, such as maintaining temperature, removing waste, altering body posture, hunger, thirst and so forth (for example, the motivational systems described in <ref> (Toates, 1986) </ref>), do not seem to involve the kind of valenced perturbant states characteristic of processes of attachment.) (h) Possession of CUE is an ability to buy processing power. The possession of CUE by a substate is a dispositional ability to buy processing power.
Reference: <author> Watkins, C. & Dayan, P. </author> <year> (1992). </year> <title> Technical note: </title> <booktitle> Q-learning. In Machine Learning 8, </booktitle> <pages> pages 279-292. </pages>
Reference-contexts: Often there may be long and uncertain delays between a decision and its outcome. 7.4.2 Q-learning: Watkins Q-learning <ref> (Watkins & Dayan, 1992) </ref> is a much studied reinforcement learning algorithm. A Q-learner maintains a reward prediction P for each state-action combination of the policy function. In Q-learning work, P is referred to as the quality of the state-action pair and is written Q (x; a). <p> In other words, the Q value is increased, making it more 128 likely that action 2 will be executed in state 65 in the future, assuming that other Q (65; a) do not also increase. Mathematical proofs show that, in certain environments, Q-Learning converges to an optimal policy <ref> (Watkins & Dayan, 1992) </ref>, although it may take a long time. However, the inability of the basic Q-Learning to generalise over the state and action space of the task is a serious weakness.
Reference: <author> Webmaster, A. </author> <year> (1996). </year> <title> The act web. </title> <institution> Home page for the ACT-R research group, Carnegie Mellon University. </institution> <note> URL http://sands.psy.cmu.edu/ACT/act-home.html. ACT Webmaster at Webmaster@sands.psy.cmu.edu. </note>
Reference-contexts: reasoning in order to solve problems, such as classical planning systems (e.g., STRIPS, NOAH, NONLIN etc., see chapter 9 of (Charniak & McDer-mott, 1985)) or models of cognition, such as GPS (section 5.7 of (Charniak & McDermott, 1985)), the SOAR architecture (Laird, Newell & Rosenbloom, 1987; Newell, 1990), Anderson's ACT-R <ref> (Webmaster, 1996) </ref> and blackboard architectures (see section 2.2.2 of (Beaudoin, 1994)). Such systems can be usefully conceived as agents when they attempt to control parts of a real or simulated world. For example, Nilsson's SHAKEY the robot (Nilsson, 1984) used a STRIPS-style planner to plan its behaviour.
Reference: <author> Weiss, G. </author> <year> (1995). </year> <title> Distributed reinforcement learning. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 15 </volume> <pages> 135-142. </pages> <note> 232 Wellman, </note> <author> M. </author> <year> (1995). </year> <title> Market-oriented programming: some early lessons. </title> <editor> In Clear--water, S. (Ed.), </editor> <title> Market-Based Control: A Paradigm for Distributed Resource Allocation. </title> <publisher> World Scientific. </publisher>
Reference: <author> Wierzbicka, A. </author> <year> (1992). </year> <title> Defining emotion concepts. </title> <journal> Cognitive Science, </journal> <volume> 16 </volume> <pages> 539-581. </pages>
Reference-contexts: Approaches to the study of emotions can be very broadly categorised as semantics-based, phenomena-based and design-based (Sloman, 1992; Sloman, 1993d). Semantics-based theories analyse the use of language to uncover implicit assumptions underlying emotion words, for example <ref> (Wierzbicka, 1992) </ref>. Phenomena-based theories assume that emotions are a well-specified category and attempt to correlate contemporaneous and measurable phenomena with the occurrence of an emotion, such as physiological changes or the firing of neural circuits.
Reference: <author> Wilson, S. W. </author> <year> (1994). </year> <title> Zcs: A zeroth level classifier system. </title> <journal> Evolutionary Computation, </journal> <volume> 2(1) </volume> <pages> 1-18. </pages>
Reference: <author> Wilson, S. W. </author> <year> (1995). </year> <title> Classifier fitness based on accuracy. </title> <journal> Evolutionary Computation, </journal> <volume> 3(2) </volume> <pages> 149-185. </pages>
Reference-contexts: It does not anticipate, or perform prior search within a world model before acting. In real-world applications it can be difficult for a classifier system to learn appropriate behaviours (Wilson & Goldberg, 1989). 7.4.3.2 XCS: Wilson XCS <ref> (Wilson, 1995) </ref> is a development of Wilson's earlier ZCS classifier system (Wil-son, 1994), which is a `minimalist' version of Holland's classifier system. In contrast to traditional classifier systems, XCS bases the fitness of a classifier on the accuracy of its predicted payoff. <p> Third, traditional classifier systems do not discover accurate generalisations in classifier conditions. Accuracy allows XCS to evolve maximally general classifiers, those that could not have any more #'s without becoming inaccurate. A number of other researchers have incorporated accuracy information in their classifier systems (for an overview see <ref> (Wilson, 1995) </ref>). Holland, in the original paper on classifier systems (Holland, 1986), suggested that accuracy information be incorporated in calculating classifier fitness, but did not develop the idea. In XCS the traditional bucket-brigade has been replaced with a learning algorithm similar to Q-learning.
Reference: <author> Wilson, S. W. & Goldberg, D. E. </author> <year> (1989). </year> <title> A critical review of classifier systems. </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 244-255, </pages> <address> Los Altos, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It tends to be an entirely reactive system with no representation of goals. It does not anticipate, or perform prior search within a world model before acting. In real-world applications it can be difficult for a classifier system to learn appropriate behaviours <ref> (Wilson & Goldberg, 1989) </ref>. 7.4.3.2 XCS: Wilson XCS (Wilson, 1995) is a development of Wilson's earlier ZCS classifier system (Wil-son, 1994), which is a `minimalist' version of Holland's classifier system. In contrast to traditional classifier systems, XCS bases the fitness of a classifier on the accuracy of its predicted payoff.
Reference: <author> Wooldridge, M. & Jennings, N. R. </author> <year> (1995). </year> <title> Agent theories, architectures, and languages. </title> <editor> In Wooldridge, M. & Jennings, N. R. (Eds.), </editor> <booktitle> Intelligent Agents, </booktitle> <pages> pages 1-22. </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Wright, I. P. </author> <year> (1994). </year> <title> An emotional agent: the detection and control of emergent states in autonomous resource-bounded agents. </title> <type> Technical Report RP-94-21, </type> <institution> School of Computer Science and Cognitive Science Research Centre, University of Birmingham. </institution>
Reference-contexts: The relations between niche-space and design-space (Sloman, 1995a) are complex, involving many trade-offs and compromises, for example sacrificing speed of processing for depth of reasoning. The relations between requirements and design are important for understanding all types of systems, from software systems, agent architectures <ref> (Sloman, Beaudoin & Wright, 1994) </ref>, robots, and organisms in the natural world. MAS are no different in this respect: there will be many types of 146 MAS that satisfy different sets of requirements.
Reference: <author> Wright, I. P. </author> <year> (1995). </year> <journal> Cognition and currency flow. </journal> <note> Unpublished research notes. </note> <institution> Department of Computer Science, University of Birmingham. </institution>
Reference: <author> Wright, I. P. </author> <year> (1996a). </year> <title> Design requirements for a computational libidinal economy. </title> <type> Technical Report CSRP-96-11, </type> <institution> School of Computer Science and Cognitive Science Research Centre, University of Birmingham. </institution> <note> Submitted to Cognition and Emotion. </note>
Reference: <author> Wright, I. P. </author> <year> (1996b). </year> <title> Reinforcement learning and animat emotions. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., Pollack, J., & Wilson, S. W. (Eds.), </editor> <booktitle> From Animals to Animats IV, Proceedings of the Fourth International Conference on the Simulation of Adaptive Behavior, </booktitle> <pages> pages 272-281. </pages> <publisher> The MIT Press. </publisher>
Reference-contexts: Chapter 11 concludes with directions for future work and a discussion of the work presented in the thesis. 15 Appendix A provides some implementation details of MINDER1. Appendix B discusses the relationship between labour power and processing power. Appendix C contains a previously published paper <ref> (Wright, 1996b) </ref>, a require ment of the Faculty of Science submission regulations. <p> In contrast to CUE, substate products have representational content: they are `about' other things, be they states-of-affairs in the environment or within the system itself. A concrete, if simple, example can be provided by the classifier system (for a fuller account see <ref> (Wright, 1996b) </ref>). The substate products in this system are messages. Imagine an artificial frog embedded in an environment of real or simulated flies 2 . The control program for simfrog is a classifier system with an adapted set of classifiers.
Reference: <author> Wright, I. P. & Aube, M. </author> <year> (1997). </year> <title> The society of mind requires an economy of mind. </title> <type> Technical Report CSRP-97-6, </type> <institution> School of Computer Science and Cognitive Science Research Centre, University of Birmingham. </institution> <note> Submitted to MAAMAW '97. </note>
Reference-contexts: However, chapter 10 has been revised for this thesis, in particular extending the analysis of grief to mental pain and pleasure. Chapter 8 first appeared as a joint technical report with Michel Aube <ref> (Wright & Aube, 1997) </ref>. Aube provided information and references on multi-agent system research, convinced me of the importance of the concept of `commitment', and provided comments on a first draft.
Reference: <author> Wright, I. P., Sloman, A., & Beaudoin, L. P. </author> <year> (1996). </year> <title> Towards a design based analysis of emotional episodes. </title> <journal> Philosophy Psychiatry and Psychology, </journal> <volume> 3(2) </volume> <pages> 101-137. </pages> <booktitle> 234 Part V APPENDICES 235 </booktitle>
Reference-contexts: build on the work presented in this thesis. (Chapter 11). 17 1.3 A note on joint work Section 2.3.1.1 on `ontology and the design-based approach', section 4.4.1 on `motive processing' and chapter 10 on `a circulation of value analysis of attachment and loss' contain joint work that first appeared in <ref> (Wright, Sloman & Beaudoin, 1996) </ref>, a paper originally written by me but revised and added to by Aaron Sloman. However, chapter 10 has been revised for this thesis, in particular extending the analysis of grief to mental pain and pleasure. <p> This does not imply that everything that happens is generated by those goals, for that would rule out intrusions, such as feeling hungry, or a new desire to help someone in trouble. These can arise without contradicting one's view of what one should be like <ref> (Wright, Sloman & Beaudoin, 1996) </ref>. There need not be only one coherent global set of goals, preferences, and so forth, since some people seem to change personality from one context to another, like the kind father who is an aggressive car driver. <p> An architecture can be specified at different levels of detail, for example at a high level of abstraction the architecture of a house will not include the occurrence of particular bricks, whereas a more detailed architectural specification would <ref> (Wright, Sloman & Beaudoin, 1996) </ref>. An agent architecture, therefore, is a description of the information processing mechanisms of a control system at a particular level of abstraction. 36 There are many examples of agent architecture design in the literature. <p> A full account of the design would be too long for this thesis, so the account concentrates on the processing of motivators. 4.4.1 Motive processing The summary of the agent architecture is taken from <ref> (Wright, Sloman & Beaudoin, 1996) </ref>. The term `motivator' is used to refer to a subclass of information structures with dispositional powers to determine action (both internal and external). This subsumes desires, goals, intentions and wishes. <p> The discussion of perturbance detection is postponed until section 5.5. Metamanagement implementation is extremely shallow when compared to the C&AP's design. A full implementation would include sophisticated `self-monitoring' processes that detect, evaluate and control management processes <ref> (Wright, Sloman & Beaudoin, 1996) </ref>. Management processing is resource limited. <p> This is a partial answer as it may account for the initial disrupting perturbance but it does not argue against the possibility that local insistence functions could be subsequently altered, or certain classes of motivators selectively ignored through the use of an exception field. <ref> (Wright, Sloman & Beaudoin, 1996) </ref> provides other possible explanations, for example the possibility that information processing attachment structures are both distributed within the architecture and highly differentiated, such that reorganisa-tion after loss may require extensive cognitive work that, by its very nature, takes time (cf. updating a large and complex belief <p> Subjectively, the `mental pain' of such emotional episodes is arguably their most prominent feature. Mourners often use analogies with physical distress when describing their mental states <ref> (Wright, Sloman & Beaudoin, 1996) </ref>, employing words such as `hurt', `pain' and `intensity'. Some emotional states may be painful and some may be pleasurable. 6.4.3.2 Thinking and feeling In everyday conversation people often make a distinction between thinking and feeling. There are grounds for this distinction. <p> Pain can have location or be caused by events but the painfulness does not refer and is not intentional. It is these kinds of mental contents that lead some to believe that the emotions are ineffable, irreducible, irredeemably subjective, and opaque to mechanistic analysis and explanation. In <ref> (Wright, Sloman & Beaudoin, 1996) </ref> the concept of perturbance was used to provide a design-based analysis of loss or grief. However, we were unable to provide a satisfactory account of perturbant states that possess a pleasure or unpleasure dimension (compare grieving with glee). <p> The mechanisms may be linked in that termination or reduction of pleasure inducing states may function as a pain inducing state and vice versa. In some cases these links will need to be built up through learnt associations. In others they may be `hard-wired' in the architecture <ref> (Wright, Sloman & Beaudoin, 1996) </ref>. In this quotation the phrase `hedonic tone' is used to refer to the implicit evaluations. Implicit evaluations are, in agreement with the COM theory, assumed to be phylogenetically older than explicit, semantic evaluations. <p> This is not a new idea: `Pain cries "Be gone!", but pleasure craves eternity ...' (Nietzsche, 1896). However, the above quotation from <ref> (Wright, Sloman & Beaudoin, 1996) </ref> does not stipulate the presence of plans: pleasure and pain mechanisms may be present in both reactive and deliberative architectures. <p> This is for two reasons: first, attachment is well studied so the consequences of the CFHN can be compared to existing theories; second, attachment and loss are characterised by valenced perturbant states <ref> (Wright, Sloman & Beaudoin, 1996) </ref>, involving just the kind of mental states and processes that existing interrupt theories encounter difficulty explaining. The adjective `libidinal' is intended to reflect this current restriction of the theory. <p> Every control state within the agent architecture has structure, aetiology, powers, transformation capabilities, liabilities, and in some cases semantics <ref> (Wright, Sloman & Beaudoin, 1996) </ref>. These ideas include analogues of the linguistic notions of syntax, semantics, pragmatics, and inference (Sloman, 1994b). <p> The circulation of value is a pattern of flow of control, as opposed to semantic, signals. Such signals have no semantic content (although see caveats in section 7.5.3) and propagate around the system altering control flow. Consider that self-monitoring mechanisms (see section 4.4 and <ref> (Wright, Sloman & Beaudoin, 1996) </ref>) are required to monitor the circulation of the CUE occurring within the libidinal economy. The reasons why this may be necessary are not discussed here.
References-found: 209

