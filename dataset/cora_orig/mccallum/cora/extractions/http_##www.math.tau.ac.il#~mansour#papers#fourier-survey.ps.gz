URL: http://www.math.tau.ac.il/~mansour/papers/fourier-survey.ps.gz
Refering-URL: 
Root-URL: 
Title: Learning Boolean Functions via the Fourier Transform  
Author: Yishay Mansour 
Date: February 23, 1994  
Abstract: We survey learning algorithms that are based on the Fourier Transform representation. In many cases we simplify the original proofs and integrate the proofs of related results. We hope that this would give the reader a complete and comprehensive un derstanding of both the results and the techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [Ajt83] <author> M. Ajtai. </author> <title> P 1 1 -formulae on finite structure. </title> <journal> Annals of Pure and Applied Logic, </journal> <volume> 24 </volume> <pages> 1-48, </pages> <year> 1983. </year>
Reference-contexts: Some of the learning results shown here are based on the lower bound techniques that were developed for proving lower bound for polynomial size constant depth circuit <ref> [Ajt83, FSS84, Yao85, Has86] </ref>. When we need to apply those results we only state the results that we use but do not prove them. The paper is organized as following. Section 2 gives the definition of the learning model and some basic results that are used throughout the paper.
Reference: [AM91] <author> William Aiello and Milena Mihail. </author> <title> Learning the fourier spectrum of probabilistic lists and trees. </title> <booktitle> In Proceedings SODA 91, </booktitle> <pages> pages 291-299. </pages> <publisher> ACM, </publisher> <month> Jan </month> <year> 1991. </year>
Reference-contexts: For the specific case of DNF the result was improved in [Man92]. In [Kha93] it is shown, based on a cryptographic assumption, that the running time of O (n polylog (n) ) for AC 0 circuits is the best possible. In <ref> [AM91] </ref> polynomial time algorithms are given for learning both probabilistic decision lists and probabilistic read once decision trees with respect to the uniform distribution. In this paper we concentrate on deterministic functions, namely deterministic decision lists, hence, some of the techniques and the results of [AM91] do not appear here. <p> In <ref> [AM91] </ref> polynomial time algorithms are given for learning both probabilistic decision lists and probabilistic read once decision trees with respect to the uniform distribution. In this paper we concentrate on deterministic functions, namely deterministic decision lists, hence, some of the techniques and the results of [AM91] do not appear here. The work of [KM91] uses the Fourier representation to derive a polynomial time learning algorithm for decision trees, with respect to the uniform distribution. The algorithm is based on a procedure that finds the significant Fourier coefficients. <p> Other works that are investigating the Fourier Transform of Boolean functions are [Bru90, BS90, SB91]. In this work we focus on the main results about the connection between Fourier Transform and learnability. The survey is mainly based on the works that appeared in <ref> [LMN89, AM91, KM91, Man92] </ref>. Some of the proofs are a simplification of the original proofs and in many cases we try to give a common structure to different results. <p> In Section 5, we show various classes of functions that can be learned using the above algorithm. We start with the simple class of decision lists (from <ref> [AM91] </ref>). We continue with 2 properties of decision trees (from [KM91]). The last class is boolean circuits there we show properties for both DNF and AC 0 circuits (from [LMN89, Man92]). 2 Preliminaries 2.1 Learning Model The learning model has a class of functions F which we wish to learn.
Reference: [Bel92] <author> Mihir Bellare. </author> <title> A technique for upper bounding the spectral norm with applications to learning. </title> <booktitle> In 5 th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 62-70, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: There has been a few successful attempts to extend some of the results to product distributions. In [FJS91] it is shown how to learn AC 0 circuit with respect to a product distribution. In <ref> [Bel92] </ref> the algorithm that searches for the significant coefficients is extended to work for product distributions. However, in this paper we concentrate on the uniform distribution. There are additional works about the Fourier Transform representation of boolean function. <p> A more general case is that the nodes in the decision tree includes arbitrary predicates. <ref> [Bel92] </ref> derives a general method for bounding the L 1 of the decision tree as a function of L 1 of the predicates in the nodes.
Reference: [BHO90] <author> Y. Brandman, J. Hennessy, and A. Orlitsky. </author> <title> A spectral lower bound technique for the size of decision trees and two level circuits. </title> <journal> IEEE Trans. on Computers., </journal> <volume> 39(2) </volume> <pages> 282-287, </pages> <year> 1990. </year>
Reference-contexts: The first work used Fourier Transform to the show results in theoretical computer science was the work of [KKL88], that proves properties about the sensitivity of boolean functions. The relation between DNFs and their Fourier Transform representation is also studied in <ref> [BHO90] </ref>. Other works that are investigating the Fourier Transform of Boolean functions are [Bru90, BS90, SB91]. In this work we focus on the main results about the connection between Fourier Transform and learnability. The survey is mainly based on the works that appeared in [LMN89, AM91, KM91, Man92].
Reference: [Bru90] <author> J. Bruck. </author> <title> Harmonic analysis of polynomial threshold functions. </title> <journal> Siam J. on Disc. Math., </journal> <volume> 3(2) </volume> <pages> 168-177, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The relation between DNFs and their Fourier Transform representation is also studied in [BHO90]. Other works that are investigating the Fourier Transform of Boolean functions are <ref> [Bru90, BS90, SB91] </ref>. In this work we focus on the main results about the connection between Fourier Transform and learnability. The survey is mainly based on the works that appeared in [LMN89, AM91, KM91, Man92].
Reference: [BS90] <author> J. Bruck and R. Smolensky. </author> <title> Polynomial threshold functions, AC 0 functions and spectral norms. </title> <booktitle> In 31 th Annual Symposium on Foundations of Computer Science, </booktitle> <address> St. Louis, </address> <publisher> Missouri, </publisher> <pages> pages 632-641, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: The relation between DNFs and their Fourier Transform representation is also studied in [BHO90]. Other works that are investigating the Fourier Transform of Boolean functions are <ref> [Bru90, BS90, SB91] </ref>. In this work we focus on the main results about the connection between Fourier Transform and learnability. The survey is mainly based on the works that appeared in [LMN89, AM91, KM91, Man92].
Reference: [FJS91] <author> Merrick L. Furst, Jeffrey C. Jackson, and Sean W. Smith. </author> <title> Improved learning of AC 0 functions. </title> <booktitle> In 4 th Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 317-325, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The algorithm is based on a procedure that finds the significant Fourier coefficients. Most of the work on learning using the Fourier Transform assumes that the underline distribution is uniform. There has been a few successful attempts to extend some of the results to product distributions. In <ref> [FJS91] </ref> it is shown how to learn AC 0 circuit with respect to a product distribution. In [Bel92] the algorithm that searches for the significant coefficients is extended to work for product distributions. However, in this paper we concentrate on the uniform distribution.
Reference: [FSS84] <author> M. Furst, J. Saxe, and M. Sipser. </author> <title> Parity, circuits, and the polynomial time hierarchy. </title> <journal> Mathematical Systems Theory, </journal> <volume> 17 </volume> <pages> 13-27, </pages> <year> 1984. </year>
Reference-contexts: Some of the learning results shown here are based on the lower bound techniques that were developed for proving lower bound for polynomial size constant depth circuit <ref> [Ajt83, FSS84, Yao85, Has86] </ref>. When we need to apply those results we only state the results that we use but do not prove them. The paper is organized as following. Section 2 gives the definition of the learning model and some basic results that are used throughout the paper.
Reference: [GL89] <author> O. Goldreich and L. Levin. </author> <title> A hard-core predicate for all one-way functions. </title> <booktitle> In Proc. 21st ACM Symposium on Theory of Computing, </booktitle> <pages> pages 25-32. </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: Section 4 establishes the connection between the Fourier Transform and learning. This section includes two important algorithms. The Low Degree algorithm, that approximates functions by considering their Fourier coefficients on small sets and the Sparse algorithm, that is based on the work of <ref> [GL89, KM91] </ref>, which learns a function by approximating its significant coefficients. In Section 5, we show various classes of functions that can be learned using the above algorithm. We start with the simple class of decision lists (from [AM91]). We continue with 2 properties of decision trees (from [KM91]).
Reference: [Has86] <author> J. Hastad. </author> <title> Computational limitations for small depth circuits. </title> <publisher> MIT Press, </publisher> <year> 1986. </year> <type> Ph.D. thesis. </type>
Reference-contexts: Some of the learning results shown here are based on the lower bound techniques that were developed for proving lower bound for polynomial size constant depth circuit <ref> [Ajt83, FSS84, Yao85, Has86] </ref>. When we need to apply those results we only state the results that we use but do not prove them. The paper is organized as following. Section 2 gives the definition of the learning model and some basic results that are used throughout the paper. <p> Then, X jSj&gt;t [DT-depth (f ) tp=2]: Note that the above lemma is non-trivial only if Pr [DT-depth (f ) tp=2] 1 2 . The property of DNF that we would use is based on random restriction. The following lemma, from <ref> [Has86] </ref>, states that a DNF after a random restriction can be described by a small decision tree. Lemma 5.12 (Hastad) Let f be given by a DNF formula where each term has size at most d, and a random restriction with parameter p (i.e. <p> It consists of circuits composed from AND, OR and NOT gates with unbounded fan-in, where the number of gates is polynomial in the number of inputs and the depth of the circuit is constant. The following lemma is from <ref> [Has86] </ref>, and can be derived by repeated applications of Lemma 5.12. Lemma 5.14 (Hastad) Let f be an AC 0 circuit with M gates and depth d.
Reference: [HR89] <author> Torben Hagerup and Christine Rub. </author> <title> A guided tour to chernoff bounds. </title> <journal> Info. Proc. Lett., </journal> <volume> 33 </volume> <pages> 305-308, </pages> <year> 1989. </year>
Reference-contexts: least 1 ffi, error (f; h) " The algorithm A learns in polynomial time if its running time is polynomial in n, 1=", and log 1=ffi. 2.2 Probability In many places we use the Chernoff bound to bound the sum of random variables. (For a presentation of the bounds see <ref> [HR89] </ref>.) Lemma 2.1 (Chernoff ) Let X 1 ; : : : ; X m be independent identically distributed random variables such that, X i 2 [1; +1], E [X i ] = p and S m = P m i=1 X i .
Reference: [Kha93] <author> Michael Kharitonov. </author> <title> Cryptographic hardness of distribution-specific learning. </title> <booktitle> In Proceedings of STOC '93, </booktitle> <pages> pages 372-381. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: For the specific case of DNF the result was improved in [Man92]. In <ref> [Kha93] </ref> it is shown, based on a cryptographic assumption, that the running time of O (n polylog (n) ) for AC 0 circuits is the best possible. <p> Then, X jAjt ^ f 2 M 2 t d For t = (20 log M " ) d the sum is bounded by ". Thus, running the Low Degree Algorithm results in time complexity of O (n polylog (n) ). Remark: From the result of <ref> [Kha93] </ref> we cannot hope to get a better running time than O (n polylog (n) ), unless some cryptographic assumption about factoring is false. 5.3.2 Sparse approximation of DNF We show that DNF with "small" terms can be approximated by a sparse function.
Reference: [KKL88] <author> J. Kahn, G. Kalai, and N. Linial. </author> <title> The influence of variables on boolean functions. </title> <booktitle> In 29 th Annual Symposium on Foundations of Computer Science, </booktitle> <address> White Plains, New York, </address> <pages> pages 68-80, </pages> <month> October </month> <year> 1988. </year> <month> 22 </month>
Reference-contexts: However, in this paper we concentrate on the uniform distribution. There are additional works about the Fourier Transform representation of boolean function. The first work used Fourier Transform to the show results in theoretical computer science was the work of <ref> [KKL88] </ref>, that proves properties about the sensitivity of boolean functions. The relation between DNFs and their Fourier Transform representation is also studied in [BHO90]. Other works that are investigating the Fourier Transform of Boolean functions are [Bru90, BS90, SB91].
Reference: [KM91] <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning decision trees using the fourier spectrum. </title> <booktitle> In Proceedings of the 23 rd Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 455-464, </pages> <month> May </month> <year> 1991. </year> <note> (To appear in Siam J. on Computing.) </note>
Reference-contexts: In this paper we concentrate on deterministic functions, namely deterministic decision lists, hence, some of the techniques and the results of [AM91] do not appear here. The work of <ref> [KM91] </ref> uses the Fourier representation to derive a polynomial time learning algorithm for decision trees, with respect to the uniform distribution. The algorithm is based on a procedure that finds the significant Fourier coefficients. <p> Other works that are investigating the Fourier Transform of Boolean functions are [Bru90, BS90, SB91]. In this work we focus on the main results about the connection between Fourier Transform and learnability. The survey is mainly based on the works that appeared in <ref> [LMN89, AM91, KM91, Man92] </ref>. Some of the proofs are a simplification of the original proofs and in many cases we try to give a common structure to different results. <p> Section 4 establishes the connection between the Fourier Transform and learning. This section includes two important algorithms. The Low Degree algorithm, that approximates functions by considering their Fourier coefficients on small sets and the Sparse algorithm, that is based on the work of <ref> [GL89, KM91] </ref>, which learns a function by approximating its significant coefficients. In Section 5, we show various classes of functions that can be learned using the above algorithm. We start with the simple class of decision lists (from [AM91]). We continue with 2 properties of decision trees (from [KM91]). <p> In Section 5, we show various classes of functions that can be learned using the above algorithm. We start with the simple class of decision lists (from [AM91]). We continue with 2 properties of decision trees (from <ref> [KM91] </ref>). The last class is boolean circuits there we show properties for both DNF and AC 0 circuits (from [LMN89, Man92]). 2 Preliminaries 2.1 Learning Model The learning model has a class of functions F which we wish to learn. <p> The algorithm runs in time polynomial in n; t, 1=" and log 1=ffi. 2 We do not show the details of the algorithm and the interested reader is referred to <ref> [KM91] </ref>. 12 -x 3 x 1 x 4 true 0 0 0 +1 -1 -1 +1 5 Properties of complexity classes In this section we show various complexity class which can be learned through their Fourier spectrum. We show two kind of properties.
Reference: [LMN89] <author> N. Linial, Y. Mansour, and N. Nisan. </author> <title> Constant depth circuits, Fourier Transform and learnability. </title> <booktitle> In 30 th Annual Symposium on Foundations of Computer Science, </booktitle> <address> Reseach Triangle Park, NC, </address> <pages> pages 574-579, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: The basis functions are based on the parity of subsets of the input variables. Every function whose inputs are boolean can be written as a linear combination of this basis, and coefficients represent the correlation between the function and the basis function. The work of <ref> [LMN89] </ref> was the first to point of the connection between the Fourier spectrum and learnability. They presented a quasi-polynomial-time (i.e. O (n polylog (n) )) algorithm for learning the class AC 0 (polynomial size constant depth circuits); the approximation is with respect to the uniform distribution. <p> Other works that are investigating the Fourier Transform of Boolean functions are [Bru90, BS90, SB91]. In this work we focus on the main results about the connection between Fourier Transform and learnability. The survey is mainly based on the works that appeared in <ref> [LMN89, AM91, KM91, Man92] </ref>. Some of the proofs are a simplification of the original proofs and in many cases we try to give a common structure to different results. <p> We start with the simple class of decision lists (from [AM91]). We continue with 2 properties of decision trees (from [KM91]). The last class is boolean circuits there we show properties for both DNF and AC 0 circuits (from <ref> [LMN89, Man92] </ref>). 2 Preliminaries 2.1 Learning Model The learning model has a class of functions F which we wish to learn. Out of this class there is a specific function f 2 F which is chosen as a target function. A learning algorithm has access to examples.
Reference: [Man92] <author> Yishay Mansour. </author> <title> An O(n log log n ) learning algorihm for DNF under the uniform distribution. </title> <booktitle> In Workshop on Computational Learning Theory, </booktitle> <pages> pages 53-61, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: For the specific case of DNF the result was improved in <ref> [Man92] </ref>. In [Kha93] it is shown, based on a cryptographic assumption, that the running time of O (n polylog (n) ) for AC 0 circuits is the best possible. <p> Other works that are investigating the Fourier Transform of Boolean functions are [Bru90, BS90, SB91]. In this work we focus on the main results about the connection between Fourier Transform and learnability. The survey is mainly based on the works that appeared in <ref> [LMN89, AM91, KM91, Man92] </ref>. Some of the proofs are a simplification of the original proofs and in many cases we try to give a common structure to different results. <p> We start with the simple class of decision lists (from [AM91]). We continue with 2 properties of decision trees (from [KM91]). The last class is boolean circuits there we show properties for both DNF and AC 0 circuits (from <ref> [LMN89, Man92] </ref>). 2 Preliminaries 2.1 Learning Model The learning model has a class of functions F which we wish to learn. Out of this class there is a specific function f 2 F which is chosen as a target function. A learning algorithm has access to examples.
Reference: [Riv87] <author> Ronald L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2(3) </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: always true.) It is clear that the length of L is at most n + 1, since if a variable appears twice we can either eliminate its second appearance or replace it by a terminal pair. (We should remark that such deterministic decision list are learnable by other methods, see <ref> [Riv87] </ref>.
Reference: [SB91] <author> Kai-Yeung Siu and Jehoshua Bruck. </author> <title> On the power of threshold circuits with small weights. </title> <journal> Siam J. on Disc. Math., </journal> <volume> 4(3) </volume> <pages> 423-435, </pages> <month> Aug </month> <year> 1991. </year>
Reference-contexts: The relation between DNFs and their Fourier Transform representation is also studied in [BHO90]. Other works that are investigating the Fourier Transform of Boolean functions are <ref> [Bru90, BS90, SB91] </ref>. In this work we focus on the main results about the connection between Fourier Transform and learnability. The survey is mainly based on the works that appeared in [LMN89, AM91, KM91, Man92].
Reference: [Yao85] <author> A. C. Yao. </author> <title> Separating the polynomial-time hierarchy by oracles. </title> <booktitle> In 26 th Annual Symposium on Foundations of Computer Science, Portland, Oregon, </booktitle> <pages> pages 1-10, </pages> <month> October </month> <year> 1985. </year> <month> 23 </month>
Reference-contexts: Some of the learning results shown here are based on the lower bound techniques that were developed for proving lower bound for polynomial size constant depth circuit <ref> [Ajt83, FSS84, Yao85, Has86] </ref>. When we need to apply those results we only state the results that we use but do not prove them. The paper is organized as following. Section 2 gives the definition of the learning model and some basic results that are used throughout the paper.
References-found: 19

