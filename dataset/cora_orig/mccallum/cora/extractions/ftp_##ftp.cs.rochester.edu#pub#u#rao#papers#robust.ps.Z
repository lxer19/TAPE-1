URL: ftp://ftp.cs.rochester.edu/pub/u/rao/papers/robust.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/rao/papers.html
Root-URL: 
Title: Robust Kalman Filters for Prediction, Recognition, and Learning  
Author: Rajesh P. N. Rao 
Note: This work is supported by NIH/PHS research grant no. 1 P41 RRO9283.  
Date: December, 1996  
Address: Rochester, New York 14627  
Affiliation: The University of Rochester Computer Science Department  
Pubnum: Technical Report 645  
Abstract: Using results from the field of robust statistics, we derive a class of Kalman filters that are robust to structured and unstructured noise in the input data stream. Each filter from this class maintains robust optimal estimates of the input process's hidden state by allowing the measurement covariance matrix to be a non-linear function of the prediction errors. This endows the filter with the ability to reject outliers in the input stream. Simultaneously, the filter also learns an internal model of input dynamics by adapting its measurement and state transition matrices using two additional Kalman filter-based adaptation rules. We present experimental results demonstrating the efficacy of such filters in mediating appearance-based segmentation and recognition of objects and image sequences in the presence of varying degrees of occlusion, clutter, and noise. 
Abstract-found: 1
Intro-found: 1
Reference: [Athans, 1974] <author> M. Athans, </author> <title> "The importance of Kalman filtering methods for economic systems," </title> <journal> Annals of Economic and Social Measurement, </journal> <volume> 3 </volume> <pages> 49-64, </pages> <year> 1974. </year>
Reference-contexts: 1 INTRODUCTION Three and a half decades after its discovery, the Kalman filter remains one of the most versatile algorithms in parameter estimation theory, having found applications in areas as diverse as economics <ref> [Athans, 1974] </ref>, engineering [Cipra, 1993], and neuroscience [Rao and Ballard, 1996b]. The Kalman filter provides optimal estimates of the hidden state of any natural process that can be modeled as a linear dynamical system.
Reference: [Ayache and Faugeras, 1986] <author> N. Ayache and O.D. Faugeras, </author> <title> "HYPER: A new approach for the recognition and positioning of two-dimensional objects," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 8(1) </volume> <pages> 44-54, </pages> <year> 1986. </year>
Reference: [Black and Jepson, 1996] <author> M.J. Black and A.D. Jepson, "Eigentracking: </author> <title> Robust matching and tracking of articulated objects using a view-based representation," </title> <booktitle> In Proc. of ECCV, </booktitle> <pages> pages 329-342, </pages> <year> 1996. </year>
Reference: [Blake and Yuille, 1992] <author> A. Blake and A. Yuille, </author> <title> editors, Active Vision, </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: sufficiently accurate predictions of the corrupted stimuli, after having successfully detected and filtered out most of the outliers (white regions in the last row of Figure 3 (e)). 6 DISCUSSION AND CONCLUSION During the past decade, Kalman filters have been applied to a wide range of problems in computer vision <ref> [Blake and Yuille, 1992] </ref> and image processing [Chou et al., 1994]. However, a majority of these approaches have used hard-wired dynamic models inferred from a priori knowledge of the task at hand.
Reference: [Broida and Chellappa, 1986] <author> T.J. Broida and R. Chellappa, </author> <title> "Estimation of object motion parameters from noisy images," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 8(1) </volume> <pages> 90-99, </pages> <year> 1986. </year>
Reference: [Bryson and Ho, 1975] <author> A.E. Bryson and Y.-C. Ho, </author> <title> Applied Optimal Control, </title> <address> New York: </address> <publisher> John Wiley and Sons, </publisher> <year> 1975. </year>
Reference-contexts: relatively difficult problem in vision, namely, appearance-based segmentation and recognition of arbitrarily complex objects and image sequences in the presence of varying degrees of occlusion, clutter, and noise. 2 THE KALMAN FILTER In this section, we briefly review the discrete Kalman filter and summarize its derivation (for more details, see <ref> [Bryson and Ho, 1975] </ref>). <p> Such an assumption is partly justified by the Central Limit Theorem [Feller, 1968]. In order to specify how the state r changes with time, the Kalman filter assumes that the process of interest can be modeled as a Gauss-Markov random process <ref> [Bryson and Ho, 1975] </ref>. <p> This function is a special case of the more general weighted least-squares criterion: J = (I U r) T 1 (I U r) + (r r) T M 1 (r r) (4) It is easy to show (see, for example, <ref> [Bryson and Ho, 1975] </ref>) that J is simply the sum of the negative log-likelihood of generating the data I given the state r, and the negative log of the prior probability of the state r. <p> In the case of dynamic (time-varying) stimuli, the EM algorithm prescribes the use of r (t) = b r (tjN ), which is the optimal temporally smoothed state estimate <ref> [Bryson and Ho, 1975] </ref> for time t ( N ), given input data for each of the time instants 1; : : :; N . Unfortunately, the smoothed state estimate requires knowledge of future inputs and is computationally quite expensive.
Reference: [Chou et al., 1994] <author> K.C. Chou, A.S. Willsky, and A. Benveniste, </author> <title> "Multiscale recursive estimation, data fusion, and regularization," </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 39(3) </volume> <pages> 464-478, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: after having successfully detected and filtered out most of the outliers (white regions in the last row of Figure 3 (e)). 6 DISCUSSION AND CONCLUSION During the past decade, Kalman filters have been applied to a wide range of problems in computer vision [Blake and Yuille, 1992] and image processing <ref> [Chou et al., 1994] </ref>. However, a majority of these approaches have used hard-wired dynamic models inferred from a priori knowledge of the task at hand.
Reference: [Cipra, 1993] <author> B. Cipra, </author> <title> "Engineers look to Kalman filtering for guidance," </title> <journal> SIAM News, </journal> <volume> 26(5), </volume> <year> 1993. </year>
Reference-contexts: 1 INTRODUCTION Three and a half decades after its discovery, the Kalman filter remains one of the most versatile algorithms in parameter estimation theory, having found applications in areas as diverse as economics [Athans, 1974], engineering <ref> [Cipra, 1993] </ref>, and neuroscience [Rao and Ballard, 1996b]. The Kalman filter provides optimal estimates of the hidden state of any natural process that can be modeled as a linear dynamical system.
Reference: [Dayan et al., 1995] <author> P. Dayan, G.E. Hinton, R.M. Neal, </author> <title> and R.S. Zemel, "The Helmholtz Machine," </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 889-904, </pages> <year> 1995. </year>
Reference-contexts: interest is characterized by a k fi 1 internal state vector r (t) that cannot be measured directly, but generates measurable outputs I (t) in the following manner: I (t) = U r (t) + n (t) (1) In the above, U is an n fi k "measurement" (or generative <ref> [Dayan et al., 1995] </ref>) matrix, and I is an n fi 1 vector representing, for example, an input image (generated by a set of 1 hidden "causes" r).
Reference: [Dempster et al., 1977] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin, </author> <title> "Maximum Likelihood from Incomplete Data via the EM Algorithm," </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: An interesting question is the issue of convergence of the overall filtering/learning scheme involving r, U , and V . Fortunately, one can appeal to the well-known Expectation-Maximization (EM) algorithm <ref> [Dempster et al., 1977] </ref> and allow the overall scheme to converge by choosing appropriate values for the state r in the above learning rules for u and v (note that in the above rules, we did not specify values for r (t) (comprising R (t)) in Equation 13 and r (t
Reference: [Dickmanns and Mysliwetz, 1992] <editor> E.D. Dickmanns and B.D. Mysliwetz, </editor> <title> "Recursive 3D road and relative ego-state recognition," </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> 14(2) </volume> <pages> 199-213, </pages> <year> 1992. </year>
Reference: [Feller, 1968] <author> W. Feller, </author> <title> An Introduction to Probability Theory and Its Applications, volume 1, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1968. </year> <month> 12 </month>
Reference-contexts: The Kalman filter assumes that the noise n follows a Gaussian distribution, with mean E (n) = 0 and covariance = E [nn T ]. Such an assumption is partly justified by the Central Limit Theorem <ref> [Feller, 1968] </ref>. In order to specify how the state r changes with time, the Kalman filter assumes that the process of interest can be modeled as a Gauss-Markov random process [Bryson and Ho, 1975].
Reference: [Hallam, 1983] <author> J. Hallam, </author> <title> "Resolving observer motion by object tracking," </title> <booktitle> In Proc. of 8th International Joint Conf. on Artificial Intelligence, </booktitle> <volume> volume 2, </volume> <pages> pages 792-798, </pages> <year> 1983. </year>
Reference: [Huber, 1981] <author> P.J. Huber, </author> <title> Robust Statistics, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Making constant however reduces the Kalman filter estimates to be standard least-squares estimates. It is well-known that least-squares estimation is highly susceptible to outliers or gross errors i.e. data points that lie far away from the bulk of the observed data <ref> [Huber, 1981] </ref>. For example, in the case where I represents an input image, occlusions and other forms of noise will cause many pixels in I to deviate significantly from corresponding pixels in the predicted image U r. <p> These components of I need to be treated as outliers and discounted for in the minimization process in order to get an accurate estimate of the state r. The problem of outliers can be tackled using robust estimation procedures <ref> [Huber, 1981] </ref>. One commonly used procedure is M-estimation (Maximum likelihood type estimation), which involves minimizing a function of the form: J 0 = i=1 where is normally taken to be a less rapidly increasing function than the square.
Reference: [Isard and Blake, 1996] <author> M. Isard and A. Blake, </author> <title> "Contour tracking by stochastic propoga-tion of conditional density," </title> <booktitle> In Proc. of ECCV, </booktitle> <pages> pages 343-356, </pages> <year> 1996. </year>
Reference-contexts: In addition, the robust Kalman filter proposed herein may serve as an alternative to more complex stochastic estimation schemes such as the CONDENSATION algorithm <ref> [Isard and Blake, 1996] </ref> for tackling the problem of occluders and background clutter in the input image stream. The robust Kalman filter presented here can be readily extended to the hierarchical Kalman filter framework proposed in [Rao and Ballard, 1996a].
Reference: [Lades et al., 1993] <author> M. Lades, J.C. Vorbruggen, J. Buhmann, J. Lange, C. von der Mals-burg, R.P. Wurtz, and W. Konen, </author> <title> "Distortion Invariant Object Recognition in the Dynamic Link Architecture," </title> <journal> IEEE Trans. Computers, </journal> <volume> 42 </volume> <pages> 300-311, </pages> <year> 1993. </year>
Reference: [Matthies et al., 1989] <author> L. Matthies, T. Kanade, and R. Szeliski, </author> <title> "Kalman Filter-based Algorithms for Estimating Depth from Image Sequences," </title> <journal> International Journal of Computer Vision, </journal> <volume> 3 </volume> <pages> 209-236, </pages> <year> 1989. </year>
Reference: [Mel, 1996] <author> B. Mel, "SEEMORE: </author> <title> A View-Based Approach to 3-D Object Recognition Using Multiple Visual Cues," </title> <editor> In D. Touretzky, M. Mozer, and M. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 865-871. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference: [Murase and Nayar, 1995] <author> H. Murase and S.K. Nayar, </author> <title> "Visual Learning and Recognition of 3D Objects from Appearance," </title> <journal> IJCV, </journal> <volume> 14 </volume> <pages> 5-24, </pages> <year> 1995. </year>
Reference-contexts: Thus, the Kalman filter based method described herein generalizes principal component (or eigenspace <ref> [Murase and Nayar, 1995] </ref>) based approaches by (a) allowing non-orthogonal basis vectors, (b) seeking more than pairwise correlations in the input data [Olshausen and Field, 1996], and (c) allowing the learning and recognition of time-varying imagery.
Reference: [Oja, 1989] <author> E. Oja, </author> <title> "Neural Networks, Principal Components, and Subspaces," </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 61-68, </pages> <year> 1989. </year>
Reference-contexts: It has previously been pointed out [Rao and Ballard, 1996a] that the feedfor-ward version of the Kalman filter-based learning rule for U is equivalent to Oja's principal subspace algorithm <ref> [Oja, 1989] </ref>, which performs a form of principal component analysis.
Reference: [Olshausen and Field, 1996] <author> B.A. </author> <title> Olshausen and D.J. Field, "Emergence of simple-cell receptive field properties by learning a sparse code for natural images," </title> <journal> Nature, </journal> <volume> 381 </volume> <pages> 607-609, </pages> <year> 1996. </year>
Reference-contexts: Thus, the Kalman filter based method described herein generalizes principal component (or eigenspace [Murase and Nayar, 1995]) based approaches by (a) allowing non-orthogonal basis vectors, (b) seeking more than pairwise correlations in the input data <ref> [Olshausen and Field, 1996] </ref>, and (c) allowing the learning and recognition of time-varying imagery. In the first experiment, we assessed the performance of the robust Kalman filter on static grayscale images of two different pairs of 3D objects (Figure 2 (a)).
Reference: [Pentland, 1992] <author> A.P. Pentland, </author> <title> "Dynamic Vision," In G.A. </title> <editor> Carpenter and S. Grossberg, editors, </editor> <booktitle> Neural Networks for Vision and Image Processing, </booktitle> <pages> pages 133-159. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference: [Poggio et al., 1985] <author> T. Poggio, V. Torre, and C. Koch, </author> <title> "Computational Vision and Regularization Theory," </title> <journal> Nature, </journal> <volume> 317 </volume> <pages> 314-319, </pages> <year> 1985. </year>
Reference-contexts: The constant ff determines the decay rate of u and prevents the values of U from assuming inordinately large values. The decay term also acts as a "regularizer" <ref> [Poggio et al., 1985] </ref> that helps prevent overfitting of data, thereby increasing the potential for generalization [Rao and Ballard, 1996a].
Reference: [Rao and Ballard, 1995] <author> R.P.N. Rao and D.H. Ballard, </author> <title> "An Active Vision Architecture based on Iconic Representations," </title> <journal> Artificial Intelligence (Special Issue on Vision), </journal> <volume> 78 </volume> <pages> 461-505, </pages> <year> 1995. </year>
Reference: [Rao and Ballard, 1996a] <author> R.P.N. Rao and D.H. Ballard, </author> <title> "Dynamic Model of Visual Recognition Predicts Neural Response Properties in the Visual Cortex," </title> <note> Neural Computation 13 (in press). Also, Technical Report 96.2, </note> <institution> National Resource Laboratory for the Study of Brain and Behavior, Department of Computer Science, University of Rochester, </institution> <year> 1996. </year>
Reference-contexts: Note that the entire filter can be implemented in a recurrent neural network, each matrix in the filter corresponding to the synaptic weights of a set of neurons, each with a linear activation function <ref> [Rao and Ballard, 1996b; Rao and Ballard, 1996a] </ref>. where c is a threshold parameter that can be modulated according to the application at hand. <p> In complex dynamic environments, the formulation of such hand-coded models becomes increasingly difficult. An alternate approach is to learn an internal model of the input dynamics from observed data, as suggested in <ref> [Rao and Ballard, 1996a] </ref>. Let u and v denote the vectorized forms of the matrices U and V respectively. <p> Note that (I U r) = (I Ru) where R is the n fi nk matrix given by: R = 6 6 4 0 r T : : : 0 . . . . . . 3 7 7 (12) By minimizing an optimization function similar to J (see <ref> [Rao and Ballard, 1996a] </ref> for details), one can derive a Kalman filter "learning rule" for the generative matrix U : b u (t) = u (t) + N u (t)R (t) T (t) 1 (I (t) R (t)u (t)) ffN u (t)u (t) (13) where u (t) = b u (t1), <p> The constant ff determines the decay rate of u and prevents the values of U from assuming inordinately large values. The decay term also acts as a "regularizer" [Poggio et al., 1985] that helps prevent overfitting of data, thereby increasing the potential for generalization <ref> [Rao and Ballard, 1996a] </ref>. As in the case of U , an estimate of the prediction matrix V can be obtained via the following Kalman filter learning rule for v [Rao and Ballard, 1996a]: b v (t) = v (t) + N v (t) b R (t) T M (t) 1 <p> acts as a "regularizer" [Poggio et al., 1985] that helps prevent overfitting of data, thereby increasing the potential for generalization <ref> [Rao and Ballard, 1996a] </ref>. As in the case of U , an estimate of the prediction matrix V can be obtained via the following Kalman filter learning rule for v [Rao and Ballard, 1996a]: b v (t) = v (t) + N v (t) b R (t) T M (t) 1 [r (t + 1) r (t + 1)] fiN v (t)v (t) (14) where v (t) = b v (t 1), N v (t) = (N v (t 1) 1 <p> Previous approaches to appearance-based recognition have used principal component analysis [Black and Jepson, 1996; Murase and Nayar, 1995; Turk and Pentland, 1991] and various feature-based methods [Lades et al., 1993; Mel, 1996; Rao and Ballard, 1995; Viola, 1993]. It has previously been pointed out <ref> [Rao and Ballard, 1996a] </ref> that the feedfor-ward version of the Kalman filter-based learning rule for U is equivalent to Oja's principal subspace algorithm [Oja, 1989], which performs a form of principal component analysis. <p> The robust Kalman filter presented here can be readily extended to the hierarchical Kalman filter framework proposed in <ref> [Rao and Ballard, 1996a] </ref>. <p> It has previously been suggested that the visual cortex may function as a hierarchical Kalman filter, given its distinctive laminar input-output structure, its roughly hierarchical organization, and the reciprocity of connections between its many distinct areas <ref> [Rao and Ballard, 1996a; Rao and Ballard, 1996b] </ref>.
Reference: [Rao and Ballard, 1996b] <author> R.P.N. Rao and D.H. Ballard, </author> <title> "The visual cortex as a hierarchical predictor," </title> <type> Technical Report 96.4, </type> <institution> National Resource Laboratory for the Study of Brain and Behavior, Department of Computer Science, University of Rochester, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: 1 INTRODUCTION Three and a half decades after its discovery, the Kalman filter remains one of the most versatile algorithms in parameter estimation theory, having found applications in areas as diverse as economics [Athans, 1974], engineering [Cipra, 1993], and neuroscience <ref> [Rao and Ballard, 1996b] </ref>. The Kalman filter provides optimal estimates of the hidden state of any natural process that can be modeled as a linear dynamical system. <p> Note that the entire filter can be implemented in a recurrent neural network, each matrix in the filter corresponding to the synaptic weights of a set of neurons, each with a linear activation function <ref> [Rao and Ballard, 1996b; Rao and Ballard, 1996a] </ref>. where c is a threshold parameter that can be modulated according to the application at hand. <p> It has previously been suggested that the visual cortex may function as a hierarchical Kalman filter, given its distinctive laminar input-output structure, its roughly hierarchical organization, and the reciprocity of connections between its many distinct areas <ref> [Rao and Ballard, 1996a; Rao and Ballard, 1996b] </ref>.
Reference: [Turk and Pentland, 1991] <author> M. Turk and A. Pentland, </author> <title> "Eigenfaces for Recognition," </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference: [Viola, 1993] <author> P. Viola, </author> <title> "Feature-Based Recognition of Objects," </title> <booktitle> In AAAI Fall Symposium on Learning and Computer Vision, </booktitle> <year> 1993. </year> <month> 14 </month>
References-found: 28

