URL: ftp://ftp.cs.washington.edu/pub/ai/cbur-uai94.ps.Z
Refering-URL: http://www.cs.washington.edu/research/projects/ai/www/cbur.html
Root-URL: 
Email: weldg@cs.washington.edu  
Title: A Probabilistic Model of Action for Least-Commitment Planning with Information Gathering  
Author: Denise Draper Steve Hanks Daniel Weld fddraper, hanks, 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering, FR-35 University of Washington  
Abstract: ally assuming that the planning agent has perfect control over and information about the world. Relaxing these assumptions requires an extension to the action representation that allows reasoning both about the changes an action makes and the information it provides. This paper presents an action representation that extends the deterministic STRIPS model, allowing actions to have both causal and informational effects, both of which can be context dependent and noisy. We also demonstrate how a standard least-commitment planning algorithm can be extended to include informational actions and contingent execution.
Abstract-found: 1
Intro-found: 1
Reference: [ Dean et al., 1993 ] <author> Thomas Dean, Leslie Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proc. 11th Nat. Conf. on A.I., </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: The longer paper discusses this work in more detail. Recent work in planning under uncertainty, e.g. [ Koenig, 1992 ] and <ref> [ Dean et al., 1993 ] </ref> , adopts a model based on fully observable Markov processes, which amounts to assuming that the planner is automatically provided with perfect information about the world state every time it executes an action.
Reference: [ Draper et al., 1993 ] <author> D. Draper, S. Hanks, and D. Weld. </author> <title> Probabilistic planning with information gathering and contingent execution. </title> <type> Technical Report 93-12-04, </type> <institution> University of Washington, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: The representation also allows indirect evidence from sensors to be considered. The plan representation exploits the informational effects reject can actually be executed, but if one succeeds the other will be a no-op. In <ref> [ Draper et al., 1993 ] </ref> we make ship and reject incompatible by making it an error to execute ship or reject when PR is true. goal state success a PR PA NO initial state FL BL PR PA FL BL PR PA b a ship PR ba g PRPR FL
Reference: [ Draper et al., 1994 ] <author> D. Draper, S. Hanks, and D. Weld. </author> <title> Probabilistic planning with information gathering and contingent execution. </title> <booktitle> In Proc. 2nd Int. Conf. on A.I. Planning Systems, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: In this paper we will concentrate on the representation for actions and plans, referring the reader to <ref> [ Draper et al., 1994 ] </ref> for a more detailed description of the planning algorithm. 1.1 Example We begin by posing a simple example that demonstrates the need for reasoning about information, planning to gather information, and acting based on that information. <p> Here we describe its data structures and the algorithm it uses to produce a solution. A companion paper <ref> [ Draper et al., 1994 ] </ref> provides a more detailed description of the algorithm. goal success a PR ER PA initial FL BL PR ER PA,NO FL BL PR ER PA,NO b a Initial and goal steps. <p> if a causal link is currently part of the plan but some other step in the plan threatens the link, then eliminating the threat might increase the probability of the link's consumer proposition, and therefore might increase the success proba bility. c-buridan inherits all of buridan's refinement methods (discussed in <ref> [ Kushmerick et al., 1993, Draper et al., 1994 ] </ref> ). We demonstrate them using the example, then describe a new method of threat elimination, branching, which introduces contingencies into the plan. Example.
Reference: [ Etzioni et al., 1992 ] <author> O. Etzioni, S. Hanks, D. Weld, D. Draper, N. Lesh, and M. Williamson. </author> <title> An Approach to Planning with Incomplete Information. </title> <booktitle> In Proc. 3rd Int. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <month> Octo-ber </month> <year> 1992. </year> <note> Available via FTP from pub/ai/ at ftp.cs.washington.edu. </note>
Reference-contexts: a blemish from it)? We cannot use the action's effects to model information gathering: doing so would confuse the difference between the changes the action makes and the information it provides, obscuring the difference between a plan that makes P true and a plan that determines whether P is true <ref> [ Etzioni et al., 1992 ] </ref> . Instead we model the information produced by an action as a separate report provided to the agent when the action is executed. inspect b a g r=0.9r=0.1 has no material effects. <p> The plan-assessment phase treats the actions as probabilistic state transitions, computing a success probability. The action representation properly distinguishes between an action's causal and informational effects, allowing the planner to discriminate between plans that make a proposition true from those that determine whether it is true <ref> [ Etzioni et al., 1992 ] </ref> . The representation makes no arbitrary distinction between sensing actions and effecting actions, however: an action's ef fects can be both causal and information, and can be noisy in the changes it makes, the information it provides, both, or neither.
Reference: [ Goldman and Boddy, 1994 ] <author> Robert P. Goldman and Mark S. Boddy. </author> <title> Representing Uncertainty in Simple Planners. </title> <booktitle> In Proc. 4th Int. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <month> June </month> <year> 1994. </year> [ <title> Haddawy and Hanks, 1993 ] Peter Haddawy and Steve Hanks. Utility Models for Goal-Directed Decision-Theoretic Planners. </title> <type> Technical Report 93-06-04, </type> <institution> Univ. of Washington, Dept. of Computer Science and Engineering, </institution> <month> September </month> <year> 1993. </year> <note> Submitted to Artificial Intelligence. Available via FTP from pub/ai/ at ftp.cs.washington.edu. </note>
Reference-contexts: Also relevant are other symbolic methods for plan-generation under uncertainty [ Kushmer-ick et al., 1993 ] , [ Mansell, 1993 ] , <ref> [ Goldman and Boddy, 1994 ] </ref> , and deterministic conditional planners [ Peot and Smith, 1992 ] , [ Pryor and Collins, 1993 ] . The longer paper discusses this work in more detail.
Reference: [ Koenig, 1992 ] <author> S. Koenig. </author> <title> Optimal probabilistic and decision-theoretic planning using markovian decision theory. </title> <address> UCB/CSD 92/685, Berkeley, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: The longer paper discusses this work in more detail. Recent work in planning under uncertainty, e.g. <ref> [ Koenig, 1992 ] </ref> and [ Dean et al., 1993 ] , adopts a model based on fully observable Markov processes, which amounts to assuming that the planner is automatically provided with perfect information about the world state every time it executes an action.
Reference: [ Kushmerick et al., 1993 ] <author> N. Kushmerick, S. Hanks, and D. Weld. </author> <title> An Algorithm for Probabilistic Planning. </title> <type> Technical Report 93-06-03, </type> <institution> Univ. of Washing-ton, Dept. of Computer Science and Engineering, </institution> <year> 1993. </year> <note> To appear in Artificial Intelligence. Available via FTP from pub/ai/ at ftp.cs.washington.edu. </note>
Reference-contexts: The initial step codes the initial probability distribution, and the goal step has a single consequence with the goal state as its trigger. Figure 5 shows initial and goal actions for the example. Plans. Following buridan <ref> [ Kushmerick et al., 1993 ] </ref> , the planner manipulates a data structure called a plan, consisting of a set of steps, ordering constraints over the steps, and a set of causal links. <p> If it finds a sequence with success probability &gt; t , it returns that sequence, otherwise it returns failure. This simple version of plan assessment is often quite inefficient. <ref> [ Kushmerick et al., 1993 ] </ref> compares the performance of four different assessment algorithms, including the simple version described here. One of the most interesting assessment algorithms uses the plan's causal links to estimate the success probability without actually enumerating any totally ordered sequences or reasoning explicitly about states. Refinement. <p> if a causal link is currently part of the plan but some other step in the plan threatens the link, then eliminating the threat might increase the probability of the link's consumer proposition, and therefore might increase the success proba bility. c-buridan inherits all of buridan's refinement methods (discussed in <ref> [ Kushmerick et al., 1993, Draper et al., 1994 ] </ref> ). We demonstrate them using the example, then describe a new method of threat elimination, branching, which introduces contingencies into the plan. Example.
Reference: [ Kushmerick et al., 1994a ] <author> N. Kushmerick, S. Hanks, and D. Weld. </author> <title> An Algorithm for Probabilistic Least-Commitment Planning. </title> <booktitle> In Proc. 12th Nat. Conf. on A.I., </booktitle> <year> 1994. </year>
Reference-contexts: The probabilistic planner buridan <ref> [ Kushmerick et al., 1994a, Kushmerick et al., 1994b ] </ref> (which cannot create contingent plans), can build a plan with a success probability of at best 0.7: it assumes the widget will not be flawed, paints it, ships it, and notifies its supervisor.
Reference: [ Kushmerick et al., 1994b ] <author> N. Kushmerick, S. Hanks, and D. Weld. </author> <title> An Algorithm for Probabilistic Planning. </title> <journal> Artificial Intelligence, </journal> <note> 1994. To appear. Available via FTP from pub/ai/ at ftp.cs.washington.edu. </note>
Reference-contexts: The probabilistic planner buridan <ref> [ Kushmerick et al., 1994a, Kushmerick et al., 1994b ] </ref> (which cannot create contingent plans), can build a plan with a success probability of at best 0.7: it assumes the widget will not be flawed, paints it, ships it, and notifies its supervisor.
Reference: [ Mansell, 1993 ] <author> T. Mansell. </author> <title> A method for planning given uncertain and incomplete information. </title> <booktitle> In Proc. 9th Conf. on Uncertainty in Artifical Intelligence, </booktitle> <year> 1993. </year>
Reference-contexts: Related work can be found in the literature on decision making under uncertainty, which deals with evaluating contingency plans with information-gathering actions [ Winkler, 1972 ] , [ Math-eson, 1990 ] . Also relevant are other symbolic methods for plan-generation under uncertainty [ Kushmer-ick et al., 1993 ] , <ref> [ Mansell, 1993 ] </ref> , [ Goldman and Boddy, 1994 ] , and deterministic conditional planners [ Peot and Smith, 1992 ] , [ Pryor and Collins, 1993 ] . The longer paper discusses this work in more detail.
Reference: [ Matheson, 1990 ] <author> James E. Matheson. </author> <title> Using Influence Diagrams to Value Information and Control. </title> <editor> In R. M. Oliver and J. Q. Smith, editors, </editor> <title> Influence Diagrams, </title> <booktitle> Belief Nets and Decision Analysis, </booktitle> <pages> pages 25-48. </pages> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1990. </year>
Reference: [ Monahan, 1982 ] <author> G. E. Monahan. </author> <title> A survey of partially observable markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <year> 1982. </year>
Reference-contexts: Our model of action and information is equivalent in expressive power to a partially observable Markov de cision process (POMDP) <ref> [ Monahan, 1982 ] </ref> . The problem we are solving is different from the one commonly addressed in that literature, however. The POMDP problem is generally posed as finding a policy that maximizes some value function over some prespecified horizon.
Reference: [ Pearl, 1988 ] <author> J. Pearl. </author> <title> Probablistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: The information generated by executing inspect is summarized by the following conditional probabilities: P [bad j BL] = 0:9 P [ok j BL] = 0:1 fi fl fi fl which is a standard probabilistic representation of a noisy evidence source (see, e.g., <ref> [ Pearl, 1988, Chapter 2 ] </ref> ).
Reference: [ Peot and Smith, 1992 ] <author> M. Peot and D. Smith. </author> <title> Conditional Nonlinear Planning. </title> <booktitle> In Proc. 1st Int. Conf. on A.I. Planning Systems, </booktitle> <pages> pages 189-197, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Also relevant are other symbolic methods for plan-generation under uncertainty [ Kushmer-ick et al., 1993 ] , [ Mansell, 1993 ] , [ Goldman and Boddy, 1994 ] , and deterministic conditional planners <ref> [ Peot and Smith, 1992 ] </ref> , [ Pryor and Collins, 1993 ] . The longer paper discusses this work in more detail.
Reference: [ Pryor and Collins, 1993 ] <author> L. Pryor and G. Collins. CASSANDRA: </author> <title> Planning for contingencies. </title> <type> Technical Report 41, </type> <institution> Northwestern University, The Institute for the Learning Sciences, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Also relevant are other symbolic methods for plan-generation under uncertainty [ Kushmer-ick et al., 1993 ] , [ Mansell, 1993 ] , [ Goldman and Boddy, 1994 ] , and deterministic conditional planners [ Peot and Smith, 1992 ] , <ref> [ Pryor and Collins, 1993 ] </ref> . The longer paper discusses this work in more detail.
Reference: [ Winkler, 1972 ] <author> Robert L. Winkler. </author> <title> Introduction to Bayesian Inference and Decision. </title> <publisher> Holt, Rinehart, and Winston, </publisher> <year> 1972. </year>
Reference-contexts: Related work. Related work can be found in the literature on decision making under uncertainty, which deals with evaluating contingency plans with information-gathering actions <ref> [ Winkler, 1972 ] </ref> , [ Math-eson, 1990 ] .
References-found: 16

