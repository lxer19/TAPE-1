URL: http://www2.cs.cornell.edu/cs71x-s97/papers/JonesNielson.ps
Refering-URL: http://www2.cs.cornell.edu/cs71x-s97/cs719bib.htm
Root-URL: http://www.cs.brown.edu/
Title: Abstract Interpretation: a Semantics-Based Tool for Program Analysis  
Author: Neil D. Jones Flemming Nielson 
Note: Contents  
Date: June 30, 1994  
Address: Denmark  Denmark  
Affiliation: DIKU, University of Copenhagen,  Computer Science Department, Aarhus University,  
Abstract-found: 0
Intro-found: 1
Reference: <institution> References </institution>
Reference: [Abramsky, 1986] <author> S.Abramsky: </author> <title> Strictness analysis and polymorphic in-variance, Programs as Data Objects, 1-23, </title> <booktitle> Proceedings of the Workshop on Programs as Data Objects, Copen-hagen, volume 217 of Lecture Notes in Computer Science. </booktitle> <publisher> (Springer-Verlag, </publisher> <year> 1985). </year>
Reference-contexts: The problem of strictness analysis briefly mentioned in Section 1.1 has reeived much attention, key papers being [Burn, 1986], [Wadler, 1987], and [Hughes, 1990]. Polymorphism, which allows a function's type to be used in several different instantiations, creates new problems in abstract interpretation. Important papers include <ref> [Abramsky, 1986] </ref> and [Hughes, 1990]. Approximating functions by functions on abstract values A natural and common approach is to let an abstraction of a function be a function on abstract values. One example is Section 3's general framework using logical relations and based on the lambda calculus as a metalanguage.
Reference: [Abramsky, 1987] <editor> Abstract Interpretation of Declarative Languages, S.Abramsky and C.Hankin, eds., </editor> <publisher> (Ellis Horwood, </publisher> <year> 1987). </year>
Reference: [Abramsky, 1990] <author> S.Abramsky: </author> <title> Abstract interpretation, logical relations and Kan extensions, </title> <journal> Journal of logic and computation, 1(1),1990. </journal>
Reference-contexts: Interesting properties that can be extracted from a denotational semantics include strictness analysis and a (rather weak form of) termination [Burn, 1986], <ref> [Abramsky, 1990] </ref>. In the approach of [Abramsky, 1990] this is done by focusing on logical relations and then developing "best interpretations" with respect to these: the notion of safety leads to a strictness analysis, whereas the dual notion of liveness leads to a termination of analysis. <p> Interesting properties that can be extracted from a denotational semantics include strictness analysis and a (rather weak form of) termination [Burn, 1986], <ref> [Abramsky, 1990] </ref>. In the approach of [Abramsky, 1990] this is done by focusing on logical relations and then developing "best interpretations" with respect to these: the notion of safety leads to a strictness analysis, whereas the dual notion of liveness leads to a termination of analysis. <p> However, while a compositional strictness analysis is indeed useful, a compositional termination analysis is not because it has to "give up" for recursion; amending this would entail finding a well-founded order with respect to which the recursive calls do decrease and this is beyond the development of <ref> [Abramsky, 1990] </ref>. Operational Semantics It would seem obvious to try to extract these properties from an operational semantics [Plotkin, 1981], [Kahn, 1987]. However this is easier said than done, for several reasons.
Reference: [Aho, Sethi and Ullman, 1986] <editor> A.V.Aho, R.Sethi, J.D.Ullman: </editor> <booktitle> Compilers: Principles, Techniques, and Tools (Addison-Wesley, </booktitle> <year> 1986). </year>
Reference-contexts: Sintzoff used it for proving a number of well-formedness aspects of programs in an imperative language, and for verifying termination properties. These ideas were applied on a larger scale to highly optimizing compilers, often under the names program flow analysis or data-flow analysis [Hecht, 1977], <ref> [Aho, Sethi and Ullman, 1986] </ref>, [Kam, 1976]. <p> The aim is to show how large the spectrum of interesting program analyses is, and how much they differ from one another. Only a few of these have been given good semantic foundations, so the list could serve as a basis for future work. References include <ref> [Aho, Sethi and Ullman, 1986] </ref> and [Muchnick, 1981]. All concern analysing the subject program's behaviour at particular program points for optimization purposes. <p> Much more efficient algorithms were developed and some theoretical frameworks were developed to make the new methods more precise; [Hecht, 1977], [Kennedy, 1981] and <ref> [Aho, Sethi and Ullman, 1986] </ref> contain good overviews. None of the "classical" approaches to program analysis can, however, be said to be formally related to the semantics of the language whose programs were being analysed. Rather, they formalized and tightened up methods used in existing practice. <p> J e This is slightly coarser than what we did in Examples 3.2.11 and 3.2.14 in that v 1 is not taken into account but this is in agreement with common practice in data flow analysis <ref> [Aho, Sethi and Ullman, 1986] </ref>. To show the correctness of this let I be a domain interpretation along the lines of the lazy standard semantics, i.e.
Reference: [Ammann, 1994] <institution> Jurgen Ammann: Elemente der Projektions-Analyse fur Funktionen hoherer Ordnung, M.Sc.-thesis, University of Kiel, forthcoming. </institution>
Reference: [Bandelt, 1980] <author> H-J.Bandelt: </author> <title> The tensor product of continuous lattices, </title> <note> Mathematische Zeitschrift 172 (1980) 89-96. </note>
Reference-contexts: Proposition A tensor product always exists (and it is unique to within isomorphism). Proof: See <ref> [Bandelt, 1980] </ref> (or [Nielson, 1984] for an elementary proof).
Reference: [Barendregt et al, 1989] <editor> Term graph rewriting. In J. W. de Bakker, A. J. Nijman, P. C. Treleaven (eds.): </editor> <booktitle> PARLE|Parallel Architectures and Languages Europe (Lecture Notes in Computer Science, </booktitle> <volume> vol. 259), </volume> <publisher> (Springer-Verlag, </publisher> <year> 1989). </year>
Reference-contexts: For efficient implementation, compile-time analyses must take sharing into account, for example to minimize costs of memory allocation and gargage collection. (Careful proofs of equivalence between the two formulations for a term rewriting language can be seen in <ref> [Barendregt et al, 1989] </ref>.) In [Jones, 1981a] a simple imperative language with Lisp-like primitives is discussed. The semantics is described operationally, using finite graphs for the store.
Reference: [Bloss and Hudak, 1985] <author> P. Hudak, A. Bloss: </author> <title> The Aggregate Update Problem in Functional Programming, </title> <booktitle> Proc. 12th ACM Symposium on Principles of Programming Languages 300-314 (1985). </booktitle>
Reference-contexts: Copy propagation finds those variables whose values equal those of other variables. Destructive updating recognizes when a new binding of a value to a variable may safely overwrite the variable's previous value, e.g. to reduce the frequency of garbage collection in Lisp <ref> [Bloss and Hudak, 1985] </ref>, [Jensen, 1991], [Mycroft, 1981], [Sestoft, 1989]. Groundness analysis (in logic programming) finds out which of a Prolog program's variables can only be instantiated to ground terms [Debray, 1986], [Stndergaard, 1986]. <p> In a functional setting, Sestoft has traced sequences of variable definitions and uses (i.e. bindings and references) in order to see which variables can be "globalized", i.e. allowed to reside in global memory instead of the computation stack [Sestoft, 1989]. A similar idea is used in <ref> [Bloss and Hudak, 1985] </ref> for efficient implementation of lazy functional languages; they trace references to functions' formal parameters to see which ones can be computed using call by value, i.e. prior to function entry. 4.2.3 Store Properties The sequencing information just mentioned is quite clearly not present in a standard semantics.
Reference: [Bondorf, 1991] <author> A. Bondorf: </author> <title> Automatic autoprojection of higher order recursive equations. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 17 </volume> <pages> 3-34, </pages> <year> 1991. </year>
Reference-contexts: The techniques developed in [Sestoft, 1989,Shivers, 1991] have shown themselves useful for a variety of practical flow analysis problems involving higher order functions, for example in efficient implementation of lazy 100 evaluation [Sestoft and Argo, 1989] and partial evaluation <ref> [Bondorf, 1991] </ref>. Similar ideas are used in [Jones, 1993b] to provide an operationally oriented denotational minimal function graph semantics for higher order programs.
Reference: [Bruynooghe, 1991] <author> M. Bruynooghe: </author> <title> A Practical Framework for the Abstract Interpretation of Logic Programs, </title> <journal> Journal of Logic Programming, </journal> <volume> 10, </volume> <pages> 91-124, </pages> <year> 1991. </year>
Reference-contexts: Other uses include binding time analysis for o*ine partial evaluation, and deciding when certain optimizing transformations can be applied. One example is deforestation [Wadler, 1988]. 4.5.6 Methods of Analysis Analysis methods can roughly be divided into the pragmatically oriented, including <ref> [Bruynooghe, 1991] </ref>, [Mellish, 1987], and [Nilsson, 1991]; and the semantically oriented, including [Cortesi, 1991], [Debray, 1986], [Jones, 1987b], and [Marriott, 1993].
Reference: [Burn, 1986] <author> G.L.Burn, C.Hankin, S.Abramsky: </author> <title> Strictness Analysis for Higher-Order Functions, </title> <note> Science of Computer Programming 7 (1986) 249-278. REFERENCES 113 </note>
Reference-contexts: This implies program verification may also be based on the accumulating semantics, a theme developed further in [Cousot, 1977b] and several subsequent works. The ideas of [Cousot, 1977a] have had a considerable impact on later work in abstract interpretation, for example [Mycroft, 1981], [Muchnick, 1981], <ref> [Burn, 1986] </ref>, [Donzeau-Gouge, 1978], [Nielson, 1982], [Nielson, 1984], [Mycroft, 1987]). 2.2.1 Overview of the Cousot Approach The article [Cousot, 1977a] begins by presenting an operational semantics for a simple flow chart language. <p> Interesting properties that can be extracted from a denotational semantics include strictness analysis and a (rather weak form of) termination <ref> [Burn, 1986] </ref>, [Abramsky, 1990]. In the approach of [Abramsky, 1990] this is done by focusing on logical relations and then developing "best interpretations" with respect to these: the notion of safety leads to a strictness analysis, whereas the dual notion of liveness leads to a termination of analysis. <p> The problem of strictness analysis briefly mentioned in Section 1.1 has reeived much attention, key papers being <ref> [Burn, 1986] </ref>, [Wadler, 1987], and [Hughes, 1990]. Polymorphism, which allows a function's type to be used in several different instantiations, creates new problems in abstract interpretation. Important papers include [Abramsky, 1986] and [Hughes, 1990]. <p> One example is Section 3's general framework using logical relations and based on the lambda calculus as a metalanguage. The first higher-order strictness analysis was the elegant method of <ref> [Burn, 1986] </ref>. In this work the domains of a function being analyzed for strictness are modeled by abstract domains of exactly the same structure, but with f?; &gt;g in place of the basis domains. Strictness information is in essence obtained by computing with these abstracted higher-order functions. <p> It is also essential to know which values f can be called with during program execution, since a compiler can exploit knowledge about constant arguments to generate better target code. For another example, the higher-order strictness analysis in <ref> [Burn, 1986] </ref> has turned out to be unacceptably slow in practice, even for rather small programs. The reason is that the abstract interpretation involves comput 98 ing fixpoints of higher-order functions. <p> Earlier methods to approximate programs containing function calls were described in [Cousot, 1977c], [Sharir, 1981] and [Jones, 1982]. 4.3.2 Higher Order Functions Higher order functions as well as first order ones may be approximated using logical relations. Examples include the analyses of Section 3.2.3 and <ref> [Burn, 1986] </ref>, but such methods cannot closely describe the way functions are used during execution. <p> Termination is guaranteed because there are only finitely many possible closure descriptions. Closure analysis describes functions globally rather than locally, and so appears to be less precise in principle than approximating functional values by mathematical functions. This is substantiated by complexity results that show the analyses of <ref> [Burn, 1986] </ref> to have a worst-case lower bound of exponential time, whereas closure analysis works in time bounded by a low-degree polynomial.
Reference: [Cortesi, 1991] <author> A.Cortesi, G.File, W.Winsborough: </author> <title> Prop Revisited: Propositional Formulas as Abstract Domain for Ground-ness Analysis. </title> <booktitle> Proceedings 6th Annual Symposium on Logic in Computer Science, </booktitle> <month> 322-327 </month> <year> (1991). </year>
Reference-contexts: Simple groundness and sharing analyses are described in [Jones, 1987b]. A more elegant method using propositional formulas built from ^ and , was introduced in [Marriott, 1987] and compared with other methods in <ref> [Cortesi, 1991] </ref>. Safely avoiding the occur check Circularity analysis is a related and more subtle problem, the goal being to discover which unifications may safely be performed without doing the time-consuming "occur check". <p> One example is deforestation [Wadler, 1988]. 4.5.6 Methods of Analysis Analysis methods can roughly be divided into the pragmatically oriented, including [Bruynooghe, 1991], [Mellish, 1987], and [Nilsson, 1991]; and the semantically oriented, including <ref> [Cortesi, 1991] </ref>, [Debray, 1986], [Jones, 1987b], and [Marriott, 1993]. A natural analogue to the accumulating semantics seen earlier was used in [Jones, 1987b] and a number of later papers, and presumes given a sequence of clauses and a single query.
Reference: [Cousot, 1977a] <author> P.Cousot, R.Cousot: </author> <title> Abstract Interpretation: a Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints, </title> <booktitle> Proc. 4th ACM Symposium on Principles of Programming Languages (1977) 238-252. </booktitle>
Reference-contexts: The components of the following goal are precisely formulated: abstract interpretation = correctness + most precise analyses + implementable analyses 1 There is a terminological problem here: <ref> [Cousot, 1977a] </ref> used the term "static semantics", but this has other meanings, so several researchers have used the more descriptive "collecting semantics". <p> Rather, they formalized and tightened up methods used in existing practice. In particular none of them was able to include precise execution as a special case of abstract interpretation (albeit an uncomputable one). This was first done in <ref> [Cousot, 1977a] </ref>, the seminal paper relating abstract interpretation to program semantics. 17 2.2 Accumulating Semantics for Imperative Programs The approach of [Cousot, 1977a] is appealing because of its generality: it expresses a large number of special program analyses in a common framework. <p> In particular none of them was able to include precise execution as a special case of abstract interpretation (albeit an uncomputable one). This was first done in <ref> [Cousot, 1977a] </ref>, the seminal paper relating abstract interpretation to program semantics. 17 2.2 Accumulating Semantics for Imperative Programs The approach of [Cousot, 1977a] is appealing because of its generality: it expresses a large number of special program analyses in a common framework. <p> It is solidly based in semantics, and precise execution of the program is included as a special case. This implies program verification may also be based on the accumulating semantics, a theme developed further in [Cousot, 1977b] and several subsequent works. The ideas of <ref> [Cousot, 1977a] </ref> have had a considerable impact on later work in abstract interpretation, for example [Mycroft, 1981], [Muchnick, 1981], [Burn, 1986], [Donzeau-Gouge, 1978], [Nielson, 1982], [Nielson, 1984], [Mycroft, 1987]). 2.2.1 Overview of the Cousot Approach The article [Cousot, 1977a] begins by presenting an operational semantics for a simple flow chart language. <p> The ideas of <ref> [Cousot, 1977a] </ref> have had a considerable impact on later work in abstract interpretation, for example [Mycroft, 1981], [Muchnick, 1981], [Burn, 1986], [Donzeau-Gouge, 1978], [Nielson, 1982], [Nielson, 1984], [Mycroft, 1987]). 2.2.1 Overview of the Cousot Approach The article [Cousot, 1977a] begins by presenting an operational semantics for a simple flow chart language. It then develops the concept of what we call the accumulating semantics (the same as Cousots' static semantics and some others' collecting semantics). <p> This associates with each program point the set of all memory stores that can ever occur when program control reaches that point, as the program is run on data from a given initial data space. It was shown in <ref> [Cousot, 1977a] </ref> that a wide variety of flow analyses (but not all!) may be realized by finding finitely computable approximations to the accumulating semantics. The (sticky) accumulating semantics maps program points to sets of program stores. <p> S 0 acc C = acc B " fn j n 2 f0; 2; 4; . . .gg acc E = acc B " fn jn 2 f1; 3; 5; . . .gg 19 The equation set can be derived mechanically from the given program's syntax, e.g. as seen in <ref> [Cousot, 1977a] </ref> or [Nielson, 1982]. 2.2.3 Abstract Interpretation of the Example Program The abstraction function ff : -(Store) ! Abs below may be used to abstract a set of stores, where Abs = f?, even, odd, &gt;g: ff (S) = &gt; &gt; &lt; ? if S = fg, else even if <p> By monotonicity of u, f nffi2 etc., these values can only grow or remain unchanged, so the iterations terminate provided the approximation lattice has no ascending chains of infinite height, as is the case here. <ref> [Cousot, 1977a] </ref> describes ways to achieve termination even when infinite chains exist, by inserting so-called widening operators in the data-flow equations at each junction point of a loop. To explain the basic idea consider the problem of finding the fixed point of a continuous function f. <p> Note: any lattice of finite height is complete. In the following we sometimes write a w a 0 in place of a 0 v a. 2.3.2 Desirable Properties of the Abstraction Function Intuitively "even" represents the set of all even numbers. This viewpoint is made explicit in <ref> [Cousot, 1977a] </ref> by relating complete lattices Conc and Abs to each other by a pair ff, fl of abstraction and concretization functions with types ff : Conc ! Abs fl : Abs ! Conc In the even-odd example above the lattice of concrete values is Conc = -(Store), and the natural <p> An approach used in <ref> [Cousot, 1977a] </ref>, [Cousot, 1977b] is to describe acc p and its approximations abs p by predicate calculus formulas.
Reference: [Cousot, 1977b] <author> P.Cousot, R.Cousot: </author> <title> Automatic Synthesis of Optimal Invariant Assertions: </title> <booktitle> Mathematical Foundations, Proc. ACM Symposium on Artificial Intelligence and Programming Languages (1977) 1-12. </booktitle>
Reference-contexts: They can be used for extraction of more general program properties [Wegbreit, 1975] and have been used for many applications including: generating assertions for program verifiers <ref> [Cousot, 1977b] </ref>, program validation [Fosdick, 1976] and [Foster, 1987], testing applicability of program transformations [Nielson, 1985a], compiler generation and partial evaluation [Jones, 1989], [Nielson, 1988b], estimating program running times [Rosendahl, 1989], and efficiently paral-lelizing sequential programs [Masdupuy, 1991,Mercouroff, 1991]. <p> It is solidly based in semantics, and precise execution of the program is included as a special case. This implies program verification may also be based on the accumulating semantics, a theme developed further in <ref> [Cousot, 1977b] </ref> and several subsequent works. <p> An approach used in [Cousot, 1977a], <ref> [Cousot, 1977b] </ref> is to describe acc p and its approximations abs p by predicate calculus formulas. <p> Program verification amounts to proving that each acc p logically implies a user-supplied program assertion for point p. Note however that this abstract interpretation framework says nothing at all about program termination. This approach is developed further in <ref> [Cousot, 1977b] </ref> and their subsequent works.
Reference: [Cousot, 1977c] <author> P.Cousot, R.Cousot: </author> <title> Static Determination of Dynamic Properties of Recursive Procedures, </title> <booktitle> IFIP Working Conference on Programming Concepts (North-Holland, </booktitle> <year> 1977) </year> <month> 237-277. </month>
Reference-contexts: In [Jones, 1986] it is shown how the "constant propagation" analysis may be done by approximating this semantics, and the idea of proving correctness by semihomomorphic mappings between various interpretations of a denotational semantics is explained. Earlier methods to approximate programs containing function calls were described in <ref> [Cousot, 1977c] </ref>, [Sharir, 1981] and [Jones, 1982]. 4.3.2 Higher Order Functions Higher order functions as well as first order ones may be approximated using logical relations. Examples include the analyses of Section 3.2.3 and [Burn, 1986], but such methods cannot closely describe the way functions are used during execution.
Reference: [Cousot, 1978] <author> P.Cousot, N.Halbwachs: </author> <title> Automatic Discovery of Linear Restraints Among Variables of a Program, </title> <booktitle> Proc. 5th ACM Symposium on Principles of Programming Languages (1978) 84-97. </booktitle>
Reference-contexts: lists and trees 4.4.1 Functions as values Some approaches were described above (approximation by functions on abstract values, and closure analysis), and several more have been studied. 4.4.2 Relations on n-tuples of numbers In this special case there is a well-developed theory: linear algebra, in particular systems of linear inequalities. <ref> [Cousot, 1978] </ref> describes a way to discover linear relationships among the variables in a Pascal-like program. Such relations may be systematically discovered and exploited for, for example, efficient compilation of array operations.
Reference: [Cousot, 1979] <author> P.Cousot, R.Cousot: </author> <title> Systematic Design of Program Analysis Frameworks, </title> <booktitle> Proc. 6th ACM Symposium on Principles of Programming Languages (1979) 269-282. </booktitle>
Reference-contexts: This motivates the development in the present subsection where we show that under certain circumstances there is a most precise analysis over a given selection of properties. Following <ref> [Cousot, 1979] </ref> we shall term this the induced analysis. As we shall see in the next subsection there may well be pragmatic reasons for adopting an analysis that is less precise than the induced analysis.
Reference: [Curien, 1986] <author> P.-L.Curien: </author> <title> Categorical Combinators, Sequential Algorithms and Functional Programming (Wiley, </title> <year> 1986). </year>
Reference: [Darlington, 1977] <author> R.M. Burstall and J. Darlington: </author> <title> A transformation system for developing recursive programs. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: In contrast, traditional program verification requires one to devise a new set of statement invariants for every new program. Abstract interpretation's major application is to determine the applicability or value of optimization and thus has similar goals to program transformation <ref> [Darlington, 1977] </ref>. However most program transformation as currently practiced still requires considerable human interaction and is so significantly less automatic than abstract interpretation.
Reference: [Debray, 1986] <author> S. Debray: </author> <title> Dataflow Analysis of Logic Programs, </title> <type> Report, </type> <institution> SUNY at Stony Brook, </institution> <address> New York (1986). </address>
Reference-contexts: Groundness analysis (in logic programming) finds out which of a Prolog program's variables can only be instantiated to ground terms <ref> [Debray, 1986] </ref>, [Stndergaard, 1986]. Sharing analysis (in logic programming) finds out which variable pairs can be instantiated to terms containing shared subterms [Debray, 1986], [Mellish, 1987], [Stndergaard, 1986]. <p> Groundness analysis (in logic programming) finds out which of a Prolog program's variables can only be instantiated to ground terms <ref> [Debray, 1986] </ref>, [Stndergaard, 1986]. Sharing analysis (in logic programming) finds out which variable pairs can be instantiated to terms containing shared subterms [Debray, 1986], [Mellish, 1987], [Stndergaard, 1986]. Circularity analysis (in logic programming) finds out which unifications in Prolog can be safely performed without the time-consuming "oc cur check" [Plaisted, 1984], [Stndergaard, 1986]. <p> One example is deforestation [Wadler, 1988]. 4.5.6 Methods of Analysis Analysis methods can roughly be divided into the pragmatically oriented, including [Bruynooghe, 1991], [Mellish, 1987], and [Nilsson, 1991]; and the semantically oriented, including [Cortesi, 1991], <ref> [Debray, 1986] </ref>, [Jones, 1987b], and [Marriott, 1993]. A natural analogue to the accumulating semantics seen earlier was used in [Jones, 1987b] and a number of later papers, and presumes given a sequence of clauses and a single query.
Reference: [Deutsch, 1990] <author> A. Deutsch: </author> <title> On Determining Lifetime and Aliasing of Dynamically Allocated Data in Higher Order Functional Specifications, </title> <booktitle> in Proceedings of the Seventeenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> 157-169, </pages> <publisher> (ACM Press, </publisher> <year> 1990). </year>
Reference-contexts: Essentially similar techniques, although formalized in terms of tables rather than grammars, have been applied to the lambda calculus [Jones, 1981b], interprocedural imperative program analysis [Jones, 1982], a language suitable as an intermediate language for ML <ref> [Deutsch, 1990] </ref>, and the Prolog Language [Heintze, 1992]. 4.5 Abstract Interpretation of Logic Programs Given the framework already developed, we concentrate on the factors that make logic program analysis different from those seen earlier.
Reference: [Dijkstra, 1976] <author> E.W. Dijkstra: </author> <title> A Discipline of Programming (Prentice-Hall, 1976). </title> <type> 114 REFERENCES </type>
Reference-contexts: From this viewpoint, the function next p;q : -(Store) ! -(Store) is clearly the forward predicate transformer <ref> [Dijkstra, 1976] </ref> associated with transitions from p to q. Further, acc p is clearly the strongest postcondition holding at program point p over all computations on input data satisfying the program's input precondition. <p> Many analysis problems can be solved by either a forwards or a backwards analysis. There can, however, be significant differences in efficiency. Backwards analysis of functional programs The backwards accumulating semantics is straightforward for imperative programs, partly because of its close connections with the well studied weakest preconditions <ref> [Dijkstra, 1976] </ref>, and because the state transition function is monadic. It is semantically less well understood, however, for functional programs, where recent works include [Hughes, 1987], [Dybjer, 1987], [Wadler, 1987], and [Nielson, 1989].
Reference: [Donzeau-Gouge, 1978] <author> V. </author> <type> Donzeau-Gouge: </type> <institution> Utilisation de la Semantique denotationelle pour l'etude d'interpretations nonstandard, </institution> <note> INRIA rapport 273 (1978). </note>
Reference-contexts: This implies program verification may also be based on the accumulating semantics, a theme developed further in [Cousot, 1977b] and several subsequent works. The ideas of [Cousot, 1977a] have had a considerable impact on later work in abstract interpretation, for example [Mycroft, 1981], [Muchnick, 1981], [Burn, 1986], <ref> [Donzeau-Gouge, 1978] </ref>, [Nielson, 1982], [Nielson, 1984], [Mycroft, 1987]). 2.2.1 Overview of the Cousot Approach The article [Cousot, 1977a] begins by presenting an operational semantics for a simple flow chart language. <p> The last step is to describe briefly a way to generalize this approach to denotational definitions of other languages; this gives a bridge to the development of section 3. Earlier papers using this approach include <ref> [Donzeau-Gouge, 1978] </ref>, [Nielson, 1982], [Jones, 1986]. Denotational semantics has three basic principles: 1. Every syntactic phrase in a program has a meaning, or denotation. 2. Denotations are well-defined mathematical objects (often higher-order functions). 3. <p> This gives in a sense all the raw material that can be used to extract "history-dependent" information, and thus a basis for a very wide range of program analyses. This approach was used in P. Cousot's thesis work, and has since been seen in <ref> [Donzeau-Gouge, 1978] </ref> and [Nielson, 1982]. In a functional setting, Sestoft has traced sequences of variable definitions and uses (i.e. bindings and references) in order to see which variables can be "globalized", i.e. allowed to reside in global memory instead of the computation stack [Sestoft, 1989].
Reference: [Dybjer, 1987] <author> P.Dybjer: </author> <title> Inverse Image Analysis, </title> <booktitle> Proc. ICALP 87, Lecture Notes in Computer Science 267 (Springer-Verlag, </booktitle> <year> 1987) </year> <month> 21-30. </month>
Reference-contexts: It is semantically less well understood, however, for functional programs, where recent works include [Hughes, 1987], <ref> [Dybjer, 1987] </ref>, [Wadler, 1987], and [Nielson, 1989].
Reference: [Fosdick, 1976] <author> L.D. Fosdick, L. Osterweil: </author> <title> Data Flow Analysis in Software Reliability. </title> <journal> Computing Surveys, </journal> <volume> 8, </volume> <month> 305-330 </month> <year> (1976). </year>
Reference-contexts: They can be used for extraction of more general program properties [Wegbreit, 1975] and have been used for many applications including: generating assertions for program verifiers [Cousot, 1977b], program validation <ref> [Fosdick, 1976] </ref> and [Foster, 1987], testing applicability of program transformations [Nielson, 1985a], compiler generation and partial evaluation [Jones, 1989], [Nielson, 1988b], estimating program running times [Rosendahl, 1989], and efficiently paral-lelizing sequential programs [Masdupuy, 1991,Mercouroff, 1991].
Reference: [Foster, 1987] <author> M. Foster: </author> <title> Software Validation using Abstract Interpretation, in Abstract Interpretation of Declarative Languages, edited by S. </title> <editor> Abramsky and C. Hankin, </editor> <address> 32-44, </address> <publisher> Ellis Horwood, </publisher> <address> Chichester, England, </address> <year> 1987. </year>
Reference-contexts: They can be used for extraction of more general program properties [Wegbreit, 1975] and have been used for many applications including: generating assertions for program verifiers [Cousot, 1977b], program validation [Fosdick, 1976] and <ref> [Foster, 1987] </ref>, testing applicability of program transformations [Nielson, 1985a], compiler generation and partial evaluation [Jones, 1989], [Nielson, 1988b], estimating program running times [Rosendahl, 1989], and efficiently paral-lelizing sequential programs [Masdupuy, 1991,Mercouroff, 1991].
Reference: [Gordon, 1979] <author> M.J.C.Gordon: </author> <title> The Denotational Description of Programming Languages: An Introduction (Springer-Verlag, </title> <year> 1979). </year>
Reference: [Granger, 1991] <author> P. Granger: </author> <title> Static Analysis of Linear Congruence Equalities among Variables of a Program, </title> <booktitle> in Proceedings of the Fourth International Joint Conference on the Theory and Practice of Software Development (TAPSOFT '91), Lecture Notes in Computer Science 493, </booktitle> <pages> 169-192, </pages> <publisher> (Springer-Verlag, </publisher> <year> 1991). </year>
Reference-contexts: Such relations may be systematically discovered and exploited for, for example, efficient compilation of array operations. Related work, involving the inference of systems of modulus equations, has been applied to pipelining and other techniques for utilizing parallelism <ref> [Granger, 1991, Mercouroff, 1991] </ref>.
Reference: [Hecht, 1977] <author> M.S. Hecht: </author> <title> Flow Analysis of Computer Programs, </title> <publisher> (El-sevier North-Holland 1977). </publisher>
Reference-contexts: Sintzoff used it for proving a number of well-formedness aspects of programs in an imperative language, and for verifying termination properties. These ideas were applied on a larger scale to highly optimizing compilers, often under the names program flow analysis or data-flow analysis <ref> [Hecht, 1977] </ref>, [Aho, Sethi and Ullman, 1986], [Kam, 1976]. <p> Much more efficient algorithms were developed and some theoretical frameworks were developed to make the new methods more precise; <ref> [Hecht, 1977] </ref>, [Kennedy, 1981] and [Aho, Sethi and Ullman, 1986] contain good overviews. None of the "classical" approaches to program analysis can, however, be said to be formally related to the semantics of the language whose programs were being analysed.
Reference: [Heintze, 1992] <author> N.Heintze: </author> <title> Set Based Program Analysis, </title> <type> Carnegie-Mellon Ph.D. thesis, </type> <year> 1992. </year>
Reference-contexts: Essentially similar techniques, although formalized in terms of tables rather than grammars, have been applied to the lambda calculus [Jones, 1981b], interprocedural imperative program analysis [Jones, 1982], a language suitable as an intermediate language for ML [Deutsch, 1990], and the Prolog Language <ref> [Heintze, 1992] </ref>. 4.5 Abstract Interpretation of Logic Programs Given the framework already developed, we concentrate on the factors that make logic program analysis different from those seen earlier. <p> Terms containing free variables have the same problem: binding one variable will change the values of all terms containing it. Finally, unification binds variables to structured terms, so approximations that do not disregard structure entirely have some work to do to obtain finite descriptions. A recent example is <ref> [Heintze, 1992] </ref>. [Marriott, 1993] is interesting in two respects: it uses the metalanguage approach described in this work, in Section 3; and it uses sets of constraints instead of substitutions, reducing some of the technical problems just mentioned. 109 5 Glossary abstraction function: Usually a function from values or sets of
Reference: [Hudak, 1991] <author> P.Hudak, J.Young: </author> <title> Collecting interpretation of expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 269-290, </pages> <month> April </month> <year> 1991. </year>
Reference: [Hughes, 1985] <author> J. Hughes: </author> <title> Strictness Detection in Non-flat Domains, </title> <booktitle> Proceedings of the Workshop on Programs as Data Objects, Copenhagen, volume 217 of Lecture Notes in Computer Science. </booktitle> <publisher> (Springer-Verlag, </publisher> <year> 1985). </year>
Reference-contexts: Both forwards and backwards algorithms exist, but backwards methods seem to be faster. Early work on backwards methods includes <ref> [Hughes, 1985] </ref> and [Hughes, 1987], later simplified for the case of domain projections in [Wadler, 1987] and [Hughes, 1990]. Storage reclamation. Methods are develped in [Jensen, 1991] to recognize when a memory cell has been used for the last time, so it may safely be freed for later use.
Reference: [Hughes, 1987] <author> John Hughes: </author> <title> Analyzing Strictness by Abstract Interpretation of Continuations, in Abstract Interpretation of Declarative Languages, edited by S. </title> <editor> Abramsky and C. Hankin, </editor> <address> 63-102, </address> <publisher> Ellis Horwood, </publisher> <address> Chichester, England, </address> <year> 1987. </year> <note> REFERENCES 115 </note>
Reference-contexts: It is semantically less well understood, however, for functional programs, where recent works include <ref> [Hughes, 1987] </ref>, [Dybjer, 1987], [Wadler, 1987], and [Nielson, 1989]. Natural connections between backwards analyses and continuation semantics are seen in [Nielson, 1982] and [Hughes, 1987]. 36 2.6 Important Points from Earlier Sections In the above we have employed a rather trivial programming language so as to motivate and illustrate one way <p> It is semantically less well understood, however, for functional programs, where recent works include <ref> [Hughes, 1987] </ref>, [Dybjer, 1987], [Wadler, 1987], and [Nielson, 1989]. Natural connections between backwards analyses and continuation semantics are seen in [Nielson, 1982] and [Hughes, 1987]. 36 2.6 Important Points from Earlier Sections In the above we have employed a rather trivial programming language so as to motivate and illustrate one way to approximate real computations by computations over a domain of abstract values: Cousot's accumulating semantics. <p> Both forwards and backwards algorithms exist, but backwards methods seem to be faster. Early work on backwards methods includes [Hughes, 1985] and <ref> [Hughes, 1987] </ref>, later simplified for the case of domain projections in [Wadler, 1987] and [Hughes, 1990]. Storage reclamation. Methods are develped in [Jensen, 1991] to recognize when a memory cell has been used for the last time, so it may safely be freed for later use. <p> What is a context, semantically? The intuition "the rest of the computation" can be expressed by the current continuation, since since a continuation is a function taking the current expression value into the program's computational future. This approach was taken in <ref> [Hughes, 1987] </ref>, but is technically rather complex since it entails that a context is an abstraction of a set of continuations|tricky to handle since continuations are higher-order functions.
Reference: [Hughes, 1988] <author> John Hughes: </author> <title> Backward Analysis of Functional Programs, Partial evaluation and mixed computation, </title> <address> 187-208, </address> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference: [Hughes, 1990] <author> R.J.M. Hughes, J. Launchbury: </author> <title> Projections for Polymorphic Strictness Analysis, </title> <note> Mathematical Structures in Computer Science (1990). </note>
Reference-contexts: The problem of strictness analysis briefly mentioned in Section 1.1 has reeived much attention, key papers being [Burn, 1986], [Wadler, 1987], and <ref> [Hughes, 1990] </ref>. Polymorphism, which allows a function's type to be used in several different instantiations, creates new problems in abstract interpretation. Important papers include [Abramsky, 1986] and [Hughes, 1990]. <p> The problem of strictness analysis briefly mentioned in Section 1.1 has reeived much attention, key papers being [Burn, 1986], [Wadler, 1987], and <ref> [Hughes, 1990] </ref>. Polymorphism, which allows a function's type to be used in several different instantiations, creates new problems in abstract interpretation. Important papers include [Abramsky, 1986] and [Hughes, 1990]. Approximating functions by functions on abstract values A natural and common approach is to let an abstraction of a function be a function on abstract values. One example is Section 3's general framework using logical relations and based on the lambda calculus as a metalanguage. <p> Both forwards and backwards algorithms exist, but backwards methods seem to be faster. Early work on backwards methods includes [Hughes, 1985] and [Hughes, 1987], later simplified for the case of domain projections in [Wadler, 1987] and <ref> [Hughes, 1990] </ref>. Storage reclamation. Methods are develped in [Jensen, 1991] to recognize when a memory cell has been used for the last time, so it may safely be freed for later use. An application is substantially to reduce the number 101 of garbage collections. <p> This approach was taken in [Hughes, 1987], but is technically rather complex since it entails that a context is an abstraction of a set of continuations|tricky to handle since continuations are higher-order functions. In the later [Wadler, 1987] and <ref> [Hughes, 1990] </ref>, the concept of context is restricted to properties given by domain projections, typically specifing which parts of a value might later be used.
Reference: [Jensen, 1991] <author> Thomas P. Jensen:. </author> <title> Strictness analysis in logical form. </title> <booktitle> In Proceedings of the 1991 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: He coined the term pseudo-evaluation for what was later described as "a process which combines the operators and operands of the source text in the manner in which an actual evaluation would have to do it, but which operates on descriptions of the operands, not on their values" <ref> [Jensen, 1991] </ref>. The same basic idea is found in [Reynolds, 1969] and [Sintzoff, 1972]. Sintzoff used it for proving a number of well-formedness aspects of programs in an imperative language, and for verifying termination properties. <p> Copy propagation finds those variables whose values equal those of other variables. Destructive updating recognizes when a new binding of a value to a variable may safely overwrite the variable's previous value, e.g. to reduce the frequency of garbage collection in Lisp [Bloss and Hudak, 1985], <ref> [Jensen, 1991] </ref>, [Mycroft, 1981], [Sestoft, 1989]. Groundness analysis (in logic programming) finds out which of a Prolog program's variables can only be instantiated to ground terms [Debray, 1986], [Stndergaard, 1986]. <p> Both forwards and backwards algorithms exist, but backwards methods seem to be faster. Early work on backwards methods includes [Hughes, 1985] and [Hughes, 1987], later simplified for the case of domain projections in [Wadler, 1987] and [Hughes, 1990]. Storage reclamation. Methods are develped in <ref> [Jensen, 1991] </ref> to recognize when a memory cell has been used for the last time, so it may safely be freed for later use. An application is substantially to reduce the number 101 of garbage collections. Partial evaluation automatially transforms general programs into versions specialized to partially known inputs.
Reference: [Jensen, 1991] <author> Thomas P. Jensen, T.. Mogensen: </author> <title> A Backwards Analysis for Compile Time Garbage Collection. </title> <booktitle> In Proceedings of the European Symposium on Programming, </booktitle> <publisher> LNCS 432 (Springer-Verlag 1991). </publisher>
Reference-contexts: He coined the term pseudo-evaluation for what was later described as "a process which combines the operators and operands of the source text in the manner in which an actual evaluation would have to do it, but which operates on descriptions of the operands, not on their values" <ref> [Jensen, 1991] </ref>. The same basic idea is found in [Reynolds, 1969] and [Sintzoff, 1972]. Sintzoff used it for proving a number of well-formedness aspects of programs in an imperative language, and for verifying termination properties. <p> Copy propagation finds those variables whose values equal those of other variables. Destructive updating recognizes when a new binding of a value to a variable may safely overwrite the variable's previous value, e.g. to reduce the frequency of garbage collection in Lisp [Bloss and Hudak, 1985], <ref> [Jensen, 1991] </ref>, [Mycroft, 1981], [Sestoft, 1989]. Groundness analysis (in logic programming) finds out which of a Prolog program's variables can only be instantiated to ground terms [Debray, 1986], [Stndergaard, 1986]. <p> Both forwards and backwards algorithms exist, but backwards methods seem to be faster. Early work on backwards methods includes [Hughes, 1985] and [Hughes, 1987], later simplified for the case of domain projections in [Wadler, 1987] and [Hughes, 1990]. Storage reclamation. Methods are develped in <ref> [Jensen, 1991] </ref> to recognize when a memory cell has been used for the last time, so it may safely be freed for later use. An application is substantially to reduce the number 101 of garbage collections. Partial evaluation automatially transforms general programs into versions specialized to partially known inputs.
Reference: [Jones, 1986] <author> N.D.Jones, A.Mycroft: </author> <title> Dataflow of Applicative Programs Using Minimal Function Graphs, </title> <booktitle> Proc.13th ACM Symposium on Principles of Programming Languages (1986) 296-306. </booktitle>
Reference-contexts: The last step is to describe briefly a way to generalize this approach to denotational definitions of other languages; this gives a bridge to the development of section 3. Earlier papers using this approach include [Donzeau-Gouge, 1978], [Nielson, 1982], <ref> [Jones, 1986] </ref>. Denotational semantics has three basic principles: 1. Every syntactic phrase in a program has a meaning, or denotation. 2. Denotations are well-defined mathematical objects (often higher-order functions). 3. The meaning of a compound syntactic phrase is a mathematical com bination of the meanings of its immediate subphrases. <p> In this approach, concretization is not mentioned at all, nor does it seem to be necessary. Example works using this approach are [Mycroft, 1986], <ref> [Jones, 1986] </ref> and [Nielson, 1984]. 4.1.3 A Method Based on a Metalanguage The previous method applies only to one language definition at a time. Yet more generality can be obtained by abstractly interpreting the meta-language used to write the denotational semantics (typically the lambda calculus). <p> a combinatorial explosion in the size of the abstract domains involved, requiring subtle techniques to be able to compute the desired strictness information and avoid having to traverse the entire abstract value space. 4.3.1 First Order Minimal Function Graphs The minimal function graph of a program function was defined in <ref> [Jones, 1986] </ref> to be the smallest set of pairs (argument, function value) suffcicient to carry out program execution on given input data. <p> In <ref> [Jones, 1986] </ref> it is shown how the "constant propagation" analysis may be done by approximating this semantics, and the idea of proving correctness by semihomomorphic mappings between various interpretations of a denotational semantics is explained.
Reference: [Jones, 1981a] <editor> S.S. Muchnick, N.D. Jones: </editor> <title> Flow Analysis and Optimization of Lisp-like Structures, in Program Flow Analysis, </title> <editor> eds. S.S. Muchnick, N.D. </editor> <publisher> Jones (Prentice-Hall 1981). </publisher>
Reference-contexts: For efficient implementation, compile-time analyses must take sharing into account, for example to minimize costs of memory allocation and gargage collection. (Careful proofs of equivalence between the two formulations for a term rewriting language can be seen in [Barendregt et al, 1989].) In <ref> [Jones, 1981a] </ref> a simple imperative language with Lisp-like primitives is discussed. The semantics is described operationally, using finite graphs for the store. <p> The following diagram describes a store with X = a :: (b :: c), Y = b :: c and Z = (b :: c) :: c, where Y is shared by X and Z. X Z b c @ @ This instrumented semantics is modelled in <ref> [Jones, 1981a] </ref> by approximating such stores by "k-limited graphs", where k is a distance parameter. The idea is that the graph structure is modelled exactly for nodes of distance k or less from a variable.
Reference: [Jones, 1981b] <author> N.D. Jones: </author> <title> Flow Analysis of Lambda Expressions, </title> <booktitle> ICALP 1981, Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag (1981). </publisher>
Reference-contexts: An early step in this direction was the rather complex <ref> [Jones, 1981b] </ref>, and more recent and applications-motivated papers include [Sestoft, 1989,Shivers, 1991]. Closure analysis This method from can be described as an operationally oriented semantics-based method. <p> References The papers [Reynolds, 1969] and [Jones, 1981a,Jones, 1987a] contain methods to construct, given a program involving structured values, a regular tree grammar describing it. Essentially similar techniques, although formalized in terms of tables rather than grammars, have been applied to the lambda calculus <ref> [Jones, 1981b] </ref>, interprocedural imperative program analysis [Jones, 1982], a language suitable as an intermediate language for ML [Deutsch, 1990], and the Prolog Language [Heintze, 1992]. 4.5 Abstract Interpretation of Logic Programs Given the framework already developed, we concentrate on the factors that make logic program analysis different from those seen earlier.
Reference: [Jones, 1982] <editor> N.D. Jones, S.S. Muchnick: </editor> <booktitle> A Flexible Approach to In-terprocedural Data Flow Analysis and Programs with Recursive Data Structures,in Proceedings of the 6th ACM Symposium on Principles of Programming Languages, </booktitle> <month> 66-74 </month> <year> (1982). </year>
Reference-contexts: This allows program analysis methods to be proven "safe", i.e. to be correct approximations to actual program be-haviour. A limitation is that the Cousot approach only applies to flow chart programs, and has been difficult to extend to, for example, programs with procedures (examples include <ref> [Jones, 1982] </ref> and [Sharir, 1981]). <p> Earlier methods to approximate programs containing function calls were described in [Cousot, 1977c], [Sharir, 1981] and <ref> [Jones, 1982] </ref>. 4.3.2 Higher Order Functions Higher order functions as well as first order ones may be approximated using logical relations. Examples include the analyses of Section 3.2.3 and [Burn, 1986], but such methods cannot closely describe the way functions are used during execution. <p> Essentially similar techniques, although formalized in terms of tables rather than grammars, have been applied to the lambda calculus [Jones, 1981b], interprocedural imperative program analysis <ref> [Jones, 1982] </ref>, a language suitable as an intermediate language for ML [Deutsch, 1990], and the Prolog Language [Heintze, 1992]. 4.5 Abstract Interpretation of Logic Programs Given the framework already developed, we concentrate on the factors that make logic program analysis different from those seen earlier.
Reference: [Jones, 1987a] <author> Neil D. Jones: </author> <title> Flow Analysis of Lazy Higher-Order Functional Programs, in Abstract Interpretation of Declarative Languages, edited by S. </title> <editor> Abramsky and C. Hankin, </editor> <address> 103-122, </address> <publisher> Ellis Horwood, </publisher> <address> Chichester, England, </address> <year> 1987. </year>
Reference-contexts: Conceptually, call "sequence (nil)" generates the infinite list [[],[1],[1,1],[1,1,1],. . .]. The possible results of the program are all of its finite prefixes: Output = f [], [[],[1]], [[],[1],[1,1]], [[],[1],[1,1],[1,1,1]],. . .g The method of <ref> [Jones, 1987a] </ref> constructs form this program a tree grammar G containing (after some simplification) the following productions.
Reference: [Jones, 1987b] <author> Neil D. Jones, Harald Stndergaard: </author> <title> A Semantics-Based Framework for the Abstract Interpretation of Prolog, in Abstract Interpretation of Declarative Languages, edited by S. </title> <editor> Abramsky and C. Hankin, </editor> <address> 123-142, </address> <publisher> Ellis Horwood, </publisher> <address> Chichester, England, </address> <year> 1987. </year> <note> 116 REFERENCES </note>
Reference-contexts: Groundness analysis is more subtle than it appears due to possible aliasing and shared substructures, since binding one variable to a ground term may affect variables appearing in another part of the program being analyzed. Simple groundness and sharing analyses are described in <ref> [Jones, 1987b] </ref>. A more elegant method using propositional formulas built from ^ and , was introduced in [Marriott, 1987] and compared with other methods in [Cortesi, 1991]. <p> One example is deforestation [Wadler, 1988]. 4.5.6 Methods of Analysis Analysis methods can roughly be divided into the pragmatically oriented, including [Bruynooghe, 1991], [Mellish, 1987], and [Nilsson, 1991]; and the semantically oriented, including [Cortesi, 1991], [Debray, 1986], <ref> [Jones, 1987b] </ref>, and [Marriott, 1993]. A natural analogue to the accumulating semantics seen earlier was used in [Jones, 1987b] and a number of later papers, and presumes given a sequence of clauses and a single query. <p> [Wadler, 1988]. 4.5.6 Methods of Analysis Analysis methods can roughly be divided into the pragmatically oriented, including [Bruynooghe, 1991], [Mellish, 1987], and [Nilsson, 1991]; and the semantically oriented, including [Cortesi, 1991], [Debray, 1986], <ref> [Jones, 1987b] </ref>, and [Marriott, 1993]. A natural analogue to the accumulating semantics seen earlier was used in [Jones, 1987b] and a number of later papers, and presumes given a sequence of clauses and a single query. It is a "sticky" semantics in which the program points are the positions just before each clause goal or the query, and at the end of each clause and the query.
Reference: [Jones, 1989] <author> Neil D. Jones, Peter Sestoft, Harald Stndergaard: </author> <title> Mix: </title>
Reference-contexts: They can be used for extraction of more general program properties [Wegbreit, 1975] and have been used for many applications including: generating assertions for program verifiers [Cousot, 1977b], program validation [Fosdick, 1976] and [Foster, 1987], testing applicability of program transformations [Nielson, 1985a], compiler generation and partial evaluation <ref> [Jones, 1989] </ref>, [Nielson, 1988b], estimating program running times [Rosendahl, 1989], and efficiently paral-lelizing sequential programs [Masdupuy, 1991,Mercouroff, 1991]. The first papers on automatic program analysis were rather ad hoc, and oriented almost entirely around one application: optimization of target or intermediate code by compilers.
References-found: 45

