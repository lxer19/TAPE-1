URL: http://http.cs.berkeley.edu/~rywang/papers/asplos96.ps
Refering-URL: http://http.cs.berkeley.edu/~rywang/papers/asplos96.html
Root-URL: 
Title: Evaluation of Architectural Support for Global Address-Based Communication in Large-Scale Parallel Machines  
Author: Arvind Krishnamurthy Klaus E. Schauser Chris J. Scheiman Randolph Y. Wang David E. Culler and Katherine Yelick 
Abstract: Large-scale parallel machines are incorporating increasingly sophisticated architectural support for user-level messaging and global memory access. We provide a systematic evaluation of a broad spectrum of current design alternatives based on our implementations of a global address language on the Thinking Machines CM-5, Intel Paragon, Meiko CS-2, Cray T3D, and Berkeley NOW. This evaluation includes a range of compilation strategies that make varying use of the network processor; each is optimized for the target architecture and the particular strategy. We analyze a family of interacting issues that determine the performance tradeoffs in each implementation, quantify the resulting latency, overhead, and bandwidth of the global access operations, and demonstrate the effects on application performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Amza, A. L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Ra-jamony, W. Yu, and W. Zwaenepoel. TreadMarks: </author> <title> Shared Memory Computing on Networks of Workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2), </volume> <year> 1996. </year>
Reference-contexts: largely orthogonal to the many architectural studies of distributed shared memory machines, which seek to avoid unnecessary communication by exploiting address translation hardware to allow consistent replication of blocks throughout the system [16, 17, 18, 20], and operating system studies, which seek the same end by extending virtual memory support <ref> [21, 1, 8] </ref>. In these efforts, communication is caused by a single load or store instruction, and the underlying hardware or operating system mechanisms move the data transparently. We focus on what happens when the communication is necessary.
Reference: [2] <author> T. E. Anderson, D. E. Culler, and D. A. Patterson. </author> <title> A Case for NOW(Network of Workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1), </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: Processors are grouped in pairs, share a network interface and block-transfer engine, and all 2-processor nodes are connected via a three-dimensional torus network with 300 MB/s links. Berkeley NOW: The Berkeley-NOW <ref> [2] </ref> is a cluster of Ul-traSparc workstations connected together by Myrinet [6]. The CP is a 167 MHz four-way super-scalar UltraSparc processor. The Myrinet NI is an I/O card that plugs into the standard SBus. It contains a 32-bit CISC-based "LANai" network processor, DMA engines, and local memory (SRAM).
Reference: [3] <author> R. Arpaci, D. Culler, A. Krishnamurthy, S. Steinberg, and K. Yelick. </author> <title> Empirical Evaluation of the CRAY-T3D: A Compiler Perspective. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: On the Paragon, we have an implementation for each of the four different message handler placement strategies discussed in this section. On the T3D, we have implemented a sole version that handles all remote memory accesses using the combined memory controller and network interface <ref> [3] </ref>. 4 Issues and Qualitative Analysis In this section we present a set of interacting issues that shape the implementation of global address operations. Our goal is to identify the hardware and software features that affect both correctness and performance. <p> Instead, the CP and the NP communicate through a queue located in the NP's local memory, which is accessible to the CP through memory mapped addresses. On the T3D, crossing the processor chip boundary is the major cost of all remote memory operations <ref> [3] </ref>; however, the external shell is designed specifically to present the network as conveniently as possible to the processor. <p> Similarly, on the T3D, reads and writes are much faster than active messages, since reads and writes are directly supported in hardware, while active message operations must be constructed from a sequence of remote memory operations <ref> [3] </ref>. In contrast, the four Proc implementations of read and write (on the CM-5, Meiko, Paragon, and NOW) are built using active messages, so they take the time of a null active message plus the additional time needed to read or write.
Reference: [4] <author> E. Barton, J. Cownie, and M. McLaren. </author> <title> Message passing on the Meiko CS-2. </title> <journal> Parallel Computing, </journal> <volume> 20(4), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: The network interface provides a pair of relatively deep input and output FIFOs (2KB each), which can be driven by either processor or by the DMA engines. The network is a 2D mesh with links operating at 175 MB/s in each direction. Meiko CS-2: The Meiko CS-2 <ref> [4] </ref> node contains a special-purpose "Elan" network processor integrated with the network interface and DMA controller. The network processor is attached to the memory bus and is cache-coherent with the compute processor, which is a 40 MHz three-way superscalar SuperSparc processor.
Reference: [5] <author> B. Bershad, S. Savage, P. Pardyak, E. G. Sirer, D. Becker, M. Fiuczynski, C. Chambers, and S. Eggers. </author> <title> Extensibility, Safety and Performance in the SPIN Operating System. </title> <booktitle> In Fifteenth ACM Symposium on Operating System Principles, </booktitle> <year> 1995. </year>
Reference-contexts: However, the effective performance of these operations depends on how they are actually used in programs. Two issues that have emerged clearly in this study are responsiveness and the frequency of remote events. 1 This set might be enlarged by using sandboxing or software fault isolation techniques <ref> [5, 30] </ref>.
Reference: [6] <author> N. J. Boden, D. Cohen, R. E. Felderman, A. E. Kulawik, C. L. Seitz, J. N. Seizovic, and W. Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1), </volume> <month> Febru-ary </month> <year> 1995. </year>
Reference-contexts: 1 Introduction In recent years several architectures have demonstrated practical scalability beyond a thousand microprocessors, including the nCUBE/2, Thinking Machines CM-5, Intel Paragon, Meiko CS-2, and Cray T3D. More recently, researchers have also demonstrated high performance communication in Network of Workstations (NOW) using scalable switched local area network technology <ref> [28, 6, 12] </ref>. While the dominant programming model at this scale is message passing, the primitives used are inherently expensive, due to buffering and scheduling overheads [29]. <p> Processors are grouped in pairs, share a network interface and block-transfer engine, and all 2-processor nodes are connected via a three-dimensional torus network with 300 MB/s links. Berkeley NOW: The Berkeley-NOW [2] is a cluster of Ul-traSparc workstations connected together by Myrinet <ref> [6] </ref>. The CP is a 167 MHz four-way super-scalar UltraSparc processor. The Myrinet NI is an I/O card that plugs into the standard SBus. It contains a 32-bit CISC-based "LANai" network processor, DMA engines, and local memory (SRAM).
Reference: [7] <author> M. C. Carlisle, A. Rogers, J. H. Reppy, and L. J. Hendren. </author> <title> Early experiences with Olden (parallel programming). </title> <booktitle> In Languages and Compilers for Parallel Computing. 6th International Workshop Proceedings. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: The network provides a link bandwidth of 80 MB/sec. 2.2 Global address space language Many parallel languages, including HPF [15], Split-C [13], CC++ [9], Cid [23], and Olden <ref> [7] </ref>, provide a global address space abstraction built from a combination of compiler and runtime support. The language implementations differ in the amount of information available at compile time and the amount of runtime support for moving and caching values.
Reference: [8] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <volume> 7(4), </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: largely orthogonal to the many architectural studies of distributed shared memory machines, which seek to avoid unnecessary communication by exploiting address translation hardware to allow consistent replication of blocks throughout the system [16, 17, 18, 20], and operating system studies, which seek the same end by extending virtual memory support <ref> [21, 1, 8] </ref>. In these efforts, communication is caused by a single load or store instruction, and the underlying hardware or operating system mechanisms move the data transparently. We focus on what happens when the communication is necessary.
Reference: [9] <author> K. M. Chandy and C. Kesselman. </author> <title> Compositional C++: Compositional Parallel Programming. </title> <booktitle> In 5th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: The Myrinet network is composed of crossbar switches with eight bidirectional ports, and the switches can be linked to obtain arbitrary topologies. The network provides a link bandwidth of 80 MB/sec. 2.2 Global address space language Many parallel languages, including HPF [15], Split-C [13], CC++ <ref> [9] </ref>, Cid [23], and Olden [7], provide a global address space abstraction built from a combination of compiler and runtime support. The language implementations differ in the amount of information available at compile time and the amount of runtime support for moving and caching values.
Reference: [10] <author> Cray Research Incorporated. </author> <title> The CRAY T3D Hardware Reference Manual, </title> <year> 1993. </year>
Reference-contexts: The network is comprised of two 4-ary fat-trees that have a link-level bandwidth of 70 MB/sec. Cray T3D: The Cray T3D <ref> [10] </ref> has a sophisticated message unit, which is integrated into the memory controller, to provide direct hardware support for remote memory operations. A node consists of a 150 MHz Alpha 21064 [27] processor, memory, and a "shell" of support circuitry to provide global memory access and synchronization.
Reference: [11] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Sumbramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Proceedings of the 1993 Conference on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Our perspective is influenced by the LogP model <ref> [11] </ref>, although we use a different definition of latency, which includes overhead.
Reference: [12] <author> D. Culler, L. T. Liu, R. P. Martin, and C. Yoshikawa. </author> <title> LogP Performance Assessment of Fast Network Interfaces. </title> <booktitle> IEEE Micro, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: 1 Introduction In recent years several architectures have demonstrated practical scalability beyond a thousand microprocessors, including the nCUBE/2, Thinking Machines CM-5, Intel Paragon, Meiko CS-2, and Cray T3D. More recently, researchers have also demonstrated high performance communication in Network of Workstations (NOW) using scalable switched local area network technology <ref> [28, 6, 12] </ref>. While the dominant programming model at this scale is message passing, the primitives used are inherently expensive, due to buffering and scheduling overheads [29].
Reference: [13] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: The Myrinet network is composed of crossbar switches with eight bidirectional ports, and the switches can be linked to obtain arbitrary topologies. The network provides a link bandwidth of 80 MB/sec. 2.2 Global address space language Many parallel languages, including HPF [15], Split-C <ref> [13] </ref>, CC++ [9], Cid [23], and Olden [7], provide a global address space abstraction built from a combination of compiler and runtime support. The language implementations differ in the amount of information available at compile time and the amount of runtime support for moving and caching values. <p> The language implementations differ in the amount of information available at compile time and the amount of runtime support for moving and caching values. We consider the problem of implementing a minimalist language, Split-C <ref> [13] </ref>, which focuses attention on the problems of naming, retrieving, and updating remote values. A program is comprised of a thread of control on each processor from a common code image (SPMD). The threads execute asynchronously, but may synchronize through global accesses or barriers.
Reference: [14] <author> W. Groscup. </author> <title> The Intel Paragon XP/S Supercomputer. </title> <booktitle> In Proceedings of the Fifth ECMWF Workshop on the Use of Parallel Processors in Meteorology., </booktitle> <month> Nov </month> <year> 1992. </year>
Reference-contexts: Thus, the network is effectively a distributed set of queues. The queues are quite shallow, holding only three 5-word messages. The network is a 4-ary fat tree that has a link bandwidth of 20 MB/sec in each direction. Intel Paragon: In the Paragon <ref> [14] </ref>, each node contains one or more compute processors (50 MHz i860 processors) and an identical CPU dedicated for use as a network processor. Our configuration has a single compute processor per node. The compute and network processors share memory over a cache-coherent memory bus.
Reference: [15] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification Version 1.0, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: The Myrinet network is composed of crossbar switches with eight bidirectional ports, and the switches can be linked to obtain arbitrary topologies. The network provides a link bandwidth of 80 MB/sec. 2.2 Global address space language Many parallel languages, including HPF <ref> [15] </ref>, Split-C [13], CC++ [9], Cid [23], and Olden [7], provide a global address space abstraction built from a combination of compiler and runtime support. The language implementations differ in the amount of information available at compile time and the amount of runtime support for moving and caching values.
Reference: [16] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary, </type> <year> 1992. </year>
Reference-contexts: Our investigation is largely orthogonal to the many architectural studies of distributed shared memory machines, which seek to avoid unnecessary communication by exploiting address translation hardware to allow consistent replication of blocks throughout the system <ref> [16, 17, 18, 20] </ref>, and operating system studies, which seek the same end by extending virtual memory support [21, 1, 8]. In these efforts, communication is caused by a single load or store instruction, and the underlying hardware or operating system mechanisms move the data transparently.
Reference: [17] <author> J. Kubiatowicz and A. Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In 7th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: Our investigation is largely orthogonal to the many architectural studies of distributed shared memory machines, which seek to avoid unnecessary communication by exploiting address translation hardware to allow consistent replication of blocks throughout the system <ref> [16, 17, 18, 20] </ref>, and operating system studies, which seek the same end by extending virtual memory support [21, 1, 8]. In these efforts, communication is caused by a single load or store instruction, and the underlying hardware or operating system mechanisms move the data transparently.
Reference: [18] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Si-moni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford Flash Multiprocessor. </title> <booktitle> In 21st International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Our investigation is largely orthogonal to the many architectural studies of distributed shared memory machines, which seek to avoid unnecessary communication by exploiting address translation hardware to allow consistent replication of blocks throughout the system <ref> [16, 17, 18, 20] </ref>, and operating system studies, which seek the same end by extending virtual memory support [21, 1, 8]. In these efforts, communication is caused by a single load or store instruction, and the underlying hardware or operating system mechanisms move the data transparently. <p> Our investigation overlaps somewhat with the cooperative shared memory work, which initiates communication transparently, but allows remote memory operations to be serviced by programmable handlers on dedicated network processors <ref> [18, 24] </ref>. The study could, in principle, be performed with other compiler-assisted shared memory implementations [31], but these do not have the necessary base of highly optimized implementations on a range of hardware alternatives. The rest of the paper is organized as follows.
Reference: [19] <author> C. E. Leiserson, Z. S. Abuhamdeh, D. C. Douglas, C. R. Feynman, M. N. Ganmukhi, J. V. Hill, W. D. Hillis, B. C. Kuszmaul, M. A. St. Pierre, D. S. Wells, M. C. Wong, S. Yang, and R. Zak. </author> <title> The Network Architecture of the CM-5. </title> <booktitle> In Symposium on Parallel and Distributed Algorithms '92, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: They range from a minimal network interface on the CM-5 to a full-fledged processor on the Paragon. Figure 1 gives a sketch of the node architecture on each machine. Thinking Machines CM-5: The CM-5 <ref> [19] </ref> has the most primitive messaging hardware of the five machines. Each node contains a single 33 MHz Sparc processor and a conventional MBus-based memory system. (We ignore the vector units in both the CM-5 and Meiko machines.) The network interface unit provides user-level access to the network.
Reference: [20] <author> D. Lenoski, J. Laundon, K. Gharachorloo, A. Gupta, and J. L. Hennessy. </author> <title> The Directory Based Cache Coherance Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: Our investigation is largely orthogonal to the many architectural studies of distributed shared memory machines, which seek to avoid unnecessary communication by exploiting address translation hardware to allow consistent replication of blocks throughout the system <ref> [16, 17, 18, 20] </ref>, and operating system studies, which seek the same end by extending virtual memory support [21, 1, 8]. In these efforts, communication is caused by a single load or store instruction, and the underlying hardware or operating system mechanisms move the data transparently.
Reference: [21] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <month> November </month> <year> 1989. </year>
Reference-contexts: largely orthogonal to the many architectural studies of distributed shared memory machines, which seek to avoid unnecessary communication by exploiting address translation hardware to allow consistent replication of blocks throughout the system [16, 17, 18, 20], and operating system studies, which seek the same end by extending virtual memory support <ref> [21, 1, 8] </ref>. In these efforts, communication is caused by a single load or store instruction, and the underlying hardware or operating system mechanisms move the data transparently. We focus on what happens when the communication is necessary.
Reference: [22] <author> L. T. Liu and D. E. Culler. </author> <title> Evaluation of the Intel Paragon on Active Message Communication. </title> <booktitle> In Intel Supercomputer Users Group Conference, </booktitle> <year> 1995. </year>
Reference-contexts: Surprisingly, on the Paragon, the cache coherency protocol (between the CP and the NP) results in at least four bus transfers for an eight-word processor-to-NP transfer, at a cost greater than a processor-to-NI transfer <ref> [22] </ref>. On the Meiko, because the NI is integrated into the NP, the CP can not directly send into the network.
Reference: [23] <author> R. S. Nikhil. Cid: </author> <title> A Parallel, "Shared Memory" C for Distributed Memory Machines. </title> <booktitle> In Languages and Compilers for Parallel Computing. 7th International Workshop Proceedings. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: The Myrinet network is composed of crossbar switches with eight bidirectional ports, and the switches can be linked to obtain arbitrary topologies. The network provides a link bandwidth of 80 MB/sec. 2.2 Global address space language Many parallel languages, including HPF [15], Split-C [13], CC++ [9], Cid <ref> [23] </ref>, and Olden [7], provide a global address space abstraction built from a combination of compiler and runtime support. The language implementations differ in the amount of information available at compile time and the amount of runtime support for moving and caching values.
Reference: [24] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Typhoon and Tempest: </author> <title> User-Level Shared Memory. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Our investigation overlaps somewhat with the cooperative shared memory work, which initiates communication transparently, but allows remote memory operations to be serviced by programmable handlers on dedicated network processors <ref> [18, 24] </ref>. The study could, in principle, be performed with other compiler-assisted shared memory implementations [31], but these do not have the necessary base of highly optimized implementations on a range of hardware alternatives. The rest of the paper is organized as follows.
Reference: [25] <author> K. E. Schauser and C. J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: On the CM-5, which has no network processors, a sole version exists. All message handlers are executed directly by the compute processors. On the Meiko and the NOW, we have an implementation that executes the message handlers on the CP and another that executes them on the NP <ref> [25, 26] </ref>. The CP does not interact with the network directly in either case. On the Paragon, we have an implementation for each of the four different message handler placement strategies discussed in this section.
Reference: [26] <author> K. E. Schauser, C. J. Scheiman, J. M. Ferguson, and P. Z. Kolano. </author> <title> Exploiting the Capabilities of Communications Co-processors. </title> <booktitle> In 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: On the CM-5, which has no network processors, a sole version exists. All message handlers are executed directly by the compute processors. On the Meiko and the NOW, we have an implementation that executes the message handlers on the CP and another that executes them on the NP <ref> [25, 26] </ref>. The CP does not interact with the network directly in either case. On the Paragon, we have an implementation for each of the four different message handler placement strategies discussed in this section. <p> The Meiko allows for optimizations of a different sort, as the handler code can be mapped directly onto some specialized operations supported by the NP, such as remote atomic writes <ref> [26] </ref>. 4.3 Application issues Considering architectural and language implementation issues in isolation, one can construct a solution that attempts to minimize the latency, overhead, and gap for the individual global access operations, striking some balance between the three metrics.
Reference: [27] <author> R. L. </author> <title> Sites. Alpha Architecture Reference Manual. </title> <institution> Digital Equipment Corporation, </institution> <year> 1992. </year>
Reference-contexts: Cray T3D: The Cray T3D [10] has a sophisticated message unit, which is integrated into the memory controller, to provide direct hardware support for remote memory operations. A node consists of a 150 MHz Alpha 21064 <ref> [27] </ref> processor, memory, and a "shell" of support circuitry to provide global memory access and synchronization.
Reference: [28] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. </author> <month> U-Net: </month>
Reference-contexts: 1 Introduction In recent years several architectures have demonstrated practical scalability beyond a thousand microprocessors, including the nCUBE/2, Thinking Machines CM-5, Intel Paragon, Meiko CS-2, and Cray T3D. More recently, researchers have also demonstrated high performance communication in Network of Workstations (NOW) using scalable switched local area network technology <ref> [28, 6, 12] </ref>. While the dominant programming model at this scale is message passing, the primitives used are inherently expensive, due to buffering and scheduling overheads [29].
References-found: 28

