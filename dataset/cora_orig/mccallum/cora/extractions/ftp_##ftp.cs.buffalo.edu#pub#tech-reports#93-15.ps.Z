URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/93-15.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Title: Behavior Based AI, Cognitive Processes, and Emergent Behaviors in Autonomous Agents  
Author: Henry Hexmoor, Johan Lammens, Guido Caicedo, and Stuart C. Shapiro 
Address: 226 Bell Hall  Buffalo, NY 14260  
Affiliation: Department of Computer Science  State University of New York at Buffalo  
Date: June 29 July 1, 1993.  
Note: This is a slightly revised version of a paper that will appear in the proceedings of AI In Engineering, Toulouse, France,  
Abstract: Behavior based AI [Brooks, 1990, Maes, 1990] has questioned the need for modeling intelligent agency using generalized cognitive modules for perception and behavior generation. Behavior based AI has demonstrated successful interactions in unpredictable environments in the mobile robot domain [Brooks, 1985, Brooks, 1990]. This has created a gulf between "traditional" approaches to modeling intelligent agency and behavior based approaches. We present an architecture for intelligent autonomous agents which we call GLAIR (Grounded Layered Architecture with Integrated Reasoning) [Hexmoor et al., 1992, Hex-moor et al., 1993b, Hexmoor et al., 1993a]. GLAIR is a general multi-level architecture for autonomous cognitive agents with integrated sensory and motor capabilities. GLAIR offers an "unconscious" layer for modeling tasks that exhibit a close affinity between sensing and acting, i.e., behavior based AI modules, and a "conscious" layer for modeling tasks that exhibit delays between sensing and acting. GLAIR provides learning mechanisms that allow for autonomous agents to learn emergent behaviors and add it to their repertoire of behaviors. In this paper we will describe the principles of GLAIR and systems we have developed that demonstrate how GLAIR based agents acquire and exhibit a repertoire of behaviors at different cognitive levels. 
Abstract-found: 1
Intro-found: 1
Reference: [Agre and Chapman, 1987] <author> Agre, P. E. and Chapman, D. </author> <year> (1987). </year> <title> Pengi: An implementation of a theory of activity. </title> <booktitle> In Proceedings of AAAI-87, </booktitle> <address> Seattle, Wa., </address> <pages> pages 268-272. </pages>
Reference-contexts: A major objective for GLAIR is learning emergent behaviors. Like Agre's improvised actions <ref> [Agre and Chapman, 1987] </ref> and Brooks's subsumption [Brooks, 1985] we believe complex behaviors emerge from interaction of the agent with its environment without planning.
Reference: [Albus et al., 1981] <author> Albus, J., Barbera, A., and Nagel, R. </author> <year> (1981). </year> <title> Theory and practice of hierarchical control. </title> <booktitle> In 23rd International IEEE Computer Society Conference, </booktitle> <pages> pages 18-38. </pages>
Reference: [Anderson, 1983] <author> Anderson, J. R. </author> <year> (1983). </year> <title> The Architecture of Cognition. </title> <publisher> Cam-bridge: Harvard University Press. </publisher>
Reference-contexts: Architectures for understanding/modeling behaviors of an anthropomorphic agent, e.g., cognitive architectures <ref> [Anderson, 1983, Pollock, 1989, Langley et al., 1991] </ref>, tend to address the relationships that exist among the structure of memory, reasoning abilities, intelligent behavior, and mental states and experiences. These architectures often do not take the body into account. Instead they primarily focus on the mind and consciousness.
Reference: [Ballard and Brown, 1982] <author> Ballard, D. H. and Brown, C. M. </author> <year> (1982). </year> <title> Computer Vision. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: Representation, reasoning (including planning), perception, and generation of behavior are distributed through all three levels. Our architecture is best described using a resolution pyramid metaphor as used in computer vision work <ref> [Ballard and Brown, 1982] </ref>, rather than a central vs. peripheral metaphor. Architectures for building physical systems, e.g. robotic architectures [Al-bus et al., 1981], tend to address the relationship between a physical entity like a robot and sensors, effectors, and tasks to be accomplished.
Reference: [Bandera and Scott, 1989] <author> Bandera, C. and Scott, P. </author> <year> (1989). </year> <title> Foveal machine vision systems. </title> <booktitle> In IEEE International Conference on Systems, Man, and Cybernetics, </booktitle> <pages> pages 596-599. </pages>
Reference-contexts: Brakes can be applied to each wheel. We assume negligible acceleration/deceleration times. As shown in Figure 6, the agent receives an image, bumper information, and wheel speeds at the SA level. The image is subsequently foveated <ref> [Bandera and Scott, 1989] </ref> to model foveation after advanced biological vision. (Foveation produces high resolution at the center of the image and decreasing resolution at the image periphery.) After some early vision processing, the processed image data in the form of blobs as well as other sensory information reaches the Perceptuo-Motor
Reference: [Berlin and Kay, 1969] <author> Berlin, B. and Kay, P. </author> <year> (1991 </year> <month> (orig. </month> <year> 1969)). </year> <title> Basic Color Terms: Their Universality and Evolution. </title> <institution> University of California Press, Berkeley CA, </institution> <note> first paperback edition. </note>
Reference-contexts: This model allows the agent to (1) name colors shown to it, and express a typicality judgement, (2) 10 point out examples of named colors in its environment, and (3) learn new names for colors. This model provides the perceptual grounding for a set of basic color terms <ref> [Berlin and Kay, 1969] </ref> represented at the Knowledge level. The color domain was chosen as a case study for embodied perception because of the relative abundance of psychological, psychophysical and neurophysiological data in the literature [Lammens, ].
Reference: [Brooks, 1985] <author> Brooks, R. </author> <year> (1985). </year> <title> A robust layered control system for a mobile robot. </title> <type> Technical Report 864, </type> <institution> MIT AI Labs, MIT. </institution>
Reference-contexts: The organization of an architecture is also influenced by adopting various philosophical positions like Fodor's modularity assumption [Fodor, 1983], or a connectionist point of view, e.g. [McClelland et al., 1986], or an anti-modularity assumption as in Brooks's subsumption ar-chitecture <ref> [Brooks, 1985] </ref>. The modularity assumption supports (among other things) a division of the mind into a central system, i.e. cognitive processes such as learning, planning, and reasoning, and a peripheral system, i.e. sensory and motor processing [Chapman, 1990]. <p> A major objective for GLAIR is learning emergent behaviors. Like Agre's improvised actions [Agre and Chapman, 1987] and Brooks's subsumption <ref> [Brooks, 1985] </ref> we believe complex behaviors emerge from interaction of the agent with its environment without planning.
Reference: [Brooks, 1990] <author> Brooks, R. A. </author> <year> (1990). </year> <title> Elephants dont play chess. </title> <editor> In Maes, P., editor, </editor> <booktitle> Designing Autonomous Agents, </booktitle> <pages> pages 3-15. </pages> <publisher> MIT Press. </publisher>
Reference: [Chapman, 1990] <author> Chapman, D. </author> <year> (1990). </year> <title> Vision, instruction, and action. </title> <type> Technical Report 1204, </type> <institution> MIT Artificial Intelligence Laboratory, MIT. </institution>
Reference-contexts: The modularity assumption supports (among other things) a division of the mind into a central system, i.e. cognitive processes such as learning, planning, and reasoning, and a peripheral system, i.e. sensory and motor processing <ref> [Chapman, 1990] </ref>. Our architecture is characterized by a three-level organization into a Knowledge Level (KL), a Perceptuo-Motor Level (PML), and a Sensori-Actuator Level (SAL). This organization is neither modular, anti-modular, hierarchical, anti-hierarchical, nor connectionist in the conventional sense.
Reference: [Fodor, 1983] <author> Fodor, J. </author> <year> (1983). </year> <title> The Modularity of Mind. </title> <publisher> MIT Press. </publisher> <pages> 14 </pages>
Reference-contexts: The organization of an architecture is also influenced by adopting various philosophical positions like Fodor's modularity assumption <ref> [Fodor, 1983] </ref>, or a connectionist point of view, e.g. [McClelland et al., 1986], or an anti-modularity assumption as in Brooks's subsumption ar-chitecture [Brooks, 1985].
Reference: [Hexmoor, 1992] <author> Hexmoor, H. </author> <year> (1992). </year> <title> Representing and learning successful rou-tine activities. </title> <type> Technical Report Unpublished PhD Proposal, </type> <institution> Dept. of Computer Science, SUNY at Buffalo, </institution> <address> New York. </address>
Reference-contexts: When our confidence in a Routine reaches a certain level, a concept is created at the Knowledge level of GLAIR for the routine and from then on, this routine can be treated as a single action at that level, <ref> [Hexmoor, 1992] </ref>. Learning routines is closely related to the second variation of learning tendencies as we discussed 7 in the previous section.
Reference: [Hexmoor et al., 1993a] <author> Hexmoor, H., Caicedo, G., Bidwell, F., and Shapiro, S. </author> <year> (1993a). </year> <title> Air battle simulation: An agent with conscious and unconscious layers. </title> <institution> In University of Buffalo Graduate Conference in Computer Science-93. Dept. of Computer Science, SUNY at Buffalo, </institution> <address> New York. </address>
Reference-contexts: In the following section, we describe a stepwise learning scheme for acquiring PMA transitions. 3 Knowledge Migration from "conscious" to "un conscious" Level Knowledge in GLAIR can migrate from conscious to unconscious levels. In our application Air Battle Simulation (ABS) described later in this paper (see <ref> [Hexmoor et al., 1993a] </ref> for detals of this system) we show how a video-game playing agent learns how to dynamically "compile" a game playing strategy that is initially formulated as explicit reasoning rules at the Knowledge level into an implicit form of knowledge at the Perceptuo-Motor level, a PMA. <p> Instead of learning action goodnesses, to learn a routine we record the sequence of actions between a significantly bad situations and a significantly good situation. 6 Applications of GLAIR 6.1 A simulation study: Air Battle We have written a program, Air Battle Simulation (ABS) <ref> [Hexmoor et al., 1993a] </ref>, that simulates World War I style airplane dog-fights. ABS is an interactive video-game in which a human player plays against a computer-driven agent. The game starts up by displaying a game window and a control panel window (Figure 3).
Reference: [Hexmoor et al., 1992] <author> Hexmoor, H., Lammens, J., and Shapiro, S. </author> <year> (1992). </year> <title> An autonomous agent architecture for integrating perception and acting with grounded, embodied symbolic reasoning. </title> <type> Technical Report CS-92-21, </type> <institution> Dept. of Computer Science, SUNY at Buffalo, </institution> <address> New York. </address>
Reference-contexts: Sensors include both world-sensors and proprio-sensors. There are several features that contribute to the robustness of our architecture. We highlight them below. For an in-depth discussion and comparison with other architectures see <ref> [Hexmoor et al., 1992] </ref>. * We differentiate conscious reasoning from unconscious Perceptuo-motor and sensori-actuator processing. * The levels of our architecture are semi-autonomous and processed in par allel. * Conscious reasoning takes place through explicit knowledge representation and reasoning.
Reference: [Hexmoor et al., 1993b] <author> Hexmoor, H., Lammens, J., and Shapiro, S. C. </author> <year> (1993b). </year> <title> Embodiment in GLAIR: A Grounded Layered Architecture with Integrated Reasoning. </title> <booktitle> In Florida AI Research Symposium. </booktitle>
Reference: [Hexmoor and Nute, 1992] <author> Hexmoor, H. and Nute, D. </author> <year> (1992). </year> <title> Methods for deciding what to do next and learning. </title> <type> Technical Report AI-1992-01, </type> <institution> AI Programs, The University of Georgia, Athens, Georgia. </institution> <note> Also available from SUNY at Buffalo, CS Department TR-92-23. </note>
Reference-contexts: Our Robot Waiter agent, for instance (section 6.2), uses distinct hardware for the three levels. 4 2.1 Perceptuo-Motor Automata At the perceptuo-motor level, the behaviors resulting in physical actions are generated by an automaton, which we will call a PM-automaton (PMA) <ref> [Hexmoor and Nute, 1992] </ref>. In other words, PMA are representation mechanisms for generating behaviors at an "unconscious" level. PMA is a family of finite state machines represented by &lt;Rewards, Goal-Transitions, Goals, Action-Transitions, Actions, Sensations&gt;. Goals, Rewards, and Goal-Transitions in a PMA can be empty. <p> When the latest sensations along with the current goal match an action transition, that action transition activates an action which is then executed. The primary mode of acquiring a PMA is intended to be by converting plans in the knowledge level into a PMA by a process described in <ref> [Hexmoor and Nute, 1992] </ref>. In the following section, we describe a stepwise learning scheme for acquiring PMA transitions. 3 Knowledge Migration from "conscious" to "un conscious" Level Knowledge in GLAIR can migrate from conscious to unconscious levels.
Reference: [Lammens, ] <author> Lammens, J. M. </author> <title> A computational model of color perception and color naming: a case study of symbol grounding for natural language semantics. Dissertation proposal, </title> <institution> SUNY/Buffalo CS department, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: This model provides the perceptual grounding for a set of basic color terms [Berlin and Kay, 1969] represented at the Knowledge level. The color domain was chosen as a case study for embodied perception because of the relative abundance of psychological, psychophysical and neurophysiological data in the literature <ref> [Lammens, ] </ref>. It is a complex enough domain to allow the usefulness of embodiment for computational models of perception to be demonstrated, yet feasible enough to be implemented in actual autonomous agents.
Reference: [Langley et al., 1991] <author> Langley, P., McKusick, K., and Allen, J. </author> <year> (1991). </year> <title> A design for the icarus architecture. </title> <journal> In ACM SIGART Bulletin, </journal> <pages> pages 104-109. </pages> <publisher> ACM publications. </publisher>
Reference-contexts: Architectures for understanding/modeling behaviors of an anthropomorphic agent, e.g., cognitive architectures <ref> [Anderson, 1983, Pollock, 1989, Langley et al., 1991] </ref>, tend to address the relationships that exist among the structure of memory, reasoning abilities, intelligent behavior, and mental states and experiences. These architectures often do not take the body into account. Instead they primarily focus on the mind and consciousness.
Reference: [Maes, 1990] <author> Maes, P. </author> <year> (1990). </year> <title> Situated agents can have goals. </title> <editor> In Maes, P., editor, </editor> <booktitle> Designing Autonomous Agents, chapter 4, </booktitle> <pages> pages 49-70. </pages> <publisher> MIT Press. </publisher>
Reference: [McClelland et al., 1986] <author> McClelland, J. L., Rumelhart, D. E., and Hinton, G. E. </author> <year> (1986). </year> <title> The appeal of parallel distributed processing. </title> <editor> In Rumelhart, D., McClelland, J., and the PDP Research Group, editors, </editor> <booktitle> Parallel Distributed Processing, chapter 1, </booktitle> <pages> pages 3-44. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: The organization of an architecture is also influenced by adopting various philosophical positions like Fodor's modularity assumption [Fodor, 1983], or a connectionist point of view, e.g. <ref> [McClelland et al., 1986] </ref>, or an anti-modularity assumption as in Brooks's subsumption ar-chitecture [Brooks, 1985].
Reference: [Pollock, 1989] <author> Pollock, J. </author> <year> (1989). </year> <title> How to Build a Person. </title> <publisher> MIT Press. </publisher>
Reference-contexts: These architectures tend to be primarily concerned with the body, that is, how to get the physical system to exhibit intelligent behavior through its physical activity. These architectures address what John Pollock calls Quick and Inflexible (Q&I) processes <ref> [Pollock, 1989] </ref>. <p> Architectures for understanding/modeling behaviors of an anthropomorphic agent, e.g., cognitive architectures <ref> [Anderson, 1983, Pollock, 1989, Langley et al., 1991] </ref>, tend to address the relationships that exist among the structure of memory, reasoning abilities, intelligent behavior, and mental states and experiences. These architectures often do not take the body into account. Instead they primarily focus on the mind and consciousness.
Reference: [Shen, 1989] <author> Shen, W.-M. </author> <year> (1989). </year> <title> Learning from the Environment Based on Actions and Percepts. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: As the game was played, we observed that the agent became more reactive since the PMA was increasingly used to generate behaviors instead of the Knowledge level. We are exeprimenting with reinforcement based learning, emergent routines, and other learning techniques such as experimentation as a form of learning <ref> [Shen, 1989] </ref>. We are also interested in developing experiments that will help in psychological validation of GLAIR and the learning strategies used in ABS.
Reference: [Sutton, 1988] <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> In Machine Learning 3, </booktitle> <pages> pages 3-44. </pages>
Reference-contexts: ATs may consider previous actions in deciding what to do next. We may take into account an estimated accumulated reward for actions. We call the latter, tendencies. Tendencies are computed using reinforcement based learning techniques <ref> [Sutton, 1988] </ref>. Goal-Transitions (GT) are transformations that update goals when the current goal is satisfied. GT is Goal1 fi Sensations 7! Goal2 where Goal1 and Goal2 are goals. The PMA maintains the current goal.
Reference: [Watkins, 1989] <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, UK. </address> <month> 15 </month>
Reference-contexts: At the PM-level, this knowledge is learned in the form of PMA transitions. The idea is that the next time circumstances make this action applicable, it can be selected for execution without having to resort to "conscious" processes. 4 Learning Tendencies Reinforcement based learning <ref> [Watkins, 1989] </ref> is a successful technique that is used for learning action consequences, also known as action models. In learning action models, we assume a Markovian environment. That is, the agent believes the world changes only due to its own actions.
References-found: 23

