URL: ftp://ftp-csli.stanford.edu/pub/prosit/papers/schuetze.meaning.ps.Z
Refering-URL: 
Root-URL: 
Title: In Proceedings of Supercomputing '92 Dimensions of Meaning  
Author: Hinrich Schutze 
Address: Ventura Hall Stanford, CA 94305-4115  
Affiliation: Center for the Study of Language and Information  
Abstract: The representation of documents and queries as vectors in a high-dimensional space is well-established in information retrieval [1]. This paper proposes to represent the semantics of words and contexts in a text as vectors. The dimensions of the space are words and the initial vectors are determined by the words occurring close to the entity to be represented which implies that the space has several thousand dimensions (words). This makes the vector representations (which are dense) too cumbersome to use directly. Therefore, dimensionality reduction by means of a singular value decomposition is employed. The paper analyzes the structure of the vector representations and applies them to word sense disambiguation and thesaurus induction. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Salton and M. J. McGill, </author> <title> Introduction to modern information retrieval. </title> <address> New York: </address> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: The approach is motivated by work on vector representations in information retrieval. In IR systems such as SMART and SIRE documents and queries are represented as vectors in term space <ref> [1] </ref>. The assumption is that two documents are similar to the extent that they contain the same words.
Reference: [2] <author> D. E. Rumelhart, J. L. McClelland, </author> <title> and the PDP research group, </title> <booktitle> Parallel Distributed Processing. </booktitle> <address> Cambridge MA: </address> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The algorithms from Mike Berry's SVD-PACK were used in this paper, mainly the Lanczos algorithm LAS2. As will be shown in section 4.3, the vector representations have the key properties of the distributed representations characteristic of parallel distributed processing <ref> [2] </ref>. They will therefore be referred to as sublexical representations in analogy to terms like "subsymbolic" and "subconceptual" in connec tionism. 2 Word Sense Disambiguation Word sense disambiguation is important for many areas of language processing.
Reference: [3] <author> P. Cheeseman, J. Kelly, M. Self, J. Stutz, W. Tay-lor, and D. Freeman, </author> <title> "AutoClass: A Bayesian classification system," </title> <booktitle> in Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <year> 1988. </year>
Reference-contexts: A less reliable, but automatic, method taken here is to cluster a training set of contexts, to assign senses to the clusters and to assign new occurrences the sense of the closest cluster. The clustering programs used are AutoClass <ref> [3] </ref> and Buckshot [4]. AutoClass is a Bayesian classification program based on the theory of finite mixtures. It determines the number of clusters automatically by imposing a penalty on each new cluster and thus counterbalancing the fact that more clusters will necessarily better account for the data.
Reference: [4] <author> D. Cutting, D. Karger, J. Pedersen, and J. Tukey, "Scatter-gather: </author> <title> A cluster-based approach to browsing large document collections," </title> <booktitle> in Proceedings of SIGIR'92, </booktitle> <year> 1992. </year>
Reference-contexts: A less reliable, but automatic, method taken here is to cluster a training set of contexts, to assign senses to the clusters and to assign new occurrences the sense of the closest cluster. The clustering programs used are AutoClass [3] and Buckshot <ref> [4] </ref>. AutoClass is a Bayesian classification program based on the theory of finite mixtures. It determines the number of clusters automatically by imposing a penalty on each new cluster and thus counterbalancing the fact that more clusters will necessarily better account for the data.
Reference: [5] <author> R. Gnanadesikan, </author> <title> Methods for Statistical Data Analysis of Multivariate Observations. </title> <address> New York: </address> <publisher> John Wiley & Sons, </publisher> <year> 1977. </year>
Reference-contexts: An average over several clusterings could be taken, but that would be time-consuming. A deterministic, less expensive method is therefore needed. Canonical Discriminant Analysis (CDA) or linear discrimination is such a method <ref> [5] </ref>. It finds the best weighting or linear combination of the dimensions of the space so that the ratio of the sum of between-group distances to the sum of the within-group distances is maximized. This task is slightly different from classification.
Reference: [6] <author> S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman, </author> <title> "Indexing by latent semantic analysis," </title> <journal> Journal of the American Society for Information Science, </journal> <volume> vol. 41, no. 6, </volume> <pages> pp. 391-407, </pages> <year> 1990. </year>
Reference-contexts: Further research is necessary to find out whether dimensions 100-200 are even better than 50-100. 5 Discussion The approach to semantic representation proposed here bears some similarity to Latent Semantic Indexing (LSI) in information retrieval in that a singular value decomposition is used <ref> [6] </ref>. However, there is an important difference: In LSI, the main purpose of the space reduction is to improve the quality of the representations, thereby achieving better performance. The initial term-by-document matrix is noisy because it contains many small counts which are inherently unreliable.
Reference: [7] <author> I. T. Jolliffe, </author> <title> Principal Component Analysis. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: There is a long tradition in the social sciences of using principal component analyses to understand variation in large data sets. For instance, a survey of "The Elderly at Home" with 20 variables is subjected to a principal component analysis in <ref> [7] </ref>. The first eleven principal components are then interpreted as corresponding to elderly people living alone vs. those that share accommodations with others etc.
Reference: [8] <author> D. Yarowsky, </author> <title> "Word-sense disambiguation using statistical models of Roget's categories trained on large corpora," </title> <booktitle> in Proceedings of Coling-92, </booktitle> <year> 1992. </year>
Reference-contexts: The only important information is the measure of similarity that can be obtained for any two words or contexts by computing their correlation coefficient. The disambiguation results achieved here compare favorably with those reported for other approaches. For instance, the methods in <ref> [8, 9] </ref> perform slightly better than the average of 92% in Table 1. However, they rely on thesauri and bilingual corpora. For many technical domains and foreign languages, thesauri or bilingual corpora are not available.
Reference: [9] <author> W. A. Gale, K. W. Church, and D. Yarowsky, </author> <title> "Using bilingual materials to develop word sense disambiguation methods," </title> <booktitle> in Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, </booktitle> <year> 1992. </year>
Reference-contexts: The only important information is the measure of similarity that can be obtained for any two words or contexts by computing their correlation coefficient. The disambiguation results achieved here compare favorably with those reported for other approaches. For instance, the methods in <ref> [8, 9] </ref> perform slightly better than the average of 92% in Table 1. However, they rely on thesauri and bilingual corpora. For many technical domains and foreign languages, thesauri or bilingual corpora are not available.
Reference: [10] <author> P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer, </author> <title> "Word-sense disambiguation using statistical methods," </title> <booktitle> in Proceedings of ACL 29, </booktitle> <year> 1991. </year>
Reference-contexts: Representations that are derived by means of a dimensionality reduction differ from other statistical approaches in that a small number of parameters (on the order of a few thousand) is estimated. Trigram-based models such as the one presented in <ref> [10] </ref> have to estimate millions or even billions of parameters. Even the largest corpus is not sufficient to estimate such a large number of parameters reliably.
Reference: [11] <author> D. B. Lenat and R. V. Guha, </author> <title> Building Large Knowledge-Based Systems. </title> <address> Reading MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The challenge is to encode all items of knowledge that may be relevant for tasks like disambiguation and to integrate them into a system that will respond appropriately. This goal has not been achieved yet and systems like Cyc <ref> [11] </ref> still seem far from coming close to it.
Reference: [12] <author> K. W. Church and P. Hanks, </author> <title> "Word association norms, mutual information and lexicography," </title> <booktitle> in Proceedings of ACL 27, </booktitle> <year> 1989. </year>
Reference-contexts: A host of recent papers on mutual information (for instance <ref> [12] </ref>) is witness to its importance in computational linguistics and lexicography. Still, even if semantic similarity is important, a more intricate set of lexical relations is needed for more ambitious natural language processing and linguistics, relations such as hyponymy or antonomy.
References-found: 12

