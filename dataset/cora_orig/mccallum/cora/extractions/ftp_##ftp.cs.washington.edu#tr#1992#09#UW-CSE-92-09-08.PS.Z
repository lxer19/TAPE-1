URL: ftp://ftp.cs.washington.edu/tr/1992/09/UW-CSE-92-09-08.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Techniques for File System Simulation  
Author: Chandramohan A. Thekkath, John Wilkes, and Edward D. Lazowska 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Technical Report 92-09-08 z Revised 07-07-94 ? y John Wilkes is with Hewlett Packard Laboratories, Palo Alto, CA. z This work also appears as Hewlett Packard Laboratories technical report HPL-92-131. ? This work will also appear in SoftwarePractice and Experience. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> AT&T. </author> <title> Unix System V AT&T C++ language system release 2.0. </title> <booktitle> Selected readings, </booktitle> <year> 1989. </year>
Reference: [2] <author> Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> PRESTO: A system for object-oriented parallel programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Our choice of the thread library was dictated by what was most conveniently available to us. Other packages such as PRESTO <ref> [2] </ref> could probably also be used with minimal modifications. Such coroutine packages typically provide a set of objects including lightweight threads, synchronization objects, and communication channels such as queues.
Reference: [3] <author> Roberta A. Bodnarchuk and Richard B. Bunt. </author> <title> A synthetic workload model for a distributed system file server. </title> <booktitle> In Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 50-59, </pages> <month> May </month> <year> 1991. </year> <month> 17 </month>
Reference-contexts: However, in order to be realistic, synthetic workload models tend to be elaborate, difficult to parameterize, and specific to a single environment. One sample of a synthetic NFS-workload generator <ref> [3] </ref> uses 24 parameters to describe the workloada wealth of detail that is not easy to gather. Instead, our approach is to use trace-driven workloads, but to extend their utility through a technique known as bootstrapping, which is described further in the next section.
Reference: [4] <author> Scott Carson and Sanjeev Setia. </author> <title> Optimal write batch size in log-structured file systems. </title> <booktitle> In Proceedings of the USENIX Workshop on File Systems, </booktitle> <pages> pages 79-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: A subsequent study [16] using synthetic workloads provided improved accuracy, but over-estimated the cost of the segment cleaner by comparison with later measurements using a more realistic workload.[17] Another group <ref> [4] </ref> looked at the effects of long running writes on read performance, an effect that had previously not been analyzed in detail.
Reference: [5] <author> Ann L. Chervenak and Randy H. Katz. </author> <title> Performance of a disk array prototype. </title> <booktitle> In Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 188-197, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Here, a simulation study [11] using a detailed disk model that included rotation timing effects produced results that were contrary to an earlier study [13] that did not. Another analytic model of a RAID controller <ref> [5] </ref> found that neglected factors such as contention within the various elements in the array controller caused actual performance to be noticeably worse than that predicted.
Reference: [6] <author> Frederick W. Clegg, Gary Shiu-Fan Ho, Steven R. Kusmer, and John R. Sontag. </author> <title> The HP-UX operating system on HP Precision Architecture computers. </title> <journal> Hewlett-Packard Journal, </journal> <volume> 37(12) </volume> <pages> 4-22, </pages> <month> December </month> <year> 1986. </year>
Reference: [7] <author> David J. DeWitt, Randy H. Katz, Frank Olken, L.D. Shapiro, Mike R. Stonebraker, and David Wood. </author> <title> Implementation techniques for main memory database systems. </title> <booktitle> In Proceedings of SIGMOD 1984, </booktitle> <pages> pages 1-8, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: MOTIVATION As processors, memories, and networks continue to speed up relative to secondary storage, file and disk systems have increasingly become the focus of attention. The Berkeley Log-structured File System (LFS),[17] Redundant Arrays of Independent Disks (RAID),[15] and log-based fault tolerant systems <ref> [7, 10] </ref> are some well-known examples of the newer innovative designs. Analysis of these systems has exposed many subtleties that affect performance.
Reference: [8] <author> Persi Diaconis and Bradley Efron. </author> <title> Computer-intensive methods in statistics. </title> <journal> Scientific American, </journal> <volume> 248(5) </volume> <pages> 116-130, </pages> <month> May </month> <year> 1983. </year>
Reference: [9] <author> Robert Geist and Stephen Daniel. </author> <title> A continuum of disk scheduling algorithms. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(1) </volume> <pages> 77-92, </pages> <month> February </month> <year> 1987. </year>
Reference: [10] <author> Robert B. Hagmann. </author> <title> A crash recovery scheme for a memory-resident database system. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 35(9) </volume> <pages> 839-843, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: MOTIVATION As processors, memories, and networks continue to speed up relative to secondary storage, file and disk systems have increasingly become the focus of attention. The Berkeley Log-structured File System (LFS),[17] Redundant Arrays of Independent Disks (RAID),[15] and log-based fault tolerant systems <ref> [7, 10] </ref> are some well-known examples of the newer innovative designs. Analysis of these systems has exposed many subtleties that affect performance.
Reference: [11] <author> Mark Holland and Garth A. Gibson. </author> <title> Parity declustering for continuous operation in redundant disk arrays. </title> <booktitle> In Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 23-35, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: The selection of a rebuild policy for a RAID disk array is one example of the need for detailed and complete disk models. Here, a simulation study <ref> [11] </ref> using a detailed disk model that included rotation timing effects produced results that were contrary to an earlier study [13] that did not.
Reference: [12] <author> Marshal Kirk McKusick, William N. Joy, Samuel J. Leffler, and Robert S. Fabry. </author> <title> A fast file system for UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-197, </pages> <month> August </month> <year> 1984. </year>
Reference: [13] <author> Richard R. Muntz and John C.S. Lui. </author> <title> Performance analysis of disk arrays under failure. </title> <booktitle> In Proceedings of the 16th Conference on Very Large Databases, </booktitle> <pages> pages 162-173, </pages> <year> 1990. </year>
Reference-contexts: The selection of a rebuild policy for a RAID disk array is one example of the need for detailed and complete disk models. Here, a simulation study [11] using a detailed disk model that included rotation timing effects produced results that were contrary to an earlier study <ref> [13] </ref> that did not. Another analytic model of a RAID controller [5] found that neglected factors such as contention within the various elements in the array controller caused actual performance to be noticeably worse than that predicted.
Reference: [14] <author> John K. Ousterhout and Fred Douglis. </author> <title> Beating the I/O bottleneck: A case for log-structured file systems. </title> <journal> Operating System Review, </journal> <volume> 23(1) </volume> <pages> 11-27, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: The Log-structured File System is a file system whose performance characterization has evolved as more detailed models and simulations have been developed. We use it here as an example of how this process works. The earliest study <ref> [14] </ref> predicted a ten-fold improvement in performance based on a simple model that was based on micro-benchmarks.
Reference: [15] <author> David Patterson, Garth Gibson, and Randy Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> ACM SIGMOD 88, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference: [16] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The LFS storage manager. </title> <booktitle> In Proceedings of the Summer 1990 USENIX Conference, </booktitle> <pages> pages 315-324, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: We use it here as an example of how this process works. The earliest study [14] predicted a ten-fold improvement in performance based on a simple model that was based on micro-benchmarks. A subsequent study <ref> [16] </ref> using synthetic workloads provided improved accuracy, but over-estimated the cost of the segment cleaner by comparison with later measurements using a more realistic workload.[17] Another group [4] looked at the effects of long running writes on read performance, an effect that had previously not been analyzed in detail.
Reference: [17] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1) </volume> <pages> 26-52, </pages> <month> February </month> <year> 1992. </year>
Reference: [18] <author> Chris Ruemmler and John Wilkes. </author> <title> Unix disk access patterns. </title> <booktitle> In Proceedings of the Winter 1993 USENIX Conference, </booktitle> <pages> pages 405-420, </pages> <month> January </month> <year> 1993. </year>
Reference: [19] <author> Chris Ruemmler and John Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March </month> <year> 1994. </year> <month> 18 </month>
Reference-contexts: The total code required to achieve this level of accuracy is modesta little over 3000 lines of C++. The particular disk model that we describe here has been extensively used in other studies. A separate paper <ref> [19] </ref> contains quantitative information of how different portions of the model contribute to its accuracy and how it compares with typical simple models. Undoubtedly, an accurate model like ours is more complicated than a simpler, less accurate, model.
Reference: [20] <author> P. H. Seaman, R. A. Lind, and T. L. Wilson. </author> <title> On teleprocessing system design: Part IV: An analysis of auxiliary-storage activity. </title> <journal> IBM Systems Journal, </journal> <volume> 5(3) </volume> <pages> 158-170, </pages> <year> 1966. </year>
Reference: [21] <author> Margo Seltzer, Keith Bostic, Marshall Kirk McKusick, and Carl Staelin. </author> <title> An implementation of a log-structured file system for UNIX. </title> <booktitle> In Proceedings of Winter 1993 USENIX, </booktitle> <pages> pages 307-326, </pages> <month> January </month> <year> 1993. </year>
References-found: 21

