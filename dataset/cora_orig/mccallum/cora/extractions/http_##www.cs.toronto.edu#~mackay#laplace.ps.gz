URL: http://www.cs.toronto.edu/~mackay/laplace.ps.gz
Refering-URL: http://www.cs.toronto.edu/~mackay/README.html
Root-URL: http://www.cs.toronto.edu
Email: mackay@mrao.cam.ac.uk  
Title: Choice of Basis for Laplace Approximation  
Author: David J.C. MacKay 
Date: February 23rd 1998  1, October 1998  
Note: Submitted to Machine Learning October 14th 1996 Accepted pending minor modifications  Revised version completed May 11th 1998 Published Volume 33, No.  
Address: Cambridge, CB3 0HE, United Kingdom.  
Affiliation: Cavendish Laboratory,  
Abstract: Maximum a posteriori optimization of parameters and the Laplace approximation for the marginal likelihood are both basis-dependent methods. This note compares two choices of basis for models parameterized by probabilities, showing that it is possible to improve on the traditional choice, the probability simplex, by transforming to the `softmax' basis. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bridle, J. S. </author> <year> (1989). </year> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. </title> <editor> In Fougelman-Soulie, F. and Herault, J., editors, Neuro-computing: </editor> <booktitle> algorithms, architectures and applications. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: I would argue that the `1' terms in the traditional posterior probability are artefacts of the choice of basis. 2 A change of basis I suggest that maximum a posteriori parameter estimation and Laplace approximations would be better conducted in the `softmax' representation (widely used in neural networks <ref> (Bridle, 1989) </ref>) in which the parameters p are replaced by parameters a: p i (a) = P : (9) [Please do not confuse p (a), the function defined in equation (9), with the probability density P (a).] The probability vector p has I components but only I 1 degrees of freedom;
Reference: <author> Chickering, D. M. and Heckerman, D. </author> <year> (1996). </year> <title> Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables. </title> <note> Microsoft Research Technical Report MSR-TR-96-08. </note>
Reference-contexts: The traditional MAP method for such models (Lee and Gauvain, 1993) is to maximize the posterior probability of the parameters p, and the traditional Laplace method for such a model is, after maximizing in the p basis, to make the Gaussian approximation in the same basis <ref> (Chickering and Heckerman, 1996) </ref>.
Reference: <author> Gelman, A. </author> <year> (1996). </year> <title> Bayesian model-building by pure thought: Some principles and examples. </title> <journal> Statistica Sinica, </journal> <volume> 6 </volume> <pages> 215-232. </pages>
Reference: <author> Gelman, A., Carlin, J., Stern, H., and Rubin, D. </author> <year> (1995). </year> <title> Bayesian Data Analysis. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Jeffreys, H. </author> <year> (1939). </year> <title> Theory of Probability. </title> <institution> Oxford Univ. </institution> <note> Press. 3rd edition reprinted in paperback 1985. </note>
Reference: <author> Lee, C. H. and Gauvain, J. L. </author> <year> (1993). </year> <title> Speaker adaptation based on MAP estimation of HMM parameters. </title> <booktitle> In IEEE Proceedings, </booktitle> <pages> pages II-558-561. </pages> <note> 9 Lindley, </note> <author> D. V. </author> <year> (1980). </year> <title> Approximate Bayesian methods. </title> <editor> In Bernardo, J. M., DeGroot, M. H., Lindley, D. V., and Smith, A. F. M., editors, </editor> <booktitle> Bayesian Statistics, </booktitle> <pages> pages 223-237. </pages> <publisher> Valencia University Press, Valencia. </publisher>
Reference-contexts: The traditional MAP method for such models <ref> (Lee and Gauvain, 1993) </ref> is to maximize the posterior probability of the parameters p, and the traditional Laplace method for such a model is, after maximizing in the p basis, to make the Gaussian approximation in the same basis (Chickering and Heckerman, 1996).
Reference: <author> MacKay, D. J. C. </author> <year> (1992). </year> <title> A practical Bayesian framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 448-472. </pages>
Reference: <author> MacKay, D. J. C. </author> <year> (1997). </year> <title> Ensemble learning for hidden Markov models. </title> <note> Available from http://wol.ra.phy.cam.ac.uk/. </note>
Reference-contexts: And deterministic Bayesian approximations that are basis independent are under development <ref> (MacKay, 1997) </ref>. But if MAP methods are used, this paper offers a way of evaluating marginal likelihoods which satisfies these two desiderata: 1. We can make a Laplace approximation for any Dirichlet priors and any amount of data.
Reference: <author> MacKay, D. J. C. and Peto, L. </author> <year> (1995). </year> <title> A hierarchical Dirichlet language model. </title> <booktitle> Natural Language Engineering, </booktitle> <volume> 1(3) </volume> <pages> 1-19. </pages>
Reference: <author> Neal, R. M. </author> <year> (1992). </year> <title> Bayesian mixture modelling. </title> <editor> In Smith, C., Erickson, G., and Neudorfer, P., editors, </editor> <title> Maximum Entropy and Bayesian Methods, </title> <booktitle> Seattle 1991, </booktitle> <pages> pages 197-211, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference-contexts: bins i (where both methods perform well) the traditional approximation can be a little more accurate. 4 Discussion This paper's aim is not to advocate the use of Laplace approximations; indeed a good case can be made for using other methods such as Markov chain Monte Carlo (see, for example, <ref> (Neal, 1992) </ref>). And deterministic Bayesian approximations that are basis independent are under development (MacKay, 1997). But if MAP methods are used, this paper offers a way of evaluating marginal likelihoods which satisfies these two desiderata: 1.
Reference: <author> O'Hagan, A. </author> <year> (1994). </year> <title> Bayesian Inference, </title> <journal> volume 2B of Kendall's Advanced Theory of Statistics. </journal>
Reference: <institution> Edward Arnold. </institution>
Reference: <author> Ripley, B. D. </author> <year> (1996). </year> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge. </publisher>
Reference: <author> Smith, A. and Spiegelhalter, D. </author> <year> (1980). </year> <title> Bayes factors and choice criteria for linear models. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 42(2) </volume> <pages> 213-220. </pages> <note> August 25, 1998 | Version 3.6 10 </note>
References-found: 14

