URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/94/tr1214.ps.Z
Refering-URL: http://www.cs.wisc.edu/~arch/uwarch/tech_reports/tech_reports.html
Root-URL: 
Email: wwt@cs.wisc.edu  
Title: Tempest and Typhoon: User-Level Shared Memory  
Author: Steven K. Reinhardt, James R. Larus, and David A. Wood 
Address: 1210 West Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department University of WisconsinMadison  
Abstract: Future parallel computers must efficiently execute not only hand-coded applications but also programs written in high-level, parallel programming languages. Todays machines limit these programs to a single communication paradigm, either message-passing or shared-memory, which results in uneven performance. This paper addresses this problem by defining an interface, Tempest, that exposes low-level communication and memory-system mechanisms so programmers and compilers can customize policies for a given application. Typhoon is a proposed hardware platform that implements these mechanisms with a fully-programmable, user-level processor in the network interface. We demonstrate the utility of Tempest with two examples. First, the Stache protocol uses Tempests fine-grain access control mechanisms to manage part of a processors local memory as a large, fully-associative cache for remote data. We simulated Typhoon on the Wisconsin Wind Tunnel and found that Stache running on Typhoon performs comparably (-30%) to an all-hardware Dir N NB cache-coherence protocol for five shared-memory programs. Second, we illustrate how programmers or compilers can use Tempests exibility to exploit an applications sharing patterns with a custom protocol. For the EM3D application, the custom protocol improves performance up to 35% over the all-hardware protocol. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera Computer System. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 16, </pages> <month> June </month> <year> 1990. </year>
Reference: [2] <author> Andrew W. Appel and Kai Li. </author> <title> Virtual Memory Primitives for User Programs. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 96107, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The access-control mechanisms must permit reads and writes to a local datum, permit reads but not writes, prevent both reads and writes, and transfer control to user-level code on an access violation. Virtual memory systems typically provide this form of access control <ref> [2] </ref>. The coarse granularity of their page-based mechanisms, however, is a poor match for many applications. In addition, access to page tables is typically an operating system privilege, so user-level changes incur a system call. User-level shared memory requires access control that is both fine grained and fast. <p> However, the single bit limits them to two states, much like the Wisconsin Wind Tunnel [6]. Typhoon is, we believe, unique in using a reverse-TLB to provide tags for a commodity processor. Tempests user-level memory management interface is similar to Appel and Lis user-level primitives <ref> [2] </ref>. Both provide mechanisms to support distributed shared memory [29]. The differences arise from Tempests fine-grain access control. The Stache memory-allocation policy bears strong similarities to distributed shared memory systems [29].
Reference: [3] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS Parallel Benchmarks. </title> <type> Technical Report RNR-91-002 Revision 2, </type> <institution> Ames Research Center, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: This is nearly correct for the simple integer core of the NP, but gives the primary CPU a big boost since it executes many oating point operations in the applications. We evaluated these two systems using five benchmarks: Appbt, a locally-parallelized version of the NAS benchmark <ref> [3] </ref>; Barnes, MP3D, and Ocean from the SPLASH suite [40]; and a transparent shared-memory version of EM3D (discussed in Section 4). Appbt is a computational uid dynamics program, which solves multiple independent systems of non-diagonally dominant, block tridiago-nal equations with a 5x5 block size.
Reference: [4] <author> J.E. Barnes and P. Hut. </author> <title> A Hierarchical O(N log N) Force Calculation Algorithm. </title> <booktitle> Nature, </booktitle> <address> 324(4):446449, </address> <month> December </month> <year> 1986. </year>
Reference: [5] <author> David Chaiken, John Kubiatowics, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The performance gain does not justify the software maintenance overhead. 5 The Stache replication policy is independent of the coherence protocol. Our default coherence protocol is similar to the LimitLESS protocol <ref> [5] </ref>, except that it is implemented entirely in software rather than partially in hardware. Specifically, the protocol preallocates 64 bits per cache blockto minimize bitfield operations, it allocates two bytes for state and six one-byte pointers.
Reference: [6] <author> Albert Chang and Mark F. Mergen. </author> <title> 801 Storage: Architecture and Programming. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1):2850, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: The IBM 801 and RS/6000 support fine-grain access control by providing a lock bit per 128 bytes in their TLB entries. However, the single bit limits them to two states, much like the Wisconsin Wind Tunnel <ref> [6] </ref>. Typhoon is, we believe, unique in using a reverse-TLB to provide tags for a commodity processor. Tempests user-level memory management interface is similar to Appel and Lis user-level primitives [2]. Both provide mechanisms to support distributed shared memory [29]. The differences arise from Tempests fine-grain access control.
Reference: [7] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 262273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Measurements of five benchmarks indicate that Stache performs comparably (-30%) to a conventional, all-hardware Dir N NB shared-memory system, despite Typhoons greater exibility. Furthermore, we show how customizing shared memory semantics to exploit sharing patterns can improve performance significantly. The EM3D application <ref> [7] </ref> runs up to 35% faster with a customized user-level protocol than on the all-hardware shared-memory system. In the next section, we present the user-level communication and memory management mechanisms that comprise the Tempest interface. Section 3 shows how these mechanisms support transparent shared memory in the Stache protocol. <p> Stache currently implements a simple FIFO replacement policy, since replacements are rare. 4 Custom User-level Shared Memory To illustrate the benefits of a user-level, application-specific memory system, we implemented a new coherence protocol for the irregularly-structured EM3D application. EM3D models electromagnetic wave propagation through three-dimensional objects <ref> [7] </ref>. This programs principle data structure is a bipartite graph, in which E nodes represent electric field values and H nodes represent magnetic field values. <p> Of course, the simple EM3D application could also be implemented efficiently with pure message passing, by a software inspection step that explicitly allocates space for remote nodes and builds an update list <ref> [7] </ref>. This approach is feasible because the graph is static and the inspector overhead can amortized over many iterations. <p> Furthermore, inspector-executor schemes are complex to implement [22,38,37]. Finally, a custom coherence protocol does not require extensive program modifications, unlike the software caching and updating in the message-passing version <ref> [7] </ref>. Although any protocol could be implemented in hardware (or system software [23]), system designers cannot anticipate the full range of protocols that programmers and compilers will devise.
Reference: [8] <author> David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain Parallelism with Minimal Hardware Support: A Compiler-Controlled Thread Abstract Machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 164175, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Typhoons low-overhead messaging draws heavily on message-driven systems for fine-grain computations [8,9,33,34]. The NPs message sending interface closely follows the J-Machines [10], but uses memory-mapped loads and stores instead of integrated instructions, and provides optimized block transfers. The NPs receiving interface is an active message model <ref> [8] </ref>, in which the sender explicitly specifies the address of the handler. Rather than queuing the message in memory [10] or requiring polling by the primary processor [8], our handlers are directly invoked on a separate message processor, much like *T FIGURE 4. <p> The NPs receiving interface is an active message model <ref> [8] </ref>, in which the sender explicitly specifies the address of the handler. Rather than queuing the message in memory [10] or requiring polling by the primary processor [8], our handlers are directly invoked on a separate message processor, much like *T FIGURE 4.
Reference: [9] <author> William J. Dally, Andrew Chien, Stuart Fiske, Waldemar Horwat, John Keen, Michael Larivee, Rich Nuth, Scott Wills, Paul Carrick, and Greg Flyer. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <editor> In G. X. Ritter, editor, </editor> <booktitle> Proc. Information Processing 89. </booktitle> <publisher> Elsevier North-Holland, Inc., </publisher> <year> 1989. </year>
Reference: [10] <author> William J. Dally, J. A. Stuart Fiske, John S. Keen, Richard A. Lethin, Michael D. Noakes, Peter R. Nuth, Roy E. Davison, and Gregory A. Flyer. </author> <title> The Message-Driven Processor: A Multicomputer Processing Node with Efficient Mechanisms. </title> <journal> IEEE Micro, </journal> <volume> 12(2):2339, </volume> <month> April 12 </month>
Reference-contexts: The Tempest interface generalizes these mechanisms and Typhoon provides first-class hardware support for them. Typhoons low-overhead messaging draws heavily on message-driven systems for fine-grain computations [8,9,33,34]. The NPs message sending interface closely follows the J-Machines <ref> [10] </ref>, but uses memory-mapped loads and stores instead of integrated instructions, and provides optimized block transfers. The NPs receiving interface is an active message model [8], in which the sender explicitly specifies the address of the handler. Rather than queuing the message in memory [10] or requiring polling by the primary <p> sending interface closely follows the J-Machines <ref> [10] </ref>, but uses memory-mapped loads and stores instead of integrated instructions, and provides optimized block transfers. The NPs receiving interface is an active message model [8], in which the sender explicitly specifies the address of the handler. Rather than queuing the message in memory [10] or requiring polling by the primary processor [8], our handlers are directly invoked on a separate message processor, much like *T FIGURE 4.
Reference: [11] <author> William J. Dally and D. Scott Wills. </author> <title> Universal Mechanism for Concurrency. </title> <booktitle> In PARLE 89: Parallel Architectures and Languages Europe, </booktitle> <publisher> page ? Springer-Verlag, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: Tagged memory, which Typhoon uses for fine-grain access control, has been implemented in many earlier machines. Machines for symbolic languages, such as Lisp, use word-granularity tags to support run-time typing [32,43]. Some parallel machines provide tags for fine-grain synchronization [1,5]. The word-granularity tags in the J-machine support shared-memory semantics <ref> [11] </ref>, as well as other functions, but do not provide the ReadOnly tag necessary for replication. The IBM 801 and RS/6000 support fine-grain access control by providing a lock bit per 128 bytes in their TLB entries.
Reference: [12] <author> Matthew I. Frank and Mary K. Vernon. </author> <title> A Hybrid Shared Memory/ Message Passing Parallel Machine. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing (Vol. I Architecture), </booktitle> <pages> pages 232236, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Recent research has begun focusing on supporting multiple paradigms in a single computerby integrating shared memory and message passingand allowing a compiler to select the model appropriate for a program or data structure [24]. Frank and Vernon proposed message-passing mechanisms for shared memory systems <ref> [12] </ref>. The MIT Alewife system handles some cache coherence events in software and allows shared-memory programs to send explicit messages . Their preliminary results show that some run-time operations, for example, task creation, are more efficiently implemented with explicit messages than shared memory [23].
Reference: [13] <author> Anoop Gupta, John Hennessy, Kourosh Gharachorloo, Todd Mowry, and Wolf-Dietrich Weber. </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254263, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Ocean is a hydrodynamic simulation of a two-dimensional cross-section of a cuboidal ocean basin. Each application was simulated for two data sets, one significantly larger than the other (see Table 3). The smaller data sets are scaled for a 4 Kbyte cache, as advocated by Gupta, et al. <ref> [13] </ref>, and fit entirely in the larger caches. relative execution time of Typhoon/Stache versus Dir N NB (application execution time on Typhoon/Stache over its time on Dir N NB).
Reference: [14] <author> Erik Hagersten. </author> <title> Toward Scalable Cache Only Memory Architectures. </title> <type> Technical report, </type> <institution> The Royal Institute of Technology Swedish Institute of Computer Science, </institution> <month> October </month> <year> 1992. </year> <title> Stockholm, SwedenPh.D. </title> <type> Thesis, </type> <institution> Swedish Institute of Computer Science Dissertation Series 08. </institution>
Reference-contexts: However, COMA machines use complex hardware and make all of local memory into a cache 11 [14,21], while Stache uses much simpler hardware and only as much of the local memory as an application chooses to use. The Swedish Institute of Computer Scien-cess Data Diffusion Machine (DDM) <ref> [14] </ref> and Kendall Square Researchs KSR-1 [21] use coherence protocols fixed in hardware. Stache uses Typhoons tag mechanisms to accelerate user-level software coherence algorithms. DDM and KSR-1 are both hierarchical machines, with hierarchical directory structures. Stache is at, like Sten-strom, et al.s proposed COMA-F machine [42].
Reference: [15] <author> John Hennesey. </author> <title> FLASH. </title> <booktitle> ARPA HPC Software Fall PI Meeting, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: Their preliminary results show that some run-time operations, for example, task creation, are more efficiently implemented with explicit messages than shared memory [23]. The Stanford FLASH goes further and replaces a hard-wired directory controller with a programmable controller that implements shared memory by explicitly sending messages <ref> [15] </ref>. However, neither Alewife nor FLASH provides protected, user-level interfaces for these mechanisms, which limits programmers and compilers to a predefined set of system-provided policies. 8 Conclusions This paper describes a new approach to designing parallel computers that is based on user-level software control of the shared address space.
Reference: [16] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A Tightly-Coupled Processor-Network Interface. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 111122, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: On the receiver, the first data word is interpreted as the receive handler PC, as in Active Messages [45]. The receive handler must pull the remainder of the message from the receive queue. Scheduling on the NP is performed by a hardware-assisted dispatch loop <ref> [16] </ref>. The dispatch hardware constructs a handler PC in a dedicated register either by taking the first word of an incoming message or by using status bits as an offset from a user-specified base. Handlers can be prioritized or disabled via a user-accessible control register.
Reference: [17] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference: [18] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(4):300318, </volume> <month> November </month> <year> 1993. </year> <note> Preliminary version appeared in ASPLOS V, </note> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Thus in each iteration, a remote e_node (or h_node) will be fetched, cached, and invalidated, which requires at least four messages (i.e., request, response, invalidate, and acknowledge). Prefetching can hide communication latency, but does not reduce the message traffic. Check_in operations, which ush a block from a processors cache <ref> [18] </ref>, cut communication and latency by replacing the invalidation/acknowledgment with an asynchronous notification, but cannot attain the minimum of one message. In addition, these operations introduce additional computation that increases program overhead.
Reference: [19] <author> Mark D. Hill and Alan Jay Smith. </author> <title> Evaluating Associativity in CPU Caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38(12):16121630, </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: Stache uses part of each processing nodes local memory to replicate remote data. In effect, Stache uses this local memory as a large second (or third-) level, fully-associative data cache, which eliminates much of the network traffic caused by capacity and conict misses in smaller hardware caches <ref> [19] </ref>. For applications in which a processor manipulates data too large to fit in the hardware cache, but small enough to fit in local memory, Stache offers a large advantage over conventional directory-based shared-TABLE 1. Operations on tagged memory blocks.
Reference: [20] <author> Intel Corporation. </author> <title> Paragon Technical Summary. Intel Supercomputer Systems Division, </title> <year> 1993. </year>
Reference-contexts: Unfortunately, no consensus has emerged on the communication modelshared memory or message pass-ingfor parallel languages. Current parallel machines take an all-or-nothing approach to providing a shared address space. Message-passing machines, such as the Thinking Machines CM-5 [44] and Intel Paragon <ref> [20] </ref>, have no hardware support, so compilers for these machines synthesize a shared address space by generating code that copies values between processors in messages. In the best case, this approach performs well and efficiently uses a machines memory and communications network. <p> However, the CS-2 is optimized for relatively long messages and provides no fine-grain access control support. The Intel Paragon also provides a message processor, but uses a standard i860 CPU and a passive network device rather than tightly integrating the two <ref> [20] </ref>. Tagged memory, which Typhoon uses for fine-grain access control, has been implemented in many earlier machines. Machines for symbolic languages, such as Lisp, use word-granularity tags to support run-time typing [32,43]. Some parallel machines provide tags for fine-grain synchronization [1,5].
Reference: [21] <institution> Kendall Square Research. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: Unfortunately, the approach relies on static program analysis and performance degrades dramatically when a compiler (or programmer) cannot fully analyze a program. On the other hand, shared-memory machines, such as the Kendall Square KSR-1 <ref> [21] </ref> and Stanford DASH [27], implement cache-coherent shared-memory policies and mechanisms entirely in hardware. <p> The Swedish Institute of Computer Scien-cess Data Diffusion Machine (DDM) [14] and Kendall Square Researchs KSR-1 <ref> [21] </ref> use coherence protocols fixed in hardware. Stache uses Typhoons tag mechanisms to accelerate user-level software coherence algorithms. DDM and KSR-1 are both hierarchical machines, with hierarchical directory structures. Stache is at, like Sten-strom, et al.s proposed COMA-F machine [42].
Reference: [22] <author> Charles Koelbel and Piyush Mehrotra. </author> <title> Compiling Global Name-Space Parallel Loops for Distributed Execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4):440451, </volume> <month> October </month> <year> 1991. </year>
Reference: [23] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 5463, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Furthermore, transferring bulk data via explicit messages is more efficient than using shared memory <ref> [23] </ref>. In Tempest, a processor initiates a bulk data transfer much like it would start a conventional DMA transaction, by specifying virtual addresses on both source and destination nodes. The transfer executes asynchronously with the computation thread. <p> Furthermore, inspector-executor schemes are complex to implement [22,38,37]. Finally, a custom coherence protocol does not require extensive program modifications, unlike the software caching and updating in the message-passing version [7]. Although any protocol could be implemented in hardware (or system software <ref> [23] </ref>), system designers cannot anticipate the full range of protocols that programmers and compilers will devise. <p> The MIT Alewife system handles some cache coherence events in software and allows shared-memory programs to send explicit messages . Their preliminary results show that some run-time operations, for example, task creation, are more efficiently implemented with explicit messages than shared memory <ref> [23] </ref>. The Stanford FLASH goes further and replaces a hard-wired directory controller with a programmable controller that implements shared memory by explicitly sending messages [15].
Reference: [24] <author> James R. Larus. </author> <title> Compiling for Shared-Memory and Message-Passing Computers. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 1(4):?, </volume> <month> December </month> <year> 1993. </year> <note> To appear. </note>
Reference-contexts: Although these machines share a common hardware base with message-passing machines (workstation-like nodes and point-to-point message passing), compilers for shared-memory machines have been constrained to use memory loads and stores for communication, even when static analysis could identify better approaches <ref> [24] </ref>. This paper describes Tempest and Typhoon. Tempest is an interface that permits programmers and compilers to use hardware communication facilities directly and to modify the semantics and performance of shared-memory operations. <p> By eliminating most synchronization and all invalidation traffic, the user-level coherence code attains near-minimum communication. In effect, this approach combines the communication efficiency of message passing with the low overhead and programming simplicity of shared memory <ref> [24] </ref>. Of course, the simple EM3D application could also be implemented efficiently with pure message passing, by a software inspection step that explicitly allocates space for remote nodes and builds an update list [7]. <p> Stache is at, like Sten-strom, et al.s proposed COMA-F machine [42]. Recent research has begun focusing on supporting multiple paradigms in a single computerby integrating shared memory and message passingand allowing a compiler to select the model appropriate for a program or data structure <ref> [24] </ref>. Frank and Vernon proposed message-passing mechanisms for shared memory systems [12]. The MIT Alewife system handles some cache coherence events in software and allows shared-memory programs to send explicit messages .
Reference: [25] <author> Alvin R. Lebeck and David A. Wood. Fast-Cache: </author> <title> A New Abstraction for Memory System Simulation. </title> <institution> Univ. of Wisconsin Technical Report. </institution>
Reference-contexts: The Stache message and fault handlers are all written in C++ and compiled using gcc. Unaltered shared-memory programs are simply re-linked with the Stache runtime library. We use a version of Fast-Cache <ref> [25] </ref> to rewrite executables with instrumentation code that calculates instruction times, implements the NP special operations, and simulates the data caches and TLBs on both the primary CPU and the NP.
Reference: [26] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fifth ACM Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: Typhoon system architecture. Level 2 caches are optional and not present in simulation. face processor (NP)that connects to the shared bus to the network, as illustrated in Figure 2. Typhoons network architecture is based on that of the Thinking Machines CM-5 <ref> [26] </ref>, but with a larger maximum packet payload (twenty 32-bit words rather than the CM-5s five). The only aspects of the network that are significant in this context are that it provides two independent virtual networks for deadlock avoidance and that it can be context-switched between user processes. <p> There are two separate issues. First, the system cannot guarantee that every user protocol is deadlock-free, but must ensure that deadlock in one user application does not impact other users or the operating system. This is dealt with by context-switching the buffers in the network, as in the CM-5 <ref> [26] </ref>. Second, the system must provide sufficient support that users can write efficient deadlock-free protocols. Typhoon addresses this through a combination of mechanisms.
Reference: [27] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3):6379, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Unfortunately, the approach relies on static program analysis and performance degrades dramatically when a compiler (or programmer) cannot fully analyze a program. On the other hand, shared-memory machines, such as the Kendall Square KSR-1 [21] and Stanford DASH <ref> [27] </ref>, implement cache-coherent shared-memory policies and mechanisms entirely in hardware.
Reference: [28] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH Prototype: Logic Overhead and Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1):4161, </volume> <month> January </month> <year> 1993. </year>
Reference-contexts: msg sent + 11 if block sent Typhoon Only NP TLB, RTLB 64 ent., fully assoc., FIFO repl. (R)TLB miss 25 cycles NP D-cache 16 Kbytes, 2-way assoc NP I-cache 8 Kbytes, direct-mapped 9 cessing nodes and use latency parameters, listed in Table 2, loosely based on the DASH prototype <ref> [28] </ref>. The network latency is probably optimistic for future systems, but the low value will tend to favor Dir N NB by making Typhoons overhead relatively larger. Our simulation of Typhoon is accurate enough to run SPARC binaries for both the primary CPU and the NP.
Reference: [29] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4):321359, </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: Typhoon is, we believe, unique in using a reverse-TLB to provide tags for a commodity processor. Tempests user-level memory management interface is similar to Appel and Lis user-level primitives [2]. Both provide mechanisms to support distributed shared memory <ref> [29] </ref>. The differences arise from Tempests fine-grain access control. The Stache memory-allocation policy bears strong similarities to distributed shared memory systems [29]. Staches default page location algorithm is similar to IVYs fixed distributed manager algorithm, where pages are assigned round-robin and the home nodes never change. <p> Tempests user-level memory management interface is similar to Appel and Lis user-level primitives [2]. Both provide mechanisms to support distributed shared memory <ref> [29] </ref>. The differences arise from Tempests fine-grain access control. The Stache memory-allocation policy bears strong similarities to distributed shared memory systems [29]. Staches default page location algorithm is similar to IVYs fixed distributed manager algorithm, where pages are assigned round-robin and the home nodes never change. However, Stache also allows pages to be allocated on specific nodes and provides support to allow explicit page migration.
Reference: [30] <author> Meiko World Inc. </author> <title> Computing Surface 2: Overview Documentation Set, </title> <year> 1993. </year>
Reference-contexts: Thus an NP page fault is a user programming error that causes program termination. An alternate solution, taken in the Meiko CS-2 <ref> [30] </ref>, NACKs the message that caused the fault, and brings in the page. However, a higher-level protocol must resend the message after the page-in completes. We expect to experiment with a combination of these schemes. User-level handlers also complicate deadlock avoidance. There are two separate issues. <p> The Thinking Machines CM-5 reduces message latency by mapping its interface at user-level, thereby eliminating the need for system calls. The Meiko CS-2 is similar to Typhoon since its tightly integrated network processor is separate from the primary CPU <ref> [30] </ref>. However, the CS-2 is optimized for relatively long messages and provides no fine-grain access control support. The Intel Paragon also provides a message processor, but uses a standard i860 CPU and a passive network device rather than tightly integrating the two [20].
Reference: [31] <author> Sun Microsystems. </author> <title> The SPARC Architecture Manual (Version 8), </title> <month> December </month> <year> 1990. </year>
Reference-contexts: Typhoon consists of homogeneous, workstation-like processor/memory nodes connected by a high-bandwidth, low-latency point-to-point network (see Figure 1). For economic reasons, commodity components are used for the processor, bus, memory controller, and DRAM. Specifically, each Typhoon node has a SuperSPARC processor connected to a level-2 MBus <ref> [31] </ref>. 1 The one custom component is the network interface devicethe network inter 1. However, the basic design should work with any coherent bus using an ownership protocol and cache-to-cache transfers. Processor N-1 L1 $ Processor 0 NP DRAM CPU Network L1 $ NP DRAM CPU FIGURE 1. <p> The NP and primary CPU both implement versions of the SPARC 8 reference MMU <ref> [31] </ref>. While the primary processor and the NP may use separate page tables, they share a single table in our current implementation.
Reference: [32] <author> David A. Moon. </author> <title> Symbolics Architecture. </title> <journal> IEEE Computer, </journal> <volume> 20(1):43 52, </volume> <month> January </month> <year> 1987. </year>
Reference: [33] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A Multithreaded Massively Parallel Architecture. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: EM3D Update Protocol Performance using the large data set (192,000 nodes, degree 15). 0 10 20 30 40 50 Percent non-local edges 0 20 40 60 Cycles per edge Typhoon/Stache Dir N NB Typhoon/Update <ref> [33] </ref>. However, Typhoon provides a single, general-purpose message processor per node, not a pair of specialized co-processors like *T. While the message models of these machines are similar, the proposed implementations differ. <p> While the message models of these machines are similar, the proposed implementations differ. The fine-grain research machines tightly integrated their network interfaces into the primary processor [34,9]. *T multiplexes its logically separate coprocessors on a single RISC-like processor <ref> [33] </ref>. Commercial machines typically implement their network interfaces as a passive memory-mapped device [26,20]. The Thinking Machines CM-5 reduces message latency by mapping its interface at user-level, thereby eliminating the need for system calls.
Reference: [34] <author> Gregory M. Papadopoulos and David E. Culler. Monsoon: </author> <title> An Explicit Token Store Architecture. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 8291, </pages> <month> May </month> <year> 1990. </year>
Reference: [35] <author> Steven K. Reinhardt, Babak Falsafi, and David A. Wood. </author> <title> Kernel Support for the Wisconsin Wind Tunnel. </title> <booktitle> In Usenix Workshop on Microkernels and Other Kernel Architectures, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: The NP and primary CPU both implement versions of the SPARC 8 reference MMU [31]. While the primary processor and the NP may use separate page tables, they share a single table in our current implementation. The operating system interface is similar to that of <ref> [35] </ref>. 5.4 Fine-Grain Access Control As described in Section 2.4, the fine-grain access-control model provides access tags on memory blocks and defines nine operations on these blocks. In Typhoon, the read and write operations (the tag-checked accesses) correspond to primary CPU cacheable loads and stores.
Reference: [36] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4860, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Protection is maintained by running the network interface processor in user mode and translating all addresses through a standard translation lookaside buffer (TLB). We have implemented a virtual prototype of Typhoon using a modified version of the Wisconsin Wind Tunnel <ref> [36] </ref>. Existing shared-memory programs only need to be linked with the Stache library to run on Typhoon. Measurements of five benchmarks indicate that Stache performs comparably (-30%) to a conventional, all-hardware Dir N NB shared-memory system, despite Typhoons greater exibility. <p> of the custom protocol increases as the fraction of edges that connect remote nodes is increased so that at 50% remote edges, the custom protocol outperforms Dir N NB by 35%. 7 Related Work The Tempest interface and Typhoon implementation evolved from our previous work on the Wisconsin Wind Tunnel <ref> [36] </ref>, a parallel simulation system that runs on a Thinking Machines CM-5. The Wisconsin Wind Tunnel models cache-coherent shared-memory systems with a Stache-like caching scheme that synthesizes fine-grain access control from the CM-5s error-correcting code (ECC) bits.
Reference: [37] <author> Joel Saltz, Kathleen Crowley, Ravi Mirchandaney, and Harry Berryman. </author> <title> Run-Time Scheduling and Execution of Loops on Message Passing Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8:303312, </volume> <year> 1990. </year>
Reference: [38] <author> Joel H. Saltz, Ravi Mirchandaney, and Kay Crowley. </author> <title> Run-Time Parallelization and Scheduling of Loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5):603612, </volume> <month> May </month> <year> 1991. </year>
Reference: [39] <author> Jaswinder Pal Singh, Truman Joe, Anoop Gupta, and John L. Hennessy. </author> <title> An Empirical Comparison of the Kendall Square Research KSR-1 and Stanford DASH Multiprocessor. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <pages> pages 214225, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Stenstrom, et al., show that a first touch page placement strategy eliminates much of the difference by allocating pages on the node that accesses them first [42]. Page migration algorithms also help, as does restructuring an algorithm to enhance locality <ref> [39] </ref>. However, most of these Dir N NB improvements require additional hardware, additional run-time overhead, or significant effort by the applications programmer. The Typhoon/Stache simulations required no modifications to the existing applications.
Reference: [40] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. Computer Architecture News, </title> <address> 20(1):544, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: We evaluated these two systems using five benchmarks: Appbt, a locally-parallelized version of the NAS benchmark [3]; Barnes, MP3D, and Ocean from the SPLASH suite <ref> [40] </ref>; and a transparent shared-memory version of EM3D (discussed in Section 4). Appbt is a computational uid dynamics program, which solves multiple independent systems of non-diagonally dominant, block tridiago-nal equations with a 5x5 block size. Barnes performs a gravitational N-body simulation using the Barnes-Hut algorithm.
Reference: [41] <author> Ellen Spertus, Seth Copen Goldstein, Klaus Erik Schauser, Thorsten von Eicken, David E. Culler, and William J. Dally. </author> <title> Evaluation of Mechanisms for Fine-Grained Parallel Programs in the J-Machine and the CM-5. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: As in systems in which message handlers interrupt the main thread, shared resources must be protected; however, critical sections are sufficient since truly concurrent threads do not suffer from a priority inversion problem <ref> [41] </ref>. 2.2 Bulk Node-to-Node Data Transfers When compilers can fully analyze a programs communication pattern, they can improve performance by exploiting hardware mechanisms to overlap communication with computation. Furthermore, transferring bulk data via explicit messages is more efficient than using shared memory [23].
Reference: [42] <author> Per Stenstrom, Truman Joe, and Anoop Gupta. </author> <title> Comparative Performance Evaluation of Cache-Coherent NUMA and COMA Architectures. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 8091, </pages> <year> 1992. </year>
Reference-contexts: tag check read-tag Return value of tag set-RW Set tag value to ReadWrite set-RO Set tag value to ReadOnly invalidate Set tag value to Invalid and invali date any local copies resume Resume suspended thread (s) memory machines, which return a cache block to its home node on cache replacement <ref> [42] </ref>. COMA systems share this advantage, but require complex hardware support. Stache is a user-level library that exploits the Tempest mechanisms. This library contains a page-fault handler, message handlers, block-access-fault handlers, and shared-memory allocation functions. <p> Of course, the Dir N NB results can be significantly improved using careful data placement to ensure that most misses are satisfied locally. Stenstrom, et al., show that a first touch page placement strategy eliminates much of the difference by allocating pages on the node that accesses them first <ref> [42] </ref>. Page migration algorithms also help, as does restructuring an algorithm to enhance locality [39]. However, most of these Dir N NB improvements require additional hardware, additional run-time overhead, or significant effort by the applications programmer. The Typhoon/Stache simulations required no modifications to the existing applications. <p> Stache differs from distributed shared memory systems because its maintains coherence on a much finer granularity. Stache is also similar to Cache-Only Memory Architectures (COMA). Both use the local main memory to cache remote data <ref> [42] </ref>. However, COMA machines use complex hardware and make all of local memory into a cache 11 [14,21], while Stache uses much simpler hardware and only as much of the local memory as an application chooses to use. <p> Stache uses Typhoons tag mechanisms to accelerate user-level software coherence algorithms. DDM and KSR-1 are both hierarchical machines, with hierarchical directory structures. Stache is at, like Sten-strom, et al.s proposed COMA-F machine <ref> [42] </ref>. Recent research has begun focusing on supporting multiple paradigms in a single computerby integrating shared memory and message passingand allowing a compiler to select the model appropriate for a program or data structure [24]. Frank and Vernon proposed message-passing mechanisms for shared memory systems [12].
Reference: [43] <author> George S. Taylor, Paul N. Hilfinger, James R. Larus, David A. Patterson, and Benjamin G. Zorn. </author> <title> Evaluation of the SPUR Lisp Architecture. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 444452, </pages> <month> June </month> <year> 1986. </year>
Reference: [44] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <year> 1991. </year>
Reference-contexts: Unfortunately, no consensus has emerged on the communication modelshared memory or message pass-ingfor parallel languages. Current parallel machines take an all-or-nothing approach to providing a shared address space. Message-passing machines, such as the Thinking Machines CM-5 <ref> [44] </ref> and Intel Paragon [20], have no hardware support, so compilers for these machines synthesize a shared address space by generating code that copies values between processors in messages. In the best case, this approach performs well and efficiently uses a machines memory and communications network.
Reference: [45] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a Mechanism for Integrating Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Low overhead messages are fundamental to the performance of most programming models. The Active Messages model, where a message specifies a user-level handler to be invoked on its reception, provides an efficient building block for many paradigms, including shared memory <ref> [45] </ref>. With user-level access to fast messages, compilers can exploit the statically-determinable properties of data structures and program communication by explicitly communicating values. In addition, low-latency message handling is critical for transparent shared memory performance. <p> Data words are moved to the send queue using stores or block transfers. The end of the message is signaled by a low-order bit in the register address. On the receiver, the first data word is interpreted as the receive handler PC, as in Active Messages <ref> [45] </ref>. The receive handler must pull the remainder of the message from the receive queue. Scheduling on the NP is performed by a hardware-assisted dispatch loop [16].
Reference: [46] <author> William A Wulf. </author> <title> Compiler and Computer Architecture. </title> <journal> IEEE Computer, </journal> <volume> 14(7):4147, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: With the user-level facilities provided by Tempest, a programmer or compiler can tailor memory semantics to fit a particular program or data structure, much as RISC processors enable compilers to tailor instruction sequences for a particular function call or data reference <ref> [46] </ref>. Tempest contains the following four types of user-level mechanisms: Low-overhead messaging, which permits the fast communication fundamental to the performance of many parallel programs. Bulk data transfer, which allows large data transfers to overlap computation. <p> System-level protocols face a difficult choice between generality and specificity: a protocol general enough for many sharing patterns may not be optimal for any of them, but a protocol tailored to a specific pattern may not support others. Instruction set designers have learned to implement primitives, rather than solutions <ref> [46] </ref>. Memory systems should also provide mechanisms that compilers can compose into efficient solutions. 5 Typhoon: A User-Level Shared-Memory System This section describes Typhoon, a system designed to implement Tempests user-level memory mechanisms.
References-found: 46

