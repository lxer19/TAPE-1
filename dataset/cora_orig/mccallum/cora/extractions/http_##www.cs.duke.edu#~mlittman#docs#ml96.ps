URL: http://www.cs.duke.edu/~mlittman/docs/ml96.ps
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: 
Email: mlittman@cs.brown.edu  szepes@math.u-szeged.hu  
Title: A Generalized Reinforcement-Learning Model: Convergence and Applications  
Author: Michael L. Littman Csaba Szepesvari 
Address: Providence, RI 02912-1910, USA  Szeged 6720, Aradi vrt tere 1. HUNGARY  
Affiliation: Department of Computer Science Brown University  Research Group of Artificial Intelligence "Jozsef Attila" University, Szeged  
Abstract: Reinforcement learning is the process by which an autonomous agent uses its experience interacting with an environment to improve its behavior. The Markov decision process (mdp) model is a popular way of formalizing the reinforcement-learning problem, but it is by no means the only way. In this paper, we show how many of the important theoretical results concerning reinforcement learning in mdps extend to a generalized mdp model that includes mdps, two-player games and mdps under a worst-case optimality criterion as special cases. The basis of this extension is a stochastic-approximation theorem that reduces asynchronous convergence to synchronous convergence.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., and Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1) </volume> <pages> 81-138. </pages>
Reference-contexts: Also, letting R t = R and P t = P for all t, this result implies that real-time dynamic programming <ref> (Barto et al., 1995) </ref> converges to the optimal value function. 5 CONCLUSIONS In this paper, we presented a generalized model of Markov decision processes, and proved the convergence of several reinforcement-learning algorithms in the generalized model.
Reference: <author> Bertsekas, D. P. and Tsitsiklis, J. N. </author> <year> (1989). </year> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: Theorem 1 therefore implies that this generalized Q-learning algorithm converges to the optimal Q function with probability 1 uniformly over X fi A. The convergence of Q-learning for discounted mdps and alternating Markov games follows trivially from this. Extensions of this result for undiscounted "all-policies-proper" mdps <ref> (Bertsekas and Tsitsiklis, 1989) </ref>, a soft state aggregation learning rule (Singh et al., 1995), and a "spreading" learning rule are given in a more detailed version of this paper (Szepesvari and Littman, 1996). 4.2 Q-LEARNING FOR MARKOV GAMES Markov games are a generalization of mdps and alternating Markov games in which
Reference: <author> Condon, A. </author> <year> (1992). </year> <title> The complexity of stochastic games. </title> <journal> Information and Computation, </journal> <volume> 96(2) </volume> <pages> 203-224. </pages>
Reference: <author> Gullapalli, V. and Barto, A. G. </author> <year> (1994). </year> <title> Convergence of indirect adaptive asynchronous value iteration algorithms. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 695-702, </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In model-based reinforcement learning, R and P are estimated on-line, and the value function is updated according to the approximate dynamic-programming operator derived from these estimates; this algorithm converges to the optimal value function under a wide variety of choices of the order states are updated <ref> (Gullapalli and Barto, 1994) </ref>. The method of Q-learning (Watkins and Dayan, 1992) uses experience to estimate the optimal value function without ever explicitly approximating R and P .
Reference: <author> Heger, M. </author> <year> (1994). </year> <title> Consideration of risk in reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 105-111, </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: In this paper, we introduce a generalized Markov decision process model with applications to reinforcement learning, and list some important results concerning the model. Generalized mdps provide a foundation for the use of reinforcement learning in mdps and games, as well as in risk-sensitive reinforcement learning <ref> (Heger, 1994) </ref>, exploration-sensitive reinforcement learning (John, 1995), and reinforcement learning in simultaneous-action games (Littman, 1994). <p> Many natural operators are non-expansions, such as max, min, midpoint, median, mean, and fixed weighted averages of these operations. Several previously described reinforcement-learning scenarios are special cases of this generalized mdp model including computing the expected return of a fixed policy (Sutton, 1988), finding the optimal risk-averse pol icy <ref> (Heger, 1994) </ref>, and finding the optimal exploration-sensitive policy (John, 1995). As with mdps, we can define a dynamic-programming operator [T V ](x) = a M x;a ! The operator T is a contraction mapping for 0 fl &lt; 1.
Reference: <author> Jaakkola, T., Jordan, M. I., and Singh, S. P. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <booktitle> Neural Computation, </booktitle> <pages> 6(6). </pages>
Reference: <author> John, G. H. </author> <year> (1995). </year> <title> When the best move isn't optimal: Q-learning with exploration. </title> <type> Unpublished manuscript. </type>
Reference-contexts: Generalized mdps provide a foundation for the use of reinforcement learning in mdps and games, as well as in risk-sensitive reinforcement learning (Heger, 1994), exploration-sensitive reinforcement learning <ref> (John, 1995) </ref>, and reinforcement learning in simultaneous-action games (Littman, 1994). <p> Several previously described reinforcement-learning scenarios are special cases of this generalized mdp model including computing the expected return of a fixed policy (Sutton, 1988), finding the optimal risk-averse pol icy (Heger, 1994), and finding the optimal exploration-sensitive policy <ref> (John, 1995) </ref>. As with mdps, we can define a dynamic-programming operator [T V ](x) = a M x;a ! The operator T is a contraction mapping for 0 fl &lt; 1.
Reference: <author> Littman, M. L. </author> <year> (1994). </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157-163, </pages> <address> San Fran-cisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Generalized mdps provide a foundation for the use of reinforcement learning in mdps and games, as well as in risk-sensitive reinforcement learning (Heger, 1994), exploration-sensitive reinforcement learning (John, 1995), and reinforcement learning in simultaneous-action games <ref> (Littman, 1994) </ref>. Our main theorem addresses conditions for the convergence of asynchronous stochastic processes and shows how these conditions relate to conditions for convergence of a corresponding synchronous process; it can be used to prove the convergence of model-free and model-based reinforcement-learning algorithms under a variety of reinforcement-learning scenarios. <p> An equivalent set of equations can be written with a stochastic choice for the minimizer, and also with the roles of the maximizer and minimizer reversed. The Q-learning update rule for Markov games <ref> (Littman, 1994) </ref> given step t experience hx t ; a t ; b t ; y t ; r t i has the form Q t+1 (x t ; a t ; b t ) := (1 ff t (x t ; a t ; b t ))Q t (x t
Reference: <author> Mahadevan, S. </author> <year> (1996). </year> <title> Average reward reinforcement learning: Foundations, algorithms, and empirical results. </title> <journal> Machine Learning, </journal> 22(1/2/3):159-196. 
Reference-contexts: The results in this paper primarily concern reinforcement-learning in contractive models (fl &lt; 1 or all-policies-proper), and there are important non-contractive reinforcement-learning scenarios, for example, reinforcement learning under an average-reward criterion <ref> (Mahadevan, 1996) </ref>. It would be interesting to develop a TD () algorithm (Sutton, 1988) for generalized mdps. Theorem 1 is not re stricted to finite state spaces, and it might be valuable to prove the convergence of a reinforcement-learning algorithm for a infinite state-space model.
Reference: <author> Moore, A. W. and Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13. </volume>
Reference-contexts: In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions. Researchers have explored model-free (direct) methods, such as Q-learning (Watkins and Dayan, 1992), and model-based (indirect) methods, such as prioritized sweeping <ref> (Moore and Atkeson, 1993) </ref>, and many converge to optimal value functions under the proper conditions (Tsitsiklis, 1994; Jaakkola et al., 1994; Gullapalli and Barto, 1994). Not all reinforcement-learning scenarios of interest can be modeled as mdps. <p> Although Q-learning shows that optimal value functions can be estimated without ever explicitly learning R and P , learning R and P makes more efficient use of experience at the expense of additional storage and computation <ref> (Moore and Atke-son, 1993) </ref>. The parameters of R and P can be gleaned from experience by keeping statistics for each state-action pair on the expected reward and the proportion of transitions to each next state.
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes| Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, NY. </address>
Reference-contexts: These simultaneous equations, known as the Bellman equations, can be solved using a variety of techniques ranging from successive approximation to linear programming <ref> (Puterman, 1994) </ref>. In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions.
Reference: <author> Robbins, H. and Monro, S. </author> <year> (1951). </year> <title> A stochastic approximation method. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22 </volume> <pages> 400-407. </pages>
Reference-contexts: For example, the theorem makes the convergence of Q-learning a consequence of the classical Robbins-Monro theo rem <ref> (Robbins and Monro, 1951) </ref>. 4 APPLICATIONS This section makes use of Theorem 1 to prove the con vergence of various reinforcement-learning algorithms. 4.1 GENERALIZED Q-LEARNING FOR EXPECTED VALUE MODELS Consider the family of finite state and action general ized mdps defined by the Bellman equations V fl (x) = a X <p> X (x t = x; a t = a)ff t (x; a) = 1 P t (x t = x; a t = a)ff t (x; a) 2 &lt; 1 with probability 1 uniformly over X fi A 2 , then a standard result from the theory of stochastic approximation <ref> (Robbins and Monro, 1951) </ref> states that T t approximates T everywhere. That is, this method of using a decayed, exponentially weighted average cor rectly computes the average one-step reward.
Reference: <author> Singh, S., Jaakkola, T., and Jordan, M. </author> <year> (1995). </year> <title> Reinforcement learning with soft state aggregation. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> Cambridge, MA. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: The convergence of Q-learning for discounted mdps and alternating Markov games follows trivially from this. Extensions of this result for undiscounted "all-policies-proper" mdps (Bertsekas and Tsitsiklis, 1989), a soft state aggregation learning rule <ref> (Singh et al., 1995) </ref>, and a "spreading" learning rule are given in a more detailed version of this paper (Szepesvari and Littman, 1996). 4.2 Q-LEARNING FOR MARKOV GAMES Markov games are a generalization of mdps and alternating Markov games in which both players simultaneously choose actions at each step.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44. </pages>
Reference-contexts: Many natural operators are non-expansions, such as max, min, midpoint, median, mean, and fixed weighted averages of these operations. Several previously described reinforcement-learning scenarios are special cases of this generalized mdp model including computing the expected return of a fixed policy <ref> (Sutton, 1988) </ref>, finding the optimal risk-averse pol icy (Heger, 1994), and finding the optimal exploration-sensitive policy (John, 1995). As with mdps, we can define a dynamic-programming operator [T V ](x) = a M x;a ! The operator T is a contraction mapping for 0 fl &lt; 1. <p> The results in this paper primarily concern reinforcement-learning in contractive models (fl &lt; 1 or all-policies-proper), and there are important non-contractive reinforcement-learning scenarios, for example, reinforcement learning under an average-reward criterion (Mahadevan, 1996). It would be interesting to develop a TD () algorithm <ref> (Sutton, 1988) </ref> for generalized mdps. Theorem 1 is not re stricted to finite state spaces, and it might be valuable to prove the convergence of a reinforcement-learning algorithm for a infinite state-space model.
Reference: <author> Szepesvari, C. </author> <year> (1995). </year> <title> General framework for reinforcement learning. </title> <booktitle> In Proceedings of ICANN'95 Paris. </booktitle>
Reference: <author> Szepesvari, C. and Littman, M. L. </author> <year> (1996). </year> <title> Generalized Markov decision processes: Dynamic-programming and reinforcement-learning algorithms. </title> <type> Technical Report CS-96-11, </type> <institution> Brown University, Providence, RI. </institution>
Reference-contexts: The theorem is proven in a more detailed version of this paper <ref> (Szepesvari and Littman, 1996) </ref>. We next describe some of the intuition behind the statement of the theorem and its conditions. <p> Extensions of this result for undiscounted "all-policies-proper" mdps (Bertsekas and Tsitsiklis, 1989), a soft state aggregation learning rule (Singh et al., 1995), and a "spreading" learning rule are given in a more detailed version of this paper <ref> (Szepesvari and Littman, 1996) </ref>. 4.2 Q-LEARNING FOR MARKOV GAMES Markov games are a generalization of mdps and alternating Markov games in which both players simultaneously choose actions at each step. The basic model is defined by the tuple hS; A; B; P; Ri and discount factor fl. <p> The proof is based on estimating the Q-learning algorithm from above by an appropriate process where the Q function is updated only if the received experience tuple is an extremity according to the optimality equation; details are given elsewhere <ref> (Szepesvari and Littman, 1996) </ref>. 4.4 EXPLORATION-SENSITIVE MODELS John (1995) considered the implications of insisting that reinforcement-learning agents keep exploring forever; he found that better learning performance can be achieved if the Q-learning rule is changed to incorporate the condition of persistent exploration. <p> Other Results We have derived a collection of results <ref> (Szepesvari and Littman, 1996) </ref> for the generalized mdp model that demonstrate its general applicability: the Bellman equations can be solved by value iteration; a myopic policy with respect to an approximately optimal value function gives an approximately optimal policy; when N x has a particular "maximiza tion" property, policy iteration converges
Reference: <author> Tesauro, G. </author> <year> (1995). </year> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <pages> pages 58-67. </pages>
Reference-contexts: Not all reinforcement-learning scenarios of interest can be modeled as mdps. For example, a great deal of reinforcement-learning research has been directed to the problem of solving two-player games <ref> (e.g, Tesauro, 1995) </ref>, and the reinforcement-learning algorithms for solving mdps and their convergence proofs do not apply directly to games.
Reference: <author> Tsitsiklis, J. N. </author> <year> (1994). </year> <title> Asynchronous stochastic approximation and Q-learning. </title> <journal> Machine Learning, </journal> <volume> 16(3). </volume>
Reference: <author> Watkins, C. J. C. H. and Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292. </pages>
Reference-contexts: In the absence of complete information regarding the transition and reward functions, reinforcement-learning methods can be used to find optimal value functions. Researchers have explored model-free (direct) methods, such as Q-learning <ref> (Watkins and Dayan, 1992) </ref>, and model-based (indirect) methods, such as prioritized sweeping (Moore and Atkeson, 1993), and many converge to optimal value functions under the proper conditions (Tsitsiklis, 1994; Jaakkola et al., 1994; Gullapalli and Barto, 1994). Not all reinforcement-learning scenarios of interest can be modeled as mdps. <p> The method of Q-learning <ref> (Watkins and Dayan, 1992) </ref> uses experience to estimate the optimal value function without ever explicitly approximating R and P . <p> The estimated Q function converges to Q fl under the proper conditions <ref> (Watkins and Dayan, 1992) </ref>. 2.2 ALTERNATING MARKOV GAMES In alternating Markov games, two players take turns issuing actions to maximize their expected discounted total reward.
References-found: 19

