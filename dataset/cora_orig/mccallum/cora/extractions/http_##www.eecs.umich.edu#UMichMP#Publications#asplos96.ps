URL: http://www.eecs.umich.edu/UMichMP/Publications/asplos96.ps
Refering-URL: http://www.eecs.umich.edu/UMichMP/abstracts.html
Root-URL: http://www.eecs.umich.edu
Abstract: Branch prediction is an important mechanism in modern microprocessor design. The focus of research in this area has been on designing new branch prediction schemes. In contrast, very few studies address the theoretical basis behind these prediction schemes. Knowing this theoretical basis helps us to evaluate how good a prediction scheme is and how much we can expect to improve its accuracy. In this paper, we apply techniques from data compression to establish a theoretical basis for branch prediction, and to illustrate alternatives for further improvement. To establish a theoretical basis, we first introduce a conceptual model to characterize each component in a branch prediction process. Then we show that current two-level or correlation based predictors are, in fact, simplifications of an optimal predictor in data compression, Prediction by Partial Matching (PPM). If the information provided to the predictor remains the same, it is unlikely that significant improvements can be expected (asymptotically) from two-level predictors, since PPM is optimal. However, there are a rich set of predictors available from data compression, several of which can still yield some improvement in cases where resources are limited. To illustrate this, we conduct trace-driven simulation running the Instruction Benchmark Suite and the SPEC CINT95 benchmarks. The results show that PPM can outperform a two-level predictor for modest sized branch tar get buffers. 
Abstract-found: 1
Intro-found: 1
Reference: [Bell90] <author> Bell, T. C., Cleary, J. G. and Witten I. H. </author> <title> Text Compression . Englewood Cliffs, </title> <address> NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1990. </year>
Reference-contexts: Thus, the net effect is to reduce the overall number of bits needed to represent the original data. In order to perform this compression effectively, a compression algorithm has to predict future data accurately to build a good probabilistic model for the next symbol <ref> [Bell90] </ref>. Then, as shown in the algorithm encodes the next symbol with a coder tuned to the probability distribution. Current coders can encode data so effectively that the number of bits used is very close to optimal and, consequently, the design of good compression relies on an accurate predictor. <p> Indeed, it usually outperforms the Lempel-Ziv algorithm (found in Unix compress ) due to implementation considerations and a faster convergence rate <ref> [Curewitz93, Bell90, Witten94] </ref>. As described above, the PPM algorithm for text compression consists of a predictor to estimate probabilities for characters and an arithmetic coder. We only make use of the predictor. We encode the outcomes of a branch, taken or not taken, as 1 or 0 respectively. <p> As mentioned in Section 3.1, PPM is a theoretically proven optimal predictor consisting of a set of Markov predictors. Though performance is inferior at the beginning, a single Markov predictor can approach the performance of PPM in the long run (asymptotically) <ref> [Bell90] </ref>. Furthermore, we have shown that a two-level predictor is a simplified Markov predictor. Therefore, we can see that a two-level predictor is an approximation of an optimal predictor, PPM. <p> Furthermore, depending on the type of application, an optimal predictor may have different levels of efficiency <ref> [Bell90] </ref>. For example, the Lempel-Ziv predictor (found in Unix compress ) and PPM are both optimal predictors. While the Lem-pel-Ziv predictor has a faster prediction speed, PPM has higher accuracy in general. Yet in the long run, they can both achieve optimal accuracy. <p> Therefore, though an efficient non-optimal predictor can never reach optimal accuracy, it may achieve higher accuracy in short or fast-changing programs. An example in data compression would be Dynamic Markov Compression (DMC) <ref> [Bell90] </ref>. 6.2 Improvement of the information processor Even with optimal predictors, we can still increase accuracy by improving the information processor. Good information selection, encoding, and dispatching can extract the essence of branch behavior and, hence, improve prediction accuracy.
Reference: [Calder94] <author> Calder, B. and Grunwald, D. </author> <title> Reducing branch costs via branch alignment . Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </title> <type> 242-251, </type> <year> 1994. </year>
Reference-contexts: A more predictable source can be derived by adding algorithmic knowledge and run-time statistics from test-runs. The goal is to decrease the entropy of the source by making the outcomes of branches more unevenly distributed. An example is code restruc turing with profiling information <ref> [Calder94, Young94] </ref>. 7. Conclusions and Further Work In this paper, we establish the connection between data compression and branch prediction. This allows us to draw techniques from data compression to form a theoretical basis for branch prediction.
Reference: [Chang94] <author> Chang, P., Hao, E., Yeh, T. and Patt, Y. </author> <title> Branch classification: </title> <booktitle> a new mechanism for improving branch predictor performance . Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <pages> 22-31, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: To establish a theoretical basis, we first introduce a conceptual system model to characterize components in a branch prediction process. Using this model, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use predictors similar to a two-level adaptive branch predictor [Yeh91]. Then, we demonstrate that these two-level like predictors are, in fact, simplified implementations of an optimal predictor in data compression, Prediction by Partial Matching (PPM) [Cleary84, Moffat90]. <p> For the information processor, it describes both the information used by the selector and the way dispatcher maps information. Finally, the predictor used in each prediction scheme is also listed. From this table, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use Markov predictors. We will explain this further in Section 4.
Reference: [Cleary84] <author> Cleary, J. G. and Witten, I. H. </author> <title> Data compression using adaptive coding and partial string matching . IEEE Transactions on Communications, </title> <journal> Vol. </journal> <volume> 32, No. 4, </volume> <pages> 396-402, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: Then, we demonstrate that these two-level like predictors are, in fact, simplified implementations of an optimal predictor in data compression, Prediction by Partial Matching (PPM) <ref> [Cleary84, Moffat90] </ref>. This establishes a theoretical basis for current two-level predictors that can draw on the relatively mature field of data compression. In particular, the potential benefit of applying data compression techniques to branch prediction is readily apparent in the similarity of predictors used in both methods. <p> In our experiments we draw on these techniques, adapting them to the new context of branch prediction. 3.1 Prediction by Partial Matching Prediction by partial matching (PPM) is a universal compression/prediction algorithm that has been theoretically proven optimal and has been applied in data compression and prefetching <ref> [Cleary84, Krishnan94, Kroeger96, Moffat90, Vitter91] </ref>. Indeed, it usually outperforms the Lempel-Ziv algorithm (found in Unix compress ) due to implementation considerations and a faster convergence rate [Curewitz93, Bell90, Witten94].
Reference: [Curewitz93] <author> Curewitz K. M., Krishnan, P. and Vitter, J. S. </author> <booktitle> Practical prefetching via data compression . Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> 257-266, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Indeed, it usually outperforms the Lempel-Ziv algorithm (found in Unix compress ) due to implementation considerations and a faster convergence rate <ref> [Curewitz93, Bell90, Witten94] </ref>. As described above, the PPM algorithm for text compression consists of a predictor to estimate probabilities for characters and an arithmetic coder. We only make use of the predictor. We encode the outcomes of a branch, taken or not taken, as 1 or 0 respectively.
Reference: [Eustace95] <author> Eustace, A. and Srivastava, A. </author> <title> ATOM: </title> <booktitle> A flexible interface for building high performance program analysis tools . Proceedings of the Winter 1995 USENIX Technical Conference on UNIX and Advanced Computing Systems, </booktitle> <pages> 303-314, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: on: one 2-bit counter frequency frequency 00 10 positive negative (same for both predictors) taken not taken -1 positive or negative count max (0s frequency, 1s frequency) counter for 1s counter for 0s 2 counters (same) (a majority vote) (the majority) For the SPEC CINT95 benchmark suite, we used ATOM <ref> [Eustace95] </ref>, a code instrumentation interface from Digital Equipment Corporation, to collect our traces. The benchmarks are first instrumented with ATOM, then executed on a DEC 21064-based workstation running the OSF/1 3.0 operating system to generate traces. These traces contain only user-level instructions.
Reference: [Krishnan94] <author> Krishnan, P. and Vitter, J. S. </author> <booktitle> Optimal prediction for prefetching in the worst case . Proceedings of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: In our experiments we draw on these techniques, adapting them to the new context of branch prediction. 3.1 Prediction by Partial Matching Prediction by partial matching (PPM) is a universal compression/prediction algorithm that has been theoretically proven optimal and has been applied in data compression and prefetching <ref> [Cleary84, Krishnan94, Kroeger96, Moffat90, Vitter91] </ref>. Indeed, it usually outperforms the Lempel-Ziv algorithm (found in Unix compress ) due to implementation considerations and a faster convergence rate [Curewitz93, Bell90, Witten94].
Reference: [Kroeger96] <author> Kroeger, T. M. and Long, D. D. E. </author> <title> Predicting file system actions from prior events . Proceedings of USENIX Winter Technical Conference, </title> <month> January </month> <year> 1996. </year>
Reference-contexts: In our experiments we draw on these techniques, adapting them to the new context of branch prediction. 3.1 Prediction by Partial Matching Prediction by partial matching (PPM) is a universal compression/prediction algorithm that has been theoretically proven optimal and has been applied in data compression and prefetching <ref> [Cleary84, Krishnan94, Kroeger96, Moffat90, Vitter91] </ref>. Indeed, it usually outperforms the Lempel-Ziv algorithm (found in Unix compress ) due to implementation considerations and a faster convergence rate [Curewitz93, Bell90, Witten94].
Reference: [Lee84] <author> Lee, J.K.F. and Smith, A. J. </author> <title> Branch prediction strategies and branch target buffer design . IEEE Computer, </title> <journal> Vol. </journal> <volume> 21, No. 7, </volume> <pages> 6-22, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Another simplification made by two-level predictors is the use of a 2-bit counter instead of an -bit counter. This is a cost-effective choice, since two bits is the minimal number needed so that the direction of the predictor is not changed by the single exit in a loop statement <ref> [Lee84, Smith81] </ref>. As an aside, note that it is not coincidental that a 2-bit saturating up-down counter is the best among 4-state predictors [Nair95a]. This is because, with four states, one 2-bit saturating up-down counter is the best way to mimic the majority vote used in the Markov predictor.
Reference: [McFarling92] <author> McFarling, S. </author> <title> Combining branch predictors . WRL Technical Note TN-36, </title> <month> June </month> <year> 1993. </year>
Reference-contexts: To establish a theoretical basis, we first introduce a conceptual system model to characterize components in a branch prediction process. Using this model, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use predictors similar to a two-level adaptive branch predictor [Yeh91]. Then, we demonstrate that these two-level like predictors are, in fact, simplified implementations of an optimal predictor in data compression, Prediction by Partial Matching (PPM) [Cleary84, Moffat90]. <p> For the information processor, it describes both the information used by the selector and the way dispatcher maps information. Finally, the predictor used in each prediction scheme is also listed. From this table, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use Markov predictors. We will explain this further in Section 4. <p> Information describing branch behavior includes: branch address, branch outcome, operation code, target address, hint bits, and statistics from previous runs. How to best exploit and represent this information still remains to be studied. Examples of prediction schemes attempting to improve the information processor are the gshare scheme <ref> [McFarling92] </ref> and the path correlation scheme [Nair95b]. 6.3 Improvement of the source We can fundamentally improve the predictability of the branches by changing the source and, thereby, their behavior. A more predictable source can be derived by adding algorithmic knowledge and run-time statistics from test-runs.
Reference: [Moffat90] <author> Moffat, A. </author> <title> Implementing the PPM data compression scheme . IEEE Transactions on Communications, </title> <journal> Vol. </journal> <volume> 38, No. 11, </volume> <pages> 1917-1921, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Then, we demonstrate that these two-level like predictors are, in fact, simplified implementations of an optimal predictor in data compression, Prediction by Partial Matching (PPM) <ref> [Cleary84, Moffat90] </ref>. This establishes a theoretical basis for current two-level predictors that can draw on the relatively mature field of data compression. In particular, the potential benefit of applying data compression techniques to branch prediction is readily apparent in the similarity of predictors used in both methods. <p> In our experiments we draw on these techniques, adapting them to the new context of branch prediction. 3.1 Prediction by Partial Matching Prediction by partial matching (PPM) is a universal compression/prediction algorithm that has been theoretically proven optimal and has been applied in data compression and prefetching <ref> [Cleary84, Krishnan94, Kroeger96, Moffat90, Vitter91] </ref>. Indeed, it usually outperforms the Lempel-Ziv algorithm (found in Unix compress ) due to implementation considerations and a faster convergence rate [Curewitz93, Bell90, Witten94].
Reference: [MReport95] <institution> Microprocessor Report, Sebastopol, CA: MicroDe-sign Resources, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: A good branch prediction scheme can increase the performance of a microprocessor by eliminating the instruction fetch stalls in the pipelines. As a result, numerous branch prediction schemes have been proposed and implemented on new microprocessors <ref> [MReport95] </ref>. Many researchers focus on designing new branch prediction schemes solely based on comparing simulation results. However, very few studies address the theoretical basis behind these prediction schemes.
Reference: [Mudge96] <author> Mudge, T., Chen, I-C. K. and Coffey, J. T. </author> <title> Limits to branch prediction . Technical Report CSE-TR-282-96, </title> <institution> University of Michigan, </institution> <year> 1996. </year>
Reference-contexts: Therefore, we may further improve the predictor in the following ways. 6.1.1 Implementation of full-edged optimal predictors We demonstrated that two-level predictors have not achieved optimal accuracy using a Quicksort program whose opti mal branch predictability can be analyzed exactly <ref> [Mudge96] </ref>. Currently, due to the limitation of hardware resources, it may not be cost-effective to implement full-fledged optimal predictors.
Reference: [Nair95a] <author> Nair, R. </author> <title> Optimal 2-bit branch predictors . IEEE Transactions on Computers, </title> <journal> Vol. </journal> <volume> 44, No. 5, </volume> <pages> 698-702, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: As an aside, note that it is not coincidental that a 2-bit saturating up-down counter is the best among 4-state predictors <ref> [Nair95a] </ref>. This is because, with four states, one 2-bit saturating up-down counter is the best way to mimic the majority vote used in the Markov predictor. In the original Markov predictor, this voting prediction is done with two frequency counters (one for each outcome). 5.
Reference: [Nair95b] <author> Nair, R. </author> <booktitle> Dynamic path-based branch correlation . Pro ceedings of the 28th Annual International Symposium on Microar-chitecture, </booktitle> <pages> 15-23, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: To establish a theoretical basis, we first introduce a conceptual system model to characterize components in a branch prediction process. Using this model, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use predictors similar to a two-level adaptive branch predictor [Yeh91]. Then, we demonstrate that these two-level like predictors are, in fact, simplified implementations of an optimal predictor in data compression, Prediction by Partial Matching (PPM) [Cleary84, Moffat90]. <p> For the information processor, it describes both the information used by the selector and the way dispatcher maps information. Finally, the predictor used in each prediction scheme is also listed. From this table, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use Markov predictors. We will explain this further in Section 4. <p> How to best exploit and represent this information still remains to be studied. Examples of prediction schemes attempting to improve the information processor are the gshare scheme [McFarling92] and the path correlation scheme <ref> [Nair95b] </ref>. 6.3 Improvement of the source We can fundamentally improve the predictability of the branches by changing the source and, thereby, their behavior. A more predictable source can be derived by adding algorithmic knowledge and run-time statistics from test-runs.
Reference: [Pan92] <author> Pan, S-T., So, K. and Rahmeh, J. T. </author> <title> I mproving the accuracy of dynamic branch prediction using branch correlation . Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </title> <type> 76-84, </type> <year> 1992. </year>
Reference-contexts: To establish a theoretical basis, we first introduce a conceptual system model to characterize components in a branch prediction process. Using this model, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use predictors similar to a two-level adaptive branch predictor [Yeh91]. Then, we demonstrate that these two-level like predictors are, in fact, simplified implementations of an optimal predictor in data compression, Prediction by Partial Matching (PPM) [Cleary84, Moffat90]. <p> For the information processor, it describes both the information used by the selector and the way dispatcher maps information. Finally, the predictor used in each prediction scheme is also listed. From this table, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use Markov predictors. We will explain this further in Section 4.
Reference: [Ross85] <author> Ross, S. M. </author> <title> Introduction to probability models . London, </title> <address> United Kingdom: </address> <publisher> Academic press, </publisher> <year> 1985. </year>
Reference-contexts: Markov predictors The basis of the PPM algorithm of order ( m + 1) Markov predictors. A Markov predictor of order predicts the next bit based upon the immediately preceding bitsit is a simple Markov chain <ref> [Ross85] </ref>. The states are the 2 possible pat terns of bits. The transition probabilities are proportional to the observed frequencies of a 1 or a 0 that occur given that the predictor is in a particular state (has seen the bit pattern associated with that state).
Reference: [Sechrest96] <author> Sechrest, S., Lee, C-C. and Mudge, T. </author> <booktitle> Correlation and aliasing in dynamic branch predictors . Proceedings of the 23rd International Symposium on Computer Architecture, </booktitle> <pages> 22-32, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The small BTB miss rate in the SPEC is because only a small number of distinct branches contribute to the vast majority of branch instances for most benchmarks <ref> [Sechrest96] </ref>. To see the effect of a small BTB, we reduce the number of entries from 1024 to 128. As shown in Figure 8, PPM again performs better than the PAg scheme for both benchmarks. Notice that, with a smaller BTB, the improvement of PPM over PAg is more pronounced.
Reference: [Smith81] <author> Smith, J. E. </author> <booktitle> A study of branch prediction strategies Proceedings of the 8th International Symposium on Computer Architecture, </booktitle> <pages> 135-148, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: It does not need to know the meaning of the input. Common examples are a constant or static predictor, a 1-bit counter, a 2-bit up-down saturating counter <ref> [Smith81] </ref>, and a Markov predictor. A Markov predictor forms the basis of recent two-level prediction schemes and is discussed in detail in Section 3. <p> In the second level, the Markov predictor uses a frequency counter for each outcome, while the two-level predictor uses a saturating up-down 2-bit counter <ref> [Smith81] </ref>. Whenever a branch is taken/not taken, the 2-bit counter increments/decrements. The decision for a two-level predictor depends on whether the value of the counter falls in the positive half or the negative half. <p> Another simplification made by two-level predictors is the use of a 2-bit counter instead of an -bit counter. This is a cost-effective choice, since two bits is the minimal number needed so that the direction of the predictor is not changed by the single exit in a loop statement <ref> [Lee84, Smith81] </ref>. As an aside, note that it is not coincidental that a 2-bit saturating up-down counter is the best among 4-state predictors [Nair95a]. This is because, with four states, one 2-bit saturating up-down counter is the best way to mimic the majority vote used in the Markov predictor.
Reference: [SPEC95] <institution> SPEC CPU95, </institution> <note> Technical Manual, </note> <month> August </month> <year> 1995. </year>
Reference-contexts: To illustrate the potential improvement using data compression techniques, we conduct trace-driven simulations. The results show that PPM outperforms equivalent two-level predictor in the Instruction Benchmark Suite (IBS) [Uhlig95] and the SPEC CINT95 <ref> [SPEC95] </ref> benchmarks. However, the improvement is not great, because two-level predictors are near optimal. In the case of modest size systems, the improvement is more significant. This paper is organized into seven sections. <p> To assess and confirm the potential improve ment, we conduct trace-driven simulations. As input for the simulation, we use the Instruction Benchmark Suite (IBS) benchmarks [Uhlig95] and the SPEC CINT95 benchmark suite <ref> [SPEC95] </ref> for our simulation. The IBS benchmarks are a set of applications designed to reflect realistic workloads. The traces of these benchmarks are generated through hardware monitoring of a MIPS R2000-based workstation.
Reference: [Uhlig95] <author> Uhlig, R., Nagle, D., Mudge, T., Sechrest, S. and Emer, J. </author> <title> Instruction Fetching: </title> <booktitle> Coping with Code Bloat . Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> 345-356, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In practice, the predictors used in branch prediction are only a very small subset of predictors developed in data compression. To illustrate the potential improvement using data compression techniques, we conduct trace-driven simulations. The results show that PPM outperforms equivalent two-level predictor in the Instruction Benchmark Suite (IBS) <ref> [Uhlig95] </ref> and the SPEC CINT95 [SPEC95] benchmarks. However, the improvement is not great, because two-level predictors are near optimal. In the case of modest size systems, the improvement is more significant. This paper is organized into seven sections. <p> To assess and confirm the potential improve ment, we conduct trace-driven simulations. As input for the simulation, we use the Instruction Benchmark Suite (IBS) benchmarks <ref> [Uhlig95] </ref> and the SPEC CINT95 benchmark suite [SPEC95] for our simulation. The IBS benchmarks are a set of applications designed to reflect realistic workloads. The traces of these benchmarks are generated through hardware monitoring of a MIPS R2000-based workstation.
Reference: [Vitter91] <author> Vitter, J. S. and Krishnan, P. </author> <booktitle> Optimal prefetching via data compression . Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> 121-130, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In our experiments we draw on these techniques, adapting them to the new context of branch prediction. 3.1 Prediction by Partial Matching Prediction by partial matching (PPM) is a universal compression/prediction algorithm that has been theoretically proven optimal and has been applied in data compression and prefetching <ref> [Cleary84, Krishnan94, Kroeger96, Moffat90, Vitter91] </ref>. Indeed, it usually outperforms the Lempel-Ziv algorithm (found in Unix compress ) due to implementation considerations and a faster convergence rate [Curewitz93, Bell90, Witten94].
Reference: [Witten94] <author> Witten I. H., Moffat, A. and Bell T. C. </author> <title> Managing Gi gabytes . New York, </title> <publisher> NY: Van Nostrand Reinhold, </publisher> <year> 1994. </year>
Reference-contexts: Indeed, it usually outperforms the Lempel-Ziv algorithm (found in Unix compress ) due to implementation considerations and a faster convergence rate <ref> [Curewitz93, Bell90, Witten94] </ref>. As described above, the PPM algorithm for text compression consists of a predictor to estimate probabilities for characters and an arithmetic coder. We only make use of the predictor. We encode the outcomes of a branch, taken or not taken, as 1 or 0 respectively.
Reference: [Yeh91] <author> Yeh, T-Y. and Patt, Y. </author> <booktitle> Two-level adaptive training branch prediction . Proceedings of the 24th Annual International Symposium on Microarchitecture, </booktitle> <pages> 51-61, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: To establish a theoretical basis, we first introduce a conceptual system model to characterize components in a branch prediction process. Using this model, we notice that many of the best prediction schemes [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] use predictors similar to a two-level adaptive branch predictor <ref> [Yeh91] </ref>. Then, we demonstrate that these two-level like predictors are, in fact, simplified implementations of an optimal predictor in data compression, Prediction by Partial Matching (PPM) [Cleary84, Moffat90]. This establishes a theoretical basis for current two-level predictors that can draw on the relatively mature field of data compression. <p> In addition, these predictors all share very similar hardware components. As have one or more tables of 2-bit counters (pattern history tables) in their second level <ref> [Yeh91] </ref>. The contents of the first level shift-registers are typically used to select a 2-bit counter in one of the second-level tables.
Reference: [Yeh92] <author> Yeh, T-Y. and Patt, Y. </author> <booktitle> Alternative implementation of Two-Level Adaptive Branch Prediction . Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> 124-134, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: To establish a theoretical basis, we first introduce a conceptual system model to characterize components in a branch prediction process. Using this model, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use predictors similar to a two-level adaptive branch predictor [Yeh91]. Then, we demonstrate that these two-level like predictors are, in fact, simplified implementations of an optimal predictor in data compression, Prediction by Partial Matching (PPM) [Cleary84, Moffat90]. <p> For the information processor, it describes both the information used by the selector and the way dispatcher maps information. Finally, the predictor used in each prediction scheme is also listed. From this table, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use Markov predictors. We will explain this further in Section 4.
Reference: [Yeh93] <author> Yeh, T-Y. and Patt, Y. </author> <title> A comparison of dynamic branch predictors that use two levels of branch history . Proceedings of the 20th International Symposium on Computer Architecture, </title> <type> 257-266, </type> <month> May </month> <year> 1993. </year>
Reference-contexts: To establish a theoretical basis, we first introduce a conceptual system model to characterize components in a branch prediction process. Using this model, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use predictors similar to a two-level adaptive branch predictor [Yeh91]. Then, we demonstrate that these two-level like predictors are, in fact, simplified implementations of an optimal predictor in data compression, Prediction by Partial Matching (PPM) [Cleary84, Moffat90]. <p> For the information processor, it describes both the information used by the selector and the way dispatcher maps information. Finally, the predictor used in each prediction scheme is also listed. From this table, we notice that many of the best prediction schemes <ref> [Pan92, Yeh92, Yeh93, McFarling92, Chang94, Nair95b] </ref> use Markov predictors. We will explain this further in Section 4.
Reference: [Young94] <author> Young, C. and Smith, M. </author> <title> Improving the accuracy of static branch prediction using branch correlation . Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </title> <type> 232-241, </type> <year> 1994. </year>
Reference-contexts: A more predictable source can be derived by adding algorithmic knowledge and run-time statistics from test-runs. The goal is to decrease the entropy of the source by making the outcomes of branches more unevenly distributed. An example is code restruc turing with profiling information <ref> [Calder94, Young94] </ref>. 7. Conclusions and Further Work In this paper, we establish the connection between data compression and branch prediction. This allows us to draw techniques from data compression to form a theoretical basis for branch prediction.
Reference: [Young95] <author> Young, C., Gloy, N. and Smith, M. </author> <title> A comparative analysis of schemes for correlated branch prediction . Proceedings of the 22nd International Symposium on Computer Architecture, </title> <type> 276-286, </type> <month> June </month> <year> 1995. </year>
Reference-contexts: This conceptual view allows us to compare various branch prediction schemes. It also enables us to focus and improve each component by clearly defining its function. This conceptual model elaborates on the one in <ref> [Young95] </ref>. Our model extends it to accommodate most popular branch prediction schemes. Analysis of Branch Prediction via Data Compression I-Cheng K. Chen, John T. Coffey, and Trevor N.
References-found: 28

