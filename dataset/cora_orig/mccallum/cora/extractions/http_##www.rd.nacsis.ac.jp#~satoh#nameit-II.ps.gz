URL: http://www.rd.nacsis.ac.jp/~satoh/nameit-II.ps.gz
Refering-URL: http://www.rd.nacsis.ac.jp/~satoh/nameit.html
Root-URL: 
Email: satoh@rd.nacsis.ac.jp yuichi@image.is.tsukuba.ac.jp tk@cs.cmu.edu  
Title: Name-It: Naming and Detecting Faces in Video by the Integration of Image and Natural Language Processing  
Author: Shin'ichi Satoh and Yuichi Nakamura and Takeo Kanade 
Address: 5000 Forbes Ave., Pittsburgh, PA 15213, USA  
Affiliation: School of Computer Science Carnegie Mellon University  
Note: Proceedings of IJCAI-97, pp. 14881493, 1997. Copyrighted by International Joint Conferences on Artificial Intelligence, Inc.  
Abstract: We have been developing Name-It, a system that associates faces and names in news videos. First, as the only knowledge source, the system is given news videos which include image sequences and transcripts obtained from audio tracks or closed caption texts. The system can then either infer the name of a given face and output the name candidates, or can locate the faces in news videos by a name. To accomplish this task, the system extracts faces from image sequences and names from transcripts, both of which might correspond to key persons in news topics. The proposed system takes full advantage of advanced image and natural language processing. The image processing contributes to the extraction of face sequences which provide rich information for face-name association. The processing also helps to select the best frontal view of a face in a face sequence to enhance the face identification which is required for the processing. On the other hand, the natural language processing effectively extracts names by using lexical/grammatical analysis and knowledge of the news video topics structure. The success of our experiments demonstrates the benefits of the advanced image and natural language processing methods and their incorporation.
Abstract-found: 1
Intro-found: 1
Reference: [ Oxford ] <institution> The Oxford Text Archive. </institution> <note> http://ota.ox.ac.uk/. </note>
Reference-contexts: Instead, just before the live video, an anchor person tends to appear and introduce him/her (See Figure 5.). The system evaluates these conditions for each word that occurs in transcripts by using the dictionary (the Oxford Advanced Learner's Dictionary <ref> [Oxford] </ref>), the thesaurus (Word-Net [ Miller, 1990 ] ), and the parser (Link Parser [ Sleator, 1993 ] ).
Reference: [ Hunke, 1994 ] <author> H. Martin Hunke. </author> <title> Locating and tracking of human faces with neural networks. </title> <type> Technical Report CMU-CS-94-155, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: g = G=(R + G + B)) as Original image Face skin region F r = 1:14 F r = 1:01 F r = 1:42 Original image Face skin region F r = 0:72 F r = 1:03 F r = 1:42 a general skin color model for face tracking <ref> [ Yang and Waibel, 1995; Hunke, 1994 ] </ref> . Instead, for our research, the Gaussian model in (R; G; B) space is used because this model is more sensitive to brightness of skin color, and thus is much more suitable for the model tailored for each face.
Reference: [ Miller, 1990 ] <author> G. Miller. </author> <title> Wordnet: An on-line lexical database. </title> <journal> International Journal of Lexicography, </journal> <volume> 3(4), </volume> <year> 1990. </year>
Reference-contexts: Instead, just before the live video, an anchor person tends to appear and introduce him/her (See Figure 5.). The system evaluates these conditions for each word that occurs in transcripts by using the dictionary (the Oxford Advanced Learner's Dictionary [Oxford]), the thesaurus (Word-Net <ref> [ Miller, 1990 ] </ref> ), and the parser (Link Parser [ Sleator, 1993 ] ).
Reference: [ Rowley et al., 1995 ] <author> H. Rowley, S. Baluja, and T. Kanade. </author> <title> Human face detection in visual scenes. </title> <type> Technical Report CMU-CS-95-158, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: Optimally, we apply the face detector at the intervals of 10 frames. The system uses the neural network-based face detector <ref> [ Rowley et al., 1995 ] </ref> which detects size-free, position-free, almost frontal, any number of faces in a given image. The detected face is output as a rectangular region that includes most of the skin, but excludes the hair and the background.
Reference: [ Satoh and Kanade, 1997 ] <author> Shin'ichi Satoh and Takeo Kanade. Name-It: </author> <title> Association of face and name in video. </title> <booktitle> Proc. of CVPR'97, </booktitle> <year> 1997. </year>
Reference-contexts: However, these techniques, by themselves, are still too immature to sufficiently handle contents. Since multimedia information is a mixture of video, audio, text, etc., a combination of these techniques is quite effective in achieving the desired goal. To accomplish this task, Satoh et al. proposed Name-It <ref> [ Satoh and Kanade, 1997 ] </ref> , a system which associates names and faces in given news videos. Name-It's basic function is to guess which face corresponds to which name in given news videos. <p> Most of that time is consumed by parsing. 5 Face-Name Association 5.1 Algorithm In this section, the algorithm for retrieving face candidates by a given name is described. We use the co-occurrence factor <ref> [ Satoh and Kanade, 1997 ] </ref> with an extension to handle face duration and name score. Let N and F be a name and a face, respectively. <p> The detailed explanation of the equations is appeared in <ref> [ Satoh and Kanade, 1997 ] </ref> . 6 Experiments We implemented the Name-It System on an SGI workstation. We processed 10 CNN Headline News videos (30 minutes each) in a total of 5 hours. From them, the system extracted 556 face sequences, and was given 752 name candidates.
Reference: [ Sleator, 1993 ] <author> D. Sleator. </author> <title> Parsing english with a link grammar. </title> <booktitle> In Third International Workshop on Parsing Technologies, </booktitle> <year> 1993. </year>
Reference-contexts: The system evaluates these conditions for each word that occurs in transcripts by using the dictionary (the Oxford Advanced Learner's Dictionary [Oxford]), the thesaurus (Word-Net [ Miller, 1990 ] ), and the parser (Link Parser <ref> [ Sleator, 1993 ] </ref> ). Finally, the system outputs the three-tuple list: a word, timing information (frame), and a normalized score. 4.3 Score Calculation Referring to the dictionaries and the parsing results, the system calculates the score for each word in transcripts.
Reference: [ Smith and Kanade, 1995 ] <author> M. Smith and T. Kanade. </author> <title> Video skimming for quick browsing based on audio and image characterization. </title> <type> Technical Report CMU-CS-95-186, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: The overlap between each of these regions and each of the face regions of the previous frame is evaluated to decide whether one of the skin candidate regions is the succeeding face region. In addition, the scene change detection based on the sub-region color histogram matching method <ref> [ Smith and Kanade, 1995 ] </ref> is applied; this face region tracking is continued until a scene change is encountered or until no succeeding face region is found. 3.2 Face Identification To infer the frequent occurrence of a face, face identification is necessary, i.e., we need to determine whether one face
Reference: [ Turk and Pentland, 1991 ] <author> M. Turk and A. Pentland. </author> <title> Eigen-faces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1):7186, </volume> <year> 1991. </year>
Reference-contexts: The factor for an ideal frontal face is 1:5. The system chooses the face having the largest F r to be the most frontal face of the face sequence. and frontal factors. Eigenface-Based Face Identification We choose the eigenface-based method to evaluate face identification <ref> [ Turk and Pentland, 1991 ] </ref> . Each of the most frontal faces is normalized into a 64 by 64 image by using the eye positions, then converted into a point in the 16-dimensional eigenface space.
Reference: [ Yang and Waibel, 1995 ] <author> Jie Yang and Alex Waibel. </author> <title> Tracking human faces in real-time. </title> <type> Technical Report CMU-CS-95-210, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: g = G=(R + G + B)) as Original image Face skin region F r = 1:14 F r = 1:01 F r = 1:42 Original image Face skin region F r = 0:72 F r = 1:03 F r = 1:42 a general skin color model for face tracking <ref> [ Yang and Waibel, 1995; Hunke, 1994 ] </ref> . Instead, for our research, the Gaussian model in (R; G; B) space is used because this model is more sensitive to brightness of skin color, and thus is much more suitable for the model tailored for each face.
References-found: 9

