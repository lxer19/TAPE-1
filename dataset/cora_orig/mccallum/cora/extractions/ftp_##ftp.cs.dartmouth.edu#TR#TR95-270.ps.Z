URL: ftp://ftp.cs.dartmouth.edu/TR/TR95-270.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR95-270/
Root-URL: http://www.cs.dartmouth.edu
Title: Interfaces for Disk-Directed I/O  
Author: David Kotz 
Address: Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth College  
Note: Available at  
Pubnum: Technical Report PCS-TR95-270  
Email: dfk@cs.dartmouth.edu  
Date: September 13, 1995  
Web: URL ftp://ftp.cs.dartmouth.edu/TR/TR95-270.ps.Z  
Abstract: In other papers I propose the idea of disk-directed I/O for multiprocessor file systems. Those papers focus on the performance advantages and capabilities of disk-directed I/O, but say little about the application-programmer's interface or about the interface between the compute processors and I/O processors. In this short note I discuss the requirements for these interfaces, and look at many existing interfaces for parallel file systems. I conclude that many of the existing interfaces could be adapted for use in a disk-directed I/O system.
Abstract-found: 1
Intro-found: 1
Reference: [BBS + 94] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference: [BdC93] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: A more realistic system should be more flexible: it should support the common matrix distributions easily, and it should support arbitrary distributions and irregular data structures. Fortunately, several compiler groups have developed compact parameterized formats for describing matrix distributions <ref> [BMS95, BdC93] </ref>. This compact description of the distribution pattern, generated by a compiler or matrix-support library, can be passed to the IOPs.
Reference: [BGST93] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: Each call specifies the bytes of the file that should be transferred. This interface is common when using the C programming language in most MIMD systems, although many have special file-pointer modes that help in a few simple situations (Intel CFS [Pie89], Intel PFS [RP95], and TMC CMMD <ref> [BGST93] </ref>, for example). None of these allow the processor to make a single file-system request for a complex distribution pattern. More sophisticated interfaces, such as the nested-batched interface [NK95], can specify a list, or a strided series, of transfers in a single request.
Reference: [BMS95] <author> Peter Brezany, Thomas A. Mueck, and Erich Schikuta. </author> <title> Language, compiler and parallel database support for I/O intensive applications. </title> <booktitle> In High Performance Computing and Networking 1995 Europe, </booktitle> <pages> pages 14-20, </pages> <publisher> Springer-Verlag, LNCS 919, </publisher> <month> May </month> <year> 1995. </year> <month> 6 </month>
Reference-contexts: A more realistic system should be more flexible: it should support the common matrix distributions easily, and it should support arbitrary distributions and irregular data structures. Fortunately, several compiler groups have developed compact parameterized formats for describing matrix distributions <ref> [BMS95, BdC93] </ref>. This compact description of the distribution pattern, generated by a compiler or matrix-support library, can be passed to the IOPs.
Reference: [CFH + 95] <author> Peter Corbett, Dror Feitelson, Yarson Hsu, Jean-Pierre Prost, Marc Snir, Sam Fineberg, Bill Nitzberg, Bernard Traversat, and Parkson Wong. </author> <title> MPI-IO: a parallel file I/O interface for MPI. </title> <type> Technical Report NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1995. </year> <note> Version 0.3. </note>
Reference-contexts: MPI-IO will have 2 Network Memory Memory Memory Disk Disk Disk I/O Processor I/O Processor I/O Processor Memory Memory Memory Compute Processor Compute Processor Compute Processor Interconnection (IOPs). 3 such support <ref> [CFH + 95] </ref>, as may Intel PFS for the Paragon [RP95]. It would also be useful to have some control over whether the collective request enforces a barrier synchronization. Complex... <p> The most notable examples of this style include a proposed nCUBE file system [DdR92], IBM PIOFS (Vesta) [CFP + 95], and MPI-IO <ref> [CFH + 95] </ref>. The third style has neither an understanding of high-level data structures, like the first, nor per-process views of the file, like the second. Each call specifies the bytes of the file that should be transferred. <p> Any of the above interfaces that support collective requests and can express non-trivial distributions of data among the processor memories, would be sufficient to support disk-directed I/O. These include (at least) HPF and other SPMD languages, the nested-batched interface [NK95], IBM PIOFS (Vesta) [CFP + 95], MPI-IO <ref> [CFH + 95] </ref>, and most of the matrix li 4 braries [GGL93, KGF94, BdC93, BBS + 94, SCJ + 95, TBC + 94]. The new nCUBE [DdR92] interface would work if it was extended to support collective I/O.
Reference: [CFP + 95] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, George S. Almasi, Sandra John-son Baylor, Anthony S. Bolmarcich, Yarsun Hsu, Julian Satran, Marc Snir, Robert Colao, Brian Herr, Joseph Kavaky, Thomas R. Morgan, and Anthony Zlotek. </author> <title> Parallel file systems for the IBM SP computers. </title> <journal> IBM Systems Journal, </journal> <pages> pages 222-248, </pages> <year> 1995. </year>
Reference-contexts: The most notable examples of this style include a proposed nCUBE file system [DdR92], IBM PIOFS (Vesta) <ref> [CFP + 95] </ref>, and MPI-IO [CFH + 95]. The third style has neither an understanding of high-level data structures, like the first, nor per-process views of the file, like the second. Each call specifies the bytes of the file that should be transferred. <p> Any of the above interfaces that support collective requests and can express non-trivial distributions of data among the processor memories, would be sufficient to support disk-directed I/O. These include (at least) HPF and other SPMD languages, the nested-batched interface [NK95], IBM PIOFS (Vesta) <ref> [CFP + 95] </ref>, MPI-IO [CFH + 95], and most of the matrix li 4 braries [GGL93, KGF94, BdC93, BBS + 94, SCJ + 95, TBC + 94]. The new nCUBE [DdR92] interface would work if it was extended to support collective I/O.
Reference: [CLR90] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw Hill, </publisher> <year> 1990. </year>
Reference-contexts: A nested-batched request is essentially a nested list, or (looked at another way) a tree. Indeed, with some preprocessing it can be treated much like an interval tree <ref> [CLR90, section 15.3] </ref>, which can be used to perform the necessary mapping from file-block numbers to (CP number, CP offset) tuples. 1 For a collective request, an IOP receives one such request from each CP.
Reference: [DdR92] <author> Erik DeBenedictis and Juan Miguel del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Proceedings of the Eleventh Annual IEEE International Phoenix Conference on Computers and Communications, </booktitle> <pages> pages 0117-0124, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: The most notable examples of this style include a proposed nCUBE file system <ref> [DdR92] </ref>, IBM PIOFS (Vesta) [CFP + 95], and MPI-IO [CFH + 95]. The third style has neither an understanding of high-level data structures, like the first, nor per-process views of the file, like the second. Each call specifies the bytes of the file that should be transferred. <p> These include (at least) HPF and other SPMD languages, the nested-batched interface [NK95], IBM PIOFS (Vesta) [CFP + 95], MPI-IO [CFH + 95], and most of the matrix li 4 braries [GGL93, KGF94, BdC93, BBS + 94, SCJ + 95, TBC + 94]. The new nCUBE <ref> [DdR92] </ref> interface would work if it was extended to support collective I/O. Of course, each of these interfaces has distributions that it can express easily, distributions that it can express with difficulty, and distributions that it cannot express at all.
Reference: [GGL93] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference: [KGF94] <author> John F. Karpovich, Andrew S. Grimshaw, and James C. </author> <title> French. Extensible file systems ELFS: An object-oriented approach to high performance file I/O. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 191-204, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The first style allows the programmer to directly read and write data structures such as matrices; Fortran provides this style of interface, as do many libraries [GGL93, KGF94, BdC93, BBS + 94, SCJ + 95, TBC + 94]. Some object-oriented interfaces go even further in this direction <ref> [Kri94, KGF94, SCJ + 95] </ref>. As long as your data structure can be described by a matrix, and the language or library also provides ways to describe distributed matrices, this interface provides a neat solution.
Reference: [Kot94] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year> <note> Updated as Dartmouth TR PCS-TR94-226 on November 8, </note> <year> 1994. </year>
Reference-contexts: 1 Introduction In other papers I propose the idea of disk-directed I/O for multiprocessor file systems <ref> [Kot94, Kot95a, Kot95b] </ref>. Those papers show that disk-directed I/O can be used to substantially improve performance (higher throughput, lower execution time, or less network traffic) when reading input data, writing results, or executing an out-of-core computation. <p> In particular, it needs to be able to compute a mapping function from a file-block number to the set of (CP number, CP offset) locations of the data in that file block. For a complete understanding of disk-directed I/O, see <ref> [Kot94, Kot95a, Kot95b] </ref>. 2 Application-programmer's interface (API) The concept of disk-directed I/O depends on the ability of the programmer to specify large, collective, possibly complex I/O activities as single file-system requests. <p> an underlying disk-directed I/O system. 3 CP-IOP interface Once the application programmer has expressed the desired data transfer, how do the compute processors communicate that information to all of the IOPs, and how do the IOPs use the information to arrange the data transfer? In my original disk-directed I/O study <ref> [Kot94] </ref>, all of the possible data-distribution patterns (e.g., block-cyclic) were understood by the IOPs, so the CPs needed only to request a particular distribution pattern and to provide a few parameters.
Reference: [Kot95a] <author> David Kotz. </author> <title> Disk-directed I/O for an out-of-core computation. </title> <booktitle> In Proceedings of the Fourth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 159-166, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: 1 Introduction In other papers I propose the idea of disk-directed I/O for multiprocessor file systems <ref> [Kot94, Kot95a, Kot95b] </ref>. Those papers show that disk-directed I/O can be used to substantially improve performance (higher throughput, lower execution time, or less network traffic) when reading input data, writing results, or executing an out-of-core computation. <p> In particular, it needs to be able to compute a mapping function from a file-block number to the set of (CP number, CP offset) locations of the data in that file block. For a complete understanding of disk-directed I/O, see <ref> [Kot94, Kot95a, Kot95b] </ref>. 2 Application-programmer's interface (API) The concept of disk-directed I/O depends on the ability of the programmer to specify large, collective, possibly complex I/O activities as single file-system requests. <p> In a MIMD-style language (typically C or Fortran plus some form of message passing or shared memory), each process (or thread) acts independently of all other processes. A collective activity requires all participating processes to call the same function, preferably at nearly the same time. In my experience <ref> [Kot95a] </ref>, it it sometimes useful to require only a subset of processes to contribute to a collective request.
Reference: [Kot95b] <author> David Kotz. </author> <title> Expanding the potential for disk-directed I/O. </title> <booktitle> In Proceedings of the 1995 IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> October </month> <year> 1995. </year> <note> To appear. Currently available as Dartmouth PCS-TR95-254. </note>
Reference-contexts: 1 Introduction In other papers I propose the idea of disk-directed I/O for multiprocessor file systems <ref> [Kot94, Kot95a, Kot95b] </ref>. Those papers show that disk-directed I/O can be used to substantially improve performance (higher throughput, lower execution time, or less network traffic) when reading input data, writing results, or executing an out-of-core computation. <p> In particular, it needs to be able to compute a mapping function from a file-block number to the set of (CP number, CP offset) locations of the data in that file block. For a complete understanding of disk-directed I/O, see <ref> [Kot94, Kot95a, Kot95b] </ref>. 2 Application-programmer's interface (API) The concept of disk-directed I/O depends on the ability of the programmer to specify large, collective, possibly complex I/O activities as single file-system requests. <p> There are some capabilities of disk-directed I/O which cannot be represented as a set of read and write transfers, including data-dependent filtering and distribution functions <ref> [Kot95b] </ref>. To support this level of functionality essentially requires the user to specify an arbitrarily complex function (a program), rather than a simple set. This topic represents future work.
Reference: [Kri94] <author> Orran Krieger. </author> <title> HFS: A flexible file system for shared-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: The first style allows the programmer to directly read and write data structures such as matrices; Fortran provides this style of interface, as do many libraries [GGL93, KGF94, BdC93, BBS + 94, SCJ + 95, TBC + 94]. Some object-oriented interfaces go even further in this direction <ref> [Kri94, KGF94, SCJ + 95] </ref>. As long as your data structure can be described by a matrix, and the language or library also provides ways to describe distributed matrices, this interface provides a neat solution.
Reference: [NK95] <author> Nils Nieuwejaar and David Kotz. </author> <title> Low-level interfaces for high-level parallel I/O. </title> <booktitle> In IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 47-62, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: None of these allow the processor to make a single file-system request for a complex distribution pattern. More sophisticated interfaces, such as the nested-batched interface <ref> [NK95] </ref>, can specify a list, or a strided series, of transfers in a single request. This latter interface is perhaps the most powerful (efficient and expressive) of this style of interface. <p> Any of the above interfaces that support collective requests and can express non-trivial distributions of data among the processor memories, would be sufficient to support disk-directed I/O. These include (at least) HPF and other SPMD languages, the nested-batched interface <ref> [NK95] </ref>, IBM PIOFS (Vesta) [CFP + 95], MPI-IO [CFH + 95], and most of the matrix li 4 braries [GGL93, KGF94, BdC93, BBS + 94, SCJ + 95, TBC + 94]. The new nCUBE [DdR92] interface would work if it was extended to support collective I/O. <p> A few calculations can tell the IOP which file blocks it should be transferring, and for each file block, the location of the data (CP number and offset within that CP's buffer). To support more complex distributions, or irregular requests, each CP can send a single nested-batched request <ref> [NK95] </ref> to each IOP. Such requests can capture complex but regular requests in a compact form, but can also capture completely irregular requests as a list. A nested-batched request is essentially a nested list, or (looked at another way) a tree. <p> Then, as each block is transferred, the IOP uses the trees to determine which CP (s) requested parts of that block, and where in the CP the data is located. The combination of the compact parameterized descriptions for common matrix distributions, and the fully general nested-batched interface <ref> [NK95] </ref>, are sufficient to efficiently support disk-directed I/O. 4 Conclusion While I do not propose any specific API or internal interface in this paper, I believe it is possible to use any of a number of existing such interfaces in the construction of a disk-directed I/O system. <p> Many existing interfaces support the common case of distributed multidimensional matrices, and there are compact forms for representing the common distributions. For more unusual (or irregular) distributions or data structures, the nested-batched interface <ref> [NK95] </ref> provides at least an internal representation for communicating between the CP and the IOP; ideally, an application-specific library would support the programmer when manipulating such data structures.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160. </pages> <publisher> Golden Gate Enterprises, </publisher> <address> Los Altos, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: Each call specifies the bytes of the file that should be transferred. This interface is common when using the C programming language in most MIMD systems, although many have special file-pointer modes that help in a few simple situations (Intel CFS <ref> [Pie89] </ref>, Intel PFS [RP95], and TMC CMMD [BGST93], for example). None of these allow the processor to make a single file-system request for a complex distribution pattern. More sophisticated interfaces, such as the nested-batched interface [NK95], can specify a list, or a strided series, of transfers in a single request.
Reference: [RP95] <author> Brad Rullman and David Payne. </author> <title> An efficient file I/O interface for parallel applications. DRAFT presented at the Workshop on Scalable I/O, </title> <booktitle> Frontiers '95, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: MPI-IO will have 2 Network Memory Memory Memory Disk Disk Disk I/O Processor I/O Processor I/O Processor Memory Memory Memory Compute Processor Compute Processor Compute Processor Interconnection (IOPs). 3 such support [CFH + 95], as may Intel PFS for the Paragon <ref> [RP95] </ref>. It would also be useful to have some control over whether the collective request enforces a barrier synchronization. Complex... The interesting characteristic of the API is its capability to specify which part of the file is desired, and how the data is distributed among the CPs' buffers. <p> Each call specifies the bytes of the file that should be transferred. This interface is common when using the C programming language in most MIMD systems, although many have special file-pointer modes that help in a few simple situations (Intel CFS [Pie89], Intel PFS <ref> [RP95] </ref>, and TMC CMMD [BGST93], for example). None of these allow the processor to make a single file-system request for a complex distribution pattern. More sophisticated interfaces, such as the nested-batched interface [NK95], can specify a list, or a strided series, of transfers in a single request.
Reference: [SCJ + 95] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year> <note> To appear. 7 </note>
Reference-contexts: The first style allows the programmer to directly read and write data structures such as matrices; Fortran provides this style of interface, as do many libraries [GGL93, KGF94, BdC93, BBS + 94, SCJ + 95, TBC + 94]. Some object-oriented interfaces go even further in this direction <ref> [Kri94, KGF94, SCJ + 95] </ref>. As long as your data structure can be described by a matrix, and the language or library also provides ways to describe distributed matrices, this interface provides a neat solution.
Reference: [TBC + 94] <author> Rajeev Thakur, Rajesh Bordawekar, Alok Choudhary, Ravi Ponnusamy, and Tarvinder Singh. </author> <title> PASSION runtime library for parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <pages> pages 119-128, </pages> <month> October </month> <year> 1994. </year> <note> Many of the above references are available via URL http://www.cs.dartmouth.edu/pario.html 8 </note>
References-found: 19

