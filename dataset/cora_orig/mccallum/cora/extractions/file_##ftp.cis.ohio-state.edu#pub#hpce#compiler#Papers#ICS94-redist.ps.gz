URL: file://ftp.cis.ohio-state.edu/pub/hpce/compiler/Papers/ICS94-redist.ps.gz
Refering-URL: http://www.cis.ohio-state.edu/~chh/Publication/compiler-papers.html
Root-URL: 
Title: An Approach to Communication-Efficient Data Redistribution  
Author: S. D. Kaushik C.-H. Huang R. W. Johnson and P. Sadayappan 
Keyword: Distributed-memory machine, High Performance For-tran, Data distribution, Data communi-cation, Tensor products.  
Address: Columbus, OH 43210. St. Cloud, MN 56301.  
Affiliation: 1 Department of Computer and Information Science, 2 Department of Computer Science, The Ohio State University, St. Cloud State University,  
Abstract: We address the development of efficient methods for performing data redistribution of arrays on distributed-memory machines. Data redistribution is important for the distributed-memory implementation of data parallel languages such as High Performance Fortran. An algebraic representation of regular data distributions is used to develop an analytical model for evaluating the communication cost of data redistribution. Using this algebraic representation and the analytical model, an approach to communication-efficient data redistribution is developed. Implementation results on the Intel iPSC/860 are reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesa-van, A. Malony, and B. Mohr. </author> <title> Implementing a parallel C++ runtime system for scalable parallel systems. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 588-597, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Distributed-memory machines have demonstrated a potential for high performance. However, their use has been restricted due to the difficulty of programming these computers. A programming model based on a single address space with provisions for explicit specification of data distributions for shared arrays has recently gained popularity <ref> [1, 3, 13] </ref>. High Performance Fortran (HPF) [6], a Fortran-90 extension, provides programmers with directives, that specify alignment of arrays with one another and distribute these aligned arrays on a user-defined virtual processor mesh. Directives to dynamically change the distribution of arrays during program execution are provided.
Reference: [2] <author> S. Bokhari. </author> <title> Communication overheads on the Intel iPSC-2 hypercube. </title> <type> Technical Report 10, </type> <institution> ICASE, </institution> <year> 1990. </year>
Reference-contexts: For circuit-switched and worm-hole routed net works, for large messages, in the absence of network contention, the time is nearly independent of the path length <ref> [2, 19] </ref>. <p> Path conflicts occur causing communication overheads. It was shown in <ref> [2] </ref>, that the impact of node conflicts is negligible, while that of link conflicts is significant. In general, data redistribution requires all-to-many personalized communication.
Reference: [3] <author> B. M. Chapman, P. Mehrotra, and H. P. Zima. </author> <title> Vienna Fortran a Fortran language extension for distributed memory multiprocessors. </title> <editor> In J. Saltz and P. Mehro-tra, editors, </editor> <booktitle> Language, Compilers and Runtime Environments for Distributed Memory Machines, </booktitle> <pages> pages 39-62. </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Distributed-memory machines have demonstrated a potential for high performance. However, their use has been restricted due to the difficulty of programming these computers. A programming model based on a single address space with provisions for explicit specification of data distributions for shared arrays has recently gained popularity <ref> [1, 3, 13] </ref>. High Performance Fortran (HPF) [6], a Fortran-90 extension, provides programmers with directives, that specify alignment of arrays with one another and distribute these aligned arrays on a user-defined virtual processor mesh. Directives to dynamically change the distribution of arrays during program execution are provided.
Reference: [4] <author> S. Chatterjee, J. R. Gilbert, F. J. E. Long, R. Schreiber, and S.-H. Teng. </author> <title> Generating local addresses and communication sets for data parallel programs. </title> <booktitle> In Proc. of ACM Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 149-158, </pages> <year> 1993. </year>
Reference-contexts: In [13], the inter-processor data communication for loops involving arrays with invertible index expressions is expressed as the composition of index and distribution functions. Schemes for efficiently executing HPF array statements for block-cyclically distributed arrays are presented in <ref> [4, 11, 21] </ref>. Schemes for communication generation and compile-time estimation of 1 communication costs are presented in [18]. These schemes are based on identifying communication primitives [7] by comparing index expressions of array references. Detailed techniques along the lines of [18] for estimation of communication cost are developed in [8].
Reference: [5] <author> S. Chatterjee, J. R. Gilbert, R. Schreiber, and S.-H. Teng. </author> <title> Optimal evaluation of array expressions on massively parallel machines. </title> <note> Technical Report CSL-92-11 1992, Xerox Corporations, </note> <institution> Palo Alto Research Center, </institution> <year> 1992. </year>
Reference-contexts: However, these schemes do not accurately identify the communication cost when the corresponding arrays are not identically dis-tributed. Schemes for generating alignments for arrays using cost met-rics for various interconnection networks are developed in <ref> [5] </ref>. The issue of efficient redistribution for arbitrary block-cyclic data distributions is addressed in [10]. The scheme presented therein performs data redistribution by communicating in one step, all the data as required by the source and target data distributions. We refer to this as a single-phase data redistribution strategy.
Reference: [6] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran langauge specification version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <year> 1993. </year>
Reference-contexts: However, their use has been restricted due to the difficulty of programming these computers. A programming model based on a single address space with provisions for explicit specification of data distributions for shared arrays has recently gained popularity [1, 3, 13]. High Performance Fortran (HPF) <ref> [6] </ref>, a Fortran-90 extension, provides programmers with directives, that specify alignment of arrays with one another and distribute these aligned arrays on a user-defined virtual processor mesh. Directives to dynamically change the distribution of arrays during program execution are provided.
Reference: [7] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, and J. Salmon. </author> <title> Solving Problems on Concurrent Processors. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Schemes for efficiently executing HPF array statements for block-cyclically distributed arrays are presented in [4, 11, 21]. Schemes for communication generation and compile-time estimation of 1 communication costs are presented in [18]. These schemes are based on identifying communication primitives <ref> [7] </ref> by comparing index expressions of array references. Detailed techniques along the lines of [18] for estimation of communication cost are developed in [8]. However, these schemes do not accurately identify the communication cost when the corresponding arrays are not identically dis-tributed.
Reference: [8] <author> M. Gupta and P. Banerjee. </author> <title> Compile-time estimation of communication costs on multicomputers. </title> <booktitle> In Proc. of Intl. Parallel Processing Symposium, </booktitle> <pages> pages 470-475, </pages> <year> 1993. </year>
Reference-contexts: Schemes for communication generation and compile-time estimation of 1 communication costs are presented in [18]. These schemes are based on identifying communication primitives [7] by comparing index expressions of array references. Detailed techniques along the lines of [18] for estimation of communication cost are developed in <ref> [8] </ref>. However, these schemes do not accurately identify the communication cost when the corresponding arrays are not identically dis-tributed. Schemes for generating alignments for arrays using cost met-rics for various interconnection networks are developed in [5]. The issue of efficient redistribution for arbitrary block-cyclic data distributions is addressed in [10].

Reference: [10] <author> S. K. S. Gupta, S. D. Kaushik, S. Mufti, S. Sharma, C.- H. Huang, and P. Sadayappan. </author> <title> On the generation of efficient data communication for distributed-memory machines. </title> <booktitle> In Proc. of Intl. Computing Symposium, </booktitle> <pages> pages 504-513, </pages> <year> 1992. </year>
Reference-contexts: The issue of reshaping perfect power-of-two sized data arrays on hypercubes is addressed in [15]. Closed form expressions for the processor sets which a processor needs to communicate with, and data sets that need to be communicated during data redistribution are constructed in <ref> [10] </ref>. Algorithms for runtime array redistribution are provided in [22]. In [17], closed-form expressions to characterize the data communication required in DoAll loops involving arrays with identical (block and cyclic) distributions and simple index expressions are developed. <p> However, these schemes do not accurately identify the communication cost when the corresponding arrays are not identically dis-tributed. Schemes for generating alignments for arrays using cost met-rics for various interconnection networks are developed in [5]. The issue of efficient redistribution for arbitrary block-cyclic data distributions is addressed in <ref> [10] </ref>. The scheme presented therein performs data redistribution by communicating in one step, all the data as required by the source and target data distributions. We refer to this as a single-phase data redistribution strategy. <p> If the arrays have only block or cyclic distribution, then the data index sets and the processor sets can be characterized using regular sections for closed forms <ref> [10, 11, 17] </ref>. For a general cyclic (b s ) to a cyclic (b d ) redistribution, closed form characterization of these sets using simple regular sections is not possible. However, when the source and destination distribution bases are compatible, closed forms can be evaluated.
Reference: [11] <author> S. K. S. Gupta, S. D. Kaushik, S. Mufti, S. Sharma, C.- H. Huang, and P. Sadayappan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <booktitle> In Proc. of Intl. Conf. on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 301-305, </pages> <year> 1993. </year>
Reference-contexts: In [13], the inter-processor data communication for loops involving arrays with invertible index expressions is expressed as the composition of index and distribution functions. Schemes for efficiently executing HPF array statements for block-cyclically distributed arrays are presented in <ref> [4, 11, 21] </ref>. Schemes for communication generation and compile-time estimation of 1 communication costs are presented in [18]. These schemes are based on identifying communication primitives [7] by comparing index expressions of array references. Detailed techniques along the lines of [18] for estimation of communication cost are developed in [8]. <p> If the arrays have only block or cyclic distribution, then the data index sets and the processor sets can be characterized using regular sections for closed forms <ref> [10, 11, 17] </ref>. For a general cyclic (b s ) to a cyclic (b d ) redistribution, closed form characterization of these sets using simple regular sections is not possible. However, when the source and destination distribution bases are compatible, closed forms can be evaluated.
Reference: [12] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler optimizations for Fortran-D on MIMD distributed-memory machines. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 86-100, </pages> <year> 1991. </year>
Reference-contexts: For instance, the alternate direction implicit method <ref> [12] </ref> consists of two phases the first phase accesses a two-dimensional array along the rows and the second phase along the columns. A distribution in which the array rows are local to a processor and the columns are distributed, eliminates communication for the first phase.
Reference: [13] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran-D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <booktitle> Language, Compilers and Runtime Environments for Distributed Memory Machines, </booktitle> <pages> pages 139-176. </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Distributed-memory machines have demonstrated a potential for high performance. However, their use has been restricted due to the difficulty of programming these computers. A programming model based on a single address space with provisions for explicit specification of data distributions for shared arrays has recently gained popularity <ref> [1, 3, 13] </ref>. High Performance Fortran (HPF) [6], a Fortran-90 extension, provides programmers with directives, that specify alignment of arrays with one another and distribute these aligned arrays on a user-defined virtual processor mesh. Directives to dynamically change the distribution of arrays during program execution are provided. <p> Algorithms for runtime array redistribution are provided in [22]. In [17], closed-form expressions to characterize the data communication required in DoAll loops involving arrays with identical (block and cyclic) distributions and simple index expressions are developed. In <ref> [13] </ref>, the inter-processor data communication for loops involving arrays with invertible index expressions is expressed as the composition of index and distribution functions. Schemes for efficiently executing HPF array statements for block-cyclically distributed arrays are presented in [4, 11, 21].
Reference: [14] <author> R.A. Horn and C.R. Johnson. </author> <title> Topics in Matrix Analysis. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1991. </year>
Reference-contexts: Preliminary performance results on the Intel iPSC/860 are reported in Section 5. Conclusions are presented in Section 6. 2 Regular Data Distributions In this section, we briefly describe the tensor product representation of regular data distributions. Details of the tensor product theory, are presented in <ref> [14] </ref>. HPF supports a two-level mapping of data objects to an abstract processor array. The language introduces a Cartesian grid referred to as a template. Array are aligned with a template and the templates are distributed onto a virtual processor array using regular data distributions.
Reference: [15] <author> S. L. Johnsson and C.-T. Ho. </author> <title> The complexity of reshaping arrays on boolean cubes. </title> <booktitle> In Proceedings of the Fifth Distributed Memory Computing Conference, </booktitle> <volume> Vol. 1, </volume> <pages> pages 370-377, </pages> <year> 1990. </year>
Reference-contexts: Thus efficient methods for performing data redistribution are of great importance in the distributed-memory implementations of HPF. Some data redistribution strategies have been presented in the literature. The issue of reshaping perfect power-of-two sized data arrays on hypercubes is addressed in <ref> [15] </ref>. Closed form expressions for the processor sets which a processor needs to communicate with, and data sets that need to be communicated during data redistribution are constructed in [10]. Algorithms for runtime array redistribution are provided in [22].
Reference: [16] <author> S.D. Kaushik, C.-H. Huang, J. R. Johnson, R. W. John-son, and P. Sadayappan. </author> <title> Efficient transposition algorithms for large matrices. </title> <type> Technical report, </type> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <year> 1994. </year>
Reference-contexts: Heuristics for the general case when Q (fi s ; fi d ) is not a perfect power are provided in <ref> [16] </ref>. 5 Performance Results In this section, we compare the performance of the multi-phase and single-phase strategies for specific source and target data distributions on a 32-node Intel iPSC/860 hypercube. Due to lack of access to a mesh-connected computer, the strategies could not be compared for a mesh network.
Reference: [17] <author> C. Koelbel. </author> <title> Compile-time generation of communication for scientific programs. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 101-110, </pages> <year> 1991. </year>
Reference-contexts: Closed form expressions for the processor sets which a processor needs to communicate with, and data sets that need to be communicated during data redistribution are constructed in [10]. Algorithms for runtime array redistribution are provided in [22]. In <ref> [17] </ref>, closed-form expressions to characterize the data communication required in DoAll loops involving arrays with identical (block and cyclic) distributions and simple index expressions are developed. In [13], the inter-processor data communication for loops involving arrays with invertible index expressions is expressed as the composition of index and distribution functions. <p> If the arrays have only block or cyclic distribution, then the data index sets and the processor sets can be characterized using regular sections for closed forms <ref> [10, 11, 17] </ref>. For a general cyclic (b s ) to a cyclic (b d ) redistribution, closed form characterization of these sets using simple regular sections is not possible. However, when the source and destination distribution bases are compatible, closed forms can be evaluated.
Reference: [18] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <year> 1991. </year>
Reference-contexts: Schemes for efficiently executing HPF array statements for block-cyclically distributed arrays are presented in [4, 11, 21]. Schemes for communication generation and compile-time estimation of 1 communication costs are presented in <ref> [18] </ref>. These schemes are based on identifying communication primitives [7] by comparing index expressions of array references. Detailed techniques along the lines of [18] for estimation of communication cost are developed in [8]. <p> Schemes for communication generation and compile-time estimation of 1 communication costs are presented in <ref> [18] </ref>. These schemes are based on identifying communication primitives [7] by comparing index expressions of array references. Detailed techniques along the lines of [18] for estimation of communication cost are developed in [8]. However, these schemes do not accurately identify the communication cost when the corresponding arrays are not identically dis-tributed. Schemes for generating alignments for arrays using cost met-rics for various interconnection networks are developed in [5].
Reference: [19] <author> L. Ni and P. K. McKinley. </author> <title> A survey of wormhole routing techniques in direct networks. </title> <journal> IEEE Computer, </journal> <volume> 3 </volume> <pages> 62-76, </pages> <year> 1993. </year>
Reference-contexts: A survey of these routing techniques is presented in <ref> [19] </ref>. <p> For circuit-switched and worm-hole routed net works, for large messages, in the absence of network contention, the time is nearly independent of the path length <ref> [2, 19] </ref>.
Reference: [20] <author> D. K. Panda. </author> <title> Optimal phase barrier synchronization in k-ary n-cube wormhole-routed systems using mul-tirendezvous primitives. </title> <booktitle> In Workshop on Fine Grain Massively Parallel Coordination, </booktitle> <pages> pages 24-26, </pages> <year> 1993. </year>
Reference-contexts: An O (p) algorithm for finding the optimal partition is presented in <ref> [20] </ref>. This case is the most frequently occurring case as the number of physical processors are usually perfect powers of two and difference in the distribution bases will be a perfect power of two.
Reference: [21] <author> J. M. Stichnoth. </author> <title> Efficient compilation of array statements for private memory multicomputers. </title> <type> Technical Report CMU-CS-93-109, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: In [13], the inter-processor data communication for loops involving arrays with invertible index expressions is expressed as the composition of index and distribution functions. Schemes for efficiently executing HPF array statements for block-cyclically distributed arrays are presented in <ref> [4, 11, 21] </ref>. Schemes for communication generation and compile-time estimation of 1 communication costs are presented in [18]. These schemes are based on identifying communication primitives [7] by comparing index expressions of array references. Detailed techniques along the lines of [18] for estimation of communication cost are developed in [8].
Reference: [22] <author> R. Thakur, A. Choudhary, and G. Fox. </author> <title> Runtime array redistribution in HPF programs. </title> <booktitle> In Proc. of Scalable High Performance Computing Conference, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Closed form expressions for the processor sets which a processor needs to communicate with, and data sets that need to be communicated during data redistribution are constructed in [10]. Algorithms for runtime array redistribution are provided in <ref> [22] </ref>. In [17], closed-form expressions to characterize the data communication required in DoAll loops involving arrays with identical (block and cyclic) distributions and simple index expressions are developed.
Reference: [23] <author> A. Youssef. </author> <title> Off-line permutation scheduling on circuit switched fixed routing networks. </title> <booktitle> In Proc. Frontiers of Massively Parallel Processing'92, </booktitle> <pages> pages 389-396, </pages> <year> 1992. </year>
Reference-contexts: If the underlying topology is a 3 fi 3 mesh then each of the permutations, Q 0 ; Q 1 and Q 2 , can be scheduled in three contention-free steps, using scheduling techniques presented in <ref> [23, 24] </ref>. Thus the maximum communication time for the data redistribution will correspond to the time for performing the three permutations.
Reference: [24] <author> A. Youssef. </author> <title> On-line communication on circuit switched fixed routing networks. </title> <booktitle> In Proc. of Intl. Parallel Processing Symposium, </booktitle> <pages> pages 389-396, </pages> <year> 1992. </year> <month> 10 </month>
Reference-contexts: If the underlying topology is a 3 fi 3 mesh then each of the permutations, Q 0 ; Q 1 and Q 2 , can be scheduled in three contention-free steps, using scheduling techniques presented in <ref> [23, 24] </ref>. Thus the maximum communication time for the data redistribution will correspond to the time for performing the three permutations.
References-found: 23

