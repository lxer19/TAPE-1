URL: ftp://borneo.gmd.de/pub/as/janus/pre_6.ps
Refering-URL: http://borneo.gmd.de/AS/janus/publi/publi.html
Root-URL: 
Email: e-mail: (Internet) smieja@gmdzi.gmd.de, (Bitnet) smieja@gmdzi.uucp  
Title: Submitted to Circuits, Systems and Signal Processing Neural Network Constructive Algorithms: Trading Generalization for Learning Efficiency?  
Author: F.J. Smieja 
Date: November 22, 1991  
Address: Schlo Birlinghoven, 5205 St. Augustin 1, Germany.  
Affiliation: German National Research Centre for Computer Science (GMD),  
Abstract: There are currently several types of constructive, or growth, algorithms available for training a feed-forward neural network. This paper describes and explains the main ones, using a fundamental approach to the multi-layer perceptron problem-solving mechanisms. The claimed convergence properties of the algorithms are verified using just two mapping theorems, which consequently enables all the algorithms to be unified under a basic mechanism. The algorithms are compared and contrasted and the deficiencies of some highlighted. The fundamental reasons for the actual success of these algorithms are extracted, and used to suggest where they might most fruitfully be applied. A suspicion that they are not a panacea for all current neural network difficulties, and that one must somewhere along the line pay for the learning efficiency they promise, is developed into an argument that their generalization abilities will lie on average below that of back-propagation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Ahmad and G. Tesauro. </author> <title> A study of scaling and generalization in neural networks. </title> <booktitle> In Abstracts of the First Annual Meeting of the INNS, supplement to Neural Networks, Vol 1, </booktitle> <address> Boston, Mass., </address> <month> September </month> <year> 1988. </year>
Reference-contexts: BP find the right solution, either the architecture of the network is designed in a way the designer thinks might be appropriate to the task [12], or the coding of the input patterns is made more favourable [12, 25], or specific significant patterns only are included in the training set <ref> [24, 31, 1] </ref>. <p> All hyperplanes will be affected in their motion to a greater or lesser extent by a particular pattern. The sharpness of the decision itself serves to determine whether this influence is significant or not. It was pointed out empirically in <ref> [1, 24] </ref> and analytically in [28], that patterns nearest a hyperplane have the greatest influence on where it will ultimately be positioned. This results in a different characteristic set of pattern representation overlaps in a back-propagation-trained network and in a constructive network, where hyperplanes are positioned practically one after another.
Reference: [2] <author> T. Ash. </author> <title> Dynamic node creation in backpropagation networks. </title> <journal> Connection Science, </journal> <volume> 1(4), </volume> <year> 1989. </year>
Reference-contexts: Dynamic node creation (DNC) as described by Ash in <ref> [2] </ref> obtains its cues for whether to add new nodes directly from the current success rate of the learning algorithm, and in this sense may not so much find a minimal architecture for the network rather than ensure that a high rate of optimization is maintained, through the introduction of more
Reference: [3] <editor> K.P. Bennett and O.L. Mangasarian. </editor> <title> Neural network training via linear programming. </title> <type> Technical Report 948, </type> <institution> Computer sciences department, University of Wisconsin-Madison, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Both approaches basically decide on the separation hyperplanes to be used for this training set, and then transfer the results simply to a network architecture. 3.1 The Multisurface Method Bennett and Mangasarian <ref> [3] </ref> approach the problem of neural network mapping from the following angle: a set of vectors exists (the training set), and we are required to classify them into one of two groups. <p> However, it may not be so easy on scaling the systems, since MR relies on a search procedure through an exponentially (with N I ) increasing number of planes. The scaling dependence of MSM was not given in <ref> [3] </ref>. One notices too that the MSM representations, in the hidden layer of the neural network finally constructed, are of similar form to the interim Tiling representations.
Reference: [4] <author> J. Denker, D. Schwartz, B. Wittner, S. Solla, R. Howard, L. Jackel, and J. </author> <title> Hopfield. Large automatic learning, rule extraction and generalization. </title> <journal> Complex Systems, </journal> <volume> 1(5), </volume> <year> 1987. </year> <title> Constructive Algorithms 32 </title>
Reference-contexts: 1 Introduction Recently a number of algorithms have appeared in the literature concerned with the issue of building the structure of a neural network as it is trained. They attempt to surmount the complexity barrier <ref> [21, 14, 17, 4, 26] </ref> concerning learning in such networks, one of the major neural network issues in the 20 years after Perceptrons [17]. <p> The method involves doing precisely the opposite to what is assumed in complexity arguments, namely starting with a system whose capabilities are severely constrained by its structure. The general argument against network scalability is based on the solution space growing at least exponentially with network size <ref> [30, 4, 17, 32, 33, 18] </ref> and likewise the failure probability of a general-purpose network in finding the desired solution.
Reference: [5] <author> S.E. Fahlman. </author> <title> Faster-learning variations on back-propagation: An empirical study. </title> <booktitle> In Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <year> 1988. </year>
Reference-contexts: In its single output unit form the cascade correlation algorithm is described as follows. Initially, as in the upstart algorithm case, we take an input layer and a single output node, with no hidden node present. Using either the PLA or something similar (Fahlman uses his own Quickprop <ref> [5] </ref> algorithm, which enables large steps to be taken over the error surface in a stable manner) the weights between input and output unit are optimized for the pattern set P, i.e. the output unit can do this well on the patterns and no better by itself.
Reference: [6] <author> S.E. Fahlman and C. Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <publisher> Morgen Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Thus convergence is guaranteed. 2.2 Cascade Correlation Although the cascade correlation algorithm is described by Fahlman and Lebiere <ref> [6] </ref> in a way that assumes more than one output unit from the start, we reduce it to the single output unit case. In the paper the unit states are taken to be in f1; 1g, and the candidate units (see later) require differentiable activation functions. <p> Are the originally correct patterns not then always correctly separated, and one can work solely on the errant ones? It would seem to be a securer way of solving the mapping. In the original description <ref> [6] </ref> the input patterns are represented in a progressively larger space and the hyperplane generated by the output weights is always attempting to separate the patterns totally unconstrained. <p> It is therefore also based on the same ideas as the Tiling algorithm (since Pointing is a derivation of Tiling). The Inverted Pyramid idea can also be seen to be very similar to the Cascade Correlation principle. This fact was also recognized in <ref> [6] </ref>. The idea of increasing the dimension of the representation space through taking inputs from all the previous output units is clearly identical. <p> The generalization information resides in a network through the way in which it codes representations of the patterns it has seen. The difference between the form of the representations produced by the constructive algorithms and those formed by the back-propagation algorithm results from the "moving-targets" <ref> [6] </ref> procedure of back-propagation and the incremental method of the constructive algorithms. Back-propagation forms its sets of decision hyperplanes by evolving their motion by the set of training patterns in a global manner [28].
Reference: [7] <author> M. Frean. </author> <title> The upstart algorithm: a method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 198-209, </pages> <year> 1990. </year>
Reference-contexts: The brief outlines of the algorithms follow closely the methods used by the respective authors in their original papers, although we will tend to use less mathematical language where possible. 2.1 The Upstart algorithm Frean's Upstart Algorithm <ref> [7] </ref> approaches the task of network learning in the following way. Consider the problem of mapping a set of input patterns to a set of corresponding output values. <p> The method then is to determine for which patterns the output unit is incorrectly responding, and to attempt to correct this behaviour through use of "daughter units". In <ref> [7] </ref> the unit states are taken to be in f0; 1g. The procedure is as follows. <p> Upstart is somewhat farther behind, graded lower because of its explicit dimension lifting. It would be higher up for multiple output units, when, as suggested by Frean <ref> [7] </ref>, hidden units are shared. The geometric algorithms are rated lower still because of their boolean sorting nature. MSM is likely to have good generalization only for large convex classification problems.
Reference: [8] <author> R. M. </author> <title> French. Using semi-distributed representations to overcome catastrophic forgetting in connectionist networks. </title> <type> Technical Report CRCC-51-1991, </type> <institution> Indiana University, Bloomington, Indiana, </institution> <year> 1991. </year>
Reference-contexts: This rates it highly in the generalization capability scale. On the other hand, procedures that generate GM or near-GM representations (exceptions) will have correspondingly poorer generalization. This was also pointed out in <ref> [8] </ref>, where a trade-off was attempted between generalization and non-destructive learning (see also [28]). So GM stands at the other end of the scale. DNC is rated here highly because of its allowance of representation overlap through not fixing any weights, and because it uses back-propagation.
Reference: [9] <author> S.I. Gallant. </author> <title> Three constructive algorithms for network learning. </title> <booktitle> In Program od the 8th Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsday, NJ, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: This guarantees convergence of this algorithm. 5.7 Tiling, Pointing, Cascade and Upstart: children of Towers and Pyra mids A relatively old paper by Gallant <ref> [9] </ref> briefly expounds on two constructive algorithms that can be seen to be legitimate forerunners of the algorithms presented here, in spite of the fact that they are very simply presented, and in the published paper are not accompanied by derivations or proofs of convergence theorems.
Reference: [10] <author> S.I. Gallant. </author> <title> Perceptron-based learning algorithms. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2), </volume> <year> 1990. </year>
Reference-contexts: Consider the problem of mapping a set of input patterns to a set of corresponding output values. If we start using no hidden units and the perceptron learning algorithm (PLA) [20] (or more efficient modifications like Gallant's pocket algorithm <ref> [10] </ref>), then there will be a point at which no further improvement in the (linearly inseparable) input-output mappings can be made. The method then is to determine for which patterns the output unit is incorrectly responding, and to attempt to correct this behaviour through use of "daughter units".
Reference: [11] <author> I. Guyon. </author> <title> Applications of neural networks to character recognition. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 5(1-2):353-382, </volume> <year> 1991. </year>
Reference-contexts: The view that comes naturally from our analyses is only for particular types of problems. When generalization is to be achieved from a large number of examples of a small number of slightly overlapping classes (low complexity M c quotients), like handwritten digit recognition <ref> [11] </ref>, or for "numerical" problems like parity, where generalization is more regularized, then the constructive algorithms are the ones to produce greatest efficiency with negligible generalization loss.
Reference: [12] <author> G. E. Hinton. </author> <title> Learning distributed representations of concepts. </title> <booktitle> In Eighth Annual Conference of the Cognitive Science Society, </booktitle> <year> 1986. </year>
Reference-contexts: It is interesting to note that in order to speed up the optimization, or to "help" BP find the right solution, either the architecture of the network is designed in a way the designer thinks might be appropriate to the task <ref> [12] </ref>, or the coding of the input patterns is made more favourable [12, 25], or specific significant patterns only are included in the training set [24, 31, 1]. <p> to note that in order to speed up the optimization, or to "help" BP find the right solution, either the architecture of the network is designed in a way the designer thinks might be appropriate to the task [12], or the coding of the input patterns is made more favourable <ref> [12, 25] </ref>, or specific significant patterns only are included in the training set [24, 31, 1]. <p> By the first type we mean problems like parity and symmetry [23] in a broad sense, and by the second type the patterns with "underlying trends", such as second order protein structure prediction, or the "family" example of Hinton <ref> [12] </ref>. Now both types of problem may be generalized, but the second type tends to require a more interdependent type of hidden representation than the first. This is expressed in the form of the set of rules describing the problem domain.
Reference: [13] <author> R. A. Jacobs and M. I. Jordan. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1), </volume> <year> 1991. </year>
Reference-contexts: We are however less inclined to use merely constructive algorithms to pull us out of the network complexity scaling and speed traps. We would support a higher-level solution: not allow network size explosion, by using modular systems <ref> [18, 29, 13, 34] </ref>. Here the networks may be kept to a reasonable size so that the better generalizing back-propagation algorithm may be used without it crunching to a crawl or searching in a hopelessly large space.
Reference: [14] <author> S. Judd. </author> <title> Learning in networks is hard. </title> <editor> In M. Caudill and C. Butler, editors, </editor> <booktitle> Proceedings of IEEE First International Conference on Neural Networks, </booktitle> <address> San Diego, </address> <pages> pages 685-692, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Recently a number of algorithms have appeared in the literature concerned with the issue of building the structure of a neural network as it is trained. They attempt to surmount the complexity barrier <ref> [21, 14, 17, 4, 26] </ref> concerning learning in such networks, one of the major neural network issues in the 20 years after Perceptrons [17].
Reference: [15] <author> O.L. Mangasarian. </author> <title> Multisurface method of pattern separation. </title> <journal> IEEE Transactions on information theory, </journal> <volume> IT-14(6):801-807, </volume> <year> 1968. </year>
Reference-contexts: Two illustrations of such an approach are the construction of a piecewise-linear classification surface in the input vector space using the Multisurface Method (MSM) of Mangasarian <ref> [15] </ref>, and a more opportunistic slicing up of the input vector space, using the Minimal Resources method of Rujan and Marchand. <p> First a method is described for finding the optimal hyperplane for separating two linearly separable sets of vectors (patterns) using the following algorithm <ref> [15] </ref>: maximize ff; fi; w &lt; ff fi j Aw eff; Bw efi; e w e = (6) where the two vector sets to be separated, A and B, are represented by the matrices A and B respectively, e is a vector filled with ones, w is the weight vector defining <p> With a zero weight matrix no information is gained from the system. Therefore the algorithm (6) was modified by Bennett and Mangasarian (originally Mangasarian <ref> [15] </ref>) in order to allow for the possibility of constructing the best hyperplanes to separate the currently non-separated patterns.
Reference: [16] <author> M. Mezard and J-P. Nadal. </author> <title> Learning in feedforward layered networks: The tiling algorithm. </title> <journal> Journal of Physics A, </journal> <volume> 22(12) </volume> <pages> 2191-2203, </pages> <year> 1989. </year>
Reference-contexts: It may not be the optimal architecture, but results imply that the network sizes are always smaller than the corresponding conventional networks. 2.3 The Tiling algorithm The Tiling algorithm was developed by Mezard and Nadal in <ref> [16] </ref> with more of a mathematical emphasis, and as such it is quite thorough in its definitions and convergence proof. The principle itself though is also very simple and can be easily explained. Once again we start with the input layer totally connected to the output unit. <p> A simple theorem makes it clear that it is always possible for the mappings of the patterns at the master unit in the layer L (counting from 1 Mezard and Nadal <ref> [16] </ref> mention that they continued with the same ancillary unit until the mapping failed; it is not clear to this author that this is at all a stable procedure, so for the purpose of this description we do not use this apparent short-cut.
Reference: [17] <author> M. Minsky and S. Papert. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference-contexts: 1 Introduction Recently a number of algorithms have appeared in the literature concerned with the issue of building the structure of a neural network as it is trained. They attempt to surmount the complexity barrier <ref> [21, 14, 17, 4, 26] </ref> concerning learning in such networks, one of the major neural network issues in the 20 years after Perceptrons [17]. <p> They attempt to surmount the complexity barrier [21, 14, 17, 4, 26] concerning learning in such networks, one of the major neural network issues in the 20 years after Perceptrons <ref> [17] </ref>. The method involves doing precisely the opposite to what is assumed in complexity arguments, namely starting with a system whose capabilities are severely constrained by its structure. <p> The method involves doing precisely the opposite to what is assumed in complexity arguments, namely starting with a system whose capabilities are severely constrained by its structure. The general argument against network scalability is based on the solution space growing at least exponentially with network size <ref> [30, 4, 17, 32, 33, 18] </ref> and likewise the failure probability of a general-purpose network in finding the desired solution.
Reference: [18] <author> H. Muhlenbein. </author> <title> Limitations of multilayer perceptrons steps towards genetic neural networks. </title> <journal> Parallel Computing, </journal> <volume> 14(3) </volume> <pages> 249-260, </pages> <year> 1990. </year>
Reference-contexts: The method involves doing precisely the opposite to what is assumed in complexity arguments, namely starting with a system whose capabilities are severely constrained by its structure. The general argument against network scalability is based on the solution space growing at least exponentially with network size <ref> [30, 4, 17, 32, 33, 18] </ref> and likewise the failure probability of a general-purpose network in finding the desired solution. <p> For parity the value of AB will be 1, and the value M c will be also have its maximum value of 1 (for any dimension). For the "=1" problem (see <ref> [18] </ref>) the maximum value is for N I = 3 (disregarding the case N I = 2, which is parity) where AB = 0:75, at M c = 0:56. <p> We are however less inclined to use merely constructive algorithms to pull us out of the network complexity scaling and speed traps. We would support a higher-level solution: not allow network size explosion, by using modular systems <ref> [18, 29, 13, 34] </ref>. Here the networks may be kept to a reasonable size so that the better generalizing back-propagation algorithm may be used without it crunching to a crawl or searching in a hopelessly large space.
Reference: [19] <author> J-P. Nadal. </author> <title> Study of a growth algorithm for a feedforward neural network. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(1) </volume> <pages> 55-59, </pages> <year> 1989. </year>
Reference-contexts: A possible end configuration as shown in figure 3. 2.4 Nadal's Tiling variant Shortly after the Tiling paper appeared, one of the authors, Nadal, released a paper <ref> [19] </ref> describing another growth algorithm that he considered to be "similar in spirit to the Tiling algorithm". It merits however a separate description because of its input unit connections, and because it will be seen to fill in a gap in the list of possibilities for constructive algorithms.
Reference: [20] <author> F. Rosenblatt. </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan Books, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: Consider the problem of mapping a set of input patterns to a set of corresponding output values. If we start using no hidden units and the perceptron learning algorithm (PLA) <ref> [20] </ref> (or more efficient modifications like Gallant's pocket algorithm [10]), then there will be a point at which no further improvement in the (linearly inseparable) input-output mappings can be made.
Reference: [21] <author> P. Rujan. </author> <title> Reliable training required negative examples. </title> <journal> Neural Network Review, </journal> <volume> 3(3) </volume> <pages> 123-125, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Recently a number of algorithms have appeared in the literature concerned with the issue of building the structure of a neural network as it is trained. They attempt to surmount the complexity barrier <ref> [21, 14, 17, 4, 26] </ref> concerning learning in such networks, one of the major neural network issues in the 20 years after Perceptrons [17].
Reference: [22] <author> P. Rujan and M. Marchand. </author> <title> Learning by minimizing resources in neural networks. </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 229-241, </pages> <year> 1989. </year>
Reference-contexts: In their paper <ref> [22] </ref> they also approach the problem from the pure generation of hyperplanes perspective and achieve this through a class of plane-cutting algorithms. The hyperplanes once discovered are mapped through the solution of a set of linear equations onto network weight values. First the concept of a "regular partitioning" is described. <p> In MSM there also exist different representations in the hidden layer, but they are so ordered that each representation is labelled by the left-most firing hidden unit. They are equivalent: both models end up with N H + 1 representations. Theorem 5 (Rujan and Marchand <ref> [22] </ref>) A regular partitioning leads to N H + 1 representations in the hidden space, and the guarantee of linear separability of these representations. Proof: We would like to show this to be the case in a simpler way than in [22]. <p> Theorem 5 (Rujan and Marchand <ref> [22] </ref>) A regular partitioning leads to N H + 1 representations in the hidden space, and the guarantee of linear separability of these representations. Proof: We would like to show this to be the case in a simpler way than in [22]. That there are N H + 1 representations results from the fact that the N H hyperplanes separate the entire (binary) input space into N H + 1 partitions when they are non-intersecting. Every pattern must fall into one of these partitions, and thus assume a particular hidden-space representation.
Reference: [23] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <journal> Nature, </journal> <volume> 323(533), </volume> <year> 1986. </year>
Reference-contexts: Another reason for the search for small compact networks that can be trained progressively is one of time. Raw neural nets with many degrees of freedom and universal problem solving capabilities would be a nice idea if they could be trained relatively quickly. Back-propagation (BP) <ref> [23] </ref> is slow, and is not designed to find a solution|it is designed to find a local minimum. <p> The algorithm itself is fairly simple to describe. One does not assume binary units, primarily because differentiable units are required in order to allow back-propagation learning <ref> [23] </ref> to take place. The effect of this form of network structuring however is independent of exact type of the unit response function, and we may approximate the sigmoidal function used by the linear threshold units used in the other algorithms. <p> Two types of problem can be easily identified: the "logical" or "number theory" type, and the "natural" type. By the first type we mean problems like parity and symmetry <ref> [23] </ref> in a broad sense, and by the second type the patterns with "underlying trends", such as second order protein structure prediction, or the "family" example of Hinton [12].
Reference: [24] <author> A. Schutte. </author> <title> How can multi-layer perceptrons classify. </title> <booktitle> In Proceedings of the Workshop on Distributed Adaptive Neural Information Processing, GMD, </booktitle> <address> St Augustin, April 1989. </address> <publisher> Oldenbourg Verlag. </publisher>
Reference-contexts: BP find the right solution, either the architecture of the network is designed in a way the designer thinks might be appropriate to the task [12], or the coding of the input patterns is made more favourable [12, 25], or specific significant patterns only are included in the training set <ref> [24, 31, 1] </ref>. <p> All hyperplanes will be affected in their motion to a greater or lesser extent by a particular pattern. The sharpness of the decision itself serves to determine whether this influence is significant or not. It was pointed out empirically in <ref> [1, 24] </ref> and analytically in [28], that patterns nearest a hyperplane have the greatest influence on where it will ultimately be positioned. This results in a different characteristic set of pattern representation overlaps in a back-propagation-trained network and in a constructive network, where hyperplanes are positioned practically one after another.
Reference: [25] <author> T. J. Sejnowski and C. R. Rosenberg. NETtalk: </author> <title> A parallel network that learns to read aloud. </title> <journal> Complex Systems, </journal> <volume> 1(1), </volume> <year> 1987. </year> <title> Constructive Algorithms 33 </title>
Reference-contexts: to note that in order to speed up the optimization, or to "help" BP find the right solution, either the architecture of the network is designed in a way the designer thinks might be appropriate to the task [12], or the coding of the input patterns is made more favourable <ref> [12, 25] </ref>, or specific significant patterns only are included in the training set [24, 31, 1].
Reference: [26] <author> H. Shvaytser. </author> <title> Even simple nets cannot be trained reliably with a polynomial number of examples. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> 2, </volume> <pages> pages 141-145, </pages> <address> Washington D.C., June 1989. </address> <publisher> IEEE Press, </publisher> <address> N.Y. </address>
Reference-contexts: 1 Introduction Recently a number of algorithms have appeared in the literature concerned with the issue of building the structure of a neural network as it is trained. They attempt to surmount the complexity barrier <ref> [21, 14, 17, 4, 26] </ref> concerning learning in such networks, one of the major neural network issues in the 20 years after Perceptrons [17].
Reference: [27] <author> F. J. Smieja. </author> <title> MLP solutions, generalization and hidden-unit representations. </title> <booktitle> In Proceedings of the Workshop on Distributed Adaptive Neural Information Processing, </booktitle> <address> St Augustin, Germany, </address> <month> April </month> <year> 1989. </year> <note> Oldenbourg Verlag. See also Edinburgh University Physics preprint 89/461. </note>
Reference-contexts: This is how we will compare the algorithms considered in this paper. The picture in figure 12 was constructed through guessing at the probability that exceptions will automatically be generated in the various networks. It is very low for back-propagation. Indeed, in <ref> [27] </ref> it was shown using an artificial diagnosis problem how generalization was not at all disturbed as the number of hidden units (and therefore hyperplanes) was increased.
Reference: [28] <author> F. J. Smieja. </author> <title> Hyperplane "spin" dynamics, network plasticity and back-propagation learning. </title> <type> Technical report, </type> <institution> Gesellschaft fur Mathematik und Datenverarbeitung, </institution> <address> St Augustin, Germany, </address> <year> 1991. </year>
Reference-contexts: DNC's basic problem is that it adds new weights in the input space and in the hidden space simultaneously. In <ref> [28] </ref> it was shown how the development of the input space hyperplane can only be significantly affected by the errors in the output layer when the input space hyperplane has a real correlation with (influence on) the orientation of the hidden space hyperplane. <p> They may find themselves in not useful orientations or positions out of which they may not easily be able to escape <ref> [28] </ref>. <p> Back-propagation forms its sets of decision hyperplanes by evolving their motion by the set of training patterns in a global manner <ref> [28] </ref>. All hyperplanes will be affected in their motion to a greater or lesser extent by a particular pattern. The sharpness of the decision itself serves to determine whether this influence is significant or not. It was pointed out empirically in [1, 24] and analytically in [28], that patterns nearest a <p> in a global manner <ref> [28] </ref>. All hyperplanes will be affected in their motion to a greater or lesser extent by a particular pattern. The sharpness of the decision itself serves to determine whether this influence is significant or not. It was pointed out empirically in [1, 24] and analytically in [28], that patterns nearest a hyperplane have the greatest influence on where it will ultimately be positioned. This results in a different characteristic set of pattern representation overlaps in a back-propagation-trained network and in a constructive network, where hyperplanes are positioned practically one after another. <p> This rates it highly in the generalization capability scale. On the other hand, procedures that generate GM or near-GM representations (exceptions) will have correspondingly poorer generalization. This was also pointed out in [8], where a trade-off was attempted between generalization and non-destructive learning (see also <ref> [28] </ref>). So GM stands at the other end of the scale. DNC is rated here highly because of its allowance of representation overlap through not fixing any weights, and because it uses back-propagation. The fixed-weight DNC is located lower in the estimation. <p> One more remark should be made with respect to the way in which back-propagation generates its decision regions. When many hidden units are used, this increases the chance of finding good hyperplanes, but it does not necessarily degrade the generalization. It was shown in <ref> [28] </ref> and noticed in [31] that the "unrequired" hyperplanes do not evolve in terms of importance.
Reference: [29] <author> F. J. Smieja. </author> <title> Multiple network systems (MINOS) modules: Task division and module discrimination. </title> <booktitle> In Proceedings of the 8th AISB conference on Artificial Intelligence, </booktitle> <address> Leeds, </address> <month> 16-19 April, </month> <year> 1991, 1991. </year>
Reference-contexts: We are however less inclined to use merely constructive algorithms to pull us out of the network complexity scaling and speed traps. We would support a higher-level solution: not allow network size explosion, by using modular systems <ref> [18, 29, 13, 34] </ref>. Here the networks may be kept to a reasonable size so that the better generalizing back-propagation algorithm may be used without it crunching to a crawl or searching in a hopelessly large space.
Reference: [30] <author> F. J. Smieja and H. Muhlenbein. </author> <title> The geometry of multilayer perceptron solutions. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 261-275, </pages> <year> 1990. </year>
Reference-contexts: The method involves doing precisely the opposite to what is assumed in complexity arguments, namely starting with a system whose capabilities are severely constrained by its structure. The general argument against network scalability is based on the solution space growing at least exponentially with network size <ref> [30, 4, 17, 32, 33, 18] </ref> and likewise the failure probability of a general-purpose network in finding the desired solution. <p> In order to understand how the models work, and to keep a perspective of what is actually being done, we always refer to the two geometric interpretations of the feed-forward network: the hyper-plane interpretation and the hidden-unit (hu) representation interpretation. These interpretations were employed in <ref> [30] </ref> to define and count "solitots" (solutions invariant to translations and orthogonal transformations) in order to place an upper limit on the probability that a general purpose network will generalize. The paper is structured as follows. <p> Constructive Algorithms 11 3. the hyperplanes do not intersect within the hypercube formed by the set of all possible binary input pattern points. One can prove trivially using these conditions that once an input space is regularly partitioned, the resulting representations in the hidden space <ref> [30] </ref> are linearly separable (see section 5.3). Rujan and Marchand then proceed to describe a plane-cutting algorithm that will generate the desired regular partitionings. The method described uses a greedy algorithm to select appropriate partitions of the input space. <p> We have previously referred to them as the hyperplane interpretation and the hidden-unit (hu) representation interpretation. They are in fact equivalent, as was shown in <ref> [30] </ref>, and the one more suitable for the understanding of the mapping depends on the network's structure. <p> Each dimension of such a hidden-unit vector space, viewed in a cartesian coordinate system, is defined by a hidden unit activation value. The space is defined by all the vectors that can be generated by running through the range of values that can be adopted by the hidden units. <ref> [30] </ref> provides definitions of these two interpretations with respect to the existence of identical MLP solutions. The fundamental job of an MLP is to separate sets of patterns into classes. <p> Finding solutions requiring less layers and units represents optimizing the mapping learnt. In this case the optimal number of hidden units required in a conventional feed-forward structure is equal to N I = 3, and that in one layer <ref> [30] </ref>. 4.4 Solution through expanding a layer In order to solve a mapping problem using only one hidden layer the network is required to transform the input patterns into a space of dimension N h N p (where N p is the total number of patterns in the training set) such <p> MR requirement that new hyperplanes do not intersect with old hyperplanes within the region occupied by the patterns is always satisfiable with (at least) a binary input space (once the classified patterns have been removed from the input space the solutions with and without hyperplane intersection represent the same solitot <ref> [30] </ref>). It is not actually necessary to think of such things as non-intersection in the MSM formulation because hyperplanes are considered in order when the taught system is operated, and the first positive classification is used.
Reference: [31] <author> F. J. Smieja and G.D. Richards. </author> <title> Hard learning the easy way: Backpropagation with deformation. </title> <journal> Complex Systems, </journal> <volume> 2(4), </volume> <year> 1988. </year>
Reference-contexts: BP find the right solution, either the architecture of the network is designed in a way the designer thinks might be appropriate to the task [12], or the coding of the input patterns is made more favourable [12, 25], or specific significant patterns only are included in the training set <ref> [24, 31, 1] </ref>. <p> One more remark should be made with respect to the way in which back-propagation generates its decision regions. When many hidden units are used, this increases the chance of finding good hyperplanes, but it does not necessarily degrade the generalization. It was shown in [28] and noticed in <ref> [31] </ref> that the "unrequired" hyperplanes do not evolve in terms of importance.
Reference: [32] <author> G. Tesauro. </author> <title> Scaling relationships in backpropagation learning: dependence on training set size. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 367-372, </pages> <year> 1987. </year>
Reference-contexts: The method involves doing precisely the opposite to what is assumed in complexity arguments, namely starting with a system whose capabilities are severely constrained by its structure. The general argument against network scalability is based on the solution space growing at least exponentially with network size <ref> [30, 4, 17, 32, 33, 18] </ref> and likewise the failure probability of a general-purpose network in finding the desired solution.
Reference: [33] <author> G. Tesauro and B. Janssens. </author> <title> Scaling relationships in backpropagation learning. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 39-44, </pages> <year> 1988. </year>
Reference-contexts: The method involves doing precisely the opposite to what is assumed in complexity arguments, namely starting with a system whose capabilities are severely constrained by its structure. The general argument against network scalability is based on the solution space growing at least exponentially with network size <ref> [30, 4, 17, 32, 33, 18] </ref> and likewise the failure probability of a general-purpose network in finding the desired solution.
Reference: [34] <author> A. Waibel. </author> <title> Modular construction of time-delay neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <year> 1989. </year>
Reference-contexts: We are however less inclined to use merely constructive algorithms to pull us out of the network complexity scaling and speed traps. We would support a higher-level solution: not allow network size explosion, by using modular systems <ref> [18, 29, 13, 34] </ref>. Here the networks may be kept to a reasonable size so that the better generalizing back-propagation algorithm may be used without it crunching to a crawl or searching in a hopelessly large space.
References-found: 34

