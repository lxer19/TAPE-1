URL: file://ftp.cs.unc.edu/pub/projects/proteus/reports/frontiers95.ps.gz
Refering-URL: http://www.cs.unc.edu/Research/proteus/proteus-publications.html
Root-URL: http://www.cs.unc.edu
Email: palmerd,prins-@cs.unc.edu  westfold@kestrel.edu  
Title: Work-Efficient Nested Data-Parallelism  
Author: Daniel W. Palmer and Jan F. Prins Stephen Westfold 
Address: Chapel Hill NC 27599-3175  Palo Alto, CA 94304  
Affiliation: Department of Computer Science University of North Carolina  Kestrel Institute  
Abstract: An apply-to-all construct is the key mechanism for expressing data-parallelism, but data-parallel programming languages like HPF and C* significantly restrict which operations can appear in the construct. Allowing arbitrary operations substantially simplifies the expression of irregular and nested data-parallel computations. The technique of flattening nested parallelism introduced by Blelloch, compiles data-parallel programs with unrestricted apply-to-all constructs into vector operations, and has achieved notable success, particularly with irregular data-parallel programs. However, these programs must be carefully constructed so that flattening them does not lead to suboptimal work complexity due to unnecessary replication in index operations. We present new flattening transformations that generate programs with correct work complexity. Because these transformations may introduce concurrent reads in parallel indexing, we developed a randomized indexing that reduces concurrent reads while maintaining work-efficiency. Experimental results show that the new rules and implementations significantly reduce memory usage and improve performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Adams, W. Brainerd, J. Martin, B. Smith and J. Wagener, </author> <title> Fortran 90 Handbook, </title> <address> Intertext-McGraw Hill, </address> <year> 1992. </year>
Reference: [2] <author> G. Blelloch, </author> <title> Vector Models for Data-Parallel Computing, </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Recently, however, Blelloch <ref> [2] </ref> described a technique known as fl at te n ing to reduce nested parallelism to vector operations. The technique is formalized as a set of program transformations in [13] and used in a number of high level languages.
Reference: [3] <author> G. Blelloch, NESL: </author> <title> A Nested Data-Parallel Language(2.6), </title> <type> Technical Report CMU-CS-93-129, </type> <institution> Carnegie Mellon University, </institution> <year> 1993. </year>
Reference-contexts: The expressive utility of nested data-parallelism was recognized long ago in high level sequential programming languages like SETL [16] and APL2 [11]. Parallel execution of nested data-parallelism has been realized by recent languages such as Paralation Lisp [15], NESL <ref> [3] </ref> and Proteus [7,9] In Proteus, our high-level, wide-spectrum parallel language, all data-parallelism is expressed using an iterator construct which is analogous to the comprehension construct of set theory. <p> The replication of indexed values is a general problem in the flattening of nested data-parallelism and not limited to our implementation. Blelloch reports this increased work complexity in NESL and recommends that the programmer be aware of the problem and not use indexing in sequence comprehensions <ref> [3, appendix C] </ref>. To aid the programmer in avoiding such uses of indexing, NESL provides many primitives that, in parallel, select values from sequences in common access patterns. These support primitives maintain work efficiency, but because they must be applied outside of iterators, they interfere with function modularity.
Reference: [4] <author> G. Blelloch, S. Chatterjee, J. Hardwick, J. Sipelstein, and M. Zagha, </author> <title> Implementation of a Portable Nested Data-Parallel Language, </title> <booktitle> Proceedings of Fourth ACM Symposium on Principles and Practices of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The flattening technique can yield competitive code. Blelloch et. al . reported that "[Compiled NESL programs] perform [ed] competitively with native code for regular data and often superior on irregular data." <ref> [4] </ref>.
Reference: [5] <author> G. Blelloch, S. Chatterjee, J. Sipelstein and M. Zahga, CVL: </author> <title> A C vector library, </title> <type> Draft Technical Report, </type> <institution> Carnegie Mellon University, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: DPL is implemented with vector operations provided by the C Vector Library (CVL) <ref> [5] </ref>. DPL provides both parallel execution and architecture independence by building upon these same features in CVL. Hence transformed programs may be run efficiently on a wide variety of machines, including the Cray C90, the TMC CM5, the MasPar MP-2 and UNIX workstations.
Reference: [6] <author> G. Blelloch and G. Sabot, </author> <title> Compiling Collection-Oriented Languages onto Massively Parallel Computers, </title> <journal> Journal of Parallel and Distributed Computing , 1990. </journal>
Reference-contexts: The same portable NESL program was compared against optimized CM-Fortran code on a CM-2 and Fortran77 on the Cray C90 and in some cases performed better by an order of magnitude. 1.3 Work inefficiency While the technique of flattening nested parallelism is sound <ref> [6, 14] </ref>, it incurs some practical problems in its application. In (1.3), A is replicated to match the number of values in S in order to agree with the signature for binsearch 1 .
Reference: [7] <author> R. Faith, L. Nyland, D. Palmer, and J. Prins, </author> <title> The Proteus NS Grammer, </title> <type> Technical Report TR94-029, </type> <institution> UNC-CH, </institution> <year> 1994. </year>
Reference: [8] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu, </author> <title> Fortran D Language Specification, </title> <institution> Report COMP TR90-141(Rice) and SCCS - 42c (Syracuse), Rice University and Syracuse University, </institution> <year> 1991. </year>
Reference: [9] <author> A. Goldberg, J. Prins, J. Reif, R. Faith, Z. Li, P. Mills, L. Nyland, D. Palmer, J. Riely, and S. Westfold, </author> <title> The Proteus System for the Development of Parallel Applications, </title> <editor> in M. Harrison, editor, </editor> <booktitle> Prototyping Technologies: The ARPA ProtoTech Project. </booktitle> <publisher> Kluwer Academic Publishers, to appear. </publisher>
Reference: [10] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification, </title> <month> January, </month> <year> 1993. </year>
Reference: [11] <author> T. </author> <title> More, The Nested Rectangular Array as a Model of Data, </title> <booktitle> APL79 Conference Proceedings. ACM 1979. </booktitle>
Reference-contexts: The expressive utility of nested data-parallelism was recognized long ago in high level sequential programming languages like SETL [16] and APL2 <ref> [11] </ref>. Parallel execution of nested data-parallelism has been realized by recent languages such as Paralation Lisp [15], NESL [3] and Proteus [7,9] In Proteus, our high-level, wide-spectrum parallel language, all data-parallelism is expressed using an iterator construct which is analogous to the comprehension construct of set theory.
Reference: [12] <author> D. Palmer, </author> <title> DPL - Data Parallel Library Manual, </title> <type> Technical Report TR93-064, </type> <institution> UNC-CH, </institution> <month> November, </month> <year> 1993. </year>
Reference-contexts: We have implemented this abstract machine using C and a Ccallable Data Parallel Library (DPL) <ref> [12] </ref> to support nested sequences and their operations.
Reference: [13] <author> J. Prins and D. Palmer, </author> <title> Transforming High-Level Data - Parallel Programs into Vector Operations, </title> <booktitle> Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and 2 Practice of Parallel Programming, </booktitle> <address> San Diego, CA, p. 119 - 128, </address> <year> 1993. </year>
Reference-contexts: Recently, however, Blelloch [2] described a technique known as fl at te n ing to reduce nested parallelism to vector operations. The technique is formalized as a set of program transformations in <ref> [13] </ref> and used in a number of high level languages. The result of flattening (1.1) is binsearch 1 (distribute (A,length (S)),S) (1.3) where distribute function generates a copy of the source sequence A for each element of S and length returns the number of elements in a sequence. <p> This paper addresses the problems that arise from the data-parallel application of indexing operations and functions that contain indexing. In section 2 we give an overview of the flattening process and present new flattening transformations that improve on those we presented in <ref> [13] </ref>. In section 3 we describe three approaches to eliminating or reducing the cost of replications in the generated code. The most important of these, work-efficient indexing, solves the increase in asymptotic work complexity and is presented in section 4. <p> We present improved transformation rules to flatten nested data-parallelism in Proteus. These new rules reflect an approach that transforms a single iterator at a time which differs from our previous strategy of transforming groups of iterators <ref> [13] </ref>. When iterators are nested, these rules are first applied at the innermost iterator, eliminating it but possibly introducing new function calls. The rules are then applied to the new innermost iterator, and repeated until no iterators remain.
Reference: [14] <author> J. Riely, S. Purushothoman Iyer, and J. Prins, </author> <title> Compilation of Nested Parallel Programs: Soundness and Efficiency, </title> <type> Technical Report, </type> <institution> UNC-CH, </institution> <year> 1994. </year>
Reference-contexts: The same portable NESL program was compared against optimized CM-Fortran code on a CM-2 and Fortran77 on the Cray C90 and in some cases performed better by an order of magnitude. 1.3 Work inefficiency While the technique of flattening nested parallelism is sound <ref> [6, 14] </ref>, it incurs some practical problems in its application. In (1.3), A is replicated to match the number of values in S in order to agree with the signature for binsearch 1 .
Reference: [15] <author> G. Sabot, </author> <title> The Paralation Model : Architecture-Independent Parallel Programming, </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: The expressive utility of nested data-parallelism was recognized long ago in high level sequential programming languages like SETL [16] and APL2 [11]. Parallel execution of nested data-parallelism has been realized by recent languages such as Paralation Lisp <ref> [15] </ref>, NESL [3] and Proteus [7,9] In Proteus, our high-level, wide-spectrum parallel language, all data-parallelism is expressed using an iterator construct which is analogous to the comprehension construct of set theory.
Reference: [16] <author> J. Schwartz, </author> <title> Set Theory as a Language for Program Specification and Programming, </title> <institution> Technical Report Computer Science Department, Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <year> 1970. </year>
Reference-contexts: The expressive utility of nested data-parallelism was recognized long ago in high level sequential programming languages like SETL <ref> [16] </ref> and APL2 [11]. Parallel execution of nested data-parallelism has been realized by recent languages such as Paralation Lisp [15], NESL [3] and Proteus [7,9] In Proteus, our high-level, wide-spectrum parallel language, all data-parallelism is expressed using an iterator construct which is analogous to the comprehension construct of set theory.
References-found: 16

