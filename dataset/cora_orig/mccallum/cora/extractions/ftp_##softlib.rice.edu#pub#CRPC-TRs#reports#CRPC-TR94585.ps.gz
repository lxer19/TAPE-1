URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94585.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: bouarich@@mcs.anl.gov.  
Title: Tensor Methods for Large, Sparse Unconstrained Optimization  
Author: Ali Bouaricha 
Keyword: Key words. tensor methods, unconstrained optimization, sparse problems, large-scale optimization, singular problems  
Address: 60439.  
Affiliation: at CERFACS (Centre Europeen de Recherche et de Formation Avancee en Calcul Scientifique). Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Illinois,  
Note: AMS(MOS) subject classification. 65K Part of this work was performed while the author was research associate  This work was supported in part by the Office of Scientific Computing, U.S. Department of Energy, under Contract W-31-109-Eng-38.  
Abstract: Tensor methods for unconstrained optimization were first introduced by Schn-abel and Chow [SIAM J. Optimization, 1 (1991), pp. 293-315], who describe these methods for small to moderate-size problems. The major contribution of this paper is the extension of these methods to large, sparse unconstrained optimization problems. This extension requires an entirely new way of solving the tensor model that makes the methods suitable for solving large, sparse optimization problems efficiently. We present test results for sets of problems where the Hessian at the minimizer is nonsingular and where it is singular. These results show that tensor methods are significantly more efficient and more reliable than standard methods based on Newton's method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anon. </author> <title> Harwell subroutine library (Release 11). </title> <institution> Theoretical Studies Department, AEA Industrial Technology, </institution> <year> 1993. </year>
Reference-contexts: The Schur complement method is implemented by first invoking MA39AD <ref> [1] </ref> to form the Schur complement S = D CH 1 B of H in the extended matrix, where D is the 2 by 2 lower right submatrix, C is the lower left 2 by n submatrix, and B is the upper right n by 2 submatrix, of the augmented matrix. <p> The Schur complement is then factored into its QR factors. Next, MA39BD <ref> [1] </ref> solves the extended system (6.1) using the following well-known scheme: 1. Solve Hu = b, for u. 2. Solve Sy = b Cu, for y. 3. Solve Hv = By, for v. 4. x = u v.
Reference: [2] <author> B. M. Averick, R. G. Carter, J. J. More, and G. L. Xue. </author> <title> The MINPACK-2 test problem collection. </title> <type> Technical Report ANL/MCS-P153-0692, </type> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: In the following we present and discuss summary statistics of the test results. All our computations were performed on a Sun Sparc 10 Model 40 machine using double-precision arithmetic. First, we tested our program on the set of unconstrained optimization problems from the CUTE [3] and the MINPACK-2 <ref> [2] </ref> collections. Most of these problems have nonsingular Hessians at the solution. We also created singular test problems as proposed in [4, 20] by modifying the nonsingular test problems from the CUTE collection as follows.
Reference: [3] <author> I. Bongartz, A. R. Conn, N. I. M. Gould, and Ph. L. Toint. CUTE: </author> <title> Constrained and Unconstrained Testing Environment. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 21(1) </volume> <pages> 123-160, </pages> <year> 1995. </year>
Reference-contexts: In the following we present and discuss summary statistics of the test results. All our computations were performed on a Sun Sparc 10 Model 40 machine using double-precision arithmetic. First, we tested our program on the set of unconstrained optimization problems from the CUTE <ref> [3] </ref> and the MINPACK-2 [2] collections. Most of these problems have nonsingular Hessians at the solution. We also created singular test problems as proposed in [4, 20] by modifying the nonsingular test problems from the CUTE collection as follows.
Reference: [4] <author> A. Bouaricha. </author> <title> Solving large sparse systems of nonlinear equations and nonlinear least squares problems using tensor methods on sequential and parallel computers. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Department, University of Colorado at Boulder, </institution> <year> 1992. </year>
Reference-contexts: Line Search Backtracking Techniques The line search global strategy we use in conjunction with our tensor method for large, sparse unconstrained optimization is similar to the one used for nonlinear equations <ref> [4, 6] </ref>. This strategy has shown to be very successful for large, sparse systems of nonlinear equations. We also found that it is superior to the approach used by Schnabel and Chow [19]. The main difference between the two approaches is that ours always tries the full tensor step first. <p> First, we tested our program on the set of unconstrained optimization problems from the CUTE [3] and the MINPACK-2 [2] collections. Most of these problems have nonsingular Hessians at the solution. We also created singular test problems as proposed in <ref> [4, 20] </ref> by modifying the nonsingular test problems from the CUTE collection as follows. <p> Then according to <ref> [4, 20] </ref>, we can create singular systems of nonlinear equations from (7.1) by forming ^ F (x) = F (x) F 0 (x fl )A (A T A) 1 A T (x x fl ); (7:2) where A 2 &lt; nfik has full column rank with 1 k n.
Reference: [5] <author> A. Bouaricha and N. I. M. Gould. </author> <type> Personal communication. </type> <institution> Centre Europeen de Recherche et de Formation Avancee en Calcul Scientifique (CERFACS), Toulouse, France, </institution> <year> 1994. </year>
Reference-contexts: We are currently investigating other possible approaches, such as a modified Newton's method in which the approximated Jacobian matrix will incorporate more useful information, or an iterative method such as a nonlinear GMRES. This work, a cooperation with Nick Gould <ref> [5] </ref>, will be reported in the near future. We are almost done with the implementation and testing of the two-dimensional trust region global strategy described in x5. This work will be reported in a forthcoming paper. We are also implementing the algorithms discussed in this paper in a software package.
Reference: [6] <author> A. Bouaricha and R. B. Schnabel. TENSOLVE: </author> <title> A software package for solving systems of nonlinear equations and nonlinear least squares problems using tensor methods. </title> <type> Preprint MCS-P463-0894, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: Line Search Backtracking Techniques The line search global strategy we use in conjunction with our tensor method for large, sparse unconstrained optimization is similar to the one used for nonlinear equations <ref> [4, 6] </ref>. This strategy has shown to be very successful for large, sparse systems of nonlinear equations. We also found that it is superior to the approach used by Schnabel and Chow [19]. The main difference between the two approaches is that ours always tries the full tensor step first. <p> d t ; ~g s are given by (5.22) and (5.23), respectively. ~ d n is obtained in an analogous way to ~ d t ; by applying transformations (5.21) and (5.22) to it. 1. if tensor model selected then Solve problem (5.25) using the procedure described in Algorithm 3.4 <ref> [6] </ref> else fstandard Newton model selectedg Solve problem (5.27) using the procedure described in Algorithm 3.4 [6] endif 2. if tensor model selected then d = ff fl ~ d t + ~g s ffi 2 fl where ff fl is the global minimizer of (5.25) else fstandard Newton model selectedg <p> obtained in an analogous way to ~ d t ; by applying transformations (5.21) and (5.22) to it. 1. if tensor model selected then Solve problem (5.25) using the procedure described in Algorithm 3.4 <ref> [6] </ref> else fstandard Newton model selectedg Solve problem (5.27) using the procedure described in Algorithm 3.4 [6] endif 2. if tensor model selected then d = ff fl ~ d t + ~g s ffi 2 fl where ff fl is the global minimizer of (5.25) else fstandard Newton model selectedg d = ff fl ~ d n + ~g s ffi 2 fl where ff fl
Reference: [7] <author> R. H. Byrd, R. B. Schnabel, and G. A. Shultz. </author> <title> Approximation solution of the trust region problem by minimization over two-dimensional subspaces. </title> <journal> Math. Programming, </journal> <volume> 40 </volume> <pages> 247-263, </pages> <year> 1988. </year>
Reference-contexts: The main reasons that lead us to adopt this approach are that it is easy to construct, closely related to dogleg type algorithms over the same subspace. This step may be close to optimal trust region step algorithms in practice. Byrd, Schnabel, and Shultz <ref> [7] </ref> have shown that for unconstrained optimization using a standard quadratic model, the analogous two-dimensional minimization approach produces nearly as much decrease in the quadratic model as the optimal trust region step in almost all cases. 12 The two-dimensional trust region approach for the tensor model computes an approximate solution to
Reference: [8] <author> T. F. Coleman, B. S. Garbow, and J. J. </author> <title> More. Estimating sparse Hessian matrices. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 11 </volume> <pages> 363-377, </pages> <year> 1985. </year>
Reference-contexts: In step 1, the gradient is either computed analytically or approximated by the algorithm A5.6.3 given in Dennis and Schnabel [9]. In step 2, the Hessian matrix is either calculated analytically or approximated by a graph coloring algorithm described in <ref> [8] </ref>. Note that it is crucial to supply an analytic gradient if the finite difference Hessian matrix requires many gradient evaluations. Otherwise, the methods described in this paper may not be practical, and inexact type of methods may be preferable. <p> For rank n 1 and n 2 test problems, we have modified the analytic gradients provided by the CUTE collection to take into account the modification (7.2). On the other hand, we used the graph coloring algorithm <ref> [8] </ref> to evaluate the finite difference approximation of the Hessian matrix. A summary for the test problems whose Hessians at the solution have ranks n, n 1, and n 2 is presented in Table 1. The descriptions of the test problems and the detailed results are given in the Appendix. <p> To firmly establish such a conclusion, additional testing is required, including test problems of very large size. On sparse problems where the function or the gradient is expensive to evaluate, the finite difference approximation of the Hessian matrix by the graph coloring algorithm <ref> [8] </ref> may be very costly. Hence, quasi-Newton methods may be preferable to use in this case. These methods involve low-rank corrections to a current approximate Hessian matrix.
Reference: [9] <author> J. E. Dennis and R. B. Schnabel. </author> <title> Numerical methods for unconstrained optimization and nonlinear equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: Standard methods for solving unconstrained optimization problems are widely described in the literature; general references on this topic include Dennis and Schnabel <ref> [9] </ref>, Fletcher [12], 2 and Gill, Murray, and Wright [14]. In this paper, we propose extensions to standard methods that use analytic or finite-difference gradients and Hessians. <p> + d t + ) if (minimizer of the tensor model was found) then if f p &lt; f c + ff slope then x + = x t else Find an acceptable x n + in the Newton direction d n using the line search given by Algorithm A6.3.1 <ref> [9, p.325] </ref> Find an acceptable x t + in the tensor direction d t using the line search given by Algorithm A6.3.1 [9, p.325] if f (x n + ) then x + = x n 11 else x + = x t endif endif else Find an acceptable x n <p> slope then x + = x t else Find an acceptable x n + in the Newton direction d n using the line search given by Algorithm A6.3.1 <ref> [9, p.325] </ref> Find an acceptable x t + in the tensor direction d t using the line search given by Algorithm A6.3.1 [9, p.325] if f (x n + ) then x + = x n 11 else x + = x t endif endif else Find an acceptable x n + in the Newton direction d n using the line search given by Algorithm A6.3.1 [9, p.325] x + = x n <p> line search given by Algorithm A6.3.1 <ref> [9, p.325] </ref> if f (x n + ) then x + = x n 11 else x + = x t endif endif else Find an acceptable x n + in the Newton direction d n using the line search given by Algorithm A6.3.1 [9, p.325] x + = x n endif 5. <p> it approximates this curve by a piecewise linear function in the subspace spanned by the Newton step and the steepest descent direction rf (x c ), and takes x + as the point on this approximation for which jj x + x c jj = ffi c . (See, e.g., <ref> [9] </ref> for more details.) Unfortunately these two methods are hard to extend to the tensor model, which is a fourth-order model. <p> The methods used for adjusting the trust radius during and between steps are given in Algorithm A6.4.5 <ref> [9, p.338] </ref>. The initial trust radius can be supplied by the user; if not, it is set to the length of the initial Cauchy step. 6. A High-Level Algorithm for the Tensor Method In this section, we present the overall algorithm for the tensor method for large, sparse unconstrained optimization. <p> In step 1, the gradient is either computed analytically or approximated by the algorithm A5.6.3 given in Dennis and Schnabel <ref> [9] </ref>. In step 2, the Hessian matrix is either calculated analytically or approximated by a graph coloring algorithm described in [8]. Note that it is crucial to supply an analytic gradient if the finite difference Hessian matrix requires many gradient evaluations.
Reference: [10] <author> I. S. Duff and J. K. Reid. MA27: </author> <title> A set of Fortran subroutines for solving sparse symmetric sets of linear equations. </title> <type> Technical Report R-10533, </type> <institution> AERE Harwell Laboratory, Harwell, UK, </institution> <year> 1983. </year>
Reference-contexts: Otherwise, the methods described in this paper may not be practical, and inexact type of methods may be preferable. The procedures for calculating T c and V c in step 3 were discussed in x2. In step 4, the Hessian matrix is factored using MA27 <ref> [10] </ref>, a sparse Cholesky decomposition package. If the Hessian matrix is nonsingular, then the tensor step d t is calculated as described in x3.1.
Reference: [11] <author> D. Feng, P. Frank, and R. B. Schnabel. </author> <title> Local convergence analysis of tensor methods for nonlinear equations. </title> <journal> Math. Prog., </journal> <volume> 62 </volume> <pages> 427-459, </pages> <year> 1993. </year>
Reference-contexts: Tensor methods for nonlinear equations problems have been shown to have 3-step Q-order 1.5 convergence on problems where the Jacobian has rank n 1 10 at the solution <ref> [11] </ref>, whereas Newton's method is linearly convergent with constant 1/2 on such problems. However, no attempt has been made yet to prove the convergence rate of tensor methods for unconstrained optimization problems where the Hessian at the solution has rank n 1.
Reference: [12] <author> R. Fletcher. </author> <title> Practical method of optimization, volume 1, Unconstrained Optimization. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: Standard methods for solving unconstrained optimization problems are widely described in the literature; general references on this topic include Dennis and Schnabel [9], Fletcher <ref> [12] </ref>, 2 and Gill, Murray, and Wright [14]. In this paper, we propose extensions to standard methods that use analytic or finite-difference gradients and Hessians.
Reference: [13] <author> P. E. Gill, W. Murray, D. B. Ponceleon, and M. A. Saunders. </author> <title> Preconditioners for indefinite systems arising in optimization and nonlinear least squares problems. </title> <type> Technical Report SOL 90-8, </type> <institution> Department of Operations Research, Stanford University, California, </institution> <year> 1990. </year>
Reference-contexts: To obtain the perturbation , we use a modification of MA27 advocated by Gill, Murray, Ponceleon, and Saunders in <ref> [13] </ref>. In this method we first compute the LDL T of the Hessian matrix using the MA27 package, then change the block diagonal matrix D to D + E. The modified matrix is block diagonal positive definite.
Reference: [14] <author> P. E. Gill, W. Murray, and M. H. Wright. </author> <title> Practical Optimization. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: Standard methods for solving unconstrained optimization problems are widely described in the literature; general references on this topic include Dennis and Schnabel [9], Fletcher [12], 2 and Gill, Murray, and Wright <ref> [14] </ref>. In this paper, we propose extensions to standard methods that use analytic or finite-difference gradients and Hessians.
Reference: [15] <author> A. Griewank and M. R. Osborne. </author> <title> Analysis of Newton's method at irregular singularities. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 18 </volume> <pages> 145-150, </pages> <year> 1981. </year>
Reference-contexts: However, Newton's method is generally linearly convergent at best if r 2 f (x fl ) is singular <ref> [15] </ref>. Methods based on (1.2) have been shown to be more reliable and more efficient than standard methods on small to moderate-size problems [19].
Reference: [16] <author> A. Griewank and Ph. L. Toint. </author> <title> On the unconstrained optimization of partially separable functions. </title> <editor> In M. J. D. Powell, editor, </editor> <booktitle> Nonlinear Optimization 1981, </booktitle> <pages> pages 301-312. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: In this variant, we explored the possibility of using exact third- and fourth-order derivative information. The calculation of these derivatives is simplified using the concept of partial separability, a structure that has already proven to be useful when building quadratic models for large-scale nonlinear problems <ref> [16] </ref>. The calculation of the minimizer of this exact tensor model is more problematic, however, because we need to solve a sparse system of nonlinear equations. An obvious approach to solve these equations is to use a Newton-like method.
Reference: [17] <author> J. J. </author> <title> More. The Levenberg-Marquardt algorithm: Implementation and theory. </title> <editor> In G. A. Watson, editor, </editor> <booktitle> Proceedings Dundee 1977, Lecture Notes in Mathematics 630, </booktitle> <pages> pages 105-116. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1978. </year>
Reference-contexts: When ffi c is shorter than the Newton step, the locally constrained optimal step <ref> [17] </ref> finds a c such that jj d ( c ) jj 2 ffi c , where d ( c ) = (r 2 f (x c ) + I) 1 rf (x c ). Then it takes x + = x c + d ( c ).
Reference: [18] <author> M. J. D. Powell. </author> <title> A new algorithm for unconstrained optimization. </title> <editor> In J. B. Rosen, O. L. Mangasarian, and K. Ritter, editors, </editor> <booktitle> Nonlinear Programming, </booktitle> <pages> pages 33-65. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: Then it takes x + = x c + d ( c ). The dogleg step is a modification of the trust region algorithm introduced by Powell <ref> [18] </ref>.
Reference: [19] <author> R. B. Schnabel and T. Chow. </author> <title> Tensor methods for unconstrained optimization using second derivatives. </title> <journal> SIAM J. Optimization, </journal> <volume> 1 </volume> <pages> 293-315, </pages> <year> 1991. </year>
Reference-contexts: However, Newton's method is generally linearly convergent at best if r 2 f (x fl ) is singular [15]. Methods based on (1.2) have been shown to be more reliable and more efficient than standard methods on small to moderate-size problems <ref> [19] </ref>. In the test results obtained for both nonsin-gular and singular problems, the improvement by the tensor method over Newton's method is substantial, ranging from 30% to 50% in iterations and in function and derivative evaluations. Furthermore, the tensor method solves several problems that Newton's method fails to solve. <p> Furthermore, the tensor method solves several problems that Newton's method fails to solve. The tensor algorithms described in <ref> [19] </ref> are QR-based algorithms involving orthogonal transformations of the variable space. These algorithms are very effective for minimizing the tensor model when the Hessian is dense because they are very stable numerically, especially when the Hessian is singular. <p> This makes our new algorithms very well suited for sparse problems. The remainder of this paper is organized as follows. In x2 we briefly review the techniques introduced by Schnabel and Chow <ref> [19] </ref> to form the tensor model. In x3 we describe efficient algorithms for minimizing the tensor model when the Hessian is sparse. In xx4 and 5 we discuss the globally convergent modifications for tensor methods for large, sparse unconstrained optimization. <p> Finally, in x8, we give a summary of our work and a discussion of future research. 2. Forming the Tensor Model In this section, we briefly review the techniques that were introduced in <ref> [19] </ref> for forming the tensor model for unconstrained optimization. <p> performance of the tensor version that allows p 1 is similar overall to that constraining p to be 1, and that the method is simpler and less expensive to implement in this case. (The derivation of the third- and fourth-order tensor terms for p 1 is explained in detail in <ref> [19] </ref>.) The interpolation conditions at the past point x 1 are given by f (x 1 ) = f (x c ) + rf (x c ) s + 2 1 T c s 3 + 24 and 1 T c s 2 + 6 where s = x 1 x <p> past point x 1 are given by f (x 1 ) = f (x c ) + rf (x c ) s + 2 1 T c s 3 + 24 and 1 T c s 2 + 6 where s = x 1 x c : Schnabel and Chow <ref> [19] </ref> choose T c and V c to satisfy (2.1) and (2.2). They first show that the interpolation conditions (2.1) and (2.2) uniquely determine T c s 3 and V c s 4 . <p> Hence, the interpolation conditions uniquely determine T c s 3 and V c s 4 . Since these are the only interpolation conditions, the choice of T c and V c is vastly underdetermined. 4 Schnabel and Chow <ref> [19] </ref> choose T c and V c by first selecting the smallest symmetric V c , in the Frobenius norm, for which V c s 4 = fi; where fi is determined by (2.4)-(2.5). <p> More precisely, Schnabel and Chow <ref> [19] </ref> choose the smallest symmetric T c and V c , in the Frobenius norm, that satisfy the equations (2.6)-(2.7). <p> s) 4 ; where the tensor V c = s s s s 2 &lt; nfinfinfin is called a fourth-order rank-one tensor for which V c (i; j; k; l) = s (i)s (j)s (k)s (l); 1 i; j; k; l n. (We use the notation to be consistent with <ref> [19] </ref>.) The solution to (2.9) is T c = b s s + s b s + s s b; (2:10) where the notation T = u v w; u; v; w 2 &lt; n , T 2 &lt; nfinfin , is called a third-order rank-one tensor for which T (i; <p> The whole process of forming the tensor model requires only O (n 2 ) arithmetic operations. The storage needed for forming and storing the tensor model is only a total of 6n. For further information we refer to <ref> [19] </ref>. 5 3. Solving the Tensor Model When the Hessian Is Sparse In this section we give algorithms for finding a minimizer of the tensor model (1.2) efficiently, when the Hessian is sparse. <p> It has been shown in practice that when r 2 f (x fl ) has rank n 1 the convergence rate of the tensor method is better than the linear convergence of the standard Newton method <ref> [19] </ref> (also see x7 for the ratios of the errors of successive iterates on the BRYBND problem with rank (r 2 f (x fl )) = n 1. <p> This strategy has shown to be very successful for large, sparse systems of nonlinear equations. We also found that it is superior to the approach used by Schnabel and Chow <ref> [19] </ref>. The main difference between the two approaches is that ours always tries the full tensor step first.
Reference: [20] <author> R. B. Schnabel and P. D. Frank. </author> <title> Tensor methods for nonlinear equations. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 21 </volume> <pages> 815-843, </pages> <year> 1984. </year> <month> 24 </month>
Reference-contexts: First, we tested our program on the set of unconstrained optimization problems from the CUTE [3] and the MINPACK-2 [2] collections. Most of these problems have nonsingular Hessians at the solution. We also created singular test problems as proposed in <ref> [4, 20] </ref> by modifying the nonsingular test problems from the CUTE collection as follows. <p> Then according to <ref> [4, 20] </ref>, we can create singular systems of nonlinear equations from (7.1) by forming ^ F (x) = F (x) F 0 (x fl )A (A T A) 1 A T (x x fl ); (7:2) where A 2 &lt; nfik has full column rank with 1 k n.
References-found: 20

