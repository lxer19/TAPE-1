URL: ftp://ftp.aic.nrl.navy.mil/pub/aswu/papers/cirffss.94.ps.gz
Refering-URL: http://www.aic.nrl.navy.mil/~aswu/papers.html
Root-URL: 
Email: aswu@engin.umich.edu  
Title: VISION BASED OBJECT POSE ESTIMATION FOR MOBILE ROBOTS performed onboard the robot using a 80486
Author: Annie Wu, Clint Bidlack, Arun Katkere Roy Feague, and Terry Weymouth 
Keyword: Task description  Marker detection  
Date: [1].  
Note: processing was  the University of Michigan entry can be found in  The  
Address: Ann Arbor, MI 48109-2110  
Affiliation: Artificial Intelligence Laboratory University of Michigan  
Abstract: Mobile robot navigation using visual sensors requires that a robot be able to detect landmarks and obtain pose information from a camera image. This paper presents a vision system for finding man made markers of known size and calculating the pose of these markers. The algorithm detects and identifies the markers using a weighted pattern matching template. Geometric constraints are then used to calculate the position of the markers relative to the robot. The selection of geometric constraints comes from the typical pose of most man made signs; such as the sign standing vertical and the dimensions of known size. This system has been tested successfully on a wide range of real images. Marker detection is reliable, even in cluttered environments, and under certain marker orientations, estimation of the orientation has proven accurate to within 2 degrees, and distance estimation to within 0.3 meters. Humans are very dependent on their sense of sight for navigation. People use both natural and manmade landmarks to help them determine where they are and which way they want to go next. What humans can do with the greatest of ease, however, can be very difficult for robots. Mobile robot navigation using visual sensors typically requires that the robot be able to obtain pose information from a camera image. This task often includes recognizing markers or other known objects in the image and calculating the object pose from the size and appearance. There are several tasks that a robot navigating by vision must deal with: the robot must to be able to extract markers from a complex environment; the robot has to recognize these markers from many different points of view; and the robot must determine, from it's view of the marker, the pose (3D position and orientation) of the marker. In addition, for all practical purposes, the robot should be able to perform all of the above tasks relatively fast (less than a few seconds in most cases). This paper describes a vision system that was implemented for the AAAI 1993 Robot Competition in Washington D. C. on July 11-16, 1993. All vision The vision system is divided into a marker extraction and identification step, and a pose estimation step. Marker extraction finds predefined markers (black 'x's and '+'s on a white background) in the environment and determines their pose relative to the robot. Thus, a robot using this system should be able to navigate autonomously using visual sensors in a semi-constrained environment. The required geometric constraints are: the marker must stand vertical; the marker and camera contain no roll; the focal length of the camera and the camera's location relative to the robot are known; the robot is oriented in the plane perpendicular to the marker; and the width and height of the marker are known. Though these constraints may seem restrictive, they are typical of most man made signs such as traffic signs and office door markers. To maximize speed, we make only one pass through the entire image. During the pass, the image is thresholded and connected components are found and labeled. One pixel components are ignored and not labeled. Size thresholding then filters out most of the non-marker components. Only one pass is made through all possible connected components. Figure 1 shows sample output from this stage. The possible markers are outlined with a bounding box. To identify or reject the remaining markers, a weighted pattern matching template is used. An nxn template matrix is created for each marker (see Figure 2). Increasing n increases the resolution of the template, but also increases the process time. We found n = 7 to be a good compromise. This weighted template indicates which areas are expected to be black and which ones white. The weights for our matrix are currently determined by trial and error, but we could easily replace these with machine gener fl Currently at University of California, San Diego
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> David Kortenkamp, Marcus Huber, Frank Koss, William Belding, Jaeho Lee, Annie Wu, Clint Bid-lack, and Seth Rogers. </author> <title> Mobile robot exploration and navigation of indoor spaces using sonar and vision. </title> <booktitle> In Conference on Intelligent Robots in Field, Factory, Service and Space, </booktitle> <month> March </month> <year> 1994. </year> <month> 7 </month>
References-found: 1

