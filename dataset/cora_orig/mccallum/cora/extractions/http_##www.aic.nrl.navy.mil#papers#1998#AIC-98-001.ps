URL: http://www.aic.nrl.navy.mil/papers/1998/AIC-98-001.ps
Refering-URL: http://www.aic.nrl.navy.mil/~aha/pub-details.html
Root-URL: 
Phone: 2  
Title: Error-Correcting Output Codes for Local Learners  
Author: Francesco Ricci and David W. Aha 
Address: 38050 Povo (TN), Italy  Code 5510 Washington, DC 20375 USA  
Affiliation: 1 Istituto per la Ricerca Scientifica e Tecnologica  Navy Center for Applied Research in Artificial Intelligence Naval Research Laboratory,  
Abstract: Error-correcting output codes (ECOCs) represent classes with a set of output bits, where each bit encodes a binary classification task corresponding to a unique partition of the classes. Algorithms that use ECOCs learn the function corresponding to each bit, and combine them to generate class predictions. ECOCs can reduce both variance and bias errors for multiclass classification tasks when the errors made at the output bits are not correlated. They work well with algorithms that eagerly induce global classifiers (e.g., C4.5) but do not assist simple local classifiers (e.g., nearest neighbor), which yield correlated predictions across the output bits. We show that the output bit predictions of local learners can be decorrelated by selecting different features for each bit. We present promising empirical results for this combination of ECOCs, near est neighbor, and feature selection.
Abstract-found: 1
Intro-found: 1
Reference: [ Aha and Bankert, 1997 ] <author> D. W. Aha and R. L. Bankert. </author> <title> Cloud classification using error-correcting output codes. </title> <booktitle> Artificial Intelligence Applications: Natural Science, Agriculture, and Environmental Science, </booktitle> <volume> 11 </volume> <pages> 13-28, </pages> <year> 1997. </year>
Reference-contexts: We found similar behavior with k &gt; 1 in other experiments (not reported here). Previous research on ECOCs did not stress feature selection, which is crucial for some tasks. Due to their low training costs, nearest neighbor classifiers are excellent for use with expensive feature selection approaches <ref> [ Aha and Bankert, 1997 ] </ref> . Perhaps the most effective feature selectors are those that guide search using feedback from the classifier itself.
Reference: [ Aha et al., 1991 ] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: Therefore, we extended IB1 <ref> [ Aha et al., 1991 ] </ref> , an implementation of nearest neighbor, to use different features when computing distances for each output bit. Figure 2 summarizes the training algorithm scheme for this extension, named IB1 cdwd .
Reference: [ Aha, 1992 ] <author> D. W. Aha. </author> <title> Tolerating noisy, irrelevant, and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 36 </volume> <pages> 267-287, </pages> <year> 1992. </year>
Reference-contexts: In one-per-class each bit function separates the examples in one class from the remaining examples (i.e., exactly one output bit in a one-per-class codeword has value 1; all others have value 0). Learning algorithms that use one-per-class encodings induce a separate concept description per class <ref> [ Quinlan, 1993; Aha, 1992 ] </ref> , where positive instances of a class c i are negative instances for all other classes c j (i6=j). The Hamming distance between all one-per-class codewords is 2, which means that even one incorrectly predicted output bit can cause a misclassification.
Reference: [ Bottou and Vapnik, 1992 ] <author> L. Bottou and V. Vapnik. </author> <title> Local learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 888-900, </pages> <year> 1992. </year>
Reference-contexts: This can increase the classification accuracy of global learning algorithms [ Dietterich and Bakiri, 1995 ] (e.g., C4.5 [ Quinlan, 1993 ] , backpropagation [ Rumelhart et al., 1986 ] ). However, ECOCs do not benefit local learning algorithms <ref> [ Kong and Diet-terich, 1995; Bottou and Vapnik, 1992 ] </ref> , which predict classifications for a query q based only on information from examples local (i.e., nearby) to q. In Section 1.2, we explain that this limitation occurs because a local learner's predictions are correlated across the output bits.
Reference: [ Breiman, 1996 ] <author> L. Breiman. </author> <title> Bias, variance, and arcing classifiers. </title> <type> Technical Report 460, </type> <institution> University of California, Berkeley, </institution> <month> April </month> <year> 1996. </year>
Reference: [ Dietterich and Bakiri, 1995 ] <author> T. G. Dietterich and G. Bakiri. </author> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 263-286, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Error-correcting output codes (ECOCs) can help distinguish classes in classification tasks with m &gt; 2 classes by encoding error-correcting capabilities in their output representation. This can increase the classification accuracy of global learning algorithms <ref> [ Dietterich and Bakiri, 1995 ] </ref> (e.g., C4.5 [ Quinlan, 1993 ] , backpropagation [ Rumelhart et al., 1986 ] ). <p> In each of these output representations, each class c i 2 C is assigned a unique codeword s i = (s i1 ; : : : ; s il ) of l codeletters (e.g., <ref> [ Dietterich and Bakiri, 1995 ] </ref> ). The most popular atomic strategy sets l = 1; it uses a single codeletter to represent class labels. Learning algorithms that use this approach (e.g., C4.5 [ Quinlan, 1993 ] ) induce a single concept description that distinguishes all class boundaries. <p> We describe in the following how to generate ECOC codewords. Atomic and one-per-class codewords are generated as explained in Section 1.1. Algorithms for generating ECOC codewords should maximize both row and column separation. For classification tasks where m7, create codewords uses the exhaustive codes technique <ref> [ Dietterich and Bakiri, 1995 ] </ref> . It creates all 2 m1 1 possible codewords that are both column and row separated. The resulting codewords have Hamming distance h = 2 m2 .
Reference: [ Kohavi and Wolpert, 1996 ] <author> R. Kohavi and D. H. Wolpert. </author> <title> Bias plus variance decomposition for zero-one loss function. </title> <booktitle> In Proceeding of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 275-283, </pages> <address> Bari, Italy, 1996. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: [ Kong and Dietterich, 1995 ] <author> E. B. Kong and T. G Dietterich. </author> <title> Error-correcting output coding corrects bias and variance. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 313-321, </pages> <address> Tahoe City, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Maron and Moore, 1997 ] <author> O. Maron and A. W. Moore. </author> <title> The racing algorithm: Model selection for lazy learners. </title> <journal> Artificial Intelligence Review, </journal> <pages> pages 193-225, </pages> <year> 1997. </year>
Reference-contexts: It returns the subset of features selected by a variant of the schemata racing algorithm <ref> [ Maron and Moore, 1997 ] </ref> (see [ Ricci and Aha, 1997b ] ). Race-schemata searches over the space of schemata strings p 2 P of length d, the number of features, whose characters are 0's (feature is not selected), 1's (selected), or ?'s (selected with probability 50%).
Reference: [ Merz and Murphy, 1996 ] <author> C. Merz and P. M. Murphy. </author> <title> UCI repository of machine learning databases. </title> <note> [http://www.ics.uci.edu/~mlearn/MLRepository.html], 1996. </note>
Reference: [ Perrone and Cooper, 1993 ] <author> M. P. Perrone and L. N. Cooper. </author> <title> When networks disagree: Ensemble methods for hybrid neural networks. </title> <editor> In R. J. Mammone, editor, </editor> <title> Neural Networks for Speech and Image Processing. </title> <publisher> Chapman and Hall, </publisher> <address> Philadelphia, PA, </address> <year> 1993. </year>
Reference-contexts: Variance results from random variation and noise in the training set and from any random behaviors of the learning algorithm itself. ECOCs reduce variance through a voting process <ref> [ Perrone and Cooper, 1993 ] </ref> : because Hamming distance determines the "winning" prediction (i.e., closest codeword), each output bit prediction corresponds to a vote for classes whose codewords match the predicted value. Bias instead refers to an algorithm's systematic errors.
Reference: [ Quinlan, 1993 ] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Error-correcting output codes (ECOCs) can help distinguish classes in classification tasks with m &gt; 2 classes by encoding error-correcting capabilities in their output representation. This can increase the classification accuracy of global learning algorithms [ Dietterich and Bakiri, 1995 ] (e.g., C4.5 <ref> [ Quinlan, 1993 ] </ref> , backpropagation [ Rumelhart et al., 1986 ] ). However, ECOCs do not benefit local learning algorithms [ Kong and Diet-terich, 1995; Bottou and Vapnik, 1992 ] , which predict classifications for a query q based only on information from examples local (i.e., nearby) to q. <p> The most popular atomic strategy sets l = 1; it uses a single codeletter to represent class labels. Learning algorithms that use this approach (e.g., C4.5 <ref> [ Quinlan, 1993 ] </ref> ) induce a single concept description that distinguishes all class boundaries. Conversely, distributed output code strategies set l &gt; 1, where each codelet-ter s ij typically has a binary value (i.e., they are bit strings). <p> In one-per-class each bit function separates the examples in one class from the remaining examples (i.e., exactly one output bit in a one-per-class codeword has value 1; all others have value 0). Learning algorithms that use one-per-class encodings induce a separate concept description per class <ref> [ Quinlan, 1993; Aha, 1992 ] </ref> , where positive instances of a class c i are negative instances for all other classes c j (i6=j). The Hamming distance between all one-per-class codewords is 2, which means that even one incorrectly predicted output bit can cause a misclassification. <p> This ensures that the errors of the output bit predictions are uncorrelated. They reported that ECOCs often significantly increased the classification accuracies for C4.5 <ref> [ Quinlan, 1993 ] </ref> and networks trained by backpropagation [ Rumelhart et al., 1986 ] , although training ECOCs is slow because they must learn l concepts (i.e., one per output bit).
Reference: [ Ricci and Aha, 1997a ] <author> F. Ricci and D. W. Aha. </author> <title> Bias, variance, and error correcting output codes for local learners. </title> <type> Technical Report 9711-10, IRST, </type> <year> 1997. </year>
Reference-contexts: Given that each instance is unique in each of our data sets, we assumed, as did Kohavi and Wolpert [1996], that the Bayes optimal error rate is zero for each data set tested (see also <ref> [ Ricci and Aha, 1997a ] </ref> ). The results of the experiments conducted on some of the data sets are shown in Table 4. &gt;From these results we conclude that ECOCs drastically reduce the bias component of the error at the cost of moderately increasing the variance.
Reference: [ Ricci and Aha, 1997b ] <author> F. Ricci and D. W. Aha. </author> <title> Extending local learners with error-correcting output codes. </title> <type> Technical Report 9701-08, IRST, </type> <year> 1997. </year>
Reference-contexts: It returns the subset of features selected by a variant of the schemata racing algorithm [ Maron and Moore, 1997 ] (see <ref> [ Ricci and Aha, 1997b ] </ref> ). Race-schemata searches over the space of schemata strings p 2 P of length d, the number of features, whose characters are 0's (feature is not selected), 1's (selected), or ?'s (selected with probability 50%). <p> If a "winning" feature is found (i.e. it is highly unlikely that the error of the other schemata is significantly less) then the interior loop terminates and p is updated by fixing the winner's bit value, changing it from ? to either 0 or 1 (see <ref> [ Ricci and Aha, 1997b ] </ref> for more details).
Reference: [ Rumelhart et al., 1986 ] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClel-land, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructures of Cognition. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Error-correcting output codes (ECOCs) can help distinguish classes in classification tasks with m &gt; 2 classes by encoding error-correcting capabilities in their output representation. This can increase the classification accuracy of global learning algorithms [ Dietterich and Bakiri, 1995 ] (e.g., C4.5 [ Quinlan, 1993 ] , backpropagation <ref> [ Rumelhart et al., 1986 ] </ref> ). However, ECOCs do not benefit local learning algorithms [ Kong and Diet-terich, 1995; Bottou and Vapnik, 1992 ] , which predict classifications for a query q based only on information from examples local (i.e., nearby) to q. <p> This ensures that the errors of the output bit predictions are uncorrelated. They reported that ECOCs often significantly increased the classification accuracies for C4.5 [ Quinlan, 1993 ] and networks trained by backpropagation <ref> [ Rumelhart et al., 1986 ] </ref> , although training ECOCs is slow because they must learn l concepts (i.e., one per output bit).
Reference: [ Stanfill and Waltz, 1986 ] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communication of ACM, </journal> <volume> 29 </volume> <pages> 1213-1229, </pages> <year> 1986. </year>
Reference-contexts: Even if ECOCs are applicable to general data sets, we avoided those with symbolic features because they often require distinct weighting met-rics (e.g., <ref> [ Stanfill and Waltz, 1986 ] </ref> ), which complicates isolating the effects of feature selection. Data sets with fewer than four classes do not greatly benefit from ECOCs. We also used three additional proprietary data sets concerning cloud classification. We will use abbreviations for the data set names.
Reference: [ Zhang et al., 1992 ] <author> X. Zhang, J. Mesirov, and D. Waltz. </author> <title> Hybrid system for protein structure prediction. </title> <journal> Journal of Molecular Biology, </journal> <volume> 225 </volume> <pages> 1049-1063, </pages> <year> 1992. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Bias instead refers to an algorithm's systematic errors. These can also be reduced by voting, but only when the individual predictions are uncorrelated, such as by averaging the contributions of different prediction algorithms (e.g., <ref> [ Zhang et al., 1992 ] </ref> ). Alternatively, the same algorithm can be used multiple times, but it must vote on different subproblems (i.e., using different class decision boundaries) that cause the algorithm to generate different bias errors.
References-found: 17

