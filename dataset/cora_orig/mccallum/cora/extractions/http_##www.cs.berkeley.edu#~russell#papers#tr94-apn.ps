URL: http://www.cs.berkeley.edu/~russell/papers/tr94-apn.ps
Refering-URL: http://www.cs.berkeley.edu/~russell/full-log.html
Root-URL: 
Title: Adaptive probabilistic networks  
Author: Stuart Russell, John Binder, Daphne Koller Daphne Koller. 
Note: This research was supported by NSF grant IRI-9058427 (PYI), and by a University of California Presi dential Postdoctoral Fellowship to  
Date: July 25, 1994  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California  
Pubnum: Technical report UCB//CSD-94-824  
Abstract: Belief networks (or probabilistic networks) and neural networks are two forms of network representations that have been used in the development of intelligent systems in the field of artificial intelligence. Belief networks provide a concise representation of general probability distributions over a set of random variables, and facilitate exact calculation of the impact of evidence on propositions of interest. Neural networks, which represent parameterized algebraic combinations of nonlinear activation functions, have found widespread use as models of real neural systems and as function approximators because of their amenability to simple training algorithms. Furthermore, the simple, local nature of most neural network training algorithms provides a certain biological plausibility and allows for a massively parallel implementation. In this paper, we show that similar local learning algorithms can be derived for belief networks, and that these learning algorithms can operate using only information that is directly available from the normal, inferential processes of the networks. This removes the main obstacle preventing belief networks from competing with neural networks on the above-mentioned tasks. The precise, local, probabilistic interpretation of belief networks also allows them to be partially or wholly constructed by humans; allows the results of learning to be easily understood; and allows them to contribute to rational decision-making in a well-defined way. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Burger, J., and Connolly, D. </author> <year> (1994). </year> <title> Probabilistic resolution of anaphoric inference. </title>
Reference-contexts: Neal's results show that an extremely close connection exists between neural and belief networks. Our results can be seen as a generalization of Neal's, and yield a more efficient algorithm. Burger and Connolly <ref> [1] </ref> also apply neural network techniques to learning belief networks, using a somewhat ad hoc error function to derive an error gradient for polytree networks. 2 Learning networks with fixed structure Experience in constructing belief networks for applications has shown that finding the topology of the network is often straightforward. <p> In addition to the constraint that each CPT column must sum to 1, the search algorithm must also respect the constraint that CPT entries must fall in the range <ref> [0; 1] </ref>. It often occurs that the gradient-following algorithm reaches the edge of the "constraint surface," such that the next step would result in probability values less than 0 or greater than 1.
Reference: [2] <author> Gregory F. Cooper. </author> <title> The computational complexity of probabilistic inference using Bayesian belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 42 </volume> <pages> 393-405, </pages> <year> 1990. </year>
Reference-contexts: For the subclass of singly connected (or polytree) networks, in which any two nodes are connected by at most one undirected path, the posterior distribution can be calculated in time linear in the size of the network. In general networks, the inference task is NP-hard <ref> [2] </ref>, as is the corresponding approximation problem [4]. For general networks, a common technique [10] is to cluster variables in the network to form a join tree that is singly connected.
Reference: [3] <author> Cooper, G., & Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <pages> 309-347. </pages>
Reference-contexts: In this case, the prior distribution over CPT values is crucial. * Unknown structure, fully observable: In this case the problem is to reconstruct the topology of the network. An MAP analysis of the most likely network structure given the data has been carried out by Cooper and Herskovitz <ref> [3] </ref>, and by Heckerman et al. [7]. The resulting algorithms are capable of recovering fairly large networks from large data sets with a high degree of accuracy. <p> Heckerman, Geiger and Chickering [7] describe an elegant and effective heuristic algorithm for recovering the structure of general networks in the fully observable case, building on the work of Cooper and Herskovits <ref> [3] </ref>. Gradient-following algorithms have been proposed by Neal [11], who derives an expression for the likelihood gradient in sigmoid networks using stochastic simulation, and uses it to show that the Boltzmann Machine (a variety of neural network) is a special case of a belief network.
Reference: [4] <author> Dagum, P., and Luby, M. </author> <year> (1993). </year> <title> Approximating probabilistic inference in Bayesian belief networks is NP-hard. </title> <journal> Artificial Intelligence, </journal> <volume> 60, </volume> <pages> 141-53. </pages>
Reference-contexts: In general networks, the inference task is NP-hard [2], as is the corresponding approximation problem <ref> [4] </ref>. For general networks, a common technique [10] is to cluster variables in the network to form a join tree that is singly connected.
Reference: [5] <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm (with discussion). </title> <journal> J. Roy. Statist. Soc. </journal> <volume> B 39, </volume> <pages> 1-38. </pages>
Reference-contexts: At present, no good, general algorithms are known for this problem. 5 1.4 Related work For a thorough introduction to belief networks, see [12]. The general problem of recovering distributions from data with missing values and hidden variables is addressed by the EM algorithm <ref> [5] </ref>. Our algorithm can be seen as as a variant of EM in which the "maximize" phase is carried out by a gradient-following method. Lauritzen [9] also considers the application of EM to belief networks.
Reference: [6] <author> David Heckerman. </author> <title> Probabilistic Similarity Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1990. </year>
Reference-contexts: The basic task of a belief network is to calculate the probability distribution for the unknown variables, given observed values for the remaining variables. Belief networks containing several thousand nodes and links have been used successfully to represent medical knowledge and to achieve high levels of diagnostic accuracy <ref> [6] </ref>, among other tasks. gives the conditional probability of each possible value of the variable, given each possible combination of values of the parent nodes. (b) A simple belief network.
Reference: [7] <author> Heckerman, D., Geiger, D., and Chickering, M. </author> <year> (1994). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <type> Technical report no. </type> <institution> MSR-TR-94-09, Microsoft Research, </institution> <address> Redmond, WA. </address>
Reference-contexts: An MAP analysis of the most likely network structure given the data has been carried out by Cooper and Herskovitz [3], and by Heckerman et al. <ref> [7] </ref>. The resulting algorithms are capable of recovering fairly large networks from large data sets with a high degree of accuracy. <p> Lauritzen [9] also considers the application of EM to belief networks. Spiegelhalter, Dawid, Lauritzen and Cowell [13] provide a thorough analysis of the statistical basis of belief network modification using Dirichlet priors, although they provide only a heuristic approximation for the hidden-variable case. Heckerman, Geiger and Chickering <ref> [7] </ref> describe an elegant and effective heuristic algorithm for recovering the structure of general networks in the fully observable case, building on the work of Cooper and Herskovits [3].
Reference: [8] <author> Kirkpatrick, S., Gelatt, J., & Vecchi, M. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220, </volume> <pages> 671-680. </pages>
Reference-contexts: Algorithms capable of finding global maxima include random-restart hillclimbing, in which a new starting point is chosen randomly after each hillclimbing phase reaches a maximum; and simulated annealing, in which some "downhill" steps are allowed stochastically, with a frequency depending on a decreasing "temperature" schedule <ref> [8] </ref>. In addition to the constraint that each CPT column must sum to 1, the search algorithm must also respect the constraint that CPT entries must fall in the range [0; 1].
Reference: [9] <author> Lauritzen, S. </author> <year> (1991). </year> <title> The EM algorithm for graphical association models with missing data. </title> <type> Technical report no. </type> <institution> TR-91-05, Department of Statistics, Aalborg Univ. </institution>
Reference-contexts: The general problem of recovering distributions from data with missing values and hidden variables is addressed by the EM algorithm [5]. Our algorithm can be seen as as a variant of EM in which the "maximize" phase is carried out by a gradient-following method. Lauritzen <ref> [9] </ref> also considers the application of EM to belief networks. Spiegelhalter, Dawid, Lauritzen and Cowell [13] provide a thorough analysis of the statistical basis of belief network modification using Dirichlet priors, although they provide only a heuristic approximation for the hidden-variable case.
Reference: [10] <author> Steffen L. Lauritzen and David J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their application to expert systems. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 50(2) </volume> <pages> 157-224, </pages> <year> 1988. </year>
Reference-contexts: In general networks, the inference task is NP-hard [2], as is the corresponding approximation problem [4]. For general networks, a common technique <ref> [10] </ref> is to cluster variables in the network to form a join tree that is singly connected.
Reference: [11] <author> Neal, R. M. </author> <year> (1991). </year> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56, </volume> <pages> 71-113. </pages>
Reference-contexts: Heckerman, Geiger and Chickering [7] describe an elegant and effective heuristic algorithm for recovering the structure of general networks in the fully observable case, building on the work of Cooper and Herskovits [3]. Gradient-following algorithms have been proposed by Neal <ref> [11] </ref>, who derives an expression for the likelihood gradient in sigmoid networks using stochastic simulation, and uses it to show that the Boltzmann Machine (a variety of neural network) is a special case of a belief network.
Reference: [12] <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Belief networks (also called causal networks and Bayesian networks) are currently the principal tool for representing probabilistic knowledge <ref> [12] </ref>. They provide a concise representation of general probability distributions over a set of propositional (or multi-valued) random variables. The basic task of a belief network is to calculate the probability distribution for the unknown variables, given observed values for the remaining variables. <p> At present, no good, general algorithms are known for this problem. 5 1.4 Related work For a thorough introduction to belief networks, see <ref> [12] </ref>. The general problem of recovering distributions from data with missing values and hidden variables is addressed by the EM algorithm [5]. Our algorithm can be seen as as a variant of EM in which the "maximize" phase is carried out by a gradient-following method.
Reference: [13] <author> Spiegelhalter, D., Dawid, P., Lauritzen, S., and Cowell, R. </author> <year> (1993). </year> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8, </volume> <pages> 219-282. </pages>
Reference-contexts: Our algorithm can be seen as as a variant of EM in which the "maximize" phase is carried out by a gradient-following method. Lauritzen [9] also considers the application of EM to belief networks. Spiegelhalter, Dawid, Lauritzen and Cowell <ref> [13] </ref> provide a thorough analysis of the statistical basis of belief network modification using Dirichlet priors, although they provide only a heuristic approximation for the hidden-variable case.
Reference: [14] <author> M.P. Wellman. </author> <title> Fundamental concepts of qualitative probabilistic networks. </title> <journal> Artificial Intelligence, </journal> <volume> 44 </volume> <pages> 257-303, </pages> <year> 1990. </year> <month> 11 </month>
Reference-contexts: Separate reports will cover the following topics: * Continuous variables. * Parameterized functional representations of conditional probability tables, including "sigmoid" and "noisy-OR" networks. * Learning in dynamic networks, which represent stochastic processes over time. * Inclusion of quantitative and qualitative prior knowledge, such as "qualitative influ ences" <ref> [14] </ref>, in the initial network to facilitate learning. * Detailed discussion of implementations and applications. 10
References-found: 14

