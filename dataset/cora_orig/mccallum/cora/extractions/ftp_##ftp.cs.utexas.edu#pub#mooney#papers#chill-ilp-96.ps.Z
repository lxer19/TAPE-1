URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/chill-ilp-96.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Title: Inductive Logic Programming for Natural Language Processing  
Author: Raymond J. Mooney 
Address: Austin TX 78712-1188, USA  
Affiliation: Department of Computer Sciences University of Texas,  
Note: Appears in the Proceedings of the Sixth International Inductive Logic Programming Workshop  
Abstract: This paper reviews our recent work on applying inductive logic programming to the construction of natural language processing systems. We have developed a system, Chill, that learns a parser from a training corpus of parsed sentences by inducing heuristics that control an initial overly-general shift-reduce parser. Chill learns syntactic parsers as well as ones that translate English database queries directly into executable logical form. The ATIS corpus of airline information queries was used to test the acquisition of syntactic parsers, and Chill performed competitively with recent statistical methods. English queries to a small database on U.S. geography were used to test the acquisition of a complete natural language interface, and the parser that Chill acquired was more accurate than an existing hand-coded system. The paper also includes a discussion of several issues this work has raised regarding the capabilities and testing of ILP systems as well as a summary of our current research directions.
Abstract-found: 1
Intro-found: 1
Reference: <author> Abramson, H., & Dahl, V. </author> <year> (1989). </year> <title> Logic Grammars. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference: <author> Allen, J. F. </author> <year> (1995). </year> <booktitle> Natural Language Understanding (2nd Ed.). </booktitle> <address> Ben--jamin/Cummings, Menlo Park, CA. </address>
Reference-contexts: 1 Introduction Developing a system capable of communicating in natural language is one of the long-standing goals of computing research. Although significant progress has been made in the last forty years <ref> (Allen, 1995) </ref>, developing a natural language processing (NLP) system for a particular application is still an extremely difficult and laborious task. However, a promising approach is to use machine learning techniques to help automate the development of NLP systems.
Reference: <author> Anoe, C., & Bennett, S. W. </author> <year> (1995). </year> <title> Evaluating automated and manual acquisition of anaphora resolution strategies. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 122-129 Cambridge, MA. </address> <booktitle> ARPA (Ed.). (1993). Proceedings of the Fifth DARPA Message Understanding Evaluation and Conference, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> Bergadano, F., & Gunetti, D. </author> <year> (1993). </year> <title> An interactive system to learn functional logic programs. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1044-1049 Chambery, France. </address>
Reference: <author> Berwick, B. </author> <year> (1985). </year> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Black, E., & et. al. </author> <year> (1991). </year> <title> A procedure for quantitatively comparing the syntactic coverage of English grammars. </title> <booktitle> In Proceedings of the Fourth DARPA Speech and Natural Language Workshop, </booktitle> <pages> pp. 306-311. </pages>
Reference: <author> Bloom, P. </author> <year> (1994). </year> <title> Overview: Controversies in language acquisition. </title> <editor> In Bloom, P. (Ed.), </editor> <booktitle> Language Acquisition: Core Readings, </booktitle> <pages> pp. 5-48. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <booktitle> Borland International (1988). Turbo Prolog 2.0 Reference Guide. Borland International, </booktitle> <address> Scotts Valley, CA. </address>
Reference-contexts: sentence/representation pairs will generally not be available and that using a closed-world assumption to explicitly generate negative examples is intractable given the large space of possible sentences and representations. 2 In addition, it is generally agreed that when children acquire language they are exposed to little if any negative feedback <ref> (Bloom, 1994) </ref>. Consequently, a method is needed for learning without explicit negative tuples.
Reference: <author> Brill, E. </author> <year> (1993). </year> <title> Automatic grammar induction and parsing free text: A transformation-based approach. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 259-265 Columbus, Ohio. </address>
Reference-contexts: However, there has been relatively little recent work using symbolic machine learning techniques for language applications, although some recent systems have employed decision trees (Magerman, 1995; Anoe & Bennett, 1995), transformation rules <ref> (Brill, 1993, 1995) </ref>, and other symbolic methods (Wermter, Riloff, & Scheler, 1996). However, all of these approaches are limited to examples represented as fixed-length feature vectors and are therefore subject to the standard limitations of propositional representations.
Reference: <author> Brill, E. </author> <year> (1995). </year> <title> Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. </title> <journal> Computational Linguistics, </journal> <volume> 21 (4), </volume> <pages> 543-565. </pages>
Reference: <author> Brill, E., & Church, K. </author> <title> (Eds.). </title> <booktitle> (1996). Proceedings of the Conference on Empirical Methods in Natural Language Processing. </booktitle> <institution> University of Pennsylvania, </institution> <address> Philadel-phia, PA. </address>
Reference: <author> Charniak, E. </author> <year> (1993). </year> <title> Statistical Language Learning. </title> <publisher> MIT Press. </publisher>
Reference: <author> Church, K. </author> <year> (1988). </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing. Association for Computational Linguistics. </booktitle>
Reference: <author> Church, K., & Mercer, R. L. </author> <year> (1993). </year> <title> Introduction to the special issue on computational linguistics using large corpora. </title> <journal> Computational Linguistics, </journal> <volume> 19 (1), </volume> <pages> 1-24. </pages>
Reference: <author> Church, K., & Patil, R. </author> <year> (1982). </year> <title> Coping with syntactic ambiguity or how to put the block in the box on the table. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> 8 (3-4), </volume> <pages> 139-149. </pages>
Reference-contexts: In fact, any standard syntactic English grammar will produce more than 2 n parses of sentences ending in n prepositional phrases, most of which are usually spurious <ref> (Church & Patil, 1982) </ref>. A truly useful parser would produce a unique or limited number of parses that correspond to meaningful interpretations of a sentence that a human would actually consider.
Reference: <author> Cohen, W. W. </author> <year> (1990). </year> <title> Learning approximate control rules of high utility. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 268-276 Austin, TX. </address>
Reference: <author> Collins, M. J. </author> <year> (1996). </year> <title> A new statistical parser based on bigram lexical dependencies. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 184-191 Santa Cruz, CA. </address>
Reference: <author> De Raedt, L. </author> <year> (1992). </year> <title> Interactive Theory Revision: An Inductive Logic Programming Approach. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> De Raedt, L., & Bruynooghe, M. </author> <year> (1993). </year> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1058-1063 Chambery, France. </address>
Reference: <author> Estlin, T. A., & Mooney, R. J. </author> <year> (1996). </year> <title> Multi-strategy learning of search control for partial-order planning. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence Portland, </booktitle> <address> OR. </address>
Reference: <author> Fillmore, C. J. </author> <year> (1968). </year> <title> The case for case. In Bach, </title> <editor> E., & Harms, R. T. (Eds.), </editor> <booktitle> Universals in Linguistic Theory. </booktitle> <publisher> Holt, Reinhart and Winston, </publisher> <address> New York. </address>
Reference-contexts: Figure 2 shows the basic components of the system. Chill employs a simple deterministic, shift-reduce parser with the current parse state represented by the content of the stack and the remaining portion of the input buffer (Tomita, 1986). Consider producing a case-role analysis <ref> (Fillmore, 1968) </ref> of the sentence: "The man ate the pasta." Parsing begins with an empty stack and an input buffer containing the entire sentence.
Reference: <author> Goodman, J. </author> <year> (1996). </year> <title> Parsing algorithms and metrics. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 177-183 Santa Cruz, CA. </address>
Reference: <author> Huffman, S. B. </author> <year> (1996). </year> <title> Learning information extraction patterns from examples. </title> <editor> In Wermter, S., Riloff, E., & Scheler, G. (Eds.), </editor> <title> Connectionist, Statistical, </title> <booktitle> and Symbolic Approaches to Learning for Natural Language Processing, </booktitle> <pages> pp. 246-260. </pages> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: <author> Kijsirikul, B., Numao, M., & Shimura, M. </author> <year> (1992). </year> <title> Discrimination-based constructive induction of logic programs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 44-49 San Jose, CA. </address>
Reference-contexts: Chill's ILP algorithm combines elements from bottom-up techniques found in systems such as Cigol (Muggleton & Buntine, 1988) and Golem (Muggleton & Feng, 1992) and top-down methods from systems like Foil (Quinlan, 1990), and is able to invent new predicates in a manner analogous to Champ <ref> (Kijsirikul, Numao, & Shimura, 1992) </ref>. Details of the Chill induction algorithm together with experimental comparisons to Golem and Foil are presented by Zelle and Mooney (1994a) and Zelle (1995).
Reference: <author> Langley, P. </author> <year> (1985). </year> <title> Learning to search: From weak methods to domain specific heuristics. </title> <journal> Cognitive Science, </journal> <volume> 9 (2), </volume> <pages> 217-260. </pages>
Reference: <editor> Lavrac, N., & Dzeroski, S. (Eds.). </editor> <year> (1994). </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood. </publisher>
Reference-contexts: The standard way of easing the problem has been to supply an ILP system with relevant background knowledge (subroutines) and induce only the top-level clauses <ref> (Lavrac & Dzeroski, 1994) </ref>. Another approach has been to revise an existing program that is partially correct (De Raedt, 1992; Richards & Mooney, 1995). Chill illustrates a third approach: specializing an existing program by learning control rules that restrict the application of specific clauses.
Reference: <author> Leckie, C., & Zuckerman, I. </author> <year> (1993). </year> <title> An inductive approach to learning search control rules for planning. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1100-1105 Chamberry,France. </pages>
Reference: <author> Lehnert, W., & Sundheim, B. </author> <year> (1991). </year> <title> A performance evaluation of text-analysis technologies. </title> <journal> AI Magazine, </journal> <volume> 12 (3), </volume> <pages> 81-94. </pages>
Reference: <author> Ling, C. X., & Marinov, M. </author> <year> (1993). </year> <title> Answering the connectionist challenge: A symbolic model of learning the past tense of English verbs. </title> <journal> Cognition, </journal> <volume> 49 (3), </volume> <pages> 235-290. </pages>
Reference: <author> Magerman, D. M. </author> <year> (1995). </year> <title> Statistical decision-tree models for parsing. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 276-283 Cambridge, MA. </address>
Reference: <author> Marcus, M. </author> <year> (1980). </year> <title> A Theory of Syntactic Recognition for Natural Language. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Treating language acquisition as a control learning problem is not in itself a new idea. Berwick (1985) used this approach to learn control rules for a Marcus-style deterministic parser <ref> (Marcus, 1980) </ref>. When the system came to a parsing impasse, a new rule was created by inferring the correct parsing action and creating a new rule using certain properties of the current parser state as trigger conditions for its application.
Reference: <author> Marcus, M., Santorini, B., & Marcinkiewicz, M. </author> <year> (1993). </year> <title> Building a large annotated corpus of English: The Penn treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19 (2), </volume> <pages> 313-330. </pages>
Reference-contexts: This approach has been facilitated by the construction of large treebanks of human-produced syntactic parse trees for thousands of sentences, such as the Penn Treebank <ref> (Marcus, Santorini, & Marcinkiewicz, 1993) </ref> which consists primarily of analyses of sentences from the Wall Street Journal.
Reference: <author> Miikkulainen, R. </author> <year> (1996). </year> <title> Subsymbolic case-role analysis of sentences with embedded clauses. </title> <journal> Cognitive Science, </journal> <volume> 20 (1), </volume> <pages> 47-73. </pages>
Reference: <author> Miikkulainen, R. </author> <year> (1993). </year> <title> Subsymbolic Natural Language Processing: An Integrated Model of Scripts, Lexicon, and Memory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Mitchell, T. </author> <year> (1983). </year> <title> Learning and problem solving. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1139-1151 Karlsruhe, West Germany. </address>
Reference: <author> Mooney, R. J., & Califf, M. E. </author> <year> (1995). </year> <title> Induction of first-order decision lists: Results on learning the past tense of English verbs. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3, </volume> <pages> 1-24. </pages>
Reference-contexts: Consequently, we had to develop new ILP systems such as Chillin (Zelle & Mooney, 1994a) and Foidl <ref> (Mooney & Califf, 1995) </ref> to overcome these limitations by using techniques that integrate bottom-up and top-down search, incorporate predicate invention, eliminate the need for explicit negative examples, and allow restricted use of cuts.
Reference: <author> Muggleton, S., & Buntine, W. </author> <year> (1988). </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 339-352 Ann Arbor, MI. </address>
Reference-contexts: This control rule comprises a definite-clause definition that covers the positive control examples for the operator but not the negative. Chill's ILP algorithm combines elements from bottom-up techniques found in systems such as Cigol <ref> (Muggleton & Buntine, 1988) </ref> and Golem (Muggleton & Feng, 1992) and top-down methods from systems like Foil (Quinlan, 1990), and is able to invent new predicates in a manner analogous to Champ (Kijsirikul, Numao, & Shimura, 1992).
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1992). </year> <title> Efficient induction of logic programs. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 281-297. </pages> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: This control rule comprises a definite-clause definition that covers the positive control examples for the operator but not the negative. Chill's ILP algorithm combines elements from bottom-up techniques found in systems such as Cigol (Muggleton & Buntine, 1988) and Golem <ref> (Muggleton & Feng, 1992) </ref> and top-down methods from systems like Foil (Quinlan, 1990), and is able to invent new predicates in a manner analogous to Champ (Kijsirikul, Numao, & Shimura, 1992).
Reference: <author> Periera, F., & Shabes, Y. </author> <year> (1992). </year> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 128-135 Newark, Delaware. </address>
Reference: <author> Pinker, S. (Ed.). </author> <year> (1994). </year> <title> The Language Instinct: How the Mind Creates Language. </title>
Reference-contexts: It is generally agreed that human 2 Explicit generation of negative examples using a closed-world assumption is per formed automatically in many systems such as Foil (Quinlan, 1990). language acquisition exploits fairly restrictive constraints or biases in order to learn complex natural languages from limited data <ref> (Pinker, 1994) </ref>. Of course, evaluating the success of this approach is, ultimately, an empirical question.
Reference: <editor> William Morrow, </editor> <address> N.Y. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1996). </year> <title> Learning first-order definitions of functions. </title> <journal> Journal of Artificial Intelligence Research, </journal> <note> to appear. </note>
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 (3), </volume> <pages> 239-266. </pages>
Reference-contexts: Parsers are complex programs, the space of possible logic programs is very large, and providing the appropriate set of background predicates is difficult. It is generally agreed that human 2 Explicit generation of negative examples using a closed-world assumption is per formed automatically in many systems such as Foil <ref> (Quinlan, 1990) </ref>. language acquisition exploits fairly restrictive constraints or biases in order to learn complex natural languages from limited data (Pinker, 1994). Of course, evaluating the success of this approach is, ultimately, an empirical question. <p> Chill's ILP algorithm combines elements from bottom-up techniques found in systems such as Cigol (Muggleton & Buntine, 1988) and Golem (Muggleton & Feng, 1992) and top-down methods from systems like Foil <ref> (Quinlan, 1990) </ref>, and is able to invent new predicates in a manner analogous to Champ (Kijsirikul, Numao, & Shimura, 1992). Details of the Chill induction algorithm together with experimental comparisons to Golem and Foil are presented by Zelle and Mooney (1994a) and Zelle (1995). <p> With respect to training, an ILP system needs to guarantee that it will generate a program that will terminate and generate ground outputs when it is queried with the outputs uninstantiated. Most ILP systems cannot provide these guarantees; those that guarantee termination <ref> (Quinlan, 1990) </ref> do not guarantee ground outputs. The use of an output completeness assumption and implicit negatives is one way to guarantee ground outputs (Zelle et al., 1995).
Reference: <editor> Reilly, R. G., & Sharkey, N. E. (Eds.). </editor> <year> (1992). </year> <title> Connectionist Approaches to Natural Language Processing. </title> <publisher> Lawrence Erlbaum and Associates, </publisher> <address> Hilldale, NJ. </address>
Reference: <author> Richards, B. L., & Mooney, R. J. </author> <year> (1995). </year> <title> Automated refinement of first-order Horn-clause domain theories. </title> <journal> Machine Learning, </journal> <volume> 19 (2), </volume> <pages> 95-131. </pages>
Reference: <author> Riloff, E. </author> <year> (1993). </year> <title> Automatically constructing a dictionary for information extraction tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. 811-816. </pages>
Reference: <author> Rumelhart, D. E., & McClelland, J. </author> <year> (1986). </year> <title> On learning the past tense of English verbs. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 216-271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Simmons, R. F., & Yu, Y. </author> <year> (1992). </year> <title> The acquisition and use of context dependent grammars for English. </title> <journal> Computational Linguistics, </journal> <volume> 18 (4), </volume> <pages> 391-418. </pages>
Reference: <author> Soderland, S., & Lehnert, W. </author> <year> (1994). </year> <title> Wrap-Up: A trainable discourse module for information extraction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 131-158. </pages>
Reference: <author> Tomita, M. </author> <year> (1986). </year> <title> Efficient Parsing for Natural Language. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference-contexts: The output is a shift-reduce parser in Prolog that maps sentences into parses. Figure 2 shows the basic components of the system. Chill employs a simple deterministic, shift-reduce parser with the current parse state represented by the content of the stack and the remaining portion of the input buffer <ref> (Tomita, 1986) </ref>. Consider producing a case-role analysis (Fillmore, 1968) of the sentence: "The man ate the pasta." Parsing begins with an empty stack and an input buffer containing the entire sentence.
Reference: <author> Waibel, A., & Lee, K. F. (Eds.). </author> <year> (1990). </year> <title> Readings in Speech Recognition. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo,CA. </address>
Reference-contexts: Almost all of this work has employed statistical techniques such as n-gram models, hidden Markov models (HMMs), and probabilistic context free grammars (PCFGs). The computational linguistics community has focused on these techniques largely due to their successful application in prior work on speech recognition <ref> (Waibel & Lee, 1990) </ref>. There has also been a fair amount of recent research on applying neural-network techniques, such as simple recurrent networks, to natural language processing (Reilly & Sharkey, 1992; Miikkulainen, 1993).
Reference: <author> Warren, D., & Pereira, F. </author> <year> (1982). </year> <title> An efficient easily adaptable system for interpreting natural language queries. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> 8 (3-4), </volume> <pages> 110-122. </pages>
Reference: <editor> Wermter, S., Riloff, E., & Scheler, G. (Eds.). </editor> <year> (1996). </year> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing. </title> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: However, there has been relatively little recent work using symbolic machine learning techniques for language applications, although some recent systems have employed decision trees (Magerman, 1995; Anoe & Bennett, 1995), transformation rules (Brill, 1993, 1995), and other symbolic methods <ref> (Wermter, Riloff, & Scheler, 1996) </ref>. However, all of these approaches are limited to examples represented as fixed-length feature vectors and are therefore subject to the standard limitations of propositional representations.
Reference: <author> Wirth, R. </author> <year> (1988). </year> <title> Learning by failure to prove. </title> <booktitle> In Proceedings of the Third European Working Session on Learning, </booktitle> <pages> pp. 237-251. </pages> <publisher> Pitman. </publisher>
Reference-contexts: Despite this fact, other than our own work, there has apparently been no application of ILP methods to language processing, with one early exception <ref> (Wirth, 1988, 1989) </ref>. Over the last four years, we have explored the application of ILP to NLP.
Reference: <author> Wirth, R. </author> <year> (1989). </year> <title> Completing logic programs by inverse resolution. </title> <booktitle> In Proceedings of the Fourth European Working Session on Learning, </booktitle> <pages> pp. 239-250. </pages> <publisher> Pitman. </publisher>
Reference: <author> Zelle, J. M. </author> <year> (1995). </year> <title> Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. </title> <type> Ph.D. thesis, </type> <institution> University of Texas, Austin, TX. </institution>
Reference-contexts: Since only positive tuples of parse/2 are available, the generic ILP approach employed a version of the induction algorithm that exploits the output-completeness assumption to learn in the context of implicit negative examples <ref> (Zelle et al., 1995) </ref> as outlined in section 2.1. The experiment was carried out using a portion of the ATIS corpus from a preliminary version of the Penn Treebank. The first example in Figure 1 is taken from this corpus. <p> Most ILP systems cannot provide these guarantees; those that guarantee termination (Quinlan, 1990) do not guarantee ground outputs. The use of an output completeness assumption and implicit negatives is one way to guarantee ground outputs <ref> (Zelle et al., 1995) </ref>. With respect to testing, experiments need to specifically evaluate the ability of the learned program to generate correct outputs given only novel inputs. Unlike evaluations of other ILP systems, experiments with Chill and Foidl specifically tested this ability.
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1993a). </year> <title> Combining FOIL and EBG to speed-up logic programs. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1106-1111 Chambery, France. </address>
Reference-contexts: These systems focus on learning control rules that improve the efficiency of an existing program, such as transforming an O (n!) naive sorting program into an O (n 2 ) insertion sort <ref> (Zelle & Mooney, 1993a) </ref>. Chill illustrates that this approach can also be used to improve the accuracy of an initial (extremely) overly-general program.
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1993b). </year> <title> Learning semantic grammars with constructive inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 817-822 Washington, D.C. </address>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994a). </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 343-351 New Brunswick, NJ. </address>
Reference-contexts: Consequently, we had to develop new ILP systems such as Chillin <ref> (Zelle & Mooney, 1994a) </ref> and Foidl (Mooney & Califf, 1995) to overcome these limitations by using techniques that integrate bottom-up and top-down search, incorporate predicate invention, eliminate the need for explicit negative examples, and allow restricted use of cuts.
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994b). </year> <title> Inducing deterministic Prolog parsers from treebanks: A machine learning approach. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 748-753 Seattle, WA. </address>
Reference-contexts: Tagging words with their appropriate part of speech can be performed with high accuracy using various techniques (Church, 1988; Brill, 1995). Zelle and Mooney (1994a) and Zelle (1995) present results both with and without part-of-speech information. Our initial experiments used a straightforward set of syntactic shift-reduce parsing operators <ref> (Zelle & Mooney, 1994b) </ref>. However, better results were obtained by making the operators more specific, effectively increasing the number of operators, but reducing the complexity of the control-rule induction task for each operator. The basic idea was to index the operators based on some relevant portion of the parsing context.
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1996a). </year> <title> Comparative results on using inductive logic programming for corpus-based parser construction. </title> <editor> In Wermter, S., Riloff, E., & Scheler, G. (Eds.), </editor> <title> Connectionist, Statistical, </title> <booktitle> and Symbolic Approaches to Learning for Natural Language Processing, </booktitle> <pages> pp. 355-369. </pages> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1996b). </year> <title> Learning to parse database queries using inductive logic programming. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence Portland, </booktitle> <address> OR. </address>
Reference: <author> Zelle, J. M., Thompson, C., Califf, M. E., & Mooney, R. J. </author> <year> (1995). </year> <title> Inducing logic programs without explicit negative examples. </title> <booktitle> In Proceedings of the Fifth International Workshop on Inductive Logic Programming, </booktitle> <pages> pp. 403-416 Leuven, </pages> <editor> Belgium. </editor> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Since only positive tuples of parse/2 are available, the generic ILP approach employed a version of the induction algorithm that exploits the output-completeness assumption to learn in the context of implicit negative examples <ref> (Zelle et al., 1995) </ref> as outlined in section 2.1. The experiment was carried out using a portion of the ATIS corpus from a preliminary version of the Penn Treebank. The first example in Figure 1 is taken from this corpus. <p> Most ILP systems cannot provide these guarantees; those that guarantee termination (Quinlan, 1990) do not guarantee ground outputs. The use of an output completeness assumption and implicit negatives is one way to guarantee ground outputs <ref> (Zelle et al., 1995) </ref>. With respect to testing, experiments need to specifically evaluate the ability of the learned program to generate correct outputs given only novel inputs. Unlike evaluations of other ILP systems, experiments with Chill and Foidl specifically tested this ability.
References-found: 62

