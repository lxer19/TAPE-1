URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/anatomy.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/anatomy.html
Root-URL: 
Title: Anatomy of a Message in the Alewife Multiprocessor  
Author: John Kubiatowicz and Anant Agarwal 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science, NE43-633 Massachusetts Institute of Technology  
Abstract: Shared-memory provides a uniform and attractive mechanism for communication. For efficiency, it is often implemented with a layer of interpretive hardware on top of a message-passing communications network. This interpretive layer is responsible for data location, data movement, and cache coherence. It uses patterns of communication that benefit common programming styles, but which are only heuristics. This suggests that certain styles of communication may benefit from direct access to the underlying communications substrate. The Alewife machine, a shared-memory multiprocessor being built at MIT, provides such an interface. The interface is an integral part of the shared memory implementation and affords direct, user-level access to the network queues, supports an efficient DMA mechanism, and includes fast trap handling for message reception. This paper discusses the design and implementation of the Alewife message-passing interface and addresses the issues and advantages of using such an interface to complement hardware-synthesized shared memory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: 1 Introduction Given current trends in network and systems design, it should come as no surprise that most distributed shared-memory machines are built on top of an underlying message-passing substrate <ref> [1, 2, 3] </ref>. Architects of shared-memory machines often obscure this topology with a layer of hardware that implements their favorite memory coherence protocol and that insulates the processor entirely from the interconnection network. In such machines, communication between processing elements can occur only through the shared-memory abstraction.
Reference: [2] <author> Thomas H. Dunigan. </author> <title> Kendall Square Multiprocessor: Early Experiences and Performance. </title> <type> Technical Report ORNL/TM-12065, </type> <institution> Oak Ridge National Laboratory, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Given current trends in network and systems design, it should come as no surprise that most distributed shared-memory machines are built on top of an underlying message-passing substrate <ref> [1, 2, 3] </ref>. Architects of shared-memory machines often obscure this topology with a layer of hardware that implements their favorite memory coherence protocol and that insulates the processor entirely from the interconnection network. In such machines, communication between processing elements can occur only through the shared-memory abstraction. <p> The barrier synchronization with message passing takes only 20 sec, while the best shared memory version runs in 50 sec. Comparable software implementations (e.g. Intel DELTA and iPSC/860, Kendall Square KSR1) take well over 400 sec <ref> [2] </ref>. These numbers can also be compared with hardware-supported synchronization mechanisms, such as on the CM5, that take only 2 or 3 sec but that require separate, log-structured (and potentially less-scalable) synchronization networks.
Reference: [3] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hen-nessy, M. Horowitz, and M. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Given current trends in network and systems design, it should come as no surprise that most distributed shared-memory machines are built on top of an underlying message-passing substrate <ref> [1, 2, 3] </ref>. Architects of shared-memory machines often obscure this topology with a layer of hardware that implements their favorite memory coherence protocol and that insulates the processor entirely from the interconnection network. In such machines, communication between processing elements can occur only through the shared-memory abstraction. <p> In some cases where we argue messages are better that shared-memory, such as the barrier in Section 8, a similar effect could be achieved by using shared-memory with a weaker consistency model. For example, the Dash multiprocessor <ref> [3, 23] </ref> has a mechanism to deposit a value from one processor's cache directly into the cache of another processor, avoiding cache coherence overhead. This mechanism might actually be faster than using a message because no interrupt occurs, but a message is much more general.
Reference: [4] <author> SPARC Architecture Manual, </author> <year> 1988. </year> <institution> SUN Microsystems, Mountain View, California. </institution>
Reference-contexts: An Alewife node consists of a 33 MHz Sparcle processor, 64K bytes of direct-mapped cache, 4M bytes of globally-shared main memory, and a floating-point coprocessor. Both the cache and floating-point units are SPARC compatible <ref> [4] </ref>. The nodes communicate via messages through a direct network [5] with a mesh topology using wormhole routing [6]. A single-chip Communications and Memory Management Unit (CMMU) on each node holds the cache tags and implements the cache coherence protocol by synthesizing messages to other nodes. <p> The CMMU interprets the first operand (or first word in memory if no operands 1 Note that this store instruction differs from normal store instructions only in the value that it produces for the SPARC alternate space indicator (ASI) field <ref> [4] </ref>. Header Operand 1 . . Operand m-1 Address 0 Length 0 Address 1 Length 1 . . Address n-1 Length n-1 are present) as a packet header. The packet descriptor can be up to eight (8) double-words long.
Reference: [5] <author> Charles L. Seitz. </author> <title> Concurrent VLSI Architectures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-33(12):1247-1265, </volume> <month> December </month> <year> 1984. </year>
Reference-contexts: An Alewife node consists of a 33 MHz Sparcle processor, 64K bytes of direct-mapped cache, 4M bytes of globally-shared main memory, and a floating-point coprocessor. Both the cache and floating-point units are SPARC compatible [4]. The nodes communicate via messages through a direct network <ref> [5] </ref> with a mesh topology using wormhole routing [6]. A single-chip Communications and Memory Management Unit (CMMU) on each node holds the cache tags and implements the cache coherence protocol by synthesizing messages to other nodes. This chip also implements the message interface.
Reference: [6] <author> William J. Dally. </author> <title> A VLSI Architecture for Concurrent Data Structures. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1987. </year>
Reference-contexts: Both the cache and floating-point units are SPARC compatible [4]. The nodes communicate via messages through a direct network [5] with a mesh topology using wormhole routing <ref> [6] </ref>. A single-chip Communications and Memory Management Unit (CMMU) on each node holds the cache tags and implements the cache coherence protocol by synthesizing messages to other nodes. This chip also implements the message interface. A unique feature of Alewife is its LimitLESS directory coherence protocol [7].
Reference: [7] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: A single-chip Communications and Memory Management Unit (CMMU) on each node holds the cache tags and implements the cache coherence protocol by synthesizing messages to other nodes. This chip also implements the message interface. A unique feature of Alewife is its LimitLESS directory coherence protocol <ref> [7] </ref>. This scheme implements a full-map directory protocol [8] by trapping into software for widely shared data items. <p> In Alewife, the memory system implements a set of pointers, called directories. Each directory keeps track of the cached copies of a corresponding memory line. In our current implementation, the size of the directory can be varied from zero to five pointers. The novel feature of the LimitLESS scheme <ref> [7] </ref> is that when there are more cached copies than there are pointers, the system traps the processor for software extension of the directory into main memory 7 cessor can then implement an algorithm of its choice in software to handle this situation.
Reference: [8] <author> Anant Agarwal, Richard Simoni, John Hennessy, and Mark Horowitz. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1988. </year> <note> IEEE. </note>
Reference-contexts: This chip also implements the message interface. A unique feature of Alewife is its LimitLESS directory coherence protocol [7]. This scheme implements a full-map directory protocol <ref> [8] </ref> by trapping into software for widely shared data items.
Reference: [9] <author> Shekhar Borkar et al. </author> <title> iWarp: An Integrated Solution to High-Speed Parallel Computing. </title> <booktitle> In Proceedings of Supercomputing '88,November 1988. </booktitle>
Reference-contexts: Thus, efficient messaging facilities should permit direct transfer of information from registers to the network interface. Direct register-to-register transmission has been suggested by a number of architects <ref> [9, 10, 11, 12] </ref>. 2. Blocks of data that reside in memory often accompany such header information. Consequently, efficient messaging facilities should allow direct memory access (DMA) mechanisms to be invoked inexpensively, possibly on multiple blocks of data. <p> Here, a thin layer of interrupt-driven operating system software can synthesize an arbitrary network queueing structure in software, although possibly at the cost of extra memory-to memory copies. 4. Permitting compilers (or users) to generate network communications code can have a number of advantages <ref> [11, 9, 16] </ref>, but requires user-level access to the message interface. Accordingly, the Alewife machine provides a uniform network interface with the following features: * Sending a message is an atomic, user-level, two-phase action: describe the message, then launch it. <p> In contrast, Alewife's network overflow mechanism provides hysteresis to ignore temporary network blockages. Further, the lack of message atomicity in the J-machine complicates the functionality of network overflow handlers. Support for multiple models of computation has been identified as a promising direction for future research. For example, the iWarp <ref> [9] </ref> integrates systolic and message passing styles of communications. Their interface supports DMA-style communication for long packets typical in message passing systems, while at the same time supporting systolic processor-to-processor communication.
Reference: [10] <author> William J. Dally et al. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <booktitle> In Proceedings of the IFIP (International Federationfor Information Processing), 11th World Congress, </booktitle> <pages> pages 1147-1153, </pages> <address> New York, 1989. </address> <publisher> Elsevier Science Publishing. </publisher>
Reference-contexts: Thus, efficient messaging facilities should permit direct transfer of information from registers to the network interface. Direct register-to-register transmission has been suggested by a number of architects <ref> [9, 10, 11, 12] </ref>. 2. Blocks of data that reside in memory often accompany such header information. Consequently, efficient messaging facilities should allow direct memory access (DMA) mechanisms to be invoked inexpensively, possibly on multiple blocks of data. <p> Some modern processors, such as the Alewife's Sparcle processor [14], MOSAIC [15], and the MDP <ref> [10] </ref>, can respond rapidly to interrupts. In particular, vectored interrupts permit dispatch directly to appropriate code segments, and reserved hardware contexts can remove the need for saving and restoring registers in interrupt handlers. This couples with efficient DMA to provide another advantage: virtual queuing. <p> The processor in the CM5 can be notified on message arrival either through an interrupt or by polling [22]. Our interface is different from that provided by the message passing J-machine <ref> [10] </ref> in that our processor is always interrupted on the arrival of a message, allowing the processor to examine the packet header, and to decide how to deal with the message.
Reference: [11] <author> Thorsten von Eicken, David Culler, Seth Goldstein, and Klaus Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Thus, efficient messaging facilities should permit direct transfer of information from registers to the network interface. Direct register-to-register transmission has been suggested by a number of architects <ref> [9, 10, 11, 12] </ref>. 2. Blocks of data that reside in memory often accompany such header information. Consequently, efficient messaging facilities should allow direct memory access (DMA) mechanisms to be invoked inexpensively, possibly on multiple blocks of data. <p> Here, a thin layer of interrupt-driven operating system software can synthesize an arbitrary network queueing structure in software, although possibly at the cost of extra memory-to memory copies. 4. Permitting compilers (or users) to generate network communications code can have a number of advantages <ref> [11, 9, 16] </ref>, but requires user-level access to the message interface. Accordingly, the Alewife machine provides a uniform network interface with the following features: * Sending a message is an atomic, user-level, two-phase action: describe the message, then launch it. <p> This suggests that unique send and receive code might be generated for each type of communication, much in the flavor of active messages <ref> [11] </ref>. Further, when a message can be launched in less than ten cycles, the time to cross a protection barrier can easily double or triple the cost of sending that message.
Reference: [12] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A Tightly-Coupled Processor-Network Interface. </title> <booktitle> In Fifth Internataional Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <address> Boston, </address> <month> October </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: Thus, efficient messaging facilities should permit direct transfer of information from registers to the network interface. Direct register-to-register transmission has been suggested by a number of architects <ref> [9, 10, 11, 12] </ref>. 2. Blocks of data that reside in memory often accompany such header information. Consequently, efficient messaging facilities should allow direct memory access (DMA) mechanisms to be invoked inexpensively, possibly on multiple blocks of data.
Reference: [13] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory; Early Experience. </title> <booktitle> In To appear in Proceedings of Practice and Principles of Parallel Programming (PPoPP) 1993, </booktitle> <address> New York, NY, </address> <month> May </month> <year> 1993. </year> <note> ACM. Also as MIT/LCS TM-478, </note> <month> January </month> <year> 1993. </year>
Reference-contexts: Consequently, efficient messaging facilities should allow direct memory access (DMA) mechanisms to be invoked inexpensively, possibly on multiple blocks of data. This is important for a number of reasons, including rapid task dispatch (where a task-frame or portion of the calling stack may be transmitted along with the continuation) <ref> [13] </ref> and distributed block I/O (where both a buffer-header struc ture and data may reside in memory). 3. Some modern processors, such as the Alewife's Sparcle processor [14], MOSAIC [15], and the MDP [10], can respond rapidly to interrupts. <p> There are a number of reasons for this. First, it provides far more mechanism than is actually needed in many cases. As Section 8 demonstrates, message passing is useful as a way of bypassing the coherence protocol. In fact, many of the applications of message-passing discussed in <ref> [13] </ref> do not require a complete mechanism. Second, a machine with a single network port cannot fetch dirty source data while in the middle of transmitting a larger packet since this requires the sending of messages. <p> since we relaunch packets from memory (output DMA) which may require storeback operations during processing (input DMA). 8 Results This section summarizes results comparing the performance of the shared-memory and integrated implementations of several library routines and applications; for a detailed discussion of our experience with this integrated interface see <ref> [13] </ref>. The results are obtained on a detailed, cycle-by-cycle machine simulator, using Alewife's run-time system based on lazy task creation [21] for load-balancing and dynamic partitioning of programs. Our results include a comparison of the performance of purely shared memory and integrated versions of several application kernels.
Reference: [14] <institution> MIT-SPARCLE Specification Version 1.1 (Preliminary). LSI Logic Corporation, Milpitas, </institution> <address> CA 95035, </address> <year> 1990. </year> <title> Addendum to the 64811 specification. </title>
Reference-contexts: Some modern processors, such as the Alewife's Sparcle processor <ref> [14] </ref>, MOSAIC [15], and the MDP [10], can respond rapidly to interrupts. In particular, vectored interrupts permit dispatch directly to appropriate code segments, and reserved hardware contexts can remove the need for saving and restoring registers in interrupt handlers. This couples with efficient DMA to provide another advantage: virtual queuing.
Reference: [15] <author> C.L. Seitz, N.J. Boden, J. Seizovic, and W.K. Su. </author> <title> The Design of the Caltech Mosaic C Multicomputer. </title> <booktitle> In Research on Integrated Systems Symposium Proceedings, </booktitle> <pages> pages 1-22, </pages> <address> Cambridge, MA, 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Some modern processors, such as the Alewife's Sparcle processor [14], MOSAIC <ref> [15] </ref>, and the MDP [10], can respond rapidly to interrupts. In particular, vectored interrupts permit dispatch directly to appropriate code segments, and reserved hardware contexts can remove the need for saving and restoring registers in interrupt handlers. This couples with efficient DMA to provide another advantage: virtual queuing.
Reference: [16] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <address> Boston, </address> <month> October </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: Here, a thin layer of interrupt-driven operating system software can synthesize an arbitrary network queueing structure in software, although possibly at the cost of extra memory-to memory copies. 4. Permitting compilers (or users) to generate network communications code can have a number of advantages <ref> [11, 9, 16] </ref>, but requires user-level access to the message interface. Accordingly, the Alewife machine provides a uniform network interface with the following features: * Sending a message is an atomic, user-level, two-phase action: describe the message, then launch it.
Reference: [17] <author> John Kubiatowicz. </author> <title> User's Manual for the A-1000 Communications and Memory Management Unit. ALEWIFE Memo No. </title> <type> 19, </type> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: As discussed in Section 5.3, global shared-memory accesses 3 distinguishes between protocol, system, and user messages. made by critical message handlers can lead to harmful inter actions. 4 Mechanisms This section describes the architectural mechanisms supported by Alewife's communications interface. A detailed programmer's interface is given in <ref> [17] </ref>.
Reference: [18] <author> John Kubiatowicz, David Chaiken, and Anant Agarwal. </author> <title> Closing the Window of Vulnerability in Multiphase Memory Transactions. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 274-284, </pages> <address> Boston, </address> <month> October </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: To some extent, these interactions arise from the fact that the network provides a single logical input and output port to the memory controller. While networks with multiple channels are possible to implement, they are invariably more expensive. 5.1 High-Availability Interrupts The need for high-availability interrupts <ref> [18] </ref> arises becauseshared-memory introduces a dependence between instruction execution and and the interconnection network. Normal asynchronous interrupts, which occur only at instruction boundaries, are effectively disabled when the processor pipeline is frozen for a remote read or write request. <p> The original request must then be reissued when the interrupt finishes. In unfortunate situations, systematic thrashing can occur. This is part of a larger issue, namely the window of vulnerability, discussedin <ref> [18] </ref>. For a single-threaded processor, the simplest solution is to defer the invalidation until after the original load or store commits. 5.2 Restrictions on Message Handlers A second issue is the interaction between message handlers and shared memory. <p> The second arises for the same reason that high-availability interrupts were introduced into the picture: any global data that is accessed may be stuck behind other messages in the input queue. The last condition prevents deadlocks in the thrash elimination mechanism. See <ref> [18] </ref> for details. 5.3 Local Coherence for DMA Since Alewife is a cache-coherent, shared-memory multiprocessor, it is natural to ask which form of data coherenceshould be supported by the DMA mechanism. Three possibilities present themselves: 1.
Reference: [19] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: This leads to a possibility for protocol deadlock, since it introduces a circular dependence between the network queues of two or more nodes. Architectures such as DASH <ref> [19] </ref> have avoided this problem by introducing independent networks for request and response messages.
Reference: [20] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <note> To appear in IEEE Micro, </note> <month> June </month> <year> 1993. </year>
Reference-contexts: IPI Input Invalidation Queue (2 entries) 1610 gates IPI Output Invalidation Queue (2 entries) 1306 gates IPI Input Interface 4181 gates IPI Output Interface 3393 gates Table 4: Module sizes in gates or bits. 7 Implementation and Status The Alewife message interface is implemented in two components, the Sparcle processor <ref> [20] </ref> and the A-1000 Communications and Memory Management Unit (CMMU). The Sparcle processor includes the special ldio and stio instructions for direct access to the IPI output descriptor queue and the IPI input window. In addition, a SPARC-like coprocessor interface enables pipelining of message sends and storeback requests.
Reference: [21] <author> E. Mohr, D. Kranz, and R. Halstead. </author> <title> Lazy Task Creation: A Technique for Increasing the Granularity of Parallel Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The results are obtained on a detailed, cycle-by-cycle machine simulator, using Alewife's run-time system based on lazy task creation <ref> [21] </ref> for load-balancing and dynamic partitioning of programs. Our results include a comparison of the performance of purely shared memory and integrated versions of several application kernels.
Reference: [22] <institution> The Connection Machine System: Programming the NI. Thinking Machines Corporation, </institution> <month> March </month> <year> 1992. </year> <note> Version 7.1. </note>
Reference-contexts: The CM5 interface does not provide support for DMA or shared memory and requires the processor to be involved in emptying out the message queue. The processor in the CM5 can be notified on message arrival either through an interrupt or by polling <ref> [22] </ref>. Our interface is different from that provided by the message passing J-machine [10] in that our processor is always interrupted on the arrival of a message, allowing the processor to examine the packet header, and to decide how to deal with the message.
Reference: [23] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1990. </year> <note> IEEE. </note>
Reference-contexts: In some cases where we argue messages are better that shared-memory, such as the barrier in Section 8, a similar effect could be achieved by using shared-memory with a weaker consistency model. For example, the Dash multiprocessor <ref> [3, 23] </ref> has a mechanism to deposit a value from one processor's cache directly into the cache of another processor, avoiding cache coherence overhead. This mechanism might actually be faster than using a message because no interrupt occurs, but a message is much more general.
Reference: [24] <author> Bob Beck, Bob Kasten, and Shreekant Thakkar. </author> <title> VLSI Assist for a Multiprocessor. </title> <booktitle> In Proceedings Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS II), </booktitle> <address> Washington, DC, </address> <month> October </month> <year> 1987. </year> <note> IEEE. </note>
Reference-contexts: This mechanism might actually be faster than using a message because no interrupt occurs, but a message is much more general. Some shared-memory machines have implemented message-like primitives in hardware. For example, Beck, Kasten, and Thakkar <ref> [24] </ref> describe the implementation of SLICa system link and interrupt controller chipfor use with the Sequent Balance system. Each SLIC chip is coupled with a processing node and communicates with the other SLIC chips on a special SLIC bus that is separate from the memory system bus.
Reference: [25] <author> A. Cox and R. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year> <note> Also as a Univ. </note> <institution> Rochester TR-263, </institution> <month> May </month> <year> 1989. </year> <month> 12 </month>
Reference-contexts: This machine provides both hardware support for block transfers and the ability to send remote interrupt requests. Nodes in the Butterfly are able to initiate DMA operations for blocks of data which reside in remote nodes. In an implementation of distributed shared memory on this machine, Cox and Fowler <ref> [25] </ref> conclude that an effective block transfer mechanism was critical to performance. They argue that a mechanism that allows more concurrency between processing and block transfer would make a bigger impact.
References-found: 25

