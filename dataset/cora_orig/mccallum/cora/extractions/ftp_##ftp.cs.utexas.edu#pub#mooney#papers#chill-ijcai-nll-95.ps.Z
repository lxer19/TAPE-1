URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/chill-ijcai-nll-95.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: zelle,mooney@cs.utexas.edu  
Title: New Approaches to Learning for Natural Language Processing A Comparison of Two Methods Employing Inductive
Author: John M. Zelle, and Raymond J. Mooney 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas  
Note: Appears in: IJCAI 95 Workshop on  
Abstract: This paper presents results from recent experiments with Chill, a corpus-based parser acquisition system. Chill treats grammar acquisition as the learning of search-control rules within a logic program. Unlike many current corpus-based approaches that use propositional or probabilistic learning algorithms, Chill uses techniques from inductive logic programming (ILP) to learn relational representations. The reported experiments compare Chill's performance to that of a more naive application of ILP to parser acquisition. The results show that ILP techniques, as employed in Chill, are a viable alternative to propositional methods and that the control-rule framework is fundamental to Chill's success. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Black, E.; Jelineck, F.; Lafferty, J.; Magerman, D.; Mercer, R.; and Roukos, S. </author> <year> 1993. </year> <title> Towards history-based grammars: Using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 31-37. </pages>
Reference: <author> Black, E.; Lafferty, J.; and Roukaos, S. </author> <year> 1992. </year> <title> Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 185-192. </pages>
Reference: <author> Black, E. e. </author> <year> 1991. </year> <title> A procedure for quantitatively comparing the syntactic coverage of English grammars. </title> <booktitle> In Proceedings of the Fourth DARPA Speech and Natural Language Workshop, </booktitle> <pages> 306-311. </pages>
Reference-contexts: Another accuracy measure, which has been used in evaluating systems that bracket the input sentence into unlabeled constituents, is the proportion of constituents in the parse that do not cross any constituent boundaries in the correct tree <ref> (Black 1991) </ref>. We have computed the number of sentences with parses containing no crossing constituents, as well as the proportion of constituents which are non-crossing over all test sentences.
Reference: <author> Brill, E. </author> <year> 1993. </year> <title> Automatic grammar induction and parsing free text: A transformation-based approach. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 259-265. </pages>
Reference: <author> Charniak, E., and Carroll, G. </author> <year> 1994. </year> <title> Context-sensitive statistics for improved grammatical language models. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle>
Reference: <author> Hindle, D., and Rooth, M. </author> <year> 1993. </year> <title> Structural ambiguity and lexical relations. </title> <booktitle> Computational Linguistics 19(1) </booktitle> <pages> 103-120. </pages>
Reference: <author> Kijsirikul, B.; Numao, M.; and Shimura, M. </author> <year> 1992. </year> <title> Discrimination-based constructive induction of logic programs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 44-49. </pages>
Reference: <author> Lavrac, N., and Dzeroski, S., eds. </author> <year> 1994. </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> El-lis Horwood. </publisher>
Reference: <author> Lehman, J. F. </author> <year> 1994. </year> <title> Toward the essential nature of satistical knowledge in sense resolution. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence. </booktitle>
Reference: <author> Magerman, D. M. </author> <year> 1994. </year> <title> Natrual Lagnuage Parsing as Statistical Pattern Recognition. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University. </institution>
Reference: <author> Manning, C. D. </author> <year> 1993. </year> <title> Automatic acquisition of a large subcategorization dictionary from corpora. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 235-242. </pages>
Reference: <author> Marcus, M.; Santorini, B.; and Marcinkiewicz, M. </author> <year> 1993. </year> <title> Building a large annotated corpus of english: The Penn treebank. </title> <booktitle> Computational Linguistics 19(2) </booktitle> <pages> 313-330. </pages>
Reference: <author> McClelland, J. L., and Kawamoto, A. H. </author> <year> 1986. </year> <title> Mechanisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In Rumelhart, D. E., and Mc-Clelland, J. L., eds., </editor> <booktitle> Parallel Distributed Processing, Vol. II. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <pages> 318-362. </pages>
Reference-contexts: This task involves a corpus of 1475 sentence/case-structure pairs originally presented in <ref> (McClelland & Kawamoto 1986) </ref>. The corpus was produced from a set of 19 sentence templates, generating sentences such as "The HUMAN ate the FOOD with the UTENSIL", where the capitalized items are replaced with words of the given category.
Reference: <author> Miikkulainen, R., and Dyer, M. G. </author> <year> 1991. </year> <title> Natural language processing with modular PDP networks and distributed lexicon. </title> <booktitle> Cognitive Science 15 </booktitle> <pages> 343-399. </pages>
Reference: <author> Miller, S.; Bobrow, R.; Ingria, R.; and Schwartz, R. </author> <year> 1994. </year> <title> Hidden understanding models of natural language. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 25-32. </pages>
Reference: <author> Mooney, R. J., and Califf, M. E. </author> <year> 1995. </year> <title> Induction of first-order decision lists: Results on learning the past tense of English verbs. </title> <note> Journal of Artificial Intelligence Research in press. </note>
Reference-contexts: Rather than attempting to generate a set of explicit negative examples, we have developed a technique of quantifying implicit negative examples that effectively overcomes this difficulty <ref> (Mooney & Califf 1995) </ref>. The problem of providing appropriate back-ground knowledge has been largely unexplored; in our experiments, we relied on the ability of the ILP algorithm to invent suitable background relations. <p> Since the naive approach requires positive and negative examples of the parse/2 concept, we employed a version of the induction algorithm which exploits the output-completeness assumption to learn in the context of implicit negative examples <ref> (Mooney & Califf 1995) </ref>. A complete discussion of this technique is beyond the scope of this paper, but the intuition is straightforward. A developing program is evaluated by using it to construct all possible representations that it can generate from a given training sentence.
Reference: <author> Muggleton, S., and Buntine, W. </author> <year> 1988. </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> 339-352. </pages>
Reference: <author> Muggleton, S., and Feng, C. </author> <year> 1992. </year> <title> Efficient induction of logic programs. </title> <editor> In Muggleton, S., ed., </editor> <booktitle> Inductive Logic Programming. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher> <pages> 281-297. </pages>
Reference-contexts: There is a growing body of research in inductive logic programming which addresses this problem. Chill combines elements from bottom-up techniques found in systems such as Cigol (Mug-gleton & Buntine 1988) and Golem <ref> (Muggleton & Feng 1992) </ref> and top-down methods from systems like Foil (Quinlan 1990), and is able to invent new predicates in a manner analogous to Champ (Ki-jsirikul, Numao, & Shimura 1992). Details of the Chill induction algorithm can be found in (Zelle & Mooney 1993; 1994a).
Reference: <author> Muggleton, S.; King, R.; and Sternberg, M. </author> <year> 1992. </year> <title> Protein secondary structure prediction using logic-based machine learning. </title> <booktitle> Protein Engineering 5(7) </booktitle> <pages> 647-657. </pages>
Reference-contexts: There is a growing body of research in inductive logic programming which addresses this problem. Chill combines elements from bottom-up techniques found in systems such as Cigol (Mug-gleton & Buntine 1988) and Golem <ref> (Muggleton & Feng 1992) </ref> and top-down methods from systems like Foil (Quinlan 1990), and is able to invent new predicates in a manner analogous to Champ (Ki-jsirikul, Numao, & Shimura 1992). Details of the Chill induction algorithm can be found in (Zelle & Mooney 1993; 1994a).
Reference: <editor> Muggleton, S. H., ed. </editor> <booktitle> 1992. Inductive Logic Programming. </booktitle> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference-contexts: There is a growing body of research in inductive logic programming which addresses this problem. Chill combines elements from bottom-up techniques found in systems such as Cigol (Mug-gleton & Buntine 1988) and Golem <ref> (Muggleton & Feng 1992) </ref> and top-down methods from systems like Foil (Quinlan 1990), and is able to invent new predicates in a manner analogous to Champ (Ki-jsirikul, Numao, & Shimura 1992). Details of the Chill induction algorithm can be found in (Zelle & Mooney 1993; 1994a).
Reference: <author> Periera, F., and Shabes, Y. </author> <year> 1992. </year> <title> Inside-outside rees-timation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 128-135. </pages>
Reference: <author> Quinlan, J. R., and Cameron-Jones, R. M. </author> <year> 1993. </year> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the Eu-ropean Conference on Machine Learning, </booktitle> <pages> 3-20. </pages>
Reference: <author> Quinlan, J. </author> <year> 1990. </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5(3) </booktitle> <pages> 239-266. </pages>
Reference-contexts: There is a growing body of research in inductive logic programming which addresses this problem. Chill combines elements from bottom-up techniques found in systems such as Cigol (Mug-gleton & Buntine 1988) and Golem (Muggleton & Feng 1992) and top-down methods from systems like Foil <ref> (Quinlan 1990) </ref>, and is able to invent new predicates in a manner analogous to Champ (Ki-jsirikul, Numao, & Shimura 1992). Details of the Chill induction algorithm can be found in (Zelle & Mooney 1993; 1994a).
Reference: <author> Zelle, J. M., and Mooney, R. J. </author> <year> 1993. </year> <title> Learning semantic grammars with constructive inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 817-822. </pages>
Reference: <author> Zelle, J. M., and Mooney, R. J. </author> <year> 1994a. </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning. </booktitle>
Reference: <author> Zelle, J. M., and Mooney, R. J. </author> <year> 1994b. </year> <title> Inducing deterministic Prolog parsers from treebanks: A machine learning approach. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 748-753. </pages>
Reference-contexts: For example, an operator to reduce a prepositional phrase might look like: op ([S1,S2|Ss], Words, [pp:[S2,S1]|Ss], Words). Our initial experiments used this simple rep-resentation of parsing actions <ref> (Zelle & Mooney 1994b) </ref>. However, better results were obtained by making the operators more specific, effectively increasing the number of operators, but reducing the complexity of the control-rule induction task for each operator. The basic idea was to index the operators based on some relevant portion of the parsing context.
References-found: 26

