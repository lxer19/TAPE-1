URL: ftp://hpsl.cs.umd.edu/pub/papers/ics96.distributions.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Email: e-mail: fujaldon, ezapatag@atc.ctima.uma.es e-mail: fshamik, saltzg@cs.umd.edu  
Title: Experimental Evaluation of Efficient Sparse Matrix Distributions  
Author: Manuel Ujaldon Shamik D. Sharma Emilio L. Zapata Joel Saltz 
Address: Campus de Teatinos. 29071 Malaga, Spain College Park, MD 20742, USA  
Affiliation: Computer Architecture Department Computer Science Department University of Malaga University of Maryland  
Abstract: Sparse matrix problems are difficult to parallelize efficiently on distributed memory machines since non-zero elements are unevenly scattered and are accessed via multiple levels of indirection. Irregular distributions that achieve good load balance and locality are hard to compute, have high memory overheads and also lead to further indirection in locating distributed data. This paper evaluates alternative semi-regular distribution strategies which trade off the quality of load-balance and locality for lower decomposition overheads and efficient lookup. The proposed techniques are compared to an irregular sparse matrix partitioner and the relative merits of each distribution method are outlined. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Barrett et al. </author> <title> Templates for the solution of linear systems: Building blocks for iterative methods. </title> <publisher> SIAM, </publisher> <year> 1994. </year>
Reference-contexts: Considerable savings in memory and computation time can be achieved by using sparse matrix storage formats that store (and allow access to) only the non-zero elements <ref> [1] </ref>. Figure 1 shows how the sparse matrix A would be stored by using two commonly used formats | CRS (Compressed Row Storage) and CCS (Compressed Column Storage). In CRS, a vector, called Data , stores the non-zero values in the matrix in a row-major order. <p> In general, a dense vector alignment with MRD will not be a good choice when the vector, apart from being used together with the matrix, appears isolated in the left hand side of several other operations, like vector-vector products (the Lanczos algorithm applied to a sparse matrix <ref> [1] </ref> is a typical example). In such cases, the loss of efficiency due to load imbalance of the dense computation outweighs the benefits in the sparse computation.
Reference: [2] <author> A.J.C. Bik and H.A.G. Wijshoff, </author> <title> Automatic Data Structure Selection and Transformation for Sparse Matrix Computations. </title> <journal> IEEE TPDS, </journal> <note> 1996 (to appear). </note>
Reference-contexts: The compiler then identifies redundant preprocessing and eliminates it. Our research is complementary to theirs by using distributions and representations that eliminate some of the levels of indirection we make the compiler's task easier. Another approach for parallelizing sparse codes is that followed by Bick and Wijshoff <ref> [2] </ref>, who have implemented a restructuring compiler which automatically transforms programs operating on dense 2-dimensional matrices into codes that operate on sparse storage schemes.
Reference: [3] <author> M. J. Berger and S.H. Bokhari, </author> <title> A Partitioning Strategy for Nonuniform Problems on Multiprocessors, </title> <journal> IEEE Trans. Comput., </journal> <volume> vol. 36, no. 5, </volume> <pages> pp. 570-580, </pages> <year> 1987. </year>
Reference-contexts: However, they do not consider the preprocessing costs required to locate distributed data and the memory overheads required to store the mapping. They are also expensive to compute and they all consider only row-wise decompositions of the sparse-matrix. The BRD partitioning method <ref> [3] </ref> does allow row as well as column decomposition of the sparse matrix. The MRD distribution is a generalization of BRD, and is oriented towards reducing lookup costs and memory overheads.
Reference: [4] <author> W.J. Camp, S.J. Plimpton, B.A. Hendrickson and R.W. Le-land. </author> <title> Massively Parallel Methods for Engineering and Science Problems, </title> <journal> Comms. of The ACM. Vol.37, </journal> <volume> No.4, </volume> <pages> pp 31-41. </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Neither the CHAOS partitioner nor MRD/BRS precludes a particular alignment choice the performance of a particular choice depends on the characteristics of the input matrix. 8 Related Work Much research has been done in sparse matrix distribution. Most of these techniques are derived from unstructured graph partitioning. <ref> [4] </ref> outlines three basic approaches of graph partitioning spectral bisection, graph bisection and coordinate bisection. All three methods result in good locality and load balance. However, they do not consider the preprocessing costs required to locate distributed data and the memory overheads required to store the mapping.
Reference: [5] <author> R. Das, J. Saltz and R. von Hanxleden. </author> <title> Slicing Analysis and Indirect Access to Distributed Arrays, </title> <booktitle> Proc. 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> Aug </month> <year> 1993, </year> <pages> pp 152-168. </pages>
Reference: [6] <author> R. Mirchandaney, J. Saltz, R.M. Smith, D.M. Nicol and K. Crowley. </author> <title> Principles of run-time support for parallel processors. </title> <booktitle> Proc. 1988 ACM International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <month> July, </month> <year> 1988. </year>
Reference: [7] <author> L.F. Romero and E.L. Zapata, </author> <title> Data distributions for sparse matrix vector multiplication solvers, </title> <journal> Parallel Computing, </journal> <volume> Vol. 21, no. 4, </volume> <pages> pp. 583-605, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Thus, processors may be assigned regions of unequal sizes but the indexes of each region can be described in a regular manner. There are many distributions that satisfy this condition. Two of these are: * Multiple Recursive Decomposition (MRD) <ref> [7] </ref> recursively decomposes the sparse matrix over P processors using horizontal and vertical partitions, until the matrix has been decomposed into P 1 fi P 2 rectangular submatrices. <p> At each stage of the partitioning process, the non-zeros in the submatrix of that stage are divided as evenly as possible (see Figure 2). * Block Row Scatter (BRS) <ref> [7] </ref> uses a cyclic mapping of the matrix among P processors. The matrix is subdivided using a stencil of size P 1 , and each processor gets the non-zero elements matching its position in the stencil (see Figure 3).
Reference: [8] <author> J. Saltz, R. Das, B. Moon, S. D. Sharma, Y. Hwang, R. Ponnusamy, M. </author> <title> Uysal A Manual for the CHAOS Runtime Library, </title> <institution> Computer Science Department, Univ. of Maryland, </institution> <month> Dec. 22, </month> <year> 1993. </year>
Reference-contexts: The percentage is the memory overhead relative to the size of the local submatrix. A 4 fi 4 processor stencil was used for both distributions. 7 Comparison with CHAOS CHAOS <ref> [8] </ref>, is a library that embodies the standard inspector-executor mechanisms for handling general indirect references as well as incorporating a number of well-known irregular partitioners. We implemented a parallel version of IterSolve using the CHAOS library to provide a baseline with which to compare our compiler and distributions.
Reference: [9] <author> S. D. Sharma, R. Ponnusamy, B. Moon, Y. Hwang, R. Das and J. Saltz. </author> <title> Run-time and Compile-time Support for Adaptive Irregular Problems, Procs. </title> <booktitle> Supercomputing '94, </booktitle> <month> Nov. </month> <year> 1994, </year> <pages> pp. 97-106. </pages>
Reference: [10] <author> M. Ujaldon and E.L. Zapata, </author> <title> Efficient Resolution of Sparse Indirections in Data-Parallel Compilers. Procs. </title> <booktitle> 9 th ACM Int'l Conference on Supercomputing, </booktitle> <month> July </month> <year> 1995, </year> <month> pp.117-126. </month>
Reference-contexts: This paper focuses on experimental evaluation of the run-time benefits of the proposed distribution techniques. Readers interested in related compiler and language issues are encouraged to read our earlier papers <ref> [11, 10] </ref> in which we discuss how these decompositions can be incorporated into data-parallel languages (Vienna Fortran in particular). 2 Data Distributions 2.1 Terminology A matrix is called sparse if only a small number of its elements are non-zero.
Reference: [11] <author> M. Ujaldon, S. D. Sharma, J. Saltz and E.L. Zapata. </author> <title> Run-time techniques for parallelizing sparse matrix problems. Procs. </title> <address> IRREGULAR'95, </address> <month> September, </month> <year> 1995. </year> <note> Printed by Springer-Verlag as LNCS, vol. </note> <month> 980, </month> <pages> pp. 43-57. </pages>
Reference-contexts: This paper focuses on experimental evaluation of the run-time benefits of the proposed distribution techniques. Readers interested in related compiler and language issues are encouraged to read our earlier papers <ref> [11, 10] </ref> in which we discuss how these decompositions can be incorporated into data-parallel languages (Vienna Fortran in particular). 2 Data Distributions 2.1 Terminology A matrix is called sparse if only a small number of its elements are non-zero.
References-found: 11

