URL: http://www.cs.columbia.edu:80/~sal/hpapers/kdd96.ps
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: pkc@cs.fit.edu  sal@cs.columbia.edu  
Title: Sharing Learned Models among Remote Database Partitions by Local Meta-learning  
Author: Philip K. Chan Salvatore J. Stolfo 
Address: Melbourne, FL 32901  New York, NY 10027  
Affiliation: Computer Science Florida Institute of Technology  Department of Computer Science Columbia University  
Abstract: We explore the possibility of importing "black-box" models learned over data sources at remote sites to improve models learned over locally available data sources. In this way, we may be able to learn more accurate knowledge from globally available data than would otherwise be possible from partial, locally available data. Proposed meta-learning strategies in our previous work are extended to integrate local and remote models. We also investigate the effect on accuracy performance when data overlap among different sites. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Boswell, R. </author> <year> 1990. </year> <note> Manual for CN2 version 6.1. Turing Institure. Int. Doc. IND: TI/MLT/4.0T/RAB/1.2. </note>
Reference-contexts: ID3 and CART are decisions tree learning algorithms and were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana 1991). BAYES is a simple Bayesian learning algorithm. CN2 is a rule learning algorithm and was obtained from Dr. Clark <ref> (Boswell 1990) </ref>. Four data sets were used in our studies. The DNA splice junction (SJ) data set (courtesy of Towell, Shav-lik, and Noordewier (Towell, Shavlik, & Noordewier 1990)) contains sequences of nucleotides and the type of splice junction, if any, at the center of each sequence.
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: Now, during meta-learning, remote classifiers will not be automatically ignored since the local classifier is also judged on "unseen" data. The next section discusses our experimental evaluation of the local meta-learning approach. Experimental Results Four inductive learning algorithms were used in our experiments reported here: ID3 (Quinlan 1986), CART <ref> (Breiman et al. 1984) </ref>, BAYES (described in (Clark & Niblett 1989)), and CN2 (Clark & Niblett 1989). ID3 and CART are decisions tree learning algorithms and were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana 1991). BAYES is a simple Bayesian learning algorithm.
Reference: <author> Buntine, W., and Caruana, R. </author> <year> 1991. </year> <title> Introduction to IND and Recursive Partitioning. </title> <institution> NASA Ames Research Center. </institution>
Reference-contexts: ID3 and CART are decisions tree learning algorithms and were obtained from NASA Ames Research Center in the IND package <ref> (Buntine & Caruana 1991) </ref>. BAYES is a simple Bayesian learning algorithm. CN2 is a rule learning algorithm and was obtained from Dr. Clark (Boswell 1990). Four data sets were used in our studies.
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1993. </year> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. on Multistrategy Learning, </booktitle> <pages> 150-165. </pages>
Reference-contexts: The next question is how we can merge the black boxes. We adopted the general approach of meta-learning <ref> (Chan & Stolfo 1993) </ref> and developed techniques for coalescing multiple learned models. During meta-learning, the learned models are treated as black boxes so that they can use any representation and can be generated by any inductive learning algorithm. That is, our meta-learning techniques are representation- and algorithm-independent. <p> We then empirically evaluate local meta-learning and the effect of data replication on performance. Meta-learning Given a number of classifiers and their predictions for a particular unlabeled instance, one may combine them by picking the prediction with the largest number of votes. Our approach introduced in <ref> (Chan & Stolfo 1993) </ref> is to meta-learn a set of new classifiers (or meta-classifiers) whose training data are based on predictions of a set of underlying base classifiers. Re sults from (Chan & Stolfo 1995) show that our meta-learning techniques are more effective than voting-based methods.
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1995. </year> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proc. Twelfth Intl. Conf. Machine Learning, </booktitle> <pages> 90-98. </pages>
Reference-contexts: Our approach introduced in (Chan & Stolfo 1993) is to meta-learn a set of new classifiers (or meta-classifiers) whose training data are based on predictions of a set of underlying base classifiers. Re sults from <ref> (Chan & Stolfo 1995) </ref> show that our meta-learning techniques are more effective than voting-based methods. Our techniques fall into two general categories: the arbiter and combiner schemes. We distinguish between base classifiers and arbiters/combiners as follows. <p> This arbiter, together with an arbitration rule, decides a final classification outcome based upon the base predictions. The arbiter is trained from examples that do not have a common prediction among the majority of the base classifiers. More details of this arbiter scheme are in <ref> (Chan & Stolfo 1995) </ref>. The aim of the combiner strategy is to coalesce the predictions from the base classifiers by learning the relationship between these predictions and the correct prediction. <p> The correct classification and predictions from the base classifiers constitute a training example in the class-combiner scheme. Attributes of the original example is added in the class-attr-combiner scheme. The details of these two schemes appear in <ref> (Chan & Stolfo 1995) </ref>. From these examples, the meta-learner generates a meta-classifier, that we call a combiner. In classifying an instance, the base classifiers first generate their predictions. Based on the same composition rule, a new instance is generated from the predictions, which is then classified by the combiner. <p> We note that a combiner computes a prediction that may be entirely different from any proposed by a base classifier, whereas an arbiter chooses one of the predictions from the base classifiers and the arbiter itself. Local Meta-learning Our previous work <ref> (Chan & Stolfo 1995) </ref> assumes a certain degree of "raw" data sharing. As we discussed earlier, situations might arise when data sharing is not feasible, but sharing of "black-box" learned models is possible. <p> The arbiter strategy maintains the accuracy in 8 out of 16 cases. For the Coding Regions data set, the arbiter strategy improves local learning by a wide margin using 3 of the 4 learners. The results obtained here are consistent with those from non-local meta-learning <ref> (Chan & Stolfo 1995) </ref>, where raw data can be shared among sites. Meta-learning improves accuracy in a distributed environ ment and the arbiter strategy is more effective than the two combiner techniques.
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1996. </year> <title> Scaling learning by meta-learning over disjoint and partially replicated data. </title> <note> In Proc. Florida AI Research Symposium. To appear. </note>
Reference-contexts: The different strategies were run on the above four data sets, each with the above four learning algorithms and the results are plotted in Figure 2. Due to space limitations, only results from two data sets are shown; the rest appears in <ref> (Chan 1996) </ref>. The plotted accuracy is the average accuracy of local meta-classifiers over 10-fold cross-validation runs. In each run, m sites generate m local classifiers and m local meta-classifiers, after "exchanging" all local classifiers. <p> The same experimental setup was used as in the prior experiments. Results for the replicated data scenario using the class-combiner and class-attr-combiner strategies are plotted in Figure 3. Due to space limitations, only 8 of the 32 cases are shown, the rest appears in <ref> (Chan 1996) </ref>. 7 out of 32 cases show significant accuracy improvement when the degree of replication increases; 6 of these 7 cases occur in the Coding Regions data set. 20 out of 32 cases show no significant accuracy changes across all subset sizes and degrees of replication. <p> That could imply that local meta-learning is quite effective in integrating models from remote sites without the help of replicated data. Our findings here are consistent with those from non-local meta-learning <ref> (Chan & Stolfo 1996) </ref>. Concluding Remarks We have presented techniques for improving local learning by integrating remote classifiers through local meta-learning. Our experimental results suggest local meta-learning techniques, especially the arbiter strategy, can significantly raise the accuracy of the local classifiers.
Reference: <author> Chan, P. </author> <year> 1996. </year> <title> An Extensible Meta-Learning Approach for Scalable and Accurate Inductive Learning. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY. </address> <publisher> (forthcoming). </publisher>
Reference-contexts: The different strategies were run on the above four data sets, each with the above four learning algorithms and the results are plotted in Figure 2. Due to space limitations, only results from two data sets are shown; the rest appears in <ref> (Chan 1996) </ref>. The plotted accuracy is the average accuracy of local meta-classifiers over 10-fold cross-validation runs. In each run, m sites generate m local classifiers and m local meta-classifiers, after "exchanging" all local classifiers. <p> The same experimental setup was used as in the prior experiments. Results for the replicated data scenario using the class-combiner and class-attr-combiner strategies are plotted in Figure 3. Due to space limitations, only 8 of the 32 cases are shown, the rest appears in <ref> (Chan 1996) </ref>. 7 out of 32 cases show significant accuracy improvement when the degree of replication increases; 6 of these 7 cases occur in the Coding Regions data set. 20 out of 32 cases show no significant accuracy changes across all subset sizes and degrees of replication. <p> That could imply that local meta-learning is quite effective in integrating models from remote sites without the help of replicated data. Our findings here are consistent with those from non-local meta-learning <ref> (Chan & Stolfo 1996) </ref>. Concluding Remarks We have presented techniques for improving local learning by integrating remote classifiers through local meta-learning. Our experimental results suggest local meta-learning techniques, especially the arbiter strategy, can significantly raise the accuracy of the local classifiers.
Reference: <author> Clark, P., and Niblett, T. </author> <year> 1989. </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 261-285. </pages>
Reference-contexts: The next section discusses our experimental evaluation of the local meta-learning approach. Experimental Results Four inductive learning algorithms were used in our experiments reported here: ID3 (Quinlan 1986), CART (Breiman et al. 1984), BAYES (described in <ref> (Clark & Niblett 1989) </ref>), and CN2 (Clark & Niblett 1989). ID3 and CART are decisions tree learning algorithms and were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana 1991). BAYES is a simple Bayesian learning algorithm. <p> The next section discusses our experimental evaluation of the local meta-learning approach. Experimental Results Four inductive learning algorithms were used in our experiments reported here: ID3 (Quinlan 1986), CART (Breiman et al. 1984), BAYES (described in <ref> (Clark & Niblett 1989) </ref>), and CN2 (Clark & Niblett 1989). ID3 and CART are decisions tree learning algorithms and were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana 1991). BAYES is a simple Bayesian learning algorithm. CN2 is a rule learning algorithm and was obtained from Dr. Clark (Boswell 1990).
Reference: <author> Craven, M., and Shavlik, J. </author> <year> 1993. </year> <title> Learning to represent codons: A challenge problem for constructive induction. </title> <booktitle> In Proc. IJCAI-93, </booktitle> <pages> 1319-1324. </pages>
Reference-contexts: Exon-intron, intron-exon, and non-junction are the three classes in this task. Each sequence has 60 nucleotides with eight different values per nucleotide (four base ones plus four combinations). The data set contains 3,190 training instances. The protein coding region (PCR) data set (courtesy of Craven and Shav-lik <ref> (Craven & Shavlik 1993) </ref>) contains DNA nucleotide sequences and their binary classifications (coding or non-coding). Each sequence has 15 nucleotides with four different values per nucleotide. The PCR data set has 20,000 sequences.
Reference: <author> Qian, N., and Sejnowski, T. </author> <year> 1988. </year> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> J. Mol. Biol. </journal> <volume> 202 </volume> <pages> 865-884. </pages>
Reference-contexts: Each sequence has 15 nucleotides with four different values per nucleotide. The PCR data set has 20,000 sequences. The secondary protein structure data set (SS) <ref> (Qian & Sejnowski 1988) </ref>, courtesy of Qian and Sejnowski, contains sequences of amino acids and the secondary structures at the corresponding positions. There are three structures (alpha-helix, beta-sheet, and coil) and 20 amino acids (21 attributes, including a spacer (Qian & Sejnowski 1988)) in the data. <p> The secondary protein structure data set (SS) <ref> (Qian & Sejnowski 1988) </ref>, courtesy of Qian and Sejnowski, contains sequences of amino acids and the secondary structures at the corresponding positions. There are three structures (alpha-helix, beta-sheet, and coil) and 20 amino acids (21 attributes, including a spacer (Qian & Sejnowski 1988)) in the data. The amino acid sequences were split into shorter sequences of length 13 according to a windowing technique used in (Qian & Sejnowski 1988). The SS data set has 21,625 sequences. <p> There are three structures (alpha-helix, beta-sheet, and coil) and 20 amino acids (21 attributes, including a spacer <ref> (Qian & Sejnowski 1988) </ref>) in the data. The amino acid sequences were split into shorter sequences of length 13 according to a windowing technique used in (Qian & Sejnowski 1988). The SS data set has 21,625 sequences. The artificial (ART) data set has 10,000 instances randomly generated from a disjunctive boolean expression that has 4 symbolic (26 values) and 4 numeric (1,000 values) variables. A total of 4:6 fi 10 17 instances are possible.
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages>
Reference-contexts: Now, during meta-learning, remote classifiers will not be automatically ignored since the local classifier is also judged on "unseen" data. The next section discusses our experimental evaluation of the local meta-learning approach. Experimental Results Four inductive learning algorithms were used in our experiments reported here: ID3 <ref> (Quinlan 1986) </ref>, CART (Breiman et al. 1984), BAYES (described in (Clark & Niblett 1989)), and CN2 (Clark & Niblett 1989). ID3 and CART are decisions tree learning algorithms and were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana 1991).
Reference: <author> Towell, G.; Shavlik, J.; and Noordewier, M. </author> <year> 1990. </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proc. AAAI-90, </booktitle> <pages> 861-866. </pages> <note> from 0% to 40%. </note>
Reference-contexts: BAYES is a simple Bayesian learning algorithm. CN2 is a rule learning algorithm and was obtained from Dr. Clark (Boswell 1990). Four data sets were used in our studies. The DNA splice junction (SJ) data set (courtesy of Towell, Shav-lik, and Noordewier <ref> (Towell, Shavlik, & Noordewier 1990) </ref>) contains sequences of nucleotides and the type of splice junction, if any, at the center of each sequence. Exon-intron, intron-exon, and non-junction are the three classes in this task.
References-found: 12

