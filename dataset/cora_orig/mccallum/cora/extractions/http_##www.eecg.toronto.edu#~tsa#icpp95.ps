URL: http://www.eecg.toronto.edu/~tsa/icpp95.ps
Refering-URL: http://wwwipd.ira.uka.de/~hopp/seminar97.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: fnmanjiki,tsag@eecg.toronto.edu  
Title: Fusion of Loops for Parallelism and Locality  
Author: Naraig Manjikian and Tarek S. Abdelrahman 
Address: Toronto, Ontario, Canada M5S 1A4  
Affiliation: Department of Electrical and Computer Engineering The University of Toronto  
Abstract: Loop fusion improves data locality and reduces synchronization in data-parallel applications. However, loop fusion is not always legal. Even when legal, fusion may introduce loop-carried dependences which reduce parallelism. In addition, performance losses result from cache conflicts in fused loops. We present new, systematic techniques which: (1) allow fusion of loop nests in the presence of fusion-preventing dependences, (2) allow parallel execution of fused loops with minimal synchronization, and (3) eliminate cache conflicts in fused loops. We evaluate our techniques on a 56-processor KSR2 multiprocessor, and show improvements of up to 20% for representative loop nest sequences. The results also indicate a performance tradeoff as more processors are used, suggesting careful evaluation of the profitability of fusion. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA., </address> <year> 1986. </year>
Reference-contexts: Interprocedural analysis can identify candidate loops for fusion-enabling transformations such as loop extraction and loop embedding [9]. Loop distribution [3] may be used to produce parallel loop nests that adhere to the model. Since the loops will be eventually fused together, there is no loss of locality. Code motion <ref> [1] </ref> may be employed to obtain a sequence of parallel loops with no intervening code.
Reference: [2] <author> W. Appelbe and K. Smith. </author> <title> Determining transformation sequences for loop parallelization. </title> <editor> In U. Banerjee et al., editors, </editor> <booktitle> Languages and Compilers for Parallel ComputingFifth International Workshop, </booktitle> <pages> pages 208-222. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: The shift-and-peel transformation can then be used to fuse the loops back together again such that serializing dependences are removed. This approach compares favorably with that of Callahan [4] and Appelbe and Smith <ref> [2] </ref> in that it does not require expensive replication of code and in that it uses algorithms with lower complexity. In this section, peeling has been described for one-dimensional loop nests. It should be emphasized that peeling is applicable for multidimensional loop nests. <p> These observations suggest using knowledge of data sizes and cache sizes at compile-time to determine the profitability of fusion. The final set of results compares our peeling transformation with the alignment and replication transformation proposed by Callahan [4] and Appelbe and Smith <ref> [2] </ref> (to be discussed in section on Related Work). Figure 15 compares the performance of the fused LL18 loop nest parallelized using peeling with the performance of the same loop nest parallelized using direct application of alignment and replication. <p> However, no systematic method is described for fusion of multiple loop nests, nor is the parallelization of the fused loop nest considered. Appelbe and Smith <ref> [2] </ref> present a graph-based algorithm for deriving the required alignment, replication, and reordering to permit parallelization of an individual loop nest with forward dependences. Their approach is similar to Callahan's approach in that it incurs overhead due to redundant execution of replicated code. <p> The adjustments needed for shifting and peeling are derived for multiple loops using a simple graph-based framework. The techniques are simpler and more efficient than the alignment and replication techniques of both Callahan [4] and Appelbe and Smith <ref> [2] </ref>. The transformation algorithms for both shifting and peeling are linear in the number of loops being fused and can be easily automated in a compiler.
Reference: [3] <author> D. F. Bacon, S. L. Graham, and O. J. Sharp. </author> <title> Compiler transformations for high-performance computing. </title> <type> Tech. Rep. </type> <institution> UCB/CSD-93-781, Computer Science Division, University of California, Berkeley, </institution> <year> 1993. </year>
Reference-contexts: Nonetheless, compatible array accesses are typical in many scientific applications. Program transformations may be used to obtain a program segment that lies in the domain of our techniques. Interprocedural analysis can identify candidate loops for fusion-enabling transformations such as loop extraction and loop embedding [9]. Loop distribution <ref> [3] </ref> may be used to produce parallel loop nests that adhere to the model. Since the loops will be eventually fused together, there is no loss of locality. Code motion [1] may be employed to obtain a sequence of parallel loops with no intervening code. <p> Since the loops will be eventually fused together, there is no loss of locality. Code motion [1] may be employed to obtain a sequence of parallel loops with no intervening code. Loop and/or array dimension permutation <ref> [3, 7] </ref> may be used to ensure compatible access patterns. 2 Loop Fusion Fusion of loops from adjacent loop nests combines their respective loop bodies into a single body and collapses their respective iteration spaces into one combined space. <p> Register locality is enhanced through reuse of register-allocated array values in a single iteration of the fused iteration space [6]. Cache locality is enhanced through reuse of array elements across multiple iterations of the fused iteration space <ref> [3] </ref>. In either case, exploiting the reuse avoids costly references to main memory, thereby improving performance. In addition, fusion permits a reduction in the number of barrier synchronizations needed between parallel loops. Fusion is not always legal. <p> Nonetheless, guards may still be required if the original iteration spaces differ. A simple alternative to the direct approach is to strip-mine <ref> [3] </ref> the original loops by a factor of s, then fuse the resulting outer controlling loops to interleave iterations in groups of s, as shown in Figure 6 (b). <p> A common solution to this problem is to pad array dimensions to perturb the mapping of data into the cache and reduce the occurrence of conflicts <ref> [3] </ref>. However, it is difficult to predict the amount of padding which minimizes the number of conflicts, particularly when the number of arrays is large.
Reference: [4] <author> C. D. Callahan. </author> <title> A Global Approach to the Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: For example, Figure 2 illustrates how the fusion of two loops may result in backward dependences. We propose a simple technique to enable legal fusion of multiple loops in the presence of backward loop-carried dependences, based on the alignment techniques described in <ref> [4, 11] </ref>. The only necessary condition for this technique is uniform dependences. The key idea is to make backward dependences loop-independent in the fused loop by shifting the iteration space containing the sink of the dependences with respect to the iteration space containing the source of the dependence. <p> The shift-and-peel transformation can then be used to fuse the loops back together again such that serializing dependences are removed. This approach compares favorably with that of Callahan <ref> [4] </ref> and Appelbe and Smith [2] in that it does not require expensive replication of code and in that it uses algorithms with lower complexity. In this section, peeling has been described for one-dimensional loop nests. It should be emphasized that peeling is applicable for multidimensional loop nests. <p> These observations suggest using knowledge of data sizes and cache sizes at compile-time to determine the profitability of fusion. The final set of results compares our peeling transformation with the alignment and replication transformation proposed by Callahan <ref> [4] </ref> and Appelbe and Smith [2] (to be discussed in section on Related Work). Figure 15 compares the performance of the fused LL18 loop nest parallelized using peeling with the performance of the same loop nest parallelized using direct application of alignment and replication. <p> However, fusion is not permitted with loop-carried dependences or incompatible loop bounds. Callahan <ref> [4] </ref> proposes loop alignment to allow synchronization-free parallel execution of a loop nest, and uses code replication to resolve conflicts in alignment requirements. <p> The adjustments needed for shifting and peeling are derived for multiple loops using a simple graph-based framework. The techniques are simpler and more efficient than the alignment and replication techniques of both Callahan <ref> [4] </ref> and Appelbe and Smith [2]. The transformation algorithms for both shifting and peeling are linear in the number of loops being fused and can be easily automated in a compiler.
Reference: [5] <author> Kendall Square Research. </author> <title> KSR1 Principles of operation. </title> <address> Waltham, Mass., </address> <year> 1991. </year>
Reference-contexts: However, these gaps enable a predictable reduction in the number of misses, unlike the unpredictable outcome of padding. 5 Experimental Results Results of experiments conducted on a Kendall Square Research KSR2 multiprocessor system <ref> [5] </ref> using two representative loop nest sequences are presented to illustrate the performance advantages that can be obtained from our techniques. From the Livermore Loops, Kernel 18 (LL18) is considered, which is an excerpt of three loop nests from a hydrodynamics code.
Reference: [6] <author> K. Kennedy and K. S. McKinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <editor> In U. Banerjee et al., editors, </editor> <booktitle> Languages and Compilers for Parallel ComputingSixth International Workshop, </booktitle> <pages> pages 301-320. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1994. </year>
Reference-contexts: We do not address the loss of parallelism in any of the original loop nests with more than k parallel loops. This issue has been addressed in previous work <ref> [6] </ref>. Our data transformation technique requires in addition that array accesses be compatible, i.e, accesses to a given array must have the same stride and direction across all loop nests. Although compatible accesses imply uniform dependences, the reverse is not necessarily true, hence compatibility is a stricter requirement. <p> In so doing, the number of iterations separating references to the same array is reduced, and array reuse can then be exploited to enhance register and cache locality. Register locality is enhanced through reuse of register-allocated array values in a single iteration of the fused iteration space <ref> [6] </ref>. Cache locality is enhanced through reuse of array elements across multiple iterations of the fused iteration space [3]. In either case, exploiting the reuse avoids costly references to main memory, thereby improving performance. In addition, fusion permits a reduction in the number of barrier synchronizations needed between parallel loops. <p> Appelbe and Smith [2] present a graph-based algorithm for deriving the required alignment, replication, and reordering to permit parallelization of an individual loop nest with forward dependences. Their approach is similar to Callahan's approach in that it incurs overhead due to redundant execution of replicated code. Kennedy and McKinley <ref> [6] </ref> use loop fusion and distribution to enhance locality and maximize parallelism. They focus on register reuse, and describe a fusion algorithm for avoiding the fusion of parallel loops with serial loops.
Reference: [7] <author> N. Manjikian and T. Abdelrahman. </author> <title> Reduction of cache conflicts in loop nests. </title> <type> Tech. Rep. </type> <institution> CSRI-318, Computer Systems Research Institute, University of Toronto, </institution> <address> Ontario, Canada, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Since the loops will be eventually fused together, there is no loss of locality. Code motion [1] may be employed to obtain a sequence of parallel loops with no intervening code. Loop and/or array dimension permutation <ref> [3, 7] </ref> may be used to ensure compatible access patterns. 2 Loop Fusion Fusion of loops from adjacent loop nests combines their respective loop bodies into a single body and collapses their respective iteration spaces into one combined space. <p> Cache partitioning directly minimizes the number of misses and prevents erratic cache behavior. In this paper, we limit our presentation to one-dimensional partitioning; higher-dimensional partitioning is described in <ref> [7] </ref>. In one-dimensional partitioning, the data in each partition is made contiguous by limiting the number of indices from only the outermost array dimension to make the data from an array fit in a partition.
Reference: [8] <author> J. McCalpin. Quasigeostrophic box model-revision 2.3. </author> <type> Technical report, </type> <institution> College of Marine Studies, University of Delaware, </institution> <year> 1992. </year>
Reference-contexts: From the Livermore Loops, Kernel 18 (LL18) is considered, which is an excerpt of three loop nests from a hydrodynamics code. These loop nests cannot be fused directly, as backward loop-carried dependences result. From the qgbox ocean modelling code <ref> [8] </ref>, a sequence of five loop nests is considered from the calc subroutine. These loop nests also cannot be fused directly due to backward loop-carried dependences. In addition, differences between the iteration spaces prevent direct fusion; one of the loops has a larger iteration space than the other four.
Reference: [9] <author> K. S. McKinley. </author> <title> Automatic and Interactive Paral-lelization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Nonetheless, compatible array accesses are typical in many scientific applications. Program transformations may be used to obtain a program segment that lies in the domain of our techniques. Interprocedural analysis can identify candidate loops for fusion-enabling transformations such as loop extraction and loop embedding <ref> [9] </ref>. Loop distribution [3] may be used to produce parallel loop nests that adhere to the model. Since the loops will be eventually fused together, there is no loss of locality. Code motion [1] may be employed to obtain a sequence of parallel loops with no intervening code.
Reference: [10] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Comm. ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Fusion places the source and sink iterations of each dependence in the same iteration space. Fusion is legal only if it does not result in a loop-carried dependence that flows backwards with respect to the iteration execution order <ref> [10, 14] </ref>. For example, Figure 2 illustrates how the fusion of two loops may result in backward dependences. We propose a simple technique to enable legal fusion of multiple loops in the presence of backward loop-carried dependences, based on the alignment techniques described in [4, 11].
Reference: [11] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: For example, Figure 2 illustrates how the fusion of two loops may result in backward dependences. We propose a simple technique to enable legal fusion of multiple loops in the presence of backward loop-carried dependences, based on the alignment techniques described in <ref> [4, 11] </ref>. The only necessary condition for this technique is uniform dependences. The key idea is to make backward dependences loop-independent in the fused loop by shifting the iteration space containing the sink of the dependences with respect to the iteration space containing the source of the dependence. <p> His approach potentially results in exponential growth in the number of statements in the loop, and can result in significant overhead because of the redundant execution of replicated code. Porterfield <ref> [11] </ref> suggests a peel-and-jam transformation in which iterations are peeled from the beginning or end of one loop nest to allow fusion with another loop nest. However, no systematic method is described for fusion of multiple loop nests, nor is the parallelization of the fused loop nest considered.
Reference: [12] <author> R. H. Saavedra, R. S. Gaines, and M. J. Carlton. </author> <title> Micro benchmark analysis of the KSR1. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 202-213, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: In our experiments, cache partitioning is applied assuming a direct-mapped cache. The KSR data cache is 2-way set-associative, but maintains only one address tag for each set of 32 contiguous cache lines and employs a random replacement policy for these sets <ref> [12] </ref>. The associativity permits two distinct addresses to map to the same cache location without incurring conflict misses. However, random replacement ejects 32 cache lines at a time. Hence, the benefit of the associativity is limited, and we opt to consider the cache as direct-mapped.
Reference: [13] <author> J. Warren. </author> <title> A hierarchical basis for reordering transformations. </title> <booktitle> In Proc. 11th ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 272-282, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: In addition, the relative cost of synchronization increases with faster processors. As a result, the benefit of reducing the number of barrier synchronizations, in conjunction with locality enhancement, will increase. 6 Related Work Warren <ref> [13] </ref> presents an algorithm for incrementally adding candidate loops to a fusible set to enhance vector register reuse, and to permit contraction of temporary arrays into scalars. However, fusion is not permitted with loop-carried dependences or incompatible loop bounds.
Reference: [14] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA., </address> <year> 1989. </year>
Reference-contexts: Fusion places the source and sink iterations of each dependence in the same iteration space. Fusion is legal only if it does not result in a loop-carried dependence that flows backwards with respect to the iteration execution order <ref> [10, 14] </ref>. For example, Figure 2 illustrates how the fusion of two loops may result in backward dependences. We propose a simple technique to enable legal fusion of multiple loops in the presence of backward loop-carried dependences, based on the alignment techniques described in [4, 11].
References-found: 14

