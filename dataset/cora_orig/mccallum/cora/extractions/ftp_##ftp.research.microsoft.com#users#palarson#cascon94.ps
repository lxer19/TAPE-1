URL: ftp://ftp.research.microsoft.com/users/palarson/cascon94.ps
Refering-URL: http://www.research.microsoft.com/~palarson/publications.htm
Root-URL: http://www.research.microsoft.com
Title: Data Reduction Through Early Grouping  
Author: W. Paul Yan and Paul Larson 
Date: November 1994.  
Note: Appears in Proceedings of the 1994 IBM CAS Conference, pages 227-235,  
Address: Waterloo, Ontario, N2L 3G1, Canada  Toronto, Ontario,  
Affiliation: Department of Computer Science University of Waterloo  
Abstract: SQL queries containing GROUP BY and aggregation occur frequently in decision support applications. Grouping with aggregation is typically done by first sorting the input and then performing the aggregation as part of the output phase of the sort. The most widely used external sorting algorithm is merge sort, consisting of a run formation phase followed by a (single) merge pass. The amount of data output from the run formation phase can be reduced by a technique that we call early grouping. The idea is straightforward: simply form groups and perform aggregation during run formation. Each run will now consist of partial groups instead of individual records. These partial groups are then combined during the merge phase. Early grouping always reduces the number of records output from the run formation phase. The relative output size depends on the amount of memory relative to the total number of groups and the distribution of records over groups. When the input data is uniformly distributed | the worst case | our simulation results show that the relative output size is proportional to the (relative) amount of memory used. When the data is skewed | the more common case in practice | the relative output size is much smaller. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Abdelguerfi and Arun K. Sood. </author> <title> Computational complexity of sorting and joining relations with duplicates. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 3(4) </volume> <pages> 497-503, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Munro and Spira [6] gave a computational bound for the number of comparisons required to sort a multiset with early duplicate removal. Several algorithms, based on various sorting algorithms, e.g., quick sort, hash sort and merge sort, have been proposed for duplicate elimination. Abdelguerfi and Sood <ref> [1] </ref> gave the computational complexity of the merge sort method based on the number of three branch comparisons; Teuhola and Wegner [7] gave a duplicate elimination algorithm based on hashing with early duplicate removal, which requires linear time on the average and O (1) extra space; and Wegner [8] gave a
Reference: [2] <author> D. Bitton and D. J. Dewitt. </author> <title> Duplicate record elimination in large data files. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(2):255, </volume> <month> June </month> <year> 1983. </year>
Reference-contexts: This is the worst case. When the groups are of different size the more common case in practice the reduction factor is much higher. The idea of early grouping is not new. Bitton and Dewitt <ref> [2] </ref> proposed and analyzed early duplicate removal during run formation, but we have not found any published results on early grouping. However, some database systems implement early grouping and duplicate removal. DB2/MVS 1 performs early grouping, but not in all cases. <p> The idea of early dupli 1 DB2/MVS is a registered trademark of IBM. 2 Oracle is a registered trademark of Oracle. cate removal during run formation phase in external sorting seems to have been investigated first by D. Bitton and D.J. Dewitt <ref> [2] </ref>. Their duplicate elimination algorithm exploits early duplicate removal during both run formation and run merging. They also analyzed the performance of the algorithm under the simplifying assumption that the external merge procedure starts with internally sorted pages (i.e., initial runs) free of duplicates.
Reference: [3] <author> Goetz Graefe. </author> <title> Query evaluation techniques for large databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Their cost analysis is based on (a) the number of multi-level merge phases and (b) the average size of runs in each phase for a two-way merge sort. This analysis is also summarized in <ref> [3] </ref>. 3 Early Grouping In most database systems, evaluation of a GROUP BY query relies on sorting. It first sorts the records by the grouping columns, which brings the records belonging to the same group together, and then performs the accumulation specified on the query.
Reference: [4] <author> Donald E. Knuth. </author> <booktitle> The art of computer programming, volume 3. </booktitle> <address> Reading, Mas-sachusetts, </address> <year> 1973. </year>
Reference-contexts: This process continues until all input records have been scanned. Variable-length run formation algorithms may produce runs that are larger than the available memory. The standard algorithm is replacement selection <ref> [4] </ref>. Replacement Selection The basic idea of replacement selection is as follows. In main memory, exactly two runs are maintained, represented by selection trees (heaps). One run is the current run and the other is the next run. <p> When the input is already sorted in correct order, only one run will be produced. This is the best case. E. F. Moore showed that, for randomly ordered input, the expected length of each run is twice the available memory size <ref> [4] </ref>. When the input exhibits some level of pre-sortedness, runs are likely to be longer than twice the available memory. Adding Early Grouping to Re placement Selection When adding early grouping to run formation by replacement selection, we maintain in memory one record for each group encountered so far. <p> Distribution of Group Sizes Assume that the input records belong to N different groups. In our experiments, the groups were labeled 1; 2; :::; N . To model the group size distribution we chose a generalized, truncated Zipf <ref> [4] </ref> distribution. The distribution function is defined by: Z (x) = ((1=x) ff )=c; x = 1; 2; ::::; N where ff is a positive constant and c is a normalizing constant ensuring that the probabilities add up to one.
Reference: [5] <author> Per -Ake Larson. </author> <title> Linear hashing with separators adynamic hashing scheme achieving one-access retrieval. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 13(3) </volume> <pages> 366-388, </pages> <month> De-cember </month> <year> 1988. </year>
Reference-contexts: This operation can be implemented efficiently by maintaining a hash table on the sort keys. In our experiments we used a linear hash table, as described in <ref> [5] </ref>. The benefit of a linear hash table is that it can grow and shrink dynamically based on the size of input data. Hash Table Overhead If early grouping is not employed, no hash table is needed and no table lookup needs to be performed.
Reference: [6] <author> I. Munro and P.M. Spira. </author> <title> Sorting and searching in multisets. </title> <journal> SIAM Journal of Computing, </journal> <volume> 5(1), </volume> <month> March </month> <year> 1976. </year>
Reference-contexts: The results are not conclusive but it appears that Oracle V7 performs early grouping in some cases. It is highly likely that several other systems do as well. 2 Previous Work Most published work on duplicate elimination deals with main-memory algorithms. Munro and Spira <ref> [6] </ref> gave a computational bound for the number of comparisons required to sort a multiset with early duplicate removal. Several algorithms, based on various sorting algorithms, e.g., quick sort, hash sort and merge sort, have been proposed for duplicate elimination.
Reference: [7] <author> Jukka Teuhola and Lutz Wegner. </author> <title> Minimal space, average linear time duplicate deletion. </title> <journal> Communications of ACM, </journal> <volume> 34(3) </volume> <pages> 62-73, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Several algorithms, based on various sorting algorithms, e.g., quick sort, hash sort and merge sort, have been proposed for duplicate elimination. Abdelguerfi and Sood [1] gave the computational complexity of the merge sort method based on the number of three branch comparisons; Teuhola and Wegner <ref> [7] </ref> gave a duplicate elimination algorithm based on hashing with early duplicate removal, which requires linear time on the average and O (1) extra space; and Wegner [8] gave a quick sort algorithm for the run formation phase and analyzed its computational complexity (N logn, where N is the number of
Reference: [8] <author> L. Wegner. </author> <title> Quicksort for equal keys. </title> <journal> IEEE Computer, </journal> <volume> C-34(4):362-367, </volume> <month> April </month> <year> 1985. </year> <month> 235 </month>
Reference-contexts: and Sood [1] gave the computational complexity of the merge sort method based on the number of three branch comparisons; Teuhola and Wegner [7] gave a duplicate elimination algorithm based on hashing with early duplicate removal, which requires linear time on the average and O (1) extra space; and Wegner <ref> [8] </ref> gave a quick sort algorithm for the run formation phase and analyzed its computational complexity (N logn, where N is the number of rows and n is the number of distinct rows). However, we are mainly interested in large-scale grouping and aggregation requiring external sorting.
References-found: 8

