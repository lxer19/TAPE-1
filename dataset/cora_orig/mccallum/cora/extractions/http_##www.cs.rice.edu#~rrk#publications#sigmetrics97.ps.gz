URL: http://www.cs.rice.edu/~rrk/publications/sigmetrics97.ps.gz
Refering-URL: http://www.cs.rice.edu/~willy/TreadMarks/papers.html
Root-URL: 
Email: frrk,alcg@rice.edu  
Title: Performance Debugging Shared Memory Parallel Programs Using Run-Time Dependence Analysis  
Author: Ramakrishnan Rajamony and Alan L. Cox 
Address: Houston, TX 77251-1892  
Affiliation: Departments of Electrical Computer Engineering and Computer Science Rice University  
Abstract: We describe a new approach to performance debugging that focuses on automatically identifying computation transformations to reduce synchronization and communication. By grouping writes together into equivalence classes, we are able to tractably collect information from long-running programs. Our performance debugger analyzes this information and suggests computation transformations in terms of the source code. We present the transformations suggested by the debugger on a suite of four applications. For Barnes-Hut and Shallow, implementing the debugger suggestions improved the performance by a factor of 1.32 and 34 times respectively on an 8-processor IBM SP2. For Ocean, our debugger identified excess synchronization that did not have a significant impact on performance. ILINK, a genetic linkage analysis program widely used by geneticists, is already well optimized. We use it only to demonstrate the feasibility of our approach to long-running applications. We also give details on how our approach can be implemented. We use novel techniques to convert control dependences to data dependences, and to compute the source operands of stores. We report on the impact of our instrumentation on the same application suite we use for performance debugging. The instrumentation slows down the execution by a factor of between 4 and 169 times. The log files produced during execution were all less than 2.5 Mbytes in size. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.V. Adve and M.D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <month> June </month> <year> 1993. </year>
Reference: [2] <author> S. P. Amarasinghe, J. M. Anderson, M. S. Lam, and C. W. Tseng. </author> <title> The SUIF compiler for scalable parallel machines. </title> <booktitle> In Proceedings of the 7th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: Arrow heads indicate the beginning and end of intervals (shown as boxed numbers). Synchronization is not depicted. fl represents writes to variables. P a and P c in interval 2 and P b in interval 3. The operand information can hence be represented compactly as Writes in <ref> [P a ; 2] </ref>, [P b ; 3] & [P c ; 2] src ops Write to t For brevity, in the remainder of this paper, we represent this just as [P a ; 2], [P b ; 3], [P c ; 2] src ops - t where this is understood <p> Synchronization is not depicted. fl represents writes to variables. P a and P c in interval 2 and P b in interval 3. The operand information can hence be represented compactly as Writes in [P a ; 2], [P b ; 3] & <ref> [P c ; 2] </ref> src ops Write to t For brevity, in the remainder of this paper, we represent this just as [P a ; 2], [P b ; 3], [P c ; 2] src ops - t where this is understood to mean that the source operands for the write <p> The operand information can hence be represented compactly as Writes in <ref> [P a ; 2] </ref>, [P b ; 3] & [P c ; 2] src ops Write to t For brevity, in the remainder of this paper, we represent this just as [P a ; 2], [P b ; 3], [P c ; 2] src ops - t where this is understood to mean that the source operands for the write on the right hand side come from the writes in the intervals on the left hand side. <p> The operand information can hence be represented compactly as Writes in [P a ; 2], [P b ; 3] & <ref> [P c ; 2] </ref> src ops Write to t For brevity, in the remainder of this paper, we represent this just as [P a ; 2], [P b ; 3], [P c ; 2] src ops - t where this is understood to mean that the source operands for the write on the right hand side come from the writes in the intervals on the left hand side. <p> Compute transformations are centered around loop interchange and loop tiling [15, 26]. These techniques can be applied successfully only to highly regular codes. Tseng has implemented an algorithm to eliminate barriers [25] in the SUIF compiler for shared memory multiprocessors <ref> [2] </ref>. Two optimizations are used to reduce overhead and synchronization. By combining adjacent SPMD (fork-join) regions, the overhead of starting up parallel tasks is reduced. By augmenting dependence analysis with communication analysis [8], the compute partitioning is taken into account when checking whether a barrier is needed.
Reference: [3] <author> T. E. Anderson and E. D. Lazowska. Quartz: </author> <title> A tool for tuning parallel program performance. </title> <booktitle> In Proceedings of the International Conference on Measurement and Modeling of Computer Systems (Sigmetrics '90), </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Synchronization is not depicted. fl represents writes to variables. P a and P c in interval 2 and P b in interval 3. The operand information can hence be represented compactly as Writes in [P a ; 2], <ref> [P b ; 3] </ref> & [P c ; 2] src ops Write to t For brevity, in the remainder of this paper, we represent this just as [P a ; 2], [P b ; 3], [P c ; 2] src ops - t where this is understood to mean that the <p> The operand information can hence be represented compactly as Writes in [P a ; 2], <ref> [P b ; 3] </ref> & [P c ; 2] src ops Write to t For brevity, in the remainder of this paper, we represent this just as [P a ; 2], [P b ; 3], [P c ; 2] src ops - t where this is understood to mean that the source operands for the write on the right hand side come from the writes in the intervals on the left hand side. <p> The search space is constructed from a set of hypotheses postulated by the tool builder, and is refined until a few hypotheses accurately reflect the performance problem. Quartz <ref> [3] </ref> uses the normalized processor time metric to rank the contribution of procedures in the program to the overall execution. The resultant listing of the "importance" or different procedures to the execution a la gprof can be used for performance tuning.
Reference: [4] <author> U. Banerjee. </author> <title> Dependence analysis for supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Massachusetts, </address> <year> 1988. </year>
Reference-contexts: In addition, it may also cause processors to block. Hence, while the parallel programmer must use sufficient synchronization in order to prevent data races (concurrent conflicting accesses) [16], any extra synchronization can lead to poor performance. A synchronization operation is unnecessary if no true, anti- or output dependences <ref> [4] </ref> need to be enforced, or if other synchronization satisfies the dependences it enforces. Excess synchronization can arise due to several reasons. Programmers often oversynchronize, forcing more processors to interact than needed. The synchronization constructs that are used may also impose more ordering than required.
Reference: [5] <author> R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Moreover, all accesses made inside doit () (and any function calls it might make) depend on v too. if (v == 0) f t = u + x doit (t) g The first step in converting control to data dependences is to compute the control flow graph (CFG) <ref> [5] </ref> of each function in the program. This is a directed graph with nodes representing basic blocks and edges, the possible flow of control. We use the assembly output to compute this graph. <p> The immediate postdom-inator is defined as follows <ref> [5] </ref>: If X and Y are CFG nodes, and X appears on every path from Y to the exit node, X postdom-inates Y . If X postdominates Y , but X 6= Y , then X strictly postdominates Y .
Reference: [6] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Between subgraphs, synchronization operations are used to create directed edges. An edge is drawn from the release of a synchronization variable to its subsequent acquire, as observed during the execution. A release is a synchronization write and an acquire, a synchronization read <ref> [6] </ref>. This graph enables us to evaluate the happens-before-1 relation ( hb1 - )[1]. Given accesses x and y, x hb1 - y iff there is a path in the partial-order graph from the release immediately succeeding x to the acquire immediately preceding y.
Reference: [7] <author> S.K. Gupta, A.A. Schaffer, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> Integrating parallelization strategies for linkage analysis. </title> <journal> Computers and Biomedical Research, </journal> <volume> 28 </volume> <pages> 116-139, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: We present the results of applying our approach to four applications. Ocean and Barnes-Hut are applications from the Splash benchmark suite [27]. Shallow is a version of the shallow water benchmark from the National Center for Atmospheric Research [20]. Unlike the other three, ILINK <ref> [7] </ref> is a "real" application widely used by geneticists for genetic linkage analysis. For Ocean, the performance debugger found excess synchronization in the program. For Barnes-Hut and Shallow, the debugger suggested computation transformations that reduced synchronization and communication. <p> We ran Shallow for 6 timesteps on a 256 fi 256 input grid. ILINK is a parallelized version of a genetic linkage analysis program and is part of the FASTLINK package <ref> [7] </ref>. In contrast to the other programs, this application is widely used by geneticists and is heavily optimized for performance. Our goal therefore, is not to detect performance problems in the program, but rather to demonstrate the feasibility of our run-time dependence analysis approach to real-world applications.
Reference: [8] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Tseng has implemented an algorithm to eliminate barriers [25] in the SUIF compiler for shared memory multiprocessors [2]. Two optimizations are used to reduce overhead and synchronization. By combining adjacent SPMD (fork-join) regions, the overhead of starting up parallel tasks is reduced. By augmenting dependence analysis with communication analysis <ref> [8] </ref>, the compute partitioning is taken into account when checking whether a barrier is needed. In some instances, the barrier is also be replaced by pairwise synchronization. The performance of the implementation is compared against code generated by the original compiler; modest improvements are reported.
Reference: [9] <author> P. Keleher, S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. </author> <title> Tread-marks: Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: For Ocean, the performance debugger found excess synchronization in the program. For Barnes-Hut and Shallow, the debugger suggested computation transformations that reduced synchronization and communication. Implementing these suggestions improved the performance of the programs by a factor of 1.32 and 34 respectively, on an 8-processor IBM SP2 running TreadMarks <ref> [9] </ref>. TreadMarks is a software distributed shared memory system that provides a shared memory abstraction on message-passing machines. Our debugger did not find any transformations that would benefit ILINK. As ILINK is already a well optimized program, this is not surprising. <p> We describe these programs below, and also the feedback suggested by our performance debugger. We also present the improvements in speedup obtained after implementing the debugger suggestions. These results are obtained from an 8-processor IBM SP2 running TreadMarks <ref> [9] </ref>, a software distributed shared memory system. TreadMarks provides a shared memory abstraction on distributed memory (message-passing) machines. 3.4.1 Applications Barnes-Hut and Ocean are complete applications that are part of the SPLASH benchmark suite [27]. Barnes-Hut is a simulation of a system of bodies influenced by gravitational forces. <p> Our run-time system is the TreadMarks software distributed shared memory system <ref> [9] </ref>. TreadMarks provides a shared memory abstraction on distributed memory (message-passing) machines. In the algorithm described in section 4, when reaching a synchronization point, we need to scan the state associated with shared locations to log data and also to clear the states.
Reference: [10] <author> J. R. Larus. </author> <title> Abstract execution: A technique for efficiently tracing programs. </title> <journal> Software Practices and Experience, </journal> 20(12) 1241-1258, December 1990. 
Reference-contexts: When executed, the mirror takes the run-time address trace from the original basic block as its argument and executes the algorithm in figure 5. This approach shares some similarities with abstract execution <ref> [10] </ref>, which expands a small set of events recorded at run-time into a full trace. However, while our mirror basic blocks can be considered abstract versions of the original basic blocks, we execute them in conjunction with the original program to efficiently compute operand information.
Reference: [11] <author> J. R. Larus and E Schnarr. EEL: </author> <title> Machine-indepent executable editing. </title> <booktitle> In Proceedings of the ACM SIGPLAN 95 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Instrumentation that we insert at the entry and exit of every function ensures that the mirror functions operate on the proper register set. Although we could have used an instrumentation system such as ATOM [23] for collecting the per-basic block address trace, or EEL <ref> [11] </ref> which also provides control flow information, we would need to extend them to convert control to data dependences and to examine the basic blocks to obtain the operand information.
Reference: [12] <author> S. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In Proceedings of the 1993 Conference on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In some instances, the barrier is also be replaced by pairwise synchronization. The performance of the implementation is compared against code generated by the original compiler; modest improvements are reported. Parallelizing compilers also resort to dependence information obtained at run-time <ref> [12, 18] </ref>. When a loop cannot be parallelized, the compiler uses the inspector-executor paradigm [21] in which the inspector determines dependences at run-time. It then prepares an execution schedule, which is used by the executor to carry out the operations in the loop.
Reference: [13] <author> M. Martonosi, A. Gupta, and T. E. Anderson. </author> <title> Tuning memory performance of sequential and parallel programs. </title> <journal> IEEE Computer, </journal> <volume> 28(4), </volume> <month> April </month> <year> 1995. </year>
Reference-contexts: Quartz [3] uses the normalized processor time metric to rank the contribution of procedures in the program to the overall execution. The resultant listing of the "importance" or different procedures to the execution a la gprof can be used for performance tuning. MemSpy <ref> [13] </ref> and ParaView [22] concentrate on the memory performance of programs. Both these tools simulate the program being tuned and present the collected trace information in various formats. MemSpy categorizes cache misses and presents data-oriented statistics. ParaView presents the times spent in computation, synchronization and the memory hierarchy.
Reference: [14] <author> B. P. Miller, M. D. Callaghan, J. M. Cargille, J. K. Hollingsworth, R. B. Irvin, K. L. Karavanic, K. Kunchitha-padam, and T. Newhall. </author> <title> The paradyn parallel performance measurement tools. </title> <journal> IEEE Computer, </journal> <volume> 28(11), </volume> <month> November </month> <year> 1995. </year>
Reference-contexts: In contrast, we present a complete, run-time dependence analysis technique that works over the whole program, also determining the source operands of computa tion. 11 7.2 Run-time Techniques Paradyn <ref> [14] </ref> dynamically instruments the program being traced, and searches the execution for performance bottlenecks. The search space is constructed from a set of hypotheses postulated by the tool builder, and is refined until a few hypotheses accurately reflect the performance problem.
Reference: [15] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Run-time performance debuggers observe the program execution and present the gathered information to the programmer. 7.1 Compile-time Techniques Compile-time techniques restructure either the data or the computation in the program to reduce synchronization and improve locality. Data dependence analysis <ref> [15] </ref> forms the core of all compile-time data transformations. However, dependence analysis is not very accurate in the presence of procedure calls, unknown loop bounds, indirection arrays and pointer aliases. Compute transformations are centered around loop interchange and loop tiling [15, 26]. <p> Data dependence analysis [15] forms the core of all compile-time data transformations. However, dependence analysis is not very accurate in the presence of procedure calls, unknown loop bounds, indirection arrays and pointer aliases. Compute transformations are centered around loop interchange and loop tiling <ref> [15, 26] </ref>. These techniques can be applied successfully only to highly regular codes. Tseng has implemented an algorithm to eliminate barriers [25] in the SUIF compiler for shared memory multiprocessors [2]. Two optimizations are used to reduce overhead and synchronization.
Reference: [16] <author> Dejan Perkovic and Pete Keleher. </author> <title> Online data-race detection via coherency guarantees. </title> <booktitle> In Proceedings of the Second USENIX Symposium on Operating System Design and Implementation, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: By requiring processors to interact, synchronization may slow down execution. In addition, it may also cause processors to block. Hence, while the parallel programmer must use sufficient synchronization in order to prevent data races (concurrent conflicting accesses) <ref> [16] </ref>, any extra synchronization can lead to poor performance. A synchronization operation is unnecessary if no true, anti- or output dependences [4] need to be enforced, or if other synchronization satisfies the dependences it enforces. Excess synchronization can arise due to several reasons.
Reference: [17] <author> R. Rajamony and A. L. Cox. </author> <title> A performance debugger for eliminating excess synchronization in shared-memory parallel programs. </title> <booktitle> In Proceedings of the Fourth International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, </booktitle> <pages> pages 250-256, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Programmers often oversynchronize, forcing more processors to interact than needed. The synchronization constructs that are used may also impose more ordering than required. In a previous paper, we describe a performance debugger that automatically analyzes data collected from a program execution to detect excess synchronization <ref> [17] </ref>. By providing feedback in terms of lines in the source program, our debugger eliminates the need for the programmer to reason about low-level execution statistics to determine the cause for poor performance.
Reference: [18] <author> L. Rauchwerger, N. M. Amato, and David A. Padua. </author> <title> Run-time methods for parallelizing partially parallel loops. </title> <booktitle> In Proceedings of the 1995 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: In some instances, the barrier is also be replaced by pairwise synchronization. The performance of the implementation is compared against code generated by the original compiler; modest improvements are reported. Parallelizing compilers also resort to dependence information obtained at run-time <ref> [12, 18] </ref>. When a loop cannot be parallelized, the compiler uses the inspector-executor paradigm [21] in which the inspector determines dependences at run-time. It then prepares an execution schedule, which is used by the executor to carry out the operations in the loop.
Reference: [19] <author> M. Ronsse and W. Zwaenepoel. </author> <title> Execution replay for treadmarks. </title> <booktitle> In Proceedings of the Fifth EUROMICRO Workshop on Parallel and Distributed Processing, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: As long as the program is free of data races, this results in the same access ordering as during the "record" phase. This technique and the perturbation it introduces is discussed in greater detail in <ref> [19] </ref>. We have not yet implemented this technique. The applications we consider here use only barriers and locks. The locks are used only to enforce mutual exclusion when computing reductions. Hence, for these applications, the instrumentation did not distort the synchronization order.
Reference: [20] <author> R. Sadourny. </author> <title> The dynamics of finite-difference models of the shallow-water equations. </title> <journal> Journal of Atmospheric Sciences, </journal> <volume> 32(4), </volume> <month> April </month> <year> 1975. </year>
Reference-contexts: We present the results of applying our approach to four applications. Ocean and Barnes-Hut are applications from the Splash benchmark suite [27]. Shallow is a version of the shallow water benchmark from the National Center for Atmospheric Research <ref> [20] </ref>. Unlike the other three, ILINK [7] is a "real" application widely used by geneticists for genetic linkage analysis. For Ocean, the performance debugger found excess synchronization in the program. For Barnes-Hut and Shallow, the debugger suggested computation transformations that reduced synchronization and communication. <p> Ocean is a simulation to study the role of eddy and boundary currents in influencing large-scale ocean movements. We used a 130 fi 130 grid as the input data set. Shallow is a version of the shallow water benchmark from the National Center for Atmospheric Research <ref> [20] </ref>. This program solves difference equations on a two dimensional grid for the purpose of weather prediction. The code we used was given to us with a request for improving its performance on TreadMarks.
Reference: [21] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Runtime paralleliza-tion and scheduling of loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The performance of the implementation is compared against code generated by the original compiler; modest improvements are reported. Parallelizing compilers also resort to dependence information obtained at run-time [12, 18]. When a loop cannot be parallelized, the compiler uses the inspector-executor paradigm <ref> [21] </ref> in which the inspector determines dependences at run-time. It then prepares an execution schedule, which is used by the executor to carry out the operations in the loop.
Reference: [22] <author> Evan Speight and John K. Bennett. Paraview: </author> <title> Performance debugging of shared-memory parallel programs. </title> <type> Technical Report ELEC TR 9403, </type> <institution> Rice University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Quartz [3] uses the normalized processor time metric to rank the contribution of procedures in the program to the overall execution. The resultant listing of the "importance" or different procedures to the execution a la gprof can be used for performance tuning. MemSpy [13] and ParaView <ref> [22] </ref> concentrate on the memory performance of programs. Both these tools simulate the program being tuned and present the collected trace information in various formats. MemSpy categorizes cache misses and presents data-oriented statistics. ParaView presents the times spent in computation, synchronization and the memory hierarchy.
Reference: [23] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the ACM SIGPLAN 94 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: Instrumentation that we insert at the entry and exit of every function ensures that the mirror functions operate on the proper register set. Although we could have used an instrumentation system such as ATOM <ref> [23] </ref> for collecting the per-basic block address trace, or EEL [11] which also provides control flow information, we would need to extend them to convert control to data dependences and to examine the basic blocks to obtain the operand information.
Reference: [24] <author> Joseph Torrellas, Monica S. Lam, and John L. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 43(6) </volume> <pages> 651-663, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: We were thus able to weaken this synchronization. On an 8-processor IBM SP2 running TreadMarks, the speedup of the program improved from 4.1 to 5.4 after restructuring. A significant portion of this improvement came about as a result of reduced false shar ing <ref> [24] </ref>. Shallow: The main data structures in this program are a number of arrays. Each processor is assigned the computation on a band (set of contiguous rows) of the arrays. Sharing is required only at the boundary rows of the bands. Periodically, the border elements of the arrays are exchanged.
Reference: [25] <author> C.-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Proceedings of the 5th Symposium on the Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Compute transformations are centered around loop interchange and loop tiling [15, 26]. These techniques can be applied successfully only to highly regular codes. Tseng has implemented an algorithm to eliminate barriers <ref> [25] </ref> in the SUIF compiler for shared memory multiprocessors [2]. Two optimizations are used to reduce overhead and synchronization. By combining adjacent SPMD (fork-join) regions, the overhead of starting up parallel tasks is reduced.
Reference: [26] <author> M. E. Wolfe and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Data dependence analysis [15] forms the core of all compile-time data transformations. However, dependence analysis is not very accurate in the presence of procedure calls, unknown loop bounds, indirection arrays and pointer aliases. Compute transformations are centered around loop interchange and loop tiling <ref> [15, 26] </ref>. These techniques can be applied successfully only to highly regular codes. Tseng has implemented an algorithm to eliminate barriers [25] in the SUIF compiler for shared memory multiprocessors [2]. Two optimizations are used to reduce overhead and synchronization.
Reference: [27] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> Splash-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <month> June </month> <year> 1995. </year> <month> 13 </month>
Reference-contexts: We have implemented methods both for collecting dependence information at run-time and for analyzing the collected data in a performance debugger. We present the results of applying our approach to four applications. Ocean and Barnes-Hut are applications from the Splash benchmark suite <ref> [27] </ref>. Shallow is a version of the shallow water benchmark from the National Center for Atmospheric Research [20]. Unlike the other three, ILINK [7] is a "real" application widely used by geneticists for genetic linkage analysis. For Ocean, the performance debugger found excess synchronization in the program. <p> These results are obtained from an 8-processor IBM SP2 running TreadMarks [9], a software distributed shared memory system. TreadMarks provides a shared memory abstraction on distributed memory (message-passing) machines. 3.4.1 Applications Barnes-Hut and Ocean are complete applications that are part of the SPLASH benchmark suite <ref> [27] </ref>. Barnes-Hut is a simulation of a system of bodies influenced by gravitational forces. We ran Barnes-Hut with 16384 bodies for 5 timesteps. Ocean is a simulation to study the role of eddy and boundary currents in influencing large-scale ocean movements.
References-found: 27

