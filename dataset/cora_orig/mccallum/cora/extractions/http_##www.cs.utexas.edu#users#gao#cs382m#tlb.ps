URL: http://www.cs.utexas.edu/users/gao/cs382m/tlb.ps
Refering-URL: http://www.cs.utexas.edu/users/gao/cs382m/tlb.html
Root-URL: 
Title: Improving TLB performance  
Author: Youxin Gao and Lisha He 
Date: May 11, 1998  
Address: Austin, Texas 78712  
Affiliation: Department of Computer Sciences University of Texas at Austin  
Abstract: In modern computers, TLB can be in the critical path of a memory access. Good TLB performance is essential to good overall performance of a machine [5]. The objective of our project is to improve the TLB performance. In our project, we propose two approaches to improve TLB performance. One approach is to use TLB prefetching, and the other is to have an additional level TLB (so we have a multi-level TLB). We only consider the impact of TLB performance on the overall performance, so we assume a perfect cache system, i.e. the size and level of caches will not be adjusted. Our TLB designs will be analyzed by a quantitative simulation using The SimpleScalar Tool Set (version 2.0) [2], where SPEC-95 benchmark programs are served as workloads. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Bala, M.F. Kaashoek and W.E Weihl, </author> <title> Software Prefetching and Caching for Translation Lookaside Buffers, </title> <booktitle> Proc. of the First Symposium on Operating Systems Design and Implementation, Usenix Association, </booktitle> <month> November, </month> <year> 1994. </year> <month> 18 </month>
Reference-contexts: The shortcoming of these methods is that it neglects the effects on operating system and also increases the TLB hit time (in the case of large number of entries). Other methods to improve TLB performance are to use software prefetching and caching. <ref> [1] </ref> shows that software prefetching itself can reduce kernel TLB misses by 6%.
Reference: [2] <author> D. </author> <title> Burger and T.M. Austin, The SimpleScalar Tool Set, Version 2.0, </title> <type> UW-Madison Tech. Report #1342, </type> <month> July, </month> <year> 1997. </year>
Reference-contexts: The performance of our TLB design will be analyzed by a quantitative simulation. The simulation tool we are going to use is The SimpleScalar Tool Set (version 2.0) <ref> [2] </ref>, which can simulate the running of real programs (e.g. SPEC benchmark programs) on a range of self designed processors and systems. The memory system it simulates can have split TLBs and caches, namely data TLB, cache and instruction TLB, cache. The cache can be implemented as a two-level cache.
Reference: [3] <author> J.B. Chen, A. </author> <title> Borg and N.P. Jouppi, A Simulation-Based Study of TLB Performance, </title> <journal> WRL Research Report, </journal> <volume> 91/2, </volume> <year> 1991. </year>
Reference-contexts: An early study shows that TLB miss penalties consume 6% of all machine cycles and 4% of execution time, and hence can have a significant impact on machine performance <ref> [3] </ref>. This effect is even larger in today's modern computers which have larger memory sizes. A recent work investigates both TLB and cache performances in different systems by running several workloads, e.g. program development, database and engineering [5]. <p> Reducing TLB misses and miss penalties becomes increasingly important to overall performance. Like cache misses, TLB misses may be classified into three different categories, capacity miss, compulsory miss and conflict miss. <ref> [3] </ref> has shown that TLB miss is primarily dominated by capacity miss, because the mapping size of TLB, which is the product of the number of entries and the page size, is not big enough to map the entire working set of the program. <p> Intuitively, capacity miss can be reduced by increasing the mapping size of the TLB. This can be achieved by increasing the number of TLB entries, or increasing the page size that one entry can map. It is shown in <ref> [3] </ref> that these two approaches indeed can reduce TLB miss rate. The shortcoming of these methods is that it neglects the effects on operating system and also increases the TLB hit time (in the case of large number of entries). <p> Conflict miss occurs if several different references map to the same location. Compulsory miss does not occur frequently, and does not have a large impact on overall performance in current systems. It is shown in <ref> [3] </ref> that TLB misses are dominated by capacity misses. To avoid capacity misses, we can increase the mapping size of TLB so that it is large enough to map almost the entire working set of a program. <p> This can be achieved by either increasing the number of entries in TLB or increasing page size (sometimes we even use variable page size). These two approaches have been shown in <ref> [3] </ref> to be effective in reducing TLB miss rate. Part of this work has been reproduced in section 6. However, large page size indeed helps to reduce compulsory misses, because it takes advantages of spatial locality. <p> These two addresses will be placed in TLB after they return. Replacement strategy can be either FIFO or LRU. We use FIFO since when the number of TLB entries is larger than two, hardware for implementing LRU becomes much more complicated than that of FIFO <ref> [3] </ref>. Except for a few cases, the performance of FIFO is comparable to that of LRU. The disadvantage of TLB prefetching is that it requires a wider bus. If we allow overlap between demand request and prefetch request, prefetch may interface with demand request. The performance may be slowed. <p> In the next few experiments, we are going to determine the appropriate sizes for TLBs. Similar experiments have been done in <ref> [3] </ref>, so our experiments can be thought of as a reproductive work. First, we perform an experiment to show how variable page size influences the miss rate of TLB. We simulate instruction-only TLB with benchmark program cc1.
Reference: [4] <author> J.L. Hennessy and D.A. Patterson, </author> <title> Computer Architecture: A Quantitative Approach, 2nd Edition, </title> <publisher> Mor-gan Kaufmann Pub. Inc., </publisher> <year> 1996. </year>
Reference-contexts: Note the length of page offset in a virtual address does not need to be the same as the length of block offset plus index in a physical address, although in some machines this is true (e.g. Alpha AXP 21064 in <ref> [4] </ref>). Main memory is addressed by physical addresses, therefore before 4 any memory access is actually performed, a translation from virtual address to physical address should be done first. The translation is first performed in TLB by searching TLB for a match from virtual address to physical address.
Reference: [5] <author> M. Rosenblum, E. Bugnion, S.A. Herrod, E. Witchel and A. Gupta, </author> <title> The Impact of Architectural Trends on Operating System Performance, </title> <booktitle> SOSP, </booktitle> <year> 1997. </year> <month> 19 </month>
Reference-contexts: The gap between the processor and the memory 1 speeds is increasingly becoming larger, which makes the memory latency an important performance bottle-neck. A recent study shows that memory system may stall CPU for over 50% of the execution time <ref> [5] </ref>. In a typical computer which supports paged virtual memory, the memory system usually consists of a translation lookaside buffer (TLB), a page table, a cache and a disk. To improve the memory performance, cache hierarchies are an important step toward addressing the latency problem. <p> This effect is even larger in today's modern computers which have larger memory sizes. A recent work investigates both TLB and cache performances in different systems by running several workloads, e.g. program development, database and engineering <ref> [5] </ref>. Their study shows that the system spends about 13% of the kernel time servicing TLB faults in a uni-processor system, and the situation becomes much worse for multi-processor. Reducing TLB misses and miss penalties becomes increasingly important to overall performance.
References-found: 5

