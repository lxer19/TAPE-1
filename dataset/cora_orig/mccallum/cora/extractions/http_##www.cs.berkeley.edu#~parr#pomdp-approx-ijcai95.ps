URL: http://www.cs.berkeley.edu/~parr/pomdp-approx-ijcai95.ps
Refering-URL: http://www.cs.berkeley.edu/~parr/index.html
Root-URL: 
Email: fparr,russellg@cs.berkeley.edu  
Phone: Tel: (510) 642-2038, Fax: (510) 642-5775  
Title: Approximating Optimal Policies for Partially Observable Stochastic Domains  
Author: Ronald Parr, Stuart Russell 
Address: Berkeley, CA 94720, USA  
Affiliation: Computer Science Division University of California  
Abstract: The problem of making optimal decisions in uncertain conditions is central to Artificial Intelligence. If the state of the world is known at all times, the world can be modeled as a Markov Decision Process (MDP). MDPs have been studied extensively and many methods are known for determining optimal courses of action, or policies. The more realistic case where state information is only partially observable, Partially Observable Markov Decision Processes (POMDPs), have received much less attention. The best exact algorithms for these problems can be very inefficient in both space and time. We introduce Smooth Partially Observable Value Approximation (SPOVA), a new approximation method that can quickly yield good approximations which can improve over time. This method can be combined with reinforcement learning methods, a combination that was very effective in our test cases.
Abstract-found: 1
Intro-found: 1
Reference: [ Bellman, 1957 ] <author> Richard Ernest Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1957. </year>
Reference-contexts: There are several methods for determining optimal policies for MDPs. One effective method for determining a value function for the infinite horizon case is value iteration <ref> [ Bellman, 1957 ] </ref> . If the transition probabilities for the model are not known, reinforcement learning [ Sutton, 1988 ] can be used to learn an optimal policy through exploration. When separate value functions are maintained for each action, these functions are often called Q-functions. <p> This gives us a strategy for improving the value function: Search for inconsistencies in our value function, then adjust the parameters in the direction that minimizes these inconsistencies. This is done by computing the Bellman residual <ref> [ Bellman, 1957 ] </ref> , E (b) = V (b) (R (b) + fi max X b 0 2next (b,a) P (b 0 jb, a)V (b 0 )) where next (b, a) is the set of belief states reachable from b on taking action a.
Reference: [ Cassandra et al., 1994 ] <author> A. R. Cassandra, L. P. Kaelbling, and M. L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pages 1023-1028, </pages> <address> Seattle, Washington, </address> <month> August </month> <year> 1994. </year> <note> AAAI Press. </note>
Reference-contexts: The first, shown in Figure 3, is a 4fi4 world from <ref> [ Cassandra et al., 1994 ] </ref> . Movement into adjacent squares is permitted in the four compass directions but an attempt to move off the edge of the world has no effect, returning the agent to its original state with no indication that anything unusual has happened. <p> Figure 4 shows a graph of the average reward garnered per step vs. the number of iterations performed. The horizontal line is the value of the optimal policy, computed using the Witness algorithm <ref> [ Cassandra et al., 1994 ] </ref> , perhaps the fastest known exact algorithm. Both algorithms required time on the order of CPU minutes. Our second problem, shown in Figure 5, is from [ Russell and Norvig, 1995 ] . It is a 4fi3 grid-world with an obstruction at (2,2).
Reference: [ Chrisman, 1992 ] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI-92), </booktitle> <pages> pages 183-188, </pages> <address> San Jose, California, July 1992. </address> <publisher> AAAI Press. </publisher>
Reference: [ Jaakola et al., in press ] <author> Tommi Jaakola, Satinder P. Singh, and Michael I. Jordan. </author> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <note> In Neural Information Processing Systems 7, to appear, in press. </note>
Reference: [ Lin and Mitchell, 1992 ] <author> Long-Ji Lin and Tom M. Mitchell. </author> <title> Memory approaches to reinforcement learning in non-Markovian domains. </title> <type> Technical report, </type> <institution> Computer Science Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania, </institution> <year> 1992. </year>
Reference-contexts: A linear value approximator is combined with a clever model learning mechanism in [ McCallum, 1993 ] and [ Chris-man, 1992 ] . It may be possible to generalize their approach to include more complex functions like those represented by SPOVA. A neural network based approach is used in <ref> [ Lin and Mitchell, 1992 ] </ref> . They consider a variety of approaches that can make use of an agent's history to learn hidden state information. The idea of a smoothed or soft max has been around for a while.
Reference: [ Littman et al., in press ] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie P. Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, to appear, </booktitle> <publisher> in press. </publisher>
Reference-contexts: In this case the Witness algorithm did not converge, although recent results in <ref> [ Littman et al., in press ] </ref> indicate that convergence or near convergence may not be necessary in all cases to obtain a good policy from the Witness Algorithm.
Reference: [ Littman, 1994 ] <author> Michael L. Littman. </author> <title> The witness algorithm: Solving partially observable Markov decision processes. </title> <type> Technical report, </type> <institution> Computer Science Department, Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1994. </year>
Reference-contexts: Each fl i in G can be shown to represent a fixed policy, meaning that we are maximizing over a set of policies to find the one that is best in a particular region of the belief space. (See <ref> [ Littman, 1994 ] </ref> for an in-depth interpretation of the fl vectors.) Graphically, fl i is a hyperplane in value space and the max of these functions forms a convex piecewise-linear surface.
Reference: [ Lovejoy, 1991 ] <author> W. S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28(1-4):47-66, </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: Unfortunately, most interesting POMDPs induce a very large or infinite number of belief states, making direct application of MPD algorithms to POMDPs impractical. A survey of existing POMDP algorithms <ref> [ Lovejoy, 1991 ] </ref> shows that many POMDP algorithms work by constructing a finite representation of a value function over belief states, then iteratively updating this representation, expanding the horizon of the policy it implies, until a desired depth is reached.
Reference: [ Martinetz et al., 1993 ] <author> Thomas M. Martinetz, Stanislav G. Berkovich, and Klaus J. Schulten. </author> <title> neural-gas network for vector quantization and its application to time series prediction. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> SSC-4:558-569, </volume> <year> 1993. </year>
Reference-contexts: See, for example, <ref> [ Martinetz et al., 1993 ] </ref> . the parameters of the function to improve our approximation. Ideally, we could use data points from the optimal value function, V fl , to construct G. <p> The idea of a smoothed or soft max has been around for a while. It is the basic idea behind the use of the Boltzman distribution for action selection in [ Watkins, 1989 ] and a similar approach has been used in neural networks in, for example, <ref> [ Martinetz et al., 1993 ] </ref> . We suspect that it may be possible to adapt these approximators for use in POMDPs using a similar approach to the one described here although we have not yet investigated this fully.
Reference: [ McCallum, 1993 ] <author> Andrew R. McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 190-196, </pages> <address> Amherst, Massachusetts, July 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Jaakola et al. [ in press ] have shown how to learn from reinforcement using randomized policies, demonstrating that the approach is not unreasonable in some cases. A linear value approximator is combined with a clever model learning mechanism in <ref> [ McCallum, 1993 ] </ref> and [ Chris-man, 1992 ] . It may be possible to generalize their approach to include more complex functions like those represented by SPOVA. A neural network based approach is used in [ Lin and Mitchell, 1992 ] .
Reference: [ Russell and Norvig, 1995 ] <author> Stuart J. Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1995. </year>
Reference-contexts: The horizontal line is the value of the optimal policy, computed using the Witness algorithm [ Cassandra et al., 1994 ] , perhaps the fastest known exact algorithm. Both algorithms required time on the order of CPU minutes. Our second problem, shown in Figure 5, is from <ref> [ Russell and Norvig, 1995 ] </ref> . It is a 4fi3 grid-world with an obstruction at (2,2). The coordinates are labeled in x,y pairs, making (1,3) the top left. There is no discounting, but a penalty of 0.04 is charged for every step that is taken in this world.
Reference: [ Russell et al., 1994 ] <author> Stuart J. Russell, John Binder, and Daphne Koller. </author> <title> Adaptive probabilistic networks. </title> <type> Technical Report UCB/CSD-94-824, </type> <institution> Computer Science Division, University of California at Berkeley, </institution> <month> July 24 </month> <year> 1994. </year>
Reference-contexts: The next steps are to tackle larger problems, to obtain convergence results, and to incorporate methods for learning the environment model. We currently are investigating the application of a new algorithm for learning dynamic probabilistic networks (DPNs) <ref> [ Russell et al., 1994 ] </ref> . Such algorithms can find decomposed representations of the environment model that should allow very large state spaces to be handled. Furthermore, the DPN provides a reduced representation of the belief state that may facilitate additional generalization in the representation of the value function.
Reference: [ Sondik, 1971 ] <author> E. J. Sondik. </author> <title> The Optimal Control of Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, California, </institution> <year> 1971. </year>
Reference-contexts: For some classes of problems <ref> [ Sondik, 1971 ] </ref> infinite-horizon policies will have finite representations and value functions can be obtained for these problems by expanding the horizon until the value function converges to a stable value.
Reference: [ Sutton, 1988 ] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: There are several methods for determining optimal policies for MDPs. One effective method for determining a value function for the infinite horizon case is value iteration [ Bellman, 1957 ] . If the transition probabilities for the model are not known, reinforcement learning <ref> [ Sutton, 1988 ] </ref> can be used to learn an optimal policy through exploration. When separate value functions are maintained for each action, these functions are often called Q-functions. When reinforcement learning is used to learn Q-functions it is called Q-learning [ Watkins, 1989 ] .
Reference: [ Watkins, 1989 ] <author> C. J. Watkins. </author> <title> Models of Delayed Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Psychology Department, Cam-bridge University, </institution> <address> Cambridge, United Kingdom, </address> <year> 1989. </year>
Reference-contexts: When separate value functions are maintained for each action, these functions are often called Q-functions. When reinforcement learning is used to learn Q-functions it is called Q-learning <ref> [ Watkins, 1989 ] </ref> . Our algorithms do not maintain separate value functions for each action. <p> They consider a variety of approaches that can make use of an agent's history to learn hidden state information. The idea of a smoothed or soft max has been around for a while. It is the basic idea behind the use of the Boltzman distribution for action selection in <ref> [ Watkins, 1989 ] </ref> and a similar approach has been used in neural networks in, for example, [ Martinetz et al., 1993 ] .
References-found: 15

