URL: ftp://ftp.csd.abdn.ac.uk/pub/claire/ibiscrc.ps
Refering-URL: http://www.csl.sony.co.jp/person/amf/iis96-schedule.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fclaire, pedwardsg@csd.abdn.ac.uk  
Title: Using Machine Learning to Enhance Software Tools for Internet Information Management  
Author: Claire L Green Peter Edwards 
Address: Aberdeen, Scotland, AB24 3UE  
Affiliation: Department of Computing Science, King's College, University of Aberdeen,  
Abstract: This paper discusses the issues involved in the application of machine learning techniques to the management of Internet-based information. We present a general architecture, and describe how this has been instantiated in several different applications. The first three of these systems provide assistance to a user sorting incoming mail, reading USENET news or identifying World-Wide Web pages of interest; the final system constructs a personalised on-line newspaper, assembled from documents gathered by a Web robot. A number of machine learning techniques have been used in the construction of these systems; some comparative results are presented. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Armstrong, R.; Freitag, D.; Joachims, T.; and Mitchell, T. </author> <year> 1995. </year> <title> WebWatcher: A Learning Apprentice for the World Wide Web. </title> <booktitle> In Working Notes of the AAAI Spring Symposium Series on Information Gathering from Distributed, Heterogeneous Environments. </booktitle> <address> Menlo Park, CA:AAAI. </address>
Reference-contexts: NewT filters new articles by converting them into their vector space representations (Salton & McGill 1983); and testing these against the profiles. Articles are ranked according to the closeness of the match, and the highest ranking articles are presented to the user. WebWatcher <ref> (Armstrong et al. 1995) </ref> is an information search assistant for the World-Wide Web which attempts to recommend links that the user should follow. The system learns by observing the user's reaction to its advice and the eventual success or failure of the user's actions.
Reference: <author> Bayer, D. </author> <year> 1995. </year> <title> A Learning Agent for Resource Discovery on the World Wide Web. </title> <type> MSc Thesis, </type> <institution> Department of Computing Science, University of Aberdeen, </institution> <address> Scotland. </address>
Reference-contexts: It is for this reason that further work on feature selection mechanisms was carried out when developing the LAW system, described in the next section. LAW A Learning Apprentice for the World-Wide Web LAW <ref> (Bayer 1995) </ref> helps a user find new and interesting information on the World-Wide Web.
Reference: <author> Cheeseman, P., and Stutz, J. </author> <year> 1996. </year> <title> Bayesian Classification (AutoClass): Theory & Results. In Advances in Knowledge Discovery and Data Mining. </title> <publisher> AAAI/MIT Press, </publisher> <address> Menlo Park. </address>
Reference-contexts: Once material has been gathered by the robot, feature selection is performed on the HTML documents (using methods analogous to those used by LAW) to generate feature sets for each document. These are passed to a Bayesian clustering algorithm, AutoClass <ref> (Cheeseman & Stutz 1996) </ref> which identifies similarities between documents from different sources and groups them into clusters (typically 5-20 documents per cluster). Associated with each document is a probability score which indicates how representative the document is of the cluster to which it has been assigned.
Reference: <author> Clark, P., and Niblett, T. </author> <year> 1989. </year> <title> The CN2 Induction Algorithm. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 261-283. </pages>
Reference-contexts: Features are extracted from the messages in this logfile and utilised by a machine learning algorithm to generate the user profile. Two different approaches were explored to create such profiles: rule induction and an instance-based method. The CN2 rule induction algorithm <ref> (Clark & Niblett 1989) </ref> takes a collection of training examples and induces symbolic rules which can be used to classify new, unseen examples. The IBPL instance-based algorithm (Payne & Edwards 1995) does not perform an explicit rule generation phase, rather it stores instances for use during the classification phase.
Reference: <author> Green, C. </author> <year> 1995. </year> <title> USENET News Agent. BSc Final Year Project Report, </title> <institution> Department of Computing Science, University of Aberdeen, </institution> <address> Scotland. </address>
Reference-contexts: The user can confirm predictions with low ratings, or reject highly rated predictions, thus overriding the agent's decision. IAN Intelligent Assistant for News IAN, an adaptation of UNA <ref> (Green 1995) </ref>, aids a user in identifying interesting USENET news articles. An existing news browser, xrn was adapted in order to accomplish this (see Figure 1).
Reference: <author> Lang, K. </author> <year> 1995. </year> <title> NewsWeeder: Learning to Filter Netnews. </title> <booktitle> In Proceedings of the 12th International Machine Learning Conference (ML95), </booktitle> <pages> 331-339. </pages> <address> San Francisco, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference-contexts: Several other interface agents have been developed in recent years to deal with Internet-based information; a selection of these systems are described in the next section. Related Work NewsWeeder <ref> (Lang 1995) </ref> is a news-filtering system which prioritises USENET news articles for a user using the Minimum Description Length algorithm (Ris-sanen 1978). The user may choose to read a newsgroup from the normal newsgroup hierarchy, or read NewsWeeder's virtual newsgroup.
Reference: <author> Lashkari, Y. </author> <title> The Webhound Personalized Document Filtering System. </title> <address> http://webhound.www.media. mit.edu/projects/webhound. </address>
Reference: <author> Mackenzie, R. </author> <year> 1996. </year> <title> An Automated Assistant for the Retrieval of On-line Newspapers. BSc Final Year Project Report, </title> <institution> Department of Computing Science, University of Aberdeen, </institution> <address> Scotland. </address>
Reference-contexts: The results demonstrate that the robot was able to discover a considerable number of relevant and interesting pages. However, the accuracy of the robot's suggestions dropped rapidly as the confidence in its predictions decreased. AARON The AARON system <ref> (Mackenzie 1996) </ref> is a specialisa-tion of the LAW approach described above. AARON 2 This was judged by one of the authors. rithms for LAW. No.
Reference: <author> Maes, P. </author> <year> 1994a. </year> <title> Agents that Reduce Work and Information Overload. </title> <journal> Communications of the ACM 37(7) </journal> <pages> 30-40. </pages>
Reference-contexts: Interface agents have been proposed as a solution to this problem and can be characterised as systems which "employ Artificial Intelligence techniques to provide assistance to users dealing with a particular computer application" <ref> (Maes 1994a) </ref>. In order to be able to assist the user, an agent must be provided with knowledge of its domain. Two approaches have traditionally been employed to achieve this. The first and most common approach is for users to provide the agent with rules.
Reference: <author> Maes, P. </author> <year> 1994b. </year> <title> Social Interface Agents: Acquiring Competence by Learning from Users and Other Agents. </title> <booktitle> In Software Agents: Papers from the 1994 AAAI Spring Symposium, </booktitle> <pages> 71-78. </pages>
Reference-contexts: A more practical approach that allows flexibility for a wide range of users is one that relies on the application of machine learning techniques. The agent is given a minimum of background knowledge, and learns appropriate behaviour from the user and perhaps other agents <ref> (Maes 1994b) </ref>. The use of machine learning methods to develop a profile of user preferences allows the agent to adapt to changes in user behaviour, as well as eliminating the need for explicit programming with rules or scripts. <p> Webhound (Lashkari) is a per-sonalised World-Wide Web document filtering system that recommends new documents to the user based on observation. An agent window enables users to interact with their own personal Web agent. The system employs social information filtering <ref> (Maes 1994b) </ref> to recommend documents to a user by comparing materials deemed to be of interest to one user with a database of other users' preferences. We will now describe four agents, MAGI, IAN, LAW and AARON, that have been developed to assist a user in dealing with Internet-based information.
Reference: <author> Malone, T.; Grant, K.; Turbak, F.; Brobst, S.; and Cohen, M. </author> <year> 1987. </year> <journal> Intelligent Information-Sharing Systems. Communications of the ACM 30(5) </journal> <pages> 390-402. </pages>
Reference-contexts: In order to be able to assist the user, an agent must be provided with knowledge of its domain. Two approaches have traditionally been employed to achieve this. The first and most common approach is for users to provide the agent with rules. For example, the Oval system <ref> (Malone et al. 1987) </ref> employs rules to determine if a mail message is of interest, and if so, what action should be performed on it. A number of systems have employed a scripting language to allow users to specify rules.
Reference: <author> Payne, T., and Edwards, P. </author> <year> 1995. </year> <title> Interface Agents that Learn: An Investigation of Learning Issues in a Mail Agent Interface. </title> <journal> Applied Artificial Intelligence, </journal> <note> in press. </note>
Reference-contexts: Two different approaches were explored to create such profiles: rule induction and an instance-based method. The CN2 rule induction algorithm (Clark & Niblett 1989) takes a collection of training examples and induces symbolic rules which can be used to classify new, unseen examples. The IBPL instance-based algorithm <ref> (Payne & Edwards 1995) </ref> does not perform an explicit rule generation phase, rather it stores instances for use during the classification phase. The stored instances are compared with descriptions of new situations and a classification determined, based on a similarity measure between the new and old instances. <p> This is done by employing a simple heuristic which labels clusters which contain a large number of lengthy documents as headline topics. The system then considers all the remaining clusters and estimates the level of user interest in each, using the instance-based learning algorithm IBPL <ref> (Payne & Edwards 1995) </ref>; each cluster is compared against a number of pre-stored instances of previously seen, interesting news stories. These instances are gathered by observing the user reading the newspaper, once it is constructed. The observation and user profile generation steps are analogous to those in the LAW system.
Reference: <author> Payne, T. </author> <year> 1994. </year> <title> Learning Email Filtering Rules with Magi, A Mail Agent Interface. </title> <type> MSc Thesis, </type> <institution> Department of Computing Science, University of Aberdeen, </institution> <address> Scotland. </address>
Reference-contexts: The user should therefore be able to override any agent decision if necessary. This paper describes the issues involved in the application of machine learning techniques to interface agents which manage Internet-based information. The agents described here all are based on the same basic architecture <ref> (Payne 1994) </ref> described below. As a Graphical User Interface (GUI) is used to interact with the underlying application, observations are made of the user's behaviour. <p> We will now describe four agents, MAGI, IAN, LAW and AARON, that have been developed to assist a user in dealing with Internet-based information. MAGI Mail AGent Interface Magi aids a user in sorting incoming electronic mail <ref> (Payne 1994) </ref>. By interacting with a modified version of Xmail (see Figure 1) the user organises their mailbox. The user's actions and the messages on which the actions were performed are stored in a session logfile.
Reference: <author> Quinlan, J. </author> <year> 1993. </year> <title> C4.5 Programs for Machine Learning. </title> <address> San Mateo, </address> <publisher> CA:Morgan Kaufmann. </publisher>
Reference-contexts: The words that occur with the highest frequency are considered to be most significant, and are selected to represent the article. As with MAGI, rule induction and instance-based approaches for generating the user profile were compared; the rule induction algorithm in this case being C4.5 <ref> (Quinlan 1993) </ref>. Periodically (e.g. every hour) a daemon queries the news server to retrieve new articles from subscribed newsgroups.
Reference: <author> Rissanen, J. </author> <year> 1978. </year> <title> Modeling by Shortest Data Description. </title> <type> Automatica 14. </type>
Reference: <author> Salton, G., and McGill, M. </author> <year> 1983. </year> <title> Introduction to Modern Information Retrieval. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: NewT (News Tailor) (Sheth 1994) adopts a genetic algorithm-based approach to identify articles of interest to the user. Any number of filtering agents may exist, each agent responsible for filtering news from a specified news hierarchy. NewT filters new articles by converting them into their vector space representations <ref> (Salton & McGill 1983) </ref>; and testing these against the profiles. Articles are ranked according to the closeness of the match, and the highest ranking articles are presented to the user. <p> Low entropy words are removed, such as the, and, etc. and the remaining words rated using a measure of word significance. A number of different measures have been compared, including term frequency, TFIDF (term frequency versus inverse document frequency), and a measure based on term relevance <ref> (Salton & McGill 1983) </ref>. Term frequency 1 (Equation 1) is a simple measure that assumes that the importance of a word is directly proportional to the frequency with which it appears within a document.
Reference: <author> Sheth, B. </author> <year> 1994. </year> <title> A Learning Approach to Personalized Information Filtering. </title> <type> Master's Thesis, </type> <institution> Department of Electrical Engineering and Computer Science, MIT. </institution>
Reference-contexts: The user may choose to read a newsgroup from the normal newsgroup hierarchy, or read NewsWeeder's virtual newsgroup. This virtual newsgroup contains a personalised list of one-line article summaries, from which the user may select a group of articles to read. NewT (News Tailor) <ref> (Sheth 1994) </ref> adopts a genetic algorithm-based approach to identify articles of interest to the user. Any number of filtering agents may exist, each agent responsible for filtering news from a specified news hierarchy.
References-found: 17

