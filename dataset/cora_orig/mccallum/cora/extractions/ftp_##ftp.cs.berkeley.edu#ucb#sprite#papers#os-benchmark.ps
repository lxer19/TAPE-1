URL: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/os-benchmark.ps
Refering-URL: http://www.cs.berkeley.edu/projects/sprite/sprite.papers.html
Root-URL: 
Title: Why Aren't Operating Systems Getting Faster As Fast as Hardware?  
Author: John K. Ousterhout 
Address: Berkeley  
Affiliation: University of California at  
Date: June 11-15, 1990  
Note: USENIX Summer Conference  Anaheim, California  
Abstract: This paper evaluates several hardware platforms and operating systems using a set of benchmarks that stress kernel entry/exit, file systems, and other things related to operating systems. The overall conclusion is that operating system performance is not improving at the same rate as the base speed of the underlying hardware. The most obvious ways to remedy this situation are to improve memory bandwidth and reduce operating systems' tendency to wait for disk operations to complete. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Accetta, M., et al. </author> <title> ``Mach: A New Kernel Foundation for UNIX Development.'' </title> <booktitle> Proceedings of the USENIX 1986 Summer Conference, </booktitle> <month> July </month> <year> 1986, </year> <pages> pp. 93-113. </pages>
Reference-contexts: It appears to be a derivative of System V with some BSD features added. HP-UX is Hewlett-Packard's UNIX product and contains a combination of System V and BSD features. Mach is a new UNIX-like operating system developed at Carnegie Mel-lon University <ref> [1] </ref>. It is compatible with UNIX, and much of the kernel (the file system in particular) is derived from BSD UNIX. However, many parts of the kernel, including the virtual memory system and interprocess communication, have been re-written from scratch. Sprite is an experimental operating system developed at U.C.
Reference: [2] <author> Hill, M., et al. </author> <title> ``Design Decisions in SPUR.'' </title> <journal> IEEE Computer, </journal> <volume> Vol. 19, No. 11, </volume> <month> November </month> <year> 1986, </year> <pages> pp. 8-22. </pages>
Reference-contexts: In order to make the results comparable between different machines, I modified the benchmark so that it always uses the same compiler, which is the GNU C compiler generating code for an experimental machine called SPUR <ref> [2] </ref>. Table 7 contains the raw Andrew results. The table lists separate times for two different phases of the benchmark. The ``copy'' phase consists of everything except the compilation (all of the file copying and scanning), and the ``compile'' phase consists of just the compilation.
Reference: [3] <author> Howard, J., et al. </author> <title> ``Scale and Performance in a Distributed File System.'' </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 6, No. 1, </volume> <month> February </month> <year> 1988, </year> <pages> pp. 51-81. </pages>
Reference-contexts: Modified Andrew Benchmark The only large-scale benchmark in my test suite is a modified version of the Andrew benchmark developed by M. Satyanarayanan for measuring the performance of the Andrew file system <ref> [3] </ref>. The benchmark operates by copying a directory hierarchy containing the source code for a program, stat-ing every file in the new hierarchy, reading the contents of every copied file, and finally compiling the code in the copied hierarchy. <p> For the Ultrix and Mach measurements the machine running the benchmark had a local disk that was used for temporary files, but all other files were accessed remotely. ``AFS'' means that the Andrew file system protocol was used for accessing remote files <ref> [3] </ref>; this configuration was similar to NFS under Ultrix in that the machine running the benchmark had a local disk, and temporary files were stored on that local disk while other files were accessed remotely. In each case the server was the same kind of machine as the client. <p> Elapsed time to execute a modified version of M. Satyanarayanan's Andrew benchmark <ref> [3] </ref>. The first column gives the total time for all of the benchmark phases except the compilation phase. The second column gives the elapsed time for compilation, and the third column gives the total time for the benchmark.
Reference: [4] <author> Nelson, M., Welch, B., and Ousterhout, J. </author> <title> ``Caching in the Sprite Network File System.'' </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 6, No. 1, </volume> <month> February </month> <year> 1988, </year> <pages> pp. 134-154. </pages>
Reference-contexts: the machine running the benchmark. ``Diskless'' refers to Sprite configurations where the machine running the benchmark had no local disk; all files, including the program binaries, the files in the directory tree being copied and compiled, and temporary files were accessed over the network using the Sprite file system protocol <ref> [4] </ref>. ``NFS'' means that the NFS protocol [7] was used to access remote files. For the SunOS NFS measurements the machine running the benchmark was diskless.
Reference: [5] <author> Ousterhout, J., et al. </author> <title> ``The Sprite Network Operating System.'' </title> <journal> IEEE Computer, </journal> <volume> Vol. 21, No. 2, </volume> <month> February </month> <year> 1988, </year> <pages> pp. 23-36. </pages>
Reference: [6] <author> Rosenblum, M., and Ousterhout, J. </author> <title> ``The LFS 9 Storage Manager.'' </title> <booktitle> Proceedings of the USENIX 1990 Summer Conference, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Another new approach is to use log-structured file systems, which decouple file system performance from disk performance and make disk I/O's more efficient. See <ref> [6] </ref> for details. A final consideration is in the area of network protocols. In my opinion, the assumptions inherent in NFS (statelessness and write-through-on-close, in particular) represent a fundamental performance limitation.

References-found: 6

