URL: ftp://ftp.cs.columbia.edu/reports/reports-1992/cucs-027-92.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1992.html
Root-URL: http://www.cs.columbia.edu
Title: Probabilistic Bounds on the Extremal Eigenvalues and Condition Number by the Lanczos Algorithm  
Author: J. Kuczynski H. Wozniakowski 
Date: February, 1992  
Affiliation: Institute of Computer Science, Polish Academy of Sciences  Department of Computer Science, Columbia University and Institute of Applied Mathematics, University of Warsaw  
Pubnum: CUCS-027-92  
Abstract: We analyze the Lanczos algorithm with a random start for approximating the extremal eigenvalues of a symmetric positive definite matrix. We present some bounds on the Lebesgue measure (probability) of the sets of these starting vectors for which the Lanczos algorithm gives at the k-th step satisfactory approximations to the largest and smallest eigenvalues. Combining these bounds we get similar estimates for the condition number of a matrix. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bischof C.H., </author> <title> "Incremental Condition Estimation", </title> <booktitle> SIMAX 11 (1990), </booktitle> <pages> 644-659. </pages>
Reference-contexts: In Section 4 we apply estimates on the smallest and largest eigenvalue to approximate the condition number, cond A, of A in the two norm, cond A = 1 : An approximation to the condition number of a matrix is often wanted when one deals with matrix calculations, see <ref> [1] </ref> and [4]. We provide bounds on the probability of the set of these b's for which the Lanczos algorithm gives at the k-th step the approximation k of the condition number of A such that k cond A ff k for any ff &gt; 1.
Reference: [2] <author> Cullum J.K., R.A.Willoughby, </author> <title> "Lanczos Algorithms for Large Symmetric Eigenvalue Computation", </title> <booktitle> Progress in Scientific Computing, </booktitle> <address> 3,4, </address> <publisher> Birkhauser, </publisher> <address> Boston 1985. </address>
Reference: [3] <author> Dixon J.H., </author> <title> "Estimating Extremal Eigenvalues and Condition Numbers of Matrices", </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 20 (1983), </volume> <pages> 812-814. </pages>
Reference-contexts: In many cases, only a rough approximation of cond A is needed. Then ff may be quite large, say, ff = 10. A similar problem of approximating the condition number has been considered by Dixon <ref> [3] </ref> for arbitrary matrices and by using modified power and 2 inverse power methods. His results are computationally applicable if the vec-tor A 1 z can be computed cheaply for any vector z. This is the case when a factorization of A is given.
Reference: [4] <author> Ferng W.R., G.H.Golub, R.J.Plemmons, </author> <title> "Adaptive Lanczos Methods for Recursive Condition Estimation", </title> <booktitle> Numerical Algorithms 1 (1991), </booktitle> <pages> 1-19. </pages>
Reference-contexts: Section 4 we apply estimates on the smallest and largest eigenvalue to approximate the condition number, cond A, of A in the two norm, cond A = 1 : An approximation to the condition number of a matrix is often wanted when one deals with matrix calculations, see [1] and <ref> [4] </ref>. We provide bounds on the probability of the set of these b's for which the Lanczos algorithm gives at the k-th step the approximation k of the condition number of A such that k cond A ff k for any ff &gt; 1. <p> For simplicity, as in [9], we assume that n 8 and k 4. Theorem 1 Let A be a symmetric positive definite matrix. (a) Let m denote the number of distinct eigenvalues of A. Then for k m; 4 for k 2 <ref> [4; m 1] </ref>; k 1 2:575 ln n ! 2 (b) Let p; p &lt; n, denote the multiplicity of the smallest eigenvalue n , and let np be the second smallest eigenvalue of A.
Reference: [5] <author> Golub G.H., C.Van Loan, </author> <title> "Matrix Computations", </title> <publisher> Johns Hopkins Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference: [6] <author> Gradshteyn I.S., I.W.Ryzhik, </author> <title> "Table of Integrals, Series and Products", </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980 </year>
Reference-contexts: i &gt;fi 2 b 2 1 1 i=j i ) + = c n [0;1] P n ( i=j i &gt; fi 2 b 2 1 i=j i ) + : Changing variables by t = q P n i and x = b 1 , the formula 4.642 of <ref> [6] </ref> yields (L 0 2 c j2 c nj+1 (n j + 1) 0 0 (j2)=2 2 c j2 c nj+1 (n j + 1) 0 0 (j2)=2 2 c j2 c nj+1 (n j + 1)fi 0 c j2 c nj+1 (n j + 1)fiB ((n j + 2)=2; j=2)
Reference: [7] <author> Kahan W., B.N.Parlett, </author> <title> "How Far Should We Go with the Lanczos Process?", in "Sparse Matrix Computations" eds. </title> <publisher> J.Bunch andD.Rose, Academic Press, </publisher> <address> New York, </address> <year> 1976, </year> <pages> 131-144. </pages>
Reference: [8] <author> Kaniel S. </author> <title> "Estimates for Some Computational Techniques in Linear Algebra", </title> <journal> Math. Comp. </journal> <volume> 20 (1966), </volume> <pages> 369-378. </pages>
Reference: [9] <author> Kuczynski J., H.Wozniakowski, </author> <title> "Estimating the Largest Eigenvalue by the Power and Lanczos Algorithms with a Random Start", </title> <note> to appear in SIMAX. </note>
Reference-contexts: The analysis of the Lanczos algorithm for a fixed vector b may be found in many books and papers, in particular, see [2],[5],[7],[8],[10],[11],[12],[14],[15], and [16]. The analysis of the Lanczos algorithm for a random vector b can be found in <ref> [9] </ref>, where average case and probabilistic estimates on the largest eigenvalue are given. These estimates are independent on distribution of eigenvalues of A. In this paper, see Section 2, we first translate the estimates of [9] for approximating the smallest eigenvalue n of A. <p> The analysis of the Lanczos algorithm for a random vector b can be found in <ref> [9] </ref>, where average case and probabilistic estimates on the largest eigenvalue are given. These estimates are independent on distribution of eigenvalues of A. In this paper, see Section 2, we first translate the estimates of [9] for approximating the smallest eigenvalue n of A. We estimate the average relative error of the Lanczos algorithm over all starting vectors b. <p> For brevity we call this the probabilistic failure. We now present an upper bound on the average error of the Lanczos algorithm. It turns out that this bound is the same as this obtained in <ref> [9] </ref> for the Lanczos algorithm approximating the largest eigenvalue 1 . For simplicity, as in [9], we assume that n 8 and k 4. Theorem 1 Let A be a symmetric positive definite matrix. (a) Let m denote the number of distinct eigenvalues of A. <p> We now present an upper bound on the average error of the Lanczos algorithm. It turns out that this bound is the same as this obtained in <ref> [9] </ref> for the Lanczos algorithm approximating the largest eigenvalue 1 . For simplicity, as in [9], we assume that n 8 and k 4. Theorem 1 Let A be a symmetric positive definite matrix. (a) Let m denote the number of distinct eigenvalues of A. <p> Then e avg (A; k) 2:589 n @ q 1 + ( np n )=( 1 n ) A : Proof: For the proof it is enough to apply Theorem 3.2 of <ref> [9] </ref> to the matrix B = 1 I A. 2 Theorem 1 states that e avg (A; k) = 0 for k m, which means that the Lanczos algorithm converges in m; m n, steps. <p> Then for " &gt; 0, f prob (A; k; ") 1:648 q 0 1 ( np n )=( 1 n ) q 1 k1 5 Proof: For the proof it is enough to apply Theorem 4.2 of <ref> [9] </ref> for the matrix B = I A. 2 Theorem 2 gives upper bounds on the probabilistic failure of the Lanczos algorithm for approximating the smallest eigenvalue. <p> Proof: The idea of the proof is similar to the proof of Theorem 4.2 in <ref> [9] </ref>. Let b = i=1 b i v i , where v i ; i = 1; : : : ; n, are orthonormal eigenvectors of A. <p> We note in passing that if fl &gt; cos 2 (=(2 (2k 1))) then ! k (x j ; ) = , (see the proof of Theorem 4.2 of <ref> [9] </ref>). Clearly, (L 0 n X b 2 1 g; where fi 2 = =( 1). Let c i be the Lebesgue measure of the unit ball in R i , c i = i=2 =(1 + i=2). As in [9], see Remark 7.2 of [9], instead of integrating over the <p> ) = , (see the proof of Theorem 4.2 of <ref> [9] </ref>). Clearly, (L 0 n X b 2 1 g; where fi 2 = =( 1). Let c i be the Lebesgue measure of the unit ball in R i , c i = i=2 =(1 + i=2). As in [9], see Remark 7.2 of [9], instead of integrating over the unit sphere 8 S n we may integrate over the unit ball jjbjj 1 with respect to normalized Lebesgue measure, fb 2 S n : i=j i &gt; fi 2 b 2 1 Z jjbjj1 ( i=j i &gt; fi <p> proof of Theorem 4.2 of <ref> [9] </ref>). Clearly, (L 0 n X b 2 1 g; where fi 2 = =( 1). Let c i be the Lebesgue measure of the unit ball in R i , c i = i=2 =(1 + i=2). As in [9], see Remark 7.2 of [9], instead of integrating over the unit sphere 8 S n we may integrate over the unit ball jjbjj 1 with respect to normalized Lebesgue measure, fb 2 S n : i=j i &gt; fi 2 b 2 1 Z jjbjj1 ( i=j i &gt; fi 2 b 2 where (Z) <p> 2 ((n j)=2 + 1) ((n j + 1)=2) 9 Then (L k ) 1 s j = 1=U 2 (k1) ( ); and since s j is a decreasing function of j, the proof is completed. 2 We now explain how Theorem 3 is related to Theorem 4.2 of <ref> [9] </ref>. Observe that using the formula (13) of [9] for n 8 we get s n = 2= &lt; s n1 = 1 &lt; ::: &lt; s 2 = p 0:824 n 1: Take j = 2. <p> 1)=2) 9 Then (L k ) 1 s j = 1=U 2 (k1) ( ); and since s j is a decreasing function of j, the proof is completed. 2 We now explain how Theorem 3 is related to Theorem 4.2 of <ref> [9] </ref>. Observe that using the formula (13) of [9] for n 8 we get s n = 2= &lt; s n1 = 1 &lt; ::: &lt; s 2 = p 0:824 n 1: Take j = 2. <p> Thus we get fb 2 S n : 1 (A) p Recall that Theorem 4.2 of <ref> [9] </ref> yields fb 2 S n : 1 (A) p p p Thus for j = 2 Theorem 3 gives essentially the same bound as Theorem 4.2. For j &gt; 2 the bound of Theorem 3 is better. Usually we do not know the 10 index j. <p> p s 1 2 (k1) ( 1 + (1 )=(M 1) ) 1 0:824 n M 1 U 1 q for k &lt; m and 1=M &lt; &lt; 1. 4 Estimating the condition number 1 = n We now apply the results of the previous section and Theorem 4.2 of <ref> [9] </ref> to estimate the condition number of a matrix. <p> Then (Z k ) = 1; f or k m; (Z k ) 1 1:648 n inf k where (t) = e (2k1) 1t k 1=ff + e (2k1) t : Proof: The proof for k m follows immediately from the corresponding parts of Theorem 4.2 in <ref> [9] </ref> and Theorem 2. Assume now that k is any integer. <p> Consider the sets W = fb 2 S n : 1 ^ ~ k =(1 " 1 )g ^ Y = fb 2 S n : n ~ k " 2 1 g for any " 1 ; " 2 2 (0; 1). From Theorem 4.2 of <ref> [9] </ref> it follows that (W ) &gt; 1 1 (" 1 ) = 1 1:648 n e p " 1 (2k1) ; while from Theorem 2 we have ( ^ Y ) &gt; 1 2 (" 2 ) = 1 1:648 n e (2k1) p " 2 : Y = fb <p> It can easily be done numerically. 5 Numerical results We tested several matrices with many pseudo-random starting vectors b. Without loss of generality, see <ref> [9] </ref>, we restricted ourselves only to diagonal matrices. Vectors b were generated in the same way as in [9]. <p> It can easily be done numerically. 5 Numerical results We tested several matrices with many pseudo-random starting vectors b. Without loss of generality, see <ref> [9] </ref>, we restricted ourselves only to diagonal matrices. Vectors b were generated in the same way as in [9]. The purpose of the numerical tests was, in particular, to verify the sharpness of the bound of Theorem 1 and to check the quality of the bounds of Theorems 5 and 7.
Reference: [10] <author> Paige C.C., </author> <title> "The Computation of Eigenvalues and Eigenvectors of Very Large Sparse Matrix", </title> <type> Ph. D. Thesis, </type> <institution> University of London, </institution> <year> 1971. </year>
Reference: [11] <author> Paige C.C., </author> <title> "Computational Variants of the Lanczos Method for the Eigenproblem", </title> <journal> J. Inst. Math. Appl. </journal> <volume> 10 (1972), </volume> <pages> 373-381. </pages>
Reference: [12] <author> Parlett B.N., </author> <title> "The Symmetric Eigenvalue Problem", </title> <publisher> Prentice Hall, Inc. </publisher> <address> Englewood Cliffs, N.J. </address> <year> 1980. </year>
Reference: [13] <editor> Paszkowski S., "Zastosowania numeryczne wielomianow i szeregow Czebyszewa", </editor> <publisher> PWN, </publisher> <address> Warszawa, </address> <year> 1975. </year>
Reference-contexts: Then ! k (x j ; ) max U 2 p U 2 p (1 ( x) 2 ) = U 2 p =: ; since U 2 (k1) (t) p 1 t 2 1 and this inequality is sharp, see e.g., <ref> [13] </ref>. We note in passing that if fl &gt; cos 2 (=(2 (2k 1))) then ! k (x j ; ) = , (see the proof of Theorem 4.2 of [9]). Clearly, (L 0 n X b 2 1 g; where fi 2 = =( 1).
Reference: [14] <author> Saad Y., </author> <title> "On the Rates of Convergence of the Lanczos and the Block Lanczos Methods", </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 17 (1980), </volume> <pages> 687-706. 26 </pages>
Reference: [15] <author> Scott D.S., </author> <title> "Analysis of the Symmetric Lanczos Process", </title> <publisher> Ph. </publisher> <address> D. The--sis, </address> <institution> University of California at Bekeley, Berkeley,CA., </institution> <note> Memorandum NCB/ERLM 78/40, </note> <year> 1978. </year>
Reference: [16] <author> Wilkinson J.H., </author> <title> "The Algebraic Eigenvalue Problem", </title> <publisher> Oxford Univ. Press, </publisher> <address> London and New York, </address> <year> 1965. </year> <month> 27 </month>
Reference-contexts: It is intuitively clear that a poor choice of the vector b can cause a bad behaviour of the Lanczos algorithm. The analysis of the Lanczos algorithm for a fixed vector b may be found in many books and papers, in particular, see [2],[5],[7],[8],[10],[11],[12],[14],[15], and <ref> [16] </ref>. The analysis of the Lanczos algorithm for a random vector b can be found in [9], where average case and probabilistic estimates on the largest eigenvalue are given. These estimates are independent on distribution of eigenvalues of A.
References-found: 16

