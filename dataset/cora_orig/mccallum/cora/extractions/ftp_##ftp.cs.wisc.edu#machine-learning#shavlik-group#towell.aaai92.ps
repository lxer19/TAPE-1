URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/towell.aaai92.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/towell.aaai92.ps.abstract.html
Root-URL: 
Email: towell@learning.siemens.com shavlik@cs.wisc.edu  
Title: Using Symbolic Learning to Improve Knowledge-Based Neural Networks  
Author: Geoffrey G. Towell Jude W. Shavlik 
Address: 1210 West Dayton Street Madison, Wisconsin 53706  
Affiliation: University of Wisconsin  
Note: Appears in Proceedings of the Tenth National Conference on Artificial Intelligence  
Abstract: The previously-reported Kbann system integrates existing knowledge into neural networks by defining the network topology and setting initial link weights. Standard neural learning techniques can then be used to train such networks, thereby refining the information upon which the network is based. However, standard neural learning techniques are reputed to have difficulty training networks with multiple layers of hidden units; Kbann commonly creates such networks. In addition, standard neural learning techniques ignore some of the information contained in the networks created by Kbann. This paper describes a symbolic inductive learning algorithm for training such networks that uses this previously-ignored information and which helps to address the problems of training "deep" networks. Empirical evidence shows that this method improves not only learning speed, but also the ability of networks to generalize correctly to testing examples. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Fahlman, S. E. & Lebiere, C. </author> <year> 1989. </year> <booktitle> The cascade-correlation learning architecture. In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 524-532, </pages> <address> Denver, </address> <publisher> CO. Morgan Kaufmann. </publisher>
Reference: <author> Minsky, M. </author> <year> 1963. </year> <title> Steps towards artificial intelligence. </title> <editor> In Feigenbaum, E. A. & Feldman, J., editors, </editor> <booktitle> Computers and Thought. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: Algorithm specification To determine correlations between inputs and intermediate conclusions, Daid must first determine the "correct" truth-value of each intermediate conclusion. Doing this perfectly would require Daid to address the full force of the "credit-assignment" problem <ref> (Minsky, 1963) </ref>. However, Daid need not be perfect in its determinations because its goal is simply to identify potentially useful input features.
Reference: <author> Noordewier, M. O.; Towell, G. G.; & Shavlik, J. W. </author> <year> 1991. </year> <title> Training knowledge-based neural networks to recognize genes in DNA sequences. </title> <booktitle> In Advances in Neural Information Processing Systems, volume 3, </booktitle> <address> Denver, </address> <publisher> CO. Morgan Kaufmann. </publisher>
Reference-contexts: These tests, as well as those later in this paper, use real-world problems from molecular biology. The promoter recognition problem set consists of 106 training examples split evenly between two classes (Towell et al., 1990). The splice-junction determination problem has 3190 examples in three classes <ref> (Noordewier et al., 1991) </ref>. Each dataset also has a partially correct domain theory. Earlier tests showing the success of Kbann did not question whether Kbann is robust to domain-theory noise. The tests presented here look at two types of domain-theory noise: deleted antecedents and added antecedents.
Reference: <author> Ourston, D. & Mooney, R. J. </author> <year> 1990. </year> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 815-820, </pages> <address> Boston, MA. </address>
Reference-contexts: The simpler of the paths is the investigation of alternate methods of estimating the appropriate link weights. One approach replaces correlations with ID3's (Quinlan, 1986) information gain metric to select the most useful features for each low-level antecedent. This approach is quite similar to some aspects of Either <ref> (Ourston and Mooney, 1990) </ref>. Another method we are investigating tracks the specific errors addressed by each of the input features. Rather than collecting error statistics across sets of examples, link weights are assigned to the features to specifically correct all of the initial errors.
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: Future Work We are actively pursuing two paths with respect to Kbann-Daid. The simpler of the paths is the investigation of alternate methods of estimating the appropriate link weights. One approach replaces correlations with ID3's <ref> (Quinlan, 1986) </ref> information gain metric to select the most useful features for each low-level antecedent. This approach is quite similar to some aspects of Either (Ourston and Mooney, 1990). Another method we are investigating tracks the specific errors addressed by each of the input features.
Reference: <author> Rosenblatt, F. </author> <year> 1962. </year> <title> Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. </title> <publisher> Spartan, </publisher> <address> New York. </address>
Reference-contexts: Conversely, on the splice-junction dataset, Daid has little effect. The difference in the effect of Daid on the two problems is almost certainly due to the nature of the respective domain theories. Specifically, the domain theory for splice-junction determination provides for little more than Perceptron-like learning <ref> (Rosenblatt, 1962) </ref>, as it has few modifiable hidden units. (Defining "depth" as the number of layers of modifiable links, the depth of the splice-junction domain theory is one for one of the output units and two for the other.) Hence, the learning bias that Daid contributes to Kbann to changes at
Reference: <author> Rumelhart, D. E.; Hinton, G. E.; & Williams, R. J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. & McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstruc-ture of cognition. Volume 1: Foundations, </booktitle> <pages> pages 318-363. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Unfortunately, the networks created by Kbann (Kbann-nets) frequently have this "deep network" property. Hence, algorithms such as backpropagation <ref> (Rumelhart et al., 1986) </ref> may not be well suited to training Kbann-nets. To address both this problem with the training of Kbann-nets and Kbann's empirically discovered weaknesses, this paper introduces the Daid (Desired Antecedent IDentification) algorithm.
Reference: <author> Towell, G. G.; Shavlik, J. W.; & Noordewier, M. O. </author> <year> 1990. </year> <title> Refinement of approximately correct domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <address> Boston, MA. </address>
Reference-contexts: Figure 2c represents the Kbann-net that results from the translation of this do main knowledge into a neural network. Units X and Y in Figure 2c are introduced into the Kbann-net to handle the disjunction in the rule set <ref> (Towell et al., 1990) </ref>. Otherwise, each unit in the Kbann-net corresponds to a consequent or an antecedent in the domain knowledge. The thick lines in Figure 2c represent heavily-weighted links in the Kbann-net that correspond to dependencies in the domain knowledge. <p> The results of these tests motivated the development of Daid. These tests, as well as those later in this paper, use real-world problems from molecular biology. The promoter recognition problem set consists of 106 training examples split evenly between two classes <ref> (Towell et al., 1990) </ref>. The splice-junction determination problem has 3190 examples in three classes (Noordewier et al., 1991). Each dataset also has a partially correct domain theory. Earlier tests showing the success of Kbann did not question whether Kbann is robust to domain-theory noise.
Reference: <author> Towell, G. G. </author> <year> 1991. </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, WI. </institution>
Reference-contexts: The tests presented here look at two types of domain-theory noise: deleted antecedents and added antecedents. Details of the method used for modifying existing rules by adding and deleting antecedents, as well as studies of other types of domain-theory noise, are given in <ref> (Towell, 1991) </ref>. promoter domain theory. All results represent an average over three different additions of noise. Eleven randomized runs of ten-fold cross-validation (Weiss and Kulikowski, 1990) are used to test generalization. Not surprisingly, this figure shows that test set error rate increases directly with the amount of noise.
Reference: <author> Weiss, S. M. & Kulikowski, C. A. </author> <year> 1990. </year> <title> Computer Systems that Learn. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Details of the method used for modifying existing rules by adding and deleting antecedents, as well as studies of other types of domain-theory noise, are given in (Towell, 1991). promoter domain theory. All results represent an average over three different additions of noise. Eleven randomized runs of ten-fold cross-validation <ref> (Weiss and Kulikowski, 1990) </ref> are used to test generalization. Not surprisingly, this figure shows that test set error rate increases directly with the amount of noise. More interesting is that the figure shows the effect of deleting antecedents is consistently larger than the effect of adding antecedents.
Reference: <author> Whewell, W. </author> <year> 1989. </year> <title> Theory of the Scientific Method. </title> <address> Hackett, Indianapolis. </address> <note> Originally published in 1840. </note>
Reference-contexts: His theory of the consilience of inductions suggests that the most uncertain rules are those which appear at the lowest levels of a rule hierarchy and that the most certain rules are those at the top of the hierarchy <ref> (Whewell, 1989) </ref>. Algorithm specification To determine correlations between inputs and intermediate conclusions, Daid must first determine the "correct" truth-value of each intermediate conclusion. Doing this perfectly would require Daid to address the full force of the "credit-assignment" problem (Minsky, 1963).
References-found: 11

