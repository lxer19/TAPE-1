URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-91-46.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: Swift: Using Distributed Disk Striping to Provide High I/O Data Rates  
Author: Luis-Felipe Cabrera Darrell D. E. Long 
Keyword: high-performance storage systems, distributed file systems, distributed disk striping, high-speed networks, high-speed I/O, client-server model, video server, multime dia, data resiliency.  
Address: Santa Cruz  
Affiliation: Computer Science Department IBM Almaden Research Center  Computer Information Sciences University of California,  
Abstract: We present an I/O architecture, called Swift, that addresses the problem of data rate mismatches between the requirements of an application, storage devices, and the interconnection medium. The goal of Swift is to support high data rates in general purpose distributed systems. Swift uses a high-speed interconnection medium to provide high data rate transfers by using multiple slower storage devices in parallel. It scales well when using multiple storage devices and interconnections, and can use any appropriate storage technology, including high-performance devices such as disk arrays. To address the problem of partial failures, Swift stores data redundantly. Using the UNIX operating system, we have constructed a simplified prototype of the Swift architecture. The prototype provides data rates that are significantly faster than access to the local SCSI disk, limited by the capacity of a single Ethernet segment, or in the case of multiple Ethernet segments by the ability of the client to drive them. We have constructed a simulation model to demonstrate how the Swift architecture can exploit advances in processor, communication and storage technology. We consider the effects of processor speed, interconnection capacity, and multiple storage agents on the utilization of the components and the data rate of the system. We show that the data rates scale well in the number of storage devices, and that by replacing the most highly stressed components by more powerful ones the data rates of the entire system increase significantly. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. C. </author> <title> Luther, Digital Video in the PC Environment. M c Graw-Hill, </title> <year> 1989. </year>
Reference-contexts: For example, multimedia applications require this level of service and include scientific visualization, image processing, and recording and play-back of color video. The data rates required by some of these applications range from 1:2 megabytes/second for DVI compressed video and 1:4 megabits/second for CD-quality audio <ref> [1] </ref>, and up to 90 megabytes/second for uncompressed full-frame color video. Advances in VLSI, data compression, processors, communication networks, and storage capacity mean that systems capable of integrating continuous multimedia will soon emerge.
Reference: [2] <author> K. Salem and H. Garcia-Molina, </author> <title> Disk striping, </title> <booktitle> in Proceeding of the 2 nd International Conference on Data Engineering, </booktitle> <pages> pp. 336-342, </pages> <publisher> IEEE, </publisher> <month> Feb. </month> <year> 1986. </year>
Reference-contexts: The architecture we present solves the problem of storing and retrieving very large data objects from slow secondary storage at very high data rates. Its goal is to support high I/O data rates in a general purpose distributed storage system. It stripes data over several disks <ref> [2] </ref>, much like RAID [3], driving the disks in parallel to provide high data rates. Swift, unlike RAID, was designed as a distributed storage system. <p> interconnection medium and the disk storage subsystem had 18 0 10 20 30 40 0 5 10 15 20 25 30 35 % Busy Number of disks 100% read 80% read 100% write spare capacity. 6 Related Research The notion of disk striping was formally introduced by Salem and Garcia-Molina <ref> [2] </ref>. The technique, however, has been in use for many years in the I/O subsystems of super computers [12] and high-performance mainframe systems [10]. Disk striping has also been used in some versions of the UNIX operating system as a means of improving swapping performance [2]. <p> introduced by Salem and Garcia-Molina <ref> [2] </ref>. The technique, however, has been in use for many years in the I/O subsystems of super computers [12] and high-performance mainframe systems [10]. Disk striping has also been used in some versions of the UNIX operating system as a means of improving swapping performance [2]. To our knowledge, Swift is the first to use disk striping in a distributed environment, striping files over multiple servers.
Reference: [3] <author> D. Patterson, G. Gibson, and R. Katz, </author> <title> A case for redundant arrays of inexpensive disks (RAID), </title> <booktitle> in Proceedings of the ACM SIGMOD Conference, </booktitle> <publisher> (Chicago), </publisher> <pages> pp. 109-116, </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: Its goal is to support high I/O data rates in a general purpose distributed storage system. It stripes data over several disks [2], much like RAID <ref> [3] </ref>, driving the disks in parallel to provide high data rates. Swift, unlike RAID, was designed as a distributed storage system. <p> The accepted solution for this problem is to use redundant data, including multiple copy [13] and computed copy (erasure-correcting codes) <ref> [3] </ref>. While either choice is compatible with our architecture, we plan to use computed copy redundancy (in the form of parity) in a future 3 version of the prototype.
Reference: [4] <institution> Imprimis Technology, ArrayMaster 9058 Controller, </institution> <year> 1989. </year>
Reference-contexts: The principle behind our architecture is simple: use a high-speed interconnection medium to aggregate arbitrarily many (slow) storage devices into a faster logical storage service, making all applications unaware of this aggregation. Several concurrent I/O architectures, such as Imprimis ArrayMaster <ref> [4] </ref>, DataVault [5], CFS [6,7], RADD [8] and RAID [3,9], are based on this observation. Mainframes [10,11] and super computers [12] have also exploited this approach. Swift is a client/server distributed architecture made up of independently replaceable components. <p> Examples of some commercial systems that utilize disk striping include super computers [12], DataVault for the CM-2 [5], the airline reservation system TPF [10], the IBM AS/400 [11], CFS from Intel [6,7], and the Imprimis ArrayMaster <ref> [4] </ref>. Hewlett-Packard is developing a system called DataMesh that uses an array of storage processors connected by a high-speed network [23]. For all of these the maximum data rate is limited by the interconnection medium which is an I/O channel.
Reference: [5] <author> Thinking Machines, </author> <title> Incorporated, Connection Machine Model CM-2 Technical Summary, </title> <month> May </month> <year> 1989. </year>
Reference-contexts: The principle behind our architecture is simple: use a high-speed interconnection medium to aggregate arbitrarily many (slow) storage devices into a faster logical storage service, making all applications unaware of this aggregation. Several concurrent I/O architectures, such as Imprimis ArrayMaster [4], DataVault <ref> [5] </ref>, CFS [6,7], RADD [8] and RAID [3,9], are based on this observation. Mainframes [10,11] and super computers [12] have also exploited this approach. Swift is a client/server distributed architecture made up of independently replaceable components. <p> To our knowledge, Swift is the first to use disk striping in a distributed environment, striping files over multiple servers. Examples of some commercial systems that utilize disk striping include super computers [12], DataVault for the CM-2 <ref> [5] </ref>, the airline reservation system TPF [10], the IBM AS/400 [11], CFS from Intel [6,7], and the Imprimis ArrayMaster [4]. Hewlett-Packard is developing a system called DataMesh that uses an array of storage processors connected by a high-speed network [23].
Reference: [6] <author> P. Pierce, </author> <title> A concurrent file system for a highly parallel mass storage subsystem, </title> <booktitle> in Proceedings of the 4 th Conference on Hypercubes, </booktitle> <address> (Monterey), </address> <month> Mar. </month> <year> 1989. </year>
Reference: [7] <author> T. W. Pratt, J. C. French, P. M. Dickens, and S. A. Janet, </author> <title> A comparison of the architecture and performance of two parallel file systems, </title> <booktitle> in Proceedings of the 4 th Conference on Hypercubes, </booktitle> <address> (Monterey), </address> <month> Mar. </month> <year> 1989. </year>
Reference: [8] <author> M. Stonebraker and G. A. Schloss, </author> <title> Distributed RAID anew multiple copy algorithm, </title> <booktitle> in Proceedings of the 6 th International Conference on Data Engineering, </booktitle> <address> (Los Angeles), </address> <pages> pp. 430-437, </pages> <publisher> IEEE Computer Society, </publisher> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: The principle behind our architecture is simple: use a high-speed interconnection medium to aggregate arbitrarily many (slow) storage devices into a faster logical storage service, making all applications unaware of this aggregation. Several concurrent I/O architectures, such as Imprimis ArrayMaster [4], DataVault [5], CFS [6,7], RADD <ref> [8] </ref> and RAID [3,9], are based on this observation. Mainframes [10,11] and super computers [12] have also exploited this approach. Swift is a client/server distributed architecture made up of independently replaceable components.
Reference: [9] <author> S. Ng, </author> <title> Pitfalls in designing disk arrays, </title> <booktitle> in Proceedings of the IEEE COMPCON Conference, </booktitle> <address> (San Francisco), </address> <month> Feb. </month> <year> 1989. </year>
Reference: [10] <author> IBM Corporation, </author> <title> TPF-3 Concepts and Structure Manual. </title>
Reference-contexts: The technique, however, has been in use for many years in the I/O subsystems of super computers [12] and high-performance mainframe systems <ref> [10] </ref>. Disk striping has also been used in some versions of the UNIX operating system as a means of improving swapping performance [2]. To our knowledge, Swift is the first to use disk striping in a distributed environment, striping files over multiple servers. <p> To our knowledge, Swift is the first to use disk striping in a distributed environment, striping files over multiple servers. Examples of some commercial systems that utilize disk striping include super computers [12], DataVault for the CM-2 [5], the airline reservation system TPF <ref> [10] </ref>, the IBM AS/400 [11], CFS from Intel [6,7], and the Imprimis ArrayMaster [4]. Hewlett-Packard is developing a system called DataMesh that uses an array of storage processors connected by a high-speed network [23].
Reference: [11] <author> B. E. Clark and M. J. Corrigan, </author> <title> Application System/400 performance characteristics, </title> <journal> IBM Systems Journal, </journal> <volume> vol. 28, no. 3, </volume> <pages> pp. 407-423, </pages> <year> 1989. </year>
Reference-contexts: To our knowledge, Swift is the first to use disk striping in a distributed environment, striping files over multiple servers. Examples of some commercial systems that utilize disk striping include super computers [12], DataVault for the CM-2 [5], the airline reservation system TPF [10], the IBM AS/400 <ref> [11] </ref>, CFS from Intel [6,7], and the Imprimis ArrayMaster [4]. Hewlett-Packard is developing a system called DataMesh that uses an array of storage processors connected by a high-speed network [23]. For all of these the maximum data rate is limited by the interconnection medium which is an I/O channel.
Reference: [12] <author> O. G. Johnson, </author> <title> Three-dimensional wave equation computations on vector computers, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 72, </volume> <month> Jan. </month> <year> 1984. </year> <month> 22 </month>
Reference-contexts: Several concurrent I/O architectures, such as Imprimis ArrayMaster [4], DataVault [5], CFS [6,7], RADD [8] and RAID [3,9], are based on this observation. Mainframes [10,11] and super computers <ref> [12] </ref> have also exploited this approach. Swift is a client/server distributed architecture made up of independently replaceable components. <p> The technique, however, has been in use for many years in the I/O subsystems of super computers <ref> [12] </ref> and high-performance mainframe systems [10]. Disk striping has also been used in some versions of the UNIX operating system as a means of improving swapping performance [2]. To our knowledge, Swift is the first to use disk striping in a distributed environment, striping files over multiple servers. <p> To our knowledge, Swift is the first to use disk striping in a distributed environment, striping files over multiple servers. Examples of some commercial systems that utilize disk striping include super computers <ref> [12] </ref>, DataVault for the CM-2 [5], the airline reservation system TPF [10], the IBM AS/400 [11], CFS from Intel [6,7], and the Imprimis ArrayMaster [4]. Hewlett-Packard is developing a system called DataMesh that uses an array of storage processors connected by a high-speed network [23].
Reference: [13] <author> S. B. Davidson, H. Garcia-Molina, and D. Skeen, </author> <title> Consistency in partitioned networks, </title> <journal> Computing Surveys, </journal> <volume> vol. 17, </volume> <pages> pp. 341-370, </pages> <month> Sept. </month> <year> 1985. </year>
Reference-contexts: For example, any object which has data in a failed storage agent would become unavailable, and any object that has data being written into the failed storage agent could be damaged. The accepted solution for this problem is to use redundant data, including multiple copy <ref> [13] </ref> and computed copy (erasure-correcting codes) [3]. While either choice is compatible with our architecture, we plan to use computed copy redundancy (in the form of parity) in a future 3 version of the prototype.
Reference: [14] <author> J. Gray, </author> <title> Notes on database operating systems, in Operating Systems: An Advanced Course (R. </title> <editor> Bayer, R. Graham, and G. Seegm uller, </editor> <booktitle> eds.), </booktitle> <pages> pp. 393-481, </pages> <publisher> Springer-Verlag, </publisher> <year> 1979. </year>
Reference: [15] <author> L.-F. Cabrera and J. Wyllie, </author> <title> QuickSilver distributed file services: an architecture for horizontal growth, </title> <booktitle> in Proceedings of the 2 nd IEEE conference on computer workstations, </booktitle> <address> Santa Clara, CA, </address> <month> Mar. </month> <year> 1988. </year>
Reference: [16] <author> M. Nelson, B. Welch, and J. Ousterhout, </author> <title> Caching in the Sprite network file system, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 6, </volume> <pages> pp. 134-154, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: For small objects, we expect that caches will be as beneficial as in other systems <ref> [16] </ref>. Swift achieves reliability through the appropriate use of redundancy. For example, object descriptors store redundant information that allows the reconstruction of all objects by scavenging the data in the storage agents, should a catastrophic failure, or a software error, render the storage mediator inoperative.
Reference: [17] <author> H. Garcia-Molina and K. Salem, </author> <title> The impact of disk striping on reliability, </title> <journal> IEEE Database Engineering Bulletin, </journal> <volume> vol. 11, </volume> <pages> pp. 26-39, </pages> <month> Mar. </month> <year> 1988. </year>
Reference: [18] <author> G. A. Gibson, L. Hellerstein, R. M. Karp, R. H. Katz, and D. A. Patterson, </author> <title> Failure correction techniques for large disk arrays, </title> <booktitle> in Proceedings of the 3 rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 123-32, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: In this way, if a disk fails it can be reconstructed using the information on the other disks. Higher level erasure-correcting codes can be used if more than one failure is to be tolerated <ref> [18] </ref>. 3 Ethernet-based Prototype of Swift A simplified prototype of the Swift architecture has been built as a set of libraries that use the standard filing and interprocess communication facilities of the UNIX operating system.
Reference: [19] <author> D. E. Comer, </author> <title> Internetworking with TCP/IP: Principles, Protocols, and Architecture. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: The current prototype has allowed us to confirm that a high aggregate data rate can be achieved with the Swift architecture. The data rates of an earlier prototype using a data transfer protocol built on the TCP <ref> [19] </ref> network protocol had proved unacceptable. In our first prototype a TCP connection was established between the client and each server. These connections were multiplexed using select. Since TCP delivers data in a stream with no message boundaries, a significant amount of data copying was necessary. <p> Initially, select seemed to be the point of congestion. A closer inspection revealed that using TCP was not appropriate since buffer-management problems prevented the prototype from achieving high data rates. The current prototype has been built using a light-weight data transfer protocol on top of the UDP <ref> [19] </ref> datagram protocol. To avoid as much unnecessary data copying as possible, scatter-gather I/O was used to have the kernel deposit the message directly into the user buffer. <p> This was done in an effort to allocate as much buffer space as possible to the client. The client services an open request by contacting a storage agent at its advertised UDP port address. Each Swift storage agent waits for open requests on a well-known UDP <ref> [19] </ref> port address. When an open request is received, a new (secondary) thread of control is established along with a private port for further communication regarding that file with the client.
Reference: [20] <author> M. G. Baker, J. H. Hartman, M. D. Kupfer, K. W. Shirriff, and J. K. Ousterhout, </author> <title> Measurements of a distributed file system, </title> <booktitle> in Proceedings of the 13 th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 198-212, </pages> <institution> Association for Computing Machinery SIGOPS, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: The system is modeled by client requests that drive storage agent processes. A generator process creates client requests using an exponential distribution to govern request interarrival times. The client requests are differentiated according to pure read, pure write, and a 13 conservative read-to-write ratio of 4:1 <ref> [20] </ref>. There is no modeling of overlapping execution, instead requests are modeled serially: only after a request is complete is the next issued. In our simulation of Swift, for a read operation, a small request packet is multicast to the storage agents.
Reference: [21] <author> M. Rosenblum and J. K. Ousterhout, </author> <title> The design and implementation of a log-structured file system, </title> <booktitle> in Proceedings of the 13 th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pp. 1-15, </pages> <institution> Association for Computing Machinery SIGOPS, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: The time to transfer a block consists of the seek time, the rotational delay and the time to transfer the data from disk. The seek time and rotational latency are assumed to be independent uniform random variables, a pessimistic assumption when advanced layout policies are used <ref> [21] </ref>. Once a block has been read from disk it is scheduled for transmission over the network.
Reference: [22] <author> L.-F. Cabrera, E. Hunter, M. J. Karels, and D. A. </author> <title> Mosher, User-process communication performance in networks of computers, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> vol. 14, </volume> <pages> pp. 38-53, </pages> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: As it is, our model provides a lower bound on the data rates that could be achieved. Transmitting a message on the network requires protocol processing, time to acquire the token, and transmission time. The protocol cost for all packets has been estimated at 1,500 instructions <ref> [22] </ref> plus one instruction per byte in the packet. The time to transmit the packet is based on the network transfer rate.
Reference: [23] <author> J. Wilkes, </author> <title> DataMesh project definition document, </title> <type> Tech. Rep. </type> <institution> HPL-CSP-90-1, Hewlett-Packard Laboratories, </institution> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: Hewlett-Packard is developing a system called DataMesh that uses an array of storage processors connected by a high-speed network <ref> [23] </ref>. For all of these the maximum data rate is limited by the interconnection medium which is an I/O channel. Higher data rates can be achieved by using multiple I/O channels.
Reference: [24] <author> S. Ng, </author> <title> Some design issues of disk arrays, </title> <booktitle> in Proceedings of the IEEE COMPCON Conference, </booktitle> <address> (San Francisco), </address> <month> Feb. </month> <year> 1989. </year>
Reference: [25] <author> J. Buzen and A. Shum, </author> <title> I/O architecture in MVS/370 and MVS/XA, </title> <journal> ICMG Transactions, </journal> <volume> vol. 54, </volume> <pages> pp. 19-26, </pages> <year> 1986. </year>
Reference-contexts: Only those resources that are required to satisfy the request need to be allocated. Swift incorporates data management techniques long present in centralized computing systems into a distributed environment. In particular, it can be viewed as a generalization to distributed systems of I/O channel architectures found in mainframe computers <ref> [25] </ref>. 6.1 Future Work There are two areas that we intend to address in the future: enhancing our current prototype and simulator, and extending the architecture. 6.1.1 Enhancements to the Prototype The current prototype needs to implement data redundancy.
Reference: [26] <author> W. Zhao, </author> <title> A heuristic approach to scheduling hard real-time tasks with resource requirements in distributed systems. </title> <publisher> Ph. </publisher> <address> D. </address> <institution> dissertation, University of Massachussetts, Amherst, </institution> <year> 1986. </year> <month> 23 </month>
Reference: [27] <author> W. Zhao, K. Ramamritham, and J. Stankovic, </author> <title> Preemptive scheduling under time and resource constraints, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 36, no. 8, </volume> <pages> pp. 949-960, </pages> <year> 1987. </year>
Reference: [28] <author> D. P. Anderson, R. G. Herrtwich, and C. Schaefer, SRP: </author> <title> A resource reservation protocol for guaranteed-performance communication in the internet, </title> <type> Tech. Rep. </type> <institution> UCB/CSD 90/596, University of California, Berkeley, </institution> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: We intend to extend the architecture with techniques for providing data rate guarantees for magnetic disk devices. While the problem of real-time processor scheduling has been 20 extensively studied [26,27], and the problem of providing guaranteed communication capacity is also an area of active research <ref> [28] </ref>, the problem of scheduling real-time disk transfers has received considerably less attention. A second area of extensions is in the co-scheduling of services. In the past, only analog storage and transmission techniques have been able to meet the stringent demands of multimedia audio and video applications.
Reference: [29] <author> D. Anderson, </author> <title> Meta-scheduling for distributed continuous media, </title> <type> Tech. Rep. </type> <institution> UCB/CSD 90/599, University of California, Berkeley, </institution> <month> Oct. </month> <year> 1990. </year> <month> 24 </month>
Reference-contexts: To support integrated continuous multimedia, resources such as the central processor, peripheral processors (audio, video), and communication network capacity must be allocated and scheduled together to provide the necessary data-rate guarantees. This meta-scheduling has been studied by Anderson <ref> [29] </ref>. 7 Conclusions This paper presents two studies conducted to validate Swift, a scalable distributed I/O architecture that achieves high data rates by striping data across several storage agents and driving them concurrently. The prototype validates the concept of distributed disk striping in a local-area network.
References-found: 29

