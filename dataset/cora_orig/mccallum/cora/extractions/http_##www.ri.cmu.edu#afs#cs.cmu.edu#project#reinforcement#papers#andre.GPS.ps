URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/reinforcement/papers/andre.GPS.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/reinforcement/mosaic/past-talks.html
Root-URL: 
Email: fdandre,nir,parrg@cs.berkeley.edu  
Title: Generalized Prioritized Sweeping  
Author: David Andre Nir Friedman Ronald Parr 
Address: 387 Soda Hall  Berkeley, CA 94720  
Affiliation: Computer Science Division,  University of California,  
Abstract: Prioritized sweeping is a model-based reinforcement learning method that attempt to focus the agent's limited computational resources to achieve a good estimate of the value of environment states. The classic account of prioritized sweeping uses an explicit, state-based representation of the value, reward, and model parameters. Such a representation is unwieldy for dealing with complex environments and there is growing interest in learning with more compact representations. We claim that classic prioritized sweeping is ill-suited for learning with such representations. To overcome this deficiency, we introduce generalized prioritized sweeping, a principled method for generating representation-specific algorithms for model-based reinforcement learning. We then apply this method for several representations, including state-based models and generalized model approximators (such as Bayesian networks). We describe preliminary experiments that compare our approach with classical prioritized sweeping.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Davies. </author> <title> Multidimensional triangulation and interpolation for reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 9. </booktitle> <year> 1996. </year>
Reference-contexts: A crucial desideratum for reinforcement learning is the ability to scale-up to complex domains. For this, we need to use compact (or generalizing) representations of the model and the value function. While it is possible to apply PS in the presence of such representations (e.g., see <ref> [1] </ref>), we claim that PS is ill-suited in this case. To see this, note that if we use a generalizing model, then after we perform an action a in state s we update our model in a way that may change our estimation of the successors of many states.
Reference: [2] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5 </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: If GenPS is used with an explicit, state-space model and value function representation, an algorithm similar to the original (classic) PS results. When a model approximator (such as a dynamic Bayesian network <ref> [2] </ref>) is used, the resulting algorithm prioritizes the states of the environment using the generalizations inherent in the model representation.
Reference: [3] <author> N. Friedman and M. Goldszmidt. </author> <title> Sequential update of Bayesian network structure. </title> <booktitle> In Proc. 13th Conf. on Uncertainty in Artificial Intelligence. </booktitle> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: For example, in Section 4, we assumed that the agent is given the DBN structure. In many applications, this might be an unreasonable assumption. We are interested in combining our approach with recent results on incremental learning of Bayesian networks <ref> [3] </ref>. We are optimistic that generalized prioritized sweeping will extend the well-known advantages of prioritized sweeping to a much broader and general class of representations.
Reference: [4] <author> G. J. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <booktitle> In Proc. 12th Int. Conf. on Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: The generalized procedure can then be applied not only in the explicit, state-based case, but in cases where approximators are used for the model. In the complete version of this paper we also show to apply GenPS to function approximators, such as averagers <ref> [4] </ref>). We are working on applying GenPS to other types of model and function approximators. The focus of this paper was on interleaving planning with execution during learning. Of course, there are many other issues that need to be dealt with in model-based reinforcement learning.
Reference: [5] <author> D. Heckerman. </author> <title> A tutorial on learning with Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <year> 1995. </year> <month> Revised November </month> <year> 1996. </year>
Reference-contexts: In this case it will easy to compute the new Bellman error as a by-product of the value propagation step. 4 Formally, we are using multinomial Dirichlet priors. See, for example, <ref> [5] </ref> for an introduction to these Bayesian methods. @ Q (s; a) = t N s;a;t +N 0 P if a 0 = a and s = s 0 , 0 otherwise @ Q (s; a) = fl p (t j s; a) Using these derivatives we can examine which states <p> This form of knowledge is often easy to assess from an expert. We discuss how to relax this assumption in Section 6. There several possible ways of learning the parameters for the DBN from experience. We refer the interested reader to <ref> [5, 8] </ref>. As in the state-based representation, we use Dirichlet priors for each multinomial distribution. In this method, we assess the conditional probability i;y;z using prior knowledge and the frequency of transitions observed in the past where Y i = y among these transitions where Pa i = z.
Reference: [6] <author> L. P. Kaelbling, M. L. Littman and A. W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: Thus, states that are rarely if ever visited can be updated based upon the agent's model of their similarity to other states that have been visited. 2 The Basic Principle We assume the reader is familiar with the basic concepts of MDPs (see, e.g., <ref> [6] </ref>). <p> This can be done in a straightforward manner, and we defer the details for now. 2 There is an issue of which action to perform. Since this issue is orthogonal to the our main topic, we do not elaborate on it here. We refer the interested reader to <ref> [6, 7, 9] </ref>. 3 In general, this will assign the state a new priority of 0, unless there is a self loop. In this case it will easy to compute the new Bellman error as a by-product of the value propagation step. 4 Formally, we are using multinomial Dirichlet priors.
Reference: [7] <author> A. W. Moore and C. G. Atkeson. </author> <title> Prioritized sweepingreinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130, </pages> <year> 1993. </year>
Reference-contexts: Model-free methods such as Q-learning [11] take one extreme on this questionthe agent does not perform any propagation of learned information. On the other end of the spectrum lie classical dynamic programming methods that reevaluate the utility of every state in the environment after every experiment. Prioritized sweeping (PS) <ref> [7] </ref> provides a middle ground in that only the most important states are updated, according to a priority metric that attempts to measure the anticipated size of the update for each state. Roughly speaking, PS interleaves performing actions in the environment with propagating the values of states. <p> This can be done in a straightforward manner, and we defer the details for now. 2 There is an issue of which action to perform. Since this issue is orthogonal to the our main topic, we do not elaborate on it here. We refer the interested reader to <ref> [6, 7, 9] </ref>. 3 In general, this will assign the state a new priority of 0, unless there is a self loop. In this case it will easy to compute the new Bellman error as a by-product of the value propagation step. 4 Formally, we are using multinomial Dirichlet priors. <p> We use a maze domain similar to the one described in <ref> [7, p.123] </ref>. The maze, shown in Figure 1 (a), contains 59 cells, and 3 binary flags, resulting in 59 fi 2 3 = 472 possible states. Initially the agent is at the start state (marked by S) and the flags are reset. <p> The i'th flag is set when the agent leaves the position marked by i. The agent receives a reward when it arrives at state G and all of the flags are set. In this situation all of the agent's actions reset the game. As noted in <ref> [7] </ref>, this environment exhibits independencies. Namely, the probability of transition from one position to another does not depend on the flag settings. These independencies can be captured easily by a simple DBN. <p> The second procedure uses a factored model of the environment for learning the model parameters, but uses the same prioritization strategy as the first one. The third procedure uses the GenPS prioritization strategy we describe in Section 4. All three procedures use the exploration strategy described in <ref> [7] </ref> with t bored = 5. Finally, in each iteration these procedures process at most 10 states from the priority queue. The results are shown in Figure 1 (b). As we can see, the GenPS procedure converged faster than the procedures that used classic PS prioritization. <p> As explained above, our hypothesis is that this procedure explioted regularties in the domain to better propogate the value function, and thus, convereged faster to the optimal policy. Note that since we are using the exploration strategy of <ref> [7] </ref>, states for which the procedure performed few value propogations are always considered more attractive. Thus, the PS procedure converged to the optimal policy only after it visited each of these states.
Reference: [8] <author> S. J. Russell and P. Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <year> 1995. </year>
Reference-contexts: This form of knowledge is often easy to assess from an expert. We discuss how to relax this assumption in Section 6. There several possible ways of learning the parameters for the DBN from experience. We refer the interested reader to <ref> [5, 8] </ref>. As in the state-based representation, we use Dirichlet priors for each multinomial distribution. In this method, we assess the conditional probability i;y;z using prior knowledge and the frequency of transitions observed in the past where Y i = y among these transitions where Pa i = z.
Reference: [9] <author> R. S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Machine Learning: Proc. 7th Int. Conf., </booktitle> <year> 1990. </year>
Reference-contexts: Fortunately, we can approximate this scheme, if we notice that the approximate model changes only slightly at each step. We can hope that the value function from the previous model can be easily repaired to reflect these changes. This approach was pursued in the DYNA <ref> [9] </ref> framework, where after the execution of an action, the agent updates its model of the environment, and then performs some bounded number of value propagation steps to update its approximation of the value function. <p> This can be done in a straightforward manner, and we defer the details for now. 2 There is an issue of which action to perform. Since this issue is orthogonal to the our main topic, we do not elaborate on it here. We refer the interested reader to <ref> [6, 7, 9] </ref>. 3 In general, this will assign the state a new priority of 0, unless there is a self loop. In this case it will easy to compute the new Bellman error as a by-product of the value propagation step. 4 Formally, we are using multinomial Dirichlet priors.
Reference: [10] <author> P. Tadepalli and D. </author> <title> Ok. Scaling up average reward reinforcement learning by approximating the domain models and the value function. </title> <booktitle> In Proc. 13th Int. Conf. on Machine Learning, </booktitle> <year> 1996. </year>
Reference-contexts: Thus, we claim that PS implicitly assumes that the agent uses an explicit state-based model. 4 Factored Representation We now examine a compact representation of p (s 0 j s; a) that is based on dynamic Bayesian networks (DBNs). DBNs have been combined with reinforcement learning before in <ref> [10] </ref>, where they were used primarily as a means of reducing the effort required to learn an accurate model. We will show that they can also be used with prioritized sweeping to focus the agent's attention on groups of states that are affected as the agent refines its environment model.
Reference: [11] <author> C. J. Watkins. </author> <title> Models of Delayed Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Psychology Department, Cambridge University. </institution> <year> 1989. </year>
Reference-contexts: 1 Introduction In reinforcement learning, there is a tradeoff between acting in the environment and planning (i.e., performing computations). Model-free methods such as Q-learning <ref> [11] </ref> take one extreme on this questionthe agent does not perform any propagation of learned information. On the other end of the spectrum lie classical dynamic programming methods that reevaluate the utility of every state in the environment after every experiment.
Reference: [12] <author> R. J. Williams and L. C. III Baird. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical report, </type> <institution> College of Computer Science, Northeastern University. </institution> <year> 1993. </year>
Reference-contexts: The motivation for this principle is fairly straightforward. The maximum Bellman error can be used to bound the maximum difference between the current value function, V (s) and the optimal value function, V fl (s) <ref> [12] </ref>. This difference bounds the policy loss, the difference between the expected discounted reward received under the policy implied by the current value function and the expected discounted reward received under the optimal policy.
References-found: 12

