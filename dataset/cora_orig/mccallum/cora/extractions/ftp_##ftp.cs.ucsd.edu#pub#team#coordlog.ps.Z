URL: ftp://ftp.cs.ucsd.edu/pub/team/coordlog.ps.Z
Refering-URL: http://www.cs.ucsd.edu/users/flaviu/publications.html
Root-URL: http://www.cs.ucsd.edu
Email: stamos@almaden.ibm.com flaviu@cs.ucsd.edu  
Title: Coordinator Log Transaction Execution Protocol  
Author: James W. Stamos Flaviu Cristian 
Date: April 9, 1993  
Address: San Jose, CA 95129-6099  
Affiliation: IBM Research Division Almaden Research Center  
Abstract: The coordinator log transaction execution protocol proposed in this paper centralizes logging on a per transaction basis and exploits piggybacking to provide the semantics of a distributed atomic commit at a minimal cost. The protocol eliminates two rounds of messages (one phase) from the presumed commit protocol and dramatically reduces the number of log forces needed for distributed atomic commit. We compare the coordinator log transaction execution protocol to existing protocols, explain when it is desirable, and discuss how it affects the write ahead log protocol and the database crash recovery algorithm. fl Author's current address: Department of Computer Science and Engineering, University of California, San Diego, La Jolla, CA 92093-0114 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal. </author> <title> A parallel logging algorithm for multiprocessor database machines. </title> <editor> In D. J. DeWitt and H. Boral, editors, </editor> <booktitle> Proceedings of the 4th International Workshop on Database Machines, </booktitle> <pages> pages 256-276, </pages> <month> March </month> <year> 1985. </year>
Reference-contexts: A prototype log server was built in support of the Camelot distributed transaction facility. 30 A DWAL protocol similar to the one we discussed was implemented as part of the Gamma database machine project [7]. The Gamma DWAL protocol, which is based on the DWAL protocol in <ref> [1] </ref>, is simpler than our DWAL protocol because each DM in Gamma uses only one remote log. Agrawal [1] describes a crash recovery algorithm for a database machine using parallel logging. <p> The Gamma DWAL protocol, which is based on the DWAL protocol in <ref> [1] </ref>, is simpler than our DWAL protocol because each DM in Gamma uses only one remote log. Agrawal [1] describes a crash recovery algorithm for a database machine using parallel logging. The algorithm does not require that the distributed logs be merged into a single log and performs the analysis phase in parallel. The algorithm, however, has several drawbacks.
Reference: [2] <author> J. F. Bartlett. </author> <title> A NonStop kernel. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 22-29, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: In such a configuration the system must ensure that, for each disk, there is a managing processor at any time <ref> [2] </ref>. Our advice for update-intensive transactions is as follows. If every server has electronic stable storage so that log forces are free, consider using early prepare. Otherwise, if either messages or log forces are expensive and the system supports high availability for log data, consider using coordinator log.
Reference: [3] <author> P. A. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: In this case the log abstraction is implemented by several logs rather than by a single log. 7 A Detailed Description This section discusses how to implement CL and assumes familiarity with a traditional log-based recovery algorithm (e.g., <ref> [3, 13] </ref>) and presumed commit [18]. We first consider transaction execution when no failures occur and then consider the handling of crashes. Finally, we discuss related work. <p> To simplify the discussion we assume that all other servers in the system function normally while the one DM crashes and entirely executes its recovery code. This assumption is relaxed in a later section. Our approach to DM crash recovery differs from the traditional recovery algorithms in <ref> [3, 13] </ref>, because our approach exploits the state of coordinators in addition to the contents of a DM's log records. We believe our approach is applicable to most log-based recovery algorithms.
Reference: [4] <author> H. Boral, W. Alexander, L. Clay, G. Copeland, S. Danforth, M. Franklin, B. Hart, M. Smith, and P. Valduriez. </author> <title> Prototyping Bubba, a highly parallel database system. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 4-24, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Several of these distributed database systems have been designed, prototyped, or even marketed <ref> [4, 7, 11, 16, 19] </ref>. Such multiprocessor systems can exploit inter-query parallelism for simple transactions and intra-query parallelism for long, complex queries by spreading data and processing across all processors in the system. A transaction that manipulates data stored on several processors is termed distributed.
Reference: [5] <author> F. Cristian. </author> <title> Understanding fault-tolerant distributed systems. </title> <journal> Commun. ACM, </journal> <volume> 34(2) </volume> <pages> 56-78, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: When the two servers open their next virtual circuit, all information about the previous virtual circuit between them is lost. The transaction execution protocols we discuss guarantee atomic commit in the presence of any number of server crash/performance failures and any number of virtual circuit crash failures <ref> [5] </ref>. Each server manages and stores a subset of the data items in the database called a database partition. We assume the database partitions are disjoint; otherwise, protocols different from the ones we discuss will be needed to maintain the consistency of replicated data.
Reference: [6] <author> D. S. Daniels, A. Z. Spector, and D. S. Thompson. </author> <title> Distributed logging for transaction processing. </title> <booktitle> In Proceedings of the 1987 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 82-96, </pages> <month> May </month> <year> 1987. </year> <month> 38 </month>
Reference-contexts: The only requirement is that the DMs in the new transactions must have acknowledged the coordinator's most recent crash. 7.5 Related Work In this section we survey related work on log servers, DWAL protocols, and crash recovery algorithms in parallel logging environments. Daniels et al. <ref> [6] </ref> discuss the cost, performance, survivability, and operational advantages of shared logging facilities for distributed transaction processing. Their design, which lets several log servers operate in parallel, can replicate log data across multiple servers to improve log availability.
Reference: [7] <author> D. J. DeWitt, S. Ghandeharizadeh, D. Schneider, A. Bricker, H.-I. Hsiao, and R. Rasmussen. </author> <title> The Gamma database machine project. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 2(1) </volume> <pages> 44-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Several of these distributed database systems have been designed, prototyped, or even marketed <ref> [4, 7, 11, 16, 19] </ref>. Such multiprocessor systems can exploit inter-query parallelism for simple transactions and intra-query parallelism for long, complex queries by spreading data and processing across all processors in the system. A transaction that manipulates data stored on several processors is termed distributed. <p> No other details of this optimized commit protocol are presented. A prototype log server was built in support of the Camelot distributed transaction facility. 30 A DWAL protocol similar to the one we discussed was implemented as part of the Gamma database machine project <ref> [7] </ref>. The Gamma DWAL protocol, which is based on the DWAL protocol in [1], is simpler than our DWAL protocol because each DM in Gamma uses only one remote log. Agrawal [1] describes a crash recovery algorithm for a database machine using parallel logging.
Reference: [8] <author> D. J. DeWitt, R. H. Katz, F. Olken, L. D. Shapiro, M. R. Stonebraker, and D. Wood. </author> <title> Implementation techniques for main memory database systems. </title> <booktitle> In Proceedings of the 1984 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 1-8, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: First, several messages with the same destination may be combined and sent as a single message by using piggybacking. Second, the CPU cost of a log force may be amortized across several transactions by using group commit <ref> [8, 12] </ref>.
Reference: [9] <author> D. Duchamp. </author> <title> Transaction Management. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> December </month> <year> 1988. </year> <note> Also available as CMU technical report CMU-CS-88-192. </note>
Reference-contexts: The rows labeled "log force delays" let different logs be forced in parallel whenever possible and contain the number of log forces that are serialized through the commit point. Once the commit point is reached, all locks may be released without incurring additional log forces <ref> [9, 10] </ref>. The rows labeled "message delays (commit)" contain the number of serialized messages through the commit 33 Table 2: Worst case overhead for committing an update transaction with D data managers.
Reference: [10] <author> D. Duchamp. </author> <title> Protocols for distributed and nested transactions. </title> <booktitle> In Proceedings of the Workshop on UNIX Transaction Processing, </booktitle> <pages> pages 45-53, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The rows labeled "log force delays" let different logs be forced in parallel whenever possible and contain the number of log forces that are serialized through the commit point. Once the commit point is reached, all locks may be released without incurring additional log forces <ref> [9, 10] </ref>. The rows labeled "message delays (commit)" contain the number of serialized messages through the commit 33 Table 2: Worst case overhead for committing an update transaction with D data managers.
Reference: [11] <author> S. Englert, J. Gray, T. Kocher, and P. Shah. </author> <title> A benchmark of NonStop SQL release 2 demonstrating near-linear speedup and scaleup on large databases. </title> <type> Technical Report 89.4, </type> <institution> Tandem Computer Inc., </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Several of these distributed database systems have been designed, prototyped, or even marketed <ref> [4, 7, 11, 16, 19] </ref>. Such multiprocessor systems can exploit inter-query parallelism for simple transactions and intra-query parallelism for long, complex queries by spreading data and processing across all processors in the system. A transaction that manipulates data stored on several processors is termed distributed.
Reference: [12] <author> D. Gawlick. </author> <title> Processing "hot spots" in high performance systems. </title> <booktitle> In Proceedings of Spring COMPCON, </booktitle> <pages> pages 249-251, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: First, several messages with the same destination may be combined and sent as a single message by using piggybacking. Second, the CPU cost of a log force may be amortized across several transactions by using group commit <ref> [8, 12] </ref>.
Reference: [13] <author> J. N. Gray. </author> <booktitle> Notes on Database Operating Systems, volume 60 of Lecture Notes in Computer Science, </booktitle> <pages> pages 393-481. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: The sequence of steps necessary to decide whether to commit or abort a distributed transaction at all processors involved in the transaction is called an atomic commit protocol and can require many messages. For example, the two-phase commit protocol <ref> [13, 15] </ref> requires four rounds of messages. For a distributed transaction involving N processors, each round in this protocol requires N 1 messages; therefore, committing this transaction with the two-phase commit protocol requires 4 (N 1) messages. <p> transaction execution protocols by comparing the number of log forces each requires, the reader should keep in mind that electronic stable storage is applicable to all the protocols. 3 Two-Phase Commit (2PC) For pedagogical purposes, we begin by describing the transaction execution protocol for the classical two-phase commit protocol (2PC) <ref> [13, 15] </ref>. Later sections show how various components of the transaction execution protocol may be safely omitted or overlapped. In 2PC, coordinators and DMs have private logs in stable storage. <p> In this case the log abstraction is implemented by several logs rather than by a single log. 7 A Detailed Description This section discusses how to implement CL and assumes familiarity with a traditional log-based recovery algorithm (e.g., <ref> [3, 13] </ref>) and presumed commit [18]. We first consider transaction execution when no failures occur and then consider the handling of crashes. Finally, we discuss related work. <p> Once the log record is forced to stable storage, the dirty disk page can be written to the disk. The requirement of writing the log record to stable storage ahead of the dirty disk page results in what is called the write ahead log protocol (WAL) <ref> [13] </ref>. Piggybacking a DM's log records on its reply to the coordinator is the major difference between CL and the other protocols. The log at each coordinator orders the log records for each DM in an order consistent with the order in which they were generated at that DM. <p> To simplify the discussion we assume that all other servers in the system function normally while the one DM crashes and entirely executes its recovery code. This assumption is relaxed in a later section. Our approach to DM crash recovery differs from the traditional recovery algorithms in <ref> [3, 13] </ref>, because our approach exploits the state of coordinators in addition to the contents of a DM's log records. We believe our approach is applicable to most log-based recovery algorithms.
Reference: [14] <author> R. P. King, N. Halim, H. Garcia-Molina, and C. A. Polyzois. </author> <title> Management of a remote backup copy for disaster recovery. </title> <journal> ACM Trans. Database Syst., </journal> <volume> 16(2) </volume> <pages> 338-368, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Our crash recovery algorithms make no assumptions about lock granularity, commit numbers, or style of logging. In addition, they support partial failures and have parallel redo and undo phases when multiple DMs recover. King et al. <ref> [14] </ref> describe disaster recovery algorithms that send redo log records and transaction read sets from a primary database to a remote backup database. For each transaction, the primary database uses a two-phase commit protocol to decide whether to commit or abort the transaction.
Reference: [15] <author> B. W. Lampson and H. E. Sturgis. </author> <title> Crash recovery in a distributed data storage system. </title> <type> Technical report, </type> <institution> Xerox Palo Alto Research Center, </institution> <year> 1976. </year> <month> 39 </month>
Reference-contexts: The sequence of steps necessary to decide whether to commit or abort a distributed transaction at all processors involved in the transaction is called an atomic commit protocol and can require many messages. For example, the two-phase commit protocol <ref> [13, 15] </ref> requires four rounds of messages. For a distributed transaction involving N processors, each round in this protocol requires N 1 messages; therefore, committing this transaction with the two-phase commit protocol requires 4 (N 1) messages. <p> Each server in the system has volatile storage (main memory) and persistent storage (disks) and is hosted by a processor with one or more CPUs. A server can use persistent storage and redundancy to create stable storage <ref> [15] </ref>, which is both persistent and reliable. Servers do not share storage and communicate only by sending messages via virtual circuits (or pipes). Once a virtual circuit 1 is established between two servers, it delivers messages between them in FIFO order. <p> transaction execution protocols by comparing the number of log forces each requires, the reader should keep in mind that electronic stable storage is applicable to all the protocols. 3 Two-Phase Commit (2PC) For pedagogical purposes, we begin by describing the transaction execution protocol for the classical two-phase commit protocol (2PC) <ref> [13, 15] </ref>. Later sections show how various components of the transaction execution protocol may be safely omitted or overlapped. In 2PC, coordinators and DMs have private logs in stable storage.
Reference: [16] <author> R. A. Lorie, J.-J. Daudenarde, J. W. Stamos, and H. C. Young. </author> <title> Exploiting database parallelism in a message-passing multiprocessor. </title> <journal> IBM Journal of Research and Development, </journal> 35(5/6):681-695, September/November 1991. 
Reference-contexts: Several of these distributed database systems have been designed, prototyped, or even marketed <ref> [4, 7, 11, 16, 19] </ref>. Such multiprocessor systems can exploit inter-query parallelism for simple transactions and intra-query parallelism for long, complex queries by spreading data and processing across all processors in the system. A transaction that manipulates data stored on several processors is termed distributed.
Reference: [17] <author> C. Mohan, D. Haderle, B. Lindsay, H. Pirahesh, and P. Schwarz. </author> <title> ARIES: A transaction recovery method supporting fine-granularity locking and partial rollbacks using write-ahead logging. </title> <journal> ACM Trans. Database Syst., </journal> <volume> 17(1) </volume> <pages> 94-162, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We believe our approach is applicable to most log-based recovery algorithms. For concreteness, we describe a DM recovery algorithm based on the algorithm in <ref> [17] </ref>, which in turn is based on the following traditional approach: 1. Analysis Phase: determine the set of transactions that were incomplete at the time of the crash; 2. Redo Phase: reestablish the state of the database at the time of the crash; and 3. <p> To implement these semantics, the recovery algorithm in <ref> [17] </ref> labels each page in the database with the LSN of the log record corresponding to the last update to that page. The redo phase ignores a log record if its LSN is less than or equal to the LSN of the page it is supposed to update. <p> The recovery-LSN for a page in the disk buffer is the LSN of the first log record generated after the page was fetched from disk or flushed back to disk <ref> [17] </ref>. All updates applied to the buffer page that may not yet be reflected in the persistent version of the page have log records whose LSNs are at least as large as the recovery-LSN.
Reference: [18] <author> C. Mohan, B. Lindsay, and R. Obermarck. </author> <title> Transaction management in the R fl distributed database management system. </title> <journal> ACM Trans. Database Syst., </journal> <volume> 11(4) </volume> <pages> 378-396, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Messages are often expensive and can limit transaction throughput, both for individual processors and for the system as a whole. This expense has led researchers to investigate optimized commit 2 protocols. The presumed commit protocol <ref> [18] </ref>, for example, can eliminate one of the message rounds needed by the classical two-phase commit protocol. Larger reductions in the number of message rounds can be realized by considering transaction execution and the commit protocol together. <p> The reader should resist the temptation to optimize a protocol solely for the worst-case update transaction we described above. Following <ref> [18] </ref>, we characterize the performance of each transaction execution protocol by the maximum number of messages and the maximum number of log forces required to execute and commit a given transaction in the absence of failures. <p> Note that two-thirds of the messages are involved with 2PC, while only one-third of the messages accomplish real work (that is, data item accesses) in this example. 10 4 Presumed Commit (PC) In the absence of failures, the presumed commit protocol (PC) <ref> [18] </ref> eliminates the last round of messages from 2PC. The presumed commit protocol also nearly halves the number of log forces for a distributed transaction compared to 2PC. We sketch the details of the protocol after discussing its basic premise. <p> In this case, during crash recovery the coordinator notices the undecided transaction, aborts the transaction, and uses the membership 3 This log record was called a collecting record in <ref> [18] </ref>. 11 record to determine which DMs it must notify about the abort. The transaction execution protocol based on PC is as follows: 1. C sends a work request to each DM. (D messages) 2. Each DM replies after executing its work request. (D messages) 3. <p> Each DM appends to its log (but need not force) a commit log record and then forgets about the transaction. In the absence of failures, the entire execution requires only 5D messages and D+2 log forces. We shall not consider the presumed abort protocol <ref> [18] </ref> because its performance is not as good as the performance of PC for the update-intensive transactions we are considering. <p> one more round of messages compared to EP, but one less round compared to PC. 6 Coordinator Log When several servers participating in a transaction share a common log, some of the log forces of existing protocols may be converted into append operations without affecting the correctness of the protocol <ref> [18] </ref>. In this section we extend this optimization to its ultimate limit. 14 In all the transaction execution protocols we have discussed so far, the log records for a single transaction have been distributed among servers in the following fashion. <p> In this case the log abstraction is implemented by several logs rather than by a single log. 7 A Detailed Description This section discusses how to implement CL and assumes familiarity with a traditional log-based recovery algorithm (e.g., [3, 13]) and presumed commit <ref> [18] </ref>. We first consider transaction execution when no failures occur and then consider the handling of crashes. Finally, we discuss related work. The algorithms we present for CL tolerate any number of server crash/performance failures as well as any number of virtual circuit crashes, including network partitioning.
Reference: [19] <author> P. M. Neches. </author> <title> The anatomy of a data base computer system. </title> <booktitle> In Proceedings of the 2nd International Conference on Supercomputing: </booktitle> <volume> Volume I, </volume> <pages> pages 102-104. </pages> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Several of these distributed database systems have been designed, prototyped, or even marketed <ref> [4, 7, 11, 16, 19] </ref>. Such multiprocessor systems can exploit inter-query parallelism for simple transactions and intra-query parallelism for long, complex queries by spreading data and processing across all processors in the system. A transaction that manipulates data stored on several processors is termed distributed.
Reference: [20] <author> B. M. Oki, B. H. Liskov, and R. W. Scheifler. </author> <title> Reliable object storage to support atomic actions. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-159, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: The coordinator C is yet another server. In the absence of failures, an execution that uses 2PC has at least six message rounds: 2 Systems that perform a substantial amount of additional work can often use spare processing cycles to do some of this work in advance <ref> [20] </ref>. 9 1. C sends a work request to each DM. (D messages) 2. Each DM replies after executing its work request. (D messages) 3. C sends a prepare message to each DM. (D messages) 4.
Reference: [21] <author> R. J. Peterson and J. P. Strickland. </author> <title> Log write-ahead protocols and IMS/VS logging. </title> <booktitle> In Proceedings of the 2nd ACM SIGACT-SIGMOD Symposium on Principles of Database Systems, </booktitle> <pages> pages 216-243, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: Third, the average rotational delay associated with a log force may be reduced from one-half of a rotation to one-half of a disk page by dedicating (part of) a disk to the log tail and writing the log tail to any disk page on a reserved track <ref> [21, 24] </ref>. We do not assume the existence of special-purpose hardware when describing transaction execution protocols, because special-purpose hardware used by one protocol could also be used for other protocols. For instance, the log tail could be kept in electronic stable storage rather than traditional volatile memory.
Reference: [22] <author> J. W. Stamos and F. Cristian. </author> <title> A low-cost atomic commit protocol. </title> <booktitle> In Proceedings of the 9th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 66-75. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: The new protocol, the coordinator log transaction execution protocol, was first presented in a preliminary version of this paper <ref> [22] </ref>. The full paper has three additional contributions: the details of the recovery algorithm for a database server; the details of the recovery algorithm for a log server; and a discussion of recovery from multiple failures.
Reference: [23] <author> M. Stonebraker. </author> <title> Concurrency control and consistency of multiple copies of data in distributed ingres. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> SE-5(3):188-194, </volume> <month> May </month> <year> 1979. </year> <month> 40 </month>
Reference-contexts: One way to reduce communication further is to have each DM 12 enter the prepared state after it performs its work and before it replies to the coordinator <ref> [23] </ref>. 4 Combining this idea with the presumed commit idea leads to what we call the early prepare protocol (EP). As shown below, the early prepare protocol overlaps the first phase of PC with the messages that perform real work.
Reference: [24] <author> J. P. Strickland, P. P. Uhrowczik, and V. L. Watts. IMS/VS: </author> <title> An evolving system. </title> <journal> IBM Systems Journal, </journal> <volume> 21(4) </volume> <pages> 490-510, </pages> <year> 1982. </year>
Reference-contexts: Third, the average rotational delay associated with a log force may be reduced from one-half of a rotation to one-half of a disk page by dedicating (part of) a disk to the log tail and writing the log tail to any disk page on a reserved track <ref> [21, 24] </ref>. We do not assume the existence of special-purpose hardware when describing transaction execution protocols, because special-purpose hardware used by one protocol could also be used for other protocols. For instance, the log tail could be kept in electronic stable storage rather than traditional volatile memory.
Reference: [25] <author> S. A. Yemini, R. E. Strom, and D. F. Bacon. </author> <title> Improving distributed protocols by decoupling recovery from concurrency control. </title> <type> Research Report RC 13326, </type> <institution> IBM Thomas J. Watson Research Center, </institution> <address> Yorktown Heights, New York, </address> <month> December </month> <year> 1987. </year> <month> 41 </month>
Reference-contexts: A DM using an intermediate approach could reply to the coordinator after executing its work request, force the prepare record asynchronously, and notify the coordinator once the record has been forced <ref> [25] </ref>. This asynchronous approach reduces the number of synchronous log forces and may increase parallelism.
References-found: 25

