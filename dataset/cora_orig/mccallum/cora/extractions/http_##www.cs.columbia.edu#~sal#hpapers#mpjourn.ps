URL: http://www.cs.columbia.edu/~sal/hpapers/mpjourn.ps
Refering-URL: http://www.cs.columbia.edu/~sal/merge-purge.html
Root-URL: http://www.cs.columbia.edu
Email: mauricio@cs.columbia.edu sal@cs.columbia.edu  
Title: A Generalization of Band Joins and The Merge/Purge Problem  
Author: Mauricio A. Hernandez Salvatore J. Stolfo 
Keyword: band joins, duplicate elimination, instance identification, multi database systems, semantic integration  
Note: This work has been supported in part by the New York State Science and Technology Foundation through the Center for Advanced Technology in Telecommunications at Polytechnic University, by NSF under grant IRI-94-13847, and by Citicorp. This author's work was supported by an AT&T Cooperative Research Program Fellowship.  
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: The problem of merging multiple databases of information about common entities is frequently encountered in large commercial and government organizations. The problem we study is often called the Merge/Purge problem and is difficult to solve both in scale and accuracy. Large repositories of data always have numerous duplicate information entries about the same entities that are difficult to cull together without an intelligent "equational theory" that identifies equivalent items by a complex, domain-dependent matching process. We have developed a system for accomplishing this task for lists of names of potential customers in a direct marketing-type application. Our results for statistically generated data are shown to be accurate and effective when processing the data multiple times using different keys for sorting. The system provides a rule programming module that is easy to program and quite good at finding duplicates especially in an environment with massive amounts of data. This paper details improvements in our system, and reports on the successful implementation for a "real-world" database that conclusively validates our results previously achieved for statistically generated data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> ACM. </editor> <booktitle> SIGMOD record, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Merging large databases acquired from different sources with heterogenous representations of information has become an increasingly important and difficult problem for many organizations. Instances of this problem appearing in the literature have been called the semantic integration problem <ref> [ 1 ] </ref> or the instance identification problem [ 16 ] . In this paper we consider the problem over very large databases of information that need to be processed as quickly, efficiently, and accurately as possible.
Reference: [2] <author> R. Agrawal and H. V. Jagadish. </author> <title> Multiprocessor Transitive Closure Algorithms. </title> <booktitle> In Proc. Int'l Symp. on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 56-66, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: The more corrupted the data, more runs might be needed to capture the matching records. The transitive closure, however, is executed on pairs of tuple id's, each at most 30 bits, and fast solutions to compute transitive closure exist <ref> [ 2 ] </ref> . From observing real world scenarios, the size of the data set over which the closure is computed is at least one order of magnitude smaller than the corresponding database of records, and thus does not contribute a large cost. <p> The results of the three independent runs were then processed with a transitive closure phase to improve the accuracy of the results. The size of the window for both the "basic" SNM and the DE-SNM varied in the range <ref> [2; 10] </ref>. The size of the "small" window for the special first window scan phase of the DE-SNM is = 3. The average results of the three experiments are shown in figure 4.
Reference: [3] <author> C. Batini, M. Lenzerini, and S. Navathe. </author> <title> A Comparative Analysis of Methodologies for Database Schema Integration. </title> <journal> ACM Computing Surverys, </journal> <volume> 18(4) </volume> <pages> 323-364, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: The first issue, where databases have different schema, has been addressed extensively in the literature and is known as the schema integration problem <ref> [ 3 ] </ref> . We are primarily interested in the second problem: heterogeneous 1 representations of data and its implication when merging or joining multiple datasets. Another simple way to find duplicates among two relational databases R and S is to compute the equijoin R 1 S.
Reference: [4] <author> D. Bitton and D. J. DeWitt. </author> <title> Duplicate Record Elimination in Large Data Files. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(2) </volume> <pages> 255-265, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: We are however only interested in sorting algorithms in which duplicates are removed from the final sorted database. To date, the most relevant work in this area is <ref> [ 4 ] </ref> . Finally, the proposed solution to the merge/purge problem resembles a sort-merge join [ 8 ] in which the join condition is a user-defined equivalence function. Of particular relevance to the merge/purge solution proposed here is the work on "band-joins" by [ 6 ] .
Reference: [5] <author> K. W. Church and W. A. Gale. </author> <title> Probability scoring for spelling correction. </title> <journal> Statistics and Computing, </journal> <volume> 1 </volume> <pages> 93-103, </pages> <year> 1991. </year> <month> 42 </month>
Reference-contexts: The errors introduced in the duplicate records range from small typographical mistakes, to complete change of last names and addresses. When setting the parameters for typographical errors, we used known frequencies from studies in spelling correction algorithms <ref> [ 15; 5; 12 ] </ref> . For this study, the generator selected from 10% to 50% of the generated records for duplication with errors, where the error in the spelling of words, names and cities was controlled according to these published statistics found for common real world datasets.
Reference: [6] <author> D. J. DeWitt, J. F. Naughton, and D. A. Schneider. </author> <title> An Evaluation of Non-Equijoin Algorithms. </title> <booktitle> In Proc. 17th Int'l. Conf. on Very Large Databases, </booktitle> <pages> pages 443-452, </pages> <address> Barcelona, Spain, </address> <year> 1991. </year>
Reference-contexts: When the "errors" in the data are not severe, we might ideally expect to find the matching instance of a tuple in R within a sorted "band" of tuples in S. This type of non-equijoin joins are called band-joins and have been studied by <ref> [ 6 ] </ref> . <p> Finally, the proposed solution to the merge/purge problem resembles a sort-merge join [ 8 ] in which the join condition is a user-defined equivalence function. Of particular relevance to the merge/purge solution proposed here is the work on "band-joins" by <ref> [ 6 ] </ref> . Here we briefly describe work on band-joins since that work is closest to the solutions of merge/purge we have developed. 2.1 Band-Joins As we mentioned, merge/purge can be thought as a relational join among two or more tables using a special function that determines equality. <p> Thus, we can define as (S:temp 5 R:temp and R:temp S:temp+5). Algorithms for executing this kind of non-equijoin predicate have been presented by <ref> [ 6 ] </ref> . Joins in which the join-predicate has the form R:A c 1 S:B R:A + c 2 , are called band-joins. The paper presents a new algorithm termed a partitioned band join to evaluate these special type of joins.
Reference: [7] <author> C. L. Forgy. </author> <title> OPS5 User's Manual. </title> <type> Technical Report CMU-CS-81-135, </type> <institution> Carnegie Mellon University, </institution> <month> July </month> <year> 1981. </year>
Reference-contexts: The results displayed in section 4 are based upon edit distance computation since the outcome of the program did not vary much among the different distance functions for the particular databases used in our study. For the purpose of experimental study, we wrote an OPS5 <ref> [ 7 ] </ref> rule program consisting of 26 rules for this particular domain of employee records and was tested repeatedly over relatively small databases of records.
Reference: [8] <author> L. Gotlieb. </author> <title> Computing joins of relations. </title> <booktitle> In Proceedings of the 1975 ACM SIGMOD Conference, </booktitle> <year> 1975. </year>
Reference-contexts: We are however only interested in sorting algorithms in which duplicates are removed from the final sorted database. To date, the most relevant work in this area is [ 4 ] . Finally, the proposed solution to the merge/purge problem resembles a sort-merge join <ref> [ 8 ] </ref> in which the join condition is a user-defined equivalence function. Of particular relevance to the merge/purge solution proposed here is the work on "band-joins" by [ 6 ] .
Reference: [9] <author> M. Hernandez and S. Stolfo. </author> <title> The Merge/Purge Problem for Large Databases. </title> <booktitle> In Proceedings of the 1995 ACM-SIGMOD Conference, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Elsewhere we have treated the case of clustering in which sorting is replaced by a single-scan process <ref> [ 9 ] </ref> . However, we demonstrate that, as one may expect, none of these basic approaches alone can guarantee high accuracy. Here, accuracy means how many of the actual duplicates appearing in the data have been matched and merged correctly. <p> The moral is simply that several distinct "cheap" passes over the data produces more accurate results than one "expensive" pass over the data. In the following sections of this paper, we detail the sorted neighborhood and multi 4 pass methods previously reported in <ref> [ 9 ] </ref> . They are repeated here for completeness of our exposition and in order to set the stage for new improvements to these basic methods. We also previously published alternative algorithms that were implemented for the fundamental merge process and comparatively evaluated their accuracy over statistically generated databases. <p> Note, each disjoint subset has reduced the complexity simply because the size of the data set is reduced. This method was presented in <ref> [ 9 ] </ref> and will be discussed no further in this paper. <p> Depending upon the complexity of the rule program and window size w, the last pass may indeed be the dominant cost. (We introduced elsewhere <ref> [ 9 ] </ref> the means of speeding up this phase by processing "parallel windows" in the sorted list.) We note with interest that the sorts of optimizations detailed in the AlphaSort paper [ 13 ] may of course be fruitfully applied here. <p> But note we pay a heavy price due to the number of sorts or clusterings of the original large data set. We presented some parallel implementation alternatives to reduce this price in <ref> [ 9 ] </ref> . 4.3 The Duplicate Elimination Method (DE-SNM) We ran a series of experiments to evaluate the performance of the DE-SNM and compared it to the performance of the "basic" counterpart. <p> Moreover, no single-pass run reaches an accuracy of more than 93% until W &gt; 7000, at which point (not shown in figure 6 (a)) their execution time are over 4,800 seconds (80 minutes). In <ref> [ 9 ] </ref> we detailed the analysis of these techniques when the process is I/O bound.
Reference: [10] <author> W. Kent. </author> <title> The Breakdown of the Information Model in Multi-Database Systems. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 20(4) </volume> <pages> 10-15, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The results on the real-world data validate our previous predictions as being quite accurate. 2 Previous Work Several lines of work in the database community have bearing on efficient solutions for the merge/purge problem. The semantic integration problem <ref> [ 10 ] </ref> seeks to identify a multiplicity of database objects that represent the same or related real-world entity, even though their database representations are different. This problem has been studied by researchers in the heterogenous multi-database community. <p> The results of the three independent runs were then processed with a transitive closure phase to improve the accuracy of the results. The size of the window for both the "basic" SNM and the DE-SNM varied in the range <ref> [2; 10] </ref>. The size of the "small" window for the special first window scan phase of the DE-SNM is = 3. The average results of the three experiments are shown in figure 4.
Reference: [11] <author> D. Knuth. </author> <title> The Art of Computer Programming: Sorting and Searching (Volume 3). </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Sorting data is probably the oldest studied problem in Computer Science and many different algorithms have been 5 presented over the years <ref> [ 11 ] </ref> . We are however only interested in sorting algorithms in which duplicates are removed from the final sorted database. To date, the most relevant work in this area is [ 4 ] .
Reference: [12] <author> K. Kukich. </author> <title> Techniques for Automatically Correcting Words in Text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439, </pages> <year> 1992. </year>
Reference-contexts: The errors introduced in the duplicate records range from small typographical mistakes, to complete change of last names and addresses. When setting the parameters for typographical errors, we used known frequencies from studies in spelling correction algorithms <ref> [ 15; 5; 12 ] </ref> . For this study, the generator selected from 10% to 50% of the generated records for duplication with errors, where the error in the spelling of words, names and cities was controlled according to these published statistics found for common real world datasets.
Reference: [13] <author> C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: </author> <title> A RISC Machine Sort. </title> <booktitle> In Proceedings of the 1994 ACM-SIGMOD Conference, </booktitle> <pages> pages 233-242, </pages> <year> 1994. </year>
Reference-contexts: In this case, at least three passes would be needed, one pass for conditioning the data and preparing keys, at least a second pass, likely more, for a high speed sort like, for example, the AlphaSort <ref> [ 13 ] </ref> , and a final pass for window processing and application of the rule program for each record entering the sliding window. <p> rule program and window size w, the last pass may indeed be the dominant cost. (We introduced elsewhere [ 9 ] the means of speeding up this phase by processing "parallel windows" in the sorted list.) We note with interest that the sorts of optimizations detailed in the AlphaSort paper <ref> [ 13 ] </ref> may of course be fruitfully applied here. We are more concerned with alternative process 10 architectures that lead to higher accuracies in the computed results while also reducing the time complexity. <p> Thus, we consider alternative metrics for the purposes of merge/purge to include how accurately can you merge/purge for a fixed dollar and given time constraint, rather than the specific cost- and time-based metrics proposed in <ref> [ 13 ] </ref> . 3.2 Selection of Keys The effectiveness of the sorted-neighborhood method highly depends on the key selected to sort the records. Here a key is defined to be a sequence of a subset of attributes, or substrings within the attributes, chosen from the record.
Reference: [14] <author> G. Paitetsky-Shapiro. </author> <note> KDD Nuggets. In http://info.gte.com/ kdd/nuggets/95/, 1995. Volume 95:7. 43 </note>
Reference-contexts: The State of Washington Department of Social and Health Services maintains large databases of transactions made over the years with state residents. In March of 1995 the Office of Children Administrative Research (OCAR) of the Department of Social and Health Services posted a request on the KDD-nuggets <ref> [ 14 ] </ref> asking for assistance analyzing one of their databases. We answered their request and this section details our results. OCAR analyzes the database of payments by the State to families and businesses that 33 provide services to needy children.
Reference: [15] <author> J. J. Pollock and A. Zamora. </author> <title> Automatic spelling correction in scientific and scholarly text. </title> <journal> ACM Computing Surveys, </journal> <volume> 27(4) </volume> <pages> 358-368, </pages> <year> 1987. </year>
Reference-contexts: The errors introduced in the duplicate records range from small typographical mistakes, to complete change of last names and addresses. When setting the parameters for typographical errors, we used known frequencies from studies in spelling correction algorithms <ref> [ 15; 5; 12 ] </ref> . For this study, the generator selected from 10% to 50% of the generated records for duplication with errors, where the error in the spelling of words, names and cities was controlled according to these published statistics found for common real world datasets.
Reference: [16] <author> Y. R. Wang and S. E. Madnick. </author> <title> The Inter-Database Instance Identification Problem in Integrating Autonomous Systems. </title> <booktitle> In Proceedings of the Sixth International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1989. </year> <month> 44 </month>
Reference-contexts: 1 Introduction Merging large databases acquired from different sources with heterogenous representations of information has become an increasingly important and difficult problem for many organizations. Instances of this problem appearing in the literature have been called the semantic integration problem [ 1 ] or the instance identification problem <ref> [ 16 ] </ref> . In this paper we consider the problem over very large databases of information that need to be processed as quickly, efficiently, and accurately as possible. For instance, one month is a typical business cycle in certain direct marketing operations.
References-found: 16

