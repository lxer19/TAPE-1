URL: http://www.cs.jhu.edu/grad/murthy/ijcai95.ps.gz
Refering-URL: http://www.cs.jhu.edu/grad/murthy/home.html
Root-URL: http://www.cs.jhu.edu
Email: murthy@cs.jhu.edu  salzberg@cs.jhu.edu  
Title: Lookahead and Pathology in Decision Tree Induction  
Author: Sreerama Murthy U. S. A. Steven Salzberg U. S. A. 
Note: 21218.  21218.  
Address: Baltimore, MD  Baltimore, MD  
Affiliation: Department of Computer Science Johns Hopkins University  Department of Computer Science Johns Hopkins University  
Abstract: The standard approach to decision tree induction is a top-down, greedy algorithm that makes locally optimal, irrevocable decisions at each node of a tree. In this paper, we empirically study an alternative approach, in which the algorithms use one-level lookahead to decide what test to use at a node. We systematically compare, using a very large number of real and artificial data sets, the quality of decision trees induced by the greedy approach to that of trees induced using lookahead. The main observations from our experiments are: (i) the greedy approach consistently produced trees that were just as accurate as trees produced with the much more expensive lookahead step; and (ii) we observed many instances of pathology, i.e., lookahead produced trees that were both larger and less accurate than trees produced without it. 
Abstract-found: 1
Intro-found: 1
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J. Friedman, R. Ol-shen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth Intl. Group, </booktitle> <year> 1984. </year>
Reference-contexts: This is T's goodness. 3. Execute steps 3,4 of GREEDY. We experimented with two pre-defined goodness measures, namely, the Gini index of diversity <ref> [ Breiman et al., 1984 ] </ref> and information gain [ Quinlan, 1986 ] . 1 This gave us four algorithms for our experiments, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [ Breiman et al., 1984 ] and Greedy-Info to the <p> pre-defined goodness measures, namely, the Gini index of diversity <ref> [ Breiman et al., 1984 ] </ref> and information gain [ Quinlan, 1986 ] . 1 This gave us four algorithms for our experiments, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [ Breiman et al., 1984 ] and Greedy-Info to the ID3 algorithm [ Quinlan, 1986 ] . <p> We augmented our algorithms (Greedy-Gini, Look-Gini, Greedy-Info and Look-Info) with pruning for these experiments, using cost complexity pruning with the one standard error rule <ref> [ Breiman et al., 1984 ] </ref> , reserving 10% of the training data as the pruning set. All results for real world data are averages of ten 5-fold cross validation experiments. The choice of the domains is important.
Reference: [ Hartmann et al., 1982 ] <author> C.R.P. Hartmann, P.K. Varsh-ney, K.G. Mehrotra, and C.L. Gerberich. </author> <title> Application of information theory to the construction of efficient decision trees. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> IT-28(4):565-577, </volume> <year> 1982. </year>
Reference-contexts: The tree sizes with and without lookahead, and with and without pruning are shown for information gain. programming and branch-and-bound methods to produce optimal trees. Hartmann et al. <ref> [ Hartmann et al., 1982 ] </ref> describe Generalized Optimum Testing Algorithm (GOTA), an algorithm based on an information theoretic criterion between branching levels in a tree. With the appropriate parameter settings, GOTA can do fixed-depth lookahead, different depths of lookahead at different branching levels or even exhaustive search.
Reference: [ Holte, 1993 ] <author> R. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 63-90, </pages> <year> 1993. </year>
Reference-contexts: The choice of the domains is important. If a greedy method can induce a highly accurate, concise classifier for a domain (e.g., the well-known Iris data), lookahead is not likely to produce significant benefits. We used a survey of results <ref> [ Holte, 1993 ] </ref> to choose six "difficult" domains for our experiments domains for which the best 2 2 1 1 2 2 2 1 1 2 1 2 2 2 1 1 1 2 2 1 2 2 2 1 1 1 2 1 1 2 1 2 2 1 <p> In addition to these domains, we experimented with two variants (V0 and V1) of the congressional voting records data. V0 is used by Nor-ton [ Norton, 1989 ] for his lookahead experiments. The V1 data <ref> [ Holte, 1993 ] </ref> is identical to the VO data, except that the "best" attribute, physician-fee freeze, is removed. All the data sets were taken from the UCI Machine Learning repository [ Murphy and Aha, 1994 ] . <p> <ref> [ Holte, 1993 ] </ref> is identical to the VO data, except that the "best" attribute, physician-fee freeze, is removed. All the data sets were taken from the UCI Machine Learning repository [ Murphy and Aha, 1994 ] . Our abbreviations for the data sets are consistent with those of Holte [ Holte, 1993 ] . All experimental results reported in this section were obtained with information gain. Results with Gini index look very similar, and are omitted for space considerations. Figures 7 and 8 summarize the results for accuracy and tree size respectively.
Reference: [ Hyafil and Rivest, 1976 ] <author> L. Hyafil and R.L. Rivest. </author> <title> Constructing optimal binary decision trees is NP-complete. </title> <journal> Info. Proc. Letters, </journal> <volume> 5(1) </volume> <pages> 15-17, </pages> <year> 1976. </year>
Reference-contexts: Finally, both of the goodness measures we used (Gini index and information gain) exhibited pathology on the real-world domains also. 5 Discussion Several versions of the optimal decision tree induction problem are known to be NP-Complete <ref> [ Hyafil and Rivest, 1976; Murphy and McCraw, 1991 ] </ref> . As a result, virtually all implemented decision tree systems use a heuristic, greedy approach. There have been, however, some exceptions to this rule.
Reference: [ Moret, 1982 ] <author> B.M.E. Moret. </author> <title> Decision trees and diagrams. </title> <journal> Computing Surveys, </journal> <volume> 14(4) </volume> <pages> 593-623, </pages> <year> 1982. </year>
Reference-contexts: As a result, virtually all implemented decision tree systems use a heuristic, greedy approach. There have been, however, some exceptions to this rule. Moret <ref> [ Moret, 1982 ] </ref> surveys early induction systems that used dynamic 4 Note that all of our "difficult" data sets happen to be quite small, probably inherently inadequate for learning. The experiments with real data are given only to substantiate the earlier observations on the artificial data.
Reference: [ Murphy and Aha, 1994 ] <author> P.M. Murphy and D. Aha. </author> <title> UCI repository of machine learning databases. </title> <note> FTP from ics.uci.edu in the directory pub/machine-learning-databases, </note> <year> 1994. </year>
Reference-contexts: We compare greedily induced trees with those induced with one-level lookahead, using two large classes of synthetic data and eight real-world data sets from the UCI machine learning repository <ref> [ Murphy and Aha, 1994 ] </ref> . The results suggest that: * Limited lookahead search does not produce significantly better decision trees. <p> V0 is used by Nor-ton [ Norton, 1989 ] for his lookahead experiments. The V1 data [ Holte, 1993 ] is identical to the VO data, except that the "best" attribute, physician-fee freeze, is removed. All the data sets were taken from the UCI Machine Learning repository <ref> [ Murphy and Aha, 1994 ] </ref> . Our abbreviations for the data sets are consistent with those of Holte [ Holte, 1993 ] . All experimental results reported in this section were obtained with information gain. Results with Gini index look very similar, and are omitted for space considerations.
Reference: [ Murphy and McCraw, 1991 ] <author> O.J. Murphy and R.L. McCraw. </author> <title> Designing storage efficient decision trees. </title> <journal> IEEE Trans. on Comp., </journal> <volume> 40(3) </volume> <pages> 315-319, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Finally, both of the goodness measures we used (Gini index and information gain) exhibited pathology on the real-world domains also. 5 Discussion Several versions of the optimal decision tree induction problem are known to be NP-Complete <ref> [ Hyafil and Rivest, 1976; Murphy and McCraw, 1991 ] </ref> . As a result, virtually all implemented decision tree systems use a heuristic, greedy approach. There have been, however, some exceptions to this rule.
Reference: [ Murphy and Pazzani, 1994 ] <author> P.M. Murphy and M.J. Pazzani. </author> <title> Exploring the decision forest: An empirical investigation of Occam's Razor in decision tree induction. J. of Art. Intel. </title> <journal> Res., </journal> <volume> 1 </volume> <pages> 257-275, </pages> <year> 1994. </year>
Reference-contexts: Experiments with other goodness measures may be interesting, but we suspect the results would be similar. 2 This style of empirical investigation is made possible by the existence of extremely fast, inexpensive computers. See <ref> [ Murphy and Pazzani, 1994 ] </ref> for another example of this style. 10 10 Root Left Right 1 1 2 0 Figure 1: Class C consists of all balanced decision trees on a 10 X 10 grid such that each tree has three test (internal) nodes and all test nodes are
Reference: [ Murthy and Salzberg, 1995 ] <author> S.K. Murthy and S. Salzberg. </author> <title> Decision tree induction: How effective is the greedy heuristic? In 1st Intl. </title> <booktitle> Conf. on Knowledge Discovery in Databases, </booktitle> <address> Montreal, Canada, </address> <year> 1995. </year>
Reference-contexts: The expected depth of a greedily induced decision tree has been observed to be very close to that of the optimal tree <ref> [ Murthy and Salzberg, 1995 ] </ref> . 0 2000 4000 -2 -1 0 1 2 3 4 # Affected Trees Improvement Due To Lookahead accuracy size depth with information gain, for class C.
Reference: [ Mutchler, 1993 ] <author> D. Mutchler. </author> <title> The multi-player version of minimax displays game pathology. </title> <journal> Artificial Intelligence, </journal> <volume> 64(2) </volume> <pages> 323-336, </pages> <year> 1993. </year>
Reference-contexts: Second, it is interesting to note that lookahead actually hurts accuracy in almost as many trees as those in which it enhances accuracy. This property, where lookahead search finds inferior solutions, is known as pathology in the context of game trees <ref> [ Nau, 1983; Mutchler, 1993 ] </ref> . We discuss pathology for decision trees further in Section 3.2, where this trend is exhibited more prominently. Pathology cannot occur for tree size or depth for class C, because one-level lookahead is equivalent to exhaustive search. <p> However, it has been observed that for some games, deeper search can actually produce an inferior program, both with two players [ Nau, 1983 ] and with multiple players <ref> [ Mutchler, 1993 ] </ref> . Decision trees, one can argue, are analogous to a one-player game tree. Our discovery that deeper search can lead to inferior decision trees thus extends the earlier pathology results to a new domain.
Reference: [ Nakamura et al., 1993 ] <author> Y. Nakamura, S. Abe, Y. Oh-sawa, and M. Sakauchi. </author> <title> A balanced hierarchical data structure for multidimensional data with highly efficient dynamic characteristics. </title> <journal> IEEE Trans. on Knowledge and Data Engg., </journal> <volume> 5(4) </volume> <pages> 682-694, </pages> <year> 1993. </year>
Reference-contexts: Although little work has been done on balancing decision trees, a great deal of research has considered balanced search trees (e.g.: <ref> [ Nakamura et al., 1993 ] </ref> ). Roughly speaking, this literature deals with techniques to restructure search trees when elements are inserted or deleted, in order to restrict the depth of these trees to a logarithmic function of the number of search keys.
Reference: [ Nau, 1983 ] <author> D.S. Nau. </author> <title> Decision quality as a function of search depth on game trees. </title> <journal> J. of the ACM, </journal> <volume> 30(4) </volume> <pages> 687-708, </pages> <year> 1983. </year>
Reference-contexts: On average, it produces trees with approximately the same classification ac curacy and size as greedy induction. * Limited lookahead search produces inferior decision trees in a significant number of cases; i.e., decision tree induction exhibits the same pathology that has been observed in game trees <ref> [ Nau, 1983 ] </ref> . * Tree post-processing techniques such as pruning are at least as beneficial as limited lookahead for a variety of real-world data sets. In this context, we describe a new post-processing technique, decision tree balancing. Section 2 describes our experimental method. <p> Second, it is interesting to note that lookahead actually hurts accuracy in almost as many trees as those in which it enhances accuracy. This property, where lookahead search finds inferior solutions, is known as pathology in the context of game trees <ref> [ Nau, 1983; Mutchler, 1993 ] </ref> . We discuss pathology for decision trees further in Section 3.2, where this trend is exhibited more prominently. Pathology cannot occur for tree size or depth for class C, because one-level lookahead is equivalent to exhaustive search. <p> Intuitively, doing more search (lookahead) should produce better decision trees, just as deeper search in game trees (e.g., for chess) produces better game-playing programs. However, it has been observed that for some games, deeper search can actually produce an inferior program, both with two players <ref> [ Nau, 1983 ] </ref> and with multiple players [ Mutchler, 1993 ] . Decision trees, one can argue, are analogous to a one-player game tree. Our discovery that deeper search can lead to inferior decision trees thus extends the earlier pathology results to a new domain.
Reference: [ Norton, 1989 ] <author> S.W. Norton. </author> <title> Generating better decision trees. </title> <booktitle> In 11th Intl. Joint Conf. on Art. Intel., </booktitle> <pages> pages 800-805, </pages> <year> 1989. </year>
Reference-contexts: In addition to these domains, we experimented with two variants (V0 and V1) of the congressional voting records data. V0 is used by Nor-ton <ref> [ Norton, 1989 ] </ref> for his lookahead experiments. The V1 data [ Holte, 1993 ] is identical to the VO data, except that the "best" attribute, physician-fee freeze, is removed. All the data sets were taken from the UCI Machine Learning repository [ Murphy and Aha, 1994 ] . <p> Though Hartmann et al. did offer a concise framework for doing arbitrary level lookahead, they did not evaluate the effects of lookahead on tree quality. The ideas in GOTA motivated Norton's IDX system <ref> [ Norton, 1989 ] </ref> , which is a variant of Quinlan's ID3 that performs lookahead. Nor-ton conducted experiments on the congressional voting records database (see Section 4), and found that looka-head reduced decision tree depth on average. <p> We considered only one-level lookahead in this paper. One can attempt to evaluate the benefits of lookahead as a function of search depth. We feel that such a systematic evaluation is not only going to be computationally prohibitive, but also probably not very useful. Norton <ref> [ Norton, 1989 ] </ref> presents experiments comparing one and two level lookahead on one data set. Observing incidences of pathology (as we did in this paper) is only the first step in several interesting research directions.
Reference: [ Quinlan, 1986 ] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: This is T's goodness. 3. Execute steps 3,4 of GREEDY. We experimented with two pre-defined goodness measures, namely, the Gini index of diversity [ Breiman et al., 1984 ] and information gain <ref> [ Quinlan, 1986 ] </ref> . 1 This gave us four algorithms for our experiments, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [ Breiman et al., 1984 ] and Greedy-Info to the ID3 algorithm [ Quinlan, 1986 ] . <p> al., 1984 ] and information gain <ref> [ Quinlan, 1986 ] </ref> . 1 This gave us four algorithms for our experiments, which we named Greedy-Gini, Greedy-Info, Look-Gini, and Look-Info. Note that Greedy-Gini is essentially identical to the CART algorithm [ Breiman et al., 1984 ] and Greedy-Info to the ID3 algorithm [ Quinlan, 1986 ] .
Reference: [ Ragavan and Rendell, 1993 ] <author> H. Ragavan and L. Ren-dell. </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In Machine Learning: Proceedings of the 10th Intl. Conf., </booktitle> <pages> pages 252-259, </pages> <year> 1993. </year>
Reference-contexts: Nor-ton conducted experiments on the congressional voting records database (see Section 4), and found that looka-head reduced decision tree depth on average. With a few exceptions, though, the advantages of lookahead were very small in Norton's experiments. Ragavan and Ren-dell considered using lookahead for feature construction in symbolic domains <ref> [ Ragavan and Rendell, 1993 ] </ref> , and pointed out that lookahead is beneficial when there is concealed attribute interaction. The emphasis of the current paper differs significantly from the existing work on lookahead.
Reference: [ Sarkar et al., 1994 ] <author> U.K. Sarkar, P.P. Chakrabarti, S. Ghose, </author> <title> and S.C. DeSarkar. Improving greedy algorithms by lookahead-search. </title> <journal> J. of Algorithms, </journal> <volume> 16(1) </volume> <pages> 1-23, </pages> <year> 1994. </year>
Reference-contexts: As the greedy approach can produce suboptimal trees in terms of tree size and depth, it is naturally of interest to explore ways to improve the greedy strategy. Fixed-depth lookahead search is a standard technique for improving greedy algorithms <ref> [ Sarkar et al., 1994 ] </ref> . Lookahead is largely unexplored in the decision tree literature barring a few scattered attempts discussed in Section 5. The advantages, or lack thereof, of looka-head search have not been systematically quantified in the context of decision tree or rule induction.
Reference: [ UCI-cleveland-data, ] <institution> Cleveland heart disease data. Collected by Roberto Detrano, V.A. Medical center, Long Beach and Cleveland Clinic Foundation. </institution>
Reference-contexts: The six "difficult" domains are the breast cancer recurrence database (BC), the Cleveland heart disease data (CL) <ref> [ UCI-cleveland-data, ] </ref> , glass identification data (GL), hepatitis diagnosis (HE), Canadian labor negotiations data (LA) and lymphography diagnosis (LY) [ UCI-lymph-data, ] . In addition to these domains, we experimented with two variants (V0 and V1) of the congressional voting records data.
Reference: [ UCI-lymph-data, ] <institution> Lymphography data. Obtained from the Univ. Med. Centre, Institute of Oncology, Ljubljana, </institution> <address> Yugoslavia. </address> <note> Data provided by M. </note> <author> Zwitter and M. </author> <month> Soklic. </month>
Reference-contexts: The six "difficult" domains are the breast cancer recurrence database (BC), the Cleveland heart disease data (CL) [ UCI-cleveland-data, ] , glass identification data (GL), hepatitis diagnosis (HE), Canadian labor negotiations data (LA) and lymphography diagnosis (LY) <ref> [ UCI-lymph-data, ] </ref> . In addition to these domains, we experimented with two variants (V0 and V1) of the congressional voting records data. V0 is used by Nor-ton [ Norton, 1989 ] for his lookahead experiments.
References-found: 18

