URL: http://www.aic.nrl.navy.mil/papers/1994/AIC-94-011.ps.Z
Refering-URL: http://ai.iit.nrc.ca/bibliographies/feature-selection.html
Root-URL: 
Email: aha@aic.nrl.navy.mil  bankert@nrlmry.navy.mil  
Title: Feature Selection for Case-Based Classification of Cloud Types: An Empirical Comparison database containing 204 continuous
Author: David W. Aha Richard L. Bankert 
Keyword: Motivation  Cloud Classification Task  
Note: types. Bankert (1994) addressed this problem. Given a  (Specht, 1990; Welch et. al, 1992) to classify cloud types. 1 Current plans include using this network in SIAMES.  
Address: Washington, DC 20375  Monterey, CA 93943  
Affiliation: Navy AI Center Naval Research Laboratory  Marine Meteorology Division Naval Research Laboratory  
Abstract: Accurate weather prediction is crucial for many activities, including Naval operations. Researchers within the meteorological division of the Naval Research Laboratory have developed and fielded several expert systems for problems such as fog and turbulence forecasting, and tropical storm movement. They are currently developing an automated system for satellite image interpretation, part of which involves cloud classification. Their cloud classification database contains 204 high-level features, but contains only a few thousand instances. The predictive accuracy of classifiers can be improved on this task by employing a feature selection algorithm. We explain why non-parametric case-based classifiers are excellent choices for use in feature selection algorithms. We then describe a set of such algorithms that use case-based classifiers, empirically compare them, and introduce novel extensions of backward sequential selection that allows it to scale to this task. Several of the approaches we tested located feature subsets that attain significantly higher accuracies than those found in previously published research, and some did so with fewer features. Accurate interpretation of maritime satellite imagery data is an important component of Navy weather forecasting, particularly in remote areas for which there are limited conventional observations. Decisions regarding tactical Naval operations depend on their accuracy. Unfortunately, frequent personnel rotations and lack of training time drastically reduce the level of shipboard image interpretation expertise. Therefore, the Naval Research Laboratory (NRL) in Monterey, CA is building an expert system named SIAMES (Satellite Image Analysis Meteorological Expert System) that aids in this task (Peak & Tag, 1992). Inputs to SIAMES, including cloud classification, are not yet fully automated; the user must manually input details on cloud This paper extends the work reported by Bankert (1994). We argue for the use of case-based classifiers in domains with large numbers of features. We show they can locate smaller feature subsets leading to significantly higher accuracies on this task. We begin by describing the cloud classification task and Bankert's study. Next, we explain the benefits of using case-based classifiers in feature selection, and argue why such approaches should outperform the approach previously used. We then describe a set of feature selection algorithms that we tested on the cloud classification task. Finally, we report our results, and discuss them in the context of related work. This paper has three primary contributions. First, we show further evidence that feature selection algorithms should use the classifier itself to evaluate feature subsets. Second, we show that a case-based algorithm is a particularly good classifier in this context. Finally, we introduce a method that allows the backward sequential selection algorithm to scale to large numbers of features. Bankert (1994) describes the cloud classification task. The original data were supplied and expertly labeled by Dr. C. Wash and Dr. F. Williams of the Naval Postgraduate School. They were in the form of four advanced very high resolution radiometer (AVHRR) local area coverage (LAC) scenes with size 512 fi 512 pixels. An additional 91 images were labeled independently by four NRL Monterey experts under the direction of Dr. C. Crosiar. The scenes were not confined fl In Proceedings of the AAAI-94 Workshop on Case-Based Reasoning
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1992a). </year> <title> Tolerating Noisy, Irrelevant, and Novel Attributes in Instance-Based Learning Algorithms. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 36, </volume> <pages> 267-287. </pages>
Reference-contexts: Again, we tested BEAM using both the full and single state evaluation functions, and using queue sizes of one and 25 respectively. Other Algorithms We included two other case-based feature selection algorithms in our study. IB4 <ref> (Aha, 1992a) </ref> was chosen as an example of a classifier that hill-climbs in the space of real-valued feature weights. Several such algorithms exist (e.g., Wettschereck & Dietterich, 1994). Although these algorithms have performed well on some datasets, they tend to work best when features are either highly relevant or irrelevant. <p> The most frequent concept in the database occurs for 15.4% of the cases. Both IB1 and PNN significantly increase accuracy, although PNN fares better here because IB1 does not perform well for high-dimensional tasks involving many partially relevant attributes <ref> (Aha, 1992a) </ref>. <p> Line seven refers to using a queue size of 25 and the greedy evaluation strategy. This latter approach performed well; it located a subset containing only five features that attained an accuracy of 85.1%. The eighth line in Table 2 refers to using IB4 <ref> (Aha, 1992a) </ref>. Like most weight-tuning case-based algorithms, IB4 does not perform well unless each feature is either highly relevant or irrelevant. Its poor performance here is not surprising. The final line refers to using C4.5 (Quinlan, 1993) to select features for IB1.
Reference: <author> Aha, D. W. </author> <year> (1992b). </year> <title> Generalizing from case studies: A case study. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 1-10). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This method has not previously been used on tasks involving more than a few dozen features, and some reports suggest that C4.5 does not always perform well when the features are all continuous <ref> (e.g., Aha, 1992b) </ref>. Thus, we suspected that it may not perform well on the cloud classification task. Empirical Comparisons We examined two hypotheses concerning the sequential search framework described in the previous section: 1. The wrapper control strategy is superior to the filter control strategy for this task. 2.
Reference: <author> Aha, D. W., & Goldstone, R. L. </author> <year> (1992). </year> <title> Concept learning and flexible weighting. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 534-539). </pages> <address> Bloomington, </address> <publisher> IN: Lawrence Erlbaum. </publisher>
Reference-contexts: Also, these algorithms work best when the features are either highly relevant or irrelevant, which doesn't appear to be true for the cloud classification task. However, it is possible that the notion of locally warping the instance space <ref> (e.g., Aha & Goldstone, 1992) </ref> can be used for feature selection (i.e., the relevance of features varies in a given instance space).
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference-contexts: We hypothesize that better results can be obtained by using a wrapper model. Non-parametric case-based classifiers are excellent choices for feature selection classifiers because they often obtain good accuracies and can also be used as evaluation functions. Thus, we used IB1 <ref> (Aha, Kibler, & Albert, 1991) </ref> in our studies for these two purposes; it is an implementation of the nearest neighbor classifier. Doak (1992) identifies three categories of search algorithms: exponential, sequential, and randomized. Exponential algorithms have exponential complexity in the number of features. <p> In both cases, feature subsets were constrained to have size no more than 25. Wrapper Control with BSS The fourth category combines a wrapper control strategy with BSS search. We again used the BEAM algorithm, but in this case we used IB1 <ref> (Aha, Kibler, & Albert, 1991) </ref> as the evaluation function. Again, we tested BEAM using both the full and single state evaluation functions, and using queue sizes of one and 25 respectively. Other Algorithms We included two other case-based feature selection algorithms in our study.
Reference: <author> Almuallim, H., & Dietterich, T. G. </author> <year> (1991). </year> <title> Learning with many irrelevant features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 547-552). </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Sequential search algorithms have polynomial complexity; they add or subtract features and use a hill-climbing search strategy. Randomized algorithms include genetic and simulated annealing search methods. Sequential search algorithms seem most appropriate for our task because exponential search algorithms (e.g., FOCUS <ref> (Almuallim & Dietterich, 1991) </ref> are prohibitively expensive and randomized algorithms, unless biased as in (Skalak, 1994), tend to yield larger subsets of features than do sequential strategies (Doak, 1992). Sequential algorithms performed comparatively well in Doak's study. Thus, we chose to use them in our study. <p> However, it is possible that the notion of locally warping the instance space (e.g., Aha & Goldstone, 1992) can be used for feature selection (i.e., the relevance of features varies in a given instance space). We suspect that FOCUS <ref> (Almuallim & Dietterich, 1991) </ref> and Relief (Kira & Rendell, 1992) would not perform well on this task since the former uses exhaustive search and the latter assumes that features are, again, either highly relevant or irrelevant.
Reference: <author> Ashley, K. D., & Rissland, E. L. </author> <year> (1988). </year> <title> Waiting on weighting: A symbolic least commitment approach. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence (pp. </booktitle> <pages> 239-244). </pages> <address> St. Paul, MN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our objectives are to maximize classification accuracy and minimize the number of features used to define the cases. Feature selection algorithms input a set of features and yield a subset of them. Several knowledge-intensive CBR algorithms have been used to perform feature selection <ref> (e.g., Ashley & Rissland, 1988) </ref>. However, domain specific knowledge is not yet available for the cloud classification task. This prevents us from using explanation-based approaches for indexing and retrieving appropriate features (e.g., Cain, Pazzani, & Silverstein, 1991; Ram, 1993).
Reference: <author> Bankert, R. L. </author> <year> (1994). </year> <title> Cloud classification of AVHRR imagery in maritime regions using a probabilistic neural network. </title> <note> To appear in Journal of Applied Meteorology. </note>
Reference-contexts: Our results strongly indicate that, when using these algorithms, the evaluation function should be the same as the classifier. This leads to significantly higher accuracies than those previously published for this task <ref> (Bankert, 1994) </ref>. We have not yet evaluated BEAM on other problems with large numbers of features to better analyze its benefits. While we anticipate that some of its properties will prove generally useful, we do not yet have evidence for such claims.
Reference: <author> Cain, T., Pazzani, M. J., & Silverstein, G. </author> <year> (1991). </year> <title> Using domain knowledge to influence similarity judgement. </title> <booktitle> In Proceedings of the Case-Based Reasoning Workshop (pp. </booktitle> <pages> 191-202). </pages> <address> Washington, DC: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Caruana, R., & Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <booktitle> To appear in Proceedings of the Eleventh Inter national Machine Learning Conference. </booktitle> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 25-32). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cover, T. M., & van Campenhout, J. M. </author> <year> (1977). </year> <title> On the possible orderings in the measurement selection problem. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 7, </volume> <pages> 657-661. </pages>
Reference: <author> Devijver, P. A., & Kittler, J. </author> <year> (1982). </year> <title> Pattern recognition: A statistical approach. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: Therefore, we do not address the cost of evaluating features (Owens, 1993). Thus, this study was restricted to using knowledge-poor feature selection approaches <ref> (e.g., Devijver & Kit-tler, 1982) </ref>, although we plan to work with knowledge-based techniques for feature extraction in the future. Feature selection algorithms have three components: 1. Search algorithm This searches the space of fea ture subsets, which has size 2 d where d is the number of features. <p> He used FSS with the Bhat-tacharya class separability index as the evaluation function it measures the degree to which classes are separated and internally cohere. Many such indices have been proposed and evaluated <ref> (e.g., Devijver & Kittler, 1982) </ref>. They are usually less computationally expensive than the classifier and are nonparametric, which allows them to be used without requiring manual parameter tuning. However, if their bias does not match the bias of the classifier, then they can lead to suboptimal performance. <p> Furthermore, neither algorithm found small-sized feature sets in this study. Related Work Feature selection has its roots in pattern recognition and statistics and is addressed in several textbooks <ref> (e.g., Devijver & Kittler, 1982) </ref>.
Reference: <author> Doak, J. </author> <year> (1992). </year> <title> An evaluation of feature selection methods and their application to computer security (Technical Report CSE-92-18). </title> <institution> Davis, CA: University of California, Department of Computer Science. </institution>
Reference-contexts: Sequential search algorithms seem most appropriate for our task because exponential search algorithms (e.g., FOCUS (Almuallim & Dietterich, 1991) are prohibitively expensive and randomized algorithms, unless biased as in (Skalak, 1994), tend to yield larger subsets of features than do sequential strategies <ref> (Doak, 1992) </ref>. Sequential algorithms performed comparatively well in Doak's study. Thus, we chose to use them in our study. We have not yet investigated strategies that combine algorithms from different categories (e.g., Doak, 1992; Caruana & Freitag, 1994). <p> The first hypothesis is based on evidence that using the classifier itself as the evaluation function yields better performance on some tasks (e.g., Doak, 1992; Vafaie & De Jong, 1993; John, Kohavi, & Pfleger, 1994). Similar evidence exists for the second hypothesis <ref> (Doak, 1992) </ref>. However, these hypotheses have not been previously investigated for tasks of this magnitude. We also have secondary hypotheses concerning the other two algorithms in our study: 3.
Reference: <author> John, G., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> To appear in Proceedings of the Eleventh International Machine Learning Conference. </booktitle> <address> New Brunswick, NJ: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Kira, K., & Rendell, L. A. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 249-256). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, it is possible that the notion of locally warping the instance space (e.g., Aha & Goldstone, 1992) can be used for feature selection (i.e., the relevance of features varies in a given instance space). We suspect that FOCUS (Almuallim & Dietterich, 1991) and Relief <ref> (Kira & Rendell, 1992) </ref> would not perform well on this task since the former uses exhaustive search and the latter assumes that features are, again, either highly relevant or irrelevant. Vafaie and De Jong (1993) instead used a genetic algorithm to select features for a rule induction program.
Reference: <author> Mucciardi, A. N., & Gose, E. E. </author> <year> (1971). </year> <title> A comparison of seven techniques for choosing subsets of pattern recognition properties. </title> <journal> IEEE Transaction on Computers, </journal> <volume> 20, </volume> <pages> 1023-1031. </pages>
Reference: <author> Owens, C. </author> <year> (1993). </year> <title> Integrating feature extraction and memory search. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 311-340. </pages>
Reference-contexts: Furthermore, the same set of features are used to describe each case in this case base, their values have been pre-computed, and no further inferencing is required to access these values. Therefore, we do not address the cost of evaluating features <ref> (Owens, 1993) </ref>. Thus, this study was restricted to using knowledge-poor feature selection approaches (e.g., Devijver & Kit-tler, 1982), although we plan to work with knowledge-based techniques for feature extraction in the future. Feature selection algorithms have three components: 1.
Reference: <author> Peak, J. E., & Tag, P. M. </author> <year> (1992). </year> <title> Towards automated interpretation of satellite imagery for navy shipboard applications. </title> <journal> Bulletin of the American Meterological Society, </journal> <volume> 73, </volume> <pages> 995-1008. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for machine learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although these algorithms have performed well on some datasets, they tend to work best when features are either highly relevant or irrelevant. We hypothesized that IB4 would not work well on the cloud classification task, which contains many partially-relevant features. We also included Cardie's (1993) approach. She used C4.5 <ref> (Quinlan, 1993) </ref> to select which features to use in a case-based classifier and reported favorable results. <p> The eighth line in Table 2 refers to using IB4 (Aha, 1992a). Like most weight-tuning case-based algorithms, IB4 does not perform well unless each feature is either highly relevant or irrelevant. Its poor performance here is not surprising. The final line refers to using C4.5 <ref> (Quinlan, 1993) </ref> to select features for IB1. Using C4.5's default parameter settings, it yields pruned trees that have a large number of features that do not deliver high predictive accuracies. We have not investigated whether alternative parameter settings will improve its performance.
Reference: <author> Ram, A. </author> <year> (1993). </year> <title> Indexing, elaboration, and refinement: Incremental learning of explanatory cases. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 201-248. </pages>
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> To appear in Proceedings of the Eleventh International Machine Learning Conference. </booktitle> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Randomized algorithms include genetic and simulated annealing search methods. Sequential search algorithms seem most appropriate for our task because exponential search algorithms (e.g., FOCUS (Almuallim & Dietterich, 1991) are prohibitively expensive and randomized algorithms, unless biased as in <ref> (Skalak, 1994) </ref>, tend to yield larger subsets of features than do sequential strategies (Doak, 1992). Sequential algorithms performed comparatively well in Doak's study. Thus, we chose to use them in our study. <p> For this category, we selected as BEAM's evaluation function a separability index that is similar to the one used by Bankert and performed best in comparison with many other indices <ref> (Skalak, 1994) </ref>. A full evaluation function was used with a queue size of 1. 2 Wrapper Control with FSS For this category, we combined FSS search with IB1 as both the evaluation function and classifier.
Reference: <author> Specht, D. F. </author> <year> (1990). </year> <title> Probabilistic neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 109-118. </pages>
Reference: <author> Vafaie, H., & De Jong, K. </author> <year> (1993). </year> <title> Robust feature selection algorithms. </title> <booktitle> In Proceedings of the Fifth Conference on Tools for Artificial Intelligence (pp. </booktitle> <pages> 356-363). </pages> <address> Boston, MA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Welch, R. M., Sengupta, S. K., Goroch, A. K., Ra-bindra, P., Rangaraj, N., & Navar, M. S. </author> <year> (1992). </year> <title> Polar cloud and surface classification using AVHRR imagery: An intercomparison of methods. </title> <journal> Journal of Applied Meteorology, </journal> <volume> 31, </volume> <pages> 405-420. </pages>
Reference: <author> Wettschereck, D., & Dietterich, T. G. </author> <year> (1994). </year> <title> An experimental comparison of the nearest neighbor and nearest hyperrectangle algorithms. </title> <note> To appear in Machine Learning. </note>
Reference-contexts: Other Algorithms We included two other case-based feature selection algorithms in our study. IB4 (Aha, 1992a) was chosen as an example of a classifier that hill-climbs in the space of real-valued feature weights. Several such algorithms exist <ref> (e.g., Wettschereck & Dietterich, 1994) </ref>. Although these algorithms have performed well on some datasets, they tend to work best when features are either highly relevant or irrelevant. We hypothesized that IB4 would not work well on the cloud classification task, which contains many partially-relevant features.
References-found: 25

