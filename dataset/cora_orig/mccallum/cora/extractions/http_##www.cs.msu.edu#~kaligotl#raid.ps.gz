URL: http://www.cs.msu.edu/~kaligotl/raid.ps.gz
Refering-URL: http://www.cs.msu.edu/~kaligotl/papers.html
Root-URL: http://www.cs.msu.edu
Title: Survey of RAID Architectures  CPS 820 Advanced Computer Architecture  
Author: Kaligotla Chakrapani Instructor: Dr. Lionel M. Ni 
Date: Fall 1994  
Affiliation: Department of Computer Science Michigan State University  
Abstract: With the increasing processing speeds, it has become important to design powerful and efficient I/O systems. Disk arrays have been proposed in 1980s as a way to use parallelism between multiple disks to improve aggregate I/O performance. Today they appear in the product lines of the most major computer manufacturers. This paper gives a comprehensive survey of Disk arrays. The paper first reviews the driving forces that have popularized disk arrays. Next the paper describes seven different disk array architectures called RAID 1 levels 0-6 and compares their performance, cost and reliability. The paper goes on to discuss various performace issues in the implementation of a RAID. The paper also describes four disk array prototypes or products and reviews their merits. Lastly several open issues that require further investigation are identified. 
Abstract-found: 1
Intro-found: 1
Reference: [Cao93] <author> Pei Cao et.al. </author> <title> The TickerTAIP Parallel RAID architecture. </title> <booktitle> In Proceedings of the 1993 International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: file systems, data is written to disk in large track-sized transfer units, reducing random access latencies and performing adaptive load balancing. 4.2 TickerTAIP/Data Mesh TickerTAIP/Data Mesh is a research project at Hewlett-Packard Labs whose goal is to develop an array of smart disk nodes linked by a fast, reliable network <ref> [Cao93] </ref>. (Figure 4) Each node contains disks, a CPU and some local memory. Disk array controller operations such as parity computation are distributed among these smart nodes and the nodes communicate by message-passing across the internal interconnect. <p> Currently TickerTAIP exists as a small, 7-node prototype. Each node consists of a T800 trans 16 puter, 4MB of local RAM and one HP79560 SCSI disk drive. The TickerTAIP project has also developed software to make the multiple, distributed processing nodes appear as a single fast storage server. <ref> [Cao93] </ref> describes a family of distributed algorithms for calculating RAID parity, discuss techniques for establishing request atomicity, sequencing and recovery. TickerTAIP architecture achieves better performance than traditional RAID architectures. Most of the performance difference result from the cost of doing parity calculations.
Reference: [Chen90a] <author> Peter M. Chen, Garth Gibson, Randy H. Katz and David A. Patterson. </author> <title> An evaluation of Redundant Arrays of Disks Using Amdhal 5890. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS Conference on Measuremetn and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: This means that the cost of small write is twice as expensive in a RAID level 1 system as in a RAID level 0 system. The table also shows the storage efficiency of each of the RAID levels. For detailed comparison of the RAID levels see [Patterson89] and <ref> [Chen90a] </ref>.
Reference: [Chen90b] <author> Peter M. Chen and David A. Patterson. </author> <title> Maximizing Performance in a Striped Disk Array. </title> <booktitle> In Proceedings of the 1990 International Symposium on Computer Architecture, </booktitle> <pages> pages 322-331, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Because the interaction between workload and striping unit can have a substantial effect on the performance of a disk array with block interleaved striping, Chen et. al. have developed rules of thumb for selecting a stripe unit size for RAID level 0 disk arrays <ref> [Chen90b] </ref>. Their simulation-based model evaluated a spindle synchronized disk array of 16 disks. They used four stochastic distributions describing the size of each request. These distributions had mean sizes of 4KB, 16KB, 400KB and 1500KB. They also varied the number of concurrent, independent requests from 1 to 20.
Reference: [Chen93] <author> Peter M. Chen and Edward K. Lee. </author> <title> Striping in a RAID Level 5 Disk Array. </title> <institution> University of Michigan, </institution> <note> Technical Report No. CSE-TR-181-93. </note>
Reference-contexts: If nothing is known about the workload's concurrency, then they found that a good compromize size for striping unit is 2/3 * average positioning time * data transfer rate. <ref> [Chen93] </ref> obtained results for choosing optimal stripe unit size for RAID level 5 disks arrays. An important conclusion of their work is that the striping unit is primary dependent on the concurrency of the applied work load and is relatively insensitive to the request size distribution of the workload.
Reference: [Chervenak91] <author> Ann L. Chervenak and Randy H. Katz. </author> <title> Performance of a Disk Array Prototype. </title> <booktitle> In Proceedings of SIGMETRICS, </booktitle> <month> May </month> <year> 1991. </year>
Reference: [Drapeau94] <author> Ann L. Drapeau et. al. </author> <title> RAID-II: A High Bandwidth Network File Server. </title> <booktitle> In Proceedings of the 1994 International Symposium on Computer Architecture. </booktitle>
Reference: [Lee90] <author> Edward K. Lee. </author> <title> Software and Performance Issues in the Implementation of a RAID prototype. </title> <institution> U.C. Berkeley, </institution> <month> May </month> <year> 1990, </year> <note> Technical Report No. UCB/CSD 90/573. </note>
Reference: [Lee91] <author> Edward K. Lee and Randy H. Katz. </author> <title> Performance Consequences of Parity Placement in Disk Arrays. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pages 190-199, </pages> <month> April </month> <year> 1991. </year>
Reference: [Lee92] <author> Edward K. Lee et. al. </author> <title> RAID-II: A Scalable Storage Architecture for High Bandwidth Network File Service. </title> <institution> U.C. Berkeley, </institution> <month> Feb. </month> <year> 1992, </year> <note> Technical Report No. UCB/CSD 92/672. </note>
Reference: [Menon91] <author> Jai Menon, Dick Mattson and Spencer Ng. </author> <title> Distributed Sparing for Improved Performance of Disk Arrays. </title> <type> Technical Report RJ 7943, </type> <institution> IBM, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: This section describes two techniques distributed sparing and parity sparing, that exploit on-line spare disks to enhance performance during normal operation of the system. 11 Distributed sparing distributes the capacity of a spare disk across all the disks in the disk ar-ray <ref> [Menon91] </ref>. The distribution of spare capacity is similar to the distribution of parity in RAID level 5 disk array. Instead of N data and one spare disk, distributed sparing uses N+1 data disks that each have 1/(N+1)th spare capacity.
Reference: [Menon93a] <author> Jai Menon and Jim Cortney. </author> <title> The Architecture of a Fault-Tolerant Cached RAID Controller. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 76-86, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Write buffering has the following advantages : improves throughput by allowing future updates to overwrite previous updates; allows small write operations to be grouped together, which elimnates the need to read old data and old parity blocks; allows better disk scheduling by writing multiple blocks at one time <ref> [Menon93a] </ref>. Read caching can improve both response time and throughput. In a RAID level 5 disk array, if the old data for computing new parity is present in the cache, one disk I/O is reduced. <p> With larger ones, the difference is more marked as parity calulations become significant. Therefore at least for parity computation, TickerTAIP achieves near linear scaling. 4.3 IBM Hagar Disk Array Controller Hagar is a disk array controller developed at the IBM Almaden Research Center <ref> [Menon93a] </ref>. Ha-gar was designed for large capacity (up to 1 TB), high bandwidth (up to 100 MB/Sec), and high I/O rate (up to 5000 4-KB I/Os per second). In addition Hagar provides high availability through the use of redundant hardware components, multiple power boundaries and on-line reconstruction of data.
Reference: [Menon93b] <author> Jai Menon, James Roche and Jim Kasson. </author> <title> Floating Parity and Data Disk Arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 17 </volume> <pages> 129-139, </pages> <year> 1993. </year>
Reference-contexts: exploits only temporal locality. 3.1.2 Floating Parity Menon and Kasson proposed a variation on the organization of parity in RAID level 5 disk array, called floating parity, that shortens the read-modify-write of parity updated by small, random writes 10 to little more than a single disk access time on average <ref> [Menon93b] </ref>. Floating parity clusters parity into cylinders, each containing a track of free blocks. Whenever a parity block needs to be updated, the new parity block can be written on the rotationally nearest unallocated block following the old parity block.
Reference: [Ng94] <author> Spencer W. Ng. </author> <title> Crosshatch Disk Array for Improved Reliability and Performance. </title> <booktitle> In Proceedings of the 1994 International Symposium on Computer Architecture. </booktitle>
Reference-contexts: This degrades the disk array's performance heavly and as it takes long to completely rebuild data on all the affected disks, a second failure can lead to data loss. <ref> [Ng94] </ref> has proposed disk array architectures that use dual ported disks to solve the above problems. Figure 2 (a) shows an architecture called Dual path horizontal array.
Reference: [Ousterhout90] <author> John K. Ousterhout. </author> <title> Why Aren't Operating Systems Getting Faster as Fast as Hardware. </title> <booktitle> In Proceedings of USENIX Technical COnference, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: But between the application and the disk subsystem, operating system is present and can play a significant role in delivering the performance of the disk subsystem to the applicaiton. <ref> [Ousterhout90] </ref> ran benchmarks on a large number of hardware and sofware configurations and concluded that the machines did not run the benchmarks as fast as they should have, gauging from their raw CPU speeds. He pointed out the following Hardware and Software issues. 13 column corresponds to a disk.
Reference: [Patterson88] <author> David A. Patterson, Garth A. Gibson and Randy H. Katz. </author> <title> A Case for Redundant Arrays of Inexpensive Disks(RAID). </title> <booktitle> In International Conference on Management of Data(SIGMOD), </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Although the basic concepts of data striping and redundancy are conceptually simple, selecting between the many possible data striping and redundancy schemes involves complex tradeoffs between reliability, performance and cost. 2.2 RAID Organizations This section describes the basic RAID levels <ref> [Patterson88] </ref> which will be used as the basis for further study of performance, cost and reliability. 2.2.1 RAID Level 0 The RAID level 0, has the lowest cost of redundancy scheme because it does not employ redundancy at all. <p> VAXsimPLUS, a product of Digital Equipment Corporation, monitors the warnings given by the disk and notifies an operator when it feels the disk is about to fail. 2.4.4 Correlated Disk Failures The reliability model of <ref> [Patterson88] </ref> assumes that all disk failures are independent when calculating mean time to data loss. This results in high MTTF of disk array.
Reference: [Patterson89] <editor> David A. Patterson et. al. </editor> <booktitle> Introduction to RAID? In Proceedings of the IEEE COMPCON, </booktitle> <pages> pages 112-117, </pages> <month> Spring </month> <year> 1989. </year> <month> 22 </month>
Reference-contexts: This means that the cost of small write is twice as expensive in a RAID level 1 system as in a RAID level 0 system. The table also shows the storage efficiency of each of the RAID levels. For detailed comparison of the RAID levels see <ref> [Patterson89] </ref> and [Chen90a].
Reference: [Rosenblum91] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The Design and Implementation of Log-Structured File System. </title> <booktitle> In proceedings of the 13th ACM Symposium on Operating System Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference: [Schulze89] <author> Martin Schulze, Garth Gibson, Randy H. Katz and David A. Patterson. </author> <booktitle> How Reliable is RAID? In Proceedings of the IEEE COMPCON, </booktitle> <pages> pages 118-123, </pages> <month> Spring </month> <year> 1989. </year>
Reference-contexts: Option 2, on the other hand loses one disk form each of the four error correction groups and still allows access to all data. This technique of organizing error-correcting groups orthogonally to common hardware (such as a string) is called orthogonal RAID <ref> [Schulze89] </ref>. Orthogonal RAID has 3 The collection of disks that share the same pathway to the controller or head of the string is called a STRING. 8 an added benefit of minimizing string conflicts when multiple disks form a group to transfer data simultaneously.
Reference: [Stodolsky93] <author> Daniel Stodolsky and Garth Gibson. </author> <title> Parity Logging: Overcoming the Small Write Problem in Redundant Disk Arrays. </title> <booktitle> In Proceedings of the 1993 International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1993. </year> <month> 23 </month>
Reference-contexts: the disk and floating data requires much more free disk space than floating only parity since there are many more data blocks than parity blocks. 3.1.3 Parity Logging Stodolsky and Gibson proposed an approach called parity logging to reduce the penatly of small writes in RAID level 5 disk arrays <ref> [Stodolsky93] </ref>. Parity logging reduces the overhead for small writes by delaying the read of the old parity and write of the new parity. Instead of immediately updating the parity, an updated image, the difference between the old and new parity is temporarily written to a log.
References-found: 19

