URL: http://robotics.stanford.edu/users/brafman/ijcai95-mental.ps
Refering-URL: http://www.robotics.stanford.edu/users/brafman/bio.html
Root-URL: http://www.robotics.stanford.edu
Email: brafman@cs.stanford.edu  moshet@ie.technion.ac.il  
Title: Towards Action Prediction Using a Mental-Level Model  
Author: Ronen I. Brafman Moshe Tennenholtz 
Address: Stanford, CA 94305-2140  Haifa 32000, Israel  
Affiliation: Dept. of Computer Science Stanford University  Faculty of Industrial Engineering and Management Technion  
Abstract: We propose a formal approach to the problem of prediction based on the following steps: First, a mental-level model is constructed based on the agent's previous actions; next, the model is updated to account for any new observations by the agent, and finally, we predict the optimal action w.r.t. the agent's mental state as its next action. This paper formalizes this prediction process. In order to carry out this process, we need to understand how a mental state can be ascribed to an agent and how this mental state should be updated. In [ Brafman and Tennenholtz, 1994b ] , we examined the first stage. Here we investigate a particular update operator and show that its ascription requires making only weak modeling assumptions.
Abstract-found: 1
Intro-found: 1
Reference: [ Alchourron et al., 1985 ] <author> C. E. Alchour-ron, P. Gardenfors, and D. Makinson. </author> <title> On the logic of theory change: partial meet functions for contraction and revision. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 50 </volume> <pages> 510-530, </pages> <year> 1985. </year>
Reference: [ Brafman and Tennenholtz, 1994a ] <author> R. I. Brafman and M. Tennenholtz. </author> <title> Belief ascription. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Two examples of decision criteria are maximin, which chooses the tuples in which the worst case outcome is maximal, and the principle of indifference which prefers tuples whose average outcome is maximal.(A fuller discussion of decision criteria appears in <ref> [ Luce and Raiffa, 1957; Brafman and Tennenholtz, 1994a ] </ref> .) We come to a key definition that ties all of the components we have discussed so far.
Reference: [ Brafman and Tennenholtz, 1994b ] <author> R. I. Brafman and M. Tennenholtz. </author> <title> Belief ascription and mental-level modelling. </title> <editor> In J. Doyle, E. Sandewall, and P. Torasso, editors, </editor> <booktitle> Proc. of Fourth Intl. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 87-98, </pages> <year> 1994. </year>
Reference-contexts: Motivated by work in decision-theory [ Luce and Raiffa, 1957 ] and work on knowledge ascription [ Halpern and Moses, 1990; Rosenschein, 1985 ] , we suggested in <ref> [ Brafman and Tennenholtz, 1994b ] </ref> a specific structure for mental-level models, consisting of beliefs, desires and a decision criterion. This model showed how these elements act as constraints on the agent's action, and how these constraints can be used to ascribe beliefs to the agent. <p> In order to perform this prediction process, we must understand how beliefs can be ascribed, how they should be updated, and how they should be used to determine the best perceived action. We have examined the first and the last question in <ref> [ Brafman and Tennenholtz, 1994b ] </ref> (although not in the context of prediction). In this paper, we wish to concentrate on the second question, that of modeling the agent's belief change. <p> Our discussion of the problem of prediction will be in the context of the framework of mental-level modeling and belief ascription investigated in <ref> [ Brafman and Tennenholtz, 1994b ] </ref> . This framework is reviewed in Section 2. In Section 3 we discuss the problem of prediction. We suggest a three-step process for prediction and highlight the importance of the ascription of a belief change operator to this process. <p> We discussed the first among these issues in our previous work <ref> [ Brafman and Tennenholtz, 1994b ] </ref> . In particular, we have shown a class of agents that can be ascribed the mental-level model discussed in Section 2. <p> Our work brings to this task a special bias in the form of the agency-hypothesis: Machines are agents of their designers; they are usually designed with a purpose in mind and with some underlying assumptions; therefore, they should be modeled accordingly. With this motivation in mind, this work and <ref> [ Brafman and Tennenholtz, 1994b ] </ref> attempt to understand the basis for modeling entities as if they have a mental state. <p> Such structure could be obtained by e.g., augmenting our purely semantic construction with an interpretation of a suitable language over the possible states. 11 This paper complements our previous work on belief ascription <ref> [ Brafman and Tennenholtz, 1994b ] </ref> and supplies initial answers to the above-mentioned questions. In this paper, we reviewed our proposed structure for mental-level models and their construction, and explained how they can be used to predict an agent's future behavior. <p> Putting these ingredients together, we get a theory of action prediction using a mental-level model, which consists of the three-step process, a theory of belief ascription (discussed in <ref> [ Brafman and Tennenholtz, 1994b ] </ref> ), and a study of belief change modeling. 12 Acknowledgment: We thank Yoav Shoham for useful discussions relating to this work. The first author was partially supported by ARPA and AFOSR through grants AF F 49620-94-1-0090 and AF F49620-92-J-0547.
Reference: [ Brafman et al., 1994 ] <author> R. I. Brafman, J. C. Latombe, Y. Moses, and Y. Shoham. </author> <title> Knowledge as a tool in motion planning under uncertainty. </title> <editor> In R. Fagin, editor, </editor> <booktitle> Proc. 5th Conf. on Theor. </booktitle> <editor> Asp. </editor> <title> of Reas. about 11 This point has been suggested to us by Hector Levesque. 12 Additional results, comparison to work on plan recognition, and discussion of our general approach can be found in [ Brafman, </title> <booktitle> 1995 ] . Know., </booktitle> <pages> pages 208-224, </pages> <address> San Francisco, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A belief assignment would assign a subset of these at each local state. If B (l) contains the world in 3 We will assume runs are finite. The extension to infinite runs is straightforward. 4 A continuous model of time may be preferred here. This is possible, e.g., <ref> [ Brafman et al., 1994 ] </ref> . which the can is at A, the robot is viewed as believing that the can is in location A.
Reference: [ Brafman, 1995 ] <author> R. I. Brafman. </author> <title> Mental State as a Modeling Tool: Theory and Applications. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1995. </year> <note> To appear 10/95. </note>
Reference: [ del Val and Shoham, 1993 ] <author> Alvaro del Val and Yoav Shoham. </author> <title> Deriving properties of belief update from theories of action (II). </title> <booktitle> In IJCAI'93, Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 732-737, </pages> <year> 1993. </year>
Reference: [ Fagin et al., 1994 ] <author> R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi. </author> <title> Reasoning about Knowledge. </title> <publisher> MIT Press, </publisher> <year> 1994. </year> <note> to appear. </note>
Reference-contexts: captured by the notion of a protocol. 1 A framework in which the environment does act can be mapped into this framework using richer state descriptions and larger sets of states, a common practice in game theory. 2 Though context is an overloaded term, its use here seems appropriate, following <ref> [ Fagin et al., 1994 ] </ref> . Definition 3 A protocol for an agent A is a function P A : L A ! A A . Example 1 (continued): The robot's protocol would specify in what direction to head in each position.
Reference: [ Friedman and Halpern, 1994 ] <author> N. Friedman and J. Y. Halpern. </author> <title> A knowledge-based framework for belief change. Part I: </title> <booktitle> Foundations. In Proc. of the Fifth Conf. on Theoretical Aspects of Reasoning About Knowledge, </booktitle> <address> San Francisco, California, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Goldszmidt and Pearl, 1992 ] <author> M. Goldszmidt and J. Pearl. </author> <title> Rank-based systems: A simple approach to belief revision, belief update and reasoning about evidence and actions. </title> <booktitle> In Principles of Knowledge Representation and Reasoning: Proc. Third Intl. Conf. (KR '92), </booktitle> <pages> pages 661-672, </pages> <year> 1992. </year>
Reference: [ Halpern and Moses, 1990 ] <author> J. Y. Halpern and Y. Moses. </author> <title> Knowledge and common knowledge in a distributed environment. </title> <journal> J. ACM, </journal> <volume> 37(3) </volume> <pages> 549-587, </pages> <year> 1990. </year>
Reference-contexts: We present a formalism that attempts to make these ideas more concrete and that will hopefully lead to better understanding of how the ascription of mental state could be mechanized. Motivated by work in decision-theory [ Luce and Raiffa, 1957 ] and work on knowledge ascription <ref> [ Halpern and Moses, 1990; Rosenschein, 1985 ] </ref> , we suggested in [ Brafman and Tennenholtz, 1994b ] a specific structure for mental-level models, consisting of beliefs, desires and a decision criterion. <p> Our framework, discussed in [ Brafman and Ten-nenholtz, 1994b ] , is motivated by the work of Halpern and Moses <ref> [ Halpern and Moses, 1990 ] </ref> and Rosenschein [ Rosenschein, 1985 ] on knowledge ascription, and by ideas from decision-theory [ Savage, 1972; Luce and Raiffa, 1957 ] . To clarify the concepts used, we will refer to the following example.
Reference: [ Katsuno and Mendelzon, 1991 ] <author> H. Katsuno and A. Mendelzon. </author> <title> On the difference between updating a knowledge base and revising it. </title> <booktitle> In Principles of Knowledge Representation and Reasoning: Proc. Second Intl. Conf. (KR '91), </booktitle> <pages> pages 387-394, </pages> <year> 1991. </year>
Reference: [ Levesque, 1984 ] <author> H. J. Levesque. </author> <title> A logic of implicit and explicit belief. </title> <booktitle> In Proc. National Conf. on Artificial Intelligence (AAAI '84), </booktitle> <pages> pages 198-202, </pages> <year> 1984. </year>
Reference: [ Levesque, 1986 ] <author> H. J. Levesque. </author> <title> Making believers out of computers. </title> <journal> Artificial Intelligence, </journal> <volume> 30 </volume> <pages> 81-108, </pages> <year> 1986. </year>
Reference-contexts: Thus, we are more concerned with modeling agent's ascribed beliefs than with designing them. An important related work that shares some of our perspective is Levesque's <ref> [ Levesque, 1986 ] </ref> , which is concerned with treating computers as believers. However, his work describes the beliefs of one particular class of agents whose actions are answering queries. Our work attempts to address a more general class of agents, whose actions are arbitrary.
Reference: [ Luce and Raiffa, 1957 ] <author> R. D Luce and H. Raiffa. </author> <title> Games and Decisions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1957. </year>
Reference-contexts: We present a formalism that attempts to make these ideas more concrete and that will hopefully lead to better understanding of how the ascription of mental state could be mechanized. Motivated by work in decision-theory <ref> [ Luce and Raiffa, 1957 ] </ref> and work on knowledge ascription [ Halpern and Moses, 1990; Rosenschein, 1985 ] , we suggested in [ Brafman and Tennenholtz, 1994b ] a specific structure for mental-level models, consisting of beliefs, desires and a decision criterion. <p> Our framework, discussed in [ Brafman and Ten-nenholtz, 1994b ] , is motivated by the work of Halpern and Moses [ Halpern and Moses, 1990 ] and Rosenschein [ Rosenschein, 1985 ] on knowledge ascription, and by ideas from decision-theory <ref> [ Savage, 1972; Luce and Raiffa, 1957 ] </ref> . To clarify the concepts used, we will refer to the following example. Example 1 We start with a robot located at an initial position. <p> Two examples of decision criteria are maximin, which chooses the tuples in which the worst case outcome is maximal, and the principle of indifference which prefers tuples whose average outcome is maximal.(A fuller discussion of decision criteria appears in <ref> [ Luce and Raiffa, 1957; Brafman and Tennenholtz, 1994a ] </ref> .) We come to a key definition that ties all of the components we have discussed so far.
Reference: [ McCarthy, 1979 ] <author> J. McCarthy. </author> <title> Ascribing mental qualities to machines. </title> <editor> In M. Ringle, editor, </editor> <booktitle> Philosophical Perspectives in Artificial Intelligence, </booktitle> <address> Atlantic Highlands, NJ, 1979. </address> <publisher> Humanities Press. </publisher>
Reference-contexts: The goal of this paper is to advance our understanding of basic questions related to the construction of a mental-level model, and in particular its application to prediction. The idea of ascribing mental qualities for the purpose of prediction is not new. John McCarthy discusses it in <ref> [ McCarthy, 1979 ] </ref> . An important aspect of his approach is that even when nothing in the internal structure of the entity modeled directly resembles beliefs, desires, or other mental qualities, it may be possible and useful to model it as if it has such qualities.
Reference: [ Newell, 1980 ] <author> A. Newell. </author> <title> The knowledge level. </title> <journal> AI Magazine, </journal> <pages> pages 1-20, </pages> <year> 1980. </year>
Reference-contexts: Thus, Mc-Carthy views mental qualities as abstractions. This view is shared by another well-known author, Allen Newell <ref> [ Newell, 1980 ] </ref> , who contemplates the possibility of viewing computer programs at a level more abstract than that of the programming language, which he calls the knowledge-level. The notion of a mental state is useful because it is abstract.
Reference: [ Rosenschein, 1985 ] <author> S. J. Rosenschein. </author> <title> Formal theories of knowledge in AI and robotics. </title> <journal> New Generation Comp., </journal> <volume> 3 </volume> <pages> 345-357, </pages> <year> 1985. </year>
Reference-contexts: We present a formalism that attempts to make these ideas more concrete and that will hopefully lead to better understanding of how the ascription of mental state could be mechanized. Motivated by work in decision-theory [ Luce and Raiffa, 1957 ] and work on knowledge ascription <ref> [ Halpern and Moses, 1990; Rosenschein, 1985 ] </ref> , we suggested in [ Brafman and Tennenholtz, 1994b ] a specific structure for mental-level models, consisting of beliefs, desires and a decision criterion. <p> Our framework, discussed in [ Brafman and Ten-nenholtz, 1994b ] , is motivated by the work of Halpern and Moses [ Halpern and Moses, 1990 ] and Rosenschein <ref> [ Rosenschein, 1985 ] </ref> on knowledge ascription, and by ideas from decision-theory [ Savage, 1972; Luce and Raiffa, 1957 ] . To clarify the concepts used, we will refer to the following example. Example 1 We start with a robot located at an initial position.
Reference: [ Savage, 1972 ] <author> L. J. Savage. </author> <title> The Foundations of Statistics. </title> <publisher> Dover Publications, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: Our framework, discussed in [ Brafman and Ten-nenholtz, 1994b ] , is motivated by the work of Halpern and Moses [ Halpern and Moses, 1990 ] and Rosenschein [ Rosenschein, 1985 ] on knowledge ascription, and by ideas from decision-theory <ref> [ Savage, 1972; Luce and Raiffa, 1957 ] </ref> . To clarify the concepts used, we will refer to the following example. Example 1 We start with a robot located at an initial position.
Reference: [ von Neumann and Morgenstern, 1944 ] <author> J. von Neu-mann and O. Morgenstern. </author> <title> Theory of Games and Economic Behavior. </title> <publisher> Princeton University Press, </publisher> <address> Prince-ton, </address> <year> 1944. </year>
Reference-contexts: We start with the agent's preference order over the set of run suffixes, represented by a utility function. This preference order embodies the relative desirability of different futures. Definition 6 A utility function u is a real-valued function on the set of run-suffixes. It is well known <ref> [ von Neumann and Morgenstern, 1944 ] </ref> that a utility function can represent preference orders satisfying certain assumptions, which in this paper we will accept.
References-found: 19

