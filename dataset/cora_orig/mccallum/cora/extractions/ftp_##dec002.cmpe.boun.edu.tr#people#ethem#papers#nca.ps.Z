URL: ftp://dec002.cmpe.boun.edu.tr/people/ethem/papers/nca.ps.Z
Refering-URL: http://www.cmpe.boun.edu.tr/~ethem/
Root-URL: 
Title: Comparison of Kernel Estimators, Perceptrons, and Radial-Basis Functions for OCR and Speech Classification  
Author: Ethem Alpaydn, Fikret Gurgen 
Keyword: Kernel Estimators; Perceptrons; Backpropagation; Radial-Basis Functions; Optical Character Recognition; Speech Recognition  
Note: Neural Computing Applications  c fl1995 Springer-Verlag London Limited  
Address: TR-80815 Istanbul Turkey  
Affiliation: Department of Computer Engineering, Bogazi~ci University,  
Email: alpaydin@boun.edu.tr  
Date: (1995) 3: 38-49  
Abstract: We compare kernel estimators, single and multi-layered perceptrons and radial-basis functions for the problems of classification of handwritten digits and speech phonemes. By taking two different applications and employing many techniques, we report here a two-dimensional study whereby a domain-independent assessment of these learning methods can be possible. We consider a feed-forward network with one hidden layer. As examples of the local methods, we use kernel estimators like k-nearest neighbor (k-nn), Parzen windows, generalized k-nn, and Grow and Learn (Condensed Nearest Neighbor). We have also considered fuzzy k-nn due to its similarity. As distributed networks, we use linear perceptron, pairwise separating linear perceptron, and multilayer perceptrons with sigmoidal hidden units. We also tested the radial-basis function network which is a combination of local and distributed networks. Four criteria are taken for comparison: Correct classification of the test set, network size, learning time, and the operational complexity. We found that perceptrons when the architecture is suitable, generalize better than local, memory-based kernel estimators but require longer training and more precise computation. Local networks are simple, learn very quickly and acceptably, but use more memory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aha DW, Kibler D, </author> <title> Albert MK. Instance-based Learning Methods, </title> <booktitle> Machine Learning 1991; 6: </booktitle> <pages> 37-66. </pages>
Reference-contexts: Minimizing both leads to the minimal consistent subset. We believe however that finding the optimal subset is an NP-complete problem and thus algorithms with feasible complexity will yield sub-optimal solutions. The Condensed nearest neighbor rule [18] or the Grow and Learn algorithm [2] or instance-based learning (e.g. IBL2 in <ref> [1] </ref>) passes one by one over the samples and adds TABLE III Performance results (average standard deviation) with the Grow and Learn method.
Reference: [2] <author> Alpaydn E. </author> <title> Neural Models of Incremental Supervised and Unsupervised Learning, </title> <type> PhD dissertation, No 869, </type> <institution> Department d'Informatique, Ecole Polytechnique Federale de Lausanne, Switzerland, </institution> <year> 1990. </year>
Reference-contexts: 1. Kernel estimators [36] like k-nearest neighbor (k-nn) [11], Parzen windows, generalized k-nn, and Grow and Learn <ref> [2] </ref>, which is a variant of the condensed nearest neighbor [18]. We have also tested fuzzy k nn [21] which is quite similar to these methods. 2. Linear and pairwise separating perceptrons trained using delta rule [11] or multi-layer perceptrons with sigmoid hidden units trained using back-propagation [33]. 3. <p> Minimizing both leads to the minimal consistent subset. We believe however that finding the optimal subset is an NP-complete problem and thus algorithms with feasible complexity will yield sub-optimal solutions. The Condensed nearest neighbor rule [18] or the Grow and Learn algorithm <ref> [2] </ref> or instance-based learning (e.g. IBL2 in [1]) passes one by one over the samples and adds TABLE III Performance results (average standard deviation) with the Grow and Learn method.
Reference: [3] <author> Alpaydn E. GAL: </author> <title> Networks That Grow When They Learn and Shrink When They Forget, </title> <institution> International Computer Science Institute TR 91-032, Berkeley, USA, </institution> <year> 1991. </year>
Reference-contexts: Our approach here is trial and error to determine the optimal number of free prameters. Another approach is the incremental modification of the network structure by piecewise adding/removing units/links (See review in <ref> [3] </ref>). Still another is to train many and take a vote where weights in vote are computed taking into account the success and complexities of the voting networks [5]. We have not tried these two latter approaches in the present article. III. Local vs. <p> The classification rule is 1-nn but other k-nn variants are also possible at the expense of more computation. This of course does not guarantee finding the minimal subset and furthermore, different subsets are found when the training set order is changed. It is shown <ref> [3] </ref> that the bias introduced due to particular training set orders can be decreased by training multiple GAL networks with the same training set but ordered differently and taking a vote. The results achieved using this method is given in Table III.
Reference: [4] <author> Alpaydn E. </author> <title> Single and Multiple Memory-Based Neural Networks for Efficient Function Approximation. </title> <editor> In: O. Kaynak G. Honderd, E. Grant (Eds.) </editor> <title> Intelligent Systems: Safety, Reliability, </title> <journal> and Maintainability (pp. </journal> <pages> 194-204), </pages> <publisher> NATO ASI series, Springer, </publisher> <year> 1993. </year>
Reference-contexts: This type of a network is also called memory-based <ref> [4] </ref> as W make up a memory and the network basically functions like a table look-up. <p> There are many consistent subsets and one normally is interested in the minimal consistent subset, i.e., the subset with the minimum cardinality. Finding this corresponds to minimizing the following error measure <ref> [4] </ref> analogous to Eq. (2): E (W ) = x i L (x i ) + fljW j (12) L (x i ) = &lt; 1; if kW c x i k = min j kW j x i k and class (x i ) 6= class (W c ); 0;
Reference: [5] <author> Alpaydn E. </author> <title> Multiple Networks for Function Learning, </title> <booktitle> International Neural Network Conference, Vol 1, </booktitle> <address> San Francisco, USA, </address> <year> 1993: </year> <pages> 9-14. </pages>
Reference-contexts: Another approach is the incremental modification of the network structure by piecewise adding/removing units/links (See review in [3]). Still another is to train many and take a vote where weights in vote are computed taking into account the success and complexities of the voting networks <ref> [5] </ref>. We have not tried these two latter approaches in the present article. III. Local vs. Distributed Network Architectures The type of neural networks we are interested in for pattern classification are of feedforward type with one hidden layer.
Reference: [6] <author> Blum EK, Li LK. </author> <title> Approximation Theory and Feedforward Networks, </title> <booktitle> Neural Networks 1991; 4: </booktitle> <pages> 511-515. </pages>
Reference-contexts: This is a universal learning method. Just as any boolean function can be learned as a disjunction of conjunctions, any continuous function can be approximated to a required precision by a juxtaposition of piecewise constant functions <ref> [6] </ref>. There is no reasonable upper bound on the size of the table. We can divide local or memory-based methods into three: * Kernel estimators. The table size is as large as the training set and we interpolate from the training samples directly.
Reference: [7] <author> Carpenter GA, Grossberg S. </author> <title> The ART of Adaptive Pattern Recognition by a Self-Organizing Neural Network, </title> <booktitle> IEEE Computer 1988; 21: </booktitle> <pages> 77-88. </pages>
Reference-contexts: One example is the Grow and Learn method which we discuss in section 4.3. Another way to achieve data reduction is by clustering as in Learning Vector Quantization [22] or Adaptive Resonance Theory <ref> [7] </ref>. Yet another way is to extract the statistically important features, like the principal components, and make classification based on these; this technique is beyond the scope of this paper. * Sparse tables. Here the table is much larger than the training set.
Reference: [8] <author> Cover TM, Hart PE. </author> <title> Nearest Neighbor Pattern Classification, </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 1967; 13: </volume> <pages> 21-27. </pages> <note> [9] le Cun Y, </note> <author> Boser B, Denker JS, Henderson D, Howard RE, Hubbard W, Jackel LD. </author> <title> Handwritten Digit Recognition with a Back-Propagation Network. </title> <editor> In Touretzky, D. (Ed.) </editor> <booktitle> Advances in Neural Information Processing Systems 2 (pp. </booktitle> <pages> 396-404), </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The good h value is to be found using trial-and-error. In the case of generalized k-nn, instead of using one global h, we use different values depending on the probability density in the region. For the k-nearest neighbor, it has been shown <ref> [8] </ref> that the performance of the 1-nearest neighbor in classification is never worse than twice the Bayesian risk where complete knowledge of the distributions are assumed. It can thus be said that at least half of this knowledge is provided by the nearest neighbor.
Reference: [10] <author> Denoeux T, Lengelle R. </author> <title> Initializing Back Propagation Networks with Prototypes, </title> <booktitle> Neural Networks 1993; 6: </booktitle> <pages> 351-363. </pages>
Reference-contexts: One approach is to use k-means clustering [11] to compute the Gaussian centers. Another approach, named the anchor method, is to take a subset of the original samples randomly from the training set [24], [15]. Donoeux and Lengelle <ref> [10] </ref> propose to initialize an MLP with inputs and weights normalized to a constant length (as MLP uses the dot product as the distance measure instead of the Eu-clidean distance as in RBF). , the spread of Gaussians, is computed as the average of inter-center distances.
Reference: [11] <author> Duda RO, Hart PE. </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: 1. Kernel estimators [36] like k-nearest neighbor (k-nn) <ref> [11] </ref>, Parzen windows, generalized k-nn, and Grow and Learn [2], which is a variant of the condensed nearest neighbor [18]. We have also tested fuzzy k nn [21] which is quite similar to these methods. 2. Linear and pairwise separating perceptrons trained using delta rule [11] or multi-layer perceptrons with sigmoid <p> [36] like k-nearest neighbor (k-nn) <ref> [11] </ref>, Parzen windows, generalized k-nn, and Grow and Learn [2], which is a variant of the condensed nearest neighbor [18]. We have also tested fuzzy k nn [21] which is quite similar to these methods. 2. Linear and pairwise separating perceptrons trained using delta rule [11] or multi-layer perceptrons with sigmoid hidden units trained using back-propagation [33]. 3. Radial-basis functions [29] which is a two layer net work with Gaussian hidden units. C. Performance Measures The four criteria we used for comparing these methods are: 1. <p> For ^p to be a legitemate density function, K should be nonnegative and integrate to one. For a smooth approximation, K is generally taken to be the normal density <ref> [11] </ref>: K (u) = 1 2 exp 2 (9) Here the kernel estimator is a sum of "bumps" placed at the observations. K determines the shape of the bumps while h determines their widths. * K-nearest neighbor (k-nn). <p> Pairwise Separation of Classes The linear perceptron defines a hyperplane that separates all the positive examples of a class from all its negative examples. When classes are not linearly separable, one can use pairwise separation where for each pair of classes we have a linear perceptron <ref> [11] </ref>. When we have n classes, we need n (n 1)=2 perceptrons. The class regions are convex and singly connected in such a case. C. <p> One approach is to use k-means clustering <ref> [11] </ref> to compute the Gaussian centers. Another approach, named the anchor method, is to take a subset of the original samples randomly from the training set [24], [15].
Reference: [12] <author> Fukushima K. </author> <title> Neocognitron: A Hierarchical Neural Network Capable of Visual Pattern Recognition, </title> <booktitle> Neural Networks 1988; 1: </booktitle> <pages> 119-130. </pages>
Reference: [13] <author> Funahashi K. </author> <title> On the Approximate Realization of Continuous Mapping by Neural Networks, </title> <booktitle> Neural Networks 1989; 2: </booktitle> <pages> 183-192. </pages>
Reference-contexts: The class regions are convex and singly connected in such a case. C. Multi-Layer Perceptrons (MLP) In the case of multi-layer perceptrons, we have a feed-forward network with one hidden layer as given in Eq. (3) with the sigmoid basis function given in Eq. (5). It has been shown <ref> [13] </ref>, [19] that this type of a neural network is a universal approximator, i.e., can approximate any continuous function with desired accuracy. It has also been shown [34] that multi-layer perceptrons estimate Bayesian posterior probabilities, building a link between multi-layer distributed networks and local kernel estimators.
Reference: [14] <author> Furui S. </author> <title> Digital Speech Processing, Synthesis, and Recognition, </title> <publisher> Marcel Dekker, </publisher> <year> 1989. </year>
Reference: [15] <author> Gallant SI. </author> <title> Neural Network Learning and Expert Systems, </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: One approach is to use k-means clustering [11] to compute the Gaussian centers. Another approach, named the anchor method, is to take a subset of the original samples randomly from the training set [24], <ref> [15] </ref>. Donoeux and Lengelle [10] propose to initialize an MLP with inputs and weights normalized to a constant length (as MLP uses the dot product as the distance measure instead of the Eu-clidean distance as in RBF). , the spread of Gaussians, is computed as the average of inter-center distances.
Reference: [16] <author> Geman S, Bienenstock E, Doursat R. </author> <title> Neural Networks and the Bias/Variance Dilemma, </title> <booktitle> Neural Computation 1992; 4: </booktitle> <pages> 1-58. </pages>
Reference-contexts: Each example is an isolated, normalized 16 by 16 binary image. One half is used for training and the second half for testing the network models. Other results on the same database are given in [17], <ref> [16] </ref>. For /b,d,g,m,n,N/ phoneme recognition experiments, the database contains 5240 Japanese isolated words and phrases. One hundred samples for each class (total of 600 samples) are taken from the even-numbered and odd-numbered words for training and testing. Phrases are used only for testing after having trained on isolated words. <p> Generalized k-nn and fuzzy k-nn that we discuss in the next section, because they give less weight to distant neighbors perform better than the k-nn proper. When h (or k) is decreased to decrease bias, variance increases and vice versa. This can intuitively be explained as follows <ref> [16] </ref>. When h (or k) is large, ^p is the weighted average of many samples and thus does not change much from one sample to another.
Reference: [17] <author> Guyon I, Poujoud I, Personnaz L, Dreyfus G, Denker J, le Cun Y. </author> <title> Comparing Different Neural Architectures for Classi fying Handwritten Digits," </title> <booktitle> International Joint Conference on Neural Networks 1989, </booktitle> <volume> vol 2, </volume> <pages> 127-132, </pages> <address> Washington USA. </address>
Reference-contexts: Each example is an isolated, normalized 16 by 16 binary image. One half is used for training and the second half for testing the network models. Other results on the same database are given in <ref> [17] </ref>, [16]. For /b,d,g,m,n,N/ phoneme recognition experiments, the database contains 5240 Japanese isolated words and phrases. One hundred samples for each class (total of 600 samples) are taken from the even-numbered and odd-numbered words for training and testing. Phrases are used only for testing after having trained on isolated words.
Reference: [18] <author> Hart PE. </author> <title> The Condensed Nearest Neighbor Rule, </title> <journal> IEEE Transactions on Information Theory 1968; 14: </journal> <pages> 515-516. </pages>
Reference-contexts: 1. Kernel estimators [36] like k-nearest neighbor (k-nn) [11], Parzen windows, generalized k-nn, and Grow and Learn [2], which is a variant of the condensed nearest neighbor <ref> [18] </ref>. We have also tested fuzzy k nn [21] which is quite similar to these methods. 2. Linear and pairwise separating perceptrons trained using delta rule [11] or multi-layer perceptrons with sigmoid hidden units trained using back-propagation [33]. 3. <p> C. Grow and Learn (GAL) The k-nn becomes infeasible when the training set is large. One possibility in minimizing the number of stored samples is by storing only a subset of the training set. This subset, named the consistent subset <ref> [18] </ref>, contains those samples that can classify the remaining samples correctly using the nearest neighbor rule. There are many consistent subsets and one normally is interested in the minimal consistent subset, i.e., the subset with the minimum cardinality. <p> Minimizing both leads to the minimal consistent subset. We believe however that finding the optimal subset is an NP-complete problem and thus algorithms with feasible complexity will yield sub-optimal solutions. The Condensed nearest neighbor rule <ref> [18] </ref> or the Grow and Learn algorithm [2] or instance-based learning (e.g. IBL2 in [1]) passes one by one over the samples and adds TABLE III Performance results (average standard deviation) with the Grow and Learn method.
Reference: [19] <author> Hornik K, Stinchcombe M, White H. </author> <title> Multilayer Feedforward Networks are Universal Approximators, </title> <booktitle> Neural Networks 1989; 2: </booktitle> <pages> 359-366. </pages>
Reference-contexts: C. Multi-Layer Perceptrons (MLP) In the case of multi-layer perceptrons, we have a feed-forward network with one hidden layer as given in Eq. (3) with the sigmoid basis function given in Eq. (5). It has been shown [13], <ref> [19] </ref> that this type of a neural network is a universal approximator, i.e., can approximate any continuous function with desired accuracy. It has also been shown [34] that multi-layer perceptrons estimate Bayesian posterior probabilities, building a link between multi-layer distributed networks and local kernel estimators.
Reference: [20] <author> Kanerva, P. </author> <title> Sparse Distributed Memory, </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: Here the table is much larger than the training set. The samples are not stored directly but are duplicated in several places where they "blend" with other samples and this causes interpolation to similar data. CMAC [28] and Sparse Distributed Memory <ref> [20] </ref> are examples. We have not tested these methods in this present article. A. Kernel Estimation of Class Densities In parametric estimation, we assume the knowledge of a certain form of density family whose parameters we estimate from the data.
Reference: [21] <author> Keller JM, </author> <title> Gray MR, Givens JA. A Fuzzy K-Nearest Neighbor Algorithm, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 1985; 15: </journal> <pages> 580-585. </pages>
Reference-contexts: 1. Kernel estimators [36] like k-nearest neighbor (k-nn) [11], Parzen windows, generalized k-nn, and Grow and Learn [2], which is a variant of the condensed nearest neighbor [18]. We have also tested fuzzy k nn <ref> [21] </ref> which is quite similar to these methods. 2. Linear and pairwise separating perceptrons trained using delta rule [11] or multi-layer perceptrons with sigmoid hidden units trained using back-propagation [33]. 3. Radial-basis functions [29] which is a two layer net work with Gaussian hidden units. C. <p> The same argument holds with k-nn, with k in place of h. Choosing h or k implies a trade-off between bias (systematic error) and variance (random error). B. Fuzzy k-nn A related work is the "fuzzy" k-nn <ref> [21] </ref> where the computation of the fuzzy memberships is similar to the computation of class densities above.
Reference: [22] <author> Kohonen, T. </author> <title> Self-Organization and Associative Memory, </title> <publisher> Springer, </publisher> <year> 1988. </year>
Reference-contexts: One way is to define a criterion of importance for samples and to store only the important samples in the table. One example is the Grow and Learn method which we discuss in section 4.3. Another way to achieve data reduction is by clustering as in Learning Vector Quantization <ref> [22] </ref> or Adaptive Resonance Theory [7]. Yet another way is to extract the statistically important features, like the principal components, and make classification based on these; this technique is beyond the scope of this paper. * Sparse tables. Here the table is much larger than the training set.
Reference: [23] <author> Lee Y, Lippmann R. </author> <title> Practical Characteristics of Neural Network and Conventional Pattern Classifiers on Artificial and Speech Problems. </title> <editor> In D. Touretzky (Ed.) </editor> <booktitle> Advances in Neural Information Processing Systems 2 (pp. </booktitle> <pages> 168-177), </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [24] <author> Lee Y. </author> <title> Handwritten Digit Recognition Using K-Nearest-Neighbor, Radial-Basis Function, and Backpropagation Neural Networks, </title> <booktitle> Neural Computation 1991; 3: </booktitle> <pages> 440-449. </pages>
Reference-contexts: One approach is to use k-means clustering [11] to compute the Gaussian centers. Another approach, named the anchor method, is to take a subset of the original samples randomly from the training set <ref> [24] </ref>, [15].
Reference: [25] <author> Lippmann RP. </author> <title> Review of Neural Networks for Speech Recognition, </title> <booktitle> Neural Computation 1989; 1: </booktitle> <pages> 1-38. </pages>
Reference: [26] <author> Martin GL, Pittman JA. </author> <title> Recognizing Hand-Printed Letters and Digits Using Backpropagation Learning. </title> <editor> In D. Touretzky (Ed.) </editor> <booktitle> Advances in Neural Information Processing Systems 2 (pp. </booktitle> <pages> 405-414), </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [27] <author> Matan O, Baird HS, Bromley J, Burges CJC, Denker JS, Jackel LD, le Cun Y, Pednault EPD, Satterfield WD, Stenard CE, Thompson TJ. </author> <title> Reading Handwritten Digits: A Zip Code Recognition System, </title> <booktitle> IEEE Computer 1992; 25(7): </booktitle> <pages> 59-62. </pages>
Reference: [28] <author> Miller III WT, Glanz FH, Kraft III LG. </author> <title> CMAC: An Associative Neural Network Alternative to Backpropagation, </title> <booktitle> Proceedings of the IEEE 1990; 78: </booktitle> <pages> 61-1567. </pages>
Reference-contexts: Here the table is much larger than the training set. The samples are not stored directly but are duplicated in several places where they "blend" with other samples and this causes interpolation to similar data. CMAC <ref> [28] </ref> and Sparse Distributed Memory [20] are examples. We have not tested these methods in this present article. A. Kernel Estimation of Class Densities In parametric estimation, we assume the knowledge of a certain form of density family whose parameters we estimate from the data.
Reference: [29] <author> Moody J, Darken CJ. </author> <title> Fast Learning in Networks of Locally-Tuned Processing Units, </title> <booktitle> Neural Computation 1989; 1: </booktitle> <pages> 281-294. </pages>
Reference-contexts: We have also tested fuzzy k nn [21] which is quite similar to these methods. 2. Linear and pairwise separating perceptrons trained using delta rule [11] or multi-layer perceptrons with sigmoid hidden units trained using back-propagation [33]. 3. Radial-basis functions <ref> [29] </ref> which is a two layer net work with Gaussian hidden units. C. Performance Measures The four criteria we used for comparing these methods are: 1. Generalization accuracy, measured in percent correct classification of patterns unseen during training, 2. Learning time, measured in epochs made over the training set, 3. <p> VI. Radial-Basis Functions A radial-basis function network <ref> [29] </ref>, [32] has one hidden layer with hidden units having Gaussian activation functions as opposed to a sigmoid: g h (W h ; x) = exp 2 (16) Unlike Parzen windows where we have one Gaussian for each sample, in radial-basis function networks we have less. <p> One may also train both layers using back-propagation. This is not a good idea as unless an additional term is added, the spread of Gaussians need not stay small so locality may be lost and also this will not be any faster or better than usual back-propagation <ref> [29] </ref>. With a small training sample, a distributed scheme is generally preferrable to a local scheme. A distributed unit is trained with all the samples; a local unit is trained by a small percentage of it.
Reference: [30] <author> Ng K, Lippmann R. </author> <title> Practical Characteristics of Neural Network and Conventional Pattern Classifiers. </title> <editor> In R. Lippmann, J. Moody, D. Touretzky (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 3 (pp. </booktitle> <pages> 970-976), </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [31] <author> Omohundro SM. </author> <title> Efficient Algorithms with Neural Network Behavior, </title> <journal> Complex Systems, </journal> <volume> 1987; 1: </volume> <pages> 273-347. </pages>
Reference-contexts: Finding the closest neighbor, requiring a "winner-take-all" mechanism implemented as a network of units, can be cheaply implemented in VLSI and there already are circuits for this. In software, finding the closest may be decreased to log time by using advanced data structures like k-d trees <ref> [31] </ref> at the expense of increasing the complexity of the program. Distributed methods use back-propagation which requires bi-directional flow of precise values and arithmetical circuitry manipulating those precise values. This is costly to do both in software simulations and hardware implementations. RBF is also somewhat costly to implement.
Reference: [32] <author> Poggio T, Girosi F. </author> <title> Networks for Approximation and Learning, </title> <booktitle> Proceedings of the IEEE 1990; 78: </booktitle> <pages> 1481-1497. </pages>
Reference-contexts: VI. Radial-Basis Functions A radial-basis function network [29], <ref> [32] </ref> has one hidden layer with hidden units having Gaussian activation functions as opposed to a sigmoid: g h (W h ; x) = exp 2 (16) Unlike Parzen windows where we have one Gaussian for each sample, in radial-basis function networks we have less.
Reference: [33] <author> Rumelhart DE, Hinton GE, Williams RJ. </author> <title> Learning Internal Representations by Error Propagation. </title> <editor> In D.E. Rumelhart, J.L. McClelland and the PDP Research Group (Eds). </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition 1 (pp. </booktitle> <pages> 318-362), </pages> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: We have also tested fuzzy k nn [21] which is quite similar to these methods. 2. Linear and pairwise separating perceptrons trained using delta rule [11] or multi-layer perceptrons with sigmoid hidden units trained using back-propagation <ref> [33] </ref>. 3. Radial-basis functions [29] which is a two layer net work with Gaussian hidden units. C. Performance Measures The four criteria we used for comparing these methods are: 1. Generalization accuracy, measured in percent correct classification of patterns unseen during training, 2.
Reference: [34] <author> Ruck DW, Rogers SK, Kabrisky M, Oxley ME, Suter BW. </author> <title> The Multi-layer Perceptron as an Approximation to a Bayes Optimal Discriminant Function, </title> <booktitle> IEEE Transactions on Neural Networks 1990; 1: </booktitle> <pages> 296-298. </pages>
Reference-contexts: It has been shown [13], [19] that this type of a neural network is a universal approximator, i.e., can approximate any continuous function with desired accuracy. It has also been shown <ref> [34] </ref> that multi-layer perceptrons estimate Bayesian posterior probabilities, building a link between multi-layer distributed networks and local kernel estimators. These theorems do not tell how many hidden units are necessary, so one should try several alternatives and choose the best.
Reference: [35] <author> Senior AW. </author> <title> Off-line Handwriting Recognition: A Review and Experiments, </title> <institution> Cambridge University Engineering Department, </institution> <address> CUED/F-INFENG/TR 105, </address> <year> 1992. </year>
Reference: [36] <author> Silverman BW. </author> <title> Density Estimation for Statistics and Data Analysis, </title> <publisher> Chapman & Hall, </publisher> <year> 1986. </year>
Reference-contexts: 1. Kernel estimators <ref> [36] </ref> like k-nearest neighbor (k-nn) [11], Parzen windows, generalized k-nn, and Grow and Learn [2], which is a variant of the condensed nearest neighbor [18]. We have also tested fuzzy k nn [21] which is quite similar to these methods. 2. <p> We can divide local or memory-based methods into three: * Kernel estimators. The table size is as large as the training set and we interpolate from the training samples directly. This is basically kernel estimation of class densities <ref> [36] </ref>. K-nearest neighbors and Parzen windows discussed in the next two subsections are of this type. * Clustering methods. When the training set is large, decreasing table size is possible by removing redundancy. <p> Then using Bayes rule, we can compute the posterior probabilities: P (! j jx) = P (7) For minimum error rate, we select the category having the highest posterior probability: c = ArgMax k [P (! k jx)] (8) Depending on the shape of K, one can have various estimators <ref> [36] </ref>: TABLE I Percent recognition when Parzen windows is used as a function of h, the window width. Generalization is sensitive to h, with both small and large values of h decreasing success.
Reference: [37] <author> Waibel A, Hanazawa T, Hinton G, Shikano K, Lang KJ. </author> <title> Phoneme Recognition Using Time-Delay Neural Networks, </title> <booktitle> IEEE Transactions on Acoustics, Speech, and Signal Processing 1989; 37: </booktitle> <pages> 328-339. </pages>
Reference: [38] <author> Waibel A, Hampshire II JB. </author> <title> Neural Network Applications to Speech. </title> <editor> In P. Antognetti & V. Milutinovic (Eds). </editor> <booktitle> Neural Networks: Concepts, Applications, and Implementations 1 (pp. </booktitle> <pages> 54-76), </pages> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
References-found: 37

