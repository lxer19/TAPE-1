URL: http://www.cs.princeton.edu/~ras/TR-524-96.ps
Refering-URL: http://www.cs.princeton.edu/~ras/
Root-URL: http://www.cs.princeton.edu
Email: fras,felteng@cs.princeton.edu  
Title: Simplifying Distributed File Systems Using a Shared Logical Disk  
Author: Robert A. Shillner and Edward W. Felten 
Address: Princeton University  
Affiliation: Dept. of Computer Science  
Abstract: We present a new approach to building non-centralized ("serverless") distributed file systems, using the shared logical disk, which provides the abstraction of a fault-tolerant, entry-consistent, shared array of persistent data blocks. By separating the storage management level from higher-level file system functions like metadata management, we significantly reduce the overall complexity of the file system's implementation. We describe the implementation of a file system and an underlying shared logical disk, designed to run on workstation networks and on the SHRIMP multicomputer. The file system level is very similar to a conventional uniprocessor file system, relying on the underlying shared logical disk to handle all communication, caching, and coherence operations. The shared logical disk provides some simple hooks to allow the file system to express its fault-tolerance requirements. We believe the resulting design is much simpler than other non-centralized distributed file systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas E. Anderson, Michael D. Dahlin, Jeanna M. Neefe, David A. Patterson, Drew S. Roselli, and Ran-dolph Y. Wang. </author> <title> Serverless network file systems. </title> <booktitle> In Proceedings of the 15th Symposium on Operating System Principles, </booktitle> <pages> pages 109-126, </pages> <year> 1995. </year>
Reference-contexts: The shared logical disk's performance suffers from false dependencs, but it uses a much simpler mechanism and does not need any knowledge about file system data structures. Both DataMesh [23] and xFS <ref> [1] </ref> seek to use a central, trusted core of machines as a scalable file server for a larger, less-trusted group of file system clients. <p> In both cases, the core communicates with the external clients using a standard protocol, such as NFS [21], while core machines communicate internally using high-performance proprietary protocols tuned to the core environment. Sawmill [22], Zebra [12] and xFS <ref> [1] </ref> are network file systems based on log-structuring [20] and RAID [19]. Sawmill is a centralized-server system which stores data on a high-bandwidth disk array; specialized hardware allows data to flow directly between the disk array and the network without going through the server's memory system.
Reference: [2] <author> C. R. Attanasio, M. Butrico, C. A. Polyzois, S. E. Smith, and J. L. Peterson. </author> <title> Design and implementation of a recoverable virtual shared disk. </title> <type> Technical Page 10 report, </type> <institution> IBM T. J. Watson Research Center, </institution> <address> York--town Heights, NY, </address> <year> 1994. </year>
Reference-contexts: From de Jonge et al. we take the logical disk concept [8], which we extend to the distributed case. From Attanasio et al. we take the idea of striping a logical device across distributed physical devices <ref> [2] </ref>, which we extend by adding caching and cache coherence. From Mann et al. we take the idea of expressing consistency requirements as ordering constraints on propagation of dirty writes to disk [14], which we extend by applying it to cache-to-cache transfers. <p> Each SHRIMP node can serve data to as many clients as the Ethernet's bandwidth allows, and total system throughput can scale as more SHRIMP nodes are added. 5 Related work IBM's Virtual Shared Disk <ref> [2] </ref> allows multiple workstations to access remote disks as if they were connected locally; multiple (possibly distributed) physical disks may be grouped together to form a single virtual disk. VSD supports read-block and write-block operations; there is no caching or locking. <p> Another area for future study is alternative logical-to-physical block mappings, both static (such as the current round-robin) and dynamic (such as a log-structured approach [8]). Finally, we may integrate the shared logical disk into the Linux kernel. IBM's Virtual Shared Disk <ref> [2] </ref> exists as a psuedo device driver with a device special file in /dev; we can do something similar.
Reference: [3] <author> Maurice J. Bach. </author> <title> The Design of the Unix Operating System. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1986. </year>
Reference-contexts: Genuine write-sharing can also be a performance problem in a distributed file system built on a shared logical disk. For instance, in the Unix file system <ref> [3] </ref>, the superblock contains a pool of free blocks; when a process needs to allocate a block, it takes one from the pool in the superblock. <p> Other examples of write-shared resources are the Unix file system's free-inode cache and the Fast File System's [15] block-use bitmaps. 4.2 On-disk data structures Our on-disk data structures are similar to those found in the traditional Unix file system <ref> [3] </ref>. Each file has an inode, which contains some metadata (file size, file type, link count, owner, timestamps, etc) as well as pointers to the file's data blocks. Directories are kept in files; each directory entry consists of a file name and an inode number.
Reference: [4] <author> Andrew D. Birrell, Andy Hisgen, Chuck Jerian, Timothy Mann, and Garret Swart. </author> <title> The Echo distributed file system. </title> <type> Research Report 111, </type> <institution> Digital Equipment Corporation Systems Research Center, </institution> <address> Palo Alto, CA, </address> <year> 1993. </year>
Reference-contexts: The designers seem to expect distributed databases to be the most common application of VSD. The shared logical disk's token-based cache-coherence protocol is based on similar protocols used in distributed file systems such as Sprite [18, 17], AFS [13] and Echo <ref> [4, 14] </ref>. While these systems vary in the details of the coherence protocol (particularly in cases of concurrent write-sharing), their basic idea is the same: clients acquire read and write tokens before accessing file data, and the servers revoke these tokens when other clients request conflicting tokens.
Reference: [5] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathan Sandberg. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <year> 1994. </year>
Reference-contexts: The numbers reflect the initial state of the implementation; we expect the performance to improve as we tune the prototype to fit our hardware and software environment. 3.1 Apparatus We took performance measurements of the shared logical disk running on two nodes of the Princeton SHRIMP multicomputer <ref> [5, 10] </ref>. SHRIMP consists of a number of ordinary Linux Pentium PCs connected by an Intel Paragon backplane. SHRIMP uses hardware support to provide protected, low-latency, user-level communication.
Reference: [6] <author> Michael D. Dahlin, Randolph Y. Wang, Thomas E. Anderson, and David A. Patterson. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In Proceedings of the 1st Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 267-280, </pages> <year> 1994. </year>
Reference-contexts: An important difference between these systems and the shared logical disk is the granularity of coherence; in these systems the granularity is an entire file, while for the shared logical disk it is an individual disk block. Cooperative caching schemes <ref> [6, 9] </ref> seek to use the disk caches of networked machines collectively. These designs perform better than a traditional memory hierarchy (local cache, server cache, server disk) because they can use the memories of idle clients. Aggressive strategies approximate a global least-recently-used cache replacement policy for all cached data.
Reference: [7] <author> Stefanos N. Damianakis, Cezary Dubnicki, and Ed-ward W. Felten. </author> <note> Stream sockets on SHRIMP. Submitted for publication. </note>
Reference-contexts: The library instances communicate via stream sockets, either over a TCP/IP internet or over the SHRIMP interconnect using a fast sockets library <ref> [7] </ref> developed for the SHRIMP multi-computer. The library runs on Linux PCs and uses a custom user-level threads package for the 80x86 processor.
Reference: [8] <author> Wiedbren de Jonge, M. Frans Kaashoek, and Wil-son C. Hsieh. </author> <title> The logical disk: A new approach to improving file systems. </title> <booktitle> In Proceedings of the 14th Symposium on Operating System Principles, </booktitle> <pages> pages 15-28, </pages> <year> 1993. </year>
Reference-contexts: The result is a major simplification of the file system implementation. Our approach is based on picking some ideas from the work of others, and combining them with some new ideas, particularly in the area of fault tolerance. From de Jonge et al. we take the logical disk concept <ref> [8] </ref>, which we extend to the distributed case. From Attanasio et al. we take the idea of striping a logical device across distributed physical devices [2], which we extend by adding caching and cache coherence. <p> The result of combining these ideas, and adding some support for fault-tolerance, is a base layer that makes it easy to write non-centralized distributed file systems. 1.1 Logical disk The shared logical disk is based on concept of logical disk <ref> [8] </ref>. A logical disk is an array of fixed-size data blocks, which processes may use to stably store persistent data. Processes refer to the data blocks via logical block numbers; the logical disk maps these block numbers onto physical disk addresses. <p> The logical disk implementation may even store the data on multiple disks or on remote disks; the mapping is transparent to the user. For example, de Jonge, et. al. <ref> [8] </ref> use a logical disk to store data in a log-structured fashion; except for the increased performance, the log-structuring is invisible to the layers above it. <p> disk before the inode. 1 The client could achieve the same effect by calling sync on the inode block after writing it, but the dependence approach uses nonblocking delayed writes, while the sync approach must wait for the synchronous write of the inode to complete. 1.3 Benefits Jonge, et. al. <ref> [8] </ref> note several advantages that a non-shared logical disk provides for single-node I/O. First, separating file layout (mapping of file blocks to logical blocks) from disk layout (mapping of logical blocks to disk blocks) leads to simpler file system design and implementation. <p> The shared logical disk can also be extended to transparently store redundant data, both for availability and for protection from media failures. Another area for future study is alternative logical-to-physical block mappings, both static (such as the current round-robin) and dynamic (such as a log-structured approach <ref> [8] </ref>). Finally, we may integrate the shared logical disk into the Linux kernel. IBM's Virtual Shared Disk [2] exists as a psuedo device driver with a device special file in /dev; we can do something similar.
Reference: [9] <author> Michael J. Feeley, William E. Morgan, Frederic H. Pighin, Anna R. Karlin, Henry M. Levy, and Chan-dramohan A. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> In Proceedings of the 15th Symposium on Operating System Principles, </booktitle> <pages> pages 201-212, </pages> <year> 1995. </year>
Reference-contexts: An important difference between these systems and the shared logical disk is the granularity of coherence; in these systems the granularity is an entire file, while for the shared logical disk it is an individual disk block. Cooperative caching schemes <ref> [6, 9] </ref> seek to use the disk caches of networked machines collectively. These designs perform better than a traditional memory hierarchy (local cache, server cache, server disk) because they can use the memories of idle clients. Aggressive strategies approximate a global least-recently-used cache replacement policy for all cached data.
Reference: [10] <author> Edward W. Felten, Richard Alpert, Angelos Bilas, Matthias A. Blumrich, Douglas W. Clark, Stefanos Damianakis, Cezary Dubnicki, Liviu Iftode, and Kai Li. </author> <title> Early experience with message-passing on the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 23rd International Symposium on Computer Architecture, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: The numbers reflect the initial state of the implementation; we expect the performance to improve as we tune the prototype to fit our hardware and software environment. 3.1 Apparatus We took performance measurements of the shared logical disk running on two nodes of the Princeton SHRIMP multicomputer <ref> [5, 10] </ref>. SHRIMP consists of a number of ordinary Linux Pentium PCs connected by an Intel Paragon backplane. SHRIMP uses hardware support to provide protected, low-latency, user-level communication.
Reference: [11] <author> Gregory R. Ganger and Yale N. Patt. </author> <title> Metadata update performance in file systems. </title> <booktitle> In Proceedings of the 1st Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 49-60, </pages> <year> 1994. </year>
Reference-contexts: Aggressive strategies approximate a global least-recently-used cache replacement policy for all cached data. Although we believe the shared logical disk is not incompatible with cooperative caching, our current prototype does not implement it. Soft updates <ref> [11] </ref> are a technique for safely using delayed writes for filesystem metadata. Like the shared logical disk's dependences, soft updates allow a filesystem to specify the order in which certain writes reach disk.
Reference: [12] <author> John H. Hartman and John K. Ousterhout. </author> <title> The Zebra striped network file system. </title> <booktitle> In Proceedings of the 14th Symposium on Operating System Principles, </booktitle> <pages> pages 29-43, </pages> <year> 1993. </year>
Reference-contexts: In both cases, the core communicates with the external clients using a standard protocol, such as NFS [21], while core machines communicate internally using high-performance proprietary protocols tuned to the core environment. Sawmill [22], Zebra <ref> [12] </ref> and xFS [1] are network file systems based on log-structuring [20] and RAID [19]. Sawmill is a centralized-server system which stores data on a high-bandwidth disk array; specialized hardware allows data to flow directly between the disk array and the network without going through the server's memory system.
Reference: [13] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: A subsequent shared logical disk implementation could support data mirroring, RAID-style parity [19] or a hierarchical combination of both [24]. 2.2 Cache coherence We use a token-based write-back cache coherence protocol similar to that used in some distributed file systems <ref> [13, 14, 17] </ref>, except that in our system the unit of coherence is a logical disk block instead of a file. <p> The designers seem to expect distributed databases to be the most common application of VSD. The shared logical disk's token-based cache-coherence protocol is based on similar protocols used in distributed file systems such as Sprite [18, 17], AFS <ref> [13] </ref> and Echo [4, 14]. While these systems vary in the details of the coherence protocol (particularly in cases of concurrent write-sharing), their basic idea is the same: clients acquire read and write tokens before accessing file data, and the servers revoke these tokens when other clients request conflicting tokens.
Reference: [14] <author> Timothy Mann, Andrew Birrell, Andy Hisgen, Charles Jerian, and Garret Swart. </author> <title> A coherent distributed file cache with directory write-behind. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 12(2) </volume> <pages> 123-164, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: From Attanasio et al. we take the idea of striping a logical device across distributed physical devices [2], which we extend by adding caching and cache coherence. From Mann et al. we take the idea of expressing consistency requirements as ordering constraints on propagation of dirty writes to disk <ref> [14] </ref>, which we extend by applying it to cache-to-cache transfers. <p> A subsequent shared logical disk implementation could support data mirroring, RAID-style parity [19] or a hierarchical combination of both [24]. 2.2 Cache coherence We use a token-based write-back cache coherence protocol similar to that used in some distributed file systems <ref> [13, 14, 17] </ref>, except that in our system the unit of coherence is a logical disk block instead of a file. <p> The designers seem to expect distributed databases to be the most common application of VSD. The shared logical disk's token-based cache-coherence protocol is based on similar protocols used in distributed file systems such as Sprite [18, 17], AFS [13] and Echo <ref> [4, 14] </ref>. While these systems vary in the details of the coherence protocol (particularly in cases of concurrent write-sharing), their basic idea is the same: clients acquire read and write tokens before accessing file data, and the servers revoke these tokens when other clients request conflicting tokens.
Reference: [15] <author> Marshall K. McKusick, William N. Joy, Samuel J. Le*er, and Robert S. Fabry. </author> <title> A fast file system for Unix. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-197, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: Other examples of write-shared resources are the Unix file system's free-inode cache and the Fast File System's <ref> [15] </ref> block-use bitmaps. 4.2 On-disk data structures Our on-disk data structures are similar to those found in the traditional Unix file system [3]. Each file has an inode, which contains some metadata (file size, file type, link count, owner, timestamps, etc) as well as pointers to the file's data blocks.
Reference: [16] <author> Sape J. Mullender and Andrew S. Tanenbaum. </author> <title> Immediate files. </title> <journal> Software Practice and Experience, </journal> <volume> 14(4) </volume> <pages> 365-368, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: To avoid false sharing between inodes, our file system expands the inode structure to fill an entire block. Since most Unix files are small, block-sized inodes waste much space. To compensate for this, the file system stores small files in the inode block itself <ref> [16] </ref>. If a small file later grows too large to fit in the inode block, the system converts the file to the normal storage method.
Reference: [17] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite network file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: A subsequent shared logical disk implementation could support data mirroring, RAID-style parity [19] or a hierarchical combination of both [24]. 2.2 Cache coherence We use a token-based write-back cache coherence protocol similar to that used in some distributed file systems <ref> [13, 14, 17] </ref>, except that in our system the unit of coherence is a logical disk block instead of a file. <p> The designers seem to expect distributed databases to be the most common application of VSD. The shared logical disk's token-based cache-coherence protocol is based on similar protocols used in distributed file systems such as Sprite <ref> [18, 17] </ref>, AFS [13] and Echo [4, 14].
Reference: [18] <author> John K. Ousterhout, Andrew R. Cherenson, Freder-ick Douglis, Michael N. Nelson, and Brent B. Welch. </author> <title> The Sprite network operating system. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The designers seem to expect distributed databases to be the most common application of VSD. The shared logical disk's token-based cache-coherence protocol is based on similar protocols used in distributed file systems such as Sprite <ref> [18, 17] </ref>, AFS [13] and Echo [4, 14].
Reference: [19] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proceedings of ACM SIGMOD, </booktitle> <pages> pages 1-15, </pages> <year> 1988. </year>
Reference-contexts: We hope to explore more ambitious mapping schemes in the future. Our implementation also does not store any redundant data, again for the sake of simplicity. A subsequent shared logical disk implementation could support data mirroring, RAID-style parity <ref> [19] </ref> or a hierarchical combination of both [24]. 2.2 Cache coherence We use a token-based write-back cache coherence protocol similar to that used in some distributed file systems [13, 14, 17], except that in our system the unit of coherence is a logical disk block instead of a file. <p> In both cases, the core communicates with the external clients using a standard protocol, such as NFS [21], while core machines communicate internally using high-performance proprietary protocols tuned to the core environment. Sawmill [22], Zebra [12] and xFS [1] are network file systems based on log-structuring [20] and RAID <ref> [19] </ref>. Sawmill is a centralized-server system which stores data on a high-bandwidth disk array; specialized hardware allows data to flow directly between the disk array and the network without going through the server's memory system.
Reference: [20] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <booktitle> In Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <pages> pages 1-15, </pages> <year> 1991. </year>
Reference-contexts: In both cases, the core communicates with the external clients using a standard protocol, such as NFS [21], while core machines communicate internally using high-performance proprietary protocols tuned to the core environment. Sawmill [22], Zebra [12] and xFS [1] are network file systems based on log-structuring <ref> [20] </ref> and RAID [19]. Sawmill is a centralized-server system which stores data on a high-bandwidth disk array; specialized hardware allows data to flow directly between the disk array and the network without going through the server's memory system.
Reference: [21] <author> R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon. </author> <title> Design and implementation of the Sun network filesystem. </title> <booktitle> In Proceedings of the 1985 Summer USENIX Technical Conference, </booktitle> <pages> pages 119-130, </pages> <year> 1985. </year>
Reference-contexts: Both DataMesh [23] and xFS [1] seek to use a central, trusted core of machines as a scalable file server for a larger, less-trusted group of file system clients. In both cases, the core communicates with the external clients using a standard protocol, such as NFS <ref> [21] </ref>, while core machines communicate internally using high-performance proprietary protocols tuned to the core environment. Sawmill [22], Zebra [12] and xFS [1] are network file systems based on log-structuring [20] and RAID [19].
Reference: [22] <author> Ken Shirriff and John Ousterhout. Sawmill: </author> <title> A high-bandwidth logging file system. </title> <booktitle> In Proceedings of the 1994 Summer USENIX Technical Conference, </booktitle> <pages> pages 49-60, </pages> <year> 1994. </year>
Reference-contexts: In both cases, the core communicates with the external clients using a standard protocol, such as NFS [21], while core machines communicate internally using high-performance proprietary protocols tuned to the core environment. Sawmill <ref> [22] </ref>, Zebra [12] and xFS [1] are network file systems based on log-structuring [20] and RAID [19].
Reference: [23] <author> John Wilkes. </author> <title> DataMesh | parallel storage systems for the 1990s. </title> <booktitle> In Proceedings of the 11th IEEE Symposium on Mass Storage, </booktitle> <pages> pages 131-136, </pages> <year> 1991. </year>
Reference-contexts: The shared logical disk's performance suffers from false dependencs, but it uses a much simpler mechanism and does not need any knowledge about file system data structures. Both DataMesh <ref> [23] </ref> and xFS [1] seek to use a central, trusted core of machines as a scalable file server for a larger, less-trusted group of file system clients.
Reference: [24] <author> John Wilkes, Richard Golding, Carl Staelin, and Tim Sullivan. </author> <title> The HP AutoRAID hierarchical storage system. </title> <booktitle> In Proceedings of the 15th Symposium on Operating System Principles, </booktitle> <pages> pages 96-108, </pages> <year> 1995. </year> <pages> Page 11 </pages>
Reference-contexts: We hope to explore more ambitious mapping schemes in the future. Our implementation also does not store any redundant data, again for the sake of simplicity. A subsequent shared logical disk implementation could support data mirroring, RAID-style parity [19] or a hierarchical combination of both <ref> [24] </ref>. 2.2 Cache coherence We use a token-based write-back cache coherence protocol similar to that used in some distributed file systems [13, 14, 17], except that in our system the unit of coherence is a logical disk block instead of a file.
References-found: 24

