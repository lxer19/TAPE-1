URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-94-03.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+94-03
Root-URL: 
Email: juffi@ai.univie.ac.at  
Title: Top-Down Pruning in Relational Learning  
Author: Johannes Furnkranz 
Keyword: Content Areas: Machine Learning, Inductive Logic Programming, Pruning  
Note: OEFAI-TR-94-03  
Address: Schottengasse 3 A-1010 Vienna Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: Pruning is an effective method for dealing with noise in Machine Learning. Recently pruning algorithms, in particular Reduced Error Pruning, have also attracted interest in the field of Inductive Logic Programming. However, it has been shown that these methods can be very inefficient, because most of the time is wasted for generating clauses that explain noisy examples and subsequently pruning these clauses. We introduce a new method which searches for good theories in a top-down fashion to get a better starting point for the pruning algorithm. Experiments show that this approach can significantly lower the complexity of the task as well as increase predictive accuracy. 
Abstract-found: 1
Intro-found: 1
Reference: [Breiman et al., 1984] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Pacific Grove, CA, </address> <year> 1984. </year> <month> 10 </month>
Reference-contexts: Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This will be subsequently gener 2 alized by cutting off branches of the decision tree (as in [Quinlan, 1987] or <ref> [Breiman et al., 1984] </ref>). In ILP, Pre-Pruning has been common in the form of stopping criteria as used in Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], and Fossil (see section 2). Post-Pruning was introduced to ILP with an adaptation of Quinlan's Reduced Error Pruning [Brunk and Pazzani, 1991]. <p> Experiments were performed with 10% of the examples having their classification reversed. Testing was done on sets of 5000 noise-free examples. Reduced Error Pruning (REP) 3 This is based on on idea in CART <ref> [Breiman et al., 1984] </ref>, where the most general pruned decision tree within one SE of the best will be returned.
Reference: [Brunk and Pazzani, 1991] <author> Clifford A. Brunk and Michael J. Pazzani. </author> <title> An investigation of noise-tolerant relational concept learning algorithms. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 389-393, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference-contexts: The most prominent use of this method in ILP is the adaptation of Reduced Error Pruning <ref> [Brunk and Pazzani, 1991] </ref>. However, it has been shown in [Cohen, 1993] that REP can be very inefficient, because most of the time is wasted for generating clauses that explain noisy examples and subsequently pruning these clauses. <p> In ILP, Pre-Pruning has been common in the form of stopping criteria as used in Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], and Fossil (see section 2). Post-Pruning was introduced to ILP with an adaptation of Quinlan's Reduced Error Pruning <ref> [Brunk and Pazzani, 1991] </ref>. First the training set is split into two subsets: a growing set and a pruning set . A concept description explaining all of the examples in the growing set is generated with a relational learning algorithm. <p> Prune the theory obtained in step 4. using Reduced Error Pruning as described in <ref> [Brunk and Pazzani, 1991] </ref>. <p> Both algorithms split the sets into the same growing (ca. 2=3) and pruning sets (ca. 1=3). REP generated the most specific theory first (Cutoff = 0), and then used the method described in <ref> [Brunk and Pazzani, 1991] </ref> for pruning. TDP is an implementation of the algorithm decribed in the last section, which made use of all optimizations described there. Our first concern, of course, is whether the new method does not loose predictive accuracy compared to REP (see table 1).
Reference: [Cohen, 1993] <author> William W. Cohen. </author> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 988-994, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: The most prominent use of this method in ILP is the adaptation of Reduced Error Pruning [Brunk and Pazzani, 1991]. However, it has been shown in <ref> [Cohen, 1993] </ref> that REP can be very inefficient, because most of the time is wasted for generating clauses that explain noisy examples and subsequently pruning these clauses. <p> The resulting concept is then generalized by deleting literals and clauses from the theory until all possible deletions would result in a decrease of predictive accuracy, measured on the pruning set. While this method proved to be very effective in avoiding noise-fitting in several domains, <ref> [Cohen, 1993] </ref> has shown that REP is a very costly process. Its time complexity on random data is as bad as (n 2 log n) for generating a concept description from n examples and (n 4 log n) for pruning the resulting set of rules. [Cohen, 1993] has then suggested a <p> avoiding noise-fitting in several domains, <ref> [Cohen, 1993] </ref> has shown that REP is a very costly process. Its time complexity on random data is as bad as (n 2 log n) for generating a concept description from n examples and (n 4 log n) for pruning the resulting set of rules. [Cohen, 1993] has then suggested a more efficient pruning method, which only has a worst-case time complexity of O (n 2 log n). This algorithm was further improved by adding some pre-pruning methods to speed up the concept generation phase. <p> Looking at the run-times, it can be seen that with increasing training set sizes, the costs of REP are dominated by the pruning process. This result is consistent with the findings of <ref> [Cohen, 1993] </ref> (see section 3.1). TDP on the other hand, even manages to decrease run-time with growing training set sizes. <p> This method | Top-Down Pruning | results in a significant speed-up compared to Reduced Error Pruning along with a small gain in accuracy. Similar results have been obtained in <ref> [Cohen, 1993] </ref>, but concentrated mostly on lowering the pruning cost only. In our approach, the entire process of TDP may be significantly faster than REP's growing phase alone. However, TDP has not yet been tested as extensively as the methods proposed in [Cohen, 1993]. <p> Similar results have been obtained in <ref> [Cohen, 1993] </ref>, but concentrated mostly on lowering the pruning cost only. In our approach, the entire process of TDP may be significantly faster than REP's growing phase alone. However, TDP has not yet been tested as extensively as the methods proposed in [Cohen, 1993]. <p> This is currently under investigation. Of course pruning methods in general are subject to the problem that learning general theories in order to avoid overfitting the noise might be inappropriate in some domains [Schaffer, 1993]. In fact it has been observed in <ref> [Cohen, 1993] </ref> that pruning sometimes leads to a decrease in predictive accuracy.
Reference: [Dzeroski and Bratko, 1992] <author> Saso Dzeroski and Ivan Bratko. </author> <title> Handling noise in Inductive Logic Programming. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference-contexts: Pre-pruning | heuristically deciding when to stop growing clauses and concepts | has been present in Inductive Logic Programming (ILP) in the form of stopping criteria for quite some time (see e.g. Foil [Quinlan, 1990], mFoil <ref> [Dzeroski and Bratko, 1992] </ref> and Fossil [Furnkranz, 1994]). The basic idea behind most post-pruning methods is to learn a concept description on one part of the training instances and to subsequently delete several parts of this theory in order to improve performance on the remaining set. <p> This will be subsequently gener 2 alized by cutting off branches of the decision tree (as in [Quinlan, 1987] or [Breiman et al., 1984]). In ILP, Pre-Pruning has been common in the form of stopping criteria as used in Foil [Quinlan, 1990], mFoil <ref> [Dzeroski and Bratko, 1992] </ref>, and Fossil (see section 2). Post-Pruning was introduced to ILP with an adaptation of Quinlan's Reduced Error Pruning [Brunk and Pazzani, 1991]. First the training set is split into two subsets: a growing set and a pruning set .
Reference: [Esposito et al., 1993] <author> Floriana Esposito, Donato Malerba, and Giovanni Semeraro. </author> <title> Decision tree pruning as a search in the state space. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 165-184, </pages> <address> Vienna, Austria, 1993. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: 1 Introduction Pruning is a standard way of dealing with noise in Machine Learning. In particular in decision tree learning pruning methods proved to be most effective (see e.g. [Mingers, 1989] or <ref> [Esposito et al., 1993] </ref>). Pre-pruning | heuristically deciding when to stop growing clauses and concepts | has been present in Inductive Logic Programming (ILP) in the form of stopping criteria for quite some time (see e.g. Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992] and Fossil [Furnkranz, 1994]). <p> In addition, modes, types and symmetries can now be declared for the literals in the background knowledge. 3 Top-Down Pruning 3.1 Pruning in Inductive Logic Programming Pruning is a standard way of dealing with noise in Decision Tree learning (see e.g. [Mingers, 1989] or <ref> [Esposito et al., 1993] </ref>). There are two fundamentally distinct approaches to pruning: Pre-Pruning means that during concept generation some training examples are deliberately ignored, so that the final concept description does not classify all training instances correctly.
Reference: [Flach, 1992] <author> Peter A. Flach. </author> <title> Generality revisited. In Logical Approaches to Machine Learning, </title> <booktitle> Workshop Notes of the 10th European Conference on AI, </booktitle> <address> Vienna, Austria, </address> <year> 1992. </year>
Reference-contexts: We consider the empty theory to be most general, because "Everything is false." is a very general statement. However, our "most specific" theory will cover more ground instances than the empty theory, and thus may be considered (extensionally) more general. See <ref> [Flach, 1992] </ref> for a discussion of related matters. 6 4.
Reference: [Furnkranz, 1993] <author> Johannes Furnkranz. Fossil: </author> <title> A robust relational learner. </title> <type> Technical Report TR-93-28, </type> <institution> Austrian Research Institute for Artificial Intelligence, </institution> <year> 1993. </year> <note> Extended version. </note>
Reference-contexts: The setup for the experimental evaluation of Top-Down Pruning was the same as described in <ref> [Furnkranz, 1993] </ref>. The only difference was that the most recent version of Fossil allows to specify modes, types and symmetries of background predicates, and this facility was used in all experiments. Experiments were performed with 10% of the examples having their classification reversed.
Reference: [Furnkranz, 1994] <author> Johannes Furnkranz. Fossil: </author> <title> A robust relational learner. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, Catania, </booktitle> <address> Italy, 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Pre-pruning | heuristically deciding when to stop growing clauses and concepts | has been present in Inductive Logic Programming (ILP) in the form of stopping criteria for quite some time (see e.g. Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992] and Fossil <ref> [Furnkranz, 1994] </ref>). The basic idea behind most post-pruning methods is to learn a concept description on one part of the training instances and to subsequently delete several parts of this theory in order to improve performance on the remaining set. <p> We solve this problem by adapting the relational learning algorithm Fossil to combine pre-pruning and post-pruning by first performing a general-to-specific search for a good starting theory and then pruning this theory. 2 Fossil and the Cutoff Stopping Criterion Fossil <ref> [Furnkranz, 1994] </ref> is a Foil-like ILP system that uses a search heuristic based on statistical correlation. <p> Experiments have shown that this method is very noise-tolerant and that a good value for the Cutoff is independent of the noise level as well as independent of the size of the training set. A more detailed description of the algorithm can be found in <ref> [Furnkranz, 1994] </ref>.
Reference: [Holte, 1993] <author> Robert C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 63-91, </pages> <year> 1993. </year>
Reference-contexts: However, there is some evidence that simple rules perform well in many real world domains (or at least in those domains that are commonly used as a test bed for machine learning algorithms) <ref> [Holte, 1993] </ref>, and TDP's general-to-specific search might be a good method in those domains. Acknowledgements This research is sponsored by the Austrian Fonds zur Forderung der Wissenschaftlichen Forschung (FWF) under grant number P8756-TEC.
Reference: [Mingers, 1989] <author> John Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 227-243, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Pruning is a standard way of dealing with noise in Machine Learning. In particular in decision tree learning pruning methods proved to be most effective (see e.g. <ref> [Mingers, 1989] </ref> or [Esposito et al., 1993]). Pre-pruning | heuristically deciding when to stop growing clauses and concepts | has been present in Inductive Logic Programming (ILP) in the form of stopping criteria for quite some time (see e.g. <p> In addition, modes, types and symmetries can now be declared for the literals in the background knowledge. 3 Top-Down Pruning 3.1 Pruning in Inductive Logic Programming Pruning is a standard way of dealing with noise in Decision Tree learning (see e.g. <ref> [Mingers, 1989] </ref> or [Esposito et al., 1993]). There are two fundamentally distinct approaches to pruning: Pre-Pruning means that during concept generation some training examples are deliberately ignored, so that the final concept description does not classify all training instances correctly.
Reference: [Quinlan, 1987] <author> John Ross Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: Post-Pruning means that first a concept description is generated that perfectly explains all training instances. This will be subsequently gener 2 alized by cutting off branches of the decision tree (as in <ref> [Quinlan, 1987] </ref> or [Breiman et al., 1984]). In ILP, Pre-Pruning has been common in the form of stopping criteria as used in Foil [Quinlan, 1990], mFoil [Dzeroski and Bratko, 1992], and Fossil (see section 2).
Reference: [Quinlan, 1990] <author> John Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Pre-pruning | heuristically deciding when to stop growing clauses and concepts | has been present in Inductive Logic Programming (ILP) in the form of stopping criteria for quite some time (see e.g. Foil <ref> [Quinlan, 1990] </ref>, mFoil [Dzeroski and Bratko, 1992] and Fossil [Furnkranz, 1994]). The basic idea behind most post-pruning methods is to learn a concept description on one part of the training instances and to subsequently delete several parts of this theory in order to improve performance on the remaining set. <p> This will be subsequently gener 2 alized by cutting off branches of the decision tree (as in [Quinlan, 1987] or [Breiman et al., 1984]). In ILP, Pre-Pruning has been common in the form of stopping criteria as used in Foil <ref> [Quinlan, 1990] </ref>, mFoil [Dzeroski and Bratko, 1992], and Fossil (see section 2). Post-Pruning was introduced to ILP with an adaptation of Quinlan's Reduced Error Pruning [Brunk and Pazzani, 1991]. First the training set is split into two subsets: a growing set and a pruning set .
Reference: [Schaffer, 1993] <author> Cullen Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference-contexts: This is currently under investigation. Of course pruning methods in general are subject to the problem that learning general theories in order to avoid overfitting the noise might be inappropriate in some domains <ref> [Schaffer, 1993] </ref>. In fact it has been observed in [Cohen, 1993] that pruning sometimes leads to a decrease in predictive accuracy.
References-found: 13

