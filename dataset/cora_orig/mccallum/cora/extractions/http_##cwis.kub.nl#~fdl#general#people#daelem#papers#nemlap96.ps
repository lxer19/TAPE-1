URL: http://cwis.kub.nl/~fdl/general/people/daelem/papers/nemlap96.ps
Refering-URL: http://ilk.kub.nl/~ilk/papers/abstracts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Morphological Analysis as Classification: an Inductive-Learning Approach  
Author: Antal van den Bosch (i) Walter Daelemans (ii) Ton Weijters (i) 
Address: PO Box 616, 6200 MD Maastricht, THE NETHERLANDS  PO Box 90153, 5000 LE Tilburg, THE NETHERLANDS  
Affiliation: (i) Dept. of Computer Science matriks Maastricht University  (ii) Computational Linguistics Tilburg University  
Date: 1996, Ankara  
Note: Proceedings of NEMLAP  
Abstract: Morphological analysis is an important subtask in text-to-speech conversion, hyphenation, and other language engineering tasks. The traditional approach to performing morphological analysis is to combine a morpheme lexicon, sets of (linguistic) rules, and heuristics to find a most probable analysis. In contrast we present an inductive learning approach in which morphological analysis is reformulated as a segmentation task. We report on a number of experiments in which five inductive learning algorithms are applied to three variations of the task of morphological analysis. Results show (i) that the generalisation performance of the algorithms is good, and (ii) that the lazy learning algorithm ib1-ig performs best on all three tasks. We conclude that lazy learning of morphological analysis as a classification task is indeed a viable approach; moreover, it has the strong advantages over the traditional approach of avoiding the knowledge-acquisition bottleneck, being fast and deterministic in learning and processing, and being language-independent.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Aha, D. W., Kibler, D., & Albert, M. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <pages> 37-66. </pages>
Reference-contexts: First, we provide a brief summary of the inductive-learning algorithms used in the experiments reported in this paper. 1.2 Algorithms and methods for inductive learning Inductive learning in its most straightforward form is exhibited by memory-based lazy learning algorithms such as ib1 <ref> [1] </ref> and variations (e.g., ib1-ig [6, 9]), in which all instances are fully stored in memory, and in which classification involves a pass along all stored instances. <p> Examples of such algorithms are the decision-tree algorithms igtree [9] and c4.5 [12]. Another popular inductive algorithm is the connectionist Back-propagation (bp) [13] learning algorithm. We provide a summary of the basic functions of these learning algorithms. 1. ib1 <ref> [1] </ref> constructs a data base of instances (the instance base) during learning. An instance consists of a fixed-length vector of n feature-value pairs, and an information field containing the classification (s) of that particular feature-value vector.
Reference: 2. <author> Allen, J., Hunnicutt, S., and Klatt, D. </author> <year> (1987). </year> <title> From Text to Speech: The MITalk System. </title> <address> Cam-bridge, UK: </address> <publisher> Cambidge University Press. </publisher>
Reference-contexts: 1 Introduction Morphological analysis is often deemed to be an important, if not essential subtask in linguistic modular systems for text-to-speech processing <ref> [2] </ref> and hyphenation [4]. In text-to-speech processing, it serves to prevent the wrong application of grapheme-phoneme conversion rules across morpheme boundaries (e.g., preventing carelessly from being pronounced as /k'r~lslai/). <p> The traditional approach to performing morphological analyses presupposes the availability of a morpheme lexicon, spelling rules, morphological rules, and heuristics to prioritise possible analyses of a word according to their plausibility (e.g., see the decomp module in the mittalk system <ref> [2] </ref>). In contrast, the approach described in this paper presupposes a morphologically analysed corpus of words (rather than a corpus of morphemes), and an inductive learning algorithm trained to segment spelling words into morphemes in the form of a simple classification task. <p> We briefly illustrate the functioning of this type of analysis by taking decomp's processing as an example, and the word scarcity as the example word <ref> [2] </ref>: 1. In a morpheme lexicon covering the English language, a first analysis divides scarcity into scar and city. 2. <p> A second characteristic of our representation of morpheme boundaries, is that it is non-hierarchic. Although morpheme hierarchy may be important in determining the part-of-speech of a word <ref> [2] </ref>, it is not necessary to have a full hierarchical analysis when the morphological analysis is used as input to a text-to-speech system. 3 Experiments 3.1 Data collection and algorithmic parameters The source for the morphological data used in our experiments is celex [3], a large lexical data base of English, <p> Future work on inductive learning of morphological analysis should include a thorough performance comparison with existing traditional systems for morphological analysis, based on linguistic theory and heuristics such as decomp <ref> [2] </ref> as well as with probabilistic systems [10].
Reference: 3. <author> Burnage, G. </author> <year> (1990). </year> <title> CELEX: A Guide for Users. Centre for Lexical Information, </title> <address> Nijmegen. </address>
Reference-contexts: in determining the part-of-speech of a word [2], it is not necessary to have a full hierarchical analysis when the morphological analysis is used as input to a text-to-speech system. 3 Experiments 3.1 Data collection and algorithmic parameters The source for the morphological data used in our experiments is celex <ref> [3] </ref>, a large lexical data base of English, Dutch, and German. We extracted from the English data base all relevant information on wordforms relating to spelling and morphology, and created a lexicon of 65,558 morphologically analysed words.
Reference: 4. <author> Daelemans, W. </author> <year> (1989). </year> <title> Automatic hyphenation: Linguistics versus engineering. </title> <editor> In: F.J. Heyvaert and F. Steurs (Eds.), </editor> <booktitle> Worlds behind Words, </booktitle> <pages> 347-364. </pages> <publisher> Leuven University Press. </publisher>
Reference-contexts: 1 Introduction Morphological analysis is often deemed to be an important, if not essential subtask in linguistic modular systems for text-to-speech processing [2] and hyphenation <ref> [4] </ref>. In text-to-speech processing, it serves to prevent the wrong application of grapheme-phoneme conversion rules across morpheme boundaries (e.g., preventing carelessly from being pronounced as /k'r~lslai/).
Reference: 5. <author> Daelemans, W. </author> <year> (1995). </year> <title> Memory-based lexical acquisition and processing. </title> <editor> In: P. Steffens (Ed.), </editor> <booktitle> Machine Translation and the Lexicon, Springer Lecture Notes in Artificial Intelligence 898, </booktitle> <pages> 85-98. </pages>
Reference-contexts: Most linguistic tasks can be described as classification tasks, i.e., given a description of an input in terms of a number of feature-values, a classification of the input is performed. Two types of classification tasks can be discerned <ref> [5] </ref>: Identification: given a set of possible classifications and an input of feature values, determine the correct classification for this input. For example, given a letter surrounded by a number of neighbours (e.g., a in have), determine the phonemic transcription of that letter. <p> For example, determine if the b in table marks the boundary of a syllable. Differences exist in the ways inductive algorithms extract knowledge from the available instances. In lazy learning (such as memory-based learning <ref> [14, 5] </ref>), there is no abstraction of higher-level data structures such as rules or decision trees at learning time; learning consists of simply storing the instances in memory.
Reference: 6. <author> Daelemans, W., Van den Bosch, A. </author> <year> (1992). </year> <title> Generalisation performance of backpropagation learning on a syllabification task. </title> <editor> In M. Drossaers & A. Nijholt (Eds.), TWLT3: </editor> <booktitle> Connectionism and Natural Language Processing. </booktitle> <institution> Enschede: Twente University. </institution>
Reference-contexts: In previous research we have demonstrated the application of the memory-based (lazy) learning approach to several linguistic problems, e.g., segmentation as in hyphenation and syllabification <ref> [6, 17] </ref>, and identification as in grapheme-phoneme conversion [18, 16, 7], and stress assignment [8]. In most cases, the memory-based (lazy) approach outdid the more eager inductive algorithms. <p> First, we provide a brief summary of the inductive-learning algorithms used in the experiments reported in this paper. 1.2 Algorithms and methods for inductive learning Inductive learning in its most straightforward form is exhibited by memory-based lazy learning algorithms such as ib1 [1] and variations (e.g., ib1-ig <ref> [6, 9] </ref>), in which all instances are fully stored in memory, and in which classification involves a pass along all stored instances. <p> ) is used (equation 2). ffi (x i ; y i ) = 0 if x i = y i ; else 1 (2) The (most frequently occurring) classification of the memory instance Y with the smallest (X; Y ) is then taken as the classification of X. 2. ib1-ig <ref> [6, 9] </ref> differs from ib1 in the weighting function W (f i ) (cf. equation 1). This function computes for each feature, over the full instance base, its information gain, a function from information theory that is also used in id3 [11] and c4.5 [12] (for more details, cf. <p> This function computes for each feature, over the full instance base, its information gain, a function from information theory that is also used in id3 [11] and c4.5 [12] (for more details, cf. Daelemans and Van den Bosch <ref> [6] </ref>). In short, the information gain of a feature expresses its relative importance compared to the other features in performing the mapping from input to classification. This weighting function gives right to the fact that for some tasks, some features are far more important than other features.
Reference: 7. <author> Daelemans, W., Van den Bosch, A. </author> <year> (1994). </year> <title> A language-independent, data-oriented architecture for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of ESCA-IEEE Speech Synthesis Conference '94, </booktitle> <address> New York. </address>
Reference-contexts: In previous research we have demonstrated the application of the memory-based (lazy) learning approach to several linguistic problems, e.g., segmentation as in hyphenation and syllabification [6, 17], and identification as in grapheme-phoneme conversion <ref> [18, 16, 7] </ref>, and stress assignment [8]. In most cases, the memory-based (lazy) approach outdid the more eager inductive algorithms. <p> In applications to linguistic tasks, igtree is shown to obtain compression factors of 90% or more as compared to ib1/ib1-ig <ref> [16, 7] </ref>. igtree also stores with each non-terminal node information concerning the most probable or default classification given the path thus far, according to the classification bookkeeping information maintained by the trie construction algorithm. This extra information is essential when processing new instances.
Reference: 8. <author> Daelemans, W., Gillis, S., & Durieux, G. </author> <year> (1994). </year> <title> The acquisition of stress, a data-oriented approach. </title> <journal> Computational Linguistics, </journal> <volume> 20, </volume> <pages> 421-451. </pages>
Reference-contexts: In previous research we have demonstrated the application of the memory-based (lazy) learning approach to several linguistic problems, e.g., segmentation as in hyphenation and syllabification [6, 17], and identification as in grapheme-phoneme conversion [18, 16, 7], and stress assignment <ref> [8] </ref>. In most cases, the memory-based (lazy) approach outdid the more eager inductive algorithms.
Reference: 9. <author> Daelemans, W., Van den Bosch, A., & Weijters, A. </author> <year> (1996). </year> <title> IGTree: Using trees for compression and classification in lazy learning algorithms. </title> <note> To appear in Artificial Intelligence Review, special issue on lazy learning. </note>
Reference-contexts: First, we provide a brief summary of the inductive-learning algorithms used in the experiments reported in this paper. 1.2 Algorithms and methods for inductive learning Inductive learning in its most straightforward form is exhibited by memory-based lazy learning algorithms such as ib1 [1] and variations (e.g., ib1-ig <ref> [6, 9] </ref>), in which all instances are fully stored in memory, and in which classification involves a pass along all stored instances. <p> To optimise memory lookup and minimise memory usage, more eager learning algorithms are available that compress the memory in such a way that most relevant knowledge is retained and stored in a quickly accessible form, and redundant knowledge is removed. Examples of such algorithms are the decision-tree algorithms igtree <ref> [9] </ref> and c4.5 [12]. Another popular inductive algorithm is the connectionist Back-propagation (bp) [13] learning algorithm. We provide a summary of the basic functions of these learning algorithms. 1. ib1 [1] constructs a data base of instances (the instance base) during learning. <p> ) is used (equation 2). ffi (x i ; y i ) = 0 if x i = y i ; else 1 (2) The (most frequently occurring) classification of the memory instance Y with the smallest (X; Y ) is then taken as the classification of X. 2. ib1-ig <ref> [6, 9] </ref> differs from ib1 in the weighting function W (f i ) (cf. equation 1). This function computes for each feature, over the full instance base, its information gain, a function from information theory that is also used in id3 [11] and c4.5 [12] (for more details, cf. <p> When information gain is used as the weighting function in the similarity function (equation 1), instances that match on an important feature are regarded as more alike than instances that match on an unimportant feature. 3. igtree <ref> [9] </ref> compresses an instance base into a decision tree. Instances are stored in the tree as paths of connected nodes, and leaves containing classification information. Nodes are connected via arcs denoting feature values. <p> For more details on igtree, see Daelemans et al. <ref> [9] </ref>. 4. c4.5 [12] is a well-known decision-tree algorithm which basically uses the same type of strategy as igtree to compress an instance base into a compact tree. <p> Interesting is the fact that igtree performs well on m1, but performs relatively bad on m2 and m3. igtree is known to perform worse when the information gain of the input features displays a low variance <ref> [9] </ref>, i.e., when there is little difference between the `relative importance' of the input features. This suggests that the information-gain values of the features with tasks m2 and m3 have less outspoken differences than with m1, which is indeed the case, as is displayed in Figure 3.
Reference: 10. <author> Heemskerk, J. </author> <year> (1993). </year> <title> A probabilistic context-free grammar for disambiguation in morphological parsing. </title> <type> Technical Report 44, </type> <institution> ITK, Tilburg University. </institution>
Reference-contexts: Morphological analysis on a probabilistic basis, using only a morpheme lexicon, an analyses generator, and a probabilistic function to determine the analysis with the highest probability <ref> [10] </ref> does not suffer from the disadvantageous knowledge acquisition and fine-tuning phase, but is nevertheless also confronted with an explosion of the number of generated analyses. 2.2 Inductive-learning approach In contrast to this decomposition into three components, we reformulate the task of morphological analysis as a one-pass segmentation task, in which <p> Future work on inductive learning of morphological analysis should include a thorough performance comparison with existing traditional systems for morphological analysis, based on linguistic theory and heuristics such as decomp [2] as well as with probabilistic systems <ref> [10] </ref>. Secondly, we aim at integrating trained models of morphological analysis into larger systems, to investigate whether the enrichment of spelling input with morpheme boundary information improves the generalisation performance of other learning systems trained on, e.g., stress assignment, grapheme-phoneme conversion, and part-of-speech prediction of unknown words.
Reference: 11. <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-206. </pages>
Reference-contexts: This function computes for each feature, over the full instance base, its information gain, a function from information theory that is also used in id3 <ref> [11] </ref> and c4.5 [12] (for more details, cf. Daelemans and Van den Bosch [6]). In short, the information gain of a feature expresses its relative importance compared to the other features in performing the mapping from input to classification.
Reference: 12. <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Examples of such algorithms are the decision-tree algorithms igtree [9] and c4.5 <ref> [12] </ref>. Another popular inductive algorithm is the connectionist Back-propagation (bp) [13] learning algorithm. We provide a summary of the basic functions of these learning algorithms. 1. ib1 [1] constructs a data base of instances (the instance base) during learning. <p> This function computes for each feature, over the full instance base, its information gain, a function from information theory that is also used in id3 [11] and c4.5 <ref> [12] </ref> (for more details, cf. Daelemans and Van den Bosch [6]). In short, the information gain of a feature expresses its relative importance compared to the other features in performing the mapping from input to classification. <p> For more details on igtree, see Daelemans et al. [9]. 4. c4.5 <ref> [12] </ref> is a well-known decision-tree algorithm which basically uses the same type of strategy as igtree to compress an instance base into a compact tree. To this purpose, standard c4.5 also uses information gain, or gain ratio [12] to select the most important feature in tree building; however, in contrast to <p> For more details on igtree, see Daelemans et al. [9]. 4. c4.5 <ref> [12] </ref> is a well-known decision-tree algorithm which basically uses the same type of strategy as igtree to compress an instance base into a compact tree. To this purpose, standard c4.5 also uses information gain, or gain ratio [12] to select the most important feature in tree building; however, in contrast to igtree, c4.5 recomputes this function for each node in the tree.
Reference: 13. <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1: Foundations, </booktitle> <pages> pages 318-362. </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Examples of such algorithms are the decision-tree algorithms igtree [9] and c4.5 [12]. Another popular inductive algorithm is the connectionist Back-propagation (bp) <ref> [13] </ref> learning algorithm. We provide a summary of the basic functions of these learning algorithms. 1. ib1 [1] constructs a data base of instances (the instance base) during learning. <p> Another difference with igtree is that c4.5 implements a pruning stage, in which parts of the tree are removed as they are estimated to contribute to instance classification below a certain utility threshold. 5. bp <ref> [13] </ref> is an artificial-neural-network learning rule, which operates on multi-layer feed-forward networks (mfns). In these networks, feature-values of instances are encoded as activation patterns in the input layer, and the network is trained to produce an activation pattern at the output layer representing the desired classification.
Reference: 14. <author> Stanfill, C., and Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29, </volume> <pages> 1213-1228. </pages>
Reference-contexts: For example, determine if the b in table marks the boundary of a syllable. Differences exist in the ways inductive algorithms extract knowledge from the available instances. In lazy learning (such as memory-based learning <ref> [14, 5] </ref>), there is no abstraction of higher-level data structures such as rules or decision trees at learning time; learning consists of simply storing the instances in memory.
Reference: 15. <author> Sejnowski, T. J., Rosenberg, C. S. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference: 16. <author> Van den Bosch, A., Daelemans, W. </author> <year> (1993). </year> <title> Data-oriented methods for grapheme-to-phoneme conversion. </title> <booktitle> In Proceedings of the 6th Conference of the EACL, </booktitle> <pages> 45-53. </pages> <address> Utrecht: OTS. </address>
Reference-contexts: In previous research we have demonstrated the application of the memory-based (lazy) learning approach to several linguistic problems, e.g., segmentation as in hyphenation and syllabification [6, 17], and identification as in grapheme-phoneme conversion <ref> [18, 16, 7] </ref>, and stress assignment [8]. In most cases, the memory-based (lazy) approach outdid the more eager inductive algorithms. <p> In applications to linguistic tasks, igtree is shown to obtain compression factors of 90% or more as compared to ib1/ib1-ig <ref> [16, 7] </ref>. igtree also stores with each non-terminal node information concerning the most probable or default classification given the path thus far, according to the classification bookkeeping information maintained by the trie construction algorithm. This extra information is essential when processing new instances.
Reference: 17. <author> Van den Bosch, A., Weijters, A., van den Herik, H. J., and Daelemans, W. </author> <year> (1995b). </year> <title> The profit of learning exceptions. </title> <booktitle> In Proceedings of the 5th Belgian-Dutch Conference on Machine Learning, BENELEARN'95, </booktitle> <pages> 118-126. </pages>
Reference-contexts: In previous research we have demonstrated the application of the memory-based (lazy) learning approach to several linguistic problems, e.g., segmentation as in hyphenation and syllabification <ref> [6, 17] </ref>, and identification as in grapheme-phoneme conversion [18, 16, 7], and stress assignment [8]. In most cases, the memory-based (lazy) approach outdid the more eager inductive algorithms.
Reference: 18. <author> Weijters, A. </author> <year> (1991). </year> <title> A simple look-up procedure superior to NETtalk? In Proceedings of the International Conference on Artificial Neural Networks, </title> <address> Espoo, Finland. </address>
Reference-contexts: In previous research we have demonstrated the application of the memory-based (lazy) learning approach to several linguistic problems, e.g., segmentation as in hyphenation and syllabification [6, 17], and identification as in grapheme-phoneme conversion <ref> [18, 16, 7] </ref>, and stress assignment [8]. In most cases, the memory-based (lazy) approach outdid the more eager inductive algorithms.
Reference: 19. <author> Weiss, S. & Kulikowski, C. </author> <year> (1991). </year> <title> Computer Systems That Learn. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A method that gives a good estimate of the generalisation performance of an algorithm on a given instance base, is 10-fold cross-validation <ref> [19] </ref>. This method generates on the basis of an instance base 10 partitionings into a training set (90%) and a test set (10%), resulting in 10 experiments and 10 results per learning algorithm and instance base. <p> output units (classes are locally coded), a learning rate of 0.1, a momentum of 0.4, and an update tolerance of 0.2. igtree's functioning is not governed by parameters. 3.2 Results We applied the five algorithms to the three tasks, performing with each algorithm and each task a 10-fold cross-validation experiment <ref> [19] </ref>. We computed for each 10-fold cross-validation experiment the average percentage of incorrectly processed test words. A word is incorrectly processed when one or more instance classifications associated with the instances derived from the word are incorrect (i.e., when one or more of the segmentations is incorrect).
References-found: 19

