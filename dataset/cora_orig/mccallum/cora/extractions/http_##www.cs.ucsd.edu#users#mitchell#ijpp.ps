URL: http://www.cs.ucsd.edu/users/mitchell/ijpp.ps
Refering-URL: http://www.cs.ucsd.edu/users/mitchell/papers.html
Root-URL: http://www.cs.ucsd.edu
Title: Quantifying the Multi-Level Nature of Tiling Interactions  
Author: Nicholas Mitchell, Karin Hogstedt, Larry Carter, Jeanne Ferrante 
Keyword: tiling, optimization, compiler, guidance, memory hierarchy, ILP, TLB, parallelism, locality  
Date: 1998 1  
Note: INTERNATIONAL JOURNAL OF PARALLEL PROGRAMMING,  
Abstract: Optimizations, including tiling, often target a single level of memory or parallelism, such as cache. These optimizations usually operate on a level-by-level basis, guided by a cost function parameterized by features of that single level. The benefit of optimizations guided by these one-level cost functions decreases as architectures tend towards a hierarchy of memory and of parallelism. We have identified three common architectural scenarios where a single tiling choice could be improved by using information from multiple levels in concert. For each scenario, we derive multi-level cost functions which guide the optimal choice of tile size and shape, and quantify the improvement gained. We give both analysis and simulation results to support our points. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michael E. Wolf and Monica S. Lam, </author> <title> "A data locality optimizing algorithm," </title> <booktitle> in Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: In contrast to these and other approaches to tiling size selection, our multi-level approach uses the block size at each level in a multi-level cost function. Single-level unification: Unimodular transformations can guide loop transformations for locality <ref> [1] </ref> and parallelism [6]. These works unify only improvement-enabling transformations such as skewing, interchange, and reversal, and do not consider locality and parallelism in concert. The work in [25] incorporates a larger set of transformations and unifies the transformation legality checks.
Reference: [2] <author> Steve Carr and Ken Kennedy, </author> <title> "Compiler blockability of numerical algorithms," </title> <journal> The Journal of Supercomputing, </journal> <pages> pp. 114-124, </pages> <month> Nov. </month> <year> 1992. </year>
Reference: [3] <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tseng, </author> <title> "Compiler optimizations for improving data locality," </title> <booktitle> in Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> San Jose, CA, </address> <month> Oct. </month> <year> 1994. </year>
Reference: [4] <author> Steve Carr and Ken Kennedy, </author> <title> "Improving the ratio of memory operations to floatingpoint operations in loops," </title> <journal> Transactions on Programming Languages and Systems, </journal> <volume> vol. 16, no. 6, </volume> <pages> pp. 1768-1810, </pages> <month> Nov. </month> <year> 1994. </year>
Reference: [5] <author> Corinne Ancourt and Fran~cois Irigoin, </author> <title> "Scanning polyhedra with DO loops," </title> <booktitle> in Principles and Practice of Parallel Programming, </booktitle> <month> Apr. </month> <year> 1991, </year> <pages> pp. 39-50. </pages>
Reference: [6] <author> Michael E. Wolf and Monica S. Lam, </author> <title> "A loop transformation theory and an algorithm to maximize parallelism," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 452-471, </pages> <year> 1991. </year>
Reference-contexts: In contrast to these and other approaches to tiling size selection, our multi-level approach uses the block size at each level in a multi-level cost function. Single-level unification: Unimodular transformations can guide loop transformations for locality [1] and parallelism <ref> [6] </ref>. These works unify only improvement-enabling transformations such as skewing, interchange, and reversal, and do not consider locality and parallelism in concert. The work in [25] incorporates a larger set of transformations and unifies the transformation legality checks. <p> The iteration space is divided into sixteen vertical swaths, one corresponding to each node of the computer. Each swath is partitioned into potentially non-rectangular node tiles of height H n . Parallelogram-shaped tiles correspond to rectangular tilings after a unimodular transformation <ref> [6] </ref> of the iteration space. After the iterations in a given node tile are executed, a message containing the values on the right-hand boundary is sent to the next node. This allows the receiving node to begin executing the tile that needs these values.
Reference: [7] <author> Paul Feautrier, </author> <title> "Some efficient solutions to the affine scheduling problem, Part I, one-dimensional time," </title> <journal> International Journal of Parallel Programming, </journal> <volume> vol. 21, no. 5, </volume> <month> Oct. </month> <year> 1992. </year>
Reference: [8] <author> Wayne Kelly and William Pugh, </author> <title> "A unifying framework for iteration reordering transformations," </title> <booktitle> in IEEE First International Conference on Algorithms and Architectures for Parallel Processing, </booktitle> <month> Apr. </month> <year> 1995. </year>
Reference: [9] <author> Daniel Lavery and Wen-mei Hwu, </author> <title> "Unrolling-based optimizations for modulo scheduling," </title> <booktitle> in 28th International Symposium on Microarchitecture, </booktitle> <month> Dec. </month> <year> 1995, </year> <pages> pp. 126-141. </pages>
Reference: [10] <author> Stephanie Coleman and Kathryn S. McKinley, </author> <title> "Tile size selection using cache organization and data layout," </title> <booktitle> in Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: B. Related Work Related work falls into four categories: quantification of performance, determining tile characteristics for a single level, unifying optimizations for a single level and unifying optimizations for multiple levels. Quantifying performance: We employ counting arguments similar to [20], [23], <ref> [10] </ref>, [24], [11] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as [20], [10], [24] give methods for choosing tile size in a nested loop; [20] uses a "fits-in" constraint based <p> Quantifying performance: We employ counting arguments similar to [20], [23], <ref> [10] </ref>, [24], [11] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as [20], [10], [24] give methods for choosing tile size in a nested loop; [20] uses a "fits-in" constraint based only on memory capacity (not block size), [10]'s "fits-in" constraint does not fully utilize block size information, and [24] limits block size to one.
Reference: [11] <author> Vivek Sarkar, Guang R. Gao, and Shaohua Han, </author> <title> "Locality analysis for distributed shared-memory multiprocessors," </title> <booktitle> in Languages and Compilers for Parallel Computing, </booktitle> <year> 1996. </year>
Reference-contexts: B. Related Work Related work falls into four categories: quantification of performance, determining tile characteristics for a single level, unifying optimizations for a single level and unifying optimizations for multiple levels. Quantifying performance: We employ counting arguments similar to [20], [23], [10], [24], <ref> [11] </ref> to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. <p> The simple miss count only counts lines from A towards capacity (and ignores many other issues, as well). Therefore, similar to <ref> [11] </ref> and the effective cache size of [13], we modify the "fits in" constraint with a "fudge factor" of 75%: B k (H; W ) 0:75C k . Again, we need only use this fudge factor when using M simple k , as this formula ignores many architecture-code interactions.

Reference: [13] <author> Michael E. Wolf, Dror Maydan, and Ding-Kai Chen, </author> <title> "Combining loop transformations considering caches and scheduling," </title> <booktitle> in 29th International Symposium on Microarchitec-ture, </booktitle> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: Bold elements highlight the important features in each architecture. number of memory levels, one level at a time. It finds the optimal execution time by searching the space of tiling choices. Rather than use multi-level cost functions, <ref> [13] </ref> performs a pruned search on the space of possible combinations of minimizations of one-level cost functions. It is not clear whether this method of extending to multiple levels is equivalent to a multi-level cost function. <p> This table can be generated either analytically, experimentally (such as in <ref> [13] </ref>), or by some combination of the two. For this paper, the processor utilization column, ILP T ime, comes from actual executions of the loop nest with M sufficiently small so that there are no cache misses. The cache column, CacheM iss, comes from a static cache model, described next. <p> In this simple case, using the cache-specific cost function or the ILP-specific cost function did not yield the best solution, which is found by minimizing the sum of the two cost level-specific functions. Others <ref> [13] </ref> have proposed a different solution to the ILP-cache trade-off. They recognize the interdependence of optimization choices and use a search procedure to take into account many of these interactions. However, they prune the search procedure by making decisions for ILP before tiling decisions for cache locality. <p> The simple miss count only counts lines from A towards capacity (and ignores many other issues, as well). Therefore, similar to [11] and the effective cache size of <ref> [13] </ref>, we modify the "fits in" constraint with a "fudge factor" of 75%: B k (H; W ) 0:75C k . Again, we need only use this fudge factor when using M simple k , as this formula ignores many architecture-code interactions.
Reference: [14] <author> Michael J. Wolfe, </author> <title> "Iteration space tiling for memory hierarchies," </title> <booktitle> in Parallel Processing for Scientific Computing, </booktitle> <year> 1987, </year> <pages> pp. 357-361. </pages>
Reference: [15] <author> J. Ramanujam and P. Sadayappan, </author> <title> "Tiling multidimensional iteration spaces for non-shared memory machines," </title> <booktitle> in Supercomputing, </booktitle> <month> Nov. </month> <year> 1991. </year>
Reference: [16] <author> David A. Padua and Michael J. Wolfe, </author> <title> "Advanced compiler optimizations for supercomputers," </title> <journal> Communications of the ACM, </journal> <volume> vol. 29, no. 12, </volume> <pages> pp. 1184-1201, </pages> <month> Dec. </month> <year> 1986. </year>
Reference: [17] <author> Dennis Gannon, William Jalby, and Kyle Gallivan, </author> <title> "Strategies for cache and local memory management by global program transformation," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 5, no. 5, </volume> <month> Oct. </month> <year> 1988. </year>
Reference: [18] <author> Fran~cois Irigoin and Remi Triolet, </author> <title> "Supernode partitioning," </title> <booktitle> in Principles of Programming Languages, </booktitle> <month> Jan. </month> <year> 1988, </year> <pages> pp. 319-328. </pages>
Reference: [19] <author> Michael J. Wolfe, </author> <title> "More iteration space tiling," </title> <booktitle> in Supercomputing, </booktitle> <year> 1989, </year> <pages> pp. 655-664. </pages>
Reference: [20] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf, </author> <title> "The cache performance and optimizations of blocked algorithms," </title> <booktitle> in ASPLOS-IV, </booktitle> <address> Palo Alto, CA, </address> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: B. Related Work Related work falls into four categories: quantification of performance, determining tile characteristics for a single level, unifying optimizations for a single level and unifying optimizations for multiple levels. Quantifying performance: We employ counting arguments similar to <ref> [20] </ref>, [23], [10], [24], [11] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as [20], [10], [24] give methods for choosing tile size in a nested loop; [20] uses a "fits-in" <p> Quantifying performance: We employ counting arguments similar to <ref> [20] </ref>, [23], [10], [24], [11] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as [20], [10], [24] give methods for choosing tile size in a nested loop; [20] uses a "fits-in" constraint based only on memory capacity (not block size), [10]'s "fits-in" constraint does not fully utilize block size information, and [24] limits block size to one. <p> arguments similar to <ref> [20] </ref>, [23], [10], [24], [11] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as [20], [10], [24] give methods for choosing tile size in a nested loop; [20] uses a "fits-in" constraint based only on memory capacity (not block size), [10]'s "fits-in" constraint does not fully utilize block size information, and [24] limits block size to one.
Reference: [21] <author> Utpal Banerjee, </author> <title> "Unimodular transformations of double loops," </title> <booktitle> in Programming Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> Aug. </month> <year> 1990. </year>

Reference: [23] <author> Jeanne Ferrante, Vivek Sarkar, and Wendy Thrash, </author> <title> "On estimating and enhancing cache effectiveness," </title> <booktitle> in Languages and Compilers for Parallel Computing, </booktitle> <year> 1991. </year>
Reference-contexts: B. Related Work Related work falls into four categories: quantification of performance, determining tile characteristics for a single level, unifying optimizations for a single level and unifying optimizations for multiple levels. Quantifying performance: We employ counting arguments similar to [20], <ref> [23] </ref>, [10], [24], [11] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy.
Reference: [24] <author> Anant Agarwal, David Kranz, and Venkat Natarajan, </author> <title> "Automatic partitioning of parallel loops and data arrays for distributed shared memory multiprocessors," </title> <booktitle> in International Conference on Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: B. Related Work Related work falls into four categories: quantification of performance, determining tile characteristics for a single level, unifying optimizations for a single level and unifying optimizations for multiple levels. Quantifying performance: We employ counting arguments similar to [20], [23], [10], <ref> [24] </ref>, [11] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as [20], [10], [24] give methods for choosing tile size in a nested loop; [20] uses a "fits-in" constraint based only <p> Quantifying performance: We employ counting arguments similar to [20], [23], [10], <ref> [24] </ref>, [11] to estimate the number of misses in a module. No previous work has applied these arguments to multiple levels of the memory hierarchy. Single-level tile characteristics: Works such as [20], [10], [24] give methods for choosing tile size in a nested loop; [20] uses a "fits-in" constraint based only on memory capacity (not block size), [10]'s "fits-in" constraint does not fully utilize block size information, and [24] limits block size to one. <p> Single-level tile characteristics: Works such as [20], [10], <ref> [24] </ref> give methods for choosing tile size in a nested loop; [20] uses a "fits-in" constraint based only on memory capacity (not block size), [10]'s "fits-in" constraint does not fully utilize block size information, and [24] limits block size to one. In contrast to these and other approaches to tiling size selection, our multi-level approach uses the block size at each level in a multi-level cost function. Single-level unification: Unimodular transformations can guide loop transformations for locality [1] and parallelism [6].
Reference: [25] <author> Vivek Sarkar and Radhika Thekkath, </author> <title> "A general framework for iteration-reordering loop transformations (Technical Summary)," </title> <booktitle> in Programming Language Design and Implementation, </booktitle> <year> 1992. </year>
Reference-contexts: Single-level unification: Unimodular transformations can guide loop transformations for locality [1] and parallelism [6]. These works unify only improvement-enabling transformations such as skewing, interchange, and reversal, and do not consider locality and parallelism in concert. The work in <ref> [25] </ref> incorporates a larger set of transformations and unifies the transformation legality checks. AI search techniques on a decision tree of possible optimizations may find a good sequence of transformations to parallelize a given program [12]. <p> Our aim is to provide a system whereby a compiler (or human) may consider (quantitatively) the combined effect of memory hierarchy optimizations. In this sense, our work is similar to the "first-class" transformation representation of unimodular matrices and the framework in <ref> [25] </ref>. III. Multi-level cost functions yield better performance or parallelism. The first, (a), represents any architecture where multiple children share a single memory, such as a cache shared by multiple processors.
Reference: [26] <author> Steve Carr, </author> <title> "Combining optimization for cache and instruction-level parallelism," </title> <booktitle> in PACT '96, </booktitle> <year> 1996, </year> <pages> pp. 238-247. </pages>
Reference-contexts: None of these works seeks to unify guidance for multiple levels of the memory hierarchy. Multi-level unification: Unroll-and-jam can guide locality and instruction level parallelism in concert <ref> [26] </ref>. Loop fusion and distribution affect both parallelism and locality [27]. These two works do not directly address tiling or the multilevel nature of the interactions.
Reference: [27] <author> Ken Kennedy and Kathryn S. McKinley, </author> <title> "Maximizing loop parallelism and improving data locality via loop fusion and distribution," </title> <booktitle> in Languages and Compilers for Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: None of these works seeks to unify guidance for multiple levels of the memory hierarchy. Multi-level unification: Unroll-and-jam can guide locality and instruction level parallelism in concert [26]. Loop fusion and distribution affect both parallelism and locality <ref> [27] </ref>. These two works do not directly address tiling or the multilevel nature of the interactions. For matrix multiply, [28] tiles an arbitrary INTERNATIONAL JOURNAL OF PARALLEL PROGRAMMING, 1998 4 architectural scenario instance superscalar TLB clustered SMP section III-A III-B III-C (a) (b) (c) Fig. 1.
Reference: [28] <author> Jeff Bilmes, Krste Asanovic, Chee-Whye Chin, and Jim Demmel, </author> <title> "Optimizing matrix multiply using PHiPAC: a portable, high-performance, ANSI C coding methodology," </title> <booktitle> in International Conference on Supercomputing, </booktitle> <year> 1997. </year>
Reference-contexts: Multi-level unification: Unroll-and-jam can guide locality and instruction level parallelism in concert [26]. Loop fusion and distribution affect both parallelism and locality [27]. These two works do not directly address tiling or the multilevel nature of the interactions. For matrix multiply, <ref> [28] </ref> tiles an arbitrary INTERNATIONAL JOURNAL OF PARALLEL PROGRAMMING, 1998 4 architectural scenario instance superscalar TLB clustered SMP section III-A III-B III-C (a) (b) (c) Fig. 1. Three architectural scenarios with deleterious tiling interactions. Higher modules represent larger, slower memory units; lower modules are smaller and faster caches or processors. <p> TLB L1 Cache entries kB line entries bytes line Power2 512 4 1024 256 PA-8000 96 4+ 16384+ 16+ UltraSPARC 64 8+ 512 32 Pentium II 64 4 512 32 21164 64 8 256 32 Fig. 4. Tiled code for matrix multiply. put into it's optimization <ref> [28] </ref>, but none has explored optimizing both cache and TLB simultaneously. Second, the relatively simple data access patterns of matrix multiply allows straightforward locality analysis. However, this latter aspect of matrix multiply implies that matrix multiply is a compute-bound application.
Reference: [29] <author> Larry Carter, Jeanne Ferrante, Susan Flynn Hummel, Bowen Alpern, and Kang Su Gatlin, </author> <title> "Hierarchical tiling: A methodology for high performance," </title> <type> Tech. Rep. </type> <institution> CS96-508, UCSD, Department of Computer Science and Engineering, </institution> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: IBM POWER2 with C c = 256, S c = 64, a = 4. (in cycles per point) planes per tile cache miss penalty (CacheM iss) execution time (I LP T ime) 2 1 5.3 The ILP T ime cost function is for the IBM Power2 with values taken from <ref> [29] </ref>. Using our example cost function table, this section considers choosing the number of planes in three information domains: only considering cache characteristics, only ILP characteristics, and both. This final domain must balance the tradeoff between ILP and cache locality.
Reference: [30] <author> Doug Burger and Todd Austin, </author> <title> "The SimpleScalar architectural research tool set, </title> <note> version 2.0," http://www.cs.wisc.edu/~mscalar/simplescalar.html. </note>
Reference-contexts: Which is best? To verify the analytical formulas for misses, we counted cache and TLB misses on the SimpleScalar 2.0 <ref> [30] </ref> cache simulator, sim-cache. The simulation is parametrized by characteristics of the target architecture: C k , S k and associativity for each module k. As our formulas do not yet account for associativity, we ran the simulations on fully associative caches.
Reference: [31] <author> Karin Hogstedt, Larry Carter, and Jeanne Ferrante, </author> <title> "Determining the idle time of a INTERNATIONAL JOURNAL OF PARALLEL PROGRAMMING, </title> <booktitle> 1998 27 tiling," in Principles of Programming Languages, </booktitle> <year> 1997. </year>
Reference-contexts: It has been shown in <ref> [31] </ref> that this can be done by including another term, the idle time I 2 , which is the time that last processor spends waiting for data from other processors. 11 The formula for idle time is: I 2 = (N 2 1)(1 + r 2 )E 0 where there are <p> Specifically, r 2 = W 0 H 0 (s 2 s 1 ), where s 2 is the slope of the iteration space, s 1 is the slope of the tile, and H 0 and W 0 are the tile dimensions <ref> [31] </ref>. 12 When we incorporate the idle time into our formula for the parallel execution time of a set of stacks of tiles, we have: 11 In [31], idle time is actually defined for any processor as the time it spends idle waiting for data from other processors plus the time <p> slope of the iteration space, s 1 is the slope of the tile, and H 0 and W 0 are the tile dimensions <ref> [31] </ref>. 12 When we incorporate the idle time into our formula for the parallel execution time of a set of stacks of tiles, we have: 11 In [31], idle time is actually defined for any processor as the time it spends idle waiting for data from other processors plus the time it is idle after it has completed its work, before the last-completing processor has finished. <p> INTERNATIONAL JOURNAL OF PARALLEL PROGRAMMING, 1998 19 Fig. 10. Two tilings of the iteration space which differ in their choices of rise. In (a) r n = r i = 0, and (b) r n &lt; 0 and r i &gt; 0. <ref> [31] </ref> defines rise as a relationship between the shape of the iteration space and the shape of the tiles. Notice that decreasing the rise at a particular level of parallelism reduces the idle time due to a shorter critical path of dependent tiles at that level.
Reference: [32] <author> Larry Carter, Jeanne Ferrante, and S. Flynn Hummel, </author> <title> "Hierarchical tiling for improved superscalar perfomance," </title> <booktitle> in International Parallel Processing Symposium, </booktitle> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: that the portion of the A matrix corresponding to a cache tile may be only 85% of the cache capacity, and assume the data values are 4 bytes each. 14 If we were concerned with instruction-level parallelism in this example, then there might be an advantage to non-rectangular cache tiles <ref> [32] </ref>. INTERNATIONAL JOURNAL OF PARALLEL PROGRAMMING, 1998 21 Fig. 11. The model for the architecture of the flattened approach. optimal value of H n = 0. The only tiling parameters that affect E n H n W n are H c and r n .
References-found: 30

