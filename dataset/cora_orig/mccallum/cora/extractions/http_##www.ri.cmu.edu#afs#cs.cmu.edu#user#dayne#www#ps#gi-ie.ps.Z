URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/dayne/www/ps/gi-ie.ps.Z
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/dayne/www/cv.html
Root-URL: 
Email: dayne@cs.cmu.edu  
Title: Using Grammatical Inference to Improve Precision in Information Extraction  
Author: Dayne Freitag 
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: The field of information extraction (IE) is concerned with applying natural language processing (NLP) and information retrieval (IR) techniques to the automatic extraction of essential details from text documents. We are exploring the use of machine learning methods for IE. While the most promising methods we have developed perform well for problems defined over a collection of electronic seminar announcements, they are imprecise in their identification of the boundaries of relevant text fragments (fields). Here, we entertain the idea of using grammatical inference (GI) methods to learn the appropriate form of a field. We describe one method for translating raw text into an abstract alphabet suitable for GI, and show that, by combining one IE learning method with the resulting inferred grammars, large improvements in precision can be realized for some fields. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ahonen, H., and Mannila, H. </author> <year> 1994. </year> <title> Forming grammars for structured documents: an application of grammatical inference. </title> <editor> In Carrasco, R. C., and Oncina, J., eds., </editor> <title> Grammatical Inference and Applications: </title> <booktitle> Second International Colloquium, ICGI-94, </booktitle> <pages> 153-167. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: We have only scratched the surface, however. Grammatical inference has been used to understand document layout, for example, something of potentially great benefit for information extraction <ref> (Ahonen & Mannila 1994) </ref>. We might also ask what would happen if textual sequences were expressed in terms that reflected linguistic information.
Reference: <author> Appelt, D. E.; Hobbs, J. R.; Bear, J.; Israel, D.; and Tyson, M. </author> <year> 1993. </year> <title> FASTUS: a finite-state processor for information extraction from real-world text. </title> <booktitle> Proceedings of IJCAI-93. </booktitle>
Reference-contexts: Several leading systems in the IE community, for instance, demonstrate that thorough syntactic parsing is mostly unnecessary, that useful extraction is frequently possible given cheap local syntactic analy sis. (See, for example, <ref> (Appelt et al. 1993) </ref> and (Lehn--ert et al. 1992).) Of course, "simple" does not mean "easy" in this case; it only means that full text understanding, along with some of its more bedeviling questions, can be avoided in favor of more superficial methods in some domains.
Reference: <author> August, S. E., and Dolan, C. P. </author> <year> 1992. </year> <title> Hughes Research Laboratories: description of the trainable text skimmer used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4). </booktitle>
Reference-contexts: Recently, however, several researchers have demonstrated the suitability of machine learning algorithms for purposes more closely tied to the problem of IE. The Hughes Trainable Text Skimmer, for example, used both k-nearest neighbor and Bayesian methods to estimate the likelihood that a text fragment instantiates a target field <ref> (August & Dolan 1992) </ref>. The CIRCUS system showed that parse fragments could be used as the basis for learned extraction patterns (Lehnert et al. 1993).
Reference: <author> Carraso, R. C., and Oncina, J. </author> <year> 1994. </year> <title> Learning stochastic regular grammars by means of a state merging method. </title> <editor> In Carrasco, R. C., and Oncina, J., eds., </editor> <title> Grammatical Inference and Applications: </title> <booktitle> Second International Colloquium, </booktitle> <address> ICGI-94. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Jones would be translated into the three-symbol sequence [word-lower+dr, word+., capitalized-p+t]. GI algorithms For these experiments we applied two GI algorithms described in the literature, Alergia and ECGI. Aler-gia is an example of a state-merging approach to GI <ref> (Carraso & Oncina 1994) </ref>. Given a set of sequences which are supposed to be drawn from some language, Alergia begins its inference with a maximally specific prefix-tree automaton which accepts all and only the sequences in the training set.
Reference: <author> Clark, P., and Niblett, T. </author> <year> 1989. </year> <title> The CN2 induction algorithm. </title> <booktitle> Machine Learning 3(4) </booktitle> <pages> 261-263. </pages>
Reference-contexts: Rather than tinker with the feature set until, for each field, we came up with a transducer that seemed to work well, we developed an inference procedure for constructing a transducer similar in flavor to covering algorithms like CN2 <ref> (Clark & Niblett 1989) </ref> and Foil (Quinlan 1990). The input to this procedure (Infer-Transducer) is two sets of tokens, Positive and Negative|all tokens occurring inside (and outside, respectively) the desired field|and a set of features defined over tokens.
Reference: <author> Doorenbos, R. B.; Etzioni, O.; and Weld, D. S. </author> <year> 1996. </year> <title> A scalable comparison-shopping agent for the worldwide web. </title> <type> Technical Report 96-01-03, </type> <institution> Department of Computer Science, University of Washington. </institution>
Reference-contexts: At the University of Washington, several projects have explored to possibility of machine learning for information extraction from World-Wide Web pages. The ShopBot, for example, is able to locate catalog listings for novel vendor sites on the Web <ref> (Doorenbos, Etzioni, & Weld 1996) </ref>. Once there, it forms a model of the listing format, in order to extract details such as product name and price. In (Kushmer-ick, Weld, & Doorenbos 1997), this idea is elaborated into a general-purpose algorithm for inferring such patterns under certain assumptions.
Reference: <author> Goan, T.; Benson, N.; and Etzioni, O. </author> <year> 1996. </year> <title> A grammar inference algorithm for the World Wide Web. </title> <booktitle> Working Notes of the AAAI-96 Spring Symposium on Machine Learning in Information Access. </booktitle>
Reference-contexts: This can be regarded as special-purpose grammar inference for information extraction. The most pertinent (for this paper) result to come from this group, is the "Web Information Learner," an adaptation of the general GI algorithm Alergia targeted at the IE problem <ref> (Goan, Benson, & Etzioni 1996) </ref>. Naive Bayes We have experimented with several machine learning techniques in the seminar announcement domain. One surprisingly successful technique is an adaptation of a fairly simple method from document classification, called Naive Bayes.
Reference: <author> Kushmerick, N.; Weld, D.; and Doorenbos, B. </author> <year> 1997. </year> <title> Wrapper induction for information extraction. </title> <booktitle> In Submitted to the 17th International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Lehnert, W.; Cardie, C.; Fisher, D.; McCarthy, J.; Riloff, E.; and Soderland, S. </author> <year> 1992. </year> <title> University of Mas-sachusetts: description of the CIRCUS system as used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference (MUC-4). </booktitle>
Reference: <author> Lehnert, W.; McCarthy, J.; Soderland, S.; Riloff, E.; Cardie, C.; Peterson, J.; and Feng, F. </author> <year> 1993. </year> <title> UMass/Hughes: description of the CIRCUS system used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5). </booktitle>
Reference-contexts: The Hughes Trainable Text Skimmer, for example, used both k-nearest neighbor and Bayesian methods to estimate the likelihood that a text fragment instantiates a target field (August & Dolan 1992). The CIRCUS system showed that parse fragments could be used as the basis for learned extraction patterns <ref> (Lehnert et al. 1993) </ref>. Soderland developed CRYSTAL, a trainable IE module, as part of the CIRCUS project, and explored the use of several traditional ML methods for IE (Soderland 1996).
Reference: <author> Lewis, D. D., and Gale, W. A. </author> <year> 1994. </year> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of the 17th International Conference on Research and Development in Information Retrieval. 1992. Proceedings of the Fourth Message Understanding Conference (MUC-4), </booktitle> <address> McLean, Virginia: </address> <publisher> Morgan Kaufmann Publisher, Inc., </publisher> <address> San Francisco. </address> <year> 1993. </year> <booktitle> Proceedings of the Fifth Message Understanding Conference (MUC-5), </booktitle> <address> Baltimore, Maryland: </address> <publisher> Morgan Kaufmann Publisher, Inc., </publisher> <address> San Francisco. </address>
Reference-contexts: Formally: Pr (DjC i ) = t2D where t is some word or term. Since we can estimate Pr (tjC i ) with a simple counting approach over the training set, this yields a decision procedure that is easy to implement and efficient to compute. (But see <ref> (Lewis & Gale 1994) </ref> for less "naive" approaches.) Consider now the problem of identifying the location in a seminar announcement.
Reference: <author> Quinlan, J. R. </author> <year> 1990. </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning. </booktitle>
Reference-contexts: Rather than tinker with the feature set until, for each field, we came up with a transducer that seemed to work well, we developed an inference procedure for constructing a transducer similar in flavor to covering algorithms like CN2 (Clark & Niblett 1989) and Foil <ref> (Quinlan 1990) </ref>. The input to this procedure (Infer-Transducer) is two sets of tokens, Positive and Negative|all tokens occurring inside (and outside, respectively) the desired field|and a set of features defined over tokens.
Reference: <author> Riloff, E., and Shoen, J. </author> <year> 1995. </year> <title> Automatically acquiring conceptual patterns without an annotated corpus. </title> <booktitle> Proceedings of the Third Workshop on Very Large Corpora. </booktitle>
Reference-contexts: Riloff showed with the Autoslog system that useful extraction knowledge could be acquired without an annotated corpus, relying only on a relevant/irrelevant distinction between documents in the collection <ref> (Riloff & Shoen 1995) </ref>. While researchers in the IE community have turned their attention to ML methods, some machine learning researchers have shown an interest in the general IE problem.
Reference: <author> Rulot, H., and Vidal, E. </author> <year> 1988. </year> <title> An efficient algorithm for the inference of circuit-free automata. </title> <editor> In Ferrate, G. A., ed., </editor> <title> Syntactic and Structural Pattern Recognition. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference-contexts: In the experiments described here, ff was set to 0.8. We used our own implementation of Alergia. ECGI is an incremental approach to GI <ref> (Rulot & Vidal 1988) </ref>. Beginning with a grammar G, which describes exactly one sequence in the training set, ECGI incrementally repairs G, so that it also accepts subsequent sequences, at each step making minimal changes to G.
Reference: <author> Salton, G., and Buckley, C. </author> <year> 1987. </year> <title> Term weighting approaches in automatic text retrieval. </title> <type> Technical Report 87-881, </type> <institution> Department of Computer Science, Cor-nell University. </institution>
Reference-contexts: Otherwise, depending on the method, it is a boolean, reflecting the occurrence of the word; an integer word frequency; or a real-valued weight that indicates the importance of the word in the document. A very common method for setting weights is TFIDF (described, for example, in <ref> (Salton & Buckley 1987) </ref>). Under a Bayesian approach to document classification, we consider each of the n possible classifications, fC 1 : : : C n g, of a document D as a competing hypothesis.
Reference: <author> Soderland, S. </author> <year> 1996. </year> <title> Learning Text Analysis Rules for Domain-specific Natural Language Processing. </title> <type> Ph.D. Dissertation, </type> <institution> University of Massachusetts. </institution> <note> Avalable as Department of Computer Science Technical Report 96-087. </note>
Reference-contexts: The CIRCUS system showed that parse fragments could be used as the basis for learned extraction patterns (Lehnert et al. 1993). Soderland developed CRYSTAL, a trainable IE module, as part of the CIRCUS project, and explored the use of several traditional ML methods for IE <ref> (Soderland 1996) </ref>. Riloff showed with the Autoslog system that useful extraction knowledge could be acquired without an annotated corpus, relying only on a relevant/irrelevant distinction between documents in the collection (Riloff & Shoen 1995).
Reference: <author> Will, C. A. </author> <year> 1993. </year> <title> Comparing human and machine performance for natural language information extraction: results for English microelectronics from the MUC-5 evaluation. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference (MUC-5). </booktitle>
Reference-contexts: Speaker Confidence: -68.97 Fragment: Dr. Confidence: -80.84 Fragment: Antal Bejczy Lecture Nov. 11 of Naive Bayes. sion at a 79% recall level <ref> (Will 1993) </ref>. Thus, whether and where to label field boundaries can be open to considerable interpretation, and inconsistent labeling essentially guarantees some alignment error.
References-found: 17

