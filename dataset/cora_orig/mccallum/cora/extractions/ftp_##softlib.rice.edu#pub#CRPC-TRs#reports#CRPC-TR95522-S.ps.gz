URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95522-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: FIAT: A Framework for Interprocedural Analysis and Transformation  
Author: Mary W. Hall John M. Mellor-Crummey Alan Carle Rene G. Rodrguez 
Abstract: Modern architectures with deep memory hierarchies or parallelism require the use of increasingly sophisticated code analysis and optimization to achieve maximum performance for large, scientific programs. In such programs, procedure boundaries can obscure an analysis system's understanding of program behavior and interfere with optimization. To address this need, we are building fiat | a framework for interprocedural analysis and transformation that enables rapid prototyping of interproce-dural techniques. Components of the framework include an interprocedural data-flow analysis engine, a procedure cloning analysis engine and parameterized templates for data-flow problems. Fiat is being used to drive interprocedural optimization in the ParaScope programming tools at Rice University and the SUIF compiler at Stanford University. Fiat's use of system-independent abstractions makes it suitable for use in systems with distinct intermediate representations of programs and enables sharing of code across research platforms. Demand-driven analysis maintains a clean separation between interprocedural problems and enables tools built upon fiat to solve only the problems of immediate interest. Fiat has proven to be an effective aid in development of a large number of interprocedural tools, including completed systems for data race detection and static performance estimation, and partial implementations of a distributed-memory compiler for Fortran D, an interactive parallelizing tool and applications within the SUIF compiler.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed-memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, SIGPLAN Notices 28(7). ACM, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: The distributed-memory and distributed-shared-memory approaches will perform analysis of reaching decompositions and communication optimization, similar to the techniques used by the Fortran D compiler. However, the SUIF compiler operates on Fortran 77, automatically partitioning data and computation, rather than relying on programmer specifications <ref> [1, 2] </ref>. This distinction will greatly impact analysis of reaching decompositions, which now requires a flow-sensitive approach. Communication optimization will be based on Last Write Trees. 8 Summary and Future Plans Fiat provides a framework for interprocedural analysis and transformation that facilitates incorporating interprocedural techniques into existing systems.
Reference: [2] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, SIGPLAN Notices 28(7). ACM, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: The distributed-memory and distributed-shared-memory approaches will perform analysis of reaching decompositions and communication optimization, similar to the techniques used by the Fortran D compiler. However, the SUIF compiler operates on Fortran 77, automatically partitioning data and computation, rather than relying on programmer specifications <ref> [1, 2] </ref>. This distinction will greatly impact analysis of reaching decompositions, which now requires a flow-sensitive approach. Communication optimization will be based on Last Write Trees. 8 Summary and Future Plans Fiat provides a framework for interprocedural analysis and transformation that facilitates incorporating interprocedural techniques into existing systems.
Reference: [3] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: To assist parallelization in making the correct optimization choices, the ParaScope system contains static performance estimation [36]. The performance estimator uses a training set methodology to estimate for a given architecture the execution time of code constructs, in order to compare the effects of different parallelization choices <ref> [3] </ref>. In order to make decisions about loops containing procedure calls, these performance estimates must be propagated interprocedurally. Compiling Fortran D. Fortran D is a version of Fortran augmented with data decompositions that specify how data is mapped onto the nodes of a parallel machine [22].
Reference: [4] <author> Banning, J.P. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Proceedings of the Sixth Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 29-41. </pages> <publisher> ACM, </publisher> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: This organization is shown in Figure 1. 2.2 Interprocedural Data-Flow Analysis Interprocedural data-flow analysis can be described as either flow insensitive or flow sensitive. Flow-insensitive analysis calculates data-flow effects at program locations without considering the control flow 3 paths that will be taken through individual procedures <ref> [4] </ref>. In contrast, flow-sensitive analysis derives the effects common to each distinct control-flow path that reaches a location [45]. <p> We use several flow-insensitive data-flow problems as examples throughout the paper: * Mod (c): the set of variables that may be modified as a result of procedure call c, either by the called procedure or its descendants in the call graph <ref> [4] </ref>. * Ref (c): the set of variables that may be referenced as a result of a procedure call c [4]. * Alias (p): the set of variable pairs that may refer to the same memory location in procedure p, resulting from Fortran's call-by-reference parameter-passing mechanism [4]. * Constants (p): the <p> * Mod (c): the set of variables that may be modified as a result of procedure call c, either by the called procedure or its descendants in the call graph <ref> [4] </ref>. * Ref (c): the set of variables that may be referenced as a result of a procedure call c [4]. * Alias (p): the set of variable pairs that may refer to the same memory location in procedure p, resulting from Fortran's call-by-reference parameter-passing mechanism [4]. * Constants (p): the set of pairs hv; ci representing that variable v has constant value c across all calls to procedure p [13]. <p> in the call graph <ref> [4] </ref>. * Ref (c): the set of variables that may be referenced as a result of a procedure call c [4]. * Alias (p): the set of variable pairs that may refer to the same memory location in procedure p, resulting from Fortran's call-by-reference parameter-passing mechanism [4]. * Constants (p): the set of pairs hv; ci representing that variable v has constant value c across all calls to procedure p [13]. <p> The information for this call, in the callsites field, shows the names and types of the parameters. The tuple <ref> [16; 4] </ref> indicates the data of length 4 bytes starting at an offset of 16 bytes from the beginning of array A. The ProcSummary abstraction for R is similar, with field formals listing the formal parameters and their types. The call site in R invokes the procedure parameter X.
Reference: [5] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmarks programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Interpro-cedural transformations restructure a program to reveal more precise data-flow information or move code across procedure boundaries. Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization <ref> [5, 29, 49] </ref>. Techniques that have proven useful for this purpose include scalar and array side-effect analysis [31, 32, 39, 51], interprocedural constant propagation [23, 44], array Kill analysis [5, 26, 49], and transformation to expose loop nests to parallelization [29]. <p> Techniques that have proven useful for this purpose include scalar and array side-effect analysis [31, 32, 39, 51], interprocedural constant propagation [23, 44], array Kill analysis <ref> [5, 26, 49] </ref>, and transformation to expose loop nests to parallelization [29]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
Reference: [6] <author> Briggs, P., Cooper, K.D., Hall, M.W., and Torczon, L. </author> <title> Goal-directed interprocedural optimization. </title> <type> Technical Report TR90-148, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: Other examples of desirable cloning filters are discussed elsewhere [17, 27]. We define a cloning strategy that filters the data-flow sets to target specific optimization opportunities as goal-directed <ref> [6] </ref>. Fiat's cloning framework supports goal-directed cloning by allowing the programmer to specify a filtering function to distinguish between data-flow facts that have an impact on code quality and those that do not. 4 Note that the change in the graph may also indirectly sharpen solutions to backward data-flow problems.
Reference: [7] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, SIGPLAN Notices 21(7), </booktitle> <pages> pages 163-275. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1986. </year>
Reference-contexts: Dependence analysis, which provides the compiler's fundamental understanding of a program's inherent parallelism, must be extended to incorporate, in addition to scalar Mod and Ref analysis, 5 more precise side-effect information about the subportions of arrays affected by a call <ref> [51, 40, 7, 12] </ref>. Regular section analysis, which derives rectangular descriptions of array accesses due to a call, will serve this purpose [31]. Finally, Constants and interprocedural symbolic analysis refine dependence information by deriving information about loop bounds and subscript expressions.
Reference: [8] <author> M. Burke and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <note> to appear 1993. </note>
Reference-contexts: This property is true even if the module is used as part of several programs. Using this three phase analysis strategy along with a technique called recompilation analysis, a whole-program compiler could restrict recompilation to only those modules to which interprocedural optimizations were applied that are no longer valid <ref> [8] </ref>. The fiat system supports the interprocedural propagation phase. This phase is independent of the intermediate representation of source code used by the other phases. Fiat's input consists of local information for each procedure represented using a system-independent abstraction that is described in the next section.
Reference: [9] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: We outline them here and describe them in detail elsewhere [24]. After experience implementing a number of flow-sensitive data-flow problems, we will consider general solutions to these problems. 3 At procedure nodes, the solution represents variables live on exit, similar to Callahan's definition <ref> [9] </ref>. 11 Unrealizable paths. In our example, we propagated information along paths in the graph that do not correspond to possible execution paths, or realizable paths [38]. Specifically, Live (b5) = fT; Y g is imprecise. <p> This program representation may be quite large, particularly if the annotations on each control flow graph node are nearly as large or larger than the original source code. Moreover, it may be an inefficient representation for data-flow analysis as compared to sparse representations such as the program summary graph <ref> [9] </ref>, the SSA graph [21] or Sparse Data Flow Evaluation Graphs [15]. These graphs provide direct 12 connections in a graph between nodes that create and use data-flow information.
Reference: [10] <author> D. Callahan. </author> <title> The program summary graph and flow-sensitive interprocedural data flow analysis. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, SIGPLAN Notices 23(7), </booktitle> <pages> pages 47-56. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1988. </year>
Reference-contexts: paths: (1) data-flow sets are tagged with path history information that describes the path taken to derive a set [14, 45, 48, 30]; (2) data-flow sets are tagged with some other identifying information related to a specific calling context [38]; or (3) a side-effect computation is performed, ignoring calling context <ref> [10] </ref>. The third alternative avoids the unrealizable path problem entirely but cannot be applied to every data-flow problem. Tagging with a path history, while general, potentially induces an exponential explosion on the number of data-flow sets (or infinite in the case of recursion) and must therefore be bounded.
Reference: [11] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: remainder of the paper, here we describe the architecture of whole-program analysis systems for which fiat was designed and introduce some terminology for interprocedural data-flow analysis and interprocedural transformation. 2.1 Three-Phase Approach to Whole-Program Analysis Fiat has been designed for use in whole-program analysis systems based on a three-phase strategy <ref> [11, 19, 25] </ref>: 1. Local Analysis. Essential information describing each procedure in a module is collected and saved. This information describes the formal parameters and call sites defining the procedure's interface. In addition, immediate interprocedural effects relevant to particular data-flow problems are collected for use in initializing interprocedural propagation. 2.
Reference: [12] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 517-550, </pages> <year> 1988. </year>
Reference-contexts: Dependence analysis, which provides the compiler's fundamental understanding of a program's inherent parallelism, must be extended to incorporate, in addition to scalar Mod and Ref analysis, 5 more precise side-effect information about the subportions of arrays affected by a call <ref> [51, 40, 7, 12] </ref>. Regular section analysis, which derives rectangular descriptions of array accesses due to a call, will serve this purpose [31]. Finally, Constants and interprocedural symbolic analysis refine dependence information by deriving information about loop bounds and subscript expressions.
Reference: [13] <author> Callahan, D., Cooper, K.D., Kennedy, K., and Torczon, L. </author> <title> Interprocedural constant propagation. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 21(7) </volume> <pages> 152-161, </pages> <year> 1986. </year> <month> 19 </month>
Reference-contexts: [4]. * Alias (p): the set of variable pairs that may refer to the same memory location in procedure p, resulting from Fortran's call-by-reference parameter-passing mechanism [4]. * Constants (p): the set of pairs hv; ci representing that variable v has constant value c across all calls to procedure p <ref> [13] </ref>. We use the following example of a flow-sensitive problem in Section 5. * Live (n): the set of variables v that may be used prior to any redefinition [45].
Reference: [14] <author> J. Choi, M. Burke, and P. Carini. </author> <title> Efficient flow-sensitive interprocedural computation of pointer-induced aliases and side effects. </title> <booktitle> In Conference Record of the Twentieth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: Three basic strategies have been used to avoid propagating information along unrealizable paths: (1) data-flow sets are tagged with path history information that describes the path taken to derive a set <ref> [14, 45, 48, 30] </ref>; (2) data-flow sets are tagged with some other identifying information related to a specific calling context [38]; or (3) a side-effect computation is performed, ignoring calling context [10]. The third alternative avoids the unrealizable path problem entirely but cannot be applied to every data-flow problem.
Reference: [15] <author> J. Choi, R. Cytron, and J. Ferrante. </author> <title> Automatic construction of sparse data flow evaluation graphs. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 55-66, </pages> <address> Orlando, FL, </address> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Moreover, it may be an inefficient representation for data-flow analysis as compared to sparse representations such as the program summary graph [9], the SSA graph [21] or Sparse Data Flow Evaluation Graphs <ref> [15] </ref>. These graphs provide direct 12 connections in a graph between nodes that create and use data-flow information.
Reference: [16] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torc-zon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 244-263, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Moreover, fiat's general framework greatly reduces the programming effort needed to incorporate new in-terprocedural data-flow analyses and transformations. Fiat currently supports interprocedural optimization in two very different host systems | the ParaScope programming tools at Rice University <ref> [16] </ref> and the Stanford SUIF Compiler [50]. This paper describes the following key aspects of fiat. Abstract representations. The representation of programs used within fiat is decoupled from the representation used by any host system in which fiat will be applied. <p> The information for this call, in the callsites field, shows the names and types of the parameters. The tuple <ref> [16; 4] </ref> indicates the data of length 4 bytes starting at an offset of 16 bytes from the beginning of array A. The ProcSummary abstraction for R is similar, with field formals listing the formal parameters and their types. The call site in R invokes the procedure parameter X.
Reference: [17] <author> K. Cooper, M. W. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Computer Language, </booktitle> <address> Oakland, CA, </address> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: A B C (a) Interprocedural Analysis A B C (b) Inline Substitution A B C (c) Procedure Cloning A D1 (d) Interprocedural Code Motion 4 Procedure cloning. To perform this transformation, the compiler replicates a procedure and tailors the procedure and its copy to distinct calling environments <ref> [17, 19] </ref>. In Figure 2 (c), procedure D is cloned, inheriting distinct calling context information from A. Procedures B and C both invoke the same copy D2. Cloning thus avoids summarization of information inherited from callers. <p> We then briefly discuss the requirements for frameworks to support other interprocedural transformations. 6.1 A Framework for Procedure Cloning The design of the cloning framework is based on previous research on how to apply cloning to expose better data-flow information while avoiding the potential for exponential code growth <ref> [17] </ref>. Two key insights about cloning motivated the implementation: Refines forward data-flow information. Cloning changes the structure of the call graph in a way that allows interprocedural analysis to proceed along distinct paths. <p> To avoid creating superfluous clones, and thus reduce code growth, we would like to filter the set of interprocedural constants such that the cloner only examines information for referenced variables. Other examples of desirable cloning filters are discussed elsewhere <ref> [17, 27] </ref>. We define a cloning strategy that filters the data-flow sets to target specific optimization opportunities as goal-directed [6]. <p> For clarity of presentation, this algorithm assumes that the call graph contains no cycles (signifying recursion). Support for recursion is described elsewhere <ref> [17] </ref>.
Reference: [18] <author> K. Cooper, M. W. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <journal> Software|Practice and Experience, </journal> <volume> 21(6) </volume> <pages> 581-601, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In Figure 2 (b), by inlining the calls to D, each version of D may be fully optimized in the context of its caller. However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time <ref> [47, 18] </ref>. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits [18, 34, 42, 35, 46]. <p> However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time [47, 18]. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits <ref> [18, 34, 42, 35, 46] </ref>. In light of the limitations of both inline substitution and interprocedural data-flow analysis, our research has explored additional techniques that provide some of the power of inlining but with less associated costs.
Reference: [19] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the R n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> Oct. </month> <year> 1986. </year>
Reference-contexts: remainder of the paper, here we describe the architecture of whole-program analysis systems for which fiat was designed and introduce some terminology for interprocedural data-flow analysis and interprocedural transformation. 2.1 Three-Phase Approach to Whole-Program Analysis Fiat has been designed for use in whole-program analysis systems based on a three-phase strategy <ref> [11, 19, 25] </ref>: 1. Local Analysis. Essential information describing each procedure in a module is collected and saved. This information describes the formal parameters and call sites defining the procedure's interface. In addition, immediate interprocedural effects relevant to particular data-flow problems are collected for use in initializing interprocedural propagation. 2. <p> A B C (a) Interprocedural Analysis A B C (b) Inline Substitution A B C (c) Procedure Cloning A D1 (d) Interprocedural Code Motion 4 Procedure cloning. To perform this transformation, the compiler replicates a procedure and tailors the procedure and its copy to distinct calling environments <ref> [17, 19] </ref>. In Figure 2 (c), procedure D is cloned, inheriting distinct calling context information from A. Procedures B and C both invoke the same copy D2. Cloning thus avoids summarization of information inherited from callers.
Reference: [20] <author> K. D. Cooper, M. Hall, and L. Torczon. </author> <title> Unexpected side effects of inline substitution a case study. </title> <journal> Letters on Programming Languages and Systems, </journal> <volume> 1(1), </volume> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Because Alias analysis is flow-insensitive, it will falsely report aliases which may lead to overly conservative dependence information <ref> [20] </ref>. Since it is a violation of the Fortran standard for two variables to be aliased across a procedure call if one of the variables is modified, ignoring aliases is safe for all standard-conforming Fortran programs. 17 Data race detection.
Reference: [21] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Conference Record of the Sixteenth Annual Symposium on Principles of Programming Languages, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Moreover, it may be an inefficient representation for data-flow analysis as compared to sparse representations such as the program summary graph [9], the SSA graph <ref> [21] </ref> or Sparse Data Flow Evaluation Graphs [15]. These graphs provide direct 12 connections in a graph between nodes that create and use data-flow information.
Reference: [22] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: In order to make decisions about loops containing procedure calls, these performance estimates must be propagated interprocedurally. Compiling Fortran D. Fortran D is a version of Fortran augmented with data decompositions that specify how data is mapped onto the nodes of a parallel machine <ref> [22] </ref>. The Fortran D compiler for distributed-memory machines uses data decompositions to partition a program so that a processor only performs computations for data stored locally; it also inserts communication for nonlocal data accessed by each processor.
Reference: [23] <author> D. Grove. </author> <title> Interprocedural constant propagation: a study of jump function implementations. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization [5, 29, 49]. Techniques that have proven useful for this purpose include scalar and array side-effect analysis [31, 32, 39, 51], interprocedural constant propagation <ref> [23, 44] </ref>, array Kill analysis [5, 26, 49], and transformation to expose loop nests to parallelization [29]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
Reference: [24] <author> M. Hall. </author> <title> Comparing fiat's support of flow-sensitive interprocedural data-flow analysis with existing techniques. </title> <note> Technical Note Available from First Author. To be published as a Stanford Dept. of Computer Science Technical Report. </note>
Reference-contexts: The flow-sensitive solver described above fails to address two important issues discussed in other research. We outline them here and describe them in detail elsewhere <ref> [24] </ref>. After experience implementing a number of flow-sensitive data-flow problems, we will consider general solutions to these problems. 3 At procedure nodes, the solution represents variables live on exit, similar to Callahan's definition [9]. 11 Unrealizable paths.
Reference: [25] <author> M. W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: remainder of the paper, here we describe the architecture of whole-program analysis systems for which fiat was designed and introduce some terminology for interprocedural data-flow analysis and interprocedural transformation. 2.1 Three-Phase Approach to Whole-Program Analysis Fiat has been designed for use in whole-program analysis systems based on a three-phase strategy <ref> [11, 19, 25] </ref>: 1. Local Analysis. Essential information describing each procedure in a module is collected and saved. This information describes the formal parameters and call sites defining the procedure's interface. In addition, immediate interprocedural effects relevant to particular data-flow problems are collected for use in initializing interprocedural propagation. 2.
Reference: [26] <author> M. W. Hall, T. Harvey, K. Kennedy, N. McIntosh, K. S. M c Kinley, J. D. Oldham, M. Paleczny, and G. Roth. </author> <title> Experiences using the ParaScope Editor: an interactive parallel programming tool. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Techniques that have proven useful for this purpose include scalar and array side-effect analysis [31, 32, 39, 51], interprocedural constant propagation [23, 44], array Kill analysis <ref> [5, 26, 49] </ref>, and transformation to expose loop nests to parallelization [29]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
Reference: [27] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: To avoid creating superfluous clones, and thus reduce code growth, we would like to filter the set of interprocedural constants such that the cloner only examines information for referenced variables. Other examples of desirable cloning filters are discussed elsewhere <ref> [17, 27] </ref>. We define a cloning strategy that filters the data-flow sets to target specific optimization opportunities as goal-directed [6]. <p> The compiler must thus propagate decompositions inter-procedurally in order to determine how data is partitioned locally. Moreover, an interprocedural approach to communication optimization would enable communication to be extracted out of loops across procedure boundaries. Techniques for interprocedural analysis and optimization of Fortran D are described elsewhere <ref> [27] </ref>. The current implementation, based on fiat, contains analysis of decompositions reaching a procedure, and analysis of storage requirements for nonlocal data; interprocedural communication optimizations have not yet been implemented. 5 It is not clear whether Alias information should be used when combining interprocedural information with dependence analysis.
Reference: [28] <author> M. W. Hall and K. Kennedy. </author> <title> Efficient call graph analysis. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 1(3), </volume> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: To complete construction of the call graph, fiat performs call graph analysis to determine the set of possible bindings for Fortran-style procedure-valued formal parameters <ref> [28] </ref>. Call graph analysis simulates parameter passing to compute, for each procedure-valued formal f , the set Boundto (f ) containing the possible procedure constants that may be passed to f at run time.
Reference: [29] <author> M. W. Hall, K. Kennedy, and K. S. M c Kinley. </author> <title> Interprocedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Interpro-cedural transformations restructure a program to reveal more precise data-flow information or move code across procedure boundaries. Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization <ref> [5, 29, 49] </ref>. Techniques that have proven useful for this purpose include scalar and array side-effect analysis [31, 32, 39, 51], interprocedural constant propagation [23, 44], array Kill analysis [5, 26, 49], and transformation to expose loop nests to parallelization [29]. <p> Techniques that have proven useful for this purpose include scalar and array side-effect analysis [31, 32, 39, 51], interprocedural constant propagation [23, 44], array Kill analysis [5, 26, 49], and transformation to expose loop nests to parallelization <ref> [29] </ref>. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information. <p> Interprocedural code motion. This transformation moves some portion of code across the procedure boundary. In addition to inline substitution, two other forms of code motion have been explored: loop embedding and loop extraction <ref> [29] </ref>. The former moves a loop surrounding a call into the invoked procedure; the latter moves an outer enclosing loop across a call and into the caller. These transformations partially inline the code. <p> Loop embedding and loop extraction require similar support to inlining. Assume that a loop is moved across exactly one call. The system could determine whether embedding and extraction are safe <ref> [29] </ref>. For instances of safe embedding or extraction, a decision procedure could be invoked to determine if the techniques are desirable.
Reference: [30] <author> W. Harrison. </author> <title> The interprocedural analysis and automatic parallelization of Scheme programs. </title> <journal> Lisp and Symbolic Computation, </journal> 2(3/4):179-396, Oct. 1989. 
Reference-contexts: Three basic strategies have been used to avoid propagating information along unrealizable paths: (1) data-flow sets are tagged with path history information that describes the path taken to derive a set <ref> [14, 45, 48, 30] </ref>; (2) data-flow sets are tagged with some other identifying information related to a specific calling context [38]; or (3) a side-effect computation is performed, ignoring calling context [10]. The third alternative avoids the unrealizable path problem entirely but cannot be applied to every data-flow problem.
Reference: [31] <author> Havlak, P. and Kennedy, K. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization [5, 29, 49]. Techniques that have proven useful for this purpose include scalar and array side-effect analysis <ref> [31, 32, 39, 51] </ref>, interprocedural constant propagation [23, 44], array Kill analysis [5, 26, 49], and transformation to expose loop nests to parallelization [29]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information. <p> Regular section analysis, which derives rectangular descriptions of array accesses due to a call, will serve this purpose <ref> [31] </ref>. Finally, Constants and interprocedural symbolic analysis refine dependence information by deriving information about loop bounds and subscript expressions. Of these, Mod and Ref are implemented and available in the ParaScope Editor, Constants is implemented but not integrated, and regular section analysis and interprocedural symbolic analysis are nearly completed.
Reference: [32] <author> M. Hind, M. Burke, P. Carini, and S. Midkiff. </author> <title> Interprocedural array analysis: How much precision do we need? In Proceedings of the Third Workshop on Compilers for Parallel Computers, </title> <address> Vienna, Austria, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization [5, 29, 49]. Techniques that have proven useful for this purpose include scalar and array side-effect analysis <ref> [31, 32, 39, 51] </ref>, interprocedural constant propagation [23, 44], array Kill analysis [5, 26, 49], and transformation to expose loop nests to parallelization [29]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
Reference: [33] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory 20 machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The compiler performs extensive program analysis as well as optimizations such as extracting messages out of loops in order to reduce communication cost <ref> [33] </ref>. The semantics of Fortran D dictate that a procedure inherits the data decompositions of its formal parameters and global variables from its callers. The compiler must thus propagate decompositions inter-procedurally in order to determine how data is partitioned locally.
Reference: [34] <author> A. M. Holler. </author> <title> A Study of the Effects of Subprogram Inlining. </title> <type> PhD thesis, </type> <institution> Univ. of Virginia, Char-lottesville, VA, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time [47, 18]. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits <ref> [18, 34, 42, 35, 46] </ref>. In light of the limitations of both inline substitution and interprocedural data-flow analysis, our research has explored additional techniques that provide some of the power of inlining but with less associated costs.
Reference: [35] <author> W. W. Hwu and P. P. Chang. </author> <title> Inline function expansion for inlining C programs. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time [47, 18]. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits <ref> [18, 34, 42, 35, 46] </ref>. In light of the limitations of both inline substitution and interprocedural data-flow analysis, our research has explored additional techniques that provide some of the power of inlining but with less associated costs.
Reference: [36] <author> K. Kennedy, N. McIntosh, and K. S. M c Kinley. </author> <title> Static performance estimation in a parallelizing compiler. </title> <type> Technical Report TR91-174, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: Moreover, often when performing loop transformations, the system has a number of possible options that may have substantially different effects on performance. To assist parallelization in making the correct optimization choices, the ParaScope system contains static performance estimation <ref> [36] </ref>. The performance estimator uses a training set methodology to estimate for a given architecture the execution time of code constructs, in order to compare the effects of different parallelization choices [3]. In order to make decisions about loops containing procedure calls, these performance estimates must be propagated interprocedurally.
Reference: [37] <author> Kildall, G. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Conference Record of the Symposium on Principles of Programming Languages, </booktitle> <pages> pages 194-206. </pages> <publisher> ACM, </publisher> <month> Jan. </month> <year> 1973. </year>
Reference-contexts: but other problems specific to compiling programs for distributed memory multiprocessors or automatically instrumenting shared-memory programs to detect data races are not included when building tools for any other purpose. 5 A Parameterized Template for Interprocedural Analysis Fiat provides a framework for interprocedural data-flow analysis including a Kildall-style iterative solver <ref> [37] </ref>. <p> The programmer provides specifications to be used by the interprocedural data-flow solver to compute values of data-flow sets at nodes and edges in the call graph. The functions provided to fiat are simply the specifications for the Kildall data-flow analysis framework <ref> [37] </ref>: * InitNode (n) and InitEdge (e) provide the initial annotations at node n and edge e, respectively.
Reference: [38] <author> W. Landi and B. Ryder. </author> <title> A safe approximate algorithm for interprocedural pointer aliasing. </title> <booktitle> In SIGPLAN '92 Conference on Programming Language Design and Implementation, SIGPLAN Notices 27(7), </booktitle> <pages> pages 235-248, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: In our example, we propagated information along paths in the graph that do not correspond to possible execution paths, or realizable paths <ref> [38] </ref>. Specifically, Live (b5) = fT; Y g is imprecise. It results from propagation of values along the following subpath in the graph: : : :, b4, b3, b10, b9, b8, b5, : : :. <p> basic strategies have been used to avoid propagating information along unrealizable paths: (1) data-flow sets are tagged with path history information that describes the path taken to derive a set [14, 45, 48, 30]; (2) data-flow sets are tagged with some other identifying information related to a specific calling context <ref> [38] </ref>; or (3) a side-effect computation is performed, ignoring calling context [10]. The third alternative avoids the unrealizable path problem entirely but cannot be applied to every data-flow problem.
Reference: [39] <author> Z. Li and P.-C. Yew. </author> <title> Interprocedural analysis and program restructuring for parallel programs. </title> <type> Technical Report CSRD-720, </type> <institution> University of Illinois, Urbana-Champaign, </institution> <month> Jan. </month> <year> 1988. </year>
Reference-contexts: Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization [5, 29, 49]. Techniques that have proven useful for this purpose include scalar and array side-effect analysis <ref> [31, 32, 39, 51] </ref>, interprocedural constant propagation [23, 44], array Kill analysis [5, 26, 49], and transformation to expose loop nests to parallelization [29]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
Reference: [40] <author> Li, Z. and Yew, P. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 23(9) </volume> <pages> 85-99, </pages> <year> 1988. </year>
Reference-contexts: Dependence analysis, which provides the compiler's fundamental understanding of a program's inherent parallelism, must be extended to incorporate, in addition to scalar Mod and Ref analysis, 5 more precise side-effect information about the subportions of arrays affected by a call <ref> [51, 40, 7, 12] </ref>. Regular section analysis, which derives rectangular descriptions of array accesses due to a call, will serve this purpose [31]. Finally, Constants and interprocedural symbolic analysis refine dependence information by deriving information about loop bounds and subscript expressions.
Reference: [41] <author> D. Maydan, S. Amarasinghe, and M. Lam. </author> <title> Array data-flow analysis and its use in array privatization. </title> <booktitle> In Conference Record of the Twentieth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: For example, the approach to interprocedural array privatization will rely on Last Write Trees, which characterize for a given read statement in a loop the iteration corresponding to the last write of that object <ref> [41] </ref>. The distributed-memory and distributed-shared-memory approaches will perform analysis of reaching decompositions and communication optimization, similar to the techniques used by the Fortran D compiler. However, the SUIF compiler operates on Fortran 77, automatically partitioning data and computation, rather than relying on programmer specifications [1, 2].
Reference: [42] <author> S. McFarling. </author> <title> Procedure merging with instruction caches. </title> <booktitle> In SIGPLAN '91 Conference on Programming Language Design and Implementation, SIGPLAN Notices 26(6), </booktitle> <pages> pages 71-79, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time [47, 18]. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits <ref> [18, 34, 42, 35, 46] </ref>. In light of the limitations of both inline substitution and interprocedural data-flow analysis, our research has explored additional techniques that provide some of the power of inlining but with less associated costs.
Reference: [43] <author> J. M. Mellor-Crummey. </author> <title> Compile-time support for efficient data race detection in shared-memory parallel programs. </title> <booktitle> In Proc. ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: However, such a naive instrumentation strategy can cause program execution time to increase by more than a factor of fifteen. The eraser data race instrumentation system was developed to identify accesses that could never participate in data races and therefore need not be instrumented and subject to run-time monitoring <ref> [43] </ref>. Built on top of the ParaScope infrastructure, eraser uses ParaScope's data dependence analysis to identify pairs of references that might be involved in data races.
Reference: [44] <author> Metzger, R. and Stroud, S. </author> <title> Interprocedural constant propagation: an empirical study. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 1(2), </volume> <month> June </month> <year> 1992. </year>
Reference-contexts: Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization [5, 29, 49]. Techniques that have proven useful for this purpose include scalar and array side-effect analysis [31, 32, 39, 51], interprocedural constant propagation <ref> [23, 44] </ref>, array Kill analysis [5, 26, 49], and transformation to expose loop nests to parallelization [29]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
Reference: [45] <author> E. Myers. </author> <title> A precise inter-procedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: Flow-insensitive analysis calculates data-flow effects at program locations without considering the control flow 3 paths that will be taken through individual procedures [4]. In contrast, flow-sensitive analysis derives the effects common to each distinct control-flow path that reaches a location <ref> [45] </ref>. <p> We use the following example of a flow-sensitive problem in Section 5. * Live (n): the set of variables v that may be used prior to any redefinition <ref> [45] </ref>. In the above definition, the node n represents either a procedure or a block of code within a procedure. 2.3 Interprocedural Transformations three calls to procedures not shown. <p> Three basic strategies have been used to avoid propagating information along unrealizable paths: (1) data-flow sets are tagged with path history information that describes the path taken to derive a set <ref> [14, 45, 48, 30] </ref>; (2) data-flow sets are tagged with some other identifying information related to a specific calling context [38]; or (3) a side-effect computation is performed, ignoring calling context [10]. The third alternative avoids the unrealizable path problem entirely but cannot be applied to every data-flow problem.
Reference: [46] <author> S. Richardson. </author> <title> Evaluating interprocedural code optimization techniques. </title> <type> PhD thesis, </type> <institution> Stanford University, Dept. of Electrical Engineering and Computer Science, Stanford, </institution> <address> CA, </address> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time [47, 18]. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits <ref> [18, 34, 42, 35, 46] </ref>. In light of the limitations of both inline substitution and interprocedural data-flow analysis, our research has explored additional techniques that provide some of the power of inlining but with less associated costs.
Reference: [47] <author> S. Richardson and M. Ganapathi. </author> <title> Interprocedural analysis versus procedure integration. </title> <journal> Information Processing Letters, </journal> <volume> 32(3) </volume> <pages> 137-142, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: In Figure 2 (b), by inlining the calls to D, each version of D may be fully optimized in the context of its caller. However, inlining can sometimes lead to unmanageable code explosion, resulting in prohibitive increases in compile time <ref> [47, 18] </ref>. Moreover, execution-time performance may degrade when optimizers fail to exploit the additional context inlining exposes or when the code size exceeds register, cache or paging system limits [18, 34, 42, 35, 46].
Reference: [48] <author> M. Sharir and A. Pnueli. </author> <title> Two approaches to interprocedural data flow analysis. </title> <editor> In S. Muchnick and N. Jones, editors, </editor> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice Hall Inc, </publisher> <year> 1981. </year>
Reference-contexts: Three basic strategies have been used to avoid propagating information along unrealizable paths: (1) data-flow sets are tagged with path history information that describes the path taken to derive a set <ref> [14, 45, 48, 30] </ref>; (2) data-flow sets are tagged with some other identifying information related to a specific calling context [38]; or (3) a side-effect computation is performed, ignoring calling context [10]. The third alternative avoids the unrealizable path problem entirely but cannot be applied to every data-flow problem.
Reference: [49] <author> J. Singh and J. Hennessy. </author> <title> An empirical investigation of the effectiveness of and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> Tokyo, Japan, </address> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: Interpro-cedural transformations restructure a program to reveal more precise data-flow information or move code across procedure boundaries. Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization <ref> [5, 29, 49] </ref>. Techniques that have proven useful for this purpose include scalar and array side-effect analysis [31, 32, 39, 51], interprocedural constant propagation [23, 44], array Kill analysis [5, 26, 49], and transformation to expose loop nests to parallelization [29]. <p> Techniques that have proven useful for this purpose include scalar and array side-effect analysis [31, 32, 39, 51], interprocedural constant propagation [23, 44], array Kill analysis <ref> [5, 26, 49] </ref>, and transformation to expose loop nests to parallelization [29]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information.
Reference: [50] <author> S. Tjiang, M. E. Wolf, M. Lam, K. Pieper, and J. Hennessy. </author> <title> Integrating scalar optimization and paral-lelization. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, Aug. 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Moreover, fiat's general framework greatly reduces the programming effort needed to incorporate new in-terprocedural data-flow analyses and transformations. Fiat currently supports interprocedural optimization in two very different host systems | the ParaScope programming tools at Rice University [16] and the Stanford SUIF Compiler <ref> [50] </ref>. This paper describes the following key aspects of fiat. Abstract representations. The representation of programs used within fiat is decoupled from the representation used by any host system in which fiat will be applied.
Reference: [51] <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of CALL statements. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year> <month> 21 </month>
Reference-contexts: Many researchers have demonstrated significant performance improvements on shared-memory multiprocessors by manually applying interprocedural analysis and transformation techniques to enhance parallelism or memory hierarchy utilization [5, 29, 49]. Techniques that have proven useful for this purpose include scalar and array side-effect analysis <ref> [31, 32, 39, 51] </ref>, interprocedural constant propagation [23, 44], array Kill analysis [5, 26, 49], and transformation to expose loop nests to parallelization [29]. In addition to automatic parallelization, many problem domains in high-performance computing can greatly benefit from exploiting interprocedural information. <p> Dependence analysis, which provides the compiler's fundamental understanding of a program's inherent parallelism, must be extended to incorporate, in addition to scalar Mod and Ref analysis, 5 more precise side-effect information about the subportions of arrays affected by a call <ref> [51, 40, 7, 12] </ref>. Regular section analysis, which derives rectangular descriptions of array accesses due to a call, will serve this purpose [31]. Finally, Constants and interprocedural symbolic analysis refine dependence information by deriving information about loop bounds and subscript expressions.
References-found: 51

