URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/BarKroNoe93.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A robust multi-resolution vision system for target tracking with a moving camera  
Author: M.G.P. Bartholomeus B.J.A. Krose A.J. Noest 
Address: Kruislaan 403, NL-1098 SJ Amsterdam  Princetonplein 5, NL-3584 CC Utrecht  
Affiliation: yFaculty of Mathematics and Computer Science, University of Amsterdam  zBiophysics Research Institute, University of Utrecht  
Abstract: Robot systems which use vision to track a moving target encounter the problem that the moving target has to be discriminated from the moving background. This paper describes a vision system which is able to detect such a moving target and is robust with respect to variations in the motion parameters of target and background and variations in the image texture. The method is based on a multi-resolution data representation and extracts "votes" for a restricted set of velocities in the image. Segmentation is carried out on the basis of these votes. The vision system is used in combination with a self-learning algorithm which maps image data to joint data for the visual servoing of a robot manipulator. keywords: robot vision, visual servoing, target tracking, visual motion
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.K. Allen, B. Yoshimi and A. Timcenko. </author> <title> "Real-time visual servoing", </title> <booktitle> Proceedings of the 1991 IEEE Conference on Robotics and Automation (1991) 851-856. </booktitle>
Reference-contexts: Visual servoing contains two aspects: the vision part and the control part. Successful approaches have been presented in which 3-D motion parameters of the target are computed from static cameras and are used to control the robot <ref> [1] </ref>. However, in most of the visual servoing applications vision information is mapped directly to the control domain, and the task of the robot is to reach setpoints which are defined in the vision domain [2][3][4][5].
Reference: [2] <author> F. Chaumette, P. Rives and B. Espiau, </author> <title> "Positioning of a robot with respect to an object, tracking it and estimating its velocity by visual servoing", </title> <booktitle> Proceedings of the 1991 IEEE Conference on Robotics and Automation (1991) 2248-2253. </booktitle>
Reference-contexts: mounted near the end-effector, and in an accurate tracking if the target is moving . fl This work has been partially sponsored by the Dutch Foundation for Neural Networks 1 Setpoints in the vision domain such as locations of specific image details, areas, etc. are usually predefined by the user <ref> [2] </ref>, [4] but can also be generated by computer models of the (moving) target [5]. In some applications however, the target is defined solely by its motion relative to the background.
Reference: [3] <author> L.E. Weiss, A.C. Sanderson and C.P. Neuman, </author> <title> "Dynamic Sensor-based control of robots with visual feedback", </title> <journal> IEEE Journal of Robotics and Information RA-3, </journal> <month> 5 </month> <year> (1987) </year> <month> 404-417. </month>
Reference: [4] <author> W. Jang and Zeungnam Bien. </author> <title> "Feature-based visual servoing of an eye-in-hand robot with improved tracking performance", </title> <booktitle> Proceedings of the 1991 IEEE Conference on Robotics and Automation (1991) 2254-2260. </booktitle>
Reference-contexts: near the end-effector, and in an accurate tracking if the target is moving . fl This work has been partially sponsored by the Dutch Foundation for Neural Networks 1 Setpoints in the vision domain such as locations of specific image details, areas, etc. are usually predefined by the user [2], <ref> [4] </ref> but can also be generated by computer models of the (moving) target [5]. In some applications however, the target is defined solely by its motion relative to the background.
Reference: [5] <author> P.A. Couvignou, </author> <title> N.P. Papanikopoulos and P.K. Khosla "Hand-eye robotic visual servoing around moving objects using active deformable models" Proceedings of the IROS (1992) 1855-1862. </title>
Reference-contexts: . fl This work has been partially sponsored by the Dutch Foundation for Neural Networks 1 Setpoints in the vision domain such as locations of specific image details, areas, etc. are usually predefined by the user [2], [4] but can also be generated by computer models of the (moving) target <ref> [5] </ref>. In some applications however, the target is defined solely by its motion relative to the background. As long as the background is stationary this causes no problem: a simple motion detection algorithm could find the setpoint for the control system.
Reference: [6] <author> P.J. Burt, J.R. Bergen, H. Hingorani, R. Kolczynski, W.A. Lee, A. Leung, J. Lubin and H. Shvaytser, </author> <title> "Object tracking with a moving camera", </title> <booktitle> Proceedings of the IEEE workshop on visual motion (1989), </booktitle> <pages> 2-12. </pages>
Reference-contexts: In order to find the target in the image domain, segmentation will have to be carried out to distinguish between target and background <ref> [6] </ref>. In our research we investigate how computational models derived from biological information processing systems can be applied to create a robust sensor-motor interaction for a robot system.
Reference: [7] <author> Koenderink, J.J. and A.J. van Doorn. </author> <title> "Representation of local geometry in the visual system", </title> <journal> Biol. Cybern. </journal> <volume> 55 (1987), </volume> <pages> 367-375. </pages>
Reference-contexts: It is useful to take these quantities to be low-order partial derivatives of the Gaussian blurred image. The whole collection of these blurred derivatives up to n-th order forms the `n-jet' representation of the image in scale-space <ref> [7] </ref>. Living visual systems actually use a formally equivalent, but redundant representation consisting of directional derivatives (up to about n = 4) in 3 many directions.
Reference: [8] <author> B.K.P. Horn and B.G. Schunck. </author> <title> "Determining optical flow", </title> <booktitle> Artif. Intell. 17 (1981), </booktitle> <pages> 185-203. </pages>
Reference-contexts: Conceptually the most straightforward schemes for velocity measurement rely on detecting oriented structure in space-time, generalizing the edge-detection scheme mentioned before. In principle, one could compute the velocity component v in the direction of the spatial gradient u explicitly <ref> [8] </ref> using v = @ t I=@ u I, but this is marred by a robustness problem: in regions of low signal to noise ratio, the problem of small divisors will produce essentially random velocities.
Reference: [9] <author> E.H. Adelson and J.R Bergen. </author> <title> "Spatiotemporal energy models for the perception of motion" J.Opt.Soc.Am.A, </title> <type> 2, 284, </type> <month> 299 </month> <year> (1985) </year>
Reference-contexts: The votes carry a weight given by a measure of 4 represent a pair of opposite nominal velocities. the relevant space-time contrast. Our method is a computationally efficient variation on the `motion energy' scheme <ref> [9] </ref>, which -in turn- is mathematically equivalent to certain correlation detection schemes [10]. All of these methods incorporate some form of velocity-tuning [11]. Spatial smoothing of the motion votes for each nominal velocity separately is an entirely sensible operation when the optic flow field is resonably smooth.
Reference: [10] <author> J.P.H. van Santen and G.Sperling. </author> <title> "Elaborated Reichardt detectors." </title> <journal> J.Opt.Soc.Am.A, </journal> <volume> 2, 300, </volume> <month> 321 </month> <year> (1985) </year>
Reference-contexts: The votes carry a weight given by a measure of 4 represent a pair of opposite nominal velocities. the relevant space-time contrast. Our method is a computationally efficient variation on the `motion energy' scheme [9], which -in turn- is mathematically equivalent to certain correlation detection schemes <ref> [10] </ref>. All of these methods incorporate some form of velocity-tuning [11]. Spatial smoothing of the motion votes for each nominal velocity separately is an entirely sensible operation when the optic flow field is resonably smooth.
Reference: [11] <author> L.M.J. Florack, B.M. ter Haar Romeny, J.J. Koenderink, and M.A. Viergever. </author> <title> "Families of tuned scale-space kernels", </title> <editor> in ECCV'92, G. Sandini (Ed.), </editor> <publisher> Springer Verlag, </publisher> <address> Berlin 1992, p.19-23. </address>
Reference-contexts: Our method is a computationally efficient variation on the `motion energy' scheme [9], which -in turn- is mathematically equivalent to certain correlation detection schemes [10]. All of these methods incorporate some form of velocity-tuning <ref> [11] </ref>. Spatial smoothing of the motion votes for each nominal velocity separately is an entirely sensible operation when the optic flow field is resonably smooth.
Reference: [12] <author> Krose B.J.A., van der Korst M.J., Groen F.C.A. </author> <title> "Learning strategies for a vision based neural controller for a robot arm", </title> <booktitle> IEEE International Workshop on Intelligent Motor Control, </booktitle> <address> O.Kaynak, ed.,Istambul, 20-22 Aug.,1990, pp.199-203. </address>
Reference-contexts: The output of the network consists of the joint angle displacement vectors 1 and 2 . For training we use an indirect learning strategy where training samples are generated by the system during operation <ref> [12] </ref>. A stationary target is needed of which the position in the image domain is given by ~x t .
Reference: [13] <author> Smagt, P.P van der, and B.J.A. Krose. </author> <title> "A real-time learning neural robot controller", </title> <booktitle> Proceedings of the 1991 Int. Conf. on Artificial Neural Networks, Finland (1991), </booktitle> <pages> 351-356. </pages>
Reference-contexts: This method results in a fast training of the network. In about 5 moves to the target a precision of approximately 3 cm in the cartesian domain is achieved <ref> [13] </ref>. 4 Experimental Results To achieve accurate tracking, we need to process about 10 image/sec. This implies doing about 20 million convolutions/sec and about 5 million non-linearities/sec (for velocity-binning).
References-found: 13

