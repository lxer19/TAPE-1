URL: http://www.cs.jhu.edu/~murthy/cluster.ps.Z
Refering-URL: http://www.cs.jhu.edu/~murthy/
Root-URL: 
Title: Statistical Preprocessing for Decision Tree Induction  
Author: Sreerama K. Murthy 
Abstract: Some apparently simple numeric data sets cause significant problems for existing decision tree induction algorithms, in that no method is able to find a small, accurate tree, even though one exists. One source of this difficulty is the goodness measures used to decide whether a particular node represents a good way to split the data. This paper points out that the commonly-used goodness measures are not equipped to take into account some patterns in numeric attribute spaces, and presents a framework for capturing some such patterns into decision tree induction. As a case study, it is demonstrated empirically that supervised clustering, when used as a preprocessing step, can improve the quality of both univariate and multivariate decision trees.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: Clearly, the criterion used to measure the goodness of a split (known as the goodness measure or impurity measure) is important in determining the quality of the resulting tree <ref> [1, 4, 3] </ref>. It is worth noting that several commonly used goodness measures were originally proposed for symbolic domains. The "adoption" of these measures into numeric domains may be less than ideal [2, 8, 9], as numeric domains have their own peculiarities. <p> Now consider typical decision trees induced on these datasets by existing tree induction methods. the RCB data by OC1 [5] and for the RGC data by C4.5. C4.5 used gain ratio as the goodness measure and OC1 used gini index <ref> [1] </ref>. This figure shows that some otherwise successful tree induction methods have trouble in these apparently simple domains. The source of this difficulty is that the only information available to the goodness measures used is the distribution of object classes across the splits. <p> Trees induced in the second experiment were postprocessed by restoring the class labels of examples and, in a bottom-up traversal, removing the nodes that were splitting examples from the same category. The decision tree induction programs used are C4.5 [7], CART <ref> [1] </ref>, and OC1 [5]. Both the univariate and multivariate versions of CART and OC1 were used, unless the correct bias for a data set was known. C4.5 used gain ratio as the goodness measure, and CART and OC1 used the twoing rule. <p> Both the univariate and multivariate versions of CART and OC1 were used, unless the correct bias for a data set was known. C4.5 used gain ratio as the goodness measure, and CART and OC1 used the twoing rule. I implemented a version of multivariate CART based on <ref> [1] </ref>, with no backward feature elimination. Table 1 summarizes the results of both the experiments, giving classification accuracies and tree sizes (#leaves) with and without the use of pre-processing. Each entry lists the mean and standard deviation of ten 5-fold cross-validation experiments.
Reference: [2] <author> U. M. Fayyad and K. B. Irani. </author> <title> On the handling of continuous-valued attributes in decision tree generation. </title> <journal> Machine Learning, </journal> <volume> 8(2) </volume> <pages> 87-102, </pages> <year> 1992. </year>
Reference-contexts: It is worth noting that several commonly used goodness measures were originally proposed for symbolic domains. The "adoption" of these measures into numeric domains may be less than ideal <ref> [2, 8, 9] </ref>, as numeric domains have their own peculiarities. This paper argues that existing goodness measures are indeed fl Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218. murthy@cs.jhu.edu 1 inadequate for some numeric domains, and suggests statistical preprocessing as an effective solution.
Reference: [3] <author> W. Z. Liu and A. P. White. </author> <title> The importance of attribute selection measures in decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 25-41, </pages> <year> 1994. </year>
Reference-contexts: Clearly, the criterion used to measure the goodness of a split (known as the goodness measure or impurity measure) is important in determining the quality of the resulting tree <ref> [1, 4, 3] </ref>. It is worth noting that several commonly used goodness measures were originally proposed for symbolic domains. The "adoption" of these measures into numeric domains may be less than ideal [2, 8, 9], as numeric domains have their own peculiarities.
Reference: [4] <author> J. Mingers. </author> <title> An empirical comparison of selection measures for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 319-342, </pages> <year> 1989. </year>
Reference-contexts: Clearly, the criterion used to measure the goodness of a split (known as the goodness measure or impurity measure) is important in determining the quality of the resulting tree <ref> [1, 4, 3] </ref>. It is worth noting that several commonly used goodness measures were originally proposed for symbolic domains. The "adoption" of these measures into numeric domains may be less than ideal [2, 8, 9], as numeric domains have their own peculiarities.
Reference: [5] <author> Sreerama K. Murthy, Simon Kasif, and Steven Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 1-33, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: The CB (checker board) data set can be described perfectly by an axis-parallel decision tree with 8 leaves. The RCB (rotated checkerboard) data can be described exactly by an oblique decision tree <ref> [5] </ref> of 16 leaves. The RGC (randomly generated clusters) data consists of 20 circular clusters, and need not necessarily have a clear decision tree partitioning. But since the generation process produced clusters, trees that separate each cluster into a distinct region are clearly preferable. <p> Each of these artificial data sets has well-separated, dense, homogeneous regions of the attribute space that call out to be separated. Now consider typical decision trees induced on these datasets by existing tree induction methods. the RCB data by OC1 <ref> [5] </ref> and for the RGC data by C4.5. C4.5 used gain ratio as the goodness measure and OC1 used gini index [1]. This figure shows that some otherwise successful tree induction methods have trouble in these apparently simple domains. <p> This can be very expensive, especially for multivariate tree methods <ref> [5] </ref> that consider large numbers of candidate splits. <p> Trees induced in the second experiment were postprocessed by restoring the class labels of examples and, in a bottom-up traversal, removing the nodes that were splitting examples from the same category. The decision tree induction programs used are C4.5 [7], CART [1], and OC1 <ref> [5] </ref>. Both the univariate and multivariate versions of CART and OC1 were used, unless the correct bias for a data set was known. C4.5 used gain ratio as the goodness measure, and CART and OC1 used the twoing rule.
Reference: [6] <author> Sreerama K. Murthy and Steven Salzberg. </author> <title> Clustering astronomical objects using minimum spanning trees. </title> <type> Technical report, </type> <institution> Dept. of Computer Science, Johns Hopkins University, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: Due to space restrictions, the user is referred to <ref> [6] </ref> for a description of our clustering method. We perform supervised clustering all clusters are constrained to be homogeneous. I ran two experiments, each using three artificial data sets CB, RCB and RGC (fig. 1). The first experiment induced decision trees in the conventional way, i.e., with no preprocessing.
Reference: [7] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: Trees induced in the second experiment were postprocessed by restoring the class labels of examples and, in a bottom-up traversal, removing the nodes that were splitting examples from the same category. The decision tree induction programs used are C4.5 <ref> [7] </ref>, CART [1], and OC1 [5]. Both the univariate and multivariate versions of CART and OC1 were used, unless the correct bias for a data set was known. C4.5 used gain ratio as the goodness measure, and CART and OC1 used the twoing rule.
Reference: [8] <author> Thierry Van de Merckt. NFDT: </author> <title> A system that learns flexible concepts based on decision trees for numerical attributes. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pages 322-331, </pages> <year> 1992. </year>
Reference-contexts: It is worth noting that several commonly used goodness measures were originally proposed for symbolic domains. The "adoption" of these measures into numeric domains may be less than ideal <ref> [2, 8, 9] </ref>, as numeric domains have their own peculiarities. This paper argues that existing goodness measures are indeed fl Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218. murthy@cs.jhu.edu 1 inadequate for some numeric domains, and suggests statistical preprocessing as an effective solution.
Reference: [9] <author> Thierry Van de Merckt. </author> <title> Decision trees in numerical attribute spaces. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1016-1021, </pages> <year> 1993. </year> <month> 7 </month>
Reference-contexts: It is worth noting that several commonly used goodness measures were originally proposed for symbolic domains. The "adoption" of these measures into numeric domains may be less than ideal <ref> [2, 8, 9] </ref>, as numeric domains have their own peculiarities. This paper argues that existing goodness measures are indeed fl Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218. murthy@cs.jhu.edu 1 inadequate for some numeric domains, and suggests statistical preprocessing as an effective solution. <p> One solution to this problem is to augment the definition of the goodness measures, to somehow take into account the "structure" of the examples in addition to the class distribution. Van de Merckt <ref> [9] </ref> used this approach to define a selection criterion that combines the proximity with class entropy. Though this certainly is a step towards using structure, it leaves open some potential problems. * [9] considers only one kind of structure information, namely clusters. <p> Van de Merckt <ref> [9] </ref> used this approach to define a selection criterion that combines the proximity with class entropy. Though this certainly is a step towards using structure, it leaves open some potential problems. * [9] considers only one kind of structure information, namely clusters. It is not clear how to deal with other important kinds of structure information, for eg. empty regions in attribute space. * [9] uses unsupervised clustering. <p> Though this certainly is a step towards using structure, it leaves open some potential problems. * <ref> [9] </ref> considers only one kind of structure information, namely clusters. It is not clear how to deal with other important kinds of structure information, for eg. empty regions in attribute space. * [9] uses unsupervised clustering. This approach fails when each class is clearly multimodal, but the entire set of examples is not. (Consider, for example, variations of fig. 1 datasets with no "space" between clusters.) * As [9] incorporates structure information into the definition of the goodness measure, this information needs to <p> other important kinds of structure information, for eg. empty regions in attribute space. * <ref> [9] </ref> uses unsupervised clustering. This approach fails when each class is clearly multimodal, but the entire set of examples is not. (Consider, for example, variations of fig. 1 datasets with no "space" between clusters.) * As [9] incorporates structure information into the definition of the goodness measure, this information needs to be calculated once for every split considered. This can be very expensive, especially for multivariate tree methods [5] that consider large numbers of candidate splits. <p> Structure extraction methods of varying complexity can be used in conjunction with univariate, multivariate and/or incremental decision tree methods in this model. The complexity of the resulting system is only a sum of the complexities of the preprocessing and tree building stages, as opposed to a product as in <ref> [9] </ref>.
References-found: 9

