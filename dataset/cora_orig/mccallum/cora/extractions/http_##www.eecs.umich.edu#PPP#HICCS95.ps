URL: http://www.eecs.umich.edu/PPP/HICCS95.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Title: Modeling Load Imbalance and Fuzzy Barriers for Scalable Shared-Memory Multiprocessors  
Author: Alexandre E. Eichenberger Santosh G. Abraham 
Address: 1501 Page Mill Road Ann Arbor, MI 48109-2122 Palo Alto, CA 94304  
Affiliation: Advanced Computer Architecture Laboratory Hewlett Packard Laboratories EECS Department, University of Michigan  
Abstract: We propose an analytical model that quantifies the overall execution time of a parallel region in the presence of non-deterministic load imbalance introduced by network contention and by random replacement policy in processor caches. We present a novel model that evaluates the expected hit ratio and variance introduced by a cache accessed with a cyclic access stream. We also model the performance improvement of fuzzy barriers, where the synchronization between processors at the end of a parallel region is relaxed. Experiments on a 64-processor KSR system which has random first-level caches confirms the general nature of the analytic results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. E. Smith and R. J. Goodman. </author> <title> Instruction cache replacement policies and organization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(3):234-241, </volume> <month> March </month> <year> 1985. </year>
Reference-contexts: We present measurements that confirm the general nature of the analytic results using experimental runs on the KSR multiprocessor system which has random first-level caches in Section 7. We conclude in Section 8. 2 Related Work Smith and Goodman <ref> [1] </ref> have compared the behavior of several cache replacement policies. They provide a closed form solution for the expected hit ratio of caches with random replacement accessed by cyclic access streams exceeding the set associativity by one cache line. <p> The advantages of this technique are its simplicity and its low cost. Another advantage of random replacement, is that it does not exhibit the same performance discontinuities as LRU for workloads differing slightly in size, as observed by Smith and Goodman <ref> [1] </ref>. Schlansker et al [3] investigated large secondary caches with random replacement and concluded that they perform better than LRU, especially with high fill ratio. Cyclic access streams have been used in previous studies of caches [1][3] and are comomon in large data-parallel applications [3], where operations are repetitively executed on
Reference: [2] <author> R. Gupta. </author> <title> The fuzzy barrier: A mechanism for high speed synchronization of processors. </title> <booktitle> 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 54-63, </pages> <year> 1989. </year>
Reference-contexts: Greenbaum [10] showed that near-optimal idle time is achievable through Boundary Synchronization, a relaxed synchronization scheme where boundary computations are executed first within each thread. Gupta <ref> [2] </ref> investigated Fuzzy Barriers, where a region of statements are executed while waiting for the synchronization. Techniques that detect and increase the number of independent operations are presented in [2] [12]. <p> Gupta <ref> [2] </ref> investigated Fuzzy Barriers, where a region of statements are executed while waiting for the synchronization. Techniques that detect and increase the number of independent operations are presented in [2] [12]. These papers bounded the performance of their weak-synchronization schemes by the performance of strong-and no- synchronization respectively for the upper and the lower bound, but do not propose tighter bounds. 3 Performance of Cache with Random Replacement Random replacement policy [13] has been implemented in several caches. <p> In the presence of systemic workload imbalance, these equations allow us to quantify which percentage of the measured idle time is generated by random delays and which percentage is due to the systemic workload imbalance. 6 Performance of a Parallel Region with Fuzzy Barriers In a fuzzy barrier <ref> [2] </ref>, processors execute independent operations between arriving at a barrier and leaving a barrier. We refer to the execution time of these independent operations as the slack of a synchronization barrier.
Reference: [3] <author> M. S. Schlansker, R. Shaw, and S. Sivaramakris-han. </author> <title> Randomization and associativity in the design of placement-insensitive caches. </title> <type> Technical Report HPL-93-41, </type> <institution> HP Laboratories, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: They provide a closed form solution for the expected hit ratio of caches with random replacement accessed by cyclic access streams exceeding the set associativity by one cache line. Recently, Schlansker et al <ref> [3] </ref> have investigated the design of placement-insensitive caches and studied the random replacement policy. Both studies recommend this replacement policy, but focus on the average random replacement performance and not on its variance. The source of execution time performance variance has been studied. <p> The advantages of this technique are its simplicity and its low cost. Another advantage of random replacement, is that it does not exhibit the same performance discontinuities as LRU for workloads differing slightly in size, as observed by Smith and Goodman [1]. Schlansker et al <ref> [3] </ref> investigated large secondary caches with random replacement and concluded that they perform better than LRU, especially with high fill ratio. Cyclic access streams have been used in previous studies of caches [1][3] and are comomon in large data-parallel applications [3], where operations are repetitively executed on large data structures. <p> Schlansker et al <ref> [3] </ref> investigated large secondary caches with random replacement and concluded that they perform better than LRU, especially with high fill ratio. Cyclic access streams have been used in previous studies of caches [1][3] and are comomon in large data-parallel applications [3], where operations are repetitively executed on large data structures. Cyclic access streams can be viewed as a worst case of temporal sequences and illustrates, as such, an important aspect of cache performance breakdown.
Reference: [4] <author> V. S. Adve and M. K. Vernon. </author> <title> The influence of random delays on parallel execution times. </title> <booktitle> ACM SIG-METRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 61-73, </pages> <year> 1993. </year>
Reference-contexts: Both studies recommend this replacement policy, but focus on the average random replacement performance and not on its variance. The source of execution time performance variance has been studied. Adve and Vernon <ref> [4] </ref> quantified the fluctuation of parallel execution times due to random delays and non-deterministic processing requirements. Dubois and Briggs [5] investigated the performance fluctuation generated by memory contentions and obtained an analytical formula describing the expected number of cycles and its variance for memory references in tightly coupled systems. <p> Using the properties of the Central Limit theorem, 2 we assume that for a sufficient number of memory references, the cache performance is normal distributed. A similar assumption was used in <ref> [4] </ref> [10] [5] , and our measurements on the KSR1 corroborate this assumption.
Reference: [5] <author> M. Dubois and F. A. Briggs. </author> <title> Performance of synchronized iterative processes in multiprocessor systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-8(4):419-431, </volume> <month> July </month> <year> 1982. </year>
Reference-contexts: The source of execution time performance variance has been studied. Adve and Vernon [4] quantified the fluctuation of parallel execution times due to random delays and non-deterministic processing requirements. Dubois and Briggs <ref> [5] </ref> investigated the performance fluctuation generated by memory contentions and obtained an analytical formula describing the expected number of cycles and its variance for memory references in tightly coupled systems. <p> Using the properties of the Central Limit theorem, 2 we assume that for a sufficient number of memory references, the cache performance is normal distributed. A similar assumption was used in [4] [10] <ref> [5] </ref> , and our measurements on the KSR1 corroborate this assumption.
Reference: [6] <author> V. Sarkar. </author> <title> Determining average program execution times and their variance. </title> <booktitle> Proceedings of the ACM SIGPLAN'89 Conference on Programming Language Design and Implementation, </booktitle> <volume> 24(7) </volume> <pages> 298-312, </pages> <year> 1989. </year>
Reference-contexts: Dubois and Briggs [5] investigated the performance fluctuation generated by memory contentions and obtained an analytical formula describing the expected number of cycles and its variance for memory references in tightly coupled systems. Sarkar <ref> [6] </ref> provided a framework to estimate the execution time and its variance based on the program's internal structure and control dependence graph. The effects of load imbalance are investigated in several articles. Kruskal and Weiss [7] investigated the total execution time required to complete k tasks for various distributions.
Reference: [7] <author> C. P. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(10):1001-1016, </volume> <month> Oc-tober </month> <year> 1985. </year>
Reference-contexts: Sarkar [6] provided a framework to estimate the execution time and its variance based on the program's internal structure and control dependence graph. The effects of load imbalance are investigated in several articles. Kruskal and Weiss <ref> [7] </ref> investigated the total execution time required to complete k tasks for various distributions. Madala and Sinclair [8] quantified the performance of parallel algorithms with regular structures and varying task execution times. Du-rand et al studied the impact of memory contention on NUMA parallel machines and provided experimental measurements [9]. <p> This factor is consistent with the estimated total time derived by Kruskal and Weiss <ref> [7] </ref>. Equation (9) specifically corresponds to the time spent idling in the synchronization barrier due to random replacement and network contention.
Reference: [8] <author> S. Madala and J. B. Sinclair. </author> <title> Performance of synchronous parallel algorithms with regular structure. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(1) </volume> <pages> 105-116, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The effects of load imbalance are investigated in several articles. Kruskal and Weiss [7] investigated the total execution time required to complete k tasks for various distributions. Madala and Sinclair <ref> [8] </ref> quantified the performance of parallel algorithms with regular structures and varying task execution times. Du-rand et al studied the impact of memory contention on NUMA parallel machines and provided experimental measurements [9].
Reference: [9] <author> M. D. Durand, T. Montaut, L. Kervella, and W. Jalby. </author> <title> Impact of memory contention on dynamic scheduling on numa multiprocessors. </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> 1 </volume> <pages> 258-267, </pages> <year> 1993. </year>
Reference-contexts: Madala and Sinclair [8] quantified the performance of parallel algorithms with regular structures and varying task execution times. Du-rand et al studied the impact of memory contention on NUMA parallel machines and provided experimental measurements <ref> [9] </ref>. Simulation results show that the idle time generated by partial synchronization barriers [10] [11] 263 is significantly lower than that generated by nor-mal synchronization barriers.
Reference: [10] <author> A. Greenbaum. </author> <title> Synchronization costs on multiprocessors. Parallel Computing, </title> <publisher> North-Holland, </publisher> <pages> 10 3-14, </pages> <year> 1989. </year>
Reference-contexts: Madala and Sinclair [8] quantified the performance of parallel algorithms with regular structures and varying task execution times. Du-rand et al studied the impact of memory contention on NUMA parallel machines and provided experimental measurements [9]. Simulation results show that the idle time generated by partial synchronization barriers <ref> [10] </ref> [11] 263 is significantly lower than that generated by nor-mal synchronization barriers. Greenbaum [10] showed that near-optimal idle time is achievable through Boundary Synchronization, a relaxed synchronization scheme where boundary computations are executed first within each thread. <p> Du-rand et al studied the impact of memory contention on NUMA parallel machines and provided experimental measurements [9]. Simulation results show that the idle time generated by partial synchronization barriers <ref> [10] </ref> [11] 263 is significantly lower than that generated by nor-mal synchronization barriers. Greenbaum [10] showed that near-optimal idle time is achievable through Boundary Synchronization, a relaxed synchronization scheme where boundary computations are executed first within each thread. Gupta [2] investigated Fuzzy Barriers, where a region of statements are executed while waiting for the synchronization. <p> Using the properties of the Central Limit theorem, 2 we assume that for a sufficient number of memory references, the cache performance is normal distributed. A similar assumption was used in [4] <ref> [10] </ref> [5] , and our measurements on the KSR1 corroborate this assumption. <p> Otherwise, the idle time behaves more like Equation (9). To the best of our knowledge, this result is the first that quantifies the performance of a weak-synchronization scheme. Previous articles <ref> [10] </ref> [11] bounded the performance of week-synchronization using the performance of strong- and no- synchronization for the upper and the lower bounds respectively. 7 Measurements We performed several experiments to validate our models.
Reference: [11] <author> C. S. Chang and R. Nelson. </author> <title> Bounds on the speedup and efficiency of partial synchronization in parallel processing systems. </title> <type> Research Report RC 16474, </type> <institution> IBM, </institution> <year> 1991. </year>
Reference-contexts: Madala and Sinclair [8] quantified the performance of parallel algorithms with regular structures and varying task execution times. Du-rand et al studied the impact of memory contention on NUMA parallel machines and provided experimental measurements [9]. Simulation results show that the idle time generated by partial synchronization barriers [10] <ref> [11] </ref> 263 is significantly lower than that generated by nor-mal synchronization barriers. Greenbaum [10] showed that near-optimal idle time is achievable through Boundary Synchronization, a relaxed synchronization scheme where boundary computations are executed first within each thread. <p> Otherwise, the idle time behaves more like Equation (9). To the best of our knowledge, this result is the first that quantifies the performance of a weak-synchronization scheme. Previous articles [10] <ref> [11] </ref> bounded the performance of week-synchronization using the performance of strong- and no- synchronization for the upper and the lower bounds respectively. 7 Measurements We performed several experiments to validate our models.
Reference: [12] <author> R. Gupta. </author> <title> Loop displacement: An approach for transforming and scheduling loops for parallel execution. </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <pages> pages 388-397, </pages> <year> 1990. </year>
Reference-contexts: Gupta [2] investigated Fuzzy Barriers, where a region of statements are executed while waiting for the synchronization. Techniques that detect and increase the number of independent operations are presented in [2] <ref> [12] </ref>. These papers bounded the performance of their weak-synchronization schemes by the performance of strong-and no- synchronization respectively for the upper and the lower bound, but do not propose tighter bounds. 3 Performance of Cache with Random Replacement Random replacement policy [13] has been implemented in several caches.
Reference: [13] <author> R. L. Mattson, J. Gecsei, D. R. Slutz, , and I. L. Traiger. </author> <title> Evaluation techniques for storage hierarchies. </title> <journal> IBM Systems Journal, </journal> <volume> 2 </volume> <pages> 78-117, </pages> <year> 1970. </year>
Reference-contexts: These papers bounded the performance of their weak-synchronization schemes by the performance of strong-and no- synchronization respectively for the upper and the lower bound, but do not propose tighter bounds. 3 Performance of Cache with Random Replacement Random replacement policy <ref> [13] </ref> has been implemented in several caches. The advantages of this technique are its simplicity and its low cost. Another advantage of random replacement, is that it does not exhibit the same performance discontinuities as LRU for workloads differing slightly in size, as observed by Smith and Goodman [1].
Reference: [14] <author> E. L. Boyd, J.-D. Wellman, S. G. Abraham, and E. S. Davidson. </author> <title> Evaluating the communication performance of mpps using synthetic sparse matrix multiplication workloads. </title> <booktitle> Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 240-250, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: For example, an algorithm using near-neighbor communication generates communication proportional to p p d=p when partitioned along one and two dimensions respectively, where d is the dimension size. Other architectural and hardware factors such as network topology and automatic update <ref> [14] </ref> also have a secondary effect on E vents . 4.1 Performance Distribution Assuming that the delays are independent identically normally distributed random variables X 1 ; ; X E vents with parameters event and 2 event : E vents number of communication events for one processor within a parallel region <p> On the KSR1, we estimated the standard deviation of a single communication event ( event ) by running our program for 56 processors and obtained 17s <ref> [14] </ref>. We notice that the idle time indeed varies as described in Equation (9).
Reference: [15] <author> H.A. David. </author> <title> Order Statistics. </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1981. </year>
Reference-contexts: The results of this equation correspond to the expected execution time of a single processor and its variance. The overall performance of this parallel region for p processors correspond to the expected maximum max (X 1 ; ; X p ). Based on the asymptotic extremal distribution <ref> [15] </ref>, the extreme value can be asymptotically approximated for independent identically distributed normal distribution with parameters p and p as follows [15]: p = M em t 2 hit + E vents 2 event E [idle time] ' p 2 log p 2 2 log p (9) The first factor of <p> Based on the asymptotic extremal distribution <ref> [15] </ref>, the extreme value can be asymptotically approximated for independent identically distributed normal distribution with parameters p and p as follows [15]: p = M em t 2 hit + E vents 2 event E [idle time] ' p 2 log p 2 2 log p (9) The first factor of Equation (9), p , is highly machine and parallel region dependent.
Reference: [16] <author> H. T. Kung. </author> <title> Synchronized and asynchronous parallel algorithms for multiprocessors. </title> <editor> In J.F. Traub, editor, </editor> <booktitle> Algorithms and Complexity: New Directions and Recent Results, </booktitle> <pages> pages 153-200. </pages> <address> New York: </address> <publisher> Academic, </publisher> <year> 1976. </year>
Reference-contexts: We can gain an intuitive understanding of the benefit of fuzzy barriers by studying a two-processor system with discrete Bernoulli delays. A similar two-processor model was mentioned by Kung <ref> [16] </ref> in the 267 context of Semi-Synchronized Iterative Algorithm but left unsolved. Consider the execution time of a parallel region, delayed by one discrete delay with probability 1=2 and bounded by a slack equivalent to t discrete delays.
Reference: [17] <author> D. Windheiser et al. </author> <title> KSR1 multiprocessor: Analysis of latency hiding techniques in a sparse solver. </title> <booktitle> Proceedings of the International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1993. </year> <month> 271 </month>
Reference-contexts: These programs were useful to demonstrate the presence of random delays and to validate our models. In this section, we investigate the behavior of a complete parallel application, to demonstrate the usefulness of our model for random delays and fuzzy barriers. We analyze the behavior of FEM-ATS <ref> [17] </ref>, an application which solves a system of complex linear equations iteratively using a diagonal-preconditioned symmetric gradient method. This application presents a highly irregular memory and communication pattern due to a sparse matrix multiplication and is therefore a good example of a complex application.
References-found: 17

