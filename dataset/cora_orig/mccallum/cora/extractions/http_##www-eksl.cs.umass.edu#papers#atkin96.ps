URL: http://www-eksl.cs.umass.edu/papers/atkin96.ps
Refering-URL: http://eksl-www.cs.umass.edu/publications.html
Root-URL: 
Email: fatkin,coheng@cs.umass.edu  
Phone: (413) 545 3616  
Title: Monitoring Strategies for Embedded Agents: Experiments and Analysis  
Author: Marc S. Atkin and Paul R. Cohen 
Address: LGRC, Box 34610  Amherst, MA 01003  
Affiliation: Experimental Knowledge Systems Laboratory Department of Computer Science,  University of Massachusetts,  
Abstract: Monitoring is an important activity for any embedded agent. To operate effectively, agents must gather information about their environment. The policy by which they do this is called a monitoring strategy. Our work has focussed on classifying different types of monitoring strategies, and understanding how strategies depend on features of the task and environment. We have discovered only a few general monitoring strategies, in particular periodic and interval reduction, and speculate that there are no more. The relative advantages and generality of each strategy will be discussed in detail. The wide applicability of interval reduction will be demonstrated both empirically and analytically. We conclude with number of general laws that state when a strategy is most appropriate. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E., & Chapman, D., </author> <year> 1987. </year> <title> Pengi: An implementation of a Theory of Activity. </title> <booktitle> Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> 268-272. </pages>
Reference-contexts: Another approach is to model the rate of change in the environment, and to use this as the basis for the monitoring period. This is the design philosophy behind reactive systems <ref> (e.g., Agre & Chapman, 1987) </ref>, which are supposed to monitor the environment fast enough to be able to act appropriately in a timely fashion. Monitoring at a high rate for all environmental conditions might be an easy solution, but is certainly not optimal in terms of monitoring costs.
Reference: <author> Agre, P. E., & Chapman, D., </author> <year> 1990. </year> <title> What are plans for? In: </title> <editor> P. Maes (Ed.), </editor> <title> Designing Autonomous Agents: Theory and Practice from Biology to Engineering and Back. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Ambros-Ingerson, J. A., and Steel, S., </author> <year> 1988. </year> <title> Integrating Planning, Execution, and Monitoring, </title> <booktitle> Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> 83-88. </pages>
Reference: <author> Atkin, M. S., </author> <year> 1991. </year> <title> Research Summary: Using a Genetic Algorithm to Monitor Cupcakes, </title> <type> EKSL Memo #24, </type> <institution> Experimental Knowledge Systems Laboratory, University of Mas-sachusetts, Amherst. </institution>
Reference: <author> Atkin, M. & Cohen, P. R., </author> <year> 1993. </year> <title> Genetic Programming to Learn an Agent's Monitoring Strategy, </title> <booktitle> Proceedings of the AAAI 93 Workshop on Learning Action Models, </booktitle> <pages> 36-41. </pages>
Reference: <author> Atkin, M. S, </author> <year> 1993. </year> <title> Using Genetic Algorithms to Learn Monitoring Strategies. </title> <note> Unpublished. </note>
Reference: <author> Atkin, M. S. & Cohen, P. R., </author> <year> 1994. </year> <title> Learning Monitoring Strategies: A Difficult Genetic Programming Application. </title> <booktitle> Proceedings on the First IEEE Conference on Evolutionary Computation, </booktitle> <address> 328-332a. </address>
Reference-contexts: We now believe that genetic programming may in general have problems scaling up to large search spaces <ref> (Atkin & Cohen, 1994) </ref>. The likely cause of our problems is a phenomenon known as premature convergence. <p> This section will attempt to demonstrate interval reduction's general applicability by presenting further corroborating evidence. We conducted an extensive experiment involving a wide range of agents and agent environments <ref> (Cohen, Atkin & Hansen, 1994) </ref>, including adult humans. There were five types of agents overall. Two were evolved by the MON and LTB systems we saw earlier. The other three will be described in the following sections.
Reference: <author> Bajscy, R., </author> <year> 1988. </year> <title> Active Perception. </title> <booktitle> Proceedings of the IEEE 76 (8), </booktitle> <pages> 996-1005. </pages>
Reference: <author> Carbonell, J. R., </author> <year> 1966. </year> <title> A queuing model model for many-instrument visual sampling. </title> <booktitle> IEEE Transactions on Human Factors in electronics, HFE-7, </booktitle> <pages> 157-164. </pages>
Reference: <author> Carbonell, J. R, Ward, J. L., and Senders, J. W., </author> <year> 1968. </year> <title> A Queueing Model of Visual Sampling; Experimental Validation. </title> <journal> IEEE Transactions on Man-Machine Systems, MMS-9, </journal> <volume> 3, </volume> <pages> 82-87. </pages>
Reference: <author> Ceci, S. J. & Bronfenbrenner, U., </author> <year> 1985. </year> <title> "Don't forget to take the cupcakes out of the oven": Prospective memory, strategic time-monitoring, and context. Child Development, </title> <booktitle> 56, </booktitle> <pages> 152-164. </pages>
Reference: <author> Chrisman, L. & Simmons, R., </author> <year> 1991. </year> <title> Sensible Planning: Focusing Perceptual Attention. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 756-761. </pages>
Reference-contexts: However, the issues addressed in these systems revolve mainly around how to combine monitoring with planning, and not on what the resulting strategies look like. Some systems already assume the agent has a basic sensing policy <ref> (e.g., Chrisman & Simmons, 1991) </ref>. Monitoring costs are also not usually taken into account. Typically, in AI planning systems, monitoring plays the role of verifying plan execution. This involves checking the unfolding execution of a plan, so as to replan if something goes wrong, or adjust the plan opportunistically.
Reference: <author> Cohen, P. R., Howe, A. E., & Hart, D. M., </author> <year> 1990. </year> <title> Intelligent Real-time Problem Solving: Issues and Examples. </title> <institution> Computer Science Technical Report 90-20. University of Massachusetts, Amherst. </institution>
Reference: <author> Cohen, P. R., </author> <year> 1990. </year> <title> Modeling How Interactions Between Agents' Architectures and Environments Produce Behaviors, for the Purpose of Design and Analysis. </title> <booktitle> Proceedings of the AAAI Workshop on Planning. </booktitle>
Reference: <author> Cohen, P. R., Atkin, M. S., and Hansen, E. A., </author> <year> 1994. </year> <title> The Interval Reduction Strategy for Monitoring Cupcake problems, </title> <booktitle> Proceedings of the Third International Conference on the Simulation of Adaptive Behavior, </booktitle> <pages> 82-90. </pages>
Reference-contexts: We now believe that genetic programming may in general have problems scaling up to large search spaces <ref> (Atkin & Cohen, 1994) </ref>. The likely cause of our problems is a phenomenon known as premature convergence. <p> This section will attempt to demonstrate interval reduction's general applicability by presenting further corroborating evidence. We conducted an extensive experiment involving a wide range of agents and agent environments <ref> (Cohen, Atkin & Hansen, 1994) </ref>, including adult humans. There were five types of agents overall. Two were evolved by the MON and LTB systems we saw earlier. The other three will be described in the following sections.
Reference: <author> Dean, T., </author> <year> 1987. </year> <title> Planning, Execution, and Control. </title> <booktitle> Proceedings of the DARPA Knowledge-Based Planning Workshop, </booktitle> <address> 29-1-29-10. </address>
Reference: <author> Doyle, R., Atkinson, D., & Doshi, R., </author> <year> 1986. </year> <title> Generating Perception Requests and Expectations to Verify the Execution of Plans. </title> <booktitle> Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> 81-88. </pages>
Reference: <author> Ferster, C. B. and Skinner, B. F., </author> <year> 1957. </year> <title> Schedules of Reinforcement. </title> <address> Appleton-Century-Crofts, New York, NY. </address>
Reference: <author> Fikes, R., Hart, P., & Nilsson, N., </author> <year> 1972. </year> <title> Learning and Executing Generalized Robot Plans. </title> <booktitle> Artificial Intelligence 3 (4), </booktitle> <pages> 251-288. </pages> <note> Reprinted in: </note> <editor> Allen, J. F., Hendler, J., and Tate, A. (Eds.), </editor> <booktitle> Readings in Planning, </booktitle> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA. </address> <note> 39 Firby, </note> <author> R. J., </author> <year> 1987. </year> <title> An Investigation into Reactive Planning in Complex Domains, </title> <booktitle> Proceed--ings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> 202-206. </pages>
Reference-contexts: This involves checking the unfolding execution of a plan, so as to replan if something goes wrong, or adjust the plan opportunistically. One of the first systems that used monitoring for both these purposes was the autonomous mobile robot "Shakey" (Raphael, 1976), in particular its execution monitoring component "PLANEX" <ref> (Fikes, Hart & Nilsson, 1972) </ref>.
Reference: <author> Georgeff, M. P., & Lansky, A. L., </author> <year> 1987. </year> <title> Reactive Reasoning and Planning, </title> <booktitle> Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> 677-682. </pages>
Reference: <author> Goldberg, D. E., </author> <year> 1989. </year> <title> Genetic Algorithms in Search, Optimization & Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: New individuals are formed either by changing a part of the individual randomly (the so-called mutation operation) or by swapping a section of one individual with a section from another (the crossing-over operation) <ref> (Goldberg, 1989) </ref>. In our work, individuals are programs that control the robot, and it is these programs that the genetic algorithm is trying to improve. In the previous section, we saw what these programs look like and how they represent monitoring strategies.
Reference: <author> Goldberg, D. E. & Kalyanmoy, D., </author> <year> 1991. </year> <title> A Comparative Analysis of Selection Schemes Used in Genetic Algorithms, in Foundations of Genetic Algorithms (Gregory J.E. </title> <editor> Rawlins ed.). </editor> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: In the reproduction phase, individuals are selected based on their fitness values. As the selection scheme, we chose tournament selection with a tournament size of two over roulette wheel selection, because of it has been shown to be less likely to cause premature convergence <ref> (Goldberg & Kalyanmoy, 1991) </ref>. Tournament selection selects individuals based on their rank in the population. Fitter individuals have higher chances of being selected, and as the best in the population can expect to be selected several times, they multiply.
Reference: <author> Hansen, E. A., </author> <year> 1992a. </year> <note> Note on monitoring cupcakes. EKSL Memo #22. </note> <institution> Experimental Knowledge Systems Laboratory, Computer Science Dept., University of Massachusetts, Amherst. </institution>
Reference: <author> Hansen, E. A., </author> <year> 1992b. </year> <title> Learning A Decision Rule for Monitoring Tasks with Deadlines. </title> <type> CMPSCI Technical Report 92-80. </type> <institution> University of Massachusetts, Amherst. </institution>
Reference-contexts: Second, these features can be dynamic, changing over the course of a trial. Third, and perhaps most importantly, monitoring actions are not necessarily independent. The optimal placement of a monitoring action cannot always be computed without knowing the expected placement of all successive ones <ref> (Hansen, 1992b) </ref>. Monitoring, like any other behavior, depends on the interaction of three factors: the agent's architecture, task, and environment.
Reference: <author> Hansen, E. A., </author> <year> 1994. </year> <title> Cost-Effective Sensing During Plan Execution. </title> <booktitle> Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 1029-1035. </pages>
Reference-contexts: This section will attempt to demonstrate interval reduction's general applicability by presenting further corroborating evidence. We conducted an extensive experiment involving a wide range of agents and agent environments <ref> (Cohen, Atkin & Hansen, 1994) </ref>, including adult humans. There were five types of agents overall. Two were evolved by the MON and LTB systems we saw earlier. The other three will be described in the following sections. <p> Hansen (1992b) has shown that the curse of dimensionality is ameliorated if utility is assumed to be a monotonic function of monitoring interval, and 18 he also suggests finding an acceptable but coarse-grained time interval. This work has also been extended to monitoring plan execution <ref> (Hansen, 1994) </ref>. Using these algorithms, the optimal strategy for a given cupcake problem has been shown to be an interval reduction strategy.
Reference: <author> Hendler, J., & Sanborn, J., </author> <year> 1987. </year> <title> A Model of Reaction for Planning in Dynamic Environments. </title> <booktitle> Proceedings of the DARPA Knowledge-Based Planning Workshop, </booktitle> <address> 24-1-24-10. </address>
Reference: <author> Kaebling, L., </author> <year> 1987, </year> <title> An Architecture for Intelligent Reactive Systems. </title> <editor> In Georgeff and Lansky (Eds.), </editor> <title> Reasoning About Actions and Plans. </title> <note> Reprinted in: </note> <editor> Allen, J. F., Hendler, J., and Tate, A. (Eds.), </editor> <booktitle> Readings in Planning. </booktitle>
Reference: <author> Koza, J. R., </author> <year> 1992. </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection and Genetics. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In order to increase their utility, more 7 Foraging strategies comparable with those found in nature have even been evolved by genetic programming <ref> (Koza, Rice & Roughgarden, 1992) </ref>. 37 work needs to be done on describing the relationship between features of the scenario and parameters of the strategy, instead of just stating which strategy to use.
Reference: <author> Koza, J. R. & Rice, J. P., </author> <year> 1992. </year> <title> Automatic Programming of Robots using Genetic Programming. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 194-207. </pages>
Reference-contexts: In order to increase their utility, more 7 Foraging strategies comparable with those found in nature have even been evolved by genetic programming <ref> (Koza, Rice & Roughgarden, 1992) </ref>. 37 work needs to be done on describing the relationship between features of the scenario and parameters of the strategy, instead of just stating which strategy to use.
Reference: <author> Koza, J. R., Rice, J. P., Roughgarden, J., </author> <year> 1992. </year> <title> Evolution of Food-Foraging Strategies for the Carribean Anolis Lizard Using Genetic Programming. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 1, </volume> <pages> 171-199. </pages>
Reference-contexts: In order to increase their utility, more 7 Foraging strategies comparable with those found in nature have even been evolved by genetic programming <ref> (Koza, Rice & Roughgarden, 1992) </ref>. 37 work needs to be done on describing the relationship between features of the scenario and parameters of the strategy, instead of just stating which strategy to use.
Reference: <author> Kvalseth, T., </author> <year> 1977. </year> <title> The effect of cost on the sampling behavior of human instrument monitors. </title> <editor> In Sheridan, T. B., and Johannsen, G. (eds.), </editor> <title> Monitoring behavior and supervisory control, </title> <publisher> Plenum, </publisher> <address> New York, NY. </address>
Reference: <author> Latombe, J.-C., Lazanas, A., and Shekhar, S., </author> <year> 1991. </year> <title> Robot Motion Planning with Uncertainty in Control and Sensing. </title> <journal> Artificial Intelligence, </journal> <volume> 52 (1), </volume> <pages> 1-47. </pages>
Reference-contexts: One can estimate the amount of uncertainty introduced by each effector command that is executed, and monitor again when this uncertainty exceeds a threshold. This mechanism was implemented in the original Shakey robot (Raphael, 1976) and other robotic systems <ref> (Latombe, Lazanas & Shekhar, 1991) </ref>. Another approach is to model the rate of change in the environment, and to use this as the basis for the monitoring period.
Reference: <author> Lowe, C. F., Harzem, P., and Bagshaw, M., </author> <year> 1978. </year> <title> Species Differences in Temporal Control of Behavior II: Human Performance. </title> <journal> Journal of the Experimental Analysis of Behavior, </journal> <volume> 29 (3), </volume> <pages> 351-361. </pages>
Reference-contexts: This is not, however, what is observed in response curves for the FI schedule. Typically, subjects start responding at a relatively high rate immediately after the first response. In fact, there was an extensive debate as to whether the response curves were actually scalloped <ref> (Lowe, Harzem & Bagshaw, 1978) </ref>. Some believed that a period of no response, followed by a period of a high rate of response (break-and-run behavior) was a more accurate characterization (e.g., Schneider, 1969).
Reference: <author> Lowe, C. F., Harzem, P., and Spencer, P. T., </author> <year> 1979. </year> <title> Temporal Control of Behavior and the Power Law. </title> <journal> Journal of the Experimental Analysis of Behavior, </journal> <volume> 31 (3), </volume> <pages> 333-343. </pages>
Reference: <author> Lowe, C. F., Beasty, A., and Bentall, R. P., </author> <year> 1983. </year> <title> The Role of Verbal Behavior in Human Learning: Infant performance on Fixed-Interval Schedules. </title> <journal> Journal of the Experimental Analysis of Behavior, </journal> <volume> 39 (1), </volume> <pages> 157-164. </pages>
Reference: <author> Mackintosh, N. J., </author> <year> 1974. </year> <title> The Psychology of Animal Learning. </title> <publisher> Academic Press, London. </publisher>
Reference: <author> McDermott, D., </author> <year> 1978. </year> <title> Planning and Acting. </title> <journal> Cognitive Science, </journal> <volume> 2 (2), </volume> <pages> 71-109. </pages> <note> Reprinted in: </note> <editor> Allen, J. F., Hendler, J., and Tate, A. (Eds.), </editor> <booktitle> Readings in Planning, </booktitle> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA. </address>
Reference: <author> McDermott, D., </author> <year> 1992. </year> <title> Robot Planning. </title> <journal> AI Magazine, </journal> <volume> Volume 13, No. </volume> <month> 2 (Summer </month> <year> 1992), </year> <pages> 55-79. </pages>
Reference-contexts: the planner is responsible for generating an appropriate monitoring strategy to go with its plan; there is no point in having a general purpose theory that finds a monitoring strategy for an arbitrary plan, since the generator of the monitoring strategy 35 requires the same reasoning capabilities as the planner <ref> (McDermott, 1992) </ref>. This does not contradict the notion of a monitoring strategy taxonomy, since the taxonomy incorporates the task of the agent, as well. As we have seen, LTB simultaneously learned the "plan" for solving the task and the monitoring strategy it employed.
Reference: <author> Moray, N., </author> <year> 1986. </year> <title> Monitoring Behavior and Supervisory Control. In Boff, </title> <publisher> Kaufman, </publisher> <editor> and Thomas (eds.), </editor> <booktitle> Handbook of Perception and Human Performance, Vol. II, chapter 40. </booktitle> <publisher> Wiley, </publisher> <address> New York, NY. </address>
Reference-contexts: Human visual sampling behavior has been studied extensively in the areas of process control|in which a human supervises a potentially unstable process such as a nuclear power plant|and pilot's and air traffic controller's instrument viewing patterns <ref> (see Moray, 1986, and Senders, 1983, for reviews) </ref>. It should be emphasized that the aim of these studies is to model and predict human performance on monitoring tasks, not to determine the theoretically optimal strategy in any given situation. Many models of human monitoring behavior have been proposed.
Reference: <author> Pyke, G. H., Pulliam, H. R., and Charnov, E. L., </author> <year> 1977. </year> <title> Optimal foraging: a selective review of theory and tests. Q. </title> <journal> Rev. Biol., </journal> <volume> 52, </volume> <pages> 137-154. </pages>
Reference: <author> Pyke, G. H., </author> <year> 1984: </year> <title> Optimal foraging: A critical review. </title> <journal> Annual Review of Ecology and Systematics, </journal> <volume> 15, </volume> <pages> 523-575. </pages>
Reference: <author> Raphael, B., </author> <year> 1976. </year> <title> The Thinking Computer: Mind Inside Matter. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> San Francisco., </address> <pages> 153-159, 169, 275-281, 287-288. </pages>
Reference-contexts: This involves checking the unfolding execution of a plan, so as to replan if something goes wrong, or adjust the plan opportunistically. One of the first systems that used monitoring for both these purposes was the autonomous mobile robot "Shakey" <ref> (Raphael, 1976) </ref>, in particular its execution monitoring component "PLANEX" (Fikes, Hart & Nilsson, 1972). <p> Some methods reminiscent of Senders' sampling models have been used in AI planning. One can estimate the amount of uncertainty introduced by each effector command that is executed, and monitor again when this uncertainty exceeds a threshold. This mechanism was implemented in the original Shakey robot <ref> (Raphael, 1976) </ref> and other robotic systems (Latombe, Lazanas & Shekhar, 1991). Another approach is to model the rate of change in the environment, and to use this as the basis for the monitoring period.
Reference: <author> Schneider, B. A., </author> <year> 1969. </year> <title> A Two-State Analysis of Fixed-Interval Responding in the Pigeon. </title> <journal> Journal of the Experimental Analysis of Behavior, </journal> <volume> 12 (5), </volume> <pages> 677-687. </pages>
Reference-contexts: In fact, there was an extensive debate as to whether the response curves were actually scalloped (Lowe, Harzem & Bagshaw, 1978). Some believed that a period of no response, followed by a period of a high rate of response (break-and-run behavior) was a more accurate characterization <ref> (e.g., Schneider, 1969) </ref>. The break-and-run strategy could be described as waiting until you think you are fairly close to the deadline, and then monitoring periodically. It seems like an intuitive strategy given an inaccurate internal clock and only binary information. <p> In a fixed interval schedule, subjects do not usually respond for a certain time since the last reinforcement. How does this pause depend on the interval length T ? Initial studies assumed a linear relationship <ref> (e.g., Schneider, 1969) </ref>, but more recently, the evidence appears to indicate a sub-linear relationship of the form p = cT x , where x is a constant smaller than one, c is a small positive number (usually ranging from about 0.5 to 2.5), and p is the pause duration (e.g., Lowe,
Reference: <author> Schoppers, M., </author> <year> 1987. </year> <title> Universal Plans for Reactive Robots in Unpredictable Domains. </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1039-1046. </pages>
Reference: <author> Schwartz, B., </author> <year> 1984. </year> <title> Psychology and Learning Behavior, 2nd edition. </title> <editor> W. W. </editor> <publisher> Norton & Company, </publisher> <address> New York, NY. </address>
Reference: <author> Senders, J. W., Elkind, J. I, Grignetti, M. C., and Smallwood, R., </author> <year> 1966. </year> <title> An Investigation of the Visual Sampling Behavior of Human Observers. </title> <institution> NASA CR-434, National Aeronautics and Space Administration, </institution> <address> Washington, DC. </address>
Reference: <author> Senders, J. W., </author> <year> 1983. </year> <title> Visual Scanning Processes. </title> <publisher> University of Tilburg Press. </publisher>
Reference: <author> Sheridan, T. B., </author> <year> 1970. </year> <title> How often the supervisor should sample. </title> <journal> IEEE Transactions on Systems, Science, and Cybernetics, SSC-6, </journal> <volume> 2, </volume> <pages> 140-145. </pages>
Reference: <author> Simmons, R. G., </author> <year> 1990. </year> <title> An Architecture for Coordinating Planning, Sensing, and Action. Proceedings of DARPA Workshop on Innovative Approaches to Planning, Scheduling, </title> <journal> and Control, </journal> <pages> 292-297. </pages>
Reference: <author> Sutton, R. S., </author> <year> 1990. </year> <title> Integrated architectures for learning, planning, and reacting based on approximately dynamic programming. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> 216-224. </pages>
Reference-contexts: Unfortunately the tradeoff between monitoring costs and penalties can be played out at every place the agent might stop to monitor. In fact, deciding when to monitor is a sequential decision problem <ref> (Sutton, 1990) </ref>. Finding an optimal control policy is also a sequential decision problem. Actions can have delayed effects that must be considered in choosing the action to take in each state, and a policy that chooses actions solely for their immediate effects may not be optimal over the long term.
Reference: <author> Wearden, J. H., </author> <year> 1985. </year> <title> The Power Law and Weber's Law in Fixed-Interval Postreinforcement Pausing: A Scalar Timing Model. </title> <journal> The Quarterly Journal of Experimental Psychology, </journal> <volume> 37B (3), </volume> <pages> 191-211. </pages>
References-found: 51

