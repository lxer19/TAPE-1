URL: http://robotics.stanford.edu/~latombe/papers/kuffner/viscomp99.ps.gz
Refering-URL: http://robotics.stanford.edu/~latombe/projects/
Root-URL: http://www.robotics.stanford.edu
Email: fkuffner,latombeg@cs.stanford.edu  
Title: Perception-Based Navigation for Animated Characters in Real-Time Virtual Environments  Visual Computer Real-time Virtual Worlds  
Author: James J. Kuffner, Jr and Jean-Claude Latombe 
Note: DRAFT: Submitted to The  
Web: http://robotics.stanford.edu/  
Address: Stanford, CA 94305-9010, USA  
Affiliation: Computer Science Robotics Lab Stanford University  
Abstract: Advances in computing hardware, software, and network technology have enabled a new class of interactive applications involving 3D animated characters to become increasingly feasible. Many such applications require algorithms that enable both autonomous and user-controlled animated characters to move naturally and realistically in response to task-level commands. This article presents a framework for high-level control of animated characters in real-time virtual environments. The framework design is inspired by research in motion planning, control, and sensing for autonomous mobile robots. We apply this framework to the problem of quickly synthesizing from navigation goals the collision-free motions for animated human figures in changing virtual environments. We combine a path planner, a path-following controller, and cyclic motion capture data to generate the underlying animation. The rendering hardware is used to simulate the visual sensing of a character, providing a feedback loop to the overall navigation strategy. We present an implementation of the algorithm that achieves real-time performance on common graphics workstations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. J. Kuffner Jr., </author> <title> An Architecture for the Design of Intelligent Animated Characters, </title> <type> Ph.D. thesis, </type> <institution> Stanford University (in preparation). </institution>
Reference-contexts: The resulting anima-tion can be generated at interactive rates and looks fairly realistic. This work is part of a larger project to build autonomous animated characters equipped with motion planning capabilities and simulated sensing <ref> [1] </ref>. The ultimate goal of this research is to create animated agents able to respond to task-level commands and behave naturally within changing virtual environments. A prolific area of research in the robotics literature has been the design and implementation of task-level motion planning algorithms for real-world robotic systems [2]. <p> The value of the time step was t = 0.0333 seconds (1/30 sec). Although these gain values were obtained via trial and error, research is underway to automatically compute the optimal gains for path following <ref> [1] </ref>. Sample output involving an office environment is illustrated in Figure 5. Multiple characters were run simultaneously, each planning around the other characters as they followed their own computed paths. Figure 6 shows snapshots at various stages of an animation involving a character exploring an unknown maze environment. <p> Knowing these optimal gains might also facilitate the calculation of conservative error-bounds on the performance of the path following controller. Research on techniques to control groups of multiple actors using the existing planning and perception modules is currently underway <ref> [1] </ref>. This includes some simple velocity prediction to take into account the estimated motion of other characters and obstacles during planning. Clearly, many challenging research issues must be faced before more interesting motions and intelligent behaviors for autonomous animated characters can be realized.
Reference: [2] <author> J. C. Latombe, </author> <title> Robot Motion Planning, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1991. </year>
Reference-contexts: The ultimate goal of this research is to create animated agents able to respond to task-level commands and behave naturally within changing virtual environments. A prolific area of research in the robotics literature has been the design and implementation of task-level motion planning algorithms for real-world robotic systems <ref> [2] </ref>. The inspiration for the fundamental ideas in this paper arises from this research. Section 2 provides a background and motivation for task-level control in the context of animation. Related work in building autonomous agents for the purposes of graphic animation is summarized in Section 3. <p> If no path exists, an appropriate stopping motion or waiting behavior is performed. 6 Path Planning The theory and analysis of motion planning algorithms is fairly well-developed in the robotics literature, and is not discussed in detail here. For a broad background in motion planning, readers are referred to <ref> [2] </ref>. For any motion planning, it is important to minimize the number of degrees of freedom (DOFs), since the time complexity of known algorithms grows exponentially with the number of DOFs [31]. <p> The character's joint hierarchy is shown, along with the number of DOF for each joint. The path planning approach adopted in this paper is one instance of an approximate cell decomposition method <ref> [2] </ref>. The search space (in this case, the walking surface) is discretized into a fine regular grid of cells. All obstacles are projected onto the grid and "grown" as detailed in Section 6.1. <p> Otherwise, the controller is notified that no path exists. The planner is resolution-complete, meaning that it is guaranteed to find a collision-free path from S to G if one exists at the current grid resolution, and otherwise report failure <ref> [2] </ref>. The running time of the obstacle projection step is proportional to the number and geometric complexity of the obstacles. Searching for a path in the bitmap using Dijkstra's algorithm runs in quadratic time with respect to the number of free cells in the grid.
Reference: [3] <author> A. Witkin and Z. Popovic, </author> <title> "Motion warping," </title> <booktitle> in Proc. SIGGRAPH '95, </booktitle> <year> 1995. </year>
Reference-contexts: Unfortunately, both keyframed-motion and motion capture data alone are inflexible in the sense that the motion is often only valid for a limited set of situations. Frequently, such motions must be redesigned if the locations of other objects or starting conditions change even slightly. Motion warping or blending algorithms <ref> [3, 4] </ref> offer some added flexibility, but usually can only be applied to a limited set of situations involving minor changes to the environment or starting conditions. Significant changes typically result in unrealistic motions.
Reference: [4] <author> A. Bruderlin and L. Williams, </author> <title> "Motion signal processing," </title> <booktitle> in Proc. SIGGRAPH '95, </booktitle> <editor> Robert Cook, Ed. </editor> <booktitle> ACM SIGGRAPH, Aug. 1995, Annual Conference Series, </booktitle> <pages> pp. 97-104, </pages> <publisher> Addison Wesley, </publisher> <address> held in Los Angeles, California, </address> <month> 06-11 August </month> <year> 1995. </year>
Reference-contexts: Unfortunately, both keyframed-motion and motion capture data alone are inflexible in the sense that the motion is often only valid for a limited set of situations. Frequently, such motions must be redesigned if the locations of other objects or starting conditions change even slightly. Motion warping or blending algorithms <ref> [3, 4] </ref> offer some added flexibility, but usually can only be applied to a limited set of situations involving minor changes to the environment or starting conditions. Significant changes typically result in unrealistic motions.
Reference: [5] <author> D. Baraff, </author> <title> "Analytical methods for dynamic simulation of non-penetrating rigid bodies," </title> <booktitle> in Proc. SIGGRAPH '89, </booktitle> <year> 1989, </year> <pages> pp. 223-231. </pages>
Reference-contexts: Significant changes typically result in unrealistic motions. Dynamic simulation and physically-based modeling techniques nicely handle the problems of physical validity and applicability to arbitrary situations. Given initial positions, velocities, forces, and dynamic properties, an object's motion is simulated according to natural physical laws <ref> [5, 6] </ref>. However, aside from specifying initial conditions, the user has no control over both the resulting motion and the final resting position of the object. Spacetime constraints provide a more general mathematical framework for addressing this problem of control [7, 8, 9].
Reference: [6] <author> B. Mirtich, </author> <title> Impulse-Based Dynamic Simulation of Rigid Body Systems, </title> <type> Ph.D. thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <year> 1996. </year>
Reference-contexts: Significant changes typically result in unrealistic motions. Dynamic simulation and physically-based modeling techniques nicely handle the problems of physical validity and applicability to arbitrary situations. Given initial positions, velocities, forces, and dynamic properties, an object's motion is simulated according to natural physical laws <ref> [5, 6] </ref>. However, aside from specifying initial conditions, the user has no control over both the resulting motion and the final resting position of the object. Spacetime constraints provide a more general mathematical framework for addressing this problem of control [7, 8, 9].
Reference: [7] <author> A. Witkin and Kass M., </author> <title> "Spacetime constraints," </title> <booktitle> in Proc. SIGGRAPH '88, </booktitle> <year> 1988, </year> <pages> pp. 159-168. </pages>
Reference-contexts: However, aside from specifying initial conditions, the user has no control over both the resulting motion and the final resting position of the object. Spacetime constraints provide a more general mathematical framework for addressing this problem of control <ref> [7, 8, 9] </ref>. Constraint equations imposed by the initial and final conditions, obstacle boundaries, and other desired properties of the motion are solved numerically. Unfortunately, the large number of constraints imposed by complex obstacle-cluttered environments can severely degrade the performance of such methods.
Reference: [8] <author> J. T. Ngo and J. Marks, </author> <title> "Spacetime constraints revisited," </title> <booktitle> in Proc. SIGGRAPH '93, </booktitle> <year> 1993, </year> <pages> pp. 343-350. </pages>
Reference-contexts: However, aside from specifying initial conditions, the user has no control over both the resulting motion and the final resting position of the object. Spacetime constraints provide a more general mathematical framework for addressing this problem of control <ref> [7, 8, 9] </ref>. Constraint equations imposed by the initial and final conditions, obstacle boundaries, and other desired properties of the motion are solved numerically. Unfortunately, the large number of constraints imposed by complex obstacle-cluttered environments can severely degrade the performance of such methods.
Reference: [9] <author> Z. Liu, S. J. Gortler, and F. C. Cohen, </author> <title> "Hierachical spacetime control," </title> <booktitle> in Proc. SIGGRAPH '94, </booktitle> <year> 1994, </year> <pages> pp. 35-42. </pages>
Reference-contexts: However, aside from specifying initial conditions, the user has no control over both the resulting motion and the final resting position of the object. Spacetime constraints provide a more general mathematical framework for addressing this problem of control <ref> [7, 8, 9] </ref>. Constraint equations imposed by the initial and final conditions, obstacle boundaries, and other desired properties of the motion are solved numerically. Unfortunately, the large number of constraints imposed by complex obstacle-cluttered environments can severely degrade the performance of such methods.
Reference: [10] <author> J. J. Kuffner, Jr., </author> <title> "Goal-directed navigation for animated characters using real-time path planning and control," in Proc. of CAPTECH '98 : Workshop on Modelling and Motion Capture Techniques for Virtual Environments. November 1998, </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: New approaches and algorithms are needed to compute natural, collision-free motions quickly in changing virtual environments. The method described in this paper combines the fast 2D path planner and controller originally proposed in <ref> [10] </ref>, with a synthetic vision and memory model to compute natural-looking motions for navigation tasks. The controller is used to synthesize cyclic motion capture data for an animated character as it follows a computed path towards a goal location.
Reference: [11] <author> R. A. Brooks, </author> <title> "A layered intelligent control system for a mobile robot," </title> <booktitle> in Robotics Research The Third International Symposium. </booktitle> <year> 1985, </year> <pages> pp. 365-372, </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: However, as processor speeds continue to increase, algorithms originally intended for off-line animations will gradually become feasible in real-time virtual environments. Much research effort in robotics has been focused on designing control architectures for autonomous agents that operate in the real world <ref> [11, 12] </ref>. Using rasterizing computer graphics hardware to assist robot motion planning algorithms was previously investigated by Lengyel, et al [13]. Recently, motion planning tools and algorithms have been applied to character animation.
Reference: [12] <author> R. C. Arkin, </author> <title> "Cooperation without communication: Multiagent schema based robot navigation," </title> <journal> Journal of Robotic Systems, </journal> <pages> pp. 351-364, </pages> <year> 1992. </year>
Reference-contexts: However, as processor speeds continue to increase, algorithms originally intended for off-line animations will gradually become feasible in real-time virtual environments. Much research effort in robotics has been focused on designing control architectures for autonomous agents that operate in the real world <ref> [11, 12] </ref>. Using rasterizing computer graphics hardware to assist robot motion planning algorithms was previously investigated by Lengyel, et al [13]. Recently, motion planning tools and algorithms have been applied to character animation.
Reference: [13] <author> J. Lengyel, M. Reichert, B. R. Donald, and D. P. Greenberg, </author> <title> "Real-time robot motion planning using rasterizing computer graphics hardware," </title> <booktitle> in Proc. </booktitle> <volume> SIG-GRAPH '90, </volume> <year> 1990. </year>
Reference-contexts: Much research effort in robotics has been focused on designing control architectures for autonomous agents that operate in the real world [11, 12]. Using rasterizing computer graphics hardware to assist robot motion planning algorithms was previously investigated by Lengyel, et al <ref> [13] </ref>. Recently, motion planning tools and algorithms have been applied to character animation. Koga et al. combined motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks [14].
Reference: [14] <author> Y. Koga, K. Kondo, J. Kuffner, and J.-C. Latombe, </author> <title> "Planning motions with intentions," </title> <booktitle> in Proc. SIGGRAPH '94, </booktitle> <year> 1994, </year> <pages> pp. 395-408. </pages>
Reference-contexts: Recently, motion planning tools and algorithms have been applied to character animation. Koga et al. combined motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks <ref> [14] </ref>. Hsu and Cohen combined path planning with motion capture data to animate a human figure navigating on uneven terrain [15].
Reference: [15] <author> D. Hsu and M. Cohen, </author> <title> "Task-level motion control for human figure animation," </title> <type> Unpublished Manuscript, </type> <year> 1997. </year>
Reference-contexts: Koga et al. combined motion planning and human arm inverse kinematics algorithms for automatically generating animation for human arm manipulation tasks [14]. Hsu and Cohen combined path planning with motion capture data to animate a human figure navigating on uneven terrain <ref> [15] </ref>. Researchers at the University of Pennsylvania have been exploring the use of motion planning to achieve postural goals using their Jack human character model [16, 17], incorporating body dynamics [18], and high-level scripting [19]. Research in designing fully-autonomous, interactive, artificial agents has also been on the rise. <p> The most severe limitation of the planner is the level-terrain requirement. Extending the algorithm to handle uneven-terrain is possible, but it might involve redesigning the geometry clipping and projection operations. Perhaps the approach taken by Hsu and Cohen would be more appropriate in this situation <ref> [15] </ref>. Possible extensions to the basic algorithm, include incorporating into the planning process the ability to step over low obstacles, or duck under overhangs. One idea might be to utilize the depth information information that is generated, but is currently being ignored during the projection process.
Reference: [16] <author> M. R. Jung, N. Badler, and T. Noma, </author> <title> "Animated human agents with motion planning capability for 3D-space postural goals," </title> <journal> The Journal of Visualization and Computer Animation, </journal> <volume> vol. 5, no. 4, </volume> <pages> pp. 225-246, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Hsu and Cohen combined path planning with motion capture data to animate a human figure navigating on uneven terrain [15]. Researchers at the University of Pennsylvania have been exploring the use of motion planning to achieve postural goals using their Jack human character model <ref> [16, 17] </ref>, incorporating body dynamics [18], and high-level scripting [19]. Research in designing fully-autonomous, interactive, artificial agents has also been on the rise. Tu and Terzopoulos implemented a realistic simulation of autonomous artificial fishes, complete with integrated simple behaviors, physically-based motion generation, and simulated perception [20].
Reference: [17] <author> J. P. Granieri, W. Becket, B. D. Reich, J. Crabtree, and N. L. Badler, </author> <title> "Behavioral control for real-time simulated human agents," in 1995 Symposium on Interactive 3D Graphics, </title> <editor> Pat Hanrahan and Jim Winget, Eds. </editor> <booktitle> ACM SIGGRAPH, </booktitle> <month> Apr. </month> <year> 1995, </year> <pages> pp. 173-180, </pages> <note> ISBN 0-89791-736-7. </note>
Reference-contexts: Hsu and Cohen combined path planning with motion capture data to animate a human figure navigating on uneven terrain [15]. Researchers at the University of Pennsylvania have been exploring the use of motion planning to achieve postural goals using their Jack human character model <ref> [16, 17] </ref>, incorporating body dynamics [18], and high-level scripting [19]. Research in designing fully-autonomous, interactive, artificial agents has also been on the rise. Tu and Terzopoulos implemented a realistic simulation of autonomous artificial fishes, complete with integrated simple behaviors, physically-based motion generation, and simulated perception [20].
Reference: [18] <author> E. Kokkevis, D. Metaxas, and N. I. Badler, </author> <title> "Autonomous animation and control of four-legged animals," in Graphics Interface '95, </title> <editor> Wayne A. Davis and Przemyslaw Prusinkiewicz, Eds. </editor> <booktitle> Canadian Information Processing Society, </booktitle> <month> May </month> <year> 1995, </year> <pages> pp. </pages> <month> 10-17, </month> <journal> Canadian Human-Computer Communications Society, </journal> <note> ISBN 0-9695338-4-5. </note>
Reference-contexts: Hsu and Cohen combined path planning with motion capture data to animate a human figure navigating on uneven terrain [15]. Researchers at the University of Pennsylvania have been exploring the use of motion planning to achieve postural goals using their Jack human character model [16, 17], incorporating body dynamics <ref> [18] </ref>, and high-level scripting [19]. Research in designing fully-autonomous, interactive, artificial agents has also been on the rise. Tu and Terzopoulos implemented a realistic simulation of autonomous artificial fishes, complete with integrated simple behaviors, physically-based motion generation, and simulated perception [20].
Reference: [19] <author> N. Badler, </author> <title> "Real-time virtual humans," </title> <journal> Pacific Graphics, </journal> <year> 1997. </year>
Reference-contexts: Researchers at the University of Pennsylvania have been exploring the use of motion planning to achieve postural goals using their Jack human character model [16, 17], incorporating body dynamics [18], and high-level scripting <ref> [19] </ref>. Research in designing fully-autonomous, interactive, artificial agents has also been on the rise. Tu and Terzopoulos implemented a realistic simulation of autonomous artificial fishes, complete with integrated simple behaviors, physically-based motion generation, and simulated perception [20].
Reference: [20] <author> X. Tu and D. Terzopoulos, </author> <title> "Artificial fishes: Physics, locomotion, perception, behavior," </title> <booktitle> in Proc. SIGGRAPH '94, </booktitle> <editor> Andrew Glassner, Ed. </editor> <booktitle> ACM SIGGRAPH, </booktitle> <month> July </month> <year> 1994, </year> <booktitle> Computer Graphics Proceedings, Annual Conference Series, </booktitle> <pages> pp. 43-50, </pages> <publisher> ACM Press, </publisher> <address> ISBN 0-89791-667-0. </address>
Reference-contexts: Research in designing fully-autonomous, interactive, artificial agents has also been on the rise. Tu and Terzopoulos implemented a realistic simulation of autonomous artificial fishes, complete with integrated simple behaviors, physically-based motion generation, and simulated perception <ref> [20] </ref>. Noser, et al. proposed a navigation system for animated characters based on synthetic vision, memory and learning [21]. These ideas were expanded to include virtual tactility and audition [22, 23]. <p> Tu and Terzopoulos implemented a synthetic vision for their artificial fishes based on ray-casting <ref> [20, 34] </ref>. Blumberg experimented with image-based motion energy techniques for obstacle avoidance for his autonomous virtual dog [35]. Ter-zopoulous and Rabie proposed using a database of pre-rendered models of objects along with an iterative pattern-matching scheme based on color histograms for object recognition [36].
Reference: [21] <author> H. Noser, O. Renault, D. Thalmann, and N. Magnenat Thalmann, </author> <title> "Navigation for digital actors based on synthetic vision, memory and learning," </title> <journal> Comput. Graphics, </journal> <volume> vol. 19, </volume> <pages> pp. 7-19, </pages> <year> 1995. </year>
Reference-contexts: Tu and Terzopoulos implemented a realistic simulation of autonomous artificial fishes, complete with integrated simple behaviors, physically-based motion generation, and simulated perception [20]. Noser, et al. proposed a navigation system for animated characters based on synthetic vision, memory and learning <ref> [21] </ref>. These ideas were expanded to include virtual tactility and audition [22, 23]. <p> Ter-zopoulous and Rabie proposed using a database of pre-rendered models of objects along with an iterative pattern-matching scheme based on color histograms for object recognition [36]. We adopt an approach to synthetic vision similar in spirit to the one described by Noser, et. al <ref> [22, 21] </ref>. The general idea is to render an unlit model of the scene (flat shading) from the character's point of view, using a unique color assigned to each object or object part. The pixel color information is extracted to obtain a list of the currently visible objects. <p> Noser et. al. used an occupancy grid model (e.g. an octree) to represent the visual memory of each character <ref> [21] </ref>. We instead rely upon the object geometry stored in the environment along with a list of object IDs and Fig. 4. The top image shows a character navigating in a virtual office. An outline of a representative portion of the character's viewing frustum is shown.
Reference: [22] <author> H. Noser and D. Thalmann, </author> <title> "Synthetic vision and audition for digital actors," </title> <booktitle> in Proc. Eurographics '95, </booktitle> <year> 1995. </year>
Reference-contexts: Noser, et al. proposed a navigation system for animated characters based on synthetic vision, memory and learning [21]. These ideas were expanded to include virtual tactility and audition <ref> [22, 23] </ref>. Other systems include Perlin and Goldberg's Improv software for interactive agents [24, 25], the ALIVE project at MIT [26, 27], Johnson's WavesWorld, and perhaps one of the earliest attempts at creating an agent ar-chitecture for the purposes of graphic animation: the Oz project at CMU [28]. <p> Ter-zopoulous and Rabie proposed using a database of pre-rendered models of objects along with an iterative pattern-matching scheme based on color histograms for object recognition [36]. We adopt an approach to synthetic vision similar in spirit to the one described by Noser, et. al <ref> [22, 21] </ref>. The general idea is to render an unlit model of the scene (flat shading) from the character's point of view, using a unique color assigned to each object or object part. The pixel color information is extracted to obtain a list of the currently visible objects.
Reference: [23] <author> D. Thalmann, H. Noser, and Z. Huang, </author> <title> Interactive Animation, </title> <booktitle> chapter 11, </booktitle> <pages> pp. 263-291, </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Noser, et al. proposed a navigation system for animated characters based on synthetic vision, memory and learning [21]. These ideas were expanded to include virtual tactility and audition <ref> [22, 23] </ref>. Other systems include Perlin and Goldberg's Improv software for interactive agents [24, 25], the ALIVE project at MIT [26, 27], Johnson's WavesWorld, and perhaps one of the earliest attempts at creating an agent ar-chitecture for the purposes of graphic animation: the Oz project at CMU [28]. <p> These objects and their current locations are added to the character's internal model of the environment, and a motion plan is computed as detailed in Section 4. As pointed out by Thalmann et. al. in <ref> [23] </ref>, synthetic vision differs from vision computations for real robots, since we can skip all of the problems of distance detection, pattern recognition, and noisy images. This allows us to implement a reasonable model of visual information flow that operates fast enough for real-time systems.
Reference: [24] <author> K. Perlin and A. Goldberg, "IMPROV: </author> <title> A system for scripting interactive actors in virtual worlds," </title> <booktitle> in Proc. SIGGRAPH '96, </booktitle> <editor> Holly Rushmeier, Ed. </editor> <booktitle> ACM SIGGRAPH, 1996, Annual Conference Series, </booktitle> <pages> pp. 205-216, </pages> <publisher> Addison Wesley. </publisher>
Reference-contexts: Noser, et al. proposed a navigation system for animated characters based on synthetic vision, memory and learning [21]. These ideas were expanded to include virtual tactility and audition [22, 23]. Other systems include Perlin and Goldberg's Improv software for interactive agents <ref> [24, 25] </ref>, the ALIVE project at MIT [26, 27], Johnson's WavesWorld, and perhaps one of the earliest attempts at creating an agent ar-chitecture for the purposes of graphic animation: the Oz project at CMU [28].
Reference: [25] <author> K. Perlin, </author> <title> "Real time responsive animation with personality," </title> <journal> IEEE Transactions on Visualization and Computer Graphics, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 5-15, </pages> <month> March </month> <year> 1995, </year> <pages> ISSN 1077-2626. </pages>
Reference-contexts: Noser, et al. proposed a navigation system for animated characters based on synthetic vision, memory and learning [21]. These ideas were expanded to include virtual tactility and audition [22, 23]. Other systems include Perlin and Goldberg's Improv software for interactive agents <ref> [24, 25] </ref>, the ALIVE project at MIT [26, 27], Johnson's WavesWorld, and perhaps one of the earliest attempts at creating an agent ar-chitecture for the purposes of graphic animation: the Oz project at CMU [28].
Reference: [26] <author> B. M. Blumberg and T. A. Galyean, </author> <title> "Multi-level direction of autonomous creatures for real-time virtual environments," </title> <booktitle> in Proc. SIGGRAPH '95, </booktitle> <editor> Robert Cook, Ed. </editor> <booktitle> ACM SIGGRAPH, Aug. 1995, Annual Conference Series, </booktitle> <pages> pp. 47-54, </pages> <publisher> Addison Wesley, </publisher> <address> held in Los Angeles, California, </address> <month> 06-11 August </month> <year> 1995. </year>
Reference-contexts: Noser, et al. proposed a navigation system for animated characters based on synthetic vision, memory and learning [21]. These ideas were expanded to include virtual tactility and audition [22, 23]. Other systems include Perlin and Goldberg's Improv software for interactive agents [24, 25], the ALIVE project at MIT <ref> [26, 27] </ref>, Johnson's WavesWorld, and perhaps one of the earliest attempts at creating an agent ar-chitecture for the purposes of graphic animation: the Oz project at CMU [28].
Reference: [27] <author> P. Maes, D. Trevor, B. Blumberg, and A. Pentland, </author> <title> "The ALIVE system full-body interaction with autonomous agents," </title> <booktitle> in Computer Animation '95, </booktitle> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Noser, et al. proposed a navigation system for animated characters based on synthetic vision, memory and learning [21]. These ideas were expanded to include virtual tactility and audition [22, 23]. Other systems include Perlin and Goldberg's Improv software for interactive agents [24, 25], the ALIVE project at MIT <ref> [26, 27] </ref>, Johnson's WavesWorld, and perhaps one of the earliest attempts at creating an agent ar-chitecture for the purposes of graphic animation: the Oz project at CMU [28].
Reference: [28] <author> J. Bates, A. B. Loyall, and W. S. Reilly, </author> <title> "An architecture for action, emotion, and social behavior," </title> <booktitle> in Artificial Social Systems : Proc of 4th European Wkshp on Modeling Autonomous Agents in a Multi-Agent World. 1994, </booktitle> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Other systems include Perlin and Goldberg's Improv software for interactive agents [24, 25], the ALIVE project at MIT [26, 27], Johnson's WavesWorld, and perhaps one of the earliest attempts at creating an agent ar-chitecture for the purposes of graphic animation: the Oz project at CMU <ref> [28] </ref>. The goals of the Oz project were to create agents with "broad" but "shallow" capabilities, rather than "deep" capabilities in a narrow area. Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics [29].
Reference: [29] <author> D. C. Brogan and J. K. Hodgins, </author> <title> "Group behaviors with significant dynamics," </title> <booktitle> in Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <year> 1995. </year>
Reference-contexts: The goals of the Oz project were to create agents with "broad" but "shallow" capabilities, rather than "deep" capabilities in a narrow area. Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics <ref> [29] </ref>. They have also designed a controller for human running in 3D [30].
Reference: [30] <author> J. K. Hodgins, </author> <title> "Three-dimensional human running," </title> <booktitle> in Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <year> 1996. </year>
Reference-contexts: Researchers at Georgia Tech have combined physically-based simulation with group behaviors for simulating human athletics [29]. They have also designed a controller for human running in 3D <ref> [30] </ref>. Despite these achievements, building autonomous agents that respond intelligently to task-level commands remains an elusive goal, particularly in real-time applications. 4 Goal-Directed Navigation Consider the case of an animated human character given the task of moving from one location to another in a flat-terrain virtual environment. <p> So-called "footstep"-driven animation systems could be applied to place the feet at points nearby the computed path, along with real-time inverse kinematics (IK) to hold them in place. As computer processing power increases, physically-based models of the character dynamics along with complex controllers such as the one presented in <ref> [30] </ref> could also potentially be used to simulate locomotion gaits along the path. For the purposes of these experiments, applying cyclic motion capture data proved to be a fast and simple method of obtaining satisfactory motion.
Reference: [31] <author> J. H. Reif, </author> <title> "Complexity of the mover's problem and generalizations," </title> <booktitle> in Proc. 20th IEEE Symp. on Foundations of Computer Science (FOCS), </booktitle> <year> 1979, </year> <pages> pp. 421-427. </pages>
Reference-contexts: For a broad background in motion planning, readers are referred to [2]. For any motion planning, it is important to minimize the number of degrees of freedom (DOFs), since the time complexity of known algorithms grows exponentially with the number of DOFs <ref> [31] </ref>. For character navigation on level-terrain, the important DOFs are the position and orientation (x; y; ) of the base of the character on the walking surface. As detailed in Section 7, the orientation (forward-facing direction) of the character is computed by the controller during the path following phase.
Reference: [32] <author> O. Renault, N. M. Thalmann, and D. Thalmann, </author> <title> "A vision-based approach to behavioral animation," </title> <journal> Visualization and Computer Animation, </journal> <volume> vol. 1, </volume> <pages> pp. 18-21, </pages> <year> 1990. </year>
Reference-contexts: To animate the remaining joints, the current velocity v t is used to index into the motion interpolation table as described in Section 7.2. 8 Sensing the Environment Previous researchers have argued the case for employing some kind of virtual perception for animated characters <ref> [32] </ref>. The key idea is to somehow realistically model the information flow from the environment to the character. Giving each character complete access to all objects in the environment is both conceptually unrealistic, and can be impractical to implement for large environments with many objects.
Reference: [33] <author> C. W. Reynolds, </author> <title> "Flocks, herds, and schools: A distributed behavioral model," </title> <journal> Computer Graphics, </journal> <volume> vol. 21(4), </volume> <pages> pp. 25-34, </pages> <year> 1987. </year>
Reference-contexts: One way to limit the information a character has access to, is to consider only objects within a sphere centered around the character <ref> [33] </ref>. However, most characters of interest (including human characters) do not have such 2 The gains represent how quickly errors (differences between the current and the desired) are resolved. Since a discrete time step is being used, some care must be taken when setting the gains.
Reference: [34] <author> X. Tu, </author> <title> Artificial Animals for Computer Animation: Biomechanics, Locomotion, Perception, and Behavior, </title> <type> Ph.D. thesis, </type> <institution> University of Toronto, Toronto, Canada, </institution> <year> 1996. </year>
Reference-contexts: Tu and Terzopoulos implemented a synthetic vision for their artificial fishes based on ray-casting <ref> [20, 34] </ref>. Blumberg experimented with image-based motion energy techniques for obstacle avoidance for his autonomous virtual dog [35]. Ter-zopoulous and Rabie proposed using a database of pre-rendered models of objects along with an iterative pattern-matching scheme based on color histograms for object recognition [36].
Reference: [35] <author> B. M. Blumberg, </author> <title> Old Tricks, New Dogs : Ethology and Interactive Creatures, </title> <type> Ph.D. thesis, </type> <institution> MIT Media Laboratory, </institution> <address> Boston, MA, </address> <year> 1996. </year>
Reference-contexts: Tu and Terzopoulos implemented a synthetic vision for their artificial fishes based on ray-casting [20, 34]. Blumberg experimented with image-based motion energy techniques for obstacle avoidance for his autonomous virtual dog <ref> [35] </ref>. Ter-zopoulous and Rabie proposed using a database of pre-rendered models of objects along with an iterative pattern-matching scheme based on color histograms for object recognition [36]. We adopt an approach to synthetic vision similar in spirit to the one described by Noser, et. al [22, 21].
Reference: [36] <author> D. Terzopoulos and T. Rabie, </author> <title> "Animat vision: </title> <booktitle> Active vision in artificial animals," in Proc. Fifth Int. Conf. on Computer Vision (ICCV'95), </booktitle> <address> Cambridge, MA, </address> <month> June </month> <year> 1995, </year> <pages> pp. 801-808. </pages>
Reference-contexts: Blumberg experimented with image-based motion energy techniques for obstacle avoidance for his autonomous virtual dog [35]. Ter-zopoulous and Rabie proposed using a database of pre-rendered models of objects along with an iterative pattern-matching scheme based on color histograms for object recognition <ref> [36] </ref>. We adopt an approach to synthetic vision similar in spirit to the one described by Noser, et. al [22, 21].
References-found: 36

