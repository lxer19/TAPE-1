URL: http://eksl-www.cs.umass.edu/~gregory/papers/synthesis.ps
Refering-URL: http://eksl-www.cs.umass.edu/~gregory/papers/
Root-URL: 
Title: A Theorist's Empirical Assistant  
Author: Dawn E. Gregory Prof. Paul Cohen and Prof. Arny Rosenberg, 
Note: My readers,  have contributed greatly to this work. Without their assistance this project would not have been possible. This material is based upon work supported under a National Science Foundation Graduate Research Fellowship. Any opinions, findings, conclusions or recommendations expressed in this document are those of the author and do not necessarily reflect the views of the National Science Foundation.  
Date: 20 November 1995  
Abstract: This research addresses a question that has challenged researchers in many fields: are there formal methods for deriving a rigorous, scientific theory directly from experience? This issue is addressed from two perspectives: that of a scientist who wants to develop a theory through empirical study, and that of an intelligent agent which is designed to perform this task. This paper reports on research activities that approach the question from both perspectives. A case study is presented in which a theory is developed through empirical evaluation of a computational problem. The experimental techniques used in this study are incorporated into the design of an automated scientific discovery agent, which is also described. This work does not constitute a complete theory of the relation between empirical and theoretical results. Rather, it is a first step in developing such a theory. As such, it uncovers many issues which need to be examined in future work. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rob St. Amant. </author> <title> A planner for exploratory data analysis. </title> <booktitle> Submitted to the Third International Conference on Artificial Intelligence Planning Systems (APIS-96). </booktitle>
Reference: [2] <author> Scott D. Anderson, Adam Carlson, David L. Westbrook, David M. Hart, and Paul R. Cohen. Clip/clasp: </author> <title> Common lisp analytical statistics package/common lisp instrumentation package. </title> <type> Technical Report 93-55, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA, </address> <year> 1993. </year> <title> Contact clasp-support@cs.umass.edu for further information. </title>
Reference-contexts: Finally, an important feature of a scientific discovery agent is its ability to design and run controlled experiments. Since computer programs are the chosen problem domain, it should be possible to partially or fully automate this task. Tools such as CLIP <ref> [2] </ref> already provide the ability to automate the data collection process, based on an experiment plan. This system will be incorporated into the agent's experiment design capabilities.
Reference: [3] <author> Paul R. Cohen, Dawn E. Gregory, Lisa Ballesteros, and Robert St.Amant. </author> <title> Two algorithms for inducing structural equation models from data. </title> <booktitle> In Preliminary Papers of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <year> 1995. </year>
Reference: [4] <author> Pat Langley, Herbert A. Simon, Gary Bradshaw, and Jan M. Zytkow. </author> <title> Scientific Discovery. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference: [5] <author> Catherine C. McGeoch. </author> <title> Analyzing algorithms by simulation: Variance reduction techniques and simulation speedups. </title> <journal> ACM Computing Surveys, </journal> <volume> 245(2) </volume> <pages> 195-212, </pages> <year> 1992. </year> <type> DRAFT - 11/20/95 17 </type>
Reference-contexts: However, as new advances in parallel and pipelined computer architectures make systematic analysis insurmountable, empirical study becomes a necessity. Empirical Algorithmics is a branch of Theoretical Computer Science which has been established to deal with these issues (e.g. <ref> [5] </ref>). The primary concern is to derive the complexity of an algorithmic problem based on empirical evaluation. Using a simulation of the algorithm, random sampling, and statistical techniques, the analyst develops a model of the algorithm's expected complexity from its behavior in a controlled experiment.
Reference: [6] <author> Eric W. Noreen. </author> <title> Computer Intensive Methods for Testing Hypotheses. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1989. </year>
Reference-contexts: Statistics introduced techniques for hypothesis testing, data exploration, and equation approximation (e.g. [11]). Recently, these techniques have been further improved by advances in computing power, which supports data visualization and intensive re-sampling techniques (e.g. <ref> [6] </ref>). More recently, the problem of learning from experience has been addressed by the field of Artificial Intelligence. Now, the question is becoming pertinent to scientists who have traditionally disregarded the empirical approach: theoreticians are looking to integrate experimental strategies into a rigorous research program.
Reference: [7] <author> Judea Pearl and T.S. Verma. </author> <title> A statistical semantics for causation. </title> <journal> Statistics and Computing, </journal> <volume> 2 </volume> <pages> 91-95, </pages> <year> 1991. </year>
Reference: [8] <author> Jeff Shrager and Pat Langley, </author> <title> editors. Computational Models of Scientific Discovery and Theory Formation. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [9] <author> Peter Spirtes, Clark Glymour, and Richard Scheines. </author> <title> Causation, Prediction, and Search. </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference: [10] <author> P.C. Suppes. </author> <title> A Probabilistic Theory of Causality. </title> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1970. </year>
Reference: [11] <author> John W. Tukey. </author> <title> Exploratory Data Analysis. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1977. </year>
Reference-contexts: The first methods for detecting the structure in real-world data were introduced early this century in a new field called statistics. Statistics introduced techniques for hypothesis testing, data exploration, and equation approximation (e.g. <ref> [11] </ref>). Recently, these techniques have been further improved by advances in computing power, which supports data visualization and intensive re-sampling techniques (e.g. [6]). More recently, the problem of learning from experience has been addressed by the field of Artificial Intelligence.
References-found: 11

