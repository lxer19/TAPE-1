URL: http://www.cs.utexas.edu/users/orb/projects/icga97.ps
Refering-URL: http://forum.swarthmore.edu/~jay/learn-game/indexes/other-papers.html
Root-URL: 
Email: orb,moriarty,paulmcq,risto@cs.utexas.edu  
Title: Evolving Neural Networks to Play Go  
Author: Norman Richards, David Moriarty, Paul McQuesten, and Risto Miikkulainen 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences The University of Texas at Austin  
Abstract: Go is a difficult game for computers to master, and the best go programs are still weaker than the average human player. Since the traditional game playing techniques have proven inadequate, new approaches to computer go need to be studied. This paper presents a new approach to learning to play go. The SANE (Symbiotic, Adaptive Neuro-Evolution) method was used to evolve networks capable of playing go on small boards with no pre-programmed go knowledge. On a 9 fi 9 go board, networks that were able to defeat a simple computer opponent were evolved within a few hundred generations. Most significantly, the networks exhibited several aspects of general go playing, which suggests the approach could scale up well.
Abstract-found: 1
Intro-found: 1
Reference: <author> Enderton, H. D. </author> <year> (1991). </year> <title> The Golem go program. </title> <type> Technical Report CMU-CS-92-101, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference: <author> Enzenberger, M. </author> <year> (1996). </year> <title> The integration of a priori knowledge into a go playing neural network. </title> <type> Manuscript. </type>
Reference: <author> Hsu, F., Anantharaman, T., Campbell, M., and Nowatzyk, A. </author> <year> (1990). </year> <title> A grandmaster chess machine. </title> <journal> Scientific American, </journal> <volume> 263 </volume> <pages> 44-50. </pages>
Reference: <author> Lee, K.-F., and Mahajan, S. </author> <year> (1990). </year> <title> The development of a world-class othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 21-36. </pages>
Reference: <author> Moriarty, D. E. </author> <year> (1997). </year> <title> Symbiotic Evolution of Neural Networks in Sequential Decision Tasks. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, The University of Texas at Austin, Austin, TX. </institution> <note> Technical Report UT-AI97-259. </note>
Reference-contexts: Each network is evaluated and the results of the evaluation control the evolution of the neuron population. ure 3). Evolving partial solutions is an effective method of insuring diversity, making evolution more effective <ref> (Moriarty 1997) </ref>. 5.1 FORMING NETWORKS SANE evolves a large population of hidden neurons for a three-layer feedforward network (figure 4). A single neuron represents only part of the solution. However, when combined with other neurons they form a complete network for the given task.
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1996a). </year> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 11-32. </pages>
Reference: <author> Moriarty, D. E., and Miikkulainen, R. </author> <year> (1996b). </year> <title> Evolving obstacle avoidance behavior in a robot arm. </title> <editor> In Maes, P., Mataric, M., Meyer, J.-A., and Pol-lack, J., editors, </editor> <booktitle> From Animals to Animats: The Fourth International Conference on Simulation of Adaptive Behavior (SAB96). </booktitle>
Reference: <author> Pell, B. </author> <year> (1991). </year> <title> Exploratory learning in the game of go. </title> <editor> In Levy, D., and Beal, D., editors, </editor> <booktitle> Heuristic Programming in Artificial Intelligence 2 The Second Computer Olympiad. </booktitle> <publisher> Ellis Horwood. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., and McClel-land, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> 318-362. </pages> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: A network could be trained to compute a mapping between the input space, that is, the current board position, and the output indicating the next move. The main problem with this approach is the credit assignment problem. Suppose a standard backpropagation neural network <ref> (Rumelhart et al. 1986) </ref> were being trained to play go. For backpropagation to work, advance knowledge about the best move at any given position would be required. Such knowledge is difficult to come by. In reality, only the final game result is available.
Reference: <author> Schaeffer, J., Lake, R., Lu, P., and Bryant, M. </author> <year> (1996). </year> <title> CHINOOK: The world man-machine checkers champion. </title> <journal> The AI Magazine, </journal> <volume> 16(1) </volume> <pages> 21-29. </pages>
References-found: 10

