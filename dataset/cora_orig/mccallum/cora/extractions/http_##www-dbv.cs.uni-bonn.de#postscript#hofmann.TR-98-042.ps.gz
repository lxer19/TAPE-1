URL: http://www-dbv.cs.uni-bonn.de/postscript/hofmann.TR-98-042.ps.gz
Refering-URL: http://www-dbv.cs.uni-bonn.de/abstracts/hofmann.TR-98-042.html
Root-URL: http://cs.uni-bonn.de
Email: hofmann@cs.berkeley.edu  jan@cs.uni-bonn.de  
Title: Unsupervised Learning from Dyadic Data  
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Thomas Hofmann and Jan Puzicha 
Date: December 1998  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  Berkeley  Bonn, Germany  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  International Computer Science Institute, Berkeley, CA Computer Science Division, UC  Institut fur Informatik III University of  
Pubnum: TR-98-042  
Abstract: Dyadic data refers to a domain with two finite sets of objects in which observations are made for dyads, i.e., pairs with one element from either set. This includes event co-occurrences, histogram data, and single stimulus preference data as special cases. Dyadic data arises naturally in many applications ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent framework for unsupervised learning from dyadic data by statistical mixture models. Our approach covers different models with flat and hierarchical latent class structures and unifies probabilistic modeling and structure discovery. Mixture models provide both, a parsimonious yet flexible parameterization of probability distributions with good generalization performance on sparse data, as well as structural information about data-inherent grouping structure. We propose an annealed version of the standard Expectation Maximization algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains. 
Abstract-found: 1
Intro-found: 1
Reference: [And97] <author> E. Anderson. </author> <title> Introduction to the Statistical Analysis of Categorial Data. </title> <publisher> Springer, </publisher> <year> 1997. </year>
Reference-contexts: Prominent techniques are, for example, Katz's back-off method [Kat87], model interpolation with held-out data [JM80, Jel85], and similarity-based smoothing techniques [ES92, DLP97]. In contrast, we propose a model-based statistical approach and present a family of latent class <ref> [And97] </ref> or finite mixture models [TSM85, MB88] as a principled approach to deal with the data sparseness problem.
Reference: [Bar87] <author> D. J. Bartholomew. </author> <title> Latent variable models and factor analysis. Number 40 in Griffin's statistical monographs & courses. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Because of the symmetry of the model, an equivalent decomposition is obtained by interchanging the role of the sets X and Y. Probabilistic Factor Analysis for Discrete Data The dimensionality reduction obtained by the aspect model is similar in spirit to factor analysis <ref> [Bar87] </ref>. The factor analysis of co-occurrence data is also known as Latent Semantic Analysis (LSA) [LS89, DTGL90].
Reference: [BAS95] <author> C. Buckley, J. Allan, and G. Salton. </author> <title> Automatic routing and retrieval using SMART - TREC2. </title> <journal> Information Processing and Managment, </journal> <volume> 31(3) </volume> <pages> 315-326, </pages> <year> 1995. </year>
Reference-contexts: Probabilistic Latent Semantic Indexing Beyond controversy, the most popular family of techniques utilized in information retrieval is the so-called Vector Space Model (VSM), introduced by Salton et al. <ref> [SM83, SB91, BAS95] </ref> in the SMART system. In the VSM, each document is represented by a term vector with (transformed) frequency counts for term occurrences as components.
Reference: [BdM + 92] <author> P.F. Brown, P.V. deSouza, R.L. Mercer, V.J. Della Pietra, and J.C. Lai. </author> <title> Class-based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 467-479, </pages> <year> 1992. </year>
Reference-contexts: Mixture and clustering models for dyadic data have been investigated before under the titles of class-based n-gram models <ref> [BdM + 92] </ref>, distributional clustering [PTL93], and aggregate Markov models [SP97a] in natural language processing. All three approaches are recovered as special cases in our general learning framework. There is also a close relation to clustering methods for qualitative data like the information clustering approach (cf. [Boc74]). <p> The latter reflects the additional "coarsening" induced by grouping objects in Y. 2.6 Related Models and Previous Work Brown et al. <ref> [BdM + 92] </ref> have proposed an information criterion for class-based n-gram models in language modeling. Word classes are formed such that the mutual information between classes of adjacent words is maximized. Their model is closely related to the two-sided clustering model with the main difference that in [BdM + 92] the <p> Brown et al. <ref> [BdM + 92] </ref> have proposed an information criterion for class-based n-gram models in language modeling. Word classes are formed such that the mutual information between classes of adjacent words is maximized. Their model is closely related to the two-sided clustering model with the main difference that in [BdM + 92] the word classes for the predicting and predicted word are identified (hence C (x)=D (x)). 7 Moreover, Brown et al. have proposed a non-probabilistic, hard clustering variant for which they have introduced a greedy cluster merging algorithm.
Reference: [BK93] <author> J.M. Buhmann and H. Kuhnel. </author> <title> Vector quantisation with complexity costs. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39 </volume> <pages> 1133-1145, </pages> <year> 1993. </year>
Reference-contexts: Annealed EM is closely related to deterministic annealing, a technique that has been applied to many clustering problems, including vectorial clustering <ref> [RGF90, RGF92, BK93] </ref>, pairwise clustering [HB97, PHB99], and in the context of co-occurrence data for distributional clustering [PTL93]. The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the free energy.
Reference: [BKS97] <author> E. Bauer, D. Koller, and Y. Singer. </author> <title> Update rules for parameter estimation in bayesian networks. </title> <booktitle> In Proceedings of the 13th Annual Conference on Uncertainty in AI (UAI), </booktitle> <address> Providence, Rhode Island, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: A simple way to accelerate EM algorithms is by over-relaxation in the M-step. This has been discussed in the context of mixture models [PW78] and was recently `rediscovered' under the title of EM (j) in <ref> [BKS97] </ref>. We found this method useful in accelerating the fitting procedure for all discussed models.
Reference: [Boc74] <author> H. H. Bock. </author> <title> Automatische Klassifikation : theoretische und praktische Methoden zur Gruppierung und Strukturierung von Daten (Cluster-Analyse). Number 24 in Stu-dia mathematica; mathematische Lehrbucher. </title> <institution> Vandenhoeck und Ruprecht, Gottingen, </institution> <year> 1974. </year>
Reference-contexts: All three approaches are recovered as special cases in our general learning framework. There is also a close relation to clustering methods for qualitative data like the information clustering approach (cf. <ref> [Boc74] </ref>). The modeling principle of latent variable models is the specification of a joint probability distribution for latent and observable variables.
Reference: [Buh98] <author> J. M. Buhmann. </author> <title> Empirical risk approximation: An induction principle for unsupervised learning. </title> <type> Technical Report IAI-TR-98-3, </type> <institution> Institut fur Informatik III, University of Bonn, </institution> <year> 1998. </year>
Reference-contexts: Annealing, thereby, has the potential to improve the generalization for otherwise overfitting models (for supervised learning problems cf. [PKM96, RMRG97]). Recent theoretical investigations emphasize the benefits of annealing to avoid overfitting phenomena <ref> [Buh98] </ref>. In this paper, the advantages of deterministic annealing are investigated experimentally (cf. Section 4). In statistical learning, deterministic annealing is used in the T ! T fin 1 limit where the stopping temperature T fin ! 1 in the inifinte data aspymptotics.
Reference: [BWD97] <author> M. Blatt, S. Wiseman, and E. Domany. </author> <title> Data clustering using a model granular magnet. </title> <journal> Neural Computation, </journal> <volume> 9(8) </volume> <pages> 1805-1842, </pages> <year> 1997. </year> <month> 29 </month>
Reference-contexts: For this restricted case, substantial progress has been achieved. However, the focus on metric data has disregarded a variety of important problems which do not fit into this setting. Examples that have recently received some attention are proximity data <ref> [HB97, BWD97] </ref> which replace metric distances by the weaker notion of pairwise similarities and ranked preference data [CSS98].
Reference: [CA80] <author> J. D. Carroll and P. Arabie. </author> <title> Multidimensional scaling. </title> <journal> Annual Review of Psychology, </journal> <volume> 31 </volume> <pages> 607-649, </pages> <year> 1980. </year>
Reference-contexts: Examples that have recently received some attention are proximity data [HB97, BWD97] which replace metric distances by the weaker notion of pairwise similarities and ranked preference data [CSS98]. A variety of other types of non-metrical data can be found, for example, in the psychometric literature <ref> [Coo64, Kru78, CA80] </ref>, in particular in the context of multidimensional scaling and correspondence analysis. 1.1 Dyadic Data In this paper, we introduce a general framework for unsupervised learning from dyadic data.
Reference: [CKP92] <author> D.R. Cutting, D.R. Karger, and J.O. Pedersen. Scatter/gather: </author> <title> a cluster-based approach to browsing large document collections. </title> <booktitle> In Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1992. </year>
Reference-contexts: The most frequently used methods in this context are linkage algorithms (single linkage, complete linkage, Wards method, cf. [JD88]), or hybrid combinations of agglomerative and centroid-based methods 22 keywords 'cluster', 'decision', 'glass', 'robust', 'segment', and 'channel'. <ref> [CKP92] </ref> which have no probabilistic interpretation and have a number of other disadvantages. In contrast, the hierarchical mixture model provide a sound statistical basis and also has many additional features which make it a suitable candidate in this context.
Reference: [Coo64] <author> C. H. Coombs. </author> <title> A Theory of Data. </title> <publisher> John Wiley & Son, </publisher> <year> 1964. </year>
Reference-contexts: Examples that have recently received some attention are proximity data [HB97, BWD97] which replace metric distances by the weaker notion of pairwise similarities and ranked preference data [CSS98]. A variety of other types of non-metrical data can be found, for example, in the psychometric literature <ref> [Coo64, Kru78, CA80] </ref>, in particular in the context of multidimensional scaling and correspondence analysis. 1.1 Dyadic Data In this paper, we introduce a general framework for unsupervised learning from dyadic data.
Reference: [CSS98] <author> W. W. Cohen, R. E. Shapire, and Y. Singer. </author> <title> Learning to order things. </title> <booktitle> In Advances in Neural Information Processing Systems 10, </booktitle> <year> 1998. </year>
Reference-contexts: However, the focus on metric data has disregarded a variety of important problems which do not fit into this setting. Examples that have recently received some attention are proximity data [HB97, BWD97] which replace metric distances by the weaker notion of pairwise similarities and ranked preference data <ref> [CSS98] </ref>. A variety of other types of non-metrical data can be found, for example, in the psychometric literature [Coo64, Kru78, CA80], in particular in the context of multidimensional scaling and correspondence analysis. 1.1 Dyadic Data In this paper, we introduce a general framework for unsupervised learning from dyadic data.
Reference: [CT91] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: The asymptotic average codeword length of a code based on P (yjc (x)) is exactly the cross entropy which governs the latent class posterior probabilities (cf. <ref> [CT91] </ref>). The one-sided clustering model can also be viewed as an unsupervised version of the naive Bayes' classifier, if we give Y the interpretation of a feature space for x 2 X .
Reference: [DLP93] <author> I. Dagan, L. Lee, and F.C.N. Pereira. </author> <title> Similarity-based estimation of word cooccurence probabilities. </title> <booktitle> In Proceedings of the Association for Computational Linguistics, </booktitle> <year> 1993. </year>
Reference-contexts: example, we have depicted a virtual interactive expansion of the CLUSTER hierarchy for a retrieval session on 'texture segmentation' in Figure 11. 4.3 Computational Linguistics In computational linguistics, the statistical analysis of word co-occurrences in lexical structures like adjective/noun or verb/direct object has recently received a considerable degree of attention <ref> [Hin90, PTL93, DLP93, DLP97] </ref>. Potential applications of these methods are in word-sense disambiguation, a problem which occurs in different linguistic tasks ranging from parsing and tagging to machine translation.
Reference: [DLP97] <author> I. Dagan, L. Lee, and F.C.N. Pereira. </author> <title> Similarity-based methods for word sense disambiguation. </title> <booktitle> In Proceedings of the Association for Computational Linguistics, </booktitle> <year> 1997. </year>
Reference-contexts: X corresponds to a document collection, Y to the vocabulary, and a dyad represents the occurrence of a token in the content of a document [HPJ99]. * Computational linguistics in the corpus-based statistical analysis of word co-occurences which has applications in probabilistic language modeling, word clustering [PTL93], word sense disambiguation <ref> [Hin90, DLP97] </ref> and discrimination [Sch98], and automated thesaurus construc tion [SP97b]. * Preference analysis and consumption behavior by identifying X with individuals and Y with objects. Dyads then correspond to single stimulus preferences. <p> Typical state-of-the-art techniques in natural language processing apply smoothing to deal with zero frequencies of unobserved events. Prominent techniques are, for example, Katz's back-off method [Kat87], model interpolation with held-out data [JM80, Jel85], and similarity-based smoothing techniques <ref> [ES92, DLP97] </ref>. In contrast, we propose a model-based statistical approach and present a family of latent class [And97] or finite mixture models [TSM85, MB88] as a principled approach to deal with the data sparseness problem. <p> example, we have depicted a virtual interactive expansion of the CLUSTER hierarchy for a retrieval session on 'texture segmentation' in Figure 11. 4.3 Computational Linguistics In computational linguistics, the statistical analysis of word co-occurrences in lexical structures like adjective/noun or verb/direct object has recently received a considerable degree of attention <ref> [Hin90, PTL93, DLP93, DLP97] </ref>. Potential applications of these methods are in word-sense disambiguation, a problem which occurs in different linguistic tasks ranging from parsing and tagging to machine translation.
Reference: [DLR77] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statist. Soc. B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: To overcome the difficulties in maximizing a log of a sum, we apply the Expectation Maximization (EM) framework <ref> [DLR77, MK97] </ref> and alternate two re-estimation steps: * an Expectation (E)-step for estimating the posterior probabilities of the unobserved mapping P (ajS; 0 ) for a given parameter estimate 0 , * a Maximization (M)-step, which involves maximization of the expected complete data log-like lihood L (j 0 ) = P
Reference: [DTGL90] <author> S. Deerwester, Dumais S. T., Furnas G.W., </author> <title> and T.K. Landauer. Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <year> 1990. </year>
Reference-contexts: Probabilistic Factor Analysis for Discrete Data The dimensionality reduction obtained by the aspect model is similar in spirit to factor analysis [Bar87]. The factor analysis of co-occurrence data is also known as Latent Semantic Analysis (LSA) <ref> [LS89, DTGL90] </ref>. In LSA, the co-occurrence matrix of counts C = (n (x i ; y j )) i;j is decomposed by Singular Value Decomposition (SVD) into C = UV t with orthogonal matrices U and V and a diagonal matrix containing the singular values of C. <p> For example, although a word y in a query may not occur in a document, the (indirect) conditional probability based on the aspect model can nevertheless be high. This is similar to the dimension-reduction approach pursued in standard LSI <ref> [DTGL90] </ref>. We tested the PLSI method on a number of medium-sized standard document test collection with relevance assessments by computing precision-recall curves. The precision-recall curve reported in the sequel have been obtained by so-called macro averaging (cf. [VR75, Chapter7]).
Reference: [ES92] <author> U. Essen and V. Steinbiss. </author> <title> Cooccurrence smoothing for stochastic language modeling. </title> <booktitle> In Proceedings of the IEEE nternational Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 161-164, </pages> <year> 1992. </year>
Reference-contexts: Typical state-of-the-art techniques in natural language processing apply smoothing to deal with zero frequencies of unobserved events. Prominent techniques are, for example, Katz's back-off method [Kat87], model interpolation with held-out data [JM80, Jel85], and similarity-based smoothing techniques <ref> [ES92, DLP97] </ref>. In contrast, we propose a model-based statistical approach and present a family of latent class [And97] or finite mixture models [TSM85, MB88] as a principled approach to deal with the data sparseness problem.
Reference: [GGGD90] <author> D. Geman, S. Geman, C. Graffigne, and P. Dong. </author> <title> Boundary detection by constrained optimization. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(7) </volume> <pages> 609-628, </pages> <year> 1990. </year>
Reference-contexts: Occasionally, the grouping process has been based on pairwise similarity measurements between image sites, where similarity is measured by a non-parametric statistical test applied to the feature distribution of a surrounding neighborhood <ref> [GGGD90, HPB98] </ref>. Pairwise similarity clustering thus provides an indirect way to group (discrete) feature distributions without reducing information in a distribution to their mean. Mixture models for dyadic data, especially the one-sided clustering model, formalize the grouping of feature distribution in a more direct manner.
Reference: [GNOD92] <author> D. Goldberg, D. Nichols, B. M. Oki, and Terry D. </author> <title> Using collaborative filtering to weave an information tapestry. </title> <journal> Communications of the ACM, </journal> <volume> 35(12) </volume> <pages> 61-70, </pages> <year> 1992. </year>
Reference-contexts: Dyads then correspond to single stimulus preferences. This type of data is the starting point for a machine learning technique known as collaborative filtering <ref> [GNOD92, KMMH97] </ref>. 1.2 Modeling Goals and Principles Across different domains, there are two main tasks which play a fundamental role in unsupervised learning from dyadic data: (i) probabilistic modeling, i.e., learning a joint or conditional probability model over X fi Y, and (ii) structure discovery, e.g., identifying clusters and data hierarchies.
Reference: [Goo53] <author> I.J. </author> <title> Good. The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40(3) </volume> <pages> 237-264, </pages> <year> 1953. </year>
Reference-contexts: For object sets with a nominal scale the empirical co-occurrence frequencies are sufficient statistics, capturing all we know about the data. However, the intrinsic difficulty in modeling dyadic data is the data sparseness, also known as the zero frequency problem <ref> [Goo53, Goo65, Kat87, WB91] </ref>. When the product set X fi Y is very large, a majority of pairs (x; y) only have a small probability of being observed in a given sample. Most of the empirical frequencies are typically zero or at least significantly 1 corrupted by sampling noise.
Reference: [Goo65] <author> I.J. </author> <title> Good. The Estimation of Probabilities. Research Monograph 30. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1965. </year>
Reference-contexts: For object sets with a nominal scale the empirical co-occurrence frequencies are sufficient statistics, capturing all we know about the data. However, the intrinsic difficulty in modeling dyadic data is the data sparseness, also known as the zero frequency problem <ref> [Goo53, Goo65, Kat87, WB91] </ref>. When the product set X fi Y is very large, a majority of pairs (x; y) only have a small probability of being observed in a given sample. Most of the empirical frequencies are typically zero or at least significantly 1 corrupted by sampling noise.
Reference: [HB97] <author> T. Hofmann and J.M. Buhmann. </author> <title> Pairwise data clustering by deterministic annealing. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(1), </volume> <year> 1997. </year>
Reference-contexts: For this restricted case, substantial progress has been achieved. However, the focus on metric data has disregarded a variety of important problems which do not fit into this setting. Examples that have recently received some attention are proximity data <ref> [HB97, BWD97] </ref> which replace metric distances by the weaker notion of pairwise similarities and ranked preference data [CSS98]. <p> Annealed EM is closely related to deterministic annealing, a technique that has been applied to many clustering problems, including vectorial clustering [RGF90, RGF92, BK93], pairwise clustering <ref> [HB97, PHB99] </ref>, and in the context of co-occurrence data for distributional clustering [PTL93]. The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the free energy.
Reference: [Hin90] <author> D. Hindle. </author> <title> Noun classification from predicate-argument structures. </title> <booktitle> In Proceedings of the ACL, </booktitle> <pages> pages 268-275, </pages> <year> 1990. </year>
Reference-contexts: X corresponds to a document collection, Y to the vocabulary, and a dyad represents the occurrence of a token in the content of a document [HPJ99]. * Computational linguistics in the corpus-based statistical analysis of word co-occurences which has applications in probabilistic language modeling, word clustering [PTL93], word sense disambiguation <ref> [Hin90, DLP97] </ref> and discrimination [Sch98], and automated thesaurus construc tion [SP97b]. * Preference analysis and consumption behavior by identifying X with individuals and Y with objects. Dyads then correspond to single stimulus preferences. <p> example, we have depicted a virtual interactive expansion of the CLUSTER hierarchy for a retrieval session on 'texture segmentation' in Figure 11. 4.3 Computational Linguistics In computational linguistics, the statistical analysis of word co-occurrences in lexical structures like adjective/noun or verb/direct object has recently received a considerable degree of attention <ref> [Hin90, PTL93, DLP93, DLP97] </ref>. Potential applications of these methods are in word-sense disambiguation, a problem which occurs in different linguistic tasks ranging from parsing and tagging to machine translation.
Reference: [HPB94] <author> F. Heitz, P. Perez, and P. Bouthemy. </author> <title> Multiscale minimization of global energy functions in some visual recovery problems. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 59(1) </volume> <pages> 125-134, </pages> <year> 1994. </year>
Reference-contexts: All results are based on ten-fold cross validation. 3.4 Multiscale EM Multiscale optimization <ref> [HPB94, PB98] </ref> is an approach for accelerating clustering algorithms whenever a neighborhood structure exists on the object space (s). In image segmentation, for example, it is a natural assumption that adjacent image sites belong with high probability to the same cluster or image segment.
Reference: [HPB98] <author> T. Hofmann, J. Puzicha, and J.M. Buhmann. </author> <title> Unsupervised texture segmentation in a deterministic annealing framework. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <year> 1998. </year> <month> 30 </month>
Reference-contexts: Some exemplary application areas of dyadic data are: * Computer vision, in particular in the context of image segmentation, where X corresponds to image locations, Y to discretized or categorical feature values, and a dyad denotes the occurrence of a feature at a particular image location <ref> [HPB98] </ref>. * Text-based information retrieval, where X corresponds to a document collection, Y to the vocabulary, and a dyad represents the occurrence of a token in the content of a document [HPJ99]. * Computational linguistics in the corpus-based statistical analysis of word co-occurences which has applications in probabilistic language modeling, word <p> Here, Q is an approximating probability distribution which is chosen in order to minimize the KL-divergence to the true posterior distribution (cf. <ref> [NH98, JGJS98, HPB98] </ref>). The approximate E-step equations are given by (cf. <p> Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme. In the modeling stage: characteristic features are extracted from the textured input image, e.g. spatial frequencies <ref> [JF91, HPB98] </ref>, MRF-models [MJ92]. In the clustering stage features are grouped into homogeneous segments, where homogeneity of features is typically formalized by a clustering optimization criterion. <p> Occasionally, the grouping process has been based on pairwise similarity measurements between image sites, where similarity is measured by a non-parametric statistical test applied to the feature distribution of a surrounding neighborhood <ref> [GGGD90, HPB98] </ref>. Pairwise similarity clustering thus provides an indirect way to group (discrete) feature distributions without reducing information in a distribution to their mean. Mixture models for dyadic data, especially the one-sided clustering model, formalize the grouping of feature distribution in a more direct manner. <p> We applied the one-sided clustering model to the unsupervised segmentation of textured images, where objects x correspond to image locations. Since the number of observed features is identical for all sites, one can simply set P (x) = 1=N . In the experiments, we have adopted the framework of <ref> [JF91, HPB98, PB98] </ref> and utilized an image representation based on the modulus of complex Gabor filters. For each site, the empirical distribution of coefficients in a surrounding (filter-specific) window is determined.
Reference: [HPJ99] <author> T. Hofmann, J. Puzicha, and M. I. Jordan. </author> <title> Learning from dyadic data. </title> <booktitle> In Advances in Neural Information Processing Systems 11, </booktitle> <year> 1999. </year>
Reference-contexts: discretized or categorical feature values, and a dyad denotes the occurrence of a feature at a particular image location [HPB98]. * Text-based information retrieval, where X corresponds to a document collection, Y to the vocabulary, and a dyad represents the occurrence of a token in the content of a document <ref> [HPJ99] </ref>. * Computational linguistics in the corpus-based statistical analysis of word co-occurences which has applications in probabilistic language modeling, word clustering [PTL93], word sense disambiguation [Hin90, DLP97] and discrimination [Sch98], and automated thesaurus construc tion [SP97b]. * Preference analysis and consumption behavior by identifying X with individuals and Y with objects.
Reference: [JD88] <author> A.K. Jain and R.C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ 07632, </address> <year> 1988. </year>
Reference-contexts: The most frequently used methods in this context are linkage algorithms (single linkage, complete linkage, Wards method, cf. <ref> [JD88] </ref>), or hybrid combinations of agglomerative and centroid-based methods 22 keywords 'cluster', 'decision', 'glass', 'robust', 'segment', and 'channel'. [CKP92] which have no probabilistic interpretation and have a number of other disadvantages.
Reference: [Jel85] <author> F. Jelinek. </author> <title> The development of an experimental discrete dictation recogniser. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 73(11), </volume> <year> 1985. </year>
Reference-contexts: Typical state-of-the-art techniques in natural language processing apply smoothing to deal with zero frequencies of unobserved events. Prominent techniques are, for example, Katz's back-off method [Kat87], model interpolation with held-out data <ref> [JM80, Jel85] </ref>, and similarity-based smoothing techniques [ES92, DLP97]. In contrast, we propose a model-based statistical approach and present a family of latent class [And97] or finite mixture models [TSM85, MB88] as a principled approach to deal with the data sparseness problem.
Reference: [JF91] <author> A. Jain and F. Farrokhnia. </author> <title> Unsupervised texture segmentation using Gabor filters. </title> <journal> Pattern Recognition, </journal> <volume> 24(12) </volume> <pages> 1167-1186, </pages> <year> 1991. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme. In the modeling stage: characteristic features are extracted from the textured input image, e.g. spatial frequencies <ref> [JF91, HPB98] </ref>, MRF-models [MJ92]. In the clustering stage features are grouped into homogeneous segments, where homogeneity of features is typically formalized by a clustering optimization criterion. <p> In the clustering stage features are grouped into homogeneous segments, where homogeneity of features is typically formalized by a clustering optimization criterion. Most widely, features are interpreted as vectors in a Euclidean space <ref> [JF91, MJ92, PH95] </ref> and a segmentation is obtained by minimizing the K-means criterion, which sums over the square distances between feature vectors and their assigned, group-specific prototype feature vectors. <p> We applied the one-sided clustering model to the unsupervised segmentation of textured images, where objects x correspond to image locations. Since the number of observed features is identical for all sites, one can simply set P (x) = 1=N . In the experiments, we have adopted the framework of <ref> [JF91, HPB98, PB98] </ref> and utilized an image representation based on the modulus of complex Gabor filters. For each site, the empirical distribution of coefficients in a surrounding (filter-specific) window is determined.
Reference: [JGJS98] <author> M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, and L.K. Saul. </author> <title> An introduction to variational methods for graphical models. In M.I. </title> <editor> Jordan, editor, </editor> <booktitle> Learning in Graphical Models, </booktitle> <pages> pages 105-161. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1998. </year>
Reference-contexts: Here, Q is an approximating probability distribution which is chosen in order to minimize the KL-divergence to the true posterior distribution (cf. <ref> [NH98, JGJS98, HPB98] </ref>). The approximate E-step equations are given by (cf.
Reference: [JJ94] <author> M.I. Jordan and R.A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: However, especially in the context of structure discovery, it is important to find a hierarchical data organization. There are well-known learning architectures like the Hierarchical Mixtures of Experts <ref> [JJ94] </ref> which can fit hierarchical models to data. Yet, in the case of dyadic data there is an alternative way to define a hierarchical model by combining aspects and clusters.
Reference: [JM80] <author> F. Jelinek and R. Mercer. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceedings of the Workshop of Pattern Recognition in Practice, </booktitle> <year> 1980. </year>
Reference-contexts: Typical state-of-the-art techniques in natural language processing apply smoothing to deal with zero frequencies of unobserved events. Prominent techniques are, for example, Katz's back-off method [Kat87], model interpolation with held-out data <ref> [JM80, Jel85] </ref>, and similarity-based smoothing techniques [ES92, DLP97]. In contrast, we propose a model-based statistical approach and present a family of latent class [And97] or finite mixture models [TSM85, MB88] as a principled approach to deal with the data sparseness problem.
Reference: [Kat87] <author> S.M. Katz. </author> <title> Estimation of probabilities for sparse data for the language model component of a speech recogniser. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 35(3) </volume> <pages> 400-401, </pages> <year> 1987. </year>
Reference-contexts: For object sets with a nominal scale the empirical co-occurrence frequencies are sufficient statistics, capturing all we know about the data. However, the intrinsic difficulty in modeling dyadic data is the data sparseness, also known as the zero frequency problem <ref> [Goo53, Goo65, Kat87, WB91] </ref>. When the product set X fi Y is very large, a majority of pairs (x; y) only have a small probability of being observed in a given sample. Most of the empirical frequencies are typically zero or at least significantly 1 corrupted by sampling noise. <p> Typical state-of-the-art techniques in natural language processing apply smoothing to deal with zero frequencies of unobserved events. Prominent techniques are, for example, Katz's back-off method <ref> [Kat87] </ref>, model interpolation with held-out data [JM80, Jel85], and similarity-based smoothing techniques [ES92, DLP97]. In contrast, we propose a model-based statistical approach and present a family of latent class [And97] or finite mixture models [TSM85, MB88] as a principled approach to deal with the data sparseness problem. <p> The latter might offer perspectives for a foundation of unsupervised learning beyond the maximum likelihood principle, which has been the basis of our work. Saul and Pereira [SP97a] have independently from our work proposed a model called aggregate Markov model which they have utilized as a back-off model <ref> [Kat87] </ref> in the context of language modeling. Their approach is equivalent to the aspect model in its asymmetric parameterization, but was restricted to a class-based bigram model (X = Y). They have also derived an EM algorithm, but without employing annealing methods (cf.
Reference: [KMMH97] <author> J.A. Konstan, B. N. Miller, D. Maltz, and J. L. Herlocker. Grouplens: </author> <title> Applying collaborative filtering to Usenet news. </title> <journal> Communications of the ACM, </journal> <volume> 40(3) </volume> <pages> 77-87, </pages> <year> 1997. </year>
Reference-contexts: Dyads then correspond to single stimulus preferences. This type of data is the starting point for a machine learning technique known as collaborative filtering <ref> [GNOD92, KMMH97] </ref>. 1.2 Modeling Goals and Principles Across different domains, there are two main tasks which play a fundamental role in unsupervised learning from dyadic data: (i) probabilistic modeling, i.e., learning a joint or conditional probability model over X fi Y, and (ii) structure discovery, e.g., identifying clusters and data hierarchies.
Reference: [Kru78] <author> J. B. Kruskal. </author> <title> Multidimensional Scaling. </title> <publisher> Sage Publications, </publisher> <address> Beverly Hills, CA, </address> <year> 1978. </year>
Reference-contexts: Examples that have recently received some attention are proximity data [HB97, BWD97] which replace metric distances by the weaker notion of pairwise similarities and ranked preference data [CSS98]. A variety of other types of non-metrical data can be found, for example, in the psychometric literature <ref> [Coo64, Kru78, CA80] </ref>, in particular in the context of multidimensional scaling and correspondence analysis. 1.1 Dyadic Data In this paper, we introduce a general framework for unsupervised learning from dyadic data.
Reference: [LDC97] <author> LDC. </author> <title> Linguistic Data Consortium: TDT pilot study corpus documentation. </title> <note> http://www.ldc.upenn.edu/TDT, 1997. </note>
Reference-contexts: To stress the advantages of annealed EM, we have investigated the effect of a temperature-based regularization in more detail. In order to take the sample set size into account, we have utilized the Topic Detection and Tracking (TDT1) corpus <ref> [LDC97] </ref> which consists of approximately 7 million words in 15863 documents and which is large enough for subsampling experiments. Since we want to focus on the control of overfitting and not on the problem of local maxima, all models have been trained at a fixed temperature.
Reference: [LS89] <author> K.E. Lochbaum and L.A. Streeter. </author> <title> Comparing and combining the effectiveness of latent semantic indexing and the ordinary vector space model for information retrieval. </title> <booktitle> Information Processing & Management, </booktitle> <year> 1989. </year>
Reference-contexts: Probabilistic Factor Analysis for Discrete Data The dimensionality reduction obtained by the aspect model is similar in spirit to factor analysis [Bar87]. The factor analysis of co-occurrence data is also known as Latent Semantic Analysis (LSA) <ref> [LS89, DTGL90] </ref>. In LSA, the co-occurrence matrix of counts C = (n (x i ; y j )) i;j is decomposed by Singular Value Decomposition (SVD) into C = UV t with orthogonal matrices U and V and a diagonal matrix containing the singular values of C.
Reference: [MB88] <author> G.J. McLachlan and K. E. Basford. </author> <title> Mixture Models. </title> <publisher> Marcel Dekker, INC, </publisher> <address> New York Basel, </address> <year> 1988. </year>
Reference-contexts: Prominent techniques are, for example, Katz's back-off method [Kat87], model interpolation with held-out data [JM80, Jel85], and similarity-based smoothing techniques [ES92, DLP97]. In contrast, we propose a model-based statistical approach and present a family of latent class [And97] or finite mixture models <ref> [TSM85, MB88] </ref> as a principled approach to deal with the data sparseness problem. Mixture and clustering models for dyadic data have been investigated before under the titles of class-based n-gram models [BdM + 92], distributional clustering [PTL93], and aggregate Markov models [SP97a] in natural language processing. <p> Cross-entropy Clustering and Naive Bayes' Classification It is illuminating to rewrite the class posterior probabilities in (15) as PfC (x) = cjS x ; g / P (c) exp &lt; n (x) @ y2Y 1 9 ; Comparing (18) with standard models like the Gaussian mixture model <ref> [MB88] </ref> or probabilistic vector quantization [RGF92] demonstrates that the cross entropy between the empirical conditional probability ^ P (yjx) and the class-conditional P (yjc) serves as a distortion measure in the one-sided clustering model.
Reference: [MJ92] <author> J. Mao and A. Jain. </author> <title> Texture classification and segmentation using multiresolution simultaneous autoregressive models. </title> <journal> Pattern Recognition, </journal> <volume> 25 </volume> <pages> 173-188, </pages> <year> 1992. </year>
Reference-contexts: Numerous approaches to texture segmentation have been proposed over the past decades, most of which obey a two-stage scheme. In the modeling stage: characteristic features are extracted from the textured input image, e.g. spatial frequencies [JF91, HPB98], MRF-models <ref> [MJ92] </ref>. In the clustering stage features are grouped into homogeneous segments, where homogeneity of features is typically formalized by a clustering optimization criterion. <p> In the clustering stage features are grouped into homogeneous segments, where homogeneity of features is typically formalized by a clustering optimization criterion. Most widely, features are interpreted as vectors in a Euclidean space <ref> [JF91, MJ92, PH95] </ref> and a segmentation is obtained by minimizing the K-means criterion, which sums over the square distances between feature vectors and their assigned, group-specific prototype feature vectors.
Reference: [MJ98] <author> M. Meila and M. I. Jordan. </author> <title> Estimating dependency structure as a hidden variable. </title> <booktitle> In Advances in Neural Information Processing Systems 10, </booktitle> <year> 1998. </year>
Reference-contexts: There are further possibilities to weaken the conditional independence assumption both for the aspect and the clustering model, e.g., by substituting the class-conditional multinomial distributions P (yjc) by log-linear Markov models [WL90, Whi87] or tree dependency models <ref> [MJ98] </ref>. This direction, however, is not further pursued in this paper. 2.3 Two-sided Clustering Model Model Specification In the two-sided clustering model, the latent structure consists of cluster partitionings c and d defined over X and Y, respectively.
Reference: [MK97] <author> G.J. McLachlan and T. Krishnan. </author> <title> The EM Algorithm and Extensions. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: To overcome the difficulties in maximizing a log of a sum, we apply the Expectation Maximization (EM) framework <ref> [DLR77, MK97] </ref> and alternate two re-estimation steps: * an Expectation (E)-step for estimating the posterior probabilities of the unobserved mapping P (ajS; 0 ) for a given parameter estimate 0 , * a Maximization (M)-step, which involves maximization of the expected complete data log-like lihood L (j 0 ) = P <p> The EM algorithm is known to increase the observed likelihood in each step, and converges to a (local) maximum under mild assumptions <ref> [MK97] </ref>. In the aspect model, contains all continuous parameters, namely P (a), P (xja) and P (yja). <p> In case that a positivity or normalization constraint is violated after performing an over-relaxed M-step, the parameter vector is projected back on the admissible parameter space (replacing negative probabilities by a small positive constant). For an overview on more elaborated acceleration methods for EM we refer to <ref> [MK97] </ref>. 15 Aspect X -cluster Hierarchical X =Y-cluster K 1=T P 1=T P 1=T P 1=T P 16 0.85 431 0.07 482 0.14 471 0.60 543 64 0.79 360 0.06 527 0.11 422 0.48 477 Table 2: Comparative results for context-dependent unigram modelling for all discussed models on the Cranfield IR
Reference: [NH98] <author> R.M. Neal and G.E. Hinton. </author> <title> A view of the EM algorithm that justifies incremental and other variants. In M.I. </title> <editor> Jordan, editor, </editor> <booktitle> Learning in Graphical Models, </booktitle> <pages> pages 355-368. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1998. </year>
Reference-contexts: Here, Q is an approximating probability distribution which is chosen in order to minimize the KL-divergence to the true posterior distribution (cf. <ref> [NH98, JGJS98, HPB98] </ref>). The approximate E-step equations are given by (cf. <p> The authors are grateful to Carl de Marcken and Joshua Goodman for sharing their expertise and data in natural language processing as well as to J.M.H. du Buf for providing the image data depicted in Fig. 13. Appendix EM, Annealed EM and Free Energy Minimization In <ref> [NH98] </ref>, it has been shown that both, the E-step and M-step of the EM algorithm, are minimizing a (generalized) free energy criterion. This fact is of importance, in particular for deriving approximate E-steps.
Reference: [PB98] <author> Jan Puzicha and Joachim Buhmann. </author> <title> Multi-scale annealing for real-time unsupervised texture segmentation. </title> <booktitle> In Proceedings of the International Conference on Computer Vision (ICCV'98), </booktitle> <pages> pages 267-273, </pages> <year> 1998. </year> <month> 31 </month>
Reference-contexts: All results are based on ten-fold cross validation. 3.4 Multiscale EM Multiscale optimization <ref> [HPB94, PB98] </ref> is an approach for accelerating clustering algorithms whenever a neighborhood structure exists on the object space (s). In image segmentation, for example, it is a natural assumption that adjacent image sites belong with high probability to the same cluster or image segment. <p> We applied the one-sided clustering model to the unsupervised segmentation of textured images, where objects x correspond to image locations. Since the number of observed features is identical for all sites, one can simply set P (x) = 1=N . In the experiments, we have adopted the framework of <ref> [JF91, HPB98, PB98] </ref> and utilized an image representation based on the modulus of complex Gabor filters. For each site, the empirical distribution of coefficients in a surrounding (filter-specific) window is determined.
Reference: [PH95] <author> D. Panjwani and G. Healey. </author> <title> Markov random field models for unsupervised segmenta-tion of textured color images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(10) </volume> <pages> 939-954, </pages> <year> 1995. </year>
Reference-contexts: In the clustering stage features are grouped into homogeneous segments, where homogeneity of features is typically formalized by a clustering optimization criterion. Most widely, features are interpreted as vectors in a Euclidean space <ref> [JF91, MJ92, PH95] </ref> and a segmentation is obtained by minimizing the K-means criterion, which sums over the square distances between feature vectors and their assigned, group-specific prototype feature vectors.
Reference: [PH98] <author> J. Puzicha and T. Hofmann. </author> <title> Histogram clustering for unsupervised segmentation and image retrieval. </title> <type> Technical report, Echtzeit-Optimierung Preprint 98-33, </type> <note> submitted to Pattern Recognition Letters, </note> <year> 1998. </year>
Reference-contexts: The segmentation quality achieved on outdoor images in Fig. 14 are both visually and semantically satisfying. A detailed evaluation of the one-sided clustering model for unsupervised texture segmentation is out of the scope of this paper and will be published elsewhere <ref> [PH98] </ref>. 5 Conclusion As the main contribution of this paper a novel class of statistical models for the analysis of co-occurrence data has been proposed and evaluated. We have introduced and discussed several different models.
Reference: [PHB99] <author> J. Puzicha, T. Hofmann, and J. Buhmann. </author> <title> A theory of proximity based clustering: Structure detection by optimization. </title> <journal> Pattern Recognition, </journal> <year> 1999. </year>
Reference-contexts: Annealed EM is closely related to deterministic annealing, a technique that has been applied to many clustering problems, including vectorial clustering [RGF90, RGF92, BK93], pairwise clustering <ref> [HB97, PHB99] </ref>, and in the context of co-occurrence data for distributional clustering [PTL93]. The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the free energy.
Reference: [PKM96] <author> K. Pawelzik, J. Kohlmorgen, and K. R. Muller. </author> <title> Annealed competition of experts for a segmentation and classification of switching dynamics. </title> <booktitle> Neural Computation, </booktitle> <year> 1996. </year>
Reference-contexts: This is the reason why annealed EM not only reduces the sensitivity to local minima but also controls the effective model complexity. Annealing, thereby, has the potential to improve the generalization for otherwise overfitting models (for supervised learning problems cf. <ref> [PKM96, RMRG97] </ref>). Recent theoretical investigations emphasize the benefits of annealing to avoid overfitting phenomena [Buh98]. In this paper, the advantages of deterministic annealing are investigated experimentally (cf. Section 4).
Reference: [PTL93] <author> F.C.N. Pereira, N.Z. Tishby, and L. Lee. </author> <title> Distributional clustering of english words. </title> <booktitle> In Proceedings of the ACL, </booktitle> <pages> pages 183-190, </pages> <year> 1993. </year>
Reference-contexts: Text-based information retrieval, where X corresponds to a document collection, Y to the vocabulary, and a dyad represents the occurrence of a token in the content of a document [HPJ99]. * Computational linguistics in the corpus-based statistical analysis of word co-occurences which has applications in probabilistic language modeling, word clustering <ref> [PTL93] </ref>, word sense disambiguation [Hin90, DLP97] and discrimination [Sch98], and automated thesaurus construc tion [SP97b]. * Preference analysis and consumption behavior by identifying X with individuals and Y with objects. Dyads then correspond to single stimulus preferences. <p> Mixture and clustering models for dyadic data have been investigated before under the titles of class-based n-gram models [BdM + 92], distributional clustering <ref> [PTL93] </ref>, and aggregate Markov models [SP97a] in natural language processing. All three approaches are recovered as special cases in our general learning framework. There is also a close relation to clustering methods for qualitative data like the information clustering approach (cf. [Boc74]). <p> Many of the ideas presented in his paper have been solicitated by the pioneering work of Pereira, Tishby, and Lee <ref> [PTL93] </ref> on distributional clustering. In [PTL93], the aspect model mixture distribution is the starting point, however, neither are latent aspect variables introduced, nor is the corresponding EM algorithm derived. <p> Many of the ideas presented in his paper have been solicitated by the pioneering work of Pereira, Tishby, and Lee <ref> [PTL93] </ref> on distributional clustering. In [PTL93], the aspect model mixture distribution is the starting point, however, neither are latent aspect variables introduced, nor is the corresponding EM algorithm derived. <p> The reason is that all x objects are treated equally, in particular irrespective of the number of observations in the sample set S x . Yet in <ref> [PTL93] </ref>, the centroid condition is derived from the maximum likelihood principle, after inserting the maximum entropy cluster membership probabilities. Although both principles are considered to be "complementary" and re-estimation equations are iterated according to a pseudo-EM scheme. In [PTL93] it remains unresolved whether an underlying common objective function exists. <p> Yet in <ref> [PTL93] </ref>, the centroid condition is derived from the maximum likelihood principle, after inserting the maximum entropy cluster membership probabilities. Although both principles are considered to be "complementary" and re-estimation equations are iterated according to a pseudo-EM scheme. In [PTL93] it remains unresolved whether an underlying common objective function exists. In a new formulation [TP98], both types of equations have been derived more rigorously in a rate-distrotion theoretic ansatz from a mutual information principle. <p> Annealed EM is closely related to deterministic annealing, a technique that has been applied to many clustering problems, including vectorial clustering [RGF90, RGF92, BK93], pairwise clustering [HB97, PHB99], and in the context of co-occurrence data for distributional clustering <ref> [PTL93] </ref>. The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the free energy. Here, we present annealing methods without reference to statistical physics. <p> example, we have depicted a virtual interactive expansion of the CLUSTER hierarchy for a retrieval session on 'texture segmentation' in Figure 11. 4.3 Computational Linguistics In computational linguistics, the statistical analysis of word co-occurrences in lexical structures like adjective/noun or verb/direct object has recently received a considerable degree of attention <ref> [Hin90, PTL93, DLP93, DLP97] </ref>. Potential applications of these methods are in word-sense disambiguation, a problem which occurs in different linguistic tasks ranging from parsing and tagging to machine translation.
Reference: [PW78] <author> B. C. Peters and H. F. Walker. </author> <title> An iterative procedure for obtaining maximum-likelihood estimates of the parameters for a mixture of normal distributions. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 35 </volume> <pages> 362-378, </pages> <year> 1978. </year>
Reference-contexts: However, for many problems the convergence speed of EM restricts its applicability to large data sets. A simple way to accelerate EM algorithms is by over-relaxation in the M-step. This has been discussed in the context of mixture models <ref> [PW78] </ref> and was recently `rediscovered' under the title of EM (j) in [BKS97]. We found this method useful in accelerating the fitting procedure for all discussed models.
Reference: [RGF90] <author> K. Rose, E. Gurewitz, and G. Fox. </author> <title> Statistical mechanics and phase transitions in clustering. </title> <journal> Physical Review Letters, </journal> <volume> 65(8) </volume> <pages> 945-948, </pages> <year> 1990. </year>
Reference-contexts: Annealed EM is closely related to deterministic annealing, a technique that has been applied to many clustering problems, including vectorial clustering <ref> [RGF90, RGF92, BK93] </ref>, pairwise clustering [HB97, PHB99], and in the context of co-occurrence data for distributional clustering [PTL93]. The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the free energy. <p> This is explained by the fact that annealing is a homotopy method, where the original objective function (such as the log-likelihood) is smoothed for large T . For hierarchical models, the annealed EM algorithm offers a natural way to generate tree topologies. As is known from adaptive vector quantization <ref> [RGF90] </ref>, starting at a high value of T and successively lowering T typically leads through a sequence of phase transitions. At each phase transition, the effective number of distinguishable clusters grows until some maximal number is reached or the annealing is stopped.
Reference: [RGF92] <author> K. Rose, E. Gurewitz, and G. Fox. </author> <title> Vector quantization by deterministic annealing. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(4) </volume> <pages> 1249-1257, </pages> <year> 1992. </year>
Reference-contexts: and Naive Bayes' Classification It is illuminating to rewrite the class posterior probabilities in (15) as PfC (x) = cjS x ; g / P (c) exp &lt; n (x) @ y2Y 1 9 ; Comparing (18) with standard models like the Gaussian mixture model [MB88] or probabilistic vector quantization <ref> [RGF92] </ref> demonstrates that the cross entropy between the empirical conditional probability ^ P (yjx) and the class-conditional P (yjc) serves as a distortion measure in the one-sided clustering model. <p> Annealed EM is closely related to deterministic annealing, a technique that has been applied to many clustering problems, including vectorial clustering <ref> [RGF90, RGF92, BK93] </ref>, pairwise clustering [HB97, PHB99], and in the context of co-occurrence data for distributional clustering [PTL93]. The key idea is to introduce a temperature parameter T and to replace the minimization of a combinatorial objective function by a substitute known as the free energy.
Reference: [RMRG97] <author> A. Rao, D. Miller, K. Rose, and A.. Gersho. </author> <title> Deterministically annealed mixture of experts models for statistical regression. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 4, </volume> <pages> pages 3201-3204. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1997. </year>
Reference-contexts: This is the reason why annealed EM not only reduces the sensitivity to local minima but also controls the effective model complexity. Annealing, thereby, has the potential to improve the generalization for otherwise overfitting models (for supervised learning problems cf. <ref> [PKM96, RMRG97] </ref>). Recent theoretical investigations emphasize the benefits of annealing to avoid overfitting phenomena [Buh98]. In this paper, the advantages of deterministic annealing are investigated experimentally (cf. Section 4).
Reference: [SB91] <author> G. Salton and C. Buckley. </author> <title> Global text matching for information retrieval. </title> <journal> Science, </journal> <volume> 253(5023) </volume> <pages> 1012-1015, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Probabilistic Latent Semantic Indexing Beyond controversy, the most popular family of techniques utilized in information retrieval is the so-called Vector Space Model (VSM), introduced by Salton et al. <ref> [SM83, SB91, BAS95] </ref> in the SMART system. In the VSM, each document is represented by a term vector with (transformed) frequency counts for term occurrences as components.
Reference: [SB95] <author> P. Schroeter and J. Bigun. </author> <title> Hierarchical image segmentation by multi-dimensional clustering and orientation-adaptive boundary refinement. </title> <journal> Pattern Recognition, </journal> <volume> 28(5) </volume> <pages> 695-709, </pages> <year> 1995. </year>
Reference-contexts: Mixture models for dyadic data, especially the one-sided clustering model, formalize the grouping of feature distribution in a more direct manner. In contrast to pairwise similarity clustering, they offer a sound generative model for texture class description which can be utilized in subsequent processing stages like edge localization <ref> [SB95] </ref>. Furthermore, there is no need to compute a large matrix of pairwise similarity scores between image sites, which greatly reduces the overall processing time and memory requirements. Compared to K-means, these techniques provides significantly more flexibility in distribution modeling.
Reference: [Sch98] <author> H. Schutze. </author> <title> Automatic word sense discrimination. </title> <journal> Computational Linguistics, </journal> <volume> 24(1) </volume> <pages> 97-123, </pages> <year> 1998. </year>
Reference-contexts: document collection, Y to the vocabulary, and a dyad represents the occurrence of a token in the content of a document [HPJ99]. * Computational linguistics in the corpus-based statistical analysis of word co-occurences which has applications in probabilistic language modeling, word clustering [PTL93], word sense disambiguation [Hin90, DLP97] and discrimination <ref> [Sch98] </ref>, and automated thesaurus construc tion [SP97b]. * Preference analysis and consumption behavior by identifying X with individuals and Y with objects. Dyads then correspond to single stimulus preferences.
Reference: [SM83] <author> G. Salton and M. J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: Probabilistic Latent Semantic Indexing Beyond controversy, the most popular family of techniques utilized in information retrieval is the so-called Vector Space Model (VSM), introduced by Salton et al. <ref> [SM83, SB91, BAS95] </ref> in the SMART system. In the VSM, each document is represented by a term vector with (transformed) frequency counts for term occurrences as components.
Reference: [SP97a] <author> L. Saul and F. Pereira. </author> <title> Aggregate and mixed-order Markov models for statistical language processing. </title> <booktitle> In Proceedings of the 2nd International Conference on Empirical Methods in Natural Language Processing, </booktitle> <year> 1997. </year>
Reference-contexts: Mixture and clustering models for dyadic data have been investigated before under the titles of class-based n-gram models [BdM + 92], distributional clustering [PTL93], and aggregate Markov models <ref> [SP97a] </ref> in natural language processing. All three approaches are recovered as special cases in our general learning framework. There is also a close relation to clustering methods for qualitative data like the information clustering approach (cf. [Boc74]). <p> The latter might offer perspectives for a foundation of unsupervised learning beyond the maximum likelihood principle, which has been the basis of our work. Saul and Pereira <ref> [SP97a] </ref> have independently from our work proposed a model called aggregate Markov model which they have utilized as a back-off model [Kat87] in the context of language modeling.
Reference: [SP97b] <author> H Schutze and J.O. Pedersen. </author> <title> A cooccurrence-based thesaurus and two applications to information retrieval. </title> <journal> Information Processing and Managment, </journal> <volume> 33(3) </volume> <pages> 307-318, </pages> <year> 1997. </year>
Reference-contexts: and a dyad represents the occurrence of a token in the content of a document [HPJ99]. * Computational linguistics in the corpus-based statistical analysis of word co-occurences which has applications in probabilistic language modeling, word clustering [PTL93], word sense disambiguation [Hin90, DLP97] and discrimination [Sch98], and automated thesaurus construc tion <ref> [SP97b] </ref>. * Preference analysis and consumption behavior by identifying X with individuals and Y with objects. Dyads then correspond to single stimulus preferences.
Reference: [TP98] <author> N.Z Tishby and F.C.N. Pereira. </author> <type> personal communication and manuscript, </type> <note> in preparation, </note> <year> 1998. </year>
Reference-contexts: Although both principles are considered to be "complementary" and re-estimation equations are iterated according to a pseudo-EM scheme. In [PTL93] it remains unresolved whether an underlying common objective function exists. In a new formulation <ref> [TP98] </ref>, both types of equations have been derived more rigorously in a rate-distrotion theoretic ansatz from a mutual information principle. The latter might offer perspectives for a foundation of unsupervised learning beyond the maximum likelihood principle, which has been the basis of our work.
Reference: [TSM85] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year> <month> 32 </month>
Reference-contexts: Prominent techniques are, for example, Katz's back-off method [Kat87], model interpolation with held-out data [JM80, Jel85], and similarity-based smoothing techniques [ES92, DLP97]. In contrast, we propose a model-based statistical approach and present a family of latent class [And97] or finite mixture models <ref> [TSM85, MB88] </ref> as a principled approach to deal with the data sparseness problem. Mixture and clustering models for dyadic data have been investigated before under the titles of class-based n-gram models [BdM + 92], distributional clustering [PTL93], and aggregate Markov models [SP97a] in natural language processing.
Reference: [VR75] <author> C. J. Van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <address> London, Boston, </address> <year> 1975. </year>
Reference-contexts: This is similar to the dimension-reduction approach pursued in standard LSI [DTGL90]. We tested the PLSI method on a number of medium-sized standard document test collection with relevance assessments by computing precision-recall curves. The precision-recall curve reported in the sequel have been obtained by so-called macro averaging (cf. <ref> [VR75, Chapter7] </ref>). For each query q we determine n-best lists from which precision/recall pairs are computed for every n.
Reference: [WB91] <author> I.H. Witten and T.C. Bell. </author> <title> The zero-frequency problem: estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(4) </volume> <pages> 1085-1094, </pages> <year> 1991. </year>
Reference-contexts: For object sets with a nominal scale the empirical co-occurrence frequencies are sufficient statistics, capturing all we know about the data. However, the intrinsic difficulty in modeling dyadic data is the data sparseness, also known as the zero frequency problem <ref> [Goo53, Goo65, Kat87, WB91] </ref>. When the product set X fi Y is very large, a majority of pairs (x; y) only have a small probability of being observed in a given sample. Most of the empirical frequencies are typically zero or at least significantly 1 corrupted by sampling noise.
Reference: [Whi87] <author> J. Whittaker. </author> <title> Graphical Models in Applied Multivariate Statistics. </title> <publisher> John Wiley & Son, </publisher> <address> Chichester, </address> <year> 1987. </year>
Reference-contexts: There are further possibilities to weaken the conditional independence assumption both for the aspect and the clustering model, e.g., by substituting the class-conditional multinomial distributions P (yjc) by log-linear Markov models <ref> [WL90, Whi87] </ref> or tree dependency models [MJ98]. This direction, however, is not further pursued in this paper. 2.3 Two-sided Clustering Model Model Specification In the two-sided clustering model, the latent structure consists of cluster partitionings c and d defined over X and Y, respectively.
Reference: [Wil88] <author> P. Willett. </author> <title> Recent trends in hierarchical document clustering: a critical review. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 24(5) </volume> <pages> 577-597, </pages> <year> 1988. </year>
Reference-contexts: In this paragraph, we will focus on an application of the hierarchical clustering model for structuring large text repositories. Clustering of documents provides a popular way of pre-structuring a database that has been applied with mixed success in the context of query-based retrieval (cf. <ref> [Wil88] </ref> for an overview), but is of great importance in interactive retrieval.
Reference: [WL90] <author> N. Wermuth and S.L. Lauritzen. </author> <title> On substantive research hypotheses, conditional independence graphs and graphical chain models. </title> <journal> Journal of the Royal Statistical Society Series B-Methodological, </journal> <year> 1990. </year> <month> 33 </month>
Reference-contexts: There are further possibilities to weaken the conditional independence assumption both for the aspect and the clustering model, e.g., by substituting the class-conditional multinomial distributions P (yjc) by log-linear Markov models <ref> [WL90, Whi87] </ref> or tree dependency models [MJ98]. This direction, however, is not further pursued in this paper. 2.3 Two-sided Clustering Model Model Specification In the two-sided clustering model, the latent structure consists of cluster partitionings c and d defined over X and Y, respectively.
References-found: 67

