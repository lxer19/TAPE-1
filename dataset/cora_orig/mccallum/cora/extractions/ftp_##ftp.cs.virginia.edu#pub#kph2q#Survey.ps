URL: ftp://ftp.cs.virginia.edu/pub/kph2q/Survey.ps
Refering-URL: http://www.cs.virginia.edu/~kph2q/props.html
Root-URL: http://www.cs.virginia.edu
Keyword: Spatial input, virtual reality, 3D interaction, two-handed input, ergonomics of virtual manipulation, haptic input  
Web: http://uvacs.cs.virginia.edu/~kph2q/.  
Note: An extended and annotated version of the references list for this paper is available on-line through mosaic at address  
Abstract: We present a survey of design issues for developing effective free-space three-dimensional (3D) user interfaces. Our survey is based upon previous work in 3D interaction, our experience in developing free-space interfaces, and our informal observations of test users. We illustrate our design issues using examples drawn from instances of 3D interfaces. For example, our first issue suggests that users have difficulty understanding three-dimensional space. We offer a set of strategies which may help users to better perceive a 3D virtual environment, including the use of spatial references, relative gesture, two-handed interaction, multisensory feedback, physical constraints, and head tracking. We describe interfaces which employ these strategies. Our major contribution is the synthesis of many scattered results, observations, and examples into a common framework. This framework should serve as a guide to researchers or systems builders who may not be familiar with design issues in spatial input. Where appropriate, we also try to identify areas in free-space 3D interaction which we see as likely candidates for additional research. 
Abstract-found: 1
Intro-found: 1
Reference: <institution> An expanded and annotated version of the following list of references is available on-line through mosaic at address http://uvacs.cs.virginia.edu/~kph2q/. The document Spatial.bib is also available via anonymous ftp from uvacs.cs.virginia.edu (128.143.8.100) in the pub/kph2q/ directory. </institution> <note> Submissions of new material or corrections to the bibliography are encouraged and may be mailed to kph2q@virginia.edu. Please make the subject line read Spatial.bib contribution. </note>
Reference: 1. <author> Adelstein, B., Johnston, E., Ellis, S., </author> <title> A Testbed for Characterizing Dynamic Response of Virtual Environment Spatial Sensors, </title> <booktitle> UIST92, </booktitle> <pages> 15-22. </pages>
Reference: 2. <author> Badler, N., Manoochehri, K., Baraff, D. </author> <title> MultiDimensional Input Techniques and Articulated Figure Positioning by Multiple Constraints, </title> <booktitle> ACM Workshop on Interactive 3D Graphics, </booktitle> <year> 1986, </year> <pages> pp. 151-170. </pages>
Reference-contexts: Rather, the designer should consider these issues as a set of approaches which might be applied to a given design problem. We now further explain these issues using examples drawn from instances of 3D interfaces. 1.1: Spatial references Badler <ref> [2] </ref> describes an interface where a stylus is used to control the position of a virtual camera. One version of the interface allows the user to indicate the desired view of an imaginary object using the stylus.
Reference: 3. <author> T. Baudel, M. Beaudouin-Lafon, Charade: </author> <title> Remote Control of Objects Using Hand Gestures, </title> <journal> Communications of the ACM, </journal> <volume> 36 (7), </volume> <year> 1993, </year> <pages> 28-35. </pages>
Reference: 4. <author> Bergman, L., Fuchs, H., Grant, E., </author> <title> Image Rendering by Adaptive Refinement, </title> <journal> Computer Graphics, </journal> <volume> 20 (4), </volume> <year> 1986, </year> <pages> pp. 29-37. </pages>
Reference: 5. <author> Bier, E. A., Stone, M. C., </author> <title> Snap-Dragging, </title> <journal> Computer Graphics, </journal> <volume> 20 (4), </volume> <year> 1986, </year> <pages> pp. 233-240. </pages>
Reference-contexts: But to perform useful work in the context of a complex application such as a document editor, direct manipulation often needs to be constrained by techniques such as gridding or snap-dragging <ref> [5] </ref>. Corresponding three-dimensional constraint techniques and feedback mechanisms need to be developed for use in spatial interfaces. Users may have difficulty controlling an interface which requires simultaneous, precise control of an objects position and orientation.
Reference: 6. <author> Bier, E. A., </author> <title> Snap-Dragging In Three Dimensions, </title> <booktitle> Proc. 1990 Symposium on Interactive 3D Graphics, Computer Graphics, </booktitle> <volume> 24 (2), </volume> <pages> pp. 193-204. </pages>
Reference-contexts: If it is necessary to select points on objects which are inside of or behind other objects in the scene, the ray casting can be augmented with a mechanism for cycling through the set of all ray-object intersection points. For disconnected 3D points, 3D snap-dragging techniques <ref> [6] </ref> can be used if the disconnected points are related to existing objects in the scene. If the disconnected points are on the interior of objects, ray casting can be combined with a cutting plane operator, which is used to expose the interior of the objects [35][43].
Reference: 7. <author> Bier, E., Stone, M., Pier, K., Buxton, W., DeRose, T., Toolglass and Magic Lenses: </author> <title> The See-Through Interface, </title> <booktitle> SIGGRAPH 93, </booktitle> <pages> pp. 73-80. </pages>
Reference-contexts: Other example uses of transparency to aid target acquisition include use of a 3D cone for object selection [43], use of a semi-transparent plane for selecting cross-sections of a polygonal brain [35], and use of a semi-transparent tool sheet in the Toolglass interface <ref> [7] </ref>. 4.2: Ray casting vs. direct positioning in 3D Perhaps the most obvious way to implement point selection is to base it on the (x, y, z) position of the tracker, but in many circumstances 3D ray casting may be a superior strategy for selecting 3D points.
Reference: 8. <author> Bolt, R., Put-That-There: </author> <title> Voice and Gesture at the Graphics Interface, </title> <booktitle> SIGGRAPH 80, </booktitle> <pages> 262-70. </pages>
Reference: 9. <editor> Bolt, R. A., Herranz, E., </editor> <booktitle> Two-Handed Gesture in Multi-Modal Natural Dialog, UIST 92, </booktitle> <pages> pp. 7-13. </pages>
Reference: 10. <author> Brooks, F. P. Jr., </author> <title> Walkthrough--a Dynamic Graphics System for Simulating Virtual Buildings, </title> <booktitle> Proc. ACM Workshop on Interactive 3D Graphics, </booktitle> <year> 1986, </year> <pages> pp. 9-21. </pages>
Reference-contexts: Ware found ying to be good for navigating through an interior, but poor for moving around a closed object [65]. Special cases of ying include the car driving metaphor, as well as the locomotion metaphor, which requires the user to phys ically walk through the scene <ref> [10] </ref>. We add a fourth metaphor: Ray casting metaphor: The user indicates a target by casting a ray or cone into the 3D scene. The metaphor can be used for object selection [43] as well as navigation [44].
Reference: 11. <author> Brooks, F., </author> <title> Grasping Reality Through Illusion: Interactive Graphics Serving Science, </title> <publisher> CHI88. </publisher>
Reference-contexts: Brooks offers many insightful observations about 3D interfaces in his 1988 SIGCHI plenary address <ref> [11] </ref>. Our hope is to supplement Brookss observations with some additional issues which are described in the literature and which we have experienced in our research. <p> Ivan Sutherland suggested this distinction between understanding 3D and experiencing 3D in the Fall of 1993. Also, Fred Brooks included this idea in his 1988 review paper where he observes that 3D understanding is difficult <ref> [11] </ref>. Spatial references Relative gesture vs. absolute gesture Two-handed interaction Multisensory feedback Physical constraints Head tracking techniques We do not wish to suggest that all spatial interfaces must consider all these issues to be usable. <p> Psychologist J. J. Gibson has long argued that information from a variety of feedback channels is crucial to our understanding of space [30]. Brooks <ref> [11] </ref> discusses interfaces which employ multisensory feedback techniques, including force feedback [12][36][46], space exclusion (collision detection), and supporting auditory feedback. To these techniques we add physical manipulation of tools with mass. <p> The selection of an appropriate control metaphor is very important: the users ability to perform 3D tasks intuitively, or to perform certain 3D tasks at all, can depend heavily on the types of manipulation which the control metaphor affords. Brooks addresses this issue under the heading metaphor matters <ref> [11] </ref>. 4: Issues in dynamic target acquisition The term dynamic target acquisition refers to target selection tasks such as 3D point selection, object translation, object selection, and docking.
Reference: 12. <author> Brooks, F., Ouh-Young, M., Batter, J., Kilpatrick, P., </author> <title> Project GROPE--Haptic Displays for Scien-tific Visualization, </title> <journal> Comp. Graph. </journal> <volume> 24 (4), </volume> <year> 1990. </year>
Reference-contexts: Even in the real world, we typically break down 6DoF tasks, such as docking, into two sub tasks: translating to the location and then matching orientations <ref> [12] </ref>. The design hurdle is this: provide an interface which effectively integrates rapid, imprecise, multiple degree-of-freedom object placement with slower, but more precise object placement, while providing feedback that makes it all comprehensible.
Reference: 13. <author> Bryson, S., Levit, C., </author> <title> The Virtual Wind Tunnel, </title> <journal> IEEE CG&A, </journal> <month> July </month> <year> 1992, </year> <pages> pp. 25-34. </pages>
Reference: 14. <author> Butterworth, J., Davidson, A., Hench, S., Olano, T. M., 3DM: </author> <title> A Three Dimensional Modeler Using a Head-mounted Display, </title> <booktitle> Proc. 1992 Symp. on Interactive 3D Graphics, </booktitle> <pages> pp. 135-138. </pages>
Reference-contexts: PREVIOUS WORK Previous work in spatial interaction consists largely of two avors: single applications built for users with specialized tasks, and formal user studies which analyze individual phenomena in isolation. Example applications include the 3DM three-dimensional modeler <ref> [14] </ref>, Ostbys system for describing and modifying free-form surfaces [49], or Sachss 3-Draw computer-aided design tool [54]. Example formal user studies include Jacobs multidimensional input experiment [37] or various studies of 3D point selection [62][64].
Reference: 15. <author> Buxton, W., Myers, B., </author> <title> A Study in Two-Handed Input, </title> <booktitle> CHI86, </booktitle> <pages> pp. 321-326. </pages>
Reference-contexts: virtual object, the userss tactile and kinesthetic feedback reinforce the visual illusion, but such correspondence is desirable, rather than strictly necessary. 1.3: Two-handed interaction Two-handed input has often been viewed as a technique to improve the efficiency of human-computer interaction, by enabling the user to perform two sub-tasks in parallel <ref> [15] </ref>, rather than as sequentially selected modes. When interacting in three dimensions, we find that using two hands not only improves efficiency, but can also help to make spatial input comprehensible to the user. <p> Our informal observation of several hundred test users of a two-handed spatial interface for neurosurgical visualization [35] strengthens and reaffirms Sachss observation: we find that most test users can operate the two-handed interface effectively within their first minute of use. This also reinforces findings by Buxton <ref> [15] </ref> and Kabbash [39] that users can transfer everyday skills for manipulating tools with two hands to the operation of a computer, with little or no training. Even when manipulating just a single object in 3D, using two hands can be useful and natural.
Reference: 16. <author> Card, S., Mackinlay, J., Robertson, G., </author> <title> The Design Space of Input Devices, </title> <booktitle> CHI89, </booktitle> <pages> 117-124. </pages>
Reference: 17. <author> Card, S., Robertson, G., Mackinlay, J., </author> <title> The Information Visualizer, an Information Workspace, </title> <booktitle> CHI91, </booktitle> <pages> pp. 181-187. </pages>
Reference: 18. <author> Chung, J. C., </author> <title> A comparison of Head-tracked and Non-head-tracked Steering Modes in the Targeting of Radiotherapy Treatment Beams, </title> <booktitle> Proc. 1992 Symp. on Interactive 3D Graphics, </booktitle> <pages> 193-196. </pages>
Reference-contexts: JDCAD, for example, uses this strategy [43] to bring the 3D cursor to the center of the visible volume. Ratcheting: Many spatial interfaces (e.g. <ref> [18] </ref>, [64]) utilize the notion of ratcheting, which allows the user to perform movements in a series of grab-release cycles. (The user presses a clutch button, moves the input device, releases the clutch button, returns his or her hand to a comfortable position, and repeats the process). <p> As an example clutching mechanism, the University of North Carolina has constructed an input device which consists of a 3D tracker encased in a pool ball, which has a clutch button mounted on its surface <ref> [18] </ref>. When the user holds the clutch button down, the virtual object follows movements of the pool ball, and when the button is released, movement of the pool ball has no effect.
Reference: 19. <author> Cohen, P., Sullivan, J., </author> <title> Synergistic Use of Direct Manipulation and Natural Language, </title> <booktitle> CHI89, </booktitle> <pages> pp. 227-233. </pages>
Reference: 20. <author> Conner, D., Snibbe, S., Herndon, K., Robbins, D., Zeleznik, R., van Dam, A., </author> <title> Three-Dimensional Widgets, </title> <booktitle> 1992 Symp. on Int. 3D Graph, </booktitle> <pages> 183-188. </pages>
Reference-contexts: For example, a screwdriver affords rotation about its vertical axis while a wrench affords rotation about a horizontal axis. This type of haptic feedback would not possible if the rotational constraints were purely visual, as is the case with graphical 3D widgets <ref> [20] </ref>. 1.5: Physical constraints and affordances Physical constraints and affordances are widely used in industrial design (Norman [48] provides many examples) and we believe spatial interfaces can take advantage of these physical properties of objects.
Reference: 21. <author> Cruz-Neira, C., Sandin, D., DeFanti, T., </author> <title> Surround-Screen Projection-Based Virtual Reality: </title> <booktitle> The Design and Implementation of the CAVE, SIGGRAPH 93, </booktitle> <pages> pp. 135-142. </pages>
Reference: 22. <author> M. Deering, </author> <title> High Resolution Virtual Reality, </title> <journal> Computer Graphics, </journal> <volume> 26 (2), </volume> <pages> pp. 195-202. </pages>
Reference: 23. <author> Feiner, S., MacIntyre, B., Haupt, M., Solomon, E., </author> <title> Windows on the World: 2D Windows for 3D Augmented Reality, </title> <booktitle> UIST93, </booktitle> <pages> pp. 145-155. </pages>
Reference: 24. <author> Feiner, S., Macintyre, B., Seligmann, D., </author> <title> Knowlege-Based Augmented Reality, </title> <journal> Comm. of the ACM, </journal> <volume> 36 (7), </volume> <year> 1993, </year> <pages> pp. 53-61. </pages>
Reference: 25. <author> Feiner, S., Shamash, A., </author> <title> Hybrid User Interfaces: Breeding Virtually Bigger Interfaces for Physically Smaller Computers, </title> <booktitle> UIST 91, </booktitle> <pages> pp. 9-17. </pages>
Reference-contexts: Feiners integration of a 3D augmented reality head-mounted display with a standard 2D desktop display <ref> [25] </ref> offers one of the few examples of which we are aware. 9: Clutching mechanisms Most spatial interfaces incorporate some type of clutching mechanism, that is, a software mode which allows the spatial input device to be moved without affecting the 3D cursor.
Reference: 26. <author> S.S. Fisher, M. McGreevy, J. Humphries, W. Robinett, </author> <title> Virtual Interface Environment for Telepresence Applications, </title> <booktitle> Oct. 1988, Proc. Human Factors Society 32nd Annual Meeting. </booktitle>
Reference: 27. <author> Fitzmaurice, G. W., </author> <title> Situated Information Spaces and Spatially Aware Palmtop Computers, </title> <journal> Comm. of the ACM, </journal> <volume> 36 (7), </volume> <year> 1993, </year> <pages> pp. 39-49. </pages>
Reference: 28. <author> Foley, J. D., Wallace, V., Chan, P., </author> <title> The Human Factors of Computer Graphics Interaction Techniques, </title> <journal> IEEE CG&A, </journal> <month> Nov. </month> <year> 1984, </year> <pages> pp. 13-48. </pages>
Reference: 29. <author> Galyean, T. A., Hughes, J. F., </author> <title> Sculpting: An Interactive Volumetric Modeling Technique, </title> <journal> Computer Graphics, </journal> <volume> 25 (4), </volume> <pages> pp. 267-274. </pages>
Reference-contexts: Ostby reported that [locating] a desired point or area [is] much easier when a real object is sitting on the Polhe-muss digitizing surface. 1.2: Relative gesture vs. absolute gesture In Galyeans 3D sculpting interface <ref> [29] </ref>, the user deforms a 3D model by positioning a single tracker in an absolute, fixed volume in front of a monitor. This leads to an interface which is not entirely intuitive. Galyean reports that controlling the tool position is not easy.
Reference: 30. <author> Gibson, J., </author> <title> The Ecological Approach to Visual Perception. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: Psychologist J. J. Gibson has long argued that information from a variety of feedback channels is crucial to our understanding of space <ref> [30] </ref>. Brooks [11] discusses interfaces which employ multisensory feedback techniques, including force feedback [12][36][46], space exclusion (collision detection), and supporting auditory feedback. To these techniques we add physical manipulation of tools with mass.
Reference: 31. <author> Gleicher, M., </author> <title> Supporting Numerical Computations in Interactive Contexts, Graphics Interface 93. </title>
Reference-contexts: As Stu Card has commented, a major challenge of the post-WIMP interface is to find and characterize appropriate mappings from high degree-of-freedom input devices to high degree-of-freedom input tasks. Applications such as 3-Draw [54] and abstractions such as Gleichers snap-together math <ref> [31] </ref> make good initial progress toward providing constrained input in 3D, but we believe the general spatial input constraint problem, and the issue of providing appropriate feedback in particular, is still a challenging area for future research. 7: Dynamics and size of the working volume of the users hands Guiards observations
Reference: 32. <author> Guiard, Y., </author> <title> Asymmetric Division of Labor in Human Skilled Bimanual Action: The Kinematic Chain as a Model, </title> <journal> The Journal of Motor Behavior, </journal> <volume> 19 (4), </volume> <year> 1987, </year> <pages> pp. 486-517. </pages>
Reference-contexts: Using two hands can also offer other practical advantages: it is often easier to grasp and rotate a spatial input device with two hands, and fatigue may be reduced since the hands can provide mutual physical support. Guiards analysis of human skilled bimanual action <ref> [32] </ref> provides an insightful theoretical framework for hypothesizing which classes of two-handed interfaces might improve performance without inducing additional cognitive load. <p> providing constrained input in 3D, but we believe the general spatial input constraint problem, and the issue of providing appropriate feedback in particular, is still a challenging area for future research. 7: Dynamics and size of the working volume of the users hands Guiards observations of subjects performing writing tasks <ref> [32] </ref> as well as observations of users of our two-handed interface [35] suggest that people tend to move their hands in a surprisingly small working volume. This volume is not only small, but also tends to move over time as the user changes body posture. <p> Guiards analysis of handwriting tasks suggests that the writer tends to define an active volume relative to his or her non-dominant hand. Guiard also reports that the writing speed of adults is reduced by some 20% when instructions prevent the nonpreferred hand from manipulating the page <ref> [32] </ref>. This suggests that users of a spatial interface which requires movements relative to a fixed frame-of-reference in their environment may experience reduced task performance due to cognitive load, fatigue, or both.
Reference: 33. <author> Hauptmann, A. G., </author> <title> Speech and Gestures for Graphic Image Manipulation, </title> <booktitle> CHI89, </booktitle> <pages> 241-245. </pages>
Reference-contexts: Even when manipulating just a single object in 3D, using two hands can be useful and natural. In a classic wizard-of-oz experiment, Hauptmann <ref> [33] </ref> observed test subjects spontaneously using two hands for single-object translation, rota tion, and scaling tasks.
Reference: 34. <author> Herndon, K., Zeleznik, R., Robbins, D., Conner, B., Snibbe, S., van Dam, A., </author> <title> Interactive Shadows, </title> <booktitle> UIST 92, </booktitle> <pages> pp. 1-6. </pages>
Reference-contexts: Extraneous input dimensions should be constrained to some meaningful value. In general, it makes good common sense to exploit task-specific needs to reduce dimensionality. For example, the mouse-based interactive shadows technique <ref> [34] </ref> allows constrained movement in 2D planes within a 3D scene. If the users task consists only of such constrained 2D move ments, this may result in a better interface than free-space 3D positioning.
Reference: 35. <author> Hinckley, K., Pausch, R, Goble, J., Kassell, N., </author> <title> Passive Real-World Interface Props for Neurosurgical Visualization, </title> <booktitle> CHI94, </booktitle> <pages> 452-458. </pages>
Reference-contexts: A spatial interface could instead base its interaction techniques upon relative motion, including motion relative to a spatial reference or the users own body. We have previously described an interface where users can manipulate virtual objects by moving real-world tools or props <ref> [35] </ref> which correspond to the virtual objects, and thus serve as spatial references. Based on our informal observations of test users at various stages of the design, using any spatial reference is better than none. <p> Regarding two-handed interaction in free space, Sachs observes that the simultaneous use of two [spatial input] sensors takes advantage of peoples innate ability--knowing precisely where their hands are relative to each other [54]. Our informal observation of several hundred test users of a two-handed spatial interface for neurosurgical visualization <ref> [35] </ref> strengthens and reaffirms Sachss observation: we find that most test users can operate the two-handed interface effectively within their first minute of use. <p> Other example uses of transparency to aid target acquisition include use of a 3D cone for object selection [43], use of a semi-transparent plane for selecting cross-sections of a polygonal brain <ref> [35] </ref>, and use of a semi-transparent tool sheet in the Toolglass interface [7]. 4.2: Ray casting vs. direct positioning in 3D Perhaps the most obvious way to implement point selection is to base it on the (x, y, z) position of the tracker, but in many circumstances 3D ray casting may <p> Continuous: In some cases recalibration can be made invisible to the user. For example, in a virtual reality system, when the user moves his body or head, the local coordinate system is automatically updated to keep their motions body-centric. Another example is provided by our desk-top system <ref> [35] </ref>, where a tool held in the non-dominant hand is used to define a dynamic frame-of-reference relative to which other tools may be moved with the dominant hand. Based on informal observations of several hundred test users, we find that the technique is natural and intuitive. <p> input constraint problem, and the issue of providing appropriate feedback in particular, is still a challenging area for future research. 7: Dynamics and size of the working volume of the users hands Guiards observations of subjects performing writing tasks [32] as well as observations of users of our two-handed interface <ref> [35] </ref> suggest that people tend to move their hands in a surprisingly small working volume. This volume is not only small, but also tends to move over time as the user changes body posture. <p> Keyboards are especially problematic because they can get in the users way. We have noted that users frequently rest their hands on the desk-top while manipulating spatial interface tools <ref> [35] </ref>; if the keyboard is present, it frequently entangles the cabling for the trackers or otherwise gets in the way. Alternatives include: Voice input: Mouse-activated commands and keyboard hotkeys can be replaced by voice commands. <p> This remains an untested idea, but we have observed neurosurgeons spontaneously reaching out to touch the screen during discussions of our neurosurgical visualiza-tion interface <ref> [35] </ref>, suggesting that surgeons will find touching the screen with spatial interface tools to be nat ural. <p> In our experience, some of the most confounding (for the user) and hard-to-fix (for the implementor) usability problems and ergonomic difficulties can arise due to poor clutch design. For example, we have seen users struggle with many different clutch designs in our two-handed spatial interface <ref> [35] </ref>. In versions of the interface which used more than one clutch (one clutch was provided for each tool), users could operate the interface easily once the operation of the clutches was explained to them, but most users could not infer the operation of the clutches without any instruction. <p> If arbitrary, large-angle rotations are required, the resulting interface can be very awkward. In such cases the clutch button should be separated from the input device. For example, one interface which requires arbitrary rotations uses a foot pedal as a clutch <ref> [35] </ref>, allowing the associated spatial input device to be rotated with ease. If the users task seldom requires arbitrary rotation, it is preferable to mount the clutch button directly on the input device. <p> It may be useful to build time-outs into the system which remind the user to take an occasional break. Also note that, based on our user observations, the posture of users hands while manipulating spatial interface tools <ref> [35] </ref> is not the same as the hand posture required during typing.
Reference: 36. <author> Iwata, H., </author> <title> Artificial Reality with Force-feedback: Development of Desktop Virtual Space with Compact Master Manipulator, </title> <journal> Computer Graphics, </journal> <volume> 24 (4), </volume> <pages> pp. 165-170. </pages>
Reference: 37. <author> Jacob, R., Sibert, L., </author> <title> The Perceptual Structure of Multidimensional Input Device Selection, </title> <booktitle> CHI92, </booktitle> <pages> pp. 211-218. </pages>
Reference-contexts: Example applications include the 3DM three-dimensional modeler [14], Ostbys system for describing and modifying free-form surfaces [49], or Sachss 3-Draw computer-aided design tool [54]. Example formal user studies include Jacobs multidimensional input experiment <ref> [37] </ref> or various studies of 3D point selection [62][64]. Unfortunately, there are few papers which attempt to bridge the gap between these two different types of research results. <p> An additional user study [51] shows performance improvement for a generic search task using an immersive head-tracked, head-mounted display vs. a non-head-tracked display. 2: User perception of multidimensional tasks: related vs. independent input dimensions The Jacob / Sibert study <ref> [37] </ref> compares user performance for two tasks: the first asks the user to match the (x, y, size) of two squares, while the second task requires matching the (x, y, greyscale) of two squares. <p> The underlying design principle, in Jacobs terminology, is that the structure of the perceptual space of an interaction task should mirror that of the control space of its input device <ref> [37] </ref>. This result points away from the standard notion of logical input devices. It may not be enough for the designer to know that a logical task requires the control of three input parameters (u, v, w).
Reference: 38. <author> Kabbash, P., MacKenzie, I. S., Buxton, W., </author> <title> Human Performance Using Computer Input Devices in the Preferred and Non-Preferred Hands, </title> <booktitle> INTERCHI93, </booktitle> <pages> pp. 474-481. </pages>
Reference: 39. <author> Kabbash, P., Buxton, W., Sellen, A., </author> <title> Two-Handed Input in a Compound Task, </title> <booktitle> CHI94, </booktitle> <pages> pp. 417-423. </pages>
Reference-contexts: Our informal observation of several hundred test users of a two-handed spatial interface for neurosurgical visualization [35] strengthens and reaffirms Sachss observation: we find that most test users can operate the two-handed interface effectively within their first minute of use. This also reinforces findings by Buxton [15] and Kabbash <ref> [39] </ref> that users can transfer everyday skills for manipulating tools with two hands to the operation of a computer, with little or no training. Even when manipulating just a single object in 3D, using two hands can be useful and natural. <p> The left hand first positions the paper, then the right hand begins to write. We note, however, that Guiards principles have not been formally demonstrated, and may also represent an incomplete set of conditions for usable two-handed interfaces. For example, Kabbash <ref> [39] </ref> describes a two-handed interface (the palette menu) where the user moves an opaque menu using a trackball in the left hand and a selection cursor using a mouse in the right hand.
Reference: 40. <author> Kaufman, A., Yagel, R., </author> <title> Tools for Interaction in Three Dimensions, </title> <booktitle> Proc. 3rd International Conf. on HCI (Boston, MA), </booktitle> <volume> Vol. 1, </volume> <month> Sept. </month> <year> 1989, </year> <pages> pp. 468-475. </pages>
Reference: 41. <author> Krueger, M., </author> <title> Environmental Technology: Making the Real World Virtual, </title> <journal> Communications of the ACM, </journal> <volume> 36 (7), </volume> <year> 1993, </year> <pages> pp. 36-37. </pages>
Reference: 42. <author> Liang, J., Shaw, C., Green, M., </author> <title> On Temporal-Spatial Realism in the Virtual Reality Environment, </title> <booktitle> UIST91, </booktitle> <pages> pp. 19-25. </pages>
Reference: 43. <author> Liang, J. , Green, M., JDCAD: </author> <title> A Highly Interactive 3D Modeling System, </title> <booktitle> 3rd International Conference on CAD and Computer Graphics, </booktitle> <address> Beijing, China, </address> <month> Aug. </month> <year> 1993, </year> <pages> 217-222. </pages>
Reference-contexts: But as Liang notes, the hand has certain kinematic constraints. For example, it is far more easy to rotate something held by the fingers than to rotate the whole hand itself <ref> [43] </ref>. The mass of the tool can damp instabilities in the users hand motion. For example, surgeons are very particular about the weight of their surgical instruments, as the proper heaviness can help decrease the amplitude of small, involuntary hand tremors. <p> We add a fourth metaphor: Ray casting metaphor: The user indicates a target by casting a ray or cone into the 3D scene. The metaphor can be used for object selection <ref> [43] </ref> as well as navigation [44]. It is not yet clear under which specific circum stances ray casting may prove useful. <p> Zhais work is the first we know of to generalize the benefits of transparent volumes for target acquisition tasks. Other example uses of transparency to aid target acquisition include use of a 3D cone for object selection <ref> [43] </ref>, use of a semi-transparent plane for selecting cross-sections of a polygonal brain [35], and use of a semi-transparent tool sheet in the Toolglass interface [7]. 4.2: Ray casting vs. direct positioning in 3D Perhaps the most obvious way to implement point selection is to base it on the (x, y, <p> Instead of directly specifying the 3D point, the spatial input device is used to shoot a ray into the scene, allowing the user to hold the input device in a comfortable position and rotate it to change the ray direction <ref> [43] </ref>. The 3D points selectable by casting a ray are constrained to lie on the surface of virtual objects in the scene. In many circumstances this is exactly what is desired. <p> Note that spotlighting visual effects afforded by many graphics workstations can provide real-time feedback for this task. We base this strategy on the implementation reported by Liang <ref> [43] </ref>. It is not presently clear if other strategies, such as using ray casting to sweep out a cone, might provide bet-ter results in some cases. 5: Recalibration mechanisms At a low level, all spatial input devices provide the software with an absolute position in a global coordinate frame. <p> We are aware of three basic recalibration strategies: Command-based: The user explicitly triggers a recalibration command, sometimes referred to as a centering command or a homing command. JDCAD, for example, uses this strategy <ref> [43] </ref> to bring the 3D cursor to the center of the visible volume.
Reference: 44. <author> Mackinlay, J., Card, S., Robertson, G., </author> <title> Rapid Controlled Movement Through a Virtual 3D Workspace, </title> <journal> Comp. Grap., </journal> <volume> 24 (4), </volume> <year> 1990, </year> <pages> 171-176. </pages>
Reference-contexts: We add a fourth metaphor: Ray casting metaphor: The user indicates a target by casting a ray or cone into the 3D scene. The metaphor can be used for object selection [43] as well as navigation <ref> [44] </ref>. It is not yet clear under which specific circum stances ray casting may prove useful.
Reference: 45. <author> McKenna, M., </author> <title> Interactive Viewpoint Control and Three-dimensional Operations, </title> <booktitle> Proc. 1992 Symposium on Interactive 3D Graphics, </booktitle> <pages> pp. 53-56. </pages>
Reference: 46. <author> Minsky, M., Ouh-young, M., Brooks, F. P., Behensky, M., </author> <title> Feeling and Seeing: Issues in Force Display, </title> <journal> Comp. Graph., </journal> <volume> 24 (2), </volume> <pages> 234-244. </pages>
Reference: 47. <author> Nielsen, J., </author> <title> Noncommand User Interfaces, </title> <journal> Communications of the ACM, </journal> <volume> 36 (4), </volume> <pages> pp. 83-99. </pages>
Reference-contexts: We reference some of Brookss observations, but the reader should be aware that many important issues presented in Brookss paper are not covered by the present survey. Nielsens discussion of noncommand user interfaces <ref> [47] </ref> covers similar ground, but the scope of Nielsens work is much broader than this survey. Nielsens goal is to describe trends in advanced interface design, while by contrast, our goal is to discuss design issues in one class of advanced interfaces, those that employ 3D free-space input.
Reference: 48. <author> Norman D., </author> <title> The Design of Everyday Things. </title> <publisher> Doubleday: </publisher> <address> New York, New York, </address> <year> 1990. </year>
Reference-contexts: This type of haptic feedback would not possible if the rotational constraints were purely visual, as is the case with graphical 3D widgets [20]. 1.5: Physical constraints and affordances Physical constraints and affordances are widely used in industrial design (Norman <ref> [48] </ref> provides many examples) and we believe spatial interfaces can take advantage of these physical properties of objects. Software constraints are often useful, but they do have limitations: the user must understand the constraints and their feedback, which may impose a small cognitive load.
Reference: 49. <author> Ostby, E., </author> <title> Describing Free-Form 3D Surfaces for Animation, </title> <booktitle> Proc. ACM Workshop on Interactive 3D Graphics, </booktitle> <month> Oct. </month> <year> 1986, </year> <pages> pp. 251-258. </pages>
Reference-contexts: PREVIOUS WORK Previous work in spatial interaction consists largely of two avors: single applications built for users with specialized tasks, and formal user studies which analyze individual phenomena in isolation. Example applications include the 3DM three-dimensional modeler [14], Ostbys system for describing and modifying free-form surfaces <ref> [49] </ref>, or Sachss 3-Draw computer-aided design tool [54]. Example formal user studies include Jacobs multidimensional input experiment [37] or various studies of 3D point selection [62][64]. Unfortunately, there are few papers which attempt to bridge the gap between these two different types of research results. <p> In 3D, using a spatial reference (such as Badlers plastic spaceship) is one way to provide this perceptual experience. More precisely, we define a spatial reference as a real-world object relative to which the user can gesture when interacting in 3D. Ostbys system for manipulating surface patches <ref> [49] </ref> was a second early system to note the importance of spatial references. <p> Digitizing points on the surface of a real object is an instance where ray casting may not be helpful. In this case, the real object provides a spatial reference for the user as well as physical support of the hand; as a result, direct 3D point selection works well <ref> [49] </ref>. 4.3: Cone casting vs. ray casting For gross object selection, ray casting may become less appropriate, especially if the object may be distant.
Reference: 50. <author> Pausch, R., </author> <title> Support for Rapid Prototyping of Two-and Three-Dimensional User Interfaces, Proposal for ARPA BAA 93-42. </title> <institution> Comp. Science Department, University of Virginia, </institution> <month> March, </month> <year> 1994. </year>
Reference-contexts: For example, during informal user observations of a virtual reality interface, we have noted that users of two-handed interaction are less likely to become disoriented versus users who interact with only one hand <ref> [50] </ref>. Enabling the use of both hands can allow users to ground themselves in the interaction space; in essence the users own body becomes a spatial reference. <p> The general issue of constructing hybrid interfaces which combine 2D and 3D interaction in a unified framework (both in terms of user interaction, and from the standpoint of support provided by user interface toolkits <ref> [50] </ref>) remains largely unexplored.
Reference: 51. <author> Pausch, R., Shackelford, M. A., Proffitt, D., </author> <title> A User Study Comparing Head-Mounted and Stationary Displays, </title> <booktitle> Proc. IEEE Symposium on Research Frontiers in Virtual Reality, </booktitle> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: We merely note head tracking as a technique for spatial feedback; previous research [45][22][66][43] discusses the advantages of head tracking and the implementation details. An additional user study <ref> [51] </ref> shows performance improvement for a generic search task using an immersive head-tracked, head-mounted display vs. a non-head-tracked display. 2: User perception of multidimensional tasks: related vs. independent input dimensions The Jacob / Sibert study [37] compares user performance for two tasks: the first asks the user to match the (x,
Reference: 52. <author> Pixsys Inc., </author> <booktitle> 3522 22nd St., </booktitle> <address> Boulder, </address> <publisher> CO 80304. </publisher> <pages> (303) 443-0771. </pages>
Reference: 53. <institution> Polhemus Navigation Sciences, Inc., </institution> <address> P. O. Box 560, Colchester, VT 05446. </address> <pages> (802) 655-3159. </pages>
Reference: 54. <author> Sachs, E., Roberts, A., Stoops, D., 3-Draw: </author> <title> A Tool for Designing 3D Shapes, </title> <journal> IEEE Computer Graphics & Applications, </journal> <month> Nov. </month> <year> 1991, </year> <pages> pp. 18-26. </pages>
Reference-contexts: Example applications include the 3DM three-dimensional modeler [14], Ostbys system for describing and modifying free-form surfaces [49], or Sachss 3-Draw computer-aided design tool <ref> [54] </ref>. Example formal user studies include Jacobs multidimensional input experiment [37] or various studies of 3D point selection [62][64]. Unfortunately, there are few papers which attempt to bridge the gap between these two different types of research results. <p> Even though the Polhemus pointer is held in a well-defined region, it is often difficult to correlate the position of the pointer in space with the position of the tool on the screen. Compare this to Sachss 3-Draw computer-aided design tool <ref> [54] </ref>, which allows the user to hold a stylus in one hand and a palette in the other (both objects are tracked by the computer). These tools serve to draw and view a 3D virtual object which is seen on a desktop monitor. <p> As Sachs notes, users require far less concentration to manipulate objects relative to each other than if one object were fixed absolutely in space while a single input sensor controlled the other <ref> [54] </ref>. Thus, users may have trouble moving in a fixed, absolute coordinate frame. A spatial interface could instead base its interaction techniques upon relative motion, including motion relative to a spatial reference or the users own body. <p> Regarding two-handed interaction in free space, Sachs observes that the simultaneous use of two [spatial input] sensors takes advantage of peoples innate ability--knowing precisely where their hands are relative to each other <ref> [54] </ref>. Our informal observation of several hundred test users of a two-handed spatial interface for neurosurgical visualization [35] strengthens and reaffirms Sachss observation: we find that most test users can operate the two-handed interface effectively within their first minute of use. <p> As Stu Card has commented, a major challenge of the post-WIMP interface is to find and characterize appropriate mappings from high degree-of-freedom input devices to high degree-of-freedom input tasks. Applications such as 3-Draw <ref> [54] </ref> and abstractions such as Gleichers snap-together math [31] make good initial progress toward providing constrained input in 3D, but we believe the general spatial input constraint problem, and the issue of providing appropriate feedback in particular, is still a challenging area for future research. 7: Dynamics and size of the
Reference: 55. <author> Schmandt, C. M., </author> <title> Spatial Input/Display Correspondence in a Stereoscopic Computer Graphic Work Station, </title> <journal> Computer Graphics, </journal> <volume> 17 (3), </volume> <year> 1983, </year> <pages> pp. 253-262. </pages>
Reference-contexts: Using physical constraints can remove this cognitive load and also lends support: users can try configurations of objects by moving their hands until they hit something. For example, Schmandt describes an interface for entering multiple layers of VLSI circuit design data in a 3D stereoscopic work space <ref> [55] </ref>. The user enters the data by pressing a stylus on a stationary 2D tablet; the user can adjust the depth of the image so that the desired plane-of-depth lines up with the 2D tablet.
Reference: 56. <author> Sears, A.,Plaisant, C., Shneiderman, B., </author> <title> A New Era for High Precision Touchscreens, in Advances in Human-Computer Interaction, Hartson, Hix, </title> <editor> eds., </editor> <volume> Vol. 3, </volume> <year> 1992, </year> <pages> pp. 1-33. </pages>
Reference-contexts: Alternatives include: Voice input: Mouse-activated commands and keyboard hotkeys can be replaced by voice commands. Touchscreen: A touchscreen could be also used for command selection, but might furthermore allow the user to perform 2D direct manipulation tasks <ref> [56] </ref>. Note the facility with which a touchscreen can be utilized: users can touch the screen directly with their spatial input devices, instead of putting them down to use a mouse.
Reference: 57. <author> Shepard, R. N., Metzler, J., </author> <title> Mental Rotation of Three-Dimensional Objects, </title> <journal> Science, </journal> <volume> Vol. 171, </volume> <year> 1971, </year> <pages> pp. 701-703. </pages>
Reference-contexts: For example, the Shep-ard-Metzler mental rotation study <ref> [57] </ref> suggests that for some classes of objects, we must mentally envision a rigid body transformation on the object to understand how it will look from different viewpoints; that is, we must perceive the motion to understand the effect of the transformation.
Reference: 58. <editor> Spaceball Technologies, </editor> <publisher> Inc. </publisher> <pages> (508) 970-0330. </pages>
Reference-contexts: INTRODUCTION The term spatial input refers to interfaces based upon free-space 3D input technologies such as camera-based or magnetic trackers [52][53], as opposed to desktop devices such as the mouse or the Spaceball <ref> [58] </ref>. In the literature, a wide variety of interfaces for manipulating three-dimensional objects have been described as 3D interfaces, but we find it useful to use the term spatial input to distinguish the class of 3D interfaces based upon free-space interaction.
Reference: 59. <author> Stoakley, R., Pausch, R., </author> <title> Virtual Kit of Parts, </title> <type> unpublished manuscript, </type> <note> available through mosaic at http://uvacs.cs.virginia.edu/~rws2v/plinth.html. </note>
Reference-contexts: For example, we use a clipboard (held in the non-dominant hand) and a stylus (held in the dominant hand) in a virtual reality application which allows the user to edit the architectural layout of the room they are standing in <ref> [59] </ref>. The stylus is used to edit a miniature model of the room, which is seen on the virtual counterpart of the real-world clipboard.
Reference: 60. <author> Sturman, D., Zeltzer, D., Pieper, S., </author> <title> Hands-On Interaction with Virtual Environments, </title> <booktitle> UIST89, </booktitle> <pages> pp. 19-24. </pages>
Reference: 61. <author> I. E. Sutherland, </author> <title> A Head-mounted Three Dimensional Display, </title> <booktitle> Proc. the Fall Joint Computer Conference, </booktitle> <year> 1968, </year> <pages> pp. 757-764. </pages>
Reference: 62. <author> Takemura, H., Tomono, A., Kayashi, Y., </author> <title> An Evaluation of 3-D Object Pointing Using a Field Sequential Stereoscopic Display, </title> <booktitle> Proc. Graphics Interface 88, </booktitle> <month> June </month> <year> 1988, </year> <pages> pp. 112-118. </pages>
Reference: 63. <author> Taylor, R., Robinett, W., Chi, V., Brooks, F., Wright, W., Williams, R., Snyder, E., </author> <note> The Nanomanipulator: </note>
References-found: 64

