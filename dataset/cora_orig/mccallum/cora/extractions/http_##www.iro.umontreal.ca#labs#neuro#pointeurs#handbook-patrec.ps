URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/handbook-patrec.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/other.html
Root-URL: http://www.iro.umontreal.ca
Email: yann@research.att.com bengioy@iro.umontreal.ca  email: yann@research.att.com  
Phone: phone: 908-949-4038, fax: 908-949-7322  
Title: Pattern Recognition and Neural Networks  RUNNING HEAD: Pattern Recognition and Neural Networks Correspondence:  
Author: Yann LeCun Yoshua Bengio Yann LeCun 
Address: 101 Crawfords Corner Road Operationnelle, Universite de Montreal, Holmdel, NJ 07733 Montreal, Qc, Canada, H3C-3J7  101 Crawfords Corner Road Holmdel, NJ 07733  
Affiliation: AT&T Bell Laboratories Dept. Informatique et Recherche  AT&T Bell Laboratories,  
Pubnum: Rm 4G332,  Rm 4G332,  
Abstract-found: 0
Intro-found: 1
Reference: <author> Bengio, Y., , Simard, P., and Frasconi, P. </author> <year> (1994). </year> <title> Learning Long-Term Dependencies with Gradient Descent is Difficult. </title> <journal> IEEE Transactions on Neural Networks. Special Issue on Recurrent Neural Networks, </journal> <month> March 94. </month>
Reference-contexts: However, theoretical and practical hurdles <ref> (Bengio et al., 1994) </ref> limit the span of long-term dependencies that can be learned efficiently. 5 RECOGNITION OF COMPOSITE OBJECTS In many real applications the difficulty is not only to recognize individual objects but also to separate them from context or background. <p> Simultaneous training of such hybrids has been reported to yield large reductions in error rates over independent training of the modules in speech recognition (TDNN/dynamic time warping (Driancourt, Bottou and Gallinari, 1991; Haffner, Franzini and Waibel, 1991), TDNN/HMM (Bengio et al., 1992)), and in on-line handwriting recognition (SDNN/HMM <ref> (Bengio, LeCun and Henderson, 1994) </ref>). LeCun & Bengio: Pattern Recognition and Neural Nets 19 6 DISCUSSION Neural Networks, particularly multi-layer back-propagation NNs, provide simple, yet powerful and general methods for synthesizing classifiers with minimal effort. However, most practical systems combine NNs with other techniques for pre- and post-processing.
Reference: <author> Bengio, Y., LeCun, Y., and Henderson, D. </author> <year> (1994). </year> <title> Globally Trained Handwritten Word Rec-ognizer using Spatial Representation, Space Displacement Neural Networks and Hidden Markov Models. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6, </volume> <pages> pages 937-944. </pages>
Reference-contexts: However, theoretical and practical hurdles <ref> (Bengio et al., 1994) </ref> limit the span of long-term dependencies that can be learned efficiently. 5 RECOGNITION OF COMPOSITE OBJECTS In many real applications the difficulty is not only to recognize individual objects but also to separate them from context or background. <p> Simultaneous training of such hybrids has been reported to yield large reductions in error rates over independent training of the modules in speech recognition (TDNN/dynamic time warping (Driancourt, Bottou and Gallinari, 1991; Haffner, Franzini and Waibel, 1991), TDNN/HMM (Bengio et al., 1992)), and in on-line handwriting recognition (SDNN/HMM <ref> (Bengio, LeCun and Henderson, 1994) </ref>). LeCun & Bengio: Pattern Recognition and Neural Nets 19 6 DISCUSSION Neural Networks, particularly multi-layer back-propagation NNs, provide simple, yet powerful and general methods for synthesizing classifiers with minimal effort. However, most practical systems combine NNs with other techniques for pre- and post-processing.
Reference: <author> Bengio, Y., Mori, R. D., Flammia, G., and Kompe, R. </author> <year> (1992). </year> <title> Global Optimization of a Neural Network-Hidden Markov Model Hybrid. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(2) </volume> <pages> 252-259. </pages>
Reference-contexts: Simultaneous training of such hybrids has been reported to yield large reductions in error rates over independent training of the modules in speech recognition (TDNN/dynamic time warping (Driancourt, Bottou and Gallinari, 1991; Haffner, Franzini and Waibel, 1991), TDNN/HMM <ref> (Bengio et al., 1992) </ref>), and in on-line handwriting recognition (SDNN/HMM (Bengio, LeCun and Henderson, 1994)). LeCun & Bengio: Pattern Recognition and Neural Nets 19 6 DISCUSSION Neural Networks, particularly multi-layer back-propagation NNs, provide simple, yet powerful and general methods for synthesizing classifiers with minimal effort.
Reference: <author> Boser, B., Guyon, I., and Vapnik, V. </author> <year> (1992). </year> <title> An algorithm for optimal margin classifiers. </title> <booktitle> In Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-152, </pages> <address> Pittsburgh. </address>
Reference-contexts: classifiers rests on the fact that, if the w i in equation 2 are computed to maximize the margin (the minimum distance between training points and the classification surface), the W obtained after training can be written as a linear combination of a small subset of the expanded training examples <ref> (Boser, Guyon and Vapnik, 1992) </ref>. Points in this subset are called support points. This leads to a surprisingly simple way of evaluating high degree polynomials in high dimensional spaces without having to explicitly compute all the terms of the polynomial.
Reference: <author> Bottou, L., Cortes, C., Denker, J., Drucker, H., Guyon, I., Jackel, L., LeCun, Y., Muller, U., Sackinger, E., Simard, P., and Vapnik, V. </author> <year> (1994). </year> <title> Comparison of classifier methods: a case study in handwritten digit recognition. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <address> Jerusalem, Israel. </address>
Reference-contexts: Learning the ff j amounts to solving a quadratic programming problem with linear inequality constraints. Excellent results on handwritten digit images have been obtained with a 4th degree polynomial computed with this method LeCun & Bengio: Pattern Recognition and Neural Nets 10 <ref> (Bottou et al., 1994) </ref>. <p> The success of convolutional nets of various types has had a major impact on several application domains: speech recognition, character recognition, object spotting. On handwriting recognition tasks, they compare favorably with other techniques <ref> (Bottou et al., 1994) </ref> in terms of LeCun & Bengio: Pattern Recognition and Neural Nets 16 accuracy, speed, and memory requirements. Character recognizers using convolutional nets have been deployed in commercial applications.
Reference: <author> Bottou, L. and Vapnik, V. </author> <year> (1992). </year> <title> Local Learning Algorithms. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 888-900. </pages> <editor> LeCun & Bengio: </editor> <title> Pattern Recognition and Neural Nets 22 Bridle, </title> <editor> J. </editor> <year> (1990). </year> <title> Alphanets: a Recurrent `Neural' Network Architecture with a Hidden Markov Model Interpretation. Speech Communication. </title>
Reference-contexts: In another interesting LeCun & Bengio: Pattern Recognition and Neural Nets 14 "semi-local" method, a simple network (e.g., single layer) is trained each time a new test pattern is presented, using training patterns in the neighborhood of this test pattern; training is done "on demand" during recognition <ref> (Bottou and Vapnik, 1992) </ref>. In general, local methods learn fast, but they are expensive at run-time, in terms of memory and, often, of computation. In addition, they may not be appropriate for problems with high-dimensional inputs.
Reference: <author> Burr, D. </author> <year> (1983). </year> <title> Designing a handwriting reader. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 5(5) </volume> <pages> 554-559. </pages>
Reference-contexts: Elastic matching comes down to finding the point closest to the input pattern on the surface of all possible deformations of the prototype. Naturally, the exhaustive search approach is prohibitively expensive in general. However, if the surface is smooth, better search techniques can be used: gradient descent <ref> (Burr, 1983) </ref>, or conjugate gradient (Hinton, Williams and Revow, 1992). If the deformations are along one dimension (like in speech), dynamic programming can find the best solution efficiently.
Reference: <author> Denker, J. and LeCun, Y. </author> <year> (1991). </year> <title> Transforming neural-net output levels to probability distributions. </title> <editor> In Lippman, R. P., Moody, R., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 853-859, </pages> <address> San Mateo CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Driancourt, X., Bottou, L., and Gallinari, P. </author> <year> (1991). </year> <title> Learning Vector Quantization, Multi Layer Perceptron and Dynamic Programming: Comparison and Cooperation. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 815-819. </pages>
Reference: <author> Duda, R. and Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: Training algorithms for Linear Classifiers are well studied (see PERCEPTRONS, ADA-LINES, AND BACKPROPAGATION). Their limitations are well known: the likelihood that a partition of P vectors of dimension N be computable by a linear classifier decreases very quickly as P increases beyond N <ref> (Duda and Hart, 1973) </ref>. One method to ensure sepa LeCun & Bengio: Pattern Recognition and Neural Nets 7 rability is to represent the patterns by high-dimensional vectors (large N ). <p> In the K-Nearest Neighbors algorithms, the K nearest prototypes to an unknown pattern vote for its label. In the Parzen windows method, the normalized sum of all the OE i (X) associated with a particular class is interpreted as the conditional probability that X belongs to that class <ref> (Duda and Hart, 1973) </ref>. In the RBF method the output is a (learned) linear combination of the outputs of the basis functions. Associating a prototype to each training sample can be very inefficient, and increases the "complexity" term. Therefore, several methods have been proposed to learn the prototypes.
Reference: <author> Franzini, M., Lee, K., and Waibel, A. </author> <year> (1990). </year> <title> Connectionist Viterbi Training: a New Hybrid Method for Continuous Speech Recognition. </title> <booktitle> In International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 425-428, </pages> <address> Albuquerque, NM. </address>
Reference-contexts: This means being able to back-propagate gradients through the HMM, down to the recognizer, or to generate desired outputs for the recognizer using the best path in the graph (see SPEECH RECOGNITION AND NEURAL NETWORKS and <ref> (Franzini, Lee and Waibel, 1990) </ref>).
Reference: <author> Haffner, P., Franzini, M., and Waibel, A. </author> <year> (1991). </year> <title> Integrating Time Alignment and Neural Networks for High Performance Continuous Speech Recognition. </title> <booktitle> In International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 105-108, </pages> <address> Toronto. </address>
Reference: <author> Hinton, G., Williams, C., and Revow, M. </author> <year> (1992). </year> <title> Adaptive elastic models for hand-printed character recognition. </title> <editor> In Moody, J., Hanson, S., and Lipmann, R., editors, </editor> <booktitle> Advances LeCun & Bengio: Pattern Recognition and Neural Nets 23 in Neural Information Processing Systems 4, </booktitle> <pages> pages 512-519, </pages> <address> San Mateo CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Naturally, the exhaustive search approach is prohibitively expensive in general. However, if the surface is smooth, better search techniques can be used: gradient descent (Burr, 1983), or conjugate gradient <ref> (Hinton, Williams and Revow, 1992) </ref>. If the deformations are along one dimension (like in speech), dynamic programming can find the best solution efficiently. In an interesting technique, recently proposed in (Simard, LeCun and Denker, 1993), the surface of a deformed prototype is approximated by its tangent plane at the prototype.
Reference: <author> LeCun, Y. </author> <year> (1989). </year> <title> Generalization and Network Design Strategies. </title> <type> Technical Report CRG-TR-89-4, </type> <institution> Department of Computer Science, University of Toronto. </institution>
Reference-contexts: An important limitation to the popularity of NN techniques for PR is that certain simple tricks must be used and many common pitfalls must be avoided that are part of the "oral culture" rather than scientific facts <ref> (LeCun, 1989) </ref>. Once back-propagation with feed-forward networks of sigmoid units and dot-products established the value of gradient-based learning, it seemed natural to extend the idea to other structures.
Reference: <author> LeCun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., and Jackel, L. </author> <year> (1990). </year> <title> Handwritten Digit Recognition with a Back-Propagation Network. </title> <editor> In Touret-zky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 396-404, </pages> <address> Denver 1989. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: The main advantage of this approach is that the feature extractor is totally integrated into the classifier, and is produced by the learning process, rather than by the hand of the designer <ref> (LeCun et al., 1990) </ref>. Due to the weight sharing technique, the number of free parameters in a convolutional network is much less than in a fully-connected network of comparable power, which has the effect of reducing the "complexity" term in equation 1, and improving the generalization.
Reference: <author> Lee, Y. </author> <year> (1991). </year> <title> Handwritten digit recognition using K nearest neighbor, radial-basis function, and backpropagation neural network. </title> <journal> Neural Computation, </journal> <volume> 3(3) </volume> <pages> 441-449. </pages>
Reference-contexts: The parameters can then be adjusted using the gradient. It has been argued that the local property leads to faster learning than standard multi-layer nets, and good rejection properties <ref> (Lee, 1991) </ref>. Several authors enhance the power of prototype-based systems by using distance measures that are more complex than just Euclidean distance between the prototypes and the input patterns LeCun & Bengio: Pattern Recognition and Neural Nets 9 (such as general bilinear forms with learned coefficients).
Reference: <author> Martin, G. and Pittman, J. </author> <year> (1991). </year> <title> Recognizing hand-printed letters and digits using backpropagation learning. </title> <journal> Neural Computation, </journal> <volume> 3(2) </volume> <pages> 258-267. </pages>
Reference-contexts: Although fully-connected networks fed with "raw" character images (or speech spectra) have very large numbers of free parameters, they have been applied with some success <ref> (Martin and Pittman, 1991) </ref>. This can be explained as follows. With small initial weights a multi-layer network is almost equivalent to a single-layer network (each layer is quasi-linear).
Reference: <author> Simard, P., LeCun, Y., and Denker, J. </author> <year> (1993). </year> <title> Efficient pattern recognition using a new transformation distance. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 50-58, </pages> <address> Denver 1992. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. LeCun & Bengio: </address> <booktitle> Pattern Recognition and Neural Nets 24 post-processor. </booktitle>
Reference-contexts: However, if the surface is smooth, better search techniques can be used: gradient descent (Burr, 1983), or conjugate gradient (Hinton, Williams and Revow, 1992). If the deformations are along one dimension (like in speech), dynamic programming can find the best solution efficiently. In an interesting technique, recently proposed in <ref> (Simard, LeCun and Denker, 1993) </ref>, the surface of a deformed prototype is approximated by its tangent plane at the prototype. The LeCun & Bengio: Pattern Recognition and Neural Nets 11 matching problem reduces to finding the minimum distance between a point and a plane, which can be done efficiently.
References-found: 18

