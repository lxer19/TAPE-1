URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92214-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Automatic and Interactive Parallelization  
Author: by Kathryn S. M c Kinley Keith D. Cooper, Associate Professor Don H. Johnson, Professor Danny C. Sorensen, Professor 
Degree: A Thesis Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Approved, Thesis Committee: Ken Kennedy Noah Harding Professor, chair Computer Science  
Date: March, 1994  
Address: Houston, Texas  
Affiliation: RICE UNIVERSITY  Computer Science  Electrical and Computer Engineering  Mathematical Sciences  
Abstract-found: 0
Intro-found: 1
Reference: [ABC + 87] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Ptran is also an automatic parallelizer with extensive program analysis. It computes the ssa and program dependence graphs, and performs constant propagation and interprocedural analysis [CFR + 89, FOW87]. Ptran introduces both task and loop parallelism, but the only other program transformations are variable privatiza-tion and loop distribution <ref> [ABC + 87, Sar90] </ref>. 32 Sigmacs, a programmable interactive parallelizer in the Faust programming environment, computes and displays call graphs, process graphs, and a statement dependence graph [GGGJ88, SG90]. In a process graph each node represents a task or a process, which is a separate entity running in parallel. <p> If vectorization or parallelization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals. For these reasons, research in automatic parallelization has concentrated on an alternative approach that uses control dependences to model control flow <ref> [FOW87, ABC + 87, ABC + 88] </ref>. Our approach uses both data and control dependence graphs (as were defined in Sections 2.1 and 2.1). <p> Assuring sufficient granularity and matching it to the machine is dependent on the architecture. Most previous research focuses on discovering parallelism and/or maximizing its granularity without regard to data locality <ref> [Cal87, WL90, ABC + 87] </ref>. Our approach addresses all of these concerns. The key component is the combination of loop interchange and strip mining developed in Chapter 6 which considers all these factors. Sufficient granularity is assured using performance estimation (see Section 6.7.1).
Reference: [ABC + 88] <author> F. Allen, M. Burke, P. Charles, J. Ferrante, W. Hsieh, and V. Sarkar. </author> <title> A framework for detecting useful parallelism. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: If vectorization or parallelization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals. For these reasons, research in automatic parallelization has concentrated on an alternative approach that uses control dependences to model control flow <ref> [FOW87, ABC + 87, ABC + 88] </ref>. Our approach uses both data and control dependence graphs (as were defined in Sections 2.1 and 2.1).
Reference: [AC72] <author> F. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In J. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for improving data locality. Each transformation is briefly introduced below. Many are found in the literature <ref> [AC72, AK87, CCK90, KM90, KMT91b, KKLW84, Lov77, Wol86] </ref>. In Ped, their novel aspect is the analysis of their applicability, safety, prof 17 itability and the incremental updates of source and dependence information. We classify the transformations implemented in Ped as follows. <p> Loop skewing may be used with loop interchange in Ped to expose wavefront parallelism [KMT91a, Wol86]. * Loop reversal reverses the order of execution of loop iterations. * Loop fusion can increase the granularity of parallel regions and promote reuse by fusing two contiguous loops when dependences are not violated <ref> [AC72, KKP + 81] </ref>. * Statement interchange interchanges two adjacent independent statements. * Unroll and jam increases the potential candidates for scalar replacement and pipelining by unrolling the body of an outer loop in a loop nest and fusing the resulting inner loops [AC72, CCK90, CCK88, KMT91a]. 3.3.2 Dependence breaking transformations <p> contiguous loops when dependences are not violated [AC72, KKP + 81]. * Statement interchange interchanges two adjacent independent statements. * Unroll and jam increases the potential candidates for scalar replacement and pipelining by unrolling the body of an outer loop in a loop nest and fusing the resulting inner loops <ref> [AC72, CCK90, CCK88, KMT91a] </ref>. 3.3.2 Dependence breaking transformations Dependence breaking transformations are used to satisfy specific dependences that inhibit parallelism. They may introduce new storage to eliminate storage-related anti or output dependences, or convert loop-carried dependences to loop-independent dependences, often enabling the safe application of other transformations. <p> It is useful for breaking dependences which arise on the first or last k iterations of the loop <ref> [AC72] </ref>. * Loop splitting, or index set splitting, separates the iteration space of one loop into two loops, where the user specifies at which iteration to split. <p> It improves the performance of the program by reducing the number of memory accesses required. * Loop unrolling decreases loop overhead and increases potential candidates for scalar replacement by unrolling the body of a loop <ref> [AC72, KMT91a] </ref>. 20 3.3.4 Miscellaneous transformations Finally Ped has a few miscellaneous transformations. * Sequential $ Parallel converts a sequential DO loop into a parallel loop, and vice versa. * Loop bounds adjusting adjusts the upper and lower bounds of a loop by a constant. <p> The update algorithm is explained more thoroughly in Section 4.2. 3.4.4 Unroll and jam Unroll and jam is a transformation that unrolls an outer loop in a loop nest, then jams (or fuses) the resulting inner loops <ref> [AC72, CCK88] </ref>. Unroll and jam can be used to convert dependences carried by the outer loop into loop independent dependences or dependences carried by some inner loop. <p> Hence, the creation of an execution variable will replace control dependences between partitions with data dependences. Execution variables are arrays, with one value for each iteration of the loop, because each iteration can give rise to a different control decision. If desired, loop invariant decisions can be detected <ref> [AC72] </ref> and represented with scalar execution variables. All previous techniques, whether they are G cd based or not, use Boolean logic when introducing arrays to record branch decisions. These methods require either testing and recording the path taken in previous loops or introducing additional arrays. <p> 3 Again, data-flow analysis is able to determine these conditions in the loop and if the last value is always stored back, live analysis of t is unnecessary. 4.3.7 Loop fusion Loop fusion places the bodies of two adjacent loops with the same number of iterations into a single loop <ref> [AC72] </ref>. Fusion is safe for two loops l 1 and l 2 if it does not result in values flowing from statements originally in l 2 back into statements originally in l 1 and vice versa. <p> The most common form of interprocedural transformation is procedure inlining. Inlining substitutes the body of a called procedure for the procedure call and optimizes it as a part of the calling procedure <ref> [AC72] </ref>. Even though regular section analysis and inlining are frequently successful at enabling optimization, each of these methods has its limitations [HK90, LY88a, Hus82]. Compilation time and space considerations require that regular section analysis summarize array side effects.
Reference: [ACK87] <author> J. R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Munich, Germany, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: Like Wolf and Lam, they exhaustively search for a loop permutation with the lowest estimated cost. Many algorithms have been proposed in the literature for introducing parallelism into programs. Callahan et al. use the metric of minimizing barrier synchronization points via loop distribution, fusion and interchange for introducing parallelism <ref> [ACK87, Cal87] </ref>. Wolf and Lam [WL90] introduce all possible parallelism via the uni-modular transformations: loop interchange, skewing, and reversal. Neither of these techniques try to map the parallelism to a machine, or try take into account data locality, nor is any loop bound information considered. <p> are fusion preventing edges Rules: 1. cannot fuse two nodes with a fusion preventing edge between them 2. cannot change relative ordering of two nodes connected by an edge 3. cannot fuse sequential and parallel nodes Callahan presents a greedy algorithm for a closely related problem that omits rule 3 <ref> [Cal87, ACK87] </ref>. His work also tries to partition a graph into minimal sets, but his model of parallelism includes loop-level and fork-join task parallelism. For example, consider the example graph in Figure 7.1. <p> P 2 is performed in parallel once they both complete. Callahan's formulation of the loop distribution problem ignores the node type, enabling the greedy algorithm to provably minimize the number of loops and maximize parallelism for a single level-level <ref> [Cal87, ACK87] </ref>. Our model of parallelism differs in that it only considers loop-level parallelism.
Reference: [AHU74] <author> A. V. Aho, J. E. Hopcroft, and J. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: Computing a transitive closure on a DAG is O (N fl E) time and space where N is the number of nodes in G o and E is the number of edges <ref> [AHU74] </ref>. Of course, transitive closure introduces additional edges that are unnecessary.
Reference: [AJ90] <author> R. Allen and S. Johnson. </author> <title> Compiling C for vectorization, parallelization, and inline expansion. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution <ref> [AJ90, CHT91, Hus82] </ref> and interprocedural analysis of array side effects [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86]. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency.
Reference: [AK84] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <booktitle> In Proceedings of the SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: The same result is obtained by previous researchers <ref> [AK84, WL91] </ref>. 6.5.2 Permuting to achieve memory order We must now decide whether the desired memory order is legal. If it is not, we must select some legal loop permutation close to memory order. To determine whether a loop permutation is legal is straightforward. <p> The loop permutation is illegal if and only if the first nonzero entry of some vector is negative, indicating that the execution order of a data dependence has been reversed <ref> [AK84, Ban90a, Ban90b, WL90] </ref>. In many cases, the loop permutation calculated by MemoryOrder is legal and we are finished. However, if the desired memory order is prevented by data dependences, we use a simple heuristic for calculating a legal loop permutation near memory order. <p> The proof by contradiction of the theorem proceeds as follows. Given an original set of legal direction vectors, each step of the "for" is guaranteed to find a loop which results in a legal direction vector, otherwise the original was not legal <ref> [AK84, Ban90a] </ref>. 92 In addition, if any loop 1 through n1 may be legally positioned prior to n it will be.
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for improving data locality. Each transformation is briefly introduced below. Many are found in the literature <ref> [AC72, AK87, CCK90, KM90, KMT91b, KKLW84, Lov77, Wol86] </ref>. In Ped, their novel aspect is the analysis of their applicability, safety, prof 17 itability and the incremental updates of source and dependence information. We classify the transformations implemented in Ped as follows. <p> When loop in 18 terchange is safe, it can be used to adjust the granularity of parallel loops <ref> [AK87, KMT91a, Wol89b] </ref>. * Loop skewing adjusts the iteration space of two perfectly nested loops by shifting the work per iteration in order to expose parallelism. When possible, Ped computes and suggests the optimal skew degree. <p> It breaks output and anti dependences which may be inhibiting parallelism [KKLW80a]. * Array renaming, also known as node splitting [KKLW80a], is used to break anti dependences by copying the source of an anti dependence into a newly introduced temporary array and renaming the sink to the new array <ref> [AK87] </ref>. Loop distribution may then be used to separate the copying statement into a separate loop, allowing both loops to be parallelized. 19 * Loop peeling peels off the first or last k iterations of a loop as specified by the user. <p> For example, if DO I = 1, 100 is split at 50, the following two loops result: DO I = 1, 50 and DO I = 51, 100. Loop splitting is useful in breaking crossing dependences, dependences that cross a specific iteration <ref> [AK87] </ref>. * Alignment moves instances of statements from one iteration to another to break loop-carried dependences [Cal87]. 3.3.3 Memory hierarchy transformations Memory optimizing transformations adjust a loop's balance between computations and memory accesses to make better use of the memory hierarchy and functional pipelines. <p> The purpose, mechanics, and safety of these transformations are presented, followed by their profitability estimates, user advice, and incremental dependence update algorithms. 3.4.1 Loop interchange Loop interchange has been used extensively in vectorizing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism <ref> [AK87, KKLW84, Wol86] </ref>. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. Ped supports interchange of triangular or skewed loops. <p> the original distance vectors (d 1 ; d 2 ) for all dependences in the nest to (d 1 ; ffd 1 + d 2 ), and then updates their interchange flags. 3.4.3 Loop distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers <ref> [AK87, KKP + 81] </ref>. It is used to expose partial parallelism by separating statements which may be parallelized from those that must be executed sequentially. It is a cornerstone of vectorization and parallelization. In Ped the user can specify whether distribution is for the purpose of vectorization or parallelization. <p> Our work on interactive parallelization bears similarities to Sigmacs, Pat, and Superb. Ped has been greatly influenced by the Rice Parallel Fortran Converter ( PFC), which has focused on the problem of automatically vectorizing and parallelizing sequential Fortran <ref> [AK87] </ref>. PFC has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [CCKT86], interprocedural side-effect analysis of scalars [CKT86a], and inter-procedural array section analysis [CK87b, HK91]. <p> This approach, called if-conversion [AKPW83, All83], has been used success 36 fully in a variety of vectorization systems which incorporate several other transformations as well <ref> [AK87, SK86, KKLW84] </ref>. However, if-conversion has several drawbacks. If vectorization or parallelization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals. <p> The method we present is designed to work on any partition that is legal, i.e., any partition that preserves the control and data dependences. A partition can preserve all dependences if and only if there exists no dependence cycle spanning more than one output loop <ref> [KKP + 81, AK87] </ref>. If there is a cycle involving control and/or data dependences, it must be entirely contained within a single partition. 2 This condition is both necessary and sufficient. <p> If step (1) is unsuccessful, step (2) attempts to satisfy as many dependences as possible with BreakDependences. The literature includes a collection of transforma tions that are used to satisfy specific dependences that inhibit parallelism. They in clude loop peeling, scalar expansion [KKLW80a], array renaming <ref> [AK87, KKLW80a] </ref>, alignment and replication [Cal87], and loop splitting [AK87]. These transformations may introduce new storage to eliminate storage-related anti or output dependences, or convert loop-carried dependences to loop-independent 115 dependences, often enabling the safe application of other transformations. <p> The literature includes a collection of transforma tions that are used to satisfy specific dependences that inhibit parallelism. They in clude loop peeling, scalar expansion [KKLW80a], array renaming [AK87, KKLW80a], alignment and replication [Cal87], and loop splitting <ref> [AK87] </ref>. These transformations may introduce new storage to eliminate storage-related anti or output dependences, or convert loop-carried dependences to loop-independent 115 dependences, often enabling the safe application of other transformations. If all the dependences carried on a loop are eliminated, the loop may then be run in parallel. <p> Dependences are preserved if any statements involved in a dependence recurrence are placed in the same loop. The dependences between the partitions then form an directed acyclic graph that can always be ordered using topological sort <ref> [AK87, KKP + 81] </ref>. By first choosing a safe partition of the loops with the finest possible granularity and then fusing partitions back together larger partitions may be formed. This transforms the loop distribution to one of loop fusion, a problem thought to be very hard. <p> Program analysis was also performed automatically. However, to overcome gaps in the current implemantation of program analysis, we used the Program Composition Editor to import dependence information from PFC . PFC is the Rice system for automatic vectorization (see Section 3.7) <ref> [AK87] </ref>. PFC 's analysis is more mature and includes important features not yet implemented in Ped. It performs advanced dependence tests which include symbolics dependence tests and it computes interprocedural constants, interprocedural symbolics and inter-procedural mod and ref information for simple array sections [GKT91, HK90, HK91].
Reference: [AKPW83] <author> J. R. Allen, K. Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In Conference Record of 145 the Tenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1983. </year>
Reference-contexts: This approach, called if-conversion <ref> [AKPW83, All83] </ref>, has been used success 36 fully in a variety of vectorization systems which incorporate several other transformations as well [AK87, SK86, KKLW84]. However, if-conversion has several drawbacks. If vectorization or parallelization fails, it is not easy to reconstruct efficient branching code. <p> Of course, the destinations of exit branches should remain as they were. 4.4 Related work One approach taken in automatic vectorizers when loops contain conditional control flow is to convert control dependences into data dependences using a technique called if-conversion <ref> [All83, AKPW83] </ref>. If-conversion is theoretically appealing because it allows for a unified treatment of data dependence without control dependences, and has been used successfully in a number of vectorization systems [KKLW84, SK86]. However, it has several drawbacks.
Reference: [All83] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: This approach, called if-conversion <ref> [AKPW83, All83] </ref>, has been used success 36 fully in a variety of vectorization systems which incorporate several other transformations as well [AK87, SK86, KKLW84]. However, if-conversion has several drawbacks. If vectorization or parallelization fails, it is not easy to reconstruct efficient branching code. <p> Of course, the destinations of exit branches should remain as they were. 4.4 Related work One approach taken in automatic vectorizers when loops contain conditional control flow is to convert control dependences into data dependences using a technique called if-conversion <ref> [All83, AKPW83] </ref>. If-conversion is theoretically appealing because it allows for a unified treatment of data dependence without control dependences, and has been used successfully in a number of vectorization systems [KKLW84, SK86]. However, it has several drawbacks.
Reference: [All90] <author> J. R. Allen. </author> <title> Private communication, </title> <month> February </month> <year> 1990. </year>
Reference-contexts: Towle [Tow76] and Baxter and Bauer [BB89] use similar approaches for inserting conditional arrays. The Stardent compiler distributes loops with structured control flow by keeping groups of statements with the same control flow constraints together <ref> [All90] </ref>. For example, all the statements in the true branch of a block if must stay together, so only the outer level of if nests can be considered.
Reference: [AS79] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: For reasonably large computations, references such as C (J,I) do not 84 provide any reuse on the I loop, because the desired cache lines have been flushed by intervening memory accesses. Previous researchers have studied techniques for improving locality of accesses for registers, cache, and pages <ref> [AS79, CCK90, WL91, GJG88] </ref>. We concentrate on improving the locality of accesses for cache; i.e. we attempt to increases the locality of access to the same cache line. Empirical results show that improving spatial reuse can be significantly more effective than techniques that consider temporal reuse alone [KMT92].
Reference: [Ban88] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference: [Ban90a] <author> U. Banerjee. </author> <title> A theory of loop permutations. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The loop permutation is illegal if and only if the first nonzero entry of some vector is negative, indicating that the execution order of a data dependence has been reversed <ref> [AK84, Ban90a, Ban90b, WL90] </ref>. In many cases, the loop permutation calculated by MemoryOrder is legal and we are finished. However, if the desired memory order is prevented by data dependences, we use a simple heuristic for calculating a legal loop permutation near memory order. <p> The proof by contradiction of the theorem proceeds as follows. Given an original set of legal direction vectors, each step of the "for" is guaranteed to find a loop which results in a legal direction vector, otherwise the original was not legal <ref> [AK84, Ban90a] </ref>. 92 In addition, if any loop 1 through n1 may be legally positioned prior to n it will be.
Reference: [Ban90b] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain 22 dependences and iteration space after skewbefore skew i i effective loop-level parallelism in a loop nest <ref> [Ban90b, WL90, Wol89a] </ref>. All of these transformations are supported in Ped. Loop skewing is applied to a pair of perfectly nested loops that both carry dependences, even after loop interchange. <p> The loop permutation is illegal if and only if the first nonzero entry of some vector is negative, indicating that the execution order of a data dependence has been reversed <ref> [AK84, Ban90a, Ban90b, WL90] </ref>. In many cases, the loop permutation calculated by MemoryOrder is legal and we are finished. However, if the desired memory order is prevented by data dependences, we use a simple heuristic for calculating a legal loop permutation near memory order. <p> Neither of these techniques try to map the parallelism to a machine, or try take into account data locality, nor is any loop bound information considered. Banerjee also considers introducing parallelism via unimodular transformations, but only for doubly nested loops <ref> [Ban90b] </ref>. Banerjee does however consider loop bound information. Because we accept some imprecision, our algorithms are simpler and may be applied to computations that have not been fully characterized in Wolf and Lam's uni-modular framework. For instance, we can support imperfectly nested loops, multiple loop nests, and imprecise data dependences.
Reference: [BB89] <author> W. Baxter and H. R. Bauer, III. </author> <title> The program dependence graph and vectorization. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: For example, any of the code generation algorithms based on the program dependence graph could be used in conjunction with the above algorithm <ref> [FM85, FMS88, CFS90, BB89] </ref>. The very simple code generation scheme described here has been is implemented in the ParaScope Editor [KMT91a]. When transformations are applied in an interactive environment, it is important to retain as much similarity to the original program as possible. <p> A straightforward relabeling of the first partition in Example 4.2 after restructuring results in the following. DO I = 1, N S 1 IF (EV 1 [I] .EQ. TRUE) GOTO 6 S 2 ENDDO Structured code generation When G cd is a tree, code generation is relatively simple <ref> [FM85, FMS88, BB89] </ref>. This discussion emphasizes properly selecting and inserting the appropriate control structures for newly created guards. Other G cd code generation algorithms must select and create control structures for all branches. <p> Because their code generation algorithm is based on G f , rather than G cd , the proof of how an execution variable is used is much more difficult and is not given. Towle [Tow76] and Baxter and Bauer <ref> [BB89] </ref> use similar approaches for inserting conditional arrays. The Stardent compiler distributes loops with structured control flow by keeping groups of statements with the same control flow constraints together [All90].
Reference: [BC86] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and par-allelization. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Recompilation analysis tests that interprocedural facts used to optimize a procedure have not been invalidated by editing changes <ref> [CKT86b, BC86, BCKT90] </ref>. To extend recompilation analysis for interprocedural transformations, a few additions are needed. When an interprocedural transformation is performed, a description of the interprocedural 69 transformations annotates the nodes and edges in the augmented call graph. <p> The array formal is replaced by a new array with the same shape as the actual. The references to the variable are translated by linearizing the formal's subscript expressions and then converting to the dimensions of the new array <ref> [BC86] </ref>. Finally, the subscript expressions for each dimension of the 71 actual are added to those for the translated reference. This method is also the one that is used in our implementation of inlining. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution [AJ90, CHT91, Hus82] and interprocedural analysis of array side effects <ref> [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency.
Reference: [BCHT90] <author> P. Briggs, K. Cooper, M. W. Hall, and L. Torczon. </author> <title> Goal-directed inter-procedural optimization. </title> <type> Technical Report TR90-147, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: For example, when the dimension size of a formal array parameter is also passed as a parameter, translating references of the formal to the actual can introduce multiplications of unknown symbolic values into subscript expressions. This situation occurs when inlining is used on the spec Benchmark program matrix300 <ref> [BCHT90] </ref>. In this chapter, a hybrid approach is developed that overcomes some of these limitations. <p> This strategy, called goal-directed inter-procedural optimization, avoids the costs of interprocedural optimization when it does not enable other performance enhancing optimizations <ref> [BCHT90] </ref>. Interprocedural transformations are applied as dictated by a code generation algorithm that explores possible transformations, selecting a choice that introduces parallelism and exploits data locality. The code generator is part of an interprocedural compilation system that efficiently supports interprocedural analysis and optimization by retaining separate compilation of procedures. <p> This chapter uses the extensions developed in Chapter 5 to design an algorithm that considers loop-based transformations even in the presence of procedure calls. The loop-based transformations determine which, if any, interprocedural transformations are necessary. The interprocedural transformations are called enablers and are applied using goal-directed interprocedural optimization <ref> [BCHT90] </ref>.
Reference: [BCKT90] <author> M. Burke, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <type> Technical Report TR90-126, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Several algorithms for performing incremental analysis are found in the literature; for example, data-flow analysis [RP88, Zad84], interprocedural analysis [Bur90, RC86], interprocedural recompilation analysis <ref> [BCKT90] </ref>, as well as dependence analysis [Ros90]. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped. <p> Recompilation analysis tests that interprocedural facts used to optimize a procedure have not been invalidated by editing changes <ref> [CKT86b, BC86, BCKT90] </ref>. To extend recompilation analysis for interprocedural transformations, a few additions are needed. When an interprocedural transformation is performed, a description of the interprocedural 69 transformations annotates the nodes and edges in the augmented call graph.
Reference: [Ber66] <author> A. J. Bernstein. </author> <title> Analysis of programs for parallel processing. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 15(5) </volume> <pages> 757-763, </pages> <month> October </month> <year> 1966. </year>
Reference: [BFKK92] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator in the Fortran D programming system. </title> <editor> In J. Saltz and 146 P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Our performance estimator predicts the cost of parallel and sequential performance using a loop model and a training set approach. The goal of our performance estimator is to assist in code generation for both shared and distributed memory multiprocessors <ref> [BFKK92, KMM91] </ref>. Modeling the target machines at an architectural level would require calculating an analytical model for each supported architecture. Instead our performance estimator uses a training set to characterize each architecture in a machine-independent fashion.
Reference: [BHMS91] <author> M. Bromley, S. Heller, T. McNerney, and G. Steele, Jr. </author> <title> Fortran at ten gigaflops: The Connection Machine convolution compiler. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: These results indicate that data locality should be the overwhelming force driving scalar compilers today. 6.6.2 Stencil computations: Jacobi and SOR Stencil computations such as Jacobi and SOR are finite difference techniques fre quently used to solve partial difference equations <ref> [BHMS91] </ref>. Jacobi runs completely in parallel, while SOR causes a computational wavefront to sweep diagonally through the array. Both kernels were written using 500 fi 500 2D arrays.
Reference: [BJ66] <author> C. Bohm and G. Jacopini. </author> <title> Flow diagrams, turing machines, and languages with only two formation rules. </title> <journal> Communications of the ACM, </journal> <volume> 19(5), </volume> <month> May </month> <year> 1966. </year>
Reference: [BK89] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: These are similar to data access descriptors or dads and they provide detailed information about references and how the loops in a called procedure access it <ref> [BK89] </ref>. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution [AJ90, CHT91, Hus82] and interprocedural analysis of array side effects <ref> [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency.
Reference: [BKK + 89] <author> V. Balasundaram, K. Kennedy, U. Kremer, K. S. M c Kinley, and J. Subhlok. </author> <title> The ParaScope Editor: An interactive parallel programming tool. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: A tool with this much functionality is bound to be complex. Ped incorporates a complete source editor and supports dependence analysis, dependence display, and a large variety of program transformations to enhance parallelism. We describe in detail elsewhere the usage, user interface and motivation of the ParaScope Editor <ref> [BKK + 89, FKMW90, KMT91b] </ref>. We also cover elsewhere the types of analyses and representations needed to support this tool and automatic parallelization (see Section 2) [KMT91a]. In this chapter, we focus on efficient algorithms for incremental updates after a transformation or edit. <p> To locate opportunities for transformations, we browsed the dependences in the program using the ParaScope Editor <ref> [BKK + 89, KMT91a, KMT91b] </ref>. Using other ParaScope tools, we determined which procedures in the program contained procedure calls. We examined the procedures containing calls, looking for interesting call structures.
Reference: [Bur90] <author> M. Burke. </author> <title> An interval-based approach to exhaustive and incremental interprocedural data-flow analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 341-395, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Several algorithms for performing incremental analysis are found in the literature; for example, data-flow analysis [RP88, Zad84], interprocedural analysis <ref> [Bur90, RC86] </ref>, interprocedural recompilation analysis [BCKT90], as well as dependence analysis [Ros90]. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped.
Reference: [Cal87] <author> D. Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: Loop splitting is useful in breaking crossing dependences, dependences that cross a specific iteration [AK87]. * Alignment moves instances of statements from one iteration to another to break loop-carried dependences <ref> [Cal87] </ref>. 3.3.3 Memory hierarchy transformations Memory optimizing transformations adjust a loop's balance between computations and memory accesses to make better use of the memory hierarchy and functional pipelines. <p> Like Wolf and Lam, they exhaustively search for a loop permutation with the lowest estimated cost. Many algorithms have been proposed in the literature for introducing parallelism into programs. Callahan et al. use the metric of minimizing barrier synchronization points via loop distribution, fusion and interchange for introducing parallelism <ref> [ACK87, Cal87] </ref>. Wolf and Lam [WL90] introduce all possible parallelism via the uni-modular transformations: loop interchange, skewing, and reversal. Neither of these techniques try to map the parallelism to a machine, or try take into account data locality, nor is any loop bound information considered. <p> Assuring sufficient granularity and matching it to the machine is dependent on the architecture. Most previous research focuses on discovering parallelism and/or maximizing its granularity without regard to data locality <ref> [Cal87, WL90, ABC + 87] </ref>. Our approach addresses all of these concerns. The key component is the combination of loop interchange and strip mining developed in Chapter 6 which considers all these factors. Sufficient granularity is assured using performance estimation (see Section 6.7.1). <p> The literature includes a collection of transforma tions that are used to satisfy specific dependences that inhibit parallelism. They in clude loop peeling, scalar expansion [KKLW80a], array renaming [AK87, KKLW80a], alignment and replication <ref> [Cal87] </ref>, and loop splitting [AK87]. These transformations may introduce new storage to eliminate storage-related anti or output dependences, or convert loop-carried dependences to loop-independent 115 dependences, often enabling the safe application of other transformations. <p> In loop distribution, it is always legal to fuse the loops back together and run the original loop sequentially. We would like to utilize all the parallelism and have the fewest loops. Callahan refers to these criteria as maximal parallelism with minimum barrier synchronization <ref> [Cal87] </ref>. The loop distribution and loop fusion problem is a graph partitioning problem on a directed acyclic graph (DAG). Each node in the graph represents a sequential or parallel loop containing a set of statements. There are data dependence edges, some of which are fusion preventing. <p> are fusion preventing edges Rules: 1. cannot fuse two nodes with a fusion preventing edge between them 2. cannot change relative ordering of two nodes connected by an edge 3. cannot fuse sequential and parallel nodes Callahan presents a greedy algorithm for a closely related problem that omits rule 3 <ref> [Cal87, ACK87] </ref>. His work also tries to partition a graph into minimal sets, but his model of parallelism includes loop-level and fork-join task parallelism. For example, consider the example graph in Figure 7.1. <p> P 2 is performed in parallel once they both complete. Callahan's formulation of the loop distribution problem ignores the node type, enabling the greedy algorithm to provably minimize the number of loops and maximize parallelism for a single level-level <ref> [Cal87, ACK87] </ref>. Our model of parallelism differs in that it only considers loop-level parallelism.
Reference: [CCH + 88] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. Para-Scope: </author> <title> A parallel programming environment. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: Figure 5.1 (c) illustrates the dad annotations for the program in Example 5.1. 5.3 Support for interprocedural optimization In this section, we present the compilation system of the ParaScope Programming Environment <ref> [CCH + 88, CKT86a] </ref>. This system was designed for the efficient support of interprocedural analysis and optimization. The tools in ParaScope cooperate to enable the compilation system to perform interprocedural analysis without direct examination of source code. <p> In Erlebacher, the parallelism was not made explicit. Here, a naive parallelization of outer loops was performed to create the parallel version. 8.2.3 Creating an automatically parallelized version To create an automatically parallelized program, the nearby sequential program was first imported into the ParaScope Programming Environment <ref> [CCH + 88, HHK + 93] </ref>. As a result of importing the program, each procedure in the program was placed in a separate module. Also, a program composition was automatically created that describes the entire program and the call graph was built.
Reference: [CCK88] <author> D. Callahan, J. Cocke, and K. Kennedy. </author> <title> Estimating interlock and improving balance for pipelined machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(4) </volume> <pages> 334-358, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: contiguous loops when dependences are not violated [AC72, KKP + 81]. * Statement interchange interchanges two adjacent independent statements. * Unroll and jam increases the potential candidates for scalar replacement and pipelining by unrolling the body of an outer loop in a loop nest and fusing the resulting inner loops <ref> [AC72, CCK90, CCK88, KMT91a] </ref>. 3.3.2 Dependence breaking transformations Dependence breaking transformations are used to satisfy specific dependences that inhibit parallelism. They may introduce new storage to eliminate storage-related anti or output dependences, or convert loop-carried dependences to loop-independent dependences, often enabling the safe application of other transformations. <p> The update algorithm is explained more thoroughly in Section 4.2. 3.4.4 Unroll and jam Unroll and jam is a transformation that unrolls an outer loop in a loop nest, then jams (or fuses) the resulting inner loops <ref> [AC72, CCK88] </ref>. Unroll and jam can be used to convert dependences carried by the outer loop into loop independent dependences or dependences carried by some inner loop. <p> If any of these dependences cross 28 between the imperfectly nested statements and the statements in the inner loop, they inhibit unroll and jam. Specifically, the intervening statements cannot be moved and prevent fusion of the inner loops. Profitability Balance describes the ratio between computation and memory access rates <ref> [CCK88] </ref>. Unroll and jam is profitable if it brings the balance of a loop closer to the balance of the underlying machine. Ped automatically calculates the optimal unroll and jam degree for a loop nest, including loops with complex iteration spaces [CCK90]. <p> Extensions are needed for transformations 63 that require dependence distance information such as loop permutation. The intra-procedural optimizations which are extended in this chapter are loop fusion and loop permutation. These results easily generalize for other transformations such as loop skewing [Wol86] and unroll and jam <ref> [CCK88] </ref>. As a motivating example, consider the Fortran code in Example 5.1 (a). The J loop in subroutine S may safely be made parallel, but the outer I loop in subroutine P may not be.
Reference: [CCK90] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for improving data locality. Each transformation is briefly introduced below. Many are found in the literature <ref> [AC72, AK87, CCK90, KM90, KMT91b, KKLW84, Lov77, Wol86] </ref>. In Ped, their novel aspect is the analysis of their applicability, safety, prof 17 itability and the incremental updates of source and dependence information. We classify the transformations implemented in Ped as follows. <p> contiguous loops when dependences are not violated [AC72, KKP + 81]. * Statement interchange interchanges two adjacent independent statements. * Unroll and jam increases the potential candidates for scalar replacement and pipelining by unrolling the body of an outer loop in a loop nest and fusing the resulting inner loops <ref> [AC72, CCK90, CCK88, KMT91a] </ref>. 3.3.2 Dependence breaking transformations Dependence breaking transformations are used to satisfy specific dependences that inhibit parallelism. They may introduce new storage to eliminate storage-related anti or output dependences, or convert loop-carried dependences to loop-independent dependences, often enabling the safe application of other transformations. <p> order of the iterations is unchanged, but used in concert with loop interchange the iteration space may be tiled [Wol89a] to utilize memory bandwidth and cache more effectively [CK89]. * Scalar replacement takes array references with consistent dependences and replaces them with scalar temporaries that may be allocated into registers <ref> [CCK90] </ref>. <p> It brings two accesses to the same memory location closer together and can significantly improve performance by enabling reuse of either registers or cache. When applied in conjunction with scalar replacement on scientific codes, unroll and jam has resulted in integer factor speedups, even for single processors <ref> [CCK90] </ref>. Unroll and jam may also be applied to imperfectly nested loops or loops with complex iteration spaces. Figure 3.3 shows an example iteration space before and after unroll and jam of degree 1. <p> Unroll and jam is profitable if it brings the balance of a loop closer to the balance of the underlying machine. Ped automatically calculates the optimal unroll and jam degree for a loop nest, including loops with complex iteration spaces <ref> [CCK90] </ref>. Update An algorithm for the incremental update of the dependence graph after unroll and jam is described elsewhere [CCK90]. However, we chose a different strategy. <p> Ped automatically calculates the optimal unroll and jam degree for a loop nest, including loops with complex iteration spaces <ref> [CCK90] </ref>. Update An algorithm for the incremental update of the dependence graph after unroll and jam is described elsewhere [CCK90]. However, we chose a different strategy. Since no global data-flow or symbolic information is changed by unroll and jam, Ped rebuilds the scalar dependence graph for the loop nest and refines it with dependence tests. <p> This optimization approach may be divided into three phases: 1. optimizing to improve data locality, 2. finding and positioning a parallel loop, and 3. performing low-level memory optimizations such as tiling for cache and placing references in registers <ref> [LRW91, CCK90] </ref>. This chapter focuses on the first two phases. We advocate the first two phases be followed by a low-level memory optimizing phase, but do not address it here. 6.2 Memory and language model Because we are evaluating reuse, we require some knowledge of the memory hierarchy. <p> For reasonably large computations, references such as C (J,I) do not 84 provide any reuse on the I loop, because the desired cache lines have been flushed by intervening memory accesses. Previous researchers have studied techniques for improving locality of accesses for registers, cache, and pages <ref> [AS79, CCK90, WL91, GJG88] </ref>. We concentrate on improving the locality of accesses for cache; i.e. we attempt to increases the locality of access to the same cache line. Empirical results show that improving spatial reuse can be significantly more effective than techniques that consider temporal reuse alone [KMT92]. <p> We believe that this approximation is a very reasonable one, especially in view of the fact that we intend to use a scalar cache tiling method as a final step in the code generation process <ref> [CCK90] </ref>. In addition, the algorithms presented here are O (n 2 ) time in the worst case, where n is the depth of the loop nest, and are a considerable improvement over work which compares all legal permutations and then picks the best, taking exponential time.
Reference: [CCKT86] <author> D. Callahan, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year> <month> 147 </month>
Reference-contexts: Ped has been greatly influenced by the Rice Parallel Fortran Converter ( PFC), which has focused on the problem of automatically vectorizing and parallelizing sequential Fortran [AK87]. PFC has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation <ref> [CCKT86] </ref>, interprocedural side-effect analysis of scalars [CKT86a], and inter-procedural array section analysis [CK87b, HK91]. Ped expands on PFC's analysis and transformation capabilities and makes them available to the user in an interactive environment.
Reference: [CDL88] <author> D. Callahan, J. Dongarra, and D. Levine. </author> <title> Vectorizing compilers: A test suite and results. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <address> Orlando, FL, </address> <month> November </month> <year> 1988. </year>
Reference: [CF87] <author> R. Cytron and J. Ferrante. </author> <title> What's in a name? or the value of renaming for parallelism detection and storage allocation. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1987. </year>
Reference-contexts: To determine if these conditions hold for scalar variables requires general data-flow information. When testing these conditions for arrays, data-flow and dependence information are required. General data-flow analysis is sophisticated enough to determine these conditions for scalars in loops containing control flow <ref> [CF87] </ref>. 55 4.3.6 Scalar expansion In vectorization, scalar expansion is often preferable to privatization. However, it is not clear in many cases which of the two is preferred. Scalar expansion has similar, but slightly less restrictive constraints than scalar privatization.
Reference: [CFR + 89] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Austin, TX, </address> <month> January </month> <year> 1989. </year>
Reference-contexts: Unfortunately, incremental dependence analysis after edits is a very difficult problem. Precise dependence analysis requires utilization of several different kinds of information. In order to calculate precise dependence information, Ped may need to 29 incrementally update the control flow graph, control dependence graph, static single assignment graph (ssa) <ref> [CFR + 89] </ref>, and call graphs, as well as recalculate scalar live range, constant, symbolic, interprocedural, and dependence testing information. <p> More advanced interprocedural and symbolic analysis is planned [HP90]. Parafrase-2 uses Faust as a front end to provide interactive parallelization and graphical displays [GGGJ88]. Ptran is also an automatic parallelizer with extensive program analysis. It computes the ssa and program dependence graphs, and performs constant propagation and interprocedural analysis <ref> [CFR + 89, FOW87] </ref>.
Reference: [CFS90] <author> R. Cytron, J. Ferrante, and V. Sarkar. </author> <title> Experiences using control dependence in PTRAN. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: For example, any of the code generation algorithms based on the program dependence graph could be used in conjunction with the above algorithm <ref> [FM85, FMS88, CFS90, BB89] </ref>. The very simple code generation scheme described here has been is implemented in the ParaScope Editor [KMT91a]. When transformations are applied in an interactive environment, it is important to retain as much similarity to the original program as possible.
Reference: [CHK92] <author> K. Cooper, M. W. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Computer Language, </booktitle> <address> Oakland, CA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: To avoid code growth, multiple callers should share the same version of the optimized procedure whenever possible. This technique of generating multiple copies of a procedure and tailoring the copies to their calling environments is called procedure cloning <ref> [CKT86a, CHK92] </ref>. 5.3.2 Recompilation analysis A unique part of the ParaScope compilation system is its recompilation analysis, which avoids unnecessary recompilation after program edits. Recompilation analysis tests that interprocedural facts used to optimize a procedure have not been invalidated by editing changes [CKT86b, BC86, BCKT90]. <p> The parallelizer is extended in Section 7.6 to perform these interprocedural transformations. This version of the parallelizer does and must perform procedure cloning for correctness. 7.2.2 Procedure cloning Procedure cloning is required when optimizations along distinct call paths want to use different versions of the same procedure <ref> [CKT86a, CHK92] </ref>. In our algorithm, this situation only occurs when a procedure is called more than once. Consider again Example 7.1. Procedure S may be called once directly and once from inside the I loop.
Reference: [CHT91] <author> K. Cooper, M. W. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <journal> Software|Practice and Experience, </journal> <volume> 21(6) </volume> <pages> 581-601, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In general, summary analysis for loop parallelization is less precise than the analysis of inlined code. On the other hand, inlining can yield an increase in code size which may disastrously increase compile time and seriously inhibit separate compilation <ref> [CHT91, RG89] </ref>. Furthermore, inlining may cause a loss of precision in dependence analysis due to the complexity of subscripts that result from array parameter reshapes. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution <ref> [AJ90, CHT91, Hus82] </ref> and interprocedural analysis of array side effects [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86]. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency.
Reference: [CK87a] <author> D. Callahan and M. Kalem. </author> <title> Control dependences. Supercomputer Software Newsletter 15, </title> <institution> Dept. of Computer Science, Rice University, </institution> <month> October </month> <year> 1987. </year>
Reference-contexts: They discuss three transformations that restructure control flow: loop fusion, dead code elimination, and branch deletion. Callahan and Kalem present two methods for generating loop distributions in the presence of control flow <ref> [CK87a] </ref>. The first, which works for structured or unstructured control flow, replicates the control flow of the original loop in each of the new loops by using G f . Branch variables are inserted to record decisions made in one loop and used in other loops.
Reference: [CK87b] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: PFC has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [CCKT86], interprocedural side-effect analysis of scalars [CKT86a], and inter-procedural array section analysis <ref> [CK87b, HK91] </ref>. Ped expands on PFC's analysis and transformation capabilities and makes them available to the user in an interactive environment. Because of its mature analysis and implementation, PFC is available as a dependence information server for the ParaScope Editor. <p> Regular section analysis is a sophisticated form of interprocedural analysis which makes 62 it possible to parallelize loops with calls (see Section 2.3). It determines if the side effects to arrays as a result of each call are limited to nonintersecting subarrays on different loop iterations <ref> [CK87b, HK90] </ref>. 2. Interprocedural transformation is the process of moving code across procedure boundaries, either as an optimization or to enable other optimizations. The most common form of interprocedural transformation is procedure inlining. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution [AJ90, CHT91, Hus82] and interprocedural analysis of array side effects <ref> [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency.
Reference: [CK89] <author> S. Carr and K. Kennedy. </author> <title> Blocking linear algebra codes for memory hierarchies. </title> <booktitle> In Proceedings of the Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Chicago, IL, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Used alone the order of the iterations is unchanged, but used in concert with loop interchange the iteration space may be tiled [Wol89a] to utilize memory bandwidth and cache more effectively <ref> [CK89] </ref>. * Scalar replacement takes array references with consistent dependences and replaces them with scalar temporaries that may be allocated into registers [CCK90].
Reference: [CKK89] <author> D. Callahan, K. Kennedy, and U. Kremer. </author> <title> A dynamic study of vector-ization in PFC. </title> <type> Technical Report TR89-97, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: analysis and algorithms presented in this thesis relieve the programmer of the burden of explicit parallel programming for a variety of shared-memory parallel machines. 8.1 Introduction A lesson to be learned from vectorization is that programmers rewrote their programs in a vectorizable style based on feedback from their vectorizing compilers <ref> [CKK89, Wol89c] </ref>. Compilers were then able to take these programs and generate machine-dependent vector code with excellent results. We are testing this same thesis for shared-memory parallel machines. The experiment described below considers the automatic parallelization of sequential program versions where parallelism is known to exist.
Reference: [CKPK90] <author> G. Cybenko, L. Kipp, L. Pointer, and D. Kuck. </author> <title> Supercomputer performance evaluation and the Perfect benchmarks. </title> <booktitle> In Proceedings of the 148 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: called routine that do not carry any dependences, the augmented call graph reflects it and many permutations can be shown safe without additional dependence testing. 5.6 Experimental results This section presents significant performance improvements due to interprocedural transformation on two scientific programs, spec77 and ocean, taken from the Perfect Benchmarks <ref> [CKPK90] </ref>. To locate opportunities for transformations, we browsed the dependences in the program using the ParaScope Editor [BKK + 89, KMT91a, KMT91b]. Using other ParaScope tools, we determined which procedures in the program contained procedure calls. We examined the procedures containing calls, looking for interesting call structures. <p> These loops may be an artifact of a vectorizable programming style. They appear frequently in the Perfect benchmarks <ref> [CKPK90] </ref>, the Level 2 BLAS [DCHH88], and the Livermore loops [McM86]. Table 6.6 illustrates the performance benefits with the organization of dmxpy generated by our algorithm on matrices of size 200 fi 200 on 19 processors.
Reference: [CKT86a] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the IR n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: PFC has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [CCKT86], interprocedural side-effect analysis of scalars <ref> [CKT86a] </ref>, and inter-procedural array section analysis [CK87b, HK91]. Ped expands on PFC's analysis and transformation capabilities and makes them available to the user in an interactive environment. Because of its mature analysis and implementation, PFC is available as a dependence information server for the ParaScope Editor. <p> Figure 5.1 (c) illustrates the dad annotations for the program in Example 5.1. 5.3 Support for interprocedural optimization In this section, we present the compilation system of the ParaScope Programming Environment <ref> [CCH + 88, CKT86a] </ref>. This system was designed for the efficient support of interprocedural analysis and optimization. The tools in ParaScope cooperate to enable the compilation system to perform interprocedural analysis without direct examination of source code. <p> In this way, the information needed from each module of source code is available at all times and need not be derived on every compilation. Interprocedural optimization is orchestrated by the program compiler, a tool that manages and provides information about the whole program <ref> [CKT86a, Hal91] </ref>. The program compiler first builds the augmented call graph described in Section 2.3. The program compiler then traverses the augmented call graph, performing inter-procedural analysis, and subsequently, code generation. <p> To avoid code growth, multiple callers should share the same version of the optimized procedure whenever possible. This technique of generating multiple copies of a procedure and tailoring the copies to their calling environments is called procedure cloning <ref> [CKT86a, CHK92] </ref>. 5.3.2 Recompilation analysis A unique part of the ParaScope compilation system is its recompilation analysis, which avoids unnecessary recompilation after program edits. Recompilation analysis tests that interprocedural facts used to optimize a procedure have not been invalidated by editing changes [CKT86b, BC86, BCKT90]. <p> The parallelizer is extended in Section 7.6 to perform these interprocedural transformations. This version of the parallelizer does and must perform procedure cloning for correctness. 7.2.2 Procedure cloning Procedure cloning is required when optimizations along distinct call paths want to use different versions of the same procedure <ref> [CKT86a, CHK92] </ref>. In our algorithm, this situation only occurs when a procedure is called more than once. Consider again Example 7.1. Procedure S may be called once directly and once from inside the I loop.
Reference: [CKT86b] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: This information is then used in code generation to make decisions about interprocedural optimizations. The code generator only examines the dependence graph for the procedure currently being compiled, not the graph for the entire program. In addition, ParaScope employs recompilation analysis after program changes to minimize program reanalysis <ref> [CKT86b] </ref>. This system was original intended for scalar compilation. This section extend the ParaScope system to support parallel code generation. 5.3.1 The ParaScope compilation system Interprocedural analysis in the ParaScope compilation system consists of two principal phases. The first takes place prior to compilation. <p> Recompilation analysis tests that interprocedural facts used to optimize a procedure have not been invalidated by editing changes <ref> [CKT86b, BC86, BCKT90] </ref>. To extend recompilation analysis for interprocedural transformations, a few additions are needed. When an interprocedural transformation is performed, a description of the interprocedural 69 transformations annotates the nodes and edges in the augmented call graph.
Reference: [CSY90] <author> D. Chen, H. Su, and P. Yew. </author> <title> The impact of synchronization and granularity on parallel systems. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The transformations are loop interchange, loop skewing, loop distribution, and unroll and jam. We discuss related work and conclude. 3.2 Work Model This thesis exploits loop-level parallelism, which comprises most of the usable parallelism in scientific codes when synchronization costs are considered <ref> [CSY90] </ref>. In the work model best supported by Ped, the user first selects a loop for parallelization. Ped then displays all of its loop-carried data dependences. Other dependences, such 14 as control dependences, may also be displayed at the option of the user.
Reference: [Cyt86] <author> R. Cytron. </author> <title> Doacross: Beyond vectorization for multiprocessors. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Most that were not contained unordered critical sections. Programs that use synchronization in order to perform loops in parallel, such as "doacross" style parallelism and critical regions are not handled by our approach <ref> [Cyt86, Sub90, HKT92] </ref>. However, these loops are an important source of parallelism that should be addressed. We have not considered the more challenging issues which arise on shared-memory machines with non-uniform access time such as the TC2000 Butterfly. Nor have we considered distributed memory architectures such as the Intel Hypercube.
Reference: [DBMS79] <author> J. Dongarra, J. Bunch, C. Moler, and G. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <year> 1979. </year>
Reference-contexts: To illustrate this point, consider the subroutine dmxpy from Linpackd written in memory order <ref> [DBMS79] </ref>. DO J = JMIN, N2 DO I = 1, N1 101 The J loop is not parallel. The I loop can be parallel. Both contain reuse. A simple parallelization that maximizes granularity would interchange the two loops and make the I loop parallel without strip mining. <p> Dr. Li indicated the I/O was for development purposes and could be ignored. A.6 Linpackd Benchmark The Linpackd benchmark is a representative set of linear algebra routines that are widely used by scientists and engineers to perform numerical operations on matrices <ref> [DBMS79] </ref>. We used a 200 fi200 matrix size for our experiment. This program was written by Jack Dongarra, at the University of Tennessee. Modifications or assertions This program was originally a sequential version. From this version, we derived the automatically parallelized version.
Reference: [DCHH88] <author> J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. </author> <title> An extended set of Fortran basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14(1) </volume> <pages> 1-17, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: These loops may be an artifact of a vectorizable programming style. They appear frequently in the Perfect benchmarks [CKPK90], the Level 2 BLAS <ref> [DCHH88] </ref>, and the Livermore loops [McM86]. Table 6.6 illustrates the performance benefits with the organization of dmxpy generated by our algorithm on matrices of size 200 fi 200 on 19 processors.
Reference: [Die88] <author> H. Dietz. </author> <title> Finding large-grain parallelism in loops with serial control dependences. </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: Branch variables are inserted to record decisions made in one loop and used in other loops. An additional pass then trims the new loops of any empty control flow. Dietz uses a very similar approach <ref> [Die88] </ref>. These approaches have some of the same drawbacks of if-conversion, i.e. the proliferation of unnecessary guards. Callahan and Kalem's second method, which works only for structured control flow, uses G f , G cd , and Boolean execution variables.
Reference: [DKMC92] <author> E. Darnell, K. Kennedy, and J. Mellor-Crummey. </author> <title> Automatic software cache coherence through vectorization. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Implementing and designing Ped also provided insight into the analysis and transformations and a testbed for experimenting with different automatic techniques. Indeed, Ped is proving to be a valuable platform for compiling for other parallel architectures as well <ref> [HKK + 91, DKMC92, HK92] </ref>, illustrating the usefulness of this type of tool for developing compilers for new types of architectures. At the same time however, we pursued more advanced and general compiler techniques for shared-memory multiprocessors.
Reference: [DT91] <author> J. E. Dennis, Jr. and V. Torczon. </author> <title> Direct search methods on parallel machines. </title> <journal> SIAM Journal of Optimization, </journal> <volume> 1(4) </volume> <pages> 448-474, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: However, as can be seen in the results section, the critical section formed a bottleneck and actually degraded performance beyond that of the sequential performance. A.3 Direct Search Method This program is a derivative-free parallel algorithm for the nonlinear unconstrained optimization problem <ref> [DT91] </ref>. It was written by Virginia Torczon at Rice University. It searches based on the previous function values, where the function is continuous on a compact level set. <p> We performed dead code elimination by hand using constant propagation to delete some special case code for nonunit stride accesses. A.7 Multidirectional Search Method The parallel multidirectional search method is a more powerful and general version of the direct search method described above <ref> [DT91] </ref>. It differs in that the parallelism available in the algorithm is proportional to both the size of the problem and the size of the search space. The search space is based on the number of processors.
Reference: [EB91] <author> R. Eigenmann and W. Blume. </author> <title> An effectiveness study of parallelizing compiler techniques. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: In this chapter, we summarize what was achieved in pursuit of supporting the thesis. Both the successes and limitations are presented. In addition, the implications of this work for programmers, other architectures, and future work are described. Due to the limited success of other researchers attacking this problem <ref> [EB91, SH91, Sar90] </ref>, we were not confident when we began that acceptable performance would result from automatic techniques. Therefore, we concentrated much effort on designing and implementing Ped. During this process, we developed fast incremental update algorithms for many transformations.
Reference: [FKMW90] <author> K. Fletcher, K. Kennedy, K. S. M c Kinley, and S. Warren. </author> <title> The Para-Scope Editor: User interface goals. </title> <type> Technical Report TR90-113, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1990. </year> <month> 149 </month>
Reference-contexts: A tool with this much functionality is bound to be complex. Ped incorporates a complete source editor and supports dependence analysis, dependence display, and a large variety of program transformations to enhance parallelism. We describe in detail elsewhere the usage, user interface and motivation of the ParaScope Editor <ref> [BKK + 89, FKMW90, KMT91b] </ref>. We also cover elsewhere the types of analyses and representations needed to support this tool and automatic parallelization (see Section 2) [KMT91a]. In this chapter, we focus on efficient algorithms for incremental updates after a transformation or edit. <p> Otherwise, the dependence is pre-marked as pending, signifying to the user that it may be the result of overly-conservative analysis. Additionally, the user may mark pending dependences as accepted, indicating that the dependence does in fact occur. A similar facility is provided for variable classification <ref> [FKMW90] </ref>. These provide users with a powerful mechanism for experimenting with different parallelization strategies. Ped's user interface is shown in Figure 3.1. The figure shows a black and white screen dump of a color Ped session.
Reference: [FM85] <author> J. Ferrante and M. Mace. </author> <title> On linearizing parallel code. </title> <booktitle> In Conference Record of the Twelfth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> New Orleans, LA, </address> <month> January </month> <year> 1985. </year>
Reference-contexts: For example, any of the code generation algorithms based on the program dependence graph could be used in conjunction with the above algorithm <ref> [FM85, FMS88, CFS90, BB89] </ref>. The very simple code generation scheme described here has been is implemented in the ParaScope Editor [KMT91a]. When transformations are applied in an interactive environment, it is important to retain as much similarity to the original program as possible. <p> A straightforward relabeling of the first partition in Example 4.2 after restructuring results in the following. DO I = 1, N S 1 IF (EV 1 [I] .EQ. TRUE) GOTO 6 S 2 ENDDO Structured code generation When G cd is a tree, code generation is relatively simple <ref> [FM85, FMS88, BB89] </ref>. This discussion emphasizes properly selecting and inserting the appropriate control structures for newly created guards. Other G cd code generation algorithms must select and create control structures for all branches. <p> However, 59 their algorithms are applicable only for structured control flow. Neither Towle or Ferrante et al. present loop distribution. Ferrante, Mace, and Simons present related algorithms whose goals are to avoid replication and branch variables when possible <ref> [FM85, FMS88] </ref>. Their code generation algorithms convert parallel programs into sequential ones, and like ours, are based on G cd . They discuss three transformations that restructure control flow: loop fusion, dead code elimination, and branch deletion.
Reference: [FMS88] <author> J. Ferrante, M. Mace, and B. Simons. </author> <title> Generating sequential code from parallel code. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: For example, any of the code generation algorithms based on the program dependence graph could be used in conjunction with the above algorithm <ref> [FM85, FMS88, CFS90, BB89] </ref>. The very simple code generation scheme described here has been is implemented in the ParaScope Editor [KMT91a]. When transformations are applied in an interactive environment, it is important to retain as much similarity to the original program as possible. <p> A straightforward relabeling of the first partition in Example 4.2 after restructuring results in the following. DO I = 1, N S 1 IF (EV 1 [I] .EQ. TRUE) GOTO 6 S 2 ENDDO Structured code generation When G cd is a tree, code generation is relatively simple <ref> [FM85, FMS88, BB89] </ref>. This discussion emphasizes properly selecting and inserting the appropriate control structures for newly created guards. Other G cd code generation algorithms must select and create control structures for all branches. <p> However, 59 their algorithms are applicable only for structured control flow. Neither Towle or Ferrante et al. present loop distribution. Ferrante, Mace, and Simons present related algorithms whose goals are to avoid replication and branch variables when possible <ref> [FM85, FMS88] </ref>. Their code generation algorithms convert parallel programs into sequential ones, and like ours, are based on G cd . They discuss three transformations that restructure control flow: loop fusion, dead code elimination, and branch deletion.
Reference: [FOW87] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: More advanced interprocedural and symbolic analysis is planned [HP90]. Parafrase-2 uses Faust as a front end to provide interactive parallelization and graphical displays [GGGJ88]. Ptran is also an automatic parallelizer with extensive program analysis. It computes the ssa and program dependence graphs, and performs constant propagation and interprocedural analysis <ref> [CFR + 89, FOW87] </ref>. <p> If vectorization or parallelization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals. For these reasons, research in automatic parallelization has concentrated on an alternative approach that uses control dependences to model control flow <ref> [FOW87, ABC + 87, ABC + 88] </ref>. Our approach uses both data and control dependence graphs (as were defined in Sections 2.1 and 2.1). <p> However, his definition of control dependence embeds, but does not extract the essential control relationship between two statements, that is if the execution of one statement directly determines the execution of another. Control dependence as formulated by Ferrante, Ottenstein, and Warren is clean, complete, and extracts this essential relationship <ref> [FOW87] </ref>. They include control and data dependences in the program dependence graph, pdg, and our approach uses the same basis. Their paper also discusses several optimizing transformations performed on the pdg: node splitting, code motion, loop fusion, and loop peeling.
Reference: [FST91] <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: Ferrante et al. present a more general formula that also approximates the number of cache lines accessed and is applicable across a wider range of loops <ref> [FST91] </ref>. However, they first compute an estimate for every array reference and then combine them, trying not to do dependence testing. Like Wolf and Lam, they exhaustively search for a loop permutation with the lowest estimated cost. Many algorithms have been proposed in the literature for introducing parallelism into programs.
Reference: [GGGJ88] <author> V. Guarna, D. Gannon, Y. Gaur, and D. Jablonowski. </author> <title> Faust: An environment for programming parallel scientific applications. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <address> Orlando, FL, </address> <month> November </month> <year> 1988. </year>
Reference-contexts: Parafrase-2 adds scheduling and improved program analysis and transformations [PGH + 90]. More advanced interprocedural and symbolic analysis is planned [HP90]. Parafrase-2 uses Faust as a front end to provide interactive parallelization and graphical displays <ref> [GGGJ88] </ref>. Ptran is also an automatic parallelizer with extensive program analysis. It computes the ssa and program dependence graphs, and performs constant propagation and interprocedural analysis [CFR + 89, FOW87]. <p> Ptran introduces both task and loop parallelism, but the only other program transformations are variable privatiza-tion and loop distribution [ABC + 87, Sar90]. 32 Sigmacs, a programmable interactive parallelizer in the Faust programming environment, computes and displays call graphs, process graphs, and a statement dependence graph <ref> [GGGJ88, SG90] </ref>. In a process graph each node represents a task or a process, which is a separate entity running in parallel. The call and process graphs may be animated dynamically at run time.
Reference: [GJG87] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformations. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference: [GJG88] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: For reasonably large computations, references such as C (J,I) do not 84 provide any reuse on the I loop, because the desired cache lines have been flushed by intervening memory accesses. Previous researchers have studied techniques for improving locality of accesses for registers, cache, and pages <ref> [AS79, CCK90, WL91, GJG88] </ref>. We concentrate on improving the locality of accesses for cache; i.e. we attempt to increases the locality of access to the same cache line. Empirical results show that improving spatial reuse can be significantly more effective than techniques that consider temporal reuse alone [KMT92]. <p> Skewing in particular is undesirable because it reduces spatial reuse. Gannon et al. also formulate the dependence testing problem to give reuse and volumetric information about array references <ref> [GJG88] </ref>. This information is then 106 used to tile and interchange the loop nests for cache, after which parallelism is inserted at the outermost possible position. They do not consider how the parallelism affects the volumetric information nor if interchange would improve the granularity of parallelism.
Reference: [GKT91] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: PFC 's analysis is more mature and includes important features not yet implemented in Ped. It performs advanced dependence tests which include symbolics dependence tests and it computes interprocedural constants, interprocedural symbolics and inter-procedural mod and ref information for simple array sections <ref> [GKT91, HK90, HK91] </ref>. PFC produces a file of dependence information that is converted into Ped's internal representations. 130 In Ped we used the call graph, program analysis and the transformations that Ped provides, to meticulously apply Driver (the parallelization algorithm from Chapter 7) to each of the programs by hand.
Reference: [GP84] <author> A. Goldberg and R. Paige. </author> <title> Stream processing. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 228-234, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: This transforms the loop distribution to one of loop fusion, a problem thought to be very hard. In fact, Goldberg and Paige prove a nearby fusion problem NP-Hard, but their result is not applicable here <ref> [GP84] </ref>. The algorithm we present is linear. In the loop fusion problem for parallelism, the partitions begin as separate loops that are either parallel or sequential and may or may not be legally fused together.
Reference: [Hal91] <author> M. W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: In this way, the information needed from each module of source code is available at all times and need not be derived on every compilation. Interprocedural optimization is orchestrated by the program compiler, a tool that manages and provides information about the whole program <ref> [CKT86a, Hal91] </ref>. The program compiler first builds the augmented call graph described in Section 2.3. The program compiler then traverses the augmented call graph, performing inter-procedural analysis, and subsequently, code generation. <p> As a result, dependence analysis is only applied to procedures where it is no longer valid, allowing separate compilation to be preserved. The recompilation process after interprocedural transformations have been applied is described in more detail elsewhere <ref> [Hal91] </ref>. 5.4 Interprocedural transformation Loop extraction and loop embedding expose the loop structure to optimization without incurring the costs of inlining. Just as inlining is always safe, these transformations are always safe. The mechanics of performing the movement of a loop header is detailed below.
Reference: [HHK + 93] <author> M. W. Hall, T. Harvey, K. Kennedy, N. McIntosh, K. S. M c Kinley, J. D. Oldham, M. Paleczny, and G. Roth. </author> <title> Experiences using the ParaScope Editor: an interactive parallel programming tool. </title> <booktitle> In Proceedings of the 150 Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: It also enables users to experiment with different mixtures of transformations without reanalyzing the entire program between transformations. Our experience with the ParaScope Editor has shown that dependence analysis can be used in an interactive tool with ample efficiency <ref> [HHK + 93] </ref>. This efficiency is due 33 to fast yet precise dependence analysis algorithms, and a dependence representation that makes it easy to find dependences and to reconstruct them after a change. <p> In Erlebacher, the parallelism was not made explicit. Here, a naive parallelization of outer loops was performed to create the parallel version. 8.2.3 Creating an automatically parallelized version To create an automatically parallelized program, the nearby sequential program was first imported into the ParaScope Programming Environment <ref> [CCH + 88, HHK + 93] </ref>. As a result of importing the program, each procedure in the program was placed in a separate module. Also, a program composition was automatically created that describes the entire program and the call graph was built.
Reference: [HHL90a] <author> L. Huelsbergen, D. Hahn, and J. Larus. </author> <title> Exact dependence analysis using data access descriptors. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution [AJ90, CHT91, Hus82] and interprocedural analysis of array side effects <ref> [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency.
Reference: [HHL90b] <author> L. Huelsbergen, D. Hahn, and J. Larus. </author> <title> Exact dependence analysis using data access descriptors. </title> <type> Technical Report 945, </type> <institution> Dept. of Computer Science, University of Wisconsin at Madison, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution [AJ90, CHT91, Hus82] and interprocedural analysis of array side effects <ref> [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency.
Reference: [HK90] <author> P. Havlak and K. Kennedy. </author> <title> Experience with interprocedural analysis of array side effects. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Regular section analysis is a sophisticated form of interprocedural analysis which makes 62 it possible to parallelize loops with calls (see Section 2.3). It determines if the side effects to arrays as a result of each call are limited to nonintersecting subarrays on different loop iterations <ref> [CK87b, HK90] </ref>. 2. Interprocedural transformation is the process of moving code across procedure boundaries, either as an optimization or to enable other optimizations. The most common form of interprocedural transformation is procedure inlining. <p> Inlining substitutes the body of a called procedure for the procedure call and optimizes it as a part of the calling procedure [AC72]. Even though regular section analysis and inlining are frequently successful at enabling optimization, each of these methods has its limitations <ref> [HK90, LY88a, Hus82] </ref>. Compilation time and space considerations require that regular section analysis summarize array side effects. In general, summary analysis for loop parallelization is less precise than the analysis of inlined code. <p> Sections represent a restricted set of the most commonly occurring array access patterns; single elements, rows, columns, grids and their higher dimensional analogs. This restriction on the shapes assists in making the implementation efficient <ref> [HK90] </ref>. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution [AJ90, CHT91, Hus82] and interprocedural analysis of array side effects <ref> [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency. <p> However, these properties make it efficient enough to be widely used by code generation. In addition, experiments with regular section analysis on the Linpack library demonstrated a 33 percent reduction in parallelism-inhibiting dependences, allowing 31 loops containing calls to be parallelized <ref> [HK90] </ref>. Comparing these numbers against published results of more precise techniques, there was no benefit to be gained by the increased precision of the other techniques [LY88a, LY88b, TIF86]. 5.8 Discussion The usefulness of this approach has been illustrated on the Perfect Benchmark programs spec77 and ocean. <p> PFC 's analysis is more mature and includes important features not yet implemented in Ped. It performs advanced dependence tests which include symbolics dependence tests and it computes interprocedural constants, interprocedural symbolics and inter-procedural mod and ref information for simple array sections <ref> [GKT91, HK90, HK91] </ref>. PFC produces a file of dependence information that is converted into Ped's internal representations. 130 In Ped we used the call graph, program analysis and the transformations that Ped provides, to meticulously apply Driver (the parallelization algorithm from Chapter 7) to each of the programs by hand.
Reference: [HK91] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: PFC has a mature dependence analyzer which performs data dependence analysis, control dependence analysis, interprocedural constant propagation [CCKT86], interprocedural side-effect analysis of scalars [CKT86a], and inter-procedural array section analysis <ref> [CK87b, HK91] </ref>. Ped expands on PFC's analysis and transformation capabilities and makes them available to the user in an interactive environment. Because of its mature analysis and implementation, PFC is available as a dependence information server for the ParaScope Editor. <p> PFC 's analysis is more mature and includes important features not yet implemented in Ped. It performs advanced dependence tests which include symbolics dependence tests and it computes interprocedural constants, interprocedural symbolics and inter-procedural mod and ref information for simple array sections <ref> [GKT91, HK90, HK91] </ref>. PFC produces a file of dependence information that is converted into Ped's internal representations. 130 In Ped we used the call graph, program analysis and the transformations that Ped provides, to meticulously apply Driver (the parallelization algorithm from Chapter 7) to each of the programs by hand.
Reference: [HK92] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Relaxing SIMD control flow constraints using loop transformations. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Implementing and designing Ped also provided insight into the analysis and transformations and a testbed for experimenting with different automatic techniques. Indeed, Ped is proving to be a valuable platform for compiling for other parallel architectures as well <ref> [HKK + 91, DKMC92, HK92] </ref>, illustrating the usefulness of this type of tool for developing compilers for new types of architectures. At the same time however, we pursued more advanced and general compiler techniques for shared-memory multiprocessors.
Reference: [HKK + 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <editor> In U. Banerjee, D. Gel-ernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: Implementing and designing Ped also provided insight into the analysis and transformations and a testbed for experimenting with different automatic techniques. Indeed, Ped is proving to be a valuable platform for compiling for other parallel architectures as well <ref> [HKK + 91, DKMC92, HK92] </ref>, illustrating the usefulness of this type of tool for developing compilers for new types of architectures. At the same time however, we pursued more advanced and general compiler techniques for shared-memory multiprocessors.
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: The code generator is part of an interprocedural compilation system that efficiently supports interprocedural analysis and optimization by retaining separate compilation of procedures. We first explored this type of system using a simple, performance estimation based parallel code generation algorithm <ref> [HKT91] </ref>. This chapter provides a more general framework and that is integrated into a more sophisticated paral-lelization algorithm discussed in Chapter 7.
Reference: [HKT92] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Most that were not contained unordered critical sections. Programs that use synchronization in order to perform loops in parallel, such as "doacross" style parallelism and critical regions are not handled by our approach <ref> [Cyt86, Sub90, HKT92] </ref>. However, these loops are an important source of parallelism that should be addressed. We have not considered the more challenging issues which arise on shared-memory machines with non-uniform access time such as the TC2000 Butterfly. Nor have we considered distributed memory architectures such as the Intel Hypercube. <p> Nor have we considered distributed memory architectures such as the Intel Hypercube. However, other research has shown that many of the same solutions prove effective on both shared-memory and distributed-memory machines [LS90]. For example, the data locality algorithm will be used in a compiler to improve distributed memory performance <ref> [HKT92] </ref>. Compiling for these architectures is more difficult, but we believe future work will show our techniques to be applicable and that they will serve as a stepping stone in compilation for these machines. This is not the end. It is not even the beginning of the end.
Reference: [HP90] <author> M. Haghighat and C. Polychronopoulos. </author> <title> Symbolic dependence analysis for high-performance parallelizing compilers. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher> <pages> 151 </pages>
Reference-contexts: Batch analysis is performed after each transformation phase to update the dependence information for the entire program. Parafrase-2 adds scheduling and improved program analysis and transformations [PGH + 90]. More advanced interprocedural and symbolic analysis is planned <ref> [HP90] </ref>. Parafrase-2 uses Faust as a front end to provide interactive parallelization and graphical displays [GGGJ88]. Ptran is also an automatic parallelizer with extensive program analysis. It computes the ssa and program dependence graphs, and performs constant propagation and interprocedural analysis [CFR + 89, FOW87].
Reference: [Hus82] <author> C. A. Huson. </author> <title> An inline subroutine expander for Parafrase. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: Inlining substitutes the body of a called procedure for the procedure call and optimizes it as a part of the calling procedure [AC72]. Even though regular section analysis and inlining are frequently successful at enabling optimization, each of these methods has its limitations <ref> [HK90, LY88a, Hus82] </ref>. Compilation time and space considerations require that regular section analysis summarize array side effects. In general, summary analysis for loop parallelization is less precise than the analysis of inlined code. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution <ref> [AJ90, CHT91, Hus82] </ref> and interprocedural analysis of array side effects [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86]. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency.
Reference: [IT88] <author> F. Irigoin and R. Triolet. </author> <title> Supernode partitioning. </title> <booktitle> In Proceedings of the Fifteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: Finally, the interchange flags are recalculated for dependences in the loop nest. 3.4.2 Loop skewing Loop skewing is a transformation that changes the shape of the iteration space to expose parallelism across a wavefront <ref> [IT88, Lam74, Mur71, Wol86] </ref>. It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain 22 dependences and iteration space after skewbefore skew i i effective loop-level parallelism in a loop nest [Ban90b, WL90, Wol89a]. All of these transformations are supported in Ped. <p> Tiling combines strip mining and loop interchange to promote reuse across a loop nest <ref> [IT88, Wol89a] </ref>. For matrix multiply, the loop nest may be tiled by strip mining the K loop by TS and then interchanging it with J.
Reference: [KKLW80a] <author> D. Kuck, R. Kuhn, B. Leasure, and M. J. Wolfe. </author> <title> Analysis and transformation of programs for parallel computation. </title> <booktitle> In Proceedings of COMP-SAC 80, the 4th International Computer Software and Applications Conference, </booktitle> <pages> pages 709-715, </pages> <address> Chicago, IL, </address> <month> October </month> <year> 1980. </year>
Reference-contexts: It breaks output and anti dependences which may be inhibiting parallelism <ref> [KKLW80a] </ref>. * Array renaming, also known as node splitting [KKLW80a], is used to break anti dependences by copying the source of an anti dependence into a newly introduced temporary array and renaming the sink to the new array [AK87]. <p> It breaks output and anti dependences which may be inhibiting parallelism <ref> [KKLW80a] </ref>. * Array renaming, also known as node splitting [KKLW80a], is used to break anti dependences by copying the source of an anti dependence into a newly introduced temporary array and renaming the sink to the new array [AK87]. <p> This algorithm is discussed in detail below. If step (1) is unsuccessful, step (2) attempts to satisfy as many dependences as possible with BreakDependences. The literature includes a collection of transforma tions that are used to satisfy specific dependences that inhibit parallelism. They in clude loop peeling, scalar expansion <ref> [KKLW80a] </ref>, array renaming [AK87, KKLW80a], alignment and replication [Cal87], and loop splitting [AK87]. These transformations may introduce new storage to eliminate storage-related anti or output dependences, or convert loop-carried dependences to loop-independent 115 dependences, often enabling the safe application of other transformations. <p> If step (1) is unsuccessful, step (2) attempts to satisfy as many dependences as possible with BreakDependences. The literature includes a collection of transforma tions that are used to satisfy specific dependences that inhibit parallelism. They in clude loop peeling, scalar expansion [KKLW80a], array renaming <ref> [AK87, KKLW80a] </ref>, alignment and replication [Cal87], and loop splitting [AK87]. These transformations may introduce new storage to eliminate storage-related anti or output dependences, or convert loop-carried dependences to loop-independent 115 dependences, often enabling the safe application of other transformations.
Reference: [KKLW80b] <author> D. Kuck, R. Kuhn, B. Leasure, and M. J. Wolfe. </author> <title> The structure of an advanced retargetable vectorizer. </title> <booktitle> In Proceedings of COMPSAC 80, the 4th International Computer Software and Applications Conference, </booktitle> <pages> pages 709-715, </pages> <address> Chicago, IL, </address> <month> October </month> <year> 1980. </year>
Reference: [KKLW84] <author> D. Kuck, R. Kuhn, B. Leasure, and M. J. Wolfe. </author> <title> The structure of an advanced retargetable vectorizer. In Supercomputers: </title> <booktitle> Design and Applications, </booktitle> <pages> pages 163-178. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> Silver Spring, MD, </address> <year> 1984. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for improving data locality. Each transformation is briefly introduced below. Many are found in the literature <ref> [AC72, AK87, CCK90, KM90, KMT91b, KKLW84, Lov77, Wol86] </ref>. In Ped, their novel aspect is the analysis of their applicability, safety, prof 17 itability and the incremental updates of source and dependence information. We classify the transformations implemented in Ped as follows. <p> The purpose, mechanics, and safety of these transformations are presented, followed by their profitability estimates, user advice, and incremental dependence update algorithms. 3.4.1 Loop interchange Loop interchange has been used extensively in vectorizing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism <ref> [AK87, KKLW84, Wol86] </ref>. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. Ped supports interchange of triangular or skewed loops. <p> On demand, the information provided by PFC is converted into the internal representations in ParaScope Editor. This functionality enables the use of PFC's more advanced analysis in Ped. Parafrase was the first automatic vectorizing compiler <ref> [KKLW84] </ref>. It supports program analysis and performs a large number of program transformations to improve parallelism. In Parafrase, program transformations are structured in phases and are always applied where applicable. Batch analysis is performed after each transformation phase to update the dependence information for the entire program. <p> This approach, called if-conversion [AKPW83, All83], has been used success 36 fully in a variety of vectorization systems which incorporate several other transformations as well <ref> [AK87, SK86, KKLW84] </ref>. However, if-conversion has several drawbacks. If vectorization or parallelization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals. <p> If-conversion is theoretically appealing because it allows for a unified treatment of data dependence without control dependences, and has been used successfully in a number of vectorization systems <ref> [KKLW84, SK86] </ref>. However, it has several drawbacks. In its original form, if-conversion is intractable and may introduce an exponential number of new logical arrays to record control flow decisions.
Reference: [KKP + 81] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: Loop skewing may be used with loop interchange in Ped to expose wavefront parallelism [KMT91a, Wol86]. * Loop reversal reverses the order of execution of loop iterations. * Loop fusion can increase the granularity of parallel regions and promote reuse by fusing two contiguous loops when dependences are not violated <ref> [AC72, KKP + 81] </ref>. * Statement interchange interchanges two adjacent independent statements. * Unroll and jam increases the potential candidates for scalar replacement and pipelining by unrolling the body of an outer loop in a loop nest and fusing the resulting inner loops [AC72, CCK90, CCK88, KMT91a]. 3.3.2 Dependence breaking transformations <p> the original distance vectors (d 1 ; d 2 ) for all dependences in the nest to (d 1 ; ffd 1 + d 2 ), and then updates their interchange flags. 3.4.3 Loop distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers <ref> [AK87, KKP + 81] </ref>. It is used to expose partial parallelism by separating statements which may be parallelized from those that must be executed sequentially. It is a cornerstone of vectorization and parallelization. In Ped the user can specify whether distribution is for the purpose of vectorization or parallelization. <p> The user may then apply or reject the distribution partition. Safety To maintain the meaning of the original loop, the partition must not put statements that are involved in recurrences into different loops <ref> [KM90, KKP + 81] </ref>. Recurrences are calculated by finding strongly connected regions in the subgraph composed of loop-independent dependences and dependences carried on the loop to be distributed. <p> The method we present is designed to work on any partition that is legal, i.e., any partition that preserves the control and data dependences. A partition can preserve all dependences if and only if there exists no dependence cycle spanning more than one output loop <ref> [KKP + 81, AK87] </ref>. If there is a cycle involving control and/or data dependences, it must be entirely contained within a single partition. 2 This condition is both necessary and sufficient. <p> Dependences are preserved if any statements involved in a dependence recurrence are placed in the same loop. The dependences between the partitions then form an directed acyclic graph that can always be ordered using topological sort <ref> [AK87, KKP + 81] </ref>. By first choosing a safe partition of the loops with the finest possible granularity and then fusing partitions back together larger partitions may be formed. This transforms the loop distribution to one of loop fusion, a problem thought to be very hard.
Reference: [KM90] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Loop distribution with arbitrary control flow. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> Novem-ber </month> <year> 1990. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for improving data locality. Each transformation is briefly introduced below. Many are found in the literature <ref> [AC72, AK87, CCK90, KM90, KMT91b, KKLW84, Lov77, Wol86] </ref>. In Ped, their novel aspect is the analysis of their applicability, safety, prof 17 itability and the incremental updates of source and dependence information. We classify the transformations implemented in Ped as follows. <p> These may also be used to optimize data locality. * Loop distribution partitions independent statements inside a loop into multiple loops with identical headers. It is used to separate statements that may be parallelized from those that must be executed sequentially <ref> [KM90, KMT91a, Kuc78] </ref>. The partitioning of the statements is targeted to vector or parallel hardware as specified by the user. * Loop interchange interchanges the headers of two perfectly nested loops, changing the order in which the iteration space is traversed. <p> The user may then apply or reject the distribution partition. Safety To maintain the meaning of the original loop, the partition must not put statements that are involved in recurrences into different loops <ref> [KM90, KKP + 81] </ref>. Recurrences are calculated by finding strongly connected regions in the subgraph composed of loop-independent dependences and dependences carried on the loop to be distributed. <p> We also present an algorithm for generating code for the body of a loop after distribution. The algorithms are very fast, both asymptotically and practically. This algorithm is also described elsewhere <ref> [KM90] </ref> 4.2.1 Mechanics Loop distribution may be separated into a three-stage process: (1) the statements in the loop body are partitioned into groups to be placed in different output loops; (2) the control and data dependence graphs are restructured to effect the new loop organization and (3) an equivalent program is
Reference: [KM92] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Our approach has appeared elsewhere <ref> [KM92] </ref>. 6.11 Discussion We have addressed the problem of choosing the best loop ordering in a nest of loops for exploiting data locality and for generating parallel code for shared-memory multipro 107 cessors.
Reference: [KMC72] <author> D. Kuck, Y. Muraoka, and S. Chen. </author> <title> On the number of operations simultaneously executable in Fortran-like programs and their resulting speedup. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21(12):1293-1310, </volume> <month> De-cember </month> <year> 1972. </year>
Reference: [KMM91] <author> K. Kennedy, N. McIntosh, and K. S. M c Kinley. </author> <title> Static performance estimation in a parallelizing compiler. </title> <type> Technical Report TR91-174, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1991. </year> <month> 152 </month>
Reference-contexts: Our performance estimator predicts the cost of parallel and sequential performance using a loop model and a training set approach. The goal of our performance estimator is to assist in code generation for both shared and distributed memory multiprocessors <ref> [BFKK92, KMM91] </ref>. Modeling the target machines at an architectural level would require calculating an analytical model for each supported architecture. Instead our performance estimator uses a training set to characterize each architecture in a machine-independent fashion.
Reference: [KMT91a] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: We describe in detail elsewhere the usage, user interface and motivation of the ParaScope Editor [BKK + 89, FKMW90, KMT91b]. We also cover elsewhere the types of analyses and representations needed to support this tool and automatic parallelization (see Section 2) <ref> [KMT91a] </ref>. In this chapter, we focus on efficient algorithms for incremental updates after a transformation or edit. All of these algorithms are implemented in Ped. We begin with an overview of the existing work model and a description of the transformation process. <p> These may also be used to optimize data locality. * Loop distribution partitions independent statements inside a loop into multiple loops with identical headers. It is used to separate statements that may be parallelized from those that must be executed sequentially <ref> [KM90, KMT91a, Kuc78] </ref>. The partitioning of the statements is targeted to vector or parallel hardware as specified by the user. * Loop interchange interchanges the headers of two perfectly nested loops, changing the order in which the iteration space is traversed. <p> When loop in 18 terchange is safe, it can be used to adjust the granularity of parallel loops <ref> [AK87, KMT91a, Wol89b] </ref>. * Loop skewing adjusts the iteration space of two perfectly nested loops by shifting the work per iteration in order to expose parallelism. When possible, Ped computes and suggests the optimal skew degree. <p> When possible, Ped computes and suggests the optimal skew degree. Loop skewing may be used with loop interchange in Ped to expose wavefront parallelism <ref> [KMT91a, Wol86] </ref>. * Loop reversal reverses the order of execution of loop iterations. * Loop fusion can increase the granularity of parallel regions and promote reuse by fusing two contiguous loops when dependences are not violated [AC72, KKP + 81]. * Statement interchange interchanges two adjacent independent statements. * Unroll and <p> contiguous loops when dependences are not violated [AC72, KKP + 81]. * Statement interchange interchanges two adjacent independent statements. * Unroll and jam increases the potential candidates for scalar replacement and pipelining by unrolling the body of an outer loop in a loop nest and fusing the resulting inner loops <ref> [AC72, CCK90, CCK88, KMT91a] </ref>. 3.3.2 Dependence breaking transformations Dependence breaking transformations are used to satisfy specific dependences that inhibit parallelism. They may introduce new storage to eliminate storage-related anti or output dependences, or convert loop-carried dependences to loop-independent dependences, often enabling the safe application of other transformations. <p> It improves the performance of the program by reducing the number of memory accesses required. * Loop unrolling decreases loop overhead and increases potential candidates for scalar replacement by unrolling the body of a loop <ref> [AC72, KMT91a] </ref>. 20 3.3.4 Miscellaneous transformations Finally Ped has a few miscellaneous transformations. * Sequential $ Parallel converts a sequential DO loop into a parallel loop, and vice versa. * Loop bounds adjusting adjusts the upper and lower bounds of a loop by a constant. <p> For example, any of the code generation algorithms based on the program dependence graph could be used in conjunction with the above algorithm [FM85, FMS88, CFS90, BB89]. The very simple code generation scheme described here has been is implemented in the ParaScope Editor <ref> [KMT91a] </ref>. When transformations are applied in an interactive environment, it is important to retain as much similarity to the original program as possible. The programmer can more easily recognize and understand transformed code when it resembles the original. <p> To locate opportunities for transformations, we browsed the dependences in the program using the ParaScope Editor <ref> [BKK + 89, KMT91a, KMT91b] </ref>. Using other ParaScope tools, we determined which procedures in the program contained procedure calls. We examined the procedures containing calls, looking for interesting call structures.
Reference: [KMT91b] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Interactive parallel programming using the ParaScope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: A tool with this much functionality is bound to be complex. Ped incorporates a complete source editor and supports dependence analysis, dependence display, and a large variety of program transformations to enhance parallelism. We describe in detail elsewhere the usage, user interface and motivation of the ParaScope Editor <ref> [BKK + 89, FKMW90, KMT91b] </ref>. We also cover elsewhere the types of analyses and representations needed to support this tool and automatic parallelization (see Section 2) [KMT91a]. In this chapter, we focus on efficient algorithms for incremental updates after a transformation or edit. <p> Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for improving data locality. Each transformation is briefly introduced below. Many are found in the literature <ref> [AC72, AK87, CCK90, KM90, KMT91b, KKLW84, Lov77, Wol86] </ref>. In Ped, their novel aspect is the analysis of their applicability, safety, prof 17 itability and the incremental updates of source and dependence information. We classify the transformations implemented in Ped as follows. <p> To locate opportunities for transformations, we browsed the dependences in the program using the ParaScope Editor <ref> [BKK + 89, KMT91a, KMT91b] </ref>. Using other ParaScope tools, we determined which procedures in the program contained procedure calls. We examined the procedures containing calls, looking for interesting call structures.
Reference: [KMT92] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Improving data locality. </title> <type> Technical Report TR92-179, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: We concentrate on improving the locality of accesses for cache; i.e. we attempt to increases the locality of access to the same cache line. Empirical results show that improving spatial reuse can be significantly more effective than techniques that consider temporal reuse alone <ref> [KMT92] </ref>. In addition, consecutive memory access results in reuse at all levels of the memory hierarchy except for registers. 6.4.2 Simplifying assumptions To simplify analysis we make two assumptions. First, our loop cost function assumes that reuse occurs only across iterations of the innermost loop.
Reference: [Knu71] <author> D. Knuth. </author> <title> An empirical study of FORTRAN programs. </title> <journal> Software| Practice and Experience, </journal> <volume> 1 </volume> <pages> 105-133, </pages> <year> 1971. </year>
Reference: [Kuc78] <author> D. Kuck. </author> <title> The Structure of Computers and Computations, Volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: These may also be used to optimize data locality. * Loop distribution partitions independent statements inside a loop into multiple loops with identical headers. It is used to separate statements that may be parallelized from those that must be executed sequentially <ref> [KM90, KMT91a, Kuc78] </ref>. The partitioning of the statements is targeted to vector or parallel hardware as specified by the user. * Loop interchange interchanges the headers of two perfectly nested loops, changing the order in which the iteration space is traversed.
Reference: [KZBG88] <author> U. Kremer, H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Advanced tools and techniques for automatic parallelization. </title> <journal> Parallel Computing, </journal> <volume> 7 </volume> <pages> 387-393, </pages> <year> 1988. </year>
Reference-contexts: Superb provides a set of interactive program transformations, including transformations that exploit data parallelism. The user specifies a data partitioning, then node programs with the necessary send and receive operations are automatically generated. Algorithms are also described for incremental update of use-def and def-use chains following structured program transformations <ref> [KZBG88] </ref>. 3.8 Discussion The ParaScope Editor provides a complementary strategy to backup automatic par-allelization. In an integrated approach that makes the compiler algorithms available, as well as the individual transformations, the user may make assertions and see the results in the automatically generated version.
Reference: [Lam74] <author> L. Lamport. </author> <title> The parallel execution of DO loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> February </month> <year> 1974. </year>
Reference-contexts: Finally, the interchange flags are recalculated for dependences in the loop nest. 3.4.2 Loop skewing Loop skewing is a transformation that changes the shape of the iteration space to expose parallelism across a wavefront <ref> [IT88, Lam74, Mur71, Wol86] </ref>. It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain 22 dependences and iteration space after skewbefore skew i i effective loop-level parallelism in a loop nest [Ban90b, WL90, Wol89a]. All of these transformations are supported in Ped.
Reference: [Lea90] <author> B. Leasure, </author> <title> editor. PCF Fortran: Language Definition, version 3.1. </title> <booktitle> The Parallel Computing Forum, </booktitle> <address> Champaign, IL, </address> <month> August </month> <year> 1990. </year>
Reference: [LL92] <author> I. J. Lustig and G. Li. </author> <title> An implementation of a parallel primal-dual interior point method for multicommondity flow problems. </title> <type> Technical Report CRPC-TR92194, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: We used this as the nearby sequential version and hand performed parallelization on this version. To create the user parallelized version, we performed a naive parallelization of outermost parallel loops. A.5 Interior Point Method This program implements a primal-Dual predictor-corrector interior point method to solve multicommodity flow problems <ref> [LL92] </ref>. It was written by Guangye Li at Rice University and Irv Lustig at Princeton. This problem is a well known application of linear programming. The block structure of the constraint matrix is exploited via parallel computation.
Reference: [Lov77] <author> D. Loveman. </author> <title> Program improvement by source-to-source transformations. </title> <journal> Journal of the ACM, </journal> <volume> 17(2) </volume> <pages> 121-145, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for improving data locality. Each transformation is briefly introduced below. Many are found in the literature <ref> [AC72, AK87, CCK90, KM90, KMT91b, KKLW84, Lov77, Wol86] </ref>. In Ped, their novel aspect is the analysis of their applicability, safety, prof 17 itability and the incremental updates of source and dependence information. We classify the transformations implemented in Ped as follows.
Reference: [LRW91] <author> M. Lam, E. Rothberg, and M. E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: This optimization approach may be divided into three phases: 1. optimizing to improve data locality, 2. finding and positioning a parallel loop, and 3. performing low-level memory optimizations such as tiling for cache and placing references in registers <ref> [LRW91, CCK90] </ref>. This chapter focuses on the first two phases. We advocate the first two phases be followed by a low-level memory optimizing phase, but do not address it here. 6.2 Memory and language model Because we are evaluating reuse, we require some knowledge of the memory hierarchy. <p> Lam et al. show that this assumption may not hold if cache lines must remain live for longer periods of time. Considerable interference may take place when loops are tiled to increase reuse across outer loops <ref> [LRW91] </ref>. 6.4.3 Loop cost Given these assumptions, we present a loop cost function LoopCost based on our memory model. Its goal is to estimate the total number of cache lines accessed when a candidate loop l is positioned as the innermost loop.
Reference: [LS90] <author> C. Lin and L. Snyder. </author> <title> A comparison of programming models for shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year> <month> 153 </month>
Reference-contexts: Nor have we considered distributed memory architectures such as the Intel Hypercube. However, other research has shown that many of the same solutions prove effective on both shared-memory and distributed-memory machines <ref> [LS90] </ref>. For example, the data locality algorithm will be used in a compiler to improve distributed memory performance [HKT92].
Reference: [LY88a] <author> Z. Li and P. Yew. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Inlining substitutes the body of a called procedure for the procedure call and optimizes it as a part of the calling procedure [AC72]. Even though regular section analysis and inlining are frequently successful at enabling optimization, each of these methods has its limitations <ref> [HK90, LY88a, Hus82] </ref>. Compilation time and space considerations require that regular section analysis summarize array side effects. In general, summary analysis for loop parallelization is less precise than the analysis of inlined code. <p> Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution [AJ90, CHT91, Hus82] and interprocedural analysis of array side effects <ref> [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency. <p> Comparing these numbers against published results of more precise techniques, there was no benefit to be gained by the increased precision of the other techniques <ref> [LY88a, LY88b, TIF86] </ref>. 5.8 Discussion The usefulness of this approach has been illustrated on the Perfect Benchmark programs spec77 and ocean. Taken as a whole, the results indicate that providing freedom to the code generator becomes more important as the number of processors increase.
Reference: [LY88b] <author> Z. Li and P. Yew. </author> <title> Interprocedural analysis and program restructuring for parallel programs. </title> <type> Technical Report 720, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution [AJ90, CHT91, Hus82] and interprocedural analysis of array side effects <ref> [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency. <p> Comparing these numbers against published results of more precise techniques, there was no benefit to be gained by the increased precision of the other techniques <ref> [LY88a, LY88b, TIF86] </ref>. 5.8 Discussion The usefulness of this approach has been illustrated on the Perfect Benchmark programs spec77 and ocean. Taken as a whole, the results indicate that providing freedom to the code generator becomes more important as the number of processors increase.
Reference: [McK90] <author> K. S. McKinley. </author> <title> Dependence analysis of arrays subscripted by index arrays. </title> <type> Technical Report TR91-162, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: However, a better solution to this problem, is to reward nicely structured multidimensional array references with excellent performance. Programmers will then have an incentive to program multidimensionally. Assertions Five of the programs in this test suite used index arrays that were permutations of the index set <ref> [McK90] </ref>. Several were monotonic non-decreasing with a well defined with a pattern. In three programs, automatic parallelization would not have been possible with out using an assertion and the testing techniques developed in our earlier research [McK90]. <p> in this test suite used index arrays that were permutations of the index set <ref> [McK90] </ref>. Several were monotonic non-decreasing with a well defined with a pattern. In three programs, automatic parallelization would not have been possible with out using an assertion and the testing techniques developed in our earlier research [McK90]. The other two used them in a way that did not interfere with parallelization. 8.5 Discussion Our results are very promising.
Reference: [McM86] <author> F. McMahon. </author> <title> The Livermore Fortran Kernels: A computer test of the numerical performance range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1986. </year>
Reference-contexts: These loops may be an artifact of a vectorizable programming style. They appear frequently in the Perfect benchmarks [CKPK90], the Level 2 BLAS [DCHH88], and the Livermore loops <ref> [McM86] </ref>. Table 6.6 illustrates the performance benefits with the organization of dmxpy generated by our algorithm on matrices of size 200 fi 200 on 19 processors.
Reference: [Mur71] <author> Y. Muraoka. </author> <title> Parallelism Exposure and Exploitation in Programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> February </month> <year> 1971. </year> <note> Report No. 71-424. </note>
Reference-contexts: Finally, the interchange flags are recalculated for dependences in the loop nest. 3.4.2 Loop skewing Loop skewing is a transformation that changes the shape of the iteration space to expose parallelism across a wavefront <ref> [IT88, Lam74, Mur71, Wol86] </ref>. It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain 22 dependences and iteration space after skewbefore skew i i effective loop-level parallelism in a loop nest [Ban90b, WL90, Wol89a]. All of these transformations are supported in Ped. <p> Therefore, the extensions and limitations necessary for both internal and exit branching are include in the discussion below. 4.2 Loop distribution Loop distribution is an integral part of transforming a sequential program into a parallel one. It was introduced by Muraoka <ref> [Mur71] </ref> and is used extensively in par-allelization, vectorization, and memory management. For loops with control flow, previous methods for loop distribution have significant drawbacks. We present a new algorithm for loop distribution in the presence of control flow modeled by a control dependence graph.
Reference: [NS91] <author> S. G. Nash and A. Sofer. </author> <title> A general-purpose parallel algorithm for unconstrained optimization. </title> <journal> SIAM Journal of Optimization, </journal> <volume> 1(4) </volume> <pages> 530-547, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: This practice is not legal Fortran and will thwart even advanced dependence analysis. It is most likely be the bug responsible for the failure of hand-coded and automatic versions. A.2 BTN Unconstrained Optimization This program solves unconstrained nonlinear optimization problems based on a block truncated-Newton method <ref> [NS91, NS92] </ref>. It was written by Stephen Nash and Ariela Sofer at George Mason University. Truncated-Newton methods obtain the search direction by approximately solving the Newton equations via some iterative method. The method used here is a block version of the Lanczos method, which is numerically stable for non-convex problems.
Reference: [NS92] <author> S. G. Nash and A. Sofer. </author> <title> BTN: software for parallel unconstrained optimization. </title> <journal> ACM TOMS, </journal> <note> 1992. to appear. </note>
Reference-contexts: This practice is not legal Fortran and will thwart even advanced dependence analysis. It is most likely be the bug responsible for the failure of hand-coded and automatic versions. A.2 BTN Unconstrained Optimization This program solves unconstrained nonlinear optimization problems based on a block truncated-Newton method <ref> [NS91, NS92] </ref>. It was written by Stephen Nash and Ariela Sofer at George Mason University. Truncated-Newton methods obtain the search direction by approximately solving the Newton equations via some iterative method. The method used here is a block version of the Lanczos method, which is numerically stable for non-convex problems.
Reference: [Ost89] <author> A. Osterhaug, </author> <title> editor. Guide to Parallel Programming on Sequent Computer Systems. </title> <publisher> Sequent Technical Publications, </publisher> <address> San Diego, CA, </address> <year> 1989. </year>
Reference-contexts: Each processor has its own 64Kbyte two-way set-associative cache and the cache line size is 4 words. In addition the Sequent has a very flexible compiler that allows the program to completely specify parallelism and does not introduce all available parallelism <ref> [Ost89] </ref>. These features gave our algorithms complete control over the parallelization process. To introduce parallelism into the programs, we used the parallel loop compiler directives suggested by the Sequent's user manual [Ost89]. <p> a very flexible compiler that allows the program to completely specify parallelism and does not introduce all available parallelism <ref> [Ost89] </ref>. These features gave our algorithms complete control over the parallelization process. To introduce parallelism into the programs, we used the parallel loop compiler directives suggested by the Sequent's user manual [Ost89]. To compile and run all the program versions, we used the version 2.1 of Sequent's Fortran ATS compiler for multiprocessing with optimization at its highest level (O3). An additional option instructed the compiler to use the Weitek 1167 floating-point accelerator.
Reference: [PGH + 90] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C. Lee, B. Leung, and D. Schouten. </author> <title> The structure of Parafrase-2: An advanced parallelizing compiler for C and Fortran. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In Parafrase, program transformations are structured in phases and are always applied where applicable. Batch analysis is performed after each transformation phase to update the dependence information for the entire program. Parafrase-2 adds scheduling and improved program analysis and transformations <ref> [PGH + 90] </ref>. More advanced interprocedural and symbolic analysis is planned [HP90]. Parafrase-2 uses Faust as a front end to provide interactive parallelization and graphical displays [GGGJ88]. Ptran is also an automatic parallelizer with extensive program analysis.
Reference: [Por89] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Porterfield presents a formula that approximates the number of cache lines accessed for a loop nest, but it is restricted to a cache line size of one and loops with uniform dependences <ref> [Por89] </ref>. Ferrante et al. present a more general formula that also approximates the number of cache lines accessed and is applicable across a wider range of loops [FST91]. However, they first compute an estimate for every array reference and then combine them, trying not to do dependence testing.
Reference: [RC86] <author> B. Ryder and M. Carroll. </author> <title> An incremental algorithm for software analysis. </title> <booktitle> In Proceedings of the Second ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <address> Palo Alto, CA, </address> <month> December </month> <year> 1986. </year> <month> 154 </month>
Reference-contexts: Several algorithms for performing incremental analysis are found in the literature; for example, data-flow analysis [RP88, Zad84], interprocedural analysis <ref> [Bur90, RC86] </ref>, interprocedural recompilation analysis [BCKT90], as well as dependence analysis [Ros90]. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped.
Reference: [RG89] <author> S. Richardson and M. Ganapathi. </author> <title> Interprocedural optimization: Experimental results. </title> <journal> Software|Practice and Experience, </journal> <volume> 19(2), </volume> <month> February </month> <year> 1989. </year>
Reference-contexts: In general, summary analysis for loop parallelization is less precise than the analysis of inlined code. On the other hand, inlining can yield an increase in code size which may disastrously increase compile time and seriously inhibit separate compilation <ref> [CHT91, RG89] </ref>. Furthermore, inlining may cause a loss of precision in dependence analysis due to the complexity of subscripts that result from array parameter reshapes.
Reference: [Ros90] <author> C. Rosene. </author> <title> Incremental Dependence Analysis. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Several algorithms for performing incremental analysis are found in the literature; for example, data-flow analysis [RP88, Zad84], interprocedural analysis [Bur90, RC86], interprocedural recompilation analysis [BCKT90], as well as dependence analysis <ref> [Ros90] </ref>. However, few of these algorithms have been implemented and evaluated in an interactive environment. Rather than tackle all these problems at once, we chose a simple yet practical strategy for the current implementation of Ped. First, the scope of each program change is evaluated.
Reference: [RP88] <author> B. Ryder and M. Paull. </author> <title> Incremental data flow analysis algorithms. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 1-50, </pages> <month> Jan-uary </month> <year> 1988. </year>
Reference-contexts: Several algorithms for performing incremental analysis are found in the literature; for example, data-flow analysis <ref> [RP88, Zad84] </ref>, interprocedural analysis [Bur90, RC86], interprocedural recompilation analysis [BCKT90], as well as dependence analysis [Ros90]. However, few of these algorithms have been implemented and evaluated in an interactive environment.
Reference: [SA88] <author> K. Smith and W. Appelbe. </author> <title> PAT an interactive Fortran parallelizing assistant tool. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: The call and process graphs may be animated dynamically at run time. Sigmacs also performs several interactive program transformations, and is planning on incorporating automatic updates of dependence information. Pat is also an interactive parallelization tool <ref> [SA88, SA89] </ref>. Its dependence analysis is restricted to Fortran programs where only one write occurs to each variable in a loop. In addition, Pat uses simple dependence tests that do not calculate general distance or direction vectors.
Reference: [SA89] <author> K. S. Smith and W. Appelbe. </author> <title> An interactive conversion of sequential to multitasking Fortran. </title> <booktitle> In Proceedings of the 1989 ACM International Conference on Supercomputing, </booktitle> <address> Crete, Greece, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The call and process graphs may be animated dynamically at run time. Sigmacs also performs several interactive program transformations, and is planning on incorporating automatic updates of dependence information. Pat is also an interactive parallelization tool <ref> [SA88, SA89] </ref>. Its dependence analysis is restricted to Fortran programs where only one write occurs to each variable in a loop. In addition, Pat uses simple dependence tests that do not calculate general distance or direction vectors.
Reference: [Sar90] <author> V. Sarkar. </author> <title> PTRAN | the IBM parallel translation system. </title> <booktitle> In Proceedings of the International Workshop on Compilers for Parallel Computers, </booktitle> <address> Paris, France, </address> <month> December </month> <year> 1990. </year> <note> To appear as a chapter in Parallel Functional Programming Languages and Compilers, </note> <editor> editor B. Szyman-ski, </editor> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: Ptran is also an automatic parallelizer with extensive program analysis. It computes the ssa and program dependence graphs, and performs constant propagation and interprocedural analysis [CFR + 89, FOW87]. Ptran introduces both task and loop parallelism, but the only other program transformations are variable privatiza-tion and loop distribution <ref> [ABC + 87, Sar90] </ref>. 32 Sigmacs, a programmable interactive parallelizer in the Faust programming environment, computes and displays call graphs, process graphs, and a statement dependence graph [GGGJ88, SG90]. In a process graph each node represents a task or a process, which is a separate entity running in parallel. <p> In this chapter, we summarize what was achieved in pursuit of supporting the thesis. Both the successes and limitations are presented. In addition, the implications of this work for programmers, other architectures, and future work are described. Due to the limited success of other researchers attacking this problem <ref> [EB91, SH91, Sar90] </ref>, we were not confident when we began that acceptable performance would result from automatic techniques. Therefore, we concentrated much effort on designing and implementing Ped. During this process, we developed fast incremental update algorithms for many transformations.
Reference: [SAS90] <author> K. Smith, W. Appelbe, and K. Stirewalt. </author> <title> Incremental dependence analysis for interactive parallelization. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: It can also insert synchronization to protect specific dependences. Pat divides analysis into scalar and dependence phases, but does not perform symbolic or interprocedural analysis. The incremental dependence update that follows transformations is simplified due to its austere analysis <ref> [SAS90] </ref>. Superb interactively converts sequential programs into data parallel SPMD programs that can be executed on the Suprenum distributed memory multiprocessor [ZBG88]. Superb provides a set of interactive program transformations, including transformations that exploit data parallelism.
Reference: [SG90] <author> B. Shei and D. Gannon. SIGMACS: </author> <title> A programmable programming environment. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Ptran introduces both task and loop parallelism, but the only other program transformations are variable privatiza-tion and loop distribution [ABC + 87, Sar90]. 32 Sigmacs, a programmable interactive parallelizer in the Faust programming environment, computes and displays call graphs, process graphs, and a statement dependence graph <ref> [GGGJ88, SG90] </ref>. In a process graph each node represents a task or a process, which is a separate entity running in parallel. The call and process graphs may be animated dynamically at run time.
Reference: [SH91] <author> J. Singh and J. Hennessy. </author> <title> An empirical investigation of the effectiveness of and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: In this chapter, we summarize what was achieved in pursuit of supporting the thesis. Both the successes and limitations are presented. In addition, the implications of this work for programmers, other architectures, and future work are described. Due to the limited success of other researchers attacking this problem <ref> [EB91, SH91, Sar90] </ref>, we were not confident when we began that acceptable performance would result from automatic techniques. Therefore, we concentrated much effort on designing and implementing Ped. During this process, we developed fast incremental update algorithms for many transformations.
Reference: [SK86] <author> R. G. Scarborough and H. G. Kolsky. </author> <title> A vectorizing Fortran compiler. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 30(2) </volume> <pages> 163-171, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: This approach, called if-conversion [AKPW83, All83], has been used success 36 fully in a variety of vectorization systems which incorporate several other transformations as well <ref> [AK87, SK86, KKLW84] </ref>. However, if-conversion has several drawbacks. If vectorization or parallelization fails, it is not easy to reconstruct efficient branching code. In addition, if-conversion may cause significant increases in the code space to hold conditionals. <p> If-conversion is theoretically appealing because it allows for a unified treatment of data dependence without control dependences, and has been used successfully in a number of vectorization systems <ref> [KKLW84, SK86] </ref>. However, it has several drawbacks. In its original form, if-conversion is intractable and may introduce an exponential number of new logical arrays to record control flow decisions.
Reference: [Sub90] <author> J. Subhlok. </author> <title> Analysis of Synchronization in a Parallel Programming Environment. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> August </month> <year> 1990. </year> <month> 155 </month>
Reference-contexts: Most that were not contained unordered critical sections. Programs that use synchronization in order to perform loops in parallel, such as "doacross" style parallelism and critical regions are not handled by our approach <ref> [Cyt86, Sub90, HKT92] </ref>. However, these loops are an important source of parallelism that should be addressed. We have not considered the more challenging issues which arise on shared-memory machines with non-uniform access time such as the TC2000 Butterfly. Nor have we considered distributed memory architectures such as the Intel Hypercube.
Reference: [TIF86] <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of CALL statements. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Using a more sophisticated fusion algorithm might result in even better execution time improvements. 5.7 Related work While the idea of interprocedural optimization is not new, previous work on inter-procedural optimization for parallelization has limited its consideration to inline substitution [AJ90, CHT91, Hus82] and interprocedural analysis of array side effects <ref> [BK89, BC86, CK87b, HK90, HHL90a, HHL90b, LY88a, LY88b, TIF86] </ref>. The various approaches to array side-effect analysis must make a tradeoff between precision and efficiency. <p> Comparing these numbers against published results of more precise techniques, there was no benefit to be gained by the increased precision of the other techniques <ref> [LY88a, LY88b, TIF86] </ref>. 5.8 Discussion The usefulness of this approach has been illustrated on the Perfect Benchmark programs spec77 and ocean. Taken as a whole, the results indicate that providing freedom to the code generator becomes more important as the number of processors increase.
Reference: [Tow76] <author> R. A. Towle. </author> <title> Control and Data Dependence for Program Transformation. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> March </month> <year> 1976. </year>
Reference-contexts: An alternative approach when control flow is present uses explicit control and data dependences. In his dissertation, Towle develops techniques for vectorizing programs with control flow using loop distribution, scalar expansion, and replication using data and control dependence information <ref> [Tow76] </ref>. However, his definition of control dependence embeds, but does not extract the essential control relationship between two statements, that is if the execution of one statement directly determines the execution of another. <p> Also, one execution variable may be needed for every successor in the descendant partition. Because their code generation algorithm is based on G f , rather than G cd , the proof of how an execution variable is used is much more difficult and is not given. Towle <ref> [Tow76] </ref> and Baxter and Bauer [BB89] use similar approaches for inserting conditional arrays. The Stardent compiler distributes loops with structured control flow by keeping groups of statements with the same control flow constraints together [All90].
Reference: [WL90] <author> M. E. Wolf and M. Lam. </author> <title> Maximizing parallelism via loop transformations. </title> <booktitle> In Proceedings of the Third Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain 22 dependences and iteration space after skewbefore skew i i effective loop-level parallelism in a loop nest <ref> [Ban90b, WL90, Wol89a] </ref>. All of these transformations are supported in Ped. Loop skewing is applied to a pair of perfectly nested loops that both carry dependences, even after loop interchange. <p> The loop permutation is illegal if and only if the first nonzero entry of some vector is negative, indicating that the execution order of a data dependence has been reversed <ref> [AK84, Ban90a, Ban90b, WL90] </ref>. In many cases, the loop permutation calculated by MemoryOrder is legal and we are finished. However, if the desired memory order is prevented by data dependences, we use a simple heuristic for calculating a legal loop permutation near memory order. <p> : : ; l n g Output: T an optimization of L Algorithm: O = MemoryOrder (L) np = Estimate (O, parallel) if np &gt; 1 (parallelism is profitable) T = Parallelize (O) endif perform f O; T g profitable, other optimization strategies that consider all loop permutations, loop skewing <ref> [WL90] </ref>, or loop distribution (see Chapter 7) should be explored. 6.9 Experimental results The overall parallelization strategy was also tested by applying it by hand to two kernels. <p> Many algorithms have been proposed in the literature for introducing parallelism into programs. Callahan et al. use the metric of minimizing barrier synchronization points via loop distribution, fusion and interchange for introducing parallelism [ACK87, Cal87]. Wolf and Lam <ref> [WL90] </ref> introduce all possible parallelism via the uni-modular transformations: loop interchange, skewing, and reversal. Neither of these techniques try to map the parallelism to a machine, or try take into account data locality, nor is any loop bound information considered. <p> Assuring sufficient granularity and matching it to the machine is dependent on the architecture. Most previous research focuses on discovering parallelism and/or maximizing its granularity without regard to data locality <ref> [Cal87, WL90, ABC + 87] </ref>. Our approach addresses all of these concerns. The key component is the combination of loop interchange and strip mining developed in Chapter 6 which considers all these factors. Sufficient granularity is assured using performance estimation (see Section 6.7.1).
Reference: [WL91] <author> M. E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: These accesses are loop-independent if they occur in the same loop iteration, and are loop-carried if they occur on different loop iterations. Wolf and Lam call this temporal reuse <ref> [WL91] </ref>. The most obvious source of temporal reuse is from loop-invariant references. For instance, consider the reference to A (J) in the following loop nest. It is invariant with respect to the I loop, and is reused by each iteration. <p> For instance, each cache line is reused multiple times on the inner I loop for B (I) in the above example. Wolf and Lam call this spatial reuse <ref> [WL91] </ref>. The actual amount of reuse is dependent on the size of B (I) relative to the cache line size and the pattern of intervening references. <p> For reasonably large computations, references such as C (J,I) do not 84 provide any reuse on the I loop, because the desired cache lines have been flushed by intervening memory accesses. Previous researchers have studied techniques for improving locality of accesses for registers, cache, and pages <ref> [AS79, CCK90, WL91, GJG88] </ref>. We concentrate on improving the locality of accesses for cache; i.e. we attempt to increases the locality of access to the same cache line. Empirical results show that improving spatial reuse can be significantly more effective than techniques that consider temporal reuse alone [KMT92]. <p> The same result is obtained by previous researchers <ref> [AK84, WL91] </ref>. 6.5.2 Permuting to achieve memory order We must now decide whether the desired memory order is legal. If it is not, we must select some legal loop permutation close to memory order. To determine whether a loop permutation is legal is straightforward. <p> Table 6.6: Dmxpy on 19 processors loop organization I loop parallel I J II I II J J I speed-up over sequential JI 16.4 13.8 2.9 6.10 Related work Our work bears the most similarity to research by Wolf and Lam <ref> [WL91] </ref>. They develop an algorithm that estimates all temporal and spatial reuse for a given loop permutation, including reuse on outer loops. This reuse is represented as a localized vector space. Vector spaces representing reuse for individual and multiple references are combined to discover all loops L carrying some reuse.
Reference: [Wol82] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1982. </year>
Reference: [Wol86] <author> M. J. Wolfe. </author> <title> Loop skewing: The wavefront method revisited. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 279-293, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Ped supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. Ped also supports transformations for improving data locality. Each transformation is briefly introduced below. Many are found in the literature <ref> [AC72, AK87, CCK90, KM90, KMT91b, KKLW84, Lov77, Wol86] </ref>. In Ped, their novel aspect is the analysis of their applicability, safety, prof 17 itability and the incremental updates of source and dependence information. We classify the transformations implemented in Ped as follows. <p> When possible, Ped computes and suggests the optimal skew degree. Loop skewing may be used with loop interchange in Ped to expose wavefront parallelism <ref> [KMT91a, Wol86] </ref>. * Loop reversal reverses the order of execution of loop iterations. * Loop fusion can increase the granularity of parallel regions and promote reuse by fusing two contiguous loops when dependences are not violated [AC72, KKP + 81]. * Statement interchange interchanges two adjacent independent statements. * Unroll and <p> The purpose, mechanics, and safety of these transformations are presented, followed by their profitability estimates, user advice, and incremental dependence update algorithms. 3.4.1 Loop interchange Loop interchange has been used extensively in vectorizing and parallelizing compilers to adjust the granularity of parallel loops and to expose parallelism <ref> [AK87, KKLW84, Wol86] </ref>. Ped interchanges pairs of adjacent loops. Loop permutations may be performed as a series of pairwise interchanges. Ped supports interchange of triangular or skewed loops. <p> Finally, the interchange flags are recalculated for dependences in the loop nest. 3.4.2 Loop skewing Loop skewing is a transformation that changes the shape of the iteration space to expose parallelism across a wavefront <ref> [IT88, Lam74, Mur71, Wol86] </ref>. It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain 22 dependences and iteration space after skewbefore skew i i effective loop-level parallelism in a loop nest [Ban90b, WL90, Wol89a]. All of these transformations are supported in Ped. <p> Extensions are needed for transformations 63 that require dependence distance information such as loop permutation. The intra-procedural optimizations which are extended in this chapter are loop fusion and loop permutation. These results easily generalize for other transformations such as loop skewing <ref> [Wol86] </ref> and unroll and jam [CCK88]. As a motivating example, consider the Fortran code in Example 5.1 (a). The J loop in subroutine S may safely be made parallel, but the outer I loop in subroutine P may not be.
Reference: [Wol89a] <author> M. J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: If the minimum distance of the dependences in the loop is less than the step size, the resultant inner loop may be parallelized. Used alone the order of the iterations is unchanged, but used in concert with loop interchange the iteration space may be tiled <ref> [Wol89a] </ref> to utilize memory bandwidth and cache more effectively [CK89]. * Scalar replacement takes array references with consistent dependences and replaces them with scalar temporaries that may be allocated into registers [CCK90]. <p> It can be applied in conjunction with loop interchange, strip mining, and loop reversal to obtain 22 dependences and iteration space after skewbefore skew i i effective loop-level parallelism in a loop nest <ref> [Ban90b, WL90, Wol89a] </ref>. All of these transformations are supported in Ped. Loop skewing is applied to a pair of perfectly nested loops that both carry dependences, even after loop interchange. <p> Tiling combines strip mining and loop interchange to promote reuse across a loop nest <ref> [IT88, Wol89a] </ref>. For matrix multiply, the loop nest may be tiled by strip mining the K loop by TS and then interchanging it with J.
Reference: [Wol89b] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: When loop in 18 terchange is safe, it can be used to adjust the granularity of parallel loops <ref> [AK87, KMT91a, Wol89b] </ref>. * Loop skewing adjusts the iteration space of two perfectly nested loops by shifting the work per iteration in order to expose parallelism. When possible, Ped computes and suggests the optimal skew degree.
Reference: [Wol89c] <author> M. J. Wolfe. </author> <title> Semi-automatic domain decomposition. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: analysis and algorithms presented in this thesis relieve the programmer of the burden of explicit parallel programming for a variety of shared-memory parallel machines. 8.1 Introduction A lesson to be learned from vectorization is that programmers rewrote their programs in a vectorizable style based on feedback from their vectorizing compilers <ref> [CKK89, Wol89c] </ref>. Compilers were then able to take these programs and generate machine-dependent vector code with excellent results. We are testing this same thesis for shared-memory parallel machines. The experiment described below considers the automatic parallelization of sequential program versions where parallelism is known to exist.
Reference: [Wri91a] <author> S. J. Wright. </author> <title> Parallel algorithms for banded linear systems. </title> <journal> SIAM Journal of Scientific and Statistical Computation, </journal> <volume> 12(4) </volume> <pages> 824-842, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Whenever analysis was required beyond the current implementation, it is noted. Otherwise, all the parallelism detection and transformations were based on the current analysis. The programs are ordered alphabetically. A.1 Banded Linear Systems This program is a partitioned Gaussian elimination algorithm with partial pivoting <ref> [Wri91a] </ref>. The system is assumed to be nonsingular. Hence, the submatrices in the chosen partitioning may be rank-deficient and this makes the algorithm more complex than those which have been proposed for diagonally dominant and symmetric positive-definite systems. It is suitable for multiprocessors with small to moderate numbers of processors.
Reference: [Wri91b] <author> S. J. Wright. </author> <title> Partitioned dynamic programming for optimal control. </title> <journal> SIAM Journal of Optimization, </journal> <volume> 1(4) </volume> <pages> 620-642, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: The sequential version was easily created by eliminating directives. The automatically parallelized version of this program employed interprocedural loop fusion to improve performance. A.9 Optimal Control This program computed solutions for linear-quadratic optimal control problems that arise from Newton's method or two-metric gradient projection methods to nonlinear problems <ref> [Wri91b] </ref>. It is a decomposition of the domain of the problem and is related to multiple shooting methods for two-point boundary value problems. Implementation details This code was written for an Alliant FX/8 using Alliant compiler directives. This version worked as the sequential version.
Reference: [Wri92] <author> S. J. Wright. </author> <title> Stable parallel algorithms for two-point boundary value problems. </title> <journal> SIAM Journal of Scientific and Statistical Computation, </journal> <note> 1992. to appear. </note>
Reference-contexts: In order to automatically par-allelize these loops, array kill analysis and one user assertion about the value of a symbolic were required. A.10 Two-Point Boundary Problems This program uses finite differences to solve two-point boundary value Bodes <ref> [Wri92] </ref>. It was written by Stephen Wright at Argonne National Laboratory. It uses a struc 143 tured orthogonal factorization technique to solve the system in an effective, stable and parallel manner. Implementation details This code was written for an Alliant FX/8 using Alliant compiler directives.
Reference: [Zad84] <author> F. Zadeck. </author> <title> Incremental data flow analysis in a structured program editor. </title> <booktitle> In Proceedings of the SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year> <month> 156 </month>
Reference-contexts: Several algorithms for performing incremental analysis are found in the literature; for example, data-flow analysis <ref> [RP88, Zad84] </ref>, interprocedural analysis [Bur90, RC86], interprocedural recompilation analysis [BCKT90], as well as dependence analysis [Ros90]. However, few of these algorithms have been implemented and evaluated in an interactive environment.
Reference: [ZBG88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semiautomatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: The incremental dependence update that follows transformations is simplified due to its austere analysis [SAS90]. Superb interactively converts sequential programs into data parallel SPMD programs that can be executed on the Suprenum distributed memory multiprocessor <ref> [ZBG88] </ref>. Superb provides a set of interactive program transformations, including transformations that exploit data parallelism. The user specifies a data partitioning, then node programs with the necessary send and receive operations are automatically generated.
References-found: 131

