URL: http://www.cs.berkeley.edu/~inderjit/public_papers/pdonfer.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~inderjit/
Root-URL: 
Title: Fernando's Solution to Wilkinson's Problem: an Application of Double Factorization  
Author: Beresford N. Parlett Inderjit S. Dhillon 
Address: Berkeley, CA 94720, USA  Berkeley, CA 94720, USA  
Affiliation: Mathematics Department, University of California,  Computer Science Division, EECS Department, University of California,  
Abstract: Suppose that one knows a very accurate approximation to an eigenvalue of a symmetric tridiagonal matrix T . A good way to approximate the eigenvector x is to discard an appropriate equation, say the rth, from the system (T I)x = 0 and then to solve the resulting underdetermined system in any of several stable ways. However the output x can be completely inaccurate if r is chosen poorly and in the absence of a quick and reliable way to choose r this method has lain neglected for over 35 years. Experts in boundary value problems have known about the special structure of the inverse of a tridiagonal matrix since the 1960s and their double triangular factorization technique (down and up) gives directly the redundancy of each equation and so reveals the set of good choices for r. The relation of double factorization to the eigenvector algorithm of Godunov and his collaborates is described in Section 4. The results extend to band matrices (Section 7) and to zero entries in eigenvectors, and have uses beyond eigenvector computation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <title> LAPACK Users' Guide, Release 2.0, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1995. </year>
Reference-contexts: One goal of this paper is to show that a large entry may be located with little extra expense. 2 The current LAPACK codes, see <ref> [1] </ref>, do not use Wilkinson's choice; instead b is chosen `at random' from an appropriate distribution but this still makes it difficult to obtain orthogonal eigenvectors for close eigenvalues and difficult to prove `correctness' of the code. The case for this approach is made in [17].
Reference: [2] <author> I. Babuska, </author> <title> Numerical stability in problems of linear algebra. </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 9 </volume> <month> 53-77 </month> <year> (1972). </year>
Reference-contexts: D. Kershaw (1970), in [18], obtained nice bounds on (J 1 ) rr =J rr using twisted LDU . Babuska (1972) wanted a specific entry in J 1 b for any b and his formula (5.29) on page 62 of <ref> [2] </ref> is one instance of our Corollary 4. Fischer et. al. (1974), in [7], discuss a twisted Toeplitz factorization of Buneman and attribute the adjective `twisted' to Strang [23].
Reference: [3] <author> J. Barlow, </author> <title> private communication, </title> <year> 1996. </year>
Reference-contexts: When (33) holds then (29) yields j sin 6 v; z (j) j gap () p kvk 1 p j j and we are going to use z (j) only when j j=gap () is O (*). We have noted, and so has Jessie Barlow <ref> [3] </ref>, that a simple recurrence will yield all values of kz (k) k for O (n) operations. Consequently it would be feasible to minimize jfl k j=kz (k) k to obtain better bounds on sin 6 (v; z (k) ); (28) (29).
Reference: [4] <author> S. H. Crandall and W. G. Strang, </author> <title> An improvement of the Holzer table based on a suggestion of Rayleigh's. </title> <journal> J. Appl. Mech. </journal> <volume> 24 </volume> <month> 228-230 </month> <year> (1957). </year> <note> Also Discussion in J. Appl. Mech. </note> 25:160-161 (1958). 
Reference-contexts: position of the largest component of u k .' Ipsen, in a very readable survey attributes the idea of omitting one equation of the system to Wilkinson, see Section 7 of [16], but we suspect that this method was routinely taught in mathematics classes before Wilkinson was born, see [15], <ref> [4] </ref>, and [22]. He was born in 1919 and [15] was published in 1921.
Reference: [5] <author> I. S. Dhillon, </author> <title> Stable computation of the condition number of a tridiagonal matrix in O(n) time. </title> <note> Submitted for publication, </note> <year> 1996. </year>
Reference-contexts: Section 7 extends the results of Section 3 to block tridiagonal matrices. Theorem 2 has applications to finding the envelope of an invariant subspace and in computing condition numbers, see [21] and <ref> [5] </ref>. Even in the eigenvector application Theorem 2 has uses beyond finding the best value of r. It allows us to omit the calculation of completely negligible entries and that enhances efficiency when n becomes large (say n 100). An example of this is given in Figure 4.
Reference: [6] <author> J. J. Dongarra et al., </author> <title> LINPACK, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: Fischer et. al. (1974), in [7], discuss a twisted Toeplitz factorization of Buneman and attribute the adjective `twisted' to Strang [23]. Dongarra et. al. (1979), in the LINPACK codes use twisted factorization meeting in the middle for improved efficiency, see <ref> [6] </ref>, and the practice has been taken up in parallel computation, see [24].
Reference: [7] <author> D. Fischer, G. H. Golub, O. Hald, C. Leiva and O. Widlund, </author> <title> On Fourier Toeplitz methods for separable elliptic problems. </title> <journal> Math. Comp. </journal> <volume> 28 </volume> <month> 349-368 </month> <year> (1974). </year>
Reference-contexts: Babuska (1972) wanted a specific entry in J 1 b for any b and his formula (5.29) on page 62 of [2] is one instance of our Corollary 4. Fischer et. al. (1974), in <ref> [7] </ref>, discuss a twisted Toeplitz factorization of Buneman and attribute the adjective `twisted' to Strang [23]. Dongarra et. al. (1979), in the LINPACK codes use twisted factorization meeting in the middle for improved efficiency, see [6], and the practice has been taken up in parallel computation, see [24].
Reference: [8] <author> K. V. Fernando, </author> <title> On computing an eigenvector of a tridiagonal matrix, </title> <publisher> TR4/95, Nag Ltd., Oxford, </publisher> <address> UK, </address> <year> 1995. </year> <note> Submitted for publication. </note>
Reference-contexts: V. Fer-nando's solution to Wilkinson's problem of choosing the index r mentioned above. This is an important contribution to the computation of satisfactory eigenvectors but it is not enough, in our opinion, to ensure mutual orthogonality. See <ref> [8] </ref> for a different viewpoint. Section 2 discusses the `obvious' solution to the problem and shows its shortcomings. The new idea, in its simplest form, comes from Theorem 2 which is established in Section 3 along with Theorem 6 which presents accurate ways to compute the determinant.
Reference: [9] <author> W. </author> <title> Givens, Numerical computation of the characteristic values of a real symmetric matrix, </title> <institution> Oak Ridge Nat. Lab. </institution> <type> Report, </type> <institution> ORNL-1574, </institution> <year> 1954. </year>
Reference: [10] <author> W. </author> <title> Givens, The characteristic value-vector problem, </title> <journal> J. ACM, </journal> <volume> 4 </volume> <month> 298-307 </month> <year> (1957). </year> <note> (Also unpublished report.) </note>
Reference-contexts: As will become clear in Section 3 choosing b = e r is equivalent to omitting Equation r from the homogeneous version of (1). The value r = n was proposed by Wallace Givens in 1957, see <ref> [10] </ref>, but no fixed value of r, independent of and T , will do. <p> When normalized in the same way x and ~ x will yield the same eigenvector. Note that the problem has been solved without the bother of computing a triangular factorization. The method described above is `obvious' and was mentioned by W. Givens in 1957, see <ref> [10] </ref>. It often gives good results when realized on a computer but, at other times, delivers vectors pointing in completely wrong directions. The preceding analysis is valid in exact arithmetic but is inapplicable to computer work for the following reasons.
Reference: [11] <author> S.K. Godunov, V. I. Kostin, and A. D. Mitchenko, </author> <title> Computation of an eigenvector of symmetric tridiagonal matrices. </title> <journal> Siberian Math. J. </journal> <volume> 26 </volume> <month> 71-85 </month> <year> (1985). </year>
Reference-contexts: It was referred to as the BABE algorithm (Begin, or Burn, At Both Ends) in [14]. 4.2 The Russian Approach This section tries to present the essence of Chapters 4 and 5 of the somewhat inscrutable book [12], but in our notation. The part described here appeared in <ref> [11] </ref> in 1985. In order to convey the main ideas we assume that the tridiag-onal matrix ~ J is unreduced, ~ J k;k1 ~ J k1;k 6= 0, k = 2; : : : ; n, and that various factorizations exist. <p> It guarantees delivering an eigenvector of a close matrix. However this alone does not guarantee that computed eigenvectors for close eigenvalues are orthogonal to working accuracy. See Theorem 12 in the next section for the reason. 4.3 Our Involvement In 1993, unaware of <ref> [11] </ref>, [12] or [19], K. V. Fernando proposed computing pivots fD + (k)g using (18) in two ways: as usual going forward from initial value D + (1) = J 11 , and secondly going in reverse from final value D + (n) = 0. <p> A usable definition of `isolated eigenvalue' emerges and, for such eigenvalues the error bounds of Lemma 13 are close to equalities. It is current practice to compute very accurately approximate eigenvalues (of tridiagonals) by the best current techniques and then turn to inverse iteration for the eigenvectors; but see <ref> [11] </ref> and [12] for an exception. This approach reveals the not-too-clustered eigenvalues and guarantees for them that jfl k j=kz (k) k is tiny for at least one k when ~ J is normal.
Reference: [12] <author> S.K. Godunov, A.G. Antonov, O. P. Kiriljuk, and V. I. Kostin, </author> <title> Guaranteed Accuracy in Numerical Linear Algebra, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year> <note> (A revised translation of a Russian text first published in 1988 in Novosibirsk.) </note>
Reference-contexts: It was referred to as the BABE algorithm (Begin, or Burn, At Both Ends) in [14]. 4.2 The Russian Approach This section tries to present the essence of Chapters 4 and 5 of the somewhat inscrutable book <ref> [12] </ref>, but in our notation. The part described here appeared in [11] in 1985. <p> Moreover, D + (1) = ~ J 11 ; D + (k) = ~ J kk ~ J k;k1 ~ J k1;k =D + (k 1); k = 2; : : : ; n: The authors of <ref> [12] </ref>, whom we abbreviate collectively as GAKK, base their algorithms (for eigenvalues and eigenvectors when ~ J is symmetric) not on the `pivots' D + (i) but on their scaled reciprocals P k = P k () = ~ J k;k+1 =D + (k); k = 1; : : : ; <p> As indicated in (23) the sequence (26) may be used to approximate an eigenvector and there are several ways to do it. Unfortunately, despite the title of Section 5.8 of <ref> [12] </ref>, no specific algorithm is given or even referenced! Earlier chapters of [12] suggest that the authors preferred method for computing an eigenvector z from a two-sided Ss fP k g is by mapping the Ss into a chain of plane rotations fc k ; s k g n k=1 and <p> As indicated in (23) the sequence (26) may be used to approximate an eigenvector and there are several ways to do it. Unfortunately, despite the title of Section 5.8 of <ref> [12] </ref>, no specific algorithm is given or even referenced! Earlier chapters of [12] suggest that the authors preferred method for computing an eigenvector z from a two-sided Ss fP k g is by mapping the Ss into a chain of plane rotations fc k ; s k g n k=1 and then using the representation z 1 = s 2 s 3 : <p> We mention, in passing, that in <ref> [12] </ref> the singular value problem for bidiagonal is solved by `expanding' it to the associated symmetric tridiagonal with a zero diagonal. We do not think that this is the most efficient approach. <p> It guarantees delivering an eigenvector of a close matrix. However this alone does not guarantee that computed eigenvectors for close eigenvalues are orthogonal to working accuracy. See Theorem 12 in the next section for the reason. 4.3 Our Involvement In 1993, unaware of [11], <ref> [12] </ref> or [19], K. V. Fernando proposed computing pivots fD + (k)g using (18) in two ways: as usual going forward from initial value D + (1) = J 11 , and secondly going in reverse from final value D + (n) = 0. <p> It is current practice to compute very accurately approximate eigenvalues (of tridiagonals) by the best current techniques and then turn to inverse iteration for the eigenvectors; but see [11] and <ref> [12] </ref> for an exception. This approach reveals the not-too-clustered eigenvalues and guarantees for them that jfl k j=kz (k) k is tiny for at least one k when ~ J is normal.
Reference: [13] <author> P. Henrici, </author> <title> Bounds for eigenvalues of certain tridiagonal matrices. </title> <journal> SIAM J., </journal> <volume> 11 </volume> <month> 281-290 </month> <year> (1963). </year>
Reference-contexts: Here r is selected in advance and elimination starts from the top and the bottom and stops at r. Our Section 5 shows where to make the twist for the eigenvector problem. Henrici (1963) used twisted LDU implicitly in deriving optimal Gersgorin bounds, in <ref> [13] </ref>, for unreduced real tridiagonals with some complex eigenvalues. D. Kershaw (1970), in [18], obtained nice bounds on (J 1 ) rr =J rr using twisted LDU .
Reference: [14] <author> N. J. Higham, </author> <title> Efficient algorithms for computing the condition number of a tridiagonal matrix. </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 7 </volume> <month> 150-165 </month> <year> (1986). </year> <month> 28 </month>
Reference-contexts: Dongarra et. al. (1979), in the LINPACK codes use twisted factorization meeting in the middle for improved efficiency, see [6], and the practice has been taken up in parallel computation, see [24]. It was referred to as the BABE algorithm (Begin, or Burn, At Both Ends) in <ref> [14] </ref>. 4.2 The Russian Approach This section tries to present the essence of Chapters 4 and 5 of the somewhat inscrutable book [12], but in our notation. The part described here appeared in [11] in 1985.
Reference: [15] <author> H. Holzer, </author> <title> `Die Berechnung der Drehschwingungen', </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1921. </year>
Reference-contexts: the position of the largest component of u k .' Ipsen, in a very readable survey attributes the idea of omitting one equation of the system to Wilkinson, see Section 7 of [16], but we suspect that this method was routinely taught in mathematics classes before Wilkinson was born, see <ref> [15] </ref>, [4], and [22]. He was born in 1919 and [15] was published in 1921. <p> Ipsen, in a very readable survey attributes the idea of omitting one equation of the system to Wilkinson, see Section 7 of [16], but we suspect that this method was routinely taught in mathematics classes before Wilkinson was born, see <ref> [15] </ref>, [4], and [22]. He was born in 1919 and [15] was published in 1921. Wilkinson abandoned the hunt for a good value of r and used b = P Le where T I = P LU denotes triangular factorization with partial pivoting and e = i=1 e i , see [25].
Reference: [16] <author> I. Ipsen, </author> <title> A history of inverse iteration, </title> <editor> in Helmut Wielandt, Mathematische Werke, B. Huppert and H. Schneider, eds., vol.II: </editor> <title> Matrix Theory and Analysis, </title> <publisher> Walter de Guyter, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: This is not a new problem and there are good programs available in libraries such as LAPACK and NAG. Nevertheless the experts do not consider the situation satisfactory, see <ref> [16] </ref>; the complexity of the programs seems out of proportion to the difficulty of the task and the adaptation of the current versions of inverse iteration to parallel mode is frustrating. Let us briefly sketch the situation. <p> This result is instructive but not particularly useful, since we will not know a priori the position of the largest component of u k .' Ipsen, in a very readable survey attributes the idea of omitting one equation of the system to Wilkinson, see Section 7 of <ref> [16] </ref>, but we suspect that this method was routinely taught in mathematics classes before Wilkinson was born, see [15], [4], and [22]. He was born in 1919 and [15] was published in 1921.
Reference: [17] <author> E. Jessup and I. Ipsen, </author> <title> Improving the accuracy of inverse iteration. </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 13 </volume> <month> 570-572 </month> <year> (1992). </year>
Reference-contexts: The case for this approach is made in <ref> [17] </ref>. Here ends our review of the situation. This paper rederives results from the differential equations community concerning double factorization and proceeds to describe and justify K. V. Fer-nando's solution to Wilkinson's problem of choosing the index r mentioned above.
Reference: [18] <author> D. Kershaw, </author> <title> Inequalities on elements of the inverse of a certain tridiagonal matrix. </title> <journal> Math. Comp. </journal> <volume> 24 </volume> <month> 155-158 </month> <year> (1970). </year>
Reference-contexts: Our Section 5 shows where to make the twist for the eigenvector problem. Henrici (1963) used twisted LDU implicitly in deriving optimal Gersgorin bounds, in [13], for unreduced real tridiagonals with some complex eigenvalues. D. Kershaw (1970), in <ref> [18] </ref>, obtained nice bounds on (J 1 ) rr =J rr using twisted LDU . Babuska (1972) wanted a specific entry in J 1 b for any b and his formula (5.29) on page 62 of [2] is one instance of our Corollary 4.
Reference: [19] <author> G. Meurant, </author> <title> A review on the inverse of symmetric tridiagonal and block tridiagonal matrices, </title> <journal> SIAM J. Matrix Anal. and Appl. </journal> <volume> 13 </volume> <month> 707-728 </month> <year> (1992). </year>
Reference-contexts: These offer more accuracy than the usual formula Q n derived from J = L + D + U + when det J is small. These expressions are well known in the differential equations community, see <ref> [19, Theorem 2.3] </ref>, for ex ample. Theorem 6 Assume the hypothesis of Theorem 2. <p> Remark 10 From (4) we see that the vector z (r) is annihilated exactly by J e r fl r e fl r , a rank one perturbation. 4 Related Work 4.1 Boundary Value Problems for Second Order Differential Equations In 1992 in <ref> [19] </ref> Meurant reviewed a significant portion of the literature on the inverses of band matrices as well as presenting the main ideas in a nice unified framework. <p> The compact representation of the inverse of a tridiagonal was in the literature in the 1960s and began to be linked to double factorization in 11 the 1980s. Theorem 2.3 of <ref> [19] </ref> gives the quotient/product form of (J 1 ) kk as D (k + 1) D (n) : This is Corollary 7 of our Theorem 6. The inexpensive additive form (Theorem 2 and Corollary 4) is included in Theorem 3.1 of [19] and our Theorem 17 extends Theorem 3.1 a little. <p> Theorem 2.3 of <ref> [19] </ref> gives the quotient/product form of (J 1 ) kk as D (k + 1) D (n) : This is Corollary 7 of our Theorem 6. The inexpensive additive form (Theorem 2 and Corollary 4) is included in Theorem 3.1 of [19] and our Theorem 17 extends Theorem 3.1 a little. The researchers on Meurant's reference list were not interested in computing eigenvectors but in obtaining analytic expressions for elements of the inverse, when possible, and the decay rate in terms of distance from the main diagonal. <p> The researchers on Meurant's reference list were not interested in computing eigenvectors but in obtaining analytic expressions for elements of the inverse, when possible, and the decay rate in terms of distance from the main diagonal. Even the better procedures in <ref> [19] </ref> for computing the diagonal of the inverse require n extra products after the forward and backward pivots are in place. So investigators interested in eigenvectors may be forgiven for not seeing that double factorization could be useful to them. In addition to the papers reviewed in [19] are several on <p> better procedures in <ref> [19] </ref> for computing the diagonal of the inverse require n extra products after the forward and backward pivots are in place. So investigators interested in eigenvectors may be forgiven for not seeing that double factorization could be useful to them. In addition to the papers reviewed in [19] are several on twisted factorization. Here r is selected in advance and elimination starts from the top and the bottom and stops at r. Our Section 5 shows where to make the twist for the eigenvector problem. <p> It guarantees delivering an eigenvector of a close matrix. However this alone does not guarantee that computed eigenvectors for close eigenvalues are orthogonal to working accuracy. See Theorem 12 in the next section for the reason. 4.3 Our Involvement In 1993, unaware of [11], [12] or <ref> [19] </ref>, K. V. Fernando proposed computing pivots fD + (k)g using (18) in two ways: as usual going forward from initial value D + (1) = J 11 , and secondly going in reverse from final value D + (n) = 0. <p> We then realized that Theorem 2 is independent of the eigenvector problem and has other applications. We were unaware at the time of the applications described in <ref> [19] </ref> that had lead to double factorization. 16 5 The Eigenvector Connection We return to the task of computing an eigenvector v for an isolated eigenvalue of a tridiagonal ~ J. Theorem 2 shows how to compute all the fl k where Let approximate .
Reference: [20] <author> B. N. Parlett, </author> <title> The Symmetric Eigenvalue Problem, </title> <publisher> Prentice-Hall, </publisher> <editor> N. J., </editor> <year> 1980. </year>
Reference-contexts: This approach reveals the not-too-clustered eigenvalues and guarantees for them that jfl k j=kz (k) k is tiny for at least one k when ~ J is normal. To demonstrate this recall the Sinfi theorem of Davis and Kahan, see <ref> [20, Chapter 11] </ref>, valid for all Hermitian matrices. Theorem 12 (Davis and Kahan) Let A = A fl have an isolated eigenvalue with normalized eigenvector v. Consider y, y fl y = 1, and real closer to than to any other eigenvalue.
Reference: [21] <author> B. N. Parlett, </author> <title> Invariant subspaces for tightly clustered eigenvalues of tridiagonals, </title> <type> BIT 36 </type> <month> 542-562 </month> <year> (1996). </year>
Reference-contexts: Section 7 extends the results of Section 3 to block tridiagonal matrices. Theorem 2 has applications to finding the envelope of an invariant subspace and in computing condition numbers, see <ref> [21] </ref> and [5]. Even in the eigenvector application Theorem 2 has uses beyond finding the best value of r. It allows us to omit the calculation of completely negligible entries and that enhances efficiency when n becomes large (say n 100). An example of this is given in Figure 4. <p> In fact the unreduced J 's with diag (J ) = 0 have eigenvalues in pairs and any tiny pairs may be found efficiently by the method described in <ref> [21] </ref>. Acknowledgement We are glad to acknowledge helpful discussions with the LAPACK group at UC, Berkeley, in particular Ming Gu and Velvel Kahan. We thank Gene Golub 27 for the Babuska reference and Vince Fernando for the references to Crandall and Strang, Henrici and Holzer. Finally we thank A.
Reference: [22] <author> R. V. Southwell, </author> <title> Relaxation Methods in Engineering Science, </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1940. </year>
Reference-contexts: the largest component of u k .' Ipsen, in a very readable survey attributes the idea of omitting one equation of the system to Wilkinson, see Section 7 of [16], but we suspect that this method was routinely taught in mathematics classes before Wilkinson was born, see [15], [4], and <ref> [22] </ref>. He was born in 1919 and [15] was published in 1921. Wilkinson abandoned the hunt for a good value of r and used b = P Le where T I = P LU denotes triangular factorization with partial pivoting and e = i=1 e i , see [25].
Reference: [23] <author> W. G. Strang, </author> <title> Implicit difference methods for initial boundary value problems. </title> <journal> J. Math. anal. Appl. </journal> <volume> 16 </volume> <month> 188-198 </month> <year> (1966). </year>
Reference-contexts: Babuska (1972) wanted a specific entry in J 1 b for any b and his formula (5.29) on page 62 of [2] is one instance of our Corollary 4. Fischer et. al. (1974), in [7], discuss a twisted Toeplitz factorization of Buneman and attribute the adjective `twisted' to Strang <ref> [23] </ref>. Dongarra et. al. (1979), in the LINPACK codes use twisted factorization meeting in the middle for improved efficiency, see [6], and the practice has been taken up in parallel computation, see [24].
Reference: [24] <author> H. A. van der Vorst, </author> <title> Analysis of a parallel solution method for tridiagonal linear systems. </title> <booktitle> Parallel Computing 5 </booktitle> <month> 303-311 </month> <year> (1987). </year>
Reference-contexts: Dongarra et. al. (1979), in the LINPACK codes use twisted factorization meeting in the middle for improved efficiency, see [6], and the practice has been taken up in parallel computation, see <ref> [24] </ref>. It was referred to as the BABE algorithm (Begin, or Burn, At Both Ends) in [14]. 4.2 The Russian Approach This section tries to present the essence of Chapters 4 and 5 of the somewhat inscrutable book [12], but in our notation.
Reference: [25] <author> J. H. Wilkinson, </author> <title> The calculation of the eigenvectors of codiagonal matrices. </title> <journal> Computer J. </journal> <volume> 1 </volume> <month> 90-96 </month> <year> (1958). </year>
Reference-contexts: He was born in 1919 and [15] was published in 1921. Wilkinson abandoned the hunt for a good value of r and used b = P Le where T I = P LU denotes triangular factorization with partial pivoting and e = i=1 e i , see <ref> [25] </ref>. However even this choice fails if some eigenvalues are equal to working accuracy and he resorted to `tweaking' the computed eigenvalues in such cases.
Reference: [26] <author> J. H. Wilkinson, </author> <title> The Algebraic Eigenvalue Problem, </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1965. </year> <title> Fig. 1. Convergence of fl ; n = 35 Fig. 2. Ritz values for W + 21 near 6 31 Fig. </title> <type> 3. </type> <note> Blow up of Figure 2 near 6 (read the x-axis as 6+x) 32 Fig. 4. log fl; negligible eigenvector entries from 42 to 105 33 </note>
Reference-contexts: Here is a quotation from Wilkinson concerning the computation of an eigenvector u k , in Chap. 5, Section 50, below Equation (50.3) of <ref> [26] </ref>: `Hence if the largest component of u k is the rth, then it is the rth equation which should be omitted when computing u k . <p> Conversely it gives disastrous results when those extreme entries are tiny. Wilkinson gives a striking example in Section 52, Chap. 5 of <ref> [26] </ref>. The purpose of this section was to show that the `obvious' method for computing eigenvectors is not adequate for finite precision arithmetic. 5 3 Diagonal of the Inverse In basic courses in matrix theory one is taught to solve a system of equations by computing a row echelon form. <p> It is well-known that the Ss consisting of the leading principal minors of ~ J I is closely related to entries in 's eigenvector, see <ref> [26, p. 316, (48.4)] </ref> and similar reasoning, in the present context, shows that for a two-sided Ss P k = z k =z k+1 ; k = 1; : : : ; n 1; (23) where z is an eigenvector of ~ J for the eigenvalue .
References-found: 26

