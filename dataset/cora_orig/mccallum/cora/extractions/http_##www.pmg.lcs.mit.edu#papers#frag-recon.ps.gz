URL: http://www.pmg.lcs.mit.edu/papers/frag-recon.ps.gz
Refering-URL: http://www.pmg.lcs.mit.edu/~adya/index.html
Root-URL: 
Title: Fragment Reconstruction: A New Cache Coherence Scheme for Split Caching Storage Systems (Looking at the
Author: Liuba Shrira, Barbara Liskov, Miguel Castro and Atul Adya 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science MIT  
Abstract: Fragment reconstruction is a cache coherence protocol for transactional storage systems based on global caching in a network of workstations. It supports fine-grained sharing and works in the presence of object-based concurrency control algorithm. When transactions commit new versions of objects, stale cached copies of these objects get invalidated. Therefore, pages in a client's cache may become fragments, i.e. contain "holes" corresponding to invalid objects. When such a page is used in the global cache, the coherence protocol fills in the holes using modifications stored in a recoverable cache at the server. Fragment reconstruction is the first coherence protocol that supports fine-grained sharing and global caching in transactional storage systems. Because it is integrated with the recoverable modification cache, it works correctly even in the presence of client failures, and can take advantage of lazy update propagation and update absorption, which is beneficial when pages are updated repeatedly. This paper describes the fragment reconstruction protocol and presents its correctness invariant wich insures that only correctly reconstructed fragments are propagated to the database. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Adya, R. Gruber, B. Liskov, and U. Mahesh-wari. </author> <title> Efficient optimistic concurrency control using loosely synchronized clocks. </title> <booktitle> In International Conference on Management of Data. Association for Computing Machinery SIGMOD, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Fragment reconstruction coherence protocol supports fine-grained sharing in split caching. The protocol works in the presence of object-based concurrency control techniques (e.g., adaptive call-back locking [2] or the optimistic approach used in Thor <ref> [1] </ref>); such techniques are desirable because they avoid conflicts due to false sharing. In such systems, when a transaction commits new versions of some objects, this may cause pages in other client caches to become fragments, i.e. pages containing stale copies of the objects modified by those transactions. <p> The server responds with a copy of the requested object. Clients cache objects across transaction boundaries. Thor uses a new optimistic concurrency control and cache consistency protocol based on loosely synchronized clocks and invalidation messages <ref> [1] </ref>. When the application requests a transaction commit, the new values of the objects modified by the transaction are sent from the client cache to the server along with other concurrency control information. The server validates the transaction, using a two-phase commit protocol if a transaction accesses multiple servers. <p> modified pages that do not include modifications of the current transaction. (The implementation optimizes cache space and only maintains clean copies of modified objects; it reconstructs a clean page when asked to forward the page to another client or return it to the server.) The optimistic concurrency control protocol in <ref> [1] </ref> insures that when a client aborts a transaction all uncommitted effects in the client cache are undone.
Reference: [2] <author> M. Carey, M. Franklin, and M. Zaharioudakis. </author> <title> Fine-Grained Sharing in a Page Server OODBMS. </title> <booktitle> In Proceedings of SIGMOD 1994, </booktitle> <year> 1994. </year>
Reference-contexts: Fragment reconstruction coherence protocol supports fine-grained sharing in split caching. The protocol works in the presence of object-based concurrency control techniques (e.g., adaptive call-back locking <ref> [2] </ref> or the optimistic approach used in Thor [1]); such techniques are desirable because they avoid conflicts due to false sharing. <p> Simulation studies show that this scheme outperforms the best pessimistic scheme (adaptive callback locking <ref> [2] </ref>) on almost all workloads [9]. 2.2 Server Organization The servers have disk for storing persistent objects, a stable transaction log and some memory. The disk is organized as a collection of large pages that contain many objects, and that are the items read and written to disk. <p> In particular, fragment reconstruction could be used in a more traditional caching architecture where servers cache pages. It could also be used with different con-currency control mechanisms although the con-currency control mechanism affects how coherence works, e.g., a system that uses locking <ref> [2] </ref> might have to send a page with holes in response to a fetch (since some of its objects are locked 9 right now).
Reference: [3] <author> Michael J. Carey, Michael J. Franklin, Miron Livny, and Eugene J. Shekita. </author> <title> Data caching tradeoffs in client-server dbms architectures. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 357-366, </pages> <year> 1991. </year>
Reference-contexts: Research indicates that bringing such pages up to date by propagating the new object versions at commit time is not efficient <ref> [3] </ref>. Therefore, we instead invalidate those stale copies, and use fragment reconstruction to bring those pages up to date using the fragment and the information in the mcache.
Reference: [4] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Techniques for reducing consistency-related communication in distributed shared memory systems. </title> <note> To appear in ACM Transactions on Computer Systems, </note> <month> August </month> <year> 1994. </year>
Reference-contexts: Work in distributed shared memory systems has addressed the problems caused by false sharing. These systems allow concurrent accesses to shared pages by supplying synchronization primitives to support weaker consistency models, e.g., release consistency [11] in Munin, lazy release consistency [10] in TreadMarks, and multiple writer protocols <ref> [4] </ref> in both Munin and Trademarks. This work differs from our work because it does not consider transactional updates to persistent data. The distributed shared memory coherence work by Feeley et al. [15] is similar to ours because it considers transactional updates. This approach assumes a single-server, main-memory-based transactional store.
Reference: [5] <author> A. Delis and N. Roussopoulos. </author> <title> Performance and scalability of client-server database architecture. </title> <booktitle> In Proceedings of the 18th Conference on Very-Large Databases, </booktitle> <year> 1992. </year>
Reference-contexts: The difference is that Franklin et al. consider coarse-grained page-based concurrency control and do not address the problem of false sharing. Another difference is that their system uses locking while our system uses optimistic concurrency control. A coherence scheme similar to fragment reconstruction is described in <ref> [5] </ref>. In this scheme, the database server manages a cache of recent updates. Before a client accesses an object, it contacts a server to retrieve the updates needed to bring the cached copy of the object up to date.
Reference: [6] <author> M. Feeley, W. Morgan, F. Pighin, A. Karlin, H. Levy, and C. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> In SOSP, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction The distributed applications of tomorrow will need to provide reliable service to a large number of users and manipulate complex user-defined data objects. Therefore, these applications will require large-scale distributed storage systems that provide scalable performance, high reliability, and support user-defined objects. Global caching techniques <ref> [14, 6] </ref> are a promising approach in scalable storage systems, that avoids the increasingly formidable disk access bottleneck, a major performance impediment as systems scale to support many users. <p> Global caching avoids disk access by taking advantage of emerging high speed local area networks to provide remote access to huge primary memories available in workstations. However, current global caching techniques only work for file systems [14] and virtual memory systems <ref> [6] </ref>, and do not provide support for transactions and fine-grained sharing (i.e., sharing of objects that are smaller than pages). In this paper we describe a new cache coherence protocol fragment reconstruction that supports fine-grained sharing and global caching in a transactional storage system architecture called split caching.
Reference: [7] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Global memory management in client-server dbms architectures. </title> <booktitle> In Proceedings of 18th VLDB Conf., </booktitle> <year> 1992. </year>
Reference-contexts: Work in the xFS file system [14] explores global caching in a scalable serverless storage architecture. The xFS coherence protocol improves on earlier file-level coherence protocols by supporting disk block level sharing. In contrast, the fragment reconstruction protocol supports sharing of fine-grained objects. Franklin et al. <ref> [7] </ref> studies global caching in a client/server database. Clients interact with each other via servers. Servers redirect fetch requests between clients. The study evaluates several global memory management algorithms. This work is similar to ours in that it considers a transactional database.
Reference: [8] <author> S. Ghemawat. </author> <title> The Modified Object Buffer: A Storage Management Technique for Object-Oriented Databases. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1995. </year>
Reference-contexts: In split caching, clients exploit network speed and large aggregate client memory by fetching pages from other clients' caches to avoid disk reads. Servers cache only new versions of recently modified objects, using a large recoverable cache called the mcache, to perform disk updates more efficiently <ref> [17, 8] </ref>. When objects are moved from the mcache to the database on disk, it is necessary to first perform an installation read to obtain the containing pages; these pages are read from client caches to avoid read 1 ing them from disk. <p> If the object is not in the cache, the server reads the object's page from disk, stores it in the cache, and replies to the client when the disk read completes. The mcache-based server architecture improves the efficiency of disk updates for small objects <ref> [17, 8] </ref>. It avoids the cost of synchronous commit time installation reads that obtain pages from disk in order to install the modifications on their containing pages. Installation reads performed at commit time can reduce the scalability of the server significantly [17, 18]. <p> Performance studies show that for most work-loads the mcache architecture outperforms other architectures <ref> [17, 8] </ref>, including conventional architectures in which the server stores modified pages, and the clients ship entire pages to the server at commit. 3 Fine-grained Sharing in Split Caching This section describes the split caching architecture and presents the new fragment reconstruction coherence protocol that suports fine-grained sharing in split caching. <p> Work on the mcache has shown that for most workloads, performance improves as memory is shifted from the server cache to the mcache <ref> [8] </ref>. 3. Fine-grained concurrency control is much better than coarse-grained concurrency control because it avoids the problem of false sharing. The first two points motivate split caching. Since the server page cache is ineffective, servers only have an mcache.
Reference: [9] <author> R. Gruber. </author> <title> Optimism vs. Locking: A Study of Con-currency Control for Client-Server Object-Oriented Databases. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: Simulation studies show that this scheme outperforms the best pessimistic scheme (adaptive callback locking [2]) on almost all workloads <ref> [9] </ref>. 2.2 Server Organization The servers have disk for storing persistent objects, a stable transaction log and some memory. The disk is organized as a collection of large pages that contain many objects, and that are the items read and written to disk.
Reference: [10] <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In ISCA, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: Work in distributed shared memory systems has addressed the problems caused by false sharing. These systems allow concurrent accesses to shared pages by supplying synchronization primitives to support weaker consistency models, e.g., release consistency [11] in Munin, lazy release consistency <ref> [10] </ref> in TreadMarks, and multiple writer protocols [4] in both Munin and Trademarks. This work differs from our work because it does not consider transactional updates to persistent data. The distributed shared memory coherence work by Feeley et al. [15] is similar to ours because it considers transactional updates.
Reference: [11] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Henessy. </author> <title> The directory based cache coherence protocol for the dash multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: Work in distributed shared memory systems has addressed the problems caused by false sharing. These systems allow concurrent accesses to shared pages by supplying synchronization primitives to support weaker consistency models, e.g., release consistency <ref> [11] </ref> in Munin, lazy release consistency [10] in TreadMarks, and multiple writer protocols [4] in both Munin and Trademarks. This work differs from our work because it does not consider transactional updates to persistent data.
Reference: [12] <author> B. Liskov, A. Adya, M. Castro, M. Day, S. Ghe-mawat, R. Gruber, U. Maheshwari, A. Myers, and L. Shrira. </author> <title> Safe and efficient sharing of persistent objects in thor. </title> <booktitle> In Proceedings of the 1994 ACM SIGMOD, </booktitle> <year> 1996. </year>
Reference-contexts: Importantly, the protocol works even in the presence of client failures. Our work has been done in the context of Thor <ref> [12] </ref>. Thor supports fine-grained sharing and uses an mcache architecture to optimize updates. However, Thor clients maintain an object cache, whereas split caching architecture uses a page cache; and Thor servers have a page cache, unlike servers in the split caching architecture that only maintain the mcache. <p> We discuss related research in Section 5 and our con clusions in Section 6. 2 Thor Architecture We are carrying out our studies in the context of the Thor object-oriented database system <ref> [12] </ref>. Thor has a distributed client/server architecture. Persistent objects are stored on disk at the servers. Application code runs at clients. Applications interact with Thor within atomic transactions that read and write persistent objects.
Reference: [13] <author> L.Shrira. </author> <title> A Correctness Proof for Fragment Reconstruction. </title> <booktitle> In umpublished manuscript, </booktitle> <year> 1996. </year>
Reference-contexts: We now discuss this issue informally (a detailed proof appears in <ref> [13] </ref>). Our basic approach is to establish an equivalence between the recoverable states of two systems: the basic Thor system configured (for simplicity) with a zero-size page cache at the server and the extended Thor system using fragment reconstruction.
Reference: [14] <author> M.D.Dahlin, R.Y. Wang, T.E.Anderson, and D.A. Patterson. </author> <title> Cooperative caching:using remote client memory to improve file system performance. </title> <booktitle> In Proceedings of Operating Systems Design and Implementation, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction The distributed applications of tomorrow will need to provide reliable service to a large number of users and manipulate complex user-defined data objects. Therefore, these applications will require large-scale distributed storage systems that provide scalable performance, high reliability, and support user-defined objects. Global caching techniques <ref> [14, 6] </ref> are a promising approach in scalable storage systems, that avoids the increasingly formidable disk access bottleneck, a major performance impediment as systems scale to support many users. <p> Global caching avoids disk access by taking advantage of emerging high speed local area networks to provide remote access to huge primary memories available in workstations. However, current global caching techniques only work for file systems <ref> [14] </ref> and virtual memory systems [6], and do not provide support for transactions and fine-grained sharing (i.e., sharing of objects that are smaller than pages). <p> To put our work in perspective, we consider related work on global caching in file systems and transactional storage systems, and work on avoiding false sharing problems in distributed shared memory systems. Work in the xFS file system <ref> [14] </ref> explores global caching in a scalable serverless storage architecture. The xFS coherence protocol improves on earlier file-level coherence protocols by supporting disk block level sharing. In contrast, the fragment reconstruction protocol supports sharing of fine-grained objects. Franklin et al. [7] studies global caching in a client/server database.
Reference: [15] <author> M.Feeley, J.Chase, V.Narasayya, and H.Levy. </author> <title> Integrating coherency and recoverability in distributed systems. </title> <booktitle> In Usenix Symposium on Operating System Design and Implementation, </booktitle> <year> 1994. </year>
Reference-contexts: This work differs from our work because it does not consider transactional updates to persistent data. The distributed shared memory coherence work by Feeley et al. <ref> [15] </ref> is similar to ours because it considers transactional updates. This approach assumes a single-server, main-memory-based transactional store. Transactions use distributed locking. To keep caches coherent, at transaction commit time, clients propagate the commit log containing fine-grained updates to other clients. This approach does not consider global caching.
Reference: [16] <author> D. Muntz and P. Honeyman. </author> <title> Multi-level caching in distributed file systems or your cache ain't nothin' but trash. </title> <booktitle> In Winter Usenix Technical Conference, </booktitle> <year> 1992. </year>
Reference-contexts: Earlier research has shown that when there is a large number of clients with caches, and when they are sharing a large database whose size is much larger than the server memory, the server cache is relatively inef fective <ref> [16] </ref>. 2. Work on the mcache has shown that for most workloads, performance improves as memory is shifted from the server cache to the mcache [8]. 3. Fine-grained concurrency control is much better than coarse-grained concurrency control because it avoids the problem of false sharing.
Reference: [17] <author> James O'Toole and Liuba Shrira. </author> <title> Opportunistic Log: Efficient Installation Reads in a Reliable Object Server. </title> <booktitle> In Proceedings of OSDI, </booktitle> <year> 1994. </year>
Reference-contexts: In split caching, clients exploit network speed and large aggregate client memory by fetching pages from other clients' caches to avoid disk reads. Servers cache only new versions of recently modified objects, using a large recoverable cache called the mcache, to perform disk updates more efficiently <ref> [17, 8] </ref>. When objects are moved from the mcache to the database on disk, it is necessary to first perform an installation read to obtain the containing pages; these pages are read from client caches to avoid read 1 ing them from disk. <p> If the object is not in the cache, the server reads the object's page from disk, stores it in the cache, and replies to the client when the disk read completes. The mcache-based server architecture improves the efficiency of disk updates for small objects <ref> [17, 8] </ref>. It avoids the cost of synchronous commit time installation reads that obtain pages from disk in order to install the modifications on their containing pages. Installation reads performed at commit time can reduce the scalability of the server significantly [17, 18]. <p> It avoids the cost of synchronous commit time installation reads that obtain pages from disk in order to install the modifications on their containing pages. Installation reads performed at commit time can reduce the scalability of the server significantly <ref> [17, 18] </ref>. Instead, installation reads are performed asynchronously by a background thread that moves modified objects from the mcache to the disk using a read-modify-write cycle. First, the modified page is read from disk. Then the system installs the modifications in the page and writes the result to disk. <p> Performance studies show that for most work-loads the mcache architecture outperforms other architectures <ref> [17, 8] </ref>, including conventional architectures in which the server stores modified pages, and the clients ship entire pages to the server at commit. 3 Fine-grained Sharing in Split Caching This section describes the split caching architecture and presents the new fragment reconstruction coherence protocol that suports fine-grained sharing in split caching.
Reference: [18] <author> Seth J. White and David J. DeWitt. </author> <title> Implementing crash recovery in quickstore: a performance study. </title> <booktitle> In ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 187-198, </pages> <year> 1995. </year> <month> 10 </month>
Reference-contexts: It avoids the cost of synchronous commit time installation reads that obtain pages from disk in order to install the modifications on their containing pages. Installation reads performed at commit time can reduce the scalability of the server significantly <ref> [17, 18] </ref>. Instead, installation reads are performed asynchronously by a background thread that moves modified objects from the mcache to the disk using a read-modify-write cycle. First, the modified page is read from disk. Then the system installs the modifications in the page and writes the result to disk.
References-found: 18

