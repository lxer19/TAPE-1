URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tm.outbox/MIT-LCS-TM-522.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/catatm.html
Root-URL: 
Title: Communication-Minimal Partitioning of Parallel Loops and Data Arrays for Cache-Coherent Distributed-Memory Multiprocessors  
Author: Rajeev Barua, David Kranz and Anant Agarwal 
Date: December 19, 1994  
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Harnessing the full performance potential of cache-coherent distributed shared memory multiprocessors without inordinate user effort requires a compilation technology that can automatically manage multiple levels of memory hierarchy. This paper describes a working compiler for such machines that automatically partitions loops and data arrays to optimize locality of access. The compiler implements a solution to the open problem of finding communication-minimal partitions of loops and data. Loop and data partitions specify the distribution of loop iterations and array data across processors. A good loop partition maximizes the cache hit rate while a good data partition minimizes remote cache misses. The problems of finding loop and data partitions interact when multiple loops access arrays with differing reference patterns. Our algorithm handles programs with multiple nested parallel loops accessing many arrays with array access indices being general affine functions of loop variables. It discovers communication-minimal partitions when communication-free partitions do not exist. The compiler also uses sub-blocking to handle finite cache sizes. A cost model that estimates the cost of a loop and data partition given machine parameters such as cache, local and remote access timings, is presented. Minimizing the cost as estimated by our model is an NP-complete problem, as is the fully general problem of partitioning. A heuristic method which provides solutions in polynomial time is presented. The loop and data partitioning algorithm has been implemented in the compiler for the MIT Alewife machine. The paper presents results obtained from a working compiler on a 16-processor machine for three real applications: Tomcatv, Erlebacher, and Conduct. Our results demonstrate that combined optimization of loops and data can result in improvements in runtime by nearly a factor of two over optimization of loops alone.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. G. Abraham and D. E. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to reduce cache coherency traffic. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 318-328, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Ju and Dietz [8] consider the problem of reducing cache-coherence traffic in bus-based multiprocessors. Their work involves finding a data layout (row or column major) for arrays in a uniform memory access (UMA) environment. We consider finding data partitions for a distributed shared memory NUMA machine. Abraham and Hudak <ref> [1] </ref> look at the problem of automatic loop partitioning for cache locality only for the case when array accesses have simple index expressions. Their method uses only a local per-loop analysis. A more general framework for loop partitioning was presented by Agarwal et. al. [2] for optimizing for cache locality.
Reference: [2] <author> Anant Agarwal, David Kranz, and Venkat Natarajan. </author> <title> Automatic Partitioning of Parallel Loops for Cache-Coherent Multiprocessors. </title> <booktitle> In 22nd International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year> <note> IEEE. To appear in IEEE TPDS. </note>
Reference-contexts: The iterative solution is seeded with an initial partitioning of each individual loop nest that disregards data locality. This initial loop partitioning is found using the method described in <ref> [2] </ref>. The iterative solution is also seeded with an initial data partition. This initial partitioning of each array is chosen to match the partitioning of the largest loop that accesses that array. Thus, by first partitioning each loop for cache locality, the initial seeding favors cache locality over data locality. <p> Abraham and Hudak [1] look at the problem of automatic loop partitioning for cache locality only for the case when array accesses have simple index expressions. Their method uses only a local per-loop analysis. A more general framework for loop partitioning was presented by Agarwal et. al. <ref> [2] </ref> for optimizing for cache locality. That framework handled fully general affine access functions, i.e. accesses of the form A [2i+j,j] and A [100-i,j] were handled. However, that work found local minima for each loop independently, giving possibly conflicting data partitioning requests across loops in NUMA machines. <p> It describes how the number of cache misses can be estimated for a given loop partition. It also gives a brief summary of the method for loop partitioning to increase cache reuse previously published <ref> [2] </ref>. This method will be used to find an initial loop partition and an initial data partition to seed the search driven by a cost model for communication-minimal loop and data partitioning. <p> The total number of cache misses for a given loop nest is the number of its first time data accesses. This number is simply the size of the combined footprint with respect to all the accesses in the loop. <ref> [2] </ref> shows how the combined footprint in a UI-set can be computed. It also shows how the loop partitioning L can be chosen to minimize the number of cache misses. <p> The only communication for B then, (that is, memory accesses to remote memory), is at the 4 periphery of LG, due to small offsets. This periphery is what was minimized in <ref> [2] </ref>. Indeed, D is chosen as shown above to seed the search process. Data partitioning performed according to the loop partitioning is termed local in our performance results. With data partitioning, the probability that a cache miss will be satisfied in the local memory is increased. <p> F f is referred to as the peripheral footprint. See <ref> [2] </ref> for details on how the peripheral footprint is computed. <p> convergence or oscillation */ for all d 2 Data set do /* force progress */ Progress flag [d] true endfor endif endfor end Procedure 10 Program Problem Size Speedup Tomcatv N = 192 15 Erlebacher N = 48 10 Conduct 153 x 133 11 11 local Uses the analysis in <ref> [2] </ref> to determine the loop partition, and then partitions each array by using the partition induced by the largest loop that accesses that array, to achieve some data locality.
Reference: [3] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: Thus overall the time is O ((n + m)mn) = O (n 2 m + m 2 n). 6 Results The algorithm described in this paper has been implemented as part of the compiler for the Alewife <ref> [3] </ref> machine. The Alewife machine is a cache-coherent multiprocessor with physically distributed memory. The nodes are configured in a 2-dimensional mesh network.
Reference: [4] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of SIGPLAN '93 Conference on Programming Languages Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: Another view of loop partitioning involving program transformations is presented by Carr et. al. [5]. This paper was focused on uniprocessors but their method could be integrated with data partitioning for multiprocessors as well. The work of Anderson and Lam <ref> [4] </ref> does a global analysis across loops, but has the following differences with our method: (1) It does not take into account the combined effect on performance of globally coherent caches and local memories. (2) It attempts to find a communication free partition by satisfying a system of constraints, failing which <p> Cyclic partitions could be handled using this method but for simplicity we leave them out. 5.1 Graph formulation Our search procedure uses bipartite graphs to represent loops and data arrays. Bipartite graphs are a popular data structure used to represent partitioning problems for loops and data <ref> [8, 4] </ref>. For a graph G = (V l ; V d ; E), the loops are pictured as a set of nodes V l on the left hand side, and the data arrays as a set of nodes V d on the right. <p> These results indicate that significant performance improvements can be obtained by looking at data locality and cache locality in a global framework. In the future we would like to add the possibility of copying data at runtime to avoid remote references as in <ref> [4] </ref>. This factor could be added to our cost model.
Reference: [5] <author> Steve Carr, Kathryn S. McKinley, and Chau-Wen Tzeng. </author> <title> Compiler Optimization for Improving Data Locality. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 252-262, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: However, that work found local minima for each loop independently, giving possibly conflicting data partitioning requests across loops in NUMA machines. Another view of loop partitioning involving program transformations is presented by Carr et. al. <ref> [5] </ref>. This paper was focused on uniprocessors but their method could be integrated with data partitioning for multiprocessors as well.
Reference: [6] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of Automatic Data Partitioning Techniques for Parallelizing Compilers on Multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year> <month> 12 </month>
Reference-contexts: Because our method uses a cost model, it can evaluate different competing alternatives, each having some amount of communication, and choose between them. (3) We guarantee a load balanced solution. Gupta and Banerjee <ref> [6] </ref> have developed an algorithm for partitioning doing a global analysis across loops. They allow simple index expression accesses of the form c 1 fl i + c 2 , but not general affine functions.
Reference: [7] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD Distributed Memory Machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: One approach to the problem is to have programmers specify data partitions explicitly in the program, as in Fortran-D <ref> [7, 12] </ref>. Loop partitions are usually determined by the owner computes rule. Though simple to implement, this requires the user to thoroughly understand the access patterns of the program, a task which is not trivial even for small programs.
Reference: [8] <author> Y. Ju and H. Dietz. </author> <title> Reduction of Cache Coherence Overhead by Compiler Data Layout and Loop Transformation. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 344-358. </pages> <publisher> Springer Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Their theory produces communication-free hyperplane partitions for loops with affine index expressions when such partitions exist. However, when communication-free partitions do not exist, they deal only with index expressions of the form variable plus a constant. Ju and Dietz <ref> [8] </ref> consider the problem of reducing cache-coherence traffic in bus-based multiprocessors. Their work involves finding a data layout (row or column major) for arrays in a uniform memory access (UMA) environment. We consider finding data partitions for a distributed shared memory NUMA machine. <p> Cyclic partitions could be handled using this method but for simplicity we leave them out. 5.1 Graph formulation Our search procedure uses bipartite graphs to represent loops and data arrays. Bipartite graphs are a popular data structure used to represent partitioning problems for loops and data <ref> [8, 4] </ref>. For a graph G = (V l ; V d ; E), the loops are pictured as a set of nodes V l on the left hand side, and the data arrays as a set of nodes V d on the right.
Reference: [9] <author> Kathleen Knobe, Joan Lukas, and Guy Steele Jr. </author> <title> Data Optimization: Allocation of Arrays to Reduce Communication on SIMD Machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: They allow simple index expression accesses of the form c 1 fl i + c 2 , but not general affine functions. They do not allow for the possibility of hyperparallelepiped data tiles, and do not account for caches. Knobe, Lucas and Steele <ref> [9] </ref> give a method of allocating arrays on SIMD machines. They align arrays to minimize communication for vector instructions, which access array regions specified by subranges on each dimension.
Reference: [10] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-Time Techniques for Data Distribution in Distributed Memory Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: For real medium-sized or large programs, the task is a very difficult one. Presence of fully general affine function accesses further complicates the process. Worse, the program would not be portable across machines with different architectural parameters. 2 Ramanujam and Sadayappan <ref> [10] </ref> consider data partitioning in multicomputers and use a matrix formulation; their results do not apply to multiprocessors with caches. Their theory produces communication-free hyperplane partitions for loops with affine index expressions when such partitions exist.
Reference: [11] <author> Bart Selman, Henry Kautz, and Bram Cohen. </author> <title> Noise Strategies for Improving Local Search. </title> <booktitle> In Proceedings, AAAI, </booktitle> <volume> volume 1, </volume> <year> 1994. </year>
Reference-contexts: Extensive work evaluating search techniques has been done by researchers in many disciplines. Simulated annealing, gradient descent and genetic algorithms are some of these. See <ref> [11] </ref> for a comparison of some methods. All techniques rely on a cost function estimating some objective value to be optimized, and a search strategy. For specific problems more may be known than in the general case, and specific strategies may do better.
Reference: [12] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> Jan </month> <year> 1993. </year> <note> Published as Rice COMP TR93-199. </note>
Reference-contexts: One approach to the problem is to have programmers specify data partitions explicitly in the program, as in Fortran-D <ref> [7, 12] </ref>. Loop partitions are usually determined by the owner computes rule. Though simple to implement, this requires the user to thoroughly understand the access patterns of the program, a task which is not trivial even for small programs.
Reference: [13] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Loop Transformation Theory and an Algorithm to Maximize Parallelism. </title> <booktitle> In The Third Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1990. </year> <institution> Irvine, </institution> <address> CA. </address> <month> 13 </month>
Reference-contexts: Knobe, Lucas and Steele [9] give a method of allocating arrays on SIMD machines. They align arrays to minimize communication for vector instructions, which access array regions specified by subranges on each dimension. Wolf and Lam <ref> [13] </ref> deal with the problem of taking sequential nested loops and applying transformations to attempt to convert them to a nest of parallel loops with at most one outer sequential loop.
References-found: 13

