URL: ftp://ftp.cc.gatech.edu/pub/people/cga/air1.ps.gz
Refering-URL: http://www.cs.gatech.edu/fac/Chris.Atkeson/publications.html
Root-URL: 
Title: Locally Weighted Learning  
Author: Christopher G. Atkeson Andrew W. Moore and Stefan Schaal 
Keyword: locally weighted regression, LOESS, LWR, lazy learning, memory-based learning, least commitment learning, distance functions, smoothing parame ters, weighting functions, global tuning, local tuning, interference.  
Address: 801 Atlantic Drive, Atlanta, GA 30332-0280  5000 Forbes Ave, Pittsburgh, PA 15213  2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-02, Japan  
Affiliation: College of Computing, Georgia Institute of Technology  Carnegie Mellon University  ATR Human Information Processing Research Laboratories  
Email: cga@cc.gatech.edu, sschaal@cc.gatech.edu  awm@cs.cmu.edu  
Web: http://www.cc.gatech.edu/fac/Chris.Atkeson http://www.cc.gatech.edu/fac/Stefan.Schaal  http://www.cs.cmu.edu/~awm/hp.html  
Date: October 12, 1996 at 10:21  
Abstract: This paper surveys locally weighted learning, a form of lazy learning and memory-based learning, and focuses on locally weighted linear regression. The survey discusses distance functions, smoothing parameters, weighting functions, local model structures, regularization of the estimates and bias, assessing predictions, handling noisy data and outliers, improving the quality of predictions by tuning fit parameters, interference between old and new data, implementing locally weighted learning efficiently, and applications of locally weighted learning. A companion paper surveys how locally weighted learning can be used in robot learning and control. 
Abstract-found: 1
Intro-found: 1
Reference: <editor> AAAI-9 (1991). </editor> <booktitle> Ninth National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press/The MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference: <author> Aha, D. W. </author> <year> (1989). </year> <title> Incremental, instance-based learning of independent and graded concept descriptions. </title> <booktitle> In Sixth International Machine Learning Workshop, </booktitle> <pages> pages 387-391. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Aha, D. W. </author> <year> (1990). </year> <title> A Study of Instance-Based Algorithms for Supervised Learning Tasks: Mathematical, Empirical, and Psychological Observations. </title> <type> PhD dissertation, </type> <institution> University of California, Irvine, Department of Information and Computer Science. </institution>
Reference-contexts: The reliability weights can be based on cross validation: whether a stored point correctly predicts or classifies its neighbors. Another approach is to only utilize stored points that have shown that they can reduce the cross validation error <ref> (Aha, 1990) </ref>. Important issues are when the weighting decision is made and how often the decision is reevaluated.
Reference: <author> Aha, D. W. </author> <year> (1991). </year> <title> Incremental constructive induction: An instance-based approach. </title> <booktitle> In Eighth International Machine Learning Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Aha, D. W. and Goldstone, R. L. </author> <year> (1990). </year> <title> Learning attribute relevance in context in instance-based learning algorithms. </title> <booktitle> In 12th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 141-148. </pages>
Reference-contexts: The reliability weights can be based on cross validation: whether a stored point correctly predicts or classifies its neighbors. Another approach is to only utilize stored points that have shown that they can reduce the cross validation error <ref> (Aha, 1990) </ref>. Important issues are when the weighting decision is made and how often the decision is reevaluated.
Reference: <author> Aha, D. W. and Goldstone, R. L. </author> <year> (1992). </year> <title> Concept learning and flexible weighting. </title> <booktitle> In 14th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 534-539, </pages> <address> Bloomington, IL. </address> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Mahwah, NJ. </address>
Reference-contexts: Another smoothing weight function is a Gaussian kernel (Deheuvels, 1977; Wand and Schucany, 1990; Schaal and Atkeson, 1994): K (d) = exp d 2 (32) This kernel also has infinite extent. A related kernel is the exponential kernel, which has been used in psychological models <ref> (Aha and Goldstone, 1992) </ref>: K (d) = exp [ jdj] (33) These kernels have infinite extent, and can be truncated when they become smaller than a threshold value to ignore data further from a particular radius from the query.
Reference: <author> Aha, D. W. and Kibler, D. </author> <year> (1989). </year> <title> Noise-tolerant instance-based learning algorithms. </title> <booktitle> In Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 794-799. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Aha, D. W. and McNulty, D. M. </author> <year> (1989). </year> <title> Learning relative attribute weights for instance-based concept descriptions. </title> <booktitle> In 11th Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 530-537. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Mahwah, NJ. </address>
Reference: <author> Aha, D. W. and Salzberg, S. L. </author> <year> (1993). </year> <title> Learning to catch: Applying nearest neighbor algorithms to dyna mic control tasks. </title> <booktitle> In Proceedings of the Fourth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 363-368, </pages> <address> Ft. Lauderdale, FL. </address>
Reference: <author> Altman, N. S. </author> <year> (1992). </year> <title> An introduction to kernel and nearest-neighbor nonparametric regression. </title> <journal> The American Statistician, </journal> <volume> 46(3) </volume> <pages> 175-185. </pages>
Reference: <author> Atkeson, C. G. </author> <year> (1990). </year> <title> Using local models to control movement. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances In Neural Information Processing Systems 2, </booktitle> <pages> pages 316-323. </pages> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Atkeson, C. G. </author> <year> (1992). </year> <title> Memory-based approaches to approximating continuous functions. </title> <booktitle> In Casdagli and Eubank (1992), </booktitle> <pages> pages 503-521. </pages> <booktitle> Proceedings of a Workshop on Nonlinear Modeling and Forecasting September 17-21, 1990, </booktitle> <address> Santa Fe, New Mexico. </address>
Reference-contexts: The architecture for the sigmoidal feedforward neural network was taken from (Goldberg and Pearlmutter, 1988, section 6) who modeled arm inverse dynamics. The ability of each of these methods to predict the torques of the simulated two joint arm at 1000 random points was compared <ref> (Atkeson, 1992) </ref>. Figure 15 plots the normalized RMS prediction error. The points were sampled uniformly using ranges comparable to those used in Miller et al. (1987), which also looked at two joint arm inverse dynamics modeling.
Reference: <author> Atkeson, C. G. </author> <year> (1996). </year> <title> Local learning. </title> <address> http://www.cc.gatech.edu/fac/Chris.Atkeson/local-learning/. </address>
Reference-contexts: We provide an example of interference to clarify this point. We briefly survey published applications of locally weighted learning. A companion paper <ref> (Atkeson et al., 1996) </ref> surveys how locally weighted learning can be used in robot learning and control. This review is augmented by a Web page (Atkeson, 1996). This review emphasizes a statistical view of learning, in which function approximation plays the central role. <p> We provide an example of interference to clarify this point. We briefly survey published applications of locally weighted learning. A companion paper (Atkeson et al., 1996) surveys how locally weighted learning can be used in robot learning and control. This review is augmented by a Web page <ref> (Atkeson, 1996) </ref>. This review emphasizes a statistical view of learning, in which function approximation plays the central role. In order to be concrete, the review focuses on a narrow problem formulation, in which training data consists of input vectors of specific attribute values and the corresponding output values.
Reference: <author> Atkeson, C. G., Moore, A. W., and Schaal, S. </author> <year> (1996). </year> <title> Locally weighted learning for control. </title> <journal> Artificial Intelligence Review. </journal> <note> in press. </note>
Reference-contexts: We provide an example of interference to clarify this point. We briefly survey published applications of locally weighted learning. A companion paper <ref> (Atkeson et al., 1996) </ref> surveys how locally weighted learning can be used in robot learning and control. This review is augmented by a Web page (Atkeson, 1996). This review emphasizes a statistical view of learning, in which function approximation plays the central role. <p> We provide an example of interference to clarify this point. We briefly survey published applications of locally weighted learning. A companion paper (Atkeson et al., 1996) surveys how locally weighted learning can be used in robot learning and control. This review is augmented by a Web page <ref> (Atkeson, 1996) </ref>. This review emphasizes a statistical view of learning, in which function approximation plays the central role. In order to be concrete, the review focuses on a narrow problem formulation, in which training data consists of input vectors of specific attribute values and the corresponding output values.
Reference: <author> Atkeson, C. G. and Reinkensmeyer, D. J. </author> <year> (1988). </year> <title> Using associative content-addressable memories to control robots. </title> <booktitle> In Proceedings of the 27th IEEE Conference on Decision and Control, </booktitle> <volume> volume 1, </volume> <pages> pages 792-797, </pages> <address> Austin, Texas. </address> <publisher> IEEE Cat. No.88CH2531-2. </publisher>
Reference: <author> Atkeson, C. G. and Reinkensmeyer, D. J. </author> <year> (1989). </year> <title> Using associative content-addressable memories to control robots. </title> <booktitle> In Proceedings, IEEE International Conference on Robotics and Automation, </booktitle> <address> Scottsdale, Arizona. </address> <note> 43 Atkeson, </note> <author> C. G. and Schaal, S. </author> <year> (1995). </year> <title> Memory-based neural networks for robot learning. </title> <journal> Neurocomputing, </journal> <volume> 9 </volume> <pages> 243-269. </pages>
Reference: <author> Baird, L. C. and Klopf, A. H. </author> <year> (1993). </year> <title> Reinforcement learning with high-dimensional, continuous actions. </title> <type> Technical Report WL-TR-93-1147, </type> <institution> Wright Laboratory, Wright-Patterson Air Force Base Ohio. </institution> <address> http://kirk.usafa.af.mil/ baird/papers/index.html. </address>
Reference: <author> Barnhill, R. E. </author> <year> (1977). </year> <title> Representation and approximation of surfaces. </title> <editor> In Rice, J. R., editor, </editor> <booktitle> Mathematical Software III, </booktitle> <pages> pages 69-120. </pages> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Batchelor, B. G. </author> <year> (1974). </year> <title> Practical Approach To Pattern Classification. </title> <publisher> Plenum Press, </publisher> <address> New York, NY. </address>
Reference: <author> Benedetti, J. K. </author> <year> (1977). </year> <title> On the nonparametric estimation of regression functions. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39 </volume> <pages> 248-253. </pages>
Reference: <author> Bentley, J. L. </author> <year> (1975). </year> <title> Multidimensional binary search trees used for associative searching. </title> <journal> Communications of the ACM, </journal> <volume> 18(9) </volume> <pages> 509-517. </pages>
Reference: <author> Bentley, J. L. and Friedman, J. H. </author> <year> (1979). </year> <title> Data structures for range searching. </title> <journal> ACM Comput. Surv., </journal> <volume> 11(4) </volume> <pages> 397-409. </pages>
Reference: <author> Bentley, J. L., Weide, B., and Yao, A. </author> <year> (1980). </year> <title> Optimal expected time algorithms for closest point problems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 6 </volume> <pages> 563-580. </pages>
Reference: <author> Blyth, S. </author> <year> (1993). </year> <title> Optimal kernel weights under a power criterion. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 88(424) </volume> <pages> 1284-1286. </pages>
Reference: <author> Bottou, L. and Vapnik, V. </author> <year> (1992). </year> <title> Local learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 888-900. </pages>
Reference-contexts: of freedom in the training process, leading to increased variance of the predictions and an increased risk of overfitting the data (Cleveland and Loader, 1994c): * Adaptation to the data density and distribution: This adaptation is in addition to the adaptation provided by the locally weighted regression procedure it self <ref> (Bottou and Vapnik, 1992) </ref>. 30 * Adaptation to variations in the noise level in the training data. These variations are known as heteroscedasticity. * Adaptation to variations in the behavior of the underlying function.
Reference: <author> Bregler, C. and Omohundro, S. M. </author> <year> (1994). </year> <title> Surface learning with applications to lipreading. </title> <editor> In Cowan et al. </editor> <year> (1994), </year> <pages> pages 43-50. </pages>
Reference: <author> Brockmann, M., Gasser, T., and Herrmann, E. </author> <year> (1993). </year> <title> Locally adaptive bandwidth choice for kernel regression estimators. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 88(424) </volume> <pages> 1302-1309. </pages>
Reference-contexts: For practical purposes it would be useful to have a clear understanding of how accurate the non-linear fit parameters should be for a good fit. Our intuition is that approximate values usually result in barely distinguishable performance from optimal parameters in practical use, although <ref> (Brockmann et al., 1993) </ref> states that this is not true for h in kernel regression. The next section considers optimizing a single set of parameters for all possible future queries (global tuning).
Reference: <author> Broder, A. J. </author> <year> (1990). </year> <title> Strategies for efficient incremental nearest neighbor search. </title> <journal> Pattern Recognition, </journal> <volume> 23 </volume> <pages> 171-178. </pages>
Reference: <author> Callan, J. P., Fawcett, T. E., and Rissland, E. L. </author> <year> (1991). </year> <title> CABOT: An adaptive approach to case based search. </title> <booktitle> In IJCAI 12 (1991), </booktitle> <pages> pages 803-808. </pages>
Reference: <author> Casdagli, M. and Eubank, S., </author> <title> editors (1992). Nonlinear Modeling and Forecasting. </title> <booktitle> Proceedings Volume XII in the Santa Fe Institute Studies in the Sciences of Complexity. </booktitle> <publisher> Addison Wesley, </publisher> <address> New York, NY. </address> <booktitle> Proceedings of a Workshop on Nonlinear Modeling and Forecasting September 17-21, 1990, </booktitle> <address> Santa Fe, New Mexico. </address>
Reference: <author> Cheng, P. E. </author> <year> (1984). </year> <title> Strong consistency of nearest neighbor regression function estimators. </title> <journal> Journal of Multivariate Analysis, </journal> <volume> 15 </volume> <pages> 63-72. </pages>
Reference: <author> Cleveland, W. S. </author> <year> (1979). </year> <title> Robust locally weighted regression and smoothing scatterplots. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 74 </volume> <pages> 829-836. </pages>
Reference-contexts: In our applications, we used 2.57, cutting off all data outside the 99% area of the normal distribution. 11.3 Robust Regression Approaches Data with outliers can be viewed as having additive noise with long-tailed symmetric distributions. Robust regression is useful for both global and local detection of outliers <ref> (Cleveland, 1979) </ref>. A bisquare weighting function is used to additionally downweight points based on their residuals: u i = &gt; &lt; 6e MED 2 0 otherwise (70) where e MED is the median of the absolute value of the residuals e i .
Reference: <author> Cleveland, W. S. </author> <year> (1993a). </year> <title> Coplots, nonparametric regression, and conditionally parametric fits. </title> <type> Technical Report 19, </type> <institution> AT&T Bell Laboratories, Statistics Department, </institution> <address> Murray Hill, NJ. http://netlib.att.com/netlib/att/stat/doc/. </address>
Reference: <author> Cleveland, W. S. </author> <year> (1993b). </year> <title> Visualizing Data. </title> <publisher> Hobart Press, Summit, </publisher> <address> NJ. books@hobart.com. </address>
Reference: <author> Cleveland, W. S. and Devlin, S. J. </author> <year> (1988). </year> <title> Locally weighted regression: An approach to regression analysis by local fitting. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 83 </volume> <pages> 596-610. </pages>
Reference: <author> Cleveland, W. S., Devlin, S. J., and Grosse, E. </author> <year> (1988). </year> <title> Regression by local fitting: Methods, properties, and computational algorithms. </title> <journal> Journal of Econometrics, </journal> <volume> 37 </volume> <pages> 87-114. </pages>
Reference: <author> Cleveland, W. S. and Grosse, E. </author> <year> (1991). </year> <title> Computational methods for local regression. </title> <journal> Statistics and Computing, </journal> <volume> 1(1) </volume> <pages> 47-62. </pages> <month> ftp://cm.bell-labs.com/cm/cs/doc/91/4-04.ps.gz. </month>
Reference: <author> Cleveland, W. S., Grosse, E., and Shyu, W. M. </author> <year> (1992). </year> <title> Local regression models. In Chambers, </title> <editor> J. M. and Hastie, T. J., editors, </editor> <booktitle> Statistical Models in S, </booktitle> <pages> pages 309-376. </pages> <publisher> Wadsworth, </publisher> <address> Pacific Grove, CA. http://netlib.att.com/netlib/a/cloess.ps.Z. </address>
Reference: <author> Cleveland, W. S. and Loader, C. </author> <year> (1994a). </year> <title> Computational methods for local regression. </title> <type> Technical Report 11, </type> <institution> AT&T Bell Laboratories, Statistics Department, </institution> <address> Murray Hill, NJ. http://netlib.att.com/netlib/att/stat/doc/. </address>
Reference: <author> Cleveland, W. S. and Loader, C. </author> <year> (1994b). </year> <title> Local fitting for semiparametric (nonparametric) regression: Comments on a paper of Fan and Marron. </title> <type> Technical Report 8, </type> <institution> AT&T Bell Laboratories, Statistics Department, </institution> <address> Murray Hill, NJ. </address> <note> http://netlib.att.com/netlib/att/stat/doc/, 94.8.ps, earlier version is 44 94.3.ps. </note>
Reference-contexts: This problem is present at edges or between data clusters and gets worse in higher dimensions. In general, fixed bandwidth selection has much larger changes in variance than nearest neighbor bandwidth selection. A fixed bandwidth smoother can also not have any data within its span, leading to undefined estimates <ref> (Cleveland and Loader, 1994b) </ref>. Fan and Marron (1994b) describe three reasons to use variable local bandwidths: to adapt to the data distribution, to adapt for different levels of noise (heteroscedasticity), and to adapt to changes in the smoothness or curvature of the function.
Reference: <author> Cleveland, W. S. and Loader, C. </author> <year> (1994c). </year> <title> Smoothing by local regression: Principles and methods. </title> <type> Technical Report 95.3, </type> <institution> AT&T Bell Laboratories, Statistics Department, </institution> <address> Murray Hill, NJ. http://netlib.att.com/netlib/att/stat/doc/. </address>
Reference-contexts: However, tuning of fit parameters can be a background process that operates on a slower time scale than adding new data and answering queries. Cross validation can also be performed locally, i.e, from just fitting a locally linear model at one query point q <ref> (Cleveland and Loader, 1994c) </ref>. <p> There are several reasons to consider local tuning, although it dramatically increases the number of degrees of freedom in the training process, leading to increased variance of the predictions and an increased risk of overfitting the data <ref> (Cleveland and Loader, 1994c) </ref>: * Adaptation to the data density and distribution: This adaptation is in addition to the adaptation provided by the locally weighted regression procedure it self (Bottou and Vapnik, 1992). 30 * Adaptation to variations in the noise level in the training data.
Reference: <author> Cleveland, W. S., Mallows, C. L., and McRae, J. E. </author> <year> (1993). </year> <title> ATS methods: Nonparametric regression for non-Gaussian data. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 88(423) </volume> <pages> 821-835. </pages>
Reference: <author> Connell, M. E. and Utgoff, P. E. </author> <year> (1987). </year> <title> Learning to control a dynamic physical system. </title> <booktitle> In Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 456-460, </pages> <address> Seattle, WA. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Cost, S. and Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10(1) </volume> <pages> 57-78. </pages>
Reference: <author> Coughran, Jr., W. M. and Grosse, E. </author> <year> (1991). </year> <title> Seeing and hearing dynamic loess surfaces. </title> <booktitle> In Interface'91 Proceedings, </booktitle> <pages> pages 224-228. </pages> <publisher> Springer-Verlag. </publisher> <address> ftp://cm.bell-labs.com/cm/cs/doc/91/4-07.ps.gz or 4-07long.ps.gz. </address>
Reference: <editor> Cowan, J. D., Tesauro, G., and Alspector, J., editors (1994). </editor> <booktitle> Advances In Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Crain, I. K. and Bhattacharyya, B. K. </author> <year> (1967). </year> <title> Treatment of nonequispaced two dimensional data with a digital computer. </title> <journal> Geoexploration, </journal> <volume> 5 </volume> <pages> 173-194. </pages>
Reference: <author> Deheuvels, P. </author> <year> (1977). </year> <title> Estimation non-parametrique del la densite par histogrammes generalises. </title> <journal> Revue Statistique Applique, </journal> <volume> 25 </volume> <pages> 5-42. </pages>
Reference: <author> Deng, K. and Moore, A. W. </author> <year> (1994). </year> <title> Multiresolution instance-based learning. </title> <booktitle> In Fourteenth International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Dennis, J. E., Gay, D. M., and Welsch, R. E. </author> <year> (1981). </year> <title> An adaptive nonlinear least-squares algorithm. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(3) </volume> <pages> 369-383. </pages>
Reference-contexts: Inevitably this is local hill climbing, with a large risk of getting stuck in local optima. The sum of the squared cross validation errors is minimized using a nonlinear parameter estimation procedure (e.g., MINPACK (More et al., 1980) or NL2SOL <ref> (Dennis et al., 1981) </ref>). As discussed in Section 9.3, in this locally weighted learning approach computing the cross validation error for a single point is no more computationally expensive than answering a query.
Reference: <author> Devroye, L. </author> <year> (1981). </year> <title> On the almost everywhere convergence of nonparametric regression function estimates. </title> <journal> The Annals of Statistics, </journal> <volume> 9(6) </volume> <pages> 1310-1319. </pages>
Reference: <author> Diebold, F. X. and Nason, J. A. </author> <year> (1990). </year> <title> Nonparametric exchange rate prediction? Journal of International Economics, </title> <publisher> 28(3-4):315-332. </publisher>
Reference: <author> Dietterich, T. G., Wettschereck, D., Atkeson, C. G., and Moore, A. W. </author> <year> (1994). </year> <title> Memory-based methods for regression and classification. </title> <editor> In Cowan et al. </editor> <year> (1994), </year> <pages> pages 1165-1166. </pages>
Reference: <author> Draper, N. R. and Smith, H. </author> <year> (1981). </year> <title> Applied Regression Analysis. </title> <publisher> John Wiley, </publisher> <address> New York, NY, 2nd edition. </address>
Reference-contexts: If there are not enough nearby points with non-zero weights in all directions, there are not enough different equations to solve for the unknown parameters fi. Ridge regression <ref> (Draper and Smith, 1981) </ref> is used to prevent problems due to a singular data matrix. <p> 5 2 6 6 6 6 6 6 6 6 v 2 fi 2 n fi n 7 7 7 7 7 7 7 7 5 Adding additional rows can be viewed as adding "fake" data, which, in the absence of sufficient real data, biases the parameter estimates to fi <ref> (Draper and Smith, 1981) </ref>. Another view of ridge regression parameters is that they are the Bayesian assumptions about the apriori distributions of the estimated parameters (Seber, 1977). As described in Section 12 on tuning, optimizing the ridge regression parameters using cross validation can identify irrelevant dimensions.
Reference: <author> Elliot, T. and Scott, P. D. </author> <year> (1991). </year> <title> Instance-based and generalization-based learning procedures applied to solving integration problems. </title> <booktitle> In Proceedings of the Eighth Conference of the Society for the Study of Artificial Intelligence, </booktitle> <pages> pages 256-265, </pages> <address> Leeds, England. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: is: C = i The inputs X i , outputs Y i , and query Q can be complex objects such as entire semantic networks, with the distance functions being graph matching algorithms or graph difference measuring algorithms, and f () being a graph transformation with fi as adjustable parameters <ref> (Elliot and Scott, 1991) </ref>.
Reference: <author> Epanechnikov, V. A. </author> <year> (1969). </year> <title> Nonparametric estimation of a multivariate probability density. </title> <journal> Theory of Probability and Its Applications, </journal> <volume> 14 </volume> <pages> 153-158. </pages>
Reference: <author> Eubank, R. L. </author> <year> (1988). </year> <title> Spline Smoothing and Nonparametric Regression. </title> <publisher> Marcel Dekker, </publisher> <address> New York, NY. </address>
Reference: <author> Falconer, K. J. </author> <year> (1971). </year> <title> A general purpose algorithm for contouring over scattered data points. </title> <type> Technical Report NAC 6, </type> <institution> National Physical Laboratory, Teddington, </institution> <address> Middlesex, United Kingdon, TW11 0LW. </address>
Reference: <author> Fan, J. </author> <year> (1992). </year> <title> Design-adaptive nonparametric regression. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 87(420) </volume> <pages> 998-1004. </pages>
Reference: <author> Fan, J. </author> <year> (1993). </year> <title> Local linear regression smoothers and their minimax efficiencies. </title> <journal> Annals of Statistics, </journal> <volume> 21 </volume> <pages> 196-216. </pages>
Reference-contexts: There are several ways to use this parameter (Scott, 1992; Cleveland and Loader, 1994c): * Fixed bandwidth selection: h is a constant value <ref> (Fan and Marron, 1993) </ref>, and therefore volumes of data with constant size and shape are used.
Reference: <author> Fan, J. </author> <year> (1995). </year> <title> Local modeling. EES Update: written for the Encyclopedia of Statistics Science, </title> <address> http://www.stat.unc.edu/faculty/fan/papers.html. </address>
Reference: <author> Fan, J. and Gijbels, I. </author> <year> (1992). </year> <title> Variable bandwidth and local linear regression smoothers. </title> <journal> The Annals of Statistics, </journal> <volume> 20(4) </volume> <pages> 2008-2036. </pages>
Reference: <author> Fan, J. and Gijbels, I. </author> <year> (1994). </year> <title> Censored regression: Local linear approximations and their applications. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 89 </volume> <pages> 560-570. </pages>
Reference: <author> Fan, J. and Gijbels, I. </author> <year> (1995a). </year> <title> Adaptive order polynomial fitting: Bandwidth robustification and bias reduction. </title>
Reference: <author> Fan, J. and Gijbels, I. </author> <year> (1995b). </year> <title> Data-driven bandwidth selection in local polynomial fitting: Variable bandwidth and spatial adaptation. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 57 </volume> <pages> 371-394. </pages>
Reference: <author> Fan, J. and Gijbels, I. </author> <year> (1996). </year> <title> Local Polynomial Modeling and its Applications. </title> <publisher> Chapman and Hall, </publisher> <address> 45 London. </address>
Reference: <author> Fan, J. and Hall, P. </author> <year> (1994). </year> <title> On curve estimation by minimizing mean absolute deviation and its implications. </title> <journal> The Annals of Statistics, </journal> <volume> 22(2) </volume> <pages> 867-885. </pages>
Reference: <author> Fan, J. and Kreutzberger, E. </author> <year> (1995). </year> <title> Automatic local smoothing for spectral density estimation. </title> <publisher> ftp://stat.unc.edu/pub/fan/spec.ps. </publisher>
Reference: <author> Fan, J. and Marron, J. S. </author> <year> (1993). </year> <title> Comment on [Hastie and Loader, 1993]. </title> <journal> Statistical Science, </journal> <volume> 8(2) </volume> <pages> 129-134. </pages>
Reference-contexts: There are several ways to use this parameter (Scott, 1992; Cleveland and Loader, 1994c): * Fixed bandwidth selection: h is a constant value <ref> (Fan and Marron, 1993) </ref>, and therefore volumes of data with constant size and shape are used.
Reference: <author> Fan, J. and Marron, J. S. </author> <year> (1994a). </year> <title> Fast implementations of nonparametric curve estimators. </title> <journal> Journal of Computational and Graphical Statistics, </journal> <volume> 3 </volume> <pages> 35-56. </pages>
Reference: <author> Fan, J. and Marron, J. S. </author> <year> (1994b). </year> <title> Rejoinder to discussion of Cleveland and Loader. </title>
Reference: <author> Farmer, J. D. and Sidorowich, J. J. </author> <year> (1987). </year> <title> Predicting chaotic time series. </title> <journal> Physical Review Letters, </journal> <volume> 59(8) </volume> <pages> 845-848. </pages>
Reference: <author> Farmer, J. D. and Sidorowich, J. J. </author> <year> (1988a). </year> <title> Exploiting chaos to predict the future and reduce noise. </title> <editor> In Lee, Y. C., editor, </editor> <title> Evolution, Learning, and Cognition, </title> <publisher> pages 277-??? World Scientific Press, </publisher> <address> NJ. </address> <note> also available as Technical Report LA-UR-88-901, </note> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, New Mexico. </address>
Reference: <author> Farmer, J. D. and Sidorowich, J. J. </author> <year> (1988b). </year> <title> Predicting chaotic dynamics. </title> <editor> In Kelso, J. A. S., Mandell, A. J., and Schlesinger, M. F., editors, </editor> <booktitle> Dynamic Patterns in Complex Systems, </booktitle> <pages> pages 265-292. </pages> <publisher> World Scientific, </publisher> <address> NJ. </address>
Reference: <author> Farwig, R. </author> <year> (1987). </year> <title> Multivariate interpolation of scattered data by moving least squares methods. </title> <editor> In Mason, J. C. and Cox, M. G., editors, </editor> <booktitle> Algorithms for Approximation, </booktitle> <pages> pages 193-211. </pages> <publisher> Clarendon Press, Oxford. </publisher>
Reference: <author> Fedorov, V. V., Hackl, P., and Muller, W. G. </author> <year> (1993). </year> <title> Moving local regression: The weight function. </title> <journal> Nonparametric Statistics, </journal> <volume> 2(4) </volume> <pages> 355-368. </pages>
Reference-contexts: Ruprecht and Muller (1994b) generalize the distance function to a point-set metric. In our view, there is no clear evidence that the choice of weighting function is critical (Scott, 1992; Cleveland and Loader, 1994a,c), however, there are examples where one can show differences <ref> (Fedorov et al., 1993) </ref>. Cleveland and Loader (1994b) criticize the uniform kernel for similar reasons as are used in signal processing and spectrum estimation. Optimal kernels are discussed by Gasser and Muller (1984), Gasser et al. (1985), Scott (1992), Blyth (1993) and Fedorov et al. (1993).
Reference: <author> Franke, R. and Nielson, G. </author> <year> (1980). </year> <title> Smooth interpolation of large sets of scattered data. </title> <journal> International Journal for Numerical Methods in Engineering, </journal> <volume> 15 </volume> <pages> 1691-1704. </pages>
Reference: <author> Friedman, J. H. </author> <year> (1984). </year> <title> A variable span smoother. </title> <type> Technical Report LCS 5, </type> <institution> Stanford University, Statistics Department, Stanford, </institution> <address> CA. </address>
Reference: <author> Friedman, J. H. </author> <year> (1994). </year> <title> Flexible metric nearest neighbor classification. </title> <address> http://playfair.stanford.edu/reports/friedman/. </address>
Reference: <author> Friedman, J. H., Bentley, J. L., and Finkel, R. A. </author> <year> (1977). </year> <title> An algorithm for finding best matches in logarithmic expected time. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 3(3) </volume> <pages> 209-226. </pages>
Reference: <author> Fritzke, B. </author> <year> (1995). </year> <title> Incremental learning of local linear mappings. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks ICANN '95, </booktitle> <pages> pages 217-222, </pages> <address> Paris, France. </address>
Reference: <author> Fukunaga, K. </author> <year> (1990). </year> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <note> second edition. </note>
Reference: <author> Gasser, T. and Muller, H. G. </author> <year> (1979). </year> <title> Kernel estimation of regression functions. </title> <editor> In Gasser, T. and Rosenblatt, M., editors, </editor> <title> Smoothing Techniques for Curve Estimation, </title> <booktitle> number 757 in Lecture Notes in Mathematics, </booktitle> <pages> pages 23-67. </pages> <publisher> Springer-Verlag, </publisher> <address> Heidelberg. </address>
Reference: <author> Gasser, T. and Muller, H. G. </author> <year> (1984). </year> <title> Estimating regression functions and their derivatives by the kernel method. </title> <journal> Scandanavian Journal of Statistics, </journal> <volume> 11 </volume> <pages> 171-185. </pages>
Reference: <author> Gasser, T., Muller, H. G., and Mammitzsch, V. </author> <year> (1985). </year> <title> Kernels for nonparametric regression. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 47 </volume> <pages> 238-252. </pages>
Reference: <author> Ge, Z., Cavinato, A. G., and Callis, J. B. </author> <year> (1994). </year> <title> Noninvasive spectroscopy for monitoring cell density in a fermentation process. </title> <journal> Analytical Chemistry, </journal> <volume> 66 </volume> <pages> 1354-1362. </pages>
Reference: <author> Goldberg, K. Y. and Pearlmutter, B. </author> <year> (1988). </year> <title> Using a neural network to learn the dynamics of the CMU Direct-Drive Arm II. </title> <type> Technical Report CMU-CS-88-160, </type> <institution> Carnegie-Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: To illustrate the differences between global parametric representations and a locally-weighted learning approach, a sigmoidal feedforward neural network approach was compared to a locally weighted learning approach on the same problem. The architecture for the sigmoidal feedforward neural network was taken from <ref> (Goldberg and Pearlmutter, 1988, section 6) </ref> who modeled arm inverse dynamics. The ability of each of these methods to predict the torques of the simulated two joint arm at 1000 random points was compared (Atkeson, 1992). Figure 15 plots the normalized RMS prediction error.
Reference: <author> Gorinevsky, D. and Connolly, T. H. </author> <year> (1994). </year> <title> Comparison of some neural network and scattered data approximations: The inverse manipulator kinematics example. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 521-542. </pages>
Reference: <author> Goshtasby, A. </author> <year> (1988). </year> <title> Image registration by local approximation methods. </title> <journal> Image and Vision Computing, </journal> <volume> 6(4) </volume> <pages> 255-261. </pages>
Reference: <author> Grosse, E. </author> <year> (1989). </year> <title> LOESS: Multivariate smoothing by moving least squares. </title> <editor> In Chui, C. K., Schumaker, L. L., and Ward, J. D., editors, </editor> <booktitle> Approximation Theory VI, </booktitle> <pages> pages 1-4. </pages> <publisher> Academic Press, </publisher> <address> Boston, MA. </address>
Reference: <author> Hammond, S. V. </author> <year> (1991). </year> <title> Nir analysis of antibiotic fermentations. </title> <editor> In Murray, I. and Cowe, I. A., editors, </editor> <title> 46 Making Light Work: </title> <booktitle> Advances in Near Infrared Spectroscopy, </booktitle> <pages> pages 584-589. </pages> <publisher> VCH: </publisher> <address> New York, NY. </address> <booktitle> Developed from the 4th International Conference on Near Infrared Spectroscopy, </booktitle> <address> Aberdeen, Scotland, </address> <month> August 19-23, </month> <year> 1991. </year>
Reference: <author> Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., and Stahel, W. A. </author> <year> (1986). </year> <title> Robust Statistics: The Approach Based On Influence Functions. </title> <publisher> John Wiley, </publisher> <address> New York, NY. </address>
Reference: <author> Hardle, W. </author> <year> (1990). </year> <title> Applied Nonparametric Regression. </title> <publisher> Cambridge University Press, </publisher> <address> New York, NY. </address>
Reference: <author> Hastie, T. and Loader, C. </author> <year> (1993). </year> <title> Local regression: Automatic kernel carpentry. </title> <journal> Statistical Science, </journal> <volume> 8(2) </volume> <pages> 120-143. </pages>
Reference: <author> Hastie, T. J. and Tibshirani, R. J. </author> <year> (1990). </year> <title> Generalized Additive Regression. </title> <publisher> Chapman Hall, London. </publisher>
Reference: <author> Hastie, T. J. and Tibshirani, R. J. </author> <year> (1994). </year> <title> Discriminant adaptive nearest neighbor classification. </title> <publisher> ftp://playfair.Stanford.EDU/pub/hastie/dann.ps.Z. </publisher>
Reference: <author> Higuchi, T., Kitano, H., Furuya, T., Handa, K., Takahashi, N., and Kokubu, A. </author> <year> (1991). </year> <title> IXM2: A parallel associative processor for knowledge processing. </title> <booktitle> In AAAI-9 (1991), </booktitle> <pages> pages 296-303. </pages>
Reference-contexts: The critical feature of the massively parallel computer system IXM2 is the use of associative memories in addition to multiple processors <ref> (Higuchi et al., 1991) </ref>. There are 64 processors (Transputers) in the IXM2, but each processor has 4Kx40 bits of associative memory, which increases the effective number of processors to 256K.
Reference: <author> Hillis, D. </author> <year> (1985). </year> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: On a massively parallel computer, such as the CM1 and CM2 <ref> (Hillis, 1985) </ref>, exhaustive search is often faster than using k-d trees, due to the limited number of experiences allocated to each processor. The Connection Machine can have up to 2 16 (65536) processors, and can simulate a parallel computer with many more processors.
Reference: <author> Huang, P. S. </author> <year> (1996). </year> <title> Planning For Dynamic Motions Using A Search Tree. </title> <type> MS thesis, </type> <institution> University of Toronto, Graduate Department of Computer Science. </institution> <address> http://www.dgp.utoronto.ca/people/psh/home.html. IJCAI 12 (1991). </address> <booktitle> Twelfth International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <booktitle> IJCAI 13 (1993). Thirteenth International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Jabbour, K., Riveros, J. F. W., Landsbergen, D., and Meyer, W. </author> <year> (1987). </year> <title> ALFA: Automated load forecasting assistant. </title> <booktitle> In Proceedings of the 1987 IEEE Power Engineering Society Summer Meeting, </booktitle> <address> San Francisco, CA. </address>
Reference-contexts: They also describe using an initial prediction of the output in an augmented distance function to select training data with similar or equal outputs ("goal restriction") <ref> (Jabbour et al., 1987) </ref>. 11 4.2 Distance Functions For Continuous Inputs The functions discussed in this section are especially appropriate for ordered (vs. categorical, symbolic, or nominal) input values, which are either continuous or an ordered set of discrete values. * Unweighted Euclidean distance: d E (x; q) = j 2
Reference: <author> James, M. </author> <year> (1985). </year> <title> Classification Algorithms. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY. </address>
Reference: <author> Jones, M. C., Davies, S. J., and Park, B. U. </author> <year> (1994). </year> <title> Versions of kernel-type regression estimators. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 89(427) </volume> <pages> 825-832. </pages>
Reference: <author> Karalic, A. </author> <year> (1992). </year> <title> Employing linear regression in regression tree leaves. </title> <editor> In Neumann, B., editor, </editor> <booktitle> ECAI 92: 10th European Conference on Artificial Intelligence, </booktitle> <pages> pages 440-441, </pages> <address> Vienna, Austria. </address> <publisher> John Wiley and Sons. </publisher>
Reference: <author> Katkovnik, V. Y. </author> <year> (1979). </year> <title> Linear and nonlinear methods of nonparametric regression analysis. </title> <journal> Soviet Automatic Control, </journal> <volume> 5 </volume> <pages> 25-34. </pages>
Reference: <author> Kazmierczak, H. and Steinbuch, K. </author> <year> (1963). </year> <title> Adaptive systems in pattern recognition. </title> <journal> IEEE Transactions on Electronic Computers, EC-12:822-835. </journal>
Reference: <author> Kibler, D., Aha, D. W., and Albert, M. </author> <year> (1989). </year> <title> Instance-based prediction of real-valued attributes. </title> <journal> Computational Intelligence, </journal> <volume> 5 </volume> <pages> 51-57. </pages>
Reference: <author> Kitano, H. </author> <year> (1993a). </year> <title> Challenges of massive parallelism. </title> <booktitle> In IJCAI 13 (1993), </booktitle> <pages> pages 813-834. </pages>
Reference: <author> Kitano, H. </author> <year> (1993b). </year> <title> A comprehensive and practical model of memory-based machine translation. </title> <booktitle> In IJCAI 13 (1993), </booktitle> <pages> pages 1276-1282. </pages>
Reference: <author> Kitano, H. and Higuchi, T. </author> <year> (1991a). </year> <title> High performance memory-based translation on IXM2 massively parallel associative memory processor. </title> <booktitle> In AAAI-9 (1991), </booktitle> <pages> pages 149-154. </pages>
Reference: <author> Kitano, H. and Higuchi, T. </author> <year> (1991b). </year> <title> Massively parallel memory-based parsing. </title> <booktitle> In IJCAI 12 (1991), </booktitle> <pages> pages 918-924. </pages>
Reference: <author> Kitano, H., Moldovan, D., and Cha, S. </author> <year> (1991). </year> <title> High performance natural language processing on semantic network array processor. </title> <booktitle> In IJCAI 12 (1991), </booktitle> <pages> pages 911-917. </pages>
Reference-contexts: The current generic parallel computer seems to be on the order of 100 standard microprocessors tightly connected with a communication network. Examples of this design are the CM5 and the SNAP system <ref> (Kitano et al., 1991) </ref>. The details of the communication network are not critical to locally weighted learning, since the time critical processing 34 consists of broadcasting the query to the processors and determining which answer is the best, which can easily be done with a prespecified communication pattern.
Reference: <author> Kozek, A. S. </author> <year> (1992). </year> <title> A new nonparametric estimation method: Local and nonlinear. </title> <booktitle> Interface, </booktitle> <volume> 24 </volume> <pages> 389-393. </pages>
Reference: <author> Lancaster, P. </author> <year> (1979). </year> <title> Moving weighted least-squares methods. </title> <editor> In Sahney, B. N., editor, </editor> <title> Polynomial and Spline Approximation, </title> <address> pages 103-120. D. </address> <publisher> Reidel Publishing, </publisher> <address> Boston, MA. </address>
Reference: <author> Lancaster, P. and Salkauskas, K. </author> <year> (1981). </year> <title> Surfaces generated by moving least squares methods. </title> <journal> Mathematics of Computation, </journal> <volume> 37(155) </volume> <pages> 141-158. </pages>
Reference: <author> Lancaster, P. and Salkauskas, K. </author> <year> (1986). </year> <title> Curve And Surface Fitting. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Lawrence, S., Tsoi, A. C., and Black, A. D. </author> <year> (1996). </year> <title> Function approximation with neural networks and local methods: Bias, variance and smoothness. </title> <booktitle> In Australian Conference on Neural Networks, </booktitle> <address> Canberra, 47 Australia, Canberra, Australia. </address> <note> available from http://www.neci.nj.nec.com/homepages/lawrence and http://www.elec.uq.edu.au/~lawrence. </note>
Reference: <author> LeBaron, B. </author> <year> (1990). </year> <title> Forecast improvements using a volatility index. </title> <note> unpublished. </note>
Reference-contexts: of this is to use measures of volatility of the stock market to measure distance between data points and a query d (X i ; Q), but use price histories and other factors to form local (with respect to volatility) predictive models for future prices f (X i ; fi) <ref> (LeBaron, 1990, 1992) </ref>.
Reference: <author> LeBaron, B. </author> <year> (1992). </year> <title> Nonlinear forecasts for the S&P stock index. </title> <booktitle> In Casdagli and Eubank (1992), </booktitle> <pages> pages 381-393. </pages> <booktitle> Proceedings of a Workshop on Nonlinear Modeling and Forecasting September 17-21, 1990, </booktitle> <address> Santa Fe, New Mexico. </address>
Reference: <author> Legg, M. P. C. and Brent, R. P. </author> <year> (1969). </year> <title> Automatic contouring. </title> <booktitle> In 4th Australian Computer Conference, </booktitle> <pages> pages 467-468. </pages>
Reference: <author> Lejeune, M. </author> <year> (1984). </year> <title> Optimization in non-parametric regression. </title> <booktitle> In COMPSTAT 1984: Proceedings in Computational Statistics, </booktitle> <pages> pages 421-426, </pages> <address> Prague. </address> <publisher> Physica-Verlag Wien. </publisher>
Reference: <author> Lejeune, M. </author> <year> (1985). </year> <title> Estimation non-parametrique par noyaux: Regression polynomial mobile. </title> <journal> Revue de Statistique Appliquee, </journal> <volume> 23(3) </volume> <pages> 43-67. </pages>
Reference: <author> Lejeune, M. and Sarda, P. </author> <year> (1992). </year> <title> Smooth estimators of distribution and density functions. </title> <journal> Computational Statistics & Data Analysis, </journal> <volume> 14 </volume> <pages> 457-471. </pages>
Reference: <author> Li, K. C. </author> <year> (1984). </year> <title> Consistency for cross-validated nearest neighbor estimates in nonparametric regression. </title> <journal> The Annals of Statistics, </journal> <volume> 12 </volume> <pages> 230-240. </pages>
Reference: <author> Loader, C. </author> <year> (1994). </year> <title> Computing nonparametric function estimates. </title> <type> Technical Report 7, </type> <institution> AT&T Bell Laboratories, Statistics Department, </institution> <address> Murray Hill, NJ. </address> <note> Available by anonymous FTP from netlib.att.com in /netlib/att/stat/doc/94/7.ps. </note>
Reference: <author> Lodwick, G. D. and Whittle, J. </author> <year> (1970). </year> <title> A technique for automatic contouring field survey data. </title> <journal> Australian Computer Journal, </journal> <volume> 2 </volume> <pages> 104-109. </pages>
Reference: <author> Lowe, D. G. </author> <year> (1995). </year> <title> Similarity metric learning for a variable-kernel classifier. </title> <journal> Neural Computation, </journal> <volume> 7 </volume> <pages> 72-85. </pages>
Reference: <author> Maron, O. and Moore, A. W. </author> <year> (1996). </year> <title> A racing algorithm: Model selection for memory based learners. </title> <journal> Artificial Intelligence Review. </journal> <note> in press. </note>
Reference-contexts: These 29 techniques compare a wide range of different types of models simultaneously, and handle models with discrete parameters. Bad models are quickly dropped from the race, which focuses computational effort on distinguishing between the good models. Typically any continuous fit parameters are discretized <ref> (Maron and Moore, 1996) </ref>. Techniques for selecting features in the distance metric and local model have been developed in statistics (Draper and Smith, 1981; Miller, 1990), including all subsets, forward regression, backwards regression, and stepwise regression.
Reference: <author> Marron, J. S. </author> <year> (1988). </year> <title> Automatic smoothing parameter selection: A survey. </title> <booktitle> Empirical Economics, </booktitle> <volume> 13 </volume> <pages> 187-208. </pages>
Reference-contexts: There are several approaches to computing the fit parameter values: * Plug-in approach: The fit parameters can be set by a direct computation. * Optimization approaches: The fit parameters can be set by an optimization process that either <ref> (Marron, 1988) </ref>: minimizes the training set error, minimizes the test or validation set error, minimizes the cross validation error (CV), minimizes the generalized cross validation error (GCV) (Myers, 1990), maximizes Akaike's information criterion (AIC), or adjusts Mallow's C p . Fit parameters cannot be optimized in isolation.
Reference: <author> McCallum, R. A. </author> <year> (1995). </year> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title>
Reference: <editor> In Prieditis and Russell (1995), </editor> <address> pages 387-395. </address>
Reference: <author> McIntyre, D. B., Pollard, D. D., and Smith, R. </author> <year> (1968). </year> <title> Computer programs for automatic contouring. </title> <type> Technical Report Kansas Geological Survey Computer Contributions 23, </type> <institution> University of Kansas, Lawrence, KA. </institution>
Reference: <author> McLain, D. H. </author> <year> (1974). </year> <title> Drawing contours from arbitrary data points. </title> <journal> The Computer Journal, </journal> <volume> 17(4) </volume> <pages> 318-324. </pages>
Reference: <author> Medin, D. L. and Shoben, E. J. </author> <year> (1988). </year> <title> Context and structure in conceptual combination. </title> <journal> Cognitive Psychology, </journal> <volume> 20 </volume> <pages> 158-190. </pages>
Reference-contexts: Distance functions can be asymmetric and nonlinear, so that a distance along a particular dimension can depend on whether the query point's value for the dimension is larger or smaller than the stored point's value for that dimension <ref> (Medin and Shoben, 1988) </ref>. The distance along a dimension can also depend on the values being compared (Nosofsky et al., 1989). 4.1 Feature Scaling Altering the distance function can serve two purposes.
Reference: <author> Meese, R. and Wallace, N. </author> <year> (1991). </year> <title> Nonparametric estimation of dynamic hedonic price models and the construction of residential housing price indices. </title> <journal> American Real Estate and Urban Economics Association Journal, </journal> <volume> 19(3) </volume> <pages> 308-332. </pages>
Reference: <author> Meese, R. A. and Rose, A. K. </author> <year> (1990). </year> <title> Nonlinear, nonparametric, nonessential exchange rate estimation. </title> <journal> The American Economic Review, May:192-196. </journal>
Reference: <author> Miller, A. J. </author> <year> (1990). </year> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Miller, W. T., Glanz, F. H., and Kraft, L. G. </author> <year> (1987). </year> <title> Application of a general learning algorithm to the control of robotic manipulators. </title> <journal> International Journal of Robotics Research, </journal> <volume> 6 </volume> <pages> 84-98. </pages>
Reference: <author> Mohri, T. and Tanaka, H. </author> <year> (1994). </year> <title> An optimal weighting criterion of case indexing for both numeric and symbolic attributes. </title> <editor> In Aha, D. W., editor, </editor> <booktitle> AAAI-94 Workshop Program: Case-Based Reasoning, Working Notes, </booktitle> <pages> pages 123-127. </pages> <publisher> AAAI Press, </publisher> <address> Seattle, WA. </address>
Reference: <author> Moore, A. W. </author> <year> (1990a). </year> <title> Acquisition of Dynamic Control Knowledge for a Robotic Manipulator. </title> <booktitle> In Seventh International Machine Learning Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Moore, A. W. </author> <year> (1990b). </year> <title> Efficient Memory-based Learning for Robot Control. </title> <type> PhD. Thesis; Technical Report No. 209, </type> <institution> Computer Laboratory, University of Cambridge. </institution>
Reference: <author> Moore, A. W., Hill, D. J., and Johnson, M. P. </author> <year> (1992). </year> <title> An empirical investigation of brute force to choose features, smoothers, and function approximators. </title> <editor> In Hanson, S., Judd, S., and Petsche, T., editors, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, </booktitle> <volume> volume 3. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, 48 MA. </address>
Reference-contexts: This robustness has lead to the use of global cross-validation in applications that attempt to achieve high autonomy by making few assumptions, such as the General Memory Based Learning (GMBL) system described in <ref> (Moore et al., 1992) </ref>. GMBL performs large amounts of cross validation search to optimize feature subsets, the diagonal elements of the distance metric, the smoothing parameter, and the order of the regression. 12.1.1 Continuous Search Continuous fit parameters make continuous search possible.
Reference: <author> Moore, A. W. and Schneider, J. </author> <year> (1995). </year> <title> Memory-based stochastic optimization. </title> <note> To appear in the proceedings of NIPS-95, Also available as Technical Report CMU-RI-TR-95-30, ftp://ftp.cs.cmu.edu/afs/cs.cmu.edu/project/reinforcement/papers/memstoch.ps. </note>
Reference-contexts: How should these different tuning processes interact? * Stochastic gradient approaches to continuous tuning: Continuous optimization based on estimates of the gradient using small numbers of random queries rather than exhaustive query sets seems a promising approach to efficient tuning algorithms <ref> (Moore and Schneider, 1995) </ref>. 41 * Properties of massive cross-validation: We have discussed the use of cross--validation, and why locally weighted learning is particularly well suited to its use.
Reference: <author> More, J. J., Garbow, B. S., and Hillstrom, K. E. </author> <year> (1980). </year> <title> User guide for MINPACK-1. </title> <type> Technical Report ANL-80-74, </type> <institution> Argonne National Laboratory, Argonne, Illinois. </institution>
Reference-contexts: Inevitably this is local hill climbing, with a large risk of getting stuck in local optima. The sum of the squared cross validation errors is minimized using a nonlinear parameter estimation procedure (e.g., MINPACK <ref> (More et al., 1980) </ref> or NL2SOL (Dennis et al., 1981)). As discussed in Section 9.3, in this locally weighted learning approach computing the cross validation error for a single point is no more computationally expensive than answering a query.
Reference: <author> Muller, H.-G. </author> <year> (1987). </year> <title> Weighted local regression and kernel methods for nonparametric curve fitting. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82 </volume> <pages> 231-238. </pages>
Reference: <author> Muller, H.-G. </author> <year> (1993). </year> <title> Comment on [Hastie and Loader, 1993]. </title> <journal> Statistical Science, </journal> <volume> 8(2) </volume> <pages> 134-139. </pages>
Reference: <author> Murphy, O. J. and Selkow, S. M. </author> <year> (1986). </year> <title> The efficiency of using k-d trees for finding nearest neighbors in discrete space. </title> <journal> Information Processing Letters, </journal> <volume> 23 </volume> <pages> 215-218. </pages>
Reference: <author> Myers, R. H. </author> <year> (1990). </year> <title> Classical and Modern Regression With Applications. </title> <address> PWS-KENT, Boston, MA. </address>
Reference-contexts: We derive least squares training algorithms for linear local models from regression procedures for linear global models. 3.2.1 Linear Global Models A global model that is linear in the parameters fi can be expressed as <ref> (Myers, 1990) </ref>: x T In what follows we will assume that the constant 1 has been appended to all the input vectors x i to include a constant term in the regression. <p> for the local value of the noise variance is ^ 2 (q) = r 2 n LWR (q) C (q) (49) where n LWR is a modified measure of how many data points there are: n LWR (q) = i=1 i = i=1 h (50) In analogy to unweighted regression <ref> (Myers, 1990) </ref>, we can reduce the bias of the estimate ^ 2 (q) by taking into account the number of parameters in the locally weighted regression: ^ 2 (q) = r 2 n LWR (q) p LWR (q) where p LWR is a measure of the local number of free parameters <p> Cross validation can also be performed locally, i.e, from just fitting a locally linear model at one query point q (Cleveland and Loader, 1994c). We first consider the locally weighted average of the squared cross validation error MSE cv at each training point <ref> (Myers, 1990) </ref>: MSE cv (q) = (e cv P (58) This estimate requires a locally weighted regression to be performed at each training point with non-zero weight K (d (x i ; q)). <p> The weighted cross validation residual r cv i;q is related to the weighted residual (r i = w i e i ) by <ref> (Myers, 1990) </ref>: r cv r i i (Z T Z + fl) 1 z i Thus, we obtain the final equation for MSE cv as: MSE cv (q) = n LWR i r i i (Z T Z + fl) 1 z i (61) This equation is a local version of <p> r i i (Z T Z + fl) 1 z i Thus, we obtain the final equation for MSE cv as: MSE cv (q) = n LWR i r i i (Z T Z + fl) 1 z i (61) This equation is a local version of the PRESS statistic <ref> (Myers, 1990) </ref>. It allows us to perform leave-one-out cross validation without recalculating the regression parameters for every excluded point. <p> Points can be outliers for some queries and not outliers for others. We can generate weights for training data at query time based on cross validation using nearby points. The PRESS statistic <ref> (Myers, 1990) </ref> can be modified to serve as a local outlier detector in locally weighted regression. <p> parameters can be set by a direct computation. * Optimization approaches: The fit parameters can be set by an optimization process that either (Marron, 1988): minimizes the training set error, minimizes the test or validation set error, minimizes the cross validation error (CV), minimizes the generalized cross validation error (GCV) <ref> (Myers, 1990) </ref>, maximizes Akaike's information criterion (AIC), or adjusts Mallow's C p . Fit parameters cannot be optimized in isolation. The combination of all fit parameters generates a particular fit quality. If one fit parameter is changed, typically the optimal values of other parameters change in response.
Reference: <author> Nadaraya, E. A. </author> <year> (1964). </year> <title> On estimating regression. </title> <journal> Theory of Probability and Its Applications, </journal> <volume> 9 </volume> <pages> 141-142. </pages>
Reference: <author> Ns, T. and Isaksson, T. </author> <year> (1992). </year> <title> Locally weighted regression in diffuse near-infrared transmittance spectroscopy. </title> <journal> Applied Spectroscopy, </journal> <volume> 46(1) </volume> <pages> 34-43. </pages>
Reference: <author> Ns, T., Isaksson, T., and Kowalski, B. R. </author> <year> (1990). </year> <title> Locally weighted regression and scatter correction for near-infrared reflectance data. </title> <journal> Analytical Chemistry, </journal> <volume> 62(7) </volume> <pages> 664-673. </pages>
Reference: <author> Nguyen, T., Czerwinsksi, M., and Lee, D. </author> <year> (1993). </year> <title> COMPAQ Quicksource: Providing the consumer with the power of artificial intelligence. </title> <booktitle> In Proceedings of the Fifth Annual Conference on Innovative Applications of Artificial Intelligence, </booktitle> <pages> pages 142-150, </pages> <address> Washington, DC. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Nosofsky, R. M., Clark, S. E., and Shin, H. J. </author> <year> (1989). </year> <title> Rules and exemplars in categorization, identification, and recognition. Journal of Experimental Psychology: Learning, Memory, </title> <journal> and Cognition, </journal> <volume> 15 </volume> <pages> 282-304. </pages>
Reference-contexts: The distance along a dimension can also depend on the values being compared <ref> (Nosofsky et al., 1989) </ref>. 4.1 Feature Scaling Altering the distance function can serve two purposes. If the feature scaling factors m j are all nonzero, the input space is warped or distorted, which might lead to more accurate predictions.
Reference: <author> Omohundro, S. M. </author> <year> (1987). </year> <title> Efficient Algorithms with Neural Network Behaviour. </title> <journal> Journal of Complex Systems, </journal> <volume> 1(2) </volume> <pages> 273-347. </pages>
Reference: <author> Omohundro, S. M. </author> <year> (1991). </year> <title> Bumptrees for Efficient Function, Constraint, and Classification Learning. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Second, there are approximate algorithms that can find one or more nearby experiences, without guaranteeing they are the nearest, that do operate in logarithmic time. Empirically, these approximations do not greatly reduce prediction accuracy (Omohundro, 1987; Moore, 1990b). Bump trees <ref> (Omohundro, 1991) </ref> are another promising efficient approximation.
Reference: <author> Palmer, J. A. B. </author> <year> (1969). </year> <title> Automatic mapping. </title> <booktitle> In 4th Australian Computer Conference, </booktitle> <pages> pages 463-466. </pages>
Reference: <author> Pelto, C. R., Elkins, T. A., and Boyd, H. A. </author> <year> (1968). </year> <title> Automatic contouring of irregularly spaced data. </title> <journal> Geophysics, </journal> <volume> 33 </volume> <pages> 424-430. </pages>
Reference: <author> Peng, J. </author> <year> (1995). </year> <title> Efficient memory-based dynamic programming. </title> <booktitle> In Prieditis and Russell (1995), </booktitle> <pages> pages 438-446. </pages>
Reference: <author> Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. </author> <year> (1988). </year> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> New York, NY. </address>
Reference-contexts: y i ) 2 (12) by solving the normal equations for fi: 1 Inverting the matrix X T X is not the numerically best way to solve the normal equations from the point of view of efficiency or accuracy, and usually other matrix techniques are used to solve Equation 13 <ref> (Press et al., 1988) </ref>. 3.2.2 Weighting the Criterion: A Physical Interpretation In fitting a line or plane to a set of points, unweighted regression gives distant points equal influence with nearby points on the ultimate answer to the query, for equally spaced data.
Reference: <author> Prieditis, A. and Russell, S., </author> <title> editors (1995). </title> <booktitle> Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Rachlin, J., Kasif, S., Salzberg, S., and Aha, D. W. </author> <year> (1994). </year> <title> Towards a better understanding of memory-based reasoning systems. </title> <editor> In Cohen, W. W. and Hirsh, H., editors, </editor> <booktitle> Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 242-250. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <note> now updated and submitted as an Artificial Intelligence Journal technical note: </note> <author> Kasif, S., Salzberg, S., Waltz, D., Rachlin, J., & Aha, D. W. </author> <year> (1995). </year> <title> "Towards a framework for memory-based reasoning". </title>
Reference: <author> Racine, J. </author> <year> (1993). </year> <title> An efficient cross-validation algorithm for window width selection for nonparametric kernel regression. </title> <journal> Communications in Statistics: Simulation and Computation, </journal> <volume> 22(4) </volume> <pages> 1107-1114. </pages>
Reference: <author> Ramasubramanian, V. and Paliwal, K. K. </author> <year> (1989). </year> <title> A generalized optimization of the k-d tree for fast nearest-neighbour search. </title> <booktitle> In International Conference on Acoustics, Speech, and Signal Processing. </booktitle>
Reference: <author> Raz, J., Turetsky, B. I., and Fein, G. </author> <year> (1989). </year> <title> Selecting the smoothing parameter for estimation of smoothly changing evoked potential signals. </title> <journal> Biometrics, </journal> <volume> 45 </volume> <pages> 851-871. </pages>
Reference: <author> Renka, R. J. </author> <year> (1988). </year> <title> Multivariate interpolation of large sets of scattered data. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 14(2) </volume> <pages> 139-152. </pages>
Reference: <author> Ruppert, D. and Wand, M. P. </author> <year> (1994). </year> <title> Multivariate locally weighted least squares regression. </title> <journal> The Annals of Statistics, </journal> <volume> 22(3) </volume> <pages> 1346-1370. </pages>
Reference: <author> Ruprecht, D. and Muller, H. </author> <year> (1992). </year> <title> Image warping with scattered data interpolation methods. </title> <type> Technical Report 443, </type> <institution> Universitat Dortmund, Fachbereich Informatik, D-44221 Dortmund, Germany. </institution> <note> Available for anonymous FTP from ftp-ls7.informatik.uni-dortmund.de in pub/reports/ls7/rr-443.ps.Z. 49 Ruprecht, </note> <author> D. and Muller, H. </author> <year> (1993). </year> <title> Free form deformation with scattered data interpolation methods. </title> <editor> In Farin, G., Hagen, H., and Noltemeier, H., editors, </editor> <booktitle> Geometric Modelling (Computing Suppl. </booktitle> <volume> 8), </volume> <pages> pages 267-281. </pages> <address> Springer Verlag. </address> <note> Available for anonymous FTP from ftp-ls7.informatik.uni-dortmund.de in pub/reports/iif/rr-41.ps.Z. </note>
Reference: <author> Ruprecht, D. and Muller, H. </author> <year> (1994a). </year> <title> Deformed cross-dissolves for image interpolation in scientific visualization. </title> <journal> The Journal of Visualization and Computer Animation, </journal> <volume> 5(3) </volume> <pages> 167-181. </pages> <note> Available for anonymous FTP from ftp-ls7.informatik.uni-dortmund.de in pub/reports/ls7/rr-491.ps.Z. </note>
Reference: <author> Ruprecht, D. and Muller, H. </author> <year> (1994b). </year> <title> A framework for generalized scattered data interpolation. </title> <type> Technical Report 517, </type> <institution> Universitat Dortmund, Fachbereich Informatik, D-44221 Dortmund, Germany. </institution> <note> Available for anonymous FTP from ftp-ls7.informatik.uni-dortmund.de in pub/reports/ls7/rr-517.ps.Z. </note>
Reference: <author> Ruprecht, D., Nagel, R., and Muller, H. </author> <year> (1994). </year> <title> Spatial free form deformation with scattered data interpolation methods. </title> <type> Technical Report 539, </type> <institution> Fachbereich Informatik der Universitat Dortmund, </institution> <note> 44221 Dort-mund, Germany. Accepted for publication by Computers & Graphics, Available for anonymous FTP from ftp-ls7.informatik.uni-dortmund.de in pub/reports/ls7/rr-539.ps.Z. </note>
Reference: <author> Rust, R. T. and Bornman, E. O. </author> <year> (1982). </year> <title> Distribution-free methods of approximating nonlinear marketing relationships. </title> <journal> Journal of Marketing Research, XIX:372-374. </journal>
Reference: <author> Sabin, M. A. </author> <year> (1980). </year> <title> Contouring a review of methods for scattered data. </title> <editor> In Brodlie, K., editor, </editor> <booktitle> Mathematical Methods in Computer Graphics and Design, </booktitle> <pages> pages 63-86. </pages> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <editor> Saitta, L., editor (1996). </editor> <booktitle> Thirteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Samet, H. </author> <year> (1990). </year> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference: <author> Schaal, S. and Atkeson, C. G. </author> <year> (1994). </year> <title> Assessing the quality of learned local models. </title> <editor> In Cowan et al. </editor> <year> (1994), </year> <pages> pages 160-167. </pages>
Reference: <author> Schaal, S. and Atkeson, C. G. </author> <year> (1995). </year> <title> From isolation to cooperation: An alternative view of a system of experts. </title> <booktitle> NIPS95 proceedings, </booktitle> <publisher> in press. </publisher>
Reference: <author> Scott, D. W. </author> <year> (1992). </year> <title> Multivariate Density Estimation. </title> <publisher> Wiley, </publisher> <address> New York, NY. </address>
Reference-contexts: We use the term scaling for this purpose having reserved the term weight for the contribution of individual points (not dimensions) in a regression. We refer to the scaling factors as m j in this paper. There are many ways to define and use distance functions <ref> (Scott, 1992) </ref>: * Global distance functions.
Reference: <author> Seber, G. A. F. </author> <year> (1977). </year> <title> Linear Regression Analysis. </title> <publisher> John Wiley, </publisher> <address> New York, NY. </address>
Reference-contexts: Another view of ridge regression parameters is that they are the Bayesian assumptions about the apriori distributions of the estimated parameters <ref> (Seber, 1977) </ref>. As described in Section 12 on tuning, optimizing the ridge regression parameters using cross validation can identify irrelevant dimensions. These techniques also help combat overfitting. 8.2 Dimensionality Reduction Principal components analysis (PCA) can also be used globally to eliminate directions in which there is no data (Wettschereck, 1994).
Reference: <author> Seifert, B., Brockmann, M., Engel, J., and Gasser, T. </author> <year> (1994). </year> <title> Fast algorithms for nonparametric curve estimation. </title> <journal> Journal of Computational and Graphical Statistics, </journal> <volume> 3(2) </volume> <pages> 192-213. </pages>
Reference: <author> Seifert, B. and Gasser, T. </author> <year> (1994). </year> <title> Variance properties of local polynomials. </title> <address> http://www.unizh.ch/biostat/manuscripts.html. </address>
Reference: <author> Shepard, D. </author> <year> (1968). </year> <title> A two-dimensional function for irregularly spaced data. </title> <booktitle> In 23rd ACM National Conference, </booktitle> <pages> pages 517-524. </pages>
Reference-contexts: Nadaraya (1964) and Watson (1964) proposed using a weighted average of a set of nearest neighbors for regression. The approach was also independently reinvented in computer graphics <ref> (Shepard, 1968) </ref>. Specht (1991) describes a memory-based neural network approach based on a probabilistic model that motivates using weighted averaging 5 as the local model for regression.
Reference: <author> Solow, A. R. </author> <year> (1988). </year> <title> Detecting changes through time in the variance of a long-term hemispheric temperature record: An application of robust locally weighted regression. </title> <journal> Journal of Climate, </journal> <volume> 1 </volume> <pages> 290-296. </pages>
Reference: <author> Specht, D. E. </author> <year> (1991). </year> <title> A general regression neural network. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(6) </volume> <pages> 568-576. </pages>
Reference: <author> Sproull, R. F. </author> <year> (1991). </year> <title> Refinements to nearest-neighbor searching in k-d trees. </title> <journal> Algorithmica, </journal> <volume> 6 </volume> <pages> 579-589. </pages>
Reference: <author> Stanfill, C. </author> <year> (1987). </year> <title> Memory-based reasoning applied to English pronunciation. </title> <booktitle> In Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 577-581. </pages>
Reference: <author> Stanfill, C. and Waltz, D. </author> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228. </pages>
Reference: <author> Steinbuch, K. </author> <year> (1961). </year> <title> Die lernmatrix. </title> <journal> Kybernetik, </journal> <volume> 1 </volume> <pages> 36-45. </pages>
Reference: <author> Steinbuch, K. and Piske, U. A. W. </author> <year> (1963). </year> <title> Learning matrices and their applications. </title> <journal> IEEE Transactions on Electronic Computers, EC-12:846-862. </journal>
Reference: <author> Stone, C. J. </author> <year> (1975). </year> <title> Nearest neighbor estimators of a nonlinear regression function. </title> <booktitle> In Computer Science and Statistics: 8th Annual Symposium on the Interface, </booktitle> <pages> pages 413-418. </pages>
Reference: <author> Stone, C. J. </author> <year> (1977). </year> <title> Consistent nonparametric regression. </title> <journal> The Annals of Statistics, </journal> <volume> 5 </volume> <pages> 595-645. </pages>
Reference: <author> Stone, C. J. </author> <year> (1980). </year> <title> Optimal rates of convergence for nonparametric estimators. </title> <journal> The Annals of Statistics, </journal> <volume> 8 </volume> <pages> 1348-1360. </pages>
Reference: <author> Stone, C. J. </author> <year> (1982). </year> <title> Optimal global rates of convergence for nonparametric regression. </title> <journal> The Annals of Statistics, </journal> <volume> 10(4) </volume> <pages> 1040-1053. </pages>
Reference: <author> Sumita, E., Oi, K., Furuse, O., Iida, H., Higuchi, T., Takahashi, N., and Kitano, H. </author> <year> (1993). </year> <title> Example 50 based machine translation on massively parallel processors. </title> <booktitle> In IJCAI 13 (1993), </booktitle> <pages> pages 1283-1288. </pages>
Reference: <author> Tadepalli, P. and Ok, D. </author> <year> (1996). </year> <title> Scaling up average reward reinforcement learning by approximating the domain models and the value function. </title> <note> In Saitta (1996). http://www.cs.orst.edu:80/~tadepall/research/publications.html. </note>
Reference: <author> Tamada, T., Maruyama, M., Nakamura, Y., Abe, S., and Maeda, K. </author> <year> (1993). </year> <title> Water demand forecasting by memory based learning. </title> <booktitle> Water Science and Technology, </booktitle> <address> 28(11-12):133-140. </address>
Reference: <author> Taylor, W. K. </author> <year> (1959). </year> <title> Pattern recognition by means of automatic analogue apparatus. </title> <booktitle> Proceedings of The Institution of Electrical Engineers, </booktitle> <address> 106B:198-209. </address>
Reference: <author> Taylor, W. K. </author> <year> (1960). </year> <title> A parallel analogue reading machine. </title> <journal> Control, </journal> <volume> 3 </volume> <pages> 95-99. </pages>
Reference: <author> Thorpe, S. </author> <year> (1995). </year> <title> Localized versus distributed representations. </title> <editor> In Arbib, M. A., editor, </editor> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 549-552. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In a local representation, each new data point affects a small subset of the parameters and answering a query involves a small subset of the parameters as well. This view of local learning stems from the distinction between local and distributed representations in neuroscience <ref> (Thorpe, 1995) </ref>. Examples of local representations are lookup tables and exemplar/prototype based classifiers. It is not necessarily the case that the number of parameters in the representation be on the order of the number of data points (i.e., a considerable amount of local averaging can occur).
Reference: <author> Thrun, S. </author> <year> (1996). </year> <title> Is learning the n-th thing any easier than learning the first? In Advances in Neural Information Processing Systems (NIPS) 8. </title> <address> http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/publications.html. </address>
Reference: <author> Thrun, S. and O'Sullivan, J. </author> <year> (1996). </year> <title> Discovering structure in multiple learning tasks: The TC algorithm. </title>
Reference: <editor> In Saitta (1996). </editor> <address> http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/publications.html. </address>
Reference: <author> Tibshirani, R. and Hastie, T. </author> <year> (1987). </year> <title> Local likelihood estimation. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82 </volume> <pages> 559-567. </pages>
Reference: <author> Ting, K. M. and Cameron-Jones, R. M. </author> <year> (1994). </year> <title> Exploring a framework for instance based learning and naive Bayesian classifiers. </title> <booktitle> In Proceedings of the Seventh Australian Joint Conference on Artificial Intelligence, </booktitle> <address> Armidale, Australia. </address> <publisher> World Scientific. </publisher>
Reference: <author> Tou, J. T. and Gonzalez, R. C. </author> <year> (1974). </year> <title> Pattern Recognition Principles. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference: <author> Townshend, B. </author> <year> (1992). </year> <title> Nonlinear prediction of speech signals. </title> <booktitle> In Casdagli and Eubank (1992), </booktitle> <pages> pages 433-453. </pages> <booktitle> Proceedings of a Workshop on Nonlinear Modeling and Forecasting September 17-21, 1990, </booktitle> <address> Santa Fe, New Mexico. </address>
Reference: <author> Tsybakov, A. B. </author> <year> (1986). </year> <title> Robust reconstruction of functions by the local approximation method. </title> <journal> Problems of Information Transmission, </journal> <volume> 22 </volume> <pages> 133-146. </pages>
Reference: <author> Tukey, J. </author> <year> (1977). </year> <title> Exploratory Data Analysis. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference: <author> Turetsky, B. I., Raz, J., and Fein, G. </author> <year> (1989). </year> <title> Estimation of trial-to-trial variation in evoked potential signals by smoothing across trials. </title> <journal> Psychophysiology, </journal> <volume> 26(6) </volume> <pages> 700-712. </pages>
Reference: <author> Turlach, B. A. and Wand, M. P. </author> <year> (1995). </year> <title> Fast computation of auxiliary quantities in local polynomial regression. </title> <editor> http://netec.wustl.edu/~adnetec/WoPEc/agsmst/agsmst95009.html. van der Smagt, P., Groen, F., and van het Groenewoud, F. </editor> <year> (1994). </year> <title> The locally linear nested network for robot manipulation. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pages 2787-2792. </pages> <month> ftp://ftp.fwi.uva.nl/pub/computer-systems/aut-sys/reports/SmaGroGro94b.ps.gz. </month>
Reference: <author> Vapnik, V. </author> <year> (1992). </year> <title> Principles of risk minimization for learning theory. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippmann, R. P., editors, </editor> <booktitle> Advances In Neural Information Processing Systems 4, </booktitle> <pages> pages 831-838. </pages> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: shape changes. * Global bandwidth selection: h is set globally by an optimization process that typically minimizes cross validation error over all the data. * Query-based local bandwidth selection: h is set on each query by an optimization process that typically minimizes cross validation error or a related crite rion <ref> (Vapnik, 1992) </ref>. * Point-based local bandwidth selection: Each stored data point has associated with it a bandwidth h. <p> of freedom in the training process, leading to increased variance of the predictions and an increased risk of overfitting the data (Cleveland and Loader, 1994c): * Adaptation to the data density and distribution: This adaptation is in addition to the adaptation provided by the locally weighted regression procedure it self <ref> (Bottou and Vapnik, 1992) </ref>. 30 * Adaptation to variations in the noise level in the training data. These variations are known as heteroscedasticity. * Adaptation to variations in the behavior of the underlying function.
Reference: <author> Vapnik, V. and Bottou, L. </author> <year> (1993). </year> <title> Local algorithms for pattern recognition and dependencies estimation. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 893-909. </pages>
Reference: <author> Walden, A. T. and Prescott, P. </author> <year> (1983). </year> <title> Identification of trends in annual maximum sea levels using robust locally weighted regression. </title> <journal> Estuarine, Coastal and Shelf Science, </journal> <volume> 16 </volume> <pages> 17-26. </pages>
Reference: <author> Walters, R. F. </author> <year> (1969). </year> <title> Contouring by machine: A user's guide. </title> <journal> American Association of Petroleum Geologists Bulletin, </journal> <volume> 53(11) </volume> <pages> 2324-2340. </pages>
Reference: <author> Waltz, D. L. </author> <year> (1987). </year> <title> Applications of the Connection Machine. </title> <journal> Computer, </journal> <volume> 20(1) </volume> <pages> 85-97. </pages>
Reference-contexts: The authors advocate simply ignoring processor failures, although it would be possible to map the faulty processors and skip them when loading data. 14.1.3 Massively Parallel Implementations Many nearest neighbor systems have been implemented on massively parallel Connection Machines <ref> (Waltz, 1987) </ref>. On a massively parallel computer, such as the CM1 and CM2 (Hillis, 1985), exhaustive search is often faster than using k-d trees, due to the limited number of experiences allocated to each processor.
Reference: <author> Wand, M. P. and Jones, M. C. </author> <year> (1993). </year> <title> Comparison of smoothing parameterizations in bivariate kernel density estimation. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 88 </volume> <pages> 520-528. </pages>
Reference: <author> Wand, M. P. and Jones, M. C. </author> <year> (1994). </year> <title> Kernel Smoothing. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Wand, M. P. and Schucany, W. R. </author> <year> (1990). </year> <title> Gaussian-based kernels for curve estimation and window width selection. </title> <journal> Canadian Journal of Statistics, </journal> <volume> 18 </volume> <pages> 197-204. </pages>
Reference: <author> Wang, Z., Isaksson, T., and Kowalski, B. R. </author> <year> (1994). </year> <title> New approach for distance measurement in locally weighted regression. </title> <journal> Analytical Chemistry, </journal> <volume> 66(2) </volume> <pages> 249-260. </pages>
Reference: <author> Watson, G. S. </author> <year> (1964). </year> <title> Smooth regression analysis. </title> <journal> Sankhya: The Indian Journal of Statistics, Series A, </journal> <volume> 26 </volume> <pages> 359-372. </pages>
Reference: <author> Weisberg, S. </author> <year> (1985). </year> <title> Applied Linear Regression. </title> <publisher> John Wiley and Sons. </publisher>
Reference: <author> Wess, S., Althoff, K.-D., and Derwand, G. </author> <year> (1994). </year> <title> Using k-d trees to improve the retrieval step in case-based reasoning. </title> <editor> In Wess, S., Althoff, K.-D., and Richter, M. M., editors, </editor> <booktitle> Topics in Case-Based Reasoning, </booktitle> <pages> pages 167-181. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, NY. </address> <booktitle> Proceedings of the First European Workshop, </booktitle> <address> EWCBR-93. </address>
Reference: <author> Wettschereck, D. </author> <year> (1994). </year> <title> A Study Of Distance-Based Machine Learning Algorithms. </title> <type> PhD dissertation, </type> <institution> Oregon State University, Department of Computer Science, </institution> <address> Corvalis, OR. </address>
Reference-contexts: As described in Section 12 on tuning, optimizing the ridge regression parameters using cross validation can identify irrelevant dimensions. These techniques also help combat overfitting. 8.2 Dimensionality Reduction Principal components analysis (PCA) can also be used globally to eliminate directions in which there is no data <ref> (Wettschereck, 1994) </ref>. However, it is rarely the case that there is 18 scattered on a 1-dimensional non linear manifold. absolutely no data in a particular direction. A closely related technique, the singular value decomposition (SVD), is typically used in locally weighted regression to perform dimensionality reduction.
Reference: <author> Wijnberg, L. and Johnson, T. </author> <year> (1985). </year> <title> Estimation of missing values in lead air quality data sets. </title> <editor> In Johnson, T. R. and Penkala, S. J., editors, </editor> <title> Quality Assurance in Air Pollution Measurements. Air Pollution Control Association, </title> <address> Pittsburgh, PA. TR-3: </address> <note> Transactions: An APCA International Specialty Conference. </note>
Reference: <author> Wolberg, G. </author> <year> (1990). </year> <title> Digital Image Warping. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference-contexts: If the data is noisy, exact interpolation is not desirable, and a weighting scheme with limited magnitude is desired. The inverse distance <ref> (Wolberg, 1990) </ref> K (d) = 1 + d p (31) can be used to approximate functions like Equation 30 and the quadratic hyperbola kernel 1=(h 2 + d 2 ) with a well defined value at d = 0.
Reference: <author> Yasunaga, M. and Kitano, H. </author> <year> (1993). </year> <title> Robustness of the memory-based reasoning implemented by wafer scale integration. </title> <journal> IEICE Transactions on Information and Systems, E76-D(3):336-344. </journal>
Reference: <author> Zografski, Z. </author> <year> (1989). </year> <title> Neuromorphic, Algorithmic, and Logical Models for the Automatic Synthesis of Robot Action. </title> <type> PhD dissertation, </type> <institution> University of Ljubljana, Ljubljana, Slovenia, </institution> <address> Yugoslavia. </address>
Reference: <author> Zografski, Z. </author> <year> (1991). </year> <title> New methods of machine learning for the construction of integrated neuromorphic and associative-memory knowledge bases. </title> <editor> In Zajc, B. and Solina, F., editors, </editor> <booktitle> Proceedings, 6th Mediter-ranean Electrotechnical Conference, </booktitle> <volume> volume II, </volume> <pages> pages 1150-1153, </pages> <institution> Ljubljana, Slovenia, Yugoslavia. IEEE catalog number 91CH2964-5. </institution>
Reference: <author> Zografski, Z. </author> <year> (1992). </year> <title> Geometric and neuromorphic learning for nonlinear modeling, control and forecasting. </title> <booktitle> In Proceedings of the 1992 IEEE International Symposium on Intelligent Control, </booktitle> <pages> pages 158-163, </pages> <address> Glasgow, Scotland. </address> <publisher> IEEE catalog number 92CH3110-4. </publisher>
Reference: <author> Zografski, Z. and Durrani, T. </author> <year> (1995). </year> <title> Comparing predictions from neural networks and memory-based learning. </title> <booktitle> In Proceedings, ICANN '95/NEURONIMES '95: International Conference on Artificial Neural Networks, </booktitle> <pages> pages 221-226, </pages> <address> Paris, France. </address> <month> 52 </month>
References-found: 228

