URL: http://www.cs.cmu.edu/afs/cs/project/amulet/www/papers/benchmarks.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/bam/www/uicourse/1997spring/bench/index.html
Root-URL: 
Email: bam@cs.cmu.edu  
Title: Using Benchmarks to Teach and Evaluate User Interface Tools  
Author: Brad A. Myers, Neal Altman, Khalil Amiri, Matthew Centurion, Fay Chang, Chienhao Chen, Herb Derby, John Huebner, Rich Kaylor, Ralph Melton, Robert O'Callahan, Matthew Tarpy, Konur Unyelioglu, Zhenyu Wang, and Randon Warner 
Keyword: Benchmarks, Teaching User Interfaces, Evaluation, HCI Education, Toolkits, User Interface Development Environments.  
Web: http://www.cs.cmu.edu/~bam  
Address: Pittsburgh, PA 15213  
Affiliation: Human Computer Interaction Institute Carnegie Mellon University  
Abstract: As part of the User Interface Software course in the Human-Computer Interaction Institute at Carnegie Mellon University, the students and instructor developed a set of 7 benchmark tasks. These benchmarks are designed to be representative of a wide range of user interface styles, and have been implemented in about 20 different toolkits on different platforms in different programming languages.. The students written reports suggest that by implementing the benchmarks, they are learning the strengths and weaknesses of the various systems. Implementations of the benchmarks by a number of more experienced toolkit users and developers suggest that the benchmarks also do a good job of identifying the effectiveness of the toolkits for different kinds of user interfaces, and point out deficiencies in the toolkits. Thus, benchmarks seem to be very useful both for teaching and for evaluating user interface development environments. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Curtis, B., </author> <title> Fifteen Years of Psychology in Software Engineering: Individual Differences and Cognitive Science, </title> <booktitle> in Proceedings of the 7th International Conference on Software Engineering, 1984, </booktitle> <publisher> IEEE Computer Society Press. </publisher> <pages> pp. 97-106. </pages>
Reference-contexts: There have been many studies of programmers (e.g., <ref> [1] </ref>), which consistently show an order of magnitude difference in capabilities, which is why the numbers in the tables might not be reliable.
Reference: 2. <author> Flatt, M., MrEd, </author> <year> 1997. </year> <institution> Department of Computer Science - MS 132, Rice University, </institution> <address> 6100 Main Street, Houston, TX 77005-1892, (713)527-8101. mflatt@cs.rice.edu, http://www.cs.rice.edu/~mflatt/mred.html. </address>
Reference-contexts: Experienced toolkit users have implemented at least one of the benchmarks in Garnet [8], CLIM [6], Apples MacApp, GINA++ and GINA/CLM [11], LispView from Sun, tcl/tk, XPCE/SWI-Prolog [12], MrEd <ref> [2] </ref>, and Amulet [9]. The results suggest that the benchmarks are useful for toolkit designers and evaluators, and are helpful when evaluating and creating toolkits. For example, a few developers commented that the benchmarks helped highlight areas where their toolkits were missing important features for certain styles of applic ations. <p> The developer said that trying the Paint benchmark with the tool was very useful since it pointed to a weakness in the image change-forwarding code that was easily fixed, making a wider range of applications feas ible. MrEd MrEd <ref> [2] </ref> is a cross-platform toolkit, and version 49/14 was used by the designer of the toolkit on Linux on a PC to implement the Cards and Text Editing benchmarks.
Reference: 3. <editor> Hewett, T.T., et al., eds. </editor> <booktitle> ACM SIGCHI Curricula for Human-Computer Interaction. 1992, </booktitle> <publisher> ACM Press: </publisher> <address> New York, NY. </address> <booktitle> ACM Order Number: </booktitle> <pages> 608920. </pages>
Reference-contexts: Benchmarks for Teaching and Evaluating UI Tools - 10 - ** Submitted for Publication** RELATED WORK There is very little prior discussion about ways to teach a user interface software course, outside of the SIGCHI Curricula for HCI <ref> [3] </ref>. Benchmarks are widely used for evaluating the performance of hardware and compilers (such as the SPECMark [13]), but we do not know of any previous attempts at using benchmarks for evaluating the effectiveness of tools or libraries.
Reference: 4. <author> Hix, D. </author> <title> A Procedure for Evaluating Human-Computer Interface Development Tools, </title> <booktitle> in Proceedings UIST'89: ACM SIGGRAPH Symposium on User Interface Software and Technology. 1989. </booktitle> <address> Williamsburg, VA: </address> <pages> pp. 53-61. </pages>
Reference-contexts: There have been many studies of programmers (e.g., [1]), which consistently show an order of magnitude difference in capabilities, which is why the numbers in the tables might not be reliable. A comprehensive survey for evaluating toolkits <ref> [4] </ref> was developed, but it seems much more fun and educational to implement a benchmark than to fill out a long survey. Surveys will also highlight different aspects.
Reference: 5. <author> Hudson, S.E. and Smith, I. </author> <title> Ultra-Lightweight Constraints, </title> <booktitle> in Proceedings UIST'96: ACM SIGGRAPH Symposium on User Interface Software and Technology. 1996. </booktitle> <address> Seattle, WA: </address> <pages> pp. 147-155. </pages> <address> http://www.cc.gatech.edu/gvu/ui/sub_arctic/. </address>
Reference-contexts: We also expect to get a few more implementations by experts, and a few new toolkits.) So far, the students have used the following toolkits to implement the benchmarks: Visual Basic, Director, Hy-perCard, tcl/tk [10], Java AWT, SubArctic <ref> [5] </ref>, Delphi from Borland, MetroWorks PowerPlant for Macintosh, 1 A description of the course is available at http://www.cs.cmu.edu/~bam/uicourse/1997spring/index.html which includes a link to the assignment for creating the benchmarks. <p> The other students just used a Java compiler. In learning AWT, the main problem was the poor documentation. The advantages are the nice language (Java) and that the resulting programs will run on any platform as well as on the World-Wide-Web. SubArctic SubArctic <ref> [5] </ref> is a new research toolkit for Java that hides much of the complexities of AWT. Advantages include lots of built-in capabilities, including animation and constraints. SubArctic is a very powerful and flexible toolkit.
Reference: 6. <author> McKay, S., CLIM: </author> <title> The Common Lisp Interface Manager. </title> <journal> CACM, 1991. </journal> <volume> 34(9): </volume> <pages> pp. 58-59. </pages>
Reference-contexts: In order to investigate whether the students observations about the benchmarks and toolkits were consistent with the results from experienced users, we asked various experts to implement the benchmarks in their favorite toolkits. Experienced toolkit users have implemented at least one of the benchmarks in Garnet [8], CLIM <ref> [6] </ref>, Apples MacApp, GINA++ and GINA/CLM [11], LispView from Sun, tcl/tk, XPCE/SWI-Prolog [12], MrEd [2], and Amulet [9]. The results suggest that the benchmarks are useful for toolkit designers and evaluators, and are helpful when evaluating and creating toolkits. <p> Garnet was created by the first author. It is the predecessor to Amulet and is most appropriate for creating direct manipulation, forms and text editing applications. It has built-in constraints and a high-level input model. The graph benchmark was implemented in Garnet in 1990. CLIM CLIM <ref> [6] </ref> is a Lisp toolkit developed by Sym-bolics which uses the CLOS Common Lisp Object Sys Benchmarks for Teaching and Evaluating UI Tools - 8 - ** Submitted for Publication** tem.
Reference: 7. <author> Myers, </author> <title> B.A., User Interface Software Tools. </title> <journal> ACM Transactions on Computer Human Interaction, 1995. </journal> <volume> 2(1): </volume> <pages> pp. 64-103. </pages>
Reference-contexts: Surveys will also highlight different aspects. Surveys might identify the features which are present or missing, but are less likely to show which toolkits are easier to learn or are more effective for which types of applications. There are many previous surveys of user interface tools (e.g., <ref> [7] </ref>), which include discussions and comparisons, but none of these are based on implementations of benchmarks. CONCLUSION Using benchmarks seems to be a useful technique for students, toolkit developers, and toolkit users.
Reference: 8. <author> Myers, B.A., Giuse, D., and Vander Zanden, B., </author> <title> Declarative Programming in a Prototype-Instance System: Object-Oriented Programming Without Writing Methods . Sigplan Notices, </title> <booktitle> 1992. </booktitle> <volume> 27(10): </volume> <pages> pp. 184-200. </pages> <booktitle> ACM Conference on Object-Oriented Programming; Systems Languages and Applications; OOPSLA'92. </booktitle>
Reference-contexts: In order to investigate whether the students observations about the benchmarks and toolkits were consistent with the results from experienced users, we asked various experts to implement the benchmarks in their favorite toolkits. Experienced toolkit users have implemented at least one of the benchmarks in Garnet <ref> [8] </ref>, CLIM [6], Apples MacApp, GINA++ and GINA/CLM [11], LispView from Sun, tcl/tk, XPCE/SWI-Prolog [12], MrEd [2], and Amulet [9]. The results suggest that the benchmarks are useful for toolkit designers and evaluators, and are helpful when evaluating and creating toolkits. <p> Graph Editing Benchmark As an attempt to evaluate the Garnet user interface development environment, the first author created a benchmark in 1990 and had it implemented using 6 toolkits in 1991 and 1992 <ref> [8] </ref>. The benchmark description was reformated for the worldwide-web, and included in the list to see if toolkits have changed in the last 7 years. This benchmark is a simple graph editor, with nodes connected by arrow-lines. <p> Amulet V3.0 does not support text editing, multimedia or painting, so it is not appropriate for those benchmarks. Garnet Garnet <ref> [8] </ref> is a toolkit for Lisp and runs on Unix or Macintosh. Garnet was created by the first author. It is the predecessor to Amulet and is most appropriate for creating direct manipulation, forms and text editing applications. It has built-in constraints and a high-level input model. <p> The first six implementations were performed in 1991 and were previously published <ref> [8] </ref>, and the others are new. Benchmarks for Teaching and Evaluating UI Tools - 10 - ** Submitted for Publication** RELATED WORK There is very little prior discussion about ways to teach a user interface software course, outside of the SIGCHI Curricula for HCI [3].
Reference: 9. <author> Myers, B.A., et al. </author> <title> Easily Adding Animations to Interfaces Using Constraints, </title> <booktitle> in Proceedings UIST'96: ACM SIGGRAPH Symposium on User Interface Software and Technology. 1996. </booktitle> <address> Seattle, WA: </address> <pages> pp. 119-128. </pages> <address> http://www.cs.cmu.edu/~amulet. </address>
Reference-contexts: Experienced toolkit users have implemented at least one of the benchmarks in Garnet [8], CLIM [6], Apples MacApp, GINA++ and GINA/CLM [11], LispView from Sun, tcl/tk, XPCE/SWI-Prolog [12], MrEd [2], and Amulet <ref> [9] </ref>. The results suggest that the benchmarks are useful for toolkit designers and evaluators, and are helpful when evaluating and creating toolkits. For example, a few developers commented that the benchmarks helped highlight areas where their toolkits were missing important features for certain styles of applic ations. <p> The styles supported make it best for direct manipulation and text editing applications. MrEd uses Scheme as its programming language which requires a lot of memory at run time. Amulet Amulet <ref> [9] </ref> V3.0 for C++ runs on Unix, Windows NT or 95, or the Mac, and the designer of the toolkit (the first author of this paper) used it to implement one benchmark.
Reference: 10. <author> Ousterhout, J.K. </author> <title> An X11 Toolkit Based on the Tcl Language, </title> <booktitle> in Winter. 1991. USENIX: </booktitle> <pages> pp. 105-115. </pages>
Reference-contexts: We also expect to get a few more implementations by experts, and a few new toolkits.) So far, the students have used the following toolkits to implement the benchmarks: Visual Basic, Director, Hy-perCard, tcl/tk <ref> [10] </ref>, Java AWT, SubArctic [5], Delphi from Borland, MetroWorks PowerPlant for Macintosh, 1 A description of the course is available at http://www.cs.cmu.edu/~bam/uicourse/1997spring/index.html which includes a link to the assignment for creating the benchmarks. <p> Win32 Win32 is the lower-level interface to the Micro-soft Windows toolkit used by MFC. One student implemented the Cards benchmark directly in Win32 in C, without using MFC. Programming at this level seemed more difficult than MFC, and shares many of the disadvantages of MFC mentioned above. tcl/tk Tcl/tk <ref> [10] </ref> uses its own scripting language (tcl) which is interpreted, and applications created with it will work on PC, Unix or Mac. The students used various versions (7.3/3.6 on Unix, 7.5/4.1 on Windows and Unix, and 7.6.0b2/4.2 for Linux).
Reference: 11. <author> Spenke, M., </author> <title> Gina++ and GINA for Lisp, </title> <booktitle> 1992. </booktitle> <address> P.O. Box 1316, D-W-5205, St. Augustin 1, Germany, +49 2241 14-2642, spenke@gmd.de. </address>
Reference-contexts: Experienced toolkit users have implemented at least one of the benchmarks in Garnet [8], CLIM [6], Apples MacApp, GINA++ and GINA/CLM <ref> [11] </ref>, LispView from Sun, tcl/tk, XPCE/SWI-Prolog [12], MrEd [2], and Amulet [9]. The results suggest that the benchmarks are useful for toolkit designers and evaluators, and are helpful when evaluating and creating toolkits. <p> MacApp MacApp is a framework for creating Macintosh applications. In 1990, the graph benchmark was written using MacApp v2.0 which used Object Pascal. MacApp provides high-level support for commands and Undo, but not for graphics or interaction. GINA++ GINA++ <ref> [11] </ref> is a research toolkit in C++ from the German National Research Center for Computer Science. In 1992, the graph benchmark was implemented using version 1.2. GINA++ provides a high-level output model, including support for selection handles around a rectangular object, and a sophisticated Undo model. CLM/GINA CLM/GINA [11] is a <p> GINA++ GINA++ <ref> [11] </ref> is a research toolkit in C++ from the German National Research Center for Computer Science. In 1992, the graph benchmark was implemented using version 1.2. GINA++ provides a high-level output model, including support for selection handles around a rectangular object, and a sophisticated Undo model. CLM/GINA CLM/GINA [11] is a Lisp implementation of the GINA toolkit from the same group as GINA++. It has similar properties to GINA++. LispView LispView is a CLOS interface to the XView toolkit from Sun. In 1990, the graph benchmark was implemented using version 1.0.1.
Reference: 12. <author> SWI, XPCE, </author> <year> 1997. </year> <institution> University of Amsterdam, </institution> <address> Roetersstraat 15, 1018 WB Amsterdam, The Netherlands, +31 20 5256121, xpce-request@swi.psy.uva.nl, http://www.swi.psy.uva.nl/projects/xpce/home.html. </address>
Reference-contexts: Experienced toolkit users have implemented at least one of the benchmarks in Garnet [8], CLIM [6], Apples MacApp, GINA++ and GINA/CLM [11], LispView from Sun, tcl/tk, XPCE/SWI-Prolog <ref> [12] </ref>, MrEd [2], and Amulet [9]. The results suggest that the benchmarks are useful for toolkit designers and evaluators, and are helpful when evaluating and creating toolkits. <p> Sk8 enabled full implementation of the Multimedia benchmark in fewer lines of code than any of the other tools. On the down side, many parts of the documentation are incomplete or unavailable, and some of its GUI tools are still buggy. XPCE/SWI-Prolog XPCE <ref> [12] </ref> is a toolkit for Prolog, and version 4.9.2/2.8.0 was used by the designer of the toolkit running on the Linux version of Unix on a PC. The developer reported that the toolkit is most appropriate for graphical editors (like the Cards and Graph benchmarks).
Reference: 13. <institution> System Performance Evaluation Cooperative, SPEC Benchmark Suite Release 1.0, </institution> <year> 1989. </year>
Reference-contexts: Benchmarks are widely used for evaluating the performance of hardware and compilers (such as the SPECMark <ref> [13] </ref>), but we do not know of any previous attempts at using benchmarks for evaluating the effectiveness of tools or libraries.
References-found: 13

