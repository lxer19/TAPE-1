URL: http://www.math.rutgers.edu/~sycon/OLDER_REPORTS/sycon-91-11.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sycon/OLDER_REPORTS/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: siegelma@paul.rutgers.edu, sontag@hilbert.rutgers.edu  
Title: POWER OF NEURAL NETS  
Author: Hava T. Siegelmann, Eduardo D. Sontag, 
Keyword: Key words: neural networks, Turing machines  
Date: November 1991  
Address: New Brunswick, NJ 08903  
Affiliation: Department of Computer Science  Department of Mathematics Rutgers University,  
Note: ON THE COMPUTATIONAL  Research supported in part by US Air Force Grant AFOSR-91-0343. Rutgers Center for Systems and Control  
Abstract: Report SYCON-91-11 ABSTRACT This paper deals with the simulation of Turing machines by neural networks. Such networks are made up of interconnections of synchronously evolving processors, each of which updates its state according to a "sigmoidal" linear combination of the previous states of all units. The main result states that one may simulate all Turing machines by nets, in linear time. In particular, it is possible to give a net made up of about 1,000 processors which computes a universal partial-recursive function. (This is an update of Report SYCON-91-08; new results include the simulation in linear time of binary-tape machines, as opposed to the unary alphabets used in the previous version.) 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alon, N., A.K. Dewdney, and T.J. Ott, </author> <title> "Efficient simulation of finite automata by neural nets," </title> <journal> J. A.C.M. </journal> <volume> 38 (1991): </volume> <pages> 495-514. </pages>
Reference-contexts: there exist vectors v 1 ; v 2 ; : : : ; v 16 2 Q 6 and scalars c 1 ; c 2 ; : : : ; c 16 2 Q such that, for each a; b; d; e; x 2 f0; 1g and each q 2 <ref> [0; 1] </ref>, fi (a; b; d; e)x = i=1 and 16 X c i (v i ) 1 ; (10) where we denote = (1; a; b; d; e; x) and "" = dot product in Q 6 . Proof. <p> On the other hand, for each t 2 f0; 1g and each q 2 <ref> [0; 1] </ref> it holds that t q = (q + t 1) (12) (just check separately for t = 0; 1), so substituting formula (11) with t = fi (a; b; d; e)x into (12) gives the desired result. <p> two inputs) such that, starting at the zero state and if the input sequences are x 1 and x 2 , where x 1 (k) = ffi [w] for some k and x 2 (t) = 0 for t &lt; k, x 2 (k) = 1 (x 1 (t) 2 <ref> [0; 1] </ref> for t 6= k, x 2 (t) 2 [0; 1] for t &gt; k), then for processors z 9 , z 10 it holds that z 9 = 1 if k + 4 t k + 3 + j!j 0 otherwise , and ( 0 otherwise . <p> if the input sequences are x 1 and x 2 , where x 1 (k) = ffi [w] for some k and x 2 (t) = 0 for t &lt; k, x 2 (k) = 1 (x 1 (t) 2 <ref> [0; 1] </ref> for t 6= k, x 2 (t) 2 [0; 1] for t &gt; k), then for processors z 9 , z 10 it holds that z 9 = 1 if k + 4 t k + 3 + j!j 0 otherwise , and ( 0 otherwise . <p> It would certainly be interesting to have a better bound. It is quite possible that with some care in the construction one may be able to drastically reduce this estimate. One useful tool here may be the result in <ref> [1] </ref> applied to the control unit (here we used a very inefficient simulation). Many other types of "machines" may be used for universality (see [18], especially Chapter 2, for general definitions of continuous machines).
Reference: [2] <author> Berstel, J. and C. Reutenauer, </author> <title> Rational series and their Languages, </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: halting problem can be reduced to it); on the other hand, the problem appears to become decidable if a linear activation is used (halting in that case is equivalent to a fact that is widely conjectured to follow from classical results due to Skolem and others on rational functions; see <ref> [2] </ref>, page 75), and is also decidable in the pure threshold case (there are only finitely many states).
Reference: [3] <author> Blum, L., M. Shub, and S. Smale, </author> <title> "On a theory of computation and complexity over the real numbers: NP completeness, recursive functions, and universal machines," </title> <journal> Bull. A.M.S. </journal> <volume> 21(1989): </volume> <pages> 1-46. </pages>
Reference-contexts: A different approach to continuous-valued models of computation is given in <ref> [3] </ref> and other papers by the same authors; in that context, our processor nets can be viewed as programs with loops in which linear operations and linear comparisons are allowed, but with an added restriction on branching that reflects the nature of the saturated response we use.
Reference: [4] <author> Franklin, S., and M. Garzon, </author> <title> "Neural computability," </title> <booktitle> in Progress In Neural Networks, </booktitle> <volume> Vol 1 )(O. </volume> <editor> M. Omidvar, ed.), </editor> <publisher> Ablex, </publisher> <address> Norwood, NJ, </address> <year> 1990, </year> <pages> pp. 128-144. </pages>
Reference-contexts: Underlying the use of high-order nets is the conjecture that their computational power is superior to that of linearly interconnected nets. Also related is the work reported in [7], <ref> [4] </ref>, and [5], some of which deals with cellular automata. There one assumes an unbounded number of neurons, as opposed to a finite number fixed in advance.
Reference: [5] <author> Garzon, M., and S. Franklin, </author> <title> "Neural computability II," </title> <booktitle> in Proc. 3rd Int. Joint Conf. Neural Networks (1989): </booktitle> <volume> I, </volume> <pages> 631-637. </pages>
Reference-contexts: Underlying the use of high-order nets is the conjecture that their computational power is superior to that of linearly interconnected nets. Also related is the work reported in [7], [4], and <ref> [5] </ref>, some of which deals with cellular automata. There one assumes an unbounded number of neurons, as opposed to a finite number fixed in advance. <p> Then there exists a processor net N that computes , and consists of [ + - + + s + 2 + 4] system without inputs + <ref> [5] </ref> input + [10 + 1] output = 12s + 50 processors. * The Universal Net A consequence of Theorm 1 is the existence of a universal processor net, which upon receiving an encoded description of a recursively computable partial function (in terms of a Turing machine) and an input string,
Reference: [6] <author> Giles, C.E., G.Z. Sun, H.H. Chen, Y.C. Lee, and D. Chen, </author> <title> "Higher order networks recurrent and grammatical inference," </title> <booktitle> in Advances in Neural Information Processing Systems 2 (D.S. </booktitle> <editor> Touretzky, ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990: </year> <month> 380-. </month>
Reference-contexts: Keywords: neural networks, Turing machines. 1 are really necessary in order to achieve universality, though he conjectured that they are. High--order models have also been used in <ref> [6] </ref> and [20], as well as by many other authors, often with an added infinite external memory device. Underlying the use of high-order nets is the conjecture that their computational power is superior to that of linearly interconnected nets.
Reference: [7] <author> Hartley, R., and H. Szu, </author> <title> "A comparison of the computational power of neural network models," </title> <booktitle> in Proc. IEEE Conf. Neural Networks (1987): III, </booktitle> <pages> 17-22. </pages>
Reference-contexts: Underlying the use of high-order nets is the conjecture that their computational power is superior to that of linearly interconnected nets. Also related is the work reported in <ref> [7] </ref>, [4], and [5], some of which deals with cellular automata. There one assumes an unbounded number of neurons, as opposed to a finite number fixed in advance.
Reference: [8] <author> John E. Hopcroft, and Jeffrey D. Ullman, </author> <title> Introduction to Automata Theory, Languages, and Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference: [9] <author> Kleene, </author> <title> S.C., "Representation of events in nerve nets and finite automata," </title> <editor> in Shannon, C.E., and J. McCarthy, eds., </editor> <title> Automata Studies, </title> <publisher> Princeton Univ. Press 1956: </publisher> <pages> 3-41. </pages>
Reference-contexts: The use of sigmoidal functions |as opposed to hard thresholds| is what distinguishes this area from older work that dealt only with finite automata. Indeed, it has long been known, at least since the classical papers by McCulloch and Pitts ([12], <ref> [9] </ref>), how to implement logic gates by threshold networks, and therefore how to simulate finite automata by such nets. For us, however, nets are essentially analog computational devices, in accordance with models currently used in neural net practice.
Reference: [10] <author> Maass, W., G. Schnitger, and E.D. Sontag, </author> <title> "On the computational power of sigmoid versus boolean threshold circuits," </title> <booktitle> Proc. of the 32nd Annual Symp. on Foundations of Computer Science, </booktitle> <year> 1991: </year> <pages> 767-776. </pages>
Reference-contexts: However, prior work considered only the special case of feedforward nets -see for instance [19] for questions of approximation and function interpolation, and <ref> [10] </ref> for questions of Boolean circuit complexity. The paper is organized as follows. First we define precisely nets and we state the main result. Then we prove the result through a few intermediate steps, and we provide an estimate of the number of processors used.
Reference: [11] <author> Marcus, </author> <title> C.M., and R.M. Westervelt, "Dynamics of iterated-map neural networks," </title> <journal> Phys. Rev. Ser. </journal> <volume> A 40(1989): </volume> <pages> 3355-3364. </pages>
Reference-contexts: When b 1 = b 2 = 0, one has a net without inputs. Processor nets appear frequently in neural network studies, and their dynamic properties are of interest (see for instance <ref> [11] </ref>). 3 It is obvious that |with zero, or more general rational, initial state| one can simulate a processor net with a Turing machine. We wish to prove, conversely, that any function com putable by a Turing machine can be computed by such a processor net.
Reference: [12] <author> McCulloch, W.S., and W. Pitts, </author> <title> "A logical calculus of the ideas immanent in nervous activity," </title> <journal> Bull. Math. Biophys. </journal> <volume> 5(1943): </volume> <pages> 115-133. </pages>
Reference: [13] <author> Minsky, </author> <title> M.L., Computation: Finite and Infinite Machines, </title> <publisher> Prentice Hall, </publisher> <address> Engelwood Cliffs, </address> <year> 1967. </year>
Reference-contexts: Minsky proved the existence of a universal Turing machine having one tape with 4 letters and 7 control states, <ref> [13] </ref>. Shannon showed in [16] how to change the number of letters in a Turing machine. Following his construction, we obtain a 2-letter 63-state Turing machine. However, we are interested in a two-stack machine rather than one tape.
Reference: [14] <author> Pollack, J.B., </author> <title> On Connectionist Models of Natural Language Processing, </title> <type> Ph.D. Dissertation, </type> <institution> Computer Science Dept, Univ. of Illinois, Urbana, </institution> <year> 1987. </year>
Reference-contexts: For us, however, nets are essentially analog computational devices, in accordance with models currently used in neural net practice. In <ref> [14] </ref>, Pollack argued that a certain recurrent net model, which he called a "neuring machine," is universal. The model in [14] consisted of a finite number of neurons of two different kinds, having identity and threshold responses, respectively. <p> For us, however, nets are essentially analog computational devices, in accordance with models currently used in neural net practice. In <ref> [14] </ref>, Pollack argued that a certain recurrent net model, which he called a "neuring machine," is universal. The model in [14] consisted of a finite number of neurons of two different kinds, having identity and threshold responses, respectively. The machine was high-order , that is, the activations were combined using multiplications as opposed to just linear combinations.
Reference: [15] <editor> Schwarzschild, R., and E.D. Sontag, </editor> <booktitle> "Algebraic theory of sign-linear systems," in Proceedings of the Automatic Control Conference, </booktitle> <address> Boston, MA, </address> <month> June </month> <year> 1991: </year> <pages> 799-804. </pages>
Reference-contexts: These models are all closely related to the classical linear systems from control theory ([18]); see for instance <ref> [15] </ref> for some basic control-theoretic facts about systems that use identity, , and t activations. Note that the simulation result has many consequences regarding the decidability, or more generally the complexity, of questions about recursive nets of the type we consider.
Reference: [16] <author> Shannon, Claude E., </author> <title> "A Universal Turing machine with two internal states," </title> <editor> in Shannon, C.E., and J. McCarthy, eds., </editor> <title> Automata Studies, </title> <publisher> Princeton Univ. Press 1956: </publisher> <pages> 157-165. </pages>
Reference-contexts: Minsky proved the existence of a universal Turing machine having one tape with 4 letters and 7 control states, [13]. Shannon showed in <ref> [16] </ref> how to change the number of letters in a Turing machine. Following his construction, we obtain a 2-letter 63-state Turing machine. However, we are interested in a two-stack machine rather than one tape.
Reference: [17] <author> Siegelmann, H.T., and E.D. Sontag, </author> <title> "Recognizing arbitrary languages with finite, first-order, neural networks with real-valued weights," </title> <note> in preparation. </note>
Reference-contexts: Note that we restrict to rational states and weights in order to preserve computability. It is possible to prove that using real valued weights results in processor nets that can "compute" arbitrary partial functions (not necessarily recursive); this can be achieved (see <ref> [17] </ref>) by encoding all information in the network weights. In fact, with zero initial states, and restricting for simplicity to language recognition, the situation is as follows: Weights Recog.
Reference: [18] <author> Sontag, E.D., </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: One useful tool here may be the result in [1] applied to the control unit (here we used a very inefficient simulation). Many other types of "machines" may be used for universality (see <ref> [18] </ref>, especially Chapter 2, for general definitions of continuous machines). For instance, we can show that systems evolving according to equations x + = x + t (Ax + bu + c) ; (15) where t takes the sign in each coordinate, again are universal in a precise sense.
Reference: [19] <author> Sontag, E.D., </author> <title> "Remarks on interpolation and recognition using neural nets," </title> <booktitle> in Advances in Neural Information Processing Systems 3 (R.P. </booktitle> <editor> Lippmann, J. Moody, and D.S. Touretzky, eds), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991, </year> <pages> pp. 939-945. </pages>
Reference-contexts: Power integer regular rational recursive real arbitrary The idea of using continuous-valued neurons in order to attain gains in computational capabilities as compared with threshold gates had been investigated before. However, prior work considered only the special case of feedforward nets -see for instance <ref> [19] </ref> for questions of approximation and function interpolation, and [10] for questions of Boolean circuit complexity. The paper is organized as follows. First we define precisely nets and we state the main result.
Reference: [20] <author> Sun, G.Z., H.-H. Chen, Y.-C. Lee, and C.L. Giles, </author> <title> "Turing equivalence of neural networks with second order connection weights," </title> <booktitle> in Int.Jt.Conf.Neural Nets, </booktitle> <address> Seattle, 1991:II,357-. </address> <month> 17 </month>
Reference-contexts: Keywords: neural networks, Turing machines. 1 are really necessary in order to achieve universality, though he conjectured that they are. High--order models have also been used in [6] and <ref> [20] </ref>, as well as by many other authors, often with an added infinite external memory device. Underlying the use of high-order nets is the conjecture that their computational power is superior to that of linearly interconnected nets.
References-found: 20

