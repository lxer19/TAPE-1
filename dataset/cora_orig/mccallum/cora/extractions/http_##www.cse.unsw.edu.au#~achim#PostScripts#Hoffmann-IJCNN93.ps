URL: http://www.cse.unsw.edu.au/~achim/PostScripts/Hoffmann-IJCNN93.ps
Refering-URL: http://www.cse.unsw.edu.au/~achim/index.html
Root-URL: http://www.cse.unsw.edu.au
Email: E-mail: achim@cs.unsw.oz.au  
Title: Complexity bounds of emerging structures in self-organizing networks  
Author: Achim G. Hoffmann 
Address: Kensington, 2033 NSW, Australia  
Affiliation: University of New South Wales School of Computer Science Engineering  
Abstract: The idea of self-organizing systems is to acquire a meaningful internal structure just by being exposed to some `natural' environment. Due to the complex network dynamics it appears very hard to analyze the structures that may emerge in such a system. Results on the complexity of classification functions and the preconditions necessary in order to allow the computation of such functions are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Buhmann and H. Kuhnel. </author> <title> Complexity optimized data clustering by competitive neural networks. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 75-88, </pages> <year> 1993. </year>
Reference-contexts: I.e. self-organization is a kind of unsupervised learning. This contrasts supervised approaches, where the system is confronted with pre-classified objects or feedback whether its predictions have been correct. Recent work on probabilistic analysis on clustering and self-organization can be found e.g. in <ref> [1] </ref>. As a consequence, the only source of information for the system is the fact, that not all possible objects will be presented, but only a certain `meaningful' subset. This subset is assumed to show some `natural' clusters which should be recognized by the self-organizing system.
Reference: [2] <author> R. Freivalds and A. G. Hoffmann. </author> <title> An inductive inference approach to classification. </title> <booktitle> In Proceedings of the 3 rd Workshop on Analogical and Inductive Inference, </booktitle> <pages> pages 187-196, </pages> <address> Schlo Dagstuhl, Germany, </address> <month> October </month> <year> 1992. </year> <note> Springer-Verlag. </note>
Reference-contexts: The length of that program is also called the Kolmogorov complexity of f and is denoted by K (f). This notion has been introduced for investigations of learning complex functions within connectionist models of computation in Hoffmann [4]. Freivalds & Hoffmann <ref> [2] </ref> used the notion to investigate the relation between clustering and inductive inference. The complexity comp (S) of S is therefore defined as the Kolmogorov complexity of its description. I.e. the minimal encoding of the systems functionality.
Reference: [3] <author> S. Grossberg. </author> <title> Adaptive pattern classification and universal recoding I & II. </title> <journal> Biological Cybernetics, </journal> <volume> 23 </volume> <pages> 121-134 & 187-202, </pages> <year> 1976. </year>
Reference-contexts: 1 Introduction Processes of self-organization are believed to be capable to capture complex environmental conditions by emerging system structures, e.g. Grossberg <ref> [3] </ref>, or Ritter et.al. [6] etc. The basic idea is, to have a system which learns to behave useful in a certain sense by just getting some unclassified objects of the respective domain. Here, the system gets no feedback whether its learning approach is correct or not.
Reference: [4] <author> A. G. Hoffmann. </author> <title> On computational limitations of neural network architectures. </title> <booktitle> In Proceedings of the 2 nd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pages 818-825, </pages> <year> 1990. </year>
Reference-contexts: The length of that program is also called the Kolmogorov complexity of f and is denoted by K (f). This notion has been introduced for investigations of learning complex functions within connectionist models of computation in Hoffmann <ref> [4] </ref>. Freivalds & Hoffmann [2] used the notion to investigate the relation between clustering and inductive inference. The complexity comp (S) of S is therefore defined as the Kolmogorov complexity of its description. I.e. the minimal encoding of the systems functionality.
Reference: [5] <author> A. N. </author> <title> Kolmogorov. Three approaches to the quantitative definition of information. </title> <journal> International Journal for Computer Mathematics, </journal> <volume> 2 </volume> <pages> 157-168, </pages> <year> 1968. </year> <note> (originally published in Rus-sian: </note> <editor> Problemi peredachi informacii, </editor> <volume> vol. 1, No. 1, </volume> <year> 1965, </year> <note> p. 3-11.). </note>
Reference-contexts: Let the description of S be denoted by descr (S). 2.1 Measuring the complexity of emerging structures For measuring the complexity of emerging system structures, Kolmogorov's notion of the complexity of finite objects as introduced in Kolmogorov <ref> [5] </ref> is used. According to this idea, the complexity of a (classification) function f is the length of the shortest (binary encoded) program for a fixed universal Turing machine U which computes the function f .
Reference: [6] <author> H. Ritter, T. Martinetz, and K. Schulten. </author> <title> Neural Computation and Self-organizing Maps. </title> <publisher> Ad-dison & Wesley, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Processes of self-organization are believed to be capable to capture complex environmental conditions by emerging system structures, e.g. Grossberg [3], or Ritter et.al. <ref> [6] </ref> etc. The basic idea is, to have a system which learns to behave useful in a certain sense by just getting some unclassified objects of the respective domain. Here, the system gets no feedback whether its learning approach is correct or not.
Reference: [7] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year> <month> 4 </month>
Reference-contexts: This is certainly a rather op timistic assumption. See figure 3 for illustration. 4 Conclusion and further work It has been shown, that the complexity of classification functions acquired under reasonable preconditions is seriously limited. Moreover, in a probabilistic setting related to Valiant's PAC-learning model <ref> [7] </ref> the complexity of a function that can be acquired does not exceed significantly the complexity of the used inference strategy. Finally, a bound on the minimal number of randomly drawn examples necessary in order that a complex structure may emerge has been given for the best possible case.
References-found: 7

