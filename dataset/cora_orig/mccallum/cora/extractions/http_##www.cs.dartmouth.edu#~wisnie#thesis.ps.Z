URL: http://www.cs.dartmouth.edu/~wisnie/thesis.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~wisnie/
Root-URL: http://www.cs.dartmouth.edu
Title: Efficient Design and Implementation of Permutation Algorithms on the Memory Hierarchy  Examining Committee:  
Author: Leonard F. Wisniewski (chairman) Thomas H. Cormen David Kotz George Cybenko Thomas Sundquist Edward Berger Dean 
Degree: A Thesis Submitted to the Faculty in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science by  
Date: March, 1996  
Address: Hanover, New Hampshire  
Affiliation: Dartmouth College  of Graduate Studies  
Abstract-found: 0
Intro-found: 0
Reference: [ACFS94] <author> B. Alpern, L. Carter, E. Feig, and T. Selker. </author> <title> The uniform memory hierarchy model of computation. </title> <journal> Algorithmica, </journal> 12(2/3):72-109, August and September 1994. 
Reference-contexts: Vitter and Vengroff use BMMC permutations as a subroutine in performing out-of-core matrix-matrix multiply [VV95]. When implementing out-of-core algorithms, we would like to have a unified approach for solving the same problem at different levels of the memory hierarchy. Several unified approaches have been pursued to model memory hierarchies <ref> [VS94b, ACFS94] </ref>. In this thesis, we present a unified approach to designing algorithms for efficiently performing the class of BMMC permutations. The approach is flexible enough to account for the structure of memory at the various levels of the memory hierarchy. <p> The interested reader can find full descriptions of these models in <ref> [ACFS94, AV88, VS90, VS94a] </ref>. Using the I/O complexity models, algorithm designers have developed out-of-core algorithms, such as the algorithms detailed in Chapters 2 and 3, that choreograph data movements to utilize all the disks in the I/O subsystem concurrently.
Reference: [ACS87] <author> Alok Aggarwal, Ashok K. Chandra, and Marc Snir. </author> <title> Hierarchical memory with block transfer. </title> <booktitle> In Proceedings of the 28th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 204-216, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: One can adapt the lower bound for BMMC permutations to show that this BPC algorithm is asymptotically optimal. The BMMC algorithm in Section 2.5, 3 Johnsson and Ho [JH91] call BPC permutations dimension permutations, and Aggarwal, Chan dra, and Snir <ref> [ACS87] </ref> call BPC permutations without complementing rational permutations. 18 Chapter 2. Performing BMMC Permutations on Parallel Disk Systems however, is asymptotically optimal for all BMMC permutations|including those that are BPC|and it reduces the innermost factor of 2 in the above bound to a factor of 1.
Reference: [AP94] <author> Alok Aggarwal and C. Greg Plaxton. </author> <title> Optimal parallel sorting in multilevel storage. </title> <booktitle> In Proceedings of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 659-668, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting <ref> [AP94, Arg95, NV93, VS94a] </ref>, general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93].
Reference: [Arg95] <author> Lars Arge. </author> <title> The buffer tree: A new technique for optimal I/O-algorithms. </title> <booktitle> In 4th International Workshop on Algorithms and Data Structures (Proceedings), Lecture Notes in Computer Science, </booktitle> <volume> number 955, </volume> <pages> pages 334-345. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting <ref> [AP94, Arg95, NV93, VS94a] </ref>, general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93].
Reference: [AV88] <author> Alok Aggarwal and Jeffrey Scott Vitter. </author> <title> The input/output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: These bounds are asymptotically tight, for they match the lower bounds proven earlier by Aggarwal and Vitter <ref> [AV88] </ref> using a model with one disk and D independent read/write heads, which is at least as powerful as the Vitter-Shriver model. Specific classes of permutations sometimes require fewer parallel I/Os than general permutations. <p> The BPC algorithm of [Cor93] is asymptotically optimal as well; see [Cor92] for details. 22 Chapter 2. Performing BMMC Permutations on Parallel Disk Systems Technique The proof of Theorem 2.3 relies heavily on the technique used by Aggarwal and Vitter <ref> [AV88] </ref> for a lower bound on I/Os in matrix transposition; their proof is based in turn on a method by Floyd [Flo72]. The basic scheme of the proof of Theorem 2.3 uses a potential-function argument. <p> The BMMC algorithm requires 2N records of disk storage, but, in Chapter 3, we shall show how to adapt the algorithm to perform BMMC permutations using only N + M records of disk storage. One can adapt the proof by Aggarwal and Vitter <ref> [AV88] </ref> to bound max precisely, rather than just asymptotically. In particular, it is a straightforward exercise to derive the bound max B 2 + lg (M=B) : Moreover, the potential change is at most zero for write operations, and so the 2.6 Conclusions 51 potential increases only during read operations. <p> The interested reader can find full descriptions of these models in <ref> [ACFS94, AV88, VS90, VS94a] </ref>. Using the I/O complexity models, algorithm designers have developed out-of-core algorithms, such as the algorithms detailed in Chapters 2 and 3, that choreograph data movements to utilize all the disks in the I/O subsystem concurrently. <p> An API for Choreographing Data Accesses 4.1 Background Researchers have developed theoretical models to capture important features of data movement between main memory and secondary storage <ref> [AV88, Flo72, VS90, VS94a] </ref>. In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94a] provides a reasonable model for the design of I/O-efficient algorithms, such as those shown in Chapters 2 and 3, and analysis at a feasible level of abstraction.
Reference: [AVV95] <author> L. Arge, D. E. Vengroff, and J. S. Vitter. </author> <title> External-memory algorithms for processing line segments in geographic information systems. </title> <booktitle> In Proceedings of the Third Annual European Symposium on Algorithms, </booktitle> <pages> pages 295-310. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1995. </year>
Reference-contexts: designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms <ref> [AVV95, GTVV93] </ref>. In this section, we define the parameters and data layout for PDM. We also describe the types of data access needed by PDM algorithms and discuss how well existing file systems support these types of data access.
Reference: [Bar68] <author> G.H. Barnes, et al. </author> <title> The ILLIAC IV computer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-17(8):746-757, </volume> <year> 1968. </year>
Reference-contexts: In particular, we have developed algorithms for efficiently performing BMMC permutations on data that reside in a distributed-memory MIMD multiprocessor with a mesh topology. One of the first multiprocessors, the ILLIAC IV computer <ref> [Bar68] </ref>, was a SIMD machine with a mesh topology. In a SIMD architecture, we perform a single operation on multiple data simultaneously in lockstep. More recently, Chapter 5.
Reference: [BBS + 94] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Each box represents one block. The number of stripes needed is N=BD = 4. Numbers indicate record indices. <ref> [BBS + 94] </ref>. <p> The Panda run-time library [SCJ + 95, SW94] achieves performance improvements by providing an API and more efficient layout alternatives for multidimensional array data. This approach takes advantage of the spatial and temporal locality of the array. Jovian <ref> [BBS + 94] </ref> and PASSION [CBH + 94] also provide support for efficient array data access, but abstract away direct disk access from the programmer.
Reference: [Ben65] <author> V.E. </author> <title> Benes. Mathematical Theory of Connecting Networks and Telephone Traffic. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1965. </year>
Reference-contexts: Performing BMMC Permutations on Multistage Networks Previous work on multistage networks Multistage networks comprise another class of interconnection networks for parallel computers. Many multistage interconnection networks have been proposed, such as the network [Law75], the indirect binary n-cube [Pea77], the Clos network [Clo53], the Benes network <ref> [Ben65] </ref>, the delta network [Pat81], and the expanded delta network (EDN) [KS83]. A multistage interconnection network consists of alternating stages of switches and wires. The switches are typically crossbars connected by wires, usually in a pattern based on the perfect shu*e [Sto71].
Reference: [BM75] <author> A. Borodin and I. Munro. </author> <title> The Computational Complexity of Algebraic and Numeric Problems. </title> <publisher> Elsevier, </publisher> <address> New York, </address> <year> 1975. </year> <note> 216 Bibliography </note>
Reference-contexts: We refer the interested reader to the original paper [ST91] for more specific details, or to an expanded discussion in [Moo94]. The polynomial division tree method The fast cosine transform algorithm uses the polynomial division tree method for evaluating a polynomial function <ref> [BM75, DHR94, Knu81, Moo94] </ref>. In the polynomial 8.1 Fast cosine transforms 183 division tree model, we represent a function as a polynomial p (z).
Reference: [Bru94] <author> Kristin Bruhl. </author> <title> BMMC permutations on a DECmpp 12000/Sx 2000. </title> <type> Technical Report PCS-TR94-224, </type> <institution> Department of Computer Science, Dartmouth College, </institution> <month> June </month> <year> 1994. </year> <type> Senior thesis. </type>
Reference-contexts: Shen [She95] shows how to perform any BPC permutation on a k-extra-stage network in a minimum number of passes. Scherson and Subrama-nian's general permutation algorithm [SS93] only requires two passes on a certain class of EDNs. Cormen and Bruhl <ref> [Bru94, CB95] </ref> study the performance of several algorithms for BMMC permutations on the restricted-access MasPar MP-2 global router.
Reference: [CB95] <author> Thomas H. Cormen and Kristin Bruhl. </author> <title> Don't be too clever: Routing BMMC permutations on the MasPar MP-2. </title> <booktitle> In Proceedings of the 7th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 288-297, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Shen [She95] shows how to perform any BPC permutation on a k-extra-stage network in a minimum number of passes. Scherson and Subrama-nian's general permutation algorithm [SS93] only requires two passes on a certain class of EDNs. Cormen and Bruhl <ref> [Bru94, CB95] </ref> study the performance of several algorithms for BMMC permutations on the restricted-access MasPar MP-2 global router.
Reference: [CBF93] <author> Peter F. Corbett, Sandra Johnson Baylor, and Dror G. Feitelson. </author> <title> Overview of the Vesta parallel file system. </title> <booktitle> In Proceedings of IPPS '93 Workshop on I/O in Parallel Computer Systems, </booktitle> <pages> pages 1-16, </pages> <month> April </month> <year> 1993. </year> <note> Reprinted in Computer Architecture News, </note> <month> December </month> <year> 1993. </year>
Reference-contexts: Another approach integrates special enhancements for I/O into the file system <ref> [CBF93, KS93] </ref>.
Reference: [CBH + 94] <author> Alok Choudhary, Rajesh Bordawekar, Michael Harry, Rakesh Krish-naiyer, Ravi Ponnusamy, Tarvinder Singh, and Rajeev Thakur. </author> <title> PASSION: parallel and scalable software for input-output. </title> <type> Technical Report SCCS-636, </type> <institution> ECE Dept., NPAC and CASE Center, Syracuse University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: At the software 78 Chapter 4. An API for Choreographing Data Accesses level, considerable effort has been made to develop new languages and compiler features that support I/O parallelism and optimizations via data layout conversion [dBC93], compiler hints [PGS93], and preprocessing for out-of-core parallel code <ref> [CBH + 94, CC94] </ref>. Another approach integrates special enhancements for I/O into the file system [CBF93, KS93]. <p> The Panda run-time library [SCJ + 95, SW94] achieves performance improvements by providing an API and more efficient layout alternatives for multidimensional array data. This approach takes advantage of the spatial and temporal locality of the array. Jovian [BBS + 94] and PASSION <ref> [CBH + 94] </ref> also provide support for efficient array data access, but abstract away direct disk access from the programmer.
Reference: [CC94] <author> Thomas H. Cormen and Alex Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: At the software 78 Chapter 4. An API for Choreographing Data Accesses level, considerable effort has been made to develop new languages and compiler features that support I/O parallelism and optimizations via data layout conversion [dBC93], compiler hints [PGS93], and preprocessing for out-of-core parallel code <ref> [CBH + 94, CC94] </ref>. Another approach integrates special enhancements for I/O into the file system [CBF93, KS93].
Reference: [CFF + 95] <author> Peter Corbett, Dror Feitelson, Sam Fineberg, Yarsun Hsu, Bill Nitzberg, Jean-Pierre Prost, Marc Snir, Bernard Traversat, and Park-son Wong. </author> <title> Overview of the MPI-IO parallel I/O interface. </title> <booktitle> In IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 1-15, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: These APIs include extensions to the conventional Unix interface (e.g., [NK95a, NK95b]), modifications to the conventional Unix interface (e.g., [GS95]), and other interfaces which differ from Unix (e.g., <ref> [CFF + 95, CFPB93] </ref>). An I/O-efficient algorithm would not be easy to program using these low-level file system APIs since the programmer must map the high-level parallel disk accesses to low-level file system operations.
Reference: [CFPB93] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, and San-dra Johnson Baylor. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: These APIs include extensions to the conventional Unix interface (e.g., [NK95a, NK95b]), modifications to the conventional Unix interface (e.g., [GS95]), and other interfaces which differ from Unix (e.g., <ref> [CFF + 95, CFPB93] </ref>). An I/O-efficient algorithm would not be easy to program using these low-level file system APIs since the programmer must map the high-level parallel disk accesses to low-level file system operations.
Reference: [CGG + 95] <author> Yi-Jen Chiang, Michael T. Goodrich, Edward F. Grove, Roberto Tamassia, Darren Erik Vengroff, and Jeffrey Scott Vitter. </author> <title> External-memory graph algorithms. </title> <booktitle> In Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 139-149, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms <ref> [CGG + 95] </ref>, and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. We also describe the types of data access needed by PDM algorithms and discuss how well existing file systems support these types of data access.
Reference: [CH96] <author> Sumit Chawla and Dennis Healy. </author> <title> The wavelet packet best basis algorithm: A fast parallel implementation on the MP-2 for real-time MRI. </title> <type> Technical Report PCS-TR95-271, </type> <institution> Dartmouth College Department of Computer Science, </institution> <year> 1996. </year> <note> Bibliography 217 </note>
Reference-contexts: Moreover, many transform algorithms used by signal-processing applications, such as the Fast Fourier Transform (FFT) [DHR94], the Fast Cosine Transform (FCT) [Moo94, MW95], and the wavelet packet transform <ref> [CH96] </ref>, perform a BMMC permutation before or after the computation. Weather modeling and data compression/decompression techniques use these various transforms. Vitter and Vengroff use BMMC permutations as a subroutine in performing out-of-core matrix-matrix multiply [VV95]. <p> We ask whether other applications use permutations that are in the class of BMMC permutations, such as wavelet transforms <ref> [CH96] </ref>. If so, we could apply the techniques of the linear transformation analysis in this chapter to determine their complexity.
Reference: [Chi95] <author> P. Chintrakulchai. </author> <title> Speeding Up Fractal Image Compression. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: Chapter 8 Complexity Analysis of BMMC Permutations for FCTs Fast cosine transform (FCT) algorithms are essential to efficient computation in many applications such as weather modeling, data compression/decompression, and convolutions on real, symmetric data <ref> [Chi95, ER82, RY90] </ref>. Recent descriptions of fast cosine transform algorithms [ST91, Ste92] perform a record minimum number of multiplication and addition operations to compute an FCT. These new algorithms have been derived using the polynomial-division-tree computational model. These algorithms include a permutation before or after the computation of the transform.
Reference: [CK94] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <type> Technical Report PCS-TR93-188, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> September </month> <year> 1994. </year> <note> Earlier version appeared in Proceedings of the 1993 DAGS/PC Symposium, </note> <editor> Hanover, </editor> <publisher> NH, </publisher> <pages> pages 64-74, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: If the parallel disk system lays out consecutive logical blocks of a file in a physically consecutive fashion, consecutive access can result in small seek times when accessing a file sequentially (e.g., by memoryload access). Parallel disk access in file systems and runtime libraries Cormen and Kotz <ref> [CK94] </ref> have identified several capabilities that a file system should support to enable high-performance implementations of the I/O-efficient algorithms. 86 Chapter 4. An API for Choreographing Data Accesses offset and the number of records in a consecutive read are multiples of the stripe size.
Reference: [Clo53] <author> C. </author> <title> Clos. A study of non-blocking switching networks. </title> <journal> Bell Systems Technical Journal, </journal> <volume> 32 </volume> <pages> 406-424, </pages> <month> March </month> <year> 1953. </year>
Reference-contexts: Performing BMMC Permutations on Multistage Networks Previous work on multistage networks Multistage networks comprise another class of interconnection networks for parallel computers. Many multistage interconnection networks have been proposed, such as the network [Law75], the indirect binary n-cube [Pea77], the Clos network <ref> [Clo53] </ref>, the Benes network [Ben65], the delta network [Pat81], and the expanded delta network (EDN) [KS83]. A multistage interconnection network consists of alternating stages of switches and wires. The switches are typically crossbars connected by wires, usually in a pattern based on the perfect shu*e [Sto71].
Reference: [Cor92] <author> Thomas H. Cormen. </author> <title> Virtual Memory for Data-Parallel Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <year> 1992. </year> <note> Available as Technical Report MIT/LCS/TR-559. </note>
Reference-contexts: leading lg M fi lg M submatrix of the characteristic matrix and H (N; M; B) = &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 4 lg B p 4 lg (N=B) p p 5 if N B M : One can adapt the lower bound proven in <ref> [Cor92, CSW94] </ref> to show that N lg Mr parallel I/Os are necessary (see Section 2.8 of [Cor92]), but until the publication of [CSW94], it had been unknown whether the fi ( N BD H (N; M; B)) term is necessary in all cases. <p> = &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 4 lg B p 4 lg (N=B) p p 5 if N B M : One can adapt the lower bound proven in [Cor92, CSW94] to show that N lg Mr parallel I/Os are necessary (see Section 2.8 of <ref> [Cor92] </ref>), but until the publication of [CSW94], it had been unknown whether the fi ( N BD H (N; M; B)) term is necessary in all cases. <p> We include the proofs here for completeness. 2.2 The lower bound and other useful concepts 21 2.2 The lower bound and other useful concepts In this section, we present the theorem from <ref> [Cor92, CSW94] </ref> that states the lower bound for BMMC permutations. After stating the lower bound, we briefly discuss its significance and several of the important concepts used in its proof that we shall use throughout the remainder of this thesis. <p> Section 2.5 presents an algorithm that achieves the bound given by Theorem 2.3, and so this algorithm is asymptotically optimal. The BPC algorithm of [Cor93] is asymptotically optimal as well; see <ref> [Cor92] </ref> for details. 22 Chapter 2. <p> We compute the initial potential (0) and final potential (t) and bound the amount ( max ) that the potential can increase in each I/O operation. Thus, we derive the lower bound by computing (t)(0) max . Ranges and preimages In <ref> [Cor92, CSW94] </ref>, the lower bound arguments for BMMC and BPC permutations use the ranges of matrices and preimages of vectors under matrix multiplication. We shall use the notions of ranges and preimages later in this thesis. Here, we reprise the definitions of range and preimage from [Cor92, CSW94] and state some <p> Ranges and preimages In <ref> [Cor92, CSW94] </ref>, the lower bound arguments for BMMC and BPC permutations use the ranges of matrices and preimages of vectors under matrix multiplication. We shall use the notions of ranges and preimages later in this thesis. Here, we reprise the definitions of range and preimage from [Cor92, CSW94] and state some relevant lemmas. <p> It is particularly satisfying that the tight bound was achieved not by raising the lower bound proven in <ref> [Cor92, CSW94] </ref>, but by decreasing the upper bound in [Cor93]. The multiplicative and additive constants in the I/O complexity of our algorithm are small, which is especially fortunate in light of the expense of disk accesses. <p> Since the quantity 2=(e ln 2) is approximately 1:06, this lower bound is quite close to the exact upper bound given by Theorem 2.20. What other permutations can be performed quickly? Several O (1)-pass permutation classes appear in <ref> [Cor92] </ref>, and this chapter has added several more (the MLD, MLD 1 , and MLD ffi MLD 1 permutations described in Section 2.3). <p> The indexing function maps each grid position p = (p 0 ; p 1 ; : : : ; p d1 ) to a unique index in row-major order: (p; m) = i=0 4 @ j=i+1 1 3 3 fi 5 mesh. The following lemma from <ref> [Cor92] </ref> states that the difference between the source and target index of each record in row-major order is the same for every grid location mapped by a mesh permutation. <p> Torus permutations Our algorithm for torus permutations improves on the algorithm of <ref> [Cor92] </ref>, which performs a torus permutation as a 2 d -monotonic route, i.e., a superposition of 2 d disjoint monotonic routes. Cormen's algorithm uses at most (2 d+1 + 1) dN=BDe parallel I/Os and requires 2N records of disk space and 2 d + 1 stripes of memory. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations <ref> [Cor92, Cor93, CSW94, Wis95] </ref>, mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations <ref> [Cor92, Wis95] </ref>, matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose <ref> [Cor92, Cor93, CSW94, VS94a] </ref>, FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> Cormen and Bruhl [Bru94, CB95] study the performance of several algorithms for BMMC permutations on the restricted-access MasPar MP-2 global router. Their algorithms adapt the block BMMC algorithms for parallel disk systems with a block size of 1 from <ref> [Cor92, Cor93] </ref> to perform BMMC permutations on the global router when the number of messages in the data set exceeds the number of inputs and outputs. They present algorithms that guarantee that each processor or cluster of processors sends and receives one message at a time.
Reference: [Cor93] <author> Thomas H. Cormen. </author> <title> Fast permuting in disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):41-57, January and February 1993. </note>
Reference-contexts: Specific classes of permutations sometimes require fewer parallel I/Os than general permutations. Vitter and Shriver showed how to transpose an R fi S matrix (N = RS) with only fi BD 1 + lg min (B;R;S;N=B) parallel I/Os. Subsequently, Cor men <ref> [Cor93] </ref> studied several classes of bit-defined permutations that include matrix transposition as a special case. Table 2.1 shows some of the classes of permutations examined and the corresponding upper bounds derived in [Cor93]. 14 Chapter 2. <p> Subsequently, Cor men <ref> [Cor93] </ref> studied several classes of bit-defined permutations that include matrix transposition as a special case. Table 2.1 shows some of the classes of permutations examined and the corresponding upper bounds derived in [Cor93]. 14 Chapter 2. <p> (bit-matrix-multiply/ complement) nonsingular matrix A 2 ~ lg (M=B) + H (N; M; B) (bit-permute/ complement) permutation matrix A 2 ~ lg (M=B) + 1 (memory rearrangement/ complement) m n m nonsingular arbitrary 0 nonsingular n m Table 2.1: Classes of permutations, their characteristic matrices, and upper bounds shown in <ref> [Cor93] </ref> on the number of passes needed to perform them. A pass consists of reading and writing each record exactly once and therefore uses exactly 2N=BD parallel I/Os. For MRC permutations, submatrix dimensions are shown on matrix borders. <p> For BMMC permutations, r is the rank of the leading lg M fi lg M submatrix of A, and the function H (N; M; B) is given by equation (2.1). For BPC permutations, the function (A) is defined in equation (2.3). BMMC permutations The most general class considered in <ref> [Cor93] </ref> is bit-matrix-multiply/complement, or BMMC, permutations. 1 In a BMMC permutation, we have an n fi n characteristic matrix A = (a ij ) whose entries are drawn from f0; 1g and is nonsingular (i.e., invertible) over GF (2), 2 and we have a complement vector c = (c 0 ; <p> That is, we perform the permutations characterized by the factors of a matrix from right to left. Proof: The proof is a simple induction, using Lemma 2.1. The BMMC algorithm in <ref> [Cor93] </ref> exploits Corollary 2.2 to factor a characteristic matrix into a product of other characteristic matrices, performing the permutations given by the factors right to left. <p> The class of BPC permutations includes many common permutations such as matrix transposition, bit-reversal permutations (used in performing FFTs), vector-reversal permutations, hypercube permutations, and matrix reblocking. Previous work <ref> [Cor93] </ref> expressed the I/O complexity of BPC permutations in terms of cross-ranks. <p> The cross-rank of A is the maximum of the band m-cross-ranks: (A) = max ( b (A); m (A)) : (2:3) The BPC algorithm in <ref> [Cor93] </ref> uses at most 2N ~ lg (M=B) + 1 parallel I/Os. One can adapt the lower bound for BMMC permutations to show that this BPC algorithm is asymptotically optimal. <p> Performing BMMC Permutations on Parallel Disk Systems however, is asymptotically optimal for all BMMC permutations|including those that are BPC|and it reduces the innermost factor of 2 in the above bound to a factor of 1. Not only is the BPC algorithm in <ref> [Cor93] </ref> improved upon by the results in this chapter, but the notion of cross-rank appears to be obviated as well. <p> Cormen <ref> [Cor93] </ref> shows that any MRC permutation requires only one pass of N=BD parallel reads and N=BD parallel writes. If we partition the N records into N=M consecutive sets of M records each, we call each set a memoryload. <p> Any MRC permutation can be performed by reading in a memoryload, permuting its records in memory, and writing them out to a (possibly different) memoryload number. The class of MRC permutations includes those characterized by unit upper triangular matrices. As <ref> [Cor93] </ref> shows, both the standard binary-reflected Gray code and its inverse have characteristic matrices of this form, and so they are MRC permutations. MLD permutations We define here a new BMMC permutation subclass, which we shall use in our asymptotically optimal BMMC algorithm. <p> Section 2.5 presents an algorithm that achieves the bound given by Theorem 2.3, and so this algorithm is asymptotically optimal. The BPC algorithm of <ref> [Cor93] </ref> is asymptotically optimal as well; see [Cor92] for details. 22 Chapter 2. <p> In this way, we need not be concerned with overwriting source records before we get a chance to read them. Note that when we chain passes together, as in the BMMC algorithm of Section 2.5 and the BPC algorithm of <ref> [Cor93] </ref>, we can avoid allocating a new target portion in each pass by reversing the roles of the source and target portions between passes. 26 Chapter 2. Performing BMMC Permutations on Parallel Disk Systems We perform an MLD permutation by processing source memoryload numbers from 0 to N=M 1. <p> It is particularly satisfying that the tight bound was achieved not by raising the lower bound proven in [Cor92, CSW94], but by decreasing the upper bound in <ref> [Cor93] </ref>. The multiplicative and additive constants in the I/O complexity of our algorithm are small, which is especially fortunate in light of the expense of disk accesses. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations <ref> [Cor92, Cor93, CSW94, Wis95] </ref>, mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose <ref> [Cor92, Cor93, CSW94, VS94a] </ref>, FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> Cormen and Bruhl [Bru94, CB95] study the performance of several algorithms for BMMC permutations on the restricted-access MasPar MP-2 global router. Their algorithms adapt the block BMMC algorithms for parallel disk systems with a block size of 1 from <ref> [Cor92, Cor93] </ref> to perform BMMC permutations on the global router when the number of messages in the data set exceeds the number of inputs and outputs. They present algorithms that guarantee that each processor or cluster of processors sends and receives one message at a time.
Reference: [CSW94] <author> Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <type> Technical Report PCS-TR94-223, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1994. </year> <booktitle> Preliminary version appeared in Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures. </booktitle>
Reference-contexts: Parallel disk access. In Chapter 2, we use the linear-algebraic technique to derive an algorithm for performing BMMC permutations on parallel disk systems which is asymptotically optimal in the number of parallel disk accesses <ref> [CSW94] </ref>. This algorithm is well-suited for practical implementation because the constants in the complexity bound are very close to the lower bound. In Chapter 3, we introduce algorithms that perform permutations in place on parallel disk systems [Wis95]. <p> leading lg M fi lg M submatrix of the characteristic matrix and H (N; M; B) = &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; 4 lg B p 4 lg (N=B) p p 5 if N B M : One can adapt the lower bound proven in <ref> [Cor92, CSW94] </ref> to show that N lg Mr parallel I/Os are necessary (see Section 2.8 of [Cor92]), but until the publication of [CSW94], it had been unknown whether the fi ( N BD H (N; M; B)) term is necessary in all cases. <p> &gt; &gt; &gt; &gt; &lt; 4 lg B p 4 lg (N=B) p p 5 if N B M : One can adapt the lower bound proven in [Cor92, CSW94] to show that N lg Mr parallel I/Os are necessary (see Section 2.8 of [Cor92]), but until the publication of <ref> [CSW94] </ref>, it had been unknown whether the fi ( N BD H (N; M; B)) term is necessary in all cases. This chapter, which is based on [CSW94], shows that it is not. 2.1 The Parallel Disk Model and previous results 17 BPC permutations By restricting the characteristic matrix A of <p> in [Cor92, CSW94] to show that N lg Mr parallel I/Os are necessary (see Section 2.8 of [Cor92]), but until the publication of <ref> [CSW94] </ref>, it had been unknown whether the fi ( N BD H (N; M; B)) term is necessary in all cases. This chapter, which is based on [CSW94], shows that it is not. 2.1 The Parallel Disk Model and previous results 17 BPC permutations By restricting the characteristic matrix A of a BMMC permutation to be a permutation matrix|having exactly one 1 in each row and each column|we obtain the class of bit-permute/complement, or BPC, permutations. 3 One <p> We include the proofs here for completeness. 2.2 The lower bound and other useful concepts 21 2.2 The lower bound and other useful concepts In this section, we present the theorem from <ref> [Cor92, CSW94] </ref> that states the lower bound for BMMC permutations. After stating the lower bound, we briefly discuss its significance and several of the important concepts used in its proof that we shall use throughout the remainder of this thesis. <p> We compute the initial potential (0) and final potential (t) and bound the amount ( max ) that the potential can increase in each I/O operation. Thus, we derive the lower bound by computing (t)(0) max . Ranges and preimages In <ref> [Cor92, CSW94] </ref>, the lower bound arguments for BMMC and BPC permutations use the ranges of matrices and preimages of vectors under matrix multiplication. We shall use the notions of ranges and preimages later in this thesis. Here, we reprise the definitions of range and preimage from [Cor92, CSW94] and state some <p> Ranges and preimages In <ref> [Cor92, CSW94] </ref>, the lower bound arguments for BMMC and BPC permutations use the ranges of matrices and preimages of vectors under matrix multiplication. We shall use the notions of ranges and preimages later in this thesis. Here, we reprise the definitions of range and preimage from [Cor92, CSW94] and state some relevant lemmas. <p> It is particularly satisfying that the tight bound was achieved not by raising the lower bound proven in <ref> [Cor92, CSW94] </ref>, but by decreasing the upper bound in [Cor93]. The multiplicative and additive constants in the I/O complexity of our algorithm are small, which is especially fortunate in light of the expense of disk accesses. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations <ref> [Cor92, Cor93, CSW94, Wis95] </ref>, mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose <ref> [Cor92, Cor93, CSW94, VS94a] </ref>, FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM.
Reference: [dBC93] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In Proceedings of the IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year> <note> Shortened version published in Computer Architecture News, </note> <month> December </month> <year> 1993. </year>
Reference-contexts: At the software 78 Chapter 4. An API for Choreographing Data Accesses level, considerable effort has been made to develop new languages and compiler features that support I/O parallelism and optimizations via data layout conversion <ref> [dBC93] </ref>, compiler hints [PGS93], and preprocessing for out-of-core parallel code [CBH + 94, CC94]. Another approach integrates special enhancements for I/O into the file system [CBF93, KS93].
Reference: [DHR94] <author> J.R. Driscoll, D. Healy, and D. Rockmore. </author> <title> Fast spherical transforms for distance transitive graphs. </title> <type> Technical Report Technical Report PCS-TR94-223, </type> <institution> Dartmouth College Department of Mathematics and Computer Science, </institution> <year> 1994. </year>
Reference-contexts: Even though BMMC permutations form a small subset of all permutations, they include commonly used permutations such as matrix transposition, bit-reversal permutations (used in performing FFTs), vector-reversal permutations, hypercube permutations, and matrix reblocking. Moreover, many transform algorithms used by signal-processing applications, such as the Fast Fourier Transform (FFT) <ref> [DHR94] </ref>, the Fast Cosine Transform (FCT) [Moo94, MW95], and the wavelet packet transform [CH96], perform a BMMC permutation before or after the computation. Weather modeling and data compression/decompression techniques use these various transforms. Vitter and Vengroff use BMMC permutations as a subroutine in performing out-of-core matrix-matrix multiply [VV95]. <p> We refer the interested reader to the original paper [ST91] for more specific details, or to an expanded discussion in [Moo94]. The polynomial division tree method The fast cosine transform algorithm uses the polynomial division tree method for evaluating a polynomial function <ref> [BM75, DHR94, Knu81, Moo94] </ref>. In the polynomial 8.1 Fast cosine transforms 183 division tree model, we represent a function as a polynomial p (z). <p> Derivation of the OUR permutation The derivation of the odd-upper/bit-reversal (OUR) permutation arises naturally when we consider the computation of a fast cosine transform as a fast Fourier transform, or more generally as a fast discrete monomial transform <ref> [DHR94, MHR93] </ref>.
Reference: [EHJ94] <author> Alan Edelman, Steve Heller, and S. Lennart Johnsson. </author> <title> Index transformation algorithms in a linear algebra framework. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(12) </volume> <pages> 1302-1309, </pages> <month> December </month> <year> 1994. </year> <note> 218 Bibliography </note>
Reference-contexts: Treating a source address x as an n-bit vector, we perform matrix-vector multiplication over GF (2) and then form the corresponding n-bit target address y 1 Edelman, Heller, and Johnsson <ref> [EHJ94] </ref> call BMMC permutations affine transformations or, if there is no complementing, linear transformations. 2 Matrix multiplication over GF (2) is like standard matrix multiplication over the reals but with all arithmetic performed modulo 2.
Reference: [EHKM94] <author> Christopher L. Elford, Jay Huber, Chris Kuszmaul, and Tara Mad-hyastha. </author> <title> Portable parallel file system detailed design. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> Septem-ber </month> <year> 1994. </year>
Reference-contexts: Unfortunately, none of these run-time libraries include routines for simultaneous, direct access to the disks. The API of PPFS provides routines that support independent access at the record level, not at the disk level <ref> [EHKM94, HER + 95] </ref>. This API provides multiple-offset routines that allow the user to access more than one range of data in a single 88 Chapter 4.
Reference: [ER82] <author> D.F. Elliot and K.R. Rao. </author> <title> Fast Transforms: Algorithms, Analyses, Applications. </title> <publisher> Academic Press, </publisher> <address> Orlando, </address> <year> 1982. </year>
Reference-contexts: Chapter 8 Complexity Analysis of BMMC Permutations for FCTs Fast cosine transform (FCT) algorithms are essential to efficient computation in many applications such as weather modeling, data compression/decompression, and convolutions on real, symmetric data <ref> [Chi95, ER82, RY90] </ref>. Recent descriptions of fast cosine transform algorithms [ST91, Ste92] perform a record minimum number of multiplication and addition operations to compute an FCT. These new algorithms have been derived using the polynomial-division-tree computational model. These algorithms include a permutation before or after the computation of the transform.
Reference: [Flo72] <author> Robert W. Floyd. </author> <title> Permuting information in idealized two-level storage. </title> <editor> In Raymond E. Miller and James W. Thatcher, editors, </editor> <booktitle> Complexity of Computer Computations, </booktitle> <pages> pages 105-109. </pages> <publisher> Plenum Press, </publisher> <year> 1972. </year>
Reference-contexts: Performing BMMC Permutations on Parallel Disk Systems Technique The proof of Theorem 2.3 relies heavily on the technique used by Aggarwal and Vitter [AV88] for a lower bound on I/Os in matrix transposition; their proof is based in turn on a method by Floyd <ref> [Flo72] </ref>. The basic scheme of the proof of Theorem 2.3 uses a potential-function argument. Time q is the time interval starting when the qth I/O completes and ending just before the (q + 1)st I/O starts. We define a potential function so that (q) is the potential at time q. <p> An API for Choreographing Data Accesses 4.1 Background Researchers have developed theoretical models to capture important features of data movement between main memory and secondary storage <ref> [AV88, Flo72, VS90, VS94a] </ref>. In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94a] provides a reasonable model for the design of I/O-efficient algorithms, such as those shown in Chapters 2 and 3, and analysis at a feasible level of abstraction.
Reference: [FMP95] <author> Faith E. Fich, J. Ian Munro, and Patricio V. Poblete. </author> <title> Permuting in place. </title> <journal> SIAM Journal on Computing, </journal> <volume> 24(2) </volume> <pages> 266-278, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: We also do not want the additional time to determine which disk blocks to read and write to be significant. Several in-place algorithms have been developed with certain limitations. Fich, Munro and Poblete <ref> [FMP95] </ref> provide in-place algorithms to perform general permutations in memory. These algorithms, however, do not apply to permuting data sets that exceed the size of memory. Vitter and Shriver did not design their out-of-core algorithm for general permuting to be performed in place. <p> The only unresolved problem is to find the disjoint cycles without rotating any disjoint cycle more than once. Fich et al. <ref> [FMP95] </ref> provide several algorithms to perform an arbitrary permutation in place when the entire data set fits into memory. The following theorem from [FMP95] reflects the tradeoff between time and additional space when permuting an array of length p in memory when an extra q bits of storage are available. <p> The only unresolved problem is to find the disjoint cycles without rotating any disjoint cycle more than once. Fich et al. <ref> [FMP95] </ref> provide several algorithms to perform an arbitrary permutation in place when the entire data set fits into memory. The following theorem from [FMP95] reflects the tradeoff between time and additional space when permuting an array of length p in memory when an extra q bits of storage are available. <p> In this context, we show how to perform a permutation on the p memoryload numbers, where p = N=M . We can adapt the simplest algorithm from <ref> [FMP95] </ref>, 60 Chapter 3. Structured Permuting in Place on Parallel Disk Systems which uses q = N=M bits of extra space to rotate each memoryload exactly once. <p> By Theorem 3.1, the additional computation time is O (N=M ), which is a constant number of operations per memoryload. Since we can easily generate the inverse permutation from the characteristic matrix, we can also adapt the more complex algorithms in <ref> [FMP95] </ref> to rotate the memoryloads when N &gt;> M , using O (log (N=M )) extra bits of storage and O M log (N=M ) time. For simplicity of presentation, we adapt the algorithm that uses N=M extra bits of storage.
Reference: [GS95] <author> Garth Gibson and Daniel Stodolsky. </author> <title> Issues arising in the SIO-OS low-level PFS API. </title> <booktitle> Presentation at the Scalable Input/Output for High Performance Computers Workshop at the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: A number of existing file systems provide low-level API support for simultaneous, independent, direct access to the multiple disks provided by modern machine architectures. These APIs include extensions to the conventional Unix interface (e.g., [NK95a, NK95b]), modifications to the conventional Unix interface (e.g., <ref> [GS95] </ref>), and other interfaces which differ from Unix (e.g., [CFF + 95, CFPB93]). An I/O-efficient algorithm would not be easy to program using these low-level file system APIs since the programmer must map the high-level parallel disk accesses to low-level file system operations.
Reference: [GTVV93] <author> Michael T. Goodrich, Jyh-Jong Tsay, Darren E. Vengroff, and Jef-frey Scott Vitter. </author> <title> External-memory computational geometry. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 714-723, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms <ref> [AVV95, GTVV93] </ref>. In this section, we define the parameters and data layout for PDM. We also describe the types of data access needed by PDM algorithms and discuss how well existing file systems support these types of data access.
Reference: [HER + 95] <author> Jay Huber, Christopher L. Elford, Daniel A. Reed, Andrew A. Chien, and David S. Blumenthal. </author> <title> PPFS: A high performance portable parallel file system. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394, </pages> <address> Barcelona, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Unfortunately, none of these run-time libraries include routines for simultaneous, direct access to the disks. The API of PPFS provides routines that support independent access at the record level, not at the disk level <ref> [EHKM94, HER + 95] </ref>. This API provides multiple-offset routines that allow the user to access more than one range of data in a single 88 Chapter 4.
Reference: [JH91] <author> S. Lennart Johnsson and Ching-Tien Ho. </author> <title> Generalized shu*e permutations on boolean cubes. </title> <type> Technical Report TR-04-91, </type> <institution> Harvard University Center for Research in Computing Technology, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: One can adapt the lower bound for BMMC permutations to show that this BPC algorithm is asymptotically optimal. The BMMC algorithm in Section 2.5, 3 Johnsson and Ho <ref> [JH91] </ref> call BPC permutations dimension permutations, and Aggarwal, Chan dra, and Snir [ACS87] call BPC permutations without complementing rational permutations. 18 Chapter 2.
Reference: [Kar93] <author> Alan H. Karp. </author> <title> Bit Reversal on Uniprocessors. </title> <type> Technical Report HPL-93-89, </type> <institution> Hewlett-Packard Laboratories, 1501 Page Mill Road, </institution> <address> Palo Alto, CA 94304, </address> <month> October </month> <year> 1993. </year> <note> Scheduled for SIAM Review, </note> <month> March </month> <year> 1996. </year>
Reference-contexts: With such a model, we can directly adapt algorithms developed on PDM to determine efficient data access patterns between main memory and cache. Karp <ref> [Kar93] </ref> provides a survey of existing algorithms for performing the bit-reversal permutation on uniprocessors. Many FFT algorithms use the bit-reversal permutation, which is a BMMC permutation, before or after the computation of the FFT. In his study, Karp implements over 30 previously published bit-reversal algorithms on machines with various architectures. <p> Thus, there are C=L lines of cache and M=C cacheloads. We illustrate the layout of a cacheload of data in Figure 7.1. For this discussion, we make the further restriction that L C=L. Karp <ref> [Kar93] </ref> states that cache lines are typically small, i.e., between 32 to 128 bytes, and that caches are typically 8 KBytes to 256 KBytes. <p> Alan Karp <ref> [Kar93] </ref> writes The idea that the memory structure is an important factor in determining the performance of the bit reversal algorithm is not new [Sin67]. However, this fact seems to have been forgotten in the 25 years since its first publication; few of the methods described in this paper ([Kar93]) consider
Reference: [Kau68] <author> W.H. Kautz. </author> <title> Cellular interconnection arrays. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-17:443-451, </volume> <month> May </month> <year> 1968. </year>
Reference-contexts: Since any arbitrary permutation can be routed through a multistage network if multiple passes are allowed, we say that the above multistage networks are universal. Waksman [Wak68] and Kautz <ref> [Kau68] </ref> describe how to perform an arbitrary permutation of N elements using basic 2-permuters. Parker [Par80] shows a one-pass algorithm for an arbitrary permutation on the three-stage Benes network and also presents a three-pass algorithm on the network.
Reference: [Kim86] <author> Michelle Y. Kim. </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(11):978-988, </volume> <month> November </month> <year> 1986. </year> <note> Bibliography 219 </note>
Reference-contexts: A number of approaches can increase I/O throughput, varying from hardware improvements to better algorithms. The hardware solutions include methods to improve the rate of I/O throughput to uniprocessor systems by introducing parallelism into the I/O subsystem. Mechanisms such as disk striping or interleaving <ref> [Kim86, SGM86] </ref>, and RAID [PGK88] have achieved fine-grain parallelism at the physical disk level. At the software 78 Chapter 4.
Reference: [Knu81] <author> D. Knuth. </author> <booktitle> The Art of Computer Programming, Volume 2: Seminu-merical Algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1981. </year>
Reference-contexts: We refer the interested reader to the original paper [ST91] for more specific details, or to an expanded discussion in [Moo94]. The polynomial division tree method The fast cosine transform algorithm uses the polynomial division tree method for evaluating a polynomial function <ref> [BM75, DHR94, Knu81, Moo94] </ref>. In the polynomial 8.1 Fast cosine transforms 183 division tree model, we represent a function as a polynomial p (z).
Reference: [KS83] <author> C.K. Kruskal and M. Snir. </author> <title> The performance of multistage interconnection networks for multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-32(12), </volume> <month> December </month> <year> 1983. </year>
Reference-contexts: Many multistage interconnection networks have been proposed, such as the network [Law75], the indirect binary n-cube [Pea77], the Clos network [Clo53], the Benes network [Ben65], the delta network [Pat81], and the expanded delta network (EDN) <ref> [KS83] </ref>. A multistage interconnection network consists of alternating stages of switches and wires. The switches are typically crossbars connected by wires, usually in a pattern based on the perfect shu*e [Sto71]. The above networks are equivalent, topologically and functionally [Pea77, Sie77]. Researchers have developed efficient algorithms on these multistage networks. <p> Finally, we show how to trace the permutations that occur at each stage of switches and wires to verify that each pass through the EDN performs the permutation specified by the algorithm. Expanded delta networks We construct an expanded delta network (EDN) <ref> [KS83] </ref> with wires and switches. The two kinds of wires in an EDN are thin wires and thick wires. A thin wire carries a single message, whereas each thick wire can carry up to K messages. The three types of switches are thin-to-thick converters, thick-to-thin converters, and hyperbars.
Reference: [KS88] <author> John Keohane and Richard E. Stearns. </author> <title> Routing linear permutations through the Omega network in two passes. </title> <type> Technical Report 88-14, </type> <institution> Department of Computer Science, University of Albany, State University of New York, </institution> <year> 1988. </year> <booktitle> Also appeared in the Proceedings of the 2nd Symposium on the Frontiers of Massively Parallel Computing. </booktitle>
Reference-contexts: For example, several papers have presented Chapter 6. Performing BMMC Permutations on Multistage Networks 139 algorithms for routing BMMC (bit-matrix-multiply/complement) permutations and its subclass of BPC (bit-permute/complement) permutations. Pease [Pea77] shows how to perform BMMC permutations in two passes on the indirect binary n-cube. Keohane and Stearns <ref> [KS88] </ref> give an algorithm for performing linear permutations (BMMC permutations without the complement) in two passes on the network. Yew and Lawrie [YL81] present a two-pass algorithm for BPC permutations on the network, and Nassimi and Sahni [NS81] give an algorithm for BPC permutations on a Benes network. <p> On the last switching stage, if necessary, we complement any of the least significant k bits. Therefore, after we complete the last switching stage, the accumulator vector will equal the complement vector. Keohane and Stearns <ref> [KS88] </ref> derived a two-pass algorithm for performing linear permutations on the network, but they did not use the linear-algebraic technique above.
Reference: [KS93] <author> Orran Krieger and Michael Stumm. </author> <title> HFS: A flexible file system for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 6-14, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Another approach integrates special enhancements for I/O into the file system <ref> [CBF93, KS93] </ref>.
Reference: [Law75] <author> D. H. Lawrie. </author> <title> Access and alignment of data in an array processor. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-24:1145-1155, </volume> <month> December </month> <year> 1975. </year>
Reference-contexts: Performing BMMC Permutations on Multistage Networks Previous work on multistage networks Multistage networks comprise another class of interconnection networks for parallel computers. Many multistage interconnection networks have been proposed, such as the network <ref> [Law75] </ref>, the indirect binary n-cube [Pea77], the Clos network [Clo53], the Benes network [Ben65], the delta network [Pat81], and the expanded delta network (EDN) [KS83]. A multistage interconnection network consists of alternating stages of switches and wires.
Reference: [MHR93] <author> S.S.B. Moore, D.M. Healy, and D.N. Rockmore. </author> <title> Symmetry stabilization for fast discrete monomial transforms and polynomial evaluation. Linear Algebra and its Applications Special Issue on Computational Linear Algebra in Algebraic and Related Problems, </title> <booktitle> 192 </booktitle> <pages> 249-299, </pages> <month> Octo-ber </month> <year> 1993. </year>
Reference-contexts: Derivation of the OUR permutation The derivation of the odd-upper/bit-reversal (OUR) permutation arises naturally when we consider the computation of a fast cosine transform as a fast Fourier transform, or more generally as a fast discrete monomial transform <ref> [DHR94, MHR93] </ref>. <p> Derivation of the OUR permutation The derivation of the odd-upper/bit-reversal (OUR) permutation arises naturally when we consider the computation of a fast cosine transform as a fast Fourier transform, or more generally as a fast discrete monomial transform [DHR94, MHR93]. It is shown in <ref> [MHR93] </ref> and elsewhere that for a real-valued sequence f = f (0); f (1); : : : ; f (N 1) generated by evaluating a function at N points, we can compute the discrete cosine transform b f cos of f as the real part of the discrete Fourier transform b <p> If the evaluation points on the unit circle are not uniformly spaced or we do not apply an appropriate permutation matrix, then the fast Fourier transform uses O (N lg 2 N ) operations <ref> [MHR93] </ref>.
Reference: [MMRW94] <author> Arthur B. Maccabe, Kevin S. McCurley, Rolf Riesen, and Stephen R. Wheat. </author> <title> SUNMOS for the Intel Paragon: A brief user's guide. </title> <booktitle> In Proceedings of the Intel Supercomputer Users Group Conference, </booktitle> <pages> pages 245-251, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: We performed our experiments on a power-of-2 number of RAIDs, up to 32 RAIDs. The interconnection network is a 16 fi 120 mesh; the I/O nodes comprise three columns of this mesh. The operating system on the Paragon for the experiments was SUNMOS (Sandia/University of New Mexico Operating System) <ref> [MMRW94] </ref>. We assume that each processor has 8 Mbytes of main memory available for data. We layout the records in a file on WFS as shown in Figure 4.2. WFS stripes the blocks of a single file across D separate files, one on each RAID.
Reference: [Moo94] <author> S.S.B. Moore. </author> <title> Efficient Stabilization Methods for Fast Polynomial Transforms. </title> <type> PhD thesis, </type> <institution> Department of Mathematics and Computer Science, Dartmouth College, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Moreover, many transform algorithms used by signal-processing applications, such as the Fast Fourier Transform (FFT) [DHR94], the Fast Cosine Transform (FCT) <ref> [Moo94, MW95] </ref>, and the wavelet packet transform [CH96], perform a BMMC permutation before or after the computation. Weather modeling and data compression/decompression techniques use these various transforms. Vitter and Vengroff use BMMC permutations as a subroutine in performing out-of-core matrix-matrix multiply [VV95]. <p> We transpose all the matrices for the inverse FCT algorithm to obtain the forward FCT algorithm. We refer the interested reader to the original paper [ST91] for more specific details, or to an expanded discussion in <ref> [Moo94] </ref>. The polynomial division tree method The fast cosine transform algorithm uses the polynomial division tree method for evaluating a polynomial function [BM75, DHR94, Knu81, Moo94]. In the polynomial 8.1 Fast cosine transforms 183 division tree model, we represent a function as a polynomial p (z). <p> We refer the interested reader to the original paper [ST91] for more specific details, or to an expanded discussion in [Moo94]. The polynomial division tree method The fast cosine transform algorithm uses the polynomial division tree method for evaluating a polynomial function <ref> [BM75, DHR94, Knu81, Moo94] </ref>. In the polynomial 8.1 Fast cosine transforms 183 division tree model, we represent a function as a polynomial p (z). <p> x, the permutation is t (x) = x (0) ; x (2) ; : : : ; x (N2) ; x (N1) ; x (N3) ; : : : ; x (3) ; x (1) : If we compose the bit-reversal permutation with t (x) 4 and apply U , <ref> [Moo94] </ref> shows that the ordering of the elements on the unit circle produces the sparse matrices needed to compute a fast discrete Fourier transform and its associated discrete cosine transform using O (N lg N) operations. An example for N = 8 may be helpful.
Reference: [MW95] <author> Sean S.B. Moore and Leonard F. Wisniewski. </author> <title> Complexity analysis of two permutations used by fast cosine transform algorithms. </title> <type> Technical Report PCS-TR95-266, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: Moreover, many transform algorithms used by signal-processing applications, such as the Fast Fourier Transform (FFT) [DHR94], the Fast Cosine Transform (FCT) <ref> [Moo94, MW95] </ref>, and the wavelet packet transform [CH96], perform a BMMC permutation before or after the computation. Weather modeling and data compression/decompression techniques use these various transforms. Vitter and Vengroff use BMMC permutations as a subroutine in performing out-of-core matrix-matrix multiply [VV95]. <p> Registers - Special-purpose hardware. In Chapter 8, we use the linear-algebraic technique to perform analysis on two permutations used by fast cosine transform (FCT) algorithms <ref> [MW95] </ref>. We show how to perform the index mapping for the permutations using constant-depth, logarithmic-width combinational circuits.
Reference: [NK95a] <author> Nils Nieuwejaar and David Kotz. </author> <title> Low-level interfaces for high-level parallel I/O. </title> <booktitle> In Proceedings of IPPS '95 Workshop on I/O in Parallel and Distributed Systems, </booktitle> <month> April </month> <year> 1995. </year> <note> 220 Bibliography </note>
Reference-contexts: A number of existing file systems provide low-level API support for simultaneous, independent, direct access to the multiple disks provided by modern machine architectures. These APIs include extensions to the conventional Unix interface (e.g., <ref> [NK95a, NK95b] </ref>), modifications to the conventional Unix interface (e.g., [GS95]), and other interfaces which differ from Unix (e.g., [CFF + 95, CFPB93]).
Reference: [NK95b] <author> Nils Nieuwejaar and David Kotz. </author> <title> A multiprocessor extension to the conventional file system interface. </title> <type> Technical Report PCS-TR95-253, </type> <institution> Dartmouth College, </institution> <year> 1995. </year>
Reference-contexts: A number of existing file systems provide low-level API support for simultaneous, independent, direct access to the multiple disks provided by modern machine architectures. These APIs include extensions to the conventional Unix interface (e.g., <ref> [NK95a, NK95b] </ref>), modifications to the conventional Unix interface (e.g., [GS95]), and other interfaces which differ from Unix (e.g., [CFF + 95, CFPB93]).
Reference: [NS80] <author> D. Nassimi and S. Sahni. </author> <title> An optimal routing algorithm for mesh-connected parallel computers. </title> <journal> Journal of the ACM, </journal> <volume> 27(1) </volume> <pages> 6-29, </pages> <month> Jan-uary </month> <year> 1980. </year>
Reference-contexts: Previous theoretical work has focused on developing algorithms to perform BMMC permutations and its subclass of BPC (bit-permute/complement) permutations on a SIMD mesh <ref> [NS80, Sib90a, Sib90b, Sib92] </ref>. The developers of these algorithms show bounds on the complexity of performing the interprocessor communication for BMMC and BPC permutations in terms of routing operations.
Reference: [NS81] <author> David Nassimi and Sartaj Sahni. </author> <title> A self-routing Benes network and parallel permutation algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-30(5):332-340, </volume> <month> May </month> <year> 1981. </year>
Reference-contexts: Keohane and Stearns [KS88] give an algorithm for performing linear permutations (BMMC permutations without the complement) in two passes on the network. Yew and Lawrie [YL81] present a two-pass algorithm for BPC permutations on the network, and Nassimi and Sahni <ref> [NS81] </ref> give an algorithm for BPC permutations on a Benes network. Shen [She95] shows how to perform any BPC permutation on a k-extra-stage network in a minimum number of passes. Scherson and Subrama-nian's general permutation algorithm [SS93] only requires two passes on a certain class of EDNs.
Reference: [NV93] <author> Mark H. Nodine and Jeffrey Scott Vitter. </author> <title> Deterministic distribution sort in shared and distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 120-129, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: the record's memoryload number, and bits b; b + 1; : : :; m 1 form the relative block number, used in Section 2.3. second term is the sorting bound fi BD lg (M=B) , which was shown by Vitter and Shriver for randomized sorting and by Nodine and Vitter <ref> [NV93] </ref> for deterministic sorting. These bounds are asymptotically tight, for they match the lower bounds proven earlier by Aggarwal and Vitter [AV88] using a model with one disk and D independent read/write heads, which is at least as powerful as the Vitter-Shriver model. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting <ref> [AP94, Arg95, NV93, VS94a] </ref>, general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93].
Reference: [OT71] <author> D.C. Opferman and N.T. Tsao-Wu. </author> <title> On a class of rearrangeable switching networks. </title> <journal> Bell Systems Technical Journal, </journal> <volume> 50 </volume> <pages> 1579-1618, </pages> <month> May-June </month> <year> 1971. </year>
Reference-contexts: The overall flavor is similar to that for the EDN: we couch each switching or wiring stage as a BMMC permutation and decompose the characteristic matrix of the permutation to be routed into factors corresponding to these stages. Decomposing the characteristic matrix Previous work by Opferman and Tsao-Wu <ref> [OT71] </ref> and Ramanujam [Ram73] indicates that we can perform BMMC permutations in two passes through the global router of the MasPar MP-2.
Reference: [Par80] <author> D. Stott Parker Jr. </author> <title> Notes on shu*e/exchange type switching networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-29(3):213-222, </volume> <month> March </month> <year> 1980. </year>
Reference-contexts: Since any arbitrary permutation can be routed through a multistage network if multiple passes are allowed, we say that the above multistage networks are universal. Waksman [Wak68] and Kautz [Kau68] describe how to perform an arbitrary permutation of N elements using basic 2-permuters. Parker <ref> [Par80] </ref> shows a one-pass algorithm for an arbitrary permutation on the three-stage Benes network and also presents a three-pass algorithm on the network. Scherson and Subramanian [SS93] give an off-line algorithm to route an arbitrary permutation in three passes on a restricted-access EDN.
Reference: [Pat81] <author> J.H. Patel. </author> <title> Performance of processor-memory interconnections for multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-30(10):771-780, </volume> <month> Oc-tober </month> <year> 1981. </year>
Reference-contexts: Many multistage interconnection networks have been proposed, such as the network [Law75], the indirect binary n-cube [Pea77], the Clos network [Clo53], the Benes network [Ben65], the delta network <ref> [Pat81] </ref>, and the expanded delta network (EDN) [KS83]. A multistage interconnection network consists of alternating stages of switches and wires. The switches are typically crossbars connected by wires, usually in a pattern based on the perfect shu*e [Sto71]. The above networks are equivalent, topologically and functionally [Pea77, Sie77].
Reference: [Pea77] <author> Marshall C. Pease. </author> <title> The indirect binary n-cube microprocessor array. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-26(5):458-473, </volume> <month> May </month> <year> 1977. </year>
Reference-contexts: Sibyn developed a number of algorithms to perform BMMC permutations on a mesh. He used basic routing operations called bit complementations to derive his algorithms. Each bit complementation requires several routing steps. On a SIMD mesh, Sibyn simulates the algorithm for performing BMMC permutations on an indirect binary n-cube <ref> [Pea77] </ref>. This simulation requires 7n 8 routing steps. This algorithm uses only invertible bit complementations, i.e., bit complementations in which every processor begins and ends each routing step with one record. Sibyn also shows how to eliminate "expensive" operations by rearranging the order of the bit complementations. <p> Performing BMMC Permutations on Multistage Networks Previous work on multistage networks Multistage networks comprise another class of interconnection networks for parallel computers. Many multistage interconnection networks have been proposed, such as the network [Law75], the indirect binary n-cube <ref> [Pea77] </ref>, the Clos network [Clo53], the Benes network [Ben65], the delta network [Pat81], and the expanded delta network (EDN) [KS83]. A multistage interconnection network consists of alternating stages of switches and wires. The switches are typically crossbars connected by wires, usually in a pattern based on the perfect shu*e [Sto71]. <p> A multistage interconnection network consists of alternating stages of switches and wires. The switches are typically crossbars connected by wires, usually in a pattern based on the perfect shu*e [Sto71]. The above networks are equivalent, topologically and functionally <ref> [Pea77, Sie77] </ref>. Researchers have developed efficient algorithms on these multistage networks. A pass through the network is a rearrangement in which some subset of the data simultaneously moves through the network to a destination. <p> Several algorithms appear in the literature for routing certain classes of permutations in fewer than three passes. For example, several papers have presented Chapter 6. Performing BMMC Permutations on Multistage Networks 139 algorithms for routing BMMC (bit-matrix-multiply/complement) permutations and its subclass of BPC (bit-permute/complement) permutations. Pease <ref> [Pea77] </ref> shows how to perform BMMC permutations in two passes on the indirect binary n-cube. Keohane and Stearns [KS88] give an algorithm for performing linear permutations (BMMC permutations without the complement) in two passes on the network.
Reference: [PGK88] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 109-116, </pages> <address> Chicago, IL, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: A number of approaches can increase I/O throughput, varying from hardware improvements to better algorithms. The hardware solutions include methods to improve the rate of I/O throughput to uniprocessor systems by introducing parallelism into the I/O subsystem. Mechanisms such as disk striping or interleaving [Kim86, SGM86], and RAID <ref> [PGK88] </ref> have achieved fine-grain parallelism at the physical disk level. At the software 78 Chapter 4.
Reference: [PGS93] <author> R. H. Patterson, G. A. Gibson, and M. Satyanarayanan. </author> <title> Informed prefetching: Converting high throughput to low latency. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: At the software 78 Chapter 4. An API for Choreographing Data Accesses level, considerable effort has been made to develop new languages and compiler features that support I/O parallelism and optimizations via data layout conversion [dBC93], compiler hints <ref> [PGS93] </ref>, and preprocessing for out-of-core parallel code [CBH + 94, CC94]. Another approach integrates special enhancements for I/O into the file system [CBF93, KS93].
Reference: [Ram73] <author> H.R. Ramanujam. </author> <title> Decomposition of permutation networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-22(7):639-643, </volume> <month> July </month> <year> 1973. </year> <note> Bibliography 221 </note>
Reference-contexts: Decomposing the characteristic matrix Previous work by Opferman and Tsao-Wu [OT71] and Ramanujam <ref> [Ram73] </ref> indicates that we can perform BMMC permutations in two passes through the global router of the MasPar MP-2.
Reference: [RY90] <author> K.R. Rao and P. Yip. </author> <title> Discrete Cosine Transforms: Algorithms, Advantages, Applications. </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1990. </year>
Reference-contexts: Chapter 8 Complexity Analysis of BMMC Permutations for FCTs Fast cosine transform (FCT) algorithms are essential to efficient computation in many applications such as weather modeling, data compression/decompression, and convolutions on real, symmetric data <ref> [Chi95, ER82, RY90] </ref>. Recent descriptions of fast cosine transform algorithms [ST91, Ste92] perform a record minimum number of multiplication and addition operations to compute an FCT. These new algorithms have been derived using the polynomial-division-tree computational model. These algorithms include a permutation before or after the computation of the transform.
Reference: [SCJ + 95] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: At a higher level of abstraction, run-time libraries achieve I/O-performance improvements on multiple disk systems, but typically lose the direct disk access in the abstraction. The Panda run-time library <ref> [SCJ + 95, SW94] </ref> achieves performance improvements by providing an API and more efficient layout alternatives for multidimensional array data. This approach takes advantage of the spatial and temporal locality of the array.
Reference: [SGM86] <author> Kenneth Salem and Hector Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In IEEE 1986 Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference-contexts: A number of approaches can increase I/O throughput, varying from hardware improvements to better algorithms. The hardware solutions include methods to improve the rate of I/O throughput to uniprocessor systems by introducing parallelism into the I/O subsystem. Mechanisms such as disk striping or interleaving <ref> [Kim86, SGM86] </ref>, and RAID [PGK88] have achieved fine-grain parallelism at the physical disk level. At the software 78 Chapter 4.
Reference: [She95] <author> Xiaojun Shen. </author> <title> Optimal realization of any BPC permutation on k-extra-stage Omega networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(5) </volume> <pages> 714-719, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Yew and Lawrie [YL81] present a two-pass algorithm for BPC permutations on the network, and Nassimi and Sahni [NS81] give an algorithm for BPC permutations on a Benes network. Shen <ref> [She95] </ref> shows how to perform any BPC permutation on a k-extra-stage network in a minimum number of passes. Scherson and Subrama-nian's general permutation algorithm [SS93] only requires two passes on a certain class of EDNs.
Reference: [Sib90a] <author> J.F. Sibyn. </author> <title> Matrix techniques for faster routing of affine permutations on a mesh interconnection network. </title> <type> Technical Report RUU-CS-90-19, </type> <institution> Department of Computer Science, Utrecht University, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Previous theoretical work has focused on developing algorithms to perform BMMC permutations and its subclass of BPC (bit-permute/complement) permutations on a SIMD mesh <ref> [NS80, Sib90a, Sib90b, Sib92] </ref>. The developers of these algorithms show bounds on the complexity of performing the interprocessor communication for BMMC and BPC permutations in terms of routing operations. <p> We also discuss the limitations of these algorithms that make them impractical for implementation on a MIMD mesh. Sibyn <ref> [Sib90a, Sib90b, Sib92] </ref> has developed several algorithms for BMMC per mutations on a SIMD mesh. These algorithms assume that data has been laid out 5.2 Previous BMMC algorithms for SIMD meshes 121 on a N fi N mesh.
Reference: [Sib90b] <author> Jop F. Sibyn. </author> <title> Routing on a mesh interconnection network. </title> <type> Technical Report A 18/90, </type> <institution> Universitat des Saarlandes, </institution> <year> 1990. </year>
Reference-contexts: Previous theoretical work has focused on developing algorithms to perform BMMC permutations and its subclass of BPC (bit-permute/complement) permutations on a SIMD mesh <ref> [NS80, Sib90a, Sib90b, Sib92] </ref>. The developers of these algorithms show bounds on the complexity of performing the interprocessor communication for BMMC and BPC permutations in terms of routing operations. <p> We also discuss the limitations of these algorithms that make them impractical for implementation on a MIMD mesh. Sibyn <ref> [Sib90a, Sib90b, Sib92] </ref> has developed several algorithms for BMMC per mutations on a SIMD mesh. These algorithms assume that data has been laid out 5.2 Previous BMMC algorithms for SIMD meshes 121 on a N fi N mesh.
Reference: [Sib92] <author> Jop Frederik Sibyn. </author> <title> Algorithms for Routing on Meshes. </title> <type> PhD thesis, </type> <institution> Utrecht University, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Previous theoretical work has focused on developing algorithms to perform BMMC permutations and its subclass of BPC (bit-permute/complement) permutations on a SIMD mesh <ref> [NS80, Sib90a, Sib90b, Sib92] </ref>. The developers of these algorithms show bounds on the complexity of performing the interprocessor communication for BMMC and BPC permutations in terms of routing operations. <p> We also discuss the limitations of these algorithms that make them impractical for implementation on a MIMD mesh. Sibyn <ref> [Sib90a, Sib90b, Sib92] </ref> has developed several algorithms for BMMC per mutations on a SIMD mesh. These algorithms assume that data has been laid out 5.2 Previous BMMC algorithms for SIMD meshes 121 on a N fi N mesh.
Reference: [Sie77] <author> Howard Jay Siegel. </author> <title> Analysis techniques for SIMD machine interconnection networks and the effects of processor address masks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-26:153-161, </volume> <month> February </month> <year> 1977. </year>
Reference-contexts: A multistage interconnection network consists of alternating stages of switches and wires. The switches are typically crossbars connected by wires, usually in a pattern based on the perfect shu*e [Sto71]. The above networks are equivalent, topologically and functionally <ref> [Pea77, Sie77] </ref>. Researchers have developed efficient algorithms on these multistage networks. A pass through the network is a rearrangement in which some subset of the data simultaneously moves through the network to a destination.
Reference: [Sin67] <author> R.C. </author> <title> Singleton. A method for computing the Fast Fourier Transform with auxiliary memory and limited high-speed storage. </title> <journal> IEEE Transactions of Audio Electroacoustics, </journal> <volume> AU-15:91-97, </volume> <year> 1967. </year>
Reference-contexts: Alan Karp [Kar93] writes The idea that the memory structure is an important factor in determining the performance of the bit reversal algorithm is not new <ref> [Sin67] </ref>. However, this fact seems to have been forgotten in the 25 years since its first publication; few of the methods described in this paper ([Kar93]) consider the memory structure of the machines.
Reference: [SS93] <author> Isaac D. Scherson and Raghu Subramanian. </author> <title> Efficient off-line routing of permutations on restricted access expanded delta networks. </title> <booktitle> In Proceedings of the 7th International Parallel Processing Symposium, </booktitle> <pages> pages 284-290, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Waksman [Wak68] and Kautz [Kau68] describe how to perform an arbitrary permutation of N elements using basic 2-permuters. Parker [Par80] shows a one-pass algorithm for an arbitrary permutation on the three-stage Benes network and also presents a three-pass algorithm on the network. Scherson and Subramanian <ref> [SS93] </ref> give an off-line algorithm to route an arbitrary permutation in three passes on a restricted-access EDN. <p> Shen [She95] shows how to perform any BPC permutation on a k-extra-stage network in a minimum number of passes. Scherson and Subrama-nian's general permutation algorithm <ref> [SS93] </ref> only requires two passes on a certain class of EDNs. Cormen and Bruhl [Bru94, CB95] study the performance of several algorithms for BMMC permutations on the restricted-access MasPar MP-2 global router. <p> Information passes through an EDN as follows. A path is the set of wires traversed by a message from its thin-wire input to its thin-wire output. Scherson and Subramanian <ref> [SS93] </ref> state that this path is unique for each input. If we have a set of processors P = fP 0 ; P 1 ; : : : ; P M1 g, we connect each input wire i and output wire i to processor P i .
Reference: [ST91] <author> G. Steidl and M. Tasche. </author> <title> A polynomial approach to fast algorithms for discrete Fourier-cosine and Fourier-sine transforms. </title> <journal> Mathematics of Computation, </journal> <volume> 56(193) </volume> <pages> 281-296, </pages> <year> 1991. </year>
Reference-contexts: Chapter 8 Complexity Analysis of BMMC Permutations for FCTs Fast cosine transform (FCT) algorithms are essential to efficient computation in many applications such as weather modeling, data compression/decompression, and convolutions on real, symmetric data [Chi95, ER82, RY90]. Recent descriptions of fast cosine transform algorithms <ref> [ST91, Ste92] </ref> perform a record minimum number of multiplication and addition operations to compute an FCT. These new algorithms have been derived using the polynomial-division-tree computational model. These algorithms include a permutation before or after the computation of the transform. <p> These new algorithms have been derived using the polynomial-division-tree computational model. These algorithms include a permutation before or after the computation of the transform. In this chapter, for power-of-2 data sets, we analyze the complexity of the two permutations used in the fast cosine transform algorithms described in <ref> [ST91] </ref> and [Ste92], and we show that the permutation in [Ste92] exhibits properties similar to those of the bit-reversal permutation. We use the following technique to analyze the complexity of these permutations. <p> Complexity Analysis of BMMC Permutations for FCTs Step 3 We translate the characteristic matrix form into a combinational circuit consisting of only XOR gates. We shall see later that the characteristic matrix forms that generate the structure of the permutation matrices described in <ref> [ST91, Ste92] </ref> clearly illustrate their translation into constant-depth, logarithmic-width circuits. From our analysis, we shall also see that the permutation from [Ste92] has properties that make it preferable to that of [ST91]. <p> From our analysis, we shall also see that the permutation from [Ste92] has properties that make it preferable to that of <ref> [ST91] </ref>. We show that the circuit to perform the inverse permutation mapping used by the fast cosine transform algorithm in [ST91] has lg lg N depth and logarithmic width. <p> From our analysis, we shall also see that the permutation from [Ste92] has properties that make it preferable to that of <ref> [ST91] </ref>. We show that the circuit to perform the inverse permutation mapping used by the fast cosine transform algorithm in [ST91] has lg lg N depth and logarithmic width. On the other hand, we show that the circuit used by the fast cosine transform algorithm in [Ste92] is constant depth, logarithmic width, and self-invertible. <p> Thus, the permutation of [Ste92] has properties similar to the bit-reversal permutation, i.e., we can use the same constant-depth circuit to compute the structure of the permutation matrix for both the forward and inverse fast cosine transforms. Therefore, the permutation of [Ste92] is preferable to that of <ref> [ST91] </ref>. Fast transform algorithms On the polynomial division tree computational model, we can represent an algorithm as the application of a sequence of discrete operators, or matrices, on a finite data vector of length N. <p> We also define and discuss the complexity of combinational circuits to perform the index mapping 8.1 Fast cosine transforms 181 for the permutations used in the FCT algorithms. In Section 8.2, we present the permutation of <ref> [ST91] </ref>, which we call the recursive complement, or RC, permutation, and use the above methodology to analyze its complexity. In Section 8.3, we perform a similar analysis of the permutation of [Ste92], which we call the odd-upper/bit-reversal, or OUR, permutation, but we also show that this permutation is self-invertible. <p> Section 8.4 concludes. 8.1 Fast cosine transforms In this section, we define the discrete cosine transform (DCT) and give an overview of the fast cosine transform (FCT) algorithms of <ref> [ST91, Ste92] </ref> to compute the DCT. The permutations used by these FCT algorithms belong to the class of BMMC (bit-matrix multiply/complement) permutations. <p> Because the transformation matrix C is orthogo nal (i.e., C 1 = C T , when properly normalized), the matrix-vector product for the inverse DCT in equation (8.3) is f = C 1 b f = C T b f : The fast inverse cosine transform algorithm of <ref> [ST91] </ref> corresponds to a sparse fac torization of the matrix C T such that C T = P T D T 1 D T where each matrix D T i is sparse (i.e., the matrix contains O (N ) non-zero entries) and the matrix P T is a permutation matrix. <p> We transpose all the matrices for the inverse FCT algorithm to obtain the forward FCT algorithm. We refer the interested reader to the original paper <ref> [ST91] </ref> for more specific details, or to an expanded discussion in [Moo94]. The polynomial division tree method The fast cosine transform algorithm uses the polynomial division tree method for evaluating a polynomial function [BM75, DHR94, Knu81, Moo94]. <p> Using this method, we can derive an algorithm for the fast Fourier transform (FFT) by evaluating at the Nth roots of unity (i.e., at uniformly spaced intervals) and by substituting e ik2=N for z k when evaluating p (z k ). In <ref> [ST91] </ref>, the polynomial division tree derivation for the DCT proceeds by first casting the inverse DCT as the evaluation of a Chebyshev polynomial p (x) such that p (x) = j=0 where we define T j (x) = cos j (arccos x) = T j as the jth Chebyshev polynomial. <p> We use the permutation effected by the permutation matrix P to reorder the data appropriately. 8.1 Fast cosine transforms 185 FCT algorithms for power-of-2 inputs When the size of the data set is a power of 2, the fast algorithms in <ref> [ST91] </ref> and [Ste92] both compute the correct evaluation but perform different permutations to achieve it. In Sections 8.2 and 8.3, we shall examine these two permutations in detail and show that the permutation of [Ste92] has more appealing properties. <p> Complexity Analysis of BMMC Permutations for FCTs addresses and target addresses. We shall extensively use the composition properties of Lemma 2.1 and Corollary 2.2 from Chapter 2 throughout Sections 8.2 and 8.3 to compose the component permutations used in the permutation algorithms of <ref> [ST91, Ste92] </ref> into a single permutation. We use the characteristic matrix to generate the structure of the permutation matrix P. <p> The sequential complexity is the complexity of generating the structure of the permutation matrix using a sequential circuit which 8.2 Recursive-complement permutations 187 consists of combinational circuitry and one or more registers (clocked memory elements). In this chapter, we shall only analyze the combinational complexities of the permutations from <ref> [ST91, Ste92] </ref>. In Sections 8.2 and 8.3, we translate the permutation algorithms used in the FCT algorithms presented in [ST91] and [Ste92], respectively, into a single characteristic matrix form that generates the structure of the permutation matrix. <p> In this chapter, we shall only analyze the combinational complexities of the permutations from [ST91, Ste92]. In Sections 8.2 and 8.3, we translate the permutation algorithms used in the FCT algorithms presented in <ref> [ST91] </ref> and [Ste92], respectively, into a single characteristic matrix form that generates the structure of the permutation matrix. In Section 8.2, we show that the permutation of [ST91], which we call the recursive-complement, or RC, permutation, requires a constant-depth, logarithmic-width circuit to compute its structure, and, therefore, has a complexity of <p> In Sections 8.2 and 8.3, we translate the permutation algorithms used in the FCT algorithms presented in <ref> [ST91] </ref> and [Ste92], respectively, into a single characteristic matrix form that generates the structure of the permutation matrix. In Section 8.2, we show that the permutation of [ST91], which we call the recursive-complement, or RC, permutation, requires a constant-depth, logarithmic-width circuit to compute its structure, and, therefore, has a complexity of O (N lg N) bit-operations. <p> transforms use O (N lg N ) bit-operations to compute the structure of the permutation matrix P, just like the bit-reversal permutation used by the FFT algorithm. 8.2 Recursive-complement (RC) permutations In this section, we describe an algorithm for performing the RC permutation used in the FCT algorithm presented in <ref> [ST91] </ref>. We translate the algorithm into a product of nonsingular matrices, each of which characterizes a BMMC permutation with a special characteristic matrix form. Moreover, the product corresponds to the 188 Chapter 8. <p> to generate the OUR permutation matrix structure for the forward and inverse fast cosine transforms, just as the bit-reversal permutation generates the structure of the permutation matrices for the forward and inverse fast Fourier transforms. 8.4 Conclusions We have translated the two permutations used by the fast cosine transforms of <ref> [ST91, Ste92] </ref> into a product of familiar linear index transformation matrices. Both of these products result in linear-index transformation matrices, which we show how to implement using constant-depth, logarithmic-width circuits. By our definition, both permutations have a complexity of O (N lg N) bit-operations.
Reference: [Ste92] <author> G. Steidl. </author> <title> Fast radix-p discrete cosine transform. </title> <booktitle> In Applicable Algebra in Engineering, Communication and Computing, </booktitle> <pages> pages 39-46. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <note> 222 Bibliography </note>
Reference-contexts: Chapter 8 Complexity Analysis of BMMC Permutations for FCTs Fast cosine transform (FCT) algorithms are essential to efficient computation in many applications such as weather modeling, data compression/decompression, and convolutions on real, symmetric data [Chi95, ER82, RY90]. Recent descriptions of fast cosine transform algorithms <ref> [ST91, Ste92] </ref> perform a record minimum number of multiplication and addition operations to compute an FCT. These new algorithms have been derived using the polynomial-division-tree computational model. These algorithms include a permutation before or after the computation of the transform. <p> These algorithms include a permutation before or after the computation of the transform. In this chapter, for power-of-2 data sets, we analyze the complexity of the two permutations used in the fast cosine transform algorithms described in [ST91] and <ref> [Ste92] </ref>, and we show that the permutation in [Ste92] exhibits properties similar to those of the bit-reversal permutation. We use the following technique to analyze the complexity of these permutations. <p> These algorithms include a permutation before or after the computation of the transform. In this chapter, for power-of-2 data sets, we analyze the complexity of the two permutations used in the fast cosine transform algorithms described in [ST91] and <ref> [Ste92] </ref>, and we show that the permutation in [Ste92] exhibits properties similar to those of the bit-reversal permutation. We use the following technique to analyze the complexity of these permutations. Step 1 We give high-level pseudocode for a series of linear permutations (i.e., BMMC permutations without the complement) that generate the structure of the permutation matrix P. <p> Complexity Analysis of BMMC Permutations for FCTs Step 3 We translate the characteristic matrix form into a combinational circuit consisting of only XOR gates. We shall see later that the characteristic matrix forms that generate the structure of the permutation matrices described in <ref> [ST91, Ste92] </ref> clearly illustrate their translation into constant-depth, logarithmic-width circuits. From our analysis, we shall also see that the permutation from [Ste92] has properties that make it preferable to that of [ST91]. <p> We shall see later that the characteristic matrix forms that generate the structure of the permutation matrices described in [ST91, Ste92] clearly illustrate their translation into constant-depth, logarithmic-width circuits. From our analysis, we shall also see that the permutation from <ref> [Ste92] </ref> has properties that make it preferable to that of [ST91]. We show that the circuit to perform the inverse permutation mapping used by the fast cosine transform algorithm in [ST91] has lg lg N depth and logarithmic width. <p> We show that the circuit to perform the inverse permutation mapping used by the fast cosine transform algorithm in [ST91] has lg lg N depth and logarithmic width. On the other hand, we show that the circuit used by the fast cosine transform algorithm in <ref> [Ste92] </ref> is constant depth, logarithmic width, and self-invertible. Thus, the permutation of [Ste92] has properties similar to the bit-reversal permutation, i.e., we can use the same constant-depth circuit to compute the structure of the permutation matrix for both the forward and inverse fast cosine transforms. Therefore, the permutation of [Ste92] is <p> On the other hand, we show that the circuit used by the fast cosine transform algorithm in <ref> [Ste92] </ref> is constant depth, logarithmic width, and self-invertible. Thus, the permutation of [Ste92] has properties similar to the bit-reversal permutation, i.e., we can use the same constant-depth circuit to compute the structure of the permutation matrix for both the forward and inverse fast cosine transforms. Therefore, the permutation of [Ste92] is preferable to that of [ST91]. <p> in <ref> [Ste92] </ref> is constant depth, logarithmic width, and self-invertible. Thus, the permutation of [Ste92] has properties similar to the bit-reversal permutation, i.e., we can use the same constant-depth circuit to compute the structure of the permutation matrix for both the forward and inverse fast cosine transforms. Therefore, the permutation of [Ste92] is preferable to that of [ST91]. Fast transform algorithms On the polynomial division tree computational model, we can represent an algorithm as the application of a sequence of discrete operators, or matrices, on a finite data vector of length N. <p> Thus, self-invertibility allows the same constant-depth circuit to be used for computing the structure of the permutation matrix for both the forward and inverse transforms. In this chapter, we show that the permutation of the fast cosine transform algorithm in <ref> [Ste92] </ref> is also self-invertible. Outline of this chapter The organization of the remainder of this chapter is as follows. In Section 8.1, we define the discrete cosine transform (DCT) and discuss in more detail the polynomial division tree model and a fast algorithm to compute the DCT. <p> In Section 8.2, we present the permutation of [ST91], which we call the recursive complement, or RC, permutation, and use the above methodology to analyze its complexity. In Section 8.3, we perform a similar analysis of the permutation of <ref> [Ste92] </ref>, which we call the odd-upper/bit-reversal, or OUR, permutation, but we also show that this permutation is self-invertible. <p> Section 8.4 concludes. 8.1 Fast cosine transforms In this section, we define the discrete cosine transform (DCT) and give an overview of the fast cosine transform (FCT) algorithms of <ref> [ST91, Ste92] </ref> to compute the DCT. The permutations used by these FCT algorithms belong to the class of BMMC (bit-matrix multiply/complement) permutations. <p> We use the permutation effected by the permutation matrix P to reorder the data appropriately. 8.1 Fast cosine transforms 185 FCT algorithms for power-of-2 inputs When the size of the data set is a power of 2, the fast algorithms in [ST91] and <ref> [Ste92] </ref> both compute the correct evaluation but perform different permutations to achieve it. In Sections 8.2 and 8.3, we shall examine these two permutations in detail and show that the permutation of [Ste92] has more appealing properties. <p> When the size of the data set is a power of 2, the fast algorithms in [ST91] and <ref> [Ste92] </ref> both compute the correct evaluation but perform different permutations to achieve it. In Sections 8.2 and 8.3, we shall examine these two permutations in detail and show that the permutation of [Ste92] has more appealing properties. Before continuing our discussion, we provide some formal definitions to clarify the difference between the permutation represented by the permutation matrix P and the characteristic matrix that generates the structure of the permutation matrix P. <p> Complexity Analysis of BMMC Permutations for FCTs addresses and target addresses. We shall extensively use the composition properties of Lemma 2.1 and Corollary 2.2 from Chapter 2 throughout Sections 8.2 and 8.3 to compose the component permutations used in the permutation algorithms of <ref> [ST91, Ste92] </ref> into a single permutation. We use the characteristic matrix to generate the structure of the permutation matrix P. <p> The sequential complexity is the complexity of generating the structure of the permutation matrix using a sequential circuit which 8.2 Recursive-complement permutations 187 consists of combinational circuitry and one or more registers (clocked memory elements). In this chapter, we shall only analyze the combinational complexities of the permutations from <ref> [ST91, Ste92] </ref>. In Sections 8.2 and 8.3, we translate the permutation algorithms used in the FCT algorithms presented in [ST91] and [Ste92], respectively, into a single characteristic matrix form that generates the structure of the permutation matrix. <p> In this chapter, we shall only analyze the combinational complexities of the permutations from [ST91, Ste92]. In Sections 8.2 and 8.3, we translate the permutation algorithms used in the FCT algorithms presented in [ST91] and <ref> [Ste92] </ref>, respectively, into a single characteristic matrix form that generates the structure of the permutation matrix. <p> We also show that the inverse of the RC permutation requires a lg lg N - depth, logarithmic-width circuit, resulting in a complexity of O (N lg N lg lg N) bit-operations. In Section 8.3, we show how to implement the permutation of <ref> [Ste92] </ref>, which we call the odd-upper/bit-reversal, or OUR, permutation, with a constant-depth, logarithmic-width circuit which is self-invertible. <p> Second, two different algorithms, or circuits, must be employed for the forward and inverse fast cosine transforms. Third, the complexity for the inverse RC permutation is greater than the complexity for the forward RC permutation. The next section shows that the permutation algorithm used in <ref> [Ste92] </ref> does not have these limitations. 200 Chapter 8. Complexity Analysis of BMMC Permutations for FCTs 8.3 Odd-upper/bit-reversal (OUR) permutations In this section, we show that the algorithm for performing the permutation used by the FCT algorithm in [Ste92], which we call the OUR permutation, also maps to a constant-depth circuit. <p> The next section shows that the permutation algorithm used in <ref> [Ste92] </ref> does not have these limitations. 200 Chapter 8. Complexity Analysis of BMMC Permutations for FCTs 8.3 Odd-upper/bit-reversal (OUR) permutations In this section, we show that the algorithm for performing the permutation used by the FCT algorithm in [Ste92], which we call the OUR permutation, also maps to a constant-depth circuit. <p> to generate the OUR permutation matrix structure for the forward and inverse fast cosine transforms, just as the bit-reversal permutation generates the structure of the permutation matrices for the forward and inverse fast Fourier transforms. 8.4 Conclusions We have translated the two permutations used by the fast cosine transforms of <ref> [ST91, Ste92] </ref> into a product of familiar linear index transformation matrices. Both of these products result in linear-index transformation matrices, which we show how to implement using constant-depth, logarithmic-width circuits. By our definition, both permutations have a complexity of O (N lg N) bit-operations. <p> Both of these products result in linear-index transformation matrices, which we show how to implement using constant-depth, logarithmic-width circuits. By our definition, both permutations have a complexity of O (N lg N) bit-operations. We have also shown that the permutation of <ref> [Ste92] </ref> is a better permutation choice because it is self-invertible, which allows the same constant-depth circuit to be used for both the forward and the inverse FCT algorithms. We ask whether other applications use permutations that are in the class of BMMC permutations, such as wavelet transforms [CH96].
Reference: [Sto71] <author> Harold S. Stone. </author> <title> Parallel processing with the perfect shu*e. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-20(2):153-161, </volume> <month> February </month> <year> 1971. </year>
Reference-contexts: A multistage interconnection network consists of alternating stages of switches and wires. The switches are typically crossbars connected by wires, usually in a pattern based on the perfect shu*e <ref> [Sto71] </ref>. The above networks are equivalent, topologically and functionally [Pea77, Sie77]. Researchers have developed efficient algorithms on these multistage networks. A pass through the network is a rearrangement in which some subset of the data simultaneously moves through the network to a destination.
Reference: [Str88] <author> Gilbert Strang. </author> <title> Linear Algebra and Its Applications. </title> <publisher> Harcourt Brace Jovanovich, </publisher> <address> San Diego, third edition, </address> <year> 1988. </year>
Reference-contexts: Lemma 2.6 Let K and L be q-column matrices for which ker K ker L. Then row L row K. Proof: It is a well-known fact from linear algebra (see Strang <ref> [Str88, p. 138] </ref> for example) that the row space is the orthogonal complement of the kernel. That is, for any q-vectors u 2 ker L and v 2 row L, the inner product u v is 0.
Reference: [SW94] <author> K. E. Seamons and M. Winslett. </author> <title> An efficient abstract interface for multidimensional array I/O. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 650-659, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: At a higher level of abstraction, run-time libraries achieve I/O-performance improvements on multiple disk systems, but typically lose the direct disk access in the abstraction. The Panda run-time library <ref> [SCJ + 95, SW94] </ref> achieves performance improvements by providing an API and more efficient layout alternatives for multidimensional array data. This approach takes advantage of the spatial and temporal locality of the array.
Reference: [SW95] <author> Elizabeth A.M. Shriver and Leonard F. Wisniewski. </author> <title> An API for choreographing data accesses. </title> <type> Technical Report PCS-TR95-267, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: Chapter 4 presents an application programmer interface (API) which allows the programmer to easily move data between the parallel disks and main memory in the manner suggested by PDM <ref> [SW95] </ref>. Traditional file-system interfaces include only a single offset to designate a single point to access data within a file. Our API gives the programmer the ability, in a single call, to access data from a different 6 Chapter 1. Permuting and the Memory Hierarchy location on each disk.
Reference: [SWC + 95] <author> Elizabeth A. M. Shriver, Leonard F. Wisniewski, Bruce G. Calder, David Greenberg, Ryan Moore, and David Womble. </author> <title> Parallel disk access using the Whiptail File System: Design and implementation. </title> <type> Manuscript, </type> <year> 1995. </year>
Reference-contexts: Our API is easy to implement; indeed, it has been implemented as the interface for the Whiptail File System (WFS), an experimental file system for I/O-efficient out-of-core problems <ref> [SWC + 95] </ref>. We show that the implementation of our API on the Intel Paragon provides scalable performance. We also demonstrate that it is easy to use by providing example code for some I/O-efficient algorithms. <p> Researchers at Sandia National Laboratories have developed the Whiptail File System (WFS) <ref> [SWC + 95] </ref>, the first implementation of our API. WFS is a prototype file system built on top of the Parallel File System (PFS) on the Intel Paragon. PFS provides direct access to each of the local RAIDs on the Intel Paragon.
Reference: [VB81] <author> L.G. Valiant and G.J. Brebner. </author> <title> Universal schemes for parallel communication. </title> <booktitle> In Proceedings of the 13th Symposium on the Theory of Computation, </booktitle> <pages> pages 263-277, </pages> <year> 1981. </year>
Reference-contexts: The rightmost factor defines a permutation of the messages to intermediate locations, as in the style of the Valiant-Brebner algorithm for hypercube routing <ref> [VB81] </ref>. (Our method is deterministic, whereas the Valiant-Brebner method is randomized.) After permuting the messages to the intermediate locations, we permute the messages to their final locations according to the factor on the left. Both factors have an important property.
Reference: [Ven94] <author> Darren Erik Vengroff. </author> <title> A transparent parallel I/O environment. </title> <booktitle> In Proceedings of the 1994 DAGS/PC Symposium, </booktitle> <pages> pages 117-134, </pages> <address> Hanover, NH, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: This approach takes advantage of the spatial and temporal locality of the array. Jovian [BBS + 94] and PASSION [CBH + 94] also provide support for efficient array data access, but abstract away direct disk access from the programmer. The Transparent Parallel I/O Environment (TPIE) <ref> [Ven94] </ref> provides a high-level access method interface (AMI) to the I/O paradigms that have already been developed, but does not allow the user to explicitly program disk accesses. Unfortunately, none of these run-time libraries include routines for simultaneous, direct access to the disks.
Reference: [VS90] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Optimal disk I/O with parallel block transfer. </title> <booktitle> In Proceedings of the Twenty Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 159-169, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Our approach is to develop out-of-core algorithms which efficiently perform the basic data-movement operations needed by these applications. We have designed algorithms for basic data-movement operations on Vitter and Shriver's Parallel Disk Model (PDM) <ref> [VS90, VS94a] </ref>. On PDM, we measure the complexity of algorithms by counting the number of parallel disk accesses. We 0 to develop out-of-core algorithms for performing various structured classes of permutations. A permutation is the rearrangement of the records in the data set. <p> Finally, Section 2.6 contains some concluding remarks. 2.1 The Parallel Disk Model and previous results 11 2.1 The Parallel Disk Model and previous results We use the parallel-disk model first proposed by Vitter and Shriver <ref> [VS90, VS94a] </ref>, who also gave asymptotically optimal algorithms for several problems including sorting and general permutations. In the Vitter-Shriver model, N records are stored on D disks D 0 ; D 1 ; : : : ; D D1 , with N=D records stored on each disk. <p> The interested reader can find full descriptions of these models in <ref> [ACFS94, AV88, VS90, VS94a] </ref>. Using the I/O complexity models, algorithm designers have developed out-of-core algorithms, such as the algorithms detailed in Chapters 2 and 3, that choreograph data movements to utilize all the disks in the I/O subsystem concurrently. <p> An API for Choreographing Data Accesses 4.1 Background Researchers have developed theoretical models to capture important features of data movement between main memory and secondary storage <ref> [AV88, Flo72, VS90, VS94a] </ref>. In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94a] provides a reasonable model for the design of I/O-efficient algorithms, such as those shown in Chapters 2 and 3, and analysis at a feasible level of abstraction.
Reference: [VS94a] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <journal> Algorithmica, </journal> 12(2/3):110-147, August and September 1994. 
Reference-contexts: Our approach is to develop out-of-core algorithms which efficiently perform the basic data-movement operations needed by these applications. We have designed algorithms for basic data-movement operations on Vitter and Shriver's Parallel Disk Model (PDM) <ref> [VS90, VS94a] </ref>. On PDM, we measure the complexity of algorithms by counting the number of parallel disk accesses. We 0 to develop out-of-core algorithms for performing various structured classes of permutations. A permutation is the rearrangement of the records in the data set. <p> Finally, Section 2.6 contains some concluding remarks. 2.1 The Parallel Disk Model and previous results 11 2.1 The Parallel Disk Model and previous results We use the parallel-disk model first proposed by Vitter and Shriver <ref> [VS90, VS94a] </ref>, who also gave asymptotically optimal algorithms for several problems including sorting and general permutations. In the Vitter-Shriver model, N records are stored on D disks D 0 ; D 1 ; : : : ; D D1 , with N=D records stored on each disk. <p> The interested reader can find full descriptions of these models in <ref> [ACFS94, AV88, VS90, VS94a] </ref>. Using the I/O complexity models, algorithm designers have developed out-of-core algorithms, such as the algorithms detailed in Chapters 2 and 3, that choreograph data movements to utilize all the disks in the I/O subsystem concurrently. <p> An API for Choreographing Data Accesses 4.1 Background Researchers have developed theoretical models to capture important features of data movement between main memory and secondary storage <ref> [AV88, Flo72, VS90, VS94a] </ref>. In particular, Vitter and Shriver's Parallel Disk Model (PDM) [VS94a] provides a reasonable model for the design of I/O-efficient algorithms, such as those shown in Chapters 2 and 3, and analysis at a feasible level of abstraction. <p> An API for Choreographing Data Accesses 4.1 Background Researchers have developed theoretical models to capture important features of data movement between main memory and secondary storage [AV88, Flo72, VS90, VS94a]. In particular, Vitter and Shriver's Parallel Disk Model (PDM) <ref> [VS94a] </ref> provides a reasonable model for the design of I/O-efficient algorithms, such as those shown in Chapters 2 and 3, and analysis at a feasible level of abstraction. A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting <ref> [AP94, Arg95, NV93, VS94a] </ref>, general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. <p> <ref> [VS94a] </ref> provides a reasonable model for the design of I/O-efficient algorithms, such as those shown in Chapters 2 and 3, and analysis at a feasible level of abstraction. A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting <ref> [VS94a] </ref>, BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose <ref> [Cor92, Cor93, CSW94, VS94a] </ref>, FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting <ref> [VS94a] </ref>, BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM.
Reference: [VS94b] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Algorithms for parallel memory II: Hierarchical multilevel memories. </title> <journal> Algorithmica, </journal> 12(2/3):148-169, August and September 1994. 
Reference-contexts: Vitter and Vengroff use BMMC permutations as a subroutine in performing out-of-core matrix-matrix multiply [VV95]. When implementing out-of-core algorithms, we would like to have a unified approach for solving the same problem at different levels of the memory hierarchy. Several unified approaches have been pursued to model memory hierarchies <ref> [VS94b, ACFS94] </ref>. In this thesis, we present a unified approach to designing algorithms for efficiently performing the class of BMMC permutations. The approach is flexible enough to account for the structure of memory at the various levels of the memory hierarchy.
Reference: [VV95] <author> Darren Erik Vengroff and Jeffrey Scott Vitter. </author> <title> I/O-efficient scientific computation using TPIE. </title> <booktitle> In Proceedings of the Seventh IEEE Symposium on Parallel and Distributed Processing, </booktitle> <month> October </month> <year> 1995. </year> <note> Bibliography 223 </note>
Reference-contexts: Weather modeling and data compression/decompression techniques use these various transforms. Vitter and Vengroff use BMMC permutations as a subroutine in performing out-of-core matrix-matrix multiply <ref> [VV95] </ref>. When implementing out-of-core algorithms, we would like to have a unified approach for solving the same problem at different levels of the memory hierarchy. Several unified approaches have been pursued to model memory hierarchies [VS94b, ACFS94]. <p> Vitter and Shriver did not design their out-of-core algorithm for general permuting to be performed in place. Furthermore, the constants implicit in the upper bound for this algorithm are rather high compared to the constants in the worst cases for the structured permutation algorithms mentioned above <ref> [VV95] </ref>. Outline of this chapter The remainder of this chapter is organized as follows. In Section 3.1, we adapt the BMMC algorithm of Chapter 2 to be performed in place, and present algorithms to 56 Chapter 3. <p> Vengroff and Vitter <ref> [VV95] </ref> also make assumptions about the values for the parameters of PDM to aid in the practical implementation of out-of-core sorting algorithms.
Reference: [Wak68] <author> Abraham Waksman. </author> <title> A permutation network. </title> <journal> Journal of the ACM, </journal> <volume> 15(1) </volume> <pages> 159-163, </pages> <month> January </month> <year> 1968. </year>
Reference-contexts: A pass through the network is a rearrangement in which some subset of the data simultaneously moves through the network to a destination. Since any arbitrary permutation can be routed through a multistage network if multiple passes are allowed, we say that the above multistage networks are universal. Waksman <ref> [Wak68] </ref> and Kautz [Kau68] describe how to perform an arbitrary permutation of N elements using basic 2-permuters. Parker [Par80] shows a one-pass algorithm for an arbitrary permutation on the three-stage Benes network and also presents a three-pass algorithm on the network.
Reference: [WGWR93] <author> David Womble, David Greenberg, Stephen Wheat, and Rolf Riesen. </author> <title> Beyond core: Making parallel computer I/O practical. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 56-63, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition <ref> [WGWR93] </ref>, graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. We also describe the types of data access needed by PDM algorithms and discuss how well existing file systems support these types of data access.
Reference: [Wis95] <author> Leonard F. Wisniewski. </author> <title> Structured permuting in place on parallel disk systems. </title> <type> Technical Report PCS-TR95-265, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> September </month> <year> 1995. </year> <booktitle> To appear in the Proceedings of the Fourth Annual Workshop on I/O in Parallel and Distributed Systems, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: This algorithm is well-suited for practical implementation because the constants in the complexity bound are very close to the lower bound. In Chapter 3, we introduce algorithms that perform permutations in place on parallel disk systems <ref> [Wis95] </ref>. Rather than moving data from a source portion of disk storage to a separate target portion, we only use the source portion with additional disk storage that is the size of memory. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations <ref> [Cor92, Cor93, CSW94, Wis95] </ref>, mesh and torus permutations [Cor92, Wis95], matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM. <p> A number of I/O-efficient algorithms have been designed on PDM to solve problems such as sorting [AP94, Arg95, NV93, VS94a], general permuting [VS94a], BMMC permutations [Cor92, Cor93, CSW94, Wis95], mesh and torus permutations <ref> [Cor92, Wis95] </ref>, matrix-matrix multiplication [VS94a], matrix transpose [Cor92, Cor93, CSW94, VS94a], FFT [VS94a], LU decomposition [WGWR93], graph algorithms [CGG + 95], and geometric algorithms [AVV95, GTVV93]. In this section, we define the parameters and data layout for PDM.
Reference: [YL81] <author> Pen-chung Yew and Duncan H. Lawrie. </author> <title> An easily controlled network for frequently used permutations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-30(4):296-298, </volume> <month> April </month> <year> 1981. </year>
Reference-contexts: Pease [Pea77] shows how to perform BMMC permutations in two passes on the indirect binary n-cube. Keohane and Stearns [KS88] give an algorithm for performing linear permutations (BMMC permutations without the complement) in two passes on the network. Yew and Lawrie <ref> [YL81] </ref> present a two-pass algorithm for BPC permutations on the network, and Nassimi and Sahni [NS81] give an algorithm for BPC permutations on a Benes network. Shen [She95] shows how to perform any BPC permutation on a k-extra-stage network in a minimum number of passes.
References-found: 87

