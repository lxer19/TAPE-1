URL: http://ebbets.poly.edu/hstein/pubs/disk.ps
Refering-URL: http://ebbets.poly.edu/hstein/journal.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Coding Techniques for Handling Failures in Large Disk Arrays  
Author: Lisa Hellerstein, Garth A. Gibson, Richard M. Karp, Randy H. Katz and David A. Patterson 
Keyword: Input/Output architecture, redundant disk arrays, RAID, error-correcting codes, reliability, availability.  
Abstract: Abstract: A crucial issue in the design of very large disk arrays is the protection of data against catastrophic disk failures. Although today single disks are highly reliable, when a disk array consists of 100 or 1000 disks, the probability that at least one disk will fail within a day or a week is high. In this paper, we address the problem of designing erasure-correcting binary linear codes that protect against the loss of data caused by disk failures in large disk arrays. We describe how such codes can be used to encode data in disk arrays, and give a simple method for data reconstruction. We discuss important reliability and performance constraints of these codes, and show how these constraints relate to properties of the parity check matrices of the codes. In so doing, we transform code design problems into combinatorial problems. Using this combinatorial framework, we present codes and prove they are optimal with respect to various reliability and performance constraints. 
Abstract-found: 1
Intro-found: 1
Reference: [ATC90] <author> Product Description, </author> <title> RAID+ Series Model RX, Revision 1.0, Array Technology Corporation, </title> <address> Boulder CO 80301, </address> <month> February </month> <year> 1990. </year>
Reference-contexts: The availability of compact encoder/decoder chip sets for Reed-Solomon codes and the wide range of industrial experience with these codes has led to the use of variations of the two-check-symbol Reed-Solomon code in disk array products under the name of P+Q parity <ref> [ATC90] </ref>. A problem in attaining very low check disk overhead through nonbinary codes with only two check symbols is the reconstruction group size; any single disk failure will involve all disks in its reconstruction.
Reference: [Bates89] <author> Bates, K. H. </author> <title> ``Performance aspects of the HSC controller,'' </title> <journal> Digital Technical Journal, </journal> <volume> Vol. 8, </volume> <month> February </month> <year> 1989. </year>
Reference-contexts: The t d-parity coding schemes are a member of the class of product codes that have been commonly used in magnetic tape systems [Peterson72]. A common, but expensive, technique for protecting disk systems from disk failures is known as shadowing <ref> [Bates89, Bit-ton88] </ref>. A shadowing code is equivalent to a t d-parity code with G = 1 because the parity of a single bit duplicates the value of that bit.
Reference: [Bitton88] <author> Bitton, D., J. Gray, </author> <title> ``Disk shadowing,'' </title> <booktitle> Proc. of the 14th Int. Conf. on Very Large Data Bases (VLDB), </booktitle> <year> 1988. </year>
Reference: [Bell85] <author> Bell, C. G.,``Multis: </author> <title> a new class of multiprocessor computers,'' </title> <journal> Science, </journal> <volume> Vol. 228, </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: 1. Background In recent years, processing power has increased dramatically through advanced VLSI tech nology [Myers86, Gelsinger89] and parallel architectures <ref> [Bell85, Bell89] </ref>. As processing power increases, so does the demand for increased Input/Output (I/O) performance.
Reference: [Bell89] <author> Bell, C. G., </author> <title> ``The future of high performance computers in science and engineering,'' </title> <journal> Comm. of the ACM, </journal> <volume> Vol. 32, No. 9, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: 1. Background In recent years, processing power has increased dramatically through advanced VLSI tech nology [Myers86, Gelsinger89] and parallel architectures <ref> [Bell85, Bell89] </ref>. As processing power increases, so does the demand for increased Input/Output (I/O) performance.
Reference: [Bollobas86] <author> Bollobas, B., </author> <title> Combinatorics, Set Systems, Hypergraphs, Families of Vectors, and Combinatorial Probability, </title> <publisher> Cambridge University Press, </publisher> <year> 1986. </year>
Reference-contexts: It follows that the graph corresponding to a code's parity check matrix cannot contain a clique of size three. By Turan's Theorem (see, for example, <ref> [Bollobas86] </ref>), a graph with c vertices that does not contain a clique of size three has at most c 2 /4 edges. Therefore, the graph corresponding to the parity check can contain at most c 2 /4 edges. <p> Thus the check disk overhead of the code is at least 6/(c -1). ` Definition: The Steiner code is based on a Steiner triple system (see, for example, <ref> [Bollobas86] </ref>). Let X = -0,1,2, . . . , c 1-. <p> A theorem of Baranyai states that if t divides c , then there exists a factorization of the set consisting of all columns of length c and weight t <ref> [Brouwer79a, Bollobas86] </ref>. This theorem implies that it is possible to achieve a balanced ordering of the full-2 code when c is even, and of the full-3 code when c is divisible by 3. <p> An explicit construction for a factorization of the set of weight 2 columns of length c , when c is even is given by Bollobas <ref> [Bollobas86] </ref>. This construction can be modified slightly to produce a balanced ordering of the full-2 code when c is odd.
Reference: [Boral83] <author> Boral, H., DeWitt, D., </author> <title> ``Database machines: an idea whose time has passed?,'' Database Machines, </title> <editor> ed. H.O. Leilich, M. Missikoff, </editor> <publisher> Springer-Verlag, </publisher> <month> Sep-tember </month> <year> 1983. </year>
Reference-contexts: The mainstay of on-line secondary storage, the magnetic disk, is providing neither the data rates required for applications that process large amounts of sequential data nor the access rates required for appli cations that process large numbers of random accesses <ref> [Boral83] </ref>.
Reference: [Brouwer79a] <author> Brouwer, A. E., Schrijver, A., </author> <title> ``Uniform Hypergraphs,'' Packing and Covering in Combinatorics, </title> <editor> Schrijver, A., ed., </editor> <publisher> Mathematical Centre Tracts 106, Mathematisch Centrum, </publisher> <address> Amsterdam 1979. </address>
Reference-contexts: A theorem of Baranyai states that if t divides c , then there exists a factorization of the set consisting of all columns of length c and weight t <ref> [Brouwer79a, Bollobas86] </ref>. This theorem implies that it is possible to achieve a balanced ordering of the full-2 code when c is even, and of the full-3 code when c is divisible by 3. <p> This construction can be modified slightly to produce a balanced ordering of the full-2 code when c is odd. A more general theorem of Baranyai <ref> [Brouwer79a] </ref> (which essentially implies the existence of an "approximate factorization") can be used to show that the full-3 code also has a balanced ordering when c is not divisible by 3. In the 2d-parity code, factorizing the columns of P is trivial.
Reference: [Brouwer79b] <author> Brouwer, A. E., </author> <title> ``Wilson's theory,'' Packing and Covering in Combinatorics, </title> <editor> Schrijver, A., ed., </editor> <publisher> Mathematical Centre Tracts 106, Mathematisch Centrum, </publisher> <address> Amsterdam 1979. </address>
Reference-contexts: Unfortunately, it is not known whether the above two conditions are always sufficient for t &gt;5. It is only known that they are sufficient provided that c is sufficiently large by a theorem of Wilson, a proof of which can be found in <ref> [Brouwer79b] </ref>. For t &gt;3, it is an open question to determine the minimum possible check disk overhead of a t -erasure-correcting code with minimum update penalty.
Reference: [Chen90] <author> Chen, Peter M., David A. Patterson, </author> <title> ``Maximizing performance in a striped disk array,'' </title> <booktitle> Proc. of the 1990 ACM SIGARCH 17th Ann. Int. Symp. of Computer Architecture, </booktitle> <address> Seattle WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: This widening gap has led to I/O systems that achieve performance through disk parallelism, using such techniques as disk striping <ref> [Chen90, Kim87, Klietz88, Livny87, Salem86] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 This paper is a revised and expanded version of material that appeared in the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), Boston, March 1989 [Gibson89]. 2 Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL
Reference: [Gray85] <author> Gray, J., </author> <title> ``Why do computers stop and what can be done about it?,'' </title> <type> Tandem Technical Report 85.7, </type> <month> June </month> <year> 1985. </year>
Reference-contexts: There are, of course, other possible causes of data loss in a disk array in addition to disk drive failures. Examples include the failure of power, cabling, memory, and processors (see [Gibson92, Gibson93] for discussions of the effect of these on RAID architecures), and incorrect or misused software <ref> [Gray85] </ref>.
Reference: [Gelsinger89] <author> Gelsinger, Patrick P., Paola A. Gargini, Gerhard H. Parker, Albert Y. C. Yu, </author> <title> ``Microprocessors circa 2000,'' </title> <journal> IEEE Spectrum, </journal> <month> October </month> <year> 1989. </year>
Reference-contexts: 1. Background In recent years, processing power has increased dramatically through advanced VLSI tech nology <ref> [Myers86, Gelsinger89] </ref> and parallel architectures [Bell85, Bell89]. As processing power increases, so does the demand for increased Input/Output (I/O) performance.
Reference: [Gibson89] <author> Gibson, Garth A., Lisa Hellerstein, Richard M. Karp, Randy H. Katz, David A. Patterson, </author> <title> ``Coding techniques for handling failures in large disk arrays,'' </title> <booktitle> Third Int. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), </booktitle> <address> Boston MA, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: that achieve performance through disk parallelism, using such techniques as disk striping [Chen90, Kim87, Klietz88, Livny87, Salem86]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 This paper is a revised and expanded version of material that appeared in the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), Boston, March 1989 <ref> [Gibson89] </ref>. 2 Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL 60208. 3 School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213-3890. 4 Computer Science Division, Electrical Engineering and Computer Sciences, University of Califor nia, Berkeley, CA 94720. 1 Although disks have not been getting much faster, they
Reference: [Gibson92] <author> Gibson, Garth A., </author> <title> Redundant Disk Arrays: Reliable, Parallel Secondary Storage, </title> <publisher> M.I.T. Press, </publisher> <year> 1992. </year>
Reference-contexts: Disk densities have been growing exponentially, cost per megabyte has been decreasing in step with density increases, and physical packaging has been achieving amazing reductions in volume [Hoagland89, Kryder89]. This trend has led some to explore the replacement of individual large form-factor drives with many smaller form-factor drives <ref> [Jilke86, Patterson88, Gibson92] </ref>, giving yet another reason to expect that future I/O systems will contain large numbers of disks. While performance improves with increasing numbers of disks, the catch is that the chance of data loss also increases. <p> While performance improves with increasing numbers of disks, the catch is that the chance of data loss also increases. A simple model for device lifetime, used for electronics in general and for magnetic disks in particular <ref> [Lin88, Gibson92] </ref>, is an exponential random variable. <p> There are, of course, other possible causes of data loss in a disk array in addition to disk drive failures. Examples include the failure of power, cabling, memory, and processors (see <ref> [Gibson92, Gibson93] </ref> for discussions of the effect of these on RAID architecures), and incorrect or misused software [Gray85]. <p> It depends on the way disks fail and are repaired. For our calculations, we assume that disk lifetimes are identical independent exponential random variables <ref> [Gibson92] </ref> and that repair is done periodically. With this model we can estimate MTTDL as the expected number of repair periods until an unrecoverable set of failures occur. <p> This support hardware is likely to be shared among a subset of the disks. If parity groups are organized orthogonally to support hardware groups, data redundancy provides protection against support hardware failures as well as catastrophic disk failures <ref> [Gibson92, Gibson93] </ref>. This technique is easily extended for the 2d-parity code by organizing support hardware groups on the diagonals of the 2d array [Newberg93]. As this section has shown, high reliability does not require immediate, automatic reconstruction to idle ``hot spare'' disks. <p> However, if a failed disk is the target of frequent accesses then the performance degradation of reconstructing each request's data until the next maintenance personnel visit may justify the more complex automatic approach. In this case a disk array's reliability depends on the spare pool replenishing process <ref> [Gibson92, Gibson93] </ref>. 9. Codes for Correcting more than Three Erasures As we have mentioned, we do not envision the use in disk arrays of t -erasure-correcting codes for t &gt;3. <p> However, there is a nonbinary analogue to the binary full-2 code. This nonbinary full-2 code protects up to c (c -1)(2 b -1)/2 data symbols against double erasure with c check symbols <ref> [Gibson92, Peterson72] </ref>. In using nonbinary analogues of our binary codes to achieve low disk overhead, we may decrease reliability by introducing new sets of t +1-erasures that are not bad, and not correctable.
Reference: [Gibson93] <author> Gibson, Garth A., David A. Patterson, </author> <title> ``Designing Disk Arrays for High Data Reliability,'' </title> <journal> J. of Parallel and Distributed Computing, </journal> <volume> Vol. 17, No. 1, </volume> <month> Janu-ary </month> <year> 1993. </year>
Reference-contexts: There are, of course, other possible causes of data loss in a disk array in addition to disk drive failures. Examples include the failure of power, cabling, memory, and processors (see <ref> [Gibson92, Gibson93] </ref> for discussions of the effect of these on RAID architecures), and incorrect or misused software [Gray85]. <p> This support hardware is likely to be shared among a subset of the disks. If parity groups are organized orthogonally to support hardware groups, data redundancy provides protection against support hardware failures as well as catastrophic disk failures <ref> [Gibson92, Gibson93] </ref>. This technique is easily extended for the 2d-parity code by organizing support hardware groups on the diagonals of the 2d array [Newberg93]. As this section has shown, high reliability does not require immediate, automatic reconstruction to idle ``hot spare'' disks. <p> However, if a failed disk is the target of frequent accesses then the performance degradation of reconstructing each request's data until the next maintenance personnel visit may justify the more complex automatic approach. In this case a disk array's reliability depends on the spare pool replenishing process <ref> [Gibson92, Gibson93] </ref>. 9. Codes for Correcting more than Three Erasures As we have mentioned, we do not envision the use in disk arrays of t -erasure-correcting codes for t &gt;3.
Reference: [Hanani61] <author> Hanani, H., </author> <title> ``The existence and construction of balanced incomplete block designs,'' </title> <journal> Ann. of Math. Stat., </journal> <volume> Vol. 32, </volume> <year> 1961, </year> <pages> pp 361-386. </pages>
Reference-contexts: Two necessary conditions for the existence of such a design are that t (t -1) divides c (c -1), and that (t -1) divides (c -1). These conditions are known to be sufficient for t = 4 and t = 5 <ref> [Hanani61, Hanani75] </ref>. Thus, there exist 4-erasure-correcting and 5-erasure-correcting codes that achieve a check disk overhead of t (t -1)/(c -1) (whenever t and c satisfy the two 29 conditions above). These codes have much lower check disk overhead than the 4d-parity and 5d-parity codes.
Reference: [Hanani75] <author> Hanani, H., </author> <title> ``Balanced incomplete block designs and related designs,'' </title> <journal> Discrete Mathematics, </journal> <volume> Vol. 11, </volume> <year> 1975, </year> <pages> pp 255-369. 33 </pages>
Reference-contexts: Two necessary conditions for the existence of such a design are that t (t -1) divides c (c -1), and that (t -1) divides (c -1). These conditions are known to be sufficient for t = 4 and t = 5 <ref> [Hanani61, Hanani75] </ref>. Thus, there exist 4-erasure-correcting and 5-erasure-correcting codes that achieve a check disk overhead of t (t -1)/(c -1) (whenever t and c satisfy the two 29 conditions above). These codes have much lower check disk overhead than the 4d-parity and 5d-parity codes.
Reference: [Hoagland89] <author> Hoagland, Albert, </author> <title> ``Information storage technology: a look at the future,'' </title> <journal> IEEE Computer, </journal> <volume> Vol. 18, </volume> <month> July, </month> <year> 1985. </year>
Reference-contexts: Disk densities have been growing exponentially, cost per megabyte has been decreasing in step with density increases, and physical packaging has been achieving amazing reductions in volume <ref> [Hoagland89, Kryder89] </ref>. This trend has led some to explore the replacement of individual large form-factor drives with many smaller form-factor drives [Jilke86, Patterson88, Gibson92], giving yet another reason to expect that future I/O systems will contain large numbers of disks.
Reference: [Holland92] <author> Holland, Mark, Garth A. Gibson, </author> <title> ``Parity Declustering for Continuous Operation in Redundant Disk Arrays,'' </title> <booktitle> Fifth Int. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <address> Boston MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The group size is an important metric because the duration of reconstruction is likely to scale linearly with the number of disks to be read. Additionally, in very large arrays, individual disk failure will be frequent enough that highly available systems must continue operation during repair and reconstruction <ref> [Muntz90, Holland92] </ref>. Until reconstruction is complete, the group size indicates the number of disks that must be accessed to read or write an unreconstructed block on a failed disk. Moreover, the group size also indicates the number of operational disks for which user access performance is degraded by a reconstruction.
Reference: [Jilke86] <author> Jilke, W., </author> <title> ``Disk array mass storage systems: the new opportunity,'' </title> <institution> Amperif Corp., </institution> <month> September </month> <year> 1986. </year>
Reference-contexts: Disk densities have been growing exponentially, cost per megabyte has been decreasing in step with density increases, and physical packaging has been achieving amazing reductions in volume [Hoagland89, Kryder89]. This trend has led some to explore the replacement of individual large form-factor drives with many smaller form-factor drives <ref> [Jilke86, Patterson88, Gibson92] </ref>, giving yet another reason to expect that future I/O systems will contain large numbers of disks. While performance improves with increasing numbers of disks, the catch is that the chance of data loss also increases.
Reference: [Kim87] <author> Kim, Michelle Y., </author> <title> Synchronously Interleaved Disk Systems with their Application to the Very Large FFT, </title> <type> PhD Dissertation, </type> <institution> Polytechnic University, </institution> <month> Janu-rary </month> <year> 1987. </year>
Reference-contexts: This widening gap has led to I/O systems that achieve performance through disk parallelism, using such techniques as disk striping <ref> [Chen90, Kim87, Klietz88, Livny87, Salem86] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 This paper is a revised and expanded version of material that appeared in the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), Boston, March 1989 [Gibson89]. 2 Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL
Reference: [Klietz88] <author> Klietz, A., J. Turner, T. C. Jacobson, ``TurboNFS: </author> <title> fast shared access for Cray disk storage,'' </title> <booktitle> Proc. of Cray User Group Convention, </booktitle> <month> April </month> <year> 1988. </year>
Reference-contexts: This widening gap has led to I/O systems that achieve performance through disk parallelism, using such techniques as disk striping <ref> [Chen90, Kim87, Klietz88, Livny87, Salem86] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 This paper is a revised and expanded version of material that appeared in the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), Boston, March 1989 [Gibson89]. 2 Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL
Reference: [Kryder89] <author> Kryder, Mark H., </author> <title> ``Data storage in 2000 trends in data storage technologies,'' </title> <journal> IEEE Trans. on Magnetics, </journal> <volume> Vol. 25, No. 6, </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: Disk densities have been growing exponentially, cost per megabyte has been decreasing in step with density increases, and physical packaging has been achieving amazing reductions in volume <ref> [Hoagland89, Kryder89] </ref>. This trend has led some to explore the replacement of individual large form-factor drives with many smaller form-factor drives [Jilke86, Patterson88, Gibson92], giving yet another reason to expect that future I/O systems will contain large numbers of disks.
Reference: [Lin88] <author> Lin, Ting-Ting Yao, </author> <title> Design and Evaluation of an On-Line Predictive Diagnostic System, </title> <type> PhD Dissertation, </type> <institution> Carnegie Mellon University, </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: While performance improves with increasing numbers of disks, the catch is that the chance of data loss also increases. A simple model for device lifetime, used for electronics in general and for magnetic disks in particular <ref> [Lin88, Gibson92] </ref>, is an exponential random variable.
Reference: [Livny87] <author> Livny, M., S. Khoshafian, H. Boral, </author> <title> ``Multi-disk management algorithms,'' </title> <booktitle> Proc. of ACM SIGMETRICS, </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: This widening gap has led to I/O systems that achieve performance through disk parallelism, using such techniques as disk striping <ref> [Chen90, Kim87, Klietz88, Livny87, Salem86] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 This paper is a revised and expanded version of material that appeared in the Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS III), Boston, March 1989 [Gibson89]. 2 Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL
Reference: [MacWilliams77] <author> Florence Jessie MacWilliams, Neil James Alexander Sloane, </author> <title> The Theory of Error-Correcting Codes, </title> <journal> North-Holland Mathematical Library, </journal> <volume> Vol. 16, </volume> <publisher> Elsevier Science Publishing Company, </publisher> <address> New York NY, </address> <year> 1977. </year>
Reference-contexts: If the symbol size is permitted to be unbounded, then any number of information disks can be protected from all double erasures with only two check disks using, for example, a two-check-symbol nonbinary Hamming code, or a two-check-symbol Reed-Solomon code <ref> [MacWilliams77] </ref>. The availability of compact encoder/decoder chip sets for Reed-Solomon codes and the wide range of industrial experience with these codes has led to the use of variations of the two-check-symbol Reed-Solomon code in disk array products under the name of P+Q parity [ATC90].
Reference: [Muntz90] <author> Muntz, Richard R., John C. S. Lui, </author> <title> ``Performance analysis of disk arrays under failure,'' </title> <booktitle> Proc. of the 16th Int. Conf. on Very Large Data Bases (VLDB), </booktitle> <editor> Dennis McLeod, Ron Sacks-Davis, Hans Schek (Eds.), </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <month> August </month> <year> 1990, </year> <pages> pp 162-173. </pages>
Reference-contexts: The group size is an important metric because the duration of reconstruction is likely to scale linearly with the number of disks to be read. Additionally, in very large arrays, individual disk failure will be frequent enough that highly available systems must continue operation during repair and reconstruction <ref> [Muntz90, Holland92] </ref>. Until reconstruction is complete, the group size indicates the number of disks that must be accessed to read or write an unreconstructed block on a failed disk. Moreover, the group size also indicates the number of operational disks for which user access performance is degraded by a reconstruction.
Reference: [Myers86] <author> Myers, Glenford J., Albert Y. C. Yu, David L. House, </author> <title> ``Microprocessor technology trends,'' </title> <journal> Proc. of the IEEE, </journal> <volume> Vol. 74, No. 12, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: 1. Background In recent years, processing power has increased dramatically through advanced VLSI tech nology <ref> [Myers86, Gelsinger89] </ref> and parallel architectures [Bell85, Bell89]. As processing power increases, so does the demand for increased Input/Output (I/O) performance.
Reference: [Newberg93] <author> Newberg, Lee, David Wolfe, </author> <title> ``String Layouts for a Redundant Array of Inexpensive Disks,'' </title> <journal> Algorithmica, </journal> <note> this issue, </note> <year> 1993. </year>
Reference-contexts: If parity groups are organized orthogonally to support hardware groups, data redundancy provides protection against support hardware failures as well as catastrophic disk failures [Gibson92, Gibson93]. This technique is easily extended for the 2d-parity code by organizing support hardware groups on the diagonals of the 2d array <ref> [Newberg93] </ref>. As this section has shown, high reliability does not require immediate, automatic reconstruction to idle ``hot spare'' disks.
Reference: [Patterson88] <author> Patterson, D. A., G. A. Gibson, R. H. Katz, </author> <title> ``A case for redundant arrays of inexpensive disks (RAID),'' </title> <booktitle> ACM SIGMOD 88, </booktitle> <address> Chicago, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Disk densities have been growing exponentially, cost per megabyte has been decreasing in step with density increases, and physical packaging has been achieving amazing reductions in volume [Hoagland89, Kryder89]. This trend has led some to explore the replacement of individual large form-factor drives with many smaller form-factor drives <ref> [Jilke86, Patterson88, Gibson92] </ref>, giving yet another reason to expect that future I/O systems will contain large numbers of disks. While performance improves with increasing numbers of disks, the catch is that the chance of data loss also increases.
Reference: [Peterson72] <author> Peterson, W. Wesley, E. J. Weldon, Jr., </author> <title> Error-Correcting Codes, Second Edition, </title> <publisher> M.I.T. Press, </publisher> <year> 1972, </year> <pages> pp 131-136. </pages>
Reference-contexts: However, the group size remains G +1 because only G +1 disks are involved in the reconstruction of any single failure. The t d-parity coding schemes are a member of the class of product codes that have been commonly used in magnetic tape systems <ref> [Peterson72] </ref>. A common, but expensive, technique for protecting disk systems from disk failures is known as shadowing [Bates89, Bit-ton88]. A shadowing code is equivalent to a t d-parity code with G = 1 because the parity of a single bit duplicates the value of that bit. <p> However, there is a nonbinary analogue to the binary full-2 code. This nonbinary full-2 code protects up to c (c -1)(2 b -1)/2 data symbols against double erasure with c check symbols <ref> [Gibson92, Peterson72] </ref>. In using nonbinary analogues of our binary codes to achieve low disk overhead, we may decrease reliability by introducing new sets of t +1-erasures that are not bad, and not correctable.
Reference: [Rabin89] <author> Rabin, Michael O., </author> <title> ``Efficient Dispersal of Information for Security, Load Balancing, and Fault Tolerance,'' </title> <journal> J. of the Assoc. for Comp. Mach., </journal> <volume> Vol. 36, </volume> <year> 1989. </year>
Reference-contexts: A goal in designing codes for these arrays is to make them at least as reliable as an individual disk. The use of erasure-correcting codes in large disk arrays has also been studied by Rabin <ref> [Rabin89] </ref>, and the method he proposed for recovering lost data is similar to the one we present in Section 5. However, he did not address the problem of designing codes particularly suited for use in disk arrays. This problem is a primary focus of our paper.
Reference: [Rubinstein81] <author> Rubinstein, R. Y., </author> <title> Simulation and the Monte Carlo Method, </title> <publisher> John Wiley & Sons, </publisher> <year> 1981. </year>

References-found: 33

