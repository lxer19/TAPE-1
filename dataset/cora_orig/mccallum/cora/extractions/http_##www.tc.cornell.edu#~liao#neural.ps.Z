URL: http://www.tc.cornell.edu/~liao/neural.ps.Z
Refering-URL: http://www.tc.cornell.edu/~liao/papers.html
Root-URL: http://www.tc.cornell.edu
Title: Second order methods for supervised learning of a multilayer feed-forward neural network 1  
Author: Aiping Liao 
Date: November 30, 1995  
Address: Ithaca, New York 14853.  
Affiliation: Advanced Computing Research Institute Cornell Theory Center,  
Abstract: Supervised learning of a multilayer feed-forward network can be viewed as an unconstrained nonlinear minimization problem. A commonly used method for solving the supervised learning problem is the back-propagation algorithm which is a first-order method and may suffer for its slow convergence. We show that the supervised learning can also be viewed as a discrete time optimal control (DTOC) problem. Thus the efficient second order methods we recently developed for DTOC problems can be used to solve the supervised learning problem. These methods are based on the batch processing and do not require the explicit Hessian of the objective function. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Annema, A. </author> <year> 1995. </year> <title> Feed-forward neural networks. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference: <author> Battiti, R. </author> <year> 1992. </year> <title> First- and second-order methods for learning: between steepest descent and Newton's method. </title> <booktitle> Neural Computation 4, </booktitle> <pages> 141-166. </pages>
Reference: <author> Coleman, T. F., and Liao, A. </author> <year> 1995. </year> <title> An efficient trust region method for unconstrained discrete-time optimal control problems. </title> <booktitle> Computational Optimization and Applications 4, </booktitle> <pages> 47-66. </pages>
Reference-contexts: The second-order methods we shall describe are based on our recently developed methods for discrete time optimal control (DTOC) problems <ref> (Coleman and Liao 1995, Liao 1993, Liao 1995a, and Liao 1995b) </ref>. The advantage of our approach is that it does not need the Hessian matrix explicitly and the major quantities, such as the Newton direction, can be calculated in O ( i=1 3 operations. This paper is organized as follows. <p> variable and T i;j the j-th component of the i-th transition function T i . 6 Many efficient methods have been proposed for solving this kind of problems, such as the differential dynamic programming algorithm (DDP) (Jocobson and Mayne 1970, Murray and Yakowitz 1984, Yakowitz 1988) the stagewise Newton procedure <ref> (Coleman and Liao 1995, Liao 1993, Liao 1995b, Pantoja 1988) </ref>, methods based on constrained optimization (Dunn 1993, Pantoja and Mayne 1991), and some parallel algorithms (Wright 1990), to name a few. <p> Treating the DTOC problem as an unconstrained minimization problem, we recently develop some efficient methods based on the stagewise Newton procedure of Pantoja 1988. Our methods can be classified into two categories: trust region methods <ref> (Coleman and Liao 1995, Liao 1993, Liao 1995b) </ref> and the modified New-ton method (see section 3.1). Our methods possess strong convergence properties yet very economical. In our methods the second order quantities, such as the Newton direction, are calculated based on the the stagewise Newton procedure.
Reference: <author> Craven, B. D. </author> <year> 1978. </year> <title> Mathematical Programming and Control Theory. </title> <publisher> Chapman and Hall, London. </publisher>
Reference-contexts: an optimization algorithm, the back-propagation algorithm converges slowly and cannot escape from a saddle point. 2.2 Discrete time optimal control Discrete time optimal control problems arise in many practical applications including multi-reservoir control problems (Murray and Yakowitz 1984), the treatment of polluted groundwater (Culver and Shoemaker 1992), and inventory control <ref> (Craven 1978) </ref>. By its nature, the discrete time optimal control problems are large scale problems with a dynamic structure.
Reference: <author> Culver, T. B., and Shoemaker, C. A. </author> <year> 1992. </year> <title> Dynamic optimal control for groundwater remediation with flexible management periods. </title> <booktitle> Water Resources Research 28, </booktitle> <pages> 629-641. </pages>
Reference-contexts: But as an optimization algorithm, the back-propagation algorithm converges slowly and cannot escape from a saddle point. 2.2 Discrete time optimal control Discrete time optimal control problems arise in many practical applications including multi-reservoir control problems (Murray and Yakowitz 1984), the treatment of polluted groundwater <ref> (Culver and Shoemaker 1992) </ref>, and inventory control (Craven 1978). By its nature, the discrete time optimal control problems are large scale problems with a dynamic structure.
Reference: <author> Dennis, Jr., J. E., and Schnabel, R. B. </author> <year> 1983. </year> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey. </address>
Reference: <author> Dunn, J. C. </author> <year> 1993. </year> <title> Second-order multiplier update calculations for optimal control problems and related large scale nonlinear programs. </title> <journal> SIAM J. Optimization 3, </journal> <pages> 489-502. </pages>
Reference-contexts: 6 Many efficient methods have been proposed for solving this kind of problems, such as the differential dynamic programming algorithm (DDP) (Jocobson and Mayne 1970, Murray and Yakowitz 1984, Yakowitz 1988) the stagewise Newton procedure (Coleman and Liao 1995, Liao 1993, Liao 1995b, Pantoja 1988), methods based on constrained optimization <ref> (Dunn 1993, Pantoja and Mayne 1991) </ref>, and some parallel algorithms (Wright 1990), to name a few. Treating the DTOC problem as an unconstrained minimization problem, we recently develop some efficient methods based on the stagewise Newton procedure of Pantoja 1988.
Reference: <author> Gay, D. M. </author> <year> 1979. </year> <title> Some convergence properties of Broyden's method. </title> <journal> SIAM Journal on Numerical Analysis 16, </journal> <pages> 623-630. </pages>
Reference: <author> Gill, P., and Murray, W. </author> <year> 1974. </year> <title> Newton-type methods for unconstrained and linearly constrained optimization. </title> <booktitle> Mathematical Programming 7, </booktitle> <pages> 311-350. </pages>
Reference-contexts: To overcome this difficulty, many techniques and ideas have been proposed. In this section we will concerned with two of them: the first one is the modified Newton method <ref> (see Gill and Murray 1974) </ref> which modifies the Hessian matrix by adding some positive numbers to the diagnal of the Hessian matrix whenever the Hessian matrix is found to be not sufficiently positive definite, the second one is the trust region method which is based on the quadratic model with restriction
Reference: <author> Gill, P., Murray, W., and Wright, M. </author> <year> 1981. </year> <title> Practical Optimization. </title> <publisher> Academic Press, London. </publisher>
Reference: <author> Golub, G. H., and Van Loan, C. </author> <year> 1989. </year> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland. </address>
Reference-contexts: Since the computations among the nodes in the same layer are independent they can be carried out parallelly. Moreover, since the computations in each stage of Algorithm 1 consist of only matrix-matrix productions and solving linear systems, the standard techniques of parallel scientific computing <ref> (see, for example, Golub and Van Loan 1989) </ref> can be used for these computations and the wall-clock time of the implementation of the algorithm is thus reduced. 3.2 Trust region method The trust region method is an iterative method.
Reference: <author> Harvey, R. L. </author> <year> 1994. </year> <title> Neural Network Principles. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Hassibi, B., Stork, D. G., and Wolff, G. J. </author> <year> 1993. </year> <title> Optimal brain surgeon and general network pruning. </title> <booktitle> IEEE International Conference on Neural Networks 1, </booktitle> <pages> 293-299. </pages>
Reference: <author> Haykin, S. </author> <year> 1995. </year> <title> Neural Networks. </title> <publisher> Macmillan College Publishing Company, </publisher> <address> New York. </address>
Reference: <author> Jacobson, D., and Mayne, D. </author> <year> 1970. </year> <title> Differential Dynamic Programming. </title> <publisher> Elsevier, </publisher> <address> New York. </address>
Reference: <author> LeCun, Y. </author> <year> 1985. </year> <title> Une procedure d'apprentissage pour reseau a seuil assymetrique. </title> <booktitle> Cognitiva 85, </booktitle> <pages> 599-604. </pages>
Reference: <author> Liao, A. </author> <year> 1993. </year> <title> Some efficient algorithms for unconstrained discrete-time optimal control problems. </title> <institution> Advanced Computing Research Institute, Cornell University, </institution> <note> Technical Report CTC93TR159. </note>
Reference: <author> Liao, A. </author> <year> 1994. </year> <title> A new parallel algorithm for global optimization with application to the molecular cluster problem. </title> <institution> Advanced Computing Research Institute, Cornell University, </institution> <note> Technical Report CTC94TR190. </note>
Reference-contexts: The proofs of (i) and (iii) can be found in Coleman and Liao 1995. (v) is quite obvious <ref> (see also Liao 1994) </ref>. (ii) and (iv) can be proven by direct yet tedious comparison of Algorithm 1 and the backward Gaussian elimination to the system that defines the Newton direction, i.e., Hd = g. 2 Although there are other methods for calculating the Newton direction, the stagewise Newton procedure Algorithm
Reference: <author> Liao, A. </author> <year> 1995a. </year> <title> Automatic Optimization. </title> <institution> Advanced Computing Research Institute, Cornell University, </institution> <note> Technical Report CTC95TR215. </note>
Reference: <author> Liao, A. </author> <year> 1995b. </year> <title> Solving unconstrained discrete-time optimal control problems using trust region method. </title> <institution> Cornell Computational Optimization Project, Cornell University, </institution> <type> Technical Report 95-3. </type>
Reference: <author> More, J. J., and Sorensen, D. C. </author> <year> 1983. </year> <title> Computing A Trust Region Step. </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 4, </volume> <pages> 553-572. </pages>
Reference-contexts: Set k = k + 1. 17 End Basically speaking, different ways of solving (subP) form different trust region algorithms. We thus have Gay's algorithm (Gay 1981), the More and Sorensen algorithm <ref> (More and Sorensen 1983) </ref>, and the dogleg algorithm (Powell 1970). We are to describe a very simple trust region algorithm. This algorithm is a variant of the dogleg algorithm of Powell 1970. Algorithm DogLeg Initialization.
Reference: <author> Murray, D. M., and Yakowitz, S. J. </author> <year> 1984. </year> <title> Differential dynamic programming and Newton's method for discrete optimal control problems. </title> <journal> J. of Optimization Theory and Applications 43, </journal> <pages> 395-414. </pages>
Reference-contexts: But as an optimization algorithm, the back-propagation algorithm converges slowly and cannot escape from a saddle point. 2.2 Discrete time optimal control Discrete time optimal control problems arise in many practical applications including multi-reservoir control problems <ref> (Murray and Yakowitz 1984) </ref>, the treatment of polluted groundwater (Culver and Shoemaker 1992), and inventory control (Craven 1978). By its nature, the discrete time optimal control problems are large scale problems with a dynamic structure.
Reference: <author> Nocedal, J. </author> <year> 1992. </year> <title> Theory of algorithms for unconstrained optimization. </title> <booktitle> Acta Numerica 1, </booktitle> <pages> 199-242. </pages>
Reference: <author> Pantoja, J. F. A. De O. </author> <year> 1988. </year> <title> Differential dynamic programming and Newton's method. </title> <editor> Intnl. J. </editor> <booktitle> Control 47, </booktitle> <pages> 1539-1553. </pages>
Reference: <author> Pantoja, J. F. A. De O., and Mayne, D. Q. </author> <year> 1991. </year> <title> Sequential quadratic programming algorithm for discrete optimal control problems with control inequality constraints. </title> <editor> Intnl. J. </editor> <booktitle> Control 53, </booktitle> <pages> 823-836. </pages>
Reference: <author> Parker, D. B. </author> <year> 1985. </year> <title> Learning-logic: Casting the cortex of the human brain in silicon. </title> <institution> Center for Computational Research in Economics and Management Science, MIT, </institution> <note> Technical Report TR-47. 24 Powell, </note> <author> M. J. D. </author> <year> 1970. </year> <title> A new algorithm for unconstrained optimization. In Non--linear Programming, </title> <editor> J. B. Rosen, O. Mangasarian, and K. Ritter, eds. </editor> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <editor> D. E. Rumelhart and J. L. McClelland, eds. </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Saarinen, S., Bramley, R. B., and Cybenko, G. </author> <year> 1991. </year> <title> The numerical solution of neural network training problems. </title> <institution> Center for Supercomputing Research and Development, University of Illinois, </institution> <note> CRSD Report No. 1089. </note>
Reference-contexts: We thus hope that the difficulty of ill-conditioning of the Jacobian can be improved by using a different form of the sigmoidal function. Nevertheless, it is probably a good technique to rewrite the least squares function as follows <ref> (with the terminology of Saarinen et al. 1991) </ref> (x) = 2 i i := 2 i where f i (x) = F (t i ; x) t i .
Reference: <author> Saarinen, S., Bramley, R. B., and Cybenko, G. </author> <year> 1992. </year> <title> Neural networks, backpropagation, and automatic differentiation. In Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. F. Corliss, eds. </editor> <publisher> SIAM, </publisher> <address> Philadelphia. </address>
Reference: <author> Wright, S. J. </author> <year> 1990. </year> <title> Solution of discrete-time optimal control problems on parallel computers. </title> <booktitle> Parallel Computing 16, </booktitle> <pages> 221-237. </pages>
Reference-contexts: kind of problems, such as the differential dynamic programming algorithm (DDP) (Jocobson and Mayne 1970, Murray and Yakowitz 1984, Yakowitz 1988) the stagewise Newton procedure (Coleman and Liao 1995, Liao 1993, Liao 1995b, Pantoja 1988), methods based on constrained optimization (Dunn 1993, Pantoja and Mayne 1991), and some parallel algorithms <ref> (Wright 1990) </ref>, to name a few. Treating the DTOC problem as an unconstrained minimization problem, we recently develop some efficient methods based on the stagewise Newton procedure of Pantoja 1988.
Reference: <author> Yakowitz, S. </author> <year> 1988. </year> <title> Theoretical and computational advances in differential dynamic programming. </title> <journal> Control and Cybernetics 17, </journal> <pages> 173-189. 25 </pages>
References-found: 31

