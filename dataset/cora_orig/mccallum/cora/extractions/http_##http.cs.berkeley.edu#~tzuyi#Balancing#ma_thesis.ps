URL: http://http.cs.berkeley.edu/~tzuyi/Balancing/ma_thesis.ps
Refering-URL: http://http.cs.berkeley.edu/~tzuyi/Balancing/ma_thesis_abstract.html
Root-URL: 
Title: Balancing Sparse Matrices for Computing Eigenvalues  
Author: Tzu-Yi Chen 
Date: May 26, 1998  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <booktitle> Addison-Wesley Series in Computer Science and Information Processing. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1974. </year>
Reference-contexts: This corresponds to finding the strongly connected components of the directed graph and doing a topological sort on the components, therefore we implemented an algorithm which uses two depth first searches to locate the strongly 31 connected components of a directed graph (see [20] for the original paper or <ref> [1] </ref> for a descrip-tion).
Reference: [2] <author> Z. Bai, D. Day, J. Demmel, and J. Dongarra. </author> <title> A test matrix collection for non-hermitian eigenvalue problems. </title> <type> Technical Report CS-97-355, </type> <institution> University of Tennessee, </institution> <month> Mar </month> <year> 1997. </year>
Reference-contexts: Table 3.1 compares the size of the largest block found when looking for strongly connected components with the size of the large block found by the Parlett-Reinsch algorithm (the block labeled C in Equation 3.1) on a variety of sparse matrices taken from practical applications (from <ref> [2] </ref>, and discussed in Appendix A). The permutation found by the Parlett-Reinsch algorithm is a special case of the more general strongly connected components algorithm. The Parlett-Reinsch algorithm begins by looking for a sink node s, which is a specific type of 1 fi 1 strongly connected component.
Reference: [3] <author> R. Barrett, M. Berry, T. F. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. van der Vorst. </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1994. </year>
Reference-contexts: The fourth column of Table 3.4 shows the best results using A and a vector of all 1s instead of jAj and a random z. difference between the Jacobi and Gauss-Seidel iterative methods for solving systems of linear equations (see, for example, <ref> [3] </ref>). 45 Again, the best result with jAj is better than the best result with A for all matrices. This makes sense given our intuition that exact row and column norms are what we want when we multiply A by a random vector of 1s.
Reference: [4] <author> S. Boyd, L. El Fhaoui, E. Feron, and V. Balakrishnan. </author> <title> Linear Matrix Inequalities in System and Control Theory. </title> <booktitle> Number 15 in SIAM studies in applied mathematics. </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1994. </year>
Reference-contexts: We discuss the algorithm in greater detail in Section 3.2.2. Boyd et al. <ref> [4] </ref> formulate the problem of finding the minimum attainable 2-norm of a matrix A via diagonal scaling, where A may not be non-negative, as a generalized eigenvalue problem which they note can be solved both in polynomial-time theoretically and very efficiently in practice. <p> a matrix and also notes connections to minimizing matrix norms. 25 Minimizing the Norm of a Matrix requires how to find lower norm unique? A 0? scaling matrix? bound attainable? 1 Y [19] N use Perron vector [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP <ref> [4] </ref> (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i j i j 2 [19] iff
Reference: [5] <author> J. W. Demmel. </author> <title> Applied Numerical Linear Algebra. </title> <publisher> SIAM, </publisher> <year> 1997. </year>
Reference-contexts: Each edge weight is then multiplied by the scale factor of the node it exits and divided by the scale factor of the node it enters. Furthermore, a matrix is irreducible if and only if its graph is strongly connected (for example, see Lemma 6:6 in <ref> [5] </ref>). The definition of a strongly connected graph is one where there is a directed path from every node to every other node. The definition of a strongly connected component of a graph is a subgraph which is strongly connected and which cannot be made larger and stay strongly connected.
Reference: [6] <author> J. W. Demmel. </author> <type> Personal communication, </type> <year> 1998. </year>
Reference-contexts: However, if M contains more than one balanced matrix, then we need to prove that there is only one balanced matrix to which S can converge. The next theorem states that given an irreducible matrix A, there is exactly one accumulation point for the iterative balancing scheme <ref> [6] </ref>. We begin by proving two lemmas. 18 Lemma 23 If A is an irreducible matrix and A 0 = DAD 1 , then D is determined uniquely, up to a scalar multiple, by A and A 0 . <p> Also interesting is the fact that the non-zero eigenvalues lie along a tear drop shape when plotted in the complex plane, as shown in the left half of Figure 2.3 for n = 200 <ref> [6] </ref>. This shape can be roughly explained by the analysis used in [17], where the authors investigate the eigenvalues of Toeplitz matrices. In our case the matrix A is not Toeplitz, however the trailing n 1 by n 1 submatrix A 0 is both Toeplitz and lower Hessenberg. <p> Given this relationship between the scaling matrix and the condition number of the eigenvalues, a scaling matrix D can be chosen to minimize the condition number of an eigenvalue directly, instead of choosing a D which balances A and hoping that c is reduced <ref> [6] </ref>. We find a D to minimize the condition number of an eigenvalue by using the spectral projection corresponding to . <p> Instead, we approximate jAje by Az and jA T je by A T z, where z is a vector whose elements are randomly chosen 1s. We calculate bounds on how good an approximation to expect <ref> [6] </ref>. Let s i , for i = 1 : : : n, be an independent random variable whose value is +1 or 1 with equal probability.
Reference: [7] <author> B. C. Eaves, A. J. Hoffman, U. G. Rothblum, and H. Schneider. </author> <title> Line-sum-symmetric scalings of square nonnegative matrices. </title> <booktitle> In Mathematical Programming Study 25, </booktitle> <pages> pages 124-141. </pages> <year> 1985. </year>
Reference-contexts: Hartfiel [10] proved that if A is irreducible, then there exists a diagonal scaling matrix, unique up to scalar multiples, which balances A. Eaves et al. <ref> [7] </ref> further showed that this diagonal scaling matrix could be calculated by minimizing a function over the positive orthant. Also for the vector 1-norm, Kalantari et al. [11] showed that balancing a nonnegative, irreducible matrix A with zeros on the diagonal is equivalent to solving a convex minimization program. <p> A question mark in the table means the answer is still unknown. Balancing a Matrix requires norm unique? relation to matrix norm A 0? how to find scaling matrix? 1 Y [10] minimizes sum of elements <ref> [7] </ref> Y find vector to minimize function [7] 2 Y [14] minimizes F-norm [14] N iterative algorithm [14, 15] 1 N (Sec 2.3) (*) (Th. 10) N cycle-based algorithm (Sec. 2.3.2) iterative algorithm (Sec 2.3.3, [15]) w Y minimizes 2-norm Y use left and right Perron vectors (*) Knowing that the <p> A question mark in the table means the answer is still unknown. Balancing a Matrix requires norm unique? relation to matrix norm A 0? how to find scaling matrix? 1 Y [10] minimizes sum of elements <ref> [7] </ref> Y find vector to minimize function [7] 2 Y [14] minimizes F-norm [14] N iterative algorithm [14, 15] 1 N (Sec 2.3) (*) (Th. 10) N cycle-based algorithm (Sec. 2.3.2) iterative algorithm (Sec 2.3.3, [15]) w Y minimizes 2-norm Y use left and right Perron vectors (*) Knowing that the largest entry in the matrix is no
Reference: [8] <author> J. R. Gilbert. </author> <type> Personal communication, </type> <month> Jan </month> <year> 1997. </year>
Reference-contexts: the correct permutation was known (for example, permuted upper triangular matrices), and on other test matrices, where we compared the sparsity structure of the permuted matrix against that of matrices returned by a MATLAB function which also finds and returns the strongly connected components of a matrix in topological order <ref> [8] </ref>. To test the correctness of the balancing phase of spbalance, we compared its output 33 on assorted matrices to the output of gebal. Both gebal and spbalance have an input parameter that specifies whether to do only the permuting phase, only the balancing phase, or both.
Reference: [9] <author> J. Grad. </author> <title> Matrix balancing. </title> <journal> The Computer Journal, </journal> <volume> 14(3) </volume> <pages> 280-284, </pages> <year> 1971. </year>
Reference-contexts: Their definition of a generalized eigenvalue problem is trying "to minimize the maximum generalized eigenvalue of a pair of matrices that depend affinely on a variable, subject to a linear matrix inequality constraint". Results on balancing in the 1-norm are prominent in the literature. Grad <ref> [9] </ref> proved that when the diagonal scaling factors are not limited to powers of the machine base, the iterative method given in [14] and [15] converges. Hartfiel [10] proved that if A is irreducible, then there exists a diagonal scaling matrix, unique up to scalar multiples, which balances A.
Reference: [10] <author> D. J. Hartfiel. </author> <title> Concerning diagonal similarity of irreducible matrices. </title> <journal> Proceedings of the American Mathematical Society, </journal> <volume> 30(3) </volume> <pages> 419-425, </pages> <month> Nov </month> <year> 1971. </year>
Reference-contexts: Results on balancing in the 1-norm are prominent in the literature. Grad [9] proved that when the diagonal scaling factors are not limited to powers of the machine base, the iterative method given in [14] and [15] converges. Hartfiel <ref> [10] </ref> proved that if A is irreducible, then there exists a diagonal scaling matrix, unique up to scalar multiples, which balances A. Eaves et al. [7] further showed that this diagonal scaling matrix could be calculated by minimizing a function over the positive orthant. <p> A question mark in the table means the answer is still unknown. Balancing a Matrix requires norm unique? relation to matrix norm A 0? how to find scaling matrix? 1 Y <ref> [10] </ref> minimizes sum of elements [7] Y find vector to minimize function [7] 2 Y [14] minimizes F-norm [14] N iterative algorithm [14, 15] 1 N (Sec 2.3) (*) (Th. 10) N cycle-based algorithm (Sec. 2.3.2) iterative algorithm (Sec 2.3.3, [15]) w Y minimizes 2-norm Y use left and right Perron
Reference: [11] <author> B. Kalantari, L. Khachiyan, and A. Shokoufandeh. </author> <title> On the complexity of matrix balancing. </title> <journal> SIAM Journal of Matrix Analysis and Applications, </journal> <volume> 18(2) </volume> <pages> 450-463, </pages> <month> Apr </month> <year> 1997. </year>
Reference-contexts: Eaves et al. [7] further showed that this diagonal scaling matrix could be calculated by minimizing a function over the positive orthant. Also for the vector 1-norm, Kalantari et al. <ref> [11] </ref> showed that balancing a nonnegative, irreducible matrix A with zeros on the diagonal is equivalent to solving a convex minimization program.
Reference: [12] <author> R. M. Karp. </author> <title> A characterization of the minimum cycle mean in a digraph. </title> <journal> Discrete Mathematics, </journal> <volume> 23 </volume> <pages> 309-311, </pages> <year> 1978. </year>
Reference-contexts: Lemma 18 Given a graph G, the directed cycle with maximum cycle geometric mean can be found in time O (n nnz), where nnz is the number of edges in G. O (n 2 ) space is needed. Proof: In <ref> [12] </ref>, Karp gives an algorithm for finding the directed cycle with minimum cycle mean in O (n nnz) time.
Reference: [13] <author> R. B. Lehoucq and J. A. Scott. </author> <title> An evaluation of software for computing eigenvalues of sparse nonsymmetric matrices. </title> <note> Submitted to ACM Transactions on Mathematical Software, 1996. 53 </note>
Reference-contexts: There are sparse eigensolvers which use only matrix-vector multiplications to access the matrix (see <ref> [13] </ref> for a survey article); a balancing algorithm which similarly uses only matrix-vector multiplications can be used as a preconditioner for these eigensolvers. 38 In addition to a function for computing Az, sometimes another function is provided which computes A T z.
Reference: [14] <author> E. E. Osborne. </author> <title> On pre-conditioning of matrices. </title> <journal> Journal of the Association for Com--puting Machinery, </journal> <volume> 7 </volume> <pages> 338-345, </pages> <year> 1960. </year>
Reference-contexts: Osborne first noted that the norm of a matrix A can often be reduced with a similarity transform of the form B = D 1 AD (1.1) where D is a diagonal matrix <ref> [14] </ref>. <p> In Chapter 3 we turn to the practical side of balancing matrices, focusing on balancing sparse matrices. The previous work on implementing balancing algorithms consists of the Parlett-Reinsch algorithm for balancing dense matrices [15], which is based on Osborne's work <ref> [14] </ref>. In Section 3.2.2 we describe the Parlett-Reinsch algorithm, which consists of a permuting phase followed by a balancing phase. In Section 3.2.3 we describe our improvement to the permuting step; this is followed by a description of our implementation of the balancing phase for sparse matrices in Section 3.2.4. <p> The previous work proves relationships between norm minimization and balancing, and various facts about norm minimization in assorted norms. Osborne <ref> [14] </ref> showed that minimizing the Frobenius norm of a matrix A using diagonal scaling is the same as balancing A in the 2-norm. He gives an iterative algorithm for finding the diagonal scaling matrix, proving that the algorithm converges to the best solution if A is irreducible. <p> He also shows that the diagonal scaling matrix is unique up to multiplication by a scalar. Parlett and Reinsch [15] give a general algorithm which resembles the iterative algorithm in <ref> [14] </ref> for balancing matrices in any vector norm. This paper focuses on practical aspects of balancing, hence the code given in the paper balances approximately rather than exactly and the authors state without proof that the algorithm converges after a finite number of 6 iterations. <p> Results on balancing in the 1-norm are prominent in the literature. Grad [9] proved that when the diagonal scaling factors are not limited to powers of the machine base, the iterative method given in <ref> [14] </ref> and [15] converges. Hartfiel [10] proved that if A is irreducible, then there exists a diagonal scaling matrix, unique up to scalar multiples, which balances A. Eaves et al. [7] further showed that this diagonal scaling matrix could be calculated by minimizing a function over the positive orthant. <p> [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y <ref> [14] </ref> N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by diag. [19] if irreducible can attain lower bound for that matrix [19] Table 2.3: This table summarizes known results on minimizing a matrix norm by diagonal scaling. <p> [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y <ref> [14] </ref> N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by diag. [19] if irreducible can attain lower bound for that matrix [19] Table 2.3: This table summarizes known results on minimizing a matrix norm by diagonal scaling. <p> A question mark in the table means the answer is still unknown. Balancing a Matrix requires norm unique? relation to matrix norm A 0? how to find scaling matrix? 1 Y [10] minimizes sum of elements [7] Y find vector to minimize function [7] 2 Y <ref> [14] </ref> minimizes F-norm [14] N iterative algorithm [14, 15] 1 N (Sec 2.3) (*) (Th. 10) N cycle-based algorithm (Sec. 2.3.2) iterative algorithm (Sec 2.3.3, [15]) w Y minimizes 2-norm Y use left and right Perron vectors (*) Knowing that the largest entry in the matrix is no greater than gives: <p> A question mark in the table means the answer is still unknown. Balancing a Matrix requires norm unique? relation to matrix norm A 0? how to find scaling matrix? 1 Y [10] minimizes sum of elements [7] Y find vector to minimize function [7] 2 Y <ref> [14] </ref> minimizes F-norm [14] N iterative algorithm [14, 15] 1 N (Sec 2.3) (*) (Th. 10) N cycle-based algorithm (Sec. 2.3.2) iterative algorithm (Sec 2.3.3, [15]) w Y minimizes 2-norm Y use left and right Perron vectors (*) Knowing that the largest entry in the matrix is no greater than gives: jjA bal jj <p> Balancing a Matrix requires norm unique? relation to matrix norm A 0? how to find scaling matrix? 1 Y [10] minimizes sum of elements [7] Y find vector to minimize function [7] 2 Y [14] minimizes F-norm [14] N iterative algorithm <ref> [14, 15] </ref> 1 N (Sec 2.3) (*) (Th. 10) N cycle-based algorithm (Sec. 2.3.2) iterative algorithm (Sec 2.3.3, [15]) w Y minimizes 2-norm Y use left and right Perron vectors (*) Knowing that the largest entry in the matrix is no greater than gives: jjA bal jj 1 fl (maximum nnz <p> The iterative nature of this balancing algorithm is based on Osborne's algorithm for balancing in the 2-norm <ref> [14] </ref>. Though the algorithm in [15] can be adapted to balance in any norm, the code given in the paper balances in the 1-norm. Parlett and Reinsch chose the 1-norm because balancing in the 1-norm requires fewer multiplications than balancing in the 2-norm. <p> One explanation comes from the standard iterative algorithm used in <ref> [14, 15] </ref>, the other is based on Perron-Frobenius theory and weighted balancing. Notice that because we have both A and A T , we can approximate both row and column norms.
Reference: [15] <author> B. Parlett and C. Reinsch. </author> <title> Balancing a matrix for calculation of eigenvalues and eigenvectors. </title> <journal> Numer. Math., </journal> <volume> 13 </volume> <pages> 293-304, </pages> <year> 1969. </year>
Reference-contexts: Section 2.3 turns to balancing in the infinity norm | here we present a new finite algorithm which runs in O (n 4 ) time for exact balancing in the infinity norm, and prove the convergence of the traditional iterative balancing algorithm given in <ref> [15] </ref>. Just as a matrix A can be diagonally scaled to balance A or to minimize its norm, A can be scaled by a diagonal matrix D so that the condition numbers of some of A's eigenvalues are reduced. <p> In Chapter 3 we turn to the practical side of balancing matrices, focusing on balancing sparse matrices. The previous work on implementing balancing algorithms consists of the Parlett-Reinsch algorithm for balancing dense matrices <ref> [15] </ref>, which is based on Osborne's work [14]. In Section 3.2.2 we describe the Parlett-Reinsch algorithm, which consists of a permuting phase followed by a balancing phase. <p> We begin by characterizing matrices balanced in the infinity norm in Section 2.3.1, then present a novel O (n 4 ) algorithm for balancing in the infinity norm in Section 2.3.2. We then analyze the convergence of the more typical iterative balancing algorithm described in <ref> [15] </ref> in Section 2.3.3. We show the iterative algorithm converges to a balanced matrix and analyze the convergence rate of two special types of matrices. In Section 2.4 we discuss finding diagonal scaling matrices to minimize eigenvalue condition numbers. <p> He gives an iterative algorithm for finding the diagonal scaling matrix, proving that the algorithm converges to the best solution if A is irreducible. He also shows that the diagonal scaling matrix is unique up to multiplication by a scalar. Parlett and Reinsch <ref> [15] </ref> give a general algorithm which resembles the iterative algorithm in [14] for balancing matrices in any vector norm. <p> Results on balancing in the 1-norm are prominent in the literature. Grad [9] proved that when the diagonal scaling factors are not limited to powers of the machine base, the iterative method given in [14] and <ref> [15] </ref> converges. Hartfiel [10] proved that if A is irreducible, then there exists a diagonal scaling matrix, unique up to scalar multiples, which balances A. Eaves et al. [7] further showed that this diagonal scaling matrix could be calculated by minimizing a function over the positive orthant. <p> In Section 2.3.2 we describe this algorithm and prove that it balances exactly in O (n 4 ) time if the matrix is dense and in less time if the matrix is sparse. The second algorithm is the iterative scheme described in <ref> [15] </ref>. In Section 2.3.3.1 we prove this algorithm converges to a balanced matrix. The proof of convergence implies the convergence rate of the iterative algorithm is related to its convergence rate on a subgraph G s of the original graph. <p> However, in each iteration we can reuse the space. Therefore O (n 2 ) space is sufficient. Since nnz n 2 , the running time is also bounded by O (n 4 ). 16 2.3.3 An Iterative Algorithm for Balancing in the Infinity Norm The iterative algorithm in <ref> [15] </ref> can be used to balance matrices in the infinity norm. <p> Balancing a Matrix requires norm unique? relation to matrix norm A 0? how to find scaling matrix? 1 Y [10] minimizes sum of elements [7] Y find vector to minimize function [7] 2 Y [14] minimizes F-norm [14] N iterative algorithm <ref> [14, 15] </ref> 1 N (Sec 2.3) (*) (Th. 10) N cycle-based algorithm (Sec. 2.3.2) iterative algorithm (Sec 2.3.3, [15]) w Y minimizes 2-norm Y use left and right Perron vectors (*) Knowing that the largest entry in the matrix is no greater than gives: jjA bal jj 1 fl (maximum nnz <p> norm A 0? how to find scaling matrix? 1 Y [10] minimizes sum of elements [7] Y find vector to minimize function [7] 2 Y [14] minimizes F-norm [14] N iterative algorithm [14, 15] 1 N (Sec 2.3) (*) (Th. 10) N cycle-based algorithm (Sec. 2.3.2) iterative algorithm (Sec 2.3.3, <ref> [15] </ref>) w Y minimizes 2-norm Y use left and right Perron vectors (*) Knowing that the largest entry in the matrix is no greater than gives: jjA bal jj 1 fl (maximum nnz in a column) jjA bal jj 1 fl (maximum nnz in a row) jjA bal jj F 2 <p> Section 3.1 summarizes the previous work on implementing balancing algorithms, which consists mainly of the Parlett-Reinsch algorithm <ref> [15] </ref>. In Section 3.2 we discuss our implementation of a sparse variant of the Parlett-Reinsch algorithm. Like the Parlett-Reinsch algorithm, our algorithm spbalance consists of a permutation phase and a balancing phase. <p> the matrix returned by the direct algorithm, spbalance. 3.1 Previous Work The balancing code found in linear algebra packages such as EISPACK (under the name balanc), LAPACK (under the name gebal 1 ), and MATLAB (under the name balance) is based on the algorithm and code by Parlett and Reinsch <ref> [15] </ref>. The Parlett-Reinsch algorithm begins by executing a permutation step. Although many papers on balancing assume an irreducible matrix, the Parlett-Reinsch algorithm works on reducible matrices. <p> The algorithm continues iterating until it has iterated once over all the rows and columns and found that no row/column pair can be scaled by a power of the machine base so that the sum of the row and column elements is reduced by some constant factor (:95 in <ref> [15] </ref>). The iterative nature of this balancing algorithm is based on Osborne's algorithm for balancing in the 2-norm [14]. Though the algorithm in [15] can be adapted to balance in any norm, the code given in the paper balances in the 1-norm. <p> be scaled by a power of the machine base so that the sum of the row and column elements is reduced by some constant factor (:95 in <ref> [15] </ref>). The iterative nature of this balancing algorithm is based on Osborne's algorithm for balancing in the 2-norm [14]. Though the algorithm in [15] can be adapted to balance in any norm, the code given in the paper balances in the 1-norm. Parlett and Reinsch chose the 1-norm because balancing in the 1-norm requires fewer multiplications than balancing in the 2-norm. <p> A complete description of the Parlett-Reinsch algorithm can be found in <ref> [15] </ref>. The Parlett-Reinsch algorithm has two phases. The first of the two phases permutes the rows and columns of the matrix so that rows which isolate an eigenvalue are at the bottom of the matrix and columns which isolate an eigenvalue are at the left of the matrix. <p> One explanation comes from the standard iterative algorithm used in <ref> [14, 15] </ref>, the other is based on Perron-Frobenius theory and weighted balancing. Notice that because we have both A and A T , we can approximate both row and column norms.
Reference: [16] <author> S. Pissanetzky. </author> <title> Sparse Matrix Technology. </title> <publisher> Academic Press Inc, </publisher> <year> 1984. </year>
Reference-contexts: The remaining arrays help with the row traversals needed to find row sums in the balancing phase of the algorithm. Two of the other three arrays implement a storage method described in <ref> [16] </ref>. One array of size n, rowbeg, points to the first element in each row. In another array of length nnz, rowptr, each element points to the location of the next element in the same row. These arrays simplify row traversal.
Reference: [17] <author> L. Reichel and L. N. Trefethen. </author> <title> Eigenvalues and pseudo-eigenvalues of Toeplitz matrices. Linear ALgebra and its Applications, </title> <address> 162-164:153-185, </address> <year> 1992. </year>
Reference-contexts: Also interesting is the fact that the non-zero eigenvalues lie along a tear drop shape when plotted in the complex plane, as shown in the left half of Figure 2.3 for n = 200 [6]. This shape can be roughly explained by the analysis used in <ref> [17] </ref>, where the authors investigate the eigenvalues of Toeplitz matrices. In our case the matrix A is not Toeplitz, however the trailing n 1 by n 1 submatrix A 0 is both Toeplitz and lower Hessenberg. <p> 5, A 0 looks as follows: A 0 = 6 4 2 3 2 2 2 1 fl 2 4 2 3 2 2 7 5 (2.8) In general, for any n, if we set the lower left entry to 2 n , hence continuing the pattern, the analysis in <ref> [17] </ref> claims the function f (z) = 1=(z (2 z)), evaluated where jzj = 1, should give us the curve along which the eigenvalues lie. In Figure 2.3 both the function and the eigenvalues of the matrix (for n = 200) are plotted. The two match well. <p> We currently lack an explanation for why dropping the first column, dropping the first row, and replacing the element A (n; 2) with an appropriate power of 2 still permits the analysis in <ref> [17] </ref> to work.
Reference: [18] <author> W. </author> <title> Rudin. </title> <booktitle> Principles of Mathematical Analysis. International Series in Pure and Applied Mathematics. </booktitle> <publisher> McGraw-Hill, Inc., </publisher> <address> New York, third edition, </address> <year> 1976. </year>
Reference-contexts: To prove convergence of the sequence S, we use the fact that every sequence in a compact metric space has at least one accumulation point in that space (see, for example, <ref> [18] </ref>, Theorem 3.6). After showing that we are in a compact space, we show that every accumulation point corresponds to a balanced matrix. We begin by defining a closed, compact set which contains every matrix in the sequence S. <p> factors are positive. (A Proposition 9 gives a tighter lower bound for the non-zero elements of the matrices in S.) If we define the set of matrices M to be all matrices whose entries are in the range [0; m], this set is closed and bounded and therefore compact ( <ref> [18] </ref>, theorem 2.41). M contains all the matrices in the sequence S. Therefore S has at least one accumulation point in M. Let B be any accumulation point in the set. We show B must be a balanced matrix. Theorem 22 B is a balanced matrix.
Reference: [19] <author> T. Strom. </author> <title> Minimization of norms and logarithmic norms by diagonal similarities. </title> <journal> Computing, </journal> <volume> 10 </volume> <pages> 1-7, </pages> <year> 1972. </year>
Reference-contexts: If B = DAD 1 , they define the relative error in the balancing of A by D as jjBe B T ejj 2 =(e T Be). Strom <ref> [19] </ref> considered using diagonal scaling to minimize various norms of a matrix, independent of balancing. For the matrix infinity norm, Strom proved that (jAj) is a lower bound on jjDAD 1 jj 1 . <p> We then prove that weighted balancing achieves the minimum 2-norm (A), hence drawing another connection between balancing and minimizing matrix norms. These results extend Strom's work, in which he shows that a companion matrix C can be scaled to achieve the minimum 2-norm (jCj) <ref> [19] </ref>. 7 Definition 1 An irreducible, non-negative matrix A is balanced in the weighted sense if A (i; :)z = z T A (:; i) for all i = 1 : : : n, where z is the eigenvector corresponding to the largest eigenvalue of A. <p> Table 2.4 summarizes results on using diagonal matrices to balance a matrix and also notes connections to minimizing matrix norms. 25 Minimizing the Norm of a Matrix requires how to find lower norm unique? A 0? scaling matrix? bound attainable? 1 Y <ref> [19] </ref> N use Perron vector [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if <p> Table 2.4 summarizes results on using diagonal matrices to balance a matrix and also notes connections to minimizing matrix norms. 25 Minimizing the Norm of a Matrix requires how to find lower norm unique? A 0? scaling matrix? bound attainable? 1 Y <ref> [19] </ref> N use Perron vector [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] <p> 2.4 summarizes results on using diagonal matrices to balance a matrix and also notes connections to minimizing matrix norms. 25 Minimizing the Norm of a Matrix requires how to find lower norm unique? A 0? scaling matrix? bound attainable? 1 Y <ref> [19] </ref> N use Perron vector [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] <p> on using diagonal matrices to balance a matrix and also notes connections to minimizing matrix norms. 25 Minimizing the Norm of a Matrix requires how to find lower norm unique? A 0? scaling matrix? bound attainable? 1 Y <ref> [19] </ref> N use Perron vector [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i <p> and also notes connections to minimizing matrix norms. 25 Minimizing the Norm of a Matrix requires how to find lower norm unique? A 0? scaling matrix? bound attainable? 1 Y <ref> [19] </ref> N use Perron vector [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by <p> of a Matrix requires how to find lower norm unique? A 0? scaling matrix? bound attainable? 1 Y <ref> [19] </ref> N use Perron vector [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by diag. [19] if irreducible can attain lower bound for that matrix [19] <p> unique? A 0? scaling matrix? bound attainable? 1 Y <ref> [19] </ref> N use Perron vector [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by diag. [19] if irreducible can attain lower bound for that matrix [19] Table 2.3: This table summarizes known results on minimizing <p> 0? scaling matrix? bound attainable? 1 Y <ref> [19] </ref> N use Perron vector [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by diag. [19] if irreducible can attain lower bound for that matrix [19] Table 2.3: This table summarizes known results on minimizing a matrix <p> attainable? 1 Y <ref> [19] </ref> N use Perron vector [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by diag. [19] if irreducible can attain lower bound for that matrix [19] Table 2.3: This table summarizes known results on minimizing a matrix norm by diagonal scaling. <p> Y <ref> [19] </ref> N use Perron vector [19] (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by diag. [19] if irreducible can attain lower bound for that matrix [19] Table 2.3: This table summarizes known results on minimizing a matrix norm by diagonal scaling. <p> use Perron vector <ref> [19] </ref> (jA T j) [19] if irreducible [19] 2 ? N solve GEVP [4] (A) [19] ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by diag. [19] if irreducible can attain lower bound for that matrix [19] Table 2.3: This table summarizes known results on minimizing a matrix norm by diagonal scaling. <p> GEVP [4] (A) <ref> [19] </ref> ? 2 Y (Th. 1) Y use l,r Perron vectors (A) [19] if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by diag. [19] if irreducible can attain lower bound for that matrix [19] Table 2.3: This table summarizes known results on minimizing a matrix norm by diagonal scaling. A question mark in the table means the answer is still unknown. <p> 2 Y (Th. 1) Y use l,r Perron vectors (A) <ref> [19] </ref> if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by diag. [19] if irreducible can attain lower bound for that matrix [19] Table 2.3: This table summarizes known results on minimizing a matrix norm by diagonal scaling. A question mark in the table means the answer is still unknown. <p> <ref> [19] </ref> if irreducible (Th. 1) (Th. 1) 1 Y [19] N [19] use Perron vector [19] (jAj) [19] if irreducible [19] F Y [14] N iterative algorithm [14] p P i j i j 2 [19] iff normalizable by diag. [19] if irreducible can attain lower bound for that matrix [19] Table 2.3: This table summarizes known results on minimizing a matrix norm by diagonal scaling. A question mark in the table means the answer is still unknown. <p> The pseudocode can be rewritten to use only black box functions A (x) and A T (x) in a similar fashion. 3.3.3.1 Theory Given a matrix A, we know the lower bound on the infinity norm of DAD 1 is (jAj) <ref> [19] </ref>. If A is irreducible and x is the right Perron vector of jAj, D = diag (1=x (1); 1=x (2); : : :; 1=x (n)) achieves the lower bound. Since x is the eigenvector corresponding to the largest eigenvalues of jAj, x can be calculated using the power method.
Reference: [20] <author> R. Tarjan. </author> <title> Depth-first search and linear graph algorithms. </title> <journal> SIAM Journal of Computing, </journal> <volume> 1(2) </volume> <pages> 146-160, </pages> <month> Jun </month> <year> 1972. </year>
Reference-contexts: This corresponds to finding the strongly connected components of the directed graph and doing a topological sort on the components, therefore we implemented an algorithm which uses two depth first searches to locate the strongly 31 connected components of a directed graph (see <ref> [20] </ref> for the original paper or [1] for a descrip-tion). <p> If this reversed graph (instead of the original graph) is given to Tarjan's algorithm for finding strongly connected components <ref> [20] </ref>, the nodes are returned in the correct order . . . that is, the first node returned corresponds to the first row and column of the permuted matrix, rather than to the last row and column of the permuted matrix.
Reference: [21] <author> L. N. Trefethan and M. Giles. </author> <type> Personal communication, </type> <month> Mar </month> <year> 1998. </year>
Reference-contexts: To determine the convergence rate of A k w, we need the magnitude of the second largest eigenvalue of A. We can compute all the eigenvalues of A by using the following ansatz <ref> [21] </ref>. Assume that an eigenvector has the form v = [a b b 2 b 3 : : : b n1 ] (2.4) Furthermore, assume that (a + b)=2 = b n = ab. Now look at Av.
Reference: [22] <author> R. S. Varga. </author> <title> Matrix Iterative Analysis. Series in Automatic Computation. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1962. </year> <month> 54 </month>
Reference-contexts: Proof: By definition, jjBjj 2 2 = (BB T ). Since Bz = qz and B T z = qz, BB T z = q 2 z. Since B 0, BB T 0. In addition, q 2 = (BB T ) (Theorem 2:2 in <ref> [22] </ref>). Therefore, jjBjj 2 = q. Reducing the norm of a matrix is a typical goal of balancing and we have shown weighted balancing achieves this goal. 2.3 Balancing in the Infinity Norm This section examines balancing in the infinity norm. <p> Then =n e . Both bounds are attainable. Proof: Balancing in the infinity norm depends only on the magnitudes of the elements, therefore we can consider jAj instead of A. Let B be jAj after balancing. From the theory of non-negative matrices (see <ref> [22] </ref>, Chapter 2), we know that min 0 n X b ij A max 0 n X b ij A By Lemma 7, the largest entry in B is e, therefore the largest a row sum can be is ne. Therefore ne, or =n e.
References-found: 22

