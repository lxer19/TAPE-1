URL: http://www.bell-labs.com/user/seung/papers/continuous.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00428.html
Root-URL: 
Email: seung@bell-labs.com  
Title: Learning Continuous Attractors in Recurrent Networks  
Author: H. Sebastian Seung 
Address: Murray Hill, NJ 07974  
Affiliation: Bell Labs, Lucent Technologies  
Abstract: One approach to invariant object recognition employs a recurrent neural network as an associative memory. In the standard depiction of the network's state space, memories of objects are stored as attractive fixed points of the dynamics. I argue for a modification of this picture: if an object has a continuous family of instantiations, it should be represented by a continuous attractor. This idea is illustrated with a network that learns to complete patterns. To perform the task of filling in missing information, the network develops a continuous attractor that models the manifold from which the patterns are drawn. From a statistical viewpoint, the pattern completion task allows a formulation of unsupervised A classic approach to invariant object recognition is to use a recurrent neural network as an associative memory[1]. In spite of the intuitive appeal and biological plausibility of this approach, it has largely been abandoned in practical applications. This paper introduces two new concepts that could help resurrect it: object representation by continuous attractors, and learning attractors by pattern completion. In most models of associative memory, memories are stored as attractive fixed points at discrete locations in state space[1]. Discrete attractors may not be appropriate for patterns with continuous variability, like the images of a three-dimensional object from different viewpoints. When the instantiations of an object lie on a continuous pattern manifold, it is more appropriate to represent objects by attractive manifolds of fixed points, or continuous attractors. To make this idea practical, it is important to find methods for learning attractors from examples. A naive method is to train the network to retain examples in short-term memory. This method is deficient because it does not prevent the network from storing spurious fixed points that are unrelated to the examples. A superior method is to train the network to restore examples that have been corrupted, so that it learns to complete patterns by filling in missing information. learning in terms of regression rather than density estimation.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci. USA, </institution> <month> 79 </month> <pages> 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: 1 DISCRETE VERSUS CONTINUOUS ATTRACTORS network dynamics. The standard way is to represent each object by an attractive fixed point <ref> [1] </ref>, as in Figure 1a. Recall of a memory is triggered by a sensory input, which sets the initial conditions. The network dynamics converges to a fixed point, thus retrieving a memory.
Reference: [2] <author> D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. </author> <title> A learning algorithm for Boltzmann machines. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 147-169, </pages> <year> 1985. </year>
Reference: [3] <author> H. S. Seung. </author> <title> How the brain keeps the eyes still. </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <month> 93 </month> <pages> 13339-13344, </pages> <year> 1996. </year>
Reference-contexts: In real networks, a continuous attractor is only approximated by a manifold in state space along which drift is very slow. This is illustrated by a simple example, a descent dynamics on a trough-shaped energy landscape <ref> [3] </ref>. If the bottom of the trough is perfectly level, it is a line of fixed points and an ideal continuous attractor of the dynamics. However, any slight imperfections cause slow drift along the line.
Reference: [4] <author> A. P. Georgopoulos, M. Taira, and A. Lukashin. </author> <title> Cognitive neurophysiology of the motor cortex. </title> <journal> Science, </journal> <volume> 260 </volume> <pages> 47-52, </pages> <year> 1993. </year>
Reference: [5] <author> K. Zhang. </author> <title> Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory. </title> <journal> J. Neurosci., </journal> <volume> 16 </volume> <pages> 2112-2126, </pages> <year> 1996. </year>
Reference: [6] <author> R. Ben-Yishai, R. L. Bar-Or, and H. Sompolinsky. </author> <title> Theory of orientation tuning in visual cortex. </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci. USA, </institution> <month> 92 </month> <pages> 3844-3848, </pages> <year> 1995. </year>
Reference: [7] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, chapter 8, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: This single iteration is equivalent to the feedforward architecture of Figure 2b. In the case where the hidden layer is smaller than the visible layers, this architecture is known as an autoen-coder network <ref> [7] </ref>. Therefore the recurrent network dynamics (1) is equivalent to repeated iterations of the feedforward autoencoder. This is just the standard trick of unfolding the dynamics of a recurrent network in time, to yield an equivalent feedforward network with many layers [7]. <p> layers, this architecture is known as an autoen-coder network <ref> [7] </ref>. Therefore the recurrent network dynamics (1) is equivalent to repeated iterations of the feedforward autoencoder. This is just the standard trick of unfolding the dynamics of a recurrent network in time, to yield an equivalent feedforward network with many layers [7]. <p> Gradient descent on this cost function can be done via backpropagation through time <ref> [7] </ref>. If the network is trained with patterns drawn from a continuous family, then it can learn to perform the short-term memory task by developing a continuous attractor that lies near the examples it is trained on.
Reference: [8] <author> G. W. Cottrell, P. Munro, and D. Zipser. </author> <title> Image compression by back propagation: an example of extensional programming. </title> <editor> In N. E. Sharkey, editor, </editor> <title> Models of cognition: a review of cognitive science. </title> <publisher> Ablex, </publisher> <address> Norwood, NJ, </address> <year> 1989. </year>
Reference-contexts: For the case of a single time step (T = 1), training the recurrent network of Figure 2a to retain patterns is equivalent to training the autoencoder of Figure 2b by minimizing the squared difference between its input and output layers, averaged over the examples <ref> [8] </ref>. From the information theoretic perspective, the small hidden layer in Figure 2b acts as a bottleneck between the input and output layers, forcing the autoencoder to learn an efficient encoding of the input.
Reference: [9] <author> P. Baldi and K. Hornik. </author> <title> Neural networks and principal component analysis: Learning from examples without local minima. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 53-58, </pages> <year> 1989. </year>
Reference-contexts: Then the input and output vectors are related by a simple matrix multiplication. The rank of the matrix is equal to the number of hidden units. The average distortion is minimized when this matrix becomes a projection operator onto the subspace spanned by the principal components of the examples <ref> [9] </ref>. From the dynamical perspective, the principal subspace is a continuous attractor of the dynamics (1). The linear network dynamics converges to this attractor in a single iteration, starting from any initial condition.
Reference: [10] <author> H. S. Seung. </author> <title> Pattern analysis and synthesis in attractor neural networks. </title> <editor> In K.-Y. M. Wong, I. King, and D.-Y. Yeung, editors, </editor> <booktitle> Theoretical Aspects of Neural Computation: </booktitle>
Reference-contexts: From the dynamical perspective, the principal subspace is a continuous attractor of the dynamics (1). The linear network dynamics converges to this attractor in a single iteration, starting from any initial condition. Therefore we can interpret principal component analysis and its variants as methods of learning continuous attractors <ref> [10] </ref>. 4 LEARNING TO COMPLETE PATTERNS Learning to retain patterns in short-term memory only works properly for architectures with a small hidden layer. The problem with a large hidden layer is evident when the hidden and visible layers are the same size, and the neurons are linear.
References-found: 10

