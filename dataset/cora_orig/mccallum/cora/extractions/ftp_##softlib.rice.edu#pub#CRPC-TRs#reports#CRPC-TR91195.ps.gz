URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR91195.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Interprocedural Compilation of Fortran D for MIMD Distributed-Memory Machines  
Author: Mary W. Hall Seema Hiranandani Ken Kennedy Chau-Wen Tseng 
Note: Center for Research on Parallel Computation  In Proceedings of Supercomputing '92,  
Date: November 1991  November 1993.  
Address: 91195  P.O. Box 1892 Houston, TX 77251-1892  Minneapolis, MN,  
Affiliation: CRPC-TR  Rice University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: Reaching Decompositions Calculation. To determine the decomposition of distributed arrays at each point in the program, the compiler calculates reaching decompositions. Locally, it is computed in the same manner as reaching definitions, with each decomposition treated as a "definition" <ref> [1] </ref>. Interprocedural reaching decompositions is a flow-sensitive data-flow problem [3, 11] since dynamic data decomposition is affected by control flow. <p> A reference to one of these arrays constitutes a use of the definition for that array. With this model, the Fortran D compiler can calculate live decompositions in the same manner as live variables <ref> [1] </ref>. Array mapping calls that are not live may be eliminated. One approach would be to calculate live decompositions during interprocedural propagation. During local analysis, we would collect summary information representing control flow and the placement of data decomposition specifications. <p> Together De-compBefore and DecompAfter represent dynamic data decompositions from P whose instantiation have been delayed. We calculate live decompositions by simply propagating uses backwards through the local control flow graph for each procedure <ref> [1] </ref>. A data decomposition statement is live with respect to a variable X only if there is some path between it and a reference to X that is not killed by another decomposition statement or by DecompKill of an intervening call. <p> There are two situations where a decomposition that is live and loop-invariant with respect to variable X may be hoisted out of a loop. They vary slightly from the requirements for loop-invariant code motion <ref> [1] </ref>: * If the decomposition is not used within the loop for X, it may be moved after the loop.
Reference: [2] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Analyze communication. Based on the computation partition, references that result in nonlocal ac cesses are marked. 5. Optimize communication. Nonlocal references are examined to determine optimization opportunities. The key optimization, message vectorization, uses the level of loop-carried true dependences to combine ele ment messages into vectors <ref> [2, 33] </ref>. 6. Manage storage. "Overlaps" [33] or buffers are al located to store nonlocal data. 7. Generate code. The compiler instantiates the communication, data and computation partition determined previously, generating the SPMD program with explicit message-passing that executes directly on the nodes of the distributed-memory machine. <p> Communication is instantiated by modifying the text of the program to insert send and recv routines or collective communication primitives. To see how this strategy works, first recall that message vectorization uses the level of the deepest loop-carried true dependence to combine messages at outer loop levels <ref> [2, 33] </ref>. Communication for loop-carried dependences is inserted at the beginning of the loop that carries the dependence. Communication for loop-independent dependences is inserted in the body of the loop enclosing both the source and sink of the dependence.
Reference: [3] <author> J. Banning. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Conference Record of the Sixth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Antonio, TX, </address> <month> January </month> <year> 1979. </year>
Reference-contexts: Reaching Decompositions Calculation. To determine the decomposition of distributed arrays at each point in the program, the compiler calculates reaching decompositions. Locally, it is computed in the same manner as reaching definitions, with each decomposition treated as a "definition" [1]. Interprocedural reaching decompositions is a flow-sensitive data-flow problem <ref> [3, 11] </ref> since dynamic data decomposition is affected by control flow. However, the restriction on the scope of dynamic data decomposition in Fortran D means that reaching decompositions for a procedure is only dependent on control flow in its callers, not its callees. <p> Formally, Appear (P ) = Gmod (P ) [ Gref (P ). Gmod and Gref represent the variables modified or referenced by a procedure or its descendants [11]. The value of Appear is readily available from interprocedural scalar side-effect analysis <ref> [3, 12] </ref>. We also define a function Filter (R; V ) that removes from R all decompositions elements hD; Xi where X 62 V , returning the remaining decomposition elements. <p> This optimization results in the program shown in Figure 16d. 6.4 Aliasing Two variables X and Y are aliased at some point in the program if X and Y may refer to the same memory location <ref> [3] </ref>. In Fortran 77, aliases arise through parameter passing, either between reference parameters of a procedure if the same memory location is passed to both formals, or between a global and formal to which it is passed.
Reference: [4] <author> P. Briggs, K. Cooper, M. W. Hall, and L. Torczon. </author> <title> Goal-directed interprocedural optimization. </title> <type> Technical Report TR90-147, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> De-cember </month> <year> 1990. </year>
Reference-contexts: An example of this translation is the Translate function in Figure 6. Translation must also deal with array reshaping across pro-cedure boundaries. Interprocedural symbolic analysis used in conjunction with linearization and delinearization of array references can discover standard reference patterns that may be compiled efficiently <ref> [4, 17, 20] </ref>. 5.2 Reaching Decompositions To effectively compile Fortran D programs, it is vital to know the data decomposition of a variable at every point it is referenced in the program. In Fortran D, procedures inherit the data decompositions of their callers.
Reference: [5] <author> M. Burke and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <note> to appear. </note>
Reference-contexts: iteration sets " Loop structure # Nonlocal index sets " Array aliasing & reshaping # Overlaps l Scalar & array side effects " Buffers " Symbolics & constants l Live decompositions " Reaching decompositions # Loop-invariant decomps " Table 1: Interprocedural Fortran D Dataflow Problems changes, thus reducing recompilation costs <ref> [5, 13] </ref>. This process is described in greater detail in Section 8. ParaScope computes interprocedural ref, mod, alias and constants. Implementations are underway to solve a number of other important interprocedural problems, including interprocedural symbolic and RSD analysis. <p> It integrates Fortran D compilation techniques with the interprocedural analysis and optimization framework of ParaScope. 8 Recompilation Analysis The Fortran D compiler will follow the ParaScope approach for limiting recompilation in the presence of interprocedural optimization <ref> [5, 13] </ref>. Recompilation analysis is used to limit recompilation of a program following changes, an important component to maintaining the advantages of separate compilation. <p> A simple test just verifies that the old information is equal to the new information. However, safe tests that generate less recompilation are possible if we consider how the information will be used. Improved recompilation tests for many scalar data-flow problems are described by Burke and Torczon <ref> [5] </ref>. To give the flavor of the recompilation tests, we describe the test for reaching decompositions. Let oldP be the representation of P from the previous compilation. <p> A little more work is needed to calculate the extent of recompilation in the presence of cloning based on reaching decompositions <ref> [5, 17] </ref>. The compiler maintains a mapping from procedures in the call graph to the list of compiled clones for that procedure. For a procedure that has been cloned, the recompilation test can be applied to all the clones in order to find a match for the procedure.
Reference: [6] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Tor-czon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: One of the prime goals for interprocedural compilation is to avoid resorting to run-time resolution. 4 Interprocedural Support in ParaScope ParaScope is a programming environment for scientific Fortran programmers. It has fostered research on aggressive optimization of scientific codes for both scalar and shared-memory machines <ref> [6] </ref>. Its pioneering work on incorporating interprocedural optimization in an efficient compilation system has also contributed to the development of the Convex Applications compiler [26]. Through careful design, the compilation process in ParaScope preserves separate compilation of procedures to a large extent. <p> Additional passes over the code can be added if necessary, but should be avoided since experience has shown that examination of source code dominates analysis time. The existing compilation system uses the following 3-phase approach <ref> [6, 12, 17] </ref>: 1. Local Analysis. At the end of an editing session, ParaScope calculates and stores summary information concerning all local interprocedural effects for each procedure. This information includes details on call sites, formal parameters, scalar and array section uses and definitions, local constants, symbolics, loops and index variables.
Reference: [7] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Symbolic and data dependence analysis is performed. 2. Partition data. Fortran D data decomposition specifications are analyzed to determine the decomposition of each array. 3. Partition computation. The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes val ues of data it owns <ref> [7, 29, 33] </ref>. 4. Analyze communication. Based on the computation partition, references that result in nonlocal ac cesses are marked. 5. Optimize communication. Nonlocal references are examined to determine optimization opportunities. <p> The resulting code is shown in Figure 2. Without interprocedural analysis, the Fortran D compiler cannot locally determine the data decomposition of X in F1. It is forced to generate code using run-time resolution techniques to explicitly calculate the ownership and communication for each reference <ref> [7, 29, 33] </ref>. As can be seen from Figure 3, run-time resolution produces code that is much slower than the equivalent compile-time generated code. Not only does the program have to explicitly check every variable reference, it generates a message for each nonlocal access. <p> It is similar to Aspar [24], Booster [28], Callahan-Kennedy <ref> [7] </ref>, MIMDizer [21], and Superb in that the compilation process is based on a decomposition of the data in the program. Few other compilation systems have discussed inter-procedural issues, especially interprocedural optimization. The CM Fortran compiler utilizes user-defined interface blocks to specify a data partition for each procedure [32].
Reference: [8] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Handling distributed data in Vienna Fortran procedures. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Vienna Fortran [9] provides data distribution specifications similar to Fortran D. Dynamic data decomposition is permitted; arrays are copied at procedure boundaries if redistribution takes place. Vienna Fortran allows the user to specify additional attributes for each distributed array <ref> [8] </ref>. Restore forces an array to be restored to its decomposition at procedure entry. Notransfer causes remapping to be performed logically, rather than actually copying the values in the array. Nocopy guarantees that its formal and actual parameters have the same data decomposition.
Reference: [9] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Vienna Fortran - a Fortran language extension for distributed memory multiprocessors. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: A clone is produced for each possible combination of classification of the procedure parameters. For local compilation, Superb modifies procedures so that arrays are always accessed according to their true number of dimensions, inserting additional parameters where necessary for newly created subscripts. Vienna Fortran <ref> [9] </ref> provides data distribution specifications similar to Fortran D. Dynamic data decomposition is permitted; arrays are copied at procedure boundaries if redistribution takes place. Vienna Fortran allows the user to specify additional attributes for each distributed array [8].
Reference: [10] <author> K. Cooper, M. W. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Computer Language, </booktitle> <address> Oakland, CA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Para-Scope also contains support for inlining and cloning, two interprocedural transformations that increase the context available for optimization. Inlining merges the body of the called procedure into the caller. Cloning creates a new version of a procedure for specific interprocedural information <ref> [10, 12] </ref>. Existing interprocedural analysis in ParaScope is useful for the Fortran D compiler, but it is not sufficient. The compiler must also incorporate analysis to understand the partitioning of data and computation and to apply communication optimizations. <p> For instance, the compiler creates two copies of procedure F1 and F2 because they possess two different reaching decompositions for Z. Edges in the call graph are updated appropriately for the clone. In pathological cases, cloning can result in an exponential growth in program size <ref> [10] </ref>. Under these circumstances, cloning may be disabled when a threshold program growth has been exceeded, forcing run-time resolution instead. 5.3 Partitioning Data and Computation Recall that a major responsibility of the Fortran D compiler is to partition the data and computation across processors.
Reference: [11] <author> K. Cooper and K. Kennedy. </author> <title> Interprocedural side-effect analysis in linear time. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Reaching Decompositions Calculation. To determine the decomposition of distributed arrays at each point in the program, the compiler calculates reaching decompositions. Locally, it is computed in the same manner as reaching definitions, with each decomposition treated as a "definition" [1]. Interprocedural reaching decompositions is a flow-sensitive data-flow problem <ref> [3, 11] </ref> since dynamic data decomposition is affected by control flow. However, the restriction on the scope of dynamic data decomposition in Fortran D means that reaching decompositions for a procedure is only dependent on control flow in its callers, not its callees. <p> We define Appear (P ) to be the set of formal parameters and global variables appearing in procedure P or its descendants. Formally, Appear (P ) = Gmod (P ) [ Gref (P ). Gmod and Gref represent the variables modified or referenced by a procedure or its descendants <ref> [11] </ref>. The value of Appear is readily available from interprocedural scalar side-effect analysis [3, 12]. We also define a function Filter (R; V ) that removes from R all decompositions elements hD; Xi where X 62 V , returning the remaining decomposition elements.
Reference: [12] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the IR n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: Additional passes over the code can be added if necessary, but should be avoided since experience has shown that examination of source code dominates analysis time. The existing compilation system uses the following 3-phase approach <ref> [6, 12, 17] </ref>: 1. Local Analysis. At the end of an editing session, ParaScope calculates and stores summary information concerning all local interprocedural effects for each procedure. This information includes details on call sites, formal parameters, scalar and array section uses and definitions, local constants, symbolics, loops and index variables. <p> Para-Scope also contains support for inlining and cloning, two interprocedural transformations that increase the context available for optimization. Inlining merges the body of the called procedure into the caller. Cloning creates a new version of a procedure for specific interprocedural information <ref> [10, 12] </ref>. Existing interprocedural analysis in ParaScope is useful for the Fortran D compiler, but it is not sufficient. The compiler must also incorporate analysis to understand the partitioning of data and computation and to apply communication optimizations. <p> Formally, Appear (P ) = Gmod (P ) [ Gref (P ). Gmod and Gref represent the variables modified or referenced by a procedure or its descendants [11]. The value of Appear is readily available from interprocedural scalar side-effect analysis <ref> [3, 12] </ref>. We also define a function Filter (R; V ) that removes from R all decompositions elements hD; Xi where X 62 V , returning the remaining decomposition elements.
Reference: [13] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: iteration sets " Loop structure # Nonlocal index sets " Array aliasing & reshaping # Overlaps l Scalar & array side effects " Buffers " Symbolics & constants l Live decompositions " Reaching decompositions # Loop-invariant decomps " Table 1: Interprocedural Fortran D Dataflow Problems changes, thus reducing recompilation costs <ref> [5, 13] </ref>. This process is described in greater detail in Section 8. ParaScope computes interprocedural ref, mod, alias and constants. Implementations are underway to solve a number of other important interprocedural problems, including interprocedural symbolic and RSD analysis. <p> It integrates Fortran D compilation techniques with the interprocedural analysis and optimization framework of ParaScope. 8 Recompilation Analysis The Fortran D compiler will follow the ParaScope approach for limiting recompilation in the presence of interprocedural optimization <ref> [5, 13] </ref>. Recompilation analysis is used to limit recompilation of a program following changes, an important component to maintaining the advantages of separate compilation.
Reference: [14] <author> J. Dongarra, J. Bunch, C. Moler, and G. Stewart. </author> <title> LIN-PACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <year> 1979. </year>
Reference-contexts: It must also pass recompilation tests for other interprocedural problems. 9 Empirical Results 9.1 Compilation Strategies for DGEFA This section demonstrates the effectiveness of inter-procedural optimization using the routine dgefa from Lin-pack, a linear algebra library <ref> [14] </ref>. dgefa is also a major component in the Linpack Benchmark Program. dgefa uses Gaussian elimination with partial pivoting to factor a double-precision floating-point array. A simplified version is shown in Figure 19. dgefa relies on three other Linpack routines: idamax, dscal, and daxpy.
Reference: [15] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kre-mer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Each dimension is distributed in a block, cyclic, or block-cyclic manner; the symbol ":" marks dimensions that are not distributed. Because the alignment and distribution statements are executable, dynamic data decomposition is possible. The complete language is described in detail elsewhere <ref> [15] </ref>. As in High Performance Fortran (HPF), each array is implicitly aligned with a default decomposition.
Reference: [16] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The user labels parameters as in or out to indicate whether their values are used and/or defined. Superb performs interprocedural data-flow analysis of parameter passing to classify each formal parameter of a procedure as unpartitioned or having a standard/nonstandard partition <ref> [16, 33] </ref>. A clone is produced for each possible combination of classification of the procedure parameters. For local compilation, Superb modifies procedures so that arrays are always accessed according to their true number of dimensions, inserting additional parameters where necessary for newly created subscripts.
Reference: [17] <author> M. W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Additional passes over the code can be added if necessary, but should be avoided since experience has shown that examination of source code dominates analysis time. The existing compilation system uses the following 3-phase approach <ref> [6, 12, 17] </ref>: 1. Local Analysis. At the end of an editing session, ParaScope calculates and stores summary information concerning all local interprocedural effects for each procedure. This information includes details on call sites, formal parameters, scalar and array section uses and definitions, local constants, symbolics, loops and index variables. <p> An example of this translation is the Translate function in Figure 6. Translation must also deal with array reshaping across pro-cedure boundaries. Interprocedural symbolic analysis used in conjunction with linearization and delinearization of array references can discover standard reference patterns that may be compiled efficiently <ref> [4, 17, 20] </ref>. 5.2 Reaching Decompositions To effectively compile Fortran D programs, it is vital to know the data decomposition of a variable at every point it is referenced in the program. In Fortran D, procedures inherit the data decompositions of their callers. <p> A little more work is needed to calculate the extent of recompilation in the presence of cloning based on reaching decompositions <ref> [5, 17] </ref>. The compiler maintains a mapping from procedures in the call graph to the list of compiled clones for that procedure. For a procedure that has been cloned, the recompilation test can be applied to all the clones in order to find a match for the procedure.
Reference: [18] <author> M. W. Hall, K. Kennedy, and K. S. M c Kinley. </author> <title> Inter-procedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Interprocedural Propagation. The compiler collects local summary information from each procedure in the program to build an augmented call graph containing loop information <ref> [18] </ref>. It then propagates the initial information on the call graph to compute inter procedural solutions. 3. Interprocedural Code Generation. The compiler directs compilation of all procedures in the program based on the results of interprocedural analysis. Another important aspect of the compilation system is what happens on subsequent compilations. <p> Since the Fortran D compiler also requires information about interprocedural loop nesting, it uses the augmented call graph (ACG) <ref> [18] </ref>. Conceptually, the ACG is simply a call graph plus loop nodes that contain the bounds, step, and index variable for each loop, plus nesting edges that indicate which nodes directly encompass other nodes. For instance, the Fortran D program in Figure 4 produces the ACG shown in Figure 5.
Reference: [19] <author> P. Hatcher and M. Quinn. </author> <title> Data-parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: The CM Fortran compiler utilizes user-defined interface blocks to specify a data partition for each procedure [32]. Array parameters are then copied to buffers of the expected form at run-time if needed, eliminating the need for interprocedural analysis. C* [30] and Dataparallel C <ref> [19] </ref> specify parallelism through the use of parallel functions.
Reference: [20] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of inter-procedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: We refer to collections of data and computation as index sets and iteration sets, respectively. In the Fortran D compiler, both are represented by regular section descriptors (RSDs) <ref> [20] </ref>. We describe RSDs using Fortran 90 triplet notation. 3.1 Compilation Example We illustrate the Fortran D compilation process for procedure F1 in Figure 1 onto a machine with four processors. <p> An example of this translation is the Translate function in Figure 6. Translation must also deal with array reshaping across pro-cedure boundaries. Interprocedural symbolic analysis used in conjunction with linearization and delinearization of array references can discover standard reference patterns that may be compiled efficiently <ref> [4, 17, 20] </ref>. 5.2 Reaching Decompositions To effectively compile Fortran D programs, it is vital to know the data decomposition of a variable at every point it is referenced in the program. In Fortran D, procedures inherit the data decompositions of their callers. <p> References within a procedure are put into RSD form, but merged only if no loss of precision will result. The resulting RSDs may be propagated to calling procedures and translated as definitions or uses to actual parameters and global variables <ref> [20] </ref>. During code generation, the Fortran D compiler uses intraprocedural algorithms to calculate nonlocal index sets, using the deepest true dependence to determine the loop level for vectorizing communication.
Reference: [21] <author> R. Hill. MIMDizer: </author> <title> A new tool for parallelization. </title> <journal> Supercomputing Review, </journal> <volume> 3(4) </volume> <pages> 26-28, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: It is similar to Aspar [24], Booster [28], Callahan-Kennedy [7], MIMDizer <ref> [21] </ref>, and Superb in that the compilation process is based on a decomposition of the data in the program. Few other compilation systems have discussed inter-procedural issues, especially interprocedural optimization. The CM Fortran compiler utilizes user-defined interface blocks to specify a data partition for each procedure [32].
Reference: [22] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The compiler applies a compilation strategy based on data dependence that incorporates and extends previous techniques. We briefly describe each major step of the compilation process below, details are presented elsewhere <ref> [22, 23] </ref>: 1. Analyze Program. Symbolic and data dependence analysis is performed. 2. Partition data. Fortran D data decomposition specifications are analyzed to determine the decomposition of each array. 3. Partition computation. <p> Communication for loop-independent dependences is inserted in the body of the loop enclosing both the source and sink of the dependence. If both loop-carried and loop-independent dependences exist at the same level, the loop-independent dependence takes priority <ref> [22] </ref>.
Reference: [23] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The compiler applies a compilation strategy based on data dependence that incorporates and extends previous techniques. We briefly describe each major step of the compilation process below, details are presented elsewhere <ref> [22, 23] </ref>: 1. Analyze Program. Symbolic and data dependence analysis is performed. 2. Partition data. Fortran D data decomposition specifications are analyzed to determine the decomposition of each array. 3. Partition computation. <p> This imprecision is not a problem since the Fortran D compiler delays instantiation of communication for nonlocal references in any case to take advantage of additional opportunities to apply message vectorization, coalescing, aggregation, and other communication optimizations <ref> [23] </ref>. For interprocedural compilation, the Fortran D compiler first performs interprocedural dependence analysis. References within a procedure are put into RSD form, but merged only if no loss of precision will result. <p> Using buffers requires additional work by the compiler to separate loop iterations accessing nonlocal data, but this is necessary in any case to perform iteration reordering, a communication optimization designed to overlap communication with computation <ref> [23] </ref>. If the overlap region is noncontiguous, using buffers also has the advantage of eliminating the need to unpack nonlocal data. Alternatively, the Fortran D compiler can rely on Fortran's ability to specify array dimensions at run time.
Reference: [24] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: We do not expect inter-procedural optimization to be required in all cases, but for many computations it can make a significant difference. 10 Related Work The Fortran D compiler is a second-generation distributed-memory compiler that integrates and extends previous analysis and optimization techniques. It is similar to Aspar <ref> [24] </ref>, Booster [28], Callahan-Kennedy [7], MIMDizer [21], and Superb in that the compilation process is based on a decomposition of the data in the program. Few other compilation systems have discussed inter-procedural issues, especially interprocedural optimization.
Reference: [25] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Array parameters are then copied to buffers of the expected form at run-time if needed, eliminating the need for interprocedural analysis. C* [30] and Dataparallel C [19] specify parallelism through the use of parallel functions. Arguments to procedures in Id Nouveau [29] and Kali <ref> [25] </ref> are labeled with their expected incoming data Run-time Interprocedural Interprocedural Problem Resolution Analysis Optimization Size P time speedup time speedup time speedup 1 sequential time = 0.84 seconds 200 2 125 149 8.7 10.4 .57 1.47 200 8 129 154 9.0 10.7 .43 1.95 32 152 181 9.4 11.2 .52
Reference: [26] <author> J. Loeliger, R. Metzger, M. Seligman, and S. Stroud. </author> <title> Pointer target tracking: An empirical study. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: It has fostered research on aggressive optimization of scientific codes for both scalar and shared-memory machines [6]. Its pioneering work on incorporating interprocedural optimization in an efficient compilation system has also contributed to the development of the Convex Applications compiler <ref> [26] </ref>. Through careful design, the compilation process in ParaScope preserves separate compilation of procedures to a large extent.
Reference: [27] <author> E. Myers. </author> <title> A precise inter-procedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williams-burg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: We would then need to compute the solution on the supergraph formed by combining local control flow graphs with the call graph, taking care to avoid paths that do not correspond to possible execution sequences <ref> [27] </ref>. To avoid this complexity, we choose instead to compute live decompositions during code generation, when control flow information is available. Live Decompositions Calculation. <p> (CYCLIC) do k = 1,T : : : = X (: : :) S 1 call F1 (X) end S 2 call F1 (X) SUBROUTINE F2 (X) enddo REAL X (100) call F2 (X) S 3 X (: : :) = : : : end end the presence of aliasing <ref> [27] </ref>. Even without aliasing, inter-procedural live variable analysis can be expensive since it requires bidirectional propagation, causing a procedure to be analyzed multiple times. We rely on two restrictions to make the live decompositions problem tractable for the Fortran D compiler. <p> Aliasing affects dynamic data decomposition because a variable may be remapped indirectly through one of its aliases. Unfortunately, precise alias analysis is computa-tionally intractable <ref> [27] </ref>. As a result, the compiler cannot efficiently prove that a decomposition that has been applied to a variable holds for a possible alias.
Reference: [28] <author> E. Paalvast, H. Sips, and A. van Gemund. </author> <title> Automatic parallel program generation and optimization from data decompositions. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: It is similar to Aspar [24], Booster <ref> [28] </ref>, Callahan-Kennedy [7], MIMDizer [21], and Superb in that the compilation process is based on a decomposition of the data in the program. Few other compilation systems have discussed inter-procedural issues, especially interprocedural optimization.
Reference: [29] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Symbolic and data dependence analysis is performed. 2. Partition data. Fortran D data decomposition specifications are analyzed to determine the decomposition of each array. 3. Partition computation. The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes val ues of data it owns <ref> [7, 29, 33] </ref>. 4. Analyze communication. Based on the computation partition, references that result in nonlocal ac cesses are marked. 5. Optimize communication. Nonlocal references are examined to determine optimization opportunities. <p> The resulting code is shown in Figure 2. Without interprocedural analysis, the Fortran D compiler cannot locally determine the data decomposition of X in F1. It is forced to generate code using run-time resolution techniques to explicitly calculate the ownership and communication for each reference <ref> [7, 29, 33] </ref>. As can be seen from Figure 3, run-time resolution produces code that is much slower than the equivalent compile-time generated code. Not only does the program have to explicitly check every variable reference, it generates a message for each nonlocal access. <p> Array parameters are then copied to buffers of the expected form at run-time if needed, eliminating the need for interprocedural analysis. C* [30] and Dataparallel C [19] specify parallelism through the use of parallel functions. Arguments to procedures in Id Nouveau <ref> [29] </ref> and Kali [25] are labeled with their expected incoming data Run-time Interprocedural Interprocedural Problem Resolution Analysis Optimization Size P time speedup time speedup time speedup 1 sequential time = 0.84 seconds 200 2 125 149 8.7 10.4 .57 1.47 200 8 129 154 9.0 10.7 .43 1.95 32 152 181
Reference: [30] <author> J. Rose and G. Steele, Jr. </author> <title> C fl : An extended C language for data parallel programming. </title> <editor> In L. Kartashev and S. Karta-shev, editors, </editor> <booktitle> Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> Santa Clara, CA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: The CM Fortran compiler utilizes user-defined interface blocks to specify a data partition for each procedure [32]. Array parameters are then copied to buffers of the expected form at run-time if needed, eliminating the need for interprocedural analysis. C* <ref> [30] </ref> and Dataparallel C [19] specify parallelism through the use of parallel functions.
Reference: [31] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: The user must ensure that the procedure is called only with the appropriately decomposed arguments. Distributed array parameters to composite procedures in Dino cause their values to be communicated to the appropriate processors <ref> [31] </ref>. The user labels parameters as in or out to indicate whether their values are used and/or defined. Superb performs interprocedural data-flow analysis of parameter passing to classify each formal parameter of a procedure as unpartitioned or having a standard/nonstandard partition [16, 33].
Reference: [32] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, version 1.0 edition, </note> <month> February </month> <year> 1991. </year>
Reference-contexts: The resulting program, shown in Figure 12, is much less efficient than the code in Figure 10. This example also points out limitations for language extensions designed to avoid interprocedural analysis. Language features such as interface blocks <ref> [32] </ref> require the user to specify information at procedure boundaries. These features impose additional burdens on the programmer, but can reduce or eliminate the need for interprocedural analysis. However, current language extensions are insufficient for interprocedural optimizations. <p> Few other compilation systems have discussed inter-procedural issues, especially interprocedural optimization. The CM Fortran compiler utilizes user-defined interface blocks to specify a data partition for each procedure <ref> [32] </ref>. Array parameters are then copied to buffers of the expected form at run-time if needed, eliminating the need for interprocedural analysis. C* [30] and Dataparallel C [19] specify parallelism through the use of parallel functions.
Reference: [33] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: Symbolic and data dependence analysis is performed. 2. Partition data. Fortran D data decomposition specifications are analyzed to determine the decomposition of each array. 3. Partition computation. The compiler partitions computation across processors using the "owner computes" rule|where each processor only computes val ues of data it owns <ref> [7, 29, 33] </ref>. 4. Analyze communication. Based on the computation partition, references that result in nonlocal ac cesses are marked. 5. Optimize communication. Nonlocal references are examined to determine optimization opportunities. <p> Analyze communication. Based on the computation partition, references that result in nonlocal ac cesses are marked. 5. Optimize communication. Nonlocal references are examined to determine optimization opportunities. The key optimization, message vectorization, uses the level of loop-carried true dependences to combine ele ment messages into vectors <ref> [2, 33] </ref>. 6. Manage storage. "Overlaps" [33] or buffers are al located to store nonlocal data. 7. Generate code. The compiler instantiates the communication, data and computation partition determined previously, generating the SPMD program with explicit message-passing that executes directly on the nodes of the distributed-memory machine. <p> Optimize communication. Nonlocal references are examined to determine optimization opportunities. The key optimization, message vectorization, uses the level of loop-carried true dependences to combine ele ment messages into vectors [2, 33]. 6. Manage storage. "Overlaps" <ref> [33] </ref> or buffers are al located to store nonlocal data. 7. Generate code. The compiler instantiates the communication, data and computation partition determined previously, generating the SPMD program with explicit message-passing that executes directly on the nodes of the distributed-memory machine. <p> The resulting code is shown in Figure 2. Without interprocedural analysis, the Fortran D compiler cannot locally determine the data decomposition of X in F1. It is forced to generate code using run-time resolution techniques to explicitly calculate the ownership and communication for each reference <ref> [7, 29, 33] </ref>. As can be seen from Figure 3, run-time resolution produces code that is much slower than the equivalent compile-time generated code. Not only does the program have to explicitly check every variable reference, it generates a message for each nonlocal access. <p> Communication is instantiated by modifying the text of the program to insert send and recv routines or collective communication primitives. To see how this strategy works, first recall that message vectorization uses the level of the deepest loop-carried true dependence to combine messages at outer loop levels <ref> [2, 33] </ref>. Communication for loop-carried dependences is inserted at the beginning of the loop that carries the dependence. Communication for loop-independent dependences is inserted in the body of the loop enclosing both the source and sink of the dependence. <p> The user labels parameters as in or out to indicate whether their values are used and/or defined. Superb performs interprocedural data-flow analysis of parameter passing to classify each formal parameter of a procedure as unpartitioned or having a standard/nonstandard partition <ref> [16, 33] </ref>. A clone is produced for each possible combination of classification of the procedure parameters. For local compilation, Superb modifies procedures so that arrays are always accessed according to their true number of dimensions, inserting additional parameters where necessary for newly created subscripts.
References-found: 33

