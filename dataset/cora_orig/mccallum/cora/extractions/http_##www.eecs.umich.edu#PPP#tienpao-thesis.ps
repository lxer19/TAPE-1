URL: http://www.eecs.umich.edu/PPP/tienpao-thesis.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Title: Goal-Directed Performance Tuning for Scientific Applications  
Author: by Tien-Pao Shih Professor Edward S. Davidson, Chair Professor John P. Hayes 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Science and Engineering) in The  Doctoral Committee:  Professor William R. Martin Professor Trevor N. Mudge  
Date: 1996  
Affiliation: University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 151 BIBLIOGRAPHY </institution>
Reference: [1] <author> Aaron J. Goldberg and John L. Hennessy. </author> <title> Mtool: An integrated system for performance debugging shared memory multiprocessor applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 28-40, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Reasonable performance objectives must be application-specific as well as machine-specific. Even if a performance objective can be determined, identifying performance problems requires expertise in both hardware and software. Recently, some tools, such as Mtool <ref> [1] </ref>, Cprof [2], and MemSpy [3], have become available to identify performance problems. However, they do not provide direct solutions. Therefore, users may still have to use costly trial and error processes to find appropriate solutions for perceived performance problems. <p> (the inner loop only) CL.2: lfdu fp2, ZX (K+11) [2] fm fp2, fp3, fp2 [3] fp3 = T fma fp0, fp5, fp0, fp2 [5] fp5 = R stfdu fp0, X (K) [7] bc CL.2 (c).Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 <ref> [1] </ref> fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] fp3 = T fma fp2, fp4, fp2, fp1 [5] fp1 = Q stfdu fp2, X (K) [6] bc CL.2 double precision floating-point store with address updated au tomatically). loaded before the loop begins. 2 Usually, removing <p> We used Goblin to disassemble the compiled code and interleaved these instructions as follows: lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 <ref> [1] </ref> fp5 = R fm fp4, fp5, fp6 [1U]fp5 = R lfdu fp0, ZX (K+12) [2U] fma fp2, fp3, fp6, fp2 [3] fp3 = T fma fp4, fp3, fp0, fp4 [3U]fp3 = T fma fp2, fp6, fp2, fp1 [5] fp1 = Q stfd fp2, X (K) [6] fma fp4, fp7, fp4, <p> For example, the hierarchical bound model MACS12*B [43] relies on hardware monitoring to acquire the actual number of cache misses in a program. It is precise but is limited to machines with special hardware. Mtool <ref> [1] </ref> uses a software profiling technique to count the number of instructions in each basic block, and estimates the ideal execution time based on these instructions. The difference between the actual and the estimated running time is then attributed to memory overhead. <p> 2 [2; n]; d k f s ; if 8k 2 [2; n]; 1 d k &lt; f Theorem 5.5.1 For index set I = (i 1 ; i 2 ; ; i n ) and constant element group size, g, over r (&gt; 1) ungrouped arrays, i.e. 8j 2 <ref> [1; r] </ref>, e j = g: (1) if 8k 2 [2; n], d k s=g, then T G (I) = T U (I)=r, (2) if 8k 2 [2; n], d k 2 [q; s=g), and c = 0, then T G (I) T U (I), and (3) if 8k 2 [2; <p> If c = 0, then Theorem 5.5.2 shows that for arbitrary reference distances T G (I) T U (I). Theorem 5.5.2 If 8j 2 <ref> [1; r] </ref> e j = g and eq = s (i.e. c = 0), then T G (I) T U (I). Proof: Since eq = s and rg = e, the number of element groups per subpage of each of the given arrays, s=g = rq, is also an integer. <p> Theorem 5.5.3 deals with the more general case where c is not necessarily 0. Theorem 5.5.3 For I = (i 1 ; i 2 ; ; i n ), t = T U (I)=r, and 8j 2 <ref> [1; r] </ref>, e j = g: (1) if e ng=t; T G (I) T U (I) 127 Proof: Since t is the number of transactions for a given array, ng=t is the average number of useful data elements in a transaction before grouping. <p> Theorem 5.5.4 extends Theorem 5.5.3 to allow different ungrouped arrays to have different element group sizes. Theorem 5.5.4 For I = (i 1 ; i 2 ; ; i n ), (1) if e n= minft j =e j jj 2 <ref> [1; r] </ref>g, T G (I) T U (I) (2) if e n=(q maxft j =e j jj 2 [1; r]g), T G (I) T U (I) where t j is the number of transactions for ungrouped array j. Proof: Note that n= minft j =e j jj 2 [1; r]g = <p> Theorem 5.5.4 For I = (i 1 ; i 2 ; ; i n ), (1) if e n= minft j =e j jj 2 <ref> [1; r] </ref>g, T G (I) T U (I) (2) if e n=(q maxft j =e j jj 2 [1; r]g), T G (I) T U (I) where t j is the number of transactions for ungrouped array j. Proof: Note that n= minft j =e j jj 2 [1; r]g = maxf (ne j )=t j jj 2 [1; r]g, which is the maximum average number of useful data <p> jj 2 <ref> [1; r] </ref>g, T G (I) T U (I) (2) if e n=(q maxft j =e j jj 2 [1; r]g), T G (I) T U (I) where t j is the number of transactions for ungrouped array j. Proof: Note that n= minft j =e j jj 2 [1; r]g = maxf (ne j )=t j jj 2 [1; r]g, which is the maximum average number of useful data elements in a transaction among all ungrouped arrays. <p> (2) if e n=(q maxft j =e j jj 2 <ref> [1; r] </ref>g), T G (I) T U (I) where t j is the number of transactions for ungrouped array j. Proof: Note that n= minft j =e j jj 2 [1; r]g = maxf (ne j )=t j jj 2 [1; r]g, which is the maximum average number of useful data elements in a transaction among all ungrouped arrays. Since after grouping, each transaction has at least e useful data elements, if e maxf (ne j )=t j jj 2 [1; r]g, then T G (I) T U (I). <p> <ref> [1; r] </ref>g = maxf (ne j )=t j jj 2 [1; r]g, which is the maximum average number of useful data elements in a transaction among all ungrouped arrays. Since after grouping, each transaction has at least e useful data elements, if e maxf (ne j )=t j jj 2 [1; r]g, then T G (I) T U (I). On the other hand, n= maxft j =e j jj 2 [1; r]g = minf (ne j )=t j jj 2 [1; r]g, which is the minimum average number of useful data elements in a transaction among all ungrouped arrays. <p> Since after grouping, each transaction has at least e useful data elements, if e maxf (ne j )=t j jj 2 <ref> [1; r] </ref>g, then T G (I) T U (I). On the other hand, n= maxft j =e j jj 2 [1; r]g = minf (ne j )=t j jj 2 [1; r]g, which is the minimum average number of useful data elements in a transaction among all ungrouped arrays. <p> after grouping, each transaction has at least e useful data elements, if e maxf (ne j )=t j jj 2 <ref> [1; r] </ref>g, then T G (I) T U (I). On the other hand, n= maxft j =e j jj 2 [1; r]g = minf (ne j )=t j jj 2 [1; r]g, which is the minimum average number of useful data elements in a transaction among all ungrouped arrays. Since after grouping a transaction has at most eq useful data elements, if eq is no greater than minf (ne j )=t j jj 2 [1; r]g, then T G (I) T <p> (ne j )=t j jj 2 <ref> [1; r] </ref>g, which is the minimum average number of useful data elements in a transaction among all ungrouped arrays. Since after grouping a transaction has at most eq useful data elements, if eq is no greater than minf (ne j )=t j jj 2 [1; r]g, then T G (I) T U (I). 2 The above theorems provide insight into why and when array grouping works. They can be used a priori to estimate whether an array grouping is beneficial.
Reference: [2] <author> Alvin R. Lebeck and David A. Wood. </author> <title> Cache profiling and the SPEC benchmarks: A case study. </title> <journal> IEEE Computer, </journal> <volume> 27(10) </volume> <pages> 15-26, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Reasonable performance objectives must be application-specific as well as machine-specific. Even if a performance objective can be determined, identifying performance problems requires expertise in both hardware and software. Recently, some tools, such as Mtool [1], Cprof <ref> [2] </ref>, and MemSpy [3], have become available to identify performance problems. However, they do not provide direct solutions. Therefore, users may still have to use costly trial and error processes to find appropriate solutions for perceived performance problems. <p> Notice that the code execution sequence had to be reordered and the initial value of ZX (K+10) had to be 15 (a).Fortran Source Program DO 1 L = 1, Loop DO 1 K = 1, n (b).Compiled Pseudo Code (the inner loop only) CL.2: lfdu fp2, ZX (K+11) <ref> [2] </ref> fm fp2, fp3, fp2 [3] fp3 = T fma fp0, fp5, fp0, fp2 [5] fp5 = R stfdu fp0, X (K) [7] bc CL.2 (c).Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma <p> (K+11) <ref> [2] </ref> fm fp2, fp3, fp2 [3] fp3 = T fma fp0, fp5, fp0, fp2 [5] fp5 = R stfdu fp0, X (K) [7] bc CL.2 (c).Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] fp3 = T fma fp2, fp4, fp2, fp1 [5] fp1 = Q stfdu fp2, X (K) [6] bc CL.2 double precision floating-point store with address updated au tomatically). loaded before the loop begins. 2 Usually, removing redundant memory operations increases performance. <p> While this information may be adequate for identifying the bottleneck portion of an application, it is not detailed enough to guide the selection of appropriate techniques for cache performance tuning. Cprof <ref> [2] </ref>, MemSpy [3] and our model all identify the performance degradation caused by each array. In the uniprocessor environment, Cprof shows compulsory, conflict, and capacity misses while MemSpy classifies misses into first-load (compulsory) and replacement (conflict and capacity) misses. Both these tools consider conflict misses as a whole. <p> If 8k 2 <ref> [2; n] </ref>, d k f , each referenced element group is in a distinct subpage and jIj = n transactions are required. If 8k 2 [2; n], 1 d k &lt; f , then a set of (x + (s=f ) fi (i n i 1 + 1) + x 0 <p> If 8k 2 <ref> [2; n] </ref>, d k f , each referenced element group is in a distinct subpage and jIj = n transactions are required. If 8k 2 [2; n], 1 d k &lt; f , then a set of (x + (s=f ) fi (i n i 1 + 1) + x 0 )=s consecutive subpages is referenced, where x is size of the initial unreferenced portion of the first subpage, and x 0 is the final unreferenced <p> In summary, t (I) = &gt; &lt; n; if 8k 2 <ref> [2; n] </ref>; d k f s ; if 8k 2 [2; n]; 1 d k &lt; f Theorem 5.5.1 For index set I = (i 1 ; i 2 ; ; i n ) and constant element group size, g, over r (&gt; 1) ungrouped arrays, i.e. 8j 2 [1; r], <p> In summary, t (I) = &gt; &lt; n; if 8k 2 <ref> [2; n] </ref>; d k f s ; if 8k 2 [2; n]; 1 d k &lt; f Theorem 5.5.1 For index set I = (i 1 ; i 2 ; ; i n ) and constant element group size, g, over r (&gt; 1) ungrouped arrays, i.e. 8j 2 [1; r], e j = g: (1) if 8k 2 [2; n], <p> 2 <ref> [2; n] </ref>; 1 d k &lt; f Theorem 5.5.1 For index set I = (i 1 ; i 2 ; ; i n ) and constant element group size, g, over r (&gt; 1) ungrouped arrays, i.e. 8j 2 [1; r], e j = g: (1) if 8k 2 [2; n], d k s=g, then T G (I) = T U (I)=r, (2) if 8k 2 [2; n], d k 2 [q; s=g), and c = 0, then T G (I) T U (I), and (3) if 8k 2 [2; n], d k 2 [1; q), and c = 0, <p> ; i 2 ; ; i n ) and constant element group size, g, over r (&gt; 1) ungrouped arrays, i.e. 8j 2 [1; r], e j = g: (1) if 8k 2 <ref> [2; n] </ref>, d k s=g, then T G (I) = T U (I)=r, (2) if 8k 2 [2; n], d k 2 [q; s=g), and c = 0, then T G (I) T U (I), and (3) if 8k 2 [2; n], d k 2 [1; q), and c = 0, then T G (I) T U (I). <p> [1; r], e j = g: (1) if 8k 2 <ref> [2; n] </ref>, d k s=g, then T G (I) = T U (I)=r, (2) if 8k 2 [2; n], d k 2 [q; s=g), and c = 0, then T G (I) T U (I), and (3) if 8k 2 [2; n], d k 2 [1; q), and c = 0, then T G (I) T U (I). Proof: (1) Since g is the constant element group size for the ungrouped arrays, s=g is the number of element groups in a subpage for an ungrouped array. <p> <ref> [2; n] </ref>, d k 2 [1; q), and c = 0, then T G (I) T U (I). Proof: (1) Since g is the constant element group size for the ungrouped arrays, s=g is the number of element groups in a subpage for an ungrouped array. The condition 8k 2 [2; n], d k s=g implies that each referenced element group of an ungrouped array is in a distinct subpage. Thus, references to each ungrouped array result in n communication transactions. <p> Thus, 8k 2 <ref> [2; n] </ref>, d k &gt; q. Hence, each referenced element group of the grouped array is also in a distinct subpage. Therefore, we have T G (I) = n Thus T G (I) = T U (I)=r. (2) In this case, since 8k 2 [2; n], d k q, we have <p> Thus, 8k 2 <ref> [2; n] </ref>, d k &gt; q. Hence, each referenced element group of the grouped array is also in a distinct subpage. Therefore, we have T G (I) = n Thus T G (I) = T U (I)=r. (2) In this case, since 8k 2 [2; n], d k q, we have T G (I) = n However, because 8k 2 [2; n], d k &lt; s=g, for each ungrouped array the n referenced element 124 groups are in a set of consecutive subpages where each subpage in the set contains at least one referenced element <p> Therefore, we have T G (I) = n Thus T G (I) = T U (I)=r. (2) In this case, since 8k 2 <ref> [2; n] </ref>, d k q, we have T G (I) = n However, because 8k 2 [2; n], d k &lt; s=g, for each ungrouped array the n referenced element 124 groups are in a set of consecutive subpages where each subpage in the set contains at least one referenced element group. <p> This number of transactions is minimal when d k = i k i k1 = q, 8k 2 <ref> [2; n] </ref>. Therefore, we have T U (I) = s = s s Since the number of blanks inserted between element groups in the grouped array, c, is 0, q = s=(e + c) = s=e. Note that gr = e. Hence, gqr = s. <p> Since (e + r (y + y 0 ))=s e=s &gt; 0, this ceiling function is at least 1. As a result, T U (I) (n 1) + 1 = n = T G (I) (3) Since 8k 2 <ref> [2; n] </ref>, d k &lt; q &lt; s=g, for each array (either an ungrouped array or the grouped array), the n referenced element groups are in consecutive subpages.
Reference: [3] <author> Margaret Martonosi, Anoop Gupta, and Thomas E. Anderson. </author> <title> Tuning memory performance of sequential and parallel programs. </title> <journal> IEEE Computer, </journal> <volume> 28(4) </volume> <pages> 32-40, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Reasonable performance objectives must be application-specific as well as machine-specific. Even if a performance objective can be determined, identifying performance problems requires expertise in both hardware and software. Recently, some tools, such as Mtool [1], Cprof [2], and MemSpy <ref> [3] </ref>, have become available to identify performance problems. However, they do not provide direct solutions. Therefore, users may still have to use costly trial and error processes to find appropriate solutions for perceived performance problems. <p> code execution sequence had to be reordered and the initial value of ZX (K+10) had to be 15 (a).Fortran Source Program DO 1 L = 1, Loop DO 1 K = 1, n (b).Compiled Pseudo Code (the inner loop only) CL.2: lfdu fp2, ZX (K+11) [2] fm fp2, fp3, fp2 <ref> [3] </ref> fp3 = T fma fp0, fp5, fp0, fp2 [5] fp5 = R stfdu fp0, X (K) [7] bc CL.2 (c).Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] <p> <ref> [3] </ref> fp3 = T fma fp0, fp5, fp0, fp2 [5] fp5 = R stfdu fp0, X (K) [7] bc CL.2 (c).Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] fp3 = T fma fp2, fp4, fp2, fp1 [5] fp1 = Q stfdu fp2, X (K) [6] bc CL.2 double precision floating-point store with address updated au tomatically). loaded before the loop begins. 2 Usually, removing redundant memory operations increases performance. <p> We used Goblin to disassemble the compiled code and interleaved these instructions as follows: lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R fm fp4, fp5, fp6 [1U]fp5 = R lfdu fp0, ZX (K+12) [2U] fma fp2, fp3, fp6, fp2 <ref> [3] </ref> fp3 = T fma fp4, fp3, fp0, fp4 [3U]fp3 = T fma fp2, fp6, fp2, fp1 [5] fp1 = Q stfd fp2, X (K) [6] fma fp4, fp7, fp4, fp1 [5U]fp1 = Q stfdu fp4, X (K) [6U] bc CL.2 Instructions 1, 3, 5 from an odd iteration are interleaved <p> While this information may be adequate for identifying the bottleneck portion of an application, it is not detailed enough to guide the selection of appropriate techniques for cache performance tuning. Cprof [2], MemSpy <ref> [3] </ref> and our model all identify the performance degradation caused by each array. In the uniprocessor environment, Cprof shows compulsory, conflict, and capacity misses while MemSpy classifies misses into first-load (compulsory) and replacement (conflict and capacity) misses. Both these tools consider conflict misses as a whole.
Reference: [4] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: We consider CPU performance with ideal memory, then consider memory performance later. The CPU performance is dependent on clock rate, clock cycles per instruction, and 2 instruction count, which are in turn determined by hardware technology, processor or-ganization, instruction set architecture, and compiler technology <ref> [4] </ref>. Hence, for a given microprocessor, the delivered CPU performance for an application can be improved by an optimizing compiler. To achieve high performance, a compiler must be aggressive enough to ensure that the bottleneck functional units are kept sufficiently busy.
Reference: [5] <author> Rabin A. Sugumar and Santosh G. Abraham. </author> <title> Efficient simulation of caches under optimal replacement with applications to miss characterization. </title> <booktitle> In ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 24-35, </pages> <address> Santa Clara, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The cache performance is determined to a first order approximation by its miss ratio. Cache misses in a single processor can generally be divided into compulsory, capacity, mapping, and replacement <ref> [5] </ref> (or compulsory, capacity, and conflict [6]). Among these types of cache misses, the compulsory misses are unavoidable; their miss penalties can, however, be hidden by latency hiding techniques such as data prefetching [7]. Capacity misses can generally be reduced effectively in scientific codes by loop blocking [8, 9]. <p> initial value of ZX (K+10) had to be 15 (a).Fortran Source Program DO 1 L = 1, Loop DO 1 K = 1, n (b).Compiled Pseudo Code (the inner loop only) CL.2: lfdu fp2, ZX (K+11) [2] fm fp2, fp3, fp2 [3] fp3 = T fma fp0, fp5, fp0, fp2 <ref> [5] </ref> fp5 = R stfdu fp0, X (K) [7] bc CL.2 (c).Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] fp3 = T fma fp2, fp4, fp2, fp1 [5] <p> <ref> [5] </ref> fp5 = R stfdu fp0, X (K) [7] bc CL.2 (c).Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] fp3 = T fma fp2, fp4, fp2, fp1 [5] fp1 = Q stfdu fp2, X (K) [6] bc CL.2 double precision floating-point store with address updated au tomatically). loaded before the loop begins. 2 Usually, removing redundant memory operations increases performance. <p> instructions as follows: lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R fm fp4, fp5, fp6 [1U]fp5 = R lfdu fp0, ZX (K+12) [2U] fma fp2, fp3, fp6, fp2 [3] fp3 = T fma fp4, fp3, fp0, fp4 [3U]fp3 = T fma fp2, fp6, fp2, fp1 <ref> [5] </ref> fp1 = Q stfd fp2, X (K) [6] fma fp4, fp7, fp4, fp1 [5U]fp1 = Q stfdu fp4, X (K) [6U] bc CL.2 Instructions 1, 3, 5 from an odd iteration are interleaved with 1U, 3U, 5U from the next even iteration to form the execution sequence 1, 1U, 3, <p> This assumption implies that communication is caused only by coherence misses, i.e. no communication is caused by any other types of cache misses, including capacity, conflict, and compulsory misses [6] (or capacity, matching, replacement, and compulsory misses <ref> [5] </ref>).
Reference: [6] <author> Mark Donald Hill. </author> <title> Aspects of Cache Memory and Instruction Buffer Performance. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <month> November </month> <year> 1987. </year> <note> UCB/CSD 87/381. </note>
Reference-contexts: The cache performance is determined to a first order approximation by its miss ratio. Cache misses in a single processor can generally be divided into compulsory, capacity, mapping, and replacement [5] (or compulsory, capacity, and conflict <ref> [6] </ref>). Among these types of cache misses, the compulsory misses are unavoidable; their miss penalties can, however, be hidden by latency hiding techniques such as data prefetching [7]. Capacity misses can generally be reduced effectively in scientific codes by loop blocking [8, 9]. <p> [7] bc CL.2 (c).Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] fp3 = T fma fp2, fp4, fp2, fp1 [5] fp1 = Q stfdu fp2, X (K) <ref> [6] </ref> bc CL.2 double precision floating-point store with address updated au tomatically). loaded before the loop begins. 2 Usually, removing redundant memory operations increases performance. <p> fm fp2, fp5, fp0 [1] fp5 = R fm fp4, fp5, fp6 [1U]fp5 = R lfdu fp0, ZX (K+12) [2U] fma fp2, fp3, fp6, fp2 [3] fp3 = T fma fp4, fp3, fp0, fp4 [3U]fp3 = T fma fp2, fp6, fp2, fp1 [5] fp1 = Q stfd fp2, X (K) <ref> [6] </ref> fma fp4, fp7, fp4, fp1 [5U]fp1 = Q stfdu fp4, X (K) [6U] bc CL.2 Instructions 1, 3, 5 from an odd iteration are interleaved with 1U, 3U, 5U from the next even iteration to form the execution sequence 1, 1U, 3, 3U, 5, 5U, thereby eliminating the two RAW <p> Cache misses in uniprocessors can be due to first time reference to data (compulsory miss), insufficient cache size (capacity miss), or references to lines that were replaced due to limited associativity or nonoptimal replacement (conflict misses) <ref> [6] </ref>. Because unexpected cache performance generally results from complicated conflict behavior, this dissertation focuses on conflict misses. We also limit this dissertation to array references, the dominant case in scientific code. Three types of conflict misses are identified for arrays: self conflict, cross conflict, and ping-pong conflict. <p> This assumption implies that communication is caused only by coherence misses, i.e. no communication is caused by any other types of cache misses, including capacity, conflict, and compulsory misses <ref> [6] </ref> (or capacity, matching, replacement, and compulsory misses [5]).
Reference: [7] <author> Tien-Fu Chen. </author> <title> Data Prefetching for High-Performance Processors. </title> <type> PhD thesis, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, Washington, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Cache misses in a single processor can generally be divided into compulsory, capacity, mapping, and replacement [5] (or compulsory, capacity, and conflict [6]). Among these types of cache misses, the compulsory misses are unavoidable; their miss penalties can, however, be hidden by latency hiding techniques such as data prefetching <ref> [7] </ref>. Capacity misses can generally be reduced effectively in scientific codes by loop blocking [8, 9]. However, mapping and replacement misses (collectively called conflict misses) which are caused by conflicts in the cache, are generally more difficult to eliminate. <p> 15 (a).Fortran Source Program DO 1 L = 1, Loop DO 1 K = 1, n (b).Compiled Pseudo Code (the inner loop only) CL.2: lfdu fp2, ZX (K+11) [2] fm fp2, fp3, fp2 [3] fp3 = T fma fp0, fp5, fp0, fp2 [5] fp5 = R stfdu fp0, X (K) <ref> [7] </ref> bc CL.2 (c).Modified Pseudo Code (the inner loop only) lfd fp0, ZX (K+10) [0] fm fp2, fp5, fp0 [1] fp5 = R lfdu fp0, ZX (K+11) [2] fma fp2, fp3, fp0, fp2 [3] fp3 = T fma fp2, fp4, fp2, fp1 [5] fp1 = Q stfdu fp2, X (K) [6]
Reference: [8] <author> Kyle Gallivan, William Jalby, Ulrike Meier, and Ahmed Sameh. </author> <title> The impact of hierarchical memory systmes on linear algebra algorithm design. </title> <type> Technical report, </type> <institution> Center for Supercomputing Research & Development, University of Illinois, </institution> <month> September </month> <year> 1987. </year> <journal> CSRD Rpt. </journal> <volume> No. </volume> <pages> 625. </pages>
Reference-contexts: Among these types of cache misses, the compulsory misses are unavoidable; their miss penalties can, however, be hidden by latency hiding techniques such as data prefetching [7]. Capacity misses can generally be reduced effectively in scientific codes by loop blocking <ref> [8, 9] </ref>. However, mapping and replacement misses (collectively called conflict misses) which are caused by conflicts in the cache, are generally more difficult to eliminate. Conflict misses can be distinguished into self, cross, and ping-pong conflict misses.
Reference: [9] <author> Michael Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <booktitle> Research Monographs in Parallel and Distributed computing. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: Among these types of cache misses, the compulsory misses are unavoidable; their miss penalties can, however, be hidden by latency hiding techniques such as data prefetching [7]. Capacity misses can generally be reduced effectively in scientific codes by loop blocking <ref> [8, 9] </ref>. However, mapping and replacement misses (collectively called conflict misses) which are caused by conflicts in the cache, are generally more difficult to eliminate. Conflict misses can be distinguished into self, cross, and ping-pong conflict misses. <p> Conclusions are presented in section 3.6. 3.1 Introduction Performance degradation due to cache miss is a critical issue in program performance that becomes increasingly critical as the latency gap widens between processor operations and memory accesses. Several code transformation methodologies, such as loop fusion, loop interchange, and loop blocking <ref> [9] </ref>, and data layout techniques, such as, array grouping [13], array padding, and copying [40], have been proposed for improving locality. Each of these techniques is capable of improving cache performance, but may be effective only for particular types of cache misses. <p> The loop transformation techniques restructure loop nests to adjust the reference order while the data layout techniques try to optimize array storage patterns. The class of loop transformation techniques includes loop fusion, loop distribution, loop interchange, loop permutation, e.t.c. <ref> [9, 23, 24, 25, 49, 50] </ref>. These techniques as well as the hardware techniques deal with gross characteristics of conflict misses, whereas data layout techniques generally deal with particular classes within a relatively fine-grain classification of conflict misses. <p> Finally, wherever A or B appear in the the inner loop body, the leftmost two index expressions are interchanged. This grouped data structure layout should improve the efficiency of communication. Loop transformations such as loop interchange [23, 24], loop permutation [55], loop distribution [77], or loop fusion <ref> [9, 78] </ref> as well as additional array dimension reordering may improve the locality of reference, further improving overall performance. Although finding the best transformations may take exponential time [68], the time may be reduced by prior array grouping since arrays with similar memory access patterns are gathered together.
Reference: [10] <author> William H. Mangione-Smith, Santosh G. Abraham, and Edward S. Davidson. </author> <title> Architectural vs. delivered performance of the IBM RS/6000 and the Astronautics ZS-1. </title> <booktitle> In Proc. Twenty-Fourth Hawaii International Conference on System Sciences, </booktitle> <pages> pages 397-408, </pages> <month> January </month> <year> 1991. </year> <month> 152 </month>
Reference-contexts: As mentioned above, system performance is also affected by cache misses. In Chapter 2, we used simple calibration loops to characterize cache performance (a more detailed cache behavior model is introduced in Chapter 3). The machine-application bound model is based on prior work by Mangione-Smith, Abraham, and Davidson <ref> [10] </ref>. Chapter 2 presents the first empirical evidence that the bound is achievable and derives a mechanism for approaching the performance bound. <p> The work presented in this chapter has been summarized in [13]. 8 CHAPTER 2 Machine-Application Performance Bound Model This chapter develops a method for determining an upper bound on optimum performance for a scientific application on a highly concurrent processor <ref> [10, 14] </ref> and a mechanism for approaching this upper bound performance. The methodology is illustrated by deriving and using these bounds to evaluate and improve the performance of the IBM RS/6000 on a set of scientific code kernels, Livermore Fortran Kernels. <p> manner in which the scientific computing community views performance and the extent to which they are willing to work to optimize their codes. 2.2 Machine-Application Performance Bound Model This section develops a method of finding an upper bound on optimum performance for a scientific application on a highly concurrent processor <ref> [10, 11, 14] </ref>. The methodology is illustrated by deriving and using these bounds to evaluate and improve the performance of the IBM RS/6000 (Figure 2.1) on a set of scientific code kernels.
Reference: [11] <author> William H. Mangione-Smith, Tien-Pao Shih, Santosh G. Abraham, and Edward S. Davidson. </author> <title> Approaching a machine-application bound in delivered performance on scientific code. </title> <journal> Procedings of the IEEE: Special Issue on Computer Performance Evaluation, </journal> <volume> 81(8):1166 - 1178, </volume> <month> August </month> <year> 1993. </year>
Reference-contexts: The machine-application bound model is based on prior work by Mangione-Smith, Abraham, and Davidson [10]. Chapter 2 presents the first empirical evidence that the bound is achievable and derives a mechanism for approaching the performance bound. The work presented in this chapter has been summarized in <ref> [11] </ref> and [12]. 1.2.2 Loop-Based Cache Model and Cache Miss Reduction Techniques The primary goals of the research on memory performance tuning are two-fold: to derive a methodology for building application-specific cache models that can provide information about different types of cache misses and thus guide the selection of appropriate techniques, <p> manner in which the scientific computing community views performance and the extent to which they are willing to work to optimize their codes. 2.2 Machine-Application Performance Bound Model This section develops a method of finding an upper bound on optimum performance for a scientific application on a highly concurrent processor <ref> [10, 11, 14] </ref>. The methodology is illustrated by deriving and using these bounds to evaluate and improve the performance of the IBM RS/6000 (Figure 2.1) on a set of scientific code kernels. <p> The issue of register requirements is not within the scope of this dissertation; this problem is treated in <ref> [11, 36] </ref>. 24 (a). Basic Program REAL*8 X (ARRAY SIZE) : TS = MCLOCK () DO 1 L = 1, LOOPS ( LOAD/STORE KERNEL ) 1 CONTINUE TE = MCLOCK () (b). <p> It is then straightforward to calculate the workload of each of the four units and the performance bound of the loop for given machine parameters [12]. In general, performance is also affected by register limitations and cache misses. Register limitations have been treated in some other related work <ref> [36, 11] </ref>. Simple load/store kernel experiments are recommended in this chapter for characterizing cache structure and performance.
Reference: [12] <author> Eric L. Boyd, Waqar Azeem, Hsien-Hsin Lee, Tien-Pao Shih, Shih-Hao Hung, and Ed-ward S. Davidson. </author> <title> A hierarchical approach to modeling and improving the performance of scientific application on the KSR1. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <volume> volume 3, </volume> <pages> pages 188-192, </pages> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The machine-application bound model is based on prior work by Mangione-Smith, Abraham, and Davidson [10]. Chapter 2 presents the first empirical evidence that the bound is achievable and derives a mechanism for approaching the performance bound. The work presented in this chapter has been summarized in [11] and <ref> [12] </ref>. 1.2.2 Loop-Based Cache Model and Cache Miss Reduction Techniques The primary goals of the research on memory performance tuning are two-fold: to derive a methodology for building application-specific cache models that can provide information about different types of cache misses and thus guide the selection of appropriate techniques, and to <p> It is then straightforward to calculate the workload of each of the four units and the performance bound of the loop for given machine parameters <ref> [12] </ref>. In general, performance is also affected by register limitations and cache misses. Register limitations have been treated in some other related work [36, 11]. Simple load/store kernel experiments are recommended in this chapter for characterizing cache structure and performance.
Reference: [13] <author> Tien-Pao Shih and Edward S. Davidson. </author> <title> Grouping array layouts to reduce communication and improve locality of parallel programs. </title> <booktitle> In 1994 International Conference on Parallel and Distributed Systems, </booktitle> <pages> pages 558 - 566, </pages> <address> Hsinchu, Taiwan, R.O.C., </address> <month> December </month> <year> 1994. </year>
Reference-contexts: The experimental results show a 15% reduction in communication, a 40% reduction in data subcache misses, and an 18% reduction in maximum user time on 56 processors of the KSR1 parallel computer. The work presented in this chapter has been summarized in <ref> [13] </ref>. 8 CHAPTER 2 Machine-Application Performance Bound Model This chapter develops a method for determining an upper bound on optimum performance for a scientific application on a highly concurrent processor [10, 14] and a mechanism for approaching this upper bound performance. <p> Several code transformation methodologies, such as loop fusion, loop interchange, and loop blocking [9], and data layout techniques, such as, array grouping <ref> [13] </ref>, array padding, and copying [40], have been proposed for improving locality. Each of these techniques is capable of improving cache performance, but may be effective only for particular types of cache misses.
Reference: [14] <author> William H. Mangione-Smith, Santosh G. Abraham, and Edward S. Davidson. </author> <title> A performance comparison of the IBM RS/6000 and the Astronautics ZS-1. </title> <journal> IEEE Computer, </journal> <volume> 24(1) </volume> <pages> 39-46, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The work presented in this chapter has been summarized in [13]. 8 CHAPTER 2 Machine-Application Performance Bound Model This chapter develops a method for determining an upper bound on optimum performance for a scientific application on a highly concurrent processor <ref> [10, 14] </ref> and a mechanism for approaching this upper bound performance. The methodology is illustrated by deriving and using these bounds to evaluate and improve the performance of the IBM RS/6000 on a set of scientific code kernels, Livermore Fortran Kernels. <p> manner in which the scientific computing community views performance and the extent to which they are willing to work to optimize their codes. 2.2 Machine-Application Performance Bound Model This section develops a method of finding an upper bound on optimum performance for a scientific application on a highly concurrent processor <ref> [10, 11, 14] </ref>. The methodology is illustrated by deriving and using these bounds to evaluate and improve the performance of the IBM RS/6000 (Figure 2.1) on a set of scientific code kernels.
Reference: [15] <author> Daniel Windheiser and William Jalby. </author> <title> Behavioral characterization of decoupled access/execute architectures. </title> <booktitle> In Proceedings of 1991 International Conference on Supercomputing, </booktitle> <pages> pages 28-39, </pages> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Fixed-Point Unit and Floating-Point Unit are in separate chips. Data Cache consists of 2 or 4 iden tical chips with 16 KByte memory each. groups. Many researchers have evaluated scientific computers by focusing on the expected performance. These studies may involve detailed measurement on implemented machines (e.g. <ref> [15, 16] </ref>), the timing of large applications (e.g. [17, 18]), or analytic performance models (e.g. [19]).
Reference: [16] <author> Joel S. Emer and Douglas W. Clark. </author> <title> A characterization of processor performance in the VAX-11/780. </title> <booktitle> In Proceedings of the 11th International Symposium on Computer Architecture, </booktitle> <pages> pages 301-309, </pages> <address> Ann Arbor, Michigan, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: Fixed-Point Unit and Floating-Point Unit are in separate chips. Data Cache consists of 2 or 4 iden tical chips with 16 KByte memory each. groups. Many researchers have evaluated scientific computers by focusing on the expected performance. These studies may involve detailed measurement on implemented machines (e.g. <ref> [15, 16] </ref>), the timing of large applications (e.g. [17, 18]), or analytic performance models (e.g. [19]).
Reference: [17] <author> Jaswinder P. Singh, Wolf Dietrich-Webber, and Anoop Gupta. </author> <title> Splash: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report 596, </type> <institution> Stanford, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Data Cache consists of 2 or 4 iden tical chips with 16 KByte memory each. groups. Many researchers have evaluated scientific computers by focusing on the expected performance. These studies may involve detailed measurement on implemented machines (e.g. [15, 16]), the timing of large applications (e.g. <ref> [17, 18] </ref>), or analytic performance models (e.g. [19]). We believe that a more appropriate approach to improving performance for scientific applications is to bound the best achievable performance that a machine could deliver on a particular code and then try to approach this bound in delivered performance.
Reference: [18] <author> M. Berry, D. Chen, P. Koss, and D. Kuck. </author> <title> The Perfect Club Benchmarks: Effective performance evaluation of supercomputers. </title> <type> CSRD Report 827, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Data Cache consists of 2 or 4 iden tical chips with 16 KByte memory each. groups. Many researchers have evaluated scientific computers by focusing on the expected performance. These studies may involve detailed measurement on implemented machines (e.g. [15, 16]), the timing of large applications (e.g. <ref> [17, 18] </ref>), or analytic performance models (e.g. [19]). We believe that a more appropriate approach to improving performance for scientific applications is to bound the best achievable performance that a machine could deliver on a particular code and then try to approach this bound in delivered performance.
Reference: [19] <author> T. N. Mudge and H. B. Al-Sadoun. </author> <title> A Semi-Markov model for the performance of multiple-bus systems. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 34(10) </volume> <pages> 934-942, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: Many researchers have evaluated scientific computers by focusing on the expected performance. These studies may involve detailed measurement on implemented machines (e.g. [15, 16]), the timing of large applications (e.g. [17, 18]), or analytic performance models (e.g. <ref> [19] </ref>). We believe that a more appropriate approach to improving performance for scientific applications is to bound the best achievable performance that a machine could deliver on a particular code and then try to approach this bound in delivered performance.
Reference: [20] <author> R. R. Oehler and R. D. Groves. </author> <title> IBM RISC System/6000 processor architecture. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 34(1) </volume> <pages> 23-36, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: These operations can be executed at best at the rate of one operation per clock in the RS/6000 Floating-Point Unit (FPU) <ref> [20] </ref>. In our workload, these are combinable multiply-add triad operations (f ma ), uncombinable adds (f a ), and uncombinable multiplies (f m ). Add and multiply operators that appear in the loop body are checked to see if they are combinable into one of the RS/6000's three-operand multiply-add instructions.
Reference: [21] <author> Gregory F. Grohoski. </author> <title> Machine organization of the IBM RISC System/6000 processor. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 34(1) </volume> <pages> 37-58, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Thus essential stores (see t m ) are counted here as well. * t m for the memory port activity, which requires at least one clock per essential floating-point load (l fl ) and floating-point store (s fl ) <ref> [21] </ref>. Counting only essential loads and stores requires inter-iteration dependence analysis. For m iterations of the inner loop, the number of distinct array elements that appear on the left hand side of assignment statements will generally be of the form am + b.
Reference: [22] <author> Kyle Gallivan, Dennis Gannon, William Jalby, Allen Malony, and Harry Wijshoff. </author> <title> Experimentally characterizing the behavior of multiprocessor memory systems: A case study. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(2) </volume> <pages> 216-223, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Some source-code transformation techniques that can be used to improve performance, such as blocking for better memory or register locality <ref> [22] </ref>, loop interchange [23, 24, 25], and cyclic reduction [26] (which reduces t d by increasing the number of iterations in a recurrence cycle), tend to alter the "essential" workload. <p> In this section, we present a simple methodology that can be developed further to predict real memory performance. This method uses the idea of load/store kernels to learn about the cache structure and calibrate its performance <ref> [22] </ref>. The basic program used here is shown in Figure 2.8a. <p> When ARRAY SIZE is less than 8K, the average access rate is one clock cycle per data element, and we conclude that all referenced data is in cache (the cache region <ref> [22] </ref>). The RS/6000 Model 730 being measured in fact has a 64 KByte cache (= 8K fi 8 bytes). However, when ARRAY SIZE &gt; 8K, some misses occur (beginning of the transition region [22]). <p> cycle per data element, and we conclude that all referenced data is in cache (the cache region <ref> [22] </ref>). The RS/6000 Model 730 being measured in fact has a 64 KByte cache (= 8K fi 8 bytes). However, when ARRAY SIZE &gt; 8K, some misses occur (beginning of the transition region [22]). On cache misses, the IBM RS/6000 replaces the referenced line with the least recently used line in the set. If ARRAY SIZE exceeds the cache size by less than the size of one full cache column (one line of each associative set), some data is never replaced. <p> In this region, the effective access time grows as ARRAY SIZE increases. Since the access time and hence the miss rate, levels off at ARRAY SIZE 10K (the memory region <ref> [22] </ref>), we can conclude that a column is 2K fi 8 bytes and that the cache must be 4-way set associative, as documented [37]. Let T c , T t , and T m be the clocks per floating point load in the cache, transient, and memory regions, respectively.
Reference: [23] <author> John R. Allen and Ken Kennedy. </author> <title> Automatic loop interchange. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 19(6) </volume> <pages> 233-246, </pages> <month> June </month> <year> 1984. </year> <booktitle> Proceedings of the ACM SIGPLAN'84 Symposium on Compiler Construction. </booktitle> <pages> 153 </pages>
Reference-contexts: Some source-code transformation techniques that can be used to improve performance, such as blocking for better memory or register locality [22], loop interchange <ref> [23, 24, 25] </ref>, and cyclic reduction [26] (which reduces t d by increasing the number of iterations in a recurrence cycle), tend to alter the "essential" workload. Such transformations must be viewed as creating a different application code for which the CPF bound would have to be recalculated. <p> The loop transformation techniques restructure loop nests to adjust the reference order while the data layout techniques try to optimize array storage patterns. The class of loop transformation techniques includes loop fusion, loop distribution, loop interchange, loop permutation, e.t.c. <ref> [9, 23, 24, 25, 49, 50] </ref>. These techniques as well as the hardware techniques deal with gross characteristics of conflict misses, whereas data layout techniques generally deal with particular classes within a relatively fine-grain classification of conflict misses. <p> Although the example contains only con 111 stant strides, our technique is also applicable to nonconsecutive references caused by ir-regular (computed) strides and indirect array references. In addition, the complete data reorganization process also includes loop transformation techniques such as loop interchange <ref> [23, 24, 25] </ref> and loop permutation [55]. 5.2 Related Work Our research aims to improve program performance by optimizing data layout and sequencing memory access in cache-based shared-memory multiprocessor systems. <p> Finally, wherever A or B appear in the the inner loop body, the leftmost two index expressions are interchanged. This grouped data structure layout should improve the efficiency of communication. Loop transformations such as loop interchange <ref> [23, 24] </ref>, loop permutation [55], loop distribution [77], or loop fusion [9, 78] as well as additional array dimension reordering may improve the locality of reference, further improving overall performance.
Reference: [24] <author> Michael Wolfe. </author> <title> Advanced loop interchanging. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 536-543, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Some source-code transformation techniques that can be used to improve performance, such as blocking for better memory or register locality [22], loop interchange <ref> [23, 24, 25] </ref>, and cyclic reduction [26] (which reduces t d by increasing the number of iterations in a recurrence cycle), tend to alter the "essential" workload. Such transformations must be viewed as creating a different application code for which the CPF bound would have to be recalculated. <p> The loop transformation techniques restructure loop nests to adjust the reference order while the data layout techniques try to optimize array storage patterns. The class of loop transformation techniques includes loop fusion, loop distribution, loop interchange, loop permutation, e.t.c. <ref> [9, 23, 24, 25, 49, 50] </ref>. These techniques as well as the hardware techniques deal with gross characteristics of conflict misses, whereas data layout techniques generally deal with particular classes within a relatively fine-grain classification of conflict misses. <p> Although the example contains only con 111 stant strides, our technique is also applicable to nonconsecutive references caused by ir-regular (computed) strides and indirect array references. In addition, the complete data reorganization process also includes loop transformation techniques such as loop interchange <ref> [23, 24, 25] </ref> and loop permutation [55]. 5.2 Related Work Our research aims to improve program performance by optimizing data layout and sequencing memory access in cache-based shared-memory multiprocessor systems. <p> Finally, wherever A or B appear in the the inner loop body, the leftmost two index expressions are interchanged. This grouped data structure layout should improve the efficiency of communication. Loop transformations such as loop interchange <ref> [23, 24] </ref>, loop permutation [55], loop distribution [77], or loop fusion [9, 78] as well as additional array dimension reordering may improve the locality of reference, further improving overall performance.
Reference: [25] <author> Hans Zima and Barbara Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Some source-code transformation techniques that can be used to improve performance, such as blocking for better memory or register locality [22], loop interchange <ref> [23, 24, 25] </ref>, and cyclic reduction [26] (which reduces t d by increasing the number of iterations in a recurrence cycle), tend to alter the "essential" workload. Such transformations must be viewed as creating a different application code for which the CPF bound would have to be recalculated. <p> The loop transformation techniques restructure loop nests to adjust the reference order while the data layout techniques try to optimize array storage patterns. The class of loop transformation techniques includes loop fusion, loop distribution, loop interchange, loop permutation, e.t.c. <ref> [9, 23, 24, 25, 49, 50] </ref>. These techniques as well as the hardware techniques deal with gross characteristics of conflict misses, whereas data layout techniques generally deal with particular classes within a relatively fine-grain classification of conflict misses. <p> Although the example contains only con 111 stant strides, our technique is also applicable to nonconsecutive references caused by ir-regular (computed) strides and indirect array references. In addition, the complete data reorganization process also includes loop transformation techniques such as loop interchange <ref> [23, 24, 25] </ref> and loop permutation [55]. 5.2 Related Work Our research aims to improve program performance by optimizing data layout and sequencing memory access in cache-based shared-memory multiprocessor systems. <p> The work of a serial section is executed by only one processor, that of a parallel section is partitioned and executed concurrently by multiple processors, and that of a replicate section is replicated and executed by every processor <ref> [25] </ref>. We focus initially on a simple, but common parallel program structure (Figure 5.1). This structure consists of a series of parallel sections, each enclosed by a parallel construct that enforces barriers (replicate sections) at the beginning and end of the section. <p> One processor is initially assigned to be the master processor responsible for executing all serial sections. A team of processors (including the master processor) is designated by 1 A loop is normalized if its initial loop index and step size are 1 <ref> [25] </ref>. 113 the system to execute each enclosed parallel section concurrently. The number of processors in a team exactly equals the number of components in the corresponding parallel section. Processors in a team must be synchronized when they encounter a barrier.
Reference: [26] <author> Harold S. Stone. </author> <title> High-Performance Computer Architecture. </title> <publisher> Addison-Wesley, </publisher> <address> New York, third edition, </address> <year> 1993. </year>
Reference-contexts: Some source-code transformation techniques that can be used to improve performance, such as blocking for better memory or register locality [22], loop interchange [23, 24, 25], and cyclic reduction <ref> [26] </ref> (which reduces t d by increasing the number of iterations in a recurrence cycle), tend to alter the "essential" workload. Such transformations must be viewed as creating a different application code for which the CPF bound would have to be recalculated. <p> This footprint is defined as the set of distinct referenced lines of each array that are in that cache set, i.e. the set of lines that would be resident in that cache set after running the application if the cache had an infinite degree of associativity, i.e. (1; S; L) <ref> [26] </ref>. Let H X (i) be the number of sets whose footprint contains exactly i lines of array X, and possibly some lines of other arrays. The function, H X (i) for i = 0; 1; 2; : : :, is then the footprint histogram of array X.
Reference: [27] <author> F. H. McMahon. </author> <title> The Livermore Fortran Kernels: A computer test of the numerical performance range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: The goal of this research and this case study is to evaluate performance relative to this simple bound, to find mechanisms for approaching the bound performance, and to explain the gaps between achievable and bound performance when they do occur. The first 12 Liver-more Fortran Kernels <ref> [27] </ref> are used as the application workload for the experimental study in this chapter. Table 2.1 lists the essential workload and the bound for LFK1-12.
Reference: [28] <author> Henry S. Warren Jr. </author> <title> Predicting execution time on the IBM RISC RS/6000. </title> <type> Technical Report IBM Research Report RC 16994(#75050), </type> <institution> IBM, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: By analyzing the compiled code, we have identified four major compiler-related issues that depressed the delivered performance: redundant loads and stores, read after write (RAW) dependence stalls, floating-point store vs. write-back conflicts <ref> [28] </ref>, and a fine grain code scheduling problem. These issues are discussed in this section. Some model and measurement refinement issues are deferred to the subsequent section. 2.3.1 Remove redundant loads/stores Redundant memory loads occur when data that are reused in later iterations are not saved in register files. <p> the stalls caused by RAW dependence among FPU arithmetic instructions. 2 In RS/6000, the second instruction of two consecutive dependent FPU arithmetic instructions will typically be stalled for one clock before entering the FPU pipeline (FPU pipeline latency for multiply-add is usually 2, sometimes more when data exceptions are possible) <ref> [28] </ref>. These stall clocks are added to the time, t f , required in the FPU. Table 2.2 shows the number of such RAW stall cycles per iteration for the compiled code. <p> In the absence of data exceptions, there is a one clock delay when the conflicting instructions pass through FPU decode three clocks apart <ref> [28] </ref>. For the workload in our experiments, this delay seems to occur only when the arithmetic destination and store registers are different. This conflict is very common: LFK1, 7, 8, 9, 11 and 12 each have at least one such conflict. <p> This data reflects that the fifth continuous stf will be delayed for one cycle <ref> [28] </ref>. From this figure, we can conclude by an analysis similar to that for Figure 2.9 that the cache is 4-way set associative with a 128-byte line size and a 64 KByte capacity.
Reference: [29] <author> Criss Stephens, Bryce Cogswell, John Heinlein, Gregory Palmer, and John P. Shen. </author> <title> Instruction level profiling and evaluation of the IBM RS/6000. </title> <booktitle> In Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 180-189, </pages> <address> Toronto, Canada, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: The loop is unrolled twice and different registers are used for odd and even iterations. Redundant stores are simply removed and final stores are inserted after the loop. Since there is no official disassembler for the RS/6000, nor an executable assembly code output from the compiler, we used Goblin <ref> [29] </ref> to disassemble the compiled code, and performed the necessary modifications on that code. To illustrate the first method, consider the following example.
Reference: [30] <author> Christine Eisenbeis, William Jalby, and Alain Lichnewsky. </author> <title> Squeezing more CPU performance out of a Cray-2 by vector block scheduling. </title> <booktitle> In Proc. of Supercomputing '88, </booktitle> <pages> pages 237-246, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: However, for loops such as LFK7 and 8, which have more floating-point arithmetic instructions than essential memory operations, eliminating all redundant memory operations does not guarantee higher performance. Moreover, when register reallocation is difficult, necessitating a large degree of unrolling or register moves through the FPU pipeline <ref> [30] </ref>, the burden on the FPU can increase. Consequently, only one memory reference was removed in LFK7, and three in LFK8. All the redundant memory operations were removed in the other loops. <p> Thus we only consider dependent FPU arithmetic instructions. 17 separated by one FPU instruction from the other iteration. Unrolling more than twice may be required if data exceptions are suspected. The following example describes this approach. Similarly, we can use cyclic scheduling here <ref> [30, 31, 32, 33] </ref> (a form of software pipelining) as an alternative. In general, the RAW dependence stall cycles can be eliminated entirely, except when there are not enough registers for the new schedule, or there are not enough independent pairs of FPU operations in a recursive loop.
Reference: [31] <author> B. R. Rau, C. D. Glaeser, and R. L. </author> <title> Picard. Efficient code generation for horizontal architectures: </title> <booktitle> Compiler techniques and architectural support. In Proc. Int. Symp. on Computer Architecture, </booktitle> <pages> pages 131-139, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Thus we only consider dependent FPU arithmetic instructions. 17 separated by one FPU instruction from the other iteration. Unrolling more than twice may be required if data exceptions are suspected. The following example describes this approach. Similarly, we can use cyclic scheduling here <ref> [30, 31, 32, 33] </ref> (a form of software pipelining) as an alternative. In general, the RAW dependence stall cycles can be eliminated entirely, except when there are not enough registers for the new schedule, or there are not enough independent pairs of FPU operations in a recursive loop.
Reference: [32] <author> R. F. Touzeau. </author> <title> A Fortran compiler for the FPS-164 scientific computer. </title> <booktitle> In Proc. ACM SIGPLAN '84 Symp. on Compiler Construction, </booktitle> <pages> pages 48-57, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Thus we only consider dependent FPU arithmetic instructions. 17 separated by one FPU instruction from the other iteration. Unrolling more than twice may be required if data exceptions are suspected. The following example describes this approach. Similarly, we can use cyclic scheduling here <ref> [30, 31, 32, 33] </ref> (a form of software pipelining) as an alternative. In general, the RAW dependence stall cycles can be eliminated entirely, except when there are not enough registers for the new schedule, or there are not enough independent pairs of FPU operations in a recursive loop.
Reference: [33] <author> Ju-ho Tang, Edward S. Davidson, and Johau Tong. </author> <title> Polycyclic vector scheduling vs. chaining on 1-port vector supercomputers. </title> <booktitle> In Proc. of Supercomputing '88, </booktitle> <pages> pages 122-129, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Thus we only consider dependent FPU arithmetic instructions. 17 separated by one FPU instruction from the other iteration. Unrolling more than twice may be required if data exceptions are suspected. The following example describes this approach. Similarly, we can use cyclic scheduling here <ref> [30, 31, 32, 33] </ref> (a form of software pipelining) as an alternative. In general, the RAW dependence stall cycles can be eliminated entirely, except when there are not enough registers for the new schedule, or there are not enough independent pairs of FPU operations in a recursive loop.
Reference: [34] <institution> AIX XL FORTRAN Compiler/6000 User's Guide Version 2.2, </institution> <note> second edition, Septem-ber 1991. SC09-1354-01. </note>
Reference-contexts: The preprocessor can be activated by turning on optimization 20 switches or/and by inserting directives. In addition to the default switch settings <ref> [34, p. 53] </ref>, the interesting switches include switch a (allows associative transformations), switch j (generates calls to Engineering/Scientific Subroutine Library routines), switch 2 (performs no dependence checking for arrays containing pointer variables), switch 4 (generates calls to BLAS library routines), switch 7 (inlines called routines automatically), and switch 9 (collapses array
Reference: [35] <author> C. Brian Hall and Kevin O'Brien. </author> <title> Performance characteristics of architectural features of the IBM RS/6000. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 303-309, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: and iteration count overhead, ignored instruction types, certain dependencies, and hardware interlocks are expected to have little or no effect on the performance of scientific inner loops due to the success of the RS/6000 branch handling and other hardware design features in masking their effects through buffering and concurrent execution <ref> [35] </ref>. However, there is some outer loop overhead to compute the loop limit and load it into the count register before the loop begins and to execute other code, if any, that is not part of an inner loop body.
Reference: [36] <author> William H. Mangione-Smith, Santosh G. Abraham, and Edward S. Davidson. </author> <title> Register requirements of pipelined processors. </title> <booktitle> In Proc. International Conference on Supercomputing, </booktitle> <pages> pages 260-271, </pages> <address> Washington D.C., USA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The issue of register requirements is not within the scope of this dissertation; this problem is treated in <ref> [11, 36] </ref>. 24 (a). Basic Program REAL*8 X (ARRAY SIZE) : TS = MCLOCK () DO 1 L = 1, LOOPS ( LOAD/STORE KERNEL ) 1 CONTINUE TE = MCLOCK () (b). <p> It is then straightforward to calculate the workload of each of the four units and the performance bound of the loop for given machine parameters [12]. In general, performance is also affected by register limitations and cache misses. Register limitations have been treated in some other related work <ref> [36, 11] </ref>. Simple load/store kernel experiments are recommended in this chapter for characterizing cache structure and performance. <p> One disadvantage of this method is that it requires additional registers for storing the accessed data, and thus may inhibit the use of some advanced compiler optimizations that also have a high register requirement, such as software pipelining <ref> [52, 36] </ref>. Rather than rely on being able to apply reference reordering to eliminate alternate reference patterns, our approach adjusts the data layout instead.
Reference: [37] <author> William R. Hardell, Dwain A. Hicks, Lawrence C. Howell Jr., Warren E. Maule, Robert Montoye, and David P. Tuttle. </author> <title> Data cache and storage control units. </title> <booktitle> In IBM RISC System/6000 Technology, </booktitle> <pages> pages 44-50. </pages> <institution> IBM Corporation, </institution> <year> 1990. </year> <month> 154 </month>
Reference-contexts: Since the access time and hence the miss rate, levels off at ARRAY SIZE 10K (the memory region [22]), we can conclude that a column is 2K fi 8 bytes and that the cache must be 4-way set associative, as documented <ref> [37] </ref>. Let T c , T t , and T m be the clocks per floating point load in the cache, transient, and memory regions, respectively. For a given stride, T c and T m are each constant.
Reference: [38] <author> Gheith A. Abandah and Edward S. Davidson. </author> <title> Characterizing shared memory and communication performance: A case study of the Convex SPP-1000. </title> <type> Technical report, </type> <institution> University of Michigan, </institution> <year> 1996. </year> <month> CSE-TR-277-96. </month>
Reference-contexts: the transient region, the value of T t is determined by the formula: 26 T t = T c ARRAY SIZE (D + 1)(ARRAY SIZE C) ARRAY SIZE + (D + 1)(ARRAY SIZE C) ARRAY SIZE (2.2) where C is the cache size, and D is the degree of associativity <ref> [38] </ref>. In the transient region, the cache capacity is exceeded by a number of elements that is less than one column of the cache, i.e. ARRAY SIZE C &lt; C=D.
Reference: [39] <author> Dennis Gannon, Jenq kuen Lee, Bruce Shei, Sekhar Sarukkai, Srinivas Narayana, Nee-lakantan Sundaresan, Daya Atapattu, and Francois Bodin. </author> <title> SIGMA II: A tool kit for building parallelizing compilers and performance analysis systems. </title> <booktitle> In Proceedings of the IFIP WG 10.3 Workshop on Programming Environments for Parallel Computing, </booktitle> <pages> pages 17-36, </pages> <address> Edinburgh, Scotland, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: The tool employs a two step process. First, SIGMA, a tool kit for building parallelizing compilers and performance analysis systems, builds a database which contains needed information such as expression trees and dependence vectors <ref> [39] </ref>. Then, the tool backtracks through the expression tree to compute the number of essential floating-point operations, groups memory operations based on their dependence vectors and computes the number of essential load/stores, and computes the length of the maximally-weighted dependence cycle.
Reference: [40] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The cache performance and optimizations of blocked algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Several code transformation methodologies, such as loop fusion, loop interchange, and loop blocking [9], and data layout techniques, such as, array grouping [13], array padding, and copying <ref> [40] </ref>, have been proposed for improving locality. Each of these techniques is capable of improving cache performance, but may be effective only for particular types of cache misses. <p> As analytic models can generally be evaluated quickly, they are potential candidates for inclusion within a compiler to enable automatic performance tuning. Models derived by Lam et al. <ref> [40] </ref> and Temam et al. [42], as well as ours, belong to this category. Lam et al. classify cache misses into intrinsic, self conflict, and cross conflict misses, and derive a model accordingly. However, these misses are not evaluated for individual arrays. <p> These techniques as well as the hardware techniques deal with gross characteristics of conflict misses, whereas data layout techniques generally deal with particular classes within a relatively fine-grain classification of conflict misses. Much of the prior research on data layout techniques has focused on the copy technique <ref> [40, 41] </ref>. The array copy technique copies array sections of concern into a contiguous 68 temporary array, replaces references to the original arrays with references to the temporary array, and finally updates copies of the elements in the temporary array back to the original arrays, if necessary. <p> If the block is read only, then we can use the copy technique <ref> [40, 41] </ref> to eliminate the self conflict misses without any coherence problem. However, there may still be significant copy overhead [41]. Due to the two potential problems in the copy technique (copy overhead and cache coherence), we focus on a different approach, array padding, for eliminating the self conflict misses.
Reference: [41] <author> Olivier Temam, Elana D. Granston, and William Jalby. </author> <title> To copy or not to copy: A compile-time technique for assessing when data copying should be used to eliminate cache conflicts. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <pages> pages 410-419, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Moreover, the same program may exhibit different 31 cache behavior as a function of input data size, and thus applying any particular technique may not achieve its expected improvement <ref> [41] </ref>. As a result, selecting a suitable code or data restructuring technique is an extremely challenging task. We propose a methodology for building application-specific cache models that can provide information about different types of cache misses and thus guide the selection of appropriate techniques. <p> These techniques as well as the hardware techniques deal with gross characteristics of conflict misses, whereas data layout techniques generally deal with particular classes within a relatively fine-grain classification of conflict misses. Much of the prior research on data layout techniques has focused on the copy technique <ref> [40, 41] </ref>. The array copy technique copies array sections of concern into a contiguous 68 temporary array, replaces references to the original arrays with references to the temporary array, and finally updates copies of the elements in the temporary array back to the original arrays, if necessary. <p> If the block is read only, then we can use the copy technique <ref> [40, 41] </ref> to eliminate the self conflict misses without any coherence problem. However, there may still be significant copy overhead [41]. Due to the two potential problems in the copy technique (copy overhead and cache coherence), we focus on a different approach, array padding, for eliminating the self conflict misses. <p> If the block is read only, then we can use the copy technique [40, 41] to eliminate the self conflict misses without any coherence problem. However, there may still be significant copy overhead <ref> [41] </ref>. Due to the two potential problems in the copy technique (copy overhead and cache coherence), we focus on a different approach, array padding, for eliminating the self conflict misses.
Reference: [42] <author> Olivier Temam, Christine Fricker, and William Jalby. </author> <title> Cache interference phenomena. In Performance Evaluation Review special issue: </title> <booktitle> 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <volume> volume 22, </volume> <pages> pages 261-271, </pages> <address> Nashville, Tennessee, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Ping-pong conflict misses are considered separately as a special class because they can severely degrade performance. For example, Temam et al. present a program whose run time normally varies within 20% as the data layout in cache is changed, but can become 500% larger when ping-pong conflict misses occur <ref> [42] </ref>. The remaining conflict misses are either self conflicts or cross conflicts. Self-conflict misses for each particular array can be counted by imagining that only that array is allowed to reside in cache. <p> As analytic models can generally be evaluated quickly, they are potential candidates for inclusion within a compiler to enable automatic performance tuning. Models derived by Lam et al. [40] and Temam et al. <ref> [42] </ref>, as well as ours, belong to this category. Lam et al. classify cache misses into intrinsic, self conflict, and cross conflict misses, and derive a model accordingly. However, these misses are not evaluated for individual arrays. In addition, their model does not include ping-pong conflict misses.
Reference: [43] <author> Eric Logan Boyd. </author> <title> Performance Evaluation and Improvement of Parallel Applications on High Performance Architectures. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, The University of Michigan, </institution> <year> 1995. </year>
Reference-contexts: For example, the hierarchical bound model MACS12*B <ref> [43] </ref> relies on hardware monitoring to acquire the actual number of cache misses in a program. It is precise but is limited to machines with special hardware.
Reference: [44] <author> Alan Jay Smith. </author> <title> Cache memories. </title> <journal> Computing Surveys, </journal> <volume> 14(3) </volume> <pages> 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: The common strategy employed by the hardware techniques is to provide additional space for conflict lines. The most direct method is to increase the degree of set associativity <ref> [44] </ref>. However, according to the experiments in Chapter 3, increasing the degree of set associativity may not solve the problem as expected. In addition, the hardware cost and cache access latency will be increased [45]. Hence, regarding reduction of conflict misses, most of the research focuses on direct mapped caches.
Reference: [45] <author> Mark D. Hill. </author> <title> A case of direct-mapped cache. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 25-40, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: The most direct method is to increase the degree of set associativity [44]. However, according to the experiments in Chapter 3, increasing the degree of set associativity may not solve the problem as expected. In addition, the hardware cost and cache access latency will be increased <ref> [45] </ref>. Hence, regarding reduction of conflict misses, most of the research focuses on direct mapped caches.
Reference: [46] <author> Norman P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Hence, regarding reduction of conflict misses, most of the research focuses on direct mapped caches. In a direct mapped cache, additional space is sometimes provided by adding a small fully-associative cache, such as a victim cache which retains a few of the most recently replaced lines among all sets <ref> [46] </ref>, by allowing an alternative cache set within the cache, such as in a column-associative cache which maps a conflict line to a different cache set [47], or by directly passing the conflict line through the cache without being stored [48].
Reference: [47] <author> Anant Agarwal and Steven D. Pudar. </author> <title> Column-associative caches: A technique for reducing the miss rate of direct-mapped caches. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 179-190, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: by adding a small fully-associative cache, such as a victim cache which retains a few of the most recently replaced lines among all sets [46], by allowing an alternative cache set within the cache, such as in a column-associative cache which maps a conflict line to a different cache set <ref> [47] </ref>, or by directly passing the conflict line through the cache without being stored [48].
Reference: [48] <author> Scott McFarling. </author> <title> Cache replacement with dynamic exclusion. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 191-200, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: few of the most recently replaced lines among all sets [46], by allowing an alternative cache set within the cache, such as in a column-associative cache which maps a conflict line to a different cache set [47], or by directly passing the conflict line through the cache without being stored <ref> [48] </ref>. The general concerns regarding such hardware methods are that programs can only benefit by running on those machines that implement the solution and that a particular program may only realize a significant benefit with only a particular subset of the possible hardware implementations.
Reference: [49] <author> Steve Carr, Ken Kennedy, Kathryn S. McKinley, and Chau-Wen Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <type> Technical Report COMP TR92-195, </type> <institution> Rice University, </institution> <month> November </month> <year> 1992. </year> <month> 155 </month>
Reference-contexts: The loop transformation techniques restructure loop nests to adjust the reference order while the data layout techniques try to optimize array storage patterns. The class of loop transformation techniques includes loop fusion, loop distribution, loop interchange, loop permutation, e.t.c. <ref> [9, 23, 24, 25, 49, 50] </ref>. These techniques as well as the hardware techniques deal with gross characteristics of conflict misses, whereas data layout techniques generally deal with particular classes within a relatively fine-grain classification of conflict misses.
Reference: [50] <author> Ken Kennedy and Kathryn S. McKinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <type> Technical Report COMP TR92-189, </type> <institution> Rice University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: The loop transformation techniques restructure loop nests to adjust the reference order while the data layout techniques try to optimize array storage patterns. The class of loop transformation techniques includes loop fusion, loop distribution, loop interchange, loop permutation, e.t.c. <ref> [9, 23, 24, 25, 49, 50] </ref>. These techniques as well as the hardware techniques deal with gross characteristics of conflict misses, whereas data layout techniques generally deal with particular classes within a relatively fine-grain classification of conflict misses. <p> For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques <ref> [50, 63, 64, 65, 66, 67] </ref>. Instead of just considering the loop restructuring techniques, Ju [68] studies the problem by involving both data layout and loop transformation in the solution.
Reference: [51] <author> Yoji Yamada, John Gyllenhall, Grant Haab, and Wen-Mei Hwu. </author> <title> Data relocation and prefetching for programs with large data sets. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 118-127, </pages> <address> San Jose, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Yamada et al. proposed a mechanism similar to the gather-scatter operations in vector machines to reduce the run time overhead by providing hardware support for data copying <ref> [51] </ref>. However, they do not include any mechanism to ease the coherence problem caused by the copy technique. Two of the three techniques used in this study are data layout techniques: namely, array padding and initial address adjustment. These two techniques only modify the storage patterns.
Reference: [52] <author> Monica Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <address> Atlanta, Georgia, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: One disadvantage of this method is that it requires additional registers for storing the accessed data, and thus may inhibit the use of some advanced compiler optimizations that also have a high register requirement, such as software pipelining <ref> [52, 36] </ref>. Rather than rely on being able to apply reference reordering to eliminate alternate reference patterns, our approach adjusts the data layout instead.
Reference: [53] <author> Ivan Niven, Herbert S. Zucherman, and Hugh L. Montgomery. </author> <title> An Introduction to The Theory of Numbers. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: The basic definitions, Definition 4.3.1, 4.3.2, and 4.3.3, and the main theorem, Theorem 4.3.1 are summarized from <ref> [53] </ref> and [54]. A congruence is a statement about divisibility. Congruence theory employs a powerful notation that has lead to many known theorems.
Reference: [54] <author> William J. Leveque. </author> <title> Elementary Theory of Numbers. </title> <publisher> Dover Publications Inc., </publisher> <year> 1990. </year>
Reference-contexts: The basic definitions, Definition 4.3.1, 4.3.2, and 4.3.3, and the main theorem, Theorem 4.3.1 are summarized from [53] and <ref> [54] </ref>. A congruence is a statement about divisibility. Congruence theory employs a powerful notation that has lead to many known theorems.
Reference: [55] <author> Utpal Banerjee. </author> <title> A theory of loop permutations. </title> <editor> In David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Research Monographs in Parallel and Distributed computing, </booktitle> <pages> pages 54-74. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1989. </year> <booktitle> (Second Workshop on Language and Compilers for Parallel Computing, </booktitle> <month> August, </month> <year> 1989, </year> <institution> Urbana, Illinois.). </institution>
Reference-contexts: Although the example contains only con 111 stant strides, our technique is also applicable to nonconsecutive references caused by ir-regular (computed) strides and indirect array references. In addition, the complete data reorganization process also includes loop transformation techniques such as loop interchange [23, 24, 25] and loop permutation <ref> [55] </ref>. 5.2 Related Work Our research aims to improve program performance by optimizing data layout and sequencing memory access in cache-based shared-memory multiprocessor systems. <p> Finally, wherever A or B appear in the the inner loop body, the leftmost two index expressions are interchanged. This grouped data structure layout should improve the efficiency of communication. Loop transformations such as loop interchange [23, 24], loop permutation <ref> [55] </ref>, loop distribution [77], or loop fusion [9, 78] as well as additional array dimension reordering may improve the locality of reference, further improving overall performance.
Reference: [56] <author> Mary E. Mace. </author> <title> Memory Storage Patterns in Parallel Processing. </title> <publisher> Kluwer Academic, </publisher> <address> Boston, </address> <year> 1987. </year>
Reference-contexts: Many studies have considered data layout techniques to improve memory performance and reduce communication for other types of architectures such as vector supercomputers <ref> [56, 57] </ref>, Single Instruction stream- Multiple Data stream (SIMD) systems [58, 59], and message-passing non-shared-memory systems [60, 61, 62]. For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques [50, 63, 64, 65, 66, 67].
Reference: [57] <author> Mary E. Mace and Robert A. Wagner. </author> <title> Globally optimum selection of memory storage patterns. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 264-271, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Many studies have considered data layout techniques to improve memory performance and reduce communication for other types of architectures such as vector supercomputers <ref> [56, 57] </ref>, Single Instruction stream- Multiple Data stream (SIMD) systems [58, 59], and message-passing non-shared-memory systems [60, 61, 62]. For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques [50, 63, 64, 65, 66, 67].
Reference: [58] <author> J.M. Frailong, W. Jalby, and J. Lenfant. XOR-SCHEMES: </author> <title> A flexible data organization in parallel memories. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 276-283, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Many studies have considered data layout techniques to improve memory performance and reduce communication for other types of architectures such as vector supercomputers [56, 57], Single Instruction stream- Multiple Data stream (SIMD) systems <ref> [58, 59] </ref>, and message-passing non-shared-memory systems [60, 61, 62]. For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques [50, 63, 64, 65, 66, 67].
Reference: [59] <author> Kathleen Knobe, Joan D. Lukas, and Guy L. Steele. Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Many studies have considered data layout techniques to improve memory performance and reduce communication for other types of architectures such as vector supercomputers [56, 57], Single Instruction stream- Multiple Data stream (SIMD) systems <ref> [58, 59] </ref>, and message-passing non-shared-memory systems [60, 61, 62]. For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques [50, 63, 64, 65, 66, 67].
Reference: [60] <author> Jingke Li and Marina Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Many studies have considered data layout techniques to improve memory performance and reduce communication for other types of architectures such as vector supercomputers [56, 57], Single Instruction stream- Multiple Data stream (SIMD) systems [58, 59], and message-passing non-shared-memory systems <ref> [60, 61, 62] </ref>. For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques [50, 63, 64, 65, 66, 67].
Reference: [61] <author> J. Ramanumjam and P. Sadayappan. </author> <title> Compile-time techniques for data distribution in distributed memory machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Many studies have considered data layout techniques to improve memory performance and reduce communication for other types of architectures such as vector supercomputers [56, 57], Single Instruction stream- Multiple Data stream (SIMD) systems [58, 59], and message-passing non-shared-memory systems <ref> [60, 61, 62] </ref>. For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques [50, 63, 64, 65, 66, 67].
Reference: [62] <author> Manish Gupta and Prithviraj Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year> <month> 156 </month>
Reference-contexts: Many studies have considered data layout techniques to improve memory performance and reduce communication for other types of architectures such as vector supercomputers [56, 57], Single Instruction stream- Multiple Data stream (SIMD) systems [58, 59], and message-passing non-shared-memory systems <ref> [60, 61, 62] </ref>. For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques [50, 63, 64, 65, 66, 67].
Reference: [63] <author> Sesh Venugopal and William Eventoff. </author> <title> Automatic transformation of FORTRAN loops to reduce cache conflicts. </title> <booktitle> In Proceedings of 1991 International Conference on Supercomputing, </booktitle> <pages> pages 183-193, </pages> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques <ref> [50, 63, 64, 65, 66, 67] </ref>. Instead of just considering the loop restructuring techniques, Ju [68] studies the problem by involving both data layout and loop transformation in the solution.
Reference: [64] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques <ref> [50, 63, 64, 65, 66, 67] </ref>. Instead of just considering the loop restructuring techniques, Ju [68] studies the problem by involving both data layout and loop transformation in the solution.
Reference: [65] <author> Daniel Windheiser. </author> <title> Data Locality Optimization. </title> <type> PhD thesis, </type> <institution> IRISA, University of Rennes, </institution> <year> 1992. </year>
Reference-contexts: For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques <ref> [50, 63, 64, 65, 66, 67] </ref>. Instead of just considering the loop restructuring techniques, Ju [68] studies the problem by involving both data layout and loop transformation in the solution.
Reference: [66] <author> David E. Hudak and Santosh G. Abraham. </author> <title> Compile-time optimization of near-neighbor communication for scalable shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15 </volume> <pages> 368-381, </pages> <year> 1992. </year>
Reference-contexts: For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques <ref> [50, 63, 64, 65, 66, 67] </ref>. Instead of just considering the loop restructuring techniques, Ju [68] studies the problem by involving both data layout and loop transformation in the solution.
Reference: [67] <author> Karen A. Tomko and Santosh G. Abraham. </author> <title> Iteration partitioning for resolving stride conflicts on cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 95-102, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques <ref> [50, 63, 64, 65, 66, 67] </ref>. Instead of just considering the loop restructuring techniques, Ju [68] studies the problem by involving both data layout and loop transformation in the solution.
Reference: [68] <author> Yeun-Jyr Ju. </author> <title> Compiler Data Layout and Code Transformation for Reducing Cache Coherence Overhead. </title> <type> PhD thesis, </type> <institution> Purdue University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: For cache-based shared-memory multiprocessor systems, cache performance improvement and communication overhead reduction are often achieved by considering loop transformation and partitioning techniques [50, 63, 64, 65, 66, 67]. Instead of just considering the loop restructuring techniques, Ju <ref> [68] </ref> studies the problem by involving both data layout and loop transformation in the solution. <p> Loop transformations such as loop interchange [23, 24], loop permutation [55], loop distribution [77], or loop fusion [9, 78] as well as additional array dimension reordering may improve the locality of reference, further improving overall performance. Although finding the best transformations may take exponential time <ref> [68] </ref>, the time may be reduced by prior array grouping since arrays with similar memory access patterns are gathered together. Read-only and consecutively-referenced arrays were not considered since grouping them does not reduce coherence communication.
Reference: [69] <author> E.F. Codd. </author> <title> Further normalization of the data base relational model. </title> <editor> In Randall Rustin, editor, </editor> <booktitle> Courant Computer Science Symposium 6, Data Base Systems, </booktitle> <pages> pages 33-64. </pages> <publisher> Prentice-Hall, </publisher> <month> May </month> <year> 1972. </year>
Reference-contexts: As in Ju's research, our proposed technique also takes both data layout and loop transformation techniques into consideration. Interleaving elements (or groups of elements) from several arrays has been used in relational data base applications <ref> [69, 70] </ref> where records that are typically accessed together are stored contiguously in memory. This idea was also suggested to reduce page faults in paged memory systems [71].
Reference: [70] <author> James Martin. </author> <title> Computer Data-Base Organization. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1977. </year>
Reference-contexts: As in Ju's research, our proposed technique also takes both data layout and loop transformation techniques into consideration. Interleaving elements (or groups of elements) from several arrays has been used in relational data base applications <ref> [69, 70] </ref> where records that are typically accessed together are stored contiguously in memory. This idea was also suggested to reduce page faults in paged memory systems [71].
Reference: [71] <author> Lung-Yu Chang and Henry G. Dietz. </author> <title> Data layout optimization and code transformation for paged memory systems. </title> <type> Technical Report TR-EE 90-43, </type> <institution> Purdue University, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Interleaving elements (or groups of elements) from several arrays has been used in relational data base applications [69, 70] where records that are typically accessed together are stored contiguously in memory. This idea was also suggested to reduce page faults in paged memory systems <ref> [71] </ref>.
Reference: [72] <institution> Kendall Square Research Corporation, </institution> <address> 170 Tracer Lane, Waltham, MA 02154-1379, USA. </address> <note> KSR1 Principles of Operation, October 1992. Rev. 6.0. </note>
Reference-contexts: Similarly, the master processor cannot begin executing the following section until every team member has finished its parallel component. Although the concepts introduced in this chapter are applicable to other cache-based shared-memory multiprocessor systems, we concentrate on Cache-Only Memory Architecture (COMA) systems, such as the KSR1 <ref> [72] </ref>, DDM [73], and COMA-FLAT [74]. In a COMA system, data items do not have static home addresses, they are instead dynamically moved and replicated among the caches in various nodes in response to an application's reference patterns. <p> The KSR1 was chosen as our experimental testbed because it was the only commercially available COMA system. The KSR1 (Figure 5.2) <ref> [72, 75, 76] </ref> is built as a group of ALLCACHE engines, connected by a fat tree hierarchy of unidirectional pipelined slotted rings. Each lowest level ring includes 32 processor cells and two ALLCACHE directories.
Reference: [73] <author> Erik Hagersten, Anders Landin, and Seif Haridi. </author> <title> DDM a cache-only memory architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Similarly, the master processor cannot begin executing the following section until every team member has finished its parallel component. Although the concepts introduced in this chapter are applicable to other cache-based shared-memory multiprocessor systems, we concentrate on Cache-Only Memory Architecture (COMA) systems, such as the KSR1 [72], DDM <ref> [73] </ref>, and COMA-FLAT [74]. In a COMA system, data items do not have static home addresses, they are instead dynamically moved and replicated among the caches in various nodes in response to an application's reference patterns.
Reference: [74] <author> Per Stenstrom, Truman Joe, and Anoop Gupta. </author> <title> Comparative performance evaluation of cache-coherent NUMA and COMA architectures. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Although the concepts introduced in this chapter are applicable to other cache-based shared-memory multiprocessor systems, we concentrate on Cache-Only Memory Architecture (COMA) systems, such as the KSR1 [72], DDM [73], and COMA-FLAT <ref> [74] </ref>. In a COMA system, data items do not have static home addresses, they are instead dynamically moved and replicated among the caches in various nodes in response to an application's reference patterns. The KSR1 was chosen as our experimental testbed because it was the only commercially available COMA system.
Reference: [75] <author> Daniel Windheiser, Eric L. Boyd, Eric Hao, Santosh G. Abraham, and Edward S. Davidson. </author> <title> KSR1 multiprocessor: Analysis of latency hiding techniques in a sparse solver. </title> <booktitle> In the 7th International Parallel Processing Symposium, </booktitle> <pages> pages 454-461, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The KSR1 was chosen as our experimental testbed because it was the only commercially available COMA system. The KSR1 (Figure 5.2) <ref> [72, 75, 76] </ref> is built as a group of ALLCACHE engines, connected by a fat tree hierarchy of unidirectional pipelined slotted rings. Each lowest level ring includes 32 processor cells and two ALLCACHE directories.
Reference: [76] <author> Eric L. Boyd and Edward S. Davidson. </author> <title> Communication in the KSR1 MPP: Performance evaluation using synthetic workload experiments. </title> <booktitle> In 8th ACM International Conference on Supercomputing, </booktitle> <pages> pages 166-175, </pages> <month> July </month> <year> 1994. </year> <month> 157 </month>
Reference-contexts: The KSR1 was chosen as our experimental testbed because it was the only commercially available COMA system. The KSR1 (Figure 5.2) <ref> [72, 75, 76] </ref> is built as a group of ALLCACHE engines, connected by a fat tree hierarchy of unidirectional pipelined slotted rings. Each lowest level ring includes 32 processor cells and two ALLCACHE directories. <p> In the KSR1, communication latency varies between 135 and 600 cycles, while subcache misses served by the requesting node's own local cache take an average of 23.4 cycles (when the corresponding block is already allocated) or 49.2 cycles (otherwise) <ref> [76] </ref>. Although nonconsecutive references affect both communication and cache performance, only communication will be considered when deciding which arrays to reorganize. However, array reorganization should also improve subcache utilization.
Reference: [77] <author> David J. Kuck. </author> <title> The Structure of Computers and Computations, </title> <journal> vol. </journal> <volume> 1. </volume> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Finally, wherever A or B appear in the the inner loop body, the leftmost two index expressions are interchanged. This grouped data structure layout should improve the efficiency of communication. Loop transformations such as loop interchange [23, 24], loop permutation [55], loop distribution <ref> [77] </ref>, or loop fusion [9, 78] as well as additional array dimension reordering may improve the locality of reference, further improving overall performance.
Reference: [78] <author> W. Abu-Sufah, D. Kuck, and D. Lawrie. </author> <title> Automatic program transformations for virtual memory computers. </title> <booktitle> In Proceedings of the National Computer Conference, </booktitle> <volume> volume 48, </volume> <pages> pages 969-974. </pages> <publisher> AFIPS Press, </publisher> <year> 1979. </year>
Reference-contexts: Finally, wherever A or B appear in the the inner loop body, the leftmost two index expressions are interchanged. This grouped data structure layout should improve the efficiency of communication. Loop transformations such as loop interchange [23, 24], loop permutation [55], loop distribution [77], or loop fusion <ref> [9, 78] </ref> as well as additional array dimension reordering may improve the locality of reference, further improving overall performance. Although finding the best transformations may take exponential time [68], the time may be reduced by prior array grouping since arrays with similar memory access patterns are gathered together.
Reference: [79] <author> L.-C. Lu and M. Chen. </author> <title> Parallelizing loops with indirect array references or pointers. </title> <booktitle> In Proceedings of The Fourth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 201-217, </pages> <address> Santa Clara, California, </address> <month> August </month> <year> 1991. </year> <month> 158 </month>
Reference-contexts: However, if arrays are referenced indirectly through the elements of another array, then we may have to insert appropriate reference-collection instrumentation code in the program and collect these references at run time. Similar 128 methods have been used in other studies for indirect reference arrays, e.g. <ref> [79] </ref>. These methods are, however, beyond the scope of this dissertation, and are not discussed further here. 5.6 Experiments and Discussion An actual finite element application code was used to demonstrate the effectiveness of array grouping.
References-found: 80

