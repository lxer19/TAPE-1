URL: http://www.cs.princeton.edu/shrimp/Papers/ipps96SS.ps
Refering-URL: http://www.cs.princeton.edu/shrimp/html/papers_stack_21.html
Root-URL: http://www.cs.princeton.edu
Title: Software Support for Virtual Memory-Mapped Communication  
Author: Cezary Dubnicki, Liviu Iftode, Edward W. Felten, Kai Li 
Affiliation: Department of Computer Science Princeton University  
Date: April, 1996.  
Note: To appear in 10th International Parallel Processing Symposium,  
Abstract: Virtual memory-mapped communication (VMMC) is a communication model providing direct data transfer between the sender's and receiver's virtual address spaces. This model eliminates operating system involvement in communication, provides full protection, supports user-level buffer management and zero-copy protocols, and minimizes software communication overhead. This paper describes system software support for the model including its API, operating system support, and software architecture, for two network interfaces designed in the SHRIMP project. Our implementations and experiments show that the VMMC model can indeed expose the available hardware performance to user programs. On two Pentium PCs with our prototype network interface hardware over a network, we have achieved user-to-user latency of 4.8 sec and sustained bandwidth of 23 MB/s, which is close to the peak hardware bandwidth. Software communication overhead is only a few user-level instructions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Alpert, A. Bilas, M. Blumrich, D.W. Clark, S. Dami-anakis, C. Dubnicki, E. Felten, L. Iftode, and K. Li. </author> <title> Early experience with message-passing on the shrimp multicomputer. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <year> 1996. </year>
Reference-contexts: The implementation of the VMMC model requires close cooperation between software and hardware. This paper describes the software support for VMMC and presents its hardware requirements. We have implemented VMMC for two network interface designs in the SHRIMP project <ref> [1, 2, 4] </ref>. We achieved user-to-user latency of 4.8 sec and sustained bandwidth of 23 MB/s, which is close to the peak hardware bandwidth. The software communication overhead is only a few user-level instructions. <p> Compared to traditional implementations of these models, we avoid receive system calls in both SHRIMP systems, and send system calls in SHRIMP-II. Depending on the model, some additional savings can be realized by using VMMC. For example, in our RPC implementation <ref> [1] </ref>, calls to the stream and network layers are replaced with calls to the VMMC library; as a result, a copy between the user buffer and the stream buffer can be avoided. <p> Compared to the standard TCP-based implementation, null RPC on SHRIMP-II executes only one fourth as many instructions and has four fewer system calls per RPC. The VMMC implementation of NX/2 <ref> [1] </ref> achieves a user-to-user four-byte message latency of 12 seconds, which is about factor of four smaller than that on the Intel Paragon.
Reference: [2] <author> M. A. Blumrich, C. Dubnicki, E.W. Felten, and K. Li. </author> <title> Protected, user-level dma for the shrimp network interface. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: The implementation of the VMMC model requires close cooperation between software and hardware. This paper describes the software support for VMMC and presents its hardware requirements. We have implemented VMMC for two network interface designs in the SHRIMP project <ref> [1, 2, 4] </ref>. We achieved user-to-user latency of 4.8 sec and sustained bandwidth of 23 MB/s, which is close to the peak hardware bandwidth. The software communication overhead is only a few user-level instructions. <p> Even with automatic update, it is still useful for limiting the CPU overhead required to initiate a large transfer <ref> [2] </ref>. * support for receive buffers larger than the size of DRAM: This would require a TLB on the network interface and some support for handling misses when the destination page is not in main memory. 5.2 Operating system requirements We assume that most of the VMMC functionality is provided by <p> Additional operating system support is required when more features are provided by the hardware. For example, if the network interface supports user-level deliberate update, then special virtual memory mappings are needed <ref> [2] </ref>. In this case, the VMMC daemon must be able to manipulate the client's virtual address space. If automatic update is supported by snooping, writes to automatic update areas should appear on the bus snooped by the network interface. <p> Destination space is now a part of the sender's virtual address space, and the destination table is maintained on the network interface. Automatic update writes are snooped off the memory bus. A user process can initiate a deliberate update transfer with just two memory-mapped I/O instructions <ref> [2] </ref>; these instructions access locations on the network interface board across I/O bus. Virtual memory mappings are used to verify permissions and provide protection between users on initiation of deliberate update. Functionally, the VMMC software architectures on the two SHRIMP systems are similar.
Reference: [3] <author> A.L. Cox and R.J. Fowler. </author> <title> The implementation of a coherent memory abstraction on a numa multiprocessor: Experiences with platinum. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Compared to VMMC, its support requires more hardware and closer integration with host processor and memory. Moreover, reading remote memory incurs substantial overhead which needs to be addressed by migrating or replicating of data <ref> [3] </ref>. 9 8 Conclusions This paper describes software support for the virtual memory-mapped communication mechanism. This approach allows data transfer directly from the sender's to the receiver's address space, and eliminates operating system involvement in communication while providing full protection.
Reference: [4] <author> C. Dubnicki, K. Li, and M. Mesarina. </author> <title> Network interface support for user-level buffer management. </title> <booktitle> In Workshop on Parallel Computer Routing and Communication Workshop. </booktitle> <publisher> Springer-Verlag, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: The implementation of the VMMC model requires close cooperation between software and hardware. This paper describes the software support for VMMC and presents its hardware requirements. We have implemented VMMC for two network interface designs in the SHRIMP project <ref> [1, 2, 4] </ref>. We achieved user-to-user latency of 4.8 sec and sustained bandwidth of 23 MB/s, which is close to the peak hardware bandwidth. The software communication overhead is only a few user-level instructions. <p> The second design implements virtual memory mapping completely in hardware. This approach provides fully protected, user-level message passing, and it allows user programs to initiate an outgoing block data transfer with two user-level instructions. Based on these network interfaces we built two implementations of VMMC. SHRIMP-I <ref> [4] </ref>, the first network interface designed by the SHRIMP team, uses the traditional DMA approach with added support for including destination physical address in the message header. On export, receive buffer pages are pinned in physical memory, so virtual memory mapping across nodes is implemented using physical memory mapping.
Reference: [5] <author> L. Iftode, C. Dubnicki, E.W. Felten, and K. Li. </author> <title> Improving release-consistent shared virtual memory using automatic update. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: The VMMC implementation of NX/2 [1] achieves a user-to-user four-byte message latency of 12 seconds, which is about factor of four smaller than that on the Intel Paragon. Shared virtual memory on SHRIMP-II <ref> [5] </ref> uses automatic update to merge fine-grain updates to shared pages and to avoid page faults if pages are shared by only two nodes. <p> As a result, this implementation achieves substantial speedups compared to start-of-the art SVM implementations, even if the same network is 2 We use the LINUX operating system. 7 used in both cases <ref> [5] </ref>. 6 Performance To verify how our two VMMC implementations compare with the traditional send/receive model, we performed a number of experiments. All performance numbers are preliminary, as our hardware is untuned and the SHRIMP system is still in the phase of debugging and integration.
Reference: [6] <author> V. Karamcheti and A. Chien. </author> <title> Software overhead in messaging layers: </title> <booktitle> Where does the time go? In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 51-60, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Their results indicate that modern distributed systems require both high throughput and low latency. As a result, support for both fast short messages and longer high bandwidth data transfers is needed. Karamcheti and Chien studied software overhead on message passing <ref> [6] </ref>. They argue that the networking hardware should provide reliable, in-order message delivery (as VMMC assumes). They show that omitting these features, can more than double software overhead for message passing. The VMMC model provides similar functionality to Active Messages [12].
Reference: [7] <author> P. Pierce. </author> <title> The NX/2 operating system. </title> <booktitle> In Proceedings of 3rd Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: For example, on the Intel Paragon multi-computer, sending and receiving a message requires at least 30 sec, of which less than 1 sec is due to time on the wire <ref> [7] </ref>. Virtual memory-mapped communication (VMMC) is a communication model that allows user-to-user data transfers with latency and bandwidth close to those supported by the underlying hardware. This model requires very little host processor involvement in data transfer. <p> To exploit the performance benefits of VMMC without extensive rewriting of applications, we provide compatibility libraries. By using the VMMC library, which hides the details of memory-mapped communication implementation, compatibility libraries are system-independent. Each compatibility library implements one programming model interface like RPC, stream sockets, NX/2 <ref> [7] </ref> message passing, or shared virtual memory. Compared to traditional implementations of these models, we avoid receive system calls in both SHRIMP systems, and send system calls in SHRIMP-II. Depending on the model, some additional savings can be realized by using VMMC.
Reference: [8] <author> Cray Research. </author> <title> Cray T3D system architecture overview. </title> <type> Technical Report HR-04033, </type> <institution> Cray Research Inc, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: Scalable, coherent shared memory is an attractive model for large scale parallel computing. The main obstacle to widespread use of shared-memory systems is high cost and high hardware complexity. The NUMA model of global address space partitioned across all nodes (implemented for example in the Cray T3D <ref> [8] </ref>) is also easy to use and provides a low latency data transfer mechanism. Compared to VMMC, its support requires more hardware and closer integration with host processor and memory.
Reference: [9] <author> A. Z. Spector. </author> <title> Performing remote operations efficiently on a local computer network. </title> <journal> Communications of the ACM, </journal> <volume> 25(4) </volume> <pages> 260-273, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Total CPU overhead is four times lower in SHRIMP-II deliberate update than in send/receive communication. 7 Related work Spector proposed a remote reference/remote operation model <ref> [9] </ref> in which a master process on a local processor performs remote reference and a slave process on another remote processor implements this reference by executing the remote operation. Compared to this model, VMMC is much simpler, as we envision hardware support only for data transfer.
Reference: [10] <author> Ch. A. Thekkath, H.M. Levy, and E.D. Lazowska. </author> <title> Separating data and control transfer in distributed operating systems. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-11, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: We also use virtual memory mappings across the network to provide protection and to eliminate software overhead on communication, whereas Spector's model did not discuss virtual memory or protection issues. Thekkath, Levy and Lazowska <ref> [10] </ref> proposed a model similar to VMMC. Both models use protected memory-based communication. However, their model is not based on memory mappings across the network; instead their API uses memory regions and offsets.
Reference: [11] <author> Ch.A. Thekkath and H.M. Levy. </author> <title> Limits to low-latency communication on high-speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: It provides some functionality useful for VMMC: user-level protected access to the network and direct transfer to receive-side user buffers. However, Hamlyn does not support automatic update and both send and receive areas must be pinned in physical memory. Thekkath and Levy <ref> [11] </ref> investigated factors limiting performance of communication in high-speed networks. Their results indicate that modern distributed systems require both high throughput and low latency. As a result, support for both fast short messages and longer high bandwidth data transfers is needed.
Reference: [12] <author> T. von Eicken, D.E. Culler, S.C. Goldstein, and K.E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of 19th ISCA, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Karamcheti and Chien studied software overhead on message passing [6]. They argue that the networking hardware should provide reliable, in-order message delivery (as VMMC assumes). They show that omitting these features, can more than double software overhead for message passing. The VMMC model provides similar functionality to Active Messages <ref> [12] </ref>. In this asynchronous communication mechanism each message contains at its head the address of a user-level handler which is executed on message arrival. A VMMC message with notification is very similar to an active message. Unlike active messages, VMMC provides a way to transfer data without transferring control.
Reference: [13] <author> J. Wilkes. </author> <title> Hamlyn an interface for sender-based communications. </title> <type> Technical Report HPL-OSR-92-13, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> November </month> <year> 1993. </year> <month> 10 </month>
Reference-contexts: Instead, their send operation requires trapping into the operating system kernel to verify send rights in system software, which leads to increased latency. Also, their model does not include support for automatic update, which is important for applications like shared virtual memory. Hamlyn <ref> [13] </ref> is a new network interface under design at HP Laboratories. It provides some functionality useful for VMMC: user-level protected access to the network and direct transfer to receive-side user buffers. However, Hamlyn does not support automatic update and both send and receive areas must be pinned in physical memory.
References-found: 13

