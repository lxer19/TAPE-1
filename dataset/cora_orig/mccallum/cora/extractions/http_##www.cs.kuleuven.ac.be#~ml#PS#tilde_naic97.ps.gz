URL: http://www.cs.kuleuven.ac.be/~ml/PS/tilde_naic97.ps.gz
Refering-URL: http://www.cs.kuleuven.ac.be/~hendrik/publications.html
Root-URL: 
Email: e-mail: fHendrik.Blockeel, Luc.DeRaedtg@cs.kuleuven.ac.be  
Title: Top-down Induction of Logical Decision Trees  
Author: Hendrik Blockeel and Luc De Raedt 
Address: Celestijnenlaan 200A, 3001 Heverlee  
Affiliation: Katholieke Universiteit Leuven Department of Computer Science  
Abstract: A first order framework for top-down induction of logical decision trees is introduced. Logical decision trees are more expressive than the flat logic programs typically induced by empirical inductive logic programming systems because logical decision trees introduce invented predicates and mix existential and universal quantification of variables. An implementation of the framework, the Tilde system, is presented and empirically evaluated.
Abstract-found: 1
Intro-found: 1
Reference: [BDR97a] <author> H. Blockeel and L. De Raedt. </author> <title> Experiments with top-down induction of logical decision trees. </title> <type> Technical Report CW 247, </type> <institution> Dept. of Computer Science, K.U.Leuven, </institution> <month> January </month> <year> 1997. </year> <note> Also in Periodic Progress Report ESPRIT Project ILP2, January 1997. http://www.cs.kuleuven.ac.be/- publicaties/rapporten/CW1997.html. </note>
Reference-contexts: For all the experiments, Tilde's default parameters were used; only the choice of the number of thresholds for dis-cretization was supplied manually. Full details on the experimental settings are contained in <ref> [BDR97a] </ref>.
Reference: [BDR97b] <author> H. Blockeel and L. De Raedt. </author> <title> Lookahead and discretization in ILP. </title> <booktitle> In Proceedings of the 7th International Workshop on Inductive Logic Programming, volume 1297 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: Tilde also provides a discretization algorithm that is based on Fayyad and Irani's [FI93] and Dougherty's [DKS95] work, and as such is capable of handling numerical data. For more details see <ref> [VLDDR96, BDR97b] </ref>. 5 Experimental Evaluation Experiments have been performed on several benchmark datasets: Mutagenesis [SMSK96], Musk [DLLP97, MM96], and Diterpenes [DSKH + 96]. For all the experiments, Tilde's default parameters were used; only the choice of the number of thresholds for dis-cretization was supplied manually.
Reference: [BM92] <author> M. Bain and S. Muggleton. </author> <title> Non-monotonic learning. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive logic programming, </booktitle> <pages> pages 145-161. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1992. </year>
Reference-contexts: FFoil [Qui96], Foidl [MC95]. In the learning from interpretations setting, decision lists and logical decision trees have the same representational power (both can trivially be transformed into the other format). From the point of view of predicate invention, our work is related to Bain et al.'s non-monotonic induction method <ref> [BM92] </ref>. In this rule-based method, auxiliary predicates are invented to accomodate exceptions to clauses. 4 Top-down Induction of Logical Decision Trees In this section, we present the Tilde system, which induces logical decision trees from data. <p> This in turn relates our work to some of the work on induction of decision lists and predicate invention <ref> [BM92, Qui96, MC95] </ref>, showing that these algorithms, too, have an expressivity advantage over algorithms inducing flat logic programs. From a practical perspective, we have developed the Tilde system, of which Quin-lan's C4.5 [Qui93a] is a special case (due to the learning from interpretations setting).
Reference: [Bos95] <author> H. Bostrom. </author> <title> Covering vs. divide-and-conquer for top-down induction of logic programs. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: It has been used to solve numerous practical problems. It employs a divide-and-conquer strategy, and in this it differs from its rule-based competitors (e.g. AQ [MMHL86]), which are based on covering strategies (cf. <ref> [Bos95] </ref>). Within attribute-value learning (or propositional concept-learning) TDIDT is more popular than the covering approach. Yet, within first order approaches to concept-learning, only a few learning systems have made use of decision tree techniques. <p> However, all this work has focused on induction techniques and has largely ignored the logical and representational aspects of decision trees, needed to fully understand the potential of this technique for first-order learning. Bostrom <ref> [Bos95] </ref> has compared the covering and divide-and-conquer paradigms in the context of ILP, but he focuses on their computational properties, not on representational aspects. His algorithms still induce flat logic programs, not layered ones.
Reference: [DKS95] <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> Supervised and unsupervised dis-cretization of continuous features. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Proc. Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: By performing successive refinement steps at once, Tilde can look ahead in the refinement lattice and discover such situations. Tilde also provides a discretization algorithm that is based on Fayyad and Irani's [FI93] and Dougherty's <ref> [DKS95] </ref> work, and as such is capable of handling numerical data. For more details see [VLDDR96, BDR97b]. 5 Experimental Evaluation Experiments have been performed on several benchmark datasets: Mutagenesis [SMSK96], Musk [DLLP97, MM96], and Diterpenes [DSKH + 96].
Reference: [DLLP97] <author> T. G. Dietterich, R. H. Lathrop, and T. Lozano-Perez. </author> <title> Solving the multiple-instance problem with axis-parallel rectangles. </title> <journal> Artificial Intelligence, </journal> <volume> 89(1-2):31-71, </volume> <year> 1997. </year>
Reference-contexts: Tilde also provides a discretization algorithm that is based on Fayyad and Irani's [FI93] and Dougherty's [DKS95] work, and as such is capable of handling numerical data. For more details see [VLDDR96, BDR97b]. 5 Experimental Evaluation Experiments have been performed on several benchmark datasets: Mutagenesis [SMSK96], Musk <ref> [DLLP97, MM96] </ref>, and Diterpenes [DSKH + 96]. For all the experiments, Tilde's default parameters were used; only the choice of the number of thresholds for dis-cretization was supplied manually. Full details on the experimental settings are contained in [BDR97a]. <p> The dataset purely consists of numerical data, which makes it a nontypical ILP application. On the other hand, it suffers from the so-called multiple instance problem (an example is described by multiple feature vectors, only one of which is relevant). Dietterich et al.'s approach <ref> [DLLP97] </ref> was to adapt propositional learning algorithms to the multiple instance problem in the specific case of learning axis-parallel rectangles (APR's). For an ILP system such as Tilde, no adaptations are necessary. Still, Tilde's performance is comparable with most other algorithms discussed in [DLLP97] (Table 2). <p> Dietterich et al.'s approach <ref> [DLLP97] </ref> was to adapt propositional learning algorithms to the multiple instance problem in the specific case of learning axis-parallel rectangles (APR's). For an ILP system such as Tilde, no adaptations are necessary. Still, Tilde's performance is comparable with most other algorithms discussed in [DLLP97] (Table 2). The Diterpene dataset describes the 13 C-NMR spectra of a number of molecules; the aim is to classify molecules based on the information in these spectra. This is a multiple class problem (more than two classes). <p> The Diterpene dataset was provided to us by Saso Dzeroski. The Musk dataset, originally used by Dietterich et al. <ref> [DLLP97] </ref>, is available at the UCI repository [MM96]. The Mutagenesis dataset, made public by King and Srinivasan [SMSK96], is available at the ILP data repository [KPS96].
Reference: [DR96] <author> L. De Raedt. </author> <title> Induction in logic. In R.S. </title> <editor> Michalski and Wnek J., editors, </editor> <booktitle> Proceedings of the 3rd International Workshop on Multistrategy Learning, </booktitle> <pages> pages 29-38, </pages> <year> 1996. </year>
Reference-contexts: This makes logical decision trees more expressive than the programs typically induced by current ILP systems. Second, in our framework each example is a Prolog program, which means that we are learning from interpretations (cf. <ref> [DRD94, DR96] </ref>). This learning setting has the classical TDIDT setting (using attribute value representations) as a special case. From the implementation point of view, our Tilde system directly upgrades Quinlan's C4.5 [Qui93a] to first order logic. <p> This setting is known under the label learning from interpretations 1 <ref> [DRD94, DR96] </ref>. Example 1 An engineer has to check a set of machines. A machine consists of several parts that may be in need of replacement. Some of these can be replaced by the engineer, others only by the manufacturer of the machine.
Reference: [DRD94] <author> L. De Raedt and S. Dzeroski. </author> <title> First order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70 </volume> <pages> 375-392, </pages> <year> 1994. </year>
Reference-contexts: This makes logical decision trees more expressive than the programs typically induced by current ILP systems. Second, in our framework each example is a Prolog program, which means that we are learning from interpretations (cf. <ref> [DRD94, DR96] </ref>). This learning setting has the classical TDIDT setting (using attribute value representations) as a special case. From the implementation point of view, our Tilde system directly upgrades Quinlan's C4.5 [Qui93a] to first order logic. <p> This setting is known under the label learning from interpretations 1 <ref> [DRD94, DR96] </ref>. Example 1 An engineer has to check a set of machines. A machine consists of several parts that may be in need of replacement. Some of these can be replaced by the engineer, others only by the manufacturer of the machine.
Reference: [DSKH + 96] <author> S. Dzeroski, S. Schulze-Kremer, K. R. Heidtke, K. Siems, and D. Wettschereck. </author> <title> Applying ILP to diterpene structure elucidation from 13C NMR spectra. </title> <booktitle> In Proceedings of the 6th International Workshop on Inductive Logic Programming, </booktitle> <pages> pages 14-27, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: For more details see [VLDDR96, BDR97b]. 5 Experimental Evaluation Experiments have been performed on several benchmark datasets: Mutagenesis [SMSK96], Musk [DLLP97, MM96], and Diterpenes <ref> [DSKH + 96] </ref>. For all the experiments, Tilde's default parameters were used; only the choice of the number of thresholds for dis-cretization was supplied manually. Full details on the experimental settings are contained in [BDR97a].
Reference: [EW96] <author> W. Emde and D. Wettschereck. </author> <title> Relational instance-based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pages 122-130. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: This is a multiple class problem (more than two classes). Several versions of the data are distinguished: purely propositional data (engineered features), relational data (non-engineered), and both. Best performance up till now was achieved by Ribl <ref> [EW96] </ref>, an instance-based relational learner. Table 3 shows that Tilde achieves slightly lower accuracy than Ribl, but outperforms Foil. Moreover, it returns a symbolic, interpretable (although complex) theory, in contrast to Ribl.
Reference: [FI93] <author> U.M. Fayyad and K.B. Irani. </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1022-1027, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: By performing successive refinement steps at once, Tilde can look ahead in the refinement lattice and discover such situations. Tilde also provides a discretization algorithm that is based on Fayyad and Irani's <ref> [FI93] </ref> and Dougherty's [DKS95] work, and as such is capable of handling numerical data. For more details see [VLDDR96, BDR97b]. 5 Experimental Evaluation Experiments have been performed on several benchmark datasets: Mutagenesis [SMSK96], Musk [DLLP97, MM96], and Diterpenes [DSKH + 96].
Reference: [KPS96] <author> D. Kazakov, L. Popelinsky, and O. Stepankova. </author> <note> ILP datasets page [http://www.gmd.de/ml-archive/datasets/ilp-res.html] , 1996. </note>
Reference-contexts: The Diterpene dataset was provided to us by Saso Dzeroski. The Musk dataset, originally used by Dietterich et al. [DLLP97], is available at the UCI repository [MM96]. The Mutagenesis dataset, made public by King and Srinivasan [SMSK96], is available at the ILP data repository <ref> [KPS96] </ref>.
Reference: [Kra96] <author> S. Kramer. </author> <title> Structural regression trees. </title> <booktitle> In Proceedings of the 13th National Conference on Artificial Intelligence (AAAI-96), </booktitle> <year> 1996. </year>
Reference-contexts: This is not the case, however, with Q; T:test and Q; not (T:test), when T:test shares variables with Q. This complication was ignored in earlier work on relational decision trees <ref> [WR91, Kra96] </ref>. For an illustration of this, consider in Figure 4 the node containing not replaceable (X). The query associated to the right subtree of this node contains the negation of the predicate p 1 associated with that node, and not just the negation of the added literal. <p> Experiments show that logical decision trees have good potential for efficiently finding simple theories that have high predictive accuracy, for a broad range of problems. Of the existing decision tree approaches within relational learning, Struct [WR91] and SRT <ref> [Kra96] </ref> are closest to ours. However, all this work has focused on induction techniques and has largely ignored the logical and representational aspects of decision trees, needed to fully understand the potential of this technique for first-order learning.
Reference: [MC95] <author> R.J. Mooney and M.E. Califf. </author> <title> Induction of first-order decision lists: Results on learning the past tense of english verbs. </title> <journal> Journal of Artificial Intelligence Research, </journal> <pages> pages 1-23, </pages> <year> 1995. </year>
Reference-contexts: The latter is equivalent to the induction of decision lists, e.g. FFoil [Qui96], Foidl <ref> [MC95] </ref>. In the learning from interpretations setting, decision lists and logical decision trees have the same representational power (both can trivially be transformed into the other format). From the point of view of predicate invention, our work is related to Bain et al.'s non-monotonic induction method [BM92]. <p> This in turn relates our work to some of the work on induction of decision lists and predicate invention <ref> [BM92, Qui96, MC95] </ref>, showing that these algorithms, too, have an expressivity advantage over algorithms inducing flat logic programs. From a practical perspective, we have developed the Tilde system, of which Quin-lan's C4.5 [Qui93a] is a special case (due to the learning from interpretations setting).
Reference: [MDR94] <author> S. Muggleton and L. De Raedt. </author> <title> Inductive logic programming : Theory and methods. </title> <journal> Journal of Logic Programming, </journal> <volume> 19,20:629-679, </volume> <year> 1994. </year>
Reference-contexts: The only point where our algorithm differs is in the computation of the tests to be placed in a node. To this aim, it employs a classical refinement operator under -subsumption <ref> [Plo70, MDR94] </ref>. A clause c 1 -subsumes another clause c 2 if and only if there is a variable substitution such that c 1 c 2 .
Reference: [MM96] <author> C.J. Merz and P.M. Murphy. </author> <title> UCI repository of machine learning databases [http://www.ics.uci.edu/~mlearn/mlrepository.html] , 1996. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference-contexts: Tilde also provides a discretization algorithm that is based on Fayyad and Irani's [FI93] and Dougherty's [DKS95] work, and as such is capable of handling numerical data. For more details see [VLDDR96, BDR97b]. 5 Experimental Evaluation Experiments have been performed on several benchmark datasets: Mutagenesis [SMSK96], Musk <ref> [DLLP97, MM96] </ref>, and Diterpenes [DSKH + 96]. For all the experiments, Tilde's default parameters were used; only the choice of the number of thresholds for dis-cretization was supplied manually. Full details on the experimental settings are contained in [BDR97a]. <p> Its theories tend to be a little bit more complex than Progol's, but much simpler than Foil's. With the Musk dataset, the main challenge was its size. We used the largest of the two Musk datasets available at the UCI repository <ref> [MM96] </ref>; this dataset is about 4.5MB large. The dataset purely consists of numerical data, which makes it a nontypical ILP application. On the other hand, it suffers from the so-called multiple instance problem (an example is described by multiple feature vectors, only one of which is relevant). <p> The Diterpene dataset was provided to us by Saso Dzeroski. The Musk dataset, originally used by Dietterich et al. [DLLP97], is available at the UCI repository <ref> [MM96] </ref>. The Mutagenesis dataset, made public by King and Srinivasan [SMSK96], is available at the ILP data repository [KPS96].
Reference: [MMHL86] <author> R. S. Michalski, I. Mozetic, J. Hong, and N. Lavrac. </author> <title> The multi-purpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> In Proceedings of the 5th National Conference on Artificial Intelligence (AAAI-86). </booktitle> <publisher> Morgan-Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: 1 Introduction Top-down induction of decision trees (TDIDT) [Qui86] is the best known and most succesful machine learning technique. It has been used to solve numerous practical problems. It employs a divide-and-conquer strategy, and in this it differs from its rule-based competitors (e.g. AQ <ref> [MMHL86] </ref>), which are based on covering strategies (cf. [Bos95]). Within attribute-value learning (or propositional concept-learning) TDIDT is more popular than the covering approach. Yet, within first order approaches to concept-learning, only a few learning systems have made use of decision tree techniques.
Reference: [Mug95] <author> S. Muggleton. </author> <title> Inverse entailment and progol. </title> <journal> New Generation Computing, </journal> <volume> 13, </volume> <year> 1995. </year>
Reference-contexts: For the tree in class (sendback) :- worn (X), not_replaceable (X), !. class (fix) :- worn (X), !. class (ok). The expressiveness of LDT's is larger than that of the hypotheses generated by classical ILP systems such as Foil [Qui93b] and Progol <ref> [Mug95] </ref>, when learning a hypothesis for classification of examples. The hypotheses generated by these systems are logic programs with only one predicate definition, namely the predicate that is to be learned. Let us call such logic programs flat. <p> The test that is put in the node, consists of Q 0 Q, i.e. the literals that have been added to Q in order to produce Q 0 . The specific refinement operator that is to be used, is defined by the user in a Progol-like manner <ref> [Mug95] </ref>. A set of specifications of the form rmode (n: conjunction) is provided, indicating which conjunctions can be added to a query, the maximal number of times the conjunction can be added (n), and the modes and types of the variables in it.
Reference: [Plo70] <author> G. Plotkin. </author> <title> A note on inductive generalization. </title> <booktitle> In Machine Intelligence, </booktitle> <volume> volume 5, </volume> <pages> pages 153-163. </pages> <publisher> Edinburgh University Press, </publisher> <year> 1970. </year>
Reference-contexts: The only point where our algorithm differs is in the computation of the tests to be placed in a node. To this aim, it employs a classical refinement operator under -subsumption <ref> [Plo70, MDR94] </ref>. A clause c 1 -subsumes another clause c 2 if and only if there is a variable substitution such that c 1 c 2 .
Reference: [Qui86] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction Top-down induction of decision trees (TDIDT) <ref> [Qui86] </ref> is the best known and most succesful machine learning technique. It has been used to solve numerous practical problems. It employs a divide-and-conquer strategy, and in this it differs from its rule-based competitors (e.g. AQ [MMHL86]), which are based on covering strategies (cf. [Bos95]).
Reference: [Qui93a] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. Morgan Kaufmann series in machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: This learning setting has the classical TDIDT setting (using attribute value representations) as a special case. From the implementation point of view, our Tilde system directly upgrades Quinlan's C4.5 <ref> [Qui93a] </ref> to first order logic. Thirdly, we report on a number of experiments on large data sets with Tilde which clearly show that Tilde is competitive both in terms of efficiency and accuracy with state-of-the-art inductive logic programming systems such as Progol and Foil. This text is organized as follows. <p> In this rule-based method, auxiliary predicates are invented to accomodate exceptions to clauses. 4 Top-down Induction of Logical Decision Trees In this section, we present the Tilde system, which induces logical decision trees from data. It employs the basic TDIDT algorithm, and behaves exactly as C4.5 <ref> [Qui93a] </ref> in that it uses the same heuristics, post-pruning algorithm etc. The only point where our algorithm differs is in the computation of the tests to be placed in a node. To this aim, it employs a classical refinement operator under -subsumption [Plo70, MDR94]. <p> From a practical perspective, we have developed the Tilde system, of which Quin-lan's C4.5 <ref> [Qui93a] </ref> is a special case (due to the learning from interpretations setting). Experiments show that logical decision trees have good potential for efficiently finding simple theories that have high predictive accuracy, for a broad range of problems.
Reference: [Qui93b] <author> J.R. Quinlan. </author> <title> FOIL: A midterm report. </title> <editor> In P. Brazdil, editor, </editor> <booktitle> Proceedings of the 6th European Conference on Machine Learning, Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: For the tree in class (sendback) :- worn (X), not_replaceable (X), !. class (fix) :- worn (X), !. class (ok). The expressiveness of LDT's is larger than that of the hypotheses generated by classical ILP systems such as Foil <ref> [Qui93b] </ref> and Progol [Mug95], when learning a hypothesis for classification of examples. The hypotheses generated by these systems are logic programs with only one predicate definition, namely the predicate that is to be learned. Let us call such logic programs flat.
Reference: [Qui96] <author> J. R. Quinlan. </author> <title> Learning first-order definitions of functions. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 5 </volume> <pages> 139-161, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The latter is equivalent to the induction of decision lists, e.g. FFoil <ref> [Qui96] </ref>, Foidl [MC95]. In the learning from interpretations setting, decision lists and logical decision trees have the same representational power (both can trivially be transformed into the other format). From the point of view of predicate invention, our work is related to Bain et al.'s non-monotonic induction method [BM92]. <p> This in turn relates our work to some of the work on induction of decision lists and predicate invention <ref> [BM92, Qui96, MC95] </ref>, showing that these algorithms, too, have an expressivity advantage over algorithms inducing flat logic programs. From a practical perspective, we have developed the Tilde system, of which Quin-lan's C4.5 [Qui93a] is a special case (due to the learning from interpretations setting).
Reference: [SMK95] <author> A. Srinivasan, S.H. Muggleton, and R.D. King. </author> <title> Comparing the use of background knowledge by inductive logic programming systems. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Proceedings of the 5th International Workshop on Inductive Logic Programming, </booktitle> <year> 1995. </year>
Reference-contexts: The molecules have to be classified into mutagenic and non-mutagenic ones. Table 1 compares Tilde's performance on this task with that of Foil and Progol, as reported in <ref> [SMK95] </ref> (four increasing levels of background knowledge B i are distinguished there). From the table it can be concluded that Tilde very efficiently finds theories with high accuracy. Its theories tend to be a little bit more complex than Progol's, but much simpler than Foil's.
Reference: [SMSK96] <author> A. Srinivasan, S.H. Muggleton, M.J.E. Sternberg, and R.D. King. </author> <title> Theories for mutagenicity: A study in first-order and feature-based induction. </title> <journal> Artificial Intelligence, </journal> <volume> 85, </volume> <year> 1996. </year>
Reference-contexts: Tilde also provides a discretization algorithm that is based on Fayyad and Irani's [FI93] and Dougherty's [DKS95] work, and as such is capable of handling numerical data. For more details see [VLDDR96, BDR97b]. 5 Experimental Evaluation Experiments have been performed on several benchmark datasets: Mutagenesis <ref> [SMSK96] </ref>, Musk [DLLP97, MM96], and Diterpenes [DSKH + 96]. For all the experiments, Tilde's default parameters were used; only the choice of the number of thresholds for dis-cretization was supplied manually. Full details on the experimental settings are contained in [BDR97a]. <p> The Diterpene dataset was provided to us by Saso Dzeroski. The Musk dataset, originally used by Dietterich et al. [DLLP97], is available at the UCI repository [MM96]. The Mutagenesis dataset, made public by King and Srinivasan <ref> [SMSK96] </ref>, is available at the ILP data repository [KPS96].
Reference: [VLDDR96] <author> W. Van Laer, S. Dzeroski, and L. De Raedt. </author> <title> Multi-class problems and dis-cretization in ICL (extended abstract). </title> <booktitle> In Proceedings of the MLnet Familiarization Workshop on Data Mining with Inductive Logic Programming (ILP for KDD), </booktitle> <year> 1996. </year>
Reference-contexts: Tilde also provides a discretization algorithm that is based on Fayyad and Irani's [FI93] and Dougherty's [DKS95] work, and as such is capable of handling numerical data. For more details see <ref> [VLDDR96, BDR97b] </ref>. 5 Experimental Evaluation Experiments have been performed on several benchmark datasets: Mutagenesis [SMSK96], Musk [DLLP97, MM96], and Diterpenes [DSKH + 96]. For all the experiments, Tilde's default parameters were used; only the choice of the number of thresholds for dis-cretization was supplied manually.
Reference: [WR91] <author> L. Watanabe and L. Rendell. </author> <title> Learning structural decision trees from examples. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 770-776, </pages> <year> 1991. </year>
Reference-contexts: This is not the case, however, with Q; T:test and Q; not (T:test), when T:test shares variables with Q. This complication was ignored in earlier work on relational decision trees <ref> [WR91, Kra96] </ref>. For an illustration of this, consider in Figure 4 the node containing not replaceable (X). The query associated to the right subtree of this node contains the negation of the predicate p 1 associated with that node, and not just the negation of the added literal. <p> Experiments show that logical decision trees have good potential for efficiently finding simple theories that have high predictive accuracy, for a broad range of problems. Of the existing decision tree approaches within relational learning, Struct <ref> [WR91] </ref> and SRT [Kra96] are closest to ours. However, all this work has focused on induction techniques and has largely ignored the logical and representational aspects of decision trees, needed to fully understand the potential of this technique for first-order learning.
References-found: 27

