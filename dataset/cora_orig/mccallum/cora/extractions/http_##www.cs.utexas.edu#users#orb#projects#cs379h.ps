URL: http://www.cs.utexas.edu/users/orb/projects/cs379h.ps
Refering-URL: http://www.cs.utexas.edu/users/orb/
Root-URL: http://www.cs.utexas.edu
Email: orb,moriarty,risto@cs.utexas.edu  
Title: Evolving Neural Networks to Play Go  
Author: Norman Richards, David Moriarty, and Risto Miikkulainen 
Address: Austin, Tx 78712  
Affiliation: The University of Texas at Austin  
Abstract: Go is a difficult game for computers to master, and the best go programs are still weaker than the average human player. Since the traditional game playing techniques have proven inadequate, new approaches to computer go need to be studied. This paper presents a new approach to learning to play go. The SANE (Symbiotic, Adaptive Neuro-Evolution) method was used to evolve networks capable of playing go on small boards with no pre-programmed go knowledge. On a 9 fi 9 go board, networks that were able to defeat a simple computer opponent were evolved within a few hundred generations. Most significantly, the networks exhibited several aspects of general go playing, which suggests the approach could scale up well. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Herbert D. Enderton. </author> <title> The Golem go program. </title> <type> Technical Report CMU-CS-92-101, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1991. </year> <month> 22 </month>
Reference-contexts: Such features could include common patterns and positions such as an eye or a group or even complicated constructs such as the ladder. These features would then be used as inputs to the neural network <ref> [8; 1] </ref>. It would still probably be useful to let the network develop its own features as well, but the pre-programmed features might allow it to learn faster and deal with more complex patterns. SANE demonstrates the feasibility of evolving structures on more than one level at the same time.
Reference: [2] <author> Markus Enzenberger. </author> <title> The integration of a priori knowledge into a go playing neural network. </title> <type> Manuscript, </type> <year> 1996. </year>
Reference-contexts: Since the networks are not given any prior knowledge about what features are relevant to playing go, they are forced to discover useful features themselves. Allowing the network to access a pre-defined feature space instead of looking at the raw board might make the task easier <ref> [2] </ref>. Such features could include common patterns and positions such as an eye or a group or even complicated constructs such as the ladder. These features would then be used as inputs to the neural network [8; 1].
Reference: [3] <author> Feng hsiung Hsu, Thomas Anantharaman, Murray Campbell, and Andreas Nowatzyk. </author> <title> A grandmaster chess machine. </title> <journal> Scientific American, </journal> <volume> 263 </volume> <pages> 44-50, </pages> <year> 1990. </year>
Reference-contexts: Whereas in games like chess, Othello and checkers the traditional game playing techniques such as minimax search and its variations are competitive even at the master level <ref> [3; 4; 10] </ref>, those techniques, when applied to go, have not been able to produce programs that can challenge even weak amateur players. The best computer programs in the world are ranked 6-8 stones below what would be considered master level.
Reference: [4] <author> Kai-Fu Lee and S. Mahajan. </author> <title> The development of a world-class othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 21-36, </pages> <year> 1990. </year>
Reference-contexts: Whereas in games like chess, Othello and checkers the traditional game playing techniques such as minimax search and its variations are competitive even at the master level <ref> [3; 4; 10] </ref>, those techniques, when applied to go, have not been able to produce programs that can challenge even weak amateur players. The best computer programs in the world are ranked 6-8 stones below what would be considered master level.
Reference: [5] <author> David E. Moriarty. </author> <title> Symbiotic Evolution of Neural Networks in Sequential Decision Tasks. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, The University of Texas at Austin, Austin, TX, </institution> <year> 1997. </year> <note> Technical Report UT-AI97-259. </note>
Reference-contexts: This paper explores the usefulness of neuro-evolution as a mechanism for learning to play go. The SANE (Symbiotic, Adaptive Neuro-Evolution <ref> [5; 6; 7] </ref>) algorithm demonstrates that networks that display a general ability in playing go on small boards can be evolved without any prior knowledge about the game. This result forms a promising foundation for scaling up to full-scale go. <p> In go, this problem is severe enough that standard learning techniques such as backpropagation cannot be effectively applied. 7 Each network is evaluated and the results of the evaluation control the evolution of the neuron population. 5 SANE SANE (Symbiotic Adaptive Neuro-Evolution <ref> [5; 6; 7] </ref> solves the credit assignment problem by using genetic algorithms to evolve networks. Instead of punishing or rewarding individual moves, entire networks are evolved and evaluated based on how well they play the game. Genetic algorithms perform a search through a large solution space. <p> SANE differs from other approaches to neuro-evolution in that SANE evolves partial solutions (neurons) instead of complete solutions (networks; figure 5). Evolving partial solutions is an effective method of insuring diversity in evolved populations, making evolution more effective <ref> [5] </ref>. 5.1 Forming Networks SANE evolves a large population of hidden neurons for a three-layer feedforward network (figure 6). A single neuron represents only part of the solution. However, when combined with other neurons they form a complete network for the given task.
Reference: [6] <author> David E. Moriarty and Risto Miikkulainen. </author> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22 </volume> <pages> 11-32, </pages> <year> 1996. </year>
Reference-contexts: This paper explores the usefulness of neuro-evolution as a mechanism for learning to play go. The SANE (Symbiotic, Adaptive Neuro-Evolution <ref> [5; 6; 7] </ref>) algorithm demonstrates that networks that display a general ability in playing go on small boards can be evolved without any prior knowledge about the game. This result forms a promising foundation for scaling up to full-scale go. <p> In go, this problem is severe enough that standard learning techniques such as backpropagation cannot be effectively applied. 7 Each network is evaluated and the results of the evaluation control the evolution of the neuron population. 5 SANE SANE (Symbiotic Adaptive Neuro-Evolution <ref> [5; 6; 7] </ref> solves the credit assignment problem by using genetic algorithms to evolve networks. Instead of punishing or rewarding individual moves, entire networks are evolved and evaluated based on how well they play the game. Genetic algorithms perform a search through a large solution space.
Reference: [7] <author> David E. Moriarty and Risto Miikkulainen. </author> <title> Evolving obstacle avoidance behavior in a robot arm. </title> <editor> In P. Maes, M. Mataric, J.-A. Meyer, and J. Pollack, editors, </editor> <booktitle> From Animals to Animats: The Fourth International Conference on Simulation of Adaptive Behavior (SAB96), </booktitle> <year> 1996. </year>
Reference-contexts: This paper explores the usefulness of neuro-evolution as a mechanism for learning to play go. The SANE (Symbiotic, Adaptive Neuro-Evolution <ref> [5; 6; 7] </ref>) algorithm demonstrates that networks that display a general ability in playing go on small boards can be evolved without any prior knowledge about the game. This result forms a promising foundation for scaling up to full-scale go. <p> In go, this problem is severe enough that standard learning techniques such as backpropagation cannot be effectively applied. 7 Each network is evaluated and the results of the evaluation control the evolution of the neuron population. 5 SANE SANE (Symbiotic Adaptive Neuro-Evolution <ref> [5; 6; 7] </ref> solves the credit assignment problem by using genetic algorithms to evolve networks. Instead of punishing or rewarding individual moves, entire networks are evolved and evaluated based on how well they play the game. Genetic algorithms perform a search through a large solution space.
Reference: [8] <author> Barney Pell. </author> <title> Exploratory learning in the game of go. </title> <editor> In D.N.L. Levy and D.F. Beal, editors, </editor> <booktitle> Heuristic Programming in Artificial Intelligence 2 TheSecond Computer Olympiad. </booktitle> <publisher> Ellis Horwood, </publisher> <year> 1991. </year>
Reference-contexts: Such features could include common patterns and positions such as an eye or a group or even complicated constructs such as the ladder. These features would then be used as inputs to the neural network <ref> [8; 1] </ref>. It would still probably be useful to let the network develop its own features as well, but the pre-programmed features might allow it to learn faster and deal with more complex patterns. SANE demonstrates the feasibility of evolving structures on more than one level at the same time.
Reference: [9] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In David E. Rumelhart and James L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: A network could be trained to compute a mapping between the input space, that is, the current board position, and the output indicating the next move. The main problem with this approach is the credit assignment problem. Suppose a standard backpropagation neural network <ref> [9] </ref> were being trained to play go. For backpropagation to work, advance knowledge about the best move at any given position would be required. Such knowledge is difficult to come by. In reality, only the final game result is available.
Reference: [10] <author> Jonathan Schaeffer, Robert Lake, Paul Lu, and Martin Bryant. CHINOOK: </author> <title> The world man-machine checkers champion. </title> <journal> The AI Magazine, </journal> <volume> 16(1) </volume> <pages> 21-29, </pages> <year> 1996. </year> <month> 23 </month>
Reference-contexts: Whereas in games like chess, Othello and checkers the traditional game playing techniques such as minimax search and its variations are competitive even at the master level <ref> [3; 4; 10] </ref>, those techniques, when applied to go, have not been able to produce programs that can challenge even weak amateur players. The best computer programs in the world are ranked 6-8 stones below what would be considered master level.
References-found: 10

