URL: ftp://ftp.csd.uu.se/pub/papers/reports/0078/6-pontelli+gupta+hermenegildo.ps.gz
Refering-URL: http://www.informatik.uni-trier.de/~ley/db/conf/iclp/iclp94-w6.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fepontell,guptag@cs.nmsu.edu herme@dia.fi.upm.es  
Title: &ACE: The And-parallel Component of ACE (A Progress Report on ACE)  
Author: Enrico Pontelli Manuel Hermenegildo Gopal Gupta 
Address: 28660-Boadilla del Monte Las Cruces NM USA Madrid, Spain  
Affiliation: Laboratory for Logic and Databases Facultad de Informatica Dept of Computer Science Universidad Politecnica de Madrid New Mexico State University  
Abstract: ACE is a computational model for full Prolog capable of concurrently exploiting both Or and Independent And-parallelism. In this paper we focus on the specific implementation of the And-parallel component of the system, describing its internal organization, some optimizations to the basic model, and finally presenting some performance figures. Keywords: Independent And-parallelism, Or-parallelism, implementation issues. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K.A.M. Ali and R. Karlsson. </author> <title> The muse or-parallel prolog model and its performance. </title> <booktitle> In 1990 N. American Conf. on Logic Prog. </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction The ACE (And-Or/Parallel Copying-based Execution) model [6] uses stack-copying <ref> [1] </ref> and recomputation [5] to efficiently support combined Or- and Independent And-parallel execution of logic programs. ACE represents an efficient combination of Or- and independent And-parallelism in the sense that penalties for supporting either form of parallelism are paid only when that form of parallelism is actually exploited.
Reference: [2] <author> K.A.M. Ali and R. Karlsson. </author> <title> Full Prolog and Scheduling Or-parallelism in Muse. </title> <journal> International Journal of Parallel Programming, 1991. </journal> <volume> 19(6) </volume> <pages> 445-475. </pages>
Reference-contexts: ACE represents an efficient combination of Or- and independent And-parallelism in the sense that penalties for supporting either form of parallelism are paid only when that form of parallelism is actually exploited. Thus, in the presence of only Or-parallelism, execution in ACE is exactly as in the MUSE <ref> [2] </ref> system|a stack-copying based purely Or-parallel system. In the presence of only independent And-parallelism, execution is exactly like the &-Prolog [7] system a recomputation based purely And-parallel system. <p> Section 5 concludes the report, presenting some results obtained by executing a set of well-known benchmarks in and-parallel. 1.1 Or-Parallelism in ACE ACE exploits Or-parallelism by using a stack-copying approach (like MUSE <ref> [2] </ref>). In this approach, a set of or-agents (workers in the case of MUSE, teams in the case of ACE|as explained later) maintain a separate but identical address space (i.e. they allocate their data structures starting at the same logical addresses).
Reference: [3] <author> D. </author> <title> DeGroot. Restricted AND-Parallelism. </title> <booktitle> In Int'l Conf. on 5th Generation Computer Systems, </booktitle> <pages> pages 471-478. </pages> <address> Tokyo, </address> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: Furthermore, in order to reduce the amount of information transferred during the sharing operation, copying is done incrementally, i.e., only the difference between T A and T B is actually copied. 1.2 And-Parallelism in ACE ACE exploits Independent And-parallelism using a recomputation based scheme <ref> [3] </ref>, no sharing of solutions is performed (at the and-parallel level). <p> ACE exploits independent and-parallelism (i.e. only subgoals that do not share any variables are executed in and-parallel), following the model designed by DeGroot <ref> [3] </ref> and successively refined by Hermenegildo in his &-Prolog system [7].
Reference: [4] <author> D. </author> <title> DeGroot. Restricted AND-Parallelism and Side-Effects. </title> <booktitle> In International Symposium on Logic Programming, </booktitle> <pages> pages 80-89. </pages> <address> San Fran-cisco, </address> <publisher> IEEE Computer Society, </publisher> <month> August </month> <year> 1987. </year>
Reference-contexts: Since we are exploiting only Independent And-parallelism, only independent subgoals are allowed to be executed concurrently by different and-agents. Dependencies are detected at the run-time by executing some simple tests introduced by the parallelizing compiler. In ACE we have adopted the technique originally designed by DeGroot <ref> [4] </ref> and refined by Hermenegildo [8] (adopted also by &-Prolog [7]) of annotating the program at compile time with Conditional Graph Expressions (CGEs). This will be explained in details in a later section.
Reference: [5] <author> G. Gupta and M. Hermenegildo. </author> <title> Recomputation based Implementation of And-Or Parallel Prolog. </title> <booktitle> In Int'l Conf. on 5th Generation Computer Sys. </booktitle> <volume> '92, </volume> <pages> pages 770-782, </pages> <year> 1992. </year> <month> 13 </month>
Reference-contexts: 1 Introduction The ACE (And-Or/Parallel Copying-based Execution) model [6] uses stack-copying [1] and recomputation <ref> [5] </ref> to efficiently support combined Or- and Independent And-parallel execution of logic programs. ACE represents an efficient combination of Or- and independent And-parallelism in the sense that penalties for supporting either form of parallelism are paid only when that form of parallelism is actually exploited.
Reference: [6] <author> G. Gupta, M. Hermenegildo, E. Pontelli, and V. Santos Costa. </author> <title> ACE: And/Or-parallel Copying-based Execution of Logic Programs. </title> <booktitle> in 1994 Int'l Conf. on Logic Progr., </booktitle> <publisher> MIT Press, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The ACE (And-Or/Parallel Copying-based Execution) model <ref> [6] </ref> uses stack-copying [1] and recomputation [5] to efficiently support combined Or- and Independent And-parallel execution of logic programs. <p> Different approaches to incremental copying have been studied and heuristics to choose the most appropriate in each situation have been developed. The interested reader is referred to <ref> [6] </ref> for a detailed discussion of this topic. 2 Independent And-parallelism The main purpose of this report is to illustrate the structure and the features of the And-parallel engine developed for the ACE system.
Reference: [7] <author> M. Hermenegildo and K. Greene. </author> <title> &-Prolog and its Performance: Exploiting Independent And-Parallelism. </title> <booktitle> In 1990 Int'l Conf. on Logic Prog., </booktitle> <pages> pages 253-268. </pages> <publisher> MIT Press, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: Thus, in the presence of only Or-parallelism, execution in ACE is exactly as in the MUSE [2] system|a stack-copying based purely Or-parallel system. In the presence of only independent And-parallelism, execution is exactly like the &-Prolog <ref> [7] </ref> system a recomputation based purely And-parallel system. This efficiency in execution is accomplished by introducing the concept of teams of processors and extending the stack-copying techniques of MUSE to deal with this new organization of processors. <p> Dependencies are detected at the run-time by executing some simple tests introduced by the parallelizing compiler. In ACE we have adopted the technique originally designed by DeGroot [4] and refined by Hermenegildo [8] (adopted also by &-Prolog <ref> [7] </ref>) of annotating the program at compile time with Conditional Graph Expressions (CGEs). This will be explained in details in a later section. <p> ACE exploits independent and-parallelism (i.e. only subgoals that do not share any variables are executed in and-parallel), following the model designed by DeGroot [3] and successively refined by Hermenegildo in his &-Prolog system <ref> [7] </ref>. This section explains the basic principles behind this computational model; the first section illustrates the notion and the meaning of Conditional Graph Expressions (CGEs), while the second and third section illustrates the computational behaviour of an engine capable of executing programs annotated with CGEs. <p> The intuitive meaning of a CGE is quite straightforward: if, at runtime, the tests present in conditions succeed, then the subgoals B 1 ; ; B n can be executed in and-parallel, otherwise they should be executed sequentially. The notion of CGE can be further extended in different ways <ref> [7] </ref>: 1. B i can actually represent an arbitrary sequential conjunction of subgoals (and the system should deal with its execution in the appro priate way); 2. we can explicitly add an else part to the CGE, specifying eventually actions different from the plain sequential execution of the subgoals. <p> We discussed the structure of the machine and the organization of the execution, putting emphasis on new ideas and optimization introduced in the design. The results obtained show good performance and good speedups. The results, if compared to those reported for &-Prolog <ref> [7] </ref>, present some slow-downs which are due to the 12 Goals &ACE agents executed 1 3 5 10 matrix mult (30) 5598/5214 1954/1768 (2.95) 1145/1059 (4.92) 573/534 (9.76) quick sort (10) 1882/1536 778/632 (2.43) 548/455 (3.38) 442/373 (4.12) takeuchi (14) 2366/1811 832/586 (3.09) 521/368 (4.92) 252/200 (9.06) hanoi (11) 2183/1671 766/550
Reference: [8] <author> M. V. Hermenegildo. </author> <title> An Abstract Machine Based Execution Model for Computer Architecture Design and Efficient Implementation of Logic Programs in Parallel. </title> <type> PhD thesis, </type> <institution> Dept. of Electrical and Computer Engineering (Dept. of Computer Science TR-86-20), University of Texas at Austin, Austin, Texas 78712, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: Dependencies are detected at the run-time by executing some simple tests introduced by the parallelizing compiler. In ACE we have adopted the technique originally designed by DeGroot [4] and refined by Hermenegildo <ref> [8] </ref> (adopted also by &-Prolog [7]) of annotating the program at compile time with Conditional Graph Expressions (CGEs). This will be explained in details in a later section. <p> The pcall and pop wait instructions are inherited from RAP-WAM <ref> [8] </ref>, while the others have been added specifically to support ACE features. 3.3 Scheduling The and-scheduler is quite simple in its structure.
Reference: [9] <author> K. Muthukumar and M. Hermenegildo. </author> <title> Combined Determination of Sharing and Freeness of Program Variables Through Abstract Interpretation. </title> <booktitle> In 1991 International Conference on Logic Programming. </booktitle> <publisher> MIT Press, </publisher> <month> June </month> <year> 1991. </year>
Reference-contexts: A standard Prolog program need to be annotated with CGEs in order to take advantage of the and-parallel engines available. This process can be done manually by the programmer or by using some specialized compile-time analysis tools (like the &-Prolog parallelizing compiler <ref> [9] </ref>). 2.2 Forward Execution Forward execution of a program annotated with CGEs is quite straightforward. Whenever a CGE is encountered, the conditions are evaluated and, if the evaluation is successful, the various subgoals in the CGE are made available for and-parallel execution.
Reference: [10] <institution> Swedish Institute of Computer Science. Industrial Sicstus Prolog Internals Manual, </institution> <year> 1989. </year>
Reference-contexts: ACE has been implemented on the top of SICStus Prolog ans as such it inherits the basic structure of the SICStus WAM architecture <ref> [10] </ref> together with most of its features and optimizations. The following section is organized as follows. In subsection 3.1 the internal memory layout of &ACE and the data structures allocated during the execution are analyzed.
Reference: [11] <author> T. DongXing, E. Pontelli, G. Gupta, and M. Carro. </author> <title> Last Parallel Call Optimization and Fast Backtracking in And-parallel Logic Programming Systems Technical Report NMSU-CSTR-9403, </title> <institution> Dept. of Computer Science, New Mexico State University, </institution> <month> March </month> <year> 1994. </year> <month> 14 </month>
Reference-contexts: Two approaches <ref> [11] </ref> have been considered: 1. a lazy approach, in which the killing of a branch is delayed until certain checking points are reached (like termination of a branch). <p> The kind of approach adopted for handling kills seems to affect the interaction with the Or-parallel components of ACE only marginally. A complete treatment of the more advanced algorithm for killing can be found in <ref> [11] </ref>. 4.2 Shallow Parallelism Innumerable optimizations can be done to a recomputation-based and-parallel system like &ACE. In this framework we have avoided many of them trying to keep the design of the and-parallel engine sufficiently simple, necessary condition for the later introduction of the or-parallel features.
References-found: 11

