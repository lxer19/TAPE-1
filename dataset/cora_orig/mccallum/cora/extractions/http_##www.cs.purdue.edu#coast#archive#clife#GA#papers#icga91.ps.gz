URL: http://www.cs.purdue.edu/coast/archive/clife/GA/papers/icga91.ps.gz
Refering-URL: http://www.cs.purdue.edu/coast/archive/clife/GA/papers/
Root-URL: http://www.cs.purdue.edu
Title: A Survey of Evolution Strategies  
Author: Thomas Back Frank Hoffmeister Hans-Paul Schwefel 
Note: well as their covariances are described.  
Address: P.O. Box 50 05 00 D-4600 Dortmund 50 Germany  
Affiliation: University of Dortmund Department of Computer Science XI  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [AK89] <author> Emile H. L. Aarts and Jan Korst. </author> <title> Simulated Annealing and Boltzmann Machines. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1989. </year>
Reference-contexts: Then, for the limit value f fl ~ f = lim t!1 f (x t ) under the assumption ~ f &gt; f fl a contradiction emerges, such that ~ f = f fl must be valid. As is well known from similar theorems for Simulated Annealing <ref> [AK89] </ref> and Genetic Algorithms [EAH91], such results are not of much practical relevance due to the unlimited time condition.
Reference: [BB79] <author> Joachim Born and Klaus Bellmann. </author> <title> Nu-merische Parameteroptimierung in mathe-matischen Modellen mittels einer Evolution-sstrategie, </title> <booktitle> volume 18 of Lecture Notes in Control and Information Sciences, </booktitle> <pages> pages 157-167. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1979. </year>
Reference-contexts: Current research concerning ESs deals with applications like the travelling salesman problem [Her91, Rud91], girder-bridge optimization [Loh91], neural networks [Sal91], vector optimization [Kur91], and parameter optimization in general <ref> [BBK84, Bor78, BB79] </ref>. Furthermore the scalable parallelism of such an Evolutionary Algorithm [Hof91] is investigated with the help of implementations on MIMD-computers at the University of Dortmund, especially on a small Transputer network with 30 T800 processors [Bor89, Rud91].
Reference: [BBK84] <author> U. Bernutat-Buchmann and J. Krieger. </author> <title> Evolution strategies in numerical optimization on vector computers. </title> <editor> In Feilmeier, Jou-bert, and Schendel, editors, </editor> <booktitle> Parallel Computing 83, </booktitle> <pages> pages 99-105. </pages> <publisher> Elsevier, </publisher> <address> Amster-dam, </address> <year> 1984. </year>
Reference-contexts: Current research concerning ESs deals with applications like the travelling salesman problem [Her91, Rud91], girder-bridge optimization [Loh91], neural networks [Sal91], vector optimization [Kur91], and parameter optimization in general <ref> [BBK84, Bor78, BB79] </ref>. Furthermore the scalable parallelism of such an Evolutionary Algorithm [Hof91] is investigated with the help of implementations on MIMD-computers at the University of Dortmund, especially on a small Transputer network with 30 T800 processors [Bor89, Rud91].
Reference: [Bor78] <author> Joachim Born. </author> <title> Evolutionsstrategien zur nu-merischen Losung von Adaptationsaufgaben. Dissertation A, Humboldt-Universitat, </title> <address> Berlin, </address> <year> 1978. </year>
Reference-contexts: For the oretical considerations all components of t are identical, i.e. 8 i; j 2 f1; : : :; ng : t i = t (1+1)-ES and a regular optimization problem a con vergence property can be shown (see <ref> [Bor78] </ref>). The regularity of the optimization problem is specified by the criteria given in definition 1. Definition 1 The optimization problem (1) is called regular , iff the following conditions are satisfied: 1. f is continuous. 2. <p> The longish proof is omitted here; it can be found in <ref> [Bor78] </ref>. The basic idea is to use the monotone sequence f (x 0 ) f (x 1 ) : : : f (x i ) : : : of objective function values generated by the process. <p> As a result, such ESs tend to reduce their genetic diversity, i.e. the number of different alleles (specific parameter settings) in a population, as soon as they are attracted by some local optimum. In order to avoid the effect of missing alleles Born <ref> [Bor78] </ref> proposed the concept of a genetic load for some kind of (+1)-ES. <p> It is important to note that in this approach the genetic load is not subject to selection, i.e. no individual of the ge netic load is replaced by offspring. For this case of a (+1)-ES with genetic load Born could also prove the global convergence analogous to theorem 1 <ref> [Bor78] </ref>. 4.1 Recombination Types Intermediate recombination is motivated by the following Gedankenexperiment. When a population moves up-hill along a ridge or down-hill along a narrow valley, the individuals will have positions either on one or the other side of the ridge / ravine. <p> Current research concerning ESs deals with applications like the travelling salesman problem [Her91, Rud91], girder-bridge optimization [Loh91], neural networks [Sal91], vector optimization [Kur91], and parameter optimization in general <ref> [BBK84, Bor78, BB79] </ref>. Furthermore the scalable parallelism of such an Evolutionary Algorithm [Hof91] is investigated with the help of implementations on MIMD-computers at the University of Dortmund, especially on a small Transputer network with 30 T800 processors [Bor89, Rud91].
Reference: [Bor89] <author> Andreas Bormann. </author> <title> Parallelisierungsmog-lichkeiten fur direkte Optimierungsverfahren auf Transputersystemen. </title> <type> Master thesis, </type> <institution> University of Dortmund, Germany, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: Furthermore the scalable parallelism of such an Evolutionary Algorithm [Hof91] is investigated with the help of implementations on MIMD-computers at the University of Dortmund, especially on a small Transputer network with 30 T800 processors <ref> [Bor89, Rud91] </ref>.
Reference: [EAH91] <author> A. E. Eiben, E. H. L. Aarts, and K. M. Van Hee. </author> <title> Global convergence of genetic algorithms: an infinite markov chain analysis. </title> <booktitle> In Schwefel and Manner [SM91], </booktitle> <pages> pages 4-12. </pages>
Reference-contexts: As is well known from similar theorems for Simulated Annealing [AK89] and Genetic Algorithms <ref> [EAH91] </ref>, such results are not of much practical relevance due to the unlimited time condition. In fact, we are interested in the expectation of the convergence rate ', which is given by the quotient of the distance covered towards the optimum and the number of trials needed for this distance.
Reference: [Har74] <author> Dietrich Hartmann. </author> <title> Optimierung balken-artiger Zylinderschalen aus Stahlbeton mit elastischem und plastischem Werkstoffver-halten. </title> <type> PhD thesis, </type> <institution> University of Dortmund, </institution> <month> July </month> <year> 1974. </year>
Reference-contexts: ESs were applied first to experimental optimization problems with more or less continuously changeable parameters only. The first numerical applications were performed by Hart-mann <ref> [Har74] </ref> and Hofler [Hof76], and a first attempt towards extending this strategy in order to solve discrete or even binary parameter optimization problems was made by Schwefel [Sch75a]. fl baeck@lumpi.informatik.uni-dortmund.de y iwan@lumpi.informatik.uni-dortmund.de z uin005@ddohrz11.bitnet The aim of this paper is to give an overview of the development of ESs, beginning with <p> Besides of simulating different versions of the strategy on the first available digital computer at the TUB, a Zuse Z23 [Sch65], computers soon were also used to solve numerical optimization problems by means of the first versions of simple ESs <ref> [Har74, Hof76] </ref>. The algorithm used in these applications was a simple mutation-selection scheme called two membered ES. It is based upon a "population" consisting of one parent individual (a real-valued vector), and one descendant, created by means of adding normally distributed random numbers.
Reference: [HB90] <author> Frank Hoffmeister and Thomas Back. </author> <title> Genetic algorithms and evolution strategies: Similarities and differences. </title> <type> Technical Report "Grune Reihe" No. 365, </type> <institution> Department of Computer Science, University of Dortmund, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: Besides of the different levels of genotypic/phenotypic information representation and of different selection mechanisms the two-level learning in ESs is the most striking difference between ESs and Genetic Algorithms <ref> [HB90, HB91] </ref>. Current research concerning ESs deals with applications like the travelling salesman problem [Her91, Rud91], girder-bridge optimization [Loh91], neural networks [Sal91], vector optimization [Kur91], and parameter optimization in general [BBK84, Bor78, BB79].
Reference: [HB91] <author> Frank Hoffmeister and Thomas Back. </author> <title> Genetic algorithms and evolution strategies: Similarities and differences. </title> <booktitle> In Schwefel and Manner [SM91], </booktitle> <pages> pages 455-470. </pages>
Reference-contexts: Besides of the different levels of genotypic/phenotypic information representation and of different selection mechanisms the two-level learning in ESs is the most striking difference between ESs and Genetic Algorithms <ref> [HB90, HB91] </ref>. Current research concerning ESs deals with applications like the travelling salesman problem [Her91, Rud91], girder-bridge optimization [Loh91], neural networks [Sal91], vector optimization [Kur91], and parameter optimization in general [BBK84, Bor78, BB79].
Reference: [Her91] <author> Michael Herdy. </author> <title> Application of the `Evolu-tionsstrategie' to discrete optimization prob lems. </title> <booktitle> In Schwefel and Manner [SM91], </booktitle> <pages> pages 188-192. </pages>
Reference-contexts: Besides of the different levels of genotypic/phenotypic information representation and of different selection mechanisms the two-level learning in ESs is the most striking difference between ESs and Genetic Algorithms [HB90, HB91]. Current research concerning ESs deals with applications like the travelling salesman problem <ref> [Her91, Rud91] </ref>, girder-bridge optimization [Loh91], neural networks [Sal91], vector optimization [Kur91], and parameter optimization in general [BBK84, Bor78, BB79].
Reference: [Hof76] <author> A. Hofler. </author> <title> Formoptimierung von Leichtbau-fachwerken durch Einsatz einer Evolutions-strategie. </title> <type> PhD thesis, </type> <institution> Technical University of Berlin, </institution> <month> June </month> <year> 1976. </year> <institution> Dept. Verkehrswesen. </institution>
Reference-contexts: ESs were applied first to experimental optimization problems with more or less continuously changeable parameters only. The first numerical applications were performed by Hart-mann [Har74] and Hofler <ref> [Hof76] </ref>, and a first attempt towards extending this strategy in order to solve discrete or even binary parameter optimization problems was made by Schwefel [Sch75a]. fl baeck@lumpi.informatik.uni-dortmund.de y iwan@lumpi.informatik.uni-dortmund.de z uin005@ddohrz11.bitnet The aim of this paper is to give an overview of the development of ESs, beginning with the first simple <p> Besides of simulating different versions of the strategy on the first available digital computer at the TUB, a Zuse Z23 [Sch65], computers soon were also used to solve numerical optimization problems by means of the first versions of simple ESs <ref> [Har74, Hof76] </ref>. The algorithm used in these applications was a simple mutation-selection scheme called two membered ES. It is based upon a "population" consisting of one parent individual (a real-valued vector), and one descendant, created by means of adding normally distributed random numbers.
Reference: [Hof91] <author> Frank Hoffmeister. </author> <title> Parallel evolutionary algorithms. </title> <editor> In Alexander N. Antamoshkin, editor, </editor> <title> Random Search as a Method for Adaptation and Optimization of Complex Systems, </title> <address> pages 90-94, Divnogorsk, USSR, </address> <month> March </month> <year> 1991. </year> <institution> Krasnojarsk Space Technology University. </institution>
Reference-contexts: Current research concerning ESs deals with applications like the travelling salesman problem [Her91, Rud91], girder-bridge optimization [Loh91], neural networks [Sal91], vector optimization [Kur91], and parameter optimization in general [BBK84, Bor78, BB79]. Furthermore the scalable parallelism of such an Evolutionary Algorithm <ref> [Hof91] </ref> is investigated with the help of implementations on MIMD-computers at the University of Dortmund, especially on a small Transputer network with 30 T800 processors [Bor89, Rud91].
Reference: [Hol75] <author> John H. Holland. </author> <title> Adaptation in natural and artificial systems. </title> <publisher> The University of Michi-gan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: Both approaches rely upon imitating the collective learning paradigm of natural populations, based upon Darwin's observations and the modern synthetic theory of evolution. In the USA Holland introduced Genetic Algorithms in the 60ies, embedded into the general framework of adaptation <ref> [Hol75] </ref>. He also mentioned the applicability to parameter optimization which was first realized in the work of De Jong [Jon75]. This article focuses on the German development called Evolution Strategies (ESs), introduced by Rechenberg at Berlin in the 60ies as well [Rec73], and further developed by Schwefel [Sch75b].
Reference: [Jon75] <author> Kenneth De Jong. </author> <title> An analysis of the be-haviour of a class of genetic adaptive systems. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1975. </year> <note> Diss. Abstr. Int. 36(10), 5140B, University Microfilms No. 76-9381. </note>
Reference-contexts: In the USA Holland introduced Genetic Algorithms in the 60ies, embedded into the general framework of adaptation [Hol75]. He also mentioned the applicability to parameter optimization which was first realized in the work of De Jong <ref> [Jon75] </ref>. This article focuses on the German development called Evolution Strategies (ESs), introduced by Rechenberg at Berlin in the 60ies as well [Rec73], and further developed by Schwefel [Sch75b]. ESs were applied first to experimental optimization problems with more or less continuously changeable parameters only.
Reference: [Kur91] <author> Frank Kursawe. </author> <title> A variant of Evolution Strategies for vector optimization. </title> <booktitle> In Schwe-fel and Manner [SM91], </booktitle> <pages> pages 193-197. </pages>
Reference-contexts: Current research concerning ESs deals with applications like the travelling salesman problem [Her91, Rud91], girder-bridge optimization [Loh91], neural networks [Sal91], vector optimization <ref> [Kur91] </ref>, and parameter optimization in general [BBK84, Bor78, BB79]. Furthermore the scalable parallelism of such an Evolutionary Algorithm [Hof91] is investigated with the help of implementations on MIMD-computers at the University of Dortmund, especially on a small Transputer network with 30 T800 processors [Bor89, Rud91].
Reference: [Loh91] <author> Reinhard Lohmann. </author> <title> Application of evolution strategy in parallel populations. </title> <booktitle> In Schwefel and Manner [SM91], </booktitle> <pages> pages 198-208. </pages>
Reference-contexts: Besides of the different levels of genotypic/phenotypic information representation and of different selection mechanisms the two-level learning in ESs is the most striking difference between ESs and Genetic Algorithms [HB90, HB91]. Current research concerning ESs deals with applications like the travelling salesman problem [Her91, Rud91], girder-bridge optimization <ref> [Loh91] </ref>, neural networks [Sal91], vector optimization [Kur91], and parameter optimization in general [BBK84, Bor78, BB79]. Furthermore the scalable parallelism of such an Evolutionary Algorithm [Hof91] is investigated with the help of implementations on MIMD-computers at the University of Dortmund, especially on a small Transputer network with 30 T800 processors [Bor89, Rud91].
Reference: [Rec73] <editor> Ingo Rechenberg. Evolutionsstrategie: Op-timierung technischer Systeme nach Prinzi-pien der biologischen Evolution. </editor> <publisher> Frommann-Holzboog Verlag, Stuttgart, </publisher> <year> 1973. </year>
Reference-contexts: He also mentioned the applicability to parameter optimization which was first realized in the work of De Jong [Jon75]. This article focuses on the German development called Evolution Strategies (ESs), introduced by Rechenberg at Berlin in the 60ies as well <ref> [Rec73] </ref>, and further developed by Schwefel [Sch75b]. ESs were applied first to experimental optimization problems with more or less continuously changeable parameters only. <p> Simplification, e.g. linearization, may help to make things easier, but it can lead to results which are far away from the true optimum. 2 The Two Membered ES According to Rechenberg <ref> [Rec73] </ref>, the first efforts towards an evolution strategy took place in 1964 at the Technical University of Berlin (TUB). Then, the idea to imitate principles of organic evolution was applied in the field of experimental parameter optimization. <p> It comprises the simplest kind of non-linear, unimodal function. For these model functions the expectations of the rates of convergence are <ref> [Rec73] </ref> 2 1 2 ! n1 ' 2 = p p 2 p p p (9) where erf (x) refers to the well-known error function.
Reference: [Rud91] <author> Gunter Rudolph. </author> <title> Global optimization by means of distributed evolution strategies. </title> <booktitle> In Schwefel and Manner [SM91], </booktitle> <pages> pages 209-213. </pages>
Reference-contexts: Besides of the different levels of genotypic/phenotypic information representation and of different selection mechanisms the two-level learning in ESs is the most striking difference between ESs and Genetic Algorithms [HB90, HB91]. Current research concerning ESs deals with applications like the travelling salesman problem <ref> [Her91, Rud91] </ref>, girder-bridge optimization [Loh91], neural networks [Sal91], vector optimization [Kur91], and parameter optimization in general [BBK84, Bor78, BB79]. <p> Furthermore the scalable parallelism of such an Evolutionary Algorithm [Hof91] is investigated with the help of implementations on MIMD-computers at the University of Dortmund, especially on a small Transputer network with 30 T800 processors <ref> [Bor89, Rud91] </ref>.
Reference: [Sal91] <author> R. Salomon. </author> <title> Improved convergence rate of back-propagation with dynamic adaptation of the learning rate. </title> <booktitle> In Schwefel and Manner [SM91], </booktitle> <pages> pages 269-273. </pages>
Reference-contexts: Current research concerning ESs deals with applications like the travelling salesman problem [Her91, Rud91], girder-bridge optimization [Loh91], neural networks <ref> [Sal91] </ref>, vector optimization [Kur91], and parameter optimization in general [BBK84, Bor78, BB79]. Furthermore the scalable parallelism of such an Evolutionary Algorithm [Hof91] is investigated with the help of implementations on MIMD-computers at the University of Dortmund, especially on a small Transputer network with 30 T800 processors [Bor89, Rud91].
Reference: [Sch65] <editor> Hans-Paul Schwefel. </editor> <title> Kybernetische Evolution als Strategie der experimentel-len Forschung in der Stromungstechnik. </title> <type> Diploma thesis, </type> <institution> Technical University of Berlin, </institution> <month> March </month> <year> 1965. </year>
Reference-contexts: Besides of simulating different versions of the strategy on the first available digital computer at the TUB, a Zuse Z23 <ref> [Sch65] </ref>, computers soon were also used to solve numerical optimization problems by means of the first versions of simple ESs [Har74, Hof76]. The algorithm used in these applications was a simple mutation-selection scheme called two membered ES.
Reference: [Sch75a] <editor> Hans-Paul Schwefel. </editor> <title> Binare Optimierung durch somatische Mutation. </title> <type> Technical report, </type> <institution> Technical University of Berlin and Medical University of Hannover, </institution> <month> May </month> <year> 1975. </year>
Reference-contexts: The first numerical applications were performed by Hart-mann [Har74] and Hofler [Hof76], and a first attempt towards extending this strategy in order to solve discrete or even binary parameter optimization problems was made by Schwefel <ref> [Sch75a] </ref>. fl baeck@lumpi.informatik.uni-dortmund.de y iwan@lumpi.informatik.uni-dortmund.de z uin005@ddohrz11.bitnet The aim of this paper is to give an overview of the development of ESs, beginning with the first simple mutation-selection mechanism with two individuals per generation only and stopping at the (,)-ES as used nowadays on single processor computers.
Reference: [Sch75b] <editor> Hans-Paul Schwefel. </editor> <title> Evolutionsstrategie und numerische Optimierung. </title> <type> Dissertation, </type> <institution> Technische Universitat Berlin, </institution> <month> May </month> <year> 1975. </year>
Reference-contexts: He also mentioned the applicability to parameter optimization which was first realized in the work of De Jong [Jon75]. This article focuses on the German development called Evolution Strategies (ESs), introduced by Rechenberg at Berlin in the 60ies as well [Rec73], and further developed by Schwefel <ref> [Sch75b] </ref>. ESs were applied first to experimental optimization problems with more or less continuously changeable parameters only.
Reference: [Sch77] <editor> Hans-Paul Schwefel. Numerische Opti-mierung von Computer-Modellen mittels der Evolutionsstrategie, </editor> <booktitle> volume 26 of Interdisciplinary systems research. </booktitle> <publisher> Birkhauser, </publisher> <address> Basel, </address> <year> 1977. </year>
Reference-contexts: Self-adaptation of the step sizes has not been possible within the (+1)-ES scheme, since offspring with reduced mutation variances are always preferred. 4 (+)-ES and (,)-ES The motivation to extend the (+1)-ES to a (+)-ES and (,)-ES has been twofold <ref> [Sch77, Sch81] </ref>: first, to make use of (at that time futuristic) parallel computers, and secondly, to enable self-adaptation of strategic parameters like the (even n different) standard deviations of the mutations. <p> In particular, severe analytical problems arise as soon as one has to look for the stationary distribution of a population. More details are omitted here, but may be found in <ref> [Sch77, Sch81] </ref>. Like Rechenberg, Schwefel considered the corridor model and the sphere model (8). <p> Like for the (1+1)-ES the maximum rate of convergence is inversely proportional to n, the number of object variables <ref> [Sch77, Sch81] </ref>. ESs which are operated with an optimum ratio of = for a maximum rate of convergence, are biased towards local search.
Reference: [Sch81] <editor> Hans-Paul Schwefel. </editor> <title> Numerical Optimization of Computer Models. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1981. </year>
Reference-contexts: Schwe-fel <ref> [Sch81] </ref> gives reasons to use the factors c d = 0:82 and c i = 1=0:82 for the adjustment, which should take place every n mutations. It should be noted that with (6) and (12) the operator m consists of a random and a deterministic component, now. As explained in [Sch81] <p> <ref> [Sch81] </ref> gives reasons to use the factors c d = 0:82 and c i = 1=0:82 for the adjustment, which should take place every n mutations. It should be noted that with (6) and (12) the operator m consists of a random and a deterministic component, now. As explained in [Sch81] the 1=5 success rule is a measure to increase the efficiency at the cost of effectiveness or robustness. <p> Self-adaptation of the step sizes has not been possible within the (+1)-ES scheme, since offspring with reduced mutation variances are always preferred. 4 (+)-ES and (,)-ES The motivation to extend the (+1)-ES to a (+)-ES and (,)-ES has been twofold <ref> [Sch77, Sch81] </ref>: first, to make use of (at that time futuristic) parallel computers, and secondly, to enable self-adaptation of strategic parameters like the (even n different) standard deviations of the mutations. <p> In particular, severe analytical problems arise as soon as one has to look for the stationary distribution of a population. More details are omitted here, but may be found in <ref> [Sch77, Sch81] </ref>. Like Rechenberg, Schwefel considered the corridor model and the sphere model (8). <p> Like for the (1+1)-ES the maximum rate of convergence is inversely proportional to n, the number of object variables <ref> [Sch77, Sch81] </ref>. ESs which are operated with an optimum ratio of = for a maximum rate of convergence, are biased towards local search. <p> By intermediate recombination, a random but unsuitable extinction of a mutation step size i is always reverted (increased again) as long as there is no mate with a similar step size adaptation. Actually, Schwe-fel's implementation of (+)-ES and (,)-ES contains five types of recombination <ref> [Sch81] </ref>: r (P t ) = a 0 = (x 0 ; 0 ) 2 I i = &gt; &gt; &gt; &gt; &gt; &lt; x a;i (A) no recombination x a;i or x b;i (B) discrete 1 2 (x a;i + x b;i ) (C) intermediate x a;i or x b <p> Their convergence rate is comparable to other algorithms, but their reliability as well as their chance to find a low-dimensional global optimum was remarkably better than for the other strategies compared <ref> [Sch81] </ref>. Best results were obtained with different recombination types for the object variables (dis crete) and the strategy parameters (intermediate). 4.2 Correlated Mutations In ESs mutation realizes a kind of hill-climbing search procedure (12), when it is considered in combination with selection. <p> Thus, an optimum rate of progress is achieved only by chance when suitable mutations coincide, i.e. when they are correlated. Otherwise, the trajectory of the population through the search space is zigzagging along the gradient. In order to avoid this reduction of the rate of progress, Schwe-fel <ref> [Sch81] </ref> extended the mutation operator to handle correlated mutations which require an additional stra tegy vector . m (a 0t i = (x 00 ; 00 ; 00 ) 2 I ; I = IR n fi IR n fi IR w 00 = 0t + N 0 () (21) where
Reference: [Sch87] <editor> Hans-Paul Schwefel. </editor> <booktitle> Collective phenomena in evolutionary systems. In Preprints of the 31st Annual Meeting of the International Society for General System Research, Bu-dapest, </booktitle> <volume> volume 2, </volume> <pages> pages 1025-1033, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: The limited life span allows to forget inappropriate internal parameter settings. This may result in short phases of recession, but it avoids long stagnation phases due to mis-adapted strategy parameters <ref> [Sch87] </ref>. The (+)-ES and (,)-ES fit into the same formal framework with the only difference being the limited life time of indi viduals in (,)-ES. <p> Schwefel has pointed to the difficulties as well as opportunities of the two-level collective learning in <ref> [Sch87] </ref>. The strategy parameters make up an internal model of the objective function, which is learned on-line during the optimum seeking without any exogenous controlling instance or additional measure of fitness.
Reference: [SM91] <editor> Hans-Paul Schwefel and Reinhard Manner, editors. </editor> <title> Parallel Problem Solving from Nature, </title> <booktitle> volume 496 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1991. </year>
References-found: 26

