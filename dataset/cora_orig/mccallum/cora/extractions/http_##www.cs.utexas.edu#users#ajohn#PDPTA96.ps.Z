URL: http://www.cs.utexas.edu/users/ajohn/PDPTA96.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/code/code-publications/
Root-URL: 
Title: Extraction of Parallelism from Constraint Specifications 1  
Author: Ajita John J. C. Browne 
Keyword: Parallel, Constraint, Language  
Address: Austin, TX 78712, U.S.A. Austin, TX 78712, U.S.A.  
Affiliation: Department of Computer Sciences Department of Computer Sciences University of Texas at Austin University of Texas at Austin  
Abstract: This paper describes the extraction of parallel procedural programs from specifications of computations as constraint systems and an initial set of input variables. It is shown that all types of parallelism - AND-OR, task and data can be extracted from constraint representations. Both computation and logic can be spanned in a single representation. Additionally, constraint programs can be compiled to parallel executables to run on a variety of architectures and the specification of execution environment directives assists in the generation of architecturally optimized code. A prototype compiler has been developed. Results for some examples are presented.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Baldwin. </author> <title> Consul: A Parallel Constraint Language. </title> <booktitle> IEEE Software 1989. </booktitle>
Reference-contexts: The programs compiled from constraint systems are surprisingly efficient for the few relatively simple examples attempted thus far. 2 Related work We shall briefly sketch related pieces of work in this section. Consul <ref> [1] </ref> is a parallel constraint language that uses an interpretive technique (local propagation) to find satisfying values for the system of constraints. This system offers performance only in the range of logic languages. <p> This condition will be relaxed in later versions of the compiler. E.g. for Rule 4: AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A <ref> [1] </ref> == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B [3] == A [2] 3.3 Translation to a Compilable Language: Encapsulated within A + B == C are three computations: A = C B, B = C A, C = A + B; <p> This condition will be relaxed in later versions of the compiler. E.g. for Rule 4: AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A <ref> [1] </ref> == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B [3] == A [2] 3.3 Translation to a Compilable Language: Encapsulated within A + B == C are three computations: A = C B, B = C A, C = A + B; and a conditional, A + B == C. <p> E.g. for Rule 4: AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A <ref> [1] </ref> == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B [3] == A [2] 3.3 Translation to a Compilable Language: Encapsulated within A + B == C are three computations: A = C B, B = C A, C = A + B; and a conditional, A + B == C.
Reference: [2] <author> Utpal Banerjee. </author> <title> Loop Parallelization, A Book Series on Loop Transformations for Restructuring Compilers, </title> <publisher> Kluwer, </publisher> <year> 1994. </year>
Reference-contexts: This condition will be relaxed in later versions of the compiler. E.g. for Rule 4: AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A [1] == A [0] AND A <ref> [2] </ref> == A [1] AND B [2] == A [1] AND B [3] == A [2] 3.3 Translation to a Compilable Language: Encapsulated within A + B == C are three computations: A = C B, B = C A, C = A + B; and a conditional, A + B <p> This condition will be relaxed in later versions of the compiler. E.g. for Rule 4: AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A [1] == A [0] AND A <ref> [2] </ref> == A [1] AND B [2] == A [1] AND B [3] == A [2] 3.3 Translation to a Compilable Language: Encapsulated within A + B == C are three computations: A = C B, B = C A, C = A + B; and a conditional, A + B == C. <p> E.g. for Rule 4: AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A [1] == A [0] AND A <ref> [2] </ref> == A [1] AND B [2] == A [1] AND B [3] == A [2] 3.3 Translation to a Compilable Language: Encapsulated within A + B == C are three computations: A = C B, B = C A, C = A + B; and a conditional, A + B == C. <p> It is clearly necessary to be able to express constraints on partitions of matrices if large scale parallelism is to be derived from constraint systems without use of the cumbersome techniques derived for array dependence analysis of scalar loop codes over arrays <ref> [2] </ref>. There are several promising approaches: object-oriented formulations of data structures are one possibility. A simpler and more algorithmic basis for definition of constraints over partitions of matrices is to utilize a simple version of the hierarchical type theory for matrices by Collins and Browne [7].
Reference: [3] <author> B. N. Freeman-Benson. </author> <title> A Module Compiler for Thinglab II. </title> <booktitle> Proc. 1989 ACM Conf. on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <address> New Orleans, </address> <month> October </month> <year> 1989. </year> <note> ACM. </note>
Reference-contexts: Declarative extensions have been added as part of High-Performance Fortran (HPF) [17], a portable data-parallel language with some optimization directives. HPF does not support task parallelism. Also, existing control flow in its procedural programming style makes analysis for parallelism difficult. Thinglab <ref> [3] </ref> transforms constraints to a compilable language rather than to an interpretive execution environment as in many constraint systems, but is not concerned with extraction of parallel structures. Vijay Saraswat described a family of concurrent constraint logic programming languages, the cc languages [18]. <p> E.g. for Rule 4: AND FOR (i 1 2) f A [i] == A [i-1], B [i+1] == A [i] g captures the constraints A [1] == A [0] AND A [2] == A [1] AND B [2] == A [1] AND B <ref> [3] </ref> == A [2] 3.3 Translation to a Compilable Language: Encapsulated within A + B == C are three computations: A = C B, B = C A, C = A + B; and a conditional, A + B == C.
Reference: [4] <author> D.C. Cann. </author> <title> The optimizing Sisal Compiler: Version 12.0. </title> <type> Technical Report UCRL-MA-110080, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: Direct compilation uses a new memory location for every iterative update to a single-assignment variable. To overcome this problem we propose a construct called new, very similar to the corresponding complementary construct old in the functional language Sisal <ref> [4] </ref>. new &lt;variable&gt; denotes a new instantiation of the variable and thus simulates a destructive update. We plan to enhance the compiler with the capability of detecting instances which require the use of new.
Reference: [5] <author> K.M. Chandy, S. Taylor. </author> <title> An Introduction to Parallel Programming, </title> <editor> Jones and Bartlett, </editor> <year> 1992. </year>
Reference-contexts: Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [18]. The performance reported for the system is only comparable with commercial Prolog and Lisp systems [15]. Parallel logic programming <ref> [5, 9, 13] </ref> is another area of related work. PCN [5] and Strand [9] are two parallel programming representations with a strong component of logic specification. <p> Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [18]. The performance reported for the system is only comparable with commercial Prolog and Lisp systems [15]. Parallel logic programming [5, 9, 13] is another area of related work. PCN <ref> [5] </ref> and Strand [9] are two parallel programming representations with a strong component of logic specification. However, both require the programmer to provide explicit operators for specification of parallelism and the dependence graph structures which could be generated were restricted to trees.
Reference: [6] <author> K.M. Chandy, J. Misra. </author> <title> Parallel Program Design : A Foundation, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: However, both require the programmer to provide explicit operators for specification of parallelism and the dependence graph structures which could be generated were restricted to trees. Equational specifications of computations is a restriction of constraint specifications. Unity <ref> [6] </ref> is the equational programming representation around which Chandy and Misra have built a powerful paradigm for the design of parallel programs. Again, Unity requires addition of explicit specifications for parallelism.
Reference: [7] <author> T.S. Collins, J.C. Browne. </author> <title> MaTrix++; An Object-Oriented Environment for Parallel High-Performance Matrix Computations. </title> <booktitle> Proc. of the 1995 Hawaii Intl. Conf. on Systems and Software. </booktitle>
Reference-contexts: The matrix subtypes currently implemented in our system are lower and upper triangular and dense matrices. We plan to extend the type system to a richer class of matrices including hierarchical matrices <ref> [7] </ref>. Specialized algorithms based on the structure of the matrix can be invoked for the matrix subtypes. Other structured types such as lists, queues, trees etc. with their associated operations could be included to broaden the class of programs which can be compactly represented. <p> There are several promising approaches: object-oriented formulations of data structures are one possibility. A simpler and more algorithmic basis for definition of constraints over partitions of matrices is to utilize a simple version of the hierarchical type theory for matrices by Collins and Browne <ref> [7] </ref>. The hierarchical type model for matrices establishes a compilable semantics for computations over hierarchical matrices.
Reference: [8] <author> J.J. Dongarra, </author> <title> D.C. Sorenson. SCHEDULE: Tools for Developing and Analyzing Parallel Fortran Programs. </title> <institution> Argonne National Laboratory, </institution> <note> MCSD Technical Memorandum No. 86, </note> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: We illustrate control over granularity through structured types by means of an example in the next subsection. 5.3.1 Block Triangular Solver (BTS) The example chosen is the solution of the AX = B linear algebra problem for a known lower triangular matrix A and vector B. The parallel algorithm <ref> [8] </ref> involves dividing the matrix into blocks.
Reference: [9] <author> I. Foster, S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming Prentice Hall, </title> <year> 1990. </year>
Reference-contexts: Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [18]. The performance reported for the system is only comparable with commercial Prolog and Lisp systems [15]. Parallel logic programming <ref> [5, 9, 13] </ref> is another area of related work. PCN [5] and Strand [9] are two parallel programming representations with a strong component of logic specification. <p> The performance reported for the system is only comparable with commercial Prolog and Lisp systems [15]. Parallel logic programming [5, 9, 13] is another area of related work. PCN [5] and Strand <ref> [9] </ref> are two parallel programming representations with a strong component of logic specification. However, both require the programmer to provide explicit operators for specification of parallelism and the dependence graph structures which could be generated were restricted to trees. Equational specifications of computations is a restriction of constraint specifications.
Reference: [10] <author> I. Foster. </author> <title> Designing and Building Parallel Programs. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: Finally, sequential and parallel C programs for shared memory machines such as the CRAY J90, SPARCcenter 2000, and Sequent and the distributed memory PVM [11] system are generated by CODE. An MPI <ref> [10] </ref> backend for CODE is under development. The next two subsections describe the hierarchical type system and the constraint representation, respectively. 3.1 Domain Specification: The Hierarchical Type System: The lowest level of the type system consists of integers, reals, and characters with the usual arithmetic operators on integers and reals.
Reference: [11] <author> Al Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, V. Sunderam. </author> <title> PVM: Parallel Virtual Machine:A Users' Guide and Tutorial for Networked Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The software architecture or execution environment to which CODE is to compile is separately specified (SMP, DSM, NOW, etc). Finally, sequential and parallel C programs for shared memory machines such as the CRAY J90, SPARCcenter 2000, and Sequent and the distributed memory PVM <ref> [11] </ref> system are generated by CODE. An MPI [10] backend for CODE is under development.
Reference: [12] <author> Ajita John, J. C. Browne. </author> <title> Compilation of Constraint Systems to Procedural Parallel Pro--grams. </title> <booktitle> Ninth Workshop on Languages and Compilers for Parallel Computers, </booktitle> <address> California, </address> <month> August, </month> <year> 1996. </year>
Reference-contexts: The compilation algorithm has been described elsewhere <ref> [12] </ref> and will not be repeated here. However, certain compilation problems such as avoiding excessive use of memory are discussed later. <p> For more detail about the compilation algorithm, refer to <ref> [12] </ref>. A program in our system is expressed as a set of constraints between the program variables and an input set consisting of a subset of the program variables. A dependence graph is derived from the constraint specification and the input set of variables. <p> A constraint program enumerates the different relationships that must be established or maintained by the executing code. Our system handles linear arithmetic constraints and linear (and with some restrictions, stated in <ref> [12] </ref>, non-linear) constraints on matrices. The usual arithmetic expressions and calls to library and user-defined C functions (defined externally and linked during execution) are allowed in a constraint. <p> A derived dependence graph can establish the constraints by computing values for some or all of the non-input or output variables. Constraints over indexed sets are compiled to loops. The derivation of dependence graphs is explained in detail in <ref> [12] </ref>. Our translation exploits the different types of parallelism implicit in the constraints. The compiler generates single-assignment variables. A constraint specification represents a family of dependence graphs. Generation of all possible dependence graphs can result in combinatorial explosion. <p> This includes the exploitation of AND-OR parallelism in the specification, extraction of control and data parallelism, and the factors governing granularity of operations. 5.1 AND-OR Parallelism Our translation process collects constraints connected by AND operators at the same node in the dependence graph <ref> [12] </ref>. Some of these constraints may be classified as computations depending on the input set. These computations have the potential for parallelism. Since the compiler generates a single-assignment system, the lone write to a variable occurs before any reads to it. <p> In the dependence graph for DefinedRoots in for r2 and c in the dependence graph for DefinedRoots in Figure 2 can be done in parallel. Both of these are instances of AND-parallelism. Constraints connected by OR operators are collected on diverging paths in the dependence graph <ref> [12] </ref>. Some of these constraints get classified as computations and some as conditionals depending on the input set. The divergent computational paths that arise as a result of constraints connected by OR operators have the potential to be executed in parallel and give rise to OR-parallelism.
Reference: [13] <author> M. V. Hermenegildo. </author> <title> An abstract machine based execution model for computer architecture design and efficient implementation of logic programs in parallel. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, </institution> <year> 1986. </year>
Reference-contexts: Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [18]. The performance reported for the system is only comparable with commercial Prolog and Lisp systems [15]. Parallel logic programming <ref> [5, 9, 13] </ref> is another area of related work. PCN [5] and Strand [9] are two parallel programming representations with a strong component of logic specification.
Reference: [14] <author> S. Lakshmivarahan, S. K. Dhall. </author> <title> Analysis and Design of Parallel Algorithms: Arithmetic and Matrix Problems. </title> <publisher> McGraw-Hill Series in SPP, </publisher> <year> 1990. </year>
Reference-contexts: It is assumed that the vectors x and d are likewise partitioned. It is further assumed that the blocks B and C are symmetric and commute. A version of the parallel algorithm (taken from <ref> [14] </ref>) programmed in our system is shown in Figure 6. It is not necessary to understand all the details in the algorithm to follow the translation to a dependence graph. <p> These results present initial evidence that constraint specifications can be compiled to competitively efficient executables. 7.1 Block Odd-Even Reduction Algorithm The authors in <ref> [14] </ref> have mentioned that the single-solution step is the major bottleneck in the algorithm. But, in our experiments we found the reduction phase resistant to scalability because the computation for BP [j] and CP [j] involves matrix-matrix multiplication: an O (n 3 ) operation.
Reference: [15] <author> M. Mehl, R. Scheidhauer, C. Schulte. </author> <title> An Abstract Machine for Oz. Programming Languages, Implementations, Logics and Programs, </title> <booktitle> Seventh Intl. Symp., </booktitle> <publisher> Springer-Verlag, LNCS 982, </publisher> <address> The Netherlands, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [18]. The performance reported for the system is only comparable with commercial Prolog and Lisp systems <ref> [15] </ref>. Parallel logic programming [5, 9, 13] is another area of related work. PCN [5] and Strand [9] are two parallel programming representations with a strong component of logic specification.
Reference: [16] <author> P. Newton, J. C. Browne. </author> <title> The Code 2.0 Graphical Parallel Programming Environment. </title> <booktitle> Proc. of the 1992 Intl. Conf. on Supercomputing. </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: A dependence graph is derived from the constraint specification and the input set of variables. The dependence graph is mapped to the target language, CODE <ref> [16] </ref>, which expresses parallel structure over sequential units of computation declaratively as a generalized dependence graph. The software architecture or execution environment to which CODE is to compile is separately specified (SMP, DSM, NOW, etc).
Reference: [17] <author> H. Richardson. </author> <title> HPF: History, Overview and Current developments. </title> <institution> Thinking Machines Corporation, </institution> <note> TMC 261, URL: http://www.crpc.rice.edu/HPFF/publications.html </note>
Reference-contexts: Consul [1] is a parallel constraint language that uses an interpretive technique (local propagation) to find satisfying values for the system of constraints. This system offers performance only in the range of logic languages. Declarative extensions have been added as part of High-Performance Fortran (HPF) <ref> [17] </ref>, a portable data-parallel language with some optimization directives. HPF does not support task parallelism. Also, existing control flow in its procedural programming style makes analysis for parallelism difficult.
Reference: [18] <author> Vijay A. Saraswat. </author> <title> Concurrent Constraint Programming Languages. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon, Pittsburgh, 1989. School of Computer Science. </institution>
Reference-contexts: Thinglab [3] transforms constraints to a compilable language rather than to an interpretive execution environment as in many constraint systems, but is not concerned with extraction of parallel structures. Vijay Saraswat described a family of concurrent constraint logic programming languages, the cc languages <ref> [18] </ref>. The logic and constraint portions are explicitly separated with the constraint part acting as an active data store for the logic portion of the program. Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [18]. <p> logic programming languages, the cc languages <ref> [18] </ref>. The logic and constraint portions are explicitly separated with the constraint part acting as an active data store for the logic portion of the program. Oz is a concurrent programming language based on an extension of the basic concurrent constraint model provided in [18]. The performance reported for the system is only comparable with commercial Prolog and Lisp systems [15]. Parallel logic programming [5, 9, 13] is another area of related work. PCN [5] and Strand [9] are two parallel programming representations with a strong component of logic specification.
References-found: 18

