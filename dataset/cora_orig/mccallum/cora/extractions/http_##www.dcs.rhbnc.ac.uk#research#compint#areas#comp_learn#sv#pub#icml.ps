URL: http://www.dcs.rhbnc.ac.uk/research/compint/areas/comp_learn/sv/pub/icml.ps
Refering-URL: http://www.cs.wisc.edu/icml98/schedule.html
Root-URL: 
Email: fcraig,alex,vovkg@dcs.rhbnc.ac.uk  
Title: Ridge Regression Learning Algorithm in Dual Variables  
Author: C. Saunders, A. Gammerman and V. Vovk 
Address: Egham, Surrey, TW20 0EX, UK  
Affiliation: Royal Holloway, University of London  
Abstract: In this paper we study a dual version of the Ridge Regression procedure. It allows us to perform non-linear regression by constructing a linear regression function in a high dimensional feature space. The feature space representation can result in a large increase in the number of parameters used by the algorithm. In order to combat this "curse of dimensionality", the algorithm allows the use of kernel functions, as used in Support Vector methods. We also discuss a powerful family of kernel functions which is constructed using the ANOVA decomposition method from the kernel corresponding to splines with an infinite number of nodes. This paper introduces a regression estimation algorithm which is a combination of these two elements: the dual version of Ridge Regression is applied to the ANOVA enhancement of the infinite-node splines. Experimental results are then presented (based on the Boston Housing data set) which indicate the performance of this algorithm relative to other algorithms.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman. </author> <title> Bagging predictors. </title> <type> Technical Report 421, </type> <institution> Department of Statistics, University of California, Berkley, </institution> <year> 1994. </year> <note> Also at: ftp:==ftp.stat.berkely.edu= pub=tech-reports=421.ps.Z. </note>
Reference-contexts: This is a well known data set for testing non-linear regression methods; see, e.g., Breiman <ref> [1] </ref> and Saunders [6]. The data set consists of 506 cases in which 12 continuous variables and 1 binary variable determine the median house price in a certain area of Boston in thousands of dollars. <p> The variance measure in the table is the average squared difference, between the squared error measured on each sample and the average squared error. There are two additional results which should be noted here. One is from Breiman <ref> [1] </ref> using bagging with average squared error of 11.7, and one from Drucker et al. [2] using Support Vector regression with polynomial kernels with average squared error of 7.2.
Reference: [2] <author> H. Drucker, C. Burges, L. Kaufman, A. Smola, and V. N. Vapnik. </author> <title> Support Vector regression machines. </title> <booktitle> In Advances in Neural Information Processing Systems 9, </booktitle> <volume> volume 9, </volume> <pages> page 155. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Least Squares and Ridge Regression are classical statistical algorithms which have been known for a long time. They have been widely used, and recently some papers such as Drucker et al. <ref> [2] </ref> have used regression in conjunction with a high dimensional feature space. That is the original input vectors are mapped into some feature space, and the algorithms are then used to construct a linear regression function in the feature space, which represents a non-linear regression in the original input space. <p> In particular, kernel functions are a way to combat the curse of dimensionality problems such as those faced in Drucker et al. <ref> [2] </ref>, where a regression function was also constructed in a feature space, but computations were carried out in the high dimensional space, leading to huge number of parameters for non-trivial problems. <p> The continuous variables represent various values pertaining to different locational, economic and structural features of the house. The prices lie between $5000 and $50,000 in units of $1000. Following the method used by Drucker et al. <ref> [2] </ref>, the data set was partitioned into a training set of 401 cases, a validation set of 80 cases and a test set of 25 cases. This partitioning was carried out randomly 100 times, in order to carry out 100 trials on the data. <p> There are two additional results which should be noted here. One is from Breiman [1] using bagging with average squared error of 11.7, and one from Drucker et al. <ref> [2] </ref> using Support Vector regression with polynomial kernels with average squared error of 7.2.
Reference: [3] <author> A. Gammerman, V. Vapnik, and V. Vovk. </author> <title> Learning by transduction. </title> <booktitle> In Uncertainty in Artificial Intelligence, </booktitle> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: We feel that a very interesting direction of developing the results of this paper would be to combine the dual version of Ridge Regression with the ideas of Gam-merman et al. <ref> [3] </ref> to obtain a measure of confidence for predictions output by our algorithms. We expect that in this case simple closed-form formulas can be obtained. Acknowledgments We thank EPSRC for providing financial support through grant GR/L35812 ("Support Vector and Bayesian Learning Algorithms"). The referees' thoughtful comments are gratefully appreciated.
Reference: [4] <author> F. Girosi. </author> <title> An equivalence between sparce approximations and Support Vector Machines. </title> <type> Technical Report A. </type> <institution> I. </institution> <note> Memo No. 1606, </note> <author> C. B. C. L. </author> <title> Paper No. </title> <type> 147, </type> <institution> Massachusetts Institute of Technology Artificial Intelligence Laboratory and Center for Biological and Computational Learning, Department of Brain and Cognitive Sciences, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: As an illustration of the idea, an example of a simple kernel function is presented here. (See Girosi <ref> [4] </ref>.) Suppose there is a mapping function which maps a two-dimensional vector into 6 dimensions: : (x 1 ; x 2 ) 7! ((x 1 ) 2 ; (x 2 ) 2 ; 2x 1 ; 2x 2 ; 2x 1 x 2 ; 1); then dot products in F take <p> Our derivation (modelled on Vapnik's) gives some extra insight: see, e.g., equations (4) and (6). For an excellent survey of connections between Support Vector Machine and the work done in statistics we refer the reader to Wahba [11, 12] and Girosi <ref> [4] </ref>. 7.2 KRIEGING Formula (8) is well known in the theory of Krieging; in this subsection we will explain the connection for readers who are familiar with Krieging.
Reference: [5] <author> G. Kimeldorf and G. Wahba. </author> <title> Some results on Tchebycheffian spline functions. </title> <journal> J. Math. Anal. Appl., </journal> <volume> 33 </volume> <pages> 82-95, </pages> <year> 1971. </year>
Reference-contexts: one-dimensional case is k (x i ; y i ), then the n-dimensional case is K n (x; y) = i=1 One such kernel (to which the ANOVA decomposition is applied here) is the spline kernel with an infinite number of nodes (see Vapnik [8, 10] and Kimeldorf and Wahba <ref> [5] </ref>).
Reference: [6] <author> C. Saunders, A. Gammerman, and V. Vovk. </author> <title> Ridge regression in dual variables. </title> <type> Technical report, </type> <institution> Royal Holloway, University of London, </institution> <year> 1998. </year>
Reference-contexts: This is a well known data set for testing non-linear regression methods; see, e.g., Breiman [1] and Saunders <ref> [6] </ref>. The data set consists of 506 cases in which 12 continuous variables and 1 binary variable determine the median house price in a certain area of Boston in thousands of dollars. The continuous variables represent various values pertaining to different locational, economic and structural features of the house.
Reference: [7] <author> M. O. Stitson, A. Gammerman, V. N. Vapnik, V. Vovk, C. Watkins, and J. Weston. </author> <title> Support Vector regression with ANOVA decomposition kernels. </title> <type> Technical report, </type> <institution> Royal Holloway, University of London, </institution> <year> 1997. </year>
Reference-contexts: = p s=1 For the purposes of this paper, when using kernels produced by ANOVA decomposition, only the order p is considered: K (x; y) = K p (x; y): An alternative method of using ANOVA decomposition would be to consider order p and all lower orders (as in Stitson <ref> [7] </ref>), i.e., K (x; y) = i=1 6 EXPERIMENTAL RESULTS Experiments were conducted on the Boston Housing data set 2 . This is a well known data set for testing non-linear regression methods; see, e.g., Breiman [1] and Saunders [6]. <p> This experiment was then repeated using a support vector machine (SVM), with the same kernels and exactly the same 100 training files (see Stitson <ref> [7] </ref> for full details). As an illustration of the number of parameters which were considered by the Ridge Regression Algorithm (and the SVM), consider the polynomial kernel which was outlined earlier, using a degree of 5. <p> a dual statement of his, and a fortiori our, problem; he does not reach, however, the closed-form expression (8) Table 1: Experimental Results on the Boston Housing Data METHOD KERNEL SQUARED ERROR VARIANCE Ridge Regression Polynomial 10.44 18.34 Ridge Regression Splines 8.51 11.19 Ridge Regression ANOVA Splines 7.69 8.27 SVM <ref> [7] </ref> Polynomial 8.14 15.13 SVM Splines 7.87 12.67 SVM Anova Splines 7.72 9.44 (because he was mainly interested in positive values of *). As we mentioned before, our derivation of formula (8) follows [8].
Reference: [8] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: We now derive a "dual version" for Ridge Regression (RR); since we allow a = 0, this includes Least Squares (LS) as a special case. In this derivation we partially follow Vapnik <ref> [8] </ref>. <p> For more information on the kernel technique, see Vap-nik <ref> [8, 10, 9] </ref> and Wahba [11]. 4 MULTIPLICATIVE KERNELS Before indicating how ANOVA decomposition can be used to form kernels, a brief description is needed of the family of kernels to which the ANOVA decomposition can be applied, this being the family of multiplicative kernels. <p> That is, if the one-dimensional case is k (x i ; y i ), then the n-dimensional case is K n (x; y) = i=1 One such kernel (to which the ANOVA decomposition is applied here) is the spline kernel with an infinite number of nodes (see Vapnik <ref> [8, 10] </ref> and Kimeldorf and Wahba [5]). <p> As we mentioned before, our derivation of formula (8) follows <ref> [8] </ref>. The dual Ridge Regression is also known in traditional statistics, but statisticians usually use some clever matrix manipulations rather than the Lagrange method. Our derivation (modelled on Vapnik's) gives some extra insight: see, e.g., equations (4) and (6).
Reference: [9] <author> V. N. Vapnik. </author> <title> Statistical learning theory. </title> <editor> In A. Gammerman, editor, </editor> <title> Computational Learning and Probabilistic Reasoning. </title> <publisher> Wiley, </publisher> <year> 1996. </year>
Reference-contexts: The question of which functions K correspond to a dot product in some feature space F is answered by Mercer's theorem and addressed by Vapnik <ref> [9] </ref> in his discussion of support vector methods. <p> For more information on the kernel technique, see Vap-nik <ref> [8, 10, 9] </ref> and Wahba [11]. 4 MULTIPLICATIVE KERNELS Before indicating how ANOVA decomposition can be used to form kernels, a brief description is needed of the family of kernels to which the ANOVA decomposition can be applied, this being the family of multiplicative kernels.
Reference: [10] <author> V. N. Vapnik. </author> <title> Statistical Learning Theory. </title> <publisher> Wiley, Forthcoming. </publisher>
Reference-contexts: Kernel functions themselves can take many forms and particular attention is paid to a family of kernel functions which are constructed using ANOVA decomposition (Vapnik <ref> [10] </ref>; see also Wahba [11, 12]). There are two major objectives of this paper: 1. To show how to use kernel functions to overcome the curse of dimensionality in the above men tioned algorithms. 2. <p> For more information on the kernel technique, see Vap-nik <ref> [8, 10, 9] </ref> and Wahba [11]. 4 MULTIPLICATIVE KERNELS Before indicating how ANOVA decomposition can be used to form kernels, a brief description is needed of the family of kernels to which the ANOVA decomposition can be applied, this being the family of multiplicative kernels. <p> That is, if the one-dimensional case is k (x i ; y i ), then the n-dimensional case is K n (x; y) = i=1 One such kernel (to which the ANOVA decomposition is applied here) is the spline kernel with an infinite number of nodes (see Vapnik <ref> [8, 10] </ref> and Kimeldorf and Wahba [5]). <p> The actual decomposition can be adapted to form kernels (as in, e.g., Vapnik <ref> [10] </ref>) which involve different subsets of the attributes of the examples up to a certain size. There are two main reasons for choosing to use ANOVA decomposition. Firstly, the different subsets which are considered may group together like variables, which can lead to greater predictive power. <p> Given a one-dimensional kernel k, the ANOVA kernels are defined as follows: K 1 (x; y) = 1kn K 2 (x; y) = 1k 1 &lt;k 2 n : : : ; From Vapnik <ref> [10] </ref> the following recurrent procedure can be used when calculating the value of K n (x; y). <p> This optimization problem (along with a similar problem corresponding to Huber's loss function) is considered in Vapnik <ref> [10] </ref>, Chapter 11 (Vapnik, however, considers more general regression functions of the form w x + b rather than w x; the difference is minor because we can always add an extra attribute which is always 1 to all examples). <p> Our problem (1)-(2) corresponds to the problem (9)- (11) with k = 2, * = 0 and C = 1=a. Vapnik <ref> [10] </ref> gives a dual statement of his, and a fortiori our, problem; he does not reach, however, the closed-form expression (8) Table 1: Experimental Results on the Boston Housing Data METHOD KERNEL SQUARED ERROR VARIANCE Ridge Regression Polynomial 10.44 18.34 Ridge Regression Splines 8.51 11.19 Ridge Regression ANOVA Splines 7.69 8.27
Reference: [11] <author> G. Wahba. </author> <title> Spline models for observational data, </title> <booktitle> volume 59 of CBMS-NSF Regional Conference Series in Applied Mathematics. </booktitle> <publisher> SIAM, </publisher> <year> 1990. </year>
Reference-contexts: Kernel functions themselves can take many forms and particular attention is paid to a family of kernel functions which are constructed using ANOVA decomposition (Vapnik [10]; see also Wahba <ref> [11, 12] </ref>). There are two major objectives of this paper: 1. To show how to use kernel functions to overcome the curse of dimensionality in the above men tioned algorithms. 2. <p> For more information on the kernel technique, see Vap-nik [8, 10, 9] and Wahba <ref> [11] </ref>. 4 MULTIPLICATIVE KERNELS Before indicating how ANOVA decomposition can be used to form kernels, a brief description is needed of the family of kernels to which the ANOVA decomposition can be applied, this being the family of multiplicative kernels. <p> Our derivation (modelled on Vapnik's) gives some extra insight: see, e.g., equations (4) and (6). For an excellent survey of connections between Support Vector Machine and the work done in statistics we refer the reader to Wahba <ref> [11, 12] </ref> and Girosi [4]. 7.2 KRIEGING Formula (8) is well known in the theory of Krieging; in this subsection we will explain the connection for readers who are familiar with Krieging.
Reference: [12] <author> G. Wahba. </author> <title> Support Vector machines, reproducing kernel Hilbert spaces and the randomized GACV. </title> <type> Technical Report 984, </type> <institution> Department of Statistics, University of Wisconsin, USA, </institution> <year> 1997. </year>
Reference-contexts: Kernel functions themselves can take many forms and particular attention is paid to a family of kernel functions which are constructed using ANOVA decomposition (Vapnik [10]; see also Wahba <ref> [11, 12] </ref>). There are two major objectives of this paper: 1. To show how to use kernel functions to overcome the curse of dimensionality in the above men tioned algorithms. 2. <p> Our derivation (modelled on Vapnik's) gives some extra insight: see, e.g., equations (4) and (6). For an excellent survey of connections between Support Vector Machine and the work done in statistics we refer the reader to Wahba <ref> [11, 12] </ref> and Girosi [4]. 7.2 KRIEGING Formula (8) is well known in the theory of Krieging; in this subsection we will explain the connection for readers who are familiar with Krieging.
References-found: 12

