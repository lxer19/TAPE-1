URL: http://www.cs.kuleuven.ac.be/~wimv/Docs/ilpkdd2.ps.gz
Refering-URL: http://www.cs.kuleuven.ac.be/~wimv/ICL/papers.html
Root-URL: 
Email: Email:fWimV,LucDRg@cs.kuleuven.ac.be, saso.dzeroski@ijs.si  
Phone: (2)  (3)  
Title: Multi-class problems and discretization in ICL Extended abstract  
Author: Wim Van Laer Saso Dzeroski ; and Luc De Raedt 
Date: June 13, 1996  
Address: Celestijnenlaan 200A, B-3001 Heverlee, Belgium  Jamova 39, 1000 Ljubljana, Slovenia  P.O.Box 1385, 711 10 Heraklion, Crete, Greece  
Affiliation: (1) Department of Computer Science, Katholieke Universiteit Leuven  Department of Intelligent Systems, Jozef Stefan Institute  Institute of Computer Science Foundation for Research and Technology-Hellas  
Abstract: Handling multi-class problems and real numbers is important in practical applications of machine learning to KDD problems. While attribute-value learners address these problems as a rule, very few ILP systems do so. The few ILP systems that handle real numbers mostly do so by trying out all real values that are applicable, thus running into efficiency or overfitting problems. This paper discusses some recent extensions of ICL that address these problems. ICL, which stands for Inductive Constraint Logic, is an ILP system that learns first order logic formulae from positive and negative examples. The main charateristic of ICL is its view on examples. These are seen as interpretations which are true or false for the clausal target theory (in CNF). We first argue that ICL can be used for learning a theory in a disjunctive normal form (DNF). With this in mind, a possible solution for handling more than two classes is given (based on some ideas from CN2). Finally, we show how to tackle problems with continuous values by adapting discretization techniques from attribute value learners. 
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> H. Blockeel, W. Van Laer, and L. De Raedt. </author> <title> Inductive constraint logic and the mutagenesis problem. </title> <booktitle> In Proceedings of the 5th Belgian-Dutch Conference on Machine Learning (BENELEARN '95), </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction Recently, a novel ILP system called ICL was developed in Leuven (see <ref> [ 8; 1 ] </ref> ). <p> Currently, we are carrying out experiments on the mutagenesis data. 2 Inductive Constraint Logic: overview An overview of ICL can be found in [ 8 ] and <ref> [ 1 ] </ref> . Here, we shortly describe the framework of ICL and expand on some practical aspects. 2.1 Framework We will use some notions of first order logic and model theory (for an introduction, see [ 12; 11 ] ).
Reference: [ 2 ] <author> J. Catlett. </author> <title> On changing continuous attributes into ordered discrete attributes. </title> <editor> In Yves Kodratoff, editor, </editor> <booktitle> Proceedings of the 5th European Working Session on Learning, volume 482 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 164-178. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year> <month> 7 </month>
Reference-contexts: Thirdly, and most importantly, ICL (as many other ILP systems) has a problem handling real numbers. In attribute value learning, discretization has recently received a lot of attention (cf. <ref> [ 2; 9 ] </ref> ) and proven its use. The advantages of discretization w.r.t. classical attribute value techniques (such as TDIDT) for handling numbers are two-fold: they are more efficient, and sometimes also yield more accurate results. <p> This has also yielded positive results in attribute value learning, cf. <ref> [ 2 ] </ref> . Though we present the procedure as applied in the ICL system, it also generalizes to other ILP sytems (working under the standard semantics).
Reference: [ 3 ] <author> P. Clark and R. Boswell. </author> <title> Rule induction with cn2: Some recent improvements. </title> <editor> In Yves Kodratoff, editor, </editor> <booktitle> Proceedings of the 5th European Working Session on Learning, volume 482 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 151-163. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: This view originates from computational learning theory, where it was originally applied to boolean concept-learning but recently upgraded towards first order logic [ 14; 7 ] . Because of its origin, the ICL system can be considered an upgrade of the attribute value learning system CN2 ( <ref> [ 4; 3 ] </ref> ). However, whereas CN2 learns boolean concepts in DNF form, ICL learns first order clausal theories in CNF form. Nevertheless, many of the other aspects of CN2 are inherited, including the efficiency. <p> Therefore, we looked at CN2 with unordered rules (see <ref> [ 3 ] </ref> ). Here, one learns a theory (in DNF form, a set of rules) for each class in turn. For a given example, all rules are tested. If only rules for one class succeed, this class is predicted. <p> Otherwise, one uses a probabilistic method to resolve the clash. One chooses the most probable class given the rules that applied. To this end, one stores with each rule the distribution of covered examples among the classes. More details can be found in the CN2 paper <ref> [ 3 ] </ref> . In the previous section, we showed that we can learn DNF formulae with ICL: learn a clausal theory for the complementary classes and transform the CNF to a DNF.
Reference: [ 4 ] <author> P. Clark and T. Niblett. </author> <title> The CN2 algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-284, </pages> <year> 1989. </year>
Reference-contexts: This view originates from computational learning theory, where it was originally applied to boolean concept-learning but recently upgraded towards first order logic [ 14; 7 ] . Because of its origin, the ICL system can be considered an upgrade of the attribute value learning system CN2 ( <ref> [ 4; 3 ] </ref> ). However, whereas CN2 learns boolean concepts in DNF form, ICL learns first order clausal theories in CNF form. Nevertheless, many of the other aspects of CN2 are inherited, including the efficiency.
Reference: [ 5 ] <author> L. De Raedt and L. Dehaspe. </author> <title> Clausal discovery. </title> <publisher> Forthcoming, </publisher> <year> 1995. </year>
Reference-contexts: This is automatically translated in a refinement operator (under -subsumption) for the specified language which is used by ICL to traverse the search space. For a complete overview of DLAB, we refer to the CLAUDIEN paper <ref> [ 5 ] </ref> . A small example: -false &lt;-- 0-len:[len-len:[lumo (Lumo), lt (Lumo, 1-1:[-1, -2])], len-len:[atom (A1, Elem1, Type1, Charge1), lt (Charge1, 1-1:[-0.2, -0.1, 0, 0.1]) ] - Min-Max:List means that at least Min and at most Max literals of List are allowed (len is the length of List).
Reference: [ 6 ] <author> L. De Raedt, L. Dehaspe, W Van Laer, H. Blockeel, and M. Bruynooghe. </author> <title> On the duality of cnf and dnf, or how to learn cnf using a dnf learner. </title> <type> Unpublished, </type> <year> 1995. </year>
Reference-contexts: This is mainly inherited from its older twin system CLAUDIEN. But as argued in <ref> [ 6 ] </ref> (cf. [ 13 ] ), we can learn DNF with a CNF learner. Classification systems such as CN2 learn a hypothesis that discriminates between examples that belong to a given class, and those that do not.
Reference: [ 7 ] <author> L. De Raedt and S. Dzeroski. </author> <title> First order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70 </volume> <pages> 375-392, </pages> <year> 1994. </year>
Reference-contexts: This view originates from computational learning theory, where it was originally applied to boolean concept-learning but recently upgraded towards first order logic <ref> [ 14; 7 ] </ref> . Because of its origin, the ICL system can be considered an upgrade of the attribute value learning system CN2 ( [ 4; 3 ] ). However, whereas CN2 learns boolean concepts in DNF form, ICL learns first order clausal theories in CNF form. <p> This view that examples are interpretations and that the aim is to discriminate between two classes of examples, is similar to classical learning from positive and negative examples and originates from <ref> [ 7 ] </ref> . Note that all variables in the clauses are universally quantified. So an interpretation I is a model for a clause iff for all grounding substitutions of c : body (c) I ! head (c) " I 6= ;.
Reference: [ 8 ] <author> L. De Raedt and W. Van Laer. </author> <title> Inductive constraint logic. </title> <booktitle> In Proceedings of the 5th Workshop on Algorithmic Learning Theory, Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction Recently, a novel ILP system called ICL was developed in Leuven (see <ref> [ 8; 1 ] </ref> ). <p> Though we do not report on any experiments in this short note, it is our intention to report on such experiments at the workshop. Currently, we are carrying out experiments on the mutagenesis data. 2 Inductive Constraint Logic: overview An overview of ICL can be found in <ref> [ 8 ] </ref> and [ 1 ] . Here, we shortly describe the framework of ICL and expand on some practical aspects. 2.1 Framework We will use some notions of first order logic and model theory (for an introduction, see [ 12; 11 ] ). <p> Here, we shortly describe the framework of ICL and expand on some practical aspects. 2.1 Framework We will use some notions of first order logic and model theory (for an introduction, see [ 12; 11 ] ). Definitions of the concepts used here can be found in <ref> [ 8 ] </ref> . ICL is a classification system that learns a clausal theory which discriminates as good as possible between two classes of examples (let's say positives and negatives). Examples are seen as interpretations I of the target theory T . <p> L H such that for all p 2 P , M (B [ p) is a true interpretation of H (Completeness); for all n 2 N , M (B [ n) is a false interpretation of H (Consistency). 2.2 Practice An overview of the ICL algorithm can be found in <ref> [ 8 ] </ref> . In short, ICL uses a covering approach and beam search to find solutions in CNF form. In this way, ICL is a kind of dual first order version of the DNF-learner CN2. To specify the hypothesis language, ICL uses the same declaritive bias as CLAU-DIEN, i.e.
Reference: [ 9 ] <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> Supervised and unsupervised dis-cretization of continuous features. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Proc. Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Thirdly, and most importantly, ICL (as many other ILP systems) has a problem handling real numbers. In attribute value learning, discretization has recently received a lot of attention (cf. <ref> [ 2; 9 ] </ref> ) and proven its use. The advantages of discretization w.r.t. classical attribute value techniques (such as TDIDT) for handling numbers are two-fold: they are more efficient, and sometimes also yield more accurate results. <p> &lt;-- 0-len:[len-len:[lumo (Lumo), lt (Lumo, c_lumo)], len-len:[atom (A1, Elem1, Type1, Charge1), lt (Charge1, c_charge) ] '). dlab_query (c_lumo, 1-1, discretize (lumo (Lumo), Lumo)). dlab_query (c_charge, 1-1, discretize (atom (A1, Elem1, Type1, Charge1), Charge1)). 6 The details of the Fayyad and Irani's method can be found in [ 10 ] and <ref> [ 9 ] </ref> .
Reference: [ 10 ] <author> U.M. Fayyad and K.B. Irani. </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1022-1027, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: follows: dlab_template ( 'false &lt;-- 0-len:[len-len:[lumo (Lumo), lt (Lumo, c_lumo)], len-len:[atom (A1, Elem1, Type1, Charge1), lt (Charge1, c_charge) ] '). dlab_query (c_lumo, 1-1, discretize (lumo (Lumo), Lumo)). dlab_query (c_charge, 1-1, discretize (atom (A1, Elem1, Type1, Charge1), Charge1)). 6 The details of the Fayyad and Irani's method can be found in <ref> [ 10 ] </ref> and [ 9 ] .
Reference: [ 11 ] <author> M. Genesereth and N. Nilsson. </author> <booktitle> Logical foundations of artificial intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: Here, we shortly describe the framework of ICL and expand on some practical aspects. 2.1 Framework We will use some notions of first order logic and model theory (for an introduction, see <ref> [ 12; 11 ] </ref> ). Definitions of the concepts used here can be found in [ 8 ] . ICL is a classification system that learns a clausal theory which discriminates as good as possible between two classes of examples (let's say positives and negatives).
Reference: [ 12 ] <author> J.W. Lloyd. </author> <title> Foundations of logic programming. </title> <publisher> Springer-Verlag, </publisher> <address> 2nd edition, </address> <year> 1987. </year>
Reference-contexts: Here, we shortly describe the framework of ICL and expand on some practical aspects. 2.1 Framework We will use some notions of first order logic and model theory (for an introduction, see <ref> [ 12; 11 ] </ref> ). Definitions of the concepts used here can be found in [ 8 ] . ICL is a classification system that learns a clausal theory which discriminates as good as possible between two classes of examples (let's say positives and negatives).
Reference: [ 13 ] <author> R.J. Mooney. </author> <title> Encouraging experimental results on learning cnf. </title> <journal> Machine Learning, </journal> <volume> 19 </volume> <pages> 79-92, </pages> <year> 1995. </year>
Reference-contexts: This is mainly inherited from its older twin system CLAUDIEN. But as argued in [ 6 ] (cf. <ref> [ 13 ] </ref> ), we can learn DNF with a CNF learner. Classification systems such as CN2 learn a hypothesis that discriminates between examples that belong to a given class, and those that do not.
Reference: [ 14 ] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: This view originates from computational learning theory, where it was originally applied to boolean concept-learning but recently upgraded towards first order logic <ref> [ 14; 7 ] </ref> . Because of its origin, the ICL system can be considered an upgrade of the attribute value learning system CN2 ( [ 4; 3 ] ). However, whereas CN2 learns boolean concepts in DNF form, ICL learns first order clausal theories in CNF form.
References-found: 14

