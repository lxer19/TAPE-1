URL: ftp://ftp.cs.wisc.edu/markhill/Papers/jpdc92_plpc.ps
Refering-URL: http://www.cs.wisc.edu/~markhill/
Root-URL: 
Title: Programming for Different Memory Consistency Models  
Author: Kourosh Gharachorloo Sarita V. Adve Anoop Gupta John L. Hennessy and Mark D. Hill 
Address: Stanford, California 94305  Madison, Wisconsin 53706  
Affiliation: Computer System Laboratory Stanford University  Computer Sciences Department University of Wisconsin  
Date: 1992  
Note: To appear in the Journal of Parallel and Distributed Systems,  
Abstract: The memory consistency model, or memory model, supported by a shared-memory multiprocessor directly affects its performance. The most commonly assumed memory model is sequential consistency (SC). While SC provides a simple model for the programmer, it imposes rigid constraints on the ordering of memory accesses and restricts the use of common hardware and compiler optimizations. To remedy the shortcomings of SC, several relaxed memory models have been proposed in the literature. These include processor consistency (PC), weak ordering (WO), release consistency (RCsc/RCpc), total store ordering (TSO), and partial store ordering (PSO). While the relaxed models provide the potential for higher performance, they present a more complex model for programmers when compared to SC. Our previous research has addressed this tradeoff by taking a programmer-centric approach. We have proposed memory models (DRF0, DRF1, PL) that allow the programmer to reason with SC, but require certain information about the memory accesses. This information is used by the system to relax the ordering among memory accesses while still maintaining SC for the programmer. Our previous models formalized the information that allowed optimizations associated with WO and RCsc to be used. This paper extends the above approach by defining a new model, PLpc, that allows optimizations of the TSO, PSO, PC, and RCpc models as well. Thus, PLpc provides a unified programming model that maintains the ease of reasoning with SC while providing for efficiency and portability across a wide range of proposed system designs. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. V. Adve and M. D. Hill, </author> <title> Weak Ordering ANew Definition, </title> <booktitle> Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 2-14. </pages>
Reference-contexts: This approach guarantees SC if certain information about the memory accesses is provided; the information is used to exploit various optimizations without violating SC. The data-race-free-0 (DRF0) <ref> [1] </ref> and data-race-free-1 (DRF1) [2] memory models, and the notion of properly labeled (PL) programs 2 [5] allow the programmer to reason with SC, and at the same time allow the optimizations of WO and RCsc. <p> Similar ordering relations are also used in <ref> [1, 2, 8] </ref>. - 4 - - -- Definition 2: Competing and Non-competing Accesses: Two conflicting accesses of an execution of a program form a competing pair if there is at least one SC execution of the program where there is no ordering chain between the accesses. <p> Such write-read competing pairs for which the order of execution is fixed allow for optimizations that are not possible for other accesses. We call such writes and reads loop accesses and formalize them below. hhhhhhhhhhhhhhhhhh 6. DRF1 and PL defined the notion of data races <ref> [1, 2] </ref> and competing accesses [5] that are similar to competing accesses defined above. The major difference is that Definition 1 allows u = w 1 and v = r n only if all accesses on the order ing chain are to the same location. <p> There are no restrictions on how the category of an access can be conveyed. Several methods are discussed in our earlier work which apply to PLpc as well <ref> [1, 2, 5] </ref>. One such method is for a system to provide easy to use high-level synchronization constructs (e.g., monitors and distributed task queues), and to require programmers to order all conflicting memory accesses through these constructs.
Reference: 2. <author> S. V. Adve and M. D. Hill, </author> <title> A Unified Formalization of Four Shared-Memory Models, </title> <type> Computer Sciences Technical Report #1051, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1991. </year> <note> Submitted for Publication. </note>
Reference-contexts: 1. Introduction A memory consistency model or a memory model for a shared-memory multiprocessor system is a formal specification of how memory read and write accesses of a program will appear to execute to the programmer <ref> [2, 5] </ref>. Sequential consistency (SC) [11] is the most commonly used memory model since it requires the execution of a parallel program to appear as some interleaving of the execution of the parallel processes on a sequential machine. <p> This approach guarantees SC if certain information about the memory accesses is provided; the information is used to exploit various optimizations without violating SC. The data-race-free-0 (DRF0) [1] and data-race-free-1 (DRF1) <ref> [2] </ref> memory models, and the notion of properly labeled (PL) programs 2 [5] allow the programmer to reason with SC, and at the same time allow the optimizations of WO and RCsc. <p> An SC execution refers to an execution of a program on an SC system (Refer to <ref> [2, 3] </ref> for formal definitions of a program, an execution, and a system.) A system is SC if the result of every execu-hhhhhhhhhhhhhhhhhh 4. An atomic read-modify-write is treated as a read access followed by a write access [2, 5]. <p> An atomic read-modify-write is treated as a read access followed by a write access <ref> [2, 5] </ref>. <p> The result of an execution has been interpreted as the values its reads return <ref> [2, 8] </ref>. Finally, two accesses are considered conflicting if they are to the same location and at least one of them is a write [17]. accesses categories. <p> Similar ordering relations are also used in <ref> [1, 2, 8] </ref>. - 4 - - -- Definition 2: Competing and Non-competing Accesses: Two conflicting accesses of an execution of a program form a competing pair if there is at least one SC execution of the program where there is no ordering chain between the accesses. <p> Such write-read competing pairs for which the order of execution is fixed allow for optimizations that are not possible for other accesses. We call such writes and reads loop accesses and formalize them below. hhhhhhhhhhhhhhhhhh 6. DRF1 and PL defined the notion of data races <ref> [1, 2] </ref> and competing accesses [5] that are similar to competing accesses defined above. The major difference is that Definition 1 allows u = w 1 and v = r n only if all accesses on the order ing chain are to the same location. <p> There are no restrictions on how the category of an access can be conveyed. Several methods are discussed in our earlier work which apply to PLpc as well <ref> [1, 2, 5] </ref>. One such method is for a system to provide easy to use high-level synchronization constructs (e.g., monitors and distributed task queues), and to require programmers to order all conflicting memory accesses through these constructs. <p> The read and write to flag are loop accesses, while the hhhhhhhhhhhhhhhhhh 7. Condition (i) is very similar to that for PL [5] and DRF1 <ref> [2] </ref>. 8. DRF1 and PL did not ignore unsuccessful accesses of a synchronization loop construct. An unsuccessful set can compete with an unset and was therefore considered competing. <p> The first level of information provided in PLpc programs distinguishes between competing and non-competing accesses. This allows for virtually the same optimizations as the PL and DRF1 models. PL and DRF1 allow for all optimizations of WO and RCsc <ref> [2, 5] </ref>. <p> This mapping is mechanical, thereby allowing for automatic and efficient portability of PLpc programs to a variety of system architectures. 4.1. Porting PLpc Programs to WO and RCsc DRF1 and PL already indicate how PLpc programs can be mapped to WO and RCsc <ref> [2, 5] </ref>. For WO, competing accesses should be mapped to synchronization accesses. For RCsc, competing reads and writes should be mapped to acquires and releases respectively.
Reference: 3. <author> S. V. Adve, K. Gharachorloo, M. D. Hill, A. Gupta and J. L. Hennessy, </author> <title> A Framework for Defining, Implementing, and Proving Equivalences Among Memory Models, </title> <institution> Computer Sciences Technical Report, University of Wisconsin, Madison, </institution> <note> To be published. </note>
Reference-contexts: Section 4 gives mappings from PLpc to the hardware-centric models, which allow programs written for PLpc to be run efficiently on the hardware-centric models without violating SC. Section 5 concludes the paper. The proofs to support the material in Sections 3 and 4 appear in another paper <ref> [3] </ref>. The paper develops a formal and general framework for defining, implementing, and proving equivalences among several memory models [3]. <p> Section 5 concludes the paper. The proofs to support the material in Sections 3 and 4 appear in another paper <ref> [3] </ref>. The paper develops a formal and general framework for defining, implementing, and proving equivalences among several memory models [3]. It illustrates the use of this framework by deriving a set of sufficient conditions for systems that satisfy the PLpc memory model and proving that the hardware-centric models satisfy these conditions with the mappings of Section 4. 2. The PLpc Memory Model This section presents the PLpc memory model. <p> An SC execution refers to an execution of a program on an SC system (Refer to <ref> [2, 3] </ref> for formal definitions of a program, an execution, and a system.) A system is SC if the result of every execu-hhhhhhhhhhhhhhhhhh 4. An atomic read-modify-write is treated as a read access followed by a write access [2, 5]. <p> An atomic read-modify-write is treated as a read access followed by a write access [2, 5]. We implicitly assume an implementation that does not allow a write to be executed between the read and the write of a read-modify-write to the same location <ref> [3] </ref>. - 3 - - -- tion on it can be obtained by some total order ( to ) of the memory accesses of the execution such that to obeys po [11]. The result of an execution has been interpreted as the values its reads return [2, 8]. <p> Thus, if all the accesses of a synchronization loop construct are replaced with only the last read or read-modify-write that exited the loop, we still get an SC execution with the same result as before <ref> [3] </ref>. Therefore, in analyzing SC executions, we treat a synchronization loop construct as a single access which is the last read or read-modify-write that terminates the loop construct. Synchronization loop constructs often have another special property. Generally, accesses in a competing pair can execute in any order. <p> A more general set of optimizations allowed by the PLpc memory model along with a proof of correctness appears in <ref> [3] </ref>. Below we first state a set of sufficient requirements for SC and then show how these requirements can be weakened for systems that satisfy only the PLpc memory model. <p> Specifically, we have shown that a loop read does not have to wait for previous writes, and a non-loop read does not have to wait for previous loop writes <ref> [3] </ref>. Equivalently, when a read follows a write, the read need wait for the write only if both are non-loop accesses. Further, we have also shown that loop writes do not have to be atomic [3]. While we do not give the proof for these optimizations here, hhhhhhhhhhhhhhhhhh 9. <p> previous writes, and a non-loop read does not have to wait for previous loop writes <ref> [3] </ref>. Equivalently, when a read follows a write, the read need wait for the write only if both are non-loop accesses. Further, we have also shown that loop writes do not have to be atomic [3]. While we do not give the proof for these optimizations here, hhhhhhhhhhhhhhhhhh 9. <p> Furthermore, PC and RCpc do not provide a direct way in which non-loop writes can be made atomic. However, we have shown that using atomic read-modify-writes for certain non-loop accesses can achieve a similar effect by satisfying a set of more general sufficient conditions for a PLpc system <ref> [3] </ref>. The exact mapping is described below. For TSO and PSO, for every non-loop write followed by a non-loop read, at least one of the accesses should be part of an atomic read-modify-write operation. <p> However, it is possible to build a more aggressive implementation that obeys the PLpc model based on the sufficient system constraints specified in <ref> [3] </ref>. 5. Conclusions Sequential consistency (SC) is a simple model for programmers, but restricts the use of common uniprocessor hardware and compiler optimizations. To achieve higher performance, several alternate memory models have been proposed. <p> Correspondingly, for system designers, the PLpc model allows for systems with various degrees of optimizations without sacrificing programming simplicity or portability. Although PLpc allows optimizations proposed by all current hardware-centric models, it is possible to allow more optimizations if more information is known about the program. In <ref> [3] </ref>, we develop a general framework that gives intuition for the type of information that can be used for different optimizations, and also provide specific examples of such information. Acknowledgements We thank Phillip Gibbons, Michael Merritt, and the anonymous referees for their comments.
Reference: 4. <author> M. Dubois, C. Scheurich and F. A. Briggs, </author> <title> Memory Access Buffering in Multiprocessors, </title> <booktitle> Proc. 13th Annual Intl. Symp. on Computer Architecture 14, </booktitle> <month> 2 (June </month> <year> 1986), </year> <pages> 434-442. </pages>
Reference-contexts: While SC allows for simple reasoning about programs, it restricts many common uniprocessor hardware and compiler optimizations that reorder or overlap the execution of memory accesses <ref> [4, 13] </ref>. hhhhhhhhhhhhhhhhhh * The Stanford University authors are supported by DARPA contract N00039-91-C-0138. Kourosh Gharachorloo is partly supported by Texas Instruments. Anoop Gupta is partly supported by a NSF Presidential Young Investigator Award with matching funds from Sumitomo, Tandem, and TRW. <p> Sarita Adve is also supported by an IBM graduate fellowship. - 1 - - -- To achieve better system performance, researchers have proposed alternate memory models: processor con sistency (PC) [5] 1 , total store ordering (TSO) [15], partial store ordering (PSO) [15], weak ordering (WO) <ref> [4] </ref>, and release consistency (RCsc/RCpc) [5]. The optimizations allowed by these models can provide substantial improvement in system performance, especially as memory latencies grow relative to processor speeds [6, 7]. However, the formal definitions of the models are presented almost completely in terms of the optimizations allowed.
Reference: 5. <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta and J. Hennessy, </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors, </title> <booktitle> Proc. 17th Annual Intl. Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> 15-26. </pages>
Reference-contexts: 1. Introduction A memory consistency model or a memory model for a shared-memory multiprocessor system is a formal specification of how memory read and write accesses of a program will appear to execute to the programmer <ref> [2, 5] </ref>. Sequential consistency (SC) [11] is the most commonly used memory model since it requires the execution of a parallel program to appear as some interleaving of the execution of the parallel processes on a sequential machine. <p> Bell Laboratories, Cray Research Foundation and Digital Equipment Corporation. Sarita Adve is also supported by an IBM graduate fellowship. - 1 - - -- To achieve better system performance, researchers have proposed alternate memory models: processor con sistency (PC) <ref> [5] </ref> 1 , total store ordering (TSO) [15], partial store ordering (PSO) [15], weak ordering (WO) [4], and release consistency (RCsc/RCpc) [5]. The optimizations allowed by these models can provide substantial improvement in system performance, especially as memory latencies grow relative to processor speeds [6, 7]. <p> Adve is also supported by an IBM graduate fellowship. - 1 - - -- To achieve better system performance, researchers have proposed alternate memory models: processor con sistency (PC) <ref> [5] </ref> 1 , total store ordering (TSO) [15], partial store ordering (PSO) [15], weak ordering (WO) [4], and release consistency (RCsc/RCpc) [5]. The optimizations allowed by these models can provide substantial improvement in system performance, especially as memory latencies grow relative to processor speeds [6, 7]. However, the formal definitions of the models are presented almost completely in terms of the optimizations allowed. <p> This approach guarantees SC if certain information about the memory accesses is provided; the information is used to exploit various optimizations without violating SC. The data-race-free-0 (DRF0) [1] and data-race-free-1 (DRF1) [2] memory models, and the notion of properly labeled (PL) programs 2 <ref> [5] </ref> allow the programmer to reason with SC, and at the same time allow the optimizations of WO and RCsc. This is achieved by requiring the programmer to explicitly identify the accesses in the program that could be involved in a race. <p> The processor consistency model considered in this paper is different from that proposed by Goodman [9]. 2. We will also use PL to imply a memory model that guarantees SC to all PL programs. 3. The PL memory model encompasses systems that guarantee SC among competing accesses (defined later) <ref> [5] </ref>. The PLpc memory model extends PL to include systems that at most guarantee PC among such accesses. Thus the name PLpc. - 2 - - -- The rest of the paper is organized as follows. Section 2 defines the PLpc memory model. <p> An atomic read-modify-write is treated as a read access followed by a write access <ref> [2, 5] </ref>. <p> We call such writes and reads loop accesses and formalize them below. hhhhhhhhhhhhhhhhhh 6. DRF1 and PL defined the notion of data races [1, 2] and competing accesses <ref> [5] </ref> that are similar to competing accesses defined above. The major difference is that Definition 1 allows u = w 1 and v = r n only if all accesses on the order ing chain are to the same location. <p> There are no restrictions on how the category of an access can be conveyed. Several methods are discussed in our earlier work which apply to PLpc as well <ref> [1, 2, 5] </ref>. One such method is for a system to provide easy to use high-level synchronization constructs (e.g., monitors and distributed task queues), and to require programmers to order all conflicting memory accesses through these constructs. <p> The write to count is a non-competing access; the fetch and increment on count, and the write and the final read to flag are competing accesses. The read and write to flag are loop accesses, while the hhhhhhhhhhhhhhhhhh 7. Condition (i) is very similar to that for PL <ref> [5] </ref> and DRF1 [2]. 8. DRF1 and PL did not ignore unsuccessful accesses of a synchronization loop construct. An unsuccessful set can compete with an unset and was therefore considered competing. <p> The first level of information provided in PLpc programs distinguishes between competing and non-competing accesses. This allows for virtually the same optimizations as the PL and DRF1 models. PL and DRF1 allow for all optimizations of WO and RCsc <ref> [2, 5] </ref>. <p> The writes in these examples would benefit from using updates (versus invalidates) in a cache-coherent environment to reduce the communication latency for the critical synchronization. However, supporting atomic updates in a large-scale system is both unnatural and inefficient <ref> [5] </ref>. Thus, there is clear benefit from allowing certain competing writes to be non-atomic with the guarantee of SC results. In summary, PLpc programs provide two types of information about shared accesses. The information distinguishing between competing and non-competing accesses allows for virtually the same optimizations as PL and DRF1. <p> This mapping is mechanical, thereby allowing for automatic and efficient portability of PLpc programs to a variety of system architectures. 4.1. Porting PLpc Programs to WO and RCsc DRF1 and PL already indicate how PLpc programs can be mapped to WO and RCsc <ref> [2, 5] </ref>. For WO, competing accesses should be mapped to synchronization accesses. For RCsc, competing reads and writes should be mapped to acquires and releases respectively. <p> To solve this problem, a base system (TSO, PSO, PC, or RCpc) can be extended to provide direct mechanisms for achieving the required order among competing accesses. For TSO and PSO, supporting a fence mechanism <ref> [5] </ref> to delay future reads for previous writes allows for directly delaying non-loop reads for previous non-loop writes.
Reference: 6. <author> K. Gharachorloo, A. Gupta and J. Hennessy, </author> <title> Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors, </title> <booktitle> Proc. 4th Intl. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991, </year> <pages> 245-257. </pages>
Reference-contexts: The optimizations allowed by these models can provide substantial improvement in system performance, especially as memory latencies grow relative to processor speeds <ref> [6, 7] </ref>. However, the formal definitions of the models are presented almost completely in terms of the optimizations allowed.
Reference: 7. <author> K. Gharachorloo, A. Gupta and J. Hennessy, </author> <title> Hiding Memory Latency using Dynamic Scheduling in Shared-Memory Multiprocessors, </title> <booktitle> Proc. 19th Intl. Symp. on Computer Architecture (to appear), </booktitle> <year> 1992. </year> <month> - 13 </month> - - -- 
Reference-contexts: The optimizations allowed by these models can provide substantial improvement in system performance, especially as memory latencies grow relative to processor speeds <ref> [6, 7] </ref>. However, the formal definitions of the models are presented almost completely in terms of the optimizations allowed.
Reference: 8. <author> P. B. Gibbons, M. Merritt and K. Gharachorloo, </author> <title> Proving Sequential Consistency of High-Performance Shared Memories, </title> <booktitle> Proc. Symp. on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991, </year> <pages> 292-303. </pages>
Reference-contexts: The result of an execution has been interpreted as the values its reads return <ref> [2, 8] </ref>. Finally, two accesses are considered conflicting if they are to the same location and at least one of them is a write [17]. accesses categories. <p> Similar ordering relations are also used in <ref> [1, 2, 8] </ref>. - 4 - - -- Definition 2: Competing and Non-competing Accesses: Two conflicting accesses of an execution of a program form a competing pair if there is at least one SC execution of the program where there is no ordering chain between the accesses.
Reference: 9. <author> J. R. Goodman, </author> <title> Cache Consistency and Sequential Consistency, </title> <type> Computer Sciences Technical Report #1006, </type> <institution> Univ. of Wisconsin, Madison, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: For system designers, specifying PLpc as the memory model allows building systems with a wide range of optimizations without sacrificing portability or ease of use. hhhhhhhhhhhhhhhhhh 1. The processor consistency model considered in this paper is different from that proposed by Goodman <ref> [9] </ref>. 2. We will also use PL to imply a memory model that guarantees SC to all PL programs. 3. The PL memory model encompasses systems that guarantee SC among competing accesses (defined later) [5].
Reference: 10. <author> A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph and M. Snir, </author> <title> The NYU Ultracomputer Designing an MIMD Shared Memory Parallel Computer, </title> <journal> IEEE Trans. on Computers, </journal> <month> February </month> <year> 1983, </year> <pages> 175-189. </pages>
Reference: 11. <author> L. Lamport, </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs, </title> <journal> IEEE Trans. on Computers C-28, </journal> <month> 9 (September </month> <year> 1979), </year> <pages> 690-691. </pages>
Reference-contexts: 1. Introduction A memory consistency model or a memory model for a shared-memory multiprocessor system is a formal specification of how memory read and write accesses of a program will appear to execute to the programmer [2, 5]. Sequential consistency (SC) <ref> [11] </ref> is the most commonly used memory model since it requires the execution of a parallel program to appear as some interleaving of the execution of the parallel processes on a sequential machine. <p> allow a write to be executed between the read and the write of a read-modify-write to the same location [3]. - 3 - - -- tion on it can be obtained by some total order ( to ) of the memory accesses of the execution such that to obeys po <ref> [11] </ref>. The result of an execution has been interpreted as the values its reads return [2, 8]. Finally, two accesses are considered conflicting if they are to the same location and at least one of them is a write [17]. accesses categories.
Reference: 12. <author> J. M. Mellor-Crummey and M. L. Scott, </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors, </title> <journal> ACM Transactions on Computer Systems, </journal> <month> February </month> <year> 1991, </year> <pages> 21-65. </pages>
Reference-contexts: In the appendix, we give a more general definition that, for example, allows implementations of locks using test&test&set [14] or back-off <ref> [12] </ref> techniques to be considered as synchronization loop constructs. Definition 3: Synchronization Loop Construct: A synchronization loop construct is a sequence of instructions in a program that satisfies the following. (i) The construct executes a read or a read-modify-write to a specific location.
Reference: 13. <author> S. Midkiff, D. Padua and R. Cytron, </author> <title> Compiling Programs with User Parallelism, </title> <booktitle> Proceedings of the Second Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: While SC allows for simple reasoning about programs, it restricts many common uniprocessor hardware and compiler optimizations that reorder or overlap the execution of memory accesses <ref> [4, 13] </ref>. hhhhhhhhhhhhhhhhhh * The Stanford University authors are supported by DARPA contract N00039-91-C-0138. Kourosh Gharachorloo is partly supported by Texas Instruments. Anoop Gupta is partly supported by a NSF Presidential Young Investigator Award with matching funds from Sumitomo, Tandem, and TRW.
Reference: 14. <author> L. Rudolph and Z. Segall, </author> <title> Dynamic Decentralized Cache Schemes for MIMD Parallel Processors, </title> <booktitle> Proc. Eleventh Intl. Symp. on Computer Architecture, </booktitle> <month> June </month> <year> 1984, </year> <pages> 340-347. </pages>
Reference-contexts: In the appendix, we give a more general definition that, for example, allows implementations of locks using test&test&set <ref> [14] </ref> or back-off [12] techniques to be considered as synchronization loop constructs. Definition 3: Synchronization Loop Construct: A synchronization loop construct is a sequence of instructions in a program that satisfies the following. (i) The construct executes a read or a read-modify-write to a specific location.
Reference: 15. <author> SUN, </author> <title> The SPARC Architecture Manual, </title> <institution> Sun Microsystems Inc., </institution> <note> No. 800-199-12, Version 8, </note> <month> January </month> <year> 1991. </year>
Reference-contexts: Bell Laboratories, Cray Research Foundation and Digital Equipment Corporation. Sarita Adve is also supported by an IBM graduate fellowship. - 1 - - -- To achieve better system performance, researchers have proposed alternate memory models: processor con sistency (PC) [5] 1 , total store ordering (TSO) <ref> [15] </ref>, partial store ordering (PSO) [15], weak ordering (WO) [4], and release consistency (RCsc/RCpc) [5]. The optimizations allowed by these models can provide substantial improvement in system performance, especially as memory latencies grow relative to processor speeds [6, 7]. <p> Sarita Adve is also supported by an IBM graduate fellowship. - 1 - - -- To achieve better system performance, researchers have proposed alternate memory models: processor con sistency (PC) [5] 1 , total store ordering (TSO) <ref> [15] </ref>, partial store ordering (PSO) [15], weak ordering (WO) [4], and release consistency (RCsc/RCpc) [5]. The optimizations allowed by these models can provide substantial improvement in system performance, especially as memory latencies grow relative to processor speeds [6, 7].
Reference: 16. <author> C. Scheurich and M. Dubois, </author> <title> Correct Memory Operation of Cache-Based Multiprocessors, </title> <booktitle> Proc. Fourteenth Annual Intl. Symp. on Computer Architecture, </booktitle> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1987, </year> <pages> 234-243. </pages>
Reference-contexts: To provide SC executions, it is sufficient if (i) accesses of a single processor are executed one at a time in program order, and (ii) a write is made visible to all processors simultaneously (referred to as atomicity) <ref> [16] </ref>. If (i) is satisfied, it follows that a weaker notion of the atomicity condition in (ii) suffices: it is sufficient if a write becomes visible simultaneously to all processors other than the one that issued the write. In the following, we use atomic to refer to this weaker notion.
Reference: 17. <author> D. Shasha and M. Snir, </author> <title> Efficient and Correct Execution of Parallel Programs that Share Memory, </title> <journal> ACM Trans. on Programming Languages and Systems 10, </journal> <month> 2 (April </month> <year> 1988), </year> <pages> 282-312. </pages>
Reference-contexts: memory access, or simply an access, is a single read or write to a specific shared memory location. 4 For every execution of a pro gram, the program text defines a partial order, called the program order ( po ), on the memory accesses of each process in the execution <ref> [17] </ref>. An SC execution refers to an execution of a program on an SC system (Refer to [2, 3] for formal definitions of a program, an execution, and a system.) A system is SC if the result of every execu-hhhhhhhhhhhhhhhhhh 4. <p> The result of an execution has been interpreted as the values its reads return [2, 8]. Finally, two accesses are considered conflicting if they are to the same location and at least one of them is a write <ref> [17] </ref>. accesses categories. The producer (P1) writes two locations and sets a flag, while the consumer (P2) waits for the flag to be set and then reads the two locations. We next formalize the access categories.
References-found: 17

