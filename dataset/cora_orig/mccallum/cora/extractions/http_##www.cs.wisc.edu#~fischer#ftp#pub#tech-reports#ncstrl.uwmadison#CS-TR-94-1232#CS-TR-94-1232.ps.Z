URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1232/CS-TR-94-1232.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-94-1232/
Root-URL: http://www.cs.wisc.edu
Title: Partitioning Mathematical Programs for Parallel Solution  
Author: Michael C. Ferris and Jeffrey D. Horn 
Date: May, 1994  
Abstract: This paper describes heuristics for partitioning a general M fiN matrix into doubly-bordered, block-diagonal form. Such heuristics are useful for decomposing large, constrained, optimization problems into forms that are amenable to parallel processing. The heuristics presented are all O((M + N ) 2 log(M + N )) and are easily implemented. The application of such techniques for solving large linear programs is described. Extensive computational results on the effectiveness of our partitioning procedures and their usefulness for parallel optimization are presented.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. J. Carolan, J. E. Hill, J. L. Kennington, and S. Niemi an S. J. Wichmann. </author> <title> An empirical evaluation of the KORBX algorithms for military airlift applications. </title> <journal> Operations Research, </journal> <volume> 38 </volume> <pages> 240-248, </pages> <year> 1990. </year>
Reference-contexts: In the second set of results, we show how well our heuristic identifies hidden structure in a problem. To do this, we consider the Patient Distribution System problem <ref> [1] </ref> which has a natural 11 block structure (see Figure 6) and was obtained from the United States Air Force. The problem we consider here is of size 1,386 by 3,729, with 11 blocks all approximately 125 by 340 with 90 column-coupling constraints.
Reference: [2] <author> B. J. Chun, S. J. Lee, and S. M. Robinson. </author> <title> An implementation of the bundle decomposition algorithm. </title> <type> Technical Report 91-6, </type> <institution> Department of Industrial Engineering, University of Wisconsin-Madison, </institution> <year> 1991. </year>
Reference-contexts: A detailed description of the form of the dual problem (due to Robinson [22, 23]) and the bundle method that we use for its solution (proposed in [14]) is the subject of the remainder of this section. Other related work on bundle methods can be found in <ref> [15, 27, 17, 2] </ref>. The final section of the paper gives some numerical results for our implementation.
Reference: [3] <author> D. Culler. </author> <title> The Split-C Programming Language. </title> <institution> Computer Science Department, University of California, Berkeley. </institution>
Reference-contexts: The key to the success of this approach is a partition with roughly equal sized blocks and few linking constraints. 23 5 Parallel Solution of Linear Programs The algorithm for solving linear programs given in the previous section has been implemented in Split-C <ref> [3] </ref> on the Thinking Machines CM-5 supercomputer and used to solve a variety of the largest linear programs in the NETLIB collection [8]. Split-C [3] is a parallel extension of the C programming language primarily intended for distributed memory multiprocessors and designed around two objectives. <p> few linking constraints. 23 5 Parallel Solution of Linear Programs The algorithm for solving linear programs given in the previous section has been implemented in Split-C <ref> [3] </ref> on the Thinking Machines CM-5 supercomputer and used to solve a variety of the largest linear programs in the NETLIB collection [8]. Split-C [3] is a parallel extension of the C programming language primarily intended for distributed memory multiprocessors and designed around two objectives.
Reference: [4] <author> G. B. Dantzig and P. Wolfe. </author> <title> Decomposition principle for linear programs. </title> <journal> Operations Research, </journal> <volume> 8 </volume> <pages> 101-111, </pages> <year> 1960. </year>
Reference-contexts: There are several known techniques for solving problems of this form in parallel <ref> [4, 17, 28, 11, 12] </ref>. We now describe the one that we shall use in this paper.
Reference: [5] <author> I. S. Duff, A. M. Erisman, and J. K. Reid. </author> <title> Direct Methods for Sparse Matrices, chapter 8. </title> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction This paper describes several heuristics for partitioning a general M fiN matrix into a doubly-bordered, block-diagonal form. Such a partitioning is important in many areas of numerical analysis where several partitioning heuristics exist for the special case of N fi N symmetric matrices <ref> [5, 9] </ref>. We use our partitioning heuristics to decompose large, constrained optimization problems into forms amenable to parallel processing.
Reference: [6] <author> M. C. Ferris and O. L. Mangasarian. </author> <title> Parallel constraint distribution. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(4) </volume> <pages> 487-500, </pages> <year> 1991. </year>
Reference-contexts: We note however, that the analysis of this paper does not rely on the linearity of the constraints. Nonlinear programs can use the same technique to exploit underlying structure in the constraint set and enable the efficient solution of such problems using decomposition techniques such as those found in <ref> [6, 7, 26] </ref>. 2 Matrix Partitioning Algorithms Definition 1 We shall call a matrix doubly-bordered, block-diagonal if it is of the following form: 0 B B B @ B 2 C 2 . . .
Reference: [7] <author> M. C. Ferris and O. L. Mangasarian. </author> <title> Parallel variable distribution. </title> <type> Technical Report 1175, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, </institution> <year> 1993. </year> <note> To appear in SIAM Journal on Optimization. </note>
Reference-contexts: We note however, that the analysis of this paper does not rely on the linearity of the constraints. Nonlinear programs can use the same technique to exploit underlying structure in the constraint set and enable the efficient solution of such problems using decomposition techniques such as those found in <ref> [6, 7, 26] </ref>. 2 Matrix Partitioning Algorithms Definition 1 We shall call a matrix doubly-bordered, block-diagonal if it is of the following form: 0 B B B @ B 2 C 2 . . .
Reference: [8] <author> D. M. Gay. </author> <title> Electronic mail distribution of linear programming test problems. </title> <journal> COAL Newsletter, </journal> <volume> 13 </volume> <pages> 10-12, </pages> <year> 1985. </year>
Reference-contexts: That is, how close do they come to producing a doubly-bordered, block-diagonal matrix with the desired number of blocks. We have run the above matrix partitioning procedure on all the sample linear programs that are publicly available via anonymous ftp from netlib.att.com (see <ref> [8] </ref>). We have attempted to partition each problem into 2, 4, 8, 16, 32, 64, 128, and 256 blocks. Let m i denote the number of rows and n i the number of columns in the i th block. <p> We apply the matrix partitioning scheme to linear programming problems arising in the NETLIB collection <ref> [8] </ref> to form a singly-bordered block-diagonal linear program (see Section 2). We then apply a variant of the bundle method to an appropriately formed dual problem and implement the resulting algorithm on the Thinking Machines CM-5 to obtain an efficient parallel method for general linear programming problems. <p> blocks and few linking constraints. 23 5 Parallel Solution of Linear Programs The algorithm for solving linear programs given in the previous section has been implemented in Split-C [3] on the Thinking Machines CM-5 supercomputer and used to solve a variety of the largest linear programs in the NETLIB collection <ref> [8] </ref>. Split-C [3] is a parallel extension of the C programming language primarily intended for distributed memory multiprocessors and designed around two objectives.
Reference: [9] <author> A. George. </author> <title> An automatic one-way dissection algorithm for irregular finite-element problems. </title> <journal> SIAM J. Numer. Anal., </journal> <pages> pages 740-751, </pages> <year> 1980. </year> <month> 25 </month>
Reference-contexts: 1 Introduction This paper describes several heuristics for partitioning a general M fiN matrix into a doubly-bordered, block-diagonal form. Such a partitioning is important in many areas of numerical analysis where several partitioning heuristics exist for the special case of N fi N symmetric matrices <ref> [5, 9] </ref>. We use our partitioning heuristics to decompose large, constrained optimization problems into forms amenable to parallel processing.
Reference: [10] <author> J. Gilbert and E. Ng. </author> <title> Predicting structure in nonsymmetric sparse matrix factorizations. </title> <note> Xerox PARC Technical Report, </note> <year> 1992. </year>
Reference-contexts: We will later give a procedure whereby all of the row-linking constraints can be removed by adding some columns to various blocks and extra column-linking constraints. An important concept in what follows is that of the associated graph of a matrix <ref> [10] </ref>. Definition 4 Given a matrix A MfiN , the associated graph of A, denoted by G (A) is the pair (V; E) satisfying: 1.
Reference: [11] <author> R. V. Helgason, J. L. Kennington, and H. A. Zaki. </author> <title> A parallelization of the simplex method. </title> <journal> Annals of Operations Research, </journal> <volume> 14 </volume> <pages> 17-40, </pages> <year> 1988. </year>
Reference-contexts: There are several known techniques for solving problems of this form in parallel <ref> [4, 17, 28, 11, 12] </ref>. We now describe the one that we shall use in this paper.
Reference: [12] <author> J. K. Ho, T. C. Lee, and R. P. Sundarraj. </author> <title> Decomposition of linear programs using parallel computation. </title> <journal> Mathematical Programming, </journal> <volume> 42 </volume> <pages> 391-405, </pages> <year> 1988. </year>
Reference-contexts: There are several known techniques for solving problems of this form in parallel <ref> [4, 17, 28, 11, 12] </ref>. We now describe the one that we shall use in this paper.
Reference: [13] <author> B. W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> The Bell System Technical Journal, </journal> <pages> pages 291-307, </pages> <year> 1970. </year>
Reference-contexts: The cost of such a partition is the number of edges in E that connect vertices in different subsets of the partition of V . Kernighan and Lin give a highly effective heuristic for partitioning graphs so as to minimize the cost of the resulting partition <ref> [13] </ref>. We briefly describe their heuristics. 4 2.1 Two-Way Uniform Partitions We first consider the problem of partitioning a graph with 2n vertices into two equally sized subsets. Heuristics for solving this problem are the building blocks for heuristics that solve more general graph partitioning problems.
Reference: [14] <author> C. Lemarechal, A. Nemirovskii, and Y. Nesterov. </author> <title> New variants of bundle methods. </title> <institution> Rapports de Recherche 1508, INRIA-Rocquencourt, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: A detailed description of the form of the dual problem (due to Robinson [22, 23]) and the bundle method that we use for its solution (proposed in <ref> [14] </ref>) is the subject of the remainder of this section. Other related work on bundle methods can be found in [15, 27, 17, 2]. The final section of the paper gives some numerical results for our implementation. <p> Thus a subgradient at y of g can be calculated by solving the K subproblems (2). Therefore, to solve (1) we use the bundle-level algorithm from <ref> [14] </ref>, which is now discussed in more detail. Suppose that we wish to min f (x); where f is a convex function and Q represents some simple convex constraint set. <p> Project x fl onto the level set of the model M L = fx 2 Qjm (x) Lg, that is x i+1 = (x fl jM L ): It can be shown (see <ref> [14] </ref>) that this technique will generate function values arbitrarily close to the optimal value under a simple compactness assumption on Q. Each iteration requires the evaluation of f (x i ) and f 0 (x i ) which can be carried out in parallel in our work as described above.
Reference: [15] <editor> C. Lemarechal, J. J. Strodiot, and A. Bihain. </editor> <title> On a bundle algorithm for nonsmooth optimization. </title> <editor> In O. L. Mangasarian, R. R. Meyer, and S. M. Robinson, editors, </editor> <booktitle> Nonlinear Programming, </booktitle> <volume> volume 4, </volume> <pages> pages 245-282. </pages> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: A detailed description of the form of the dual problem (due to Robinson [22, 23]) and the bundle method that we use for its solution (proposed in [14]) is the subject of the remainder of this section. Other related work on bundle methods can be found in <ref> [15, 27, 17, 2] </ref>. The final section of the paper gives some numerical results for our implementation.
Reference: [16] <author> D. Medhi. </author> <title> Decomposition of structured large-scale optimization problems and parallel optimization. </title> <type> Technical Report 718, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, Wisconsin 53706, </institution> <year> 1987. </year> <type> Ph.D. thesis. </type>
Reference-contexts: These multiple copies are used to decouple the corresponding C i 's. We then add column-linking constraints that force these variables all to be equal. This technique is the same as one used in stochastic programming to treat non-anticipativity (see [20]). Other techniques are described in <ref> [16] </ref>.
Reference: [17] <author> D. Medhi. </author> <title> Parallel bundle-based decompostion for large-scale structured mathematical programming problems. </title> <journal> Annals of Operations Research, </journal> <volume> 22 </volume> <pages> 101-127, </pages> <year> 1990. </year>
Reference-contexts: A detailed description of the form of the dual problem (due to Robinson [22, 23]) and the bundle method that we use for its solution (proposed in [14]) is the subject of the remainder of this section. Other related work on bundle methods can be found in <ref> [15, 27, 17, 2] </ref>. The final section of the paper gives some numerical results for our implementation. <p> There are several known techniques for solving problems of this form in parallel <ref> [4, 17, 28, 11, 12] </ref>. We now describe the one that we shall use in this paper.
Reference: [18] <author> R. Mi*in. </author> <title> A stable method for solving certain constrained least squares problems. </title> <booktitle> Mathematical Programming, </booktitle> <pages> pages 141-158, </pages> <year> 1979. </year>
Reference-contexts: The synchronization steps solve the linear programs using the same code as the parallel steps, the quadratic program resulting from the projection is solved using a method due to Mi*in <ref> [18] </ref>. In Table 7 we report the results on the subset of the NETLIB problems that had very good values.
Reference: [19] <author> J. L. Nazareth. </author> <title> Computer Solution of Linear Programs. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1987. </year>
Reference-contexts: Once the partitioning is complete, we apply the bundle-level method to the resulting linear program. For the function and gradient evaluation steps we use an implementation of the revised simplex method written in C which incorporates the Reid basis updating technique [21] and other computational enhancements <ref> [19] </ref>. The synchronization steps solve the linear programs using the same code as the parallel steps, the quadratic program resulting from the projection is solved using a method due to Mi*in [18].
Reference: [20] <author> S. S. Nielsen and S. A. Zenios. </author> <title> Solving linear stochastic network programs using massively parallel proximal algorithms. </title> <type> Technical Report 92-01-05, </type> <institution> Decision Sciences Department, The Wharton School, University of Pennsylvania, </institution> <address> Philadelphia, </address> <year> 1992. </year>
Reference-contexts: These multiple copies are used to decouple the corresponding C i 's. We then add column-linking constraints that force these variables all to be equal. This technique is the same as one used in stochastic programming to treat non-anticipativity (see <ref> [20] </ref>). Other techniques are described in [16].
Reference: [21] <author> J. K. Reid. </author> <title> A sparsity-exploiting variant of the Bartels-Golub decompostion for linear programming bases. </title> <journal> Mathematical Programming, </journal> <volume> 24 </volume> <pages> 55-69, </pages> <year> 1982. </year>
Reference-contexts: This hypothesis could be tested in future work. Once the partitioning is complete, we apply the bundle-level method to the resulting linear program. For the function and gradient evaluation steps we use an implementation of the revised simplex method written in C which incorporates the Reid basis updating technique <ref> [21] </ref> and other computational enhancements [19]. The synchronization steps solve the linear programs using the same code as the parallel steps, the quadratic program resulting from the projection is solved using a method due to Mi*in [18].
Reference: [22] <author> S. M. Robinson. </author> <title> Bundle-based decomposition: Description and preliminary results. </title> <booktitle> In </booktitle>
Reference-contexts: We then apply a variant of the bundle method to an appropriately formed dual problem and implement the resulting algorithm on the Thinking Machines CM-5 to obtain an efficient parallel method for general linear programming problems. A detailed description of the form of the dual problem (due to Robinson <ref> [22, 23] </ref>) and the bundle method that we use for its solution (proposed in [14]) is the subject of the remainder of this section. Other related work on bundle methods can be found in [15, 27, 17, 2].
References-found: 22

