URL: http://forum.swarthmore.edu/~jay/learn-game/archive/NeuroDraughts-IAI.ps
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: 
Email: email: niall.griffith@ul.ie  email: mark@did.com  
Title: NeuroDraughts: the role of representation, search, training regime and architecture in a TD draughts player  
Author: N.J.L. Griffith M. Lynch 
Address: Limerick, Ireland.  Limerick, Ireland.  
Affiliation: Department of Computer Science and Information Systems University of  Department of Computer Science and Information Systems University of  
Abstract: NeuroDraughts is a draughts playing program similar in approach to NeuroGammon and NeuroChess [Tesauro, 1992, Thrun, 1995]. It uses an artificial neural network trained by the method of temporal difference learning to learn by self-play how to play the game of draughts. This paper discusses the relative contribution of board representation, search depth, training regime, architecture and run time parameters to the strength of the TDplayer produced by the system. Keywords: Temporal Difference Learning, Input representation, Search, Draughts. 
Abstract-found: 1
Intro-found: 1
Reference: [Samuel, 1959] <author> Samuel, A. L. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3 </volume> <pages> 210-229. </pages>
Reference-contexts: Notable among these are NeuroGammon and NeuroChess [Tesauro, 1992, Thrun, 1995]. However, the principle of TD is not new, having first been applied by Samuels, who pioneered the idea of updating evaluations based on successive predictions in his 1959 checkers program <ref> [Samuel, 1959] </ref>. Successfully assigning credit over time means that a system can select actions according to how they contribute to the realization of some desired future outcome. For example, in a game each move contributes to a future (potential) victory, but only the last move actually wins the game.
Reference: [Sharkey and Sharkey, 1995] <author> Sharkey, N. and Sharkey, A. </author> <year> (1995). </year> <title> An analysis of catastrophic interference. </title> <journal> Connection Science, </journal> <volume> 7 </volume> <pages> 301-329. </pages>
Reference-contexts: The implementation of TD used a feedforward architecture and standard backpropagation (BP) with momentum. This form of learning is highly dependent on the training set used <ref> [Sharkey and Sharkey, 1995] </ref>, and any inherent bias in the coverage of the function being learned may also be exacerbated by the degrees of freedom in the network.
Reference: [Sutton, 1988] <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference-contexts: 1 Introduction The Temporal Difference (TD) family of Reinforcement Learning procedures <ref> [Sutton, 1988] </ref>, have been applied successfully to a number of domains over the last decade including playing games. Notable among these are NeuroGammon and NeuroChess [Tesauro, 1992, Thrun, 1995]. <p> TD learning provides a computational mechanism that allows the positive or negative reward given at the end of a game to be in effect passed back in time to previous moves. The procedure has been formalized and convergence proven for TD (0), <ref> [Sutton, 1988] </ref>. The work reported here describes a set of simulations in which TD was set to learn the game of draughts 1 .
Reference: [Tesauro, 1992] <author> Tesauro, G. J. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <booktitle> Machine Learning, </booktitle> <pages> pages 257-279. </pages>
Reference-contexts: 1 Introduction The Temporal Difference (TD) family of Reinforcement Learning procedures [Sutton, 1988], have been applied successfully to a number of domains over the last decade including playing games. Notable among these are NeuroGammon and NeuroChess <ref> [Tesauro, 1992, Thrun, 1995] </ref>. However, the principle of TD is not new, having first been applied by Samuels, who pioneered the idea of updating evaluations based on successive predictions in his 1959 checkers program [Samuel, 1959]. <p> The use of a search strategy such a MiniMax while not changing the reward regime has the advantage of dramatically reducing the space of potential moves that TD has to search to find optimal moves. Tesauro <ref> [Tesauro, 1992] </ref> found this to be the case for NeuroGammon. However, while the reduction in training time is desirable it is possible that distortions will be introduced by this approach.
Reference: [Thrun, 1995] <author> Thrun, S. </author> <year> (1995). </year> <title> Learning to play the game of chess. </title> <editor> In D.Touretzky, G. T. and Leen, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> San Mateo, CA:. </address>
Reference-contexts: 1 Introduction The Temporal Difference (TD) family of Reinforcement Learning procedures [Sutton, 1988], have been applied successfully to a number of domains over the last decade including playing games. Notable among these are NeuroGammon and NeuroChess <ref> [Tesauro, 1992, Thrun, 1995] </ref>. However, the principle of TD is not new, having first been applied by Samuels, who pioneered the idea of updating evaluations based on successive predictions in his 1959 checkers program [Samuel, 1959].
References-found: 5

