URL: http://www.iro.umontreal.ca/~lisa/pointeurs/ieeeproc98.ps
Refering-URL: http://www.iro.umontreal.ca/~bengioy/ift6266/convo/convo.html
Root-URL: http://www.iro.umontreal.ca
Title: Gradient-Based Learning Applied to Document Recognition systems for on-line handwriting recognition are described. Experiments demonstrate
Author: Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner 
Keyword: Neural Networks, OCR, Document Recognition, Machine Learning, Gradient-Based Learning, Convolutional Neural Networks, Graph Transformer Networks, Finite State Transducers.  
Date: 1997 1  
Note: SUBMITTED TO PROCEEDINGS OF THE IEEE,  Two  month.  
Abstract: Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique. Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called Graph Transformer Networks (GTN), allows such multi-module systems to be trained globally using Gradient-Based methods so as to minimize an overall performance measure. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification And Scene Analysis, </title> <publisher> Wiley and Son, </publisher> <year> 1973. </year>
Reference-contexts: On the regular data, the error rate is 12%. The network has 7850 free parameters. On the deslanted images, the test error rate is 8.4% The network has 4010 free parameters. The deficiencies of the linear classifier are well documented <ref> [1] </ref> and it is included here simply to form a basis of comparison for more sophisticated classifiers. Various combinations of SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 12 Fig. 9.
Reference: [2] <author> Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, </author> <title> "Backpropagation applied to handwritten zip code recognition," </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 541-551, </pages> <month> Winter </month> <year> 1989. </year>
Reference: [3] <author> S. Seung, H. Sompolinsky, and N. Tishby, </author> <title> "Statistical mechanics of learning from examples," </title> <journal> Physical Review A, </journal> <volume> vol. 45, </volume> <pages> pp. 6056-6091, </pages> <year> 1992. </year>
Reference: [4] <author> V. N. Vapnik, E. Levin, and Y. LeCun, </author> <title> "Measuring the vc-dimension of a learning machine," </title> <journal> Neural Computation, </journal> <volume> vol. 6, no. 5, </volume> <pages> pp. 851-876, </pages> <year> 1994. </year>
Reference: [5] <author> C. Cortes, L. Jackel, S. Solla, V. N. Vapnik, and J. Denker, </author> <title> "Learning curves: asymptotic values and rate of convergence," </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <editor> J. D. Cowan, G. Tesauro, and J. Alspector, Eds., </editor> <address> San Mateo, CA, </address> <year> 1994, </year> <pages> pp. 327-334, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: [6] <author> V. N. Vapnik, </author> <title> The Nature of Statistical Learning Theory, </title> <publisher> Springer, </publisher> <address> New-York, </address> <year> 1995. </year>
Reference-contexts: As the learning proceeds, the weights grow, which progressively increases the effective capacity of the network. This seems to be an almost perfect, if fortuitous, implementation of Vapnik's "Structural Risk Minimization" principle <ref> [6] </ref>. A better theoretical understanding of these phenomena, and more empirical evidence, are definitely needed. C.6 Two-Hidden Layer Fully Connected Multilayer Neural Network To see the effect of the architecture, several two-hidden layer multilayer neural networks were trained. <p> Unfortunately, they are impractical for high-dimensional problems, because the number of product terms is prohibitive. The Support Vector technique is an extremely economical way of representing complex surfaces in high-dimensional spaces, including polynomials and many other types of surfaces <ref> [6] </ref>. A particularly interesting subset of decision surfaces is the ones that correspond to hyperplanes that are at a maximum distance from the convex hulls of the two classes in the high-dimensional space of the product terms. <p> This is not a discriminant approach in that it does not focus on the ultimate goal of learning, which in this case is to learn the classification decision surface. Theoretical arguments <ref> [6] </ref>, [7] suggest that estimating input densities when the real goal is to obtain a discriminant function for classification is a suboptimal strategy. In theory, the problem of estimating densities in high-dimensional spaces is much more Fig. 22.
Reference: [7] <author> V. N. Vapnik, </author> <title> Statistical Learning Theory, </title> <publisher> John Wiley & Sons, </publisher> <address> New-York, </address> <year> 1998. </year>
Reference-contexts: This is not a discriminant approach in that it does not focus on the ultimate goal of learning, which in this case is to learn the classification decision surface. Theoretical arguments [6], <ref> [7] </ref> suggest that estimating input densities when the real goal is to obtain a discriminant function for classification is a suboptimal strategy. In theory, the problem of estimating densities in high-dimensional spaces is much more Fig. 22.
Reference: [8] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vet-terling, </author> <title> Numerical Recipes: </title> <booktitle> The Art of Scientific Computing, </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: The larger h kk , the smaller the weight update. The parameter prevents the step size from becoming too large when the second derivative is small, very much like the "model-trust" methods, and the Levenberg-Marquardt methods in non-linear optimization <ref> [8] </ref>. The exact formula to compute h kk from the second derivatives with respect to the connection weights is: h kk = (i;j)2V k (k;l)2V k @u ij @u kl However, we make three approximations.
Reference: [9] <author> S. I. Amari, </author> <title> "A theory of adaptive pattern classifiers," </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> vol. EC-16, </volume> <pages> pp. 299-307, </pages> <year> 1967. </year>
Reference: [10] <author> Ya. Tsypkin, </author> <title> Adaptation and Learning in automatic systems, </title> <publisher> Academic Press, </publisher> <year> 1971. </year>
Reference: [11] <author> Ya. Tsypkin, </author> <title> Foundations of the theory of learning systems, </title> <publisher> Academic Press, </publisher> <year> 1973. </year>
Reference: [12] <author> M. Minsky and O. Selfridge, </author> <title> "Learning in random nets," </title> <booktitle> in 4th London symposium on Information Theory, </booktitle> <address> London, </address> <year> 1961. </year>
Reference: [13] <author> D. H. Ackley, G. E. Hinton, and T. J. Sejnowski, </author> <title> "A learning algorithm for boltzmann machines," </title> <journal> Cognitive Science, </journal> <volume> vol. 9, </volume> <pages> pp. 147-169, </pages> <year> 1985. </year>
Reference-contexts: Readers familiar with the Boltzmann machine connectionist model might recognize the constrained and unconstrained graphs as analogous to the "clamped" (constrained by the observed values of the output variable) and "free" (unconstrained) phases of the Boltzmann machine algorithm <ref> [13] </ref>. Back-propagating derivatives through the discriminative Forward GTN distributes gradient more evenly that in the Viterbi case. Derivatives are back-propagated through the left half of the the GTN in Figure 21 down to the interpretation graph.
Reference: [14] <author> G. E. Hinton and T. J. Sejnowski, </author> <title> "Learning and relearning in Boltzmann machines," </title> <booktitle> in Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, </booktitle> <editor> D. E. Rumelhart and J. L. McClelland, Eds. </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference: [15] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> "Learning internal representations by error propagation," </title> <booktitle> in Parallel distributed processing: Explorations in the microstructure of cognition, </booktitle> <volume> vol. I, </volume> <pages> pp. 318-362. </pages> <publisher> Bradford Books, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference: [16] <author> A. E. Jr. Bryson and Yu-Chi Ho, </author> <title> Applied Optimal Control, </title> <publisher> Blaisdell Publishing Co., </publisher> <year> 1969. </year>
Reference: [17] <author> Y. LeCun, </author> <title> "A learning scheme for asymmetric threshold networks," </title> <booktitle> in Proceedings of Cognitiva 85, </booktitle> <address> Paris, France, </address> <year> 1985, </year> <pages> pp. 599-604. </pages>
Reference: [18] <author> Y. LeCun, </author> <title> "Learning processes in an asymmetric threshold network," in Disordered systems and biological organization, </title> <editor> E. Bienenstock, F. Fogelman-Soulie, and G. Weisbuch, Eds., Les Houches, </editor> <address> France, </address> <year> 1986, </year> <pages> pp. 233-240, </pages> <publisher> Springer-Verlag. </publisher>
Reference: [19] <author> D. B. Parker, "Learning-logic," </author> <type> Tech. Rep., </type> <institution> TR-47, Sloan School of Management, MIT, </institution> <address> Cambridge, Mass., </address> <month> April </month> <year> 1985. </year>
Reference: [20] <author> Y. LeCun, </author> <title> Modeles connexionnistes de l'apprentissage (connectionist learning models), </title> <type> Ph.D. thesis, </type> <institution> Universite P. et M. Curie (Paris 6), </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: The cause of this problem is that in weight space the origin is a fixed point of the learning dynamics, and, although it is a saddle point, it is attractive in almost all directions [114]. For our simulations, we use A = 1:7159 and S = 2 3 (see <ref> [20] </ref>, [32]). With this choice of parameters, the equalities f (1) = 1 and f (1) = 1 are satisfied. <p> The patterns are presented in a constant random order, and the training set is typically repeated 20 times. Our update algorithm is dubbed the Stochastic Diagonal Levenberg-Marquardt method where an individual learning rate (step size) is computed for each parameter (weight) before each pass through the training set <ref> [20] </ref>, [119], [32]. These learning rates are computed using the diagonal terms of an estimate of the Gauss-Newton approximation to the Hessian (second derivative) matrix. This algorithm is not believed to bring a tremendous increase in learning speed but it converges reliably without requiring extensive adjustments of the learning parameters. <p> Interestingly, there is an efficient algorithm to compute those second derivatives which is very similar to the back-propagation procedure used to compute the first derivatives <ref> [20] </ref>, [119]: @ 2 E p i X u 2 @ 2 E p k @E p (26) Unfortunately, using those derivatives leads to well-known problems associated with every Newton-like algorithm: these terms can be negative, and can cause the gradient algorithm to move uphill instead of downhill.
Reference: [21] <author> Y. LeCun, </author> <title> "A theoretical framework for back-propagation," </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <editor> D. Touretzky, G. Hinton, and T. Sejnowski, Eds., </editor> <address> CMU, Pitts-burgh, Pa, </address> <year> 1988, </year> <pages> pp. 21-28, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is based on a home-grown object-oriented dialect of Lisp with a compiler to C. The fact that derivatives can be computed by propagation in the reverse graph is easy to understand intuitively. The best way to justify it theoretically is through the use of Lagrange functions <ref> [21] </ref>, [22]. The same formalism can be used to extend the procedures to networks with recurrent connections. B. Special Modules Neural networks and many other standard pattern recognition techniques can be formulated in terms of multi-modular systems trained with Gradient-Based Learning.
Reference: [22] <author> L. Bottou and P. Gallinari, </author> <title> "A framework for the cooperation of learning algorithms," </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> D. Touretzky and R. Lippmann, Eds., </editor> <booktitle> Denver, 1991, </booktitle> <volume> vol. 3, </volume> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is based on a home-grown object-oriented dialect of Lisp with a compiler to C. The fact that derivatives can be computed by propagation in the reverse graph is easy to understand intuitively. The best way to justify it theoretically is through the use of Lagrange functions [21], <ref> [22] </ref>. The same formalism can be used to extend the procedures to networks with recurrent connections. B. Special Modules Neural networks and many other standard pattern recognition techniques can be formulated in terms of multi-modular systems trained with Gradient-Based Learning.
Reference: [23] <author> C. Y. Suen, C. Nadal, R. Legault, T. A. Mai, and L. Lam, </author> <title> "Computer recognition of unconstrained handwritten numerals," </title> <journal> Proceedings of the IEEE, Special issue on Optical Character Recognition, </journal> <volume> vol. 80, no. 7, </volume> <pages> pp. 1162-1180, </pages> <month> July </month> <year> 1992. </year>
Reference: [24] <author> S. N. Srihari, </author> <title> "High-performance reading machines," </title> <journal> Proceedings of the IEEE, Special issue on Optical Character Recognition, </journal> <volume> vol. 80, no. 7, </volume> <pages> pp. 1120-1132, </pages> <month> July </month> <year> 1992. </year>
Reference: [25] <author> Y. LeCun, L. D. Jackel, B. Boser, J. S. Denker, H. P. Graf, I. Guyon, D. Henderson, R. E. Howard, and W. Hubbard, </author> <title> "Handwritten digit recognition: Applications of neural net chips and automatic learning," </title> <journal> IEEE Communication, </journal> <pages> pp. 41-46, </pages> <month> November </month> <year> 1989, </year> <type> invited paper. </type>
Reference: [26] <author> L. R. Rabiner, </author> <title> "A tutorial on hidden Markov models and selected applications in speech recognition," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 77, no. 2, </volume> <pages> pp. 257-286, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: However, it is almost impossible to devise image analysis techniques that will infallibly segment naturally written sequences of characters into well formed characters. The recent history of automatic speech recognition <ref> [26] </ref>, [64] is here to remind us that training a recognizer by optimizing a global criterion (at the word or sentence level) is much preferable to merely training it on hand-segmented phonemes or other units. <p> Viterbi training, though formulated differently, is often use in HMM-based speech recognition systems <ref> [26] </ref>. Similar algorithms have been applied to speech recognition systems that integrate neural networks with time alignment [69], [70], [71] or hybrid neural-network/HMM systems [27], [72], [73]. While it seems simple and satisfying, this training architecture has a flaw that can potentially be fatal. <p> The forward algorithm gets its name from the forward pass of the well-known Baum-Welsh algorithm for training Hidden Markov Models <ref> [26] </ref>. Section VIII-E gives more details on the relation between this work and HMMs.
Reference: [27] <author> H. A. Bourlard and N. Morgan, </author> <title> CONNECTIONIST SPEECH RECOGNITION: A Hybrid Approach, </title> <publisher> Kluwer Academic Publisher, </publisher> <address> Boston, </address> <year> 1994. </year>
Reference-contexts: Viterbi training, though formulated differently, is often use in HMM-based speech recognition systems [26]. Similar algorithms have been applied to speech recognition systems that integrate neural networks with time alignment [69], [70], [71] or hybrid neural-network/HMM systems <ref> [27] </ref>, [72], [73]. While it seems simple and satisfying, this training architecture has a flaw that can potentially be fatal. The problem was already mentioned in Section II-C. <p> In Section IX we present experimental results with an on-line handwriting recognition system that confirm the advantages of using global training versus separate training. Experiments in speech recognition with hybrids of neural networks and HMMs also showed marked improvements brought by global training [79], <ref> [27] </ref>, [64], [80]. VII. Multiple Object Recognition: Space Displacement Neural Network . There is a simple alternative to explicitly segmenting images of character strings using heuristics.
Reference: [28] <author> D. H. Hubel and T. N. Wiesel, </author> <title> "Receptive fields, binocular interaction, and functional architecture in the cat's visual cortex," </title> <journal> Journal of Physiology (London), </journal> <volume> vol. 160, </volume> <pages> pp. 106-154, </pages> <year> 1962. </year>
Reference: [29] <author> K. Fukushima, "Cognitron: </author> <title> A self-organizing multilayered neural network," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 20, </volume> <pages> pp. 121-136, </pages> <year> 1975. </year>
Reference: [30] <author> K. Fukushima and S. Miyake, </author> <title> "Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position," </title> <journal> Pattern Recognition, </journal> <volume> vol. 15, </volume> <pages> pp. 455-469, </pages> <year> 1982. </year>
Reference: [31] <author> M. C. Mozer, </author> <title> The perception of multiple objects: A connectionist approach, </title> <publisher> MIT Press-Bradford Books, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference: [32] <author> Y. LeCun, </author> <title> "Generalization and network design strategies," in Connectionism in Perspective, </title> <editor> R. Pfeifer, Z. Schreter, F. Fogel-man, and L. Steels, Eds., </editor> <address> Zurich, Switzerland, </address> <year> 1989, </year> <note> Elsevier, an extended version was published as a technical report of the University of Toronto. </note>
Reference-contexts: For our simulations, we use A = 1:7159 and S = 2 3 (see [20], <ref> [32] </ref>). With this choice of parameters, the equalities f (1) = 1 and f (1) = 1 are satisfied. The rationale behind this is that the overall gain of the squashing transformation is around 1 in normal operating conditions, and the interpretation of the state of the network is simplified. <p> Our update algorithm is dubbed the Stochastic Diagonal Levenberg-Marquardt method where an individual learning rate (step size) is computed for each parameter (weight) before each pass through the training set [20], [119], <ref> [32] </ref>. These learning rates are computed using the diagonal terms of an estimate of the Gauss-Newton approximation to the Hessian (second derivative) matrix. This algorithm is not believed to bring a tremendous increase in learning speed but it converges reliably without requiring extensive adjustments of the learning parameters.
Reference: [33] <author> Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel, </author> <title> "Handwritten digit recognition with a back-propagation network," </title> <booktitle> in Advances in Neural Information Processing Systems 2 (NIPS*89), </booktitle> <editor> David Touretzky, Ed., </editor> <address> Denver, CO, 1990, </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The LeNet-1 architecture was developed using our own version of the USPS (US Postal Service zip codes) database and its size was tuned to match the available data <ref> [33] </ref>. LeNet-1 achieved 1.7% test error. The fact that a network with such a small number of parameters can attain such a good error rate is an indication that the architecture is appropriate for the task.
Reference: [34] <author> J. Wang and J Jean, </author> <title> "Multi-resolution neural networks for om-nifont character recognition," </title> <booktitle> in Proceedings of International Conference on Neural Networks, 1993, </booktitle> <volume> vol. III, </volume> <pages> pp. 1588-1593. </pages>
Reference: [35] <author> G. L. Martin, </author> <title> "Centered-object integrated segmentation and recognition of overlapping hand-printed characters," </title> <journal> Neural Computation, </journal> <volume> vol. 5, </volume> <pages> pp. 419-429, </pages> <year> 1993. </year>
Reference: [36] <author> S. Lawrence, C. Lee Giles, A. C. Tsoi, and A. D. </author> <title> Back, "Face recognition: A convolutional neural network approach," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 8, no. 1, </volume> <pages> pp. 98-113, </pages> <year> 1997. </year>
Reference: [37] <author> K. J. Lang and G. E. Hinton, </author> <title> "A time delay neural network architecture for speech recognition," </title> <type> Tech. Rep. </type> <institution> CMU-CS-88-152, Carnegie-Mellon University, </institution> <address> Pittsburgh PA, </address> <year> 1988. </year>
Reference: [38] <author> A. H. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang, </author> <title> "Phoneme recognition using time-delay neural networks," </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> vol. 37, </volume> <pages> pp. 328-339, </pages> <month> March </month> <year> 1989. </year>
Reference: [39] <author> L. Bottou, F. Fogelman, P. Blanchet, and J. S. Lienard, </author> <title> "Speaker independent isolated digit recognition: Multilayer perceptron vs dynamic time warping," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 3, </volume> <pages> pp. 453-465, </pages> <year> 1990. </year>
Reference: [40] <author> P. Haffner and A. H. Waibel, </author> <title> "Time-delay neural networks embedding time alignment: a performance analysis," </title> <booktitle> in EU-ROSPEECH'91, 2nd European Conference on Speech Communication and Technology, </booktitle> <address> Genova, Italy, </address> <month> Sept. </month> <year> 1991. </year>
Reference: [41] <author> I. Guyon, P. Albrecht, Y. LeCun, J. S. Denker, and W. Hub-bard, </author> <title> "Design of a neural network character recognizer for a touch terminal," </title> <journal> Pattern Recognition, </journal> <volume> vol. 24, no. 2, </volume> <pages> pp. 105-119, </pages> <year> 1991. </year>
Reference-contexts: The recognition of handwritten characters from a pen trajectory on a digitizing surface is often done in the time domain [108], <ref> [41] </ref>, [109]. Typically, trajectories are normalized, and local geometrical or dynamical features are extracted. The recognition may then be performed using curve matching [108], or other classification techniques such as TDNNs [41], [109]. <p> of handwritten characters from a pen trajectory on a digitizing surface is often done in the time domain [108], <ref> [41] </ref>, [109]. Typically, trajectories are normalized, and local geometrical or dynamical features are extracted. The recognition may then be performed using curve matching [108], or other classification techniques such as TDNNs [41], [109]. While these representations have several advantages, their dependence on stroke ordering and individual writing styles makes them difficult to Fig. 30. An on-line handwriting recognition GTN based on heuristic over-segmentation use in high accuracy, writer independent systems that integrate the segmentation with the recognition.
Reference: [42] <author> J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore, E. Sackinger, and R. Shah, </author> <title> "Signature verification using a siamese time delay neural network," </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> vol. 7, no. 4, </volume> <month> August </month> <year> 1993. </year>
Reference: [43] <author> Y. LeCun, I. Kanter, and S. Solla, </author> <title> "Eigenvalues of covariance matrices: application to neural-network learning," </title> <journal> Physical Review Letters, </journal> <volume> vol. 66, no. 18, </volume> <pages> pp. 2396-2399, </pages> <month> May </month> <year> 1991. </year>
Reference: [44] <author> T. G. Dietterich and G. Bakiri, </author> <title> "Solving multiclass learning SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 44 problems via error-correcting output codes.," </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> vol. 2, </volume> <pages> pp. 263-286, </pages> <year> 1995. </year>
Reference-contexts: The components of those parameters vectors were set to -1 or +1. While they could have been chosen at random with equal probabilities for -1 and +1, or even chosen to form an error correcting code as suggested by <ref> [44] </ref>, they were instead designed to represent a stylized image of the corresponding character class drawn on a 7x12 bitmap (hence the number 84).
Reference: [45] <author> L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer, </author> <title> "Maximum mutual information of hidden Markov model parameters for speech recognition," </title> <booktitle> in Proc. Int. Conf. Acoust., Speech, Signal Processing, </booktitle> <year> 1986, </year> <pages> pp. 49-52. </pages>
Reference-contexts: The second problem is that there is no competition between the classes. Such a competition can be obtained by using a more discriminative training criterion, dubbed the MAP (maximum a posteriori) criterion, similar to Maximum Mutual Information criterion sometimes used to train HMMs <ref> [45] </ref>, [46], [47]. It corresponds to maximizing the posterior probability of the correct class D p (or minimizing the logarithm of the probability of the correct class), given that the input image can come from one of the classes or from a background "rubbish" class label. <p> On the other hand, if the probabilistic assumptions in an HMM (or other probabilistic model) are not realistic, discriminative training, discussed in Section VI, can improve performance as this has been clearly shown for speech recognition systems <ref> [45] </ref>, [46], [47], [105], [106]. The Input-Output HMM model (IOHMM) [103], [107], is strongly related to graph transformers. Viewed as a probabilistic model, an IOHMM represents the conditional distribution of output sequences given input sequences (of the same or a different length).
Reference: [46] <author> L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer, </author> <title> "Speech recognition with continuous-parameter hidden Markov models," </title> <booktitle> Computer, Speech and Language, </booktitle> <volume> vol. 2, </volume> <pages> pp. 219-234, </pages> <year> 1987. </year>
Reference-contexts: The second problem is that there is no competition between the classes. Such a competition can be obtained by using a more discriminative training criterion, dubbed the MAP (maximum a posteriori) criterion, similar to Maximum Mutual Information criterion sometimes used to train HMMs [45], <ref> [46] </ref>, [47]. It corresponds to maximizing the posterior probability of the correct class D p (or minimizing the logarithm of the probability of the correct class), given that the input image can come from one of the classes or from a background "rubbish" class label. <p> On the other hand, if the probabilistic assumptions in an HMM (or other probabilistic model) are not realistic, discriminative training, discussed in Section VI, can improve performance as this has been clearly shown for speech recognition systems [45], <ref> [46] </ref>, [47], [105], [106]. The Input-Output HMM model (IOHMM) [103], [107], is strongly related to graph transformers. Viewed as a probabilistic model, an IOHMM represents the conditional distribution of output sequences given input sequences (of the same or a different length).
Reference: [47] <author> B. H. Juang and S. Katagiri, </author> <title> "Discriminative learning for minimum error classification," </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing, </journal> <volume> vol. 40, </volume> <pages> pp. 3043-3054, </pages> <year> 1992. </year>
Reference-contexts: The second problem is that there is no competition between the classes. Such a competition can be obtained by using a more discriminative training criterion, dubbed the MAP (maximum a posteriori) criterion, similar to Maximum Mutual Information criterion sometimes used to train HMMs [45], [46], <ref> [47] </ref>. It corresponds to maximizing the posterior probability of the correct class D p (or minimizing the logarithm of the probability of the correct class), given that the input image can come from one of the classes or from a background "rubbish" class label. <p> On the other hand, if the probabilistic assumptions in an HMM (or other probabilistic model) are not realistic, discriminative training, discussed in Section VI, can improve performance as this has been clearly shown for speech recognition systems [45], [46], <ref> [47] </ref>, [105], [106]. The Input-Output HMM model (IOHMM) [103], [107], is strongly related to graph transformers. Viewed as a probabilistic model, an IOHMM represents the conditional distribution of output sequences given input sequences (of the same or a different length).
Reference: [48] <author> Y. LeCun, L. D. Jackel, L. Bottou, A. Brunot, C. Cortes, J. S. Denker, H. Drucker, I. Guyon, U. A. Muller, E. Sackinger, P. Simard, and V. N. Vapnik, </author> <title> "Comparison of learning algorithms for handwritten digit recognition," </title> <booktitle> in International Conference on Artificial Neural Networks, </booktitle> <editor> F. Fogelman and P. Gallinari, Eds., </editor> <address> Paris, </address> <year> 1995, </year> <pages> pp. 53-60, </pages> <publisher> EC2 & Cie. </publisher>
Reference-contexts: Below each image is displayed the correct answers (left) and the network answer (right). These errors are mostly caused either by genuinely ambiguous patterns, or by digits written in a style that are underrepresented in the training set. early subset of these results was presented in <ref> [48] </ref>. The error rates on the test set for the various methods are shown in figure 9. C.1 Linear Classifier, and Pairwise Linear Classifier Possibly the simplest classifier that one might consider is a linear classifier. Each input pixel value contributes to a weighted sum for each output unit.
Reference: [49] <author> I Guyon, I. Poujaud, L. Personnaz, G. Dreyfus, J. Denker, and Y. LeCun, </author> <title> "Comparing different neural net architectures for classifying handwritten digits," </title> <booktitle> in Proc. of IJCNN, Washing-ton DC. 1989, </booktitle> <volume> vol. II, </volume> <pages> pp. 127-132, </pages> <publisher> IEEE. </publisher>
Reference-contexts: The uncertainty in the quoted error rates is about 0.1%. sigmoid units, linear units, gradient descent learning, and learning by directly solving linear systems gave similar results. A simple improvement of the basic linear classifier was tested <ref> [49] </ref>. The idea is to train each unit of a single-layer network to separate each class from each other class. In our case this layer comprises 45 units labeled 0/1, 0/2,...0/9, 1/2....8/9.
Reference: [50] <author> R. Ott, </author> <title> "construction of quadratic polynomial classifiers," </title> <booktitle> in Proc. of International Conference on Pattern Recognition. </booktitle> <year> 1976, </year> <pages> pp. 161-165, </pages> <publisher> IEEE. </publisher>
Reference-contexts: C.3 Principal Component Analysis and Polynomial Clas sifier Following <ref> [50] </ref>, [51], a preprocessing stage was constructed which computes the projection of the input pattern on the 40 principal components of the set of training vectors. To compute the principal components, the mean of each input component was first computed and subtracted from the training vectors.
Reference: [51] <author> J. Schurmann, </author> <title> "A multi-font word recognition system for postal address reading," </title> <journal> IEEE Transactions, </journal> <volume> vol. G-27, no. 3, </volume> <year> 1978. </year>
Reference-contexts: C.3 Principal Component Analysis and Polynomial Clas sifier Following [50], <ref> [51] </ref>, a preprocessing stage was constructed which computes the projection of the input pattern on the 40 principal components of the set of training vectors. To compute the principal components, the mean of each input component was first computed and subtracted from the training vectors.
Reference: [52] <author> Y. Lee, </author> <title> "Handwritten digit recognition using k-nearest neighbor, radial-basis functions, and backpropagation neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 3, no. 3, </volume> <year> 1991. </year>
Reference-contexts: This classifier can be seen as a linear classifier with 821 inputs, preceded by a module that computes all products of pairs of input variables. The error on the regular test set was 3.3%. C.4 Radial Basis Function Network Following <ref> [52] </ref>, an RBF network was constructed. The first layer was composed of 1,000 Gaussian RBF units with 28x28 inputs, and the second layer was a simple 1000 inputs SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 13 / 10 outputs linear classifier.
Reference: [53] <author> D. Saad and S. A. Solla, </author> <title> "Dynamics of on-line gradient descent learning for multilayer neural networks," </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, Eds. </editor> <booktitle> 1996, </booktitle> <volume> vol. 8, </volume> <pages> pp. 302-308, </pages> <publisher> The MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: We conjecture that the dynamics of gradient descent learning in multilayer nets has a "self-regularization" effect. Because the origin of weight space is a saddle point that is attractive in almost every direction, the weights invariably shrink during the first few epochs (recent theoretical analysis seem to confirm this <ref> [53] </ref>). Small weights cause the sigmoids to operate in the quasi-linear region, making the network essentially equivalent to a low-capacity, single-layer network. As the learning proceeds, the weights grow, which progressively increases the effective capacity of the network.
Reference: [54] <author> G. Cybenko, </author> <title> "Approximation by superpositions of sigmoidal functions," Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 303-314, </pages> <year> 1989. </year>
Reference-contexts: C.6 Two-Hidden Layer Fully Connected Multilayer Neural Network To see the effect of the architecture, several two-hidden layer multilayer neural networks were trained. Theoretical results have shown that any function can be approximated by a one-hidden layer neural network <ref> [54] </ref>. However, several authors have observed that two-hidden layer architectures sometimes yield better performance in practical situations. This phenomenon was also observed here. The test error rate of a 28x28-300-100-10 network was 3.05%, a much better result than the one-hidden layer network obtained using marginally more weights and connections.
Reference: [55] <author> L. Bottou and V. N. Vapnik, </author> <title> "Local learning algorithms," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 6, </volume> <pages> pp. 888-900, </pages> <year> 1992. </year>
Reference-contexts: LeNet-4 contains about 260,000 connections and has about 17,000 free parameters. Test error was 1.1%. In a series of experiments, we replaced the last layer of LeNet-4 with a Euclidean Nearest Neighbor classifier, and with the "local learning" method of Bottou and Vapnik <ref> [55] </ref>, in which a local linear classifier is retrained each time a new test pattern is shown. Neither of those methods improved the raw error rate, although they did improve the rejection performance. C.9 Boosted LeNet-4 Following theoretical work by R.
Reference: [56] <author> R. E. Schapire, </author> <title> "The strength of weak learnability," </title> <journal> Machine Learning, </journal> <volume> vol. 5, no. 2, </volume> <pages> pp. 197-227, </pages> <year> 1990. </year>
Reference-contexts: Neither of those methods improved the raw error rate, although they did improve the rejection performance. C.9 Boosted LeNet-4 Following theoretical work by R. Schapire <ref> [56] </ref>, Drucker et al. [57] developed the "boosting" method for combining multiple classifiers.
Reference: [57] <author> H. Drucker, R. Schapire, and P. Simard, </author> <title> "Improving performance in neural networks using a boosting algorithm," </title> <booktitle> in Advances in Neural Information Processing Systems 5, </booktitle> <editor> S. J. Han-son, J. D. Cowan, and C. L. Giles, Eds., </editor> <address> San Mateo, CA, </address> <year> 1993, </year> <pages> pp. 42-49, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Neither of those methods improved the raw error rate, although they did improve the rejection performance. C.9 Boosted LeNet-4 Following theoretical work by R. Schapire [56], Drucker et al. <ref> [57] </ref> developed the "boosting" method for combining multiple classifiers.
Reference: [58] <author> P. Simard, Y. LeCun, and Denker J., </author> <title> "Efficient pattern recognition using a new transformation distance," </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> S. Hanson, J. Cowan, and L. Giles, Eds., </editor> <volume> vol. 5. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: C.10 Tangent Distance Classifier (TDC) The Tangent Distance classifier (TDC) is a nearest-neighbor method where the distance function is made insensitive to small distortions and translations of the input image <ref> [58] </ref>. If we consider an image as a point in a high dimensional pixel space (where the dimensionality equals the number of pixels), then an evolving distortion of a character traces out a curve in pixel space. Taken together, all these distortions define a low-dimensional manifold in pixel space.
Reference: [59] <author> B. Boser, I. Guyon, and V. Vapnik, </author> <title> "A training algorithm for optimal margin classifiers," </title> <booktitle> in Proceedings of the Fifth Annual Workshop on Computational Learning Theory, 1992, </booktitle> <volume> vol. 5, </volume> <pages> pp. 144-152. </pages>
Reference-contexts: A particularly interesting subset of decision surfaces is the ones that correspond to hyperplanes that are at a maximum distance from the convex hulls of the two classes in the high-dimensional space of the product terms. Boser, Guyon, and Vapnik <ref> [59] </ref> realized that any polynomial of degree k in this "maximum margin" set can be computed by first computing the dot product of the input image with a subset of the training samples (called the "support vectors"), elevating the result to the k-th power, and linearly combining the numbers thereby obtained.
Reference: [60] <author> C. J. C. Burges and B. Schoelkopf, </author> <title> "Improving the accuracy and speed of support vector machines," </title> <booktitle> in Advances in Neural Information Processing Systems 9, </booktitle> <editor> M. Jordan M. Mozer and T. Petsche, Eds. </editor> <booktitle> 1997, </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge. </address>
Reference-contexts: Finding the support vectors and the coefficients amounts to solving a high-dimensional quadratic minimization problem with linear inequality constraints. For the sake of comparison, we include here the results obtained by Burges and Scholkopf reported in <ref> [60] </ref>. With a regular SVM, their error rate on the regular test set was 1.4%. Cortes and Vapnik had reported an error rate of 1.1% with SVM on the same data using a slightly different technique. <p> Percent of test patterns rejected to achieve 0.5% error on the remaining test examples for some of the systems. Fig. 11. Number of multiply-accumulate operations for the recogni tion of a single character starting with a size-normalized image. (RS-SVM), which attained 1.1% on the regular test set <ref> [60] </ref>, with a computational cost of only 650,000 multiply-adds per recognition, i.e. only about 60% more expensive than LeNet-5. D. Discussion A summary of the performance of our classifiers is shown in Figures 9 to 12.
Reference: [61] <author> Eduard Sackinger, Bernhard Boser, Jane Bromley, Yann Le-Cun, and Lawrence D. Jackel, </author> <title> "Application of the ANNA neural network chip to high-speed character recognition," </title> <journal> IEEE Transaction on Neural Networks, </journal> <volume> vol. 3, no. 2, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Convolutional Neural Networks are particularly well suited to hardware implementations because of their regular structure and their low memory requirements for the weights. Single chip mixed analog-digital implementations of LeNet-5's predecessors have been shown to operate at speeds in excess of 1000 characters per second <ref> [61] </ref>. However, the rapid progress of mainstream computer technology renders those exotic technologies quickly obsolete. Cost-effective implementations of memory-based techniques are more elusive, due to their enormous memory requirements, and computational requirements. Training time was also measured. K-nearest neighbors and TDC have essentially zero training time.
Reference: [62] <author> J. S. Bridle, </author> <title> "Probabilistic interpretation of feedforward classification networks outputs, with relationship to statistical pattern recognition," in Neurocomputing, Algorithms, Architectures and Applications, </title> <editor> F. Fogelman, J. Herault, and Y. Burnod, Eds., Les Arcs, </editor> <address> France, 1989, </address> <publisher> Springer. </publisher>
Reference-contexts: Commonly used modules include matrix multiplications and sigmoidal modules, the combination of which can be used to build conventional neural networks. Other modules include convolutional layers, sub-sampling layers, RBF layers, and "softmax" layers <ref> [62] </ref>. Loss functions are also represented as modules whose single output produces the value of the loss. Commonly used modules have simple bprop methods. In general, the bprop method of a function F is a multiplication by the Jacobian of F . Here are a few commonly used examples.
Reference: [63] <author> Y. LeCun, L. Bottou, and Y. Bengio, </author> <title> "Reading checks with graph transformer networks," </title> <booktitle> in International Conference on Acoustics, Speech, and Signal Processing, Munich, 1997, </booktitle> <volume> vol. 1, </volume> <pages> pp. 151-154, </pages> <publisher> IEEE. </publisher>
Reference-contexts: Such modules are called Graph Transformers, and the complete systems are called Graph Transformer Networks, or GTN. Modules in a GTN communicate their states and gradients in the form of directed graphs whose arcs carry numerical information (scalars or vectors) <ref> [63] </ref>. From the statistical point of view, the fixed-size state vectors of conventional networks can be seen as representing the means of distributions in state space. In variable-size networks such as the Space-Displacement Neural Networks described in section VII, the states are variable-length sequences of fixed size vectors.
Reference: [64] <author> Y. Bengio, </author> <title> Neural Networks for Speech and Sequence Recognition, </title> <publisher> International Thompson Computer Press, </publisher> <address> London, UK, </address> <year> 1996. </year>
Reference-contexts: However, it is almost impossible to devise image analysis techniques that will infallibly segment naturally written sequences of characters into well formed characters. The recent history of automatic speech recognition [26], <ref> [64] </ref> is here to remind us that training a recognizer by optimizing a global criterion (at the word or sentence level) is much preferable to merely training it on hand-segmented phonemes or other units. <p> Although separate training is simple, it requires additional supervision information that is often lacking or incomplete (the correct segmentation and incorrect candidate segments). Furthermore it can be shown that separate training is sub-optimal <ref> [64] </ref>. A proof sketch is given in Appendix D. The following section describes three different gradient-based methods for training GTN-based handwriting recog-nizers at the string level: Viterbi training, discriminative Viterbi training, forward training, and discriminative forward training. <p> In Section IX we present experimental results with an on-line handwriting recognition system that confirm the advantages of using global training versus separate training. Experiments in speech recognition with hybrids of neural networks and HMMs also showed marked improvements brought by global training [79], [27], <ref> [64] </ref>, [80]. VII. Multiple Object Recognition: Space Displacement Neural Network . There is a simple alternative to explicitly segmenting images of character strings using heuristics. <p> This is somewhat equivalent to modeling the output of an SDNN with a Hidden Markov Model. Globally trained, variable-size TDNN/HMM hybrids have been used for speech recognition and on-line handwriting recognition [79], [88], [89], <ref> [64] </ref>. Space Displacement Neural Networks have been used in combination with HMMs or other elastic matching methods for handwritten word recognition [81], [90]. SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 29 Fig. 26. An SDNN applied to a noisy image of digit string. <p> A. Previous Work Numerous authors in speech recognition have used Gradient-Based Learning methods that integrate graph-based statistical models (notably HMM) with acoustic recognition modules, mainly Gaussian mixture models, but also neural networks [96], [83], [97], <ref> [64] </ref>. Similar ideas have been applied to handwriting recognition (see [65] for a review). However, there has been no proposal for a systematic approach to multi-layer graph-based trainable systems.
Reference: [65] <author> Y. Bengio, Y. LeCun, C. Nohl, and C. Burges, "Lerec: </author> <title> A NN/HMM hybrid for on-line handwriting recognition," </title> <journal> Neural Computation, </journal> <volume> vol. 7, no. 5, </volume> <year> 1995. </year>
Reference-contexts: Several recent works have shown that the same is true for handwriting recognition <ref> [65] </ref>: optimizing a word-level criterion is preferable to solely training a recognizer on pre-segmented characters because the recognizer can learn not only to recognize individual characters, but also to reject mis-segmented characters thereby minimizing the overall word error. <p> The above training procedure can be equivalently formulated in term of HMM. Early experiments in SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 30 zip code recognition [81], and more recent experiments in on-line handwriting recognition <ref> [65] </ref> have demonstrated the idea of globally-trained SDNN/HMM hybrids. SDNN is an extremely promising and attractive technique for OCR, but so far it has not yielded better results than Heuristic Over-Segmentation. We hope that these results will improve as more experience is gained with these models. D. <p> A. Previous Work Numerous authors in speech recognition have used Gradient-Based Learning methods that integrate graph-based statistical models (notably HMM) with acoustic recognition modules, mainly Gaussian mixture models, but also neural networks [96], [83], [97], [64]. Similar ideas have been applied to handwriting recognition (see <ref> [65] </ref> for a review). However, there has been no proposal for a systematic approach to multi-layer graph-based trainable systems. The idea of transforming graphs into other graphs has received considerable interest in computer science, through the concept of weighted finite-state transducers [85]. <p> For this purpose we have designed a representation scheme, called AMAP <ref> [65] </ref>, where pen trajectories are represented by low-resolution images in which each picture element contains information about the local properties of the trajectory.
Reference: [66] <author> C. Burges, O. Matan, Y. LeCun, J. Denker, L. Jackel, C. Ste-nard, C. Nohl, and J. Ben, </author> <title> "Shortest path segmentation: A method for training a neural network to recognize character strings," </title> <booktitle> in International Joint Conference on Neural Networks, Baltimore, 1992, </booktitle> <volume> vol. 3, </volume> <pages> pp. 165-172. </pages>
Reference-contexts: The method avoids the expensive and unreliable task of hand-truthing the result of the segmentation often required in more traditional systems trained on individually labeled character images. A. Segmentation Graph A now-classical method for word segmentation and recognition is called Heuristic Over-Segmentation <ref> [66] </ref>, [67]. Its main advantages over other approaches to segmentation are that it avoids making hard decisions about the segmentation by taking a large number of different segmentations into consideration. The idea is to use heuristic image pro Fig. 16.
Reference: [67] <author> T. M. Breuel, </author> <title> "A system for the off-line recognition of handwritten text," </title> <editor> in ICPR'94, IEEE, Ed., </editor> <address> Jerusalem 1994, </address> <year> 1994, </year> <pages> pp. 129-134. </pages>
Reference-contexts: The method avoids the expensive and unreliable task of hand-truthing the result of the segmentation often required in more traditional systems trained on individually labeled character images. A. Segmentation Graph A now-classical method for word segmentation and recognition is called Heuristic Over-Segmentation [66], <ref> [67] </ref>. Its main advantages over other approaches to segmentation are that it avoids making hard decisions about the segmentation by taking a large number of different segmentations into consideration. The idea is to use heuristic image pro Fig. 16.
Reference: [68] <author> A. </author> <title> Viterbi, "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm," </title> <journal> IEEE Transactions on Information Theory, </journal> <pages> pp. 260-269, </pages> <year> 1967. </year>
Reference-contexts: The result of the recognition can be produced by reading off the labels of the arcs along the graph G vit extracted by the Viterbi transformer. The Viterbi transformer owes its name to the famous Viterbi algorithm <ref> [68] </ref>, an application of the principle of dynamic programming to find the shortest path Fig. 18. The recognition transformer refines each arc of the segmen tation arc into a set of arcs in the interpretation graph, one per character class, with attached penalties and labels. in a graph efficiently.
Reference: [69] <author> Lippmann R. P. and Gold B., </author> <title> "Neural-net classifiers useful for speech recognition," </title> <booktitle> in Proceedings of the IEEE First International Conference on Neural Networks, </booktitle> <address> San Diego, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Viterbi training, though formulated differently, is often use in HMM-based speech recognition systems [26]. Similar algorithms have been applied to speech recognition systems that integrate neural networks with time alignment <ref> [69] </ref>, [70], [71] or hybrid neural-network/HMM systems [27], [72], [73]. While it seems simple and satisfying, this training architecture has a flaw that can potentially be fatal. The problem was already mentioned in Section II-C.
Reference: [70] <author> H. Sakoe, R. Isotani, K. Yoshida, K. Iso, and T. Watan-abe, </author> <title> "Speaker-independent word recognition using dynamic programming neural networks," </title> <booktitle> in International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Glasgow, </address> <year> 1989. </year>
Reference-contexts: Viterbi training, though formulated differently, is often use in HMM-based speech recognition systems [26]. Similar algorithms have been applied to speech recognition systems that integrate neural networks with time alignment [69], <ref> [70] </ref>, [71] or hybrid neural-network/HMM systems [27], [72], [73]. While it seems simple and satisfying, this training architecture has a flaw that can potentially be fatal. The problem was already mentioned in Section II-C.
Reference: [71] <author> X. Driancourt and L. Bottou, </author> <title> "MLP, LVQ and DP: Comparison & cooperation," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks, </booktitle> <address> Seattle, </address> <year> 1991. </year>
Reference-contexts: Viterbi training, though formulated differently, is often use in HMM-based speech recognition systems [26]. Similar algorithms have been applied to speech recognition systems that integrate neural networks with time alignment [69], [70], <ref> [71] </ref> or hybrid neural-network/HMM systems [27], [72], [73]. While it seems simple and satisfying, this training architecture has a flaw that can potentially be fatal. The problem was already mentioned in Section II-C. <p> The arc had a low penalty, but should have had a higher penalty since it is not part of the desired answer. Variations of this technique have been used for the speech recognition. Driancourt and Bottou <ref> [71] </ref> used a version of it where the loss function is saturated to a fixed value. This can be seen as a generalization of the LVQ-2 loss function [74]. Other variations of this method use not only the Viterbi path, but the K-best paths.
Reference: [72] <author> M. A. Franzini, K. F. Lee, and A. H. Waibel, </author> <title> "Connectionist viterbi training: a new hybrid method for continuous speech recognition," </title> <booktitle> in International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Albuquerque, NM, </address> <year> 1990, </year> <pages> pp. 425-428. </pages>
Reference-contexts: Viterbi training, though formulated differently, is often use in HMM-based speech recognition systems [26]. Similar algorithms have been applied to speech recognition systems that integrate neural networks with time alignment [69], [70], [71] or hybrid neural-network/HMM systems [27], <ref> [72] </ref>, [73]. While it seems simple and satisfying, this training architecture has a flaw that can potentially be fatal. The problem was already mentioned in Section II-C.
Reference: [73] <author> L. T. Niles and H. F. Silverman, </author> <title> "Combining hidden markov models and neural network classifiers," </title> <booktitle> in International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Albu-querque, NM, </address> <year> 1990, </year> <pages> pp. 417-420. </pages>
Reference-contexts: Viterbi training, though formulated differently, is often use in HMM-based speech recognition systems [26]. Similar algorithms have been applied to speech recognition systems that integrate neural networks with time alignment [69], [70], [71] or hybrid neural-network/HMM systems [27], [72], <ref> [73] </ref>. While it seems simple and satisfying, this training architecture has a flaw that can potentially be fatal. The problem was already mentioned in Section II-C.
Reference: [74] <author> T. Kohonen, G. Barna, and R. Chrisley, </author> <title> "Statistical pattern recognition with neural network: Benchmarking studies," </title> <booktitle> in Proceedings of the IEEE Second International Conference on Neural Networks, </booktitle> <address> San Diego, </address> <booktitle> 1988, </booktitle> <volume> vol. 1, </volume> <pages> pp. 61-68. </pages>
Reference-contexts: Variations of this technique have been used for the speech recognition. Driancourt and Bottou [71] used a version of it where the loss function is saturated to a fixed value. This can be seen as a generalization of the LVQ-2 loss function <ref> [74] </ref>. Other variations of this method use not only the Viterbi path, but the K-best paths. The Discriminative Viterbi algorithm does not have the flaws of the non-discriminative version, but there are problems nonetheless. The main problem is that the criterion does not build a margin between the classes.
Reference: [75] <author> J. S. Bridle, "Alphanets: </author> <title> a recurrent `neural' network architecture with a hidden markov model interpretation," Speech Communication, </title> <year> 1990. </year>
Reference-contexts: The derivatives with respect to the other arcs are 0. Several authors have applied the idea of back-propagating gradients through a forward scorer to train speech recognition systems, including Bridle and his ff-net model <ref> [75] </ref> and Haffner and his fffi-TDNN model [76], but these authors recommended discriminative training as in the next section. D. Discriminative Forward Training The information contained in the forward penalty can be used in another discriminative training criterion which we will call the discriminative forward criterion.
Reference: [76] <author> P. Haffner, </author> <title> "Connectionist speech recognition with a global MMI algorithm," </title> <booktitle> in EUROSPEECH'93, 3rd European Conference on Speech Communication and Technology, </booktitle> <address> Berlin, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: The derivatives with respect to the other arcs are 0. Several authors have applied the idea of back-propagating gradients through a forward scorer to train speech recognition systems, including Bridle and his ff-net model [75] and Haffner and his fffi-TDNN model <ref> [76] </ref>, but these authors recommended discriminative training as in the next section. D. Discriminative Forward Training The information contained in the forward penalty can be used in another discriminative training criterion which we will call the discriminative forward criterion.
Reference: [77] <author> J. S. Denker and C. J. Burges, </author> <title> "Image segmentation and recognition," in The Mathematics of Induction. 1995, </title> <publisher> Addison Wes-ley. </publisher>
Reference-contexts: Let us first discuss the first case (class posteriors normalization). This local normalizatin of penalties may eliminate information that is important for locally rejecting all the classes <ref> [77] </ref>, e.g., when a piece of image does not correspond to a valid character class, because some of the segmentation candidates may be wrong. <p> Although an explicit "garbage class" can be introduced in a probabilistic framework to address that question, some problems remain because it is difficult to characterize such a class probabilistically and to train a system in this way (it would require a density model of unseen or unlabeled). Following <ref> [77] </ref>, we therefore prefer to postpone normalization as far as possible (in fact, until the final decision stage of the system). Without normalization, the quantities manipulated in the system do not have a direct probabilistic interpretation. Let us now discuss the second case (using a generative model of the input).
Reference: [78] <author> L. Bottou, Une Approche theorique de l'Apprentissage Connex-ionniste: </author> <title> Applications a la Reconnaissance de la Parole, </title> <type> Ph.D. thesis, </type> <institution> Universite de Paris XI, </institution> <address> 91405 Orsay cedex, France, </address> <year> 1991. </year>
Reference-contexts: The sum of those posteriors for all the possible label sequences is 1. The discriminative forward loss function is a continuous approximation of the expected number of classification errors over the training set <ref> [78] </ref>. We will see in Section X-C that this is a good way to obtain scores on which to base a rejection strategy. The important point being made here is that one is free to choose any parameterization deemed appropriate for a classification model.
Reference: [79] <author> Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, </author> <title> "Global optimization of a neural network-hidden Markov model hybrid," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 252-259, </pages> <year> 1992. </year>
Reference-contexts: In Section IX we present experimental results with an on-line handwriting recognition system that confirm the advantages of using global training versus separate training. Experiments in speech recognition with hybrids of neural networks and HMMs also showed marked improvements brought by global training <ref> [79] </ref>, [27], [64], [80]. VII. Multiple Object Recognition: Space Displacement Neural Network . There is a simple alternative to explicitly segmenting images of character strings using heuristics. <p> This is somewhat equivalent to modeling the output of an SDNN with a Hidden Markov Model. Globally trained, variable-size TDNN/HMM hybrids have been used for speech recognition and on-line handwriting recognition <ref> [79] </ref>, [88], [89], [64]. Space Displacement Neural Networks have been used in combination with HMMs or other elastic matching methods for handwritten word recognition [81], [90]. SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 29 Fig. 26. An SDNN applied to a noisy image of digit string. <p> Even lower error rates can be obtained by drastically reducing the size of the dictionary to 350 words, yielding 1.6% and 0.94% word and character errors. These results clearly demonstrate the usefulness of globally trained Neural-Net/HMM hybrids for handwriting recognition. This confirms similar results obtained earlier in speech recognition <ref> [79] </ref>. X. A Check Reading System This section describes a GTN based Check Reading System, intended for immediate industrial deployment. It also shows how the use of Gradient Based-Learning and GTNs make this deployment fast and cost-effective while yielding an accurate and reliable solution.
Reference: [80] <author> M. Rahim, Y. Bengio, and Y. LeCun, </author> <title> "Discriminative feature and model design for automatic speech recognition," </title> <booktitle> in Proc. of Eurospeech, </booktitle> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: In Section IX we present experimental results with an on-line handwriting recognition system that confirm the advantages of using global training versus separate training. Experiments in speech recognition with hybrids of neural networks and HMMs also showed marked improvements brought by global training [79], [27], [64], <ref> [80] </ref>. VII. Multiple Object Recognition: Space Displacement Neural Network . There is a simple alternative to explicitly segmenting images of character strings using heuristics.
Reference: [81] <author> O. Matan, J. Bromley, C. Burges, J. Denker, L. Jackel, Y. Le-Cun, E Pednault, W. Satterfield, C Stenard, and T. Thompson, </author> <title> "Reading handwritten digits: A zip code recognition system.," </title> <booktitle> IEEE Computer, </booktitle> <month> July </month> <year> 1992. </year>
Reference-contexts: These properties take care of the latter two problems mentioned in the previous paragraph. Second, convolutional networks provide a drastic saving in computational requirement when replicated over large input fields. A replicated convolutional network, also called a Space Displacement Neural Network or SDNN <ref> [81] </ref>, is shown in Figure 23. While scanning a recognizer can be prohibitively expensive in general, convolutional networks can be scanned or replicated very efficiently over large, variable-size input fields. Consider one instance of a convolutional net and its alter ego at a nearby location. <p> Although the idea of SDNN is quite old, and very attractive by its simplicity, it has not generated wide interest until recently because as stated above it puts enormous demands on the recognizer [82], <ref> [81] </ref>. In speech recognition, where the recognizer is at least one order of magnitude smaller, replicated convolutional networks are easier to implement, for instance in Haffner's Multi-State TDNN model [83], [84]. A. <p> Globally trained, variable-size TDNN/HMM hybrids have been used for speech recognition and on-line handwriting recognition [79], [88], [89], [64]. Space Displacement Neural Networks have been used in combination with HMMs or other elastic matching methods for handwritten word recognition <ref> [81] </ref>, [90]. SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 29 Fig. 26. An SDNN applied to a noisy image of digit string. The digits shown in the SDNN output represent the winning class labels, with a lighter grey level for high-penalty answers. Fig. 27. <p> The above training procedure can be equivalently formulated in term of HMM. Early experiments in SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 30 zip code recognition <ref> [81] </ref>, and more recent experiments in on-line handwriting recognition [65] have demonstrated the idea of globally-trained SDNN/HMM hybrids. SDNN is an extremely promising and attractive technique for OCR, but so far it has not yielded better results than Heuristic Over-Segmentation.
Reference: [82] <author> J. Keeler, D. Rumelhart, and W. K. Leow, </author> <title> "Integrated segmentation and recognition of hand-printed numerals," </title> <booktitle> in Neural Information Processing Systems, </booktitle> <editor> R. P. Lippmann, J. M. Moody, and D. S. Touretzky, Eds., </editor> <volume> vol. 3, </volume> <pages> pp. 557-563. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: The SDNN architecture seems particularly attractive for recognizing cursive handwriting where no reliable segmentation heuristic exists. Although the idea of SDNN is quite old, and very attractive by its simplicity, it has not generated wide interest until recently because as stated above it puts enormous demands on the recognizer <ref> [82] </ref>, [81]. In speech recognition, where the recognizer is at least one order of magnitude smaller, replicated convolutional networks are easier to implement, for instance in Haffner's Multi-State TDNN model [83], [84]. A.
Reference: [83] <author> P. Haffner and A. H. Waibel, </author> <title> "Multi-state time-delay neural networks for continuous speech recognition," </title> <booktitle> in Advances in Neural Information Processing Systems. 1992, </booktitle> <volume> vol. 4, </volume> <pages> pp. 579-588, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: In speech recognition, where the recognizer is at least one order of magnitude smaller, replicated convolutional networks are easier to implement, for instance in Haffner's Multi-State TDNN model <ref> [83] </ref>, [84]. A. Interpreting the Output of an SDNN with a GTN The output of an SDNN is a sequence of vectors which encode the likelihoods, penalties, or scores of finding character of a particular class label at the corresponding location in the input. <p> A. Previous Work Numerous authors in speech recognition have used Gradient-Based Learning methods that integrate graph-based statistical models (notably HMM) with acoustic recognition modules, mainly Gaussian mixture models, but also neural networks [96], <ref> [83] </ref>, [97], [64]. Similar ideas have been applied to handwriting recognition (see [65] for a review). However, there has been no proposal for a systematic approach to multi-layer graph-based trainable systems.
Reference: [84] <author> U. Bodenhausen, S. Manke, and A. Waibel, </author> <title> "Connectionist architectural learning for high performance character and speech recognition," </title> <booktitle> in International Conference on Acoustics, Speech, and Signal Processing, Minneapolis, 1993, </booktitle> <volume> vol. </volume> <pages> 1. </pages>
Reference-contexts: In speech recognition, where the recognizer is at least one order of magnitude smaller, replicated convolutional networks are easier to implement, for instance in Haffner's Multi-State TDNN model [83], <ref> [84] </ref>. A. Interpreting the Output of an SDNN with a GTN The output of an SDNN is a sequence of vectors which encode the likelihoods, penalties, or scores of finding character of a particular class label at the corresponding location in the input.
Reference: [85] <author> F. Pereira, M. Riley, and R. Sproat, </author> <title> "Weighted rational trans-ductions and their application to human language processing," </title> <booktitle> in ARPA Natural Language Processing workshop, </booktitle> <year> 1994. </year>
Reference-contexts: This graph is called the SDNN Output Graph. The second input graph to the transformer is a grammar transducer, more specifically a finite-state transducer <ref> [85] </ref>, that encodes the relationship between input strings of class labels and corresponding output strings of recognized characters. The graph transformer shown in figure 24 performs a composition between the recognition graph and the grammar transducer. <p> However, there has been no proposal for a systematic approach to multi-layer graph-based trainable systems. The idea of transforming graphs into other graphs has received considerable interest in computer science, through the concept of weighted finite-state transducers <ref> [85] </ref>. Transducers have been applied to speech recognition [98] and language translation [99], and proposals have been made for handwriting recognition [100]. <p> What is proposed in this paper is a systematic approach to automatic training in graph-manipulating systems. A different approach to graph-based trainable systems, called Input-Output HMM, was proposed in [102], [103]. B. Standard Transduction In the established framework of finite-state transducers <ref> [85] </ref>, discrete symbols are attached to arcs in the graphs. Acceptor graphs have a single symbol attached to each arc whereas transducer graphs have two symbols (an input symbol and an output symbol). <p> On the other hand, Graph Transformer Networks extend HMMs by allowing to combine in a well-principled framework multiple levels of processing, or multiple models (e.g., Pereira et al. have been using the transducer framework for stacking HMMs representing different levels of processing in automatic speech recognition <ref> [85] </ref>). Unfolding a HMM in time yields a graph that is very similar to our interpretation graph (at the final stage of processing of the Graph Transformer Network, before Viterbi recognition). It has nodes n (t; i) associated to each time step t and state i in the model.
Reference: [86] <author> M. Lades, J. C. Vorbruggen, J. Buhmann, and C. von der Mals-burg, </author> <title> "Distortion invariant object recognition in the dynamic SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 45 link architecture," </title> <journal> IEEE Trans. Comp., </journal> <volume> vol. 42, no. 3, </volume> <pages> pp. 300-311, </pages> <year> 1993. </year>
Reference-contexts: Standard techniques based on Heuristic Over-Segmentation would fail miserably on many of those examples. As can be seen on these examples, the network exhibits striking invariance and noise resistance properties. While some authors have argued that invariance requires more sophisticated models than feed-forward neural networks <ref> [86] </ref>, LeNet-5 exhibits these properties to a large extent. Similarly, it has been suggested that accurate recognition of multiple overlapping objects require explicit mechanisms that would solve the so-called feature binding problem [86]. <p> While some authors have argued that invariance requires more sophisticated models than feed-forward neural networks <ref> [86] </ref>, LeNet-5 exhibits these properties to a large extent. Similarly, it has been suggested that accurate recognition of multiple overlapping objects require explicit mechanisms that would solve the so-called feature binding problem [86]. As can be seen on Figures 25 and 26, the network is able to tell the characters apart, even when they are closely intertwined, a task that would be impossible to achieve with the more classical Heuristic Over-Segmentation technique.
Reference: [87] <author> B. Boser, E. Sackinger, J. Bromley, Y. LeCun, and L. Jackel, </author> <title> "An analog neural network processor with programmable topology," </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <volume> vol. 26, no. 12, </volume> <pages> pp. 2017-2025, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Another important advantage of SDNN is the ease with which they can be implemented on parallel hardware. Specialized analog/digital chips have been designed and used in character recognition, and in image preprocessing applications <ref> [87] </ref>. However the rapid progress of conventional processor technology with reduced-precision vector arithmetic instructions (such as Intel's MMX) make the success of specialized hardware hypothetical at best. Short video clips of the LeNet-5 SDNN can be viewed at http://www.research.att.com/~yann/ocr. C.
Reference: [88] <author> M. Schenkel, H. Weissman, I. Guyon, C. Nohl, and D. Hender-son, </author> <title> "Recognition-based segmentation of on-line hand-printed words," </title> <booktitle> in Advances in Neural Information Processing Systems 5, </booktitle> <editor> S. J. Hanson, J. D. Cowan, and C. L. Giles, Eds., </editor> <address> Denver, CO, </address> <year> 1993, </year> <pages> pp. 723-730. </pages>
Reference-contexts: This is somewhat equivalent to modeling the output of an SDNN with a Hidden Markov Model. Globally trained, variable-size TDNN/HMM hybrids have been used for speech recognition and on-line handwriting recognition [79], <ref> [88] </ref>, [89], [64]. Space Displacement Neural Networks have been used in combination with HMMs or other elastic matching methods for handwritten word recognition [81], [90]. SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 29 Fig. 26. An SDNN applied to a noisy image of digit string.
Reference: [89] <author> C. Dugast, L. Devillers, and X. Aubert, </author> <title> "Combining TDNN and HMM in a hybrid system for improved continuous-speech recognition," </title> <journal> IEEE Transactions on Speech and Audio Processing, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 217-224, </pages> <year> 1994. </year>
Reference-contexts: This is somewhat equivalent to modeling the output of an SDNN with a Hidden Markov Model. Globally trained, variable-size TDNN/HMM hybrids have been used for speech recognition and on-line handwriting recognition [79], [88], <ref> [89] </ref>, [64]. Space Displacement Neural Networks have been used in combination with HMMs or other elastic matching methods for handwritten word recognition [81], [90]. SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 29 Fig. 26. An SDNN applied to a noisy image of digit string.
Reference: [90] <author> Y. Bengio and Y. Le Cun, </author> <title> "Word normalization for on-line handwritten word recognition," </title> <booktitle> in Proc. of the International Conference on Pattern Recognition, </booktitle> <editor> IAPR, Ed., </editor> <address> Jerusalem, 1994, </address> <publisher> IEEE. </publisher>
Reference-contexts: Globally trained, variable-size TDNN/HMM hybrids have been used for speech recognition and on-line handwriting recognition [79], [88], [89], [64]. Space Displacement Neural Networks have been used in combination with HMMs or other elastic matching methods for handwritten word recognition [81], <ref> [90] </ref>. SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 29 Fig. 26. An SDNN applied to a noisy image of digit string. The digits shown in the SDNN output represent the winning class labels, with a lighter grey level for high-penalty answers. Fig. 27. <p> A. Preprocessing Input normalization reduces intra-character variability, simplifying character recognition. We have used a word normalization scheme <ref> [90] </ref> based on fitting a geometrical model of the word structure. Our model has four "flexible" lines representing respectively the ascenders line, the core line, the base line and the descenders line. The lines are fitted to local minima or maxima of the pen trajectory.
Reference: [91] <author> R. Vaillant, C. Monrocq, and Y. LeCun, </author> <title> "Original approach for the localization of objects in images," </title> <booktitle> IEE Proc on Vision, Image, and Signal Processing, </booktitle> <volume> vol. 141, no. 4, </volume> <month> August </month> <year> 1994. </year>
Reference-contexts: Since the size of the objects to be detected within the image are unknown, the image can be presented to the network at multiple resolutions, and the results at multiple resolutions combined. The idea has been applied to face location, <ref> [91] </ref>, address block location on envelopes [92], and hand tracking in video [93]. To illustrate the method, we will consider the case of face detection in images as described in [91]. First, images containing faces at various scales are collected. <p> The idea has been applied to face location, <ref> [91] </ref>, address block location on envelopes [92], and hand tracking in video [93]. To illustrate the method, we will consider the case of face detection in images as described in [91]. First, images containing faces at various scales are collected. Those images are filtered through a zero-mean Laplacian filter so as to remove variations in global illumination and low spatial frequency illumination gradients. Then, training samples of faces and non-faces are manually extracted from those images.
Reference: [92] <author> R. Wolf and J. Platt, </author> <title> "Postal address block location using a convolutional locator network," </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <editor> J. D. Cowan, G. Tesauro, and J. Alspector, Eds., </editor> <year> 1994, </year> <pages> pp. 745-752. </pages>
Reference-contexts: Since the size of the objects to be detected within the image are unknown, the image can be presented to the network at multiple resolutions, and the results at multiple resolutions combined. The idea has been applied to face location, [91], address block location on envelopes <ref> [92] </ref>, and hand tracking in video [93]. To illustrate the method, we will consider the case of face detection in images as described in [91]. First, images containing faces at various scales are collected.
Reference: [93] <author> S. Nowlan and J. Platt, </author> <title> "A convolutional neural network hand tracker," </title> <booktitle> in Advances in Neural Information Processing Systems 7, </booktitle> <editor> G. Tesauro, D. Touretzky, and T. Leen, Eds., </editor> <address> San Ma-teo, CA, </address> <year> 1995, </year> <pages> pp. 901-908, </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The idea has been applied to face location, [91], address block location on envelopes [92], and hand tracking in video <ref> [93] </ref>. To illustrate the method, we will consider the case of face detection in images as described in [91]. First, images containing faces at various scales are collected.
Reference: [94] <author> H. A. Rowley, S. Baluja, and T. Kanade, </author> <title> "Neural network-based face detection," </title> <booktitle> in Proceedings of CVPR'96. </booktitle> <year> 1996, </year> <pages> pp. 203-208, </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Each possible location is seen as an alternative interpretation, i.e. one of several parallel arcs in a simple graph that only contains a start node and an end node. Other authors have used Neural Networks, or other classifiers such as Support Vector Machines for face detection with great success <ref> [94] </ref>, [95]. Their systems are very similar to the one described above, including the idea of presenting the image to the network at multiple scales.
Reference: [95] <author> E. Osuna, R. Freund, and F. Girosi, </author> <title> "Training support vector machines: an application to face detection," </title> <booktitle> in Proceedings of CVPR'96. </booktitle> <year> 1997, </year> <pages> pp. 130-136, </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Other authors have used Neural Networks, or other classifiers such as Support Vector Machines for face detection with great success [94], <ref> [95] </ref>. Their systems are very similar to the one described above, including the idea of presenting the image to the network at multiple scales.
Reference: [96] <author> H. Bourlard and C. J. Wellekens, </author> <title> "Links between Markov models and multilayer perceptrons," </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> D. Touretzky, Ed., </editor> <booktitle> Denver, 1989, </booktitle> <volume> vol. 1, </volume> <pages> pp. 186-187, </pages> <publisher> Morgan-Kaufmann. </publisher>
Reference-contexts: A. Previous Work Numerous authors in speech recognition have used Gradient-Based Learning methods that integrate graph-based statistical models (notably HMM) with acoustic recognition modules, mainly Gaussian mixture models, but also neural networks <ref> [96] </ref>, [83], [97], [64]. Similar ideas have been applied to handwriting recognition (see [65] for a review). However, there has been no proposal for a systematic approach to multi-layer graph-based trainable systems.
Reference: [97] <author> Y. Bengio, R. De Mori, G. Flammia, and R. Kompe, </author> <title> "Neural network gaussian mixture hybrid for speech recognition or density estimation," </title> <booktitle> in Advances in Neural Information Processing Systems 4, </booktitle> <editor> J. E. Moody, S. J. Hanson, and R. P. Lippmann, Eds., </editor> <address> Denver, CO, </address> <year> 1992, </year> <pages> pp. 175-182, </pages> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: A. Previous Work Numerous authors in speech recognition have used Gradient-Based Learning methods that integrate graph-based statistical models (notably HMM) with acoustic recognition modules, mainly Gaussian mixture models, but also neural networks [96], [83], <ref> [97] </ref>, [64]. Similar ideas have been applied to handwriting recognition (see [65] for a review). However, there has been no proposal for a systematic approach to multi-layer graph-based trainable systems.
Reference: [98] <author> F. C. N. Pereira and M. Riley, </author> <title> "Speech recognition by composition of weighted finite automata," in Finite-State Devices for Natural Langue Processing, </title> <address> Cambridge, Massachusetts, 1997, </address> <publisher> MIT Press. </publisher>
Reference-contexts: However, there has been no proposal for a systematic approach to multi-layer graph-based trainable systems. The idea of transforming graphs into other graphs has received considerable interest in computer science, through the concept of weighted finite-state transducers [85]. Transducers have been applied to speech recognition <ref> [98] </ref> and language translation [99], and proposals have been made for handwriting recognition [100].
Reference: [99] <author> M. Mohri, </author> <title> "Finite-state transducers in language and speech processing," </title> <journal> Computational Linguistics, </journal> <volume> vol. 23, no. 2, </volume> <year> 1997. </year>
Reference-contexts: However, there has been no proposal for a systematic approach to multi-layer graph-based trainable systems. The idea of transforming graphs into other graphs has received considerable interest in computer science, through the concept of weighted finite-state transducers [85]. Transducers have been applied to speech recognition [98] and language translation <ref> [99] </ref>, and proposals have been made for handwriting recognition [100].
Reference: [100] <author> I. Guyon, M. Schenkel, and J. Denker, </author> <title> "Overview and synthesis of on-line cursive handwriting recognition techniques," in Handbook on Optical Character Recognition and Document Image Analysis, </title> <editor> P. S. P. Wang and Bunke H., Eds. </editor> <booktitle> 1996, </booktitle> <publisher> World Scientific. </publisher>
Reference-contexts: The idea of transforming graphs into other graphs has received considerable interest in computer science, through the concept of weighted finite-state transducers [85]. Transducers have been applied to speech recognition [98] and language translation [99], and proposals have been made for handwriting recognition <ref> [100] </ref>. This line of work has been mainly focused on efficient search algorithms [101] and on the algebraic aspects of combining transducers and graphs (called acceptors in this context), but very little effort has been devoted to building globally trainable systems out of transducers.
Reference: [101] <author> M. Mohri and M. Riley, </author> <title> "Weighted determinization and minimization for large vocabulary recognition," </title> <booktitle> in Proceedings of Eurospeech '97, </booktitle> <address> Rhodes, Greece, </address> <month> September </month> <year> 1997. </year>
Reference-contexts: Transducers have been applied to speech recognition [98] and language translation [99], and proposals have been made for handwriting recognition [100]. This line of work has been mainly focused on efficient search algorithms <ref> [101] </ref> and on the algebraic aspects of combining transducers and graphs (called acceptors in this context), but very little effort has been devoted to building globally trainable systems out of transducers. What is proposed in this paper is a systematic approach to automatic training in graph-manipulating systems.
Reference: [102] <author> Y. Bengio and P. Frasconi, </author> <title> "An input/output HMM architecture," </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> G. Tesauro, D Touretzky, and T. Leen, Eds. </editor> <booktitle> 1996, </booktitle> <volume> vol. 7, </volume> <pages> pp. 427-434, </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: What is proposed in this paper is a systematic approach to automatic training in graph-manipulating systems. A different approach to graph-based trainable systems, called Input-Output HMM, was proposed in <ref> [102] </ref>, [103]. B. Standard Transduction In the established framework of finite-state transducers [85], discrete symbols are attached to arcs in the graphs. Acceptor graphs have a single symbol attached to each arc whereas transducer graphs have two symbols (an input symbol and an output symbol).
Reference: [103] <author> Y. Bengio and P. Frasconi, </author> <title> "Input/Output HMMs for sequence processing," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> vol. 7, no. 5, </volume> <pages> pp. 1231-1249, </pages> <year> 1996. </year>
Reference-contexts: What is proposed in this paper is a systematic approach to automatic training in graph-manipulating systems. A different approach to graph-based trainable systems, called Input-Output HMM, was proposed in [102], <ref> [103] </ref>. B. Standard Transduction In the established framework of finite-state transducers [85], discrete symbols are attached to arcs in the graphs. Acceptor graphs have a single symbol attached to each arc whereas transducer graphs have two symbols (an input symbol and an output symbol). <p> On the other hand, if the probabilistic assumptions in an HMM (or other probabilistic model) are not realistic, discriminative training, discussed in Section VI, can improve performance as this has been clearly shown for speech recognition systems [45], [46], [47], [105], [106]. The Input-Output HMM model (IOHMM) <ref> [103] </ref>, [107], is strongly related to graph transformers. Viewed as a probabilistic model, an IOHMM represents the conditional distribution of output sequences given input sequences (of the same or a different length). It is parameterized from an emission probability module and a transition probability module.
Reference: [104] <author> M. Mohri, F. C. N. Pereira, and M. Riley, </author> <title> A rational design for a weighted finite-state transducer library, </title> <booktitle> Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1997. </year>
Reference-contexts: The above procedure produces a tree, but a simple technique described in Section VIII-C can be used to avoid generating multiple copies of certain subgraphs by detecting when a particular output state has already been seen. The transduction operation can be performed very efficiently <ref> [104] </ref>, but presents complex book-keeping problems concerning the handling of all combinations of null and non null symbols.
Reference: [105] <author> M. Rahim, C. H. Lee, and B. H. Juang, </author> <title> "Discriminative utterance verification for connected digits recognition," </title> <journal> IEEE Trans. on Speech & Audio Proc., </journal> <volume> vol. 5, </volume> <pages> pp. 266-277, </pages> <year> 1997. </year>
Reference-contexts: On the other hand, if the probabilistic assumptions in an HMM (or other probabilistic model) are not realistic, discriminative training, discussed in Section VI, can improve performance as this has been clearly shown for speech recognition systems [45], [46], [47], <ref> [105] </ref>, [106]. The Input-Output HMM model (IOHMM) [103], [107], is strongly related to graph transformers. Viewed as a probabilistic model, an IOHMM represents the conditional distribution of output sequences given input sequences (of the same or a different length).
Reference: [106] <author> M. Rahim, Y. Bengio, and Y. LeCun, </author> <title> "Discriminative feature and model design for automatic speech recognition," </title> <booktitle> in Eu-rospeech '97, </booktitle> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: On the other hand, if the probabilistic assumptions in an HMM (or other probabilistic model) are not realistic, discriminative training, discussed in Section VI, can improve performance as this has been clearly shown for speech recognition systems [45], [46], [47], [105], <ref> [106] </ref>. The Input-Output HMM model (IOHMM) [103], [107], is strongly related to graph transformers. Viewed as a probabilistic model, an IOHMM represents the conditional distribution of output sequences given input sequences (of the same or a different length).
Reference: [107] <author> S. Bengio and Y. Bengio, </author> <title> "An EM algorithm for asynchronous input/output hidden Markov models," </title> <booktitle> in International Conference On Neural Information Processing, </booktitle> <editor> L. Xu, Ed., Hong-Kong, </editor> <year> 1996, </year> <pages> pp. 328-334. </pages>
Reference-contexts: On the other hand, if the probabilistic assumptions in an HMM (or other probabilistic model) are not realistic, discriminative training, discussed in Section VI, can improve performance as this has been clearly shown for speech recognition systems [45], [46], [47], [105], [106]. The Input-Output HMM model (IOHMM) [103], <ref> [107] </ref>, is strongly related to graph transformers. Viewed as a probabilistic model, an IOHMM represents the conditional distribution of output sequences given input sequences (of the same or a different length). It is parameterized from an emission probability module and a transition probability module.
Reference: [108] <author> C. Tappert, C. Suen, and T. Wakahara, </author> <title> "The state of the art in on-line handwriting recognition," </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 8, no. 12, </volume> <pages> pp. 787-808, </pages> <year> 1990. </year>
Reference-contexts: The recognition of handwritten characters from a pen trajectory on a digitizing surface is often done in the time domain <ref> [108] </ref>, [41], [109]. Typically, trajectories are normalized, and local geometrical or dynamical features are extracted. The recognition may then be performed using curve matching [108], or other classification techniques such as TDNNs [41], [109]. <p> The recognition of handwritten characters from a pen trajectory on a digitizing surface is often done in the time domain <ref> [108] </ref>, [41], [109]. Typically, trajectories are normalized, and local geometrical or dynamical features are extracted. The recognition may then be performed using curve matching [108], or other classification techniques such as TDNNs [41], [109]. While these representations have several advantages, their dependence on stroke ordering and individual writing styles makes them difficult to Fig. 30.
Reference: [109] <author> S. Manke and U. Bodenhausen, </author> <title> "A connectionist recognizer for on-line cursive handwriting recognition," </title> <booktitle> in International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> Adelaide, </address> <year> 1994. </year>
Reference-contexts: The recognition of handwritten characters from a pen trajectory on a digitizing surface is often done in the time domain [108], [41], <ref> [109] </ref>. Typically, trajectories are normalized, and local geometrical or dynamical features are extracted. The recognition may then be performed using curve matching [108], or other classification techniques such as TDNNs [41], [109]. <p> handwritten characters from a pen trajectory on a digitizing surface is often done in the time domain [108], [41], <ref> [109] </ref>. Typically, trajectories are normalized, and local geometrical or dynamical features are extracted. The recognition may then be performed using curve matching [108], or other classification techniques such as TDNNs [41], [109]. While these representations have several advantages, their dependence on stroke ordering and individual writing styles makes them difficult to Fig. 30. An on-line handwriting recognition GTN based on heuristic over-segmentation use in high accuracy, writer independent systems that integrate the segmentation with the recognition.
Reference: [110] <author> M. Gilloux and M. Leroux, </author> <title> "Recognition of cursive script amounts on postal checks," </title> <booktitle> in European Conference dedicated to Postal Technologies, </booktitle> <address> Nantes, France, </address> <month> June </month> <year> 1993, </year> <pages> pp. 705-712. </pages>
Reference-contexts: The verification of the amount on a check is a task that is extremely time and money consuming for banks. As a consequence, there is a very high interest in automating the process as much as possible (see for example <ref> [110] </ref>, [111], [112]). Even a partial automation would result in considerable cost reductions. The threshold of economic viability for automatic check readers, as set by the bank, is when 50% of the checks are read with less than 1% error.
Reference: [111] <author> D. Guillevic and C. Y. Suen, </author> <title> "Cursive script recognition applied to the processing of bank checks," </title> <booktitle> in Int. Conf. on Document Analysis and Recognition, </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1995, </year> <pages> pp. 11-14. </pages>
Reference-contexts: The verification of the amount on a check is a task that is extremely time and money consuming for banks. As a consequence, there is a very high interest in automating the process as much as possible (see for example [110], <ref> [111] </ref>, [112]). Even a partial automation would result in considerable cost reductions. The threshold of economic viability for automatic check readers, as set by the bank, is when 50% of the checks are read with less than 1% error.
Reference: [112] <author> L. Lam, C. Y. Suen, D. Guillevic, N. W. Strathy, M. Cheriet, K. Liu, and J. N. Said, </author> <title> "Automatic processing of information on checks," </title> <booktitle> in Int. Conf. on Systems, Man & Cybernetics, </booktitle> <address> Vancouver, Canada, </address> <month> October </month> <year> 1995, </year> <pages> pp. 2353-2358. </pages>
Reference-contexts: The verification of the amount on a check is a task that is extremely time and money consuming for banks. As a consequence, there is a very high interest in automating the process as much as possible (see for example [110], [111], <ref> [112] </ref>). Even a partial automation would result in considerable cost reductions. The threshold of economic viability for automatic check readers, as set by the bank, is when 50% of the checks are read with less than 1% error.
Reference: [113] <author> C. J. C. Burges, J. I. Ben, J. S. Denker, Y. LeCun, and C. R. Nohl, </author> <title> "Off line recognition of handwritten postal words using neural networks," </title> <journal> Int. Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> vol. 7, no. 4, </volume> <pages> pp. 689, </pages> <year> 1993, </year> <title> Special Issue on Applications of Neural Networks to Pattern Recognition (I. </title> <publisher> Guyon Ed.). </publisher>
Reference-contexts: As before using a differentiable function for computing the penalties will ensure that the parameters can be optimized globally. SUBMITTED TO PROCEEDINGS OF THE IEEE, 1997 38 The segmenter uses a variety of heuristics to find candidate cut. One of the most important ones is called "hit and deflect" <ref> [113] </ref>. The idea is to cast lines downward from the top of the field image. When a line hits a black pixel, it is deflected so as to follow the contour of the object.
Reference: [114] <author> Y. LeCun, Y. Bengio, D. Henderson, A. Weisbuch, H. Weiss-man, and Jackel. L., </author> <title> "On-line handwriting recognition with neural networks: spatial representation versus temporal representation.," </title> <booktitle> in Proc. International Conference on handwriting and drawing. </booktitle> <year> 1993, </year> <institution> Ecole Nationale Superieure des Telecommunications. </institution>
Reference-contexts: The cause of this problem is that in weight space the origin is a fixed point of the learning dynamics, and, although it is a saddle point, it is attractive in almost all directions <ref> [114] </ref>. For our simulations, we use A = 1:7159 and S = 2 3 (see [20], [32]). With this choice of parameters, the equalities f (1) = 1 and f (1) = 1 are satisfied.
Reference: [115] <author> U. Muller, A. Gunzinger, and W. Guggenbuhl, </author> <title> "Fast neural net simulation with a DSP processor array," </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 6, no. 1, </volume> <pages> pp. 203-213, </pages> <year> 1995. </year>
Reference-contexts: An empirical result of considerable practical importance is that on tasks with large, redundant data sets, the stochastic version is considerably faster than the batch version, sometimes by orders of magnitude <ref> [115] </ref>. Although the reasons for this are not totally understood theoretically, an intuitive explanation can be found in the following extreme example. Let us take an example where the training database is composed of two copies of the same subset.
Reference: [116] <author> R. Battiti, </author> <title> "First- and second-order methods for learning: Between steepest descent and newton's method.," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 2, </volume> <pages> pp. 141-166, </pages> <year> 1992. </year>
Reference-contexts: In fact stochastic update must be better when there is redundancy, i.e., when a certain level of generalization is expected. Many authors have claimed that second-order methods should be used in lieu of gradient descent for neural net training. The literature abounds with recommendations <ref> [116] </ref> for classical second-order methods such as the Gauss-Newton or Levenberg-Marquardt algorithms, for Quasi-Newton methods such as BFGS, Limited-storage BFGS, or for various versions of the Conjugate Gradients (CG) method. Unfortunately, all of the above methods are unsuitable for training large neural networks on large data sets.
Reference: [117] <author> A. H. Kramer and A. Sangiovanni-Vincentelli, </author> <title> "Efficient parallel learning algorithms for neural networks," </title> <booktitle> in Advances in Neural Information Processing Systems, </booktitle> <editor> D.S. Touretzky, Ed., </editor> <booktitle> Denver 1988, 1989, </booktitle> <volume> vol. 1, </volume> <pages> pp. 40-48, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: For large data sets, the speed-up brought by these methods over regular batch gradient descent cannot match the enormous speed up brought by the use of stochastic gradient. Several authors have attempted to use Conjugate Gradient with small batches, or batches of increasing sizes <ref> [117] </ref>, [118], but those attempts have not yet been demonstrated to surpass a carefully tuned stochastic gradient. Our experiments were performed with a stochastic method that scales the parameter axes so as to minimize the eccentricity of the error surface. C.
Reference: [118] <author> M. Moller, </author> <title> Efficient Training of Feed-Forward Neural Networks, </title> <type> Ph.D. thesis, </type> <institution> Aarhus University, Aarhus, Denmark, </institution> <year> 1993. </year>
Reference-contexts: For large data sets, the speed-up brought by these methods over regular batch gradient descent cannot match the enormous speed up brought by the use of stochastic gradient. Several authors have attempted to use Conjugate Gradient with small batches, or batches of increasing sizes [117], <ref> [118] </ref>, but those attempts have not yet been demonstrated to surpass a carefully tuned stochastic gradient. Our experiments were performed with a stochastic method that scales the parameter axes so as to minimize the eccentricity of the error surface. C.

References-found: 118

