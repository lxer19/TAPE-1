URL: http://www.cs.utah.edu/~cs686/Previous/s97/margo.ps
Refering-URL: http://www.cs.utah.edu/~cs686/Previous/s97/
Root-URL: 
Email: Email: office@usenix.org  
Title: World Wide Web Cache Consistency  
Phone: 1. Phone: 510 528-8649 2. FAX: 510 548-5738 3.  4.  
Author: James Gwertzman and Margo Seltzer 
Affiliation: Harvard University  
Web: WWW URL: http://www.usenix.org  
Date: January 1996  
Note: The following paper was originally published in the Proceedings of the USENIX 1996 Annual Technical Conference San Diego, California,  For more information about USENIX Association contact:  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Andreessen, M., </author> <title> private email correspondence. </title>
Reference-contexts: Caching can be quite effective at reducing network bandwidth consumption as well as server load. Netscape, a vendor of Web servers, claimed in March of 1995 that a single local proxy server can reduce internetwork demands by up to 65% <ref> [1] </ref>. The value of caching is greatly reduced, however, if cached copies are not updated when the original data change. Cache consistency mechanisms ensure that cached copies of data are eventually updated to reect changes to the original data.
Reference: [2] <author> Berners-Lee, T., </author> <title> Hypertext Transfer Protocol HTTP/1.0, HTTP Working Group Internet Draft, </title> <month> October 14, </month> <year> 1995. </year>
Reference-contexts: When the TTL elapses, the data is considered invalid; the next request for the object will cause the object to be requested from its original source. TTLs are very simple to implement in HTTP using the optional expires header field specified by the protocol standard <ref> [2] </ref>. The challenge in supporting TTLs lies in selecting the appropriate time out value. Frequently, the TTL is set to a relatively short interval, so that data may be reloaded unnecessarily, but stale data are rarely returned.
Reference: [3] <author> Bestavros, A., </author> <title> Demand-based Resource Allocation to Reduce Traffic and Balance Load in Distributed Information Systems, </title> <booktitle> to appear in Proceedings of the SPDP95: The 7 th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> San Antonio, TX, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: The at lifetime distribution coupled with the fact that all files were assigned equal retrieval probability seemed to be the leading cause. The analysis of traces gathered in our local environment coupled with results by Bestavros <ref> [3] </ref> convinced us to consider an alternative workload generator. Bestavros found that on any given server only a few files change rapidly. Furthermore, he observed that globally popular files are the least likely to change.
Reference: [4] <author> Bestavros, A., </author> <title> Speculative Data Dissemination and Service to Reduce Server Load, Network Traffic and Service Time for Distributed Information Systems, </title> <booktitle> Proceedings of 1996 International Conference on Data Engineering, </booktitle> <address> New Orleans, Louisiana, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: period [10]. (It was this observation that led us to believe that the Alex protocol would be well suited to Web cache consistency.) Additionally, Worrell used a uniform distribution to generate file requests, but Bestavros has shown that the more popular a file is, the less frequently the file changes <ref> [4] </ref>. We modified the simulator to use a trace-driven workload. This (a) Data changed, never accessed again. Time-based Protocol (b) Data changed, accessed again before timing out. (c) Data changed, accessed after timing out. (d) Data did not change, timed out and later accessed. <p> To understand what files are the most likely to change, we analyzed the data gathered from the Boston University web server. Each day between March 28 and October 7, Bestavros sampled the server and recorded all the files that were modified since the previous day <ref> [4] </ref>. The logs contain approximately 2,500 file references and 14,000 changes during that 186 day time period. Categorizing this data by file type, we can determine the average life span per file type. This data is shown in the last two columns of Table 2.
Reference: [5] <author> Blaze, M., </author> <title> Caching in Large-Scale Distributed File Systems, </title> <institution> Princeton University Technical Report, TR-397-92, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: While there are a number of approaches for maintaining cache consistency in distributed file systems, there has been little work aimed specifically at evaluating cache consistency protocols on the World Wide Web. Blaze explored constructing large-scale hierarchical file systems <ref> [5] </ref>. While his architecture is similar to the one we posit for the web [10], the systems are sufficiently different that his results cannot be directly applied. In his model clients can also act as servers and can cache files on a long term basis.
Reference: [6] <author> Cate, V., </author> <title> Alex A Global Filesystem, </title> <booktitle> Proceedings of the 1992 USENIX File System Workshop, </booktitle> <address> Ann Arbor, MI, </address> <month> May </month> <year> 1992, </year> <month> 112. </month>
Reference-contexts: Client polling is a technique where clients periodically check back with the server to determine if cached objects are still valid. The specific variant of client polling in which we are interested originated with the Alex FTP cache <ref> [6] </ref> and is based on the assumptions that young files are modified more frequently than old files and that the older a file is the less likely it is to be modified. Adopting these assumptions implies that clients need to poll less frequently for older objects.
Reference: [7] <author> Chankhunthod, A., Danzig, P., Neerdaels, C., Schwartz, M., Worrell, K., </author> <title> A Hierarchical Internet Object Cache, </title> <booktitle> Proceedings of the 1996 USENIX Technical Conference, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: They found that FTP traffic across the backbone could be reduced by as much as 42%, simply by caching FTP files at the juncture between the backbone and regional nets. This result inspired the design of the Harvest object cache, which is a hierarchical proxy-cache <ref> [7] </ref>. Once the need for caching has been established, it is instructive to consider how to maintain consistency among the caches.
Reference: [8] <author> Dahlin, M., Mather, C., Wang, R., Anderson, T, Patterson, D., </author> <title> A Quantitative Analysis of Cache Policies for Scalable File Systems, </title> <booktitle> Proceedings of the 1994 Sigmetrics Conference, </booktitle> <month> May </month> <year> 1994, </year> <month> 150160. </month>
Reference-contexts: In his model clients can also act as servers and can cache files on a long term basis. This is not necessarily true in the web where clients are often personal computers with limited resources. The Berkeley xFS system <ref> [8] </ref> suggests a model of cooperative caching that is also similar to the one we propose for the web [10]. However, it relies on clients, not only for long-term caching, but also to retain the master copy of data.
Reference: [9] <author> Danzig, P., Hall, R., Schwartz, M., </author> <title> A Case for Caching File Objects Inside Internetworks, </title> <type> Technical Report, </type> <institution> University of Colorado, Boulder, CU-CS-642-93, </institution> <year> 1993. </year>
Reference-contexts: supported by the National Science Foundation on grant CCR-9502156. suggestion that weakly consistent protocols are a good choice for web consistency. 2.0 Related Work Danzig et al. motivate the need for hierarchical object caches for Web objects on the Internet by examining how strategically located FTP caches affect Internet traffic <ref> [9] </ref>. They found that FTP traffic across the backbone could be reduced by as much as 42%, simply by caching FTP files at the juncture between the backbone and regional nets. This result inspired the design of the Harvest object cache, which is a hierarchical proxy-cache [7].
Reference: [10] <author> Gwertzman, J., </author> <title> Autonomous Replication in Wide-Area Distributed Information Systems, </title> <type> Technical Report TR-95-17, </type> <institution> Harvard University Division of Applied Sciences, Center for Research in Computing Technology, </institution> <year> 1995. </year>
Reference-contexts: Blaze explored constructing large-scale hierarchical file systems [5]. While his architecture is similar to the one we posit for the web <ref> [10] </ref>, the systems are sufficiently different that his results cannot be directly applied. In his model clients can also act as servers and can cache files on a long term basis. This is not necessarily true in the web where clients are often personal computers with limited resources. <p> This is not necessarily true in the web where clients are often personal computers with limited resources. The Berkeley xFS system [8] suggests a model of cooperative caching that is also similar to the one we propose for the web <ref> [10] </ref>. However, it relies on clients, not only for long-term caching, but also to retain the master copy of data. <p> The results of trace analysis from a modified campus Web server show that this is an inappropriate model. Files tend to exhibit bimodal lifetimes. Either a file will remain unmodified for a long period of time or it will be modified frequently within a short time period <ref> [10] </ref>. (It was this observation that led us to believe that the Alex protocol would be well suited to Web cache consistency.) Additionally, Worrell used a uniform distribution to generate file requests, but Bestavros has shown that the more popular a file is, the less frequently the file changes [4].
Reference: [11] <author> Howard, J., Kazar, M., Menees, S., Nichols, D., Satyanarayanan, M., Sidebotham, R., West, M., </author> <title> Scale and Performance in a Distributed File System, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6, 1, </volume> <month> February </month> <year> 1988, </year> <month> 5181. </month>
Reference-contexts: However, it relies on clients, not only for long-term caching, but also to retain the master copy of data. Like other distributed file systems (e.g. the Sprite Distributed File System [13], the Andrew File System <ref> [11] </ref>), it also assumes objects can be changed by any machine while web objects can be modified only on their primary server. The web is fundamentally different from a distributed file system in its access patterns. The web is currently orders of magnitude larger than any distributed file system.
Reference: [12] <author> Luotonen, A., Frystyk, H, Berners-Lee, T., </author> <note> W3C httpd, http://www.w3.org/hypertext/WWW/Daemon/St atus.html. </note>
Reference-contexts: Each item on the web has a single master site from which changes can be made. This suggests that consistency issues may be simpler because conicting updates should never arise. The most widely used web cache is the original server distributed by CERN <ref> [12] </ref>. The CERN server assigns cached objects times to live based on (in order), the expires header field, a configurable fraction of the Last-Modified header field, and a configurable default expiration time.
Reference: [13] <author> Nelson, M., Welch, B., Ousterhout, J., </author> <title> Caching in the Sprite Network File System, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6, 1, </volume> <month> February </month> <year> 1988, </year> <month> 134154. </month>
Reference-contexts: However, it relies on clients, not only for long-term caching, but also to retain the master copy of data. Like other distributed file systems (e.g. the Sprite Distributed File System <ref> [13] </ref>, the Andrew File System [11]), it also assumes objects can be changed by any machine while web objects can be modified only on their primary server. The web is fundamentally different from a distributed file system in its access patterns.
Reference: [14] <author> Sandberg, R., Goldberg, D., Kleiman, S., Walsh, D., and Lyon, B., </author> <title> Design and Implementation of the Sun Network Filesystem, </title> <booktitle> Proceedings of the Summer 1985 USENIX Conference, </booktitle> <address> Portland OR, </address> <month> June </month> <year> 1985, 119130. </year>
Reference-contexts: Staleness is determined using both TTLs and invalidation callbacks from cooperating primary servers. Proxy caches are registered with the primary server so that they can receive invalidation notices. If one views the CERN proxy cache as implementing an NFS-like consistency protocol <ref> [14] </ref>, the new server implements an AFS-like protocol. The comparison focuses on the performance differences between the two servers and does not examine the relative behavior of the different consistency protocols, which is the focus of this work.
Reference: [15] <author> Wessels, D., </author> <title> Intelligent Caching for World-Wide Web Objects, </title> <booktitle> Proceedings of INET-95, </booktitle> <year> 1995. </year>
Reference-contexts: In Section 5, we suggest some areas for future research and conclude in Section 6, with the requests cause an If-Modified-Since request to be issued. One study compares the performance of the CERN proxy cache to a specially designed lightweight caching server <ref> [15] </ref>. The lightweight cache has an independent process that periodically examines cached objects to determine if they have become stale. Staleness is determined using both TTLs and invalidation callbacks from cooperating primary servers. Proxy caches are registered with the primary server so that they can receive invalidation notices.

References-found: 15

