URL: http://www.das.harvard.edu/users/students/Stanley_Chen/papers/acl95.ps.gz
Refering-URL: http://www.das.harvard.edu/users/students/Stanley_Chen/Stanley_Chen.html
Root-URL: 
Email: sfc@das.harvard.edu  
Title: Bayesian Grammar Induction for Language Modeling  
Author: Stanley F. Chen 
Keyword: Inside-Outside algorithm.  
Address: Cambridge, MA 02138  
Affiliation: Aiken Computation Laboratory Division of Applied Sciences Harvard University  
Abstract: We describe a corpus-based induction algorithm for probabilistic context-free grammars. The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm. We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks. In two of the tasks, the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques. The third task involves naturally-occurring data, and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the 
Abstract-found: 1
Intro-found: 1
Reference: <author> D. Angluin and C.H. Smith. </author> <year> 1983. </year> <title> Inductive inference: theory and methods. </title> <journal> ACM Computing Surveys, </journal> <volume> 15 </volume> <pages> 237-269. </pages>
Reference-contexts: The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm. 2 Grammar Induction as Search Grammar induction can be framed as a search problem, and has been framed as such almost without exception in past research <ref> (Angluin and Smith, 1983) </ref>. The search space is taken to be some class of grammars; for example, in our work we search within the space of probabilistic context-free grammars.
Reference: <author> L.R. Bahl, J.K. Baker, P.S. Cohen, F. </author> <title> Jelinek, </title> <address> B.L. </address>
Reference: <author> Lewis, and R.L. Mercer. </author> <year> 1978. </year> <title> Recognition of a continuously read natural corpus. </title> <booktitle> In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 422-424, </pages> <institution> Tulsa, Oklahoma, </institution> <month> April. </month>
Reference: <author> Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. </author> <year> 1983. </year> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5(2):179-190, </volume> <month> March. </month>
Reference: <author> J.K. Baker. </author> <year> 1975. </year> <title> The DRAGON system - an overview. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 23 </volume> <pages> 24-29, </pages> <month> February. </month>
Reference: <author> J.K. Baker. </author> <year> 1979. </year> <title> Trainable grammars for speech recognition. </title> <booktitle> In Proceedings of the Spring Conference of the Acoustical Society of America, </booktitle> <pages> pages 547-550, </pages> <address> Boston, MA, </address> <month> June. </month>
Reference-contexts: However, to date there has been little success in con structing grammar-based language models competitive with n-gram models in problems of any magnitude. In this paper, we describe a corpus-based induction algorithm for probabilistic context-free grammars that outperforms n-gram models and the Inside-Outside algorithm <ref> (Baker, 1979) </ref> in medium-sized domains. This result marks the first time a grammar-based language model has surpassed n-gram modeling in a task of at least moderate size.
Reference: <author> L.E. Baum and J.A. Eagon. </author> <year> 1967. </year> <title> An inequality with application to statistical estimation for probabilistic functions of Markov processes and to a model for ecology. </title> <journal> Bulletin of the American Mathematicians Society, </journal> <volume> 73 </volume> <pages> 360-363. </pages>
Reference-contexts: The smoothing parameters i;c are trained through the Forward-Backward algorithm <ref> (Baum and Eagon, 1967) </ref> on held-out data. Parameters i;c are tied together for similar c to prevent data sparsity. For the Inside-Outside algorithm, we follow the methodology described by Lari and Young.
Reference: <author> Peter F. Brown, Vincent J. DellaPietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. </author> <year> 1992. </year> <title> Class-based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 467-479, </pages> <month> December. </month>
Reference: <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, 39(B):1-38. </journal>
Reference-contexts: The grammar induction algorithms most successful in language modeling include the Inside-Outside algorithm (Lari and Young, 1990; Lari and Young, 1991; Pereira and Schabes, 1992), a special case of the Expectation-Maximization algorithm <ref> (Dempster et al., 1977) </ref>, and work by McCandless and Glass (1993). In the latter work, McCandless uses a heuristic search procedure similar to ours, but a very different search criteria.
Reference: <author> Frederick Jelinek and Robert L. Mercer. </author> <year> 1980. </year> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceedings of the Workshop on Pattern Recognition in Practice, </booktitle> <address> Amsterdam, The Netherlands: </address> <publisher> North-Holland, </publisher> <month> May. </month>
Reference: <author> M.D. Kernighan, K.W. Church, and W.A. Gale. </author> <year> 1990. </year> <title> A spelling correction program based on a noisy channel model. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Computational Linguistics, </booktitle> <pages> pages 205-210. </pages>
Reference: <author> K. Lari and S.J. Young. </author> <year> 1990. </year> <title> The estimation of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 35-56. </pages>
Reference: <author> K. Lari and S.J. Young. </author> <year> 1991. </year> <title> Applications of stochastic context-free grammars using the inside-outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 237-257. </pages>
Reference: <author> Ming Li and Paul Vitanyi. </author> <year> 1993. </year> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer-Verlag. </publisher>
Reference: <author> Michael K. McCandless and James R. Glass. </author> <year> 1993. </year> <title> Empirical acquisition of word and phrase classes in the ATIS domain. </title> <booktitle> In Third European Confer--ence on Speech Communication and Technology, </booktitle> <address> Berlin, Germany, </address> <month> September. </month>
Reference: <author> Fernando Pereira and Yves Schabes. </author> <year> 1992. </year> <title> Inside-outside reestimation from partially bracket corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the ACL, </booktitle> <pages> pages 128-135, </pages> <address> Newark, Delaware. </address>
Reference: <author> P. Resnik. </author> <year> 1992. </year> <title> Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics. </booktitle>
Reference: <author> J. Rissanen. </author> <year> 1978. </year> <title> Modeling by the shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference-contexts: We satisfy the goal of favoring smaller grammars by choosing a prior that assigns higher probabilities to such grammars. In particular, Solomonoff proposes the use of the universal a priori probability (Solomonoff, 1960), which is closely related to the minimum description length principle later proposed by <ref> (Rissanen, 1978) </ref>. In the case of grammatical lan guage modeling, this corresponds to taking p (G) = 2 l (G) where l (G) is the length of the description of the grammar in bits.
Reference: <author> Y. Schabes. </author> <year> 1992. </year> <title> Stochastic lexicalized tree-adjoining grammars. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics. </booktitle>
Reference: <author> C.E. Shannon. </author> <year> 1951. </year> <title> Prediction and entropy of printed English. </title> <journal> Bell Systems Technical Journal, </journal> <volume> 30 </volume> <pages> 50-64, </pages> <month> January. </month>
Reference-contexts: However, static language modeling performance has remained basically unchanged since the advent of n-gram language models forty years ago <ref> (Shannon, 1951) </ref>. Yet, n-gram language models can only capture dependencies within an n-word window, where currently the largest practical n for natural language is three, and many dependencies in natural language occur beyond a three-word window.
Reference: <author> R.J. Solomonoff. </author> <year> 1960. </year> <title> A preliminary report on a general theory of inductive inference. </title> <type> Technical Report ZTB-138, </type> <institution> Zator Company, </institution> <address> Cambridge, MA, </address> <month> November. </month>
Reference-contexts: We satisfy the goal of favoring smaller grammars by choosing a prior that assigns higher probabilities to such grammars. In particular, Solomonoff proposes the use of the universal a priori probability <ref> (Solomonoff, 1960) </ref>, which is closely related to the minimum description length principle later proposed by (Rissanen, 1978). In the case of grammatical lan guage modeling, this corresponds to taking p (G) = 2 l (G) where l (G) is the length of the description of the grammar in bits.
Reference: <author> R.J. Solomonoff. </author> <year> 1964. </year> <title> A formal theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22, 224-254, </pages> <month> March, June. </month>
Reference: <author> Rohini Srihari and Charlotte Baltus. </author> <year> 1992. </year> <title> Combining statistical and syntactic methods in recognizing handwritten sentences. </title> <booktitle> In AAAI Symposium: Probabilistic Approaches to Natural Language, </booktitle> <pages> pages 121-127. </pages>
References-found: 23

