URL: http://www.cs.cmu.edu/~quake-papers/hpca98tr.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/quake/public/www/papers.html
Root-URL: 
Title: Architectural Implications of a Family of Irregular Applications  
Author: David O'Hallaron, Jonathan Richard Shewchuk, and Thomas Gross 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: November 14, 1997  
Pubnum: CMU-CS-97-189  
Abstract: Irregular applications based on sparse matrices are at the core of many important scientific computations. Since the importance of such applications is likely to increase in the future, high-performance parallel and distributed systems must provide adequate support for such applications. We characterize a family of irregular scientific applications and derive the demands they will place on the communication systems of future parallel systems. Running time of these applications is dominated by repeated sparse matrix vector product (SMVP) operations. Using simple performance models of the SMVP, we investigate requirements for bisection bandwidth, sustained bandwidth on each processing element (PE), burst bandwidth during block transfers, and block latencies for PEs under different assumptions about sustained computational throughput. Our model indicates that block latencies are likely to be the most problematic engineering challenge for future communication networks. Effort sponsored in part by the Advanced Research Projects Agency and Rome Laboratory, Air Force Materiel Command, USAF, under agreement number F30602-96-1-0287, in part by the National Science Foundation under Grant CMS-9318163, and in part by grant from Intel Corporation and Digital Equipment Corporation. The Pittsburgh Supercomputing Center provided time on the Cray T3D and T3E sytems. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Advanced Research Projects Agency, Rome Laboratory, or the U.S. Government. Authors' email addresses: fdroh,jrs,trgg@cs.cmu.edu 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Amza, A. Cox, S. Dwarkadas, C. Hyams, Z. Li, and W. Zwaenepoel, Treadmarks: </author> <title> Shared memory computing on networks of workstations, </title> <booktitle> IEEE Computer 29 (1996), </booktitle> <volume> no. 2, </volume> <pages> 1828. </pages>
Reference-contexts: Blocks may be fixed-sized or variable-sized. For example, a block might be a cache line in a CC-NUMA system [11], a message in a message passing system [13], a bulk asynchronous data transfer between two PEs' memories [16], or a page in a software distributed shared memory system <ref> [1] </ref>. The transfer time for a block i of l i words is T l + l i T w , where T l is the constant block latency, T w is the constant marginal cost of transferring each additional word, and T 1 w is the burst bandwidth.
Reference: [2] <author> H. Bao, J. Bielak, O. Ghattas, L. Kallivokas, D. O'Hallaron, J. Shewchuk, and J. Xu, </author> <title> Large-scale Simulation of Elastic Wave Propagation in Heterogeneous Media on Parallel Computers, </title> <booktitle> Computer Methods in Applied Mechanics and Engineering 152 (1998), </booktitle> <volume> no. 12, </volume> <pages> 85102. </pages>
Reference-contexts: The Quake applications, described in Section 2, simulate the motion of the ground during strong earthquakes. They were developed as part of an ongoing project at Carnegie Mellon to model earthquakes in the Los Angeles Basin and other alluvial valleys <ref> [2] </ref>. The running time of the Quake applications is dominated by a sparse matrix-vector product (SMVP) operation that is repeated thousands of times, and the SMVP is the only operation besides I/O that requires the transfer of data between processors. <p> communication latency will need to be a central focus of future efforts to engineer effective communication networks and software. 2 The family of Quake applications The Quake applications are unstructured finite element codes that were developed to predict ground motion in the San Fernando Valley of Southern California during earthquakes <ref> [2] </ref>. There are four Quake applications, denoted sf10, sf5, sf2, and sf1. The sf is an abbreviation for San Fernando, and the number indicates the period (in seconds) of the highest frequency wave that the simulation is able to resolve. <p> The simulations are parallelized using Archimedes, a domain-specific tool chain for finite element problems <ref> [2, 17] </ref>. To generate a simulation that will run on p PEs, Archimedes partitions the mesh into p disjoint sets of elements. Each set is called a subdomain; a one-to-one mapping is established between PEs and subdomains. <p> For example, even though the optimal throughput of strided copies on the Cray T3D is 3040 MBytes/sec [18], current implementations of sf2 achieve at best a measured and sustained bandwidth of 10 MBytes/sec using the C interface to the vendor-supplied MPI library <ref> [2] </ref>. Communication requirements 17 (a) Arbitrarily large block size. (b) Four-word block size. Even more daunting is the goal of running the Quake applications at roughly 80% efficiency on high-speed networks of workstations.
Reference: [3] <author> S. Barnard and H. Simon, </author> <title> A fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems, </title> <type> Tech. Report RNR-92-033, </type> <institution> NASA Ames Research Center, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: The geometric partitioning algorithm has provable asymptotic upper bounds on the number of shared nodes, and in practice generates partitions that are competitive with those produced by other modern partitioning algorithms <ref> [7, 3, 8] </ref>. 4 O'Hallaron, Shewchuk, and Gross X represents a 3 fi 3 submatrix. 2.3 The Parallel SMVP The running time of the Quake applications is dominated by the execution of SMVP operations, which consume over 80% of the total running time in the sequential case.
Reference: [4] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, E. Santos, K. Schauser, R. Subramonian, and T. von Eicken, </author> <title> LogP: A practical model of parallel computation, </title> <journal> Communications of the ACM 39 (1996), </journal> <volume> no. 11, </volume> <pages> 7985. </pages>
Reference-contexts: Section 5, we use this model to explore bandwidth and latency tradeoffs in communication systems as the base microprocessors continue to improve. The models in Equations (1) and (2) are similar in some ways and different in others to the LogP model <ref> [4] </ref>. LogP is a general performance model for bulk-synchronous parallel (BSP) computations [19], where a program is viewed as a series of supersteps separated by barrier synchronizations. During a superstep, each PE performs local computation and transfers a limited number of messages.
Reference: [5] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina, </author> <title> Architetural requirements of parallel scientific applications with explicit communication, </title> <booktitle> Proc. 20th Intl. Symp. Computer Arch., </booktitle> <address> ACM/IEEE, </address> <month> May </month> <year> 1993, </year> <pages> pp. 213. </pages>
Reference-contexts: It is crucial to understand communication requirements because some parts of a high-performance communication system cannot be commodity, and will therefore be expensive. In general it is important to understand the communication requirements of real applications <ref> [5, 10] </ref>, and these requirements are especially difficult to characterize for the important class of irregular scientific applications that manipulate sparse matrices. <p> For systems with a sustained computational rate of 200 MFLOPS, PEs will need about 300 MBytes/sec of sustained bandwidth and 600 MBytes/sec of burst bandwidth to run irregular codes with good efficiency. Our work is similar in spirit to that of Cypher, Ho, Konstantinidou, and Messina <ref> [5] </ref>, who characterize eight regular and irregular scientific applications in terms of memory, processing, communication, and I/O requirements, and build scalability models for three of the simpler regular applications. However, our approach is different in that our goal is depth rather than breadth. <p> To show that the generality of our model seems to extend beyond this single family of irregular applications, we observe that there is some evidence that the Quake applications are typical of unstructured finite element simulations. For example, the EXFLOW application from Cypher et al. <ref> [5] </ref> is a 3D unstructured finite element program that simulates a fluid dynamics problem on 512 PEs. Interestingly, EXFLOW has 2 O'Hallaron, Shewchuk, and Gross nearly identical computational properties as a similarly sized Quake application (called sf2/128, which resolves a wave with a two-second period on 128 PEs). <p> Irregular finite element applications like EXFLOW and Quake have an average total communication volume similar to that of the regular applications studied earlier by Cypher et al. <ref> [5] </ref>, but they transfer more messages having a smaller average size than most of those regular applications.
Reference: [6] <author> S. Dwarkadas, A. Cox, and W. Zwaenepoel, </author> <title> An integrated compile-time/run-time software distributed shared memory system, </title> <booktitle> Proc. Sixth Intl. Conf. on Architectural Support for Prog. Languages and Operating Systems (ASPLOS VI) (Boston, </booktitle> <address> MA), </address> <publisher> ACM, </publisher> <month> October </month> <year> 1996, </year> <pages> pp. 186197. </pages>
Reference-contexts: This is the norm in message passing systems or DSMs that aggregate blocks <ref> [6] </ref>. The interesting point about this graph is that latency matters for the SMVP. Even if burst bandwidth is driven to infinity, observed block latency must not exceed 3 s if the code is to run at 90% efficiency.
Reference: [7] <author> J. Gilbert, G. Miller, and S. Teng, </author> <title> Geometric mesh partitioning: Implementation and experiments, </title> <booktitle> 9th International Parallel Processing Symposium (Santa Barbara, </booktitle> <address> CA), </address> <publisher> IEEE, </publisher> <month> April </month> <year> 1995, </year> <pages> pp. </pages> <note> 418427. 20 O'Hallaron, Shewchuk, and Gross </note>
Reference-contexts: The geometric partitioning algorithm has provable asymptotic upper bounds on the number of shared nodes, and in practice generates partitions that are competitive with those produced by other modern partitioning algorithms <ref> [7, 3, 8] </ref>. 4 O'Hallaron, Shewchuk, and Gross X represents a 3 fi 3 submatrix. 2.3 The Parallel SMVP The running time of the Quake applications is dominated by the execution of SMVP operations, which consume over 80% of the total running time in the sequential case.
Reference: [8] <author> B. Hendrickson and R. Leland, </author> <title> The Chaco user's guide Version 2.0, </title> <type> Tech. Report SAND95-2344, </type> <institution> Sandia National Laboratories, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: The geometric partitioning algorithm has provable asymptotic upper bounds on the number of shared nodes, and in practice generates partitions that are competitive with those produced by other modern partitioning algorithms <ref> [7, 3, 8] </ref>. 4 O'Hallaron, Shewchuk, and Gross X represents a 3 fi 3 submatrix. 2.3 The Parallel SMVP The running time of the Quake applications is dominated by the execution of SMVP operations, which consume over 80% of the total running time in the sequential case.
Reference: [9] <author> J. L Hennessy and D. A. Patterson, </author> <title> Computer architecture: A quantitative approach (2nd edition), </title> <publisher> Morgan Kaufman, </publisher> <year> 1995. </year>
Reference-contexts: This separation of factors is similar in spirit to the familiar CPI model for uniprocessor performance <ref> [9] </ref>.
Reference: [10] <author> C. Holt, J. Singh, and J. Hennessy, </author> <title> Application and architectural bottlenecks in large scale distributed shared memory machines, </title> <booktitle> Proc. 23nd Intl. Symp. Comp. Arch. </booktitle> <address> (Philadelphia, PA), </address> <publisher> ACM, </publisher> <month> May </month> <year> 1996, </year> <pages> pp. 134145. </pages>
Reference-contexts: It is crucial to understand communication requirements because some parts of a high-performance communication system cannot be commodity, and will therefore be expensive. In general it is important to understand the communication requirements of real applications <ref> [5, 10] </ref>, and these requirements are especially difficult to characterize for the important class of irregular scientific applications that manipulate sparse matrices.
Reference: [11] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam, </author> <title> The Stanford DASH multiprocessor, </title> <booktitle> IEEE Computer 25 (1992), </booktitle> <volume> no. 3, </volume> <pages> 6379. </pages>
Reference-contexts: The time for this transfer is called the block transfer time. A block is a transfer unit, a group of words that move together from one PE to another. Blocks may be fixed-sized or variable-sized. For example, a block might be a cache line in a CC-NUMA system <ref> [11] </ref>, a message in a message passing system [13], a bulk asynchronous data transfer between two PEs' memories [16], or a page in a software distributed shared memory system [1].
Reference: [12] <author> G. Miller, S. Teng, W. Thurston, and S. Vavasis, </author> <title> Automatic mesh partitioning, Graph Theory and Sparse Matrix Computation (New York) (Alan George, </title> <editor> John R. Gilbert, and Joseph W. H. Liu, eds.), </editor> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: Each set is called a subdomain; a one-to-one mapping is established between PEs and subdomains. The program that partitions each mesh into subdomains is based on a recursive geometric bisection algorithm <ref> [12] </ref> that divides the elements equally among the subdomains while attempting to minimize the total number of nodes that are shared by multiple subdomains, and hence the total communication volume.
Reference: [13] <author> MPI Forum, </author> <title> MPI: A Message Passing Interface, </title> <booktitle> Proc. Supercomputing '93 (Portland, </booktitle> <address> OR), ACM/IEEE, </address> <month> November </month> <year> 1993, </year> <pages> pp. 878883. </pages>
Reference-contexts: A block is a transfer unit, a group of words that move together from one PE to another. Blocks may be fixed-sized or variable-sized. For example, a block might be a cache line in a CC-NUMA system [11], a message in a message passing system <ref> [13] </ref>, a bulk asynchronous data transfer between two PEs' memories [16], or a page in a software distributed shared memory system [1].
Reference: [14] <author> D. O'Hallaron, Spark98: </author> <title> Sparse matrix kernels for shared memory and message passing systems, </title> <type> Tech. Report CMU-CS-97-178, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> October </month> <year> 1997. </year>
Reference: [15] <author> D. O'Hallaron and J. Shewchuk, </author> <title> Properties of a family of parallel finite element simulations, </title> <type> Tech. Report CMU-CS-96-141, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: The stiffness matrix is extremely sparse; each node is connected to an average of 13 neighbors (in addition to itself), so each row of K contains an average of 14 fi 3 = 42 nonzero floating point numbers <ref> [15] </ref>. The simulations are parallelized using Archimedes, a domain-specific tool chain for finite element problems [2, 17]. To generate a simulation that will run on p PEs, Archimedes partitions the mesh into p disjoint sets of elements. <p> to discuss how architects can accommodate scientific users, and not vice versa.) By not modeling any overlap, we obtain conservative bandwidth and latency estimates, and avoid possibly slowing the program by complicating the runtime system. 6 O'Hallaron, Shewchuk, and Gross do an excellent job of distributing computation evenly across PEs <ref> [15] </ref>, so we will assume that each PE performs F = 2m flops and thus the running time for the computation phase is T comp = F T f : Since T f includes all hardware and software overheads (e.g., loads, stores, various miss penalties, pipeline stalls, etc.) it is difficult <p> Unfortunately they do less well in balancing the total number of blocks B i and the total volume in words C i sent and received by each PE i <ref> [15] </ref>. As a simplifying assumption, we pessimistically assume that the PE that transfers the maximum number of words (C max ) is the same PE that transfers the maximum number of blocks (B max ). <p> the Quake SMVP instances (including the parameters F , B max , and C max ), describe a simple method for estimating T l and T w , and then plug these parameters into our models to predict running time on a Cray T3E. 4.1 Properties of the Quake applications <ref> [15] </ref> for a complete characterization of the applications.) The values of B max and C max in this table are always even, because each message from PE i to PE j is matched by a message from j to i of equal length. (The values of C max are also divisible
Reference: [16] <author> S. Scott, </author> <title> Synchronization and communication in the T3E multiprocessor, </title> <booktitle> Proc. 7th. Intl. Conf. on Arch. Support for Prog. Lang. and Oper. Systems (Boston, </booktitle> <address> MA), </address> <publisher> ACM, </publisher> <month> October </month> <year> 1996, </year> <pages> pp. 2636. </pages>
Reference-contexts: Blocks may be fixed-sized or variable-sized. For example, a block might be a cache line in a CC-NUMA system [11], a message in a message passing system [13], a bulk asynchronous data transfer between two PEs' memories <ref> [16] </ref>, or a page in a software distributed shared memory system [1].
Reference: [17] <author> J. Shewchuk, </author> <title> Delaunay refinement mesh generation, </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1997, </year> <note> Available as CMU Tech Report CMU-CS-97-137. </note>
Reference-contexts: The simulations are parallelized using Archimedes, a domain-specific tool chain for finite element problems <ref> [2, 17] </ref>. To generate a simulation that will run on p PEs, Archimedes partitions the mesh into p disjoint sets of elements. Each set is called a subdomain; a one-to-one mapping is established between PEs and subdomains.
Reference: [18] <author> T. Stricker and T. Gross, </author> <title> Optimizing memory system performance for communication in parallel computers, </title> <booktitle> Proc. 22nd Intl. Symp. </booktitle> <institution> Comp. Arch. (Santa Marguerita di Ligure, Italy), ACM, </institution> <month> June </month> <year> 1995, </year> <pages> pp. 308319. </pages>
Reference-contexts: We only model the overhead of transferring data between the network interface and local memory, in view of previous findings that most of the cost of communication on modern systems is incurred at the individual PEs <ref> [18] </ref>. In essence, we assume that the interconnection network has infinite capacity and constant latency. In Section 4.2, we offer empirical evidence that this is a reasonable assumption for the SMVP running on tightly coupled systems. <p> For example, even though the optimal throughput of strided copies on the Cray T3D is 3040 MBytes/sec <ref> [18] </ref>, current implementations of sf2 achieve at best a measured and sustained bandwidth of 10 MBytes/sec using the C interface to the vendor-supplied MPI library [2]. Communication requirements 17 (a) Arbitrarily large block size. (b) Four-word block size.
Reference: [19] <author> L. Valiant, </author> <title> A bridging model for parallel computation, </title> <journal> Communications of the ACM 33 (1990), </journal> <volume> no. 8, </volume> <pages> 103111. </pages>
Reference-contexts: The models in Equations (1) and (2) are similar in some ways and different in others to the LogP model [4]. LogP is a general performance model for bulk-synchronous parallel (BSP) computations <ref> [19] </ref>, where a program is viewed as a series of supersteps separated by barrier synchronizations. During a superstep, each PE performs local computation and transfers a limited number of messages.
References-found: 19

