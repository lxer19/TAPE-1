URL: http://polaris.cs.uiuc.edu/reports/1431.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Title: INLINE EXPANSION FOR THE POLARIS RESEARCH COMPILER  
Author: BY JOHN ROBERT GROUT 
Degree: 1981 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1995 Urbana, Illinois  
Affiliation: B.S., Worcester Polytechnic Institute,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> William Blume and Rudolf Eigenmann. </author> <title> The Range Test: A Dependence Test for Symbolic, Non-linear Expressions. </title> <booktitle> Proceedings of Supercomputing '94, November 1994, </booktitle> <address> Washington D.C., </address> <pages> pages 528-537, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: One technique which PFA performs which Polaris does not is partial linearization of subprogram array references (e.g., mapping a three-dimension formal array reference to a two-dimensional actual array); however, in practice, the Polaris range test <ref> [1] </ref> has been able to recreate the dependence information lost by full linearization of subprogram array references. 35 CHAPTER 6 REFLECTIONS AND CONCLUSIONS As the Polaris inliner grew, it became clear that source-to-source inline expansion of Fortran routines in many real programs was a very complicated problem, and so the Polaris
Reference: [2] <author> William Blume, Rudolf Eigenmann, Keith Faigin, John Grout, Jay Hoeflinger, David Padua, Paul Petersen, William Pottenger, Lawrence Rauchwerger, Peng Tu, and Stephen Weatherford. </author> <title> Polaris: Improving the Effectiveness of Parallelizing Compilers. </title> <booktitle> Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York; also: </address> <booktitle> Lecture Notes in Computer Science 892, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pages 141-154, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: INTRODUCTION For many years, the Center for Supercomputing Research and Development (CSRD) has been working to make parallel computing more practical. After an effort to manually parallelize the Perfect Benchmarks R fl to identify effective program transformations [6, 7] for paralleliza-tion, CSRD began to implement the Polaris research compiler <ref> [2] </ref>, which provides both a basic infrastructure for manipulating Fortran programs and many of the program transformations shown to be effective. <p> could generate corresponding facade subprograms containing namelist I/O statements whose arguments were formal variables with correct names and bounds. 4.4 Performance Considerations To minimize the costs of an individual inline expansion and of the whole process, the Polaris inliner uses a number of high-performance services provided by other Polaris components <ref> [2, 8] </ref>. 28 4.4.1 ProgramUnit cloning The Polaris base implements a set of routines which support "cloning": making a usable copy of an existing ProgramUnit object.
Reference: [3] <author> K. D. Cooper, M. W. Hall, R. Hood, K. Kennedy, K. McKinley, J. Mellor-Crummey, L. Tor-czon, and S. Warren. </author> <title> The ParaScope Parallel Programming Environment. </title> <booktitle> Proceedings of the IEEE, </booktitle> <pages> pages 244-263, </pages> <month> February </month> <year> 1993. </year>
Reference: [4] <author> K. D. Cooper, M. W. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proc. IEEE International Conference on Computer Languages, </booktitle> <pages> pages 1-11, </pages> <month> April </month> <year> 1992. </year>
Reference: [5] <author> Keith D. Cooper, Mary W. Hall, and Linda Torczon. </author> <title> An experiment with inline substitution. </title> <journal> Software-Practice and Experience, </journal> <volume> 21(6) </volume> <pages> 581-601, </pages> <month> June </month> <year> 1991. </year>
Reference: [6] <author> Rudolf Eigenmann, Jay Hoeflinger, G. Jaxon, and David Padua. </author> <title> The Cedar Fortran Project. </title> <type> Technical Report 1262, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res, & Dev., </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: INTRODUCTION For many years, the Center for Supercomputing Research and Development (CSRD) has been working to make parallel computing more practical. After an effort to manually parallelize the Perfect Benchmarks R fl to identify effective program transformations <ref> [6, 7] </ref> for paralleliza-tion, CSRD began to implement the Polaris research compiler [2], which provides both a basic infrastructure for manipulating Fortran programs and many of the program transformations shown to be effective.
Reference: [7] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science 589. Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 65-83, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: INTRODUCTION For many years, the Center for Supercomputing Research and Development (CSRD) has been working to make parallel computing more practical. After an effort to manually parallelize the Perfect Benchmarks R fl to identify effective program transformations <ref> [6, 7] </ref> for paralleliza-tion, CSRD began to implement the Polaris research compiler [2], which provides both a basic infrastructure for manipulating Fortran programs and many of the program transformations shown to be effective. <p> Though the Polaris compiler prototype implemented some new techniques to perform the transformations described in <ref> [7] </ref>, it was decided that flow-sensitive interprocedural analysis would be performed initially by using source-to-source inline expansion to transform interpro-cedural data flow problems into intraprocedural form, and that more efficient interprocedural analysis techniques would be implemented later.
Reference: [8] <author> Keith A. Faigin, Jay P. Hoeflinger, David A. Padua, Paul M. Petersen, and Stephen A. Weatherford. </author> <title> The Polaris Internal Representation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 22(5) </volume> <pages> 553-286, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: could generate corresponding facade subprograms containing namelist I/O statements whose arguments were formal variables with correct names and bounds. 4.4 Performance Considerations To minimize the costs of an individual inline expansion and of the whole process, the Polaris inliner uses a number of high-performance services provided by other Polaris components <ref> [2, 8] </ref>. 28 4.4.1 ProgramUnit cloning The Polaris base implements a set of routines which support "cloning": making a usable copy of an existing ProgramUnit object.
Reference: [9] <author> Mary W. Hall, Ken Kennedy, and Kathryn S. McKinley. </author> <title> Interprocedural transformations for parallel code generation. </title> <booktitle> In Proc. Supercomputing '91, </booktitle> <pages> pages 424-434, </pages> <year> 1991. </year> <month> 37 </month>
Reference-contexts: inability to express some language constructs after expansion in source form, the potential for exponential growth in program size, and the inability of modern optimizing compilers to effectively compile large programs after complete inline expansion ([5]), 4 sparked the development of interprocedural data flow analysis techniques, which are surveyed in <ref> [9] </ref>, [12] and [19]. Most of the early interprocedural analysis techniques merge information into a flow independent summary form, which saves time or space but loses the precision needed to perform techniques such as array privatization ([20]).
Reference: [10] <author> Mary W. Hall, John M. Mellor-Cummey, Alan Carle, and Rene G. Rodrguez. Fiat: </author> <title> A framework for interprocedural analysis and transformation. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computers, </booktitle> <volume> vol. 2, </volume> <month> February </month> <year> 1995. </year>
Reference: [11] <author> Mary W. Hall, Brian R. Murphy, and Saman P. Amarasinghe. </author> <title> Interprocedural paralleliza-tion analysis: A case study. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> February </month> <year> 1995. </year>
Reference: [12] <author> Michael Hind, Michael Burke, Paul Carini, and Sam Midkiff. </author> <title> A comparison of interproce-dural array analysis methods. </title> <journal> Scientific Programming, </journal> <volume> 3 </volume> <pages> 255-271, </pages> <year> 1994. </year>
Reference-contexts: to express some language constructs after expansion in source form, the potential for exponential growth in program size, and the inability of modern optimizing compilers to effectively compile large programs after complete inline expansion ([5]), 4 sparked the development of interprocedural data flow analysis techniques, which are surveyed in [9], <ref> [12] </ref> and [19]. Most of the early interprocedural analysis techniques merge information into a flow independent summary form, which saves time or space but loses the precision needed to perform techniques such as array privatization ([20]).
Reference: [13] <author> Anne M. Holler. </author> <title> A study of the effects of subprogram inlining. </title> <type> Technical Report No. </type> <institution> TR-91-06 (Computer Science), University of Virginia, </institution> <month> March 20, </month> <year> 1991. </year>
Reference-contexts: Though it may exponentially increase the size of the program, complete inline expansion removes any barriers which might prevent data-flow analysis of a whole program. Inline expansion of Fortran code for the Parafrase compiler is described in [14], and a general-purpose inliner for C code is described in <ref> [13] </ref>.
Reference: [14] <author> Christopher Alan Huson. </author> <title> An In-Line Subroutine Expander for Parafrase. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Dept. of Computer Sci., </institution> <month> Dec., </month> <year> 1982. </year>
Reference-contexts: Though it may exponentially increase the size of the program, complete inline expansion removes any barriers which might prevent data-flow analysis of a whole program. Inline expansion of Fortran code for the Parafrase compiler is described in <ref> [14] </ref>, and a general-purpose inliner for C code is described in [13].
Reference: [15] <author> Zhiyuan Li and Pen-Chung Yew. </author> <title> Efficient Interprocedural Analysis for Program Paral-lelization and Restructuring. </title> <booktitle> Proceedings of ACM/SIGPLAN PPEALS, </booktitle> <address> New Haven, CT, </address> <pages> pages 85-99, </pages> <month> July </month> <year> 1988. </year>
Reference: [16] <author> David A. Padua, Rudolf Eigenmann, and Jay P. Hoeflinger. </author> <title> Automatic Program Restructuring for Parallel Computing and the Polaris Fortran Translator. </title> <booktitle> Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, CA, </address> <month> February 15-17, </month> <year> 1995, </year> <pages> pages 647-649, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: As discussed in <ref> [16] </ref>, the Polaris prototype compiler has shown excellent results on a number of real Fortran codes, from many of the Perfect Benchmarks R fl to several "Grand Challenge" codes provided by the National Center for Supercomputing Applications (NCSA).
Reference: [17] <author> Lawrence Rauchwerger and David Padua. </author> <title> The LRPD Test: Speculative Run-Time Par-allelization of Loops with Privatization and Reduction Parallelization. </title> <booktitle> Proceedings of the SIGPLAN'95 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference: [18] <author> S. Richardson and M. Ganapathi. </author> <title> Code Optimization Across Procedures. </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1989. </year>
Reference: [19] <author> Dale Allan Schouten. </author> <title> An Overview of Interprocedural Analysis Techniques for High Performance Parallelizing Compilers. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: some language constructs after expansion in source form, the potential for exponential growth in program size, and the inability of modern optimizing compilers to effectively compile large programs after complete inline expansion ([5]), 4 sparked the development of interprocedural data flow analysis techniques, which are surveyed in [9], [12] and <ref> [19] </ref>. Most of the early interprocedural analysis techniques merge information into a flow independent summary form, which saves time or space but loses the precision needed to perform techniques such as array privatization ([20]).
Reference: [20] <author> Peng Tu and David Padua. </author> <title> Automatic Array Privatization. </title> <editor> In Utpal BanerjeeDavid GelernterAlex NicolauDavid Padua, editor, </editor> <booktitle> Proc. Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <booktitle> Lecture Notes in Computer Science., </booktitle> <volume> volume 768, </volume> <pages> pages 500-521, </pages> <month> August 12-14, </month> <year> 1993. </year>
Reference: [21] <author> Peng Tu and David Padua. </author> <title> Gated SSA-Based Demand-Driven Symbolic Analysis for Parallelizing Compilers. </title> <type> Technical Report 1399, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: has already allowed reuse of its technology by several other Polaris transformations. 1 As Polaris evolves, the technology implemented in the Polaris inliner, in combination with other Polaris transformations and the Polaris infrastructure, should allow the implementation of demand-driven flow-sensitive interprocedural analysis techniques which use existing facilities for gated SSA <ref> [21] </ref> and give the power of complete inline expansion without the cost of analyzing any extra code produced by inline expansion which was not needed for parallelization.
Reference: [22] <author> Stephen Weatherford. </author> <title> High-Level Pattern-Matching Extensions to C++ for Fortran Program Manipulation in Polaris. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The support routines for FORBOL, a high-level pattern matching language built using Polaris <ref> [22] </ref>, provide high-performance search and replace routines for trees of expressions. The Polaris inliner uses these FORBOL support routines and several random-access maps to find each expression within a subprogram which contains a pointer to a local symbol.
References-found: 22

