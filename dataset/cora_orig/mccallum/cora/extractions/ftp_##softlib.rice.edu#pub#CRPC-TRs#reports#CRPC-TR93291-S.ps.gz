URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93291-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: An Optimizing FORTRAN D Compiler for MIMD Distributed Memory Machines (Ph.D thesis)  
Author: Chau-Wen Tseng 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  
Date: January 1993  
Pubnum: CRPC-TR93291-S  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah, D. Kuck, and D. Lawrie. </author> <title> On the performance enhancement of paging systems through program analysis and transformations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(5):341-356, </volume> <month> May </month> <year> 1981. </year>
Reference-contexts: Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 8, 11, 40, 77, 132, 150, 189, 202] </ref>.
Reference: [2] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Systems such as Alewife [42], April <ref> [2] </ref>, DASH [137], KSR-1, and Willow [24] attempt to provide a coherent global address space through innovations in hardware and operating systems. Early experiences with these systems have been positive, but show that locality of reference is a major factor in determining performance.
Reference: [3] <author> I. Ahmad, A. Choudhary, G. Fox, K. Parasuram, R. Ponnusamy, S. Ranka, and R. Thakur. </author> <title> Implementation and scalability of Fortran 90D intrinsic functions on distributed memory machines. </title> <type> Technical Report SCCS-256, </type> <institution> NPAC, Syracuse University, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: The run-time library is built on top of the Express communication package to ensure portability across different architectures [167]. Table 8.2 presents some sample performance numbers for a subset of the intrinsic functions on an iPSC/860, details are presented elsewhere <ref> [3] </ref>. The times in the table include both the computation and communication times for each function. These measurements are also displayed in Figure 8.3. Timings in seconds are plotted logarithmically along the Y-axis. The number of processors is plotted along the X-axis.
Reference: [4] <author> A. V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: Reaching Decompositions Calculation. To determine the decomposition of distributed arrays at each point in the program, the compiler calculates reaching decompositions. Locally, it is computed in the same manner as reaching definitions, with each decomposition treated as a "definition" <ref> [4] </ref>. Interprocedural reaching decompositions is a flow-sensitive data-flow problem [20, 61] since dynamic data decomposition is affected by control flow. <p> A reference to one of these arrays constitutes a use of the definition for that array. With this model, the Fortran D compiler can calculate live decompositions in the same manner as live variables <ref> [4] </ref>. Array mapping calls that are not live may be eliminated. One approach would be to calculate live decompositions during interprocedural propagation. During local analysis, we would collect summary information representing control flow and the placement of data decomposition specifications. <p> Together DecompBefore and DecompAfter represent dynamic data decompositions from P whose instantiation have been delayed. We calculate live decompositions by simply propagating uses backwards through the local control flow graph for each procedure <ref> [4] </ref>. A data decomposition statement is live with respect to a variable X only if there is some path between it and a reference to X that is not killed by another decomposition statement or by DecompKill of an intervening call. <p> There are two situations where a decomposition that is live and loop-invariant with respect to variable X may be hoisted out of a loop. They vary slightly from the requirements for loop-invariant code motion <ref> [4] </ref>: * If the decomposition is not used within the loop for X, it may be moved after the loop.
Reference: [5] <author> E. Albert, K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Compiling Fortran 8x array features for the Connection Machine computer system. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Major differences between Kali and the Fortran D compiler include mandatory on clauses for parallel loops, support for alignment, collective communications, and dynamic decomposition. CM Fortran CM Fortran is a version of Fortran 77 extended with vector notation, alignment, and data layout specifications <ref> [5, 196] </ref>. Programmers must explicitly specify data-parallelism through the use of array operations and by marking array dimensions as parallel. The CM Fortran compiler utilizes user-defined interface blocks to specify a data partition for each procedure.
Reference: [6] <author> E. Albert, J. Lukas, and G. Steele, Jr. </author> <title> Data parallel computers and the forall statement. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(2) </volume> <pages> 185-192, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In other words, each iteration gets its own copy of the entire data space that exists before the execution of the loop, and writes its results to a new data space at the end of the loop <ref> [6, 148, 22] </ref>. At the end of a forall loop, any variables that are assigned new values by different iterations have these values merged at the end of the loop. Merges are performed deterministically, by using the value assigned from the latest sequential iteration. <p> In particular, forall loops are deterministic. As a result we believe that it will be easy to understand and use for scientific programmers. The forall loop possesses similar semantics to the CM Fortran forall statement <ref> [196, 6] </ref> and the Myrias PARDO loop [22]. <p> We have adopted many such features into Fortran D. In particular, we have been influenced by alignment specifications from CM Fortran [196], distribution specifications from Kali [127, 153], and structures to handle irregular distributions from Parti [188]. We also incorporated the forall statement from Myrias [22] and CM Fortran <ref> [6] </ref>. The reduce statement in Fortran D is patterned after equivalent reduction functions in Fortran 90 [13]. 11.3 Shared-Memory Compilers Data-parallelism can usually be utilized as loop-level functional parallelism; it comprises most of the usable parallelism in scientific codes when synchronization costs are considered [52].
Reference: [7] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc [9, 10], Ptran <ref> [7] </ref>, ParaScope [35, 59], and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208].
Reference: [8] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: This process, known as scalarization, generally involves making temporary copies of rhs values where necessary to prevent them from being overwritten before they are used. Optimizations known as sectioning may be used to reduce the amount of buffering needed <ref> [8, 11] </ref>. Forall scalarization is discussed in greater detail later in Chapter 8. 4.4.7 Storage Management One of the major responsibilities of the Fortran D compiler is to select and manage storage for all nonlocal array references. The simplest approach is to allocate full-sized arrays on each processor. <p> Fortran 90 array operations allow the programmer to access and modify entire arrays atomically, even if the underlying machine lacks this capability. The Fortran D compiler must divide array operations into sections that fit the hardware of the target machine <ref> [8, 11] </ref>. We defer both loop fusion and sectioning to the Fortran D back end. Loop fusion is deferred because even hand-written Fortran 77 programs can benefit significantly [116, 150]. Sectioning is needed in the back end because forall loops may also be present in Fortran 77D. <p> This enhances the clarity and conciseness of the program, and has the advantage of making parallelism explicit. It is the responsibility of the compiler to efficiently implement array constructs for scalar machines. Previous research has shown that this is a difficult problem <ref> [8, 11] </ref>. One problem is that when Fortran 90 array constructs are used in assignment statements, the entire right-hand side (rhs) must be evaluated before storing the results in the left-hand side (lhs). <p> A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests <ref> [8, 202, 205] </ref>. The current Fortran D back end fuses all adjacent loop nests where legal, if no loop-carried true dependences are introduced. This heuristic does not adversely affect the parallelism or communication overhead 126 CHAPTER 8. <p> Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 8, 11, 40, 77, 132, 150, 189, 202] </ref>. <p> Parameters are added where necessary to provide necessary data partitioning information. Sectioning The final phase of the Fortran D back end completes the scalarization process. After partitioning is performed, the compiler applies sectioning to convert forall loops into do loops <ref> [8, 11] </ref> in the node program. The Fortran D back end detects cases where temporary storage may be needed using data dependence analysis.
Reference: [9] <author> J. R. Allen, D. Callahan, and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Munich, Germany, </address> <month> January </month> <year> 1987. </year>
Reference-contexts: Figure 5.24 presents an example where the statement assigning to Y is aligned by one iteration of the i loop. Loop alignment is used in conjunction with replication by shared-memory parallelizing compilers to break loop-carried dependences <ref> [9, 33] </ref>. In the context of distributed-memory compilation, loop alignment may be used to change the iteration set of a given statement. This can be used to improve guard introduction by adjusting statements in a loop nest so that they possess the same iteration sets. <p> This heuristic does not adversely affect the parallelism or communication overhead 126 CHAPTER 8. COMPILING FORTRAN 77D AND 90D of the resulting program, and should perform well for the simple cases found in practice. More sophisticated algorithms are discussed elsewhere <ref> [9, 84, 115, 150, 202] </ref>. Loop fusion also has the added advantage of being able to improve memory reuse in the resulting program. Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. <p> Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc <ref> [9, 10] </ref>, Ptran [7], ParaScope [35, 59], and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208].
Reference: [10] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: This is especially true for irregular computations, since many parallel loops cannot be detected by the compiler. The compiler is forced to assume loop-carried or inter-iteration dependences that force synchronization to be inserted <ref> [10] </ref>. In a separate situation, the user may wish to design computations such as Jacobi iteration where existing values in an array are used to calculate new values. <p> This can be accomplished by calculating Send and Receive iteration sets. For simple loop nests which do not contain loop-carried (inter-iteration) true dependences <ref> [10] </ref>, These iteration sets may also be used to generate In and Out array index sets that combine messages to a single processor into one message. We describe the formation and use of these sets in more detail in the following sections. <p> Input dependences do not restrict statement order. Dependences may be either loop-independent or loop-carried. Loop-independent dependences occur on the same loop iteration; loop-carried dependences occur on different iterations of a particular loop. The level of a loop-carried dependence is the depth of the loop carrying the dependence <ref> [10] </ref>. Loop-independent dependences have infinite depth. The number of loop iterations separating the source and sink of the loop-carried dependence may be characterized by a dependence distance or direction [205]. The level of a dependence is determined by the first non-zero entry in its distance or direction vector. <p> In Figure 5.1, we use ffi, *, and ? to mark the extent of our contribution to each optimization technique. Shared-memory parallelizing compilers apply program transformations to expose or enhance parallelism in scientific codes, using dependence information to determine their legality and profitability <ref> [10, 117, 132, 205] </ref>. We have found that transformations such as loop interchange, fusion, distribution, alignment, and strip-mining to be also quite useful for optimizing Fortran D programs. <p> In the next section we describe optimizations that try to hide T copy and T transit by overlapping communication with computation. 5.2.1 Message Vectorization Message vectorization has been discussed in the previous chapter. Basically, it uses the results of data dependence analysis <ref> [10, 132] </ref> to combine element messages into vectors. Message vectorization is a loop 5.2. REDUCING COMMUNICATION OVERHEAD 63 based optimization. It extracts communication from within loops, replacing sending a message per loop iteration to one vectorized message preceding the loop. <p> It may be applied only if the source and sink of each dependence are not reversed in the resulting program. This may be determined by examining the distance or direction vector associated with each dependence <ref> [10, 205] </ref>. Strip-mining increases the step size of an existing loop and adds an additional inner loop. The legality of applying strip-mine followed by loop interchange is determined in the same manner as for unroll-and-jam [117]. <p> .gt. 0) send (ZA (2:99,1),my$p-1) if (my$p .lt. 3) recv (ZA (2:99,26),my$p+1) do kk = 2,99,B if (my$p .gt. 0) recv (ZA (kk:kk+B-1,0),my$p-1) do j = 1,25 QA = F 1 (ZA (k,j+1),ZA (k,j-1),ZA (k+1,j),ZA (k-1,j)) enddo enddo if (my$p .lt. 3) send (ZA (kk:kk+B-1,25),my$p+1) enddo enddo for shared-memory programs <ref> [10, 117, 132] </ref>. Because pipelining disturbs the original computation order, the node compiler later permutes inner loops in memory order, to ensure data locality for the local computation [116]. 5.5 Improve Partitioning One of the responsibilities of the Fortran D compiler is to partition computation across processors. <p> We find that the desired order for compilation phases is to apply loop fusion first, followed by partitioning and sectioning. Loop fusion is performed first because it simplifies partitioning by reducing the need to consider inter-loop interactions. It also enables optimizations such as strip-mining and loop interchange <ref> [10, 205] </ref>. In addition, loop fusion does not increase the difficulty of later compiler phases. On the other hand, sectioning is performed last because it can significantly disrupt the existing program structure, increasing the difficulty of partitioning analysis and optimization. <p> Fusing such loops simplifies the partitioning process and enables additional optimizations. Data dependence is a concept developed for vectorizing and parallelizing compilers to characterize memory access patterns at compile time <ref> [10, 132, 205] </ref>. A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests [8, 202, 205]. <p> Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc <ref> [9, 10] </ref>, Ptran [7], ParaScope [35, 59], and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208].
Reference: [11] <author> J. R. Allen and K. Kennedy. </author> <title> Vector register allocation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(10) </volume> <pages> 1290-1317, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: This process, known as scalarization, generally involves making temporary copies of rhs values where necessary to prevent them from being overwritten before they are used. Optimizations known as sectioning may be used to reduce the amount of buffering needed <ref> [8, 11] </ref>. Forall scalarization is discussed in greater detail later in Chapter 8. 4.4.7 Storage Management One of the major responsibilities of the Fortran D compiler is to select and manage storage for all nonlocal array references. The simplest approach is to allocate full-sized arrays on each processor. <p> Fortran 90 array operations allow the programmer to access and modify entire arrays atomically, even if the underlying machine lacks this capability. The Fortran D compiler must divide array operations into sections that fit the hardware of the target machine <ref> [8, 11] </ref>. We defer both loop fusion and sectioning to the Fortran D back end. Loop fusion is deferred because even hand-written Fortran 77 programs can benefit significantly [116, 150]. Sectioning is needed in the back end because forall loops may also be present in Fortran 77D. <p> This enhances the clarity and conciseness of the program, and has the advantage of making parallelism explicit. It is the responsibility of the compiler to efficiently implement array constructs for scalar machines. Previous research has shown that this is a difficult problem <ref> [8, 11] </ref>. One problem is that when Fortran 90 array constructs are used in assignment statements, the entire right-hand side (rhs) must be evaluated before storing the results in the left-hand side (lhs). <p> Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 8, 11, 40, 77, 132, 150, 189, 202] </ref>. <p> Parameters are added where necessary to provide necessary data partitioning information. Sectioning The final phase of the Fortran D back end completes the scalarization process. After partitioning is performed, the compiler applies sectioning to convert forall loops into do loops <ref> [8, 11] </ref> in the node program. The Fortran D back end detects cases where temporary storage may be needed using data dependence analysis. <p> In some cases, the Fortran D compiler can eliminate buffering through program transformations such as loop reversal. In other cases, the compiler can reduce the amount of temporary storage required through data prefetching <ref> [11] </ref>. For instance, in the Jacobi example a more efficient translation would result in: X = A (1) Y = 0.5 * (X + A (i+1)) A (i) = Y endfor This reduces the temporary memory required significantly, from an entire array to two scalars.
Reference: [12] <author> F. Andre, J. Pazat, and H. Thomas. </author> <title> Pandore: A system to manage data distribution. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: It is employed by a number of systems to support irregular computations, including the Fortran D compiler. Pandore Pandore is a compiler for distributed-memory machines that takes as input C programs extended with block, cyclic, and overlapping data distributions <ref> [12, 203] </ref>. Distributed arrays are mapped by the compiler onto a user-declared virtual distributed machine that may be configured as a vector, ring, grid, or torus. The compiler then outputs code in the vdm l intermediate language.
Reference: [13] <author> ANSI X3J3/S8.115. </author> <title> Fortran 90, </title> <month> June </month> <year> 1990. </year>
Reference-contexts: The design of the run-time library is discussed, and an example is used to illustrate the compilation process. 8.1 Introduction Fortran D provides data decomposition specifications that can be applied to Fortran 77 and Fortran 90 <ref> [13] </ref> to produce Fortran 77D and Fortran 90D, respectively. In this chapter, we describe a unified strategy for compiling both Fortran 77D and Fortran 90D into efficient SPMD message-passing programs. <p> We assign to the Fortran 90D front end the remaining task, scalarizing Fortran 90 constructs that have no equivalent in the Fortran 77D intermediate form. There are three principal Fortran 90 language features that must be scalarized: array constructs, where statements, and intrinsic functions <ref> [13] </ref>. Array Constructs Fortran 90 array constructs allow entire arrays to be manipulated atomically. Array sections may also be specified using triplet notation. This enhances the clarity and conciseness of the program, and has the advantage of making parallelism explicit. <p> We also incorporated the forall statement from Myrias [22] and CM Fortran [6]. The reduce statement in Fortran D is patterned after equivalent reduction functions in Fortran 90 <ref> [13] </ref>. 11.3 Shared-Memory Compilers Data-parallelism can usually be utilized as loop-level functional parallelism; it comprises most of the usable parallelism in scientific codes when synchronization costs are considered [52].
Reference: [14] <institution> Applied Parallel Research, Placerville, CA. </institution> <note> Forge 90 Distributed Memory Parallelizer: User's Guide, version 8.0 edition, </note> <year> 1992. </year>
Reference-contexts: However, it does limit the amount of optimization that may be applied at compile-time for simpler computations. Forge90, MIMDizer Forge90, formerly Mimdizer, is an interactive parallelization system for MIMD shared and distributed-memory machines from Applied Parallel Research <ref> [14, 100] </ref>. It performs data-flow and dependence analyses, and also supports loop-level transformations. Associated tools graphically display call graph, control flow, dependence, and profiling information. When programming for distributed-memory machines, users may interactively select block or cyclic distributions for selected array dimensions.
Reference: [15] <author> V. Balasundaram. </author> <title> Translating control parallelism to data parallelism. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Houston, TX, </address> <month> March </month> <year> 1991. </year> <note> 182 BIBLIOGRAPHY </note>
Reference-contexts: The amount of data communicated may be reduced by half if the computation is first performed by the processor owning B and C, then sent to the processor owning A. This optimization is a simple application of the "owner stores" rule proposed by Balasundaram <ref> [15] </ref>. In particular, it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on a single processor, such as in Kali and Arf [127, 209].
Reference: [16] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Analyze communication. Based on the work partition, references that result in nonlocal accesses are marked. 5. Optimize communication. Nonlocal references are examined to determine optimization opportunities. The key optimization, message vectorization, uses the level of loop-carried true dependences to combine element messages into vectors <ref> [16, 212] </ref>. 6. Manage storage. "Overlaps" [212] or buffers are allocated to store nonlocal data. 7. Generate code. The compiler instantiates the communication, data and work partition determined previously, generating a Fortran 77 program with explicit message-passing. <p> To handle loop-carried dependences, In and Out index sets need to be constructed at each loop level. Dependence information may be used to calculate the appropriate loop level for each message, using the algorithms described by Balasundaram et al. and Gerndt <ref> [16, 80] </ref>. <p> Message vectorization forms the basis of our algorithm for introducing and placing communication for nonlocal accesses. Algorithm We use the message vectorization algorithm developed by Balasundaram et al. and Gerndt to calculate the appropriate loop level to insert messages for nonlocal references <ref> [16, 80] </ref>. We define the commlevel for loop 50 CHAPTER 4. BASIC COMPILATION carried dependences to be the level of the dependence. For loop-independent dependences we define it to be the level of the deepest loop common to both the source and sink of the dependence. <p> Recall that message vectorization first calculates commlevel, the level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence. This determines the outermost loop where element messages resulting from the same array reference may be legally combined <ref> [16, 80] </ref>. Vectorized nonlocal accesses are represented as RSDs and stored at the loop at commlevel. <p> In these cases dynamic data decomposition may be used to temporarily change the ownership of data during program execution, exposing parallelism by internalizing cross-processor dependences <ref> [16] </ref>. For instance, consider the two substitution phases in the Alternating Direction Implicit (ADI) integration example in Figure 5.13. The computation wavefront only crosses one spatial dimension in each phase. A fixed column or row data distribution would result in one parallel and one sequential phase. <p> To see how this strategy works, first recall from Chapter 4 that message vectorization uses the level of the deepest loop-carried true dependence to combine messages at outer loop levels <ref> [16, 212] </ref>. Communication for loop-carried dependences is inserted at the beginning of the loop that carries the dependence. Communication for loop-independent dependences is inserted in the body of the loop enclosing both the source and sink of the dependence.
Reference: [17] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 [26]. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs <ref> [17, 103, 113, 114] </ref>. Accurate static estimates of communication and computation are also needed by the compiler to calculate block sizes for coarse-grain pipelining. 6.4 Scalability The scalability of an optimization describes whether its effectiveness increases, decreases, or remains constant in proportion to some characteristic. <p> The results of executing the training set on a parallel machine are summarized and used to train the performance estimator for that machine. By utilizing training sets, the performance estimator achieves both accuracy and portability across different machine architectures <ref> [17, 114] </ref>. As discussed in Chapter 6, the Fortran D compiler also uses training set information to guide optimizations, particularly balancing communication and parallelism for coarse-grain pipelining. 10.2.2 Automatic Data Partitioner The goal of the automatic data partitioner is to choose an efficient data decomposition. <p> The Fortran D data partitioner, shown in Figure 10.1, supports this through the use of static performance estimation. Data decompositions are selected using standard heuristics, then evaluated using a combination of the Fortran D compiler and static performance estimation <ref> [17, 103, 113] </ref>. The resulting data decompositions should be efficient for both the compiler and parallel machine. Note that even though it is desirable, to assist compilation the static performance estimator does not need to predict absolute performance. <p> Instead, it only needs to accurately predict the performance of a program version relative to other versions. The prototype performance estimator has proved quite precise, especially in predicting the relative performances of different data decompositions <ref> [17] </ref>. We believe that for regular loosely synchronous problems written in a data-parallel programming style, the automatic data partitioner can determine an efficient partitioning scheme without user interaction. The automatic data partitioner can also be used to interactively suggest possible data decompositions and estimate their efficiency.
Reference: [18] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Regular section descriptors (RSDs) Regular section descriptors (RSDs) are widely used in the Fortran D compiler as an internal representation. Originally developed to summarize array side effects across procedure boundaries, RSDs are compact representations of rectangular or right-triangular array sections and their higher dimension analogs <ref> [18, 37, 98] </ref>. They may also possess some constant step. The union and intersection of RSDs can be calculated inexpensively, making them highly useful for the Fortran D compiler. RSDs have also proven to be quite precise in practice, due to the regular memory access patterns exhibited by scientific programs. <p> These codes are poor targets in any case for massively-parallel machines. Regular section descriptors appear to be adequate for describing index and iteration sets for parallel stencil programs. More complex descriptors such as simple sections <ref> [18] </ref> may be required for blocked linear algebra computations, as they frequently access data in trapezoidal sections. Kill analysis for scalars and array sections can aid the Fortran D compiler but are not essential; they seem more useful for guiding automatic data decomposition.
Reference: [19] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases <ref> [19, 83, 133, 208] </ref>. Program transformations based on dependences may also be used to expose additional parallelism [117, 118, 119, 164]. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings.
Reference: [20] <author> J. Banning. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Conference Record of the Sixth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> San Antonio, TX, </address> <month> January </month> <year> 1979. </year>
Reference-contexts: Reaching Decompositions Calculation. To determine the decomposition of distributed arrays at each point in the program, the compiler calculates reaching decompositions. Locally, it is computed in the same manner as reaching definitions, with each decomposition treated as a "definition" [4]. Interprocedural reaching decompositions is a flow-sensitive data-flow problem <ref> [20, 61] </ref> since dynamic data decomposition is affected by control flow. However, the restriction on the scope of dynamic data decomposition in Fortran D means that reaching decompositions for a procedure is only dependent on control flow in its callers, not its callees. <p> Formally, Appear (P ) = Gmod (P ) [ Gref (P ). Gmod and Gref represent the variables modified or referenced by a procedure or its descendants [61]. The value of Appear is readily available from interprocedural scalar side-effect analysis <ref> [20, 62] </ref>. We also define a function Filter (R; V ) that removes from R all decompositions elements hD; Xi where X 62 V , returning the remaining decomposition elements. <p> is locally assigned a decomposition D that differs from the inherited decomposition D 0 then add hD 0 ; Xi to DecompAfter endfor endfor 7.4.4 Aliasing Two variables X and Y are aliased at some point in the program if X and Y may refer to the same memory location <ref> [20] </ref>. In Fortran 77, aliases arise through parameter passing, either between reference parameters of a procedure if the same memory location is passed to both formals, or between a global and formal to which it is passed.
Reference: [21] <author> A. Beguelin, J. Dongarra, G. Geist, R. Manchek, and V. Sunderam. </author> <title> Graphical development tools for network-based concurrent supercomputing. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: High-level languages such as Delirium [147], Linda [41], PCN [71] and Strand [70] are valuable when used to coordinate coarse-grained functional parallelism, as are graphical programming languages such as CODE [159], HeNCE <ref> [21] </ref>, and Schedule [68]. Both require the user to develop explicit parallel programs. In comparison, declarative languages such as Jade can exploit coarse-grain parallelism at run-time, using side effect information collected from user annotations [135]. However, all these parallel languages tend to be inefficient or burdensome for exploiting data-parallelism.
Reference: [22] <author> M. Beltrametti, K. Bobey, and J. Zorbas. </author> <title> The control mechanism for the Myrias parallel computer system. </title> <journal> Computer Architecture News, </journal> <volume> 16(4) </volume> <pages> 21-30, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: In other words, each iteration gets its own copy of the entire data space that exists before the execution of the loop, and writes its results to a new data space at the end of the loop <ref> [6, 148, 22] </ref>. At the end of a forall loop, any variables that are assigned new values by different iterations have these values merged at the end of the loop. Merges are performed deterministically, by using the value assigned from the latest sequential iteration. <p> In particular, forall loops are deterministic. As a result we believe that it will be easy to understand and use for scientific programmers. The forall loop possesses similar semantics to the CM Fortran forall statement [196, 6] and the Myrias PARDO loop <ref> [22] </ref>. In fact, the forall statement in CM Fortran is simply a special form of the forall loop|one that has only one statement in the loop body. 2.9.3 Restrictions With the assistance of dependence analysis, forall loops with regular computation patterns may be compiled into efficient code. <p> We have adopted many such features into Fortran D. In particular, we have been influenced by alignment specifications from CM Fortran [196], distribution specifications from Kali [127, 153], and structures to handle irregular distributions from Parti [188]. We also incorporated the forall statement from Myrias <ref> [22] </ref> and CM Fortran [6]. The reduce statement in Fortran D is patterned after equivalent reduction functions in Fortran 90 [13]. 11.3 Shared-Memory Compilers Data-parallelism can usually be utilized as loop-level functional parallelism; it comprises most of the usable parallelism in scientific codes when synchronization costs are considered [52].
Reference: [23] <author> J. Bennett, J. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: Early experiences with these systems have been positive, but show that locality of reference is a major factor in determining performance. Researchers are also developing techniques to provide a global address space on distributed-memory machines entirely through software. Distributed shared-memory systems such as Amber [48], Ivy [142], Munin <ref> [23] </ref>, and Platinum [64] utilize the operating system to detect and expedite interprocessor data motion. These software-based systems incur significantly greater overhead, but have the advantage of not requiring additional specialized hardware.
Reference: [24] <author> J. Bennett, S. Dwarkadas, J. Greenwood, and E. Speight. </author> <title> Willow: A scalable shared memory multiprocessor. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minnesota, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Systems such as Alewife [42], April [2], DASH [137], KSR-1, and Willow <ref> [24] </ref> attempt to provide a coherent global address space through innovations in hardware and operating systems. Early experiences with these systems have been positive, but show that locality of reference is a major factor in determining performance.
Reference: [25] <author> G. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Vcode Vcode is data-parallel intermediate language [49, 51]. It is designed to allow easy porting of data-parallel languages between parallel architectures. Initial implementations target the Thinking Machines CM-2, Encore Multimax, and Cray Y-MP. In Vcode, computation is performed as segmented scans on vectors <ref> [25] </ref>. Vcode demonstrates the usefulness of segmented scans, but possesses severe shortcomings. A major problem is that most information available in the original program is lost during the translation to Vcode, and must must be recaptured through difficult analysis to enable efficient compilation [50].
Reference: [26] <author> S. Bokhari. </author> <title> Complete exchange on the iPSC-860. </title> <type> ICASE Report 91-4, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: The nonlocal RSDs for P roc (1) and P roc (2:3) are both <ref> [2:99, 26] </ref> and are therefore combined. The RSD for P roc (4) consists of only local data and is discarded. The sending processor is determined by computing the owners of the section [2:99,26] @ P roc (1:3), resulting in P roc (2:4) sending data to their left processors. <p> For most MIMD distributed-memory machines, the cost to send the first byte is significantly higher than the cost for additional bytes. For instance, the Intel iPSC/860 requires approximately 95 sec to send one byte versus .4 sec for each additional byte <ref> [26] </ref>. The following optimizations seek to reduce T start by eliminating messages, reducing the total number of messages sent. <p> However, when communication takes place between groups of processors in regular patterns, message overhead can be reduced by utilizing fast collective communication routines instead of generating individual messages <ref> [26] </ref>. Examples of collective communication routines include broadcast, all-to-all, transpose, global concatenation, and global sum. Differing interdimensional alignments between lhs and rhs references point out the need for transpose, while symbolic analysis detects reductions that can use collective communication to accumulate partial results. <p> Fortunately this is relatively true for the small block sizes that are selected. More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 <ref> [26] </ref>. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs [17, 103, 113, 114].
Reference: [27] <author> T. Brandes. </author> <title> Efficient data parallel programming without explicit message passing for distributed memory multiprocessors. </title> <type> Internal Report AHR-92-4, </type> <institution> High Performance Computing Center, GMD, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Additional Compilers and Systems We briefly review the large number of distributed-memory compilers and systems that have been developed. The first group of compilers target Fortran 90. Adapt [154] and Adaptor <ref> [27] </ref> both perform translations relying on run-time support from a portable library. Wu & Fox describe development of a Fortran 90D compiler developed and validated via a test-suite approach [210]. These compilers perform little analysis or optimization, extracting parallelism from Fortran 90 array syntax or parallel loop annotations.
Reference: [28] <author> P. Brezany, M. Gerndt, P. Mehrotra, and H. Zima. </author> <title> Concurrent file operations in a High Performance Fortran. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minnesota, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: The current Fortran D compiler inserts guards to ensure all I/O is performed on a single processor. More advanced systems will need to deal with parallel I/O is a more comprehensive manner <ref> [28] </ref>. BIBLIOGRAPHY 181
Reference: [29] <author> P. Brezany, M. Gerndt, V. Sipkova, and H. Zima. </author> <title> SUPERB support for irregular scientific computations. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Vienna Fortran Superb has recently been adapted for a new language called Vienna Fortran [45]. Vienna Fortran does not provide a decomposition, but possesses alignment and distribution specifications similar to Fortran D. It supports explicit processor array declarations, irregular computations, performance estimation, and automatic data decomposition <ref> [29, 43, 69] </ref>. Dynamic data decomposition is permitted. Vienna Fortran allows the user to specify additional attributes for each distributed array [44]. Restore forces an array to be restored to its decomposition at procedure entry.
Reference: [30] <author> P. Briggs, K. Cooper, M. W. Hall, and L. Torczon. </author> <title> Goal-directed interprocedural optimization. </title> <type> Technical Report TR90-147, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: An example of this translation is the Translate function in Figure 7.3. Translation must also deal with array reshaping across procedure boundaries. Interprocedural symbolic analysis used in conjunction with linearization and delinearization of array references can discover standard reference patterns that may be compiled efficiently <ref> [30, 92, 98] </ref>. The augmented call graph construction algorithm has a local and interprocedural phase. During local analysis, a node is created for each procedure and augmented with loop information. Loops nodes are created for each loop.
Reference: [31] <author> M. Bromley, S. Heller, T. McNerney, and G. Steele, Jr. </author> <title> Fortran at ten gigaflops: The Connection Machine convolution compiler. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: In the remainder of this chapter, we describe each optimization and provide motivating examples using a small selection of scientific program kernels adapted from the Livermore Kernels and finite-difference algorithms [151]. They contain stencil computations and reductions, techniques commonly used by scientific programmers to solve partial differential equations (PDEs) <ref> [31, 74] </ref>. For clarity we ignore boundary conditions and use constant loop bounds and machine size in the examples, though this is not required by the optimizations. <p> Later compilers improve performance by operating slice-wise, using 32 bit slices to take advantage of the 32-bit Weitek floating-point processors on the CM-2 [187]. The stencil compiler avoids unnecessary intra-processor data motion, inserting communication only for data located on a separate physical, rather than virtual, processor <ref> [31] </ref>. It resembles a highly specialized Fortran D compiler for stencil computations, including optimizations at the assembly code level to improve register usage. The CM Fortran compiler has been retargeted for the CM-5, but results show that it fails to fully exploit the CM-5 architecture [186].
Reference: [32] <author> M. Burke and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <note> to appear 1993. </note>
Reference-contexts: Rather than recompiling the entire program after each change, ParaScope performs recompilation analysis to pinpoint modules that may have been affected by program changes, thus reducing recompilation costs <ref> [32, 63] </ref>. This process is described in greater detail in Section 7.6. ParaScope computes interprocedural ref, mod, alias and constants. Implementations are underway to solve a number of other important interprocedural problems, including interprocedural symbolic and RSD analysis. <p> classification partition data and computation analyze and optimize communication calculate number, size, and type of overlaps & buffers calculate live, loop-invariant decompositions generate code, collect information for callers endfor 7.6 Recompilation Analysis The Fortran D compiler will follow the ParaScope approach for limiting recompilation in the presence of interprocedural optimization <ref> [32, 63] </ref>. Recompilation analysis is used to limit recompilation of a program following changes, an important component to maintaining the advantages of separate compilation. <p> A simple test just verifies that the old information is equal to the new information. However, safe tests that generate less recompilation are possible if we consider how the information will be used. Improved recompilation tests for many scalar data-flow problems are described by Burke and Torczon <ref> [32] </ref>. To give the flavor of the recompilation tests, we describe the test for reaching decompositions. Let oldP be the representation of P from the previous compilation. <p> A little more work is needed to calculate the extent of recompilation in the presence of cloning based on reaching decompositions <ref> [32, 92] </ref>. The compiler maintains a mapping from procedures in the call graph to the list of compiled clones for that procedure. For a procedure that has been cloned, the recompilation test can be applied to all the clones in order to find a match for the procedure.
Reference: [33] <author> D. Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1987. </year> <note> BIBLIOGRAPHY 183 </note>
Reference-contexts: Figure 5.24 presents an example where the statement assigning to Y is aligned by one iteration of the i loop. Loop alignment is used in conjunction with replication by shared-memory parallelizing compilers to break loop-carried dependences <ref> [9, 33] </ref>. In the context of distributed-memory compilation, loop alignment may be used to change the iteration set of a given statement. This can be used to improve guard introduction by adjusting statements in a loop nest so that they possess the same iteration sets.
Reference: [34] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Compilation techniques have been developed to improve data locality on scalar and shared-memory machines. Program transformations can enhance temporal and spatial locality of scientific codes, improving the usage of higher levels of the memory hierarchy such as registers and cache <ref> [34, 40, 116, 173, 204] </ref>. Heuristics have been developed for managing multiprocessors cache coherence in software through the use of block cache invalidate, prefetch, and update instructions [65, 85]. Taken to the limit, these optimizations begin to resemble message vectorization and code generation for distributed-memory machines.
Reference: [35] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: We are implementing the prototype Fortran D compiler in the context of the ParaScope programming environment in order to utilize its analysis and transformation capabilities <ref> [35, 117] </ref>. The prototype compiler automatically derives from the data decomposition node programs for MIMD distributed-memory machines; its goal is to minimize both load imbalance and communications costs. This thesis describes the design, implementation, and evaluation of the prototype Fortran D compiler. <p> We show that it is also highly useful for guiding compiler optimizations for distributed-memory machines. The prototype Fortran D compiler is being developed in the context of the ParaScope programming environment and incorporates the following analysis capabilities <ref> [35, 117] </ref>. Scalar data-flow analysis Control flow, control dependence, and live range information are computed during the scalar data-flow analysis phase. <p> A case study of dgefa is used to demonstrate the effectiveness of interprocedural analysis and optimization. 7.2 Interprocedural Support in ParaScope ParaScope is a programming environment for scientific Fortran programmers. It has fostered research on aggressive optimization of scientific codes for both scalar and shared-memory machines <ref> [35] </ref>. Its pioneering work on incorporating interprocedural optimization in an efficient compilation system has also contributed the development of the Convex Applications compiler [146]. Through careful design, the compilation process in ParaScope preserves separate compilation of procedures to a large extent. <p> Additional passes over the code can be added if necessary, but should be avoided since experience has shown that examination of source code dominates analysis time. The existing compilation system uses the following 3-phase approach <ref> [35, 62, 92] </ref>: 100 CHAPTER 7. INTERPROCEDURAL COMPILATION 1. Local Analysis. At the end of an editing session, ParaScope calculates and stores summary information concerning all local interprocedural effects for each procedure. <p> The Fortran 90D and 77D front ends process input programs into the common intermediate form. The Fortran D back end then compiles this to the SPMD message-passing node program. The Fortran D compiler is implemented in the context of the ParaScope programming environment <ref> [35] </ref>. 8.3.1 Fortran 90D Front End The function of the Fortran 90D front end is to scalarize the Fortran 90D program, translating it to an equivalent Fortran 77D program. <p> Results are discussed and used to point out directions for future research. 9.2 Fortran D Compiler Prototype The prototype Fortran D compiler is implemented as a source-to-source Fortran translator in the context of the ParaScope parallel programming environment <ref> [35, 59] </ref>. It utilizes existing tools for performing dependence analysis, program transformations, and interprocedural analysis [62, 83, 117]. The design of the prototype compiler has been described in the preceding chapters. <p> Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc [9, 10], Ptran [7], ParaScope <ref> [35, 59] </ref>, and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208].
Reference: [36] <author> D. Callahan, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Call sites contained directly in X are also recorded. Loop bounds and step are stored as constants or jump functions, functions that describe the value of a variable as a function of input variables to the procedure (i.e., formal parameters and global variables that may have constant values) <ref> [36] </ref>. During interprocedural propagation call edges are added to the call graph for each call site. Straightforward examination of recorded information is usually sufficient. If a procedure-valued formal parameter is invoked, further analysis is required to determine all procedure names that could be bound to it [92].
Reference: [37] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Regular section descriptors (RSDs) Regular section descriptors (RSDs) are widely used in the Fortran D compiler as an internal representation. Originally developed to summarize array side effects across procedure boundaries, RSDs are compact representations of rectangular or right-triangular array sections and their higher dimension analogs <ref> [18, 37, 98] </ref>. They may also possess some constant step. The union and intersection of RSDs can be calculated inexpensively, making them highly useful for the Fortran D compiler. RSDs have also proven to be quite precise in practice, due to the regular memory access patterns exhibited by scientific programs.
Reference: [38] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: There are two major concerns in compiling Fortran D for MIMD distributed-memory machines. * Partition data and computation across processors. * Generate communications where needed to access nonlocal data. Our philosophy is to use the "owner computes" rule, where every processor only performs computation for data it owns <ref> [212, 38, 178] </ref>. <p> For clarity, we assume that the compiler targets a machine with four processors. 3.2.1 Run-time Resolution A simple compilation technique known as run-time resolution yields code that explicitly calculates the ownership and communication for each reference at run-time <ref> [38, 178, 212] </ref>. For instance, for the previous example it generates the code shown in Figure 3.2. Run-time resolution does not require much compiler analysis, but the resulting programs are likely to extremely inefficient. In fact, they may execute much slower than the original sequential code. <p> Other communication and parallelism optimizations are deferred until Chapter 5. 4.3.1 Message Vectorization A naive but workable algorithm known as run-time resolution inserts guarded send and/or recv operations directly preceding each nonlocal reference <ref> [38, 178, 212] </ref>. Unfortunately, this simple approach generates many small messages that prove extremely inefficient due to communication overhead [178]. The most basic communication optimization performed by the Fortran D compiler is message vectoriza-tion. <p> With multiple statements in the loop, the local iteration set of a statement may be a subset of the reduced loop bounds. For these statements the compiler needs to add explicit guards based on membership tests for the local iteration set of the statement <ref> [38, 178, 212] </ref>. In other cases, the compiler may not be able to localize loop bounds and indices because a processors executes some statement on all iterations of the loop. Statement groups formed during partitioning analysis help detect this situation. <p> The inspector strategy is not applicable for unanalyzable references causing loop-carried true dependences. In this case the Fortran D compiler inserts guards to resolve the needed communication and program execution at run-time <ref> [38, 178, 212] </ref>. 4.4.6 Forall Scalarization Another responsibility of the Fortran D compiler is to convert forall loops into do loops, inserting additional code where necessary to maintain the semantics of the forall loop. <p> This approach is simpler and provides greater flexibility. For instance, the Fortran D compiler performs transformations on the original program without needing to consider potentially introducing deadlock due to message reordering. Callahan & Kennedy Callahan & Kennedy proposed distributed-memory compilation techniques based on data-dependence driven program transformations <ref> [38] </ref>. These techniques were implemented in a prototype compiler in the ParaScope programming environment. In the compiler, standard and user-defined distribution functions are used to specify the data decomposition for sequential Fortran programs.
Reference: [39] <author> D. Callahan, K. Kennedy, and U. Kremer. </author> <title> A dynamic study of vectorization in PFC. </title> <type> Technical Report TR89-97, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: These compilers can take Fortran or C programs written in a vectorizable style and automatically convert them to run efficiently on any vector machine <ref> [206, 39] </ref>. This provides a machine-independent programming model that allows scientific programmers to concentrate on their actual algorithms, introducing high-level parallelism where needed. The compiler handles machine-dependent optimizations for efficient execution. The resulting programs are easily maintained and portable. Compare this with the task of programming existing parallel machines.
Reference: [40] <author> S. Carr, K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <type> Technical Report TR92-195, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 8, 11, 40, 77, 132, 150, 189, 202] </ref>. <p> Compilation techniques have been developed to improve data locality on scalar and shared-memory machines. Program transformations can enhance temporal and spatial locality of scientific codes, improving the usage of higher levels of the memory hierarchy such as registers and cache <ref> [34, 40, 116, 173, 204] </ref>. Heuristics have been developed for managing multiprocessors cache coherence in software through the use of block cache invalidate, prefetch, and update instructions [65, 85]. Taken to the limit, these optimizations begin to resemble message vectorization and code generation for distributed-memory machines.
Reference: [41] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: We believe that Fortran D provides a simple machine-independent programming model for most data-parallel computations. 2.1 Introduction High-level parallel languages such as Delirium [147], Linda <ref> [41] </ref>, and Strand [70] are valuable when used to coordinate coarse-grained functional parallelism. However, these languages do not meet the needs of computational scientists because they do not elegantly describe data-parallel computations of the type described by Hillis and Steele [101] and Karp [111]. <p> However, these parallel programming models are intended to guide the development of new data-parallel algorithms; they cannot be used to help scientists write parallel programs since no language or compiler support is provided. High-level languages such as Delirium [147], Linda <ref> [41] </ref>, PCN [71] and Strand [70] are valuable when used to coordinate coarse-grained functional parallelism, as are graphical programming languages such as CODE [159], HeNCE [21], and Schedule [68]. Both require the user to develop explicit parallel programs.
Reference: [42] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Systems such as Alewife <ref> [42] </ref>, April [2], DASH [137], KSR-1, and Willow [24] attempt to provide a coherent global address space through innovations in hardware and operating systems. Early experiences with these systems have been positive, but show that locality of reference is a major factor in determining performance.
Reference: [43] <author> B. Chapman, H. Herbeck, and H. Zima. </author> <title> Automatic support for data distribution. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Vienna Fortran Superb has recently been adapted for a new language called Vienna Fortran [45]. Vienna Fortran does not provide a decomposition, but possesses alignment and distribution specifications similar to Fortran D. It supports explicit processor array declarations, irregular computations, performance estimation, and automatic data decomposition <ref> [29, 43, 69] </ref>. Dynamic data decomposition is permitted. Vienna Fortran allows the user to specify additional attributes for each distributed array [44]. Restore forces an array to be restored to its decomposition at procedure entry.
Reference: [44] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Handling distributed data in Vienna Fortran procedures. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: It supports explicit processor array declarations, irregular computations, performance estimation, and automatic data decomposition [29, 43, 69]. Dynamic data decomposition is permitted. Vienna Fortran allows the user to specify additional attributes for each distributed array <ref> [44] </ref>. Restore forces an array to be restored to its decomposition at procedure entry. Notransfer causes remapping to be performed logically, rather than actually copying the values in the array. Nocopy guarantees that its formal and actual parameters have the same data decomposition.
Reference: [45] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Communications utilizing Express primitives are then automatically generated. Like Forge90, Aspar performs less compile-time analysis and optimization, relying instead heavily on run-time support. Vienna Fortran Superb has recently been adapted for a new language called Vienna Fortran <ref> [45] </ref>. Vienna Fortran does not provide a decomposition, but possesses alignment and distribution specifications similar to Fortran D. It supports explicit processor array declarations, irregular computations, performance estimation, and automatic data decomposition [29, 43, 69]. Dynamic data decomposition is permitted.
Reference: [46] <author> C. Chase, A. Cheung, A. Reeves, and M. Smith. </author> <title> Paragon: A parallel programming environment for scientific applications using communication structures. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: RELATED WORK support parallel task creation, data and computation mapping, synchronization, and communication [180]. Dyno provides support for irregular and adaptive numeric programs [203]. Paragon Paragon is a C-based programming environment targeted at supporting SIMD programs on MIMD distributed-memory machines <ref> [46, 177] </ref>. It provides both language extensions to C and run-time support for task management and load balancing. Data distribution in Paragon may either be performed by the user or the system. Parallel arrays are mapped onto shapes that consist of arbitrary rectangular distributions.
Reference: [47] <author> C. Chase, K. Crowley, J. Saltz, and A. Reeves. </author> <title> Compiler and runtime support for irregularly coupled regular meshes. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The distribution functions for decomposition A and array X are then computed through run-time preprocessing techniques [188, 153]. Researchers are examining more sophisticated methods of specify irregular distributions for Fortran D programs <ref> [47, 172, 209] </ref>. 3.3.2 Computation We continue to describe some additional notation we will employ later in this paper. <p> The location of each array element may be determined at run-time by checking the distribution map stored on each processor. Redistribution and replication of arrays and subarrays, as well as permutation and reduction mechanism are supported. Paragon has been extended to handle the special class of irregularly-coupled regular-meshes <ref> [47] </ref>. It does not currently perform analysis or transformations to detect or enhance parallelism. ARF, PARTI Arf is a compiler for irregular computations [209]. It provides block and cyclic distributions, and is the first compiler to support user-defined irregular distributions.
Reference: [48] <author> J. Chase, F. Amador, E. Lazowska, H. Levy, and R. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: Early experiences with these systems have been positive, but show that locality of reference is a major factor in determining performance. Researchers are also developing techniques to provide a global address space on distributed-memory machines entirely through software. Distributed shared-memory systems such as Amber <ref> [48] </ref>, Ivy [142], Munin [23], and Platinum [64] utilize the operating system to detect and expedite interprocessor data motion. These software-based systems incur significantly greater overhead, but have the advantage of not requiring additional specialized hardware.
Reference: [49] <author> S. Chatterjee and G. Blelloch. </author> <title> VCODE: A data-parallel intermediate language. </title> <booktitle> In Frontiers'90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: No alignment and or distribution specifications are provided. It is not clear how Spot will support computation patterns that cannot be described by stencils. Vcode Vcode is data-parallel intermediate language <ref> [49, 51] </ref>. It is designed to allow easy porting of data-parallel languages between parallel architectures. Initial implementations target the Thinking Machines CM-2, Encore Multimax, and Cray Y-MP. In Vcode, computation is performed as segmented scans on vectors [25]. Vcode demonstrates the usefulness of segmented scans, but possesses severe shortcomings.
Reference: [50] <author> S. Chatterjee, G. Blelloch, and A. Fisher. </author> <title> Size and access inference for data-parallel programs. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year> <note> 184 BIBLIOGRAPHY </note>
Reference-contexts: Vcode demonstrates the usefulness of segmented scans, but possesses severe shortcomings. A major problem is that most information available in the original program is lost during the translation to Vcode, and must must be recaptured through difficult analysis to enable efficient compilation <ref> [50] </ref>. Additional Compilers and Systems We briefly review the large number of distributed-memory compilers and systems that have been developed. The first group of compilers target Fortran 90. Adapt [154] and Adaptor [27] both perform translations relying on run-time support from a portable library.
Reference: [51] <author> S. Chatterjee, G. Blelloch, and M. Zagha. </author> <title> Scan primitives for vector computers. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: Scans are similar but perform parallel-prefix operations instead. A sum scan would return the sums of all the prefixes of an array. Scans are used to solve a number of computations in scientific codes, including linear recurrences and tridiagonal systems <ref> [51, 128] </ref>. The Fortran D compiler applies dependence analysis to recognize reductions and scans. If the reduction or scan accesses data in a manner that sequentializes computation across processors, the Fortran D compiler may parallelize it by relaxing the "owner computes" rule and providing methods to combine partial results. <p> Figure 5.12 demonstrates how a prefix sum may be computed, using a global-concat collective communication routine to collect the partial sums from each processor in S. The partial sums of all preceding processors are combined locally and used as a basis for computing local prefix sums <ref> [51] </ref>. 5.4. <p> No alignment and or distribution specifications are provided. It is not clear how Spot will support computation patterns that cannot be described by stencils. Vcode Vcode is data-parallel intermediate language <ref> [49, 51] </ref>. It is designed to allow easy porting of data-parallel languages between parallel architectures. Initial implementations target the Thinking Machines CM-2, Encore Multimax, and Cray Y-MP. In Vcode, computation is performed as segmented scans on vectors [25]. Vcode demonstrates the usefulness of segmented scans, but possesses severe shortcomings.
Reference: [52] <author> D. Chen, H. Su, and P. Yew. </author> <title> The impact of synchronization and granularity on parallel systems. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The reduce statement in Fortran D is patterned after equivalent reduction functions in Fortran 90 [13]. 11.3 Shared-Memory Compilers Data-parallelism can usually be utilized as loop-level functional parallelism; it comprises most of the usable parallelism in scientific codes when synchronization costs are considered <ref> [52] </ref>. Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc [9, 10], Ptran [7], ParaScope [35, 59], and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines.
Reference: [53] <author> M. Chen, Y. Choo, and J. Li. </author> <title> Theory and pragmatics of compiling efficient parallel code. </title> <type> Technical Report YALEU/DCS/TR-760, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: It can also vectorize messages in sequential regions, such as those found in SOR or ADI integration. Many language-driven compilers are improving their compile-time analysis and optimization, especially CM Fortran. Crystal Crystal is a high-level functional language <ref> [53, 138, 139, 140, 141] </ref>. Because it targets a functional language, the Crystal compiler possesses markedly different program analysis techniques than compilers for imperative languages such as Fortran. However, it performs significant compile-time analysis and optimization, pioneering both automatic data decomposition and collective communications generation techniques.
Reference: [54] <author> M. Chen and J. Cowie. </author> <title> Prototyping Fortran-90 compilers for massively parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Static execution time estimation is used to choose between sequential and parallel code. P 3 C emphasizes portability rather than complex program analysis. Fortran-90-Y The Fortran-90-Y compiler is designed to apply formal specification techniques to generate efficient code for the CM-2 and CM-5 <ref> [54] </ref>. It is intended to support rapid prototyping of compilation and optimization techniques. The Fortran-90-Y compile uses Yale Intermediate Representation as the basis for performing machine-independent program transformations. It achieves performance comparable to that of the production CM Fortran compiler.
Reference: [55] <author> M. Chen and Y. Hu. </author> <title> Optimizations for compiling iterative spatial loops to massively parallel machines. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: The Fortran-90-Y compiler also supports research on optimization techniques for distributed-memory machines. A transformation strategy is presented to improve parallelism for iterative spatial Fortran 90 loops on the CM-2, using a combination of loop interchange, skew, and strip-mining <ref> [55] </ref>. These transformations are similar to Fortran D program transformations for optimizing pipelined computations on MIMD distributed-memory machines. Techniques are also developed for algebraic representation of data motion and data layout for Yale Extensions, a set of data layout declarations [56].
Reference: [56] <author> M. Chen and J. Wu. </author> <title> Optimizing FORTRAN-90 programs for data motion on massively parallel systems. </title> <type> Technical Report YALE/DCS/TR-882, </type> <institution> Dept. of Computer Science, Yale University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: These transformations are similar to Fortran D program transformations for optimizing pipelined computations on MIMD distributed-memory machines. Techniques are also developed for algebraic representation of data motion and data layout for Yale Extensions, a set of data layout declarations <ref> [56] </ref>. Variable references are translated into communication expressions and optimized using communication algebra through idioms. 175 Chapter 12 Conclusions The Fortran D compiler demonstrates that with minimal language and run-time support, advanced compilation technology can produce efficient programs for MIMD distributed-memory machines.
Reference: [57] <author> A. Choudhary, G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, S. Ranka, and C. Tseng. </author> <title> Compiling Fortran 77D and 90D for MIMD distributed-memory machines. </title> <booktitle> In Frontiers'92: The 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Here we briefly examine alternatives explored by other researchers, including parallel architectures & operating systems, programming models & languages, and shared-memory paral-lelizing compilers. We also describe other distributed-memory compilation systems in relation to Fortran D <ref> [57, 73, 93, 104, 105, 106, 107] </ref>. 11.1 Parallel Architectures and Operating Systems Because of high interprocessor communication costs, many researchers believe efficient general-purpose parallel computing on distributed-memory machines requires some form of architectural and operating systems support.
Reference: [58] <author> T. Clark, R. v. Hanxleden, K. Kennedy, C. Koelbel, and L. R. Scott. </author> <title> Evaluating parallel languages for molecular dynamics computations. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Fortunately, researchers have been investigating techniques for efficiently handling such computations through a combination of compile-time and run-time approaches [125, 155]. There is an ongoing effort to develop and improve support for such irregular computations in the framework of the Fortran D compiler and programming system <ref> [58, 95, 172] </ref>. 12.5.5 Support for Parallel Input/Output Finally, one aspect of automatic parallelization that most researchers prefer to avoid is that of providing support for parallel I/O. The current Fortran D compiler inserts guards to ensure all I/O is performed on a single processor.
Reference: [59] <author> K. Cooper, M. W. Hall, R. T. Hood, K. Kennedy, K. S. McKinley, J. M. Mellor-Crummey, L. Torczon, and S. K. Warren. </author> <title> The ParaScope parallel programming environment. </title> <journal> Proceedings of the IEEE, </journal> <note> to appear 1993. </note>
Reference-contexts: Results are discussed and used to point out directions for future research. 9.2 Fortran D Compiler Prototype The prototype Fortran D compiler is implemented as a source-to-source Fortran translator in the context of the ParaScope parallel programming environment <ref> [35, 59] </ref>. It utilizes existing tools for performing dependence analysis, program transformations, and interprocedural analysis [62, 83, 117]. The design of the prototype compiler has been described in the preceding chapters. <p> Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc [9, 10], Ptran [7], ParaScope <ref> [35, 59] </ref>, and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208].
Reference: [60] <author> K. Cooper, M. W. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Computer Language, </booktitle> <address> Oakland, CA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: ParaScope also contains support for inlining and cloning, two interprocedural transformations that increase the context available for optimization. Inlining merges the body of the called procedure into the caller. Cloning creates a new version of a procedure for specific interprocedural information <ref> [60, 62] </ref>. Existing interprocedural analysis in ParaScope is useful for the Fortran D compiler, but it is not sufficient. The compiler must also incorporate analysis to understand the partitioning of data & computation, and to apply communication optimizations. <p> For instance, the compiler creates two copies of procedure F1 and F2 because they possess two different reaching decompositions for Z. Edges in the call graph are updated appropriately for the clone. In pathological cases, cloning can result in an exponential growth in program size <ref> [60] </ref>. Under these circumstances, cloning may be disabled when a threshold program growth has been exceeded, forcing run-time resolution instead. 7.3.
Reference: [61] <author> K. Cooper and K. Kennedy. </author> <title> Interprocedural side-effect analysis in linear time. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Program Language Design and Implementation, </booktitle> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: Reaching Decompositions Calculation. To determine the decomposition of distributed arrays at each point in the program, the compiler calculates reaching decompositions. Locally, it is computed in the same manner as reaching definitions, with each decomposition treated as a "definition" [4]. Interprocedural reaching decompositions is a flow-sensitive data-flow problem <ref> [20, 61] </ref> since dynamic data decomposition is affected by control flow. However, the restriction on the scope of dynamic data decomposition in Fortran D means that reaching decompositions for a procedure is only dependent on control flow in its callers, not its callees. <p> We define Appear (P ) to be the set of formal parameters and global variables appearing in procedure P or its descendants. Formally, Appear (P ) = Gmod (P ) [ Gref (P ). Gmod and Gref represent the variables modified or referenced by a procedure or its descendants <ref> [61] </ref>. The value of Appear is readily available from interprocedural scalar side-effect analysis [20, 62]. We also define a function Filter (R; V ) that removes from R all decompositions elements hD; Xi where X 62 V , returning the remaining decomposition elements.
Reference: [62] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the IR n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: Additional passes over the code can be added if necessary, but should be avoided since experience has shown that examination of source code dominates analysis time. The existing compilation system uses the following 3-phase approach <ref> [35, 62, 92] </ref>: 100 CHAPTER 7. INTERPROCEDURAL COMPILATION 1. Local Analysis. At the end of an editing session, ParaScope calculates and stores summary information concerning all local interprocedural effects for each procedure. <p> ParaScope also contains support for inlining and cloning, two interprocedural transformations that increase the context available for optimization. Inlining merges the body of the called procedure into the caller. Cloning creates a new version of a procedure for specific interprocedural information <ref> [60, 62] </ref>. Existing interprocedural analysis in ParaScope is useful for the Fortran D compiler, but it is not sufficient. The compiler must also incorporate analysis to understand the partitioning of data & computation, and to apply communication optimizations. <p> Formally, Appear (P ) = Gmod (P ) [ Gref (P ). Gmod and Gref represent the variables modified or referenced by a procedure or its descendants [61]. The value of Appear is readily available from interprocedural scalar side-effect analysis <ref> [20, 62] </ref>. We also define a function Filter (R; V ) that removes from R all decompositions elements hD; Xi where X 62 V , returning the remaining decomposition elements. <p> It utilizes existing tools for performing dependence analysis, program transformations, and interprocedural analysis <ref> [62, 83, 117] </ref>. The design of the prototype compiler has been described in the preceding chapters. <p> Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc [9, 10], Ptran [7], ParaScope [35, 59], and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls <ref> [62, 98, 143] </ref>, and array reference aliases [19, 83, 133, 208]. Program transformations based on dependences may also be used to expose additional parallelism [117, 118, 119, 164]. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings.
Reference: [63] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Rather than recompiling the entire program after each change, ParaScope performs recompilation analysis to pinpoint modules that may have been affected by program changes, thus reducing recompilation costs <ref> [32, 63] </ref>. This process is described in greater detail in Section 7.6. ParaScope computes interprocedural ref, mod, alias and constants. Implementations are underway to solve a number of other important interprocedural problems, including interprocedural symbolic and RSD analysis. <p> classification partition data and computation analyze and optimize communication calculate number, size, and type of overlaps & buffers calculate live, loop-invariant decompositions generate code, collect information for callers endfor 7.6 Recompilation Analysis The Fortran D compiler will follow the ParaScope approach for limiting recompilation in the presence of interprocedural optimization <ref> [32, 63] </ref>. Recompilation analysis is used to limit recompilation of a program following changes, an important component to maintaining the advantages of separate compilation.
Reference: [64] <author> A. Cox and R. Fowler. </author> <title> The implementation of a coherent memory abstraction on a NUMA multiprocessor: Experiences with Platinum. </title> <booktitle> In Proceedings of the Twelfth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: Researchers are also developing techniques to provide a global address space on distributed-memory machines entirely through software. Distributed shared-memory systems such as Amber [48], Ivy [142], Munin [23], and Platinum <ref> [64] </ref> utilize the operating system to detect and expedite interprocessor data motion. These software-based systems incur significantly greater overhead, but have the advantage of not requiring additional specialized hardware. Because of their wide availability, researchers in this area have been particularly interested in targeting networks of high-performance workstations.
Reference: [65] <author> E. Darnell, K. Kennedy, and J. Mellor-Crummey. </author> <title> Automatic software cache coherence through vector-ization. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Heuristics have been developed for managing multiprocessors cache coherence in software through the use of block cache invalidate, prefetch, and update instructions <ref> [65, 85] </ref>. Taken to the limit, these optimizations begin to resemble message vectorization and code generation for distributed-memory machines. Researchers have proposed merging these data locality optimizations with parallelism information for distributed-memory machines.
Reference: [66] <author> E. D'Hollander. </author> <title> Partitioning and labeling of index sets in do loops with constant dependence. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1989. </year> <note> BIBLIOGRAPHY 185 </note>
Reference-contexts: The basic premise is that since parallel loops exhibiting data locality are guaranteed to compute fairly disjoint data sets, the partition of parallel loop iterations among processors can also be used to assign data to each processor <ref> [66, 168, 190] </ref>. Where data accesses are not entirely disjoint, grouping methods can be applied to reduce communication between loop iterations [120, 191]. However, in order to take advantage of data locality, the compiler must take into account affinity, the interaction between data placement and task scheduling [149].
Reference: [67] <author> J. Dongarra, J. Bunch, C. Moler, and G. Stewart. </author> <title> LINPACK User's Guide. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <year> 1979. </year>
Reference-contexts: It must also pass recompilation tests for other interprocedural problems. 7.7 Empirical Results 7.7.1 Compilation Strategies for DGEFA This section demonstrates the effectiveness of interprocedural optimization using the routine dgefa from Linpack, a linear algebra library <ref> [67] </ref>. dgefa is also a major component in Linpackd, the Linpack Benchmark Program. dgefa uses Gaussian elimination with partial pivoting to factor a double-precision floating-point array. A simplified version is shown in Figure 7.16. dgefa relies on three other Linpack routines: idamax, dscal, and daxpy.
Reference: [68] <author> J. Dongarra and D. Sorensen. </author> <title> SCHEDULE: Tools for developing and analyzing parallel Fortran programs. </title> <editor> In D. Gannon, L. Jamieson, and R. Douglass, editors, </editor> <title> The Characteristics of Parallel Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: High-level languages such as Delirium [147], Linda [41], PCN [71] and Strand [70] are valuable when used to coordinate coarse-grained functional parallelism, as are graphical programming languages such as CODE [159], HeNCE [21], and Schedule <ref> [68] </ref>. Both require the user to develop explicit parallel programs. In comparison, declarative languages such as Jade can exploit coarse-grain parallelism at run-time, using side effect information collected from user annotations [135]. However, all these parallel languages tend to be inefficient or burdensome for exploiting data-parallelism.
Reference: [69] <author> T. Fahringer, R. Blasko, and H. Zima. </author> <title> Automatic performance prediction to support parallelization of Fortran programs for massively parallel systems. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Vienna Fortran Superb has recently been adapted for a new language called Vienna Fortran [45]. Vienna Fortran does not provide a decomposition, but possesses alignment and distribution specifications similar to Fortran D. It supports explicit processor array declarations, irregular computations, performance estimation, and automatic data decomposition <ref> [29, 43, 69] </ref>. Dynamic data decomposition is permitted. Vienna Fortran allows the user to specify additional attributes for each distributed array [44]. Restore forces an array to be restored to its decomposition at procedure entry.
Reference: [70] <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: We believe that Fortran D provides a simple machine-independent programming model for most data-parallel computations. 2.1 Introduction High-level parallel languages such as Delirium [147], Linda [41], and Strand <ref> [70] </ref> are valuable when used to coordinate coarse-grained functional parallelism. However, these languages do not meet the needs of computational scientists because they do not elegantly describe data-parallel computations of the type described by Hillis and Steele [101] and Karp [111]. <p> However, these parallel programming models are intended to guide the development of new data-parallel algorithms; they cannot be used to help scientists write parallel programs since no language or compiler support is provided. High-level languages such as Delirium [147], Linda [41], PCN [71] and Strand <ref> [70] </ref> are valuable when used to coordinate coarse-grained functional parallelism, as are graphical programming languages such as CODE [159], HeNCE [21], and Schedule [68]. Both require the user to develop explicit parallel programs.
Reference: [71] <author> I. Foster and S. Tuecke. </author> <title> Parallel programming with PCN. </title> <type> Technical Report ANL-91/32, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: However, these parallel programming models are intended to guide the development of new data-parallel algorithms; they cannot be used to help scientists write parallel programs since no language or compiler support is provided. High-level languages such as Delirium [147], Linda [41], PCN <ref> [71] </ref> and Strand [70] are valuable when used to coordinate coarse-grained functional parallelism, as are graphical programming languages such as CODE [159], HeNCE [21], and Schedule [68]. Both require the user to develop explicit parallel programs.
Reference: [72] <author> G. Fox. </author> <title> Achievements and prospects for parallel computing. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 3(6) </volume> <pages> 725-739, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Though these still comprise the majority of scientific applications, there is an accelerating trend in the computational science community towards irregular computations involving sparse matrixes and adaptive algorithms <ref> [72] </ref>. Fortunately, researchers have been investigating techniques for efficiently handling such computations through a combination of compile-time and run-time approaches [125, 155].
Reference: [73] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Here we briefly examine alternatives explored by other researchers, including parallel architectures & operating systems, programming models & languages, and shared-memory paral-lelizing compilers. We also describe other distributed-memory compilation systems in relation to Fortran D <ref> [57, 73, 93, 104, 105, 106, 107] </ref>. 11.1 Parallel Architectures and Operating Systems Because of high interprocessor communication costs, many researchers believe efficient general-purpose parallel computing on distributed-memory machines requires some form of architectural and operating systems support.
Reference: [74] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: In the remainder of this chapter, we describe each optimization and provide motivating examples using a small selection of scientific program kernels adapted from the Livermore Kernels and finite-difference algorithms [151]. They contain stencil computations and reductions, techniques commonly used by scientific programmers to solve partial differential equations (PDEs) <ref> [31, 74] </ref>. For clarity we ignore boundary conditions and use constant loop bounds and machine size in the examples, though this is not required by the optimizations. <p> After the copy is performed, both buffered and unbuffered messages can overlap communication and computation. 5.4 Exploiting Parallelism 5.4.1 Partitioning Computation Most scientific applications are completely parallel in either a synchronous or loosely synchronous manner <ref> [74] </ref>. In these computations all processors execute SPMD programs in a loose lockstep, alternating between phases of local computation and synchronous global communication. These problems achieve good load balance because all processors are utilized during the computation phase. For instance, Jacobi and Red-black SOR are loosely synchronous computations.
Reference: [75] <author> E. Gabber. </author> <title> VMMP: A practical tool for the development of portable and efficient programs for multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 304-317, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: P 3 C, VMMP P 3 C, the Portable Parallelizing Pascal Compiler, translates sequential Pascal programs into explicit parallel code [76]. The output program relys on Vmmp <ref> [75] </ref>, a portable software environment running on many multiprocessors. P 3 C performs simple analysis to detect and parallelize parallel loops and reductions. Static execution time estimation is used to choose between sequential and parallel code. P 3 C emphasizes portability rather than complex program analysis.
Reference: [76] <author> E. Gabber, A. Averbuch, and A. Yehudai. </author> <title> Experience with a portable parallelizing Pascal compiler. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Its successor, the iWarp compiler, targets the difficult task of simultaneously supporting data-parallelism, coarse-grain task parallelism, and fine-grain systolic parallelism [86, 102, 134]. P 3 C, VMMP P 3 C, the Portable Parallelizing Pascal Compiler, translates sequential Pascal programs into explicit parallel code <ref> [76] </ref>. The output program relys on Vmmp [75], a portable software environment running on many multiprocessors. P 3 C performs simple analysis to detect and parallelize parallel loops and reductions. Static execution time estimation is used to choose between sequential and parallel code.
Reference: [77] <author> G. Gao, R. Olsen, V. Sarkar, and R. Thekkath. </author> <title> Collective loop fusion for array contraction. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 8, 11, 40, 77, 132, 150, 189, 202] </ref>.
Reference: [78] <author> G. Geist and C. Romine. </author> <title> LU factorization algorithms on distributed-memory multiprocessor architectures. </title> <journal> SIAM Journal of Scientific Stat. Computing, </journal> <volume> 9 </volume> <pages> 639-649, </pages> <year> 1988. </year>
Reference-contexts: Due to the large number of global broadcasts required to communicate pivot values and multipliers, performance of Dgefa actually degrades when solving small problems on many processors. To determine whether improved performance is attainable, we created a hand-coded version of Dgefa based on optimizations described in the literature <ref> [78, 157] </ref>. First, we combined the two messages broadcast on each iteration of the outermost k loop. Instead of broadcasting the pivot value immediately, we wait until multipliers are also computed. The values can then be combined in one broadcast.
Reference: [79] <author> M. Gerndt. </author> <title> Automatic Parallelization for Distributed-Memory Multiprocessing Systems. </title> <type> PhD thesis, </type> <institution> University of Bonn, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: The Callahan-Kennedy compiler prototype was eventually abandoned in favor of the greater flexibility of the Fortran D compiler. SUPERB Superb is a semi-automatic parallelization tool designed for MIMD distributed-memory machines <ref> [79, 80, 212] </ref>. It supports arbitrary user-specified contiguous rectangular distributions, and performs dependence analysis to guide interactive program transformations in a manner similar to the ParaScope Editor [129]. Superb originated overlaps as a means to both specify and store nonlocal data accesses.
Reference: [80] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: To handle loop-carried dependences, In and Out index sets need to be constructed at each loop level. Dependence information may be used to calculate the appropriate loop level for each message, using the algorithms described by Balasundaram et al. and Gerndt <ref> [16, 80] </ref>. <p> Message vectorization forms the basis of our algorithm for introducing and placing communication for nonlocal accesses. Algorithm We use the message vectorization algorithm developed by Balasundaram et al. and Gerndt to calculate the appropriate loop level to insert messages for nonlocal references <ref> [16, 80] </ref>. We define the commlevel for loop 50 CHAPTER 4. BASIC COMPILATION carried dependences to be the level of the dependence. For loop-independent dependences we define it to be the level of the deepest loop common to both the source and sink of the dependence. <p> Storage management may be separated into two phases, selection of the desired storage types, and coordinating their usage. The Fortran D compiler analyzes nonlocal references and selects one of the following three storage schemes. Overlaps Overlaps are expansions of local array sections to accommodate neighboring nonlocal elements <ref> [80] </ref>. For regular stencil computations, overlaps are useful because they allow the generation of clear and readable code. For other computations overlaps are inefficient, because all array elements between the local section and the one accessed must also be part of the overlap, even if they are not used. <p> Recall that message vectorization first calculates commlevel, the level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence. This determines the outermost loop where element messages resulting from the same array reference may be legally combined <ref> [16, 80] </ref>. Vectorized nonlocal accesses are represented as RSDs and stored at the loop at commlevel. <p> The Callahan-Kennedy compiler prototype was eventually abandoned in favor of the greater flexibility of the Fortran D compiler. SUPERB Superb is a semi-automatic parallelization tool designed for MIMD distributed-memory machines <ref> [79, 80, 212] </ref>. It supports arbitrary user-specified contiguous rectangular distributions, and performs dependence analysis to guide interactive program transformations in a manner similar to the ParaScope Editor [129]. Superb originated overlaps as a means to both specify and store nonlocal data accesses.
Reference: [81] <author> M. Gerndt. </author> <title> Work distribution in parallel programs for distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Superb originated overlaps as a means to both specify and store nonlocal data accesses. Once program analysis and transformation is complete, communication is automatically generated and vectorized utilizing data dependence information. Computation is partitioned via explicit guards, which may be eliminated by loop bounds reduction <ref> [81] </ref>. Unlike the Fortran D compiler, the original version of Superb did not 170 CHAPTER 11. RELATED WORK support data alignment, cyclic distributions, automatic compilation, collective communications, dynamic data decomposition, and storage of nonlocal values in temporary buffers. Superb performs interprocedural data-flow analysis of parameter passing.
Reference: [82] <author> M. Gerndt. </author> <title> Program analysis and transformations for message-passing programs. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: To perform optimizing transformations, these compilers must keep track of intermediate program versions. However, the presence of guards and explicit communication affects program semantics in ways that are difficult to determine, especially whether program restructuring would cause deadlock <ref> [82] </ref>. In comparison, the Fortran D compiler performs its analysis up front and uses the results to drive code generation. This approach is simpler and provides greater flexibility.
Reference: [83] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: It utilizes existing tools for performing dependence analysis, program transformations, and interprocedural analysis <ref> [62, 83, 117] </ref>. The design of the prototype compiler has been described in the preceding chapters. <p> Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases <ref> [19, 83, 133, 208] </ref>. Program transformations based on dependences may also be used to expose additional parallelism [117, 118, 119, 164]. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings.
Reference: [84] <author> A. Goldberg and R. Paige. </author> <title> Stream processing. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, </booktitle> <pages> pages 228-234, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: This heuristic does not adversely affect the parallelism or communication overhead 126 CHAPTER 8. COMPILING FORTRAN 77D AND 90D of the resulting program, and should perform well for the simple cases found in practice. More sophisticated algorithms are discussed elsewhere <ref> [9, 84, 115, 150, 202] </ref>. Loop fusion also has the added advantage of being able to improve memory reuse in the resulting program. Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs.
Reference: [85] <author> E. Granston and A. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year> <note> 186 BIBLIOGRAPHY </note>
Reference-contexts: Heuristics have been developed for managing multiprocessors cache coherence in software through the use of block cache invalidate, prefetch, and update instructions <ref> [65, 85] </ref>. Taken to the limit, these optimizations begin to resemble message vectorization and code generation for distributed-memory machines. Researchers have proposed merging these data locality optimizations with parallelism information for distributed-memory machines.
Reference: [86] <author> T. Gross, S. Hinrichs, G. Lueh, D. O'Hallaron, J. Stichnoth, and J. Subhlok. </author> <title> Compiling task and data parallel programs for iWarp. </title> <booktitle> In Proceedings of the Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <address> Boulder, CO, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Only one dimension of each darray may be distributed, and computations must be linearly related. Its successor, the iWarp compiler, targets the difficult task of simultaneously supporting data-parallelism, coarse-grain task parallelism, and fine-grain systolic parallelism <ref> [86, 102, 134] </ref>. P 3 C, VMMP P 3 C, the Portable Parallelizing Pascal Compiler, translates sequential Pascal programs into explicit parallel code [76]. The output program relys on Vmmp [75], a portable software environment running on many multiprocessors.
Reference: [87] <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: The RSDs are stored for use during array kill analysis and future optimizations. 46 CHAPTER 4. BASIC COMPILATION Array Kills Scalar data-flow analysis can detect private scalar variables. By combining control-flow information with array section information for array definitions, the Fortran D compiler can also calculate array kills <ref> [87] </ref>. This aids the compiler in detecting private arrays, and can significantly improve communication. 4.2.3 Data Decomposition Analysis The Fortran D compiler requires a new type of program analysis to generate the proper program|it must determine the data decomposition for each reference to a distributed array.
Reference: [88] <author> M. Gupta and P. Banerjee. </author> <title> Compile-time estimation of communication costs on multicomputers. </title> <booktitle> In Proceedings of the 6th International Parallel Processing Symposium, </booktitle> <address> Beverly Hills, CA, </address> <month> March </month> <year> 1992. </year>
Reference-contexts: Several researchers have proposed techniques to automatically derive data decompositions based on simple machine models <ref> [88, 89, 109, 141, 175, 193] </ref>.
Reference: [89] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Several researchers have proposed techniques to automatically derive data decompositions based on simple machine models <ref> [88, 89, 109, 141, 175, 193] </ref>.
Reference: [90] <author> M. Gupta and P. Banerjee. </author> <title> A methodology for high-level synthesis of communication for multicom-puters. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Orca-1 supports explicit parallelism through phase abstractions [145]. Modula-2* provides a superset of data-parallel constructs [169]. MetaMP provides a high-level interface for explicit message-passing [162]. Pyrros statically partitions task graphs for multiprocessors [211]. Gupta & Banerjee develop a methodology for generating complex collective communication <ref> [90] </ref>. Wolfe introduces loop rotation, a combination of loop skew, reversal, and interchange to reduce contention for common data [207]. Ramanujam & Sadayappan tile loop nests for distributed-memory machines [176].
Reference: [91] <author> M. Haghighat and C. Polychronopoulos. </author> <title> Symbolic program analysis and optimization for parallelizing compilers. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc [9, 10], Ptran [7], ParaScope [35, 59], and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics <ref> [91] </ref>, procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208]. Program transformations based on dependences may also be used to expose additional parallelism [117, 118, 119, 164]. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings.
Reference: [92] <author> M. W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Additional passes over the code can be added if necessary, but should be avoided since experience has shown that examination of source code dominates analysis time. The existing compilation system uses the following 3-phase approach <ref> [35, 62, 92] </ref>: 100 CHAPTER 7. INTERPROCEDURAL COMPILATION 1. Local Analysis. At the end of an editing session, ParaScope calculates and stores summary information concerning all local interprocedural effects for each procedure. <p> An example of this translation is the Translate function in Figure 7.3. Translation must also deal with array reshaping across procedure boundaries. Interprocedural symbolic analysis used in conjunction with linearization and delinearization of array references can discover standard reference patterns that may be compiled efficiently <ref> [30, 92, 98] </ref>. The augmented call graph construction algorithm has a local and interprocedural phase. During local analysis, a node is created for each procedure and augmented with loop information. Loops nodes are created for each loop. <p> During interprocedural propagation call edges are added to the call graph for each call site. Straightforward examination of recorded information is usually sufficient. If a procedure-valued formal parameter is invoked, further analysis is required to determine all procedure names that could be bound to it <ref> [92] </ref>. For greater precision, jump functions for loop information are evaluated using results from interprocedural constant and symbolic analysis. 102 CHAPTER 7. <p> A little more work is needed to calculate the extent of recompilation in the presence of cloning based on reaching decompositions <ref> [32, 92] </ref>. The compiler maintains a mapping from procedures in the call graph to the list of compiled clones for that procedure. For a procedure that has been cloned, the recompilation test can be applied to all the clones in order to find a match for the procedure.
Reference: [93] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Here we briefly examine alternatives explored by other researchers, including parallel architectures & operating systems, programming models & languages, and shared-memory paral-lelizing compilers. We also describe other distributed-memory compilation systems in relation to Fortran D <ref> [57, 73, 93, 104, 105, 106, 107] </ref>. 11.1 Parallel Architectures and Operating Systems Because of high interprocessor communication costs, many researchers believe efficient general-purpose parallel computing on distributed-memory machines requires some form of architectural and operating systems support.
Reference: [94] <author> M. W. Hall, K. Kennedy, and K. S. M c Kinley. </author> <title> Interprocedural transformations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Interprocedural Propagation. The compiler collects local summary information from each procedure in the program to build an augmented call graph containing loop information <ref> [94] </ref>. It then propagates the initial information on the call graph to compute interprocedural solutions. 3. Interprocedural Code Generation. The compiler directs compilation of all procedures in the program based on the results of interprocedural analysis. Another important aspect of the compilation system is what happens on subsequent compilations. <p> Since the Fortran D compiler also requires information about interprocedural loop nesting, it uses the augmented call graph (ACG) <ref> [94] </ref>. Conceptually, the ACG is simply a call graph plus loop nodes that contain the bounds, step, and index variable for each loop, plus nesting edges that indicate which nodes directly encompass other nodes. For instance, the Fortran D program in Figure 7.1 produces the ACG shown in Figure 7.2.
Reference: [95] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Fortunately, researchers have been investigating techniques for efficiently handling such computations through a combination of compile-time and run-time approaches [125, 155]. There is an ongoing effort to develop and improve support for such irregular computations in the framework of the Fortran D compiler and programming system <ref> [58, 95, 172] </ref>. 12.5.5 Support for Parallel Input/Output Finally, one aspect of automatic parallelization that most researchers prefer to avoid is that of providing support for parallel I/O. The current Fortran D compiler inserts guards to ensure all I/O is performed on a single processor.
Reference: [96] <author> P. Hatcher and M. Quinn. </author> <title> Data-parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1991. </year>
Reference-contexts: Some languages specify parallelism through a local view of computation, where the programmer specifies computation for an individual data point, relying on the compiler and run-time system to replicate the computation for all data points <ref> [96, 183] </ref>. In comparison, Fortran D supports a global view of computation, where the program specifies the overall computation, counting on the compiler to partition the computation. In comparison to language-driven compilers, systems like Fortran D reduce the burden on the user through program analysis, the key to advanced optimization. <p> The CM Fortran compiler has been retargeted for the CM-5, but results show that it fails to fully exploit the CM-5 architecture [186]. C*, Dataparallel C C* and Dataparallel C are extensions of C similar to C++ that supports SIMD data-parallel programs <ref> [96, 97, 179] </ref>. They both provide a local view of computation. Data is labeled as mono (local) or poly (distributed). There are no alignment or distribution specifications; the compiler automatically chooses the data decomposition. <p> Instead of generating virtual processors for each element of a domain, compile-time analysis enables contraction, emulating virtual processors via loops. Researchers have also examined synchronization problems encountered when translating SIMD programs into equivalent SPMD programs, as well as several communication optimizations <ref> [96, 174] </ref>. Experience will show whether SIMD languages such as C* provide sufficient flexibility for a wide class of scientific computations. DINO Dino is an extended version of C supporting general-purpose distributed computation [181, 182].
Reference: [97] <author> P. Hatcher, M. Quinn, A. Lapadula, B. Seevers, R. Anderson, and R. Jones. </author> <title> Data-parallel programming on MIMD computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 377-383, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The CM Fortran compiler has been retargeted for the CM-5, but results show that it fails to fully exploit the CM-5 architecture [186]. C*, Dataparallel C C* and Dataparallel C are extensions of C similar to C++ that supports SIMD data-parallel programs <ref> [96, 97, 179] </ref>. They both provide a local view of computation. Data is labeled as mono (local) or poly (distributed). There are no alignment or distribution specifications; the compiler automatically chooses the data decomposition.
Reference: [98] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: This single-program multiple-data (SPMD) program can then be compiled and executed directly on the nodes of the distributed-memory machine [111]. In the Fortran D compiler, collections of data and work are referred to as index sets and iteration sets, respectively. Both are represented by regular section descriptors (RSDs) <ref> [98] </ref>, which we describe using Fortran 90 triplet notation. Compiler Optimizations A number of Fortran D compiler optimizations are introduced and classified. Program transformations modify the program execution order to enable optimizations. <p> Regular section descriptors (RSDs) Regular section descriptors (RSDs) are widely used in the Fortran D compiler as an internal representation. Originally developed to summarize array side effects across procedure boundaries, RSDs are compact representations of rectangular or right-triangular array sections and their higher dimension analogs <ref> [18, 37, 98] </ref>. They may also possess some constant step. The union and intersection of RSDs can be calculated inexpensively, making them highly useful for the Fortran D compiler. RSDs have also proven to be quite precise in practice, due to the regular memory access patterns exhibited by scientific programs. <p> An example of this translation is the Translate function in Figure 7.3. Translation must also deal with array reshaping across procedure boundaries. Interprocedural symbolic analysis used in conjunction with linearization and delinearization of array references can discover standard reference patterns that may be compiled efficiently <ref> [30, 92, 98] </ref>. The augmented call graph construction algorithm has a local and interprocedural phase. During local analysis, a node is created for each procedure and augmented with loop information. Loops nodes are created for each loop. <p> References within a procedure are put into RSD form, but merged only if no loss of precision will result. The resulting RSDs may be propagated to calling procedures and translated as definitions or uses to actual parameters and global variables <ref> [98] </ref>. During code generation, the Fortran D compiler uses intraprocedural algorithms to calculate nonlocal index sets, using the deepest true dependence to determine the loop level for vectorizing communication. <p> Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc [9, 10], Ptran [7], ParaScope [35, 59], and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls <ref> [62, 98, 143] </ref>, and array reference aliases [19, 83, 133, 208]. Program transformations based on dependences may also be used to expose additional parallelism [117, 118, 119, 164]. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings.
Reference: [99] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 0.2. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, TX, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: As one measure of its relevance, we note that many features from Fortran D have been adopted by High Performance Fortran (HPF), a new proposed Fortran standard <ref> [99] </ref>. HPF is being used by Cray Research, DEC, IBM, and Thinking Machines for programming their newest generation of parallel machines. Our goal with Fortran D is to provide a simple yet efficient machine-independent parallel programming model. <p> Members include Convex, Cray Research, DEC, Fujitsu, IBM, Intel SSD, and Thinking Machines. After much effort and a year of meetings, High Performance Fortran (HPF) was presented for public comment at Supercomputing '92 <ref> [99] </ref>. Many core features of Fortran D were adopted by HPF, along with numerous additional extensions and refinements. Several hardware vendors and software companies are in the process of developing commercial HPF compilers, some scheduled for distribution as early as 1993.
Reference: [100] <author> R. Hill. MIMDizer: </author> <title> A new tool for parallelization. </title> <journal> Supercomputing Review, </journal> <volume> 3(4) </volume> <pages> 26-28, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: However, it does limit the amount of optimization that may be applied at compile-time for simpler computations. Forge90, MIMDizer Forge90, formerly Mimdizer, is an interactive parallelization system for MIMD shared and distributed-memory machines from Applied Parallel Research <ref> [14, 100] </ref>. It performs data-flow and dependence analyses, and also supports loop-level transformations. Associated tools graphically display call graph, control flow, dependence, and profiling information. When programming for distributed-memory machines, users may interactively select block or cyclic distributions for selected array dimensions.
Reference: [101] <author> W. Hillis and G. Steele, Jr. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <year> 1986. </year>
Reference-contexts: It must be easy to use yet perform with acceptable efficiency on different parallel architectures. In particular, we focus on data-parallel scientific codes that apply identical operations across large data sets, since these applications easily scale up to take advantage of massive parallelism <ref> [101] </ref>. One approach would be to identify a data-parallel programming style for Fortran that may be compiled to execute efficiently on a variety of parallel architectures. However, researchers working in the area, including ourselves, have concluded that such a programming style is needed but not sufficient in general. <p> However, these languages do not meet the needs of computational scientists because they do not elegantly describe data-parallel computations of the type described by Hillis and Steele <ref> [101] </ref> and Karp [111]. Parallelism must be explicitly specified because these languages do not provide compilers that can automatically detect and exploit parallelism. In addition, these languages also lack both language and compiler support to assist in efficient data placement, the partitioning and mapping of data to individual processors [165].
Reference: [102] <author> S. Hinrichs and T. Gross. </author> <title> Utilizing new communication features in compilation for private memory machines. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Only one dimension of each darray may be distributed, and computations must be linearly related. Its successor, the iWarp compiler, targets the difficult task of simultaneously supporting data-parallelism, coarse-grain task parallelism, and fine-grain systolic parallelism <ref> [86, 102, 134] </ref>. P 3 C, VMMP P 3 C, the Portable Parallelizing Pascal Compiler, translates sequential Pascal programs into explicit parallel code [76]. The output program relys on Vmmp [75], a portable software environment running on many multiprocessors.
Reference: [103] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. BIBLIOGRAPHY 187 </note>
Reference-contexts: More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 [26]. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs <ref> [17, 103, 113, 114] </ref>. Accurate static estimates of communication and computation are also needed by the compiler to calculate block sizes for coarse-grain pipelining. 6.4 Scalability The scalability of an optimization describes whether its effectiveness increases, decreases, or remains constant in proportion to some characteristic. <p> The Fortran D data partitioner, shown in Figure 10.1, supports this through the use of static performance estimation. Data decompositions are selected using standard heuristics, then evaluated using a combination of the Fortran D compiler and static performance estimation <ref> [17, 103, 113] </ref>. The resulting data decompositions should be efficient for both the compiler and parallel machine. Note that even though it is desirable, to assist compilation the static performance estimator does not need to predict absolute performance.
Reference: [104] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Here we briefly examine alternatives explored by other researchers, including parallel architectures & operating systems, programming models & languages, and shared-memory paral-lelizing compilers. We also describe other distributed-memory compilation systems in relation to Fortran D <ref> [57, 73, 93, 104, 105, 106, 107] </ref>. 11.1 Parallel Architectures and Operating Systems Because of high interprocessor communication costs, many researchers believe efficient general-purpose parallel computing on distributed-memory machines requires some form of architectural and operating systems support.
Reference: [105] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Here we briefly examine alternatives explored by other researchers, including parallel architectures & operating systems, programming models & languages, and shared-memory paral-lelizing compilers. We also describe other distributed-memory compilation systems in relation to Fortran D <ref> [57, 73, 93, 104, 105, 106, 107] </ref>. 11.1 Parallel Architectures and Operating Systems Because of high interprocessor communication costs, many researchers believe efficient general-purpose parallel computing on distributed-memory machines requires some form of architectural and operating systems support.
Reference: [106] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Here we briefly examine alternatives explored by other researchers, including parallel architectures & operating systems, programming models & languages, and shared-memory paral-lelizing compilers. We also describe other distributed-memory compilation systems in relation to Fortran D <ref> [57, 73, 93, 104, 105, 106, 107] </ref>. 11.1 Parallel Architectures and Operating Systems Because of high interprocessor communication costs, many researchers believe efficient general-purpose parallel computing on distributed-memory machines requires some form of architectural and operating systems support.
Reference: [107] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Here we briefly examine alternatives explored by other researchers, including parallel architectures & operating systems, programming models & languages, and shared-memory paral-lelizing compilers. We also describe other distributed-memory compilation systems in relation to Fortran D <ref> [57, 73, 93, 104, 105, 106, 107] </ref>. 11.1 Parallel Architectures and Operating Systems Because of high interprocessor communication costs, many researchers believe efficient general-purpose parallel computing on distributed-memory machines requires some form of architectural and operating systems support.
Reference: [108] <author> S. Hiranandani, J. Saltz, P. Mehrotra, and H. Berryman. </author> <title> Performance of hashed cache data migration schemes on multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(4), </volume> <month> August </month> <year> 1991. </year>
Reference-contexts: Hash tables Hash tables may be used when the set of nonlocal elements accessed is sparse, as for many irregular computations. They also provide a quick lookup mechanism for arbitrary sets of nonlocal values <ref> [108, 156] </ref>. 4.5. DISCUSSION 59 Maintenance Selecting storage types is fairly simple. Stencil computations that result in nonlocal accesses at boundaries of local array sections are satisfied by providing overlaps. <p> Arf is designed to interface Fortran application programs with Parti, a set of run-time library routines that support irregular computations on MIMD distributed-memory machines [188]. Parti is first to propose and implement user-defined irregular distributions [155] and a hashed cache for nonlocal values <ref> [108, 156] </ref>. It is employed by a number of systems to support irregular computations, including the Fortran D compiler. Pandore Pandore is a compiler for distributed-memory machines that takes as input C programs extended with block, cyclic, and overlapping data distributions [12, 203].
Reference: [109] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Several researchers have proposed techniques to automatically derive data decompositions based on simple machine models <ref> [88, 89, 109, 141, 175, 193] </ref>.
Reference: [110] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Forge90 automatically generates communications corresponding to nonlocal memory accesses at the end of the parallelization session. 174 CHAPTER 11. RELATED WORK ASPAR Aspar is a compiler that performs automatic data decomposition and communications generation for loops containing a single distributed array <ref> [110] </ref>. It utilizes collective communication primitives from the Express run-time system for distributed-memory machines [167]. Aspar automatically selects block distributions; no alignment or distribution specifications are provided. Aspar performs simple dependence analysis using A-lists to detect parallelizable loops.
Reference: [111] <author> A. Karp. </author> <title> Programming for parallelism. </title> <journal> IEEE Computer, </journal> <volume> 20(5) </volume> <pages> 43-57, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Generate code. The compiler instantiates the communication, data and work partition determined previously, generating a Fortran 77 program with explicit message-passing. This single-program multiple-data (SPMD) program can then be compiled and executed directly on the nodes of the distributed-memory machine <ref> [111] </ref>. In the Fortran D compiler, collections of data and work are referred to as index sets and iteration sets, respectively. Both are represented by regular section descriptors (RSDs) [98], which we describe using Fortran 90 triplet notation. <p> However, these languages do not meet the needs of computational scientists because they do not elegantly describe data-parallel computations of the type described by Hillis and Steele [101] and Karp <ref> [111] </ref>. Parallelism must be explicitly specified because these languages do not provide compilers that can automatically detect and exploit parallelism. In addition, these languages also lack both language and compiler support to assist in efficient data placement, the partitioning and mapping of data to individual processors [165].
Reference: [112] <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: They cannot hide memory latency by prefetching data before it is needed, or reduce data movement costs by fetching entire logical blocks at once. At best they can relax memory consistency, maintain a history of past accesses, and try to guess future reference patterns <ref> [112, 213] </ref>. Unlike compilers, operating systems are incapable of arranging data layout to avoid contention, reordering computation to improve data movement, or replicating computation to eliminate communication.
Reference: [113] <author> K. Kennedy and U. Kremer. </author> <title> Automatic data alignment and distribution for loosely synchronous problems in an interactive programming environment. </title> <type> Technical Report TR91-155, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 [26]. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs <ref> [17, 103, 113, 114] </ref>. Accurate static estimates of communication and computation are also needed by the compiler to calculate block sizes for coarse-grain pipelining. 6.4 Scalability The scalability of an optimization describes whether its effectiveness increases, decreases, or remains constant in proportion to some characteristic. <p> The Fortran D data partitioner, shown in Figure 10.1, supports this through the use of static performance estimation. Data decompositions are selected using standard heuristics, then evaluated using a combination of the Fortran D compiler and static performance estimation <ref> [17, 103, 113] </ref>. The resulting data decompositions should be efficient for both the compiler and parallel machine. Note that even though it is desirable, to assist compilation the static performance estimator does not need to predict absolute performance.
Reference: [114] <author> K. Kennedy, N. McIntosh, and K. S. M c Kinley. </author> <title> Static performance estimation in a parallelizing compiler. </title> <type> Technical Report TR91-174, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: More accurate analytical models can be developed, but may be hindered by unpredictable system discontinuities. For instance, communication cost increases abruptly past 100 bytes on the iPSC/860 [26]. The Fortran D compiler will employ a flexible and precise approach using training sets to estimate communication and computation costs <ref> [17, 103, 113, 114] </ref>. Accurate static estimates of communication and computation are also needed by the compiler to calculate block sizes for coarse-grain pipelining. 6.4 Scalability The scalability of an optimization describes whether its effectiveness increases, decreases, or remains constant in proportion to some characteristic. <p> The results of executing the training set on a parallel machine are summarized and used to train the performance estimator for that machine. By utilizing training sets, the performance estimator achieves both accuracy and portability across different machine architectures <ref> [17, 114] </ref>. As discussed in Chapter 6, the Fortran D compiler also uses training set information to guide optimizations, particularly balancing communication and parallelism for coarse-grain pipelining. 10.2.2 Automatic Data Partitioner The goal of the automatic data partitioner is to choose an efficient data decomposition.
Reference: [115] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Maximizing loop parallelism and improving data locality via loop fusion and distribution. </title> <type> Technical Report TR92-189, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: This heuristic does not adversely affect the parallelism or communication overhead 126 CHAPTER 8. COMPILING FORTRAN 77D AND 90D of the resulting program, and should perform well for the simple cases found in practice. More sophisticated algorithms are discussed elsewhere <ref> [9, 84, 115, 150, 202] </ref>. Loop fusion also has the added advantage of being able to improve memory reuse in the resulting program. Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs.
Reference: [116] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Optimizing for parallelism and data locality. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Because pipelining disturbs the original computation order, the node compiler later permutes inner loops in memory order, to ensure data locality for the local computation <ref> [116] </ref>. 5.5 Improve Partitioning One of the responsibilities of the Fortran D compiler is to partition computation across processors. As we have seen, the compiler instantiates this partition by reducing loop bounds or introducing guards in the program. Guards are typically inefficient, since they must be evaluated frequently at run-time. <p> The Fortran D compiler must divide array operations into sections that fit the hardware of the target machine [8, 11]. We defer both loop fusion and sectioning to the Fortran D back end. Loop fusion is deferred because even hand-written Fortran 77 programs can benefit significantly <ref> [116, 150] </ref>. Sectioning is needed in the back end because forall loops may also be present in Fortran 77D. We assign to the Fortran 90D front end the remaining task, scalarizing Fortran 90 constructs that have no equivalent in the Fortran 77D intermediate form. <p> For this example, we measured improvements of up to 30% for some problem sizes on an Intel i860, as shown in Figure 8.2. Additional transformations to enhance memory reuse and increase unit-stride memory accesses are also quite important; they are described elsewhere <ref> [116, 150] </ref>. Program Partitioning The major step in compiling Fortran D for MIMD distributed-memory machines is to partition the data and computation across processors, introducing communication where needed. This process has been discussed in previous chapters. <p> Compilation techniques have been developed to improve data locality on scalar and shared-memory machines. Program transformations can enhance temporal and spatial locality of scientific codes, improving the usage of higher levels of the memory hierarchy such as registers and cache <ref> [34, 40, 116, 173, 204] </ref>. Heuristics have been developed for managing multiprocessors cache coherence in software through the use of block cache invalidate, prefetch, and update instructions [65, 85]. Taken to the limit, these optimizations begin to resemble message vectorization and code generation for distributed-memory machines.
Reference: [117] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: We are implementing the prototype Fortran D compiler in the context of the ParaScope programming environment in order to utilize its analysis and transformation capabilities <ref> [35, 117] </ref>. The prototype compiler automatically derives from the data decomposition node programs for MIMD distributed-memory machines; its goal is to minimize both load imbalance and communications costs. This thesis describes the design, implementation, and evaluation of the prototype Fortran D compiler. <p> We show that it is also highly useful for guiding compiler optimizations for distributed-memory machines. The prototype Fortran D compiler is being developed in the context of the ParaScope programming environment and incorporates the following analysis capabilities <ref> [35, 117] </ref>. Scalar data-flow analysis Control flow, control dependence, and live range information are computed during the scalar data-flow analysis phase. <p> In Figure 5.1, we use ffi, *, and ? to mark the extent of our contribution to each optimization technique. Shared-memory parallelizing compilers apply program transformations to expose or enhance parallelism in scientific codes, using dependence information to determine their legality and profitability <ref> [10, 117, 132, 205] </ref>. We have found that transformations such as loop interchange, fusion, distribution, alignment, and strip-mining to be also quite useful for optimizing Fortran D programs. <p> Strip-mining increases the step size of an existing loop and adds an additional inner loop. The legality of applying strip-mine followed by loop interchange is determined in the same manner as for unroll-and-jam <ref> [117] </ref>. The Fortran D compiler may apply strip-mining in order to reduce storage requirements for computations. It may also be used with loop interchange to help exploit pipeline parallelism, as discussed in the next section. Loop fusion combines multiple loops with identical headers into a single loop. <p> .gt. 0) send (ZA (2:99,1),my$p-1) if (my$p .lt. 3) recv (ZA (2:99,26),my$p+1) do kk = 2,99,B if (my$p .gt. 0) recv (ZA (kk:kk+B-1,0),my$p-1) do j = 1,25 QA = F 1 (ZA (k,j+1),ZA (k,j-1),ZA (k+1,j),ZA (k-1,j)) enddo enddo if (my$p .lt. 3) send (ZA (kk:kk+B-1,25),my$p+1) enddo enddo for shared-memory programs <ref> [10, 117, 132] </ref>. Because pipelining disturbs the original computation order, the node compiler later permutes inner loops in memory order, to ensure data locality for the local computation [116]. 5.5 Improve Partitioning One of the responsibilities of the Fortran D compiler is to partition computation across processors. <p> COMPILER OPTIMIZATIONS 5.5.2 Loop Distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers. Loop distribution may be applied only if the statements are not involved in a recurrence and the direction of existing loop-carried dependences are not reversed in the resulting loops <ref> [117, 132] </ref>. It can separate statements in loop nests with different local iteration sets, avoiding the need to evaluate guards at run-time. Loop distribution may also separate the source and sink of loop-carried or loop-independent cross-processor dependences, allowing individual messages to be combined into a single vector message. <p> It utilizes existing tools for performing dependence analysis, program transformations, and interprocedural analysis <ref> [62, 83, 117] </ref>. The design of the prototype compiler has been described in the preceding chapters. <p> Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208]. Program transformations based on dependences may also be used to expose additional parallelism <ref> [117, 118, 119, 164] </ref>. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings. First, the main goal of parallelization techniques for shared-memory machines is to exploit parallelism.
Reference: [118] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Interactive parallel programming using the ParaScope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208]. Program transformations based on dependences may also be used to expose additional parallelism <ref> [117, 118, 119, 164] </ref>. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings. First, the main goal of parallelization techniques for shared-memory machines is to exploit parallelism.
Reference: [119] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in an interactive parallel programming tool. </title> <journal> Concurrency: Practice & Experience, </journal> <note> to appear 1993. </note>
Reference-contexts: Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208]. Program transformations based on dependences may also be used to expose additional parallelism <ref> [117, 118, 119, 164] </ref>. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings. First, the main goal of parallelization techniques for shared-memory machines is to exploit parallelism.
Reference: [120] <author> C. King and L. Ni. </author> <title> Grouping in nested loops for parallel execution on multicomputers. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1989. </year> <note> 188 BIBLIOGRAPHY </note>
Reference-contexts: Where data accesses are not entirely disjoint, grouping methods can be applied to reduce communication between loop iterations <ref> [120, 191] </ref>. However, in order to take advantage of data locality, the compiler must take into account affinity, the interaction between data placement and task scheduling [149].
Reference: [121] <author> C. Koelbel. </author> <title> Compiling Programs for Nonshared Memory Machines. </title> <type> PhD thesis, </type> <institution> Purdue University, West Lafayette, IN, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Later phases of the compiler generate message-passing C programs for the physical machine. BLAZE, Kali Blaze is one of the first distributed-memory compilers [126, 152]. Kali, its successor, is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines <ref> [121, 122, 123, 127, 124, 153] </ref>. Programs written for Kali must specify a virtual processor array and assign distributed arrays to block, cyclic, or user-specified distributions. Instead of deriving a functional 11.4.
Reference: [122] <author> C. Koelbel. </author> <title> Compile-time generation of regular communications patterns. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 101-110, </pages> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Later phases of the compiler generate message-passing C programs for the physical machine. BLAZE, Kali Blaze is one of the first distributed-memory compilers [126, 152]. Kali, its successor, is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines <ref> [121, 122, 123, 127, 124, 153] </ref>. Programs written for Kali must specify a virtual processor array and assign distributed arrays to block, cyclic, or user-specified distributions. Instead of deriving a functional 11.4.
Reference: [123] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Despite the expense of additional communication, experimental evidence from several systems show that it can improve performance by grouping communication to access nonlocal data outside of the loop nest, especially if the information generated may be reused on later iterations <ref> [123, 155] </ref>. The inspector strategy is not applicable for unanalyzable references causing loop-carried true dependences. <p> However, a combination of compile-time analysis and run-time processing can be applied to optimize communication. As previously discussed, if no loop-carried true dependences are present, inspectors and executors may be created at compile-time during code generation to combine messages at run-time <ref> [123, 155] </ref>. The inspector performs the equivalent of message coalescing and aggregation at run-time. The executor then utilizes collective communication specialized for irregular computations. <p> Later phases of the compiler generate message-passing C programs for the physical machine. BLAZE, Kali Blaze is one of the first distributed-memory compilers [126, 152]. Kali, its successor, is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines <ref> [121, 122, 123, 127, 124, 153] </ref>. Programs written for Kali must specify a virtual processor array and assign distributed arrays to block, cyclic, or user-specified distributions. Instead of deriving a functional 11.4.
Reference: [124] <author> C. Koelbel and P. Mehrotra. </author> <title> Programming data parallel algorithms on distributed memory machines using Kali. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Where this is not the case, iteration reordering may be applied to change the order of program execution, subject to dependence constraints. This allows loop iterations accessing only local data to be separated and placed between send and recv statements to hide T transit <ref> [124] </ref>. 70 CHAPTER 5. <p> By applying dynamic data decomposition using collective communication routines to change the array decomposition after each phase, the Fortran D compiler can internalize the computation wavefront in both phases, allowing processors to execute in parallel without communication <ref> [124] </ref>. However, dynamic data decomposition is only applicable when there are full dimensions of parallelism available in the computation. For instance, it cannot be used to exploit parallelism for SOR or Livermore 23 in Figure 5.10, because the computation wavefront crosses both spatial dimensions. <p> Later phases of the compiler generate message-passing C programs for the physical machine. BLAZE, Kali Blaze is one of the first distributed-memory compilers [126, 152]. Kali, its successor, is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines <ref> [121, 122, 123, 127, 124, 153] </ref>. Programs written for Kali must specify a virtual processor array and assign distributed arrays to block, cyclic, or user-specified distributions. Instead of deriving a functional 11.4.
Reference: [125] <author> C. Koelbel, P. Mehrotra, J. Saltz, and S. Berryman. </author> <title> Parallel loops on distributed machines. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Note that irregular computations are different from irregular distributions, which are irregular mappings of data to processors. It is not possible to determine the Send, Receive, In, and Out sets at compile-time for these computations. However, an inspector <ref> [155, 125] </ref> may be constructed to preprocess the loop body at run-time to determine what nonlocal data will be accessed. This in effect calculates the In index set for each processor. A global transpose operation between processors can then be used to calculate the Out index sets as well. <p> each ~ j 2 receive iter set t p X (ff X (g ( ~ j)))) = get value (Y (h ( ~ j))) endfor 3.3.7 Resulting Program Once the Send and Receive sets have been calculated, the example loop nest is transformed into the loops pictured in Figure 3.10 <ref> [125] </ref>. In the send loop, every processor sends data they own to processors that need the data. The Out index set for rhs of the statement in the example loop has already been calculated. <p> Arguments to procedures are labeled with their expected incoming data partition. The user must ensure that the procedure is called only with the appropriately decomposed arguments. An inspector/executor strategy is used for run-time preprocessing of communication for irregularly distributed arrays <ref> [125, 155] </ref>. Major differences between Kali and the Fortran D compiler include mandatory on clauses for parallel loops, support for alignment, collective communications, and dynamic decomposition. CM Fortran CM Fortran is a version of Fortran 77 extended with vector notation, alignment, and data layout specifications [5, 196]. <p> Though these still comprise the majority of scientific applications, there is an accelerating trend in the computational science community towards irregular computations involving sparse matrixes and adaptive algorithms [72]. Fortunately, researchers have been investigating techniques for efficiently handling such computations through a combination of compile-time and run-time approaches <ref> [125, 155] </ref>.
Reference: [126] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Semi-automatic domain decomposition in BLAZE. </title> <editor> In S. Sahni, editor, </editor> <booktitle> Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1987. </year> <institution> Pennsylvania State University Press. </institution>
Reference-contexts: Communication patterns are synthesized syntactically from the computation, evaluated for a variety of block distributions, then matched with Crystal collective communication routines. Later phases of the compiler generate message-passing C programs for the physical machine. BLAZE, Kali Blaze is one of the first distributed-memory compilers <ref> [126, 152] </ref>. Kali, its successor, is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines [121, 122, 123, 127, 124, 153]. Programs written for Kali must specify a virtual processor array and assign distributed arrays to block, cyclic, or user-specified distributions.
Reference: [127] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: If there are multiple 32 CHAPTER 2. FORTRAN D LANGUAGE elements with the minimum or maximum value, the assignment is performed only for the first such value found. 2.11 On Clause Fortran D provides a feature from Kali <ref> [127] </ref>, an optional on clause. The on clause is used to specify the processor which will execute each iteration of a loop. <p> A global transpose operation between processors can then be used to calculate the Out index sets as well. An inspector is the most general way to generate In and Out sets for loops without loop-carried dependences. Despite the expense of additional communications, experimental evidence from several systems <ref> [127, 209] </ref> proves that it can improve performance by combining communications to access nonlocal data outside of the loop nest. In addition it also allows multiple messages to the same processor to be combined. The Fortran D compiler plans to automatically generate inspectors where needed for irregular computations. <p> This optimization is a simple application of the "owner stores" rule proposed by Balasundaram [15]. In particular, it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on a single processor, such as in Kali and Arf <ref> [127, 209] </ref>. This technique may improve communication and provides greater control over load balance, especially for irregular computations. It also eliminates the need for individual statement masks and simplifies handling of control flow within the loop body. <p> Though most parallel languages concentrate on specifying synchronization for task-level parallelism, we found several languages constructs useful for data-parallel programming. We have adopted many such features into Fortran D. In particular, we have been influenced by alignment specifications from CM Fortran [196], distribution specifications from Kali <ref> [127, 153] </ref>, and structures to handle irregular distributions from Parti [188]. We also incorporated the forall statement from Myrias [22] and CM Fortran [6]. <p> Later phases of the compiler generate message-passing C programs for the physical machine. BLAZE, Kali Blaze is one of the first distributed-memory compilers [126, 152]. Kali, its successor, is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines <ref> [121, 122, 123, 127, 124, 153] </ref>. Programs written for Kali must specify a virtual processor array and assign distributed arrays to block, cyclic, or user-specified distributions. Instead of deriving a functional 11.4.
Reference: [128] <author> P. Kogge and H. Stone. </author> <title> A parallel algorithm for the efficient solution of a general class of recurrence equations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-22(8):786-793, </volume> <month> August </month> <year> 1973. </year>
Reference-contexts: Scans are similar but perform parallel-prefix operations instead. A sum scan would return the sums of all the prefixes of an array. Scans are used to solve a number of computations in scientific codes, including linear recurrences and tridiagonal systems <ref> [51, 128] </ref>. The Fortran D compiler applies dependence analysis to recognize reductions and scans. If the reduction or scan accesses data in a manner that sequentializes computation across processors, the Fortran D compiler may parallelize it by relaxing the "owner computes" rule and providing methods to combine partial results.
Reference: [129] <author> U. Kremer, H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Advanced tools and techniques for automatic parallelization. </title> <journal> Parallel Computing, </journal> <volume> 7 </volume> <pages> 387-393, </pages> <year> 1988. </year>
Reference-contexts: SUPERB Superb is a semi-automatic parallelization tool designed for MIMD distributed-memory machines [79, 80, 212]. It supports arbitrary user-specified contiguous rectangular distributions, and performs dependence analysis to guide interactive program transformations in a manner similar to the ParaScope Editor <ref> [129] </ref>. Superb originated overlaps as a means to both specify and store nonlocal data accesses. Once program analysis and transformation is complete, communication is automatically generated and vectorized utilizing data dependence information. Computation is partitioned via explicit guards, which may be eliminated by loop bounds reduction [81].
Reference: [130] <author> D. Kuck. </author> <title> The Structure of Computers and Computations, Volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: New analysis techniques are required to compile shared-memory programs for distributed memory machines. Internal data structures used in the compilation process are described. The Fortran D compiler utilizes a compilation strategy based on the concept of data dependence <ref> [130] </ref> that unifies and extends previous techniques. The major step of the compilation process are: 1. Analyze Program. Symbolic and data dependence analysis is performed. 2. Partition data. Fortran D data decomposition specifications are analyzed to determine the decom position of each array in a program. 3. Partition computation. <p> A data dependence between two references R 1 and R 2 indicates that they read or write a common memory location in a way that requires their execution order to be maintained <ref> [130, 132] </ref>. We call R 1 the source and R 2 the sink of the dependence if R 1 must be executed before R 2 . There are four types of data dependence: True (flow) dependence occurs when S 1 writes a memory location that S 2 later reads. <p> Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc [9, 10], Ptran [7], ParaScope [35, 59], and Suif [197] use data dependence <ref> [130, 132] </ref> to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208].
Reference: [131] <author> D. Kuck, R. Kuhn, B. Leasure, and M. J. Wolfe. </author> <title> The structure of an advanced retargetable vec-torizer. </title> <booktitle> In Proceedings of COMPSAC 80, the 4th International Computer Software and Applications Conference, </booktitle> <pages> pages 709-715, </pages> <address> Chicago, IL, </address> <month> October </month> <year> 1980. </year>
Reference-contexts: Shared-memory parallelizing compilers such as Parafrase <ref> [131, 171] </ref>, Pfc [9, 10], Ptran [7], ParaScope [35, 59], and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208].
Reference: [132] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: A data dependence between two references R 1 and R 2 indicates that they read or write a common memory location in a way that requires their execution order to be maintained <ref> [130, 132] </ref>. We call R 1 the source and R 2 the sink of the dependence if R 1 must be executed before R 2 . There are four types of data dependence: True (flow) dependence occurs when S 1 writes a memory location that S 2 later reads. <p> In Figure 5.1, we use ffi, *, and ? to mark the extent of our contribution to each optimization technique. Shared-memory parallelizing compilers apply program transformations to expose or enhance parallelism in scientific codes, using dependence information to determine their legality and profitability <ref> [10, 117, 132, 205] </ref>. We have found that transformations such as loop interchange, fusion, distribution, alignment, and strip-mining to be also quite useful for optimizing Fortran D programs. <p> In the next section we describe optimizations that try to hide T copy and T transit by overlapping communication with computation. 5.2.1 Message Vectorization Message vectorization has been discussed in the previous chapter. Basically, it uses the results of data dependence analysis <ref> [10, 132] </ref> to combine element messages into vectors. Message vectorization is a loop 5.2. REDUCING COMMUNICATION OVERHEAD 63 based optimization. It extracts communication from within loops, replacing sending a message per loop iteration to one vectorized message preceding the loop. <p> It may also be used with loop interchange to help exploit pipeline parallelism, as discussed in the next section. Loop fusion combines multiple loops with identical headers into a single loop. It is legal if the direction of existing dependences are not reversed after fusion <ref> [132, 205] </ref>. Loop fusion can improve data locality, but its main use in the Fortran D compiler is to fuse imperfectly nested loops in order to enable loop interchange and strip-mine. 5.4.7 Fine-grain Pipelining We present two optimizations to exploit pipeline parallelism. <p> .gt. 0) send (ZA (2:99,1),my$p-1) if (my$p .lt. 3) recv (ZA (2:99,26),my$p+1) do kk = 2,99,B if (my$p .gt. 0) recv (ZA (kk:kk+B-1,0),my$p-1) do j = 1,25 QA = F 1 (ZA (k,j+1),ZA (k,j-1),ZA (k+1,j),ZA (k-1,j)) enddo enddo if (my$p .lt. 3) send (ZA (kk:kk+B-1,25),my$p+1) enddo enddo for shared-memory programs <ref> [10, 117, 132] </ref>. Because pipelining disturbs the original computation order, the node compiler later permutes inner loops in memory order, to ensure data locality for the local computation [116]. 5.5 Improve Partitioning One of the responsibilities of the Fortran D compiler is to partition computation across processors. <p> COMPILER OPTIMIZATIONS 5.5.2 Loop Distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers. Loop distribution may be applied only if the statements are not involved in a recurrence and the direction of existing loop-carried dependences are not reversed in the resulting loops <ref> [117, 132] </ref>. It can separate statements in loop nests with different local iteration sets, avoiding the need to evaluate guards at run-time. Loop distribution may also separate the source and sink of loop-carried or loop-independent cross-processor dependences, allowing individual messages to be combined into a single vector message. <p> Fusing such loops simplifies the partitioning process and enables additional optimizations. Data dependence is a concept developed for vectorizing and parallelizing compilers to characterize memory access patterns at compile time <ref> [10, 132, 205] </ref>. A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests [8, 202, 205]. <p> Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 8, 11, 40, 77, 132, 150, 189, 202] </ref>. <p> Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc [9, 10], Ptran [7], ParaScope [35, 59], and Suif [197] use data dependence <ref> [130, 132] </ref> to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208].
Reference: [133] <author> D. Kuck, Y. Muraoka, and S. Chen. </author> <title> On the number of operations simultaneously executable in Fortran-like programs and their resulting speedup. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-21(12):1293-1310, </volume> <month> December </month> <year> 1972. </year>
Reference-contexts: Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases <ref> [19, 83, 133, 208] </ref>. Program transformations based on dependences may also be used to expose additional parallelism [117, 118, 119, 164]. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings.
Reference: [134] <author> H. T. Kung and J. Subhlok. </author> <title> A new approach for automatic parallelization of blocked linear algebra computations. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Only one dimension of each darray may be distributed, and computations must be linearly related. Its successor, the iWarp compiler, targets the difficult task of simultaneously supporting data-parallelism, coarse-grain task parallelism, and fine-grain systolic parallelism <ref> [86, 102, 134] </ref>. P 3 C, VMMP P 3 C, the Portable Parallelizing Pascal Compiler, translates sequential Pascal programs into explicit parallel code [76]. The output program relys on Vmmp [75], a portable software environment running on many multiprocessors.
Reference: [135] <author> M. Lam and M. Rinard. </author> <title> Coarse-grain parallel programming in Jade. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Both require the user to develop explicit parallel programs. In comparison, declarative languages such as Jade can exploit coarse-grain parallelism at run-time, using side effect information collected from user annotations <ref> [135] </ref>. However, all these parallel languages tend to be inefficient or burdensome for exploiting data-parallelism. As a result, many researchers have turned to advanced parallelizing compilers or lower-level parallel languages. Though most parallel languages concentrate on specifying synchronization for task-level parallelism, we found several languages constructs useful for data-parallel programming.
Reference: [136] <author> B. Leasure, </author> <title> editor. PCF Fortran: Language Definition, version 3.1. </title> <booktitle> The Parallel Computing Forum, </booktitle> <address> Champaign, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Fortran commonly used for programming on each of them: Fortran 77 for the sequential machine, Fortran 90 for the SIMD parallel machine (e.g., the TMC CM-2, MasPar MP-1), message-passing Fortran for the MIMD distributed-memory machine (e.g., the Intel iPSC/860, Intel Paragon, Thinking Machines CM-5) and Parallel Computing Forum (PCF) Fortran <ref> [136, 166] </ref> for the MIMD shared-memory machine (e.g., the Cray Research C90 Y-MP, BBN TC2000 Butterfly). Each of these languages seems to be a plausible candidate for use as a machine-independent parallel programming model. Research on automatic parallelization has already shown that Fortran 77 is unsuitable for general parallel programming.
Reference: [137] <author> D. Lenoski, J. Laudon, T. Joe, D. Nakahira, L. Stevens, A. Gupta, and J. Hennessy. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <note> BIBLIOGRAPHY 189 </note>
Reference-contexts: Systems such as Alewife [42], April [2], DASH <ref> [137] </ref>, KSR-1, and Willow [24] attempt to provide a coherent global address space through innovations in hardware and operating systems. Early experiences with these systems have been positive, but show that locality of reference is a major factor in determining performance.
Reference: [138] <author> J. Li and M. Chen. </author> <title> Generating explicit communication from shared-memory program references. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: It can also vectorize messages in sequential regions, such as those found in SOR or ADI integration. Many language-driven compilers are improving their compile-time analysis and optimization, especially CM Fortran. Crystal Crystal is a high-level functional language <ref> [53, 138, 139, 140, 141] </ref>. Because it targets a functional language, the Crystal compiler possesses markedly different program analysis techniques than compilers for imperative languages such as Fortran. However, it performs significant compile-time analysis and optimization, pioneering both automatic data decomposition and collective communications generation techniques.
Reference: [139] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers'90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: It can also vectorize messages in sequential regions, such as those found in SOR or ADI integration. Many language-driven compilers are improving their compile-time analysis and optimization, especially CM Fortran. Crystal Crystal is a high-level functional language <ref> [53, 138, 139, 140, 141] </ref>. Because it targets a functional language, the Crystal compiler possesses markedly different program analysis techniques than compilers for imperative languages such as Fortran. However, it performs significant compile-time analysis and optimization, pioneering both automatic data decomposition and collective communications generation techniques.
Reference: [140] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: For these shifts resulting from stencil computations, individual calls to send and recv primitives are very efficient. This is the case for the Jacobi, SOR, and Red-black SOR examples previously discussed. More complicated subscript expressions indicate the need for collective communication <ref> [140] </ref>. For exam ple, the loop-invariant subscript for B (c; j) in S 2 can be efficiently communicated using broadcast. Collective 66 CHAPTER 5. COMPILER OPTIMIZATIONS communication is selected because these communication patterns are not well-described by individual messages, and can be performed significantly faster using special purpose routines. <p> Collective 66 CHAPTER 5. COMPILER OPTIMIZATIONS communication is selected because these communication patterns are not well-described by individual messages, and can be performed significantly faster using special purpose routines. The Fortran D compiler applies techniques pioneered by Li and Chen to recognize these patterns through syntactic analysis <ref> [140] </ref>. In other words, message vectorization, coalescing, and aggregation determine the extent to which communication for nonlocal accesses may be combined into a single message. For stencil computations these are point-to-point interprocessor communication and can be performed quite efficiently by individual calls to send and recv primitives. <p> It can also vectorize messages in sequential regions, such as those found in SOR or ADI integration. Many language-driven compilers are improving their compile-time analysis and optimization, especially CM Fortran. Crystal Crystal is a high-level functional language <ref> [53, 138, 139, 140, 141] </ref>. Because it targets a functional language, the Crystal compiler possesses markedly different program analysis techniques than compilers for imperative languages such as Fortran. However, it performs significant compile-time analysis and optimization, pioneering both automatic data decomposition and collective communications generation techniques.
Reference: [141] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(2) </volume> <pages> 213-221, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Several researchers have proposed techniques to automatically derive data decompositions based on simple machine models <ref> [88, 89, 109, 141, 175, 193] </ref>. <p> It can also vectorize messages in sequential regions, such as those found in SOR or ADI integration. Many language-driven compilers are improving their compile-time analysis and optimization, especially CM Fortran. Crystal Crystal is a high-level functional language <ref> [53, 138, 139, 140, 141] </ref>. Because it targets a functional language, the Crystal compiler possesses markedly different program analysis techniques than compilers for imperative languages such as Fortran. However, it performs significant compile-time analysis and optimization, pioneering both automatic data decomposition and collective communications generation techniques.
Reference: [142] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Early experiences with these systems have been positive, but show that locality of reference is a major factor in determining performance. Researchers are also developing techniques to provide a global address space on distributed-memory machines entirely through software. Distributed shared-memory systems such as Amber [48], Ivy <ref> [142] </ref>, Munin [23], and Platinum [64] utilize the operating system to detect and expedite interprocessor data motion. These software-based systems incur significantly greater overhead, but have the advantage of not requiring additional specialized hardware.
Reference: [143] <author> Z. Li and P. Yew. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc [9, 10], Ptran [7], ParaScope [35, 59], and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls <ref> [62, 98, 143] </ref>, and array reference aliases [19, 83, 133, 208]. Program transformations based on dependences may also be used to expose additional parallelism [117, 118, 119, 164]. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings.
Reference: [144] <author> C. Lin and L. Snyder. </author> <title> A comparison of programming models for shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: Distributed-memory compilation techniques are still required. In fact, even shared-memory machines may benefit. Experiments have shown that non-uniform memory access (NUMA) shared-memory machines can actually achieve improved performance when programmed using a 11.4. DISTRIBUTED-MEMORY COMPILERS 169 distributed-memory programming model <ref> [144] </ref>, since resulting programs have been restructured to reduce interprocessor contention and expensive global memory references. 11.4 Distributed-Memory Compilers Compared with other approaches, distributed-memory compilers can achieve improved performance through compile-time analysis and optimization of both parallelism and interprocessor data movement.
Reference: [145] <author> C. Lin and L. Snyder. </author> <title> Data ensembles in Orca-1. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: These compilers perform little analysis or optimization, extracting parallelism from Fortran 90 array syntax or parallel loop annotations. The remaining systems are quite varied. Booster provides a system to annotate data placement through user-defined shapes and views [163]. Orca-1 supports explicit parallelism through phase abstractions <ref> [145] </ref>. Modula-2* provides a superset of data-parallel constructs [169]. MetaMP provides a high-level interface for explicit message-passing [162]. Pyrros statically partitions task graphs for multiprocessors [211]. Gupta & Banerjee develop a methodology for generating complex collective communication [90].
Reference: [146] <author> J. Loeliger, R. Metzger, M. Seligman, and S. Stroud. </author> <title> Pointer target tracking: An empirical study. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: It has fostered research on aggressive optimization of scientific codes for both scalar and shared-memory machines [35]. Its pioneering work on incorporating interprocedural optimization in an efficient compilation system has also contributed the development of the Convex Applications compiler <ref> [146] </ref>. Through careful design, the compilation process in ParaScope preserves separate compilation of procedures to a large extent. Tools in the environment cooperate so that a procedure only needs to be examined once during compilation.
Reference: [147] <author> S. Lucco and O. Sharp. </author> <title> Parallel programming with coordination structures. </title> <booktitle> In Proceedings of the Eighteenth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Orlando, FL, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: We believe that Fortran D provides a simple machine-independent programming model for most data-parallel computations. 2.1 Introduction High-level parallel languages such as Delirium <ref> [147] </ref>, Linda [41], and Strand [70] are valuable when used to coordinate coarse-grained functional parallelism. However, these languages do not meet the needs of computational scientists because they do not elegantly describe data-parallel computations of the type described by Hillis and Steele [101] and Karp [111]. <p> However, these parallel programming models are intended to guide the development of new data-parallel algorithms; they cannot be used to help scientists write parallel programs since no language or compiler support is provided. High-level languages such as Delirium <ref> [147] </ref>, Linda [41], PCN [71] and Strand [70] are valuable when used to coordinate coarse-grained functional parallelism, as are graphical programming languages such as CODE [159], HeNCE [21], and Schedule [68]. Both require the user to develop explicit parallel programs.
Reference: [148] <author> S. Lundstrom and G. Barnes. </author> <title> Controllable MIMD architectures. </title> <booktitle> In Proceedings of the 1980 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1980. </year>
Reference-contexts: In other words, each iteration gets its own copy of the entire data space that exists before the execution of the loop, and writes its results to a new data space at the end of the loop <ref> [6, 148, 22] </ref>. At the end of a forall loop, any variables that are assigned new values by different iterations have these values merged at the end of the loop. Merges are performed deterministically, by using the value assigned from the latest sequential iteration.
Reference: [149] <author> E. Markatos and T. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: Where data accesses are not entirely disjoint, grouping methods can be applied to reduce communication between loop iterations [120, 191]. However, in order to take advantage of data locality, the compiler must take into account affinity, the interaction between data placement and task scheduling <ref> [149] </ref>. Though shared-memory compilation approaches are useful for reducing memory contention, in the end they prove insufficient for distributed-memory machines because the resulting data partition may be highly complex and frequently change between loop nests. In addition, no support is provided for generating or optimizing communications where needed.
Reference: [150] <author> K. S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: The Fortran D compiler must divide array operations into sections that fit the hardware of the target machine [8, 11]. We defer both loop fusion and sectioning to the Fortran D back end. Loop fusion is deferred because even hand-written Fortran 77 programs can benefit significantly <ref> [116, 150] </ref>. Sectioning is needed in the back end because forall loops may also be present in Fortran 77D. We assign to the Fortran 90D front end the remaining task, scalarizing Fortran 90 constructs that have no equivalent in the Fortran 77D intermediate form. <p> This heuristic does not adversely affect the parallelism or communication overhead 126 CHAPTER 8. COMPILING FORTRAN 77D AND 90D of the resulting program, and should perform well for the simple cases found in practice. More sophisticated algorithms are discussed elsewhere <ref> [9, 84, 115, 150, 202] </ref>. Loop fusion also has the added advantage of being able to improve memory reuse in the resulting program. Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. <p> Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 8, 11, 40, 77, 132, 150, 189, 202] </ref>. <p> For this example, we measured improvements of up to 30% for some problem sizes on an Intel i860, as shown in Figure 8.2. Additional transformations to enhance memory reuse and increase unit-stride memory accesses are also quite important; they are described elsewhere <ref> [116, 150] </ref>. Program Partitioning The major step in compiling Fortran D for MIMD distributed-memory machines is to partition the data and computation across processors, introducing communication where needed. This process has been discussed in previous chapters.
Reference: [151] <author> F. McMahon. </author> <title> The Livermore Fortran Kernels: A computer test of the numerical performance range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1986. </year>
Reference-contexts: However, their profitability criteria are now totally different. In the remainder of this chapter, we describe each optimization and provide motivating examples using a small selection of scientific program kernels adapted from the Livermore Kernels and finite-difference algorithms <ref> [151] </ref>. They contain stencil computations and reductions, techniques commonly used by scientific programmers to solve partial differential equations (PDEs) [31, 74]. For clarity we ignore boundary conditions and use constant loop bounds and machine size in the examples, though this is not required by the optimizations.
Reference: [152] <author> P. Mehrotra and J. Van Rosendale. </author> <title> The BLAZE language: A parallel language for scientific programming. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 339-361, </pages> <year> 1987. </year>
Reference-contexts: Communication patterns are synthesized syntactically from the computation, evaluated for a variety of block distributions, then matched with Crystal collective communication routines. Later phases of the compiler generate message-passing C programs for the physical machine. BLAZE, Kali Blaze is one of the first distributed-memory compilers <ref> [126, 152] </ref>. Kali, its successor, is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines [121, 122, 123, 127, 124, 153]. Programs written for Kali must specify a virtual processor array and assign distributed arrays to block, cyclic, or user-specified distributions.
Reference: [153] <author> P. Mehrotra and J. Van Rosendale. </author> <title> Programming distributed memory architectures using Kali. </title> <booktitle> In Advances in Languages and Compilers for Parallel Computing, </booktitle> <address> Irvine, CA, August 1990. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: The distribution functions for decomposition A and array X are then computed through run-time preprocessing techniques <ref> [188, 153] </ref>. Researchers are examining more sophisticated methods of specify irregular distributions for Fortran D programs [47, 172, 209]. 3.3.2 Computation We continue to describe some additional notation we will employ later in this paper. <p> Though most parallel languages concentrate on specifying synchronization for task-level parallelism, we found several languages constructs useful for data-parallel programming. We have adopted many such features into Fortran D. In particular, we have been influenced by alignment specifications from CM Fortran [196], distribution specifications from Kali <ref> [127, 153] </ref>, and structures to handle irregular distributions from Parti [188]. We also incorporated the forall statement from Myrias [22] and CM Fortran [6]. <p> Later phases of the compiler generate message-passing C programs for the physical machine. BLAZE, Kali Blaze is one of the first distributed-memory compilers [126, 152]. Kali, its successor, is the first compiler system that supports both regular and irregular computations on MIMD distributed-memory machines <ref> [121, 122, 123, 127, 124, 153] </ref>. Programs written for Kali must specify a virtual processor array and assign distributed arrays to block, cyclic, or user-specified distributions. Instead of deriving a functional 11.4.
Reference: [154] <author> J. Merlin. </author> <title> ADAPTing Fortran-90 array programs for distributed memory architectures. </title> <booktitle> In First International Conference of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Additional Compilers and Systems We briefly review the large number of distributed-memory compilers and systems that have been developed. The first group of compilers target Fortran 90. Adapt <ref> [154] </ref> and Adaptor [27] both perform translations relying on run-time support from a portable library. Wu & Fox describe development of a Fortran 90D compiler developed and validated via a test-suite approach [210].
Reference: [155] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year> <note> 190 BIBLIOGRAPHY </note>
Reference-contexts: Note that irregular computations are different from irregular distributions, which are irregular mappings of data to processors. It is not possible to determine the Send, Receive, In, and Out sets at compile-time for these computations. However, an inspector <ref> [155, 125] </ref> may be constructed to preprocess the loop body at run-time to determine what nonlocal data will be accessed. This in effect calculates the In index set for each processor. A global transpose operation between processors can then be used to calculate the Out index sets as well. <p> Additional communication is also appended following loops containing reductions to accumulate the results of each reduction. Run-time Processing Run-time processing is applied to computations whose nonlocal data requirements are not known at compile time. An inspector <ref> [155] </ref> is constructed to preprocess the loop body at run-time to determine what nonlocal data will be accessed. This in effect calculates the receive index set for each processor. A global transpose 58 CHAPTER 4. BASIC COMPILATION operation between processors is then used to calculate the send index sets. <p> Despite the expense of additional communication, experimental evidence from several systems show that it can improve performance by grouping communication to access nonlocal data outside of the loop nest, especially if the information generated may be reused on later iterations <ref> [123, 155] </ref>. The inspector strategy is not applicable for unanalyzable references causing loop-carried true dependences. <p> However, a combination of compile-time analysis and run-time processing can be applied to optimize communication. As previously discussed, if no loop-carried true dependences are present, inspectors and executors may be created at compile-time during code generation to combine messages at run-time <ref> [123, 155] </ref>. The inspector performs the equivalent of message coalescing and aggregation at run-time. The executor then utilizes collective communication specialized for irregular computations. <p> Arguments to procedures are labeled with their expected incoming data partition. The user must ensure that the procedure is called only with the appropriately decomposed arguments. An inspector/executor strategy is used for run-time preprocessing of communication for irregularly distributed arrays <ref> [125, 155] </ref>. Major differences between Kali and the Fortran D compiler include mandatory on clauses for parallel loops, support for alignment, collective communications, and dynamic decomposition. CM Fortran CM Fortran is a version of Fortran 77 extended with vector notation, alignment, and data layout specifications [5, 196]. <p> It does not currently generate messages at compile-time for regular computations. Arf is designed to interface Fortran application programs with Parti, a set of run-time library routines that support irregular computations on MIMD distributed-memory machines [188]. Parti is first to propose and implement user-defined irregular distributions <ref> [155] </ref> and a hashed cache for nonlocal values [108, 156]. It is employed by a number of systems to support irregular computations, including the Fortran D compiler. <p> Though these still comprise the majority of scientific applications, there is an accelerating trend in the computational science community towards irregular computations involving sparse matrixes and adaptive algorithms [72]. Fortunately, researchers have been investigating techniques for efficiently handling such computations through a combination of compile-time and run-time approaches <ref> [125, 155] </ref>.
Reference: [156] <author> S. Mirchandaney, J. Saltz, P. Mehrotra, and H. Berryman. </author> <title> A scheme for supporting automatic data migration on multicomputers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Hash tables Hash tables may be used when the set of nonlocal elements accessed is sparse, as for many irregular computations. They also provide a quick lookup mechanism for arbitrary sets of nonlocal values <ref> [108, 156] </ref>. 4.5. DISCUSSION 59 Maintenance Selecting storage types is fairly simple. Stencil computations that result in nonlocal accesses at boundaries of local array sections are satisfied by providing overlaps. <p> Arf is designed to interface Fortran application programs with Parti, a set of run-time library routines that support irregular computations on MIMD distributed-memory machines [188]. Parti is first to propose and implement user-defined irregular distributions [155] and a hashed cache for nonlocal values <ref> [108, 156] </ref>. It is employed by a number of systems to support irregular computations, including the Fortran D compiler. Pandore Pandore is a compiler for distributed-memory machines that takes as input C programs extended with block, cyclic, and overlapping data distributions [12, 203].
Reference: [157] <author> M. Mu and J. Rice. </author> <title> Row oriented Gauss elimination on distributed memory multiprocessors. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 4(2) </volume> <pages> 143-168, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Due to the large number of global broadcasts required to communicate pivot values and multipliers, performance of Dgefa actually degrades when solving small problems on many processors. To determine whether improved performance is attainable, we created a hand-coded version of Dgefa based on optimizations described in the literature <ref> [78, 157] </ref>. First, we combined the two messages broadcast on each iteration of the outermost k loop. Instead of broadcasting the pivot value immediately, we wait until multipliers are also computed. The values can then be combined in one broadcast.
Reference: [158] <author> E. Myers. </author> <title> A precise inter-procedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: We would then need to compute the solution on the supergraph formed by combining local control flow graphs with the call graph, taking care to avoid paths that do not correspond to possible execution sequences <ref> [158] </ref>. To avoid this complexity, we choose instead to compute live decompositions during code generation, when control flow information is available. Live Decompositions Calculation. Interprocedural live variable analysis has been proven Co-NP-complete in the presence of aliasing [158]. <p> taking care to avoid paths that do not correspond to possible execution sequences <ref> [158] </ref>. To avoid this complexity, we choose instead to compute live decompositions during code generation, when control flow information is available. Live Decompositions Calculation. Interprocedural live variable analysis has been proven Co-NP-complete in the presence of aliasing [158]. Even without aliasing, interprocedural live variable analysis can be expensive since it requires bidirectional propagation, causing a procedure to be analyzed multiple times. We rely on two restrictions to make the live decompositions problem tractable for the Fortran D compiler. <p> Aliasing affects dynamic data decomposition because a variable may be remapped indirectly through one of its aliases. Unfortunately, precise alias analysis is computationally intractable <ref> [158] </ref>. As a result, the compiler cannot efficiently prove that a decomposition that has been applied to a variable holds for a possible alias.
Reference: [159] <author> P. Newton and J. C. Browne. </author> <title> The CODE 2.0 graphical parallel programming language. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: High-level languages such as Delirium [147], Linda [41], PCN [71] and Strand [70] are valuable when used to coordinate coarse-grained functional parallelism, as are graphical programming languages such as CODE <ref> [159] </ref>, HeNCE [21], and Schedule [68]. Both require the user to develop explicit parallel programs. In comparison, declarative languages such as Jade can exploit coarse-grain parallelism at run-time, using side effect information collected from user annotations [135].
Reference: [160] <author> M. O'Boyle and G. Hedayat. </author> <title> A transformational approach to compiling Sisal for distributed memory architectures. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Wash-ington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Wolfe introduces loop rotation, a combination of loop skew, reversal, and interchange to reduce contention for common data [207]. Ramanujam & Sadayappan tile loop nests for distributed-memory machines [176]. O'Boyle & Hedayat develop a linear algebraic framework for parallelizing Sisal <ref> [160] </ref>. 11.4.3 Analysis-Driven Compilers The final group of distributed-memory compilers are analysis-driven; they rely more on compile-time analysis than language features or user annotations. These compilers typically accept Fortran 77 or 90 programs with data decomposition annotations. They perform analysis to automatically detect parallel operations.
Reference: [161] <author> D. Olander and R. Schnabel. </author> <title> Preliminary experience in developing a parallel thin-layer Navier Stokes code and implications for parallel language design. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Special language constructs are provided for reductions. Dino programs are deterministic unless special asynchronous distributed arrays are used. Dino is a powerful and flexible language. Programmers can use it to specify optimizations such as coarse-grain pipelining and iteration reordering for pipelined and parallel computations <ref> [161] </ref>. However, its many features may prove burdensome to users. Dino2 proposes a large set of additional language features to 172 CHAPTER 11. RELATED WORK support parallel task creation, data and computation mapping, synchronization, and communication [180]. Dyno provides support for irregular and adaptive numeric programs [203].
Reference: [162] <author> S. Otto and M. J. Wolfe. </author> <title> The MetaMP approach to parallel programming. </title> <booktitle> In Frontiers'92: The 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The remaining systems are quite varied. Booster provides a system to annotate data placement through user-defined shapes and views [163]. Orca-1 supports explicit parallelism through phase abstractions [145]. Modula-2* provides a superset of data-parallel constructs [169]. MetaMP provides a high-level interface for explicit message-passing <ref> [162] </ref>. Pyrros statically partitions task graphs for multiprocessors [211]. Gupta & Banerjee develop a methodology for generating complex collective communication [90]. Wolfe introduces loop rotation, a combination of loop skew, reversal, and interchange to reduce contention for common data [207]. Ramanujam & Sadayappan tile loop nests for distributed-memory machines [176].
Reference: [163] <author> E. Paalvast, A. van Gemund, and H. Sips. </author> <title> A method for parallel program generation with an application to the Booster language. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: These compilers perform little analysis or optimization, extracting parallelism from Fortran 90 array syntax or parallel loop annotations. The remaining systems are quite varied. Booster provides a system to annotate data placement through user-defined shapes and views <ref> [163] </ref>. Orca-1 supports explicit parallelism through phase abstractions [145]. Modula-2* provides a superset of data-parallel constructs [169]. MetaMP provides a high-level interface for explicit message-passing [162]. Pyrros statically partitions task graphs for multiprocessors [211]. Gupta & Banerjee develop a methodology for generating complex collective communication [90].
Reference: [164] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208]. Program transformations based on dependences may also be used to expose additional parallelism <ref> [117, 118, 119, 164] </ref>. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings. First, the main goal of parallelization techniques for shared-memory machines is to exploit parallelism.
Reference: [165] <author> C. Pancake and D. Bergmark. </author> <title> Do parallel languages respond to the needs of scientific programmers? IEEE Computer, </title> <booktitle> 23(12) </booktitle> <pages> 13-23, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: data decompositions are provided for sequential programs written in a data-parallel programming style, an advanced compiler can automatically generate parallel programs that execute efficiently on MIMD distributed-memory machines. 1.3 Fortran D Unfortunately, most current parallel programming languages concentrate on constructs to express parallelism; they provide little support for data decomposition <ref> [165] </ref>. For these reasons, we have developed an enhanced version of Fortran that introduces data decomposition specifications. We call the extended language Fortran D, where "D" suggests data, decomposition, or distribution. <p> Parallelism must be explicitly specified because these languages do not provide compilers that can automatically detect and exploit parallelism. In addition, these languages also lack both language and compiler support to assist in efficient data placement, the partitioning and mapping of data to individual processors <ref> [165] </ref>. To overcome this deficiency, we have designed Fortran D, a version of Fortran enhanced with a rich set of data decomposition specifications. Fortran D is targeted at data-parallel numeric applications that are not supported by existing parallel languages.
Reference: [166] <author> Parallel Computing Forum. </author> <title> PCF: Parallel Fortran extensions. </title> <journal> Fortran Forum, </journal> <volume> 10(3), </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: Fortran commonly used for programming on each of them: Fortran 77 for the sequential machine, Fortran 90 for the SIMD parallel machine (e.g., the TMC CM-2, MasPar MP-1), message-passing Fortran for the MIMD distributed-memory machine (e.g., the Intel iPSC/860, Intel Paragon, Thinking Machines CM-5) and Parallel Computing Forum (PCF) Fortran <ref> [136, 166] </ref> for the MIMD shared-memory machine (e.g., the Cray Research C90 Y-MP, BBN TC2000 Butterfly). Each of these languages seems to be a plausible candidate for use as a machine-independent parallel programming model. Research on automatic parallelization has already shown that Fortran 77 is unsuitable for general parallel programming.
Reference: [167] <institution> Parasoft Corporation. </institution> <note> Express User's Manual, </note> <year> 1989. </year>
Reference-contexts: We have selected Fortran 77 with calls to communication and run-time libraries based on Express, a collection of portable message-passing primitives <ref> [167] </ref>. Evaluating our experiences with this node interface is the first step towards defining an "optimal" level of support for programming individual nodes of a parallel machine. 8.3 Unified Compiler The Fortran D compiler thus consists of three parts. <p> The run-time library is built on top of the Express communication package to ensure portability across different architectures <ref> [167] </ref>. Table 8.2 presents some sample performance numbers for a subset of the intrinsic functions on an iPSC/860, details are presented elsewhere [3]. The times in the table include both the computation and communication times for each function. These measurements are also displayed in Figure 8.3. <p> DISCUSSION 165 10.3.2 Run-Time System The prototype Fortran D compiler generates calls to Intel NX/2 message-passing libraries, and will soon be extended to also target the Thinking Machines CMMD library. Future targets may include portable, efficient communication libraries such as Express <ref> [167] </ref> or PVM [195]. Because these communication libraries have similar message-passing interfaces, adding new targets is a straightforward task. <p> RELATED WORK ASPAR Aspar is a compiler that performs automatic data decomposition and communications generation for loops containing a single distributed array [110]. It utilizes collective communication primitives from the Express run-time system for distributed-memory machines <ref> [167] </ref>. Aspar automatically selects block distributions; no alignment or distribution specifications are provided. Aspar performs simple dependence analysis using A-lists to detect parallelizable loops.
Reference: [168] <author> J. Peir and R. Cytron. </author> <title> Minimum distance: A method for partitioning recurrences for multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(8) </volume> <pages> 1203-1211, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: The basic premise is that since parallel loops exhibiting data locality are guaranteed to compute fairly disjoint data sets, the partition of parallel loop iterations among processors can also be used to assign data to each processor <ref> [66, 168, 190] </ref>. Where data accesses are not entirely disjoint, grouping methods can be applied to reduce communication between loop iterations [120, 191]. However, in order to take advantage of data locality, the compiler must take into account affinity, the interaction between data placement and task scheduling [149].
Reference: [169] <author> M. Philippsen and W. Tichy. </author> <title> Compiling for massively parallel machines. </title> <booktitle> In First International Conference of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: The remaining systems are quite varied. Booster provides a system to annotate data placement through user-defined shapes and views [163]. Orca-1 supports explicit parallelism through phase abstractions [145]. Modula-2* provides a superset of data-parallel constructs <ref> [169] </ref>. MetaMP provides a high-level interface for explicit message-passing [162]. Pyrros statically partitions task graphs for multiprocessors [211]. Gupta & Banerjee develop a methodology for generating complex collective communication [90]. Wolfe introduces loop rotation, a combination of loop skew, reversal, and interchange to reduce contention for common data [207].
Reference: [170] <author> K. Pingali and A. Rogers. </author> <title> Compiling for locality. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: For local compilation, Superb modifies procedures so that arrays are always accessed according to their true number of dimensions, inserting additional parameters where necessary for newly created subscripts. Id Nouveau Id Nouveau is a functional language extended with single assignment arrays called I-structures <ref> [170, 178] </ref>. User-specified block distributions are provided. Initially, send and recv statements are inserted to communicate each nonlocal array access. Message vectorization is applied to combine messages for previously written array elements. Loop jamming (fusion) and strip-mining are used when writing array elements. Global accumulate & reduction operations are supported.
Reference: [171] <author> C. Polychronopoulos, M. Girkar, M. Haghighat, C. Lee, B. Leung, and D. Schouten. </author> <title> The structure of Parafrase-2: An advanced parallelizing compiler for C and Fortran. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Shared-memory parallelizing compilers such as Parafrase <ref> [131, 171] </ref>, Pfc [9, 10], Ptran [7], ParaScope [35, 59], and Suif [197] use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208].
Reference: [172] <author> R. Ponnusamy, J. Saltz, R. Das, C. Koelbel, and A. Choudhary. </author> <title> Embedding data mappers with distributed memory machine compilers. </title> <booktitle> In Proceedings of the Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <address> Boulder, CO, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The distribution functions for decomposition A and array X are then computed through run-time preprocessing techniques [188, 153]. Researchers are examining more sophisticated methods of specify irregular distributions for Fortran D programs <ref> [47, 172, 209] </ref>. 3.3.2 Computation We continue to describe some additional notation we will employ later in this paper. <p> Fortunately, researchers have been investigating techniques for efficiently handling such computations through a combination of compile-time and run-time approaches [125, 155]. There is an ongoing effort to develop and improve support for such irregular computations in the framework of the Fortran D compiler and programming system <ref> [58, 95, 172] </ref>. 12.5.5 Support for Parallel Input/Output Finally, one aspect of automatic parallelization that most researchers prefer to avoid is that of providing support for parallel I/O. The current Fortran D compiler inserts guards to ensure all I/O is performed on a single processor.
Reference: [173] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Compilation techniques have been developed to improve data locality on scalar and shared-memory machines. Program transformations can enhance temporal and spatial locality of scientific codes, improving the usage of higher levels of the memory hierarchy such as registers and cache <ref> [34, 40, 116, 173, 204] </ref>. Heuristics have been developed for managing multiprocessors cache coherence in software through the use of block cache invalidate, prefetch, and update instructions [65, 85]. Taken to the limit, these optimizations begin to resemble message vectorization and code generation for distributed-memory machines.
Reference: [174] <author> M. Quinn and P. Hatcher. </author> <title> Data parallel programming on multicomputers. </title> <journal> IEEE Software, </journal> <volume> 7(5) </volume> <pages> 69-76, </pages> <month> September </month> <year> 1990. </year> <note> BIBLIOGRAPHY 191 </note>
Reference-contexts: Instead of generating virtual processors for each element of a domain, compile-time analysis enables contraction, emulating virtual processors via loops. Researchers have also examined synchronization problems encountered when translating SIMD programs into equivalent SPMD programs, as well as several communication optimizations <ref> [96, 174] </ref>. Experience will show whether SIMD languages such as C* provide sufficient flexibility for a wide class of scientific computations. DINO Dino is an extended version of C supporting general-purpose distributed computation [181, 182].
Reference: [175] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-time techniques for data distribution in distributed memory machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Several researchers have proposed techniques to automatically derive data decompositions based on simple machine models <ref> [88, 89, 109, 141, 175, 193] </ref>.
Reference: [176] <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling multidimensional iteration spaces for nonshared memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Pyrros statically partitions task graphs for multiprocessors [211]. Gupta & Banerjee develop a methodology for generating complex collective communication [90]. Wolfe introduces loop rotation, a combination of loop skew, reversal, and interchange to reduce contention for common data [207]. Ramanujam & Sadayappan tile loop nests for distributed-memory machines <ref> [176] </ref>. O'Boyle & Hedayat develop a linear algebraic framework for parallelizing Sisal [160]. 11.4.3 Analysis-Driven Compilers The final group of distributed-memory compilers are analysis-driven; they rely more on compile-time analysis than language features or user annotations. These compilers typically accept Fortran 77 or 90 programs with data decomposition annotations.
Reference: [177] <author> A. Reeves. </author> <title> The Paragon programming paradigm and distributed memory compilers. </title> <type> Technical Report EE-CEG-90-7, </type> <institution> Cornell University Computer Engineering Group, </institution> <address> Ithaca, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: RELATED WORK support parallel task creation, data and computation mapping, synchronization, and communication [180]. Dyno provides support for irregular and adaptive numeric programs [203]. Paragon Paragon is a C-based programming environment targeted at supporting SIMD programs on MIMD distributed-memory machines <ref> [46, 177] </ref>. It provides both language extensions to C and run-time support for task management and load balancing. Data distribution in Paragon may either be performed by the user or the system. Parallel arrays are mapped onto shapes that consist of arbitrary rectangular distributions.
Reference: [178] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: There are two major concerns in compiling Fortran D for MIMD distributed-memory machines. * Partition data and computation across processors. * Generate communications where needed to access nonlocal data. Our philosophy is to use the "owner computes" rule, where every processor only performs computation for data it owns <ref> [212, 38, 178] </ref>. <p> For clarity, we assume that the compiler targets a machine with four processors. 3.2.1 Run-time Resolution A simple compilation technique known as run-time resolution yields code that explicitly calculates the ownership and communication for each reference at run-time <ref> [38, 178, 212] </ref>. For instance, for the previous example it generates the code shown in Figure 3.2. Run-time resolution does not require much compiler analysis, but the resulting programs are likely to extremely inefficient. In fact, they may execute much slower than the original sequential code. <p> Other communication and parallelism optimizations are deferred until Chapter 5. 4.3.1 Message Vectorization A naive but workable algorithm known as run-time resolution inserts guarded send and/or recv operations directly preceding each nonlocal reference <ref> [38, 178, 212] </ref>. Unfortunately, this simple approach generates many small messages that prove extremely inefficient due to communication overhead [178]. The most basic communication optimization performed by the Fortran D compiler is message vectoriza-tion. <p> Unfortunately, this simple approach generates many small messages that prove extremely inefficient due to communication overhead <ref> [178] </ref>. The most basic communication optimization performed by the Fortran D compiler is message vectoriza-tion. It uses the level of loop-carried data dependences to calculate whether communication may be legally performed at outer loops. This replaces many small messages with one large message, reducing both message startup cost and latency. <p> With multiple statements in the loop, the local iteration set of a statement may be a subset of the reduced loop bounds. For these statements the compiler needs to add explicit guards based on membership tests for the local iteration set of the statement <ref> [38, 178, 212] </ref>. In other cases, the compiler may not be able to localize loop bounds and indices because a processors executes some statement on all iterations of the loop. Statement groups formed during partitioning analysis help detect this situation. <p> The inspector strategy is not applicable for unanalyzable references causing loop-carried true dependences. In this case the Fortran D compiler inserts guards to resolve the needed communication and program execution at run-time <ref> [38, 178, 212] </ref>. 4.4.6 Forall Scalarization Another responsibility of the Fortran D compiler is to convert forall loops into do loops, inserting additional code where necessary to maintain the semantics of the forall loop. <p> The same optimizations can also hide T copy , the message copy time, by using unbuffered messages. 5.3.1 Message Pipelining Message pipelining inserts a send for each nonlocal reference as soon as it is defined <ref> [178] </ref>. The recv is placed immediately before the value is used. Any computation performed between the definition and use of the value can then help hide T transit . Unfortunately, message pipelining prevents optimizations such as message vectorization, resulting in significantly greater total communication cost. <p> The degree of pipeline parallelism depends on how soon each processor is able to begin work after its predecessor starts. One method of exploiting parallelism in pipelined computations through message pipelining|sending a message when its value is first computed, rather than waiting until its value is needed <ref> [178] </ref>. Rogers and Pingali applied this optimization to a Gauss-Seidel computation (a special case of SOR) that is distributed cyclically. However, more sophisticated approaches are usually required. when optimizing pipelined computations. It presents the program text, data space traversal order, and a processor trace for three versions of the computation. <p> For local compilation, Superb modifies procedures so that arrays are always accessed according to their true number of dimensions, inserting additional parameters where necessary for newly created subscripts. Id Nouveau Id Nouveau is a functional language extended with single assignment arrays called I-structures <ref> [170, 178] </ref>. User-specified block distributions are provided. Initially, send and recv statements are inserted to communicate each nonlocal array access. Message vectorization is applied to combine messages for previously written array elements. Loop jamming (fusion) and strip-mining are used when writing array elements. Global accumulate & reduction operations are supported.
Reference: [179] <author> J. Rose and G. Steele, Jr. </author> <title> C fl : An extended C language for data parallel programming. </title> <editor> In L. Kartashev and S. Kartashev, editors, </editor> <booktitle> Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> Santa Clara, CA, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: The CM Fortran compiler has been retargeted for the CM-5, but results show that it fails to fully exploit the CM-5 architecture [186]. C*, Dataparallel C C* and Dataparallel C are extensions of C similar to C++ that supports SIMD data-parallel programs <ref> [96, 97, 179] </ref>. They both provide a local view of computation. Data is labeled as mono (local) or poly (distributed). There are no alignment or distribution specifications; the compiler automatically chooses the data decomposition.
Reference: [180] <author> M. Rosing. </author> <title> Efficient Language Constructs for Complex Data Parallelism on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Colorado, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: However, its many features may prove burdensome to users. Dino2 proposes a large set of additional language features to 172 CHAPTER 11. RELATED WORK support parallel task creation, data and computation mapping, synchronization, and communication <ref> [180] </ref>. Dyno provides support for irregular and adaptive numeric programs [203]. Paragon Paragon is a C-based programming environment targeted at supporting SIMD programs on MIMD distributed-memory machines [46, 177]. It provides both language extensions to C and run-time support for task management and load balancing. <p> Lower-level communication primitives, on the other hand, can avoid 180 CHAPTER 12. CONCLUSIONS much of the overhead of a standard typed message-passing communication layer by making optimistic assumptions or limiting generality. Experience has show this can yield integer factors of improvement in performance <ref> [180] </ref>. By utilizing such communication primitives, the Fortran D compiler can generate programs with better performance. However, greater effort is required on the part of the compiler to ensure these primitives are used correctly.
Reference: [181] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> Expressing complex parallel algorithms in DINO. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: Experience will show whether SIMD languages such as C* provide sufficient flexibility for a wide class of scientific computations. DINO Dino is an extended version of C supporting general-purpose distributed computation <ref> [181, 182] </ref>. Dino supports block, cyclic, and special stencil-based data distributions with overlaps, but provides no alignment specifications. Like C*, it provides the programmer with a local view of the computation. A Dino program contains a virtual parallel machine declared to be an environment.
Reference: [182] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Experience will show whether SIMD languages such as C* provide sufficient flexibility for a wide class of scientific computations. DINO Dino is an extended version of C supporting general-purpose distributed computation <ref> [181, 182] </ref>. Dino supports block, cyclic, and special stencil-based data distributions with overlaps, but provides no alignment specifications. Like C*, it provides the programmer with a local view of the computation. A Dino program contains a virtual parallel machine declared to be an environment.
Reference: [183] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> Scientific programming languages for distributed memory multiprocessors: Paradigms and research issues. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: Some languages specify parallelism through a local view of computation, where the programmer specifies computation for an individual data point, relying on the compiler and run-time system to replicate the computation for all data points <ref> [96, 183] </ref>. In comparison, Fortran D supports a global view of computation, where the program specifies the overall computation, counting on the compiler to partition the computation. In comparison to language-driven compilers, systems like Fortran D reduce the burden on the user through program analysis, the key to advanced optimization.
Reference: [184] <author> R. Ruhl. </author> <title> Evaluation of compiler-generated parallel programs on three multicomputers. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: The compiler then outputs code in the vdm l intermediate language. Calls to the Pandore communication library to access nonlocal data is also automatically generated by the compiler. Guard introduction and communications optimization techniques are under development. Oxygen Oxygen is a compiler for the K2 distributed-memory machine <ref> [184, 185] </ref>. Unlike most systems, Oxygen follows a functional rather than data decomposition strategy. Task-level parallelism is specified by labeling each parallel block of code with a p block directive. Loop-level parallelism is specified by labeling parallel loops with either split or scatter directives.
Reference: [185] <author> R. Ruhl and M. Annaratone. </author> <title> Parallelization of fortran code on distributed-memory parallel processors. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The compiler then outputs code in the vdm l intermediate language. Calls to the Pandore communication library to access nonlocal data is also automatically generated by the compiler. Guard introduction and communications optimization techniques are under development. Oxygen Oxygen is a compiler for the K2 distributed-memory machine <ref> [184, 185] </ref>. Unlike most systems, Oxygen follows a functional rather than data decomposition strategy. Task-level parallelism is specified by labeling each parallel block of code with a p block directive. Loop-level parallelism is specified by labeling parallel loops with either split or scatter directives.
Reference: [186] <author> G. Sabot. </author> <title> A compiler for a massively parallel distributed memory MIMD computer. </title> <booktitle> In Frontiers'92: The 4th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> McLean, VA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: It resembles a highly specialized Fortran D compiler for stencil computations, including optimizations at the assembly code level to improve register usage. The CM Fortran compiler has been retargeted for the CM-5, but results show that it fails to fully exploit the CM-5 architecture <ref> [186] </ref>. C*, Dataparallel C C* and Dataparallel C are extensions of C similar to C++ that supports SIMD data-parallel programs [96, 97, 179]. They both provide a local view of computation. Data is labeled as mono (local) or poly (distributed).
Reference: [187] <author> G. Sabot. </author> <title> Optimized CM Fortran compiler for the Connection Machine computer. </title> <booktitle> In Proceedings of the 25th Annual Hawaii International Conference on System Sciences, </booktitle> <address> Kauai, HI, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: The first CM Fortran compilers treat the underlying machine field-wise as a collection of 1-bit processors, resulting in inefficient code. Later compilers improve performance by operating slice-wise, using 32 bit slices to take advantage of the 32-bit Weitek floating-point processors on the CM-2 <ref> [187] </ref>. The stencil compiler avoids unnecessary intra-processor data motion, inserting communication only for data located on a separate physical, rather than virtual, processor [31]. It resembles a highly specialized Fortran D compiler for stencil computations, including optimizations at the assembly code level to improve register usage.
Reference: [188] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Multiprocessors and run-time compilation. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The distribution functions for decomposition A and array X are then computed through run-time preprocessing techniques <ref> [188, 153] </ref>. Researchers are examining more sophisticated methods of specify irregular distributions for Fortran D programs [47, 172, 209]. 3.3.2 Computation We continue to describe some additional notation we will employ later in this paper. <p> We have adopted many such features into Fortran D. In particular, we have been influenced by alignment specifications from CM Fortran [196], distribution specifications from Kali [127, 153], and structures to handle irregular distributions from Parti <ref> [188] </ref>. We also incorporated the forall statement from Myrias [22] and CM Fortran [6]. <p> It does not currently generate messages at compile-time for regular computations. Arf is designed to interface Fortran application programs with Parti, a set of run-time library routines that support irregular computations on MIMD distributed-memory machines <ref> [188] </ref>. Parti is first to propose and implement user-defined irregular distributions [155] and a hashed cache for nonlocal values [108, 156]. It is employed by a number of systems to support irregular computations, including the Fortran D compiler.
Reference: [189] <author> V. Sarkar and G. Gao. </author> <title> Optimization of array accesses by collective loop transformations. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 8, 11, 40, 77, 132, 150, 189, 202] </ref>.
Reference: [190] <author> W. Shang and J. Fortes. </author> <title> Independent partitioning of algorithms with uniform dependences. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: The basic premise is that since parallel loops exhibiting data locality are guaranteed to compute fairly disjoint data sets, the partition of parallel loop iterations among processors can also be used to assign data to each processor <ref> [66, 168, 190] </ref>. Where data accesses are not entirely disjoint, grouping methods can be applied to reduce communication between loop iterations [120, 191]. However, in order to take advantage of data locality, the compiler must take into account affinity, the interaction between data placement and task scheduling [149].
Reference: [191] <author> J. Sheu and T. Tai. </author> <title> Partitioning and mapping nested for-loops on multiprocessor systems. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Where data accesses are not entirely disjoint, grouping methods can be applied to reduce communication between loop iterations <ref> [120, 191] </ref>. However, in order to take advantage of data locality, the compiler must take into account affinity, the interaction between data placement and task scheduling [149].
Reference: [192] <author> D. Skillicorn. </author> <title> Architecture-independent parallel computation. </title> <journal> IEEE Computer, </journal> <volume> 23(12), </volume> <month> December </month> <year> 1990. </year> <note> 192 BIBLIOGRAPHY </note>
Reference-contexts: RELATED WORK 11.2 Programming Models and Languages The proliferation of parallel architectures has focused much attention on machine-independent parallel programming. Selecting an efficient level of parallel programming is an open research issue. Some researchers have proposed elegant architecture-independent programming models such as Bird-Meertens formalism <ref> [192] </ref> and the Bulk-Synchronous bridging model [201]. However, these parallel programming models are intended to guide the development of new data-parallel algorithms; they cannot be used to help scientists write parallel programs since no language or compiler support is provided.
Reference: [193] <author> L. Snyder and D. Socha. </author> <title> An algorithm producing balanced partitionings of data arrays. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Several researchers have proposed techniques to automatically derive data decompositions based on simple machine models <ref> [88, 89, 109, 141, 175, 193] </ref>. <p> Messages are inserted at points in the program called checkpoints to enforce coarse-grain synchronization. Work is in progress to automatically generate Oxygen directives for functional and data decomposition. SPOT Spot is a point-based SIMD data-parallel programming language <ref> [193, 194] </ref>. Distributed arrays are defined as regions. Computations are specified from the point of view of a single element in the region, called a point. Locations relative to a given point are assigned symbolic names by neighbor declarations.
Reference: [194] <author> D. Socha. </author> <title> Compiling single-point iterative programs for distributed memory computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Messages are inserted at points in the program called checkpoints to enforce coarse-grain synchronization. Work is in progress to automatically generate Oxygen directives for functional and data decomposition. SPOT Spot is a point-based SIMD data-parallel programming language <ref> [193, 194] </ref>. Distributed arrays are defined as regions. Computations are specified from the point of view of a single element in the region, called a point. Locations relative to a given point are assigned symbolic names by neighbor declarations.
Reference: [195] <author> V. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: DISCUSSION 165 10.3.2 Run-Time System The prototype Fortran D compiler generates calls to Intel NX/2 message-passing libraries, and will soon be extended to also target the Thinking Machines CMMD library. Future targets may include portable, efficient communication libraries such as Express [167] or PVM <ref> [195] </ref>. Because these communication libraries have similar message-passing interfaces, adding new targets is a straightforward task. Ongoing standardization efforts for message-passing libraries will ease the burden of the compiler writer. 10.4 Discussion The Fortran D compiler automates the time consuming task of deriving node programs based on the data decomposition.
Reference: [196] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, version 1.0 edition, </note> <month> February </month> <year> 1991. </year>
Reference-contexts: The extensions proposed in Fortran D are compatible with both Fortran 77 and Fortran 90, a version of Fortran with explicit manipulation of high-level array structures. Fortran 90D can be viewed as a refinement of CM Fortran <ref> [196] </ref> consistent with a parallel Fortran 77. We consider Fortran D to be one of the first of a new generation of data-placement programming languages. <p> In particular, forall loops are deterministic. As a result we believe that it will be easy to understand and use for scientific programmers. The forall loop possesses similar semantics to the CM Fortran forall statement <ref> [196, 6] </ref> and the Myrias PARDO loop [22]. <p> The resulting program, shown in Figure 7.9, is much less efficient than the code in Figure 7.7. This example also points out limitations for language extensions designed to avoid interprocedural analysis. Language features such as interface blocks <ref> [196] </ref> require the user to specify information at procedure boundaries. These features impose additional burdens on the programmer, but can reduce or eliminate the need for interprocedural analysis. However, current language extensions are insufficient for inter-procedural optimizations. <p> Though most parallel languages concentrate on specifying synchronization for task-level parallelism, we found several languages constructs useful for data-parallel programming. We have adopted many such features into Fortran D. In particular, we have been influenced by alignment specifications from CM Fortran <ref> [196] </ref>, distribution specifications from Kali [127, 153], and structures to handle irregular distributions from Parti [188]. We also incorporated the forall statement from Myrias [22] and CM Fortran [6]. <p> Major differences between Kali and the Fortran D compiler include mandatory on clauses for parallel loops, support for alignment, collective communications, and dynamic decomposition. CM Fortran CM Fortran is a version of Fortran 77 extended with vector notation, alignment, and data layout specifications <ref> [5, 196] </ref>. Programmers must explicitly specify data-parallelism through the use of array operations and by marking array dimensions as parallel. The CM Fortran compiler utilizes user-defined interface blocks to specify a data partition for each procedure.
Reference: [197] <author> S. Tjiang, M. E. Wolf, M. Lam, K. Pieper, and J. Hennessy. </author> <title> Integrating scalar optimization and parallelization. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Fourth International Workshop, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: Shared-memory parallelizing compilers such as Parafrase [131, 171], Pfc [9, 10], Ptran [7], ParaScope [35, 59], and Suif <ref> [197] </ref> use data dependence [130, 132] to detect and exploit parallel loops on MIMD shared-memory machines. Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases [19, 83, 133, 208].
Reference: [198] <author> P.-S. Tseng. </author> <title> A parallelizing compiler for distributed memory parallel computers. </title> <booktitle> In Proceedings of the SIGPLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: No copies take place, but an error results if different decompositions are encountered. We attempt to achieve the same benefits in the Fortran D compiler through interprocedural analysis and optimization. AL, iWarp Al is a language designed for the Warp distributed-memory systolic processor <ref> [198, 199] </ref>. The programmer utilizes darray declarations to mark parallel arrays. The Al compiler then applies data relations to automatically align and distribute each darray, detect parallelism, and generate communication. Only one dimension of each darray may be distributed, and computations must be linearly related.
Reference: [199] <author> P.-S. Tseng. </author> <title> A systolic array parallelizing compiler. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9(2) </volume> <pages> 116-127, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: No copies take place, but an error results if different decompositions are encountered. We attempt to achieve the same benefits in the Fortran D compiler through interprocedural analysis and optimization. AL, iWarp Al is a language designed for the Warp distributed-memory systolic processor <ref> [198, 199] </ref>. The programmer utilizes darray declarations to mark parallel arrays. The Al compiler then applies data relations to automatically align and distribute each darray, detect parallelism, and generate communication. Only one dimension of each darray may be distributed, and computations must be linearly related.
Reference: [200] <author> T. v. Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In addition, send and receive messages can be translated into hints for optimizing inter-processor data movement using block data prefetch and update. 12.5.3 Low-level Communication Primitives Another improvement we plan for the Fortran D compiler is to exploit low-level communication primitives such as active messages <ref> [200] </ref>. Standard message-passing libraries incur significant software overhead due to their generality and fault tolerance. Lower-level communication primitives, on the other hand, can avoid 180 CHAPTER 12. CONCLUSIONS much of the overhead of a standard typed message-passing communication layer by making optimistic assumptions or limiting generality.
Reference: [201] <author> L. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Selecting an efficient level of parallel programming is an open research issue. Some researchers have proposed elegant architecture-independent programming models such as Bird-Meertens formalism [192] and the Bulk-Synchronous bridging model <ref> [201] </ref>. However, these parallel programming models are intended to guide the development of new data-parallel algorithms; they cannot be used to help scientists write parallel programs since no language or compiler support is provided.
Reference: [202] <author> J. Warren. </author> <title> A hierachical basis for reordering transformations. </title> <booktitle> In Conference Record of the Eleventh Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <month> January </month> <year> 1984. </year>
Reference-contexts: A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests <ref> [8, 202, 205] </ref>. The current Fortran D back end fuses all adjacent loop nests where legal, if no loop-carried true dependences are introduced. This heuristic does not adversely affect the parallelism or communication overhead 126 CHAPTER 8. <p> This heuristic does not adversely affect the parallelism or communication overhead 126 CHAPTER 8. COMPILING FORTRAN 77D AND 90D of the resulting program, and should perform well for the simple cases found in practice. More sophisticated algorithms are discussed elsewhere <ref> [9, 84, 115, 150, 202] </ref>. Loop fusion also has the added advantage of being able to improve memory reuse in the resulting program. Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. <p> Modern high-performance processors are so fast that memory latency and bandwidth limitations become the performance bottlenecks for most scientific programs. Transformations such as loop fusion promote memory reuse and can significantly improve program efficiency for both scalar and vector machines <ref> [1, 8, 11, 40, 77, 132, 150, 189, 202] </ref>.
Reference: [203] <author> R. Weaver and R. Schnabel. </author> <title> Automatic mapping and load balancing of pointer-based dynamic data structures on distributed memory machines. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: However, its many features may prove burdensome to users. Dino2 proposes a large set of additional language features to 172 CHAPTER 11. RELATED WORK support parallel task creation, data and computation mapping, synchronization, and communication [180]. Dyno provides support for irregular and adaptive numeric programs <ref> [203] </ref>. Paragon Paragon is a C-based programming environment targeted at supporting SIMD programs on MIMD distributed-memory machines [46, 177]. It provides both language extensions to C and run-time support for task management and load balancing. Data distribution in Paragon may either be performed by the user or the system. <p> It is employed by a number of systems to support irregular computations, including the Fortran D compiler. Pandore Pandore is a compiler for distributed-memory machines that takes as input C programs extended with block, cyclic, and overlapping data distributions <ref> [12, 203] </ref>. Distributed arrays are mapped by the compiler onto a user-declared virtual distributed machine that may be configured as a vector, ring, grid, or torus. The compiler then outputs code in the vdm l intermediate language.
Reference: [204] <author> M. E. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Compilation techniques have been developed to improve data locality on scalar and shared-memory machines. Program transformations can enhance temporal and spatial locality of scientific codes, improving the usage of higher levels of the memory hierarchy such as registers and cache <ref> [34, 40, 116, 173, 204] </ref>. Heuristics have been developed for managing multiprocessors cache coherence in software through the use of block cache invalidate, prefetch, and update instructions [65, 85]. Taken to the limit, these optimizations begin to resemble message vectorization and code generation for distributed-memory machines.
Reference: [205] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: The level of a loop-carried dependence is the depth of the loop carrying the dependence [10]. Loop-independent dependences have infinite depth. The number of loop iterations separating the source and sink of the loop-carried dependence may be characterized by a dependence distance or direction <ref> [205] </ref>. The level of a dependence is determined by the first non-zero entry in its distance or direction vector. <p> In Figure 5.1, we use ffi, *, and ? to mark the extent of our contribution to each optimization technique. Shared-memory parallelizing compilers apply program transformations to expose or enhance parallelism in scientific codes, using dependence information to determine their legality and profitability <ref> [10, 117, 132, 205] </ref>. We have found that transformations such as loop interchange, fusion, distribution, alignment, and strip-mining to be also quite useful for optimizing Fortran D programs. <p> It may be applied only if the source and sink of each dependence are not reversed in the resulting program. This may be determined by examining the distance or direction vector associated with each dependence <ref> [10, 205] </ref>. Strip-mining increases the step size of an existing loop and adds an additional inner loop. The legality of applying strip-mine followed by loop interchange is determined in the same manner as for unroll-and-jam [117]. <p> It may also be used with loop interchange to help exploit pipeline parallelism, as discussed in the next section. Loop fusion combines multiple loops with identical headers into a single loop. It is legal if the direction of existing dependences are not reversed after fusion <ref> [132, 205] </ref>. Loop fusion can improve data locality, but its main use in the Fortran D compiler is to fuse imperfectly nested loops in order to enable loop interchange and strip-mine. 5.4.7 Fine-grain Pipelining We present two optimizations to exploit pipeline parallelism. <p> We find that the desired order for compilation phases is to apply loop fusion first, followed by partitioning and sectioning. Loop fusion is performed first because it simplifies partitioning by reducing the need to consider inter-loop interactions. It also enables optimizations such as strip-mining and loop interchange <ref> [10, 205] </ref>. In addition, loop fusion does not increase the difficulty of later compiler phases. On the other hand, sectioning is performed last because it can significantly disrupt the existing program structure, increasing the difficulty of partitioning analysis and optimization. <p> Fusing such loops simplifies the partitioning process and enables additional optimizations. Data dependence is a concept developed for vectorizing and parallelizing compilers to characterize memory access patterns at compile time <ref> [10, 132, 205] </ref>. A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests [8, 202, 205]. <p> A true dependence indicates definition followed by use, while an anti-dependence shows use before definition. Data dependences may be either loop-carried or loop-independent. Loop fusion is legal if it does not reverse the direction of any data dependence between two loop nests <ref> [8, 202, 205] </ref>. The current Fortran D back end fuses all adjacent loop nests where legal, if no loop-carried true dependences are introduced. This heuristic does not adversely affect the parallelism or communication overhead 126 CHAPTER 8.
Reference: [206] <author> M. J. Wolfe. </author> <title> Semi-automatic domain decomposition. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: These compilers can take Fortran or C programs written in a vectorizable style and automatically convert them to run efficiently on any vector machine <ref> [206, 39] </ref>. This provides a machine-independent programming model that allows scientific programmers to concentrate on their actual algorithms, introducing high-level parallelism where needed. The compiler handles machine-dependent optimizations for efficient execution. The resulting programs are easily maintained and portable. Compare this with the task of programming existing parallel machines.
Reference: [207] <author> M. J. Wolfe. </author> <title> Loop rotation. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: MetaMP provides a high-level interface for explicit message-passing [162]. Pyrros statically partitions task graphs for multiprocessors [211]. Gupta & Banerjee develop a methodology for generating complex collective communication [90]. Wolfe introduces loop rotation, a combination of loop skew, reversal, and interchange to reduce contention for common data <ref> [207] </ref>. Ramanujam & Sadayappan tile loop nests for distributed-memory machines [176]. O'Boyle & Hedayat develop a linear algebraic framework for parallelizing Sisal [160]. 11.4.3 Analysis-Driven Compilers The final group of distributed-memory compilers are analysis-driven; they rely more on compile-time analysis than language features or user annotations.
Reference: [208] <author> M. J. Wolfe and C. Tseng. </author> <title> The Power test for data dependence. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5) </volume> <pages> 591-601, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Precise program analysis is needed to handle symbolics [91], procedure calls [62, 98, 143], and array reference aliases <ref> [19, 83, 133, 208] </ref>. Program transformations based on dependences may also be used to expose additional parallelism [117, 118, 119, 164]. Shared-memory parallelizing compilers can aid the programming process on distributed shared-memory machines, but possess several shortcomings.
Reference: [209] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The distribution functions for decomposition A and array X are then computed through run-time preprocessing techniques [188, 153]. Researchers are examining more sophisticated methods of specify irregular distributions for Fortran D programs <ref> [47, 172, 209] </ref>. 3.3.2 Computation We continue to describe some additional notation we will employ later in this paper. <p> A global transpose operation between processors can then be used to calculate the Out index sets as well. An inspector is the most general way to generate In and Out sets for loops without loop-carried dependences. Despite the expense of additional communications, experimental evidence from several systems <ref> [127, 209] </ref> proves that it can improve performance by combining communications to access nonlocal data outside of the loop nest. In addition it also allows multiple messages to the same processor to be combined. The Fortran D compiler plans to automatically generate inspectors where needed for irregular computations. <p> This optimization is a simple application of the "owner stores" rule proposed by Balasundaram [15]. In particular, it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on a single processor, such as in Kali and Arf <ref> [127, 209] </ref>. This technique may improve communication and provides greater control over load balance, especially for irregular computations. It also eliminates the need for individual statement masks and simplifies handling of control flow within the loop body. <p> Paragon has been extended to handle the special class of irregularly-coupled regular-meshes [47]. It does not currently perform analysis or transformations to detect or enhance parallelism. ARF, PARTI Arf is a compiler for irregular computations <ref> [209] </ref>. It provides block and cyclic distributions, and is the first compiler to support user-defined irregular distributions. The goal of Arf is to demonstrate that inspector/executors can be automatically generated by the compiler for user-specified parallel loops. It does not currently generate messages at compile-time for regular computations.
Reference: [210] <author> M. Wu and G. Fox. </author> <title> A test suite approach for Fortran 90D compilers on MIMD distributed memory parallel computers. </title> <booktitle> In Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year> <note> BIBLIOGRAPHY 193 </note>
Reference-contexts: The first group of compilers target Fortran 90. Adapt [154] and Adaptor [27] both perform translations relying on run-time support from a portable library. Wu & Fox describe development of a Fortran 90D compiler developed and validated via a test-suite approach <ref> [210] </ref>. These compilers perform little analysis or optimization, extracting parallelism from Fortran 90 array syntax or parallel loop annotations. The remaining systems are quite varied. Booster provides a system to annotate data placement through user-defined shapes and views [163]. Orca-1 supports explicit parallelism through phase abstractions [145].
Reference: [211] <author> T. Yang and A. Gerasoulis. </author> <title> PYRROS: Static task scheduling and code generation for message passing multiprocessors. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Booster provides a system to annotate data placement through user-defined shapes and views [163]. Orca-1 supports explicit parallelism through phase abstractions [145]. Modula-2* provides a superset of data-parallel constructs [169]. MetaMP provides a high-level interface for explicit message-passing [162]. Pyrros statically partitions task graphs for multiprocessors <ref> [211] </ref>. Gupta & Banerjee develop a methodology for generating complex collective communication [90]. Wolfe introduces loop rotation, a combination of loop skew, reversal, and interchange to reduce contention for common data [207]. Ramanujam & Sadayappan tile loop nests for distributed-memory machines [176].
Reference: [212] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: Analyze communication. Based on the work partition, references that result in nonlocal accesses are marked. 5. Optimize communication. Nonlocal references are examined to determine optimization opportunities. The key optimization, message vectorization, uses the level of loop-carried true dependences to combine element messages into vectors <ref> [16, 212] </ref>. 6. Manage storage. "Overlaps" [212] or buffers are allocated to store nonlocal data. 7. Generate code. The compiler instantiates the communication, data and work partition determined previously, generating a Fortran 77 program with explicit message-passing. <p> Based on the work partition, references that result in nonlocal accesses are marked. 5. Optimize communication. Nonlocal references are examined to determine optimization opportunities. The key optimization, message vectorization, uses the level of loop-carried true dependences to combine element messages into vectors [16, 212]. 6. Manage storage. "Overlaps" <ref> [212] </ref> or buffers are allocated to store nonlocal data. 7. Generate code. The compiler instantiates the communication, data and work partition determined previously, generating a Fortran 77 program with explicit message-passing. <p> There are two major concerns in compiling Fortran D for MIMD distributed-memory machines. * Partition data and computation across processors. * Generate communications where needed to access nonlocal data. Our philosophy is to use the "owner computes" rule, where every processor only performs computation for data it owns <ref> [212, 38, 178] </ref>. <p> For clarity, we assume that the compiler targets a machine with four processors. 3.2.1 Run-time Resolution A simple compilation technique known as run-time resolution yields code that explicitly calculates the ownership and communication for each reference at run-time <ref> [38, 178, 212] </ref>. For instance, for the previous example it generates the code shown in Figure 3.2. Run-time resolution does not require much compiler analysis, but the resulting programs are likely to extremely inefficient. In fact, they may execute much slower than the original sequential code. <p> Other communication and parallelism optimizations are deferred until Chapter 5. 4.3.1 Message Vectorization A naive but workable algorithm known as run-time resolution inserts guarded send and/or recv operations directly preceding each nonlocal reference <ref> [38, 178, 212] </ref>. Unfortunately, this simple approach generates many small messages that prove extremely inefficient due to communication overhead [178]. The most basic communication optimization performed by the Fortran D compiler is message vectoriza-tion. <p> With multiple statements in the loop, the local iteration set of a statement may be a subset of the reduced loop bounds. For these statements the compiler needs to add explicit guards based on membership tests for the local iteration set of the statement <ref> [38, 178, 212] </ref>. In other cases, the compiler may not be able to localize loop bounds and indices because a processors executes some statement on all iterations of the loop. Statement groups formed during partitioning analysis help detect this situation. <p> The inspector strategy is not applicable for unanalyzable references causing loop-carried true dependences. In this case the Fortran D compiler inserts guards to resolve the needed communication and program execution at run-time <ref> [38, 178, 212] </ref>. 4.4.6 Forall Scalarization Another responsibility of the Fortran D compiler is to convert forall loops into do loops, inserting additional code where necessary to maintain the semantics of the forall loop. <p> To see how this strategy works, first recall from Chapter 4 that message vectorization uses the level of the deepest loop-carried true dependence to combine messages at outer loop levels <ref> [16, 212] </ref>. Communication for loop-carried dependences is inserted at the beginning of the loop that carries the dependence. Communication for loop-independent dependences is inserted in the body of the loop enclosing both the source and sink of the dependence. <p> The Callahan-Kennedy compiler prototype was eventually abandoned in favor of the greater flexibility of the Fortran D compiler. SUPERB Superb is a semi-automatic parallelization tool designed for MIMD distributed-memory machines <ref> [79, 80, 212] </ref>. It supports arbitrary user-specified contiguous rectangular distributions, and performs dependence analysis to guide interactive program transformations in a manner similar to the ParaScope Editor [129]. Superb originated overlaps as a means to both specify and store nonlocal data accesses.
Reference: [213] <author> R. Zucker and J.-L. Baer. </author> <title> A performance study of memory consistency models. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: They cannot hide memory latency by prefetching data before it is needed, or reduce data movement costs by fetching entire logical blocks at once. At best they can relax memory consistency, maintain a history of past accesses, and try to guess future reference patterns <ref> [112, 213] </ref>. Unlike compilers, operating systems are incapable of arranging data layout to avoid contention, reordering computation to improve data movement, or replicating computation to eliminate communication.
References-found: 213

