URL: ftp://ftp.irit.fr/pub/IRIT/APARA/Superscalar/R_IRIT_96_09.ps.gz
Refering-URL: http://www.irit.fr/ACTIVITES/EQ_APARA/publi_equipe.html
Root-URL: 
Title: Multiple-Block Ahead Branch Predictors  
Author: R A P P O R T N I R I T R e t I R I S Andr Seznec, Stphan Jourdan, Pascal Sainrat, Pierre Michaud 
Date: 9 9 0  
Note: A  
Affiliation: INSTITUT DE RECHERCHE EN INFORMATIQUE DE TOULOUSE Centre National de la Recherche ScientifiqueInstitut National Polytechnique de ToulouseUniversit Paul Sabatier  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Butler and Y. N. Patt, </author> <title> An Investigation of the Performance of Various Dynamic Scheduling Techniques, </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: The upper bound of the number of instructions dispatched each cycle defines the dispatch width. Registers are renamed using a map table during the dispatch process. Instructions in the issue buffer may be issued out-of-order when all their operands are available, and a max-dependent selection mechanism as described in <ref> [1] </ref> is used when more than one instruction compete for the same functional unit access. To enforce precise interrupt management, a history buffer [22] similar to the active list of the R10000, records the previous mappings discarded by the renaming process during the dispatch stage.
Reference: [2] <author> B. Calder and D. Grunwald, </author> <title> Next Cache Line and Set Prediction, </title> <booktitle> Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference: [3] <author> T. M. Conte, K. N. Menezes, P. M. Mills, and B. A. Patel, </author> <title> Optimization of Instruction Fetch Mechanisms for High Issue Rates, </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The prediction mechanism gives good accuracy results compared to non-hybrid schemes without a lot of additional logic. But their scheme mostly relies o n compiler work to partition the CFG into treelike subgraphs. This approach may be a good alternative to conventional branch prediction schemes in the future. In <ref> [3] </ref>, the authors introduced a mechanism called the Collapsing Buffer that achieved merging [12].
Reference: [4] <author> S. Dutta and M. Franklin, </author> <title> Control Flow Prediction with TreeLike Subgraphs for Superscalar Processors, </title> <booktitle> Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: A processor featuring such a mechanism would have to predict multiple targets and branch outcomes in a single cycle. Several recent papers have proposed implementations for such a predictor [26] <ref> [4] </ref>. These studies have established the viability of predicting two branches in a single cycle, but are not attractive because of the large hardware costs required and the prediction scheme used. <p> In [18], the authors proposed to split the Control Flow Graph (CFG) into subgraphs. In order to fetch two nonconsecutive basic blocks even when they belong to different cache lines, they use treelike subgraphs of depth 3 <ref> [4] </ref>. Nodes of the subgraphs are straightline pieces of code (basic blocks).
Reference: [5] <author> L. Gwennap, </author> <title> Comparing RISC Microprocessors, </title> <booktitle> Proceedings of the Microprocessor Forum, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: The amount of information stored in the two-block ahead branch predictor is in the same range as in a conventional branch prediction mechanism. Moreover, any prediction scheme can be used to determine the outcome of conditional branches leading to better prediction accuracy. As described in <ref> [5] </ref>, two different approaches are used to achieve high performance in superscalar machines: brainiacs vs. speed demons. The two-block ahead predictor is really useful in double I-fetch brainiac processors since it increases the amount of potential ILP.
Reference: [6] <author> L. Gwennap, </author> <title> Digital Leads the Pack with 21164, </title> <type> Microprocessor Report, </type> <month> Sept. 94. </month>
Reference-contexts: block address is used to predict the next instruction block, either the instruction address generator can compute the starting address of the next instruction block in a single cycle as in the PentiumPro, or bubbles are inserted in the pipeline in the case of branches as in the DEC 21164 <ref> [6] </ref>. The instruction-address generation process is quite complex because it includes several consecutive steps: 1. Parallel accesses to the BTB, the Prediction Table (PT), and the return address stack. Computation of the fall-through address. 2. Prediction of the outcome and selection of the generated address.
Reference: [7] <author> L. Gwennap, </author> <title> PA-8000 Combines Complexity and Speed, Microprocessor Report, </title> <journal> Vol. 8 Num. </journal> <volume> 15, </volume> <year> 1994. </year>
Reference-contexts: To best exploit the available ILP, wide-dispatch processors featuring a dozen functional units working in parallel would be required. The HP PA-8000 <ref> [7] </ref> has already been implemented with a comparable number of functional units. Unfortunately, the instruction-fetch mechanisms implemented in current commercial microprocessors do not fully exploit this potential parallelism.
Reference: [8] <author> L. Gwennap, </author> <title> Intels P6 Uses Decoupled Superscalar Design, </title> <type> Microprocessor Report, </type> <note> Vol. 9 Num. 2, 1995. 18 Pascal Sainrat </note>
Reference-contexts: By the end of the cycle, the starting address of the next instruction block must be generated. In some of the processors, the I-cache access time is longer than the cycle time, leading to a pipeline structure depicted in figure 6 (a) 1 . For instance, the Intel PentiumPro <ref> [8] </ref> features a pipelined I-cache access completed within two and a half cycles.
Reference: [9] <author> W. M. Hwu, Y. N. Patt, </author> <title> Checkpoint Repair for Out-of-Order Execution Machines, </title> <booktitle> Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1987. </year>
Reference-contexts: Buffer Instruction-Dispatch Instruction-Issue Buffer Pool of Functional Units Map Table Regs Free in-order out-of-order Issued Dispatched Executed Processor State Checkpoints Buffer History Completed Fetched Checkpoints <ref> [9] </ref> of the map table (architectural state) are established at every branch in order to recover from branch misprediction in one cycle, regardless of the number of mapping modifications recorded in the history buffer.
Reference: [10] <author> W. M. Hwu et al., </author> <title> The superblock: An effective technique for VLIW and superscalar compilation, </title> <journal> The Journal of Supercomputing, </journal> <month> January </month> <year> 1993. </year>
Reference-contexts: One should note that another way to solve this problem is to enlarge the size of the basic blocks by means of such techniques as predicated execution [15] or superblock scheduling <ref> [10] </ref>. Despite the fact that this paper focuses on hardware schemes, the two approaches are not exclusive. In superscalar processors, blocks of consecutive instructions are fetched in parallel.
Reference: [11] <author> IBM and Motorola, </author> <title> PowerPC 604 RISC Microprocessor Users Manual, </title> <address> MPR604UMU-01, </address> <year> 1994. </year>
Reference-contexts: The instruction latencies used in the simulations were those of the PowerPC 604 <ref> [11] </ref>. The mean-IPC values varied from 3.6 to 6.5 on integer programs according to the dispatch width (4, 6, and 8 instructions dispatched per cycle). We keep their configurations (DW 4, DW 6 and DW 8) in order to evaluate the fetch mechanisms.
Reference: [12] <author> M. Johnson, </author> <title> Superscalar Microprocessor Design, </title> <publisher> Prentice-Hall, </publisher> <year> 1991. </year>
Reference-contexts: But their scheme mostly relies o n compiler work to partition the CFG into treelike subgraphs. This approach may be a good alternative to conventional branch prediction schemes in the future. In [3], the authors introduced a mechanism called the Collapsing Buffer that achieved merging <ref> [12] </ref>. This mechanism can fetch multiple basic blocks in a single cycle as long as they belong to the same cache line, and otherwise performs some alignments between two basic blocks by means of a pipelined fetch mechanism (banked sequential).
Reference: [13] <author> S. Jourdan, P. Sainrat, and D. Litaize, </author> <title> Exploring Configurations of Functional Units in an Out-of-Order Superscalar Processor, </title> <booktitle> Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: 1 Introduction With the advance of technology, current superscalar processors can fetch and execute several instructions in parallel on each cycle. For instance, the MIPS R10000 [17] fetches up to four instructions and executes up to five instructions per cycle by means of five functional units. Recent studies <ref> [13] </ref> have shown that the potentially available instruction-level parallelism (ILP) in general-purpose integer applications is higher than six instructions per cycle while assuming a perfect instruction-fetch mechanism. To best exploit the available ILP, wide-dispatch processors featuring a dozen functional units working in parallel would be required.
Reference: [14] <author> S. Jourdan, P. Sainrat, and D. Litaize, </author> <title> An Investigation of the Performance of Various Instruction-Issue Buffer Topologies, </title> <booktitle> Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: In the latter instruction class, subsequent entries may be dequeued as soon as the address is computed. Each cycle, multiple out-of-order instruction retirements can be made, freeing the physical registers to be reused in the renaming process.A previous study <ref> [14] </ref> has shown that configurations reported in table 1 of such out-of-order architectures give almost no performance loss over perfect configurations only limited by the size of the lookahead window, assuming an ideal-fetch mechanism and no misprediction.
Reference: [15] <author> S. A. Mahlke, et al., </author> <title> Characterizing the Impact of Predicted Execution on Branch Prediction, </title> <booktitle> Proceedings of the 27th Annual International Symposium on Microarchitecture, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: One should note that another way to solve this problem is to enlarge the size of the basic blocks by means of such techniques as predicated execution <ref> [15] </ref> or superblock scheduling [10]. Despite the fact that this paper focuses on hardware schemes, the two approaches are not exclusive. In superscalar processors, blocks of consecutive instructions are fetched in parallel.
Reference: [16] <author> S. McFarling, </author> <title> Combing Branch Predictors, </title> <note> Technical Note TN-36, DEC-WRL, </note> <month> June </month> <year> 1993. </year>
Reference: [17] <author> Mips Technologies Incorporated, </author> <title> R10000 Microprocessor Product Overview, </title> <type> Technical Report, </type> <month> October </month> <year> 1994. </year>
Reference-contexts: 1 Introduction With the advance of technology, current superscalar processors can fetch and execute several instructions in parallel on each cycle. For instance, the MIPS R10000 <ref> [17] </ref> fetches up to four instructions and executes up to five instructions per cycle by means of five functional units. Recent studies [13] have shown that the potentially available instruction-level parallelism (ILP) in general-purpose integer applications is higher than six instructions per cycle while assuming a perfect instruction-fetch mechanism.
Reference: [18] <author> D. N. Pnevmatikatos, M. Franklin, and G. S. Sohi, </author> <title> Control Flow Prediction for Dynamic ILP Processors, </title> <booktitle> Proceedings of the 26th Annual International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Finally, a basic block can belong to several BAC entries, so their mechanism does not require any static partitioning. Redundancies are however created, but the scheme does not rely on any compiler work. In <ref> [18] </ref>, the authors proposed to split the Control Flow Graph (CFG) into subgraphs. In order to fetch two nonconsecutive basic blocks even when they belong to different cache lines, they use treelike subgraphs of depth 3 [4]. Nodes of the subgraphs are straightline pieces of code (basic blocks).
Reference: [19] <editor> S. Simone et al., </editor> <booktitle> Implementation Tradeoffs in Using a Restricted Data Flow Architecture in a High Performance RISC Microprocessor,Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The lookahead window is the maximum number of dispatched instructions that can be processed at the same time, sometimes referred as the instruction window. Moreover, we assumed for all the models a unified issue-buffer, and a maximum number of 16 checkpoints as in the Sparc64 <ref> [19] </ref>. Such a value does not degrade the performance of any of the models. Finally, one should note that our results do not rely on any misfetch or mispredict penalty value since the simulator models a real processor except for the data cache which is modeled as an infinite cache.
Reference: [20] <author> A. Seznec, </author> <title> Dont use the page number, but a pointer to it, </title> <booktitle> to appear in Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference: [21] <author> J. E. Smith, </author> <title> A Study of Branch Prediction Strategies, </title> <booktitle> Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1981. </year>
Reference-contexts: The Collapsing Buffer scheme is efficient as long as branch targets address the same cache line, and performs well on their execution model retiring less than 2.5 instructions per cycle on integer applications. Another major drawback is the requisite use of a 2-bit counter prediction scheme <ref> [21] </ref> which results in a higher misprediction rates than other schemes such as local schemes [16][27]. Moreover, as the I-cache line size keeps growing in current processors, the interleaving factor of the BTB grows as much and the collapsing logic becomes more complex.
Reference: [22] <author> J. E. Smith and A. R. Pleszkun, </author> <title> Implementation of Precise Interrupts in Pipelined Processors, </title> <booktitle> Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1985. </year>
Reference-contexts: Instructions in the issue buffer may be issued out-of-order when all their operands are available, and a max-dependent selection mechanism as described in [1] is used when more than one instruction compete for the same functional unit access. To enforce precise interrupt management, a history buffer <ref> [22] </ref> similar to the active list of the R10000, records the previous mappings discarded by the renaming process during the dispatch stage.
Reference: [23] <author> M. D. Smith, </author> <title> Tracing with Pixie, </title> <type> Technical report, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Two subsets are defined: CINT92 (integer) and CFP92 (floating-point). All programs from both sets were used to make our evaluations. The benchmarks were compiled on a R4600-based SGI workstation using cc and the standard makefiles provided with the suite (with all optimizations turned on). We used the pixie profiler <ref> [23] </ref> to collect instruction traces from a real processing of the SPEC benchmarks, including library calls. These traces fed our simulator which performs a cycle-by-cycle simulation and gathers the mean number of instructions retired per cycle (IPC).
Reference: [24] <institution> SPEC 92, </institution> <type> Technical report, </type> <month> December </month> <year> 1992. </year>
Reference-contexts: One should note that our approach is not incompatible with collapsing. 3 Experimental Setup 3 . 1 Traces The experimental results presented in this paper are based on the programs from the SPEC92 suite <ref> [24] </ref>. Two subsets are defined: CINT92 (integer) and CFP92 (floating-point). All programs from both sets were used to make our evaluations. The benchmarks were compiled on a R4600-based SGI workstation using cc and the standard makefiles provided with the suite (with all optimizations turned on).
Reference: [25] <author> S. Weiss and J. E. Smith, </author> <title> POWER and PowerPC: </title> <booktitle> Principles, Architecture and Implementation, </booktitle> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <year> 1994. </year>
Reference-contexts: One way to partially remove the instruction fetch bottleneck, is to fetch instructions belonging to multiple consecutive basic blocks, as is done in several processors such as the POWER2 <ref> [25] </ref>. In order to solve the whole problem, multiple nonconsecutive basic blocks must be fetched in a single cycle as most basic blocks are only four to six instructions long. A processor featuring such a mechanism would have to predict multiple targets and branch outcomes in a single cycle.
Reference: [26] <author> T. Yeh, D. T. Marr, and Y. N. Patt, </author> <title> Increasing the Instruction Fetch Rate via Multiple Branch Prediction and a Branch Address Cache, </title> <booktitle> Proceedings of the 7th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: A processor featuring such a mechanism would have to predict multiple targets and branch outcomes in a single cycle. Several recent papers have proposed implementations for such a predictor <ref> [26] </ref> [4]. These studies have established the viability of predicting two branches in a single cycle, but are not attractive because of the large hardware costs required and the prediction scheme used. <p> Only a few studies <ref> [26] </ref>[18][3] have addressed the problem of fetching multiple nonconsecutive basic blocks in a single cycle. In order to fetch two basic blocks in a single cycle, Yeh et al. [26] proposed to store 6 addresses in each entry of their Branch Address Cache (BAC): T, N, TT, TN, NT, and NN, where N and T refer to the outcome of the branches (not-taken and taken respectively). The branch prediction mechanism can predict two branches in a single cycle. <p> Nevertheless, an entry AaX identifies a single basic block except when Aa is an indirect branch or a return. In the case of returns, several branches could conflict for the same entry indexed by the return address as mentioned in <ref> [26] </ref>.
Reference: [27] <author> T. Yeh, </author> <title> Two-Level Adaptive Branch Prediction and Instruction Fetch Mechanisms for High Performance Superscalar Processors, </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor (MI) , 1993. INSTITUT DE RECHERCHE EN INFORMATIQUE DE TOULOUSE UPS, 118 route de Narbonne. </institution> <address> 31062 Toulouse Cedex, </address> <booktitle> France Tl. </booktitle> <volume> (33) 61 55 67 65 / fax. (33) 61 55 62 58 ENSEEIHT, </volume> <booktitle> 2 rue Camichel. 31071 Toulouse Cedex, France Tl. </booktitle> <volume> (33) 61 62 78 62 / fax. (33) 61 58 82 09 w :http://www.irit.fr/ </volume>
References-found: 27

