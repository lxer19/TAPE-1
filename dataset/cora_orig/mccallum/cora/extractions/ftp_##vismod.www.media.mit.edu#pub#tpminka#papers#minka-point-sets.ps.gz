URL: ftp://vismod.www.media.mit.edu/pub/tpminka/papers/minka-point-sets.ps.gz
Refering-URL: http://www.media.mit.edu/~tpminka/papers/learning.html
Root-URL: http://www.media.mit.edu
Title: Learning How to Learn is Learning with Point Sets  
Author: Thomas P. Minka Rosalind W. Picard 
Address: 20 Ames Street Cambridge, MA 02139  20 Ames Street Cambridge, MA 02139  
Affiliation: Vision and Modeling Group MIT Media Laboratory  Vision and Modeling Group MIT Media Laboratory  
Abstract: It has been proposed that learning how to learn be understood in terms of "learning a prior," from a Bayesian point-of-view (Baxter, 1996b). This paper presents an alternative interpretation: learning how to learn is ordinary learning but on point sets, rather than points. The idea behind learning how to learn is to partition the data, learn a model for the partitions, and then apply this model to new partitions. Ordinary learning methods do the same thing but with individual data points as the partitions. The partitioning for learning how to learn may be recovered automatically and may be applied recursively, leading to "task clustering" models. Virtually all existing approaches fit naturally into this unifying framework, including learning a distance metric and learning internal represen tations.
Abstract-found: 1
Intro-found: 1
Reference: <author> C. Bregler and S. M. Omohundro, </author> <title> "Surface learning with applications to lipreading," </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <pages> pp. 43-50, </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1994. </year>
Reference-contexts: A generalization of this is that we have multidimensional points, not necessarily defining a function, and want to complete a new, partially observed point <ref> (Bregler and Omohundro, 1994) </ref>. Furthermore, some points in the training set may themselves be partially observed (missing data) (Ghahramani and Jordan, 1994). Now consider generalizing all of this to the case where instead of IID points, we have IID sets of points.
Reference: <author> Z. Ghahramani and M. I. Jordan, </author> <title> "Supervised learning from incomplete data via an em approach," </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: A generalization of this is that we have multidimensional points, not necessarily defining a function, and want to complete a new, partially observed point (Bregler and Omohundro, 1994). Furthermore, some points in the training set may themselves be partially observed (missing data) <ref> (Ghahramani and Jordan, 1994) </ref>. Now consider generalizing all of this to the case where instead of IID points, we have IID sets of points. <p> This is good since it will interpolate the covariance matrix to unseen input locations, which the maximum likelihood solution would not. Thus the MLP is a direct competitor with maximum-likelihood techniques for fitting a covariance in the presence of missing data (e.g. <ref> (Ghahramani and Jordan, 1994) </ref>), which typically assume the covariance is finite. Figure 1 shows an example of using an MLP to fit a covariance. As discussed in the last section, the optimal distance metric for nearest-neighbor classification and regression is determined by the covariance function.
Reference: <author> J. Baxter, </author> <title> "A Bayesian/information theoretic model of bias learning," </title> <booktitle> in Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <pages> pp. 77-88, </pages> <month> July </month> <year> 1996. </year>
Reference: <author> S. Thrun, </author> <title> "Is learning the n-th thing any easier than learning the first?," </title> <booktitle> in Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference: <author> R. Caruana, </author> <title> "Learning many related tasks at the same time with backpropagation," </title> <booktitle> in Advances in Neural Information Processing Systems 7 MIT Press, </booktitle> <year> 1995. </year>
Reference: <author> S. M. Omohundro, </author> <title> "Family discovery," </title> <booktitle> in Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference: <author> C. K. I. Williams and C. E. Rasmussen, </author> <title> "Gaussian processes for regression," </title> <booktitle> in Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference: <author> D. J. C. MacKay, </author> <title> "Bayesian interpolation," </title> <journal> Neural Computation, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 415-447, </pages> <year> 1992. </year>
Reference-contexts: Sampling Sample w's from p (wjD) and only perform the integral over them (e.g. (Williams and Rasmussen, 1995)). Analytic Fit a parametric model, e.g. Gaussian, to p (wjD) and perform the inte gral analytically (e.g. <ref> (MacKay, 1992) </ref>). Now suppose D = ff 1 ::f n g contains IID functions, or more generally point sets, instead of IID points. The objective is to compute a density over f m given a partial point set f o = f nf m . <p> Here w plays the role of a prior on s j , hence learning w from D corresponds to "learning a prior" (Baxter, 1996b) (Omohundro, 1995). It also parallels the hierarchical Bayes approach in <ref> (MacKay, 1992) </ref>. The combined model can be expressed concisely by a tree-structured Bayesian network with w at the root, s j as its children, and the points in f j as the children of each s j .
Reference: <author> D. Heckerman, </author> <title> "A tutorial on learning with Bayesian networks," </title> <type> Tech. Rep. </type> <institution> MSR-TR-95-06, Microsoft Research, </institution> <year> 1996. </year>
Reference-contexts: Many Bayesian network algorithms, including exact algorithms, can be used to efficiently evaluate probabilities on this simple structure <ref> (Heckerman, 1996) </ref>. Alternatively, we can find the most probable values for w and s j and use the MAP approximation on all of the integrals. <p> The choice of distance metric for nearest-neighbor, the kernels in a radial basis function network, and the covariance of a Gaussian process can all be understood in this way. The situation is analogous to the use of "virtual examples" to represent conjugate priors in statistics <ref> (Heckerman, 1996) </ref>. Just as one can fit a conjugate prior to an existing prior, one can find the example tasks that underlie the hy-perparameter choices made in these learning models. An example is Baxter's proof that Euclidean distance implies a set of example tasks which are simple hyperplanes (Baxter, 1996a).
Reference: <author> J. B. Tenenbaum and W. T. Freeman, </author> <title> "Separating style and content," </title> <booktitle> in Advances in Neural Information Processing Systems 9, </booktitle> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: This means that we can skip the covariance estimation step and fit a linear combination of f j to f o (plus an offset for the mean), leading to a very simple and yet mathematically justified algorithm for learning from related tasks. The linear approach of <ref> (Tenenbaum and Freeman, 1997) </ref> can be understood in this way. 3 LEARNING HOW TO LEARN Two of the main topics in this literature have been (1) learning a distance metric and (2) learning several tasks at the same time.
Reference: <author> R. Caruana, </author> <title> "Algorithms and applications for multitask learning," </title> <booktitle> in Proc. 13th International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: Learning w from related tasks f j then corresponds to learning a metric. This can be done in several ways, say by feature selection or weighting features <ref> (Caruana, 1996) </ref>. However, Baxter (Baxter, 1996a) showed that the best metric, over the space of all metrics, is available in closed form given the distribution of functions f . <p> This similarity measure on point sets can be used in other ways. For example, when we just want the tasks relevant to f o to provide the prior for f m . This is the generalization of "local learning" to sets of points. The task weighting in <ref> (Caruana, 1996) </ref> could be done this way instead of searching over weights for all tasks. 6 CONCLUSION Virtually all of the techniques for "learning how to learn" presented in the neural network and machine learning literatures can be understood in terms of a generalization of points to point sets.
Reference: <author> J. Baxter, </author> <title> "The canonical distortion measure for vector quantization and approximation." </title> <note> http://keating.anu.edu.au/~jon/research.html, 1996. </note>
Reference: <author> J. Baxter, </author> <title> "Learning internal representations," </title> <booktitle> in Proc. 8th International Conference on Computational Learning Theory, </booktitle> <address> (Santa Cruz, CA), </address> <publisher> ACM Press, </publisher> <year> 1995. </year>
Reference-contexts: Of course, if the Gaussian process parameters are known exactly, then it is better to use the Gaussian estimator rather than nearest-neighbor. 3.2 MULTITASK LEARNING Several authors have proposed learning several tasks at the same time using one multilayer perceptron (MLP) (Caruana, 1994) <ref> (Baxter, 1995) </ref>. The tasks constrain each other since they share the same input-to-hidden weights. In this scenario, s j is the weights of the subnetwork devoted to task f j .
Reference: <author> M. I. Jordan and R. A. Jacobs, </author> <title> "Hierarchical mixtures of experts and the em algorithm," </title> <journal> Neural Computation, </journal> <volume> vol. 6, </volume> <pages> pp. 181-214, </pages> <year> 1994. </year>
Reference-contexts: A generalization of this is that we have multidimensional points, not necessarily defining a function, and want to complete a new, partially observed point (Bregler and Omohundro, 1994). Furthermore, some points in the training set may themselves be partially observed (missing data) <ref> (Ghahramani and Jordan, 1994) </ref>. Now consider generalizing all of this to the case where instead of IID points, we have IID sets of points. <p> This is good since it will interpolate the covariance matrix to unseen input locations, which the maximum likelihood solution would not. Thus the MLP is a direct competitor with maximum-likelihood techniques for fitting a covariance in the presence of missing data (e.g. <ref> (Ghahramani and Jordan, 1994) </ref>), which typically assume the covariance is finite. Figure 1 shows an example of using an MLP to fit a covariance. As discussed in the last section, the optimal distance metric for nearest-neighbor classification and regression is determined by the covariance function. <p> The idea is to introduce indicator variables z ij which, if all known, would render the partitions independent <ref> (Jordan and Jacobs, 1994) </ref>. When x i belongs to partition f j , z ij = 1.
Reference: <author> S. Thrun and J. O'Sullivan, </author> <title> "Discovering structure in multiple learning tasks: The TC algorithm," </title> <booktitle> in Proc. 13th International Conference on Machine Learning, </booktitle> <address> (Bari, Italy), </address> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
References-found: 15

