URL: ftp://ftp.cs.wisc.edu/computer-vision/iccv95-seitz.ps
Refering-URL: http://www.cs.wisc.edu/computer-vision/pubs.html
Root-URL: 
Email: seitz@cs.wisc.edu dyer@cs.wisc.edu  
Title: Complete Scene Structure from Four Point Correspondences  
Author: Steven M. Seitz Charles R. Dyer 
Date: 330-337.  
Note: In Proc. Fifth Intl. Conf. on Computer Vision, Cambridge MA, 1995, pp.  
Address: Madison, WI 53706  
Affiliation: Department of Computer Sciences University of Wisconsin  
Abstract: A new technique is presented for computing 3D scene structure from point and line features in monocular image sequences. Unlike previous methods, the technique guarantees the completeness of the recovered scene, ensuring that every scene feature that is detected in each image is reconstructed. The approach relies on the presence of four or more reference features whose correspondences are known in all the images. Under an orthographic or affine camera model, the parallax of the reference features provides constraints that simplify the recovery of the rest of the visible scene. An efficient recursive algorithm is described that uses a unified framework for point and line features. The algorithm integrates the tasks of feature correspondence and structure recovery, ensuring that all reconstructible features are tracked. In addition, the algorithm is immune to outliers and feature-drift, two weaknesses of existing structure-from-motion techniques. Experimental results are presented for real images. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. D. Lucas and T. Kanade, </author> <title> "An iterative image registration technique with an application to stereo vision," </title> <booktitle> in Proc. 7th International Joint Conference on Artificial Intelligence, </booktitle> <year> 1981. </year>
Reference-contexts: 1 Introduction Many existing structure-from-motion algorithms generate optimal structure estimates but typically reconstruct very little of the visible scene. This shortcoming is due primarily to the reliance on error-prone, nearest-neighbor-based, feature tracking methods <ref> [1, 2] </ref> that can reliably track only a subset of the visible image features. Since the set of trackable image features is generally a small subset of the detected image features, the 3D reconstruction is necessarily incomplete.
Reference: [2] <author> I. J. Cox, </author> <title> "A review of statistical data association techniques for motion correspondence," </title> <journal> Intl. Journal of Computer Vision, </journal> <volume> vol. 10, no. 1, </volume> <pages> pp. 53-66, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Many existing structure-from-motion algorithms generate optimal structure estimates but typically reconstruct very little of the visible scene. This shortcoming is due primarily to the reliance on error-prone, nearest-neighbor-based, feature tracking methods <ref> [1, 2] </ref> that can reliably track only a subset of the visible image features. Since the set of trackable image features is generally a small subset of the detected image features, the 3D reconstruction is necessarily incomplete. <p> Finally, the worst-case computational complexity of the algorithm is O (nm 3 ) for n images and m reconstructible scene features, in contrast to the exponential growth generally exhibited by complete algorithms <ref> [2] </ref>. The rest of the paper is organized as follows: Section 2 reviews related work on structure-from-motion and feature tracking. Section 3 discusses the constraints provided by a set of reference features, including the derivation of an affine reference frame and the determination of epipolar lines. <p> Their approach provided guarantees on the completeness of the recovered scene, but required continuous and controlled camera motion. There has been some work on feature-tracking using multiple hypotheses to generate and maintain different sets of possible feature correspondences <ref> [2] </ref>. Unfortunately, these algorithms have exponential complexity so suboptimal approximations are used in practice. Moreover, the strategies for hypothesis pruning are based on assumptions such as motion continuity that are often violated in practical applications.
Reference: [3] <author> C. Tomasi and T. Kanade, </author> <title> "Shape and motion from image streams under orthography: A factorization method," </title> <journal> Intl. Journal of Computer Vision, </journal> <volume> vol. 9, no. 2, </volume> <pages> pp. 137-154, </pages> <year> 1992. </year>
Reference-contexts: In particular, outlier detec-tion and removal occur naturally in the process of structure recovery, avoiding the need for special outlier detection algorithms. Outliers pose a significant problem for structure recovery algorithms based on least-squares techniques <ref> [3, 4] </ref> because a few bad features will bias the entire reconstruction. In our approach, each feature is recovered independently so outliers do not bias the recovery of other features. <p> Several methods have been proposed for determining these quantities from point correspondences [4, 6]. We use an adaptation of the method proposed by Tomasi and Kanade <ref> [3] </ref> for an orthographic camera and modified by Shapiro [4] for the affine case. This method was chosen because (1) more than four points may be used, and (2) it has been adapted for recursive estimation of the projection matrices [12] when the images are processed incrementally. <p> The original (non-recursive) method is reviewed briefly below. 3.1 Projection Matrices From four or more non-coplanar image feature correspondences it is possible to obtain the projection equations that map a point from an affine 3D space to each image. We use the factorization approach <ref> [3, 4] </ref> to determine the affine projection matrices. For simplicity, assume that the origin of each image I i is chosen to be the centroid of the k reference features. <p> As in [4], the projection matrices need not be orthogonal, in which case the reconstructed scene will be an affine transformation of the true scene. For further details of the algorithm, see <ref> [3, 4, 12] </ref>. It is useful to know the direction of the optical axis K i of I i , also known as the direction of projection for an orthographic camera.
Reference: [4] <author> L. Shapiro, A. Zisserman, and M. Brady, </author> <title> "Motion from point matches using affine epipolar geometry," </title> <booktitle> in Proc. Third European Conference on Computer Vision, </booktitle> <pages> pp. 73-84, </pages> <year> 1994. </year>
Reference-contexts: In particular, outlier detec-tion and removal occur naturally in the process of structure recovery, avoiding the need for special outlier detection algorithms. Outliers pose a significant problem for structure recovery algorithms based on least-squares techniques <ref> [3, 4] </ref> because a few bad features will bias the entire reconstruction. In our approach, each feature is recovered independently so outliers do not bias the recovery of other features. <p> Koenderink and van Doorn [6] showed that four points determine a global, object-centered, affine reference frame in which 3D structure information can be recovered. Several researchers have noted that epipolar lines can be determined from a number of feature correspondences in uncalibrated images <ref> [4, 7, 8, 9, 10] </ref>. The epipolar lines constrain the set of possible feature correspondences and can be ob tained from as few as four corresponding points under orthographic or affine projection. <p> The problem is that these epipolar lines are not generally known in advance so they are not used in feature tracking. Recent results <ref> [4, 9, 10] </ref>, however, have shown that epipolar lines can be determined from as few as four feature correspondences, without the need for camera calibration. <p> Our algorithm for recovering scene structure depends upon the acquisition of a set of affine projection matrices, i , that allow inverse projection of image features into a global 3D affine space. Several methods have been proposed for determining these quantities from point correspondences <ref> [4, 6] </ref>. We use an adaptation of the method proposed by Tomasi and Kanade [3] for an orthographic camera and modified by Shapiro [4] for the affine case. <p> Several methods have been proposed for determining these quantities from point correspondences [4, 6]. We use an adaptation of the method proposed by Tomasi and Kanade [3] for an orthographic camera and modified by Shapiro <ref> [4] </ref> for the affine case. This method was chosen because (1) more than four points may be used, and (2) it has been adapted for recursive estimation of the projection matrices [12] when the images are processed incrementally. <p> The original (non-recursive) method is reviewed briefly below. 3.1 Projection Matrices From four or more non-coplanar image feature correspondences it is possible to obtain the projection equations that map a point from an affine 3D space to each image. We use the factorization approach <ref> [3, 4] </ref> to determine the affine projection matrices. For simplicity, assume that the origin of each image I i is chosen to be the centroid of the k reference features. <p> The projection matrices 1 ; : : : ; n are the successive 2 fi 3 blocks of the first three columns of U. As in <ref> [4] </ref>, the projection matrices need not be orthogonal, in which case the reconstructed scene will be an affine transformation of the true scene. For further details of the algorithm, see [3, 4, 12]. <p> As in [4], the projection matrices need not be orthogonal, in which case the reconstructed scene will be an affine transformation of the true scene. For further details of the algorithm, see <ref> [3, 4, 12] </ref>. It is useful to know the direction of the optical axis K i of I i , also known as the direction of projection for an orthographic camera. <p> An implicit form 2 of L p can be found in terms of the affine fundamen-tal matrix <ref> [4] </ref>. We present a different derivation which involves aligning the affine coordinate system with I 1 . The advantage of the latter approach is that the epipo-lar geometry is given directly by the projection matrices. <p> In this respect, our approach is similar to methods based on the Kalman filter <ref> [4, 11] </ref> which is a popular tool for solving linear equations recursively. The primary advantage of our approach is the implicit formulation of structure which treats points and lines uniformly. Observe that each bin implicitly represents either a 3D point or a line.
Reference: [5] <author> K. N. Kutulakos and C. R. Dyer, </author> <title> "Global surface reconstruction by purposive control of observer motion," </title> <booktitle> in Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pp. 331-338, </pages> <year> 1994. </year>
Reference-contexts: The complete algorithm is presented in Section 6, and Section 7 presents experimental results on real images. 2 Related Work Despite its importance, the completeness problem has been neglected in the structure-from-motion literature, although a few vision researchers have considered completeness in related problems. Kutulakos and Dyer <ref> [5] </ref> introduced a provable algorithm for global surface reconstruction using an active observer. Their approach provided guarantees on the completeness of the recovered scene, but required continuous and controlled camera motion.
Reference: [6] <author> J. J. Koenderink and A. J. van Doorn, </author> <title> "Affine structure from motion," </title> <journal> Opt. Soc. Am. A, </journal> <volume> vol. 8, </volume> <pages> pp. 377-385, </pages> <year> 1991. </year>
Reference-contexts: Moreover, the strategies for hypothesis pruning are based on assumptions such as motion continuity that are often violated in practical applications. Other researchers have noted that the image motion of a few reference features provides useful geometric constraints. Koenderink and van Doorn <ref> [6] </ref> showed that four points determine a global, object-centered, affine reference frame in which 3D structure information can be recovered. Several researchers have noted that epipolar lines can be determined from a number of feature correspondences in uncalibrated images [4, 7, 8, 9, 10]. <p> Our algorithm for recovering scene structure depends upon the acquisition of a set of affine projection matrices, i , that allow inverse projection of image features into a global 3D affine space. Several methods have been proposed for determining these quantities from point correspondences <ref> [4, 6] </ref>. We use an adaptation of the method proposed by Tomasi and Kanade [3] for an orthographic camera and modified by Shapiro [4] for the affine case.
Reference: [7] <author> H. C. Longuet-Higgins, </author> <title> "A computer algorithm for reconstructing a scene from two projections," </title> <journal> Nature, </journal> <volume> vol. 293, </volume> <pages> pp. 133-135, </pages> <year> 1981. </year>
Reference-contexts: Koenderink and van Doorn [6] showed that four points determine a global, object-centered, affine reference frame in which 3D structure information can be recovered. Several researchers have noted that epipolar lines can be determined from a number of feature correspondences in uncalibrated images <ref> [4, 7, 8, 9, 10] </ref>. The epipolar lines constrain the set of possible feature correspondences and can be ob tained from as few as four corresponding points under orthographic or affine projection.
Reference: [8] <author> O. D. Faugeras, Q.-T. Luong, and S. J. Maybank, </author> <title> "Camera self-calibration: Theory and experiments," </title> <booktitle> in Proc. European Conference on Computer Vision, </booktitle> <pages> pp. 321-334, </pages> <year> 1992. </year>
Reference-contexts: Koenderink and van Doorn [6] showed that four points determine a global, object-centered, affine reference frame in which 3D structure information can be recovered. Several researchers have noted that epipolar lines can be determined from a number of feature correspondences in uncalibrated images <ref> [4, 7, 8, 9, 10] </ref>. The epipolar lines constrain the set of possible feature correspondences and can be ob tained from as few as four corresponding points under orthographic or affine projection.
Reference: [9] <author> C.-H. Lee and T. Huang, </author> <title> "Finding point correspondences and determining motion of a rigid object from two weak perspective views," Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 52, </volume> <pages> pp. 309-327, </pages> <year> 1990. </year>
Reference-contexts: Koenderink and van Doorn [6] showed that four points determine a global, object-centered, affine reference frame in which 3D structure information can be recovered. Several researchers have noted that epipolar lines can be determined from a number of feature correspondences in uncalibrated images <ref> [4, 7, 8, 9, 10] </ref>. The epipolar lines constrain the set of possible feature correspondences and can be ob tained from as few as four corresponding points under orthographic or affine projection. <p> The problem is that these epipolar lines are not generally known in advance so they are not used in feature tracking. Recent results <ref> [4, 9, 10] </ref>, however, have shown that epipolar lines can be determined from as few as four feature correspondences, without the need for camera calibration.
Reference: [10] <author> R. Basri, </author> <title> "On the uniqueness of correspondence under orthographic and perspective projections," </title> <booktitle> in Proc. Image Understanding Workshop, </booktitle> <pages> pp. 875-884, </pages> <year> 1992. </year>
Reference-contexts: Koenderink and van Doorn [6] showed that four points determine a global, object-centered, affine reference frame in which 3D structure information can be recovered. Several researchers have noted that epipolar lines can be determined from a number of feature correspondences in uncalibrated images <ref> [4, 7, 8, 9, 10] </ref>. The epipolar lines constrain the set of possible feature correspondences and can be ob tained from as few as four corresponding points under orthographic or affine projection. <p> The epipolar lines constrain the set of possible feature correspondences and can be ob tained from as few as four corresponding points under orthographic or affine projection. Other work has shown that additional images further constrain correspondences and this property has been used in trinoc-ular stereopsis <ref> [10, 11] </ref>. 3 Constraints from Four Features Image motion of a rigid scene is known to be highly constrained; the projections of any scene feature are limited to a set of epipolar lines. <p> The problem is that these epipolar lines are not generally known in advance so they are not used in feature tracking. Recent results <ref> [4, 9, 10] </ref>, however, have shown that epipolar lines can be determined from as few as four feature correspondences, without the need for camera calibration.
Reference: [11] <author> N. Ayache and F. Lustman, </author> <title> "Trinocular stereo vision for robotics," </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 13, no. 1, </volume> <pages> pp. 73-85, </pages> <year> 1991. </year>
Reference-contexts: The epipolar lines constrain the set of possible feature correspondences and can be ob tained from as few as four corresponding points under orthographic or affine projection. Other work has shown that additional images further constrain correspondences and this property has been used in trinoc-ular stereopsis <ref> [10, 11] </ref>. 3 Constraints from Four Features Image motion of a rigid scene is known to be highly constrained; the projections of any scene feature are limited to a set of epipolar lines. <p> In this respect, our approach is similar to methods based on the Kalman filter <ref> [4, 11] </ref> which is a popular tool for solving linear equations recursively. The primary advantage of our approach is the implicit formulation of structure which treats points and lines uniformly. Observe that each bin implicitly represents either a 3D point or a line. <p> In contrast, previous approaches based on the Kalman filter employed an explicit description of the degrees of freedom of the reconstructible sub-space, which differs for points and lines. In particular, points require a three parameter representational space and lines require a minimum of four parameters <ref> [11] </ref>. In contrast, an implicit formulation permits reconstruction of both points and lines within a common three-dimensional parameter space. <p> The task is ameliorated by appropriately transforming the images or edge maps so that epipolar lines are specially aligned. This technique, known as image rectification, has been previously applied to perspective imagery to simplify matching for binocular and trinocular stereo <ref> [11] </ref>. For perspective cameras in general position, image-rectification involves a non-linear image transform 1 . Under an affine or orthographic projection model, all epipolar lines in each image are parallel so the rectification process is simplified considerably.
Reference: [12] <author> T. Morita and T. Kanade, </author> <title> "A sequential factorization method for recovering shape and motion from image streams," </title> <type> Tech. Rep. </type> <institution> CMU-CS-94-158, Carnegie Mel-lon University, </institution> <address> Pittsburgh, PA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: This method was chosen because (1) more than four points may be used, and (2) it has been adapted for recursive estimation of the projection matrices <ref> [12] </ref> when the images are processed incrementally. The original (non-recursive) method is reviewed briefly below. 3.1 Projection Matrices From four or more non-coplanar image feature correspondences it is possible to obtain the projection equations that map a point from an affine 3D space to each image. <p> As in [4], the projection matrices need not be orthogonal, in which case the reconstructed scene will be an affine transformation of the true scene. For further details of the algorithm, see <ref> [3, 4, 12] </ref>. It is useful to know the direction of the optical axis K i of I i , also known as the direction of projection for an orthographic camera.
Reference: [13] <author> G. Strang, </author> <title> Linear Algebra and its Applications. </title> <address> San Diego, CA: </address> <publisher> Harcourt Brace Jovanovich Inc., </publisher> <year> 1988. </year>
Reference-contexts: If W is the identity, Eq. (4) gives the orthogonal squared distance from the given subspace <ref> [13] </ref>. The weight matrix can be determined from the measurement covariance as follows: let fl be the covariance of an image feature p in I i .
Reference: [14] <author> G. Borgefors, </author> <title> "Distance transformations in arbitrary dimensions," Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 27, </volume> <pages> pp. 321-345, </pages> <year> 1984. </year> <month> 8 </month>
Reference-contexts: The interval table is computed as follows: the set of features that are within a tolerated vertical distance of each scanline is found using a distance transform <ref> [14] </ref>. For each such feature, an interval is created for the region of the scanline within tolerance of that feature. A table is constructed that maps each scanline to the list of its intervals that are sufficiently close to an image feature.
References-found: 14

