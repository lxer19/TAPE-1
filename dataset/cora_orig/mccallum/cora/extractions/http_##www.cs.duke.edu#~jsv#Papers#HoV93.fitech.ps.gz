URL: http://www.cs.duke.edu/~jsv/Papers/HoV93.fitech.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node37.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Fast and Efficient Lossless Image Compression  
Author: Paul G. Howard and Jeffrey Scott Vitter 
Note: appears in the proceedings of the IEEE Computer Society/NASA/CESDIS Data Compression Conference, Snowbird, Utah, March 30-April 1, 1993, pages 351-360.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. Elias, </author> <title> "Universal Codeword Sets and Representations of Integers," </title> <journal> IEEE Trans. Inform. Theory IT-21 (Mar. </journal> <year> 1975), </year> <pages> 194-203. </pages>
Reference-contexts: The idea of using a simple code in conjunction with ordered distributions is similar in spirit to the universal coding methods developed by Elias <ref> [1] </ref> and Rissanen [12], although their work involves finding a single universal code, not a family. Rissanen [13] presents a parameterized method for finite alphabets, the parameter being the reciprocal of the most probable event. The second extension is to more general parameter estimation.
Reference: [2] <author> N. Faller, </author> <title> "An Adaptive System for Data Compression," </title> <booktitle> Record of the 7th Asilo-mar Conference on Circuits, Systems, and Computers, </booktitle> <year> 1973. </year>
Reference: [3] <author> R. G. Gallager, </author> <title> "Variations on a Theme by Huffman," </title> <journal> IEEE Trans. Inform. Theory IT-24 (Nov. </journal> <year> 1978), </year> <pages> 668-674. </pages>
Reference: [4] <author> R. G. Gallager & D. C. Van Voorhis, </author> <title> "Optimal Source Codes for Geometrically Distributed Integer Alphabets," </title> <journal> IEEE Trans. Inform. Theory IT-21 (Mar. </journal> <year> 1975), </year> <pages> 228-230. </pages>
Reference-contexts: It can be shown 3 that the correct choice of the parameter m produces an optimal prefix code for a given exponential distribution <ref> [4] </ref>. Rice [10] independently discovered the special case of Golomb codes where m = 2 k for some integer k. Restricting m to be a power of 2 leads to exceptionally simple codes.
Reference: [5] <author> S. W. Golomb, </author> <title> "Run-Length Encodings," </title> <journal> IEEE Trans. Inform. Theory IT-12 (July 1966), </journal> <pages> 399-401. </pages>
Reference-contexts: annoying problem present in many image compression schemes of having to assign different length codewords to values with theoretically equal probabilities. 2.3 Exponential prefix codes (Golomb and Rice codes) When the distribution of values to be encoded is exponential, we can use an exponential prefix code, first discussed by Golomb <ref> [5] </ref>. The codes in this family are parameterized by a positive integer parameter m. Given the value of m, we encode non-negative integer n by encoding bn=mc in unary, then encoding n mod m using an adjusted binary code for the range [0; m 1], as in Section 2.2.
Reference: [6] <author> P. G. Howard & J. S. Vitter, </author> <title> "New Methods for Lossless Image Compression Using Arithmetic Coding," </title> <booktitle> Information Processing and Management 28 (1992), </booktitle> <pages> 765-779. </pages>
Reference-contexts: 1 Introduction Most lossless image compression methods in the literature consist of four main components <ref> [6] </ref>: a selector to choose the next pixel to be encoded, a predictor to estimate the intensity of the pixel, an error modeler to estimate the distribution of the prediction error, and a statistical coder to code the prediction error using the error distribution.
Reference: [7] <author> P. G. Howard & J. S. Vitter, </author> <title> "Error Modeling for Hierarchical Lossless Image Compression," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <pages> 24-26, </pages> <year> 1992, </year> <pages> 269-278. </pages>
Reference-contexts: This distribution clearly differs from the Laplace distribution commonly assumed in predictive image coding, but it is consistent with the error modeling treatment that we presented in <ref> [7] </ref>. 2.2 Adjusted binary codes To encode an in-range pixel value P , we must encode P L in the range [0; ].
Reference: [8] <author> P. G. Howard & J. S. Vitter, </author> <title> "Design and Analysis of Fast Text Compression Based on Quasi-Arithmetic Coding," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <journal> 30-Apr. </journal> <volume> 1, </volume> <year> 1993, </year> <pages> 98-107. </pages>
Reference-contexts: The lists and cumulative counts can be maintained for each of a number of contexts. Here the contexts are the intensity ranges, but we have also successfully applied the method to text compression using groups of preceding symbols as the contexts <ref> [8] </ref>. Golomb-Rice coding gives the fast, flexible modeling obtained with arithmetic coding without the time-consuming arithmetic. It gives faster coding even than Huffman coding because of the especially simple prefix codes involved, and adaptive modeling is possible without the complicated data structure manipulations required in dynamic Huffman coding [2,3,9,15,16].
Reference: [9] <author> D. E. Knuth, </author> <title> "Dynamic Huffman Coding," </title> <journal> J. </journal> <note> Algorithms 6 (June 1985), 163-180. </note>
Reference: [10] <author> R. F. Rice, </author> <title> "Some Practical Universal Noiseless Coding Techniques," </title> <institution> Jet Propulsion Laboratory, JPL Publication 79-22, Pasadena, California, </institution> <month> Mar. </month> <year> 1979. </year>
Reference-contexts: It can be shown 3 that the correct choice of the parameter m produces an optimal prefix code for a given exponential distribution [4]. Rice <ref> [10] </ref> independently discovered the special case of Golomb codes where m = 2 k for some integer k. Restricting m to be a power of 2 leads to exceptionally simple codes.
Reference: [11] <author> R. F. Rice, </author> <title> "Some Practical Universal Noiseless Coding Techniques|Part III, Module PSI14,K+," </title> <institution> Jet Propulsion Laboratory, JPL Publication 91-3, Pasadena, California, </institution> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: In this section we describe an on-line algorithm for parameter estimation and prove a bound on its effectiveness. 3.1 Selection algorithm A common method of obtaining and transmitting coding parameters in similar algorithms <ref> [11] </ref> is to divide the image into blocks and compute the code lengths that would be obtained using each of a set of reasonable parameter values.
Reference: [12] <author> J. Rissanen, </author> <title> "A Universal Prior for Integers and Estimation by Minimum Description Length," </title> <journal> Ann. Statist. </journal> <volume> 11 (1983), </volume> <pages> 416-432. </pages>
Reference-contexts: The idea of using a simple code in conjunction with ordered distributions is similar in spirit to the universal coding methods developed by Elias [1] and Rissanen <ref> [12] </ref>, although their work involves finding a single universal code, not a family. Rissanen [13] presents a parameterized method for finite alphabets, the parameter being the reciprocal of the most probable event. The second extension is to more general parameter estimation.
Reference: [13] <author> J. Rissanen, </author> <title> "Minimax Codes for Finite Alphabets," </title> <journal> IEEE Trans. Inform. </journal> <note> Theory IT-24 (May 1978), 389-392. </note>
Reference-contexts: The idea of using a simple code in conjunction with ordered distributions is similar in spirit to the universal coding methods developed by Elias [1] and Rissanen [12], although their work involves finding a single universal code, not a family. Rissanen <ref> [13] </ref> presents a parameterized method for finite alphabets, the parameter being the reciprocal of the most probable event. The second extension is to more general parameter estimation.
Reference: [14] <author> J. Venbrux, N. Liu, K. Liu, P. Vincent & R. Merrell, </author> <title> "A Very High Speed Lossless Compression/Decompression Chip Set," </title> <institution> Jet Propulsion Laboratory, JPL Publication 91-13, Pasadena, California, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Then we send the k least significant bits directly. Rice coding has been used as the basis for a lossless hardware compressor <ref> [14] </ref>. Its compression effectiveness is analyzed in [17]. Both Golomb coding and Rice coding require estimation of the coding parameter. We present an effective technique in Section 3. Examples of some Golomb and Rice codes are shown in Table 1.
Reference: [15] <author> J. S. Vitter, </author> <title> "Dynamic Huffman Coding," </title> <journal> ACM Trans. Math. </journal> <note> Software 15 (June 1989), 158-167, also appears as Algorithm 673, Collected Algorithms of ACM, </note> <year> 1989. </year>
Reference: [16] <author> J. S. Vitter, </author> <title> "Design and Analysis of Dynamic Huffman Codes," </title> <journal> Journal of the ACM 34 (Oct. </journal> <year> 1987), </year> <pages> 825-845. </pages>
Reference: [17] <author> P.-S. Yeh, R. F. Rice & W. Miller, </author> <title> "On the Optimality of Code Options for a Universal Noiseless Coder," </title> <institution> Jet Propulsion Laboratory, JPL Publication 91-2, Pasadena, California, </institution> <month> Feb. </month> <year> 1991. </year> <month> 12 </month>
Reference-contexts: Then we send the k least significant bits directly. Rice coding has been used as the basis for a lossless hardware compressor [14]. Its compression effectiveness is analyzed in <ref> [17] </ref>. Both Golomb coding and Rice coding require estimation of the coding parameter. We present an effective technique in Section 3. Examples of some Golomb and Rice codes are shown in Table 1. Golomb codes give slightly better compression by providing finer control in choosing the model.
References-found: 17

