URL: http://www.cs.indiana.edu/pub/dswise/PPoPP97.mm.ps.Z
Refering-URL: http://www.cs.indiana.edu/pub/dswise/
Root-URL: http://www.cs.indiana.edu
Email: jfrens, dswise@cs.indiana.edu  
Title: Auto-Blocking Matrix-Multiplication or Tracking BLAS3 Performance from Source Code  
Author: Jeremy D. Frens and David S. Wise 
Keyword: Categories and subject descriptors: G.1.3 [Numerical Analysis]: Numerical Linear Algebralinear systems; E.1 [Data Structures]: Arrays; D.4.2 [Operating Systems]: Storage Managementsegmentation, swapping, virtual memory; B.3.2 [Memory Structures]: Design Stylesprimary memory; F.2.1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problemscomputations on matrices; G.4 [Mathematical Software]: Algorithm analysis. General Term: Performance. Additional Key Words and Phrases: storage management, indexing, quadtrees, swapping, cache misses, paging.  
Web: CDA9303189.  
Note: Supported, in part, by the National Science Foundation under a grant numbered  
Address: Bloomington, Indiana 474054101, USA  
Affiliation: Computer Science Dept., Indiana University  
Abstract: An elementary, machine-independent, recursive algorithm for matrix multiplication C+=A*B provides implicit blocking at every level of the memory hierarchy and tests out faster than classically optimal code, tracking hand-coded BLAS3 routines. Proof of concept is demonstrated by racing the in-place algorithm against manufacturer's hand-tuned BLAS3 routines; it can win. The recursive code bifurcates naturally at the top level into independent block-oriented processes, that each writes to a disjoint and contiguous region of memory. Experience has shown that the indexing vastly improves the patterns of memory access at all levels of the memory hierarchy, independently of the sizes of caches or pages and without ad hoc programming. It also exposed a weakness in SGI's C compilers that merrily unroll loops for the superscalar R8000 processor, but do not analogously unfold the base cases of the most elementary recursions. Such deficiencies might deter future programmers from using this rich class of recursive algorithms. c fl1997 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Publications Dept, ACM Inc., fax +1 (212) 869-0481, or permissions@acm.org. Proc. 1997 ACM Symp. on Principles and Practice of Parallel Programming, SIGPLAN Notices 32, 7 (July 1997) 206216. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. M. Anderson, S. P. Amarainghe, & M. S. Lam. </author> <title> Data and computation transformations for multiprocessors. </title> <booktitle> Proc. 5th ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, SIGPLAN Notices 30, </booktitle> <month> 8 (August </month> <year> 1995), </year> <pages> 166 178. </pages>
Reference-contexts: The latter strategy uses well the compiler's knowledge of the hardware parameters to fit the target machine, but is also limited to patterns of source code and transformations anticipated by the compiler writer. Recent work of this sort includes data transformations between row- and column-major <ref> [1, 6] </ref>, blocking of high-level code specifically to admit lower-level BLAS3 invocations [2], and more recently such blocking to bypass BLAS3 entirely [5]. One could interpret our representations and algorithms as a new way to introduce blocking into existing programs.
Reference: [2] <author> R. Bordawekar, A. Choudhary, K. Kennedy, C. Koelbel, & M. Paleczny. </author> <title> A model and compilation strategy for out-of-core data parallel programs. </title> <booktitle> Proc. 5th ACM SIGPLAN Symp. on Principles and Practice of Parallel Programming, SIGPLAN Notices 30, </booktitle> <month> 8 (August </month> <year> 1995), </year> <month> 117. </month>
Reference-contexts: Recent work of this sort includes data transformations between row- and column-major [1, 6], blocking of high-level code specifically to admit lower-level BLAS3 invocations <ref> [2] </ref>, and more recently such blocking to bypass BLAS3 entirely [5]. One could interpret our representations and algorithms as a new way to introduce blocking into existing programs. We prefer to view them, instead, as results from a different style of expressing the same high-level algorithms.
Reference: [3] <author> F. W. Burton & J. G. Kollias. </author> <title> Comment on `The explicit quad tree as a structure for computer graphics.' </title> <journal> Comput. J. </journal> <volume> 26, </volume> <month> 2 (May </month> <year> 1983), </year> <month> 188. </month>
Reference-contexts: Ignoring the offset, this definition provides a zero-based, level-order indexing 1 across a matrix's tree [18, p. 350, 401], as illustrated in Figure 4 <ref> [7, 3] </ref>. It prompts several observations. * Every block/subtree is indexed consecutively at each of its levels. The scalars in any subblock, as terminal nodes at the same level in some subtree, are therefore indexed consecutively.
Reference: [4] <author> D. Cann. </author> <title> Retire FORTRAN? : a debate rekindled. </title> <journal> Comm. ACM 35, </journal> <month> 8 (August </month> <year> 1992), </year> <month> 8189. </month>
Reference-contexts: Viewing the matrix as decomposed by rows or by columns is a bottom-up approach; sub-spaces may still be visible, but they assemble themselves differently. Our philosophy derives from our experience with functional programming and our view of its critical role for parallel programming <ref> [4] </ref>. Linear systems, in particular, come to us with a rich algebra that is best visible in a functional program that reads like interdependent formulae [13, 24]. Discovery of good formulations of knownand maybe even unknowntechniques follows top-down, partition-and-conquer of the underlying vector space.
Reference: [5] <author> S. Carr & R. B. Lehoucq. </author> <title> Compiler Blockability of dense matrix factorizations. </title> <journal> ACM Trans. </journal> <note> Math.Software (to appear). </note>
Reference-contexts: Recent work of this sort includes data transformations between row- and column-major [1, 6], blocking of high-level code specifically to admit lower-level BLAS3 invocations [2], and more recently such blocking to bypass BLAS3 entirely <ref> [5] </ref>. One could interpret our representations and algorithms as a new way to introduce blocking into existing programs. We prefer to view them, instead, as results from a different style of expressing the same high-level algorithms.
Reference: [6] <author> M. Cierniak & W. Li. </author> <title> Unifying data and control transformations for distributed shared-memory machines. </title> <booktitle> Proc. ACM SIGPLAN '95 Conf. on Programming Lang. Design and Implementation, SIGPLAN Notices 30, </booktitle> <month> 6 (June </month> <year> 1995), 205217. </year>
Reference-contexts: The latter strategy uses well the compiler's knowledge of the hardware parameters to fit the target machine, but is also limited to patterns of source code and transformations anticipated by the compiler writer. Recent work of this sort includes data transformations between row- and column-major <ref> [1, 6] </ref>, blocking of high-level code specifically to admit lower-level BLAS3 invocations [2], and more recently such blocking to bypass BLAS3 entirely [5]. One could interpret our representations and algorithms as a new way to introduce blocking into existing programs.
Reference: [7] <author> J. Cohen & M. Roth. </author> <title> On the implementation of Strassen's fast multiplication algorithm. </title> <journal> Acta Informat. </journal> <volume> 6, </volume> <month> 4 (August </month> <year> 1976), </year> <month> 341355. </month>
Reference-contexts: Ignoring the offset, this definition provides a zero-based, level-order indexing 1 across a matrix's tree [18, p. 350, 401], as illustrated in Figure 4 <ref> [7, 3] </ref>. It prompts several observations. * Every block/subtree is indexed consecutively at each of its levels. The scalars in any subblock, as terminal nodes at the same level in some subtree, are therefore indexed consecutively.
Reference: [8] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, & T. von Eicken. </author> <title> LogP: a practical model of parallel computation. </title> <journal> Comm. ACM 39, </journal> <month> 11 (November </month> <year> 1996), </year> <month> 7885. </month>
Reference: [9] <author> J. W. Demmel & N. J. Higham. </author> <title> Stability of block algorithms with fast Level 3 BLAS. </title> <journal> ACM Trans. Math. </journal> <volume> Software 18, </volume> <month> 3 (September </month> <year> 1992), </year> <month> 274291. </month>
Reference-contexts: Moreover, they demonstrate a new way to balance the parallel schedules top-down, following the recursion and partitioning the matrices for favorable run-time locality. It is well known that linear systems are best solved by algorithms that decompose matrices into blocks <ref> [9, 10] </ref>. This paper focuses on an elementary, machine-independent, recursive algorithm for matrix multiplication C+=A*B that provides implicit blocking at every level of the memory hierarchy and tests out faster than classically optimal code.
Reference: [10] <author> J. Dongarra, J. DuCroz, S. Hammarling, & R. Hanson. </author> <title> An extended set of FORTRAN basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Softw. </journal> <volume> 14 (1988), </volume> <pages> 117. </pages>
Reference-contexts: 1 Introduction This paper revisits matrix algebra, specifically multiplication, to explore an algorithm nearly as simple as traditional ones, and certainly better for hierarchical memory. We present a simple recursive algorithm and a matrix representation suited to it that have outperformed hand-optimized BLAS3 matrix multiplication <ref> [10] </ref>. Hand-coded by the manufacturer for salesmen's performance claims, this BLAS3 multiplication is widely thought to offer optimal performance. Experiments with the algorithm have exposed weaknesses in production compilers: loops are unrolled but recursions are not unfolded. <p> Moreover, they demonstrate a new way to balance the parallel schedules top-down, following the recursion and partitioning the matrices for favorable run-time locality. It is well known that linear systems are best solved by algorithms that decompose matrices into blocks <ref> [9, 10] </ref>. This paper focuses on an elementary, machine-independent, recursive algorithm for matrix multiplication C+=A*B that provides implicit blocking at every level of the memory hierarchy and tests out faster than classically optimal code.
Reference: [11] <author> R. J. Fateman. </author> <title> Symbolic mathematics system evaluators (extended abstract). </title> <editor> In Y. N. Lakshman (ed.), </editor> <booktitle> Proc 1996 Intl. Symp. on Symbolic and Algebraic Computation, </booktitle> <address> New York, </address> <publisher> ACM Press, </publisher> <pages> 8694. </pages>
Reference-contexts: Importantly, no conversions between data representations are necessary there (aside from the usual ones at input and output.) Fateman's recent treatise that, in part, distinguishes matrices from arrays <ref> [11, x4:3] </ref> is relevant here. Matrix problems deal with underlying vector spaces that are better decomposed top-down into subspaces. While quadtree decomposition may not be the most efficient one for any given problem, it does enforce the divide-and-conquer perspective that algebra allows us.
Reference: [12] <author> P. C. Fischer & R. L. Probert. </author> <title> Storage reorganization techniques for matrix computation in a paging environment. </title> <journal> Comm. ACM 22, </journal> <month> 7 (July </month> <year> 1979), </year> <month> 405415. </month>
Reference-contexts: The anomaly is invisible in Figure 11. 6 Analysis This analysis explainsas supported by experiments over the years <ref> [19, 12] </ref>that matrices should be partitioned into square blocks, independently of the size and shape of the problem, for swapping across boundaries in a layered memory.
Reference: [13] <author> J. Frens & D. S. Wise. </author> <title> Matrix inversion using quadtrees implemented in GOFER. </title> <type> Technical Report 433, </type> <institution> Computer Science Dept., </institution> <note> Indiana University (May 1995). </note>
Reference-contexts: We prefer to view them, instead, as results from a different style of expressing the same high-level algorithms. The style is, however, one that also inspires a new perspective, new insights, andperhaps alsonew algorithms. Recursive and parallel versions of practical algorithms already exist that use this structure well <ref> [24, 13] </ref>; in such contexts this matrix multiplication is not only the most natural, it is also the fastest. <p> Our philosophy derives from our experience with functional programming and our view of its critical role for parallel programming [4]. Linear systems, in particular, come to us with a rich algebra that is best visible in a functional program that reads like interdependent formulae <ref> [13, 24] </ref>. Discovery of good formulations of knownand maybe even unknowntechniques follows top-down, partition-and-conquer of the underlying vector space. Our experience is that many insights and efficiencies are to be found here.
Reference: [14] <author> K. A. Gallivan, R. J. Plemmons, & A. H. Sameh. </author> <title> Parallel Algorithms for dense linear algebra. </title> <note> SIAM Review 32, </note> <month> 1 (March </month> <year> 1990), </year> <note> 54135. Reprinted in Gallivan et al. Parallel Algorithms for Matrix Computation, Philadelphia, SIAM (1990), 182. </note>
Reference-contexts: The benchmark code for matrix-matrix multiplication, C+=A*B, is the conventional inner-product code of Figure 1, as presented in most linear algebra courses. Following the associativity of addition, the three nested-loop controls can be permuted <ref> [14, p. 19] </ref> and reordered to obtain outer- and middle-product alternatives. <p> Figure 1), it is well known that only two cache misses are necessary between the innermost steps. Optimal block sizes seem to depend on all of l; m; n; p; q; r; we seek good values for the last three, independent of the first three. The six familiar algorithms <ref> [14, p. 19] </ref> for the product (two using inner, two middle, and two outer-products), correspond to each of the six permutations of the three for-loops in Figure 1.
Reference: [15] <author> G. H. Golub & C. F. Van Loan. </author> <title> Matrix Computations 2nd edition. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore (1989). </address>
Reference-contexts: If a third or fourth processor were available, then it would be committed in another bifurcation at the next level of the quadtree. If we would allow temporary storage to the algorithm, then this is the case where Strassen's recurrence <ref> [21, 15, x1.3.8] </ref> applies best (except on sparse or tiny matrices).
Reference: [16] <editor> J. Fasel, P. Hudak, S. Peyton Jones, & P. Wadler (eds.) </editor> <title> HASKELL special issue. </title> <journal> ACM SIGPLAN Notices 27, </journal> <month> 5 (May </month> <year> 1992). </year>
Reference-contexts: This paper represents our effort to carry this philosophy back to serially addressed, hierarchical memory using concise and efficient source code. The C programs here should be read, first, as an idealized compilation of functional source code (like a multiplication of HASKELL arrays <ref> [16] </ref>) to run well on extant high-performance systems. 1.2 Outline of paper. The remainder of this paper is in six parts.
Reference: [17] <author> N. J. Higham. </author> <title> Exploiting fast matrix multiplication within the Level 3 BLAS. </title> <journal> ACM Trans. Math. </journal> <volume> Software 16, </volume> <month> 4 (December </month> <year> 1990), </year> <month> 352368. </month>
Reference-contexts: These two insightscode dovetailed for cache reuse and unfolding of its base cases for superscalar architecturesalone account for almost a four-fold improvement in running times during the course of our development. We have not tested the attractive hybrid composed of Strassen's recurrence and this one; stability <ref> [17] </ref> and in-place constraints that are already satisfied here would have to be relaxed there. However, for the balanced, dense matrix (Case 0) Strassen's original 18-addition presentation also bifurcates nicely into 3-and-3 parallel block multiplications, followed by 1 serially.
Reference: [18] <author> D. E. Knuth. </author> <title> The Art of Computer Programming I, Fundamental Algorithms (2nd ed.), </title> <address> Reading, MA, </address> <publisher> Addison-Wesley, </publisher> <year> (1973). </year>
Reference-contexts: Ignoring the offset, this definition provides a zero-based, level-order indexing 1 across a matrix's tree <ref> [18, p. 350, 401] </ref>, as illustrated in Figure 4 [7, 3]. It prompts several observations. * Every block/subtree is indexed consecutively at each of its levels. The scalars in any subblock, as terminal nodes at the same level in some subtree, are therefore indexed consecutively.
Reference: [19] <author> A. C. McKellar & E. G. Coffman, Jr. </author> <title> Organizing matrices and matrix operations for paged-memory systems Comm. </title> <booktitle> ACM 12, </booktitle> <month> 3 (March </month> <year> 1969), </year> <month> 153165. </month>
Reference-contexts: The anomaly is invisible in Figure 11. 6 Analysis This analysis explainsas supported by experiments over the years <ref> [19, 12] </ref>that matrices should be partitioned into square blocks, independently of the size and shape of the problem, for swapping across boundaries in a layered memory.
Reference: [20] <author> J. Spiess. </author> <title> Untersuchungen des Zeitgewinns durch neue Algo-rithmen zur Matrix-Multiplikation. </title> <booktitle> Computing 17, 1 (1976), </booktitle> <pages> 2336. </pages>
Reference-contexts: That suggests a hybrid scheme in which our indexing and recursion is used at the highest, unbalanced levels (Cases 17) and Strassen's is used on the intermediate, balanced subproblems. Ours again becomes appropriate for the tiny blocks <ref> [20] </ref> that fit in cache. Figure 4's indexing, of course, is used throughout any recurrenceof these two or of other algorithms. REFERENCES 11 The underlying matrix representation is critical to these results.
Reference: [21] <author> V. Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numer. Math. </journal> <volume> 13 (1969), </volume> <pages> 354356. </pages>
Reference-contexts: It is not too different from Figure 5 which exposes the blockwise algorithm, but obfuscates both the functional syntax and the sparse-matrix algebra that motivated this representation. As observed elsewhere <ref> [23, 21] </ref>, the quadrants of the answer can provide a partitioning into 4, 16, 64, . . . processes to compute the answer independently of one another. 4 RECURSIVE MATRIX MULTIPLICATION 4 4.1 Balanced parallel multiplication Matrix multiplication can be balanced across a small pool of processors top-down, even at compile <p> If a third or fourth processor were available, then it would be committed in another bifurcation at the next level of the quadtree. If we would allow temporary storage to the algorithm, then this is the case where Strassen's recurrence <ref> [21, 15, x1.3.8] </ref> applies best (except on sparse or tiny matrices).
Reference: [22] <author> G. L. Steele, Jr. </author> <title> Debunking the expensive procedure call myth, or Procedure call implementations considered harmful, or LAMBDA: the ultimate GOTO. </title> <booktitle> ACM77: Proc. 1977 Annual Conf., </booktitle> <address> New York, </address> <note> ACM (1977), 153162. </note>
Reference-contexts: Management for the instruction cache ought to be handled by compilers, targeted as they are to specific hardware. Unfortunately, these compilers do a poor job with ordinary function linkage, stacking when it is unnecessary, and not unfolding recursions at all. (Nothing new there <ref> [22] </ref>!) So C's macro facility was used to unfold the base case manually (within constraints of the instruction cache, as a good compiler would). A typical result of unfolding appears in Figure 8.
Reference: [23] <author> D. S. Wise. </author> <title> Representing matrices as quadtrees for parallel processors (extended abstract). </title> <journal> ACM SIGSAM Bulletin 18, </journal> <note> 3 (August 1984), 2425. REFERENCES 12 </note>
Reference-contexts: Although additional storage may be available at distributed processors, it uses none, except for local variables in a recursion stack of depth lg n for order n matricesconstant space for all practical purposes. Four insights underlie the algorithms. First is the quadtree decomposition of matrices <ref> [23] </ref>, as well as the algorithms that manipulate them using recursive descent [24]. Second is a familiar indexing, newly applied to the map of matrices onto the address space. <p> This restriction is relaxed later with negligible overhead; the timing curves are smooth. Definition 1 <ref> [23] </ref> A complete matrix has index 0. A matrix at index i is either scalar, or it is composed of four submatrices northwest, southwest, southeast, and northeasteach of half the order and with indices 4i+1, 4i+2, 4i+3, and 4i+4, respectively. quadtree-matrix codes. <p> All this is generic code that contrasts with the competing BLAS3 routines that have been polished over decades to maximal performance on each architecture. A functional programmer's model for matrix multiplication uses mapping functions over quadruples <ref> [24, 23] </ref> to decompose matrix problems into square blocks. It is not too different from Figure 5 which exposes the blockwise algorithm, but obfuscates both the functional syntax and the sparse-matrix algebra that motivated this representation. <p> It is not too different from Figure 5 which exposes the blockwise algorithm, but obfuscates both the functional syntax and the sparse-matrix algebra that motivated this representation. As observed elsewhere <ref> [23, 21] </ref>, the quadrants of the answer can provide a partitioning into 4, 16, 64, . . . processes to compute the answer independently of one another. 4 RECURSIVE MATRIX MULTIPLICATION 4 4.1 Balanced parallel multiplication Matrix multiplication can be balanced across a small pool of processors top-down, even at compile
Reference: [24] <author> D. S. Wise. </author> <title> Undulant block elimination and integer-preserving matrix inversion. </title> <institution> Sci. Comput. </institution> <note> Programming (to appear). Technical Report 418, </note> <institution> Computer Science Department, Indiana University (revised, </institution> <month> August </month> <year> 1995). </year>
Reference-contexts: Four insights underlie the algorithms. First is the quadtree decomposition of matrices [23], as well as the algorithms that manipulate them using recursive descent <ref> [24] </ref>. Second is a familiar indexing, newly applied to the map of matrices onto the address space. Next is a decomposition of the usual eight recursive, quadrant multiplications into two parallel streams, balancing computational loads when the factors have known padding (east or south) with zeroes. <p> We prefer to view them, instead, as results from a different style of expressing the same high-level algorithms. The style is, however, one that also inspires a new perspective, new insights, andperhaps alsonew algorithms. Recursive and parallel versions of practical algorithms already exist that use this structure well <ref> [24, 13] </ref>; in such contexts this matrix multiplication is not only the most natural, it is also the fastest. <p> Our philosophy derives from our experience with functional programming and our view of its critical role for parallel programming [4]. Linear systems, in particular, come to us with a rich algebra that is best visible in a functional program that reads like interdependent formulae <ref> [13, 24] </ref>. Discovery of good formulations of knownand maybe even unknowntechniques follows top-down, partition-and-conquer of the underlying vector space. Our experience is that many insights and efficiencies are to be found here. <p> All this is generic code that contrasts with the competing BLAS3 routines that have been polished over decades to maximal performance on each architecture. A functional programmer's model for matrix multiplication uses mapping functions over quadruples <ref> [24, 23] </ref> to decompose matrix problems into square blocks. It is not too different from Figure 5 which exposes the blockwise algorithm, but obfuscates both the functional syntax and the sparse-matrix algebra that motivated this representation. <p> The use of this structure and this indexing should not be limited to this particular problem. Other solations can take good advantage of it. For instance, in LU decomposition it provides convenient pivoting on any block that is a subtree <ref> [24] </ref>, whose elimination needs only a single traversal using exactly this multiplication.
References-found: 24

