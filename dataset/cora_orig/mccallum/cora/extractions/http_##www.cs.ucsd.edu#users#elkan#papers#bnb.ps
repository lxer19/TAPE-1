URL: http://www.cs.ucsd.edu/users/elkan/papers/bnb.ps
Refering-URL: http://www.cs.ucsd.edu/users/elkan/
Root-URL: http://www.cs.ucsd.edu
Email: elkan@cs.ucsd.edu  
Title: BOOSTING AND NAIVE BAYESIAN LEARNING  
Author: Charles Elkan 
Note: First version  
Date: September 1997.  May 1997.  
Address: La Jolla, California 92093-0114  
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Pubnum: Technical Report No. CS97-557,  
Abstract: Although so-called naive Bayesian classification makes the unrealistic assumption that the values of the attributes of an example are independent given the class of the example, this learning method is remarkably successful in practice, and no uniformly better learning method is known. Boosting is a general method of combining multiple classifiers due to Yoav Freund and Rob Schapire. This paper shows that boosting applied to naive Bayesian classifiers yields combination classifiers that are representationally equivalent to standard feedforward multilayer perceptrons. (An ancillary result is that naive Bayesian classification is a nonparametric, nonlinear generalization of logistic regression.) As a training algorithm, boosted naive Bayesian learning is quite different from backpropagation, and has definite advantages. Boosting requires only linear time and constant space, and hidden nodes are learned incrementally, starting with the most important. On the real-world datasets on which the method has been tried so far, generalization performance is as good as or better than the best published result using any other learning method. Unlike all other standard learning algorithms, naive Bayesian learning, with and without boosting, can be done in logarithmic time with a linear number of parallel computing units. Accordingly, these learning methods are highly plausible computationally as models of animal learning. Other arguments suggest that they are plausible behaviorally also. 
Abstract-found: 1
Intro-found: 1
Reference: [ Bechara et al., 1997 ] <author> A. Bechara, H. Damasio H, D. Tranel, and A. R. Damasio. </author> <title> Deciding advantageously before knowing the advantageous strategy. </title> <booktitle> Science, </booktitle> <address> 275:12931295, </address> <month> February </month> <year> 1997. </year>
Reference-contexts: only 200 including 100 extra complete examples incomplete examples round error (%) error (%) 1 24.7 20.2 3 22.9 19.9 5 22.9 19.6 7 22.9 19.0 9 22.6 19.0 7 Neurocomputational and psychological plausibility There is strong experimental evidence that humans are highly sensitive to class-conditional probabilities; see for example <ref> [ Bechara et al., 1997 ] </ref> . Learning a single naive Bayesian classifier can be done incrementally, so it is computationally straightforward for the brain to do this in the course of everyday experience. Learning a boosted naive Bayesian classifier can be done by rehearsing past 8 experiences.
Reference: [ Blum and Rivest, 1992 ] <author> A. L. Blum and R. L. Rivest. </author> <title> Training a 3-node neural network is NP-complete. Neural Networks, </title> <address> 5(1):117127, </address> <year> 1992. </year>
Reference-contexts: With a constant number of rounds of boosting, boosted naive Bayesian learning is also in NC. A literature search reveals that no other practical learning problem is known to have an NC algorithm. Training simple linear threshold neural nets optimally is NP-complete <ref> [ Blum and Rivest, 1992 ] </ref> . 6 Experimental results It is important to stress that the experiments described here are exploratory in nature. They use datasets selected as likely to yield particularly informative results. The German credit dataset. This dataset has 700 positive and 300 negative examples.
Reference: [ Bures et al., 1997 ] <author> J. Bures, A. A. Fenton, Y. Kaminsky, and L. Zinyuk. </author> <title> Place cells and place navigation. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <address> 94(1):343350, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: Such an encoding, which is also called a population code, is quite plausible in many domains. For example, rats have place cells that fire when and only when the animal is in a specific spatial location <ref> [ Bures et al., 1997 ] </ref> and they have localist neural representations of other environmental features also.
Reference: [ Domingos and Pazzani, 1996 ] <author> P. Domingos and M. Pazzani. </author> <title> Beyond independence: Conditions for the optimality of the simple bayesian classifier. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 105112. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1996. </year>
Reference-contexts: Although this assumption is almost always violated in practice, recent work has shown that naive Bayesian learning is remarkably effective in practice and difficult to improve upon systematically <ref> [ Domingos and Pazzani, 1996 ] </ref> . This paper shows that it is possible to improve reliably the generalization ability of naive Bayesian classifiers using a technique called boosting due to Freund and Schapire [1995].
Reference: [ Dougherty et al., 1995 ] <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> Supervised and unsupervised discretization of continuous features. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, pages 194202. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1995. </year>
Reference-contexts: The simplest quantization method is to divide the range of the attribute into a fixed number M of equal-width bins. In the experiments described below M = 10 is chosen arbitrarily. Previous experimental work has shown that the benefits of more complex quantization methods are small at best <ref> [ Dougherty et al., 1995 ] </ref> . Using bins of equal width tends to work well because they allow good non-parametric approximation of skewed, multimodal, and/or heavy-tailed probability distributions. Let each A j be a numerical (integer-valued or real-valued) attribute.
Reference: [ Freund and Schapire, 1995 ] <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <booktitle> In Proceedings of the Second European Conference on Computational Learning Theory, </booktitle> <pages> pages 2337, </pages> <year> 1995. </year>
Reference: [ Friedman and Goldszmidt, 1996 ] <author> Nir Friedman and Moises Goldszmidt. </author> <title> Building classifers using Bayesian networks. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 12771284, </pages> <address> Portland, Oregon, August 1996. </address> <publisher> AAAI Press (distributed by MIT Press). </publisher>
Reference: [ Garner and Sutliff, 1974 ] <author> W. R. Garner and Donna Sutliff. </author> <title> The effect of goodness on encoding time in visual pattern discrimination. </title> <journal> Perception and Psychophysics, </journal> <volume> 16(3):426430, </volume> <year> 1974. </year>
Reference-contexts: This discussion will show that naive Bayesian learning reproduces each of the phenomena of human category learning discussed by Kruschke [1993]. In particular, naive Bayesian learning performs better at so-called filtration tasks than at condensation tasks <ref> [ Garner and Sutliff, 1974 ] </ref> . While humans also find filtration tasks easier, backpropagation works equally well for tasks of both types.
Reference: [ Kalocsai et al., 1996 ] <author> P. Kalocsai, I. Biederman, and C. von der Malsburg. </author> <title> Predicting face recognition from a spatial filter similarity space. </title> <type> Personal communication, </type> <year> 1996. </year>
Reference-contexts: In humans, face recognition may be based on localist features such as Gabor transforms of small facial regions that are in fixed positions relative to each other <ref> [ Kalocsai et al., 1996 ] </ref> . It is important to note that the claim here is not that humans or animals implement the Ad-aBoost algorithm precisely. Rather, the claim is that it is computationally feasible for low-level brain processes to implement some generally similar algorithm.
Reference: [ Kruschke, 1993 ] <author> John K. Kruschke. </author> <title> Human category learning: Implications for backpropagation models. </title> <booktitle> Connection Science, </booktitle> <address> 5(1):336, </address> <year> 1993. </year>
Reference: [ Lehky and Sejnowski, 1988 ] <author> S. R. Lehky and T. J. Sejnowski. </author> <title> Network model of shape-from-shading: neural function arises from both receptive and projective fields. </title> <journal> Nature, </journal> <volume> 333(6172):452 454, </volume> <year> 1988. </year>
Reference: [ Mezard and Nadal, 1989 ] <author> M. Mezard and J. P. Nadal. </author> <title> Learning in feedforward layered networks: The tiling algorithm. </title> <journal> Journal of Physics A, </journal> <volume> 22(12):21912203, </volume> <year> 1989. </year> <month> 10 </month>
Reference: [ Minsky and Papert, 1969 ] <author> Marvin Minsky and Seymour Papert. </author> <title> Perceptrons; an introduction to computational geometry. </title> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference: [ Quinlan, 1996 ] <author> John R. Quinlan. </author> <title> Boosting, </title> <booktitle> bagging, and C4.5. In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 725730, </pages> <address> Portland, Oregon, August 1996. </address> <publisher> AAAI Press (distributed by MIT Press). </publisher>
Reference-contexts: The best published prediction error rate is 25.3%, for a naive Bayesian classifier with five-fold cross-validation [ Friedman and Gold-szmidt, 1996 ] . Although boosting improves the performance of C4.5 on most datasets, on this dataset boosting increases the prediction error rate of C4.5 from 28.44% to 29.14% <ref> [ Quinlan, 1996 ] </ref> . 6 The table below shows the results of boosted naive Bayesian learning on this dataset.
Reference: [ Ripley, 1996 ] <author> Brian Ripley. </author> <title> Pattern Recognition and Neural Networks. </title> <publisher> Cambridge University Press, </publisher> <year> 1996. </year>
Reference: [ Schapire et al., 1997 ] <author> R. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. </author> <title> Boosting the margin; a new explanation for the effectiveness of voting methods. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1997. </year> <note> To appear. 11 </note>
Reference-contexts: As the number of rounds of boosting is increased beyond a dataset-dependent threshold, the error of the combined classifier increases slowly. This phenomenon is not surprising, but it has not been observed when boosting is used with other base learning methods <ref> [ Schapire et al., 1997 ] </ref> . Future work will aim to explain this phenomenon. For practical applications, cross-validation is a good way of deciding how many rounds of boosting to use. The monk's problems.
References-found: 16

