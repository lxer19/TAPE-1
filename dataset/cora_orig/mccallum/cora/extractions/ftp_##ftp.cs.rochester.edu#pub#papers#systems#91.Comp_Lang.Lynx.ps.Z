URL: ftp://ftp.cs.rochester.edu/pub/papers/systems/91.Comp_Lang.Lynx.ps.Z
Refering-URL: http://www.cs.rochester.edu/u/scott/pubs.html
Root-URL: 
Title: The Lynx Distributed Programming Language: Motivation, Design, and Experience  
Author: Michael L. Scott 
Date: August 1990  
Address: Rochester, NY 14627  
Affiliation: University of Rochester Department of Computer Science  
Abstract: A programming language can provide much better support for interprocess communication than a library package can. Most message-passing languages limit this support to communication between the pieces of a single program, but this need not be the case. Lynx facilitates convenient, typesafe message passing not only within applications, but also between applications, and among distributed collections of servers. Specifically, it addresses issues of compiler statelessness, late binding, and protection that allow run-time interaction between processes that were developed independently and that do not trust each other. Implementation experience with Lynx has yielded important insights into the relationship between distributed operating systems and language run-time support packages, and into the inherent costs of high-level message-passing semantics. Keywords: Distributed programming languages, message passing, remote procedure call, late binding, server processes, links. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. R. Andrews, R. A. Olsson, M. Coffin, I. J. P. Elshoff, K. Nilsen, T. Purdin and G. Town-send, </author> <title> ``An Overview of the SR Language and Implementation,'' </title> <journal> ACM Transactions on Programming Languages and Systems 10:1 (January 1988), </journal> <pages> pp. 51-86. </pages>
Reference-contexts: The problem with naming remote operations is that a resource may provide different sets of operations to different clients, or at different points in time. Even with facilities for bundling related operations (such as the resource and operation capabilities of SR <ref> [1] </ref>), the set of operations provided by an abstraction must be known to every client; servers cannot change the set of available operations to reflect changes in the state of the abstraction or to implement access control. <p> Emerald provides an object-oriented model that eliminates the distinction between local and remote invocations, but it still requires monitors to synchronize concurrent invocations within a single object. SR <ref> [1] </ref> takes a different approach to unifying remote and local invocations, but still requires semaphores for certain kinds of local synchronization. No matter how elegant the synchronization mechanism, its use is still a burden to the programmer. <p> Our experience with Lynx suggests that an operating system kernel should either be designed to support a single high-level language (as, for example, in the dedicated implementations of Argus [20], Linda [8], and SR <ref> [1] </ref>), or else should provide only the lowest common denominator for things that will be built upon it. A middle-level interface is likely to be both awkward and slow: awkward because it has sacrificed the flexibility of the more primitive system; slow because it has sacrificed its simplicity.
Reference: [2] <author> Y. Artsy, H. Chang and R. Finkel, </author> <title> ``Interprocess Communication in Charlotte,'' </title> <booktitle> IEEE Software 4:1 (January 1987), </booktitle> <pages> pp. 22-28. </pages>
Reference-contexts: Motivation for Lynx is discussed in section 2. Lynx was developed at the University of Wisconsin, where it was first implemented on the Charlotte multicomputer operating system <ref> [2, 12] </ref>. Charlotte was designed without Lynx, but experience with a conventional library interface to the kernel suggested that language support for communication could make the programmer's life much easier. <p> A detailed performance study of the Chrysalis implementation has helped to provide a deeper understanding of the inherent costs of message-passing systems [26]. These lessons are 3 summarized in section 6. 2. Motivation Motivation for Lynx grew out of experience with the Charlotte distributed operating system <ref> [2, 12] </ref>.
Reference: [3] <author> B. N. Bershad, T. E. Anderson, E. D. Lazowska and H. M. Levy, </author> <title> ``Lightweight Remote Procedure Call,'' </title> <journal> ACM Transactions on Computer Systems 8:1 (February 1990), </journal> <pages> pp. 37-55. </pages> <booktitle> Also in ACM SIGOPS Operating Systems Review 23:5; originally presented at the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> 3-6 December </month> <year> 1989. </year>
Reference-contexts: (0.6). 6% protocol option testing Checking for link movement (2.0), asynchronous notifications (1.0), premature requests (1.6), optional acknowledgments (1.8). 22% miscellaneous overhead Timing loop overhead (0.6), dispatcher loop and case statement overhead (1.4), procedure-call linkage (14.7), caching of constants in registers (5.6). hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Recent implementations of lightweight remote procedure call <ref> [3, 22] </ref> have broken the 1000 instruction barrier decisively by pre-computing significant portions of the invocation mechanism during an explicit connect-to-service operation.
Reference: [4] <author> A. D. Birrell and B. J. Nelson, </author> <title> ``Implementing Remote Procedure Calls,'' </title> <journal> ACM Transactions on Computer Systems 2:1 (February 1984), </journal> <pages> pp. 39-59. </pages> <booktitle> Originally presented at the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <month> 10-13 October </month> <year> 1983. </year>
Reference-contexts: As a result, most of the message-based operating systems still in use employ stub generators that augment the kernel call interface with a customized ``wrapper'' routine for each type of message in a program. Birrell and Nelson's Lupine stub generator <ref> [4] </ref> and the Accent Matchmaker [16] are particularly worthy of note. <p> Message-passing Languages Subroutines provide the natural mechanism for trapping into an operating system kernel for service, and for most services this mechanism works well. Experience suggests, however <ref> [4, 5, 12, 16] </ref>, that users do not find it acceptable for interprocess communication. The crux of the problem is that communication is significantly more complicated, from the user's point of view, than are most other kernel services.
Reference: [5] <author> A. P. Black, </author> <title> ``Supporting Distributed Applications: Experience with Eden,'' </title> <booktitle> Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <month> 1-4 December </month> <year> 1985, </year> <pages> pp. 181-193. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 19:5. </booktitle>
Reference-contexts: Message-passing Languages Subroutines provide the natural mechanism for trapping into an operating system kernel for service, and for most services this mechanism works well. Experience suggests, however <ref> [4, 5, 12, 16] </ref>, that users do not find it acceptable for interprocess communication. The crux of the problem is that communication is significantly more complicated, from the user's point of view, than are most other kernel services. <p> Dissatisfaction with a similar approach in Washington's Eden project <ref> [5] </ref> was a principal motivation for the development of the Emerald language [6]. Emerald provides an object-oriented model that eliminates the distinction between local and remote invocations, but it still requires monitors to synchronize concurrent invocations within a single object.
Reference: [6] <author> A. Black, N. Hutchinson, E. Jul and H. Levy, </author> <title> ``Object Structure in the Emerald System,'' </title> <booktitle> OOPSLA'86 Conference Proceedings, </booktitle> <month> 29 September - 2 October </month> <year> 1986, </year> <pages> pp. 78-86. </pages> <booktitle> In ACM SIGPLAN Notices 21:11. </booktitle>
Reference-contexts: Dissatisfaction with a similar approach in Washington's Eden project [5] was a principal motivation for the development of the Emerald language <ref> [6] </ref>. Emerald provides an object-oriented model that eliminates the distinction between local and remote invocations, but it still requires monitors to synchronize concurrent invocations within a single object. SR [1] takes a different approach to unifying remote and local invocations, but still requires semaphores for certain kinds of local synchronization.
Reference: [7] <author> C. M. Brown, R. J. Fowler, T. J. LeBlanc, M. L. Scott, M. Srinivas and others, </author> <title> ``DARPA Parallel Architecture Benchmark Study,'' </title> <type> BPR 13, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> October </month> <year> 1986. </year>
Reference-contexts: Summary and Conclusion Lynx is a programming language providing convenient, typesafe message passing among application and server processes in a distributed environment. Numerous programs have been written in Lynx over the course of the past five years, both as research projects and as coursework <ref> [7, 11] </ref>. In comparison to programs that perform communication through library routines, Lynx programs are consistently shorter, easier to debug, easier to write, and easier to read.
Reference: [8] <author> N. Carriero and D. Gelernter, </author> <title> ``The S/Net's Linda Kernel,'' </title> <journal> ACM Transactions on Computer Systems 4:2 (May 1986), </journal> <pages> pp. 110-129. </pages> <booktitle> Originally presented at the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <month> 1-4 December </month> <year> 1985. </year>
Reference-contexts: Our experience with Lynx suggests that an operating system kernel should either be designed to support a single high-level language (as, for example, in the dedicated implementations of Argus [20], Linda <ref> [8] </ref>, and SR [1]), or else should provide only the lowest common denominator for things that will be built upon it.
Reference: [9] <author> D. R. Cheriton and W. Zwaenepoel, </author> <title> ``The Distributed V Kernel and its Performance for Diskless Workstations,'' </title> <booktitle> Proceedings of the Ninth ACM Symposium on Operating Systems Principles, </booktitle> <month> 10-13 October </month> <year> 1983, </year> <pages> pp. 129-140. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 17:5. </booktitle>
Reference-contexts: Small savings could undoubtedly be realized here and there, but there does not seem to be any way to achieve significant performance gains without eliminating language features. Work by other researchers tends to confirm the hypothesis that data transmission times do not dominate the cost of practical message-passing systems <ref> [9, 18, 21, 30] </ref>. High-level semantic functions such as addressing, dispatching, bookkeeping, testing, and error handling are at least as significant, and often more so.
Reference: [10] <author> D. J. DeWitt, R. Finkel and M. Solomon, </author> <title> ``The Crystal Multicomputer: Design and Implementation Experience,'' </title> <journal> IEEE Transactions on Software Engineering SE-13:8 (August 1987), </journal> <pages> pp. 953-966. </pages>
Reference-contexts: Implementation Experience An implementation of Lynx for Wisconsin's Charlotte operating system was completed in 1984. It was ported to a simplified version of Charlotte in 1987. Charlotte runs on a collection of VAXen connected by a token ring <ref> [10] </ref>. An implementation for the Chrysalis operating system on the BBN Butterfly multiprocessor was completed in 1986. Other designs exist for Unix (using TCP/IP) and for an experimental system known as SODA [17]. Implementation of the Unix design was begun but not completed; the SODA design exists on paper only.
Reference: [11] <author> R. Finkel and others, </author> <title> ``Experience with Crystal, Charlotte, </title> <journal> and LYNX,'' </journal> <note> Computer Sciences Technical Reports #630, #649, and #673, </note> <institution> University of Wisconsin Madison, </institution> <month> February, July, and November </month> <year> 1986. </year>
Reference-contexts: Summary and Conclusion Lynx is a programming language providing convenient, typesafe message passing among application and server processes in a distributed environment. Numerous programs have been written in Lynx over the course of the past five years, both as research projects and as coursework <ref> [7, 11] </ref>. In comparison to programs that perform communication through library routines, Lynx programs are consistently shorter, easier to debug, easier to write, and easier to read.
Reference: [12] <author> R. A. Finkel, M. L. Scott, Y. Artsy and H. Chang, </author> <title> ``Experience with Charlotte: Simplicity and Function in a Distributed Operating System,'' </title> <journal> IEEE Transactions on Software Engineering 15:6 (June 1989), </journal> <pages> pp. 676-685. </pages> <booktitle> Extended abstract presented at the IEEE Workshop on Design Principles for Experimental Distributed Systems, </booktitle> <institution> Purdue University, </institution> <month> 15-17 October </month> <year> 1986. </year>
Reference-contexts: Motivation for Lynx is discussed in section 2. Lynx was developed at the University of Wisconsin, where it was first implemented on the Charlotte multicomputer operating system <ref> [2, 12] </ref>. Charlotte was designed without Lynx, but experience with a conventional library interface to the kernel suggested that language support for communication could make the programmer's life much easier. <p> A detailed performance study of the Chrysalis implementation has helped to provide a deeper understanding of the inherent costs of message-passing systems [26]. These lessons are 3 summarized in section 6. 2. Motivation Motivation for Lynx grew out of experience with the Charlotte distributed operating system <ref> [2, 12] </ref>. <p> Message-passing Languages Subroutines provide the natural mechanism for trapping into an operating system kernel for service, and for most services this mechanism works well. Experience suggests, however <ref> [4, 5, 12, 16] </ref>, that users do not find it acceptable for interprocess communication. The crux of the problem is that communication is significantly more complicated, from the user's point of view, than are most other kernel services. <p> An implementation (based on the Chrysalis version) is being developed for the Psyche multiprocessor operating system [28]. In addition to providing a testbed for evaluating Lynx, our implementation experience has led to unexpected insights into the relationship between a language run-time package and the underlying operating system <ref> [12, 24] </ref>, and also into the factors that contribute to message passing overhead [26]. 6.1. The Language/Kernel Interface A distributed operating system provides a process abstraction and primitives for communication between processes. A distributed programming language can regularize the use of the primitives, making them both safer and more convenient.
Reference: [13] <author> J. P. Fishburn, </author> <title> ``An Analysis of Speedup in Parallel Algorithms,'' </title> <type> Ph. D. thesis, </type> <institution> Computer Sciences Technical Report #431, University of Wisconsin Madison, </institution> <month> May </month> <year> 1981. </year>
Reference-contexts: Since our principal goal was to evaluate Lynx and not to investigate the design of parallel algorithms, we adopted an existing parallelization of alpha-beta search <ref> [13] </ref>. The basic idea behind the algorithm can be seen in figure 4. There are three different kinds of processes. One process (the ``master'') manages the user interface (in our case, this is a graphic display under the X window system).
Reference: [14] <author> D. Gelernter, </author> <title> ``Generative Communication in Linda,'' </title> <journal> ACM Transactions on Programming Languages and Systems 7:1 (January 1985), </journal> <pages> pp. 80-112. </pages>
Reference-contexts: Languages in which processes can send messages to arbitrary destinations, or in which a process is unable to specify the senders from whom it is willing to receive must generally resort to user-level mechanisms for probabilistic protection (e.g. with keys drawn randomly from a sparse set). Linda <ref> [14] </ref> is in some sense at the opposite extreme from Lynx. Its tuple space mechanism provides the equivalent of a completely-connected communication graph, freeing the programmer from all concern with links or their equivalent.
Reference: [15] <author> D. Hensgen and R. Finkel, </author> <title> ``Dynamic Server Squads in Yackos,'' </title> <booktitle> Proceedings of the First Workshop on Experiences with Building Distributed and Multiprocessor Systems, </booktitle> <month> 5-6 October, </month> <year> 1989, </year> <pages> pp. 73-89. 33 </pages>
Reference-contexts: It can also be passed on to a new server (functionally equivalent to the old one, presumably) in order to balance work load or otherwise improve performance. In a large distributed environment, many servers are likely to be implemented by dynamic squads of processes <ref> [15] </ref>.
Reference: [16] <author> M. B. Jones, R. F. Rashid and M. R. Thompson, ``Matchmaker: </author> <title> An Interface Specification Language for Distributed Processing,'' </title> <booktitle> Conference Record of the Twelfth ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1985, </year> <pages> pp. 225-235. </pages>
Reference-contexts: As a result, most of the message-based operating systems still in use employ stub generators that augment the kernel call interface with a customized ``wrapper'' routine for each type of message in a program. Birrell and Nelson's Lupine stub generator [4] and the Accent Matchmaker <ref> [16] </ref> are particularly worthy of note. <p> Message-passing Languages Subroutines provide the natural mechanism for trapping into an operating system kernel for service, and for most services this mechanism works well. Experience suggests, however <ref> [4, 5, 12, 16] </ref>, that users do not find it acceptable for interprocess communication. The crux of the problem is that communication is significantly more complicated, from the user's point of view, than are most other kernel services.
Reference: [17] <author> J. Kepecs and M. Solomon, </author> <title> ``SODA: A Simplified Operating System for Distributed Applications,'' </title> <booktitle> ACM SIGOPS Operating Systems Review 19:4 (October 1985), </booktitle> <pages> pp. 45-56. </pages> <booktitle> Originally presented at the Third Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <month> 27-29 August </month> <year> 1984. </year>
Reference-contexts: Charlotte runs on a collection of VAXen connected by a token ring [10]. An implementation for the Chrysalis operating system on the BBN Butterfly multiprocessor was completed in 1986. Other designs exist for Unix (using TCP/IP) and for an experimental system known as SODA <ref> [17] </ref>. Implementation of the Unix design was begun but not completed; the SODA design exists on paper only. An implementation (based on the Chrysalis version) is being developed for the Psyche multiprocessor operating system [28].
Reference: [18] <author> T. J. LeBlanc and R. P. Cook, </author> <title> ``An Analysis of Language Models for High-Performance Communication in Local-Area Networks,'' </title> <booktitle> Proceedings of the SIGPLAN '83 Symposium on Programming Language Issues in Software Systems, </booktitle> <month> 27-29 June </month> <year> 1983, </year> <pages> pp. 65-72. </pages> <booktitle> In ACM SIGPLAN Notices 18:6. </booktitle>
Reference-contexts: Small savings could undoubtedly be realized here and there, but there does not seem to be any way to achieve significant performance gains without eliminating language features. Work by other researchers tends to confirm the hypothesis that data transmission times do not dominate the cost of practical message-passing systems <ref> [9, 18, 21, 30] </ref>. High-level semantic functions such as addressing, dispatching, bookkeeping, testing, and error handling are at least as significant, and often more so.
Reference: [19] <author> B. Liskov and R. Scheifler, </author> <title> ``Guardians and Actions: Linguistic Support for Robust, Distributed Programs,'' </title> <journal> ACM Transactions on Programming Languages and Systems 5:3 (July 1983), </journal> <pages> pp. 381-404. </pages>
Reference-contexts: This facility allows the run-time system to implement message passing on a shared-memory machine by moving pointers, without worrying that a sender will subsequently modify the variables it has ``sent.'' In Argus <ref> [19] </ref>, one can send messages between processes that use completely different 20 implementations of a common abstract type. The compiler inserts code in the sender to translate data into a universal, machine-independent format.
Reference: [20] <author> B. Liskov, D. Curtis, P. Johnson and R. Scheifler, </author> <title> ``Implementation of Argus,'' </title> <booktitle> Proceedings of the Eleventh ACM Symposium on Operating Systems Principles, </booktitle> <month> 8-11 November </month> <year> 1987, </year> <pages> pp. 111-122. </pages> <booktitle> In ACM SIGOPS Operating Systems Review 21:5. </booktitle>
Reference-contexts: Our experience with Lynx suggests that an operating system kernel should either be designed to support a single high-level language (as, for example, in the dedicated implementations of Argus <ref> [20] </ref>, Linda [8], and SR [1]), or else should provide only the lowest common denominator for things that will be built upon it.
Reference: [21] <author> B. J. Nelson, </author> <title> ``Remote Procedure Call,'' </title> <type> Ph. D. Thesis, Technical Report CMU-CS-81-119, </type> <institution> Carnegie-Mellon University, </institution> <year> 1981. </year>
Reference-contexts: Small savings could undoubtedly be realized here and there, but there does not seem to be any way to achieve significant performance gains without eliminating language features. Work by other researchers tends to confirm the hypothesis that data transmission times do not dominate the cost of practical message-passing systems <ref> [9, 18, 21, 30] </ref>. High-level semantic functions such as addressing, dispatching, bookkeeping, testing, and error handling are at least as significant, and often more so.
Reference: [22] <author> M. Schroeder and M. Burrows, </author> <title> ``Performance of Firefly RPC,'' </title> <journal> ACM Transactions on Computer Systems 8:1 (February 1990), </journal> <pages> pp. 1-17. </pages> <booktitle> Also in ACM SIGOPS Operating Systems Review 23:5; originally presented at the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> 3-6 December </month> <year> 1989. </year>
Reference-contexts: (0.6). 6% protocol option testing Checking for link movement (2.0), asynchronous notifications (1.0), premature requests (1.6), optional acknowledgments (1.8). 22% miscellaneous overhead Timing loop overhead (0.6), dispatcher loop and case statement overhead (1.4), procedure-call linkage (14.7), caching of constants in registers (5.6). hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh Recent implementations of lightweight remote procedure call <ref> [3, 22] </ref> have broken the 1000 instruction barrier decisively by pre-computing significant portions of the invocation mechanism during an explicit connect-to-service operation.
Reference: [23] <author> M. L. Scott, </author> <title> ``A Framework for the Evaluation of High-Level Languages for Distributed Computing,'' </title> <type> Computer Sciences Technical Report #563, </type> <institution> University of Wisconsin Madison, </institution> <month> October </month> <year> 1984. </year>
Reference-contexts: Upon surveying the state of the art in distributed programming languages <ref> [23] </ref>, it became apparent that most existing notations had been oriented toward communication among the processes of a single distributed program, and that new problems would arise when attempting to communicate across program boundaries with servers, for example.
Reference: [24] <author> M. L. Scott, </author> <title> ``The Interface Between Distributed Operating System and High-Level Programming Language,'' </title> <booktitle> Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <month> 19-22 August </month> <year> 1986, </year> <pages> pp. 242-249. </pages>
Reference-contexts: Designs or partial implementations have been developed for four additional systems. Experience with these implementations has led to important insights into the relationship between a language run-time package and the underlying operating system <ref> [24] </ref>. A detailed performance study of the Chrysalis implementation has helped to provide a deeper understanding of the inherent costs of message-passing systems [26]. These lessons are 3 summarized in section 6. 2. Motivation Motivation for Lynx grew out of experience with the Charlotte distributed operating system [2, 12]. <p> An implementation (based on the Chrysalis version) is being developed for the Psyche multiprocessor operating system [28]. In addition to providing a testbed for evaluating Lynx, our implementation experience has led to unexpected insights into the relationship between a language run-time package and the underlying operating system <ref> [12, 24] </ref>, and also into the factors that contribute to message passing overhead [26]. 6.1. The Language/Kernel Interface A distributed operating system provides a process abstraction and primitives for communication between processes. A distributed programming language can regularize the use of the primitives, making them both safer and more convenient. <p> A process that discovers its hint to be incorrect must resort to a broadcast mechanism for link-end location discovery. The resulting protocol is substantially simpler than the one employed in Charlotte. Previous papers have cited this experience as evidence that ``hints can be better than absolutes'' <ref> [24] </ref>. The caveat in this lesson is that the correctness of the overall algorithm must not depend on the hints. If the work they facilitate is important, it must be possible to detect when they fail, and there must be a fallback mechanism that can be counted on to work.
Reference: [25] <author> M. L. Scott, </author> <title> ``Language Support for Loosely-Coupled Distributed Programs,'' </title> <journal> IEEE Transactions on Software Engineering SE-13:1 (January 1987), </journal> <pages> pp. 88-103. </pages>
Reference-contexts: Unfortunately, most existing distributed languages are better suited to communication between the processes of a single application than they are to communication between processes that are developed independently. Such independent development is characteristic both of the systems software for multicomputers and of the applications software for geographically-distributed networks. Lynx <ref> [25, 29] </ref> is a message-passing language designed to support both application and system software in a single conceptual framework.
Reference: [26] <author> M. L. Scott and A. L. Cox, </author> <title> ``An Empirical Study of Message-Passing Overhead,'' </title> <booktitle> Proceedings of the Seventh International Conference on Distributed Computing Systems, </booktitle> <month> 21-25 Sep-tember </month> <year> 1987, </year> <pages> pp. 536-543. </pages>
Reference-contexts: Experience with these implementations has led to important insights into the relationship between a language run-time package and the underlying operating system [24]. A detailed performance study of the Chrysalis implementation has helped to provide a deeper understanding of the inherent costs of message-passing systems <ref> [26] </ref>. These lessons are 3 summarized in section 6. 2. Motivation Motivation for Lynx grew out of experience with the Charlotte distributed operating system [2, 12]. <p> In addition to providing a testbed for evaluating Lynx, our implementation experience has led to unexpected insights into the relationship between a language run-time package and the underlying operating system [12, 24], and also into the factors that contribute to message passing overhead <ref> [26] </ref>. 6.1. The Language/Kernel Interface A distributed operating system provides a process abstraction and primitives for communication between processes. A distributed programming language can regularize the use of the primitives, making them both safer and more convenient.
Reference: [27] <author> M. L. Scott and R. A. Finkel, </author> <title> ``A Simple Mechanism for Type Security Across Compilation Units,'' </title> <journal> IEEE Transactions on Software Engineering 14:8 (August 1988), </journal> <pages> pp. 1238-1239. </pages>
Reference-contexts: In particular, it does not depend on a database of type definitions or interface descriptions in order to enforce type consistency for messages. A novel application of hashing <ref> [27] </ref> provides efficient run-time type checking at negligible cost and at a very high level of confidence. A description of language features appears in section 3. This is followed in section 4 by a pair of example applications: a file system server and a game-playing program. <p> Since canonical forms can be of arbitrary length, run-time comparisons are potentially costly. To minimize this cost, the Lynx compiler uses a hash function to compress its type descriptions into 32-bit codes <ref> [27] </ref>. Hashing reduces the cost of type checking to half a dozen instructions per 11 remote operation. It introduces the possibility of undetected type clashes, 4 but with a good hash function the probability of this problem is less than one in a billion.
Reference: [28] <author> M. L. Scott, T. J. LeBlanc and B. D. Marsh, </author> <booktitle> ``Multi-Model Parallel Programming in Psyche,'' Proceedings of the Second ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> 14-16 March, </month> <year> 1990, </year> <pages> pp. 70-78. </pages>
Reference-contexts: Other designs exist for Unix (using TCP/IP) and for an experimental system known as SODA [17]. Implementation of the Unix design was begun but not completed; the SODA design exists on paper only. An implementation (based on the Chrysalis version) is being developed for the Psyche multiprocessor operating system <ref> [28] </ref>. In addition to providing a testbed for evaluating Lynx, our implementation experience has led to unexpected insights into the relationship between a language run-time package and the underlying operating system [12, 24], and also into the factors that contribute to message passing overhead [26]. 6.1.
Reference: [29] <author> M. L. Scott, </author> <title> ``LYNX Reference Manual,'' </title> <type> BPR 7, </type> <institution> Computer Science Department, University of Rochester, </institution> <note> August 1986 (revised). </note>
Reference-contexts: Unfortunately, most existing distributed languages are better suited to communication between the processes of a single application than they are to communication between processes that are developed independently. Such independent development is characteristic both of the systems software for multicomputers and of the applications software for geographically-distributed networks. Lynx <ref> [25, 29] </ref> is a message-passing language designed to support both application and system software in a single conceptual framework.
Reference: [30] <author> A. Z. Spector, </author> <title> ``Performing Remote Operations Efficiently on a Local Computer Network,'' </title> <journal> Communications of the ACM 25:4 (April 1982), </journal> <pages> pp. 246-260. </pages>
Reference-contexts: Small savings could undoubtedly be realized here and there, but there does not seem to be any way to achieve significant performance gains without eliminating language features. Work by other researchers tends to confirm the hypothesis that data transmission times do not dominate the cost of practical message-passing systems <ref> [9, 18, 21, 30] </ref>. High-level semantic functions such as addressing, dispatching, bookkeeping, testing, and error handling are at least as significant, and often more so.
Reference: [31] <author> R. E. Strom and S. Yemini, </author> <title> ``NIL: An Integrated Language and System for Distributed Programming,'' </title> <booktitle> Proceedings of the SIGPLAN '83 Symposium on Programming Language Issues in Software Systems, </booktitle> <month> 27-29 June </month> <year> 1983, </year> <pages> pp. 73-82. </pages> <booktitle> In ACM SIGPLAN Notices 18:6. </booktitle>
Reference-contexts: In a similar vein, the compiler for NIL <ref> [31] </ref> tracks the status of every program variable and treats a variable that has just been sent in a message as if it were uninitialized.
Reference: [32] <author> D. Swinehart, P. Zellweger, R. Beach and R. Hagmann, </author> <title> ``A Structural View of the Cedar Programming Environment,'' </title> <journal> ACM Transactions on Programming Languages and Systems 8:4 (October 1986), </journal> <pages> pp. 419-490. 34 </pages>
Reference-contexts: It is of course attractive to have a lightweight thread mechanism that addresses both goals at once, but even the appearance of genuine parallelism introduces the need for fine-grained synchronization on data that is shared between threads. In a monitor-based language with a stub generator (Cedar <ref> [32] </ref> for example), the programmer must keep track of two very different forms of synchronization: monitors shared by threads in the same address space and remote procedure calls between threads in different address spaces.
Reference: [33] <author> W. F. Tichy, </author> <title> ``Smart Recompilation,'' </title> <journal> ACM Transactions on Programming Languages and Systems 8:3 (July 1986), </journal> <pages> pp. 273-291. </pages> <note> Relevant correspondence appears in Volume 10, Nunber 4. </note>
Reference-contexts: If changes are made to certain routines but not to others, we should recompile only those processes whose behavior would otherwise be incorrect. It is possible to build a compiler that incorporates a formal notion of upward compatibility <ref> [33] </ref>. Such a compiler could implement run-time checking of name equivalence for types, but its construction would not be easy (even in the absence of multiple sites), and its checking would likely be costly.
Reference: [34] <institution> United States Department of Defense, </institution> <note> ``Reference Manual for the Ada Programming Language,'' Available as Lecture Notes in Computer Science #106, </note> <editor> Springer-Verlag, </editor> <address> New York, </address> <year> 1981, </year> <month> 17 February </month> <year> 1983. </year>
Reference-contexts: In a more general sense, a message-based programming language can ease the life of the programmer by providing special communication syntax or by implementing useful side effects for communication statements. It would be difficult, for example, to provide the functionality of an Ada select statement <ref> [34] </ref> without its distinctive syntax. The select system call of Berkeley Unix, for example, is much less convenient to use. A less widely appreciated feature of Ada is its carefully-designed semantics for data sharing between tasks.
Reference: [35] <author> J. Welsh and A. Lister, </author> <title> ``A Comparative Study of Task Communication in Ada,'' </title> <booktitle> Software Practice and Experience 11 (1981), </booktitle> <pages> pp. 257-290. </pages>
Reference-contexts: The sequence of operations need not be known at the time that access is obtained; a client can, for example, obtain read access, read an index, and read a location calculated from that index in one protected session. Solving the same problem in Ada <ref> [35] </ref> requires a complicated system of unforgable keys, implemented in user code. It is the ability of a server to refer to links by name that permits it to implement access control.
Reference: [36] <author> N. Wirth, </author> <title> ``Modula: a Language for Modular Multiprogramming,'' </title> <booktitle> Software Practice and Experience 7 (1977), </booktitle> <pages> pp. 3-35. </pages>
Reference-contexts: Like many other systems developed in the 1970's and early 1980's, Charlotte was designed to provide many of its services in user-level processes, rather than in the kernel. 2 The first generation of Charlotte servers was written in a conventional sequential language (Modula-1 <ref> [36] </ref>, sequential features only), augmented with procedure calls to access kernel message-passing primitives. Problems with this approach soon became apparent, and suggested the need for a special programming language. Section 5 discusses these problems in detail, considering candidate solutions and justifying the Lynx approach. <p> A static set of synchronization conditions (as, for example, in Modula-1 <ref> [36] </ref>) does not suffice. 21 organization. It is quite another to talk about a program that has pieces under the control of a thousand different travel agencies, possibly written in different languages and running on different types of hardware.
References-found: 36

