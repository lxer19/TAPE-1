URL: http://www.cs.rochester.edu/u/meira/submitted/npaper.ps.gz
Refering-URL: http://www.cs.rochester.edu/stats/oldmonths/1998.03/docs-name.html
Root-URL: 
Email: fmeira,leblancg@cs.rochester.edu  
Title: Cause-Effect Analysis of Parallel Program Performance  
Author: Wagner Meira Jr. and Thomas J. LeBlanc 
Note: Submitted to PPOPP'97.  
Address: Rochester, Rochester NY 14627  
Affiliation: Department of Computer Science University of  
Abstract: This paper describes a general framework and several specific techniques for cause-effect analysis: an automated inference process that presents explanations for dynamic phenomena of parallel program executions in terms of underlying causes and the related source code. We illustrate the framework by describing the implementation of three analysis techniques: waiting time analysis identifies the cause of synchronization overhead as the differences in execution paths taken by synchronizing processors; protocol analysis identifies the sharing patterns to pages that produce invalidations in a DSM protocol; and transaction analysis identifies conflicts between transactions in a parallel file system that cause aborts. We present examples of how each technique can be used to understand observed performance effects, and how insights derived from these techniques suggest program modifications that lead to improved performance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. S. Adve, J. Wang, J. Mellor-Crummey, D. A. Reed, M. Anderson, and K. Kennedy. </author> <title> An integrated compilation and performance analysis environment for data parallel programs. </title> <type> Technical Report CRPC-TR94513-S, </type> <institution> Rice University, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: There are many tools that help the programmer in assessing performance phenomena. The integration of Fortran D and Pablo <ref> [1] </ref> correlates static information (e.g., data dependences) and fl This research was supported by NSF grant CCR-9510173, an NSF CISE Institutional Infrastructure Grant No. CDA-9401142, and an equipment grant from Digital Equipment Corporation's External Research Program. Wagner Meira Jr. is supported by CNPq-Brazil, Grant 200.862/93-6 1 dynamic measurements.
Reference: [2] <author> C. Amza, A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, and W. Zwaenepoel. Treadmarks: </author> <title> shared memory computing on networks of workstations. </title> <booktitle> IEEE Computer, </booktitle> <month> Febru-ary </month> <year> 1996. </year>
Reference-contexts: The code executed by the processor that waits is given on the left. In cases where one processor does everything the other processor does and more, the left side of the figure would be empty. 4 Protocol Analysis In distributed shared memory environments like Treadmarks <ref> [2] </ref>, communication occurs as a consequence of coherence protocol activity.
Reference: [3] <author> T. Chilimbi, T. Ball, S. Eick, and J. Larus. Stormwatch: </author> <title> A tool for visualizing memory system protocols. </title> <booktitle> In Proceedings of Supercomputing'95, </booktitle> <address> San Diego, CA, </address> <month> December </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: Although all of these tools give some insight into the causes of performance phenomena, the programmer is still responsible for the inference process that leads from observed effects to root causes. Some tools focus on a limited form of causality information. StormWatch <ref> [3] </ref> is a visualization tool for memory system protocols that captures relationships between individual memory events, thus exposing causality in memory operations. Another tool, a performance debugger implemented by Rajamony and Cox [10], automatically detects unnecessary and excessive synchronization by verifying data accesses between synchronization intervals.
Reference: [4] <author> A. Goldberg and J. Hennessy. </author> <title> MTool: An integrated system for performance debugging shared memory multiprocessor applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 28-40, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: CDA-9401142, and an equipment grant from Digital Equipment Corporation's External Research Program. Wagner Meira Jr. is supported by CNPq-Brazil, Grant 200.862/93-6 1 dynamic measurements. Paradyn [13] identifies both synchronization and shared-memory bottle-necks, and provides profiles and information about sharing patterns. MTool <ref> [4] </ref> and MemSpy [6] focus on memory operations, quantifying their influence on execution time and identifying the relevant portions of code and data structures.
Reference: [5] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of ACM, </journal> <pages> pages 189-194, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: Each event record in the trace file contains an event type, the processor id, a static scope identifier, and a global timestamp, which may be based on physical or logical clocks <ref> [5] </ref>. 1 The trace file is used to construct execution-time profiles that help the programmer to identify sources of inefficiency. Based on an analysis of the profiles, the programmer may select one or more cause-effect analysis techniques to be applied to the trace file.
Reference: [6] <author> M. Martonosi, A. Gupta, and T. Anderson. Memspy: </author> <title> Analyzing memory system bottlenecks in programs. Performance Evaluation Review, </title> <address> 20(1):1 - 12, </address> <month> June </month> <year> 1992. </year> <note> Reprint of a paper presented in Sigmetrics' 92. </note>
Reference-contexts: CDA-9401142, and an equipment grant from Digital Equipment Corporation's External Research Program. Wagner Meira Jr. is supported by CNPq-Brazil, Grant 200.862/93-6 1 dynamic measurements. Paradyn [13] identifies both synchronization and shared-memory bottle-necks, and provides profiles and information about sharing patterns. MTool [4] and MemSpy <ref> [6] </ref> focus on memory operations, quantifying their influence on execution time and identifying the relevant portions of code and data structures.
Reference: [7] <author> W. Meira Jr., T. LeBlanc, and A. Poulos. </author> <title> Waiting time analysis and performance visualization in Carnival. </title> <booktitle> In Proceedings of SPDT96: SIGMETRICS Symposium on Parallel and Distributed Tools, </booktitle> <pages> pages 1-10, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: In this paper we introduce a general framework for automating the inference process that relates observed performance phenomena to underlying causes, which we call cause-effect analysis. This framework, which has been implemented in the Carnival performance visualization tool <ref> [7] </ref>, uses performance profiles to quantify classes of phenomena (such as idle cycles, communication overhead, and load imbalance) and abstractions of execution traces to explain them. <p> and produce an explanation for each one in terms of events, (3) simplify (as appropriate) the explanations for each individual instance of a performance phenomena, (4) combine explanations for phenomena that occur at the same place, and (5) create visualizations for the resulting explanations Our visualizations are created using Carnival <ref> [7, 8] </ref>, a tool under development at Rochester that supports hierarchical abstraction in the presentation of performance data, maintains links between dynamic measurements and the source code, and provides visualization of cause-effect explanations of performance phenomena. <p> Waiting time analysis <ref> [7] </ref> identifies each occurrence of waiting time in the execution trace, and the set of basic blocks traversed by each processor leading up to a synchronization point. We divide a processor's execution path into synchronization intervals, where the intervals are delimited by synchronization points between the same pair of processors.
Reference: [8] <author> W. Meira Jr., T. J. LeBlanc, N. Hardavellas, and C. Amorim. </author> <title> Understanding the performance of DSM applications. </title> <booktitle> In Proceedings of the Workshop on Communication and Architectural Support for Network-based Parallel Computing (CANPC' 97), </booktitle> <address> San Antonio, TX, </address> <month> February </month> <year> 1997. </year> <title> IEEE, </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: and produce an explanation for each one in terms of events, (3) simplify (as appropriate) the explanations for each individual instance of a performance phenomena, (4) combine explanations for phenomena that occur at the same place, and (5) create visualizations for the resulting explanations Our visualizations are created using Carnival <ref> [7, 8] </ref>, a tool under development at Rochester that supports hierarchical abstraction in the presentation of performance data, maintains links between dynamic measurements and the source code, and provides visualization of cause-effect explanations of performance phenomena.
Reference: [9] <author> S. A. Moyer and V. S. Sunderan. </author> <title> PIOUS for PVM: User's guide and reference manual version 1, 1995. </title> <type> Technical report, </type> <institution> Emory University, </institution> <year> 1995. </year> <note> Available via the PIOUS home page at http://www.mathcs.emory.edu/pious.html. </note>
Reference-contexts: Given that a single file operation can involve multiple servers in parallel, the file system must use 7 some form of concurrency control to maintain consistency. The PIOUS file system <ref> [9] </ref>, in which every file operation is a transaction, uses strict two-phase locking to ensure sequential consistency. When an application process issues a file operation, messages requesting the operation are sent to the appropriate disk servers, which perform the operation (when possible) and return a result. <p> These applications require frequent I/O operations, which may represent a significant source of performance degradation [14], especially in systems that serialize I/O accesses. One way to alleviate the I/O bottleneck is to employ a parallel file system such as PIOUS <ref> [9] </ref>. Our implementation of data mining on basket data [14] employed a fully distributed file layout, where each file was striped across all disk servers.
Reference: [10] <author> R. Rajamony and A. Cox. </author> <title> A performance debugger for eliminating excess synchronization in shared-memory parallel programs. </title> <booktitle> In Proceedings of the 4th International Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of COmputer and Telecommunication Systems (MASCOTS), </institution> <month> February </month> <year> 1996. </year> <month> 11 </month>
Reference-contexts: Some tools focus on a limited form of causality information. StormWatch [3] is a visualization tool for memory system protocols that captures relationships between individual memory events, thus exposing causality in memory operations. Another tool, a performance debugger implemented by Rajamony and Cox <ref> [10] </ref>, automatically detects unnecessary and excessive synchronization by verifying data accesses between synchronization intervals. In this paper we introduce a general framework for automating the inference process that relates observed performance phenomena to underlying causes, which we call cause-effect analysis.
Reference: [11] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In particular, lock operations in hardware shared memory and in software distributed shared memory have very different costs. Waiting time analysis is a valuable technique for analyzing and understanding these costs, and the implications for the synchronization behavior of the application. Water, from the Splash benchmark suite <ref> [11] </ref>, is an example of an application whose performance changed drastically when it was ported from hardware shared memory to Treadmarks. On hardware shared memory, waiting time was not a significant part of the overall execution time and yet on Treadmarks, waiting time accounted for 60% of the execution time.
Reference: [12] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: Since communication can result in waiting time, waiting time analysis allows us to quantify the effect of false sharing on synchronization overhead. Protocol analysis provides details about the access patterns of the application, and identifies data structures associated with excessive communication. When executing Ocean <ref> [12] </ref>, an application from the Splash2 suite, on Treadmarks, we observed that over 90% of the execution time was spent either synchronizing or waiting for coherence operations to complete. Using protocol analysis we discovered that the variable multi accounts for 75% of all communication.
Reference: [13] <author> Z. Xu, J. R. Larus, and B. P. Miller. </author> <title> Shared-memory performance profiling. </title> <note> Available at ftp://grilled.cs.wisc.edu/technical papers/ppopp97.ps.Z, </note> <month> December </month> <year> 1996. </year>
Reference-contexts: CDA-9401142, and an equipment grant from Digital Equipment Corporation's External Research Program. Wagner Meira Jr. is supported by CNPq-Brazil, Grant 200.862/93-6 1 dynamic measurements. Paradyn <ref> [13] </ref> identifies both synchronization and shared-memory bottle-necks, and provides profiles and information about sharing patterns. MTool [4] and MemSpy [6] focus on memory operations, quantifying their influence on execution time and identifying the relevant portions of code and data structures.
Reference: [14] <author> M. J. Zaki, M. Ogihara, S. Parthasarathy, and W. Li. </author> <title> Parallel data mining for association rules on shared-memory multi-processors. </title> <booktitle> In Proceedings of Supercomputing' 96, </booktitle> <address> Pittsburgh, PA, </address> <month> November </month> <year> 1996. </year> <month> 12 </month>
Reference-contexts: By using a blocked data distribution instead of tiling, we improved the performance of the application under Treadmarks by a factor of 8 on 4 processors. 6.3 Contention vs. Throughput in Parallel File Systems Parallel data mining applications <ref> [14] </ref> are usually iterative, and during each iteration every process "mines" several records. Mining involves reading a record, applying the mining procedures to it, and storing the results for future processing. These applications require frequent I/O operations, which may represent a significant source of performance degradation [14], especially in systems that <p> Parallel data mining applications <ref> [14] </ref> are usually iterative, and during each iteration every process "mines" several records. Mining involves reading a record, applying the mining procedures to it, and storing the results for future processing. These applications require frequent I/O operations, which may represent a significant source of performance degradation [14], especially in systems that serialize I/O accesses. One way to alleviate the I/O bottleneck is to employ a parallel file system such as PIOUS [9]. Our implementation of data mining on basket data [14] employed a fully distributed file layout, where each file was striped across all disk servers. <p> These applications require frequent I/O operations, which may represent a significant source of performance degradation <ref> [14] </ref>, especially in systems that serialize I/O accesses. One way to alleviate the I/O bottleneck is to employ a parallel file system such as PIOUS [9]. Our implementation of data mining on basket data [14] employed a fully distributed file layout, where each file was striped across all disk servers. Our profiles showed that under this layout scheme a significant amount of time was devoted to I/O operations, many of which were associated with repeated trials of aborted write transactions.
References-found: 14

