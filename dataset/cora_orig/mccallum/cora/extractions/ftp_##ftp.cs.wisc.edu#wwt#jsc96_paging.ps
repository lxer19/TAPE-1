URL: ftp://ftp.cs.wisc.edu/wwt/jsc96_paging.ps
Refering-URL: http://www.cs.wisc.edu/~david/david.html
Root-URL: 
Title: Paging Tradeoffs in Distributed-Shared-Memory Multiprocessors  
Author: DOUGLAS C. BURGER AND RAHMAT S. HYDER BARTON P. MILLER AND DAVID A. WOOD Editor: Richard Draper and John Riganati 
Keyword: Shared-memory multiprocessors, virtual memory, scheduling, synchronization  
Address: Madison, WI 53706  
Affiliation: Computer Sciences Department, University of Wisconsin-Madison,  
Note: Journal of Supercomputing,  c 1996 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  This work is supported in part by NSF Presidential Young Investigator Award CCR-9157366, NSF Grants MIP-9225097, CCR-9100968, and CDA-9024618, Office of Naval Research Grant N00014-89-J-1222, Department of Energy Grant DE-FG02-93ER25176, and donations from Thinking Machines Corporation, Xerox Corporation, and Digital Equipment Corporation.  
Email: wwt@cs.wisc.edu  
Date: 8, 1-30 (1996)  
Abstract: Massively parallel processors have begun using commodity operating systems that support demand-paged virtual memory. To evaluate the utility of virtual memory, we measured the behavior of seven shared-memory parallel application programs on a simulated distributed-shared-memory machine. Our results (1) confirm the importance of gang CPU scheduling, (2) show that a page-faulting processor should spin rather than invoke a parallel context switch, (3) show that our parallel programs frequently touch most of their data, and (4) indicate that memory, not just CPUs, must be "gang scheduled." Overall, our experiments demonstrate that demand paging has limited value on current parallel machines because of the applications' synchronization and memory reference patterns and the machines' high page-fault and parallel context-switch overheads. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Agarwal, R. Simoni, M. Horowitz, and J. Hennessy, </author> <title> "An evaluation of directory schemes PAGING TRADEOFFS IN DISTRIBUTED-SHARED-MEMORY MULTIPROCESSORS 29 for cache coherence," </title> <booktitle> in Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 280-289, </pages> <year> 1988. </year>
Reference-contexts: The nodes are connected by a point-to-point network (100-cycle constant latency) and coherence is maintained through a full-map directory protocol (i.e., dir n N B <ref> [1] </ref>). The address space is globally shared, with 4-Kbyte shared pages assigned to nodes round-robin. On a page fault the target operating system selects a victim page using the Clock [5] algorithm. To maintain inclusion, the system invalidates all cached blocks from the victim page.
Reference: 2. <author> T. Anderson, </author> <title> "NOW: Distributed supercomputing on a network of workstations," September 1993. </title> <booktitle> Presentation at 1993 Fall ARPA HPC Software PI's meeting. </booktitle>
Reference-contexts: Furthermore, simple schemes that allocate fixed memory partitions on each node are unlikely to be effective, since many applications have unequal requirements for private pages. While there have been numerous proposals to manage processors globally <ref> [2] </ref>, we believe these are the first results indicating the importance of doing so for physical memory. Demand paging becomes more attractive for parallel applications if either the page-fault service time (T pf ) or parallel context-switch overhead (T pcs ) decreases.
Reference: 3. <author> T. Anderson, D. Culler, and D. Patterson, </author> <title> "A case for networks of workstations: NOW," </title> <booktitle> IEEE Micro, </booktitle> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Recently, however, massively parallel supercomputers have begun running modified workstation operating systems: the Intel Paragon and Convex SPP-1 run modified versions of Mach OSF/1 AD and the Meiko CS-2 runs a modified version of Solaris. Furthermore, clusters of workstations are emerging as increasingly popular alternatives to dedicated parallel supercomputers <ref> [3] </ref>, [19], [7]. Parallel applications on these systems must coexist with the operating system's demand-paged virtual memory. In this paper we examine the performance of seven shared-memory scientific applications and argue that demand paging has limited value on distributed-shared-memory parallel computers.
Reference: 4. <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon, </author> <title> "The MAS parallel benchmarks," </title> <type> Technical Report RNR-91-002 Revision 2, </type> <institution> NASA Ames Research Center, </institution> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Methodology The performance studies in this paper are based on a hypothetical distributed-shared-memory (DSM) system running seven shared-memory programs. The benchmark programs are scientific applications drawn from a variety of sources, including PAGING TRADEOFFS IN DISTRIBUTED-SHARED-MEMORY MULTIPROCESSORS 5 the Stanford SPLASH benchmarks [16], the NAS benchmarks <ref> [4] </ref>, and the Schlum-berger Corporation. We simulated the execution of these programs on our hypothetical DSM system using the Wisconsin Wind Tunnel [14]. 2.1. <p> Appbt is a locally parallelized version of one of the NAS Parallel Benchmarks <ref> [4] </ref>. Laplace was developed at Wisconsin [17], and Wave is a proprietary code from the Schlumberger Corporation. The last column in Table 1 contains the total number of data pages touched by the applications.
Reference: 5. <author> F. J. Corbato, </author> <title> "A paging experiment with the Multics system," </title> <type> Technical Report MAC-M-384, </type> <institution> MIT, </institution> <month> May </month> <year> 1968. </year>
Reference-contexts: The address space is globally shared, with 4-Kbyte shared pages assigned to nodes round-robin. On a page fault the target operating system selects a victim page using the Clock <ref> [5] </ref> algorithm. To maintain inclusion, the system invalidates all cached blocks from the victim page. <p> In this experiment, each simulated processing node periodically invalidates a virtual memory page, to approximate the effect of having that page frame assigned to another process. Pages are selected for invalidation using the Clock <ref> [5] </ref> algorithm; pages that have not been referenced recently (as calculated by Clock) are candidates for invalidation. Pages are invalidated at random times (exponential interarrival time distribution with a mean of 512 ms). We timed the execution of each program in two ways.
Reference: 6. <author> P. J. Denning, </author> <title> "The working set model of program behavior," </title> <journal> Communications of the ACM, </journal> <volume> vol. 11, </volume> <pages> pp. 323-333, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: Application working set behavior In the next two sections we examine the memory reference patterns of the applications to identify why demand paging performs poorly. We first analyze the applications using the Working Set model of program behavior <ref> [6] </ref>, which describes memory access patterns in terms of localities. A program's working set at time t with parameter t is defined as the set of pages touched by the program during 18 D. BURGER, R. S. HYDER, B. P. MILLER, D. A.
Reference: 7. <author> J. Dongarra, G. A. Geist, R. Manchek, and V. S. Sunderam, </author> <title> "Integrated PVM framework supports heterogeneous network computing," </title> <journal> Computers in Physics, </journal> <volume> vol. 7, </volume> <pages> pp. 166-174, </pages> <month> March-April </month> <year> 1993. </year>
Reference-contexts: Furthermore, clusters of workstations are emerging as increasingly popular alternatives to dedicated parallel supercomputers [3], [19], <ref> [7] </ref>. Parallel applications on these systems must coexist with the operating system's demand-paged virtual memory. In this paper we examine the performance of seven shared-memory scientific applications and argue that demand paging has limited value on distributed-shared-memory parallel computers.
Reference: 8. <author> D. L. Eager, J. Zahorjan, and E. D. Lazowska, </author> <title> "Speedup versus efficiency in parallel systems," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 38, </volume> <pages> pp. 408-423, </pages> <month> Mar. </month> <year> 1989. </year>
Reference-contexts: BURGER, R. S. HYDER, B. P. MILLER, D. A. WOOD To address this problem, some previous studies have argued that parallel machines should employ gang scheduling, in which all processing nodes simultaneously switch to the same parallel job [9], [11], [13]. Others have argued for space-sharing <ref> [8] </ref>, in which processing nodes are dedicated to a parallel program until it completes. Space sharing can also be thought of as the limiting case of gang scheduling, with the scheduling quantum set to infinity.
Reference: 9. <author> D. G. Feitelson and L. Rudolph, </author> <title> "Gang scheduling performance benefits for fine-grained synchronization," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 16, </volume> <pages> pp. 306-318, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: BURGER, R. S. HYDER, B. P. MILLER, D. A. WOOD To address this problem, some previous studies have argued that parallel machines should employ gang scheduling, in which all processing nodes simultaneously switch to the same parallel job <ref> [9] </ref>, [11], [13]. Others have argued for space-sharing [8], in which processing nodes are dedicated to a parallel program until it completes. Space sharing can also be thought of as the limiting case of gang scheduling, with the scheduling quantum set to infinity.
Reference: 10. <author> L. Iftode, K. Li, and K. Petersen, </author> <title> "Memory servers for multicomputers," </title> <booktitle> in Proceedings of 1993 Spring CompCon, </booktitle> <pages> pp. 538-547, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: The high parallel context-switch overhead on current massively-parallel processors (e.g, the CM-5) is greater than the cycles lost by spinning. This is especially true when paging to a memory server <ref> [10] </ref> or fast paging device rather than a traditional disk. * The parallel programs we studied frequently access most of their data. <p> A lower page-fault service time would reduce both the magnitude of any synchronization delays and the frequency of these delays. T pf could be reduced using standard techniques such as faster disks or dedicated paging memory (e.g., "solid-state disks"). Alternatively, Iftode et al. <ref> [10] </ref> have proposed dedicating some processing nodes as "memory servers," which use their physical memories as fast paging stores. Such an approach could make use of the memory of idle or underutilized workstations in a network or cluster of workstations.
Reference: 11. <author> S. T. </author> <title> Leutenegger, Issues in multiprogrammed multiprocessor scheduling. </title> <type> Ph.D. thesis, </type> <institution> University of Wisconsin-Madison, </institution> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: BURGER, R. S. HYDER, B. P. MILLER, D. A. WOOD To address this problem, some previous studies have argued that parallel machines should employ gang scheduling, in which all processing nodes simultaneously switch to the same parallel job [9], <ref> [11] </ref>, [13]. Others have argued for space-sharing [8], in which processing nodes are dedicated to a parallel program until it completes. Space sharing can also be thought of as the limiting case of gang scheduling, with the scheduling quantum set to infinity.
Reference: 12. <author> S. T. Leutenegger and X.-H. Sun, </author> <title> "Distributed computing feasibility in a non-dedicated homogenous distributed system," </title> <booktitle> in Proceedings of ACM Supercomputing '93, </booktitle> <year> 1993. </year>
Reference-contexts: Even a single memory-constrained node can degrade performance by more than a factor of two. The frequency of (blocking or spinning) synchronization is key to determining the appropriate CPU and memory scheduling policies. Coarse-grained parallel applications|those with little synchronization|can be scheduled exactly as sequential tasks <ref> [12] </ref>. As synchronization grows more frequent, however, the impact of delaying any one node increases dramatically. A page fault on one node can cause cascading delays on other nodes.
Reference: 13. <author> J. K. Ousterhout, D. A. Scelza, and P. S. Sindhu, </author> <title> "Medusa: An experiment in distributed operating system structure," </title> <journal> Communications of the ACM, </journal> <volume> vol. 23, </volume> <pages> pp. 92-105, </pages> <month> Feb. </month> <year> 1980. </year> <note> 30 D. </note> <author> BURGER, R. S. HYDER, B. P. MILLER, D. A. </author> <note> WOOD </note>
Reference-contexts: BURGER, R. S. HYDER, B. P. MILLER, D. A. WOOD To address this problem, some previous studies have argued that parallel machines should employ gang scheduling, in which all processing nodes simultaneously switch to the same parallel job [9], [11], <ref> [13] </ref>. Others have argued for space-sharing [8], in which processing nodes are dedicated to a parallel program until it completes. Space sharing can also be thought of as the limiting case of gang scheduling, with the scheduling quantum set to infinity.
Reference: 14. <author> S. K. Reinhardt, M. D. Hill, J. R. Larus, A. R. Lebeck, J. C. Lewis, and D. A. Wood, </author> <title> "The Wisconsin Wind Tunnel: Virtual prototyping of parallel computers," </title> <booktitle> in Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 48-60, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The benchmark programs are scientific applications drawn from a variety of sources, including PAGING TRADEOFFS IN DISTRIBUTED-SHARED-MEMORY MULTIPROCESSORS 5 the Stanford SPLASH benchmarks [16], the NAS benchmarks [4], and the Schlum-berger Corporation. We simulated the execution of these programs on our hypothetical DSM system using the Wisconsin Wind Tunnel <ref> [14] </ref>. 2.1. A DSM machine model Our target hardware system contains 32 processing nodes, each with a 33-MHz SPARC CPU, 256-Kbyte cache (four-way associative, 32-byte blocks, random replacement), and the local portion of the distributed shared memory. <p> All applica tions use a locally modified version of the PARMACS macro package and assume a process-per-processor computation model (i.e., processes are always scheduled on the same processing node). 2.3. Simulation environment The Wisconsin Wind Tunnel (WWT) <ref> [14] </ref> is a parallel, discrete-event simulator for cache-coherent, shared-memory multiprocessors that runs on a Thinking Machines CM-5. By exploiting similarities between the hypothetical target system and the CM-5 host, WWT permits simulation of large applications.
Reference: 15. <author> E. Sharakan. </author> <type> Personal communication., </type> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: BURGER, R. S. HYDER, B. P. MILLER, D. A. WOOD mately 5 ms. Unfortunately, current massively parallel processors incur substantial overhead for a full parallel context switch. For example, the Thinking Machines CM-5 incurs a minimum overhead of 4 ms, with typical times closer to 10 ms <ref> [15] </ref>. Under these assumptions, the SPIN policy is clearly superior|for both throughput and latency|to the PCS policy. 4. Performance of virtual memory On a uniprocessor demand-paged virtual memory "automatically" manages physical memory and allows processes to execute with only a subset of their code and data pages resident in memory.
Reference: 16. <author> J. P. Singh, W.-D. Weber, and A. Gupta, </author> <title> "SPLASH: Stanford parallel applications for shared memory," </title> <journal> Computer Architecture News, </journal> <volume> vol. 20, </volume> <pages> pp. 5-44, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Methodology The performance studies in this paper are based on a hypothetical distributed-shared-memory (DSM) system running seven shared-memory programs. The benchmark programs are scientific applications drawn from a variety of sources, including PAGING TRADEOFFS IN DISTRIBUTED-SHARED-MEMORY MULTIPROCESSORS 5 the Stanford SPLASH benchmarks <ref> [16] </ref>, the NAS benchmarks [4], and the Schlum-berger Corporation. We simulated the execution of these programs on our hypothetical DSM system using the Wisconsin Wind Tunnel [14]. 2.1. <p> 1735 (256 bodies, 20 elem./body, 10 iter.) Locus VLSI standard cell router 671 (Primary1.grin) Mp3d Monte Carlo rarefied fluid flow 522 (32000 molecules, 50 iterations) Ocean Column-blocked 2D hydrodynamics 7613 (384 fi 384, 1 day) Wave 3D acoustic finite difference 2711 (48 fi 48 fi 48, 20 iterations) benchmark suite <ref> [16] </ref>. Appbt is a locally parallelized version of one of the NAS Parallel Benchmarks [4]. Laplace was developed at Wisconsin [17], and Wave is a proprietary code from the Schlumberger Corporation. The last column in Table 1 contains the total number of data pages touched by the applications.
Reference: 17. <author> F. Traenkle, </author> <title> "Parallel programming models and boundary integral equation methods for microstructure electrostatics," </title> <type> Master's thesis, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1993. </year>
Reference-contexts: Appbt is a locally parallelized version of one of the NAS Parallel Benchmarks [4]. Laplace was developed at Wisconsin <ref> [17] </ref>, and Wave is a proprietary code from the Schlumberger Corporation. The last column in Table 1 contains the total number of data pages touched by the applications.
Reference: 18. <author> D. Thiebaut and H. Stone, </author> <title> "Footprints in the cache," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 5, </volume> <pages> pp. 305-329, </pages> <month> Nov. </month> <year> 1987. </year>
Reference-contexts: In the remainder of this section, we focus on evaluating the tradeoffs between the SPIN and PCS policies. The efficacy of the LUG policy depends highly on its having a mix of parallel and sequential jobs and on the cache and TLB "footprint" <ref> [18] </ref> of the scheduled local process. While we believe such hybrid workloads may become common, characterizing their overheads is outside the scope of this paper.
Reference: 19. <author> S. Zhou, J. Wang, X. Zheng, and P. Delisle, </author> <title> "Utopia: A load sharing system for large, heterogeneous distributed computer systems," </title> <type> CSRI Technical Report 257, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: Furthermore, clusters of workstations are emerging as increasingly popular alternatives to dedicated parallel supercomputers [3], <ref> [19] </ref>, [7]. Parallel applications on these systems must coexist with the operating system's demand-paged virtual memory. In this paper we examine the performance of seven shared-memory scientific applications and argue that demand paging has limited value on distributed-shared-memory parallel computers.
References-found: 19

