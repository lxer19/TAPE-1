URL: ftp://ftp.cs.wisc.edu/wwt/pldi98_dataflow.ps
Refering-URL: http://www.cs.wisc.edu/~wwt/wwt_papers.html
Root-URL: 
Email: ammons@cs.wisc.edu larus@cs.wisc.edu  
Title: Improving Data-flow Analysis with Path Profiles  
Author: Glenn Ammons James R. Larus 
Address: 1210 West Dayton St. Madison, WI 53706  
Affiliation: Department of Computer Sciences University of Wisconsin-Madison  
Abstract: This paper describes a new approach to analyzing and optimizing programs, which improves the precision of data flow analysis along hot paths. Our technique identifies and duplicates hot paths, creating a hot path graph in which these paths are isolated. After flow analysis, the graph is reduced to eliminate unnecessary duplicates of unprofitable paths. In experiments on SPEC95 benchmarks, path qualification identified 2-112 times more non-local constants (weighted dynamically) than the Wegman-Zadek conditional constant algorithm, which translated into 1-7% more dynamic instructions with constant results. 
Abstract-found: 1
Intro-found: 1
Reference: [ABL97] <author> Glenn Ammons, Thomas Ball, and James R. Larus. </author> <title> Exploiting hardware performance counters with flow and context sensitive profiling. </title> <booktitle> In Proceedings of the SIGPLAN '97 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1997. </year> <title> 12 (a) go (b) Other benchmarks </title>
Reference-contexts: Even moderately large programs execute only a few tens of thousands of paths (out of a universe of billions of acyclic paths) and, moreover, programs' execution time and cost is concentrated in a far smaller subset of hot paths <ref> [BL96, ABL97] </ref>. This paper presents a new data-flow analysis technique that attempts to compute more precise solutions along the hot paths in a program. <p> Path profiles, along with a low-overhead algorithm to obtain them, are described in papers by Ball and Larus [BL96] and by Am-mons, Ball, and Larus <ref> [ABL97] </ref>. The acyclic paths recorded in a profile start and end at recording edges. The set of recording edges, R, is, at a minimum: edges from the entry vertex, edges into the exit vertex, and retreating edges. Thus, removing the recording edges turns G into an acyclic graph.
Reference: [Aho94] <author> Alfred V. Aho. </author> <title> Algorithms for finding patterns in strings. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume A, chapter 5, </booktitle> <pages> pages 255-300. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: They created an automaton in which infeasible paths ended in a failure state. The best solution at v, given that A is not in the failure state, is the meet over the non-failure states of A. For path qualification, we use the Aho-Corasick algorithm <ref> [Aho94] </ref> to construct an automaton that recognizes hot paths in a path profile. Holley and Rosen describe two techniques for solving qualified problems, data-flow tracing and context tupling. <p> The algorithm is an application of the Aho-Corasick algorithm for matching keywords in a string <ref> [Aho94] </ref>. In our case, the keywords are hot Ball-Larus paths. The constructed DFA is used as the qualification automaton for data-flow tracing. The Aho-Corasick algorithm begins by constructing a retrieval tree (also known as a trie) from a set of keywords.
Reference: [BA98] <author> Rastislav Bodk and Sadun Anik. </author> <title> Path-sensitive value-flow analysis. </title> <booktitle> In Proceedings of the SIGPLAN '98 Symposium on Principles of Programming Languages (POPL), </booktitle> <month> January </month> <year> 1998. </year>
Reference-contexts: This further explains why we did not see speedups for most of the benchmarks. In the above discussion, we assumed that the MOP is not attainable for constant propagation. This is true for the non-distributive Wegman-Zadek formulation. Recently, Bodk and Anik published a distributive formulation of constant propagation <ref> [BA98] </ref>. It would be interesting to compare path-qualified analysis against this formulation. 6.3 Cost of Path Qualification This section examines the cost of path qualification. 6.3.1 Cost of Duplication for go and that the reduction algorithm successfully controlled the increase in CFG size.
Reference: [BGS97a] <author> Rastislav Bodk, Rajiv Gupta, and Mary Lou Soffa. </author> <title> Interprocedural conditional branch elimination. </title> <booktitle> In Proceedings of the SIGPLAN '97 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 146-158, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Mueller and Whalley's code duplication algorithm can be seen as a qualification algorithm in which states in the qualification automaton encode information about the direction of the partially redundant branches. Bodk et al. used a limited form of interproce-dural analysis to detect redundant branches along interpro-cedural paths <ref> [BGS97a] </ref>. This work differs by incorporating paths into a more precise and general framework, by using paths to derive more precise data-flow analyses, and by using path frequencies to overcome the costs of exploiting increased precision (code duplication).
Reference: [BGS97b] <author> Rastislav Bodk, Rajiv Gupta, and Mary Lou Soffa. </author> <title> Refining data flow information using infeasible paths. </title> <booktitle> In Fifth ACM SIGSOFT Symposium on Foundations of Software Engineering and Sixth European Software Engineering Conference, </booktitle> <month> September </month> <year> 1997. </year>
Reference-contexts: Bodk et al. used a weaker (but less expensive) decision technique to determine if all paths between a definition and use were infeasible, and therefore the def-use pair actually did not exist <ref> [BGS97b] </ref>. Our work differs from these, as we focus on directly improving the precision of 10 (a) Local and unknowable (b) Other categories (c A = 1). program analysis along a subset of important paths, rather than improving analysis everywhere by eliminating spurious paths.
Reference: [BGS98] <author> Rastislav Bodk, Rajiv Gupta, and Mary Lou Soffa. </author> <title> Complete removal of redundant computations. </title> <booktitle> In Proceedings of the SIGPLAN '98 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <month> June </month> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: Bodk et al. presented an algorithm for complete partial redundancy elimination using both code motion and code duplication <ref> [BGS98] </ref>. Their technique also used profiles (ei-ther edge or path) to drive code duplication. Our paper is not directly comparable with their paper, as their paper used duplication to carry out an optimization while our paper uses duplication to improve analysis.
Reference: [BL96] <author> T. Ball and J. R. Larus. </author> <title> Efficient path profiling. </title> <booktitle> In Proceedings of MICRO 96, </booktitle> <pages> pages 46-57, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Even moderately large programs execute only a few tens of thousands of paths (out of a universe of billions of acyclic paths) and, moreover, programs' execution time and cost is concentrated in a far smaller subset of hot paths <ref> [BL96, ABL97] </ref>. This paper presents a new data-flow analysis technique that attempts to compute more precise solutions along the hot paths in a program. <p> To appear at the 1998 ACM SIGPLAN Conference on Programming Language Design and Implementation, Montreal, Canada, June 17-19, 1998. tions of a program. Path-qualified data-flow analysis consists of the following steps: 1. Identify hot paths by profiling a program. We use a Ball-Larus path profile <ref> [BL96] </ref> to determine how often acyclic paths in a program execute. 2. Identify and isolate the hot paths in the program's control-flow graph (CFG). This step produces a new CFG in which each hot path is duplicated. <p> Path profiles, along with a low-overhead algorithm to obtain them, are described in papers by Ball and Larus <ref> [BL96] </ref> and by Am-mons, Ball, and Larus [ABL97]. The acyclic paths recorded in a profile start and end at recording edges. The set of recording edges, R, is, at a minimum: edges from the entry vertex, edges into the exit vertex, and retreating edges.
Reference: [Fis81] <author> Joseph A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(7):478-490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: However, the two techniques are certainly complementary, as our technique would work well in a CFG from which infeasible paths were eliminated. Paths have long been used in program analysis and optimization. Fisher's trace scheduling technique heavily optimized the hot paths (called traces) in a CFG <ref> [Fis81] </ref>. Trace scheduling did not duplicate paths, instead it introduced fixup code along control flow edges into or out of the middle of a trace.
Reference: [Gri73] <author> David Gries. </author> <title> Describing an algorithm by hopcroft. </title> <journal> Acta Informatica, </journal> <volume> 2 </volume> <pages> 97-109, </pages> <year> 1973. </year>
Reference-contexts: In the example, since H13 and H14 are the only hot vertices, is fEntry*g; fA0g; fB0; B1g; fC*; C3g; fD2; D4g; fE*; E5; E6; E7g; fF*; F8; F10; F11g; fG*; G9g; fExit0g 3. Use the standard DFA minimization algorithm <ref> [Gri73] </ref> to produce a partition 0 which respects the data-flow solutions. The complexity of this algorithm is O (n log n). Why is this algorithm applicable? The HPG can be thought of as a finite automaton with edges labelled by the edges of the original graph.
Reference: [GWZ94] <author> Allen Goldberg, T. C. Wang, and David Zimmer man. </author> <title> Applications of feasible path analysis to program testing. </title> <booktitle> In International Symposium on Software Testing and Analysis. ACM SIGSOFT, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Holley and Rosen introduced qualified data-flow analysis to separate known infeasible paths from the remaining paths, some of which might be feasible [HR81]. Goldberg et al. applied theorem proving techniques to identify infeasible paths in testing a program's path coverage <ref> [GWZ94] </ref>. Bodk et al. used a weaker (but less expensive) decision technique to determine if all paths between a definition and use were infeasible, and therefore the def-use pair actually did not exist [BGS97b].
Reference: [HR81] <author> L. Howard Holley and Barry K. Rosen. </author> <title> Qualified data flow problems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-7(1):60-78, </volume> <month> January </month> <year> 1981. </year>
Reference-contexts: On go, the hot-path graphs were 184% larger and the reduced hot-path graphs 70% larger. 1.1 Qualified Flow Analysis Our implementation is based on Holley and Rosen's qualified flow analysis technique <ref> [HR81] </ref>. A qualified data-flow problem is a conventional data-flow problem together with a deterministic finite automaton, A, whose transitions are labelled by the edges of the control-flow graph, G. A encodes additional information about the program|in path-qualified analysis, it recognizes hot paths. <p> ; e 1 ; : : : ; e k ] in G to a function f : L ! L: f = M (p) = M (e k ) ffi M (e k1 ) ffi ffi M (e 0 ) The next three definitions come from Holley and Rosen <ref> [HR81] </ref>. Definition 2 A solution I of D is a map I : V ! L such that, for any path p from r to a vertex u, I (u) (M (p))(l r ). <p> D A is a data-flow problem that can be solved by conventional means. Holley and Rosen prove the following theorem <ref> [HR81, Theorem 4.2] </ref>: Theorem 1 If I A is a good solution of D A , then the solution I of D given by I (v) = ^fI A ((u; q)) : (u; q) 2 V A ; u = vg, for all v 2 V , is a good solution <p> Lemma 2 would fail if recording edges targeted more than one B vertex. This could happen if, for example, tracing were allowed to unroll loops. 4.3 Context Tupling In their original paper on qualified data-flow problems, Hol-ley and Rosen presented two techniques for solving qualified problems <ref> [HR81] </ref>. Their first technique, data-flow tracing, has already been discussed. Their other technique is context tupling. While data-flow tracing solves a conventional data-flow problem over an expanded graph, context tupling solves a "tupled" data-flow problem over the original graph. <p> Holley and Rosen introduced qualified data-flow analysis to separate known infeasible paths from the remaining paths, some of which might be feasible <ref> [HR81] </ref>. Goldberg et al. applied theorem proving techniques to identify infeasible paths in testing a program's path coverage [GWZ94].
Reference: [MW95] <author> Frank Mueller and David B. Whalley. </author> <title> Avoiding conditional branches by code replication. </title> <booktitle> In Proceedings of the SIGPLAN '95 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 56-66, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Finally, both scheduling techniques attempted to maximize the size of traces. This work evaluates the improvement from duplication, and eliminates duplicated blocks that provide little or no improvement. Mueller and Whalley used an ad-hoc framework and code duplication to eliminate certain partially redundant branches <ref> [MW95] </ref>. Mueller and Whalley's code duplication algorithm can be seen as a qualification algorithm in which states in the qualification automaton encode information about the direction of the partially redundant branches. Bodk et al. used a limited form of interproce-dural analysis to detect redundant branches along interpro-cedural paths [BGS97a].
Reference: [mWHMC + 93] <author> Wen mei W. Hwu, Scott A. Mahlke, William Y. Chen, Pohua P. Chang, Nancy J. Warter, Roger A. Bringmann, Roland G. Ouellette, Richard E. Hank, Tokuzo Kiyohara, Grant E. Haab, John G. Holm, and Daniel M. Lavery. </author> <title> The superblock: An effective technique for VLIW and superscalar compilation. </title> <journal> The Journal of Supercomputing, </journal> <volume> 7(1-2):229-248, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: For example, go had a good speedup, but its size increased by the largest amount (Figure 11). To be fair, this experiment did not control for several significant factors. First, the IMPACT group's work on superblock scheduling <ref> [mWHMC + 93] </ref> found that tail duplication, like that done to isolate hot paths, can expose large amounts of instruction-level parallelism. Thus, running time improvements are not necessarily due to improvements in constant propagation. <p> More recently, Hwu et al. eliminated this fixup code by duplicating paths to form superblocks, which is a collection of traces without control flow into the middle of a trace <ref> [mWHMC + 93] </ref>. Our approach differs from both techniques. First, it is a technique for improving program analysis, not a technique for optimization and instruction scheduling. Second, although it duplicates paths, like superblocks, its duplication is guided by path profiles.
Reference: [Ram96] <author> G. Ramalingam. </author> <title> Data flow frequency analysis. </title> <booktitle> In Proceedings of the SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 267-277, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: By contrast, their approach starts and performs analysis over a smaller graph. Our approach, however, can find solutions not found by a meet-over-all-paths analysis. Ramalingam combined data-flow analysis with program frequency information by associating probabilities with data-flow values and developing a data-flow framework for combining these pairs of values <ref> [Ram96] </ref>. Our goal differs. Instead of incorporating frequencies into the meet-over-all-paths framework, we use frequency information to improve analysis precision in heavily executed code. 8 Conclusion This paper describes a new approach to analyzing and optimizing programs.
Reference: [WFW + ] <author> Robert P. Wilson, Robert S. French, Christopher S. Wilson, Saman P. Amarasinghe, Jennifer M. An-derson, Steve W. K. Tjiang, Shih-Wei Liao, Chau-Wen Tseng, Mary W. Hall, Monica S. Lam, and John L. Hennessy. </author> <title> An overview of the SUIF compiler system. </title> <note> Published on the World Wide Web at http://suif.stanford.edu/suif/suif1/suif-overview/suif.html. </note>
Reference-contexts: We implemented the analysis as two new passes in the SUIF compiler <ref> [WFW + ] </ref>. The first pass, PP, instrumented a C program for path profiling. The other pass, PW, used a path profile to perform path-qualified constant propagation. The first step was to produce a path profile for each routine in the program.
Reference: [WZ91] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> April </month> <year> 1991. </year> <month> 13 </month>
Reference-contexts: As SUIF did not directly generate assembly or machine code, our evaluations are in terms of the SUIF intermediate code. In this paper, by "instruction" we always mean SUIF instructions, not machine instructions. The constant propagator in PW uses Wegman and Zadek's Conditional Constant algorithm <ref> [WZ91] </ref>. This algorithm is a worklist algorithm that symbolically executes a routine, starting at its entry node and propagating values only across the legs of branches that can execute, given the current assignment of values to variables.
References-found: 16

