URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr94/tr94-038.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr94-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: AN UNSYMMETRIC-PATTERN MULTIFRONTAL METHOD FOR SPARSE LU FACTORIZATION (Sparse matrices), 65F05 (Direct methods for linear
Author: TIMOTHY A. DAVIS AND IAIN S. DUFF 
Keyword: Key words. LU factorization, unsymmetric sparse matrices, multifrontal methods  
Note: AMS subject classifications. 65F50  
Abstract: Computer and Information Sciences Dept., University of Florida, Technical Report TR-94-038, November, 1994. Abstract. Sparse matrix factorization algorithms for general problems are typically characterized by irregular memory access patterns that limit their performance on parallel-vector supercomputers. For symmetric problems, methods such as the multifrontal method avoid indirect addressing in the innermost loops by using dense matrix kernels. However, no efficient LU factorization algorithm based primarily on dense matrix kernels exists for matrices whose pattern is very unsymmetric. We address this deficiency and present a new unsymmetric-pattern multifrontal method based on dense matrix kernels. As in the classical multifrontal method, advantage is taken of repetitive structure in the matrix by factorizing more than one pivot in each frontal matrix thus enabling the use of Level 2 and Level 3 BLAS. The performance is compared with the classical multifrontal method and other unsymmetric solvers on a CRAY YMP. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. R. Amestoy and I. S. Duff, </author> <title> Vectorization of a multiprocessor multifrontal code, </title> <journal> Int. J. Supercomputer Appl., </journal> <volume> 3 (1989), </volume> <pages> pp. 41-59. </pages>
Reference-contexts: In contrast, the multifrontal method of Duff and Reid [9, 10, 14, 15] is designed with regular memory access in the innermost loops and has been modified by Amestoy and Duff to use standard kernels <ref> [1] </ref>. This multifrontal method assumes structural symmetry and bases the factorization on an assembly tree generated from the original matrix and an ordering such as minimum degree. <p> Section 5 presents a full outline of the algorithm, using the notation introduced in previous sections. In Section 6, we compare the performance of our algorithm with two algorithms based on the classical multifrontal method: MUPS <ref> [1, 2] </ref> and SSGETRF [3], and two algorithms based on conventional (compressed sparse vector) data structures: Gilbert and Peierls' partial-pivoting code (GPLU [23]) and MA48 [16] (a successor to MA28 [13]). GPLU does not use dense matrix kernels. <p> Algorithm. Algorithm 3 is a full outline of the UMFPACK (version 1.0) algorithm. 6. Performance results. In this section, we compare the performance of UMFPACK version 1.0 with MUPS <ref> [1] </ref>, MA48 [16], GPLU [23], and SSGETRF [3] on a single processor of a CRAY YMP (although MUPS and SSGETRF are parallel codes). Each method has a set of input parameters that control its behavior. <p> MUPS performs a minimum degree ordering and symbolic factorization on the nonzero pattern of A + A T , and constructs an assembly tree for the numerical factorization phase <ref> [1] </ref>. During numerical factorization, candidate pivot entries must pass a threshold partial pivoting test similar to Equation (3.1), except that the test is by rows instead of by columns.
Reference: [2] <author> P. R. Amestoy and I. S. Duff, MUPS: </author> <title> a parallel package for solving sparse unsymmetric sets of linear equations, </title> <type> Tech. </type> <note> Report (to appear), </note> <institution> CERFACS, Toulouse, France, </institution> <year> 1994. </year> <title> 20 T. </title> <editor> A. DAVIS AND I. S. </editor> <publisher> DUFF </publisher>
Reference-contexts: Section 5 presents a full outline of the algorithm, using the notation introduced in previous sections. In Section 6, we compare the performance of our algorithm with two algorithms based on the classical multifrontal method: MUPS <ref> [1, 2] </ref> and SSGETRF [3], and two algorithms based on conventional (compressed sparse vector) data structures: Gilbert and Peierls' partial-pivoting code (GPLU [23]) and MA48 [16] (a successor to MA28 [13]). GPLU does not use dense matrix kernels.
Reference: [3] <author> Cray Research, Inc., </author> <title> SSGETRF factors a real sparse general matrix, (online manual), </title> <institution> Cray Research, Inc., Eagan, Minn., </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Section 5 presents a full outline of the algorithm, using the notation introduced in previous sections. In Section 6, we compare the performance of our algorithm with two algorithms based on the classical multifrontal method: MUPS [1, 2] and SSGETRF <ref> [3] </ref>, and two algorithms based on conventional (compressed sparse vector) data structures: Gilbert and Peierls' partial-pivoting code (GPLU [23]) and MA48 [16] (a successor to MA28 [13]). GPLU does not use dense matrix kernels. <p> Algorithm. Algorithm 3 is a full outline of the UMFPACK (version 1.0) algorithm. 6. Performance results. In this section, we compare the performance of UMFPACK version 1.0 with MUPS [1], MA48 [16], GPLU [23], and SSGETRF <ref> [3] </ref> on a single processor of a CRAY YMP (although MUPS and SSGETRF are parallel codes). Each method has a set of input parameters that control its behavior. We used the recommended defaults for most of these, with a few exceptions that we indicate below.
Reference: [4] <author> T. A. Davis, </author> <title> Users' guide to the unsymmetric-pattern multifrontal package (UMFPACK), </title> <type> Tech. Report TR-93-020, </type> <institution> CIS Dept., Univ. of Florida, </institution> <address> Gainesville, FL, </address> <year> 1993. </year> <title> For a copy of UMFPACK, send e-mail to netlib@ornl.gov with the one-line message send umfpack.shar from linalg. </title>
Reference-contexts: In the current work we do not explicitly use this structure. We have developed a new unsymmetric-pattern multifrontal approach <ref> [4, 5] </ref>. As in the symmetric multifrontal case, advantage is taken of repetitive structure in the matrix by factorizing more than one pivot in each frontal matrix. Thus the algorithm can use higher level dense matrix kernels in its innermost loops (Level 3 BLAS [6]). <p> Thus the algorithm can use higher level dense matrix kernels in its innermost loops (Level 3 BLAS [6]). We refer to the unsymmetric-pattern multifrontal method described in this paper as UMFPACK version 1.0 <ref> [4] </ref>. It is is available in Netlib [7]. A parallel factorize-only version of UMFPACK, based on the assembly dag, is discussed in Hadfield's dissertation [24, 26, 27, 25]. The multifrontal method for symmetric positive definite matrices is reviewed in [29].
Reference: [5] <author> T. A. Davis and I. S. Duff, </author> <title> Unsymmetric-pattern multifrontal methods for parallel sparse LU factorization, </title> <type> Tech. Report TR-91-023, </type> <institution> CIS Dept., Univ. of Florida, </institution> <address> Gainesville, FL, </address> <year> 1991. </year>
Reference-contexts: A. DAVIS AND I. S. DUFF an assembly tree and the more general structure of an assembly dag (directed acyclic graph) <ref> [5] </ref> similar to that of Gilbert and Liu [22] and Eisenstat and Liu [17, 18] is required. In the current work we do not explicitly use this structure. We have developed a new unsymmetric-pattern multifrontal approach [4, 5]. <p> In the current work we do not explicitly use this structure. We have developed a new unsymmetric-pattern multifrontal approach <ref> [4, 5] </ref>. As in the symmetric multifrontal case, advantage is taken of repetitive structure in the matrix by factorizing more than one pivot in each frontal matrix. Thus the algorithm can use higher level dense matrix kernels in its innermost loops (Level 3 BLAS [6]).
Reference: [6] <author> J. J. Dongarra, J. J. Du Croz, I. S. Duff, and S. Hammarling, </author> <title> A set of Level 3 Basic Linear Algebra Subprograms., </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16 (1990), </volume> <pages> pp. 1-17. </pages>
Reference-contexts: As in the symmetric multifrontal case, advantage is taken of repetitive structure in the matrix by factorizing more than one pivot in each frontal matrix. Thus the algorithm can use higher level dense matrix kernels in its innermost loops (Level 3 BLAS <ref> [6] </ref>). We refer to the unsymmetric-pattern multifrontal method described in this paper as UMFPACK version 1.0 [4]. It is is available in Netlib [7]. A parallel factorize-only version of UMFPACK, based on the assembly dag, is discussed in Hadfield's dissertation [24, 26, 27, 25]. <p> Instead, they are delayed until there are updates pending from b pivots to allow the efficient use of Level 3 BLAS <ref> [6] </ref>. On a CRAY YMP, a good value for the parameter b is 16. Let b L and b U denote the portions of L 00 and U 00 , respectively, whose updates have yet to be fully applied to C.
Reference: [7] <author> J. J. Dongarra and E. Grosse, </author> <title> Distribution of mathematical software via electronic mail, </title> <journal> Comm. ACM, </journal> <volume> 30 (1987), </volume> <pages> pp. 403-407. </pages>
Reference-contexts: Thus the algorithm can use higher level dense matrix kernels in its innermost loops (Level 3 BLAS [6]). We refer to the unsymmetric-pattern multifrontal method described in this paper as UMFPACK version 1.0 [4]. It is is available in Netlib <ref> [7] </ref>. A parallel factorize-only version of UMFPACK, based on the assembly dag, is discussed in Hadfield's dissertation [24, 26, 27, 25]. The multifrontal method for symmetric positive definite matrices is reviewed in [29]. Section 2 presents an overview of the basic approach, and a brief outline of the algorithm.
Reference: [8] <author> I. S. Duff, </author> <title> On algorithms for obtaining a maximum transversal, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7 (1981), </volume> <pages> pp. </pages> <month> 315-330. </month> <title> [9] , Parallel implementation of multifrontal schemes, </title> <booktitle> Parallel Computing, 3 (1986), </booktitle> <pages> pp. 193-204. </pages>
Reference-contexts: When the matrix becomes dense enough near the end of factorization (default of 50% dense), MA48 switches to a dense factorization code. MA48 can preorder a matrix to block upper triangular form (always preceded by finding a maximum transversal <ref> [8] </ref>), and then factorize each block on the diagonal [12]. Off-diagonal blocks do not suffer fill-in. <p> MUPS optionally preorders a matrix to maximize the modulus of the smallest entry on the diagonal (using a maximum transversal algorithm <ref> [8] </ref>). MUPS always attempts to preserve symmetry. It does not permute the matrix to block upper triangular form. SSGETRF is a classical multifrontal method in the Cray Research, Inc., library (version 1.1) installed on the CRAY YMP.
Reference: [10] <author> I. S. Duff, A. M. Erisman, and J. K. Reid, </author> <title> Direct Methods for Sparse Matrices, </title> <publisher> London: Oxford Univ. Press, </publisher> <year> 1986. </year>
Reference-contexts: 1. Introduction. Conventional sparse matrix factorization algorithms for general problems rely heavily on indirect addressing. This gives them an irregular memory access pattern that limits their performance on typical parallel-vector supercomputers and on cache-based RISC architectures. In contrast, the multifrontal method of Duff and Reid <ref> [9, 10, 14, 15] </ref> is designed with regular memory access in the innermost loops and has been modified by Amestoy and Duff to use standard kernels [1]. <p> F) numerically update part of C (Level 2 and Level 3 BLAS) local pivot search within C endwhile 3: complete the factorization of F using Level 3 BLAS endwhile The initialization phase of the algorithm (step 0) converts the original matrix into two compressed sparse vector forms (row-oriented and column-oriented <ref> [10] </ref>) with numerical values, A, and symbolic pattern, A. Rows and columns are used and deleted from A and A during factorization when they are assembled into frontal matrices. <p> Among those nsrch columns, we select as pivot the entry a 0 rc with the smallest approximate Markowitz cost [30], (d r (r) 1)(d c (c) 1), such that a 0 rc also satisfies a threshold partial pivoting condition <ref> [10] </ref> ja 0 i ic j; 0 &lt; u 1:(3.1) Note that we have the true column degree since the column entries were generated explicitly to enable the threshold test in Equation (3.1). When the pivot is chosen its row and column structure define the frontal matrix. <p> GPLU is always improved however trivial the BTF is. Symmetry is usually worth preserving if the pattern is symmetric, or moderately so. One class of matrices for which this is not so are those with zeros on the diagonal. We note that none of these methods use 2-by-2 pivots <ref> [10] </ref> and so are unable to preserve symmetry if there are zeros on the diagonal that are not filled by earlier pivot steps. If MA48 is unable to find stable diagonal pivots when the diagonal pivoting option is requested, it immediately switches to full code.
Reference: [11] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis, </author> <title> Users' guide for the Harwell-Boeing sparse matrix collection (Release 1), </title> <type> Tech. Report RAL-92-086, </type> <institution> Rutherford Appleton Laboratory, </institution> <address> Didcot, Oxon, England, </address> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: The table is subdivided by discipline, and disciplines are in order of decreasing average symmetry of the matrices in that discipline. Matrices within a discipline are ordered by size (n). All matrices are available via anonymous ftp. They include matrices from the Harwell/Boeing collection <ref> [11] </ref> (at orion.cerfacs.fr or numerical.cc.rl.ac.uk) and Saad's SPARSKIT2 collection (at ftp.cs.umn.edu). All other test matrices in the table are available from ftp.cis.ufl.edu:pub/umfpack/matrices. All petroleum engineering problems listed are from oil reservoir simulations. The best time from runs listed in Table 6.1 for each method is shown in Table 6.3.

Reference: [17] <author> S. C. Eisenstat and J. W. H. Liu, </author> <title> Exploiting structural symmetry in unsymmetric sparse symbolic factorization, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 (1992), </volume> <pages> pp. </pages> <month> 202-211. </month> <title> [18] , Exploiting structural symmetry in a sparse partial pivoting code, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 253-257. </pages>
Reference-contexts: A. DAVIS AND I. S. DUFF an assembly tree and the more general structure of an assembly dag (directed acyclic graph) [5] similar to that of Gilbert and Liu [22] and Eisenstat and Liu <ref> [17, 18] </ref> is required. In the current work we do not explicitly use this structure. We have developed a new unsymmetric-pattern multifrontal approach [4, 5]. As in the symmetric multifrontal case, advantage is taken of repetitive structure in the matrix by factorizing more than one pivot in each frontal matrix.
Reference: [19] <author> A. George and J. W. H. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice-Hall, </publisher> <year> 1981. </year> <title> [20] , The evolution of the minimum degree ordering algorithm, </title> <journal> SIAM Review, </journal> <volume> 31 (1989), </volume> <pages> pp. 1-19. </pages>
Reference-contexts: 0 l$ X [C e ] mfl A l$ A k with pattern Struct (A 0 ifl ) = [ U e [ A k There is an interesting correspondence between our data structures and George and Liu's quotient graph representation of the factorization of a symmetric positive definite matrix <ref> [19] </ref>. Suppose we factorize a symmetric positive definite matrix using our algorithm and restrict the pivots to the diagonal. <p> The true degree d c (j) = jStruct (A 0 flj )j = jAdj G k (x j )j is the degree of node x j in the implicitly represented elimination graph, G k <ref> [19] </ref>. If indistinguishable uneliminated nodes are present in the quotient graph (as used in [28], for example), both of these time complexity bounds are reduced, but computing the true degree still takes much more time than computing our approximate degree.
Reference: [21] <author> A. George and E. Ng, </author> <title> An implementation of Gaussian elimination with partial pivoting for sparse systems, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 6 (1985), </volume> <pages> pp. 390-409. </pages>
Reference-contexts: It factorizes A using threshold partial pivoting with row interchanges only. We first explicitly form A T A, find a fill-reducing ordering via Liu's multiple minimum degree algorithm [20, 28], and use that permutation as the column order for A (as suggested in <ref> [21] </ref>). The time we report includes this analysis phase. We also tested GPLU on the block upper triangular form of A (as found by MA48), by applying GPLU and our preordering to each block on the diagonal. GPLU does not have an option for preserving symmetry.
Reference: [22] <author> J. R. Gilbert and J. W. H. Liu, </author> <title> Elimination structures for unsymmetric sparse LU factors, </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 14 (1993), </volume> <pages> pp. 334-354. </pages>
Reference-contexts: A. DAVIS AND I. S. DUFF an assembly tree and the more general structure of an assembly dag (directed acyclic graph) [5] similar to that of Gilbert and Liu <ref> [22] </ref> and Eisenstat and Liu [17, 18] is required. In the current work we do not explicitly use this structure. We have developed a new unsymmetric-pattern multifrontal approach [4, 5].
Reference: [23] <author> J. R. Gilbert and T. Peierls, </author> <title> Sparse partial pivoting in time proportional to arithmetic operations, </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 9 (1988), </volume> <pages> pp. 862-874. </pages>
Reference-contexts: In Section 6, we compare the performance of our algorithm with two algorithms based on the classical multifrontal method: MUPS [1, 2] and SSGETRF [3], and two algorithms based on conventional (compressed sparse vector) data structures: Gilbert and Peierls' partial-pivoting code (GPLU <ref> [23] </ref>) and MA48 [16] (a successor to MA28 [13]). GPLU does not use dense matrix kernels. MA48 uses dense matrix kernels only after switching to a dense factorization code towards the end of factorization when the active submatrix is fairly dense. 2. The basic approach. <p> Algorithm. Algorithm 3 is a full outline of the UMFPACK (version 1.0) algorithm. 6. Performance results. In this section, we compare the performance of UMFPACK version 1.0 with MUPS [1], MA48 [16], GPLU <ref> [23] </ref>, and SSGETRF [3] on a single processor of a CRAY YMP (although MUPS and SSGETRF are parallel codes). Each method has a set of input parameters that control its behavior. We used the recommended defaults for most of these, with a few exceptions that we indicate below. <p> Like MUPS, it always preserves symmetry, and does not permute the matrix to block upper triangular form. The GPLU code of Gilbert and Peierls <ref> [23] </ref> does not include a pre-ordering phase. It factorizes A using threshold partial pivoting with row interchanges only.
Reference: [24] <author> S. M. Hadfield, </author> <title> On the LU Factorization of Sequences of Identically Structured Sparse Matrices within a Distributed Memory Environment, </title> <type> PhD thesis, </type> <institution> Computer and Information Sciences Department, University of Florida, Gainesville, Florida, 1994. (Also Univ. of Fl. </institution> <note> tech report TR-94-019). </note>
Reference-contexts: We refer to the unsymmetric-pattern multifrontal method described in this paper as UMFPACK version 1.0 [4]. It is is available in Netlib [7]. A parallel factorize-only version of UMFPACK, based on the assembly dag, is discussed in Hadfield's dissertation <ref> [24, 26, 27, 25] </ref>. The multifrontal method for symmetric positive definite matrices is reviewed in [29]. Section 2 presents an overview of the basic approach, and a brief outline of the algorithm. <p> We construct our rectangular frontal matrices dynamically, since we do not know their structure prior to factorization. Although based on an assembly dag that can be constructed during this analyze-factorize phase, we do not use it here although Hadfield and Davis <ref> [24, 26, 27, 25] </ref> develop it further and use it in a factorize-only algorithm. <p> The factorize-only option in UMFPACK version 1.0 is not as fast as it could be since most of our library-code development effort has gone towards the combined analysis-factorize phase. The parallel factorize-only code <ref> [24, 26, 27] </ref> is not included in UMFPACK version 1.0. Overall, these results show that the unsymmetric-pattern multifrontal method is a competitive algorithm when compared with both the classical multifrontal approach (MUPS and SSGETRF) and algorithms based on more conventional sparse matrix data structures (MA48 and GPLU). Acknowledgments.
Reference: [25] <author> S. M. Hadfield and T. A. Davis, </author> <title> Lost pivot recovery for an unsymmetric-pattern multifrontal method, </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <note> (1994). (submitted. Also Univ. of Fl. tech report TR-94-029). [26] , A parallel unsymmetric-pattern multifrontal method, SIAM J. Sci. Computing, (1994). (submitted. Also Univ. </note> <author> of Fl. </author> <title> tech report TR-94-028). [27] , Potential and achievable parallelism in the unsymmetric-pattern multifrontal LU factorization method for sparse matrices, </title> <booktitle> in Proceedings of the Fifth SIAM Conf. on Applied Linear Algebra, </booktitle> <address> Snowbird, Utah, </address> <year> 1994, </year> <note> SIAM. (Also Univ. of Fl. tech report TR-94-006). </note>
Reference-contexts: We refer to the unsymmetric-pattern multifrontal method described in this paper as UMFPACK version 1.0 [4]. It is is available in Netlib [7]. A parallel factorize-only version of UMFPACK, based on the assembly dag, is discussed in Hadfield's dissertation <ref> [24, 26, 27, 25] </ref>. The multifrontal method for symmetric positive definite matrices is reviewed in [29]. Section 2 presents an overview of the basic approach, and a brief outline of the algorithm. <p> We construct our rectangular frontal matrices dynamically, since we do not know their structure prior to factorization. Although based on an assembly dag that can be constructed during this analyze-factorize phase, we do not use it here although Hadfield and Davis <ref> [24, 26, 27, 25] </ref> develop it further and use it in a factorize-only algorithm.
Reference: [28] <author> J. W. H. Liu, </author> <title> Modification of the minimum-degree algorithm by multiple elimination, </title> <journal> ACM UNSYMMETRIC-PATTERN MULTIFRONTAL METHOD 21 Trans. Math. Softw., </journal> <volume> 11 (1985), </volume> <pages> pp. </pages> <month> 141-153. </month> <title> [29] , The multifrontal method for sparse matrix solution: </title> <journal> Theory and practice, SIAM Review, </journal> <volume> 34 (1992), </volume> <pages> pp. 82-109. </pages>
Reference-contexts: The true degree d c (j) = jStruct (A 0 flj )j = jAdj G k (x j )j is the degree of node x j in the implicitly represented elimination graph, G k [19]. If indistinguishable uneliminated nodes are present in the quotient graph (as used in <ref> [28] </ref>, for example), both of these time complexity bounds are reduced, but computing the true degree still takes much more time than computing our approximate degree. <p> MUPS always attempts to preserve symmetry. It does not permute the matrix to block upper triangular form. SSGETRF is a classical multifrontal method in the Cray Research, Inc., library (version 1.1) installed on the CRAY YMP. It uses Liu's multiple minimum degree (MMD) algorithm <ref> [28] </ref> on the pattern of A + A T . It includes a threshold partial pivoting test. It is not specified in the documentation, but from our results we conclude that SSGETRF always uses a maximum transversal algorithm. <p> The GPLU code of Gilbert and Peierls [23] does not include a pre-ordering phase. It factorizes A using threshold partial pivoting with row interchanges only. We first explicitly form A T A, find a fill-reducing ordering via Liu's multiple minimum degree algorithm <ref> [20, 28] </ref>, and use that permutation as the column order for A (as suggested in [21]). The time we report includes this analysis phase.
Reference: [30] <author> H. M. Markowitz, </author> <title> The elimination form of the inverse and its application to linear programming, </title> <booktitle> Management Science, 3 (1957), </booktitle> <pages> pp. 255-269. </pages>
Reference-contexts: The number searched is controlled by an input parameter (which we denote by nsrch and whose default value is four). Among those nsrch columns, we select as pivot the entry a 0 rc with the smallest approximate Markowitz cost <ref> [30] </ref>, (d r (r) 1)(d c (c) 1), such that a 0 rc also satisfies a threshold partial pivoting condition [10] ja 0 i ic j; 0 &lt; u 1:(3.1) Note that we have the true column degree since the column entries were generated explicitly to enable the threshold test in
Reference: [31] <author> E. Ng, </author> <title> A comparison of some direct methods for solving sparse nonsymmetric linear systems, </title> <booktitle> in Proceedings of the Fifth SIAM Conf. on Applied Linear Algebra, </booktitle> <address> Snowbird, Utah, </address> <year> 1994, </year> <note> SIAM. </note>
Reference-contexts: The peak performance of GPLU was 1.9 Mflops (for the bcsstk08 matrix), primarily because its inner-most loops do not readily vectorize (even with the appropriate compiler directives). This is not a fundamental limitation of GPLU, however. Ng <ref> [31] </ref> reports that GPLU attains a much higher relative performance on an IBM RS/6000. For example, GPLU is faster than UMFPACK on the RS/6000 for the lns 3937 matrix, although the fastest BLAS were not used in his comparisons.
Reference: [32] <author> Z. Zlatev, </author> <title> On some pivotal strategies in Gaussian elimination by sparse technique, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 17 (1980), </volume> <pages> pp. 18-30. </pages> <note> Note: all University of Florida technical reports in this list of references are available in postscript form via anonymous ftp to ftp.cis.ufl.edu in the directory cis/tech-reports. </note>
Reference-contexts: At a particular stage, the frontal matrix is initialized through choosing a pivot from all the active matrix (called a global pivot search) using a Zlatev-style pivot search <ref> [32] </ref>, except that we keep track of upper bounds on the degrees of rows and columns in the active submatrix, rather than the true degrees (the degree of a row or column is simply the number of entries in the row or column). <p> Step 1: Perform global pivot search and form frontal matrix. The algorithm performs pivoting both to maintain numerical stability and to reduce fill-in. The first pivot in each frontal matrix is chosen using a global Zlatev-style search <ref> [32] </ref>. A few candidate columns with the lowest upper bound degrees are searched. The number searched is controlled by an input parameter (which we denote by nsrch and whose default value is four).
References-found: 21

