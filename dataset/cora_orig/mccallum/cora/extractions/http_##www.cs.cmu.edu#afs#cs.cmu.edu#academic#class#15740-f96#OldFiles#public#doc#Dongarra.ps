URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15740-f96/OldFiles/public/doc/Dongarra.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15740-f96/OldFiles/public/doc/
Root-URL: http://www.cs.cmu.edu
Title: An Introduction to the MPI Standard  
Author: Jack J. Dongarra Steve W. Otto Marc Snir David Walker 
Date: April 29, 1995  
Affiliation: University of Tennessee and Oak Ridge National Laboratory  Oregon Graduate Institute of Science Technology  IBM, T.J. Watson Research Center  Oak Ridge National Laboratory  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. Butler and E. Lusk. </author> <title> Monitors, Messages, and Clusters: The P4 Parallel Programming System. </title> <journal> Parallel Computing, </journal> <volume> 20 </volume> <pages> 547-64, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Other sources of information on MPI are available or are under development (see sidebar B). Researchers incorporated into MPI the most useful features of several systems, rather than choosing one system to adopt as the standard. MPI has roots in PVM [3, 6], Express [9], P4 <ref> [1] </ref>, Zipcode [10], and Parmacs [2], and in systems sold by IBM, Intel, Meiko, Cray Research, and Ncube. 2 Overview MPI is used to specify the communication between a set of processes forming a concurrent program. The message-passing paradigm is attractive because of its wide portability and scalability.
Reference: [2] <author> R. Calkin, R. Hempel, H. Hoppe, and P. Wypior. </author> <title> Portable Programming with the PARMACS Message-Passing Library. Parallel Computing, </title> <journal> Special issue on message-passing interfaces, </journal> <volume> 20 </volume> <pages> 615-32, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Researchers incorporated into MPI the most useful features of several systems, rather than choosing one system to adopt as the standard. MPI has roots in PVM [3, 6], Express [9], P4 [1], Zipcode [10], and Parmacs <ref> [2] </ref>, and in systems sold by IBM, Intel, Meiko, Cray Research, and Ncube. 2 Overview MPI is used to specify the communication between a set of processes forming a concurrent program. The message-passing paradigm is attractive because of its wide portability and scalability.
Reference: [3] <author> J. Dongarra, A. Geist, R. Manchek, and V. Sunderam. </author> <title> Integrated PVM Framework Supports Heterogeneous Network Computing. </title> <journal> Computers in Physics, </journal> <volume> 7(2) </volume> <pages> 166-75, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Other sources of information on MPI are available or are under development (see sidebar B). Researchers incorporated into MPI the most useful features of several systems, rather than choosing one system to adopt as the standard. MPI has roots in PVM <ref> [3, 6] </ref>, Express [9], P4 [1], Zipcode [10], and Parmacs [2], and in systems sold by IBM, Intel, Meiko, Cray Research, and Ncube. 2 Overview MPI is used to specify the communication between a set of processes forming a concurrent program.
Reference: [4] <author> J. Dongarra and E. Grosse. </author> <title> Distribution of Mathematical Software via Electronic Mail. </title> <journal> Communications of the ACM, </journal> <volume> 30(5) </volume> <pages> 403-7, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The official version of the specification document can be obtained from netlib <ref> [4] </ref> by sending an email message to netlib@www.netlib.org with the message: "send mpi-report.ps from mpi". A postscript file will be mailed back to you by the netlib server. The document may also be obtained via anonymous ftp from www.netlib.org/mpi/mpi-report.ps, and a hypertext version is available through the world-wide-web at http://www.mcs.anl.gov/mpi/mpi-report/mpi-report.html.
Reference: [5] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard . International Journal of Supercomputer Applications and High Performance Computing, </title> <type> 8(3/4), </type> <year> 1994. </year> <note> Special issue on MPI. Also available electronically, the url is ftp://www.netlib.org/mpi/mpi-report.ps. </note>
Reference-contexts: Many vendors of concurrent computers were involved, along with researchers from universities, government laboratories, and industry. This effort culminated in the publication of the MPI specification <ref> [5] </ref>. Other sources of information on MPI are available or are under development (see sidebar B). Researchers incorporated into MPI the most useful features of several systems, rather than choosing one system to adopt as the standard.
Reference: [6] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sun-deram. </author> <title> PVM: A Users' Guide and Tutorial for Networked Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year> <note> The book is available electronically, the url is ftp://www.netlib.org/pvm3/book/pvm-book.ps. </note>
Reference-contexts: Other sources of information on MPI are available or are under development (see sidebar B). Researchers incorporated into MPI the most useful features of several systems, rather than choosing one system to adopt as the standard. MPI has roots in PVM <ref> [3, 6] </ref>, Express [9], P4 [1], Zipcode [10], and Parmacs [2], and in systems sold by IBM, Intel, Meiko, Cray Research, and Ncube. 2 Overview MPI is used to specify the communication between a set of processes forming a concurrent program.
Reference: [7] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year> <month> 13 </month>
Reference: [8] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The standard defines the syntax and semantics of a core of library routines useful to a wide range of users writing portable message-passing programs in Fortran 77 or C. MPI also forms a possible target for compilers of languages such as High Performance Fortran <ref> [8] </ref>. Commercial and free, public-domain implementations of MPI already exist (see sidebar A). These run on both tightly-coupled, massively-parallel machines (MPPs), and on networks of workstations (NOWs).
Reference: [9] <institution> Parasoft Corporation, Monrovia, CA. </institution> <note> Express User's Guide, version 3.2.5 edition, 1992. Parasoft can be reached, electronically, at parasoft@Parasoft.COM. </note>
Reference-contexts: Other sources of information on MPI are available or are under development (see sidebar B). Researchers incorporated into MPI the most useful features of several systems, rather than choosing one system to adopt as the standard. MPI has roots in PVM [3, 6], Express <ref> [9] </ref>, P4 [1], Zipcode [10], and Parmacs [2], and in systems sold by IBM, Intel, Meiko, Cray Research, and Ncube. 2 Overview MPI is used to specify the communication between a set of processes forming a concurrent program. The message-passing paradigm is attractive because of its wide portability and scalability.
Reference: [10] <author> A. Skjellum and A. Leung. </author> <title> Zipcode: a Portable Multicomputer Communication Library atop the Reactive Kernel. </title> <editor> In D. W. Walker and Q. F. Stout, editors, </editor> <booktitle> Proceedings of the Fifth Distributed Memory Concurrent Computing Conference, </booktitle> <pages> pages 767-76. </pages> <publisher> IEEE Press, </publisher> <year> 1990. </year> <month> 14 </month>
Reference-contexts: Other sources of information on MPI are available or are under development (see sidebar B). Researchers incorporated into MPI the most useful features of several systems, rather than choosing one system to adopt as the standard. MPI has roots in PVM [3, 6], Express [9], P4 [1], Zipcode <ref> [10] </ref>, and Parmacs [2], and in systems sold by IBM, Intel, Meiko, Cray Research, and Ncube. 2 Overview MPI is used to specify the communication between a set of processes forming a concurrent program. The message-passing paradigm is attractive because of its wide portability and scalability.
References-found: 10

