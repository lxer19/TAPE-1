URL: http://www.ai.mit.edu/people/cohn/SAL95/Contributions/gil.ps
Refering-URL: http://www.ai.mit.edu/people/cohn/SAL95/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: gil@isi.edu  
Title: Learning from the Environment by Experimentation: The Need for Few and Informative Examples  
Author: Yolanda Gil 
Address: 4676 Admiralty Way Marina del Rey, CA 90292  
Affiliation: USC/Information Sciences Institute  
Abstract: An intelligent system must be able to adapt and learn to correct and update its model of the environment incrementally and deliberately. In complex environments that have many parameters and where interactions have a cost, sampling the possible range of states to test the results of action executions is not a practical approach. We present a practical approach based on continuous and selective interaction with the environment that pinpoints the type of fault in the domain knowledge that causes any unexpected behavior of the environment, and resorts to experimentation when additional information is needed to correct the system's knowledge. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1987. </year>
Reference: <author> Carbonell J. G. and Gil, Y. </author> <title> Learning by experimentation: The operator refinement method. </title> <editor> In Y. Kodratoff and R. S. Michalski, editors, </editor> <booktitle> Machine Learning, An Artificial Intelligence Approach, Volume III. </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference: <author> Gil, Y. </author> <title> Acquiring Domain Knowledge for Planning by Experimentation. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Empirical Results EXPO was tested in two different domains: a robot planning domain that is frequently used in the planning literature, and a complex process planning domain that has dozens of operators and states of large size. <ref> (Gil 1992) </ref> describes these domains in detail as well as other performance tests done with EXPO. To control the amount of missing knowledge that EXPO was given in the tests, we first wrote a complete domain D with all the operators with all their corresponding conditions and effects.
Reference: <author> Gil, Y. </author> <title> Efficient domain-independent experimentation. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Leaning, </booktitle> <address> Amherst, MA. </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference: <author> Gil, Y. </author> <title> Learning by Experimentation: Incremental Refinement of Incomplete Planning Domains. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Leaning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann, 1994 Kulkarni, </publisher> <editor> S. R., Mitter, S. K., and Tsitsiklis, J. N. </editor> <title> Active learning using arbitrary binary valued queries. </title> <journal> Machine Learning, </journal> <volume> 11(1), </volume> <year> 1993. </year>
Reference: <author> Ling, X. </author> <title> Inductive learning from good examples. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <address> Sydney, Australia, </address> <year> 1991. </year>
Reference: <author> Mahadevan, S. and Connell, J. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence 55(2-3):311-365, </journal> <year> 1992. </year>
Reference: <author> Rivest, R. L. and Sloan, R. </author> <title> Learning complicated concepts reliably and usefully. </title> <booktitle> In Proceedings of the Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, PA, </address> <year> 1988. </year>
Reference: <author> Shen, W. M. </author> <title> Discovery as autonomous learning from the environment. </title> <journal> Machine Learning, </journal> <volume> 12(1/2/3), </volume> <year> 1993. </year>
Reference: <author> Watkins, C. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Kings College, </institution> <year> 1989. </year>
References-found: 10

