URL: http://www.cs.purdue.edu/homes/ayg/publications/ps/arch_ind.ps
Refering-URL: http://www.cs.purdue.edu/homes/ayg/publications/work.html
Root-URL: http://www.cs.purdue.edu
Email: -ananth, kumar-@cs.umn.edu  ranka@cis.ufl.edu  vsingh@hpl.hp.com  
Title: On Architecture Independent Design and Analysis of Parallel Programs  
Author: Ananth Grama Vipin Kumar Sanjay Ranka and Vineet Singh flfl 
Note: kumar  
Web: http://www.cs.umn.edu/ananth http://www.cs.umn.edu/  
Address: Minneapolis, MN.  Gainesville, FL.  1500 Page Mill Road Palo Alto, CA.  
Affiliation: Department of Computer Science, University of Minnesota  Department of Computer Science, University of Florida  flfl Hewlett Packard Labs,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Agarwal, A. K. Chandra, and M. Snir. </author> <title> Communication complexity of PRAMs. </title> <type> Technical Report RC 14998 (No.64644), </type> <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1989. </year>
Reference-contexts: Therefore, models based on global p-ported memory take a much stronger view of the architecture than is realizable for reasonable machine sizes. Most PRAM models (PRAM, LPRAM <ref> [1] </ref>, BPRAM [2]) assume a global memory. The switch complexity can be reduced by organizing memory into banks (p banks of single-ported memories). When a processor is accessing a bank, access to all other words in the bank by other processors is blocked. <p> Ranade's technique, however, pays a log p cost for each instruction. This excess cost is often unacceptable. In addition to the log p cost, there are also constant overheads involved in the simulation. These render the simulation technique less useful in practice. 4.2 LPRAM The LPRAM <ref> [1] </ref> model of computing is a global memory model. The model is based on the premise that non-local memory accesses are more expensive than local memory 17 accesses. It therefore tries to minimize the volume of remote data accessed and rewards data volume locality.
Reference: [2] <author> A. Agarwal, A. K. Chandra, and M. Snir. </author> <title> On communication latency in PRAM computations. </title> <type> Technical Report RC 14973 (No.66882), </type> <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY, </address> <year> 1989. </year>
Reference-contexts: Therefore, models based on global p-ported memory take a much stronger view of the architecture than is realizable for reasonable machine sizes. Most PRAM models (PRAM, LPRAM [1], BPRAM <ref> [2] </ref>) assume a global memory. The switch complexity can be reduced by organizing memory into banks (p banks of single-ported memories). When a processor is accessing a bank, access to all other words in the bank by other processors is blocked. <p> Algorithms designed for this model may potentially pay significant startup overheads and make poor use of available communication bandwidth. Architectural features assumed by the model (negligible startup overhead, global non-blocking memory) render it unrealizable in real parallel computers. 4.3 BPRAM The BPRAM <ref> [2] </ref> model of computing is a global memory model. This model is based on the premise that startup communication latencies of conventional computers are often high. It therefore rewards bulk access of remote data (bulk access locality). BPRAM assumes that the communication bandwidth is identical to local memory bandwidth.
Reference: [3] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: Although CCN has never been formally proposed as a model for architecture independent programming, its variants have been used extensively <ref> [3, 11, 7] </ref>. It assumes the presence of p processing elements connected over a completely connected network. The cost of sending a message of size m from one processor to any other is given by t s +t w m.
Reference: [4] <author> D. Culler, R. Karp, D. Patterson, and et al. </author> <title> Logp: Towards a realistic model of parallel computation. </title> <year> 1992. </year>
Reference-contexts: Such a model is referred to as a Seclusive Memory model. When combined with appropriate communication cost models, seclusive memory models approximate machine architectures much better than global memory models. Algorithm design models based on seclusive memory models include MPC [12], BSP (XPRAM)[17], CCN, and LogP <ref> [4] </ref>. All practical parallel computers (including shared address space and message passing machines) use single ported memory and thus belong to the class of seclusive memory machines. 3.2 Bulk Access Locality In conventional parallel computers, it takes a certain amount of time to access the first word of a message.
Reference: [5] <author> W. J. Dally. </author> <title> Analysis of k-ary n-cube interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(6), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: For many communication operations that are not bisection width insensitive, low dimension networks are more cost effective than hypercubes and related networks <ref> [11, 5] </ref>. Popular interconnection topologies are buses (SGI Challenge), 2D meshes (Paragon, Delta), 3D meshes (Cray T3D), hypercubes (nCUBE), fat tree (CM5), hierarchical networks (cedar, DASH). 2.1 The Communication Model The communication delays arise from various sources. <p> The per-word transfer time t w is determined by the link bandwidth. Lower degree networks 7 typically have higher link bandwidth (and consequently lower t w ). This is because the individual channels can be made fatter for the same overall cost <ref> [11, 5] </ref>. The per-word transfer time t w is often higher than t c , the time to do a unit computation on data available in the cache. The per-hop component t h d can often be subsumed into the startup time t s without significant loss of efficiency.
Reference: [6] <author> Pilar de la Torre and Clyde P. Kruskal. </author> <title> Towards a single model of efficient computation in real parallel machines. </title> <booktitle> In Future Generation Computer Systems, </booktitle> <pages> pages 395 - 408, 8(1992). </pages>
Reference-contexts: This is difficult to realize unless communication is highly clustered. Architectural assumptions such as non-blocking global memory and balanced communication and local memory bandwidth make this model difficult to construct. 4.4 Hierarchical PRAM models (HPRAM and YPRAM) The HPRAM [10, 9] and YPRAM <ref> [6] </ref> models of computation allow for (recursively) decomposing the machine till the algorithm only uses one processor and executes sequentially. 18 For more than one processor, the communication cost charged is a function of the number of processors available. <p> Assigning different cost to data redistributions in which communication is limited to smaller subsets of processors allow for designing algorithms which are more effective in utilizing locality especially for divide and conquer type applications. Despite the availability of variable cost redistributions, HPRAM <ref> [10, 6] </ref> suffers from limitations similar to the BSP model in terms of slack and worst case bandwidth requirements for every step. This is because HPRAM does not reward bulk access locality. Note that YPRAM does not suffer from this drawback since it rewards bulk access locality.
Reference: [7] <author> G. C. Fox, M. Johnson, G. Lyzenga, S. W. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors: </title> <journal> Vol. </journal> <volume> 1. </volume> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: We do not propose another universal machine model based on arbitrary data redistributions. Instead, we recognize that a large number of scientific, engineering and computer science applications have been solved using collective communication operations <ref> [11, 7] </ref>. Based on this rich experience, we propose a strategy for algorithm analysis based on aggregate communication operations. A limited number of these aggregate communication operations cover the vast majority of high-performance applications. Application developers can simply design algorithms 4 using these aggregate operations. <p> Although CCN has never been formally proposed as a model for architecture independent programming, its variants have been used extensively <ref> [3, 11, 7] </ref>. It assumes the presence of p processing elements connected over a completely connected network. The cost of sending a message of size m from one processor to any other is given by t s +t w m.
Reference: [8] <author> Ananth Grama, Anshul Gupta, and Vipin Kumar. </author> <title> Isoefficiency function: A scalability metric for parallel algorithms and architectures. </title> <journal> IEEE Parallel and Distributed Technology, Special Issue on Parallel and Distributed Systems: From Theory to Practice, </journal> <volume> 1 </volume> (3):12-21, 1993. Also available as Technical Report TR-93-24, Department of Computer Science, University of Minnesota and from anonymous ftp site ftp.cs.umn.edu, file users/kumar/isoeff-tutorial.ps. 
Reference-contexts: As the figure shows, proces 2 Isoefficiency is a scalability metric. It relates the rate of growth of problem size to the number of processors for maintaining constant efficiency. For a detailed discussion on the isoefficiency metric, the reader is referred to <ref> [11, 8] </ref> 32 exchange data. sor 0 communicates with processors 1, 2, 4, 8, 16, and 32. Note that all these processors lie in the same row or column of the mesh as that of processor 0.
Reference: [9] <author> T. Heywood and S. Ranka. </author> <title> A practical hierarchical model of parallel computation. i. the model. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(3) </volume> <pages> 212-32, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: This is difficult to realize unless communication is highly clustered. Architectural assumptions such as non-blocking global memory and balanced communication and local memory bandwidth make this model difficult to construct. 4.4 Hierarchical PRAM models (HPRAM and YPRAM) The HPRAM <ref> [10, 9] </ref> and YPRAM [6] models of computation allow for (recursively) decomposing the machine till the algorithm only uses one processor and executes sequentially. 18 For more than one processor, the communication cost charged is a function of the number of processors available.
Reference: [10] <author> T. Heywood and S. Ranka. </author> <title> A practical hierarchical model of parallel computation. ii. binary tree and fft algorithms. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16(3) </volume> <pages> 233-49, </pages> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: This is difficult to realize unless communication is highly clustered. Architectural assumptions such as non-blocking global memory and balanced communication and local memory bandwidth make this model difficult to construct. 4.4 Hierarchical PRAM models (HPRAM and YPRAM) The HPRAM <ref> [10, 9] </ref> and YPRAM [6] models of computation allow for (recursively) decomposing the machine till the algorithm only uses one processor and executes sequentially. 18 For more than one processor, the communication cost charged is a function of the number of processors available. <p> Assigning different cost to data redistributions in which communication is limited to smaller subsets of processors allow for designing algorithms which are more effective in utilizing locality especially for divide and conquer type applications. Despite the availability of variable cost redistributions, HPRAM <ref> [10, 6] </ref> suffers from limitations similar to the BSP model in terms of slack and worst case bandwidth requirements for every step. This is because HPRAM does not reward bulk access locality. Note that YPRAM does not suffer from this drawback since it rewards bulk access locality.
Reference: [11] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Algorithm Design and Analysis. </title> <publisher> Benjamin Cummings/ Addison Wesley, </publisher> <address> Redwod City, </address> <year> 1994. </year>
Reference-contexts: We do not propose another universal machine model based on arbitrary data redistributions. Instead, we recognize that a large number of scientific, engineering and computer science applications have been solved using collective communication operations <ref> [11, 7] </ref>. Based on this rich experience, we propose a strategy for algorithm analysis based on aggregate communication operations. A limited number of these aggregate communication operations cover the vast majority of high-performance applications. Application developers can simply design algorithms 4 using these aggregate operations. <p> There is also a large class of communication patterns whose performance is not critically impacted by the cross-section bandwidth. Some important examples are one-to-all broadcast and its variants, all-to-all broadcast, and one-to-all personalized communications <ref> [11] </ref>. Low dimensional networks such as 2-D meshes are cheaper to construct than high dimensional networks such as hypercube (and related networks such as multistage networks and fat trees), but they have smaller bisection width. <p> For many communication operations that are not bisection width insensitive, low dimension networks are more cost effective than hypercubes and related networks <ref> [11, 5] </ref>. Popular interconnection topologies are buses (SGI Challenge), 2D meshes (Paragon, Delta), 3D meshes (Cray T3D), hypercubes (nCUBE), fat tree (CM5), hierarchical networks (cedar, DASH). 2.1 The Communication Model The communication delays arise from various sources. <p> The per-word transfer time t w is determined by the link bandwidth. Lower degree networks 7 typically have higher link bandwidth (and consequently lower t w ). This is because the individual channels can be made fatter for the same overall cost <ref> [11, 5] </ref>. The per-word transfer time t w is often higher than t c , the time to do a unit computation on data available in the cache. The per-hop component t h d can often be subsumed into the startup time t s without significant loss of efficiency. <p> Instances of personalized operations in this category include NEWS, general permutations, and all-to-all personalized communications. In the second category, no restriction is placed on the data fan-out and data fan-in. An example of such an operation is one-to-all personalized communication. These operations are described in detail in <ref> [11, 14] </ref>. 13 Broadcast Operations: In these operations, each data item goes to all the processors in the group. Instances of these operations include one-to-all, k-to-all, and all-to-all broadcasts. These operations are described in detail in [11]. <p> These operations are described in detail in [11, 14]. 13 Broadcast Operations: In these operations, each data item goes to all the processors in the group. Instances of these operations include one-to-all, k-to-all, and all-to-all broadcasts. These operations are described in detail in <ref> [11] </ref>. Data-redistribution Operation: In a data redistribution operation, each processor has one data item that needs to be communicated to another processor. Each data item has a unique destination processor. The size of the data items can vary. <p> Although CCN has never been formally proposed as a model for architecture independent programming, its variants have been used extensively <ref> [3, 11, 7] </ref>. It assumes the presence of p processing elements connected over a completely connected network. The cost of sending a message of size m from one processor to any other is given by t s +t w m. <p> For a more detailed description of these operations, the reader is referred to Kumar et al. <ref> [11] </ref> and Shankar and Ranka [14, 15]. Personalized communication appears in such applications as matrix transpose, hash joins, and fast fourier transforms. Broadcasts and scans are ubiquitous. NEWS communication is required in finite difference and finite element methods, and a variety of image processing applications. <p> We assume knowledge of the algorithms used for basic communication operations. Whereas this knowledge is not necessary for analyzing algorithms in our framework, it is necessary for analyzing them for the LogP model. The reader is referred to <ref> [11] </ref> for detailed descriptions of these analyses. 7.1 Fast Fourier Transforms There are two main parallel formulations of the Cooley and Tukey Fast Fourier Transform: binary-exchange and transpose based algorithm. <p> As the figure shows, proces 2 Isoefficiency is a scalability metric. It relates the rate of growth of problem size to the number of processors for maintaining constant efficiency. For a detailed discussion on the isoefficiency metric, the reader is referred to <ref> [11, 8] </ref> 32 exchange data. sor 0 communicates with processors 1, 2, 4, 8, 16, and 32. Note that all these processors lie in the same row or column of the mesh as that of processor 0. <p> This is because it assumes that all-to-all communication is a bandwidth sensitive operation, which is not the case. 7.3 Sample Sort Sample sort is a commonly used technique for sorting keys <ref> [11] </ref>. This technique is based on partitioning the list into buckets that can be independently sorted. Since the distribution of particles is a-priori unknown, it is difficult to determine evenly spaced splitters. <p> The p splitters thus picked are communicated to all processors using an all-to-all broadcast. Each processor then splits its local sample into p parts based on the splitters and communicates elements to designated processors. This is done using a single all-to-all personalized communication operation <ref> [11] </ref>. The elements received by each processor as a result of this operation are accrued appropriately into a sorted sequence. <p> For a detailed description of the algorithm and its parallel formulations, the reader is referred to Kumar et. al. <ref> [11] </ref>. For a dense linear system with n unknowns, Gaussian elimination involves approximately n 2 =2 divisions and approximately (n 3 =3) (n 2 =2) subtractions and multiplications. <p> Assuming that each scalar arithmetic operation takes unit time, the sequential run time of the procedure is approximately 2n 3 =3 (for large n). Parallel formulations of Gaussian elimination are based on partitioning the matrix. Popular partitioning techniques include striping, checkerboarding, and cyclic mappings <ref> [11] </ref>. In this section, we compare various models on the basis of their performance prediction of the checkerboard version of parallel Gaussian elimination.
Reference: [12] <author> K. Mehlhorn and U. Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memories. </title> <journal> Acta Informatica, </journal> <volume> 21(4) </volume> <pages> 339-374, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Such a model is referred to as a Seclusive Memory model. When combined with appropriate communication cost models, seclusive memory models approximate machine architectures much better than global memory models. Algorithm design models based on seclusive memory models include MPC <ref> [12] </ref>, BSP (XPRAM)[17], CCN, and LogP [4].
Reference: [13] <author> A. G. Ranade. </author> <title> How to emulate shared memory. </title> <booktitle> In Proceedings of the 28th IEEE Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 185 - 194, </pages> <year> 1987. </year>
Reference-contexts: Data locality is therefore not an issue while designing PRAM algorithms. This is a major drawback since communication costs are often the dominant overhead in parallel algorithms. In fact, many PRAM algorithms are not even cost optimal when ported to real architectures. Ranade <ref> [13] </ref> had earlier demonstrated how it is possible to simulate a p processor PRAM on a p processor seclusive memory machine with a butterfly interconnect. Ranade's technique, however, pays a log p cost for each instruction. This excess cost is often unacceptable. <p> Recall that a p processor PRAM can be emulated on a p processor seclusive memory machine (with a butterfly interconnect) incurring a log p cost for each instruction <ref> [13] </ref>. This emulation does not require any slack. Although BSP [16] is an attractive theoretical model, it has several drawbacks.
Reference: [14] <author> R. Shankar and S. Ranka. </author> <title> Random data access on a coarse grained parallel machine i. one-to-one mappings. </title> <type> Technical report, </type> <institution> School of Computer and Information Science, Syracuse University, Syracuse, </institution> <address> NY, 13244, </address> <year> 1995. </year>
Reference-contexts: Instances of personalized operations in this category include NEWS, general permutations, and all-to-all personalized communications. In the second category, no restriction is placed on the data fan-out and data fan-in. An example of such an operation is one-to-all personalized communication. These operations are described in detail in <ref> [11, 14] </ref>. 13 Broadcast Operations: In these operations, each data item goes to all the processors in the group. Instances of these operations include one-to-all, k-to-all, and all-to-all broadcasts. These operations are described in detail in [11]. <p> Real machine topologies in this class are the hypercube, fat-tree, butterfly, and the crossbar network. It is possible to perform this operation in fi (m) time only in the presence of slack and sufficient cross-section bandwidth <ref> [14, 15] </ref>. Models which assume that communication cost is independent of the number of processors and structure of communication are called equal cost redistribution models. All other models are called variable redistribution cost models. <p> Both models assume that this permutation can be performed in fi (m) time. However, there are no known algorithms that can perform the permutation in this time even on fi (p) cross-section bandwidth networks such as hypercubes without slack. (The transportation primitive of Shankar and Ranka <ref> [14, 15] </ref> performs this permutation in fi (m) but it requires a slack of fi (p 2 ).) Therefore, the cost model is a lower bound for data redistributions on all architectures. Now consider an adaptation of these models for a mesh. <p> For a more detailed description of these operations, the reader is referred to Kumar et al. [11] and Shankar and Ranka <ref> [14, 15] </ref>. Personalized communication appears in such applications as matrix transpose, hash joins, and fast fourier transforms. Broadcasts and scans are ubiquitous. NEWS communication is required in finite difference and finite element methods, and a variety of image processing applications. <p> The all-to-all broadcast takes time t s log p + t w p. The third step is an unbalanced personalized communication. Traditional algorithms for personalized communication may yield very poor performance. We therefore resort to the transportation primitive of Shankar and Ranka <ref> [14, 15] </ref>. Using this primitive, the communication time can be bounded by 2 (t s + t w fi 2n=p 2 ) fi p. This expression holds provided n=p &gt; p 2 .
Reference: [15] <author> R. Shankar and S. Ranka. </author> <title> Random data access on a coarse grained parallel machine ii. one-to-many and many-to-one mappings. </title> <type> Technical report, </type> <institution> School of Computer and Information Science, Syracuse University, Syracuse, </institution> <address> NY, 13244, </address> <year> 1995. </year> <month> 45 </month>
Reference-contexts: Real machine topologies in this class are the hypercube, fat-tree, butterfly, and the crossbar network. It is possible to perform this operation in fi (m) time only in the presence of slack and sufficient cross-section bandwidth <ref> [14, 15] </ref>. Models which assume that communication cost is independent of the number of processors and structure of communication are called equal cost redistribution models. All other models are called variable redistribution cost models. <p> Both models assume that this permutation can be performed in fi (m) time. However, there are no known algorithms that can perform the permutation in this time even on fi (p) cross-section bandwidth networks such as hypercubes without slack. (The transportation primitive of Shankar and Ranka <ref> [14, 15] </ref> performs this permutation in fi (m) but it requires a slack of fi (p 2 ).) Therefore, the cost model is a lower bound for data redistributions on all architectures. Now consider an adaptation of these models for a mesh. <p> For a more detailed description of these operations, the reader is referred to Kumar et al. [11] and Shankar and Ranka <ref> [14, 15] </ref>. Personalized communication appears in such applications as matrix transpose, hash joins, and fast fourier transforms. Broadcasts and scans are ubiquitous. NEWS communication is required in finite difference and finite element methods, and a variety of image processing applications. <p> The all-to-all broadcast takes time t s log p + t w p. The third step is an unbalanced personalized communication. Traditional algorithms for personalized communication may yield very poor performance. We therefore resort to the transportation primitive of Shankar and Ranka <ref> [14, 15] </ref>. Using this primitive, the communication time can be bounded by 2 (t s + t w fi 2n=p 2 ) fi p. This expression holds provided n=p &gt; p 2 .
Reference: [16] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8), </volume> <year> 1990. </year>
Reference-contexts: Recall that a p processor PRAM can be emulated on a p processor seclusive memory machine (with a butterfly interconnect) incurring a log p cost for each instruction [13]. This emulation does not require any slack. Although BSP <ref> [16] </ref> is an attractive theoretical model, it has several drawbacks.
Reference: [17] <author> L. G. Valiant. </author> <title> General purpose parallel architectures. </title> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <year> 1990. </year>
Reference-contexts: Further, important communication primitives such as NEWS are difficult to analyze in these models. Both HPRAM and YPRAM can accurately predict the cost of those aggregate communication operations that can be optimally implemented as a hierarchical sequence of data redistributions. 4.5 Bulk Synchronous Parallel Valiant proposed the XPRAM <ref> [17] </ref> model that allows PRAM algorithms to run within a constant factor of optimal as long as there is sufficient slack. The XPRAM model forms the basis for Bulk Synchronous Parallelism (BSP).
Reference: [18] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <year> 1992. </year> <month> 46 </month>
Reference-contexts: For a hypercube, the bisection width is p=2, for a wraparound mesh 2 p p, and for a shared bus 1. The values of constants associated with small fixed size messages can be somewhat different. For example, using active messages <ref> [18] </ref>, it is possible to reduce the startup time for message transfer compared to arbitrary sized messages. This is because fixed size messages use simple buffer allocation protocols. However, each packet must now carry message tags and control information.
References-found: 18

