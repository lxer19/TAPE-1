URL: http://www.cs.purdue.edu/coast/archive/clife/CFS/papers/sab94.ps.gz
Refering-URL: http://www.cs.purdue.edu/coast/archive/clife/CFS/papers/
Root-URL: http://www.cs.purdue.edu
Email: bersini, mdorigo@ulb.ac.be  dorigo@elet.polimi.it  
Title: A COMPARISON OF Q-LEARNING AND CLASSIFIER SYSTEMS  
Author: Marco Dorigo and Hugues Bersini 
Date: AUGUST 812, 1994  
Address: BEHAVIOR (SAB94), BRIGHTON, UK,  Avenue Franklin Roosvelt 50, CP 194/6, 1050 Bruxelles, Belgium  Piazza Leonardo da Vinci 32, 20133 Milano, Italy  
Affiliation: OF ADAPTIVE  IRIDIA Universit Libre de Bruxelles  Progetto di Intelligenza Artificiale e Robotica Dipartimento di Elettronica e Informazione, Politecnico di Milano  
Note: TO APPEAR IN THE PROCEEDINGS OF FROM ANIMALS TO ANIMATS, THIRD INTERNATIONAL CONFERENCE ON SIMULATION  
Abstract: Reinforcement Learning is a class of problems in which an autonomous agent acting in a given environment improves its behavior by progressively maximizing a function calculated just on the basis of a succession of scalar responses received from the environment. Q-learning and classifier systems (CS) are two methods among the most used to solve reinforcement learning problems. Notwithstanding their popularity and their shared goal, they have been in the past often considered as two different models. In this paper we first show that the classifier system, when restricted to a sharp simplification called discounted max very simple classifier system (D MAX - VSCS), boils down to tabular Q-learning. It follows that D MAX -VSCS converges to the optimal policy as proved by Watkins & Dayan (1992), and that it can draw profit from the results of experimental and theoretical works dedicated to improve Q-learning and to facilitate its use in concrete applications. In the second part of the paper, we show that three of the restrictions we need to impose to the CS for deriving its equivalence with Q-learning, that is, no internal states, no don't care symbols, and no structural changes, turn out so essential as to be recently rediscovered and reprogrammed by Q-learning adepts. Eventually, we sketch further similarities among ongoing work within both research contexts. The main contribution of the paper is therefore to make explicit the strong similarities existing between Q-learning and classifier systems, and to show that experience gained with research within one domain can be useful to direct future research in the other one.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bersini H., </author> <year> 1993. </year> <title> Immune network and adaptive control. </title> <booktitle> Toward a Practice of Autonomous Systems - Proceedings of the First ECAL, </booktitle> <editor> Varela and Bourgine (Eds.), </editor> <volume> 217225, </volume> <publisher> MIT Press. </publisher>
Reference-contexts: In the field of process control, we observe today a similar kind of convergence between two types of methods, likewise having the two behaviorist and symbolic distinct origins: neural networks and fuzzy systems <ref> (Bersini and Gorrini, 1993) </ref>.
Reference: <author> Bersini H. and Gorrini V., </author> <year> 1993. </year> <title> FUNNY (FUzzY or Neural Net) methods for adaptive process control. </title> <booktitle> Proceedings of EUFIT '93, ELITE Foundation, </booktitle> <address> Aachen, Germany, </address> <pages> 55-61. </pages>
Reference-contexts: In the field of process control, we observe today a similar kind of convergence between two types of methods, likewise having the two behaviorist and symbolic distinct origins: neural networks and fuzzy systems <ref> (Bersini and Gorrini, 1993) </ref>.
Reference: <author> Bersini H. and Varela F., </author> <year> 1994. </year> <title> The immune learning mechanisms: Recruitment reinforcement and their applications. To appear in Computing with Biological Metaphors, </title> <editor> R. Patton (Ed.), Chapman and Hall Bersini H., Now A., Caironi P.V.C., Dorigo M., </editor> <year> 1994. </year> <title> A family of reinforcement learning algorithms. </title> <type> Tech. Rep. </type> <institution> IRIDIA/94-1, Universit Libre de Bruxelles, Belgium. </institution>
Reference-contexts: can happen that a classifier is activated by an environmental message, and therefore the standard bucket brigade contains the implicit bucket brigade as a special case. 3 They are the only cause in static environments, one of the causes in dynamic environments. 4 Regarding point (i), in a companion paper <ref> (Bersini, Now, Caironi, Dorigo, 1994) </ref> we have proposed a generalization of formula (2), which allows the definition of a more general class of RL algorithms. An original aspect of the generalization we propose is the use of a next state evaluation (NSE) operator which generalizes the MAX operator.
Reference: <author> Bonarini A., </author> <year> 1993. </year> <title> ELF: Learning incomplete fuzzy rule sets for an autonomous robot. </title> <booktitle> Proceedings of EUFIT '93, ELITE Foundation, </booktitle> <address> Aachen, Germany, </address> <month> 6975. </month>
Reference: <author> Booker L., Goldberg D.E., and Holland J.H., </author> <year> 1989. </year> <title> Classifier systems and genetic algorithms. </title> <journal> Artificial Intelligence, </journal> <volume> 40, </volume> <pages> 1-3, 235282. </pages>
Reference-contexts: No complementary guidance is provided for helping the exploration/exploitation of the problem space, and therefore the learning agent can rely only on a trial-and-error strategy. Q-learning (Watkins, 1989) and classifier systems (CSs) <ref> (Booker, Goldberg, Holland, 1989) </ref> have been separately proposed as two general frameworks for treating reinforcement learning problems. Despite their shared goal, only a few researchers (Sutton, 1988; Twardowski, 1993; Roberts, 1993; Wilson, 1994) have discussed the relationship between them, and they are largely regarded as different approaches. <p> the possible classifiers (state-action pairs), and therefore there is no need to use the genetic algorithm to modify the covering of the state space. 1 In this paper we follow the following convention: (i) basic Q-learning is Watkin's Q-learning (1989) in tabular form; (ii) the classifier system, is Holland's CS <ref> (Booker, Goldberg, Holland, 1989) </ref>. 3 Constraint (ii) says that only one message is possible at each time step, and that this message comes from the sensors (this is often called an environmental message and represents the state of the environment as perceived by the agent).
Reference: <author> Chapman D. and Kaelbling L.P., </author> <year> 1991. </year> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparison. </title> <booktitle> Proceeding of the Twelth International Joint Conference on Artificial Intelligence (IJCAI-91), </booktitle> <pages> 726731. </pages>
Reference: <author> Chrisman L., </author> <year> 1992. </year> <title> Reinforcement learning with perceptual aliasing: The perceptual distinction approach. </title> <booktitle> Proceeding of the Tenth National Conference on Artificial Intelligence (AAAI-92), </booktitle> <pages> 183188. </pages>
Reference: <author> Colombetti M. and Dorigo M., </author> <year> 1992. </year> <title> Learning to control an autonomous robot by distributed genetic algorithms. </title> <booktitle> Proceedings of From Animals To Animats, Second International Conference on Simulation of Adaptive Behavior (SAB92), </booktitle> <address> Honolulu, 305312, </address> <publisher> Bradford Books, MIT Press. </publisher>
Reference: <author> Dayan P. and Hinton G.E., </author> <year> 1992. </year> <title> Feudal reinforcement learning. </title> <editor> In C.L. Giles, S.J. Hanson and J.D. Cowan (Eds), </editor> <title> Advances in Neural Information Processing S y s t e m s 5 , 271-278. </title> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dorigo M., </author> <year> 1992. </year> <title> AL E C S Y S and the AutonoMouse: Learning to control a real robot by distributed classifier systems. </title> <type> Technical Report No.92-011, </type> <institution> Politecnico di Milano, Italy. </institution>
Reference: <author> Dorigo M., </author> <year> 1993. </year> <title> Genetic and nongenetic operators in ALECSYS. </title> <journal> Evolutionary Computation, </journal> <volume> 1, 2, 151164, </volume> <publisher> MIT Press. </publisher>
Reference: <author> Dorigo M. and Colombetti M., </author> <year> 1994. </year> <title> Robot shaping: Developing autonomous agents through learning. </title> <journal> Artificial Intelligence, </journal> <note> to appear. </note>
Reference-contexts: can happen that a classifier is activated by an environmental message, and therefore the standard bucket brigade contains the implicit bucket brigade as a special case. 3 They are the only cause in static environments, one of the causes in dynamic environments. 4 Regarding point (i), in a companion paper <ref> (Bersini, Now, Caironi, Dorigo, 1994) </ref> we have proposed a generalization of formula (2), which allows the definition of a more general class of RL algorithms. An original aspect of the generalization we propose is the use of a next state evaluation (NSE) operator which generalizes the MAX operator. <p> Q-learning Continuous representation Fuzzy CS Q-learning with neural nets Short-term memory Internal messages Recurrent neural nets Generalization Don't cares Neural nets Statistical clustering Double plasticity Genetic algorithm plus bucket brigade Incremental net division New action recruitment Galgorithm Partitioned Q-learning Hierarchy Explicitly designed with learning of both basic tasks and coordination <ref> (Dorigo and Colombetti, 1994) </ref> Explicitly designed with learning of both basic tasks and coordination (Lin, 1993) 7
Reference: <author> Dorigo M. and Schnepf U., </author> <year> 1993. </year> <title> Genetics-based machine learning and behavior-based robotics: A New Synthesis. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23, 1, </volume> <pages> 141154. </pages>
Reference: <author> Farmer D. </author> <year> 1991. </year> <title> A Rosetta Stone to connectionism. I n Emergent Computation, </title> <editor> Forrest S. (Ed.) </editor> <publisher> MIT Press. </publisher>
Reference-contexts: This generalization capacity has been investigated by Q-learning users either by means of NN (Lin, 1992) or by statistical clustering techniques (Mahadevan and Connell, 1992). The double level plasticity, parametric and structural, whose biological inspiration is largely discussed in <ref> (Farmer, 1991) </ref> and (Bersini, 1993; Bersini and Varela, 1994), refers to the strong adaptive capacity of a system which automatically adjust its numerical parameters with the simultaneous possibility to modify the structure of the system by the generation of new agents.
Reference: <author> Goldberg D.E. </author> <year> 1989. </year> <title> Genetic algorithms in search, optimization, and machine learning. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: No complementary guidance is provided for helping the exploration/exploitation of the problem space, and therefore the learning agent can rely only on a trial-and-error strategy. Q-learning (Watkins, 1989) and classifier systems (CSs) <ref> (Booker, Goldberg, Holland, 1989) </ref> have been separately proposed as two general frameworks for treating reinforcement learning problems. Despite their shared goal, only a few researchers (Sutton, 1988; Twardowski, 1993; Roberts, 1993; Wilson, 1994) have discussed the relationship between them, and they are largely regarded as different approaches. <p> the possible classifiers (state-action pairs), and therefore there is no need to use the genetic algorithm to modify the covering of the state space. 1 In this paper we follow the following convention: (i) basic Q-learning is Watkin's Q-learning (1989) in tabular form; (ii) the classifier system, is Holland's CS <ref> (Booker, Goldberg, Holland, 1989) </ref>. 3 Constraint (ii) says that only one message is possible at each time step, and that this message comes from the sensors (this is often called an environmental message and represents the state of the environment as perceived by the agent).
Reference: <author> Holland J.H., </author> <year> 1980. </year> <title> Adaptive algorithms for discovering and using general patterns in growing knowledge bases. </title> <journal> International Journal of Policy Analysis and Information Systems, </journal> <volume> 4, 2, </volume> <pages> 217-240. </pages>
Reference-contexts: A second reason for this situation is that while the Q-learning algorithm is the heart of the first reinforcement strategy to be compared, the bucket brigade (BB) algorithm <ref> (Holland, 1980) </ref>, that is the counterpart of Q-learning in the CS framework, is only a component of the CS, making its restricted analysis harder to achieve.
Reference: <author> Lin L-J., </author> <year> 1992. </year> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 3-4, 293322. </pages>
Reference-contexts: This generalization capacity has been investigated by Q-learning users either by means of NN <ref> (Lin, 1992) </ref> or by statistical clustering techniques (Mahadevan and Connell, 1992). <p> This winning message, beside causing an action to be performed, is also appended to ML a .) Lin, in his neural net implementation of Q-learning <ref> (Lin, 1992) </ref>, uses a feedforward neural net for each action. Each net receives as input a message, which is the equivalent of the environmental message in CSs, and proposes an action with a given strength.
Reference: <author> Lin L-J., </author> <year> 1993. </year> <title> Hierarchical learning of robot skills by reinforcement. </title> <booktitle> Proceedings of 1993 IEEE International Conference on Neural Networks, IEEE, </booktitle> <pages> 181186. </pages>
Reference-contexts: neural nets Generalization Don't cares Neural nets Statistical clustering Double plasticity Genetic algorithm plus bucket brigade Incremental net division New action recruitment Galgorithm Partitioned Q-learning Hierarchy Explicitly designed with learning of both basic tasks and coordination (Dorigo and Colombetti, 1994) Explicitly designed with learning of both basic tasks and coordination <ref> (Lin, 1993) </ref> 7
Reference: <author> Lin L-J. and Mitchell T.M., </author> <year> 1992. </year> <title> Memory approaches to reinforcement learning in non-Markovian domains. </title> <editor> Tec h.Re p. </editor> <address> CMU- CS-92- 138, </address> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: This generalization capacity has been investigated by Q-learning users either by means of NN <ref> (Lin, 1992) </ref> or by statistical clustering techniques (Mahadevan and Connell, 1992). <p> This winning message, beside causing an action to be performed, is also appended to ML a .) Lin, in his neural net implementation of Q-learning <ref> (Lin, 1992) </ref>, uses a feedforward neural net for each action. Each net receives as input a message, which is the equivalent of the environmental message in CSs, and proposes an action with a given strength.
Reference: <author> Mc Callum R.A., </author> <year> 1993. </year> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, 190196. </address>
Reference: <author> Mahadevan S. and Connell J., </author> <year> 1992. </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55, 2, </volume> <pages> 311365. </pages>
Reference-contexts: This generalization capacity has been investigated by Q-learning users either by means of NN (Lin, 1992) or by statistical clustering techniques <ref> (Mahadevan and Connell, 1992) </ref>.
Reference: <author> Mitchell T.M., </author> <year> 1982. </year> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18, 2, </volume> <pages> 203-226. </pages>
Reference-contexts: This is more easily achieved in Q-learning systems enriched by neural nets, like Lin's system (1992). Observing reinforcement learning algorithms, we are in presence of the two traditional and opposite ways to obtain partitions of the input space: bottom-up and top-down <ref> (Mitchell, 1982) </ref>. In the bottom-up approach, the input space is initially very fine grained, and the partitions are created by clustering together input regions showing similar properties. In the top-down approach, the initial input space is uniformly coded and is progressively subdivided in finer parts.
Reference: <author> Munos R., Patinel J. and Goyeau P., </author> <year> 1994. </year> <title> Partitioned Q-learning. </title> <booktitle> Proceedings of From Animals To Animats, Third International Conference on Simulation of Adaptive Behavior (SAB94), </booktitle> <address> Brighton, United Kingdom, </address> <month> August 812, </month> <year> 1994. </year>
Reference: <editor> Riolo R.L., </editor> <year> 1987. </year> <title> Bucket Brigade performance: II. Default hierarchies. </title> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <editor> J.J. Grefenstette (Ed.), </editor> <publisher> Lawrence Erlbaum, </publisher> <pages> 184195. </pages>
Reference: <editor> Riolo R.L., </editor> <year> 1989. </year> <title> The emergence of default hierarchies in learning classifier systems. </title> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <address> J.D. </address>
Reference: <editor> Schaffer (Ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <pages> 322327. </pages>
Reference: <author> Roberts G., </author> <year> 1993. </year> <title> Dynamic planning for classifier systems. </title> <booktitle> Proceedings of Fifth International Conference on Genetic Algorithms, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, 231 237. </address>
Reference: <author> Samuel A.L., </author> <year> 1959. </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, </journal> <volume> 3, </volume> <pages> 210229. </pages> <note> Reprinted in E.A. </note>
Reference: <editor> Feigenbaum and J.Feldman (Eds.) </editor> <booktitle> Computers and thought. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Schwartz A., </author> <year> 1993. </year> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> Proceedings of Tenth 8 International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <month> 298305. </month>
Reference-contexts: Experimental results have shown that this choice gives rise to a worse performance than with the MAX operator, but that it does not seem to affect the convergence properties of the algorithm. With respect to the discounted nature of the algorithm (i.e., g &lt; 1), this is usually justified <ref> (Schwartz, 1993) </ref> by Q-learning adepts with the need to guarantee the boundness of the final expected value of an action.
Reference: <author> Sutton R.S., </author> <year> 1988. </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, 1, </volume> <pages> 944. </pages>
Reference: <author> Tsitsiklis J.N, </author> <year> 1993. </year> <title> Asynchronous stochastic approximation and Q-learning. Internal Report from Laboratory for Information and Decision Systems and the Operations Research Center, </title> <publisher> MIT. </publisher>
Reference: <author> Twardowski K., </author> <year> 1993. </year> <title> Credit assignment for pole balancing with learning classifier systems. </title> <booktitle> Proceedings of Fifth International Conference on Genetic Algorithms, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <month> 238245. </month>
Reference: <author> Valenzuela-Rendn M., </author> <year> 1991. </year> <title> The fuzzy classifier system: A classifier system for continuously varying variables. </title> <booktitle> Proceeding of the Fourth International Conference on Genetic Algorithms, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <month> 346353. </month>
Reference: <author> Watkins C. J. C. H., </author> <year> 1989. </year> <title> Learning with delayed rewards. </title>
Reference-contexts: No complementary guidance is provided for helping the exploration/exploitation of the problem space, and therefore the learning agent can rely only on a trial-and-error strategy. Q-learning <ref> (Watkins, 1989) </ref> and classifier systems (CSs) (Booker, Goldberg, Holland, 1989) have been separately proposed as two general frameworks for treating reinforcement learning problems.
Reference: <author> Ph. D. </author> <type> dissertation, </type> <institution> Psychology Department, University of Cambridge, </institution> <address> England. </address>
Reference: <author> Watkins C. J. C. H. and Dayan P., </author> <year> 1992. </year> <title> Technical Note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8, </volume> <pages> 3-4, 279292. </pages>
Reference: <author> Whitehead S.D. and Lin L.J., </author> <year> 1994. </year> <title> Reinforcement learning in non-Markov environments. </title> <journal> Artificial Intelligence, </journal> <note> to appear. </note>
Reference: <author> Wilson S.W., </author> <year> 1985. </year> <title> Knowledge growth in an artificial animal. </title> <booktitle> Proceedings of the First International Conference on Genetic Algorithms and their Applications, </booktitle> <address> J.J. </address>
Reference: <editor> Grefenstette (Ed.), </editor> <publisher> Lawrence Erlbaum, </publisher> <pages> 1623. </pages>
Reference: <author> Wilson S.W., </author> <year> 1994. </year> <title> ZCS: a zeroth level classifier system. Evolutionary Computation, </title> <publisher> MIT Press, to appear. </publisher>
References-found: 41

