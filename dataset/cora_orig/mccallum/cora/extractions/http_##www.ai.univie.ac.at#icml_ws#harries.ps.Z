URL: http://www.ai.univie.ac.at/icml_ws/harries.ps.Z
Refering-URL: http://www.ai.univie.ac.at/icml_ws/program.html
Root-URL: 
Email: mbh@cse.unsw.edu.au  kim@ags.com.au  
Title: Learning stable concepts in domains with hidden changes in context  
Author: Michael Harries Kim Horn 
Address: NSW, Sydney 2052, Australia  Sydney, Australia  
Affiliation: Department of Artificial Intelligence School of Computer Science and Engineering University of  Predictive Strategies Unit Australian Gilt Securities Limited  
Abstract: This paper presents Splice, a batch meta-learning system, designed to learn locally stable concepts in domains with hidden changes in context. The majority of machine learning algorithms assume that target concepts remain stable over time. In many domains this assumption is invalid. For example, financial prediction, medical diagnosis, and network performance are domains in which target concepts may not remain stable. Unstable target concepts are often due to changes in a hidden context. Existing works on learning in the presence of hidden changes in con text use an incremental learning approach.
Abstract-found: 1
Intro-found: 1
Reference: <author> Clark, P. and Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-283. </pages>
Reference-contexts: The ability of decision trees to capture context is associated with the fact that decision tree algorithms use a form of context-sensitive feature selection (CSFS) (Domingos, in press). A number of machine learning algorithms can be regarded as using CSFS including decision tree algorithms (Quinlan, 1993), rule induction algorithms <ref> (Clark and Niblett, 1989) </ref> and ILP systems (Quinlan, 1990). CSFS learning systems select the attributes used in each rule, node or clause in the context of all locally relevant prior selections.
Reference: <author> Domingos, P. </author> <title> (in press). Context-sensitive feature selection for lazy learners. </title> <note> to appear in Artificial Intelligence Review. </note>
Reference: <author> Harries, M. and Horn, K. </author> <year> (1995). </year> <title> Detecting concept drift in financial time series prediction using symbolic machine learning. </title> <editor> In Yao, X., editor, </editor> <booktitle> Eighth Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 91-98, </pages> <address> Singapore. </address> <publisher> World Scientific Publishing. </publisher>
Reference-contexts: Two of these are re-training and identification of the relevant local concept. In a real world predictive application, we anticipate that new training could be triggered by poor predictive performance or by recognising that new cases fall in previously unseen areas of attribute space <ref> (Harries and Horn, 1995) </ref>. Re-training could either employ all past cases or employ a window overlapping the previous training set. If the second of these options were used, any new local concepts thus identified by re-training could be added to the set of previously identified local concepts.
Reference: <author> Kilander, F. and Jansson, C. G. </author> <year> (1993). </year> <title> COBBIT a control procedure for COBWEB in the presence of concept drift. </title> <editor> In Brazdil, P. B., editor, </editor> <booktitle> European Conference on Machine Learning, </booktitle> <pages> pages 244-261, </pages> <address> Berlin. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Such an approach was initially suggested in (Kubat, 1989). Examples of this approach include (Widmer and Kubat, 1996), (Kubat and Wid-mer, 1995), <ref> (Kilander and Jansson, 1993) </ref>, and an adaptation in (Salganicoff, 1993). The rationale for windowing approaches is that the window is likely to contain mainly instances from the most recent context, and thus will allow the most current concept to be correctly learnt.
Reference: <author> Kubat, M. </author> <year> (1989). </year> <title> Floating approximation in time-varying knowledge bases. </title> <journal> Pattern Recognition Letters, </journal> <volume> 10 </volume> <pages> 223-227. </pages>
Reference-contexts: The most common approach to learning in domains with hidden changes in context is to generalise from a window that moves over recently past instances and use the learnt concepts for prediction only in the immediate future. Such an approach was initially suggested in <ref> (Kubat, 1989) </ref>. Examples of this approach include (Widmer and Kubat, 1996), (Kubat and Wid-mer, 1995), (Kilander and Jansson, 1993), and an adaptation in (Salganicoff, 1993).
Reference: <author> Kubat, M. and Widmer, G. </author> <year> (1995). </year> <title> Adapting to drift in continuous domains. </title> <booktitle> In Proceedings of the 8th European Conference on Machine Learning, </booktitle> <pages> pages 307-310, </pages> <address> Berlin. </address> <publisher> Springer. </publisher>
Reference-contexts: Such an approach was initially suggested in (Kubat, 1989). Examples of this approach include (Widmer and Kubat, 1996), <ref> (Kubat and Wid-mer, 1995) </ref>, (Kilander and Jansson, 1993), and an adaptation in (Salganicoff, 1993). The rationale for windowing approaches is that the window is likely to contain mainly instances from the most recent context, and thus will allow the most current concept to be correctly learnt.
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266. </pages>
Reference-contexts: A number of machine learning algorithms can be regarded as using CSFS including decision tree algorithms (Quinlan, 1993), rule induction algorithms (Clark and Niblett, 1989) and ILP systems <ref> (Quinlan, 1990) </ref>. CSFS learning systems select the attributes used in each rule, node or clause in the context of all locally relevant prior selections.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <address> San Mateo, California. </address>
Reference-contexts: These can be used to assist domain understanding or for prediction. In this paper, we present a batch meta-learning sys tem, Splice, designed to recognise and extract stable concepts from domains with hidden changes in context. The system utilises an existing machine learning system, C4.5 <ref> (Quinlan, 1993) </ref>. We present the Splice algorithm and evaluate its application on a simple domain drawn from the literature. 1.1 INCREMENTAL `VS' BATCH APPROACHES Machine learning techniques can be broadly cate-gorised as either batch or incremental. <p> The ability of decision trees to capture context is associated with the fact that decision tree algorithms use a form of context-sensitive feature selection (CSFS) (Domingos, in press). A number of machine learning algorithms can be regarded as using CSFS including decision tree algorithms <ref> (Quinlan, 1993) </ref>, rule induction algorithms (Clark and Niblett, 1989) and ILP systems (Quinlan, 1990). CSFS learning systems select the attributes used in each rule, node or clause in the context of all locally relevant prior selections. <p> Furthermore, context could be delineated over two or more dimensions, giving several environmental attributes such as space and time. Throughout this paper, we use time as the environmental attribute. 2 SPLICE Splice uses a meta-level algorithm that incorporates an existing batch learner. In this study we employed C4.5 <ref> (Quinlan, 1993) </ref>, without any modifications, as the underlying learning system. The underlying learner for Splice could, in principle, be replaced by any other CSFS machine learner able to provide splits on time.
Reference: <author> Salganicoff, M. </author> <year> (1993). </year> <title> Density adaptive learning and forgetting. </title> <booktitle> In Machine Learning Proceedings of the Tenth International Conference, </booktitle> <pages> pages 276-283, </pages> <address> San Mateo, California. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Such an approach was initially suggested in (Kubat, 1989). Examples of this approach include (Widmer and Kubat, 1996), (Kubat and Wid-mer, 1995), (Kilander and Jansson, 1993), and an adaptation in <ref> (Salganicoff, 1993) </ref>. The rationale for windowing approaches is that the window is likely to contain mainly instances from the most recent context, and thus will allow the most current concept to be correctly learnt.
Reference: <author> Schlimmer, J. and Granger, Jr., R. </author> <year> (1986). </year> <title> Incremental learning from noisy data. </title> <journal> Machine Learning, </journal> <volume> 1(3) </volume> <pages> 317-354. </pages>
Reference-contexts: Batch systems learn by examining a large collection of instances en masse and forming a single concept. Incremental systems evolve and change a classification scheme as new observations are processed <ref> (Schlimmer and Granger, 1986) </ref>. The window based systems, described above, could all be described as incremental in approach. In many situations, there is no constraint to learn incrementally. For example, many organisations maintain large data bases of historical data that are prime targets for data mining. <p> The third experiment investigates the effect of noise and training duration on Splice's ability to correctly report target local concepts. 3.1 DATA SETS The data sets used in the following experiments are based on those used in Stagger <ref> (Schlimmer and Granger, 1986) </ref> and subsequently used by (Widmer and Kubat, 1996). While our approach and underlying philosophy are substantially different, this allows some comparison of results. The domain chosen is artificial and a program was used to generate the data. <p> In experiment 2, Splice was shown to perform well on a task similar to that approached by both Flora (Wid-mer and Kubat, 1996) and Stagger <ref> (Schlimmer and Granger, 1986) </ref>. The combination of Splice with a simple strategy for selection of the current local concept was demonstrated to be effective on a prediction task. The results show that each change in context is associated with a brief but sharp dip in accuracy.
Reference: <author> Widmer, G. </author> <year> (1996). </year> <title> Recognition and exploitation of contextual clues via incremental meta-learning. </title> <type> Technical Report oefai-96-01, </type> <institution> Austrian Research Institute for Artificial Intelligence. </institution>
Reference-contexts: Such an approach was initially suggested in (Kubat, 1989). Examples of this approach include <ref> (Widmer and Kubat, 1996) </ref>, (Kubat and Wid-mer, 1995), (Kilander and Jansson, 1993), and an adaptation in (Salganicoff, 1993). The rationale for windowing approaches is that the window is likely to contain mainly instances from the most recent context, and thus will allow the most current concept to be correctly learnt. <p> The assumption made is that instances of a context are contiguous in time and hence that intervals of time can be used to delineate contexts. In domains of this nature, quick adaption to new contexts is the primary requirement for learning systems. The Flora family of algorithms <ref> (Widmer and Kubat, 1996) </ref> directly address fast adaptation to new, previously unseen, contexts by dynamically adjusting the window size in response to changes in accuracy and concept complexity. There are a large number of domains in which context can be expected to repeat. <p> The third experiment investigates the effect of noise and training duration on Splice's ability to correctly report target local concepts. 3.1 DATA SETS The data sets used in the following experiments are based on those used in Stagger (Schlimmer and Granger, 1986) and subsequently used by <ref> (Widmer and Kubat, 1996) </ref>. While our approach and underlying philosophy are substantially different, this allows some comparison of results. The domain chosen is artificial and a program was used to generate the data. <p> Table 1 shows the Local Accuracy Matrix built in stage 2 of the algorithm. Figure 3 shows the local concepts learnt from this particular data set. class was randomly selected with a probability of n%. This method for generating noise was chosen to be consistent with <ref> (Widmer and Kubat, 1996) </ref>. 4 In all experiments reported Splice was run with the threshold accuracy parameter set to a default of 10%. The underlying learner, C4.5, was run with default pruning parameters and with subsetting. <p> This suggests a tradeoff between the strength of prediction and the sensitivity to change. It also suggests that incorporating of a non-context specific concept could improve the prediction strategy. In published results on a similar domain the Flora family <ref> (Widmer and Kubat, 1996) </ref> (in particular Flora 3, the learner designed to exploit recurring context) appear to reach much the same level of accuracy as Splice. As expected for an on-line learning approach, however, they require some time to fully reflect changes in context. <p> More complex domains will require more complex methods to identify the relevant local concept. One approach may be to reason about duration and likely sequences of the identified contexts as suggested in <ref> (Widmer, 1996) </ref>. 5 CONCLUSION A new meta-learning algorithm, Splice, enables an underlying machine learning system to learn local concepts from domains with hidden changes in context. We demonstrated that Splice is viable in at least a simple domain and is robust in the presence of noise in the domain presented.
Reference: <author> Widmer, G. and Kubat, M. </author> <year> (1993). </year> <title> Effective learning in dynamic environments by explicit concept tracking. </title> <editor> In Brazdil, P. B., editor, </editor> <booktitle> European Conference on Machine Learning, </booktitle> <pages> pages 227-243, </pages> <address> Berlin. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: There are a large number of domains in which context can be expected to repeat. In these domains, it would be valuable to store existing knowledge about past contexts for re-use. Flora 3 <ref> (Widmer and Ku-bat, 1993) </ref> addresses domains in which contexts recur, and does so as a secondary consideration to the adaptive learner. This is achieved by storing and retrieving concepts that appear stable as the learner traverses the series.
Reference: <author> Widmer, G. and Kubat, M. </author> <year> (1996). </year> <title> Learning in the presence of concept drift and hidden contexts. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 69-101. </pages>
Reference-contexts: Such an approach was initially suggested in (Kubat, 1989). Examples of this approach include <ref> (Widmer and Kubat, 1996) </ref>, (Kubat and Wid-mer, 1995), (Kilander and Jansson, 1993), and an adaptation in (Salganicoff, 1993). The rationale for windowing approaches is that the window is likely to contain mainly instances from the most recent context, and thus will allow the most current concept to be correctly learnt. <p> The assumption made is that instances of a context are contiguous in time and hence that intervals of time can be used to delineate contexts. In domains of this nature, quick adaption to new contexts is the primary requirement for learning systems. The Flora family of algorithms <ref> (Widmer and Kubat, 1996) </ref> directly address fast adaptation to new, previously unseen, contexts by dynamically adjusting the window size in response to changes in accuracy and concept complexity. There are a large number of domains in which context can be expected to repeat. <p> The third experiment investigates the effect of noise and training duration on Splice's ability to correctly report target local concepts. 3.1 DATA SETS The data sets used in the following experiments are based on those used in Stagger (Schlimmer and Granger, 1986) and subsequently used by <ref> (Widmer and Kubat, 1996) </ref>. While our approach and underlying philosophy are substantially different, this allows some comparison of results. The domain chosen is artificial and a program was used to generate the data. <p> Table 1 shows the Local Accuracy Matrix built in stage 2 of the algorithm. Figure 3 shows the local concepts learnt from this particular data set. class was randomly selected with a probability of n%. This method for generating noise was chosen to be consistent with <ref> (Widmer and Kubat, 1996) </ref>. 4 In all experiments reported Splice was run with the threshold accuracy parameter set to a default of 10%. The underlying learner, C4.5, was run with default pruning parameters and with subsetting. <p> This suggests a tradeoff between the strength of prediction and the sensitivity to change. It also suggests that incorporating of a non-context specific concept could improve the prediction strategy. In published results on a similar domain the Flora family <ref> (Widmer and Kubat, 1996) </ref> (in particular Flora 3, the learner designed to exploit recurring context) appear to reach much the same level of accuracy as Splice. As expected for an on-line learning approach, however, they require some time to fully reflect changes in context. <p> More complex domains will require more complex methods to identify the relevant local concept. One approach may be to reason about duration and likely sequences of the identified contexts as suggested in <ref> (Widmer, 1996) </ref>. 5 CONCLUSION A new meta-learning algorithm, Splice, enables an underlying machine learning system to learn local concepts from domains with hidden changes in context. We demonstrated that Splice is viable in at least a simple domain and is robust in the presence of noise in the domain presented.
References-found: 13

