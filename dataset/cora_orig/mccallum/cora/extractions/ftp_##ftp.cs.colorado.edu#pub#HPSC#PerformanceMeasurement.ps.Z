URL: ftp://ftp.cs.colorado.edu/pub/HPSC/PerformanceMeasurement.ps.Z
Refering-URL: http://www.cs.colorado.edu/current/courses/materials.hpsc.html
Root-URL: http://www.cs.colorado.edu
Title: Computer Performance: An Introduction  High Performance Scientific  
Author: Lloyd D. Fosdick Carolyn J. C. Schauble Elizabeth R. Jessup 
Address: Boulder  
Affiliation: University of Colorado at  
Date: September 28, 1995  
Pubnum: Computing  
Abstract-found: 0
Intro-found: 1
Reference: [Adams et al 92] <author> ADAMS, JEANNE C., WALTER S. BRAINERD, JEANNE T. MARTIN, BRIAN T. SMITH, AND JERROLD L. WAGENER. </author> <year> [1992]. </year> <title> Fortran 90 Handbook. </title> <publisher> Intertext Publications. McGraw-Hill Book Company, </publisher> <address> New York, NY. </address>
Reference-contexts: Most manufacturers of vector and parallel computers provide special operating systems and compilers with extensions that exploit the parallel (and vector) mechanisms of the machine. Both Fortran 90 <ref> [Adams et al 92] </ref> and HPF (High Performance Fortran) [Koelbel et al 94] are designed to consolidate multiple extensions into a uniform version of the Fortran programming language, allowing additional operations that could be compiled by different machines into parallel or vector operations.
Reference: [Almasi & Gottlieb 94] <author> ALMASI, GEORGE S. AND ALLAN GOTTLIEB. </author> <year> [1994]. </year> <title> Highly Parallel Computing. </title> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, CA, 2nd edition. </address>
Reference-contexts: Data can be passed from the memory of one processor to the memory of another processor without explicit message-passing, thereby allowing simpler programs. The BBN Butterfly was one of the first of this type of machine; the connection of the processors was done with a butterfly switch network (see <ref> [Almasi & Gottlieb 94] </ref> or [Hwang 93]). The KSR1 by Kendall Square CUBoulder : HPSC Course Notes Computer Performance 59 Research is also a member of this group of multiprocessors; this architecture links 32 processors in a ring and connects the rings in a tree structure.
Reference: [Anderson et al 92] <author> ANDERSON, E., Z. BAI, C. BISCHOF, J. DEMMEL, J. DON-GARRA, J. DUCROZ, A. GREENBAUM, AND S. HAMMARLING. </author> <year> [1992]. </year> <title> LAPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA. </address>
Reference-contexts: While LINPACK is still used for benchmarking purposes, it is no longer recommended for solving linear algebra problems. Released in 1992, LAPACK <ref> [Anderson et al 92] </ref> replaces both LINPACK and EISPACK [Smith et al 76], the collection of routines for eigenvalue problems introduced in 1970. LAPACK makes use of the newest, most accurate algorithms in numerical linear algebra.
Reference: [Baer 80] <author> BAER, JEAN-LOUP. </author> <year> [1980]. </year> <title> Computer Systems Architecture. </title> <publisher> Computer Society Press, </publisher> <address> Rockville, MD. </address>
Reference: [Bailey et al 94] <author> BAILEY, DAVID H., ERIC BARSZCZ, LEONARDO DAGUM, AND HORST D. SIMON. </author> <month> [Mar </month> <year> 1994]. </year> <title> NAS Parallel Benchmark results 3-94. </title> <type> RNR Technical Report RNR-94-006, </type> <institution> NASA Ames Research Center. </institution>
Reference-contexts: Current NAS performance data can be found via the mosaic home page: http://www.nas.nasa.gov/RNR/Parallel/NPB For more information, see <ref> [Bailey et al 94] </ref> or contact NAS Parallel Benchmarks NAS Systems Division Mail Stop 2588 NASA Ames Research Center Moffett Field, CA 94035 4 The effect of optimizing compilers One of the goals of the original Fortran compiler was to optimize the generated assembly code as much as possible. <p> Performance figures taken from <ref> [Bailey et al 94] </ref>, [van der Steen 94], and [Dongarra 94].
Reference: [DEC 91] <institution> Digital Equipment Corporation, </institution> <address> Palo Alto, CA 94301. </address> <month> [Dec </month> <year> 1991]. </year> <title> DECstation 5000 Model 240: </title> <type> Technical Overview. </type>
Reference-contexts: Such units include SPECmarks and the other definition of mips. These are discussed in the next section. 2.4 Timing elementary operations In this section we discuss how to time the elementary arithmetic operations on the DECstation 5000/240. This system uses a MIPS R3000 CPU and R3010 FPU chipset <ref> [Kane 88, DEC 91] </ref>, operating at 40 MHz (25 nsec cycle time). Thus we can expect that arithmetic operations require some small multiple of 25 nsec.
Reference: [Dongarra & Sorensen 87] <author> DONGARRA, JACK J. AND DANNY C. SORENSEN. </author> <year> [1987]. </year> <title> SCHEDULE: Tools for developing and analyzing parallel Fortran programs. </title> <editor> In JAMIESON, LEAH J., DENNIS B. GAN-NON, AND ROBERT J. DOUGLASS, editors, </editor> <booktitle> The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 363-394. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Tracing and visualization tools assist the programmer in studying the execution of a parallel program. Schedule is one such tool. During the execution of a program, it collects information in order to provide data-dependency graphs and visual representations of the flow of given tasks through the processors <ref> [Dongarra & Sorensen 87] </ref>. These graphs display the allocation of different parallel tasks to different processors and so show where performance might be improved by a reallocation of those tasks. Another tool called PICL (Portable Instrumented Communication Library) facilitates the collection of traces of send/receive operations in a DM-MIMD machine.
Reference: [Dongarra 94] <author> DONGARRA, J. J. </author> <year> [1994]. </year> <title> Performance of various computers using standard linear equations software. </title> <type> Technical Report CS-89-85, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN 37831. </institution> <note> netlib version as of November 1, 1994. CUBoulder : HPSC Course Notes 68 Computer Performance </note>
Reference-contexts: This is simply the maximum number of flops that a machine can obtain theoretically. It is almost never achieved. According to Dongarra <ref> [Dongarra 94] </ref>, The theoretical peak performance is determined by counting the number of floating-point additions and multiplications (in full precision) that can be completed during a period of time, usually the cycle time of the machine. <p> This information was taken from <ref> [Dongarra 94] </ref> and [van der Steen 94]. <p> The SPECmarks quoted came from various articles posted on comp.benchmarks or from manufacturers' brochures; the LINPACK values came from <ref> [Dongarra 94] </ref>. 3.4 Common benchmarks Some common benchmarks are described in the following subsections. <p> Both of these benchmarks are described in further detail below. The SPECmarks quoted are from various articles posted on comp.benchmarks or from manufacturers' brochures; the LINPACK values are from <ref> [Dongarra 94] </ref>. CUBoulder : HPSC Course Notes Computer Performance 29 3.4.1 LINPACK In the 1970s, a set of Fortran subroutines was developed to solve systems of linear equations. This set of routines is called LINPACK [Dongarra et al 79]. <p> 6.0 nsec 4.2 nsec Max Memory 1Mbytes 16 Mbytes 128 Mbytes 16 Gbytes Word Size 64 bits 64 bits 64 bits 64 bits Vector Reg Size 64 words 64 words 64 words 64 words Table 3: A comparison of the characteristics of some Cray vector supercomputers, based on data from <ref> [Dongarra 94] </ref> and [Hockney & Jesshope 88]. * R 1 The asymptotic value of R n as n ! 1. <p> This is called the half-performance length of the machine and represents the length of a vector needed to produce half the peak performance. Table 3 summarizes the characteristics of four Cray models while table 4 compares the performance of those four Crays. These figures are based on data from <ref> [Dongarra 94] </ref> and [Hockney & Jesshope 88]. For more information on vector processors and their performance, see [Schauble 95b]. 6.2 Parallel computers A parallel computer is a machine with two or more connected processors that may operate in parallel. Such a machine is also called a multiprocessor. <p> 324 2144 902 10780 Shallow Water (Mflops) | | 560 | 1532 | | R 1 (Mflops) 22 70 70 | | 15000 15000 n 1=2 18 53 53 | | 650 650 Table 4: A comparison of the performance of some Cray vector supercomputers, partially based on data from <ref> [Dongarra 94] </ref>. may act independently of one other on different data, while the processors of a SIMD machine perform the same instruction at the same time but on different data. The category of MIMD multiprocessors is also divided into two subtypes: those with distributed memory and those with shared memory. <p> Performance figures taken from [Bailey et al 94], [van der Steen 94], and <ref> [Dongarra 94] </ref>.
Reference: [Dongarra et al 79] <author> DONGARRA, J. J., C. B. MOLER, J. R. BUNCH, AND G. W. STEWART. </author> <year> [1979]. </year> <title> LINPACK User's Guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA. </address>
Reference-contexts: CUBoulder : HPSC Course Notes Computer Performance 29 3.4.1 LINPACK In the 1970s, a set of Fortran subroutines was developed to solve systems of linear equations. This set of routines is called LINPACK <ref> [Dongarra et al 79] </ref>. Since LINPACK was popular in many areas of scientific computing, it is logical that one of its routines has become the source of a kernel benchmark for comparing the floating-point performance of different machines.
Reference: [Dowd 93] <author> DOWD, KEVIN. </author> <year> [1993]. </year> <title> High Performance Computing. </title> <publisher> OReilly & Associates, Inc., </publisher> <address> Sebastopol, CA. </address>
Reference: [Fox et al 94] <author> FOX, GEOFFREY C., ROY D. WILLIAMS, AND PAUL C. MESSINA. </author> <title> [1994]. </title> <publisher> Parallel Computing Works! Morgan Kaufmann Publishers, Inc., </publisher> <address> San Francisco, CA. </address>
Reference: [Geist et al 95] <author> GEIST, AL, ADAM BEGUELIN, JACK DONGARRA, WEICHING JIANG, ROBERT MANCHEK, AND VAIDY SUNDERAM. </author> <year> [1995]. </year> <title> PVM - Parallel Virtual Machine: A Users' Guide and Tutorial for Network Parallel Computing. Scientific and Engineering Computation. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The PVM (Parallel Virtual Machine) system was developed to handle the message-passing details for this type of multiprocessor. The MPI (Message-Passing Interface) System is currently being developed as a standard for all message-passing machines, including workstation clusters. See <ref> [Geist et al 95] </ref> for more details concerning these languages. 6.2.4 SIMD multiprocessors A SIMD multiprocessor directs all its processors to execute the same instruction at the same cycle, but with different data.
Reference: [Golub & Van Loan 83] <author> GOLUB, G.H. AND C.F. Van LOAN. </author> <year> [1983]. </year> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, 1st edition. </address>
Reference-contexts: In some contexts, all floating-point operations are included in the count of flops. In others, only the multiplications and additions are included. (The LINPACK benchmark uses this definition of the flop.) Yet another definition of flop proposed by C.B. Moler <ref> [Golub & Van Loan 83] </ref> includes some time for access of array elements in addition to multiplications and additions.
Reference: [Heath et al 90] <author> HEATH, M. T., G. A. GEIST, B. W. PEYTON, AND P. H. WOR-LEY. </author> <year> [1990]. </year> <title> A user's guide to PICL. </title> <type> Technical Report ORNL/TM-11616, </type> <institution> Oak Ridge National Laboratory. </institution>
Reference-contexts: Another tool called PICL (Portable Instrumented Communication Library) facilitates the collection of traces of send/receive operations in a DM-MIMD machine. These traces can later plotted by a subtool called ParaGraph to show the parallel execution of the traced program. This display can even be animated <ref> [Heath et al 90] </ref>. 7 Summary As we have seen, many factors may influence the performance of a computer. Because of this, it is difficult to compare the performance of computers with different architectures. Just determining the speed of the CPU of a computer is an intricate task.
Reference: [Hennessy & Patterson 90] <author> HENNESSY, JOHN L. AND DAVID A. PATTERSON. </author> <year> [1990]. </year> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. </address>
Reference: [Higham 93] <author> HIGHAM, NICHOLAS J. </author> <year> [1993]. </year> <title> Handbook of Writing for the Mathematical Sciences. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA. </address>
Reference: [Hockney & Jesshope 88] <author> HOCKNEY, R. W. AND C. R. JESSHOPE. </author> <year> [1988]. </year> <title> Parallel Computers 2. Adam Hilger, </title> <publisher> Bristol. </publisher>
Reference-contexts: nsec Max Memory 1Mbytes 16 Mbytes 128 Mbytes 16 Gbytes Word Size 64 bits 64 bits 64 bits 64 bits Vector Reg Size 64 words 64 words 64 words 64 words Table 3: A comparison of the characteristics of some Cray vector supercomputers, based on data from [Dongarra 94] and <ref> [Hockney & Jesshope 88] </ref>. * R 1 The asymptotic value of R n as n ! 1. <p> Table 3 summarizes the characteristics of four Cray models while table 4 compares the performance of those four Crays. These figures are based on data from [Dongarra 94] and <ref> [Hockney & Jesshope 88] </ref>. For more information on vector processors and their performance, see [Schauble 95b]. 6.2 Parallel computers A parallel computer is a machine with two or more connected processors that may operate in parallel. Such a machine is also called a multiprocessor. <p> The first of these includes the common von Neumann computer; this machine performs one instruction at a time on one set of data. The second set is considered to be empty. <ref> [Hockney & Jesshope 88] </ref> CUBoulder : HPSC Course Notes Computer Performance 55 from manufacturers and universities. The USENET newsgroup named comp.parallel includes discussions of current and new parallel architectures, as well as performance measurements on these machines.
Reference: [Hoffmann et al 88] <author> HOFFMANN, G. R., P. N. SWARZTRAUBER, AND R. A. SWEET. </author> <year> [1988]. </year> <title> Aspects of using multiprocessors for meteroro-logical modelling. </title> <editor> In HOFFMAN, G.-R. AND D. F. SNELLING, editors, </editor> <booktitle> Multiprocessing in Meteorological Models, </booktitle> <pages> pages 127-196. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, NY. </address> <note> CUBoulder : HPSC Course Notes Computer Performance 69 </note>
Reference-contexts: The purpose of the benchmark is to compare the performance of supercomputers on this code. Results are given both in elapsed time and in Mflops <ref> [Sadourney 75, Hoffmann et al 88] </ref>. 3.4.8 SPICE SPICE (Simulation Program with Integrated Circuit Emphasis) is a circuit-level simulator developed in the early 1970s at Berkeley. Many versions of it are still being used to assist in the analysis of processor design today.
Reference: [Hwang 93] <author> HWANG, KAI. </author> <year> [1993]. </year> <title> Advanced Computer Architecture. </title> <publisher> McGraw-Hill, Inc., </publisher> <address> New York, NY. </address>
Reference-contexts: The BBN Butterfly was one of the first of this type of machine; the connection of the processors was done with a butterfly switch network (see [Almasi & Gottlieb 94] or <ref> [Hwang 93] </ref>). The KSR1 by Kendall Square CUBoulder : HPSC Course Notes Computer Performance 59 Research is also a member of this group of multiprocessors; this architecture links 32 processors in a ring and connects the rings in a tree structure.
Reference: [Jessup 95] <author> JESSUP, ELIZABETH R. </author> <year> [1995]. </year> <title> Distributed-memory MIMD computing: An introduction. </title> <booktitle> HPSC Course Notes. </booktitle>
Reference-contexts: This information is included here for completeness. More thorough discussions of the architectures and the performance of these machines can be found in the tutorials on vector computing [Schauble 95b], distributed-memory MIMD computing <ref> [Jessup 95] </ref>, and SIMD computing [Schauble 95a]. 6 6.1 Vector processors A vector computer or vector processor contains a set of special arithmetic units known as vector or arithmetic pipelines. <p> The category of MIMD multiprocessors is also divided into two subtypes: those with distributed memory and those with shared memory. Look at the tutorials on distributed-memory MIMD computing <ref> [Jessup 95] </ref>, and SIMD computing [Schauble 95a] for more information on these multiprocessors. Benchmarks for the purpose of comparing the performance of various parallel computers are still being developed. <p> Another example of a DM-MIMD multiprocessor is the CM-5 by Thinking Machines Corp. Each of its nodes contains four vector processors; hence, this machine can also be described as a distributed-memory/vector-processor MIMD multiprocessor (DM/V-MIMD). For more information on DM-MIMD multiprocessors, see <ref> [Jessup 95] </ref>. The theoretical peak performance for some of the machines mentioned above is given in table 5 on page 58. Table 6 on page 61 compares benchmark results for these machines. <p> Furthermore, communication and synchronization issues in parallel computers introduce additional factors in CUBoulder : HPSC Course Notes 66 Computer Performance fluencing computer performance. More on this can be found in the tutorials on Vector computing [Schauble 95b], distributed-memory MIMD computing <ref> [Jessup 95] </ref>, and SIMD computing [Schauble 95a]. CUBoulder : HPSC Course Notes Computer Performance 67
Reference: [Kane 88] <author> KANE, GERRY. </author> <year> [1988]. </year> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: Such units include SPECmarks and the other definition of mips. These are discussed in the next section. 2.4 Timing elementary operations In this section we discuss how to time the elementary arithmetic operations on the DECstation 5000/240. This system uses a MIPS R3000 CPU and R3010 FPU chipset <ref> [Kane 88, DEC 91] </ref>, operating at 40 MHz (25 nsec cycle time). Thus we can expect that arithmetic operations require some small multiple of 25 nsec.
Reference: [Koelbel et al 94] <author> KOELBEL, CHARLES H., DAVID B. LOVEMAN, ROBERT S. SCHREIBER, JR. GUY L. STEELE, AND MARY E. ZOSEL. </author> <year> [1994]. </year> <title> The High Performance Fortran Handbook. Scientific and Engineering Computation. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Most manufacturers of vector and parallel computers provide special operating systems and compilers with extensions that exploit the parallel (and vector) mechanisms of the machine. Both Fortran 90 [Adams et al 92] and HPF (High Performance Fortran) <ref> [Koelbel et al 94] </ref> are designed to consolidate multiple extensions into a uniform version of the Fortran programming language, allowing additional operations that could be compiled by different machines into parallel or vector operations. Debugging tools for parallel programs are currently under research.
Reference: [Krol 92] <author> KROL, ED. </author> <year> [1992]. </year> <title> The Whole Internet: User's Guide and Catalog. </title> <publisher> OReilly & Associates, Inc., </publisher> <address> Sebastapol, CA. </address>
Reference-contexts: by electronic mail to netlib@netlib.att.com or netlib@ornl.gov with a single line as the message body: send index anonymous ftp: To reach netlib by anonymous ftp, type ftp netlib.att.com and change to the directory called /netlib. xnetlib: In an X-windows environment where xnetlib is installed, the com mand xnetlib 4 See <ref> [Krol 92] </ref> for more on the Internet. CUBoulder : HPSC Course Notes Computer Performance 27 brings up an interactive window directly connected to netlib.
Reference: [McMahon 86] <author> MCMAHON, F. H. </author> <month> [Dec </month> <year> 1986]. </year> <title> The Livermore Fortran Kernels: A computer test of the numerical performance range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA. </address>
Reference-contexts: A similar version of the kernels exists in C. More information on the benchmark, including both code and results, is available from netlib. Also see <ref> [McMahon 86] </ref>. 3.4.3 Whetstones This benchmark is designed to measure floating-point performance. It is a single program originally written in Algol-60, and there is a Fortran version as well.
Reference: [Patterson & Hennessy 94] <author> PATTERSON, DAVID A. AND JOHN L. HENNESSY. </author> <year> [1994]. </year> <title> Computer Organization & Design: The Hardware/Software Interface. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. </address>
Reference: [Sadourney 75] <author> SADOURNEY, ROBERT. </author> <month> [Apr </month> <year> 1975]. </year> <title> The dynamics of finite-difference models of the shallow-water equations. </title> <journal> Journal of Atmospheric Sciences, </journal> <volume> 32 </volume> <pages> 680-689. </pages>
Reference-contexts: The purpose of the benchmark is to compare the performance of supercomputers on this code. Results are given both in elapsed time and in Mflops <ref> [Sadourney 75, Hoffmann et al 88] </ref>. 3.4.8 SPICE SPICE (Simulation Program with Integrated Circuit Emphasis) is a circuit-level simulator developed in the early 1970s at Berkeley. Many versions of it are still being used to assist in the analysis of processor design today.
Reference: [Schauble 95a] <author> SCHAUBLE, CAROLYN J. C. </author> <year> [1995]. </year> <title> SIMD computing: An introduction. </title> <booktitle> HPSC Course Notes. </booktitle>
Reference-contexts: This information is included here for completeness. More thorough discussions of the architectures and the performance of these machines can be found in the tutorials on vector computing [Schauble 95b], distributed-memory MIMD computing [Jessup 95], and SIMD computing <ref> [Schauble 95a] </ref>. 6 6.1 Vector processors A vector computer or vector processor contains a set of special arithmetic units known as vector or arithmetic pipelines. <p> The category of MIMD multiprocessors is also divided into two subtypes: those with distributed memory and those with shared memory. Look at the tutorials on distributed-memory MIMD computing [Jessup 95], and SIMD computing <ref> [Schauble 95a] </ref> for more information on these multiprocessors. Benchmarks for the purpose of comparing the performance of various parallel computers are still being developed. <p> Array processors, such as the ICL DAP, also fall into this category. Table 5, page 58, provides the theoretical peak performance for some SIMD multiprocessors, and table 6, page 61, compares benchmark results for these machines. For more information on SIMD multiprocessors, see <ref> [Schauble 95a] </ref>. 6.2.5 Performance of parallel programs When considering the performance of a parallel program on a multiprocessor, it is usually compared to the performance of the same program on a single processor of that machine. <p> Furthermore, communication and synchronization issues in parallel computers introduce additional factors in CUBoulder : HPSC Course Notes 66 Computer Performance fluencing computer performance. More on this can be found in the tutorials on Vector computing [Schauble 95b], distributed-memory MIMD computing [Jessup 95], and SIMD computing <ref> [Schauble 95a] </ref>. CUBoulder : HPSC Course Notes Computer Performance 67
Reference: [Schauble 95b] <author> SCHAUBLE, CAROLYN J. C. </author> <year> [1995]. </year> <title> Vector computing: An introduction. </title> <booktitle> HPSC Course Notes. </booktitle>
Reference-contexts: Note, by the way, that the final value of X is printed in the rightmost column of the output as a sanity check. 3 See the vector computing tutorial <ref> [Schauble 95b] </ref> for more about recurrences. <p> In this section, we briefly describe the basic design of some supercomputers and the effect of these designs on their performance. This information is included here for completeness. More thorough discussions of the architectures and the performance of these machines can be found in the tutorials on vector computing <ref> [Schauble 95b] </ref>, distributed-memory MIMD computing [Jessup 95], and SIMD computing [Schauble 95a]. 6 6.1 Vector processors A vector computer or vector processor contains a set of special arithmetic units known as vector or arithmetic pipelines. <p> Table 3 summarizes the characteristics of four Cray models while table 4 compares the performance of those four Crays. These figures are based on data from [Dongarra 94] and [Hockney & Jesshope 88]. For more information on vector processors and their performance, see <ref> [Schauble 95b] </ref>. 6.2 Parallel computers A parallel computer is a machine with two or more connected processors that may operate in parallel. Such a machine is also called a multiprocessor. <p> Furthermore, communication and synchronization issues in parallel computers introduce additional factors in CUBoulder : HPSC Course Notes 66 Computer Performance fluencing computer performance. More on this can be found in the tutorials on Vector computing <ref> [Schauble 95b] </ref>, distributed-memory MIMD computing [Jessup 95], and SIMD computing [Schauble 95a]. CUBoulder : HPSC Course Notes Computer Performance 67
Reference: [Smith et al 76] <author> SMITH, B.T., J.M. BOYLE, J.J. DONGARRA, B.S. GARBOW, Y. IKEBE, V.C. KLEMA, AND C.B. MOLER. </author> <year> [1976]. </year> <title> Matrix Eigen-system Routines | EISPACK Guide, </title> <booktitle> volume 6 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <booktitle> 2nd edition. CUBoulder : HPSC Course Notes 70 Computer Performance </booktitle>
Reference-contexts: While LINPACK is still used for benchmarking purposes, it is no longer recommended for solving linear algebra problems. Released in 1992, LAPACK [Anderson et al 92] replaces both LINPACK and EISPACK <ref> [Smith et al 76] </ref>, the collection of routines for eigenvalue problems introduced in 1970. LAPACK makes use of the newest, most accurate algorithms in numerical linear algebra. It also makes efficient use of memory by dividing large matrix operations into smaller block matrix operations.
Reference: [Stone 87] <editor> STONE, HAROLD S., editor. </editor> <booktitle> [1987]. High-Performance Computer Architecture. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, 2nd edition. </address>
Reference: [van der Steen 94] <author> VAN DER STEEN, AAD J. </author> <month> [Sep </month> <year> 1994]. </year> <title> Overview of recent supercomputers. </title> <type> Technical report, </type> <institution> Stichting Nationale Computer Faciliteiten, The Netherlands. </institution> <note> Fourth revised edition, netlib. </note>
Reference-contexts: This information was taken from [Dongarra 94] and <ref> [van der Steen 94] </ref>. <p> See <ref> [van der Steen 94] </ref>. any of these variables is modified, then each of the caches must be updated to reflect the change; this is called the cache coherency problem. <p> Performance figures taken from [Bailey et al 94], <ref> [van der Steen 94] </ref>, and [Dongarra 94].
Reference: [Weicker 84] <author> WEICKER, REINHOLD P. </author> <month> [Oct </month> <year> 1984]. </year> <title> Dhrystone: A synthetic systems programming benchmark. </title> <journal> Communications of the ACM, </journal> <volume> 27(10) </volume> <pages> 1013-1030. </pages> <note> CUBoulder : HPSC Course Notes </note>
Reference-contexts: One problem with this benchmark is that the small loops often fit into the cache of the machine being tested. This means that the results may show a better performance than the user should expect with larger programs. <ref> [Weicker 84] </ref> Again these benchmarks are publically available by ftp from netlib@ornl.gov ftp.nosc.mil:pub/aburto More information is available from netlib. 3.4.5 MIPS The MIPS rating for a computer is determined from a set of small programs known as the MIPS benchmark.
References-found: 32

