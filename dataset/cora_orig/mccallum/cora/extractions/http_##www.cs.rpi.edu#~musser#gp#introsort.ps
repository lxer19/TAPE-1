URL: http://www.cs.rpi.edu/~musser/gp/introsort.ps
Refering-URL: http://www.cs.rpi.edu/~musser/gp/index_1.html
Root-URL: http://www.cs.rpi.edu
Email: musser@cs.rpi.edu  
Title: Introspective Sorting and Selection Algorithms  
Author: David R. Musser 
Keyword: key words Quicksort Heapsort Sorting algorithms Introspective algorithms Hybrid algorithms Generic algorithms STL  
Address: Troy, NY 12180  
Affiliation: Computer Science Department Rensselaer Polytechnic Institute,  
Abstract: Quicksort is the preferred in-place sorting algorithm in many contexts, since its average computing time on uniformly distributed inputs is fi(N log N ) and it is in fact faster than most other sorting algorithms on most inputs. Its drawback is that its worst-case time bound is fi(N 2 ). Previous attempts to protect against the worst case by improving the way quicksort chooses pivot elements for partitioning have increased the average computing time too much|one might as well use heapsort, which has a fi(N log N ) worst-case time bound but is on the average 2 to 5 times slower than quicksort. A similar dilemma exists with selection algorithms (for finding the i-th largest element) based on partitioning. This paper describes a simple solution to this dilemma: limit the depth of partitioning, and for subproblems that exceed the limit switch to another algorithm with a better worst-case bound. Using heapsort as the "stopper" yields a sorting algorithm that is just as fast as quicksort in the average case but also has an fi(N log N ) worst case time bound. For selection, a hybrid of Hoare's find algorithm, which is linear on average but quadratic in the worst case, and the Blum-Floyd-Pratt-Rivest-Tarjan algorithm is as fast as Hoare's algorithm in practice, yet has a linear worst-case time bound. Also discussed are issues of implementing the new algorithms as generic algorithms and accurately measuring their performance in the framework of the C++ Standard Template Library. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. A. R. Hoare. </author> <title> Quicksort. </title> <journal> Computer Journal, </journal> <volume> 5(1) </volume> <pages> 10-15, </pages> <year> 1962. </year>
Reference-contexts: Introduction Among sorting algorithms with O (N log N ) average computing time, median-of-3 quicksort <ref> [1, 2] </ref> is considered to be a good choice in most contexts. It sorts in place, except for fi (log N ) stack space, and is usually faster than other in-place algorithms such as heapsort [3], mainly because it does substantially fewer data assignments and other operations. <p> Assume the three elements chosen by a median-of-3 quicksort algorithm on an array a [1::2k] are a <ref> [1] </ref>; a [k + 1], and a [2k]. Thus for K 2k , the three values chosen are 1; 2, and 2k, and the median value chosen as the pivot for partitioning is 2.
Reference: [2] <author> R. C. </author> <title> Singleton. </title> <journal> Communications of the ACM, </journal> <volume> 12 </volume> <pages> 195-187, </pages> <year> 1969. </year>
Reference-contexts: Introduction Among sorting algorithms with O (N log N ) average computing time, median-of-3 quicksort <ref> [1, 2] </ref> is considered to be a good choice in most contexts. It sorts in place, except for fi (log N ) stack space, and is usually faster than other in-place algorithms such as heapsort [3], mainly because it does substantially fewer data assignments and other operations. <p> While the ideal choice would be the median value, the amount of computation required is too large. Instead, one of the most frequently used methods is to choose the first, middle, and last elements and then choose the median of these three values <ref> [2] </ref>. This method produces good partitions in most cases, including the sorted or almost sorted cases that cause the simpler pivoting methods to blow up. 3 Nevertheless, there are sequences that can cause median-of-3 quicksort to make many bad partitions and take quadratic time. <p> In the partition, the only elements exchanged are k + 1 and 2, resulting in i 1 2 3 k + 3 5 : : : 2k 3 k 1 2k 1 k + 1 4 6 : : : 2k 2 2k where the partition point is after a <ref> [2] </ref> = 2. Now the array a [3::k] now holds a sequence with the structure of K 2k2 , as can be seen by subtracting 2 from every element.
Reference: [3] <author> J. W. J. Williams. </author> <title> Algorithm 132 (heapsort). </title> <journal> Communications of the ACM, </journal> <volume> 7 </volume> <pages> 347-348, </pages> <year> 1964. </year>
Reference-contexts: Introduction Among sorting algorithms with O (N log N ) average computing time, median-of-3 quicksort [1, 2] is considered to be a good choice in most contexts. It sorts in place, except for fi (log N ) stack space, and is usually faster than other in-place algorithms such as heapsort <ref> [3] </ref>, mainly because it does substantially fewer data assignments and other operations.
Reference: [4] <author> D. R. Musser and A. A. Stepanov. </author> <title> Algorithm-oriented generic libraries. </title> <journal> Software Practice and Experience, </journal> <volume> 24(7), </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: These characteristics make median-of-3 quicksort a good candidate for a standard library sorting routine, and it is in fact used as such in the C++ Standard Template Library (STL) <ref> [4, 5, 6] </ref> as well as in older C libraries|it is the algorithm most commonly used for qsort, for example. <p> Specifically, instead of passing an array base address and using integer indexing, the boundaries of the sequence are passed via iterators, which are a generalization of ordinary C/C++ pointers. The iterator type is a template parameter of the algorithms, as in <ref> [4, 5] </ref>. <p> The most useful way of defining a counting class is as an adaptor <ref> [4] </ref>, which is a generic component that takes another component and outfits it with a new interface, or provides the same interface but with different or additional "behind-the-scenes" functionality.
Reference: [5] <author> A. A. Stepanov and M. Lee. </author> <title> The Standard Template Library, </title> <type> Technical Report HPL-94-34, </type> <institution> Hewlett-Packard Laboratories, </institution> <month> May 31, </month> <year> 1994, </year> <note> revised October 31, </note> <institution> 1995; incorporated into Accredited Standards Committee X3 (American National Standards Institute), </institution> <note> Information Processing Systems, Working Paper for Draft Proposed International Standard for Information Processing Systems|Programming Language C++. Doc. No. X3J16/96-0185, WG21/N0785, </note> <month> April </month> <year> 1995. </year>
Reference-contexts: These characteristics make median-of-3 quicksort a good candidate for a standard library sorting routine, and it is in fact used as such in the C++ Standard Template Library (STL) <ref> [4, 5, 6] </ref> as well as in older C libraries|it is the algorithm most commonly used for qsort, for example. <p> Specifically, instead of passing an array base address and using integer indexing, the boundaries of the sequence are passed via iterators, which are a generalization of ordinary C/C++ pointers. The iterator type is a template parameter of the algorithms, as in <ref> [4, 5] </ref>.
Reference: [6] <author> D. R. Musser and A. Saini. </author> <title> STL Tutorial and Reference Guide: C++ Programming with the Standard Template Library. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1996. </year>
Reference-contexts: These characteristics make median-of-3 quicksort a good candidate for a standard library sorting routine, and it is in fact used as such in the C++ Standard Template Library (STL) <ref> [4, 5, 6] </ref> as well as in older C libraries|it is the algorithm most commonly used for qsort, for example.
Reference: [7] <author> C. A. R. Hoare. </author> <title> Algorithm 63 (partition) and algorithm 65 (find). </title> <journal> Communications of the ACM, </journal> <volume> 4(7) </volume> <pages> 321-322, </pages> <year> 1961. </year>
Reference-contexts: A similar dilemma appears with selection algorithms for finding the i-th smallest element of a sequence. Hoare's algorithm <ref> [7] </ref> based on partitioning has a linear bound in the average case but is quadratic in the worst case. The Blum-Floyd-Pratt-Rivest-Tarjan linear-time worst-case algorithm [8] is much slower on the average than Hoare's algorithm. <p> A counting adaptor is an example of adding both to the interface (for initializing and reporting counts) and internal functionality (incrementing the counts). Introspective Selection Algorithms Hoare's find algorithm <ref> [7] </ref> for selecting the i-smallest element of a sequence is similar to quick-sort, but only one of the two subproblems generated by partitioning must be pursued. With even splits, the computing time is O (N + N=2 + N=4 + : : :), or O (N ).
Reference: [8] <author> M. Blum, R. W. Floyd, V. Pratt, R. L. Rivest, and R. E. Tarjan. </author> <title> Time bounds for selection. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 7(4) </volume> <pages> 448-461, </pages> <year> 1973. </year>
Reference-contexts: A similar dilemma appears with selection algorithms for finding the i-th smallest element of a sequence. Hoare's algorithm [7] based on partitioning has a linear bound in the average case but is quadratic in the worst case. The Blum-Floyd-Pratt-Rivest-Tarjan linear-time worst-case algorithm <ref> [8] </ref> is much slower on the average than Hoare's algorithm. In this paper, we concentrate on the sorting problem and return to the selection problem only briefly in a later section. <p> Another way of obtaining an O (N log N ) worst-case time bound with an algorithm based on quicksort is to install a linear-time median-finding algorithm for choosing partition pivots <ref> [8] </ref>. However the resulting algorithm is useless in practice since the large overhead of the median-finding algorithm makes the overall sorting algorithm much slower than heapsort. This algorithm could however be used as the "stopper" in an introspective algorithm, in place of heapsort.
Reference: [9] <author> J. L. Bentley and M. D. McIlroy. </author> <title> Engineering a sort function. </title> <journal> Software Practice and Experience, </journal> <volume> 23(11), </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: Instead of finding the median exactly, other less expensive alternatives have been suggested, such as adaptively selecting from larger arrays a larger sample of elements from which to estimate the median. A new version of the C library function qsort based on this technique and other improvements <ref> [9] </ref> outperforms median-of-3 versions in most cases, but still takes fi (N 2 ) time in the worst-case. Yet another possibility is to use a randomized version of quicksort, in which pivot elements are chosen at random. <p> However, because the simpler choice of pivot does not partition as evenly as median-of-3 partitioning does, the average time is higher. Another possibility is to apply depth limiting to the algorithm described in <ref> [9] </ref>, which adaptively selects from larger arrays a larger sample of elements from which to estimate the median. Experimentation with this algorithm and other variations of will be the subject of a future paper. Acknowledgments C. Stewart suggested the idea for the class of sequences K N . J.
Reference: [10] <author> C. V. Stewart, </author> <title> Private communication. </title>
Reference-contexts: For example, let k be any even positive integer, and consider the following permutation of 1; 2; : : : ; 2k <ref> [10] </ref>: 1 2 3 4 5 : : : k 2 k 1 k k + 1 k + 2 k + 3 : : : 2k 1 2k j Call this permutation K 2k ; for reasons to be seen, it will also be called a "median-of-3 killer" permutation (or
Reference: [11] <author> M. Li and P. M. B. Vitanyi. </author> <title> Average case complexity under the universal distribution equals worst-case complexity. </title> <journal> Information Processing Letters, </journal> <volume> 42 </volume> <pages> 145-149, </pages> <year> 1992. </year>
Reference-contexts: Indeed, some authors have proposed substituting for the uniform distribution a "universal distribution" that assigns much higher probability to sequences that can be produced by short programs, as can the K N sequences, than to random sequences (those that require programs of length proportional to the sequence length) <ref> [11] </ref>. Under the universal distribution, both quicksort's worst-case and average computing times are fi (N 2 ). The average and worst-case times for heapsort|and introsort|remain fi (N log N ) under the universal distribution.
Reference: [12] <author> A. A. Stepanov, M. Lee, and D. R. Musser. </author> <title> Hewlett-Packard Laboratories reference implementation of the Standard Template Library, source files available from ftp://ftp.cs.rpi.edu/pub/stl. </title>
Reference-contexts: The Algorithm The details of the introsort algorithm are perhaps easiest to understand in terms of its differences from the following version of median-of-3 quicksort|the one used in the HP implementation of the C++ Standard Template Library <ref> [12] </ref> for the sort function. This version uses a constant called size threshold to stop generating subproblems for small sequences, leaving the problem instead for a later pass using insertion sort.
Reference: [13] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Leaving small subproblems to insertion sort is one of the usual optimizations of quicksort; the merits of leaving them until a final pass rather than calling insertion sort immediately are discussed later. As in <ref> [13] </ref>, block structure is indicated in the pseudo-code using indentation rather than bracketing.
Reference: [14] <author> R. Sedgewick. </author> <title> Implementing quicksort programs, </title> <journal> Communications of the ACM, </journal> <volume> 21(10) </volume> <pages> 847-857, </pages> <year> 1978. </year>
Reference-contexts: The one-final-pass position is one of the quicksort optimizations suggested by Sedgewick <ref> [14] </ref>.
Reference: [15] <author> A. LaMarca and R. E. Ladner. </author> <title> The influence of caches on the performance of sorting. </title> <booktitle> Proceedings of Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <month> January </month> <year> 1997. </year>
Reference-contexts: The one-final-pass position is one of the quicksort optimizations suggested by Sedgewick [14]. But with modern memory caches the savings in overhead may be cancelled or outweighed for large arrays, since the complete pass at the end can double the number of cache misses <ref> [15] </ref>. 4 However, in these experiments the many-insertion-sort-call version almost never ran faster than the one-final-pass version, and in some cases ran considerably slower.
References-found: 15

