URL: http://www.cs.cornell.edu/Info/People/cardie/papers/scott-a-exam.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/cardie/cardie.html
Root-URL: 
Email: cardie@cs.cornell.edu  mardis@cs.cornell.edu  
Title: Proposal for A Framework for the High-Precision Identification of Linguistic Relationships  
Author: Claire Cardie Scott Mardis 
Date: May 28, 1997  
Address: Ithaca NY, 14853  Ithaca NY, 14853  
Affiliation: Cornell University  Cornell University  
Abstract: Current research in Information Retrieval and Information Extraction demands high-precision syntactic and semantic information from natural language text. We propose a plan for developing a framework to identify, with high-precision, the linguistic relationships between pairs of words in natural language text. Related research is reviewed and preliminary results are given. In our plan we outline the linguistic identification task: a new way to evaluate NLP systems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Steven Abney. </author> <title> Parsing By Chunks. </title> <editor> In Robert Berwick and Steven Ab-ney and Carol Tenny, editor, </editor> <title> Principle-Based Parsing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: If required, a second subsystem, often using largely semantic clues, can make attachment decisions. This division of labor, between forming chunks and computing attachments may play some part in human sentence processing <ref> [1, 2] </ref>. A number of approaches to partial parsing have been tried. One simple method involves dividing words into two classes: stop-words (typically function words) and non-stop-words [21]. Chunks are sequences of non-stop-words and the sequences of stop- words are termed chinks.
Reference: [2] <author> Steven Abney. </author> <title> Chunks and dependencies: Bringing processing evidence to bear on syntax. </title> <editor> In Jennifer Cole and Georgia Green and Jerry Morgan, editor, </editor> <booktitle> Computational Linguistics and the Foundations of Linguistic Theory, </booktitle> <pages> pages 145-164. </pages> <publisher> CSLI, </publisher> <year> 1995. </year>
Reference-contexts: If required, a second subsystem, often using largely semantic clues, can make attachment decisions. This division of labor, between forming chunks and computing attachments may play some part in human sentence processing <ref> [1, 2] </ref>. A number of approaches to partial parsing have been tried. One simple method involves dividing words into two classes: stop-words (typically function words) and non-stop-words [21]. Chunks are sequences of non-stop-words and the sequences of stop- words are termed chinks.
Reference: [3] <author> Steven Abney. </author> <title> Part-of-Speech Tagging and Partial Parsing. In Ken Church and Steve Young and Gerrit Bloothooft, editor, Corpus-Based Methods in Language and Speech. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, </address> <year> 1996. </year> <month> 19 </month>
Reference-contexts: Partial parsing seeks to avoid these difficulties by sacrificing the goal of a complete and accurate parse. A good overview of partial-parsing techniques is given by Abney <ref> [3] </ref>. By only conducting partial analyses, partial parsers are better equipped to handle a wider variety of constructions, including ungrammatical ones, typically by ignoring anything for which it does not have a good model. Many partial parsers try to recover only the core of noun phrases. <p> One initial idea would be to try to trim rules that 16 lead to such inappropriate "glomming" from the NP grammar. It may suf-fice to use a better, more thoroughly proven noun-phrase chunker from other researchers <ref> [9, 3] </ref>. The PCFG failed in some cases because of attachment decisions. Since the PCFG parser had to construct a full parse, it was forced to make decisions with inadequate information. Additionally, for the PCFG experiment, there were a number of errors that had no single obvious problem.
Reference: [4] <author> Steven Abney. </author> <title> Partial Parsing via Finite-State Cascades. </title> <editor> In John Carroll, editor, </editor> <booktitle> Workshop on Robust Parsing, </booktitle> <pages> pages 8-15, </pages> <year> 1996. </year>
Reference-contexts: Fidditch tries to compute a complete parse but is able to punt when it is unable to correctly determine the proper attachment for a phrase. The Fidditch grammar is fairly complex and hand-coded. Cascades of finite-state machines have also been used in partial parsing <ref> [4] </ref>. Such a cascade is an ordered sequence of finite-state-machines (FSM) each defined by a set of regular expressions and where each FSM operates on the output of the previous FSM. Early FSMs typically correspond to the familiar noun-phrase chunks. Later ones may try to recognize basic phrase structures.
Reference: [5] <author> James Allen. </author> <title> Natural Language Understanding. </title> <journal> Benjamin/Cummings, </journal> <note> Second edition, </note> <year> 1995. </year>
Reference-contexts: Some researchers approach the parsing problem from the point of view that language is a set statistical regularities that may be learned <ref> [11, 5] </ref>. The n-gram model is the logical extreme of this view. Many others base their work on more traditional grammatical models of language. Most of the grammatical approaches come from linguistic theory and are typically based upon models of human linguistic performance. <p> The training text and tag-set are from the Penn TreeBank. The second step was to produce a complete parse on test texts using a best-first PCFG parser based on that presented in Allen <ref> [5] </ref>. We chose a PCFG parser because a) it enjoys wide success as a general parser and b) output parses are associated with probabilities, which we might be able to use for confidence factors in obtaining high-precision.
Reference: [6] <author> Eric Brill. </author> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing. Association for Computational Linguistics, </booktitle> <year> 1992. </year>
Reference-contexts: A summary and analysis of the results follow detailed descriptions of the experiments. 4.1 Experiment 1 The structure of the first experiment is depicted in Figure 5. Initially, all the words (includes punctuation lexemes) are assigned part-of-speech tags using the Brill transformational tagger <ref> [6] </ref>. The training text and tag-set are from the Penn TreeBank. The second step was to produce a complete parse on test texts using a best-first PCFG parser based on that presented in Allen [5].
Reference: [7] <author> Eric Brill. </author> <title> Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach. </title> <booktitle> In Proceedings of the 31th Annual Meeting of the ACL. Association for Computational Linguistics, </booktitle> <year> 1993. </year>
Reference-contexts: PCFG statistics are easy to gather from bracketed corpora such as the Penn TreeBank. Another successful parser incorporating many statistical regularities of language but without an explicit statistical model is that of Brill <ref> [7] </ref>. Brill's parser produced results comparable to many PCFG-based parsers through the use of transformation rules. The parser begins with a sentence bracketed in some regular way, such as all right-branching, and then applies an ordered set of transformations.
Reference: [8] <author> Eugene Charniak. </author> <title> Tree-bank Grammars. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <year> 1996. </year>
Reference-contexts: The geometric mean has been used before and was suggested to us by Charniak <ref> [8] </ref>. We trained our PCFG on 280 files (approx. 150,000 words) from the Penn TreeBank, a set disjoint from the test sentences. Once a sentence has been parsed, a parse-tree traversal algorithm searches the tree for the verb-object pairs that we are identifying.
Reference: [9] <author> Ken Church. </author> <title> A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing, </booktitle> <pages> pages 136-143. </pages> <institution> Association for Computational Linguistics, </institution> <year> 1988. </year>
Reference-contexts: A number of approaches to partial parsing have been tried. One simple method involves dividing words into two classes: stop-words (typically function words) and non-stop-words [21]. Chunks are sequences of non-stop-words and the sequences of stop- words are termed chinks. Another method, by Church <ref> [9] </ref>, involves finding the appropriate noun-phrase brackets by inserting open and close brackets into a part-of-speech sequence. Church used a stochastic model based on the local parts-of-speech. A more traditional parser, that functions as a partial parser, is the Fidditch parser [12]. <p> One initial idea would be to try to trim rules that 16 lead to such inappropriate "glomming" from the NP grammar. It may suf-fice to use a better, more thoroughly proven noun-phrase chunker from other researchers <ref> [9, 3] </ref>. The PCFG failed in some cases because of attachment decisions. Since the PCFG parser had to construct a full parse, it was forced to make decisions with inadequate information. Additionally, for the PCFG experiment, there were a number of errors that had no single obvious problem.
Reference: [10] <author> Michael John Collins. </author> <title> A New Statistical Parser Based on Bigram Lexical Dependencies. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the ACL, </booktitle> <pages> pages 184-191, </pages> <address> San Francisco, CA, 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: From the IE literature, we have selected the pattern-based work of the Autoslog system [20], in conjunction with the CIRCUS project at the University of Massachusetts at Amherst, as being most relevant. From the parsing literature, we outline recent work by Collins <ref> [10] </ref> where a probabilistic parser is built based on relationships between lexical heads. This work distinguishes itself from other parsing research in that its particular statistical model is closely related to our linguistic relationship task. 3.1 Parsing Parsing techniques vary widely based upon their theoretical underpinnings. <p> We believe that such partial parsing approaches are likely to yield better long term results for our task because of their speed and their robustness. 3.4 Collins' Statistical Parser The statistical parser of Collins <ref> [10] </ref> is built on a model that emphasizes the identification of relationships that are of interest to us. The parser combines partial-parsing (noun phrase bracketing) and statistical parsing. The Collins statistical model is based on the probabilities between head-words in parse trees.
Reference: [11] <editor> R. Garside, G. Leech, and G. Sampson. </editor> <title> Computational Analysis of English: </title>
Reference-contexts: Some researchers approach the parsing problem from the point of view that language is a set statistical regularities that may be learned <ref> [11, 5] </ref>. The n-gram model is the logical extreme of this view. Many others base their work on more traditional grammatical models of language. Most of the grammatical approaches come from linguistic theory and are typically based upon models of human linguistic performance.
References-found: 11

