URL: http://www.cs.cmu.edu/afs/cs/project/theo-11/www/kamal/www/papers/emactive-icml98s.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/project/theo-11/www/kamal/www/resume.html
Root-URL: 
Email: mccallum@jprc.com  knigam@cs.cmu.edu  
Title: Employing EM in Pool-Based Active Learning for Text Classification  
Author: Andrew McCallum zy Kamal Nigam 
Keyword: text classification active learning unsupervised learning information retrieval  
Address: 4616 Henry Street Pittsburgh, PA 15213  Pittsburgh, PA 15213  
Affiliation: Just Research  School of Computer Science Carnegie Mellon University  
Note: Submitted (3/98) to the 15th International Conference on Machine Learning (ICML-98).  
Abstract: This paper shows how a text classifier's need for labeled training data can be reduced by a combination of active learning and Expectation Maximization (EM) on a pool of unlabeled data. Query-by-Committee is used to actively select documents for labeling, then EM with a naive Bayes model further improves classification accuracy by concurrently estimating probabilistic labels for the remaining unlabeled documents and using them to improve the model. We also present a metric for better measuring disagreement among committee members; it accounts for the strength of their disagreement and for the distribution of the documents. Experimental results show that our method of combining EM and active learning requires only half as many labeled training examples to achieve the same accuracy as either EM or active learning alone.
Abstract-found: 1
Intro-found: 1
Reference: [ Cohn et al., 1996 ] <author> D. Cohn, Z. Ghahramani, and M. Jordan. </author> <title> Active learning with statistical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 129-145, </pages> <year> 1996. </year>
Reference-contexts: Active learning aims to select the most informative examples for labeling from the pool. Informative examples are those that, if their class label were known, would reduce classification error and variance over the distribution of examples. Some methods measure this expected classification variance reduction directly <ref> [ Cohn et al., 1996 ] </ref> . Another method, Query-by-Committee (QBC), is easier to apply when closed-form calculation of variance would be 2 prohibitively complex (although traditional approaches do not explicitly model the distribution of examples) [ Freund et al., 1997 ] . <p> is: D (P 1 (C)jjP 2 (C)) = j=1 P 1 (c j ) : (7) (3) In addition to preferring documents with differing committee classifications, we incorporate into our disagreement metric a preference for documents that will reduce the classification variance of many other documents (as prescribed by theory <ref> [ Cohn et al., 1996 ] </ref> ). The stream approach approximates this implicitly in that the stream is produced by sampling the underlying distribution. However, we accomplish this more accurately, especially when labeling only a small number of documents, by modeling the document density explicitly.
Reference: [ Cohn, 1994 ] <author> David Cohn. </author> <title> Neural network exploration using optimal experiment design. </title> <booktitle> In NIPS 6, </booktitle> <year> 1994. </year>
Reference-contexts: With more accurate committee members, QBC should pick more informative documents to label. The complete active learning algorithm, both with and without EM, is summarized in Table 1. Unlike previous work in which queries must be generated <ref> [ Cohn, 1994 ] </ref> , and previous work in which the unlabeled data is available only as a stream [ Dagan and Engelson, 1995; Freund et al., 1997 ] , our assumption about the availability of a pool of unlabeled data makes the leverage possible.
Reference: [ Dagan and Engelson, 1995 ] <author> Ido Dagan and Sean P. Engelson. </author> <title> Committee-based sampling for training probabilistic classifiers. </title> <booktitle> In ICML-95, </booktitle> <year> 1995. </year>
Reference-contexts: Query-by-Committee measures expected reductions in classification variance indirectly by creating a set of classifier variants (committee members), and measuring the disagreement among committee members for each potential query. Following theoretical and empirical work on QBC <ref> [ Freund et al., 1997; Dagan and Engelson, 1995 ] </ref> , our committee members are created by sampling classifiers according to the distribution of classifier parameters specified by 6 the training data. <p> The complete active learning algorithm, both with and without EM, is summarized in Table 1. Unlike previous work in which queries must be generated [ Cohn, 1994 ] , and previous work in which the unlabeled data is available only as a stream <ref> [ Dagan and Engelson, 1995; Freund et al., 1997 ] </ref> , our assumption about the availability of a pool of unlabeled data makes the leverage possible. <p> This pool is often present for many real-world tasks in which efficient use of labels is important, especially in text learning. 5 Related Work Our approach to QBC active learning without EM follows that of Dagan and Engelson <ref> [ Dagan and Engelson, 1995 ] </ref> . They use stream-based sampling and vote entropy; instead, we use pool-based sampling and density-weighted KL. Several other studies have investigated active learning for text categorization. Lewis and Gale examine uncertainty sampling and relevance sampling [ Lewis and Gale, 1994; Lewis, 1995 ] .
Reference: [ Dagan et al., 1994 ] <author> Ido Dagan, Fernando Pereira, and Lillian Lee. </author> <title> Similarity-based estimation of word cooccurrence probabilities. </title> <booktitle> In 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1994. </year>
Reference-contexts: However, we accomplish this more accurately, especially when labeling only a small number of documents, by modeling the document density explicitly. We approximate the density estimation by measuring a document's distance to its class centroid, using a "similarity-based" metric proposed for word co-occurrence probabilities <ref> [ Dagan et al., 1994 ] </ref> . This is an appropriate approximation because given the assumption that data is generated according to a mixture model, density is highest near the centroids of the mixture components.
Reference: [ Dempster et al., 1977 ] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM. algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year> <month> 15 </month>
Reference-contexts: This section describes 5 how to use EM to combine these pools for better parameter estimation within the probabilistic framework of the previous section. EM is a class of iterative algorithms for maximum likelihood estimation in problems with incomplete data <ref> [ Dempster et al., 1977 ] </ref> . Given a model of data generation, and data with some missing values, EM will converge to a set of generative parameters that locally maximizes the likelihood of both the labeled and unlabeled data.
Reference: [ Domingos and Pazzani, 1997 ] <author> P. Domingos and M. Pazzani. </author> <title> Beyond independence: Conditions for the optimality of the simple Bayesian classifier. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 103-130, </pages> <year> 1997. </year>
Reference-contexts: The next two sections add EM and active learning by building on this framework. We approach the task of text classification from a Bayesian learning perspective. With four commonly used assumptions <ref> [ Domingos and Pazzani, 1997; Joachims, 1997 ] </ref> about the nature of the generative parametric model, we use training data to calculate Bayes optimal estimates of the model parameters. <p> Despite the fact that the mixture model and word independence assumptions are strongly violated with real-world data, naive Bayes performs classification very well. Domingos and Pazzani discuss why the violation of the word independence assumption does little damage to classification accuracy <ref> [ Domingos and Pazzani, 1997 ] </ref> . 3 Using EM to Incorporate Unlabeled Data When naive Bayes is given just a small set of labeled training data, classification accuracy will suffer because variance in the parameter estimates of the generative model will be high.
Reference: [ Freund et al., 1997 ] <author> Y. Freund, H. Seung, E. Shamir, and N. Tishby. </author> <title> Selective sampling using the query by committee algorithm. </title> <journal> Machine Learning, </journal> <volume> 28 </volume> <pages> 133-168, </pages> <year> 1997. </year>
Reference-contexts: Some methods measure this expected classification variance reduction directly [ Cohn et al., 1996 ] . Another method, Query-by-Committee (QBC), is easier to apply when closed-form calculation of variance would be 2 prohibitively complex (although traditional approaches do not explicitly model the distribution of examples) <ref> [ Freund et al., 1997 ] </ref> . QBC measures the variance indirectly, by examining the disagreement among class labels assigned by a set of classifier variants|the variants being sampled from the probability distribution of classifiers resulting from the labeled training examples. <p> Query-by-Committee measures expected reductions in classification variance indirectly by creating a set of classifier variants (committee members), and measuring the disagreement among committee members for each potential query. Following theoretical and empirical work on QBC <ref> [ Freund et al., 1997; Dagan and Engelson, 1995 ] </ref> , our committee members are created by sampling classifiers according to the distribution of classifier parameters specified by 6 the training data. <p> The complete active learning algorithm, both with and without EM, is summarized in Table 1. Unlike previous work in which queries must be generated [ Cohn, 1994 ] , and previous work in which the unlabeled data is available only as a stream <ref> [ Dagan and Engelson, 1995; Freund et al., 1997 ] </ref> , our assumption about the availability of a pool of unlabeled data makes the leverage possible.
Reference: [ Ghahramani and Jordan, 1994 ] <author> Zoubin Ghahramani and Michael Jordan. </author> <title> Supervised learning from incomplete data via an EM approach. </title> <booktitle> In NIPS 6, </booktitle> <year> 1994. </year>
Reference-contexts: This is a special case of a more general missing values formulation <ref> [ Ghahramani and Jordan, 1994 ] </ref> . In practice, EM is an iterative two-step process. The E-step calculates probabilistic class labels, P (c j jd i ), for every unlabeled document using a current estimate of and Equation 5. <p> Two other studies have used EM to combine labeled and unlabeled data without active learning for classification, but on non-text tasks [ Miller and Uyar, 1997; Shahshahani and Landgrebe, 1994 ] . Ghahramani and Jordan use EM with mixture models to fill in missing values <ref> [ Ghahramani and Jordan, 1994 ] </ref> . 10 6 Experimental Results This section provides evidence that using a combination of active learning and EM does better than using either individually.
Reference: [ Joachims, 1997 ] <author> Thorsten Joachims. </author> <title> A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In ICML-97, </booktitle> <year> 1997. </year>
Reference-contexts: The next two sections add EM and active learning by building on this framework. We approach the task of text classification from a Bayesian learning perspective. With four commonly used assumptions <ref> [ Domingos and Pazzani, 1997; Joachims, 1997 ] </ref> about the nature of the generative parametric model, we use training data to calculate Bayes optimal estimates of the model parameters. <p> The results are based on data sets from UseNet and newswires. 2 The Newsgroups data set, collected by Ken Lang, contains about 20,000 articles evenly divided among 20 UseNet discussion groups <ref> [ Joachims, 1997 ] </ref> . We use five confusable comp.* classes as our data set. When tokenizing this data, we skip the UseNet headers (thereby discarding the subject line); tokens are formed from contiguous alphabetic characters.
Reference: [ Joachims, 1998 ] <author> Thorsten Joachims. </author> <title> Text categorization with Support Vector Machines: Learning with many relevant features. </title> <booktitle> In ECML-98, </booktitle> <year> 1998. </year>
Reference-contexts: On each trial, 20% of the documents are randomly selected for placement in the test set. The `ModApte' train/test split of the Reuters 21578 Distribution 1.0 data set consists of 12902 Reuters newswire articles in 135 overlapping topic categories. Following several other studies <ref> [ Joachims, 1998; Liere and Tadepalli, 1997 ] </ref> we build binary classifiers for each of the 10 most populous classes. We ignore words on a stoplist, but do not use stemming. The resulting vocabulary has 19371 words.
Reference: [ Lewis and Gale, 1994 ] <author> D. Lewis and W. Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of ACM SIGIR Conference, </booktitle> <year> 1994. </year>
Reference-contexts: They use stream-based sampling and vote entropy; instead, we use pool-based sampling and density-weighted KL. Several other studies have investigated active learning for text categorization. Lewis and Gale examine uncertainty sampling and relevance sampling <ref> [ Lewis and Gale, 1994; Lewis, 1995 ] </ref> . These techniques select queries based on only a single classifier instead of a committee, and thus cannot approximate classification variance reduction. Liere and Tadepalli use committees of Winnow learners for active text learning [ Liere and Tadepalli, 1997 ] . <p> In comparison to previous active learning studies in text classification domains <ref> [ Lewis and Gale, 1994; Liere and Tadepalli, 1997 ] </ref> , the magnitude of our classification accuracy increase is relatively modest. Both of these previous studies consider binary classifiers in which the positive class has very small priors.
Reference: [ Lewis, 1995 ] <author> David D. Lewis. </author> <title> A sequential algorithm for training text classifiers: Corrigendum and additional data. </title> <journal> SIGIR Forum, </journal> <volume> 29(2) </volume> <pages> 13-19, </pages> <year> 1995. </year>
Reference-contexts: They use stream-based sampling and vote entropy; instead, we use pool-based sampling and density-weighted KL. Several other studies have investigated active learning for text categorization. Lewis and Gale examine uncertainty sampling and relevance sampling <ref> [ Lewis and Gale, 1994; Lewis, 1995 ] </ref> . These techniques select queries based on only a single classifier instead of a committee, and thus cannot approximate classification variance reduction. Liere and Tadepalli use committees of Winnow learners for active text learning [ Liere and Tadepalli, 1997 ] .
Reference: [ Liere and Tadepalli, 1997 ] <author> Ray Liere and Prasad Tadepalli. </author> <title> Active learning with committees for text categorization. </title> <booktitle> In AAAI-97, </booktitle> <year> 1997. </year>
Reference-contexts: These techniques select queries based on only a single classifier instead of a committee, and thus cannot approximate classification variance reduction. Liere and Tadepalli use committees of Winnow learners for active text learning <ref> [ Liere and Tadepalli, 1997 ] </ref> . They select documents for which two randomly selected committee members disagree on the class label. Both text studies learn binary classifiers for classes with low frequency, as in our Reuters results, and use only titles instead of full documents. <p> On each trial, 20% of the documents are randomly selected for placement in the test set. The `ModApte' train/test split of the Reuters 21578 Distribution 1.0 data set consists of 12902 Reuters newswire articles in 135 overlapping topic categories. Following several other studies <ref> [ Joachims, 1998; Liere and Tadepalli, 1997 ] </ref> we build binary classifiers for each of the 10 most populous classes. We ignore words on a stoplist, but do not use stemming. The resulting vocabulary has 19371 words. <p> In comparison to previous active learning studies in text classification domains <ref> [ Lewis and Gale, 1994; Liere and Tadepalli, 1997 ] </ref> , the magnitude of our classification accuracy increase is relatively modest. Both of these previous studies consider binary classifiers in which the positive class has very small priors.
Reference: [ Miller and Uyar, 1997 ] <author> David J. Miller and Hasan S. Uyar. </author> <title> A mixture of experts classifier with learning based on both labelled and unlabelled data. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS 9), </booktitle> <year> 1997. </year>
Reference-contexts: In previous work, we show that EM with unlabeled data reduces text classification error by one-third [ Nigam et al., 1998 ] . Two other studies have used EM to combine labeled and unlabeled data without active learning for classification, but on non-text tasks <ref> [ Miller and Uyar, 1997; Shahshahani and Landgrebe, 1994 ] </ref> .
Reference: [ Nigam et al., 1998 ] <author> Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom Mitchell. </author> <title> Learning to classify text from labeled and unlabeled documents. </title> <note> In Submitted to AAAI-98, 1998. http://www.cs.cmu.edu/~mccallum. </note>
Reference-contexts: This paper presents the integration of Active Learning and Expectation Maximization for efficient learning of naive Bayes text classifiers. The resulting method works well with limited amounts of training data by taking advantage of a large pool of unlabeled data. In previous work <ref> [ Nigam et al., 1998 ] </ref> we show that clustering of unlabeled and labeled documents via Expectation Maximization (EM) reduces text classification error by one-third over traditional supervised learning on several real-world data sets. <p> By treating the class labels of the unlabeled data as missing values, and running EM on the entire data set, the resulting parameter estimates give higher classification accuracy for new documents when the pool of labeled examples is small <ref> [ Nigam et al., 1998 ] </ref> . This is a special case of a more general missing values formulation [ Ghahramani and Jordan, 1994 ] . In practice, EM is an iterative two-step process. <p> We initialize the process with parameter estimates using just the labeled training data, and iterate until ^ reaches a fixed point. The resulting ^ has smaller variance in its parameter estimates because it used more documents in forming the estimates. See <ref> [ Nigam et al., 1998 ] </ref> for more details. 4 Active Learning with EM Instead of estimating class labels for unlabeled documents as EM does, active learning requests the true class labels for selected unlabeled documents. <p> Both text studies learn binary classifiers for classes with low frequency, as in our Reuters results, and use only titles instead of full documents. In previous work, we show that EM with unlabeled data reduces text classification error by one-third <ref> [ Nigam et al., 1998 ] </ref> . Two other studies have used EM to combine labeled and unlabeled data without active learning for classification, but on non-text tasks [ Miller and Uyar, 1997; Shahshahani and Landgrebe, 1994 ] . <p> As expected, Random selection and straight QBC give the slowest learning rates, 595 and 365 labelings to reach 75% accuracy respectively. Random-then-EM improves upon both, especially before random performance starts to plateau; it needs 255 labelings to reach 75%. These results are consistent with earlier results in this domain <ref> [ Nigam et al., 1998 ] </ref> . QBC-then-EM is impressive, needing only 165 labelings.
Reference: [ Pereira et al., 1993 ] <author> Fernando Pereira, Naftali Tishby, and Lillian Lee. </author> <title> Distributional clustering of English words. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1993. </year>
Reference-contexts: Rather than tuning a heuristic for picking documents from a stream, we choose the documents from the entire pool with the highest disagreement. (2) Rather than using vote entropy, we measure committee disagreement for each document using Kullback-Leibler divergence to the mean <ref> [ Pereira et al., 1993 ] </ref> . All the unlabeled documents are probabilistically classified by each committee member, resulting in class distributions, P m (Cjd i ), from each committee member m for each document d i .
Reference: [ Shahshahani and Landgrebe, 1994 ] <author> B. Shahshahani and D. Landgrebe. </author> <title> The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon. </title> <journal> IEEE Trans. on Geoscience and Remote Sensing, </journal> <volume> 32(5) </volume> <pages> 1087-1095, </pages> <month> Sept </month> <year> 1994. </year>
Reference-contexts: In previous work, we show that EM with unlabeled data reduces text classification error by one-third [ Nigam et al., 1998 ] . Two other studies have used EM to combine labeled and unlabeled data without active learning for classification, but on non-text tasks <ref> [ Miller and Uyar, 1997; Shahshahani and Landgrebe, 1994 ] </ref> .
Reference: [ Vapnik, 1982 ] <author> V. Vapnik. </author> <title> Estimations of dependences based on statistical data. </title> <publisher> Springer Publisher, </publisher> <year> 1982. </year> <month> 16 </month>
Reference-contexts: These are maximum likelihood estimates, straightforward counting of events, supplemented by simple `smoothing' that primes each word's count with a count of one to avoid probabilities of zero <ref> [ Vapnik, 1982 ] </ref> . We define N (w t ; d i ) to be the count of the number of times word w t occurs in document d i , and define P (c j jd i ) = f0; 1g as given by the document's class label.
References-found: 18

