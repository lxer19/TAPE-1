URL: http://www.eecs.umich.edu/techreports/cse/1996/CSE-TR-316-96.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse96.html
Root-URL: http://www.eecs.umich.edu
Email: thalerd@eecs.umich.edu ravi@eecs.umich.edu  
Title: A Name-Based Mapping Scheme for Rendezvous multicast routing protocol PIMv2 [2] as its mechanism for
Author: David G. Thaler and Chinya V. Ravishankar 
Note: HRW has now been adopted by the  
Date: November 13, 1996  
Address: Ann Arbor, Michigan 48109-2122  
Affiliation: Electrical Engineering and Computer Science Department The University of Michigan,  
Abstract: Clusters of identical intermediate servers are often created to improve availability and robustness in many domains. The use of proxy servers for the WWW and of Rendezvous Points [1] in multicast routing are two such situations. However, this approach is inefficient if identical requests are received and processed by multiple servers. We present an analysis of this problem, and develop a method called the Highest Random Weight (HRW) Mapping that eliminates these difficulties. Given an object name, HRW maps it to a server within a given cluster using the object name, rather than any a priori knowledge of server states. Since HRW always maps a given object name to the same server within a given cluster, it may be used locally at client sites to achieve consensus on object-server mappings. We present an analysis of HRW and validate it with simulation results showing that it gives faster service times than traditional request allocation schemes such as round-robin or least-loaded, and adapts well to changes in the set of servers. HRW is particularly applicable to domains in which there are a large number of requestable objects, and where there is a significant probability that a requested object will be requested again. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Stephen Deering, Deborah Estrin, Dino Farinacci, Van Jacobson, Ching-Gung Liu, and Liming Wei. </author> <title> An architecture for wide-area multicast routing. </title> <booktitle> In Proceedings of the ACM SIGCOMM, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: In an eager-provider scheme, the provider initiates contact with the proxy, and provides it with the object data. In this case, the proxy server need not know the provider's identity. For example, in multicast routing protocols such as CBT [6] and PIM <ref> [1] </ref>, receivers' routers request data for a specific session by sending a join request towards the root of a distribution tree for that session, and sources send data to a session via the root of its tree. <p> The first is in an eager-provider domain, and the second is in a lazy-server domain. 7.1 Eager-Provider Case Study: Shared-Tree Multicast Routing In shared-tree multicast routing protocols such as PIM <ref> [1] </ref> and CBT [6], receivers' routers request packets for a specific session by sending a "join session" request toward the root of a distribution tree for that session. Sources send data to a session by sending it toward the root of its tree. <p> The conditions of Corollary 1 are thus satisfied, and the situation is ideal for the use of HRW. We focus on Sparse-Mode PIM in particular, since its evolution illustrates many of the concepts and goals discussed in Sections 2 and 3. The original description of PIMv1 <ref> [1] </ref> allowed a separate cluster of servers (i.e., RPs) per object (i.e., session), causing the the state requirement for cluster information to grow rapidly as the number of objects grew. PIMv1 also did not specify any mapping algorithm for assigning join requests to servers.
Reference: [2] <author> Estrin, Farinacci, Helmy, Thaler, Deering, Handley, Jacobson, Liu, Sharma, and Wei. </author> <title> Protocol independent multicast-sparse mode (PIM-SM): Specification. </title> <type> Internet Draft, </type> <month> September </month> <year> 1996. </year>
Reference-contexts: Therefore, the output of such a hash function must be an ordered list of servers rather than a single server name. In some domains, such as PIMv2 <ref> [2] </ref>, the list of servers is dynamically updated to exclude unreachable servers. In this case, it suffices for the hash function to map a name to a single server. <p> To obviate the need to obtain cluster information per object, Handley and Crowcroft [24] first proposed the idea of using an "algorithmic" mapping to pick a server from a single cluster. PIMv2 <ref> [2] </ref> thus solves the problems just described by using a single cluster for a range of objects. (A single cluster may even be used for all objects, when the range is the entire object namespace). <p> HRW has already been applied to multicast routing, and has been recently incorporated as part of the PIM routing protocol <ref> [2] </ref>. HRW is also applicable to the World Wide Web. WWW clients could improve response time by using HRW to select servers in a cluster, rather than simply using the order presented by DNS.
Reference: [3] <author> Mahadev Satyanarayanan. </author> <title> Scalable, secure, and highly available distributed file access. </title> <journal> IEEE Computer, </journal> <volume> 23(5) </volume> <pages> 9-21, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Such clusters may be formed by grouping providers by location or by functionality, or, more typically, both. Thus, we define a cluster as a set of providers with similar functionality and/or location. For example, the set of fileservers for a specific AFS volume <ref> [3] </ref> is a cluster. In this paper, we will refer to the actual exporters of objects and services as providers, and use the term server in a more generic sense (see Section 1.1). <p> When all servers are equally distant, this mapping is similar to the least-loaded scheme (with the same advantages and disadvantages), since the server with the least load typically responds first. The Harvest [5] web cache implementation and the Andrew File System (AFS) <ref> [3] </ref> both use this method. 3.4 Round-Robin Mapping A simpler scheme is round-robin, where successive requests are sent to consecutive servers. For example, when a name in the Domain Name Service (DNS) resolves to multiple IP addresses, DNS returns this list of IP addresses, rotated circularly after each request. <p> the request rate increases (i.e., when large batches of requests arrive within a short period of time). 3.5 Random Mapping Another way to balance the expected number of requests assigned to each server, is to send requests to a random server (e.g., f (i) = random ()), as suggested in <ref> [3] </ref>. This is referred to in queueing theory as a random split. 10 As before, let N be the number of requests in the batch or train. Let r be a random variable describing the service time for one request.
Reference: [4] <author> James Gwertzman and Margo Seltzer. </author> <title> World-wide web cache consistency. </title> <booktitle> In Proceedings of the 1996 USENIX Technical Conference, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: For example, in the World Wide Web (WWW), pages can be cached at proxy servers <ref> [4, 5] </ref>. All client requests can then go through a local proxy server. If the proxy server has the page cached, the page is returned to the client without accessing the remote provider. Otherwise, the page is retrieved and cached for future use. <p> One solution to this problem is to cache web pages at HTTP proxies <ref> [4, 5, 25] </ref>. Client requests then go through a local proxy server. If the proxy server has the page cached, the page is returned to the client without accessing the remote server. Otherwise, the page is retrieved and cached for future use.
Reference: [5] <author> C. Mic Bowman, Peter B. Danzig, Darren R. Hardy, Udi Manber, and Michael F. Schwartz. </author> <title> The Harvest information discovery and access system. </title> <booktitle> Proceedings of the Second International World Wide Web Conference, </booktitle> <pages> pages 763-771, </pages> <month> October </month> <year> 1994. </year> <note> Available from ftp://ftp.cs.colorado.edu/pub/cs/techreports/schwartz/Harvest.Conf.ps.Z. </note>
Reference-contexts: For example, in the World Wide Web (WWW), pages can be cached at proxy servers <ref> [4, 5] </ref>. All client requests can then go through a local proxy server. If the proxy server has the page cached, the page is returned to the client without accessing the remote provider. Otherwise, the page is retrieved and cached for future use. <p> Thus, f t (i) = (response time for S i ). When all servers are equally distant, this mapping is similar to the least-loaded scheme (with the same advantages and disadvantages), since the server with the least load typically responds first. The Harvest <ref> [5] </ref> web cache implementation and the Andrew File System (AFS) [3] both use this method. 3.4 Round-Robin Mapping A simpler scheme is round-robin, where successive requests are sent to consecutive servers. <p> One solution to this problem is to cache web pages at HTTP proxies <ref> [4, 5, 25] </ref>. Client requests then go through a local proxy server. If the proxy server has the page cached, the page is returned to the client without accessing the remote server. Otherwise, the page is retrieved and cached for future use.
Reference: [6] <author> Tony Ballardie, Paul Francis, and Jon Crowcroft. </author> <title> An architecture for scalable inter-domain multicast routing. </title> <booktitle> In Proceedings of the ACM SIGCOMM, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: In an eager-provider scheme, the provider initiates contact with the proxy, and provides it with the object data. In this case, the proxy server need not know the provider's identity. For example, in multicast routing protocols such as CBT <ref> [6] </ref> and PIM [1], receivers' routers request data for a specific session by sending a join request towards the root of a distribution tree for that session, and sources send data to a session via the root of its tree. <p> One option is for clients to be statically configured with the list of servers. For example, instead of configuring a WWW browser with a single HTTP proxy, it might be configured with a list of proxies. This approach is also used in the CBT <ref> [6] </ref> multicast routing protocol. Since manual configuration can make management time-consuming when the list changes, it is often more reasonable to have all clients dynamically obtain the list of servers from a common location. <p> The first is in an eager-provider domain, and the second is in a lazy-server domain. 7.1 Eager-Provider Case Study: Shared-Tree Multicast Routing In shared-tree multicast routing protocols such as PIM [1] and CBT <ref> [6] </ref>, receivers' routers request packets for a specific session by sending a "join session" request toward the root of a distribution tree for that session. Sources send data to a session by sending it toward the root of its tree.
Reference: [7] <author> P. Mockapetris. </author> <title> Domain names concepts and facilities, </title> <month> November </month> <year> 1987. </year> <month> RFC-1034. </month>
Reference-contexts: Thus, a second possibility is for clients to use a name service to resolve the list of servers in a cluster. A good example of such a lazy (or "pull") scheme for retrieving this list is the Domain Name System (DNS) <ref> [7] </ref>. With DNS, a client can resolve a hostname into a list of IP addresses. Thus, if each IP address refers to one server, a lookup on a well-known hostname representing the request class can retrieve the list of server addresses.
Reference: [8] <author> Vern Paxson and Sally Floyd. </author> <title> Wide-area traffic: The failure of poisson modeling. </title> <booktitle> In Proceedings of the ACM SIGCOMM, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: Theorem 1 below allows us to obtain the mean and variance of such a sample from the mean and variance of the original distribution. Thus, we need only consider the characteristics of the distribution of load on a single server. It is well-known (e.g., <ref> [8] </ref>) that patterns of requests can be very bursty when the request stream includes machine-initiated requests, so that the arrival process of requests is not in general Poisson.
Reference: [9] <author> Raj Jain and Shawn A. Routhier. </author> <title> Packet trains measurements and a new model for computer network traffic. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 4(6) </volume> <pages> 986-995, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: It is well-known (e.g., [8]) that patterns of requests can be very bursty when the request stream includes machine-initiated requests, so that the arrival process of requests is not in general Poisson. The Packet Train <ref> [9] </ref> model is a widely-used alternative to the traditional Poisson arrival model, and appears to model such request patterns well. We therefore adopt this model, and assume that requests arrive in batches, or "trains". <p> We now demonstrate formally that round-robin achieves load balancing when the request rate is high. As discussed earlier, we will use the packet-train model <ref> [9] </ref> for our analysis. Let N be the number of requests in the batch or train. Let r be a random variable describing the service time for one request.
Reference: [10] <author> Marek Fisz. </author> <title> Probability Theory and Mathematical Statistics. </title> <publisher> John Wiley & sons, Inc., </publisher> <year> 1963. </year>
Reference-contexts: Then, E [^] = , and E [^oe 2 ] = m oe 2 . Proof: See any standard book dealing with sampling theory, <ref> [10] </ref>, for example. fl We are ultimately interested in the conditions when the variance of the loads across the m servers tends to zero. Theorem 1 tell us that this happens precisely when the variance of the load on a single server tends to zero.
Reference: [11] <author> Peter B. Danzig, Richard S. Hall, and Michael F. Schwartz. </author> <title> A case for caching file objects inside internetworks. </title> <type> Technical Report CU-CS-642-93, </type> <institution> University of Colorado, Boulder, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Replication can also reduce hit rates in lazy and eager-server caching schemes by decreasing the effective cache size of the servers in the cluster. For example, a study conducted in 1992 reported <ref> [11] </ref> that a 4 GB cache was necessary for intermediaries to achieve a cache hit rate of 45% for FTP transfers.
Reference: [12] <author> Mark S. Squillante and Edward D. Lazowska. </author> <title> Using processor-cache affinity information in shared-memory multiprocessor scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> February </month> <year> 1993. </year> <month> 29 </month>
Reference-contexts: Cache-Affinity Scheduling It is possible to view the case where all clients send requests for the same object to the same server as defining affinities between objects and servers in a cluster (Figure 2). A number of studies (e.g., <ref> [12, 13, 14] </ref>) have examined the related notion of "cache-affinity" scheduling in the context of shared-memory multiprocessors, in which tasks are sent to processors which already have data cached. This achieves higher cache hit rates at the possible expense of load balancing.
Reference: [13] <author> Raj Vaswani and John Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multiprogrammed, shared memory multiprocessors. </title> <booktitle> In Proc. 13th Symp. Operating Syst. Principles, </booktitle> <pages> pages 26-40, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Cache-Affinity Scheduling It is possible to view the case where all clients send requests for the same object to the same server as defining affinities between objects and servers in a cluster (Figure 2). A number of studies (e.g., <ref> [12, 13, 14] </ref>) have examined the related notion of "cache-affinity" scheduling in the context of shared-memory multiprocessors, in which tasks are sent to processors which already have data cached. This achieves higher cache hit rates at the possible expense of load balancing.
Reference: [14] <author> James D. Salehi, James F. Kurose, and Don Towsley. </author> <title> The effectiveness of affinity-based scheduling in multiprocessor network protocol processing (extended version). </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 4(4) </volume> <pages> 516-530, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Cache-Affinity Scheduling It is possible to view the case where all clients send requests for the same object to the same server as defining affinities between objects and servers in a cluster (Figure 2). A number of studies (e.g., <ref> [12, 13, 14] </ref>) have examined the related notion of "cache-affinity" scheduling in the context of shared-memory multiprocessors, in which tasks are sent to processors which already have data cached. This achieves higher cache hit rates at the possible expense of load balancing.
Reference: [15] <author> Cisco Systems. </author> <title> Scaling the world wide web. </title> <note> Available from http://cio.cisco.com/warp/public/751/advtg/swww wp.htm. </note>
Reference-contexts: In the worst case, all clients issue requests to the same, previously idle, server, resulting in a very high load. However, this is exactly the approach taken in many existing systems. For example, Cisco's LocalDirec-tor <ref> [15] </ref>, which redirects WWW requests to one of a set of local servers, periodically queries the servers for status information.
Reference: [16] <author> Reid G. Smith. </author> <title> The Contract Net protocol: High-level communication and control in a distributed problem solver. </title> <journal> ACM Transactions on Computers, </journal> <pages> pages 1104-1113, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: However, this is exactly the approach taken in many existing systems. For example, Cisco's LocalDirec-tor [15], which redirects WWW requests to one of a set of local servers, periodically queries the servers for status information. In the Contract Net protocol <ref> [16] </ref>, servers are queried for load information when a request is ready. 3.3 Fastest-Response Mapping In the fastest-response scheme, a client "pings" the servers, and picks the one that responds first. Thus, f t (i) = (response time for S i ).
Reference: [17] <author> T. Brisco. </author> <title> DNS support for load balancing, </title> <month> April </month> <year> 1995. </year> <month> RFC-1794. </month>
Reference-contexts: If clients use the first address on this list, requests 9 will be sent to the various IP addresses in round-robin fashion, thereby balancing the number of requests sent to each <ref> [17] </ref>. To get an ordered list, round-robin is logically equivalent to assigning weights in our model as: f 0 (i) = i (mod m) where r denotes the number of previous requests sent, and m is the number of servers.
Reference: [18] <author> D. Estrin, A. Helmy, P. Huang, and D. Thaler. </author> <title> PIM RP paper. </title> <booktitle> Work in progress. </booktitle>
Reference-contexts: On the other hand, when server lists are obtained from a "push" name service, this convergence time is simply the time until all clients receive a new sever list from the name service. A more detailed analysis of convergence time can be found in <ref> [18] </ref>. 15 5.1.3 Minimal disruption When a server S i goes down, all objects which hashed to that server will be reassigned (i.e., fk : W eight (k; S i ) &gt; W eight (k; S j ) 8S j 2 S; S j 6= S i ; S j is
Reference: [19] <author> Donald E. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> volume 1. </volume> <publisher> Addison-Wesley, </publisher> <address> 2nd edition, </address> <year> 1973. </year>
Reference-contexts: After changing the summation index appropriately, this reduces to E [p] = m i=1 i It is well-known (see <ref> [19] </ref>, for example) that m X 1 = ln m + fl + 2m m 2 : Substituting above, the theorem follows. fl It is clear that full replication is achieved rather quickly.
Reference: [20] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and intractability: A guide to the theory of NP-completeness, </title> <month> June </month> <year> 1988. </year>
Reference-contexts: Thus, the optimal set of objects to cache at time t in a cache of size C are those maximizing P k2C p k (t), subject to the constraint that P This is an example of the Knapsack problem, which is known to be NP-complete <ref> [20] </ref>. Cache replacement strategies can then be logically viewed as heuristics to solve this problem. Since the future is unknown, they must use local estimates of the current p k (t)'s based on statistics from past history such as recency or frequency of reference.
Reference: [21] <author> Ivan Niven, Herbert S. Zuckerman, and Hugh L. Montgomery. </author> <title> An Introduction to the Theory of Numbers. </title> <publisher> John Wiley & sons, Inc., </publisher> <address> 5th edition, </address> <year> 1991. </year>
Reference-contexts: But A and 2 31 are relatively prime, so a standard result from number theory <ref> [21] </ref> tells us that we may cancel A, leaving us with: (AS i + B) XOR D (k) j (AS j + B) XOR D (k) (mod 2 31 ) Using the fact that modulo-2 31 congruence is preserved under the XOR operation, then by repeating the procedures above, we finally
Reference: [22] <author> Stephen K. Park and Keith W. Miller. </author> <title> Random number generators: Good ones are hard to find. </title> <journal> CACM, </journal> <volume> 31(10) </volume> <pages> 1192-1201, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The first competing weight function we consider is based on the Unix system functions random and srandom in place of rand and srand, resulting in a weight function we denote W random . The second function we consider uses the Minimal Standard random number generator <ref> [22, 23] </ref>, resulting in the weight function: W minstd (k; S i ) = (16807 ((16807 S i ) XOR D (k))) (mod (2 31 1)) 20 WeightSi D (k) (a) W rand Weight Si (b) W rand2 Our third alternative is to modify the W rand function as follows: W
Reference: [23] <author> David G. Carta. </author> <title> Two fast implementations of the "minimal standard" random number generator. </title> <journal> CACM, </journal> <volume> 33(1), </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: The first competing weight function we consider is based on the Unix system functions random and srandom in place of rand and srand, resulting in a weight function we denote W random . The second function we consider uses the Minimal Standard random number generator <ref> [22, 23] </ref>, resulting in the weight function: W minstd (k; S i ) = (16807 ((16807 S i ) XOR D (k))) (mod (2 31 1)) 20 WeightSi D (k) (a) W rand Weight Si (b) W rand2 Our third alternative is to modify the W rand function as follows: W
Reference: [24] <author> Mark Handley and Jon Crowcroft. </author> <title> Hierarchical PIM. </title> <booktitle> In Proceedings of the 34th IETF, </booktitle> <month> December </month> <year> 1995. </year> <note> Slides available at http://www.cs.ucl.ac.uk/staff/M.Handley/hpim.ps. </note>
Reference-contexts: The Static Priority scheme also meant that the liveness of higher-priority servers in the cluster had to be tracked, incurring additional complexity. To obviate the need to obtain cluster information per object, Handley and Crowcroft <ref> [24] </ref> first proposed the idea of using an "algorithmic" mapping to pick a server from a single cluster.
Reference: [25] <author> Stephen Williams, Marc Abrams, Charles R. Standridge, Ghaleb Abdulla, and Edward A. Fox. </author> <title> Removal policies in network caches for world-wide web documents. </title> <booktitle> In Proceedings of ACM SIG-COMM'96, </booktitle> <pages> pages 293-305, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: One solution to this problem is to cache web pages at HTTP proxies <ref> [4, 5, 25] </ref>. Client requests then go through a local proxy server. If the proxy server has the page cached, the page is returned to the client without accessing the remote server. Otherwise, the page is retrieved and cached for future use.
Reference: [26] <author> Marc Abrams, Charles R. Standridge, Ghaleb Abdulla, Stephen Willians, and Edward A. Fox. </author> <title> Caching proxies: Limitations and potentials. </title> <booktitle> Proc. 4th International World-Wide Web Conference, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Client requests then go through a local proxy server. If the proxy server has the page cached, the page is returned to the client without accessing the remote server. Otherwise, the page is retrieved and cached for future use. Various studies (e.g., <ref> [26, 27] </ref>) have found that a cache hit rate of up to 50% can be achieved. Thus, since the number of possible objects is again large, while a significant concentration of requests exists, the conditions are appropriate for HRW. <p> 3.1 Gbytes Unique URLs Requested 93956 Mean unique object size 22882 bytes Unique Bytes Requested 2.1 Gbytes Table 2: Trace Summary Note that about 50% of the URLs requested were unique, giving an upper bound on the cache hit rate of around 50%, which agrees with the bound observed by <ref> [26] </ref> and [27].
Reference: [27] <author> Steven Glassman. </author> <title> A caching relay for the world wide web. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 27(2), </volume> <month> November </month> <year> 1994. </year> <month> 30 </month>
Reference-contexts: Client requests then go through a local proxy server. If the proxy server has the page cached, the page is returned to the client without accessing the remote server. Otherwise, the page is retrieved and cached for future use. Various studies (e.g., <ref> [26, 27] </ref>) have found that a cache hit rate of up to 50% can be achieved. Thus, since the number of possible objects is again large, while a significant concentration of requests exists, the conditions are appropriate for HRW. <p> Unique URLs Requested 93956 Mean unique object size 22882 bytes Unique Bytes Requested 2.1 Gbytes Table 2: Trace Summary Note that about 50% of the URLs requested were unique, giving an upper bound on the cache hit rate of around 50%, which agrees with the bound observed by [26] and <ref> [27] </ref>. <p> By comparison, Glassman <ref> [27] </ref> found the average response time o MISS seen by a client for an uncached page to be between 6 and 9 seconds, compared with o HIT = 1:5 seconds for a cached page, using a Digital Web relay.
Reference: [28] <institution> Netscape Communications Corp. </institution> <note> Netscape Navigator software. Available from http://www.netscape.com. </note>
Reference-contexts: Various studies (e.g., [26, 27]) have found that a cache hit rate of up to 50% can be achieved. Thus, since the number of possible objects is again large, while a significant concentration of requests exists, the conditions are appropriate for HRW. Popular WWW browsers such as Netscape Navigator <ref> [28] </ref>, NCSA Mosaic, and lynx, now allow specifying one or more proxy servers through which requests for remote objects are sent. Relying on a single proxy, however, does not provide any fault tolerance if the proxy goes down. For a robust deployment, multiple proxies are required.
Reference: [29] <author> Carlos R. Cunha, Azer Bestavros, and Mark E. Crovella. </author> <title> Characteristics of WWW client-based traces. </title> <type> Technical Report BU-CS-95-010, </type> <institution> Boston University, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: This will allow the terminology to apply to the more general problem. In the following simulations, the objects and object sizes are taken from the publicly-available WWW client-based traces described in <ref> [29] </ref>, where all URLs accessed from 37 workstations at Boston University were logged over a period of five months.
Reference: [30] <author> Mark R. Nelson. </author> <title> File verification using CRC. </title> <journal> Dr. Dobb's Journal, </journal> <month> May </month> <year> 1992. </year>
Reference-contexts: Since a unique object name k can, in general, have arbitrary length, and we wish to obtain a digest with which we can do 32-bit arithmetic, our simulation defined D (k) to be the 32-bit digest of the object name obtained by computing its CRC-32 <ref> [30] </ref> checksum. 7.2.1 Simulation Our simulator implemented the hash algorithm as described in Section 4, as well as a round-robin scheme, where each request was sent to the next server in succession, and a random allocation scheme, where each request was allocated to an random server.
Reference: [31] <author> Donald Neal. </author> <title> The Harvest object cache in New Zealand. </title> <booktitle> In Fifth International World Wide Web Conference, </booktitle> <month> May </month> <year> 1996. </year> <note> Available at http://www.waikato.ac.nz/harvest/www5/. 31 </note>
Reference-contexts: We expect the improvement to be much more pronounced for 26 2.8 3.2 3.6 4 2 4 6 8 Average Latency (sec) # Servers HRW LeastJobs Random Round-Robin 0 10 20 30 % Speed Improvement # Servers sites with low bandwidth connectivity to the outside world, such as New Zealand <ref> [31] </ref>. assume that no objects are invalidated during the lifetime of the simulation, no time-based expirations were simulated. As shown, hash allocation again achieves a much higher hit rate and lower space requirement as the number of servers increases.
References-found: 31

