URL: ftp://grilled.cs.wisc.edu/technical_papers/hollingsworth_thesis.ps.Z
Refering-URL: http://www.cs.wisc.edu/~paradyn/papers.html
Root-URL: 
Title: FINDING BOTTLENECKS IN LARGE SCALE PARALLEL PROGRAMS  
Author: by JEFFREY KENNETH HOLLINGSWORTH 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1994  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> T. E. Anderson and E. D. Lazowska, Quartz: </author> <title> A Tool for Tuning Parallel Program Performance, </title> <booktitle> 1990 SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> Boston, </address> <month> May </month> <year> 1990, </year> <pages> pp. 115-125. </pages>
Reference-contexts: For example, time spent in a sequential routine has a greater impact on the execution time of a program than the same total time split among three simultaneous instances of a parallel procedure. Anderson and Lazowska developed the Quartz Normalized Process Time (NPT) metric <ref> [1] </ref> to compensate for the 5 6 inequities of treating all CPU time the same. NPT normalizes CPU time for each procedure based on the effective parallelism while that procedure is executing. Normalization assigns relatively higher values (indicating they are more important) to sequential procedures than to parallel ones.
Reference: 2. <author> Z. Aral and I. Gertner, </author> <title> A high-level debugger/profiler architecture for shared-memory multiprocessors, </title> <booktitle> 1988 International Conference on Supercomputing, </booktitle> <address> New York, NY, </address> <year> 1988, </year> <pages> pp. 131-139. </pages>
Reference-contexts: By using these system calls, data that could only be collected by directly instrumenting the operating system is available to external data collection processes. Special collection processes can also be used to gather information about specific applications. For example, parasight <ref> [2] </ref> uses dedicated collection processes to gather both operating system and application statistics via shared memory. 2.2.3. Filtering Using software based instrumentation makes it possible to track a vast number of metrics and events during an application's execution. <p> The constraint definition iterates through a list of message passing functions at metric insertion time and inserts calls to set the counter MSGTagPred to positive if the second argument $argv <ref> [2] </ref> to the function is equal to the target message tag ($constraint). <p> This constraint also clears the counter at the return point from each message passing routine. msgTagFuncs = - "cmmd_send", "cmmd_recv" -; constraint MSGTagPred /SyncObject/MsgTag is counter - foreach func in msgTagFuncs - prepend at lookupFunction (func)-&gt;entry [ if ($arg <ref> [2] </ref> == $constraint) setCounter (MSGTagPred, 1); ] append at lookupFunction (func)-&gt;exit [ setCounter (MSGTagPred, 0); ] - The next example constraint demonstrates the use of the replace clause to define a constraint that (when requested) is used in lieu of the base metric.
Reference: 3. <institution> Butterfly Product Overview, BBN Advanced Computers, Inc., </institution> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: A similar approach is to provide a set of instrumented primitives and require the user to write their applications using these primitives. For example, Instant Replay [53] provides primitives to operate on events, queues, shared memory objects, and processes on the BBN Butterfly <ref> [3] </ref>. Likewise, PIE [82] works with a programming model called MPC [94] which provides parallelism via operations on shared memory. Basing instrumentation on a particular set of primitives is appropriate when the performance monitoring system is being developed in conjunction with the programming model. <p> The implementation of our instrumentation system is divided into two parts. The first part, the Metric Manager, translates requests for metric and resource combinations into primitives and predicates. The second part, the Instrumentation Manager, modifies code sequences in the application foo () . . addCounter (bytes, param <ref> [3] </ref> * param [4]) addCounter (fooCount,1) SendMsg (dest, ptr, cnt, size) - . - foo () . . SendMsg (dest, ptr, cnt, size) - . - addCounter (fooFlag,1) subCounter (fooFlag,1) if (fooFlag) addCounter (msgsSent, 1) 43 being monitored. Figure 5.5 shows the structure of the Dynamic Instrumentation mechanism.
Reference: 4. <author> T. Ball and J. R. Larus, </author> <title> Optimally Profiling and Tracing Programs, </title> <booktitle> 19th ACM Symposium on Principles of Programming Languages, </booktitle> <address> Albuquerque, NM, </address> <month> January 19-22, </month> <year> 1992, </year> <pages> pp. 59-70. </pages>
Reference-contexts: Alternatively the instrumentation can be inserted after the executable image has been built. This approach, called binary re-writing, inserts instrumentation into an object file after it has been compiled and assembled. QPT <ref> [4] </ref>, Mtool [33], jprof [77], and the Stardent Postloader [45] all use this style of instrumentation. Binary rewriting has the advantage that it is language independent, and that compilers and libraries do not need to be modified. <p> The first part, the Metric Manager, translates requests for metric and resource combinations into primitives and predicates. The second part, the Instrumentation Manager, modifies code sequences in the application foo () . . addCounter (bytes, param [3] * param <ref> [4] </ref>) addCounter (fooCount,1) SendMsg (dest, ptr, cnt, size) - . - foo () . . SendMsg (dest, ptr, cnt, size) - . - addCounter (fooFlag,1) subCounter (fooFlag,1) if (fooFlag) addCounter (msgsSent, 1) 43 being monitored. Figure 5.5 shows the structure of the Dynamic Instrumentation mechanism. <p> It would be helpful if additional symbol table information were available that indicated exactly what is code and what is data in a program. Many compilers place read-only data into a program's text (code) segment. This creates problems for post-linker tools (correctness debuggers and performance tools). Larus and Ball <ref> [4] </ref> have also noted this problem, and developed heuristics to differentiate code from data. We use many of their heuristics in our implementation. A better solution was used by Sardent [45].
Reference: 5. <author> P. C. Bates and J. C. Wileden, EDL: </author> <title> A Basis For Distributed System Debugging Tools, </title> <booktitle> 15th Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1982, </year> <pages> pp. 86-93. </pages>
Reference-contexts: However, tools are often limited by the large amount of performance data they can generate. One approach to reduce the amount of data is to filter it and store only interesting events. EDL <ref> [5] </ref>, Segall et. al's receptacles [81], and Jade [46] use predicates and actions to recognize desired events (or sequences of events) and then generate synthetic events.
Reference: 6. <author> P. Bates, </author> <title> Debugging Heterogeneous Distributed Systems Using Event-Based Models of Behavior, </title> <booktitle> ACM SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <address> Madison, WI, </address> <month> May 5-6, </month> <year> 1988, </year> <pages> pp. 11-22. </pages> <note> appears as SIGPLAN Notices, </note> <month> January </month> <year> 1989. </year>
Reference-contexts: Filtering greatly reduces the amount of performance data collected. However, all of these tools force the user to select the desired events prior to starting their program. Several tools incorporate dynamic control of the event recognition and filter process. EBBA <ref> [6, 7] </ref>, based on EDL, permits dynamic insertion of predicates. BEE [16] allows the dynamic control of the filtering of events based on both event type and by insertion of more complex predicates called "event interpreters".
Reference: 7. <author> P. Bates, </author> <title> Distributed Debugging Tools for Heterogeneous Distributed Systems, </title> <booktitle> 8th Int'l Conf. on Distributed Computing Systems, </booktitle> <address> San Jose, Calif., </address> <month> June </month> <year> 1988, </year> <pages> pp. 308-315. </pages>
Reference-contexts: Filtering greatly reduces the amount of performance data collected. However, all of these tools force the user to select the desired events prior to starting their program. Several tools incorporate dynamic control of the event recognition and filter process. EBBA <ref> [6, 7] </ref>, based on EDL, permits dynamic insertion of predicates. BEE [16] allows the dynamic control of the filtering of events based on both event type and by insertion of more complex predicates called "event interpreters".
Reference: 8. <author> A. Beguelin, J. Dongarra, A. Geist and V. S. Sunderam, </author> <title> Visualization and debugging in a heterogeneous environment, </title> <booktitle> IEEE Computer 26, </booktitle> <month> 6 (June </month> <year> 1993), </year> <pages> pp. 88-95. </pages>
Reference-contexts: TOPSYS [9] and JEWEL [51] require that predicates be defined before the program starts execution, but permit dynamic control of which ones are enabled. XAB <ref> [8] </ref> provides dynamic selection of tracing of messages by message type for PVM [25] applications. A limitation of dynamically configured filters is that they necessitate runtime overhead even when no data is being collected.
Reference: 9. <author> T. Bemmerl, A. Bode, P. Braum, O. Hansen, T. Tremi and R. Wismuller, </author> <title> The Design and Implementation of TOPSYS, </title> <institution> TUM-INFO-07-71-440, Technische Universitat Muenchen, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: EBBA [6, 7], based on EDL, permits dynamic insertion of predicates. BEE [16] allows the dynamic control of the filtering of events based on both event type and by insertion of more complex predicates called "event interpreters". TOPSYS <ref> [9] </ref> and JEWEL [51] require that predicates be defined before the program starts execution, but permit dynamic control of which ones are enabled. XAB [8] provides dynamic selection of tracing of messages by message type for PVM [25] applications. <p> Examples of predicates are the first time a selected procedure is called, or when the synchronization wait time is above a selected threshold. This method requires less direct involvement by the user. Existing correctness debuggers (e.g., Spider [86], and TOPSYS <ref> [9] </ref>) use similar predicates. Also, PMPL [26] uses similar predicates to trigger data recording. A third approach is to have programmers annotate their programs with calls to library routines to indicate major parts of the computation.
Reference: 10. <author> D. Bernstein, A. Bolmarcich and K. </author> <title> So, Performance visualization of parallel programs on a shared memory multiprocessor system, </title> <booktitle> International Conference on Parallel Processing (ICPP), </booktitle> <address> University Park, PA, USA, </address> <month> August </month> <year> 1989, </year> <pages> pp. 1-10. </pages>
Reference-contexts: Unfortunately not every visualization is useful for every program (some are only useful for a few programs), and the user is left with the formidable task of selecting appropriate visualizations and resources to display. Moviola [54] and the performance tool for PREFACE <ref> [10] </ref> also provide large libraries of pre-defined visualizations. No matter how many visualizations a tool provides, users will eventually want more. A solution to this problem is being explored in two systems, Pablo [75] and JEWEL [51]. Both of these systems contain toolkits for building visualization modules.
Reference: 11. <author> M. Bishop, </author> <title> Profiling Under UNIX by Patching, </title> <journal> SoftwarePractice & Experience 17, </journal> <month> 10 (Oct. </month> <year> 1987), </year> <pages> pp. 729-739. </pages>
Reference-contexts: They employ sophisticated program analysis techniques to minimize the overhead of instrumentation for data breakpoints. Massalin and Pu [62] have a novel application of code patch-up in their Synthesis operating system. They modify a program to automatically schedule another thread when it is about to block. Bishop <ref> [11] </ref> developed a performance monitor that patches a binary program for performance monitoring. However, his patches inserted breakpoint instructions, and required multiple context switches per instrumentation point (with a considerable performance penalty). Our application of runtime code generation to performance instrumentation is new.
Reference: 12. <author> W. C. Brantley, K. P. McAuliffe and T. A. Ngo, </author> <title> RP3 Performance Monitoring Hardware, in Instrumentation for Future Parallel Computer Systems, </title> <editor> M. Simmons, R. Koskela and I. Bucker (eds), </editor> <publisher> Addison-Wesley, </publisher> <year> 1989, </year> <pages> pp. 35-47. </pages>
Reference-contexts: Passive hardware monitors observe activity on a parallel computer and record results for later analysis. They are often implemented as a set of programmable counters that record events on the bus, in the memory hierarchy, and on the processor. The performance monitors built into the Sequent Symmetry [92], RP3 <ref> [12] </ref>, Power2 (RS-6000)[97] and the PEM Monitor [17] use programmable hardware counters. Advantages of passive monitoring include no perturbation of the observed system, and a simple hardware design. Since these passive monitors record information for all activity on a system, it is difficult to correlate performance information with its source.
Reference: 13. <author> O. Brewer, J. Dongarra and D. Sorensen, </author> <title> Tools to aid in the analysis of memory access patterns for FORTRAN Programs, </title> <booktitle> Parallel Computing 9, </booktitle> <volume> 1 (1988/89), </volume> <pages> pp. 25-35. </pages>
Reference-contexts: We divide the existing performance visualization tools into three categories: single visualizations for one type of bottleneck, a fixed collection of visualizations, and toolkits for building user defined visualizations. A basic visualization is a representation of the physical machine. For example, MAP <ref> [13] </ref> presents memory access patterns for sequential FORTRAN programs running on vector supercomputers. MAP shows memory as a two dimensional grid.
Reference: 14. <author> M. H. Brown, Zeus: </author> <title> a system for algorithm animation and multi-view editing., </title> <booktitle> 1991 IEEE Workshop on Visual Languages, </booktitle> <address> Kobe, Japan, </address> <month> Oct. </month> <year> 1991, </year> <pages> pp. 4-9. </pages>
Reference-contexts: The difficult part is relating events on the physical machine back to an appropriate abstract model. Two approaches have been tried to express these relationships. IVE [29], Voyeur [88], POLKA [90], and Zeus <ref> [14] </ref> require the programmer to make calls to animation routines from their program. A second approach, used in Belvedere [42, 43], provides an event stream and the user defines reduction and animation operations over it.
Reference: 15. <author> J. S. Brown, </author> <title> The Application of Code Instrumentation Technology in the Los Alamos Debugger, </title> <institution> Los Alamos National Laboratory, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Inserting the instrumentation into the program is accomplished by a simple runtime instrumentation compiler, and a mechanism called trampolines. Generating code during program execution is not a new idea. A number of correctness debuggers have been built that modify an executing program for assertion checking and conditional breakpoints. Brown <ref> [15] </ref> developed a debugger for Cray Computers that provided this feature. Kessler at Xerox Parc [47] built a system that used dynamic modification of a program to insert breakpoints. Work has also been done by Wahbe, et. al. on fast data breakpoints [95].
Reference: 16. <author> B. Bruegge, </author> <title> A Portable Platform for Distributed Event Environments, </title> <booktitle> 1991 ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <address> Santa Cruz, CA, </address> <month> May 20-21, </month> <year> 1991, </year> <pages> pp. 184-193. </pages> <note> appears as SIGPLAN Notices, </note> <month> December </month> <year> 1991. </year>
Reference-contexts: However, all of these tools force the user to select the desired events prior to starting their program. Several tools incorporate dynamic control of the event recognition and filter process. EBBA [6, 7], based on EDL, permits dynamic insertion of predicates. BEE <ref> [16] </ref> allows the dynamic control of the filtering of events based on both event type and by insertion of more complex predicates called "event interpreters". TOPSYS [9] and JEWEL [51] require that predicates be defined before the program starts execution, but permit dynamic control of which ones are enabled.
Reference: 17. <author> H. Burkhart and R. Millen, </author> <title> Performance Measurement Tools in a Multiprocessor Environment, </title> <journal> IEEE Trans. Computers 38, </journal> <month> 5 (May </month> <year> 1989), </year> <pages> pp. 725-737. 99 100 </pages>
Reference-contexts: They are often implemented as a set of programmable counters that record events on the bus, in the memory hierarchy, and on the processor. The performance monitors built into the Sequent Symmetry [92], RP3 [12], Power2 (RS-6000)[97] and the PEM Monitor <ref> [17] </ref> use programmable hardware counters. Advantages of passive monitoring include no perturbation of the observed system, and a simple hardware design. Since these passive monitors record information for all activity on a system, it is difficult to correlate performance information with its source.
Reference: 18. <institution> UNICOS File Formats and Special Files Reference Manual, SR-2014 5.0, Cray Research Inc. </institution>
Reference-contexts: Advantages of passive monitoring include no perturbation of the observed system, and a simple hardware design. Since these passive monitors record information for all activity on a system, it is difficult to correlate performance information with its source. The hardware monitor on the Cray Y-MP <ref> [18] </ref> is passive, but it is able to track performance data on a per process basis, albeit with additional overhead during context switching. Per process passive monitoring is useful for application performance debugging in a multi-user environment. However, passive monitors have a number of limitations. <p> Usually, this information can be read by user processes. Also, a number of machines provide hardware based counters that are a source of useful performance information. For example, the Power2 [97], Cray Y-MP <ref> [18] </ref>, and Sequent Symmetry [92] systems provide detailed counters of processor events. We can combine external information with direct instrumentation to get precise information to relate the external events back to specific parts of the program.
Reference: 19. <institution> Cray T3D System Architecture Overview, HR-04033, Cray Research Inc., Eagan, MN. </institution>
Reference-contexts: Data can been presented at the granularity of subroutines, loops, parallel blocks, or case statements. Recently, Cray Research has released a new tool, the MPP Apprentice [98], that recognizes performance problems for message passing programs on the Cray T3D <ref> [19] </ref>. The MPP Apprentice's approach is similar to ATExpert, but it works for a more general programming model. Crovella and LeBlanc's predicate profiling [20] provides a search system to compare different algorithms for the same problem and allow them to evaluate the scalability of a particular algorithm. <p> The spanning tree technique achieves logarithmic time, instead of unit time cost. However, most new machines come with some form of broadcast facility, including the Intel Paragon (though it is currently not accessible to application software), Cray T3D <ref> [19] </ref>, and Meiko CS-2 [41]. Efficient operations such as these are crucial to dealing with the issue of scale in tools for large parallel machines. 5.4. Clock Synchronization An important question for many parallel tools is how to synchronize the timestamps from different processors.
Reference: 20. <author> M. E. Crovella and T. J. LeBlanc, </author> <title> Performance Debugging Using Parallel Performance Predicates, </title> <booktitle> 1993 ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <month> May 17-18, </month> <year> 1993, </year> <pages> pp. 140-150. </pages>
Reference-contexts: Recently, Cray Research has released a new tool, the MPP Apprentice [98], that recognizes performance problems for message passing programs on the Cray T3D [19]. The MPP Apprentice's approach is similar to ATExpert, but it works for a more general programming model. Crovella and LeBlanc's predicate profiling <ref> [20] </ref> provides a search system to compare different algorithms for the same problem and allow them to evaluate the scalability of a particular algorithm. They define a set of rules that test for possible losses in performance of a parallel program.
Reference: 21. <author> J. Csirik, J. B. G. Frenk, M. Labbe and S. Zhang, </author> <title> On the multidimensional vector bin packing., </title> <journal> Acta Cybernetica 9, </journal> <volume> 4 (1990), </volume> <pages> pp. 361-9. </pages>
Reference-contexts: So, we would not wish to re-order the refinements haphazardly. In addition, finding a better ordering is a variation on the the bin packing problem, which is NP-complete [31]. When we consider multiple resource constraints, we have an instance of the multi-dimensional bin packing problem <ref> [21, 30] </ref>. This problem is also NP-complete, and the best known heuristics have a divergence from optimal packing that is linear with the number of dimensions (resources).
Reference: 22. <author> D. DeWitt and R. Gerber, </author> <title> Multiprocessor Hash-Based Join Algorithms, </title> <booktitle> 1985 VLDB Conference, </booktitle> <address> Stockholm, Sweden, </address> <month> August </month> <year> 1985, </year> <pages> pp. 151-164. </pages>
Reference-contexts: Shared Memory Join The third application used in this study is an implementation of a join function for a relational database. It implements a hash-join algorithm using shared memory for inter-process communication <ref> [22] </ref>. The program was written to study shared-memory and shared-nothing join algorithms. We ran the program on a dedicated four processor Sequent Symmetry. Our test case ran for 93 seconds. The Performance Consultant identified one bottleneck in the program due to excessive page faults.
Reference: 23. <author> P. J. Denning, </author> <title> Working Sets Past and Present, </title> <journal> IEEE Transactions on Software Engineering SE-6, </journal> <month> 1 (Jan. </month> <year> 1980), </year> <pages> pp. 64-84. </pages>
Reference-contexts: In the 1950's, programmers drew diagrams by hand of the phase behavior of their programs to manage manual memory overlays [24]. More recently, Denning noted the importance of recognizing phases and phase changes automatically in a memory management system <ref> [23] </ref>. In parallel computing, tools such as IPS-2 [65] provide ways to restrict program measurement to a specific phase of execution. Experience with watching programmers use this feature of IPS-2 convinced us that phase analysis is a major way programmers look for performance bottlenecks.
Reference: 24. <author> J. B. Dennis, </author> <title> Segmentation and the Design of Multiprogrammed Computer Systems, </title> <journal> Journal of the Association for Computing Machinery 12, </journal> <month> 4 (Oct. </month> <year> 1965), </year> <pages> pp. 589-602. </pages>
Reference-contexts: Phase analysis has been used to understand and tune programs for years. In the 1950's, programmers drew diagrams by hand of the phase behavior of their programs to manage manual memory overlays <ref> [24] </ref>. More recently, Denning noted the importance of recognizing phases and phase changes automatically in a memory management system [23]. In parallel computing, tools such as IPS-2 [65] provide ways to restrict program measurement to a specific phase of execution.
Reference: 25. <author> J. Dongarra, A. Geist, R. Manchek and V. S. Sunderam, </author> <title> Integrated PVM framework supports heterogeneous network computing, </title> <booktitle> Computers in Physics 7, 2 (March-April 1993), </booktitle> <pages> pp. 166-174. </pages>
Reference-contexts: TOPSYS [9] and JEWEL [51] require that predicates be defined before the program starts execution, but permit dynamic control of which ones are enabled. XAB [8] provides dynamic selection of tracing of messages by message type for PVM <ref> [25] </ref> applications. A limitation of dynamically configured filters is that they necessitate runtime overhead even when no data is being collected. Also, none of these systems provide any assistance to the user in selecting what events to collect and when to collect them.
Reference: 26. <author> C. E. Fineman and P. J. Hontalas, </author> <title> Selective Monitoring Using Performance Metric Predicates, </title> <booktitle> 1992 Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, Virginia, </address> <month> April 26-29, </month> <year> 1992, </year> <pages> pp. 162-165. </pages>
Reference-contexts: Because performance assertions come from programmers, they can contain precise descriptions of the expected performance. However, writing assertions does create additional work for the programmer. Another tool that employs user defined predicates is PMPL <ref> [26] </ref>. PMPL can be used with existing trace monitors such as AIMS [101] and PICL [32]. PMPL uses user defined predicates to control performance data logging in large scale parallel computers. When a performance predicate is detected, event logs from a circular buffer are written to disk for latter analysis. <p> Examples of predicates are the first time a selected procedure is called, or when the synchronization wait time is above a selected threshold. This method requires less direct involvement by the user. Existing correctness debuggers (e.g., Spider [86], and TOPSYS [9]) use similar predicates. Also, PMPL <ref> [26] </ref> uses similar predicates to trigger data recording. A third approach is to have programmers annotate their programs with calls to library routines to indicate major parts of the computation. This approach can be quite effective, but is not very elegant because it requires the programmer to modify code.
Reference: 27. <author> R. J. Fowler, T. J. LeBlanc and J. M. Mellor-Crummey, </author> <title> An Integrated Approach to Parallel Program Debugging and Performance Analysis on Large-Scale Multiprocessors, </title> <booktitle> SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <address> Madison, WI, </address> <month> May 5-6, </month> <year> 1988, </year> <pages> pp. 163-173. </pages> <note> appears as SIGPLAN Notices, </note> <month> January </month> <year> 1989. </year>
Reference-contexts: However, the type of metrics that can be defined by the user is limited by the base metrics provided by the tool developer. 7 Another way to make performance tools extensible is to provide a toolkit for building metrics. PPUTT <ref> [27] </ref> provides a toolkit for building new metrics and analysis modules built on top of a basic event stream. Extensible systems provide a powerful tool for sophisticated users, and a convenient way for tool developers to explore new metrics.
Reference: 28. <author> J. M. Francioni, L. Albright and J. A. Jackson, </author> <title> Debugging Parallel Programs Using Sound, </title> <booktitle> 1991 ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <address> Santa Cruz, CA, </address> <month> May 20-21, </month> <year> 1991, </year> <pages> pp. 68-75. </pages> <note> appears as SIGPLAN Notices, </note> <month> December </month> <year> 1991. </year>
Reference-contexts: A problem with algorithm animation is that it adds the burden of creating and debugging animations to the process of writing a parallel application. A variation on algorithm animation is auralization <ref> [28, 87, 89] </ref>. Instead of representing a program graphically, auralization animates it using sound, or a combination of sound and pictures.
Reference: 29. <author> M. Friedell, M. LaPolla, S. Kochhar, S. Sistare and J. Juda, </author> <title> Visualizing the Behavior of Massively Parallel Programs, </title> <address> Supercomputing'91 , Albuquerque, NM, </address> <month> Nov. </month> <pages> 18-22, </pages> <year> 1991, </year> <pages> pp. 472-480. </pages>
Reference-contexts: The difficult part is relating events on the physical machine back to an appropriate abstract model. Two approaches have been tried to express these relationships. IVE <ref> [29] </ref>, Voyeur [88], POLKA [90], and Zeus [14] require the programmer to make calls to animation routines from their program. A second approach, used in Belvedere [42, 43], provides an event stream and the user defines reduction and animation operations over it.
Reference: 30. <author> M. R. Garey, R. L. Graham and D. S. Johnson, </author> <title> Resource Constrained Scheduling as Generalized Bin Packing, </title> <booktitle> Combinatorial Theory 21 (1976), </booktitle> <pages> pp. 257-298. </pages>
Reference-contexts: So, we would not wish to re-order the refinements haphazardly. In addition, finding a better ordering is a variation on the the bin packing problem, which is NP-complete [31]. When we consider multiple resource constraints, we have an instance of the multi-dimensional bin packing problem <ref> [21, 30] </ref>. This problem is also NP-complete, and the best known heuristics have a divergence from optimal packing that is linear with the number of dimensions (resources).
Reference: 31. <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability, </title> <year> 1979. </year>
Reference-contexts: So, we would not wish to re-order the refinements haphazardly. In addition, finding a better ordering is a variation on the the bin packing problem, which is NP-complete <ref> [31] </ref>. When we consider multiple resource constraints, we have an instance of the multi-dimensional bin packing problem [21, 30]. This problem is also NP-complete, and the best known heuristics have a divergence from optimal packing that is linear with the number of dimensions (resources).
Reference: 32. <author> G. A. Geist, M. T. Heath, B. W. Peyton and P. H. Worley, </author> <title> PICL A Portable Instrumented Communication Library, </title> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: Because performance assertions come from programmers, they can contain precise descriptions of the expected performance. However, writing assertions does create additional work for the programmer. Another tool that employs user defined predicates is PMPL [26]. PMPL can be used with existing trace monitors such as AIMS [101] and PICL <ref> [32] </ref>. PMPL uses user defined predicates to control performance data logging in large scale parallel computers. When a performance predicate is detected, event logs from a circular buffer are written to disk for latter analysis.
Reference: 33. <author> A. J. Goldberg and J. L. Hennessy, </author> <title> Performance Debugging Shared Memory Multiprocessor Programs with MTOOL, </title> <address> Supercomputing'91 , Albuquerque, NM, </address> <month> Nov. </month> <pages> 18-22, </pages> <year> 1991, </year> <pages> pp. 481-490. </pages>
Reference-contexts: By necessity, the memory hierarchy in parallel computers is more complex than in sequential machines. Shared memory parallel computers typically have several levels of caches and some form of cache coherency protocol. Effective use of this memory hierarchy is essential for high performance applications. MTOOL <ref> [33] </ref> is a metric based tool that includes information about the memory hierarchy. MTOOL compares the observed execution time for an application to its predicted execution time to characterize the influence of the memory hierarchy on the computation. <p> Alternatively the instrumentation can be inserted after the executable image has been built. This approach, called binary re-writing, inserts instrumentation into an object file after it has been compiled and assembled. QPT [4], Mtool <ref> [33] </ref>, jprof [77], and the Stardent Postloader [45] all use this style of instrumentation. Binary rewriting has the advantage that it is language independent, and that compilers and libraries do not need to be modified.
Reference: 34. <author> S. L. Graham, P. B. Kessler and M. K. McKusick, </author> <title> gprof: a Call Graph Execution Profiler, </title> <booktitle> SIGPLAN '82 Symposium on Compiler Construction, </booktitle> <address> Boston, </address> <month> June </month> <year> 1982, </year> <pages> pp. 120-126. </pages>
Reference-contexts: The simplest explanations consist of print statements that describe the type of bottleneck for the hypothesis. More sophisticated explanations report additional information about the program. For example, the explanation for a CPU bottleneck prints a gprof <ref> [34] </ref> style profile table for the current focus along the ``where'' axis. Finally, it is important to understand how we configured the Performance Consultant for our study.
Reference: 35. <author> D. Haban and D. Wybranietz, </author> <title> A Hybrid Monitor for Behavior and Performance Analysis of Distributed Systems, </title> <journal> IEEE Transactions on Software Engineering 16, </journal> <month> 2 (Feb </month> <year> 1990), </year> <pages> pp. 197-211. </pages>
Reference-contexts: Rather than trying to find a single metric to characterize the entire application, many tools provide sorted profiles for several different resources. Some commonly profiled resources are: CPU utilization, synchronization time, disk operations, vectorization, and cache performance. INCAS <ref> [35] </ref>, JEWEL [51], and ANALYZER/SX [48] are examples of this approach. These tools provide a wealth of information to the programmer, but force them to select the appropriate resources to profile their application. Building a tool that includes all potentially useful metrics is difficult. <p> The hardware keeps a count of the event occurrences. Another approach is to have the hardware collector generate trace data and send it to a data reduction station over a separate bus or interconnection network. MultiKron [68, 69], TMP <ref> [35, 99] </ref>, M31 [76] and HYPERMON [60] are examples of this type of monitor. An advantage of hybrid instrumentation is that it eliminates most perturbation of the CPU and inter-connection network, and permits information visible only to the application to be collected.
Reference: 36. <author> G. J. Hansen, C. A. Linthicum and G. Brooks, </author> <title> Experience with a Performance Analyzer for Multithreaded Application, </title> <booktitle> 1990 International Conference on Supercomputing, </booktitle> <address> Amsterdam, </address> <month> June 11-15, </month> <year> 1990, </year> <pages> pp. 124-131. </pages>
Reference-contexts: To collect performance information about individual program constructs, such as loops or statements, instrumenting runtime libraries or primitives is not sufficient. Collecting data at this granularity requires instrumentation be interspersed with the statements of the user's program. CXpa <ref> [36] </ref>, AE [52], Prism [84], and MPP Apprentice [98] use a modified compiler to insert instrumentation at the desired location. Compiler based instrumentation affords access to the wealth of information that is available during compilation. For example, information about data and loop dependencies is difficult to gather without compiler information.
Reference: 37. <author> M. T. Heath and J. A. Etheridge, </author> <title> Visualizing Performance of Parallel Programs, </title> <booktitle> IEEE Software 8, </booktitle> <month> 5 (Sept </month> <year> 1991). </year> <month> 101 </month>
Reference-contexts: The visualization tools discussed so far use a single visualization. Providing a single visualization limits the types of bottlenecks that can be discovered. Several tools have been developed that incorporate multiple visualizations. An example of this type of tool is Paragraph <ref> [37] </ref>, which supports over twenty different types of displays. Many of the displays can be configured to plot values for different resources (e.g., CPU and disk utilization), greatly increasing the number of available performance displays.
Reference: 38. <author> U. Hecksen, R. Klar, W. Kleinoder and F. Kneibl, </author> <title> Measuring Simultaneous Events in a Multiprocessor System, </title> <booktitle> 1982 SIGMETRICS Conference, </booktitle> <month> August </month> <year> 1982, </year> <pages> pp. 77-88. </pages>
Reference-contexts: Second, since there are more event signals than counters, the user must select the appropriate events to monitor. An approach to solve the visibility limitations of passive hardware monitors is a hybrid software and hardware monitor. One example of a hybrid monitor, used in the ZAHLMONITOR <ref> [38] </ref>, is to have the application signal an event occurrence to the hardware by writing to a special range of memory locations. The hardware keeps a count of the event occurrences.
Reference: 39. <author> J. K. Hollingsworth, R. B. Irvin and B. P. Miller, </author> <title> The Integration of Application and System Based Metrics in A Parallel Program Performance Tool, </title> <booktitle> 1991 ACM SIGPLAN Symposium on Principals and Practice of Parallel Programming , Williamsburg, </booktitle> <address> VA, </address> <month> April 21-24 </month> <year> 1991, </year> <pages> pp. 189-200. </pages> <note> appears as SIGPLAN Notices, </note> <month> July </month> <year> 1991. </year>
Reference-contexts: Building a tool that includes all potentially useful metrics is difficult. An alternative is provide a library of metrics, and make the tool extensible to permit users to create their own metrics. One such tool is IPS-2 <ref> [39] </ref> which permits users to create new metrics as algebraic expressions of previously defined metrics. For example, IPS-2 does not have a built-in metric to indicate what fraction of the actual CPU time used went to each process. <p> The Mach Kernel Monitor [56] instruments context switches to trace the state of processes through time. If modifying the operating system is not feasible or desirable, monitoring can be accomplished via dedicated data collection processes. The External Data Collection facility of IPS-2 <ref> [39] </ref> provides this capability. Information from the operating system is gathered by the collector processes and reported to the user. Most operating systems collect many statistics and make this information available via system calls. <p> IPS-2 records event traces during a program's execution. Each event (e.g., procedure call or synchronization operation) contains both wall-clock and process time stamps in addition to some event specific data. In addition to normal IPS-2 instrumentation, we ran the programs with two External Data Collectors <ref> [39] </ref>. External Data Collectors are dedicated sampling processes that 28 29 collect additional information not available via tracing. One collector gathered information about the behavior of the operating system (e.g., page faults, context switch rate). The other collected data about the hardware (e.g., cache miss rates and bus utilization). <p> This flexible approach to finding bottlenecks is an important characteristic of this work. To validate this result, we again used the IPS-2 performance tools. Since we had previously studied this program <ref> [39] </ref>, we recognized the page fault problem as one of the problems in this program. The problem was due to the creation of new user data in the program. <p> Varying the reporting rate affects only our rate of decision making and granularity of phase boundaries; it does not affect the accuracy of the underlying performance data. Collected data is stored in a data structure called a time histogram <ref> [39] </ref>. A time histogram is a fixed-size array whose elements store values of a performance metric for successive time intervals. Two parameters determine the granularity of the data stored in time histograms: initial bucket width (time interval) and number of buckets.
Reference: 40. <author> J. K. Hollingsworth and B. P. Miller, </author> <title> Parallel Program Performance Metrics: A Comparison and Validation, </title> <booktitle> Supercomputing 1992, </booktitle> <address> Minneapolis, MN, </address> <month> November </month> <year> 1992, </year> <pages> pp. 4-13. </pages>
Reference-contexts: Each of these techniques assumes the programmer knows what picture to draw or which metric is appropriate. Unfortunately, no single approach applies to all parallel programs. In one study we conducted <ref> [40] </ref>, four performance metrics were compared head-to-head for a number of real parallel programs. In addition, we employed a technique to calibrate the accuracy of the guidance supplied by the metrics. That study showed that for different programs, different metrics provided the best guidance. <p> However, many users do not know what additional metrics might be useful and are unable to use this functionality. The major problem with having many metrics is knowing which one to use. In a previous paper <ref> [40] </ref>, we compared several different metrics (including Critical Path, Logical Zeroing, and NPT) and concluded that there was no single best metric. However, we were able to characterize the types of applications where each metric would be useful.
Reference: 41. <author> M. Homewood and M. McLaren, </author> <title> Meiko CS-2 interconnect, </title> <booktitle> Proceedings of the 1993 World Transputer Congress, </booktitle> <month> Sept. </month> <year> 1993, </year> <pages> pp. 1209-18. </pages>
Reference-contexts: The spanning tree technique achieves logarithmic time, instead of unit time cost. However, most new machines come with some form of broadcast facility, including the Intel Paragon (though it is currently not accessible to application software), Cray T3D [19], and Meiko CS-2 <ref> [41] </ref>. Efficient operations such as these are crucial to dealing with the issue of scale in tools for large parallel machines. 5.4. Clock Synchronization An important question for many parallel tools is how to synchronize the timestamps from different processors.
Reference: 42. <author> A. A. Hough and J. E. Cuny, </author> <title> Initial Experiences with a Pattern-Oriented Parallel Debugger, </title> <booktitle> SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <address> Madison, WI, </address> <month> May 5-6, </month> <year> 1988, </year> <pages> pp. 195-205. </pages> <note> appears as SIGPLAN Notices, </note> <month> January </month> <year> 1989. </year>
Reference-contexts: Two approaches have been tried to express these relationships. IVE [29], Voyeur [88], POLKA [90], and Zeus [14] require the programmer to make calls to animation routines from their program. A second approach, used in Belvedere <ref> [42, 43] </ref>, provides an event stream and the user defines reduction and animation operations over it. A problem with algorithm animation is that it adds the burden of creating and debugging animations to the process of writing a parallel application. A variation on algorithm animation is auralization [28, 87, 89].
Reference: 43. <author> A. A. Hough and J. E. Cuny, </author> <title> Perspective Views: A Technique for Enhancing Parallel Program Visualization, </title> <booktitle> International Conference on Parallel Processing (ICPP), </booktitle> <month> August </month> <year> 1990, </year> <pages> pp. </pages> <month> II.124-132. </month>
Reference-contexts: Two approaches have been tried to express these relationships. IVE [29], Voyeur [88], POLKA [90], and Zeus [14] require the programmer to make calls to animation routines from their program. A second approach, used in Belvedere <ref> [42, 43] </ref>, provides an event stream and the user defines reduction and animation operations over it. A problem with algorithm animation is that it adds the burden of creating and debugging animations to the process of writing a parallel application. A variation on algorithm animation is auralization [28, 87, 89].
Reference: 44. <institution> Paragon X/PS Product Overview, Intel Supercomputer Systems Division, </institution> <address> 15201 N.W. Greenbrier Parkway, Beaverton OR, </address> <year> 1992. </year>
Reference-contexts: Either option has a perturbing effect. On machines without a synchronous scheduling model, such as the Intel Paragon <ref> [44] </ref>, we use this model of data transport but it is more difficult to factor out the cost. The Instrumentation Manager may make frequent changes to the instrumentation during the application's execution.
Reference: 45. <author> S. C. Johnson, </author> <title> Postloading for Fun and Profit, </title> <booktitle> USENIX Winter Conf., </booktitle> <month> Jan. </month> <pages> 22-26, </pages> <year> 1990, </year> <pages> pp. 325-330. </pages>
Reference-contexts: Alternatively the instrumentation can be inserted after the executable image has been built. This approach, called binary re-writing, inserts instrumentation into an object file after it has been compiled and assembled. QPT [4], Mtool [33], jprof [77], and the Stardent Postloader <ref> [45] </ref> all use this style of instrumentation. Binary rewriting has the advantage that it is language independent, and that compilers and libraries do not need to be modified. It is also possible to hand modify an application to insert calls to collect performance data. JEWEL [51] uses this approach. <p> This creates problems for post-linker tools (correctness debuggers and performance tools). Larus and Ball [4] have also noted this problem, and developed heuristics to differentiate code from data. We use many of their heuristics in our implementation. A better solution was used by Sardent <ref> [45] </ref>. They added additional information to their executable format to facilitate post-linker tools. 48 Save Registers SetUp Arg Restore Registers Base Trampoline Mini Trampoline Program Relocated Instruction (s) Local Global Local Global Primitive startTimer Function`A' The base trampoline provides slots to call instrumentation before and after the relocated instruction.
Reference: 46. <author> J. Joyce, G. Lomow, K. Slind and B. Unger, </author> <title> Monitoring Distributed Systems, </title> <journal> ACM Transactions on Computer Systems 5, </journal> <month> 2 (May </month> <year> 1987), </year> <pages> pp. 121-150. </pages>
Reference-contexts: However, tools are often limited by the large amount of performance data they can generate. One approach to reduce the amount of data is to filter it and store only interesting events. EDL [5], Segall et. al's receptacles [81], and Jade <ref> [46] </ref> use predicates and actions to recognize desired events (or sequences of events) and then generate synthetic events. ISSOS [80] and Meta [61] also use predicates to detect interesting states in an application, but require the user program to periodically call the filtering system.
Reference: 47. <author> P. B. Kessler, </author> <title> Fast Breakpoints: </title> <booktitle> Design and Implementation, ACM SIGPLAN'90 Conf. on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June 20-22, </month> <year> 1990, </year> <pages> pp. 78-84. </pages>
Reference-contexts: Generating code during program execution is not a new idea. A number of correctness debuggers have been built that modify an executing program for assertion checking and conditional breakpoints. Brown [15] developed a debugger for Cray Computers that provided this feature. Kessler at Xerox Parc <ref> [47] </ref> built a system that used dynamic modification of a program to insert breakpoints. Work has also been done by Wahbe, et. al. on fast data breakpoints [95]. They employ sophisticated program analysis techniques to minimize the overhead of instrumentation for data breakpoints.
Reference: 48. <author> K. Kinoshita, </author> <title> An Experience with the ANALYZER/SX Performance Tuning Tool, in Instrumentation for Future Parallel Computer Systems, </title> <editor> M. Simmons, R. Koskela and I. Bucker (eds), </editor> <publisher> Addison-Wesley, </publisher> <year> 1989, </year> <pages> pp. 223-231. </pages>
Reference-contexts: Rather than trying to find a single metric to characterize the entire application, many tools provide sorted profiles for several different resources. Some commonly profiled resources are: CPU utilization, synchronization time, disk operations, vectorization, and cache performance. INCAS [35], JEWEL [51], and ANALYZER/SX <ref> [48] </ref> are examples of this approach. These tools provide a wealth of information to the programmer, but force them to select the appropriate resources to profile their application. Building a tool that includes all potentially useful metrics is difficult.
Reference: 49. <author> J. Kohn and W. Williams, </author> <title> ATExpert, </title> <journal> Journal of Parallel and Distributed Computing 18, </journal> <month> 2 (June </month> <year> 1993), </year> <pages> pp. 205-222. </pages>
Reference-contexts: To provide better guidance to the user several tools have been developed that treat the problem of finding a performance bottleneck as a search problem. These systems attempt to both identify the problem (describe) and to give advice on how to correct it (prescribe). ATExpert <ref> [49] </ref> from Cray Research uses rules to recognize performance problems in Fortran programs. Information about the achieved speedup, overhead of parallel routines, and sequential time are used to select appropriate suggestions about how the program's execution time might be improved.
Reference: 50. <author> L. Lamport, </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System, </title> <journal> Comm. of the ACM 21, </journal> <month> 7 (July </month> <year> 1978), </year> <pages> pp. 558-564. </pages>
Reference-contexts: Clock Synchronization An important question for many parallel tools is how to synchronize the timestamps from different processors. A full solution to this problem requires the use of complex algorithms such as logical clocks <ref> [50] </ref>. However, since our instrumentation is based on counters and timers that are located in individual processors, global time is not important to dynamic instrumentation. Our main concern is how to combine samples from different sources (processors, processes and threads) to form aggregate values.
Reference: 51. <author> F. Lange, R. Kroger and M. Gergeleit, </author> <title> JEWEL: Design and Implementation of a Distributed Measurement System, </title> <journal> IEEE Transactions on Parallel and Distributed Systems 3, </journal> <month> 6 (Nov </month> <year> 1992), </year> <pages> pp. 657-671. </pages>
Reference-contexts: Rather than trying to find a single metric to characterize the entire application, many tools provide sorted profiles for several different resources. Some commonly profiled resources are: CPU utilization, synchronization time, disk operations, vectorization, and cache performance. INCAS [35], JEWEL <ref> [51] </ref>, and ANALYZER/SX [48] are examples of this approach. These tools provide a wealth of information to the programmer, but force them to select the appropriate resources to profile their application. Building a tool that includes all potentially useful metrics is difficult. <p> Moviola [54] and the performance tool for PREFACE [10] also provide large libraries of pre-defined visualizations. No matter how many visualizations a tool provides, users will eventually want more. A solution to this problem is being explored in two systems, Pablo [75] and JEWEL <ref> [51] </ref>. Both of these systems contain toolkits for building visualization modules. The user constructs visualizations from building blocks. This method permits an almost unlimited number of visualization modules to be built. <p> Binary rewriting has the advantage that it is language independent, and that compilers and libraries do not need to be modified. It is also possible to hand modify an application to insert calls to collect performance data. JEWEL <ref> [51] </ref> uses this approach. Hand instrumentation provides the user a great degree of selectivity, but it is a tedious process. Finally, it possible to use source to source translation of the program. <p> EBBA [6, 7], based on EDL, permits dynamic insertion of predicates. BEE [16] allows the dynamic control of the filtering of events based on both event type and by insertion of more complex predicates called "event interpreters". TOPSYS [9] and JEWEL <ref> [51] </ref> require that predicates be defined before the program starts execution, but permit dynamic control of which ones are enabled. XAB [8] provides dynamic selection of tracing of messages by message type for PVM [25] applications.
Reference: 52. <author> J. R. Larus, </author> <title> Abstract Execution: A Technique for Efficiently Tracing Programs, </title> <journal> SoftwarePractice & Experience 20, </journal> <month> 12 (Dec </month> <year> 1990), </year> <pages> pp. 1241-1258. </pages>
Reference-contexts: To collect performance information about individual program constructs, such as loops or statements, instrumenting runtime libraries or primitives is not sufficient. Collecting data at this granularity requires instrumentation be interspersed with the statements of the user's program. CXpa [36], AE <ref> [52] </ref>, Prism [84], and MPP Apprentice [98] use a modified compiler to insert instrumentation at the desired location. Compiler based instrumentation affords access to the wealth of information that is available during compilation. For example, information about data and loop dependencies is difficult to gather without compiler information.
Reference: 53. <author> T. J. LeBlanc and J. M. Mellor-Crummey, </author> <title> Debugging Parallel Programs with Instant Replay, </title> <journal> IEEE Transactions on Computers 36, </journal> <month> 4 (April </month> <year> 1987), </year> <pages> pp. 471-482. </pages>
Reference-contexts: A similar approach is to provide a set of instrumented primitives and require the user to write their applications using these primitives. For example, Instant Replay <ref> [53] </ref> provides primitives to operate on events, queues, shared memory objects, and processes on the BBN Butterfly [3]. Likewise, PIE [82] works with a programming model called MPC [94] which provides parallelism via operations on shared memory.
Reference: 54. <author> T. J. LeBlanc, J. M. Mellor-Crummey and R. J. Fowler, </author> <title> Analyzing Parallel Program Executions Using Multiple Views, </title> <journal> Journal of Parallel and Distributed Computing 9, </journal> <volume> 6 (1990), </volume> <pages> pp. 203-217. </pages>
Reference-contexts: Unfortunately not every visualization is useful for every program (some are only useful for a few programs), and the user is left with the formidable task of selecting appropriate visualizations and resources to display. Moviola <ref> [54] </ref> and the performance tool for PREFACE [10] also provide large libraries of pre-defined visualizations. No matter how many visualizations a tool provides, users will eventually want more. A solution to this problem is being explored in two systems, Pablo [75] and JEWEL [51].
Reference: 55. <author> T. Lehr, Z. Segall, D. F. Vrsalovic, E. Caplan, A. L. Chung and C. E. Fineman, </author> <title> Visualizing Performance Debugging, </title> <booktitle> IEEE Computer 21, </booktitle> <month> 10 (October </month> <year> 1989), </year> <pages> pp. 38-51. </pages>
Reference-contexts: This visualization is simple, but effective for identifying memory hot spots. A limitation of MAP is that it does not relate memory accesses to the source code. Another tool that provides a single visualization of the resources in a parallel computation is PIE <ref> [55, 82] </ref>. PIE provides a color coded time-line tracking CPU state (e.g., useful work, blocked, spinning) through time. The tool can also show which procedures are executing by assigning a different color to each procedure in the program.
Reference: 56. <author> T. Lehr, D. Black, Z. Segall and D. Vrsalovic, MKM: </author> <title> Mach Kernel Monitor Description, Examples and Measurements, </title> <type> CMU, </type> <institution> Carnegie-Mellon University, Pittsburgh, PA-CS-89-131, </institution> <month> March </month> <year> 1989. </year> <month> 102 </month>
Reference-contexts: Smith's Spider debugger [86] used kernel based instrumentation to track all inter-process communication, but it only worked on a single machine. The Distributed Program Monitor [64] traces inter-process message passing in a distributed environment. The Mach Kernel Monitor <ref> [56] </ref> instruments context switches to trace the state of processes through time. If modifying the operating system is not feasible or desirable, monitoring can be accomplished via dedicated data collection processes. The External Data Collection facility of IPS-2 [39] provides this capability.
Reference: 57. <author> G. Lewandowski, </author> ????, <type> PhD Thesis, </type> <institution> University of Wisconsin-Madison, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Graph Coloring Application. The first application we measured with the Paradyn system was a graph coloring program called ``matchmaker''. Match-maker was one of several programs written by another PhD student at the University of Wisconsin to compare different parallel graph coloring algorithms <ref> [57] </ref>. Match-maker is a branch and bound search with a central manager that brokers work to idle processors. It uses TMC's explicit message passing library CMMD. The program is written in C++ and contains 4,600 lines of code in 37 files.
Reference: 58. <author> A. D. Malony, </author> <title> Program Tracing in Cedar, </title> <type> CSRD Report No. 660, </type> <institution> Center for Supercomputing Res. and Dev., Univ. of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: Figure 2.1 shows the stages of a compilation, and where different tools insert their instrumentation. Collecting data via the runtime library (e.g., the C library) provides an easy way to collect data about the interactions between processes. IPS-2 [65] and Cedar tracing <ref> [58] </ref> use this approach. Runtime library instrumentation does not require the user to modify their application; it just requires re-linking the program with a modified version of the library.
Reference: 59. <author> A. Malony, </author> <title> Performance Observability, </title> <type> PhD Dissertation, </type> <institution> Department of Computer Science, University of Illinois, </institution> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: A closely related topic to our two part cost model is the work that has been done on perturbation compensation <ref> [59, 100] </ref>. The goal of perturbation compensation is to reconstruct the performance of an un-perturbed execution from a perturbed one. These techniques generally require a trace based instrumentation system and postmortem analysis to reconstruct the correct ordering of events.
Reference: 60. <author> A. D. Malony and D. A. Reed, </author> <title> A Hardware-Based Performance Monitor for the Intel iPSC/2 Hypercube, </title> <booktitle> 1990 International Conference on Supercomputing, </booktitle> <address> Amsterdam, </address> <month> June 11-15, </month> <year> 1990, </year> <pages> pp. 213-226. </pages>
Reference-contexts: The hardware keeps a count of the event occurrences. Another approach is to have the hardware collector generate trace data and send it to a data reduction station over a separate bus or interconnection network. MultiKron [68, 69], TMP [35, 99], M31 [76] and HYPERMON <ref> [60] </ref> are examples of this type of monitor. An advantage of hybrid instrumentation is that it eliminates most perturbation of the CPU and inter-connection network, and permits information visible only to the application to be collected.
Reference: 61. <author> K. Marzullo and M. D. Wood, </author> <title> Tools for Constructing Distributed Reactive Systems, </title> <type> 91-1187, </type> <institution> Cornell University, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: One approach to reduce the amount of data is to filter it and store only interesting events. EDL [5], Segall et. al's receptacles [81], and Jade [46] use predicates and actions to recognize desired events (or sequences of events) and then generate synthetic events. ISSOS [80] and Meta <ref> [61] </ref> also use predicates to detect interesting states in an application, but require the user program to periodically call the filtering system. Filtering greatly reduces the amount of performance data collected. However, all of these tools force the user to select the desired events prior to starting their program.
Reference: 62. <author> H. Massalin and C. Pu, </author> <title> Threads and Input/Output in the Synthesis Kernel, </title> <booktitle> ACM Symp. on Operating Systems Principles, </booktitle> <address> Litchfield Park, AZ, </address> <month> Dec 3-6 </month> <year> 1989, </year> <pages> pp. 191-201. </pages>
Reference-contexts: Work has also been done by Wahbe, et. al. on fast data breakpoints [95]. They employ sophisticated program analysis techniques to minimize the overhead of instrumentation for data breakpoints. Massalin and Pu <ref> [62] </ref> have a novel application of code patch-up in their Synthesis operating system. They modify a program to automatically schedule another thread when it is about to block. Bishop [11] developed a performance monitor that patches a binary program for performance monitoring.
Reference: 63. <author> G. Mcdaniel, </author> <title> METRIC: a kernel instrumentation system for distributed environments, </title> <booktitle> 6th Symp. on Operating System Prin., </booktitle> <month> November </month> <year> 1977, </year> <pages> pp. 93-99. </pages>
Reference-contexts: However, kernel based approaches suffer several limitations: modifying a kernel is time consuming, error prone, and users need to run a modified kernel to use the tool. METRIC <ref> [63] </ref> is an early example of using a modified kernel for data collection. Smith's Spider debugger [86] used kernel based instrumentation to track all inter-process communication, but it only worked on a single machine. The Distributed Program Monitor [64] traces inter-process message passing in a distributed environment.
Reference: 64. <author> B. P. Miller, C. Macrander and S. Sechrest, </author> <title> A Distributed Programs Monitor for Berkeley UNIX, </title> <booktitle> 5th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1985, </year> <pages> pp. 43-54. </pages>
Reference-contexts: METRIC [63] is an early example of using a modified kernel for data collection. Smith's Spider debugger [86] used kernel based instrumentation to track all inter-process communication, but it only worked on a single machine. The Distributed Program Monitor <ref> [64] </ref> traces inter-process message passing in a distributed environment. The Mach Kernel Monitor [56] instruments context switches to trace the state of processes through time. If modifying the operating system is not feasible or desirable, monitoring can be accomplished via dedicated data collection processes.
Reference: 65. <author> B. P. Miller, M. Clark, J. Hollingsworth, S. Kierstead, S. Lim and T. Torzewski, IPS-2: </author> <title> The Second Generation of a Parallel Program Measurement System, </title> <journal> IEEE Transactions on Parallel and Distributed Systems 1, </journal> <month> 2 (April </month> <year> 1990), </year> <pages> pp. 206-217. </pages>
Reference-contexts: Before it is possible to provide a meaningful methodology for decision support and advice, it is vital to understand the process of performance debugging parallel programs. Based on our informal studies of how programmers used a previous performance tool, IPS-2 <ref> [65] </ref>, several trends can be inferred. Programmers generally start by 1 2 looking at a high level view of the performance of their application, and iteratively isolate the source of their program's poor performance. This process continues until they understand their program's performance well enough to start tuning it. <p> Yang and Miller developed Critical Path Analysis [102, 103] to provide more accurate guidance. Critical path provides an estimate of the improvement in the runtime possible if a single procedure is optimized. This estimate is an upper bound, and an extension to Critical Path called Logical Zeroing <ref> [65] </ref> provides a better estimate. These metrics provide detailed information about how to improve a parallel application, but building the data structures and calculating the metrics require significant space and time. Prof, NPT, and Critical Path Analysis focused on finding CPU time bottlenecks. <p> Each of these approaches has advantages and disadvantages. Figure 2.1 shows the stages of a compilation, and where different tools insert their instrumentation. Collecting data via the runtime library (e.g., the C library) provides an easy way to collect data about the interactions between processes. IPS-2 <ref> [65] </ref> and Cedar tracing [58] use this approach. Runtime library instrumentation does not require the user to modify their application; it just requires re-linking the program with a modified version of the library. <p> In the 1950's, programmers drew diagrams by hand of the phase behavior of their programs to manage manual memory overlays [24]. More recently, Denning noted the importance of recognizing phases and phase changes automatically in a memory management system [23]. In parallel computing, tools such as IPS-2 <ref> [65] </ref> provide ways to restrict program measurement to a specific phase of execution. Experience with watching programmers use this feature of IPS-2 convinced us that phase analysis is a major way programmers look for performance bottlenecks. The third component of the W 3 Search Model is the ``when'' axis. <p> We were interested in studying the W 3 Search Model in isolation, so for this study we decided to simulate the Dynamic Instrumentation system on top of IPS-2 <ref> [65] </ref>, which uses trace based instrumentation. Using trace data meant we could make multiple runs of the Performance Consultant using the same data to calibrate our search system. <p> We used our prototype to study three applications: two are from the Splash benchmark suite [83] and one is a database application. To validate the guidance supplied by the Performance Consultant, we also studied each application program using the IPS-2 <ref> [65] </ref> performance tools. This manual check permitted us to verify that the bottlenecks identified by the search model were real, and to check that no obvious bottlenecks were omitted. Running programs and simulating Dynamic Instrumentation permitted us to study the search model in isolation.
Reference: 66. <author> D. L. Mills, </author> <title> On the accuracy and stability of clocks synchronized by the Network Time Protocol in the Internet system, </title> <journal> Computer Communication Review 20, </journal> <month> 1 (Jan. </month> <year> 1990), </year> <pages> pp. 65-75. </pages>
Reference-contexts: We depend on the clocks of each processor to provide timestamps for the samples; however, the granularity of sampling is relatively infrequent compared to clock drift. Typically, we sample metrics at the rate of 1-2 times 50 per second, but clock synchronization algorithms such as NTP <ref> [66, 67] </ref> can keep clock drift to a few milli-seconds. As a result, clock drift causes a bit of uncertainty about the end of our time intervals, but the fraction of a sample interval affected is small. Aggregation is complicated because sampling from different sources is (potentially) asynchronous.
Reference: 67. <author> D. L. Mills, </author> <title> Internet time synchronization: the network time protocol, </title> <journal> IEEE Transactions on Communications 39, </journal> <month> 10 (Oct. </month> <year> 1991), </year> <pages> pp. 1482-93. </pages>
Reference-contexts: We depend on the clocks of each processor to provide timestamps for the samples; however, the granularity of sampling is relatively infrequent compared to clock drift. Typically, we sample metrics at the rate of 1-2 times 50 per second, but clock synchronization algorithms such as NTP <ref> [66, 67] </ref> can keep clock drift to a few milli-seconds. As a result, clock drift causes a bit of uncertainty about the end of our time intervals, but the fraction of a sample interval affected is small. Aggregation is complicated because sampling from different sources is (potentially) asynchronous.
Reference: 68. <author> A. Mink and R. Carpenter, </author> <title> A VLSI Chip Set For A Multiprocessor Performance Measurement System, in Performance Instrumentation and Visualization, </title> <editor> M. Simmons and R. Koskela (eds), </editor> <publisher> Addison-Wesley, </publisher> <year> 1990, </year> <pages> pp. 213-232. </pages>
Reference-contexts: The hardware keeps a count of the event occurrences. Another approach is to have the hardware collector generate trace data and send it to a data reduction station over a separate bus or interconnection network. MultiKron <ref> [68, 69] </ref>, TMP [35, 99], M31 [76] and HYPERMON [60] are examples of this type of monitor. An advantage of hybrid instrumentation is that it eliminates most perturbation of the CPU and inter-connection network, and permits information visible only to the application to be collected.
Reference: 69. <author> A. Mink, R. Carpenter, G. Nacht and J. Roberts, </author> <title> Multiprocessor Performance Measurement Instrumentation, </title> <booktitle> IEEE Computer 23, </booktitle> <month> 9 (September </month> <year> 1990), </year> <pages> pp. 63-75. </pages>
Reference-contexts: The hardware keeps a count of the event occurrences. Another approach is to have the hardware collector generate trace data and send it to a data reduction station over a separate bus or interconnection network. MultiKron <ref> [68, 69] </ref>, TMP [35, 99], M31 [76] and HYPERMON [60] are examples of this type of monitor. An advantage of hybrid instrumentation is that it eliminates most perturbation of the CPU and inter-connection network, and permits information visible only to the application to be collected. <p> It is impractical due to the cost and time to construct it. More importantly, because of the ability Dynamic Instrumentation to adjust what data is collected, custom trace co-processors such as Multikron <ref> [69] </ref> or TMP [99] are no longer required. We believe the role of hardware based instrumentation should be to efficiently count those events which are not visible to software based instrumentation. For example, hardware counters are useful for measuring cache misses, floating point operations, and bus utilization.
Reference: 70. <author> B. A. Murtagh and M. A. Saunders, </author> <title> Large-scale Linearly Constrained Optimization, </title> <booktitle> Mathematical Programming 14, </booktitle> <month> 1 (Jan. </month> <year> 1978), </year> <pages> pp. 41-72. </pages>
Reference-contexts: The observed cost is shown as total instrumentation across the 32 processors in the partition (color original). In addition, this program makes use of a sequential constrained optimization package, Minos <ref> [70] </ref>. We choose to instrument only the application specific code. Again, we used the ignore ``where'' axis option to suppress instrumentation of the library.
Reference: 71. <author> C. M. Pancake and S. Utter, </author> <title> Models for visualization in parallel debuggers, </title> <booktitle> Supercomputing 1989, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989, </year> <pages> pp. 627-636. </pages>
Reference-contexts: The visualizations discussed so far have been designed to work for any program being studied. A different approach is algorithm animation. Algorithm animation tries to graphically represent an application's execution at a level of abstraction similar to the programmer's mental model of the program. According to Pancake and Utter <ref> [71] </ref>, if the programmer can see the program the way they think about it, rather than how it is represented on the physical machine, it will be easier to understand a program's performance. The difficult part is relating events on the physical machine back to an appropriate abstract model.
Reference: 72. <author> C. M. Pancake and C. Cook, </author> <title> What Users Need in Parallel Tools Support: Survey Results and Analysis, </title> <booktitle> 1994 Scalable High-Performance Computing Conference, </booktitle> <month> May </month> <year> 1994, </year> <pages> pp. 40-47. </pages>
Reference-contexts: Much to the dismay of tool builders, it has been observed that even on 15 16 platforms with fairly sophisticated performance tools, most parallel programers do not use the available performance tools <ref> [72] </ref>. Instead, programmers often manually insert code into their program to understand its performance. Looking at the way in which programmers manually search for bottlenecks provides hints about how to automate the process.
Reference: 73. <author> S. E. Perl and W. E. Weihl, </author> <title> Performance Assertion Checking, </title> <booktitle> 14th ACM Symposium on Operating Systems Principles, </booktitle> <month> December 5-8, </month> <year> 1993, </year> <pages> pp. 134-145. </pages>
Reference-contexts: Another way to search for performance bottlenecks is to have the programmer specify assertions about the expected performance of their program. For example, the programmer can express the expected CPU utilization or a cache miss tolerance. The PSpec language <ref> [73] </ref> uses this approach. It is a post-mortem tool that checks for 8 assertion failures by scanning log files created during program execution. Because performance assertions come from programmers, they can contain precise descriptions of the expected performance. However, writing assertions does create additional work for the programmer.
Reference: 74. <author> C. Ponder and R. Fateman, </author> <title> Inaccuracies in Program Profilers, </title> <journal> SoftwarePractice & Experience 18, </journal> <month> 5 (May </month> <year> 1988), </year> <pages> pp. 459-476. </pages>
Reference-contexts: Our periodic summaries can yield more accurate information than can be provided by statistical sampling. For example, if you statistically sample the program counter to compute the amount of time spent in a procedure, the accuracy of the result will depend on the sampling rate. Ponder and Fatemen <ref> [74] </ref> describe a number of other limitations of statistical sampling approaches. Instead, we insert code to start and stop timers at the procedure entry and exit to accurately record time spent in the procedure.
Reference: 75. <author> D. A. Reed, R. A. Aydt, R. J. Noe, P. C. Roth, K. A. Shields, B. W. Schwartz and L. F. Tavera, </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment, in Scalable Parallel Libraries Conference, </title> <editor> A. Skjellum (ed), </editor> <publisher> IEEE Computer Society, </publisher> <year> 1993. </year> <month> 103 </month>
Reference-contexts: Moviola [54] and the performance tool for PREFACE [10] also provide large libraries of pre-defined visualizations. No matter how many visualizations a tool provides, users will eventually want more. A solution to this problem is being explored in two systems, Pablo <ref> [75] </ref> and JEWEL [51]. Both of these systems contain toolkits for building visualization modules. The user constructs visualizations from building blocks. This method permits an almost unlimited number of visualization modules to be built. <p> The instrumentation system is also portable across platforms, but is tied to a single language. Also, the data dependencies are not available, and of course it requires that the program be re-compiled to collect the data. Pablo <ref> [75] </ref> and AIMS [101] 12 use source to source translation. The techniques described so far all require the instrumentation to be inserted prior to program execution. One system that defers instrumentation until the program has started to execute is the TAM facility [78] provided by Intel for the Paragon.
Reference: 76. <author> M. Reilly, </author> <title> Instrumentation for Application Performance Tuning: The M31 System, in Instrumentation for Future Parallel Computer Systems, </title> <editor> M. Simmons, R. Koskela and I. Bucker (eds), </editor> <publisher> Addison-Wesley, </publisher> <year> 1989, </year> <pages> pp. 143-158. </pages>
Reference-contexts: The hardware keeps a count of the event occurrences. Another approach is to have the hardware collector generate trace data and send it to a data reduction station over a separate bus or interconnection network. MultiKron [68, 69], TMP [35, 99], M31 <ref> [76] </ref> and HYPERMON [60] are examples of this type of monitor. An advantage of hybrid instrumentation is that it eliminates most perturbation of the CPU and inter-connection network, and permits information visible only to the application to be collected.
Reference: 77. <author> J. F. Reiser and J. P. Skudlarek, </author> <title> Program Profiling Problems, and a Solution via Machine Language Rewriting, </title> <journal> SIGPLAN Notices 29, </journal> <month> 1 (Jan. </month> <year> 1994), </year> <pages> pp. 37-45. </pages>
Reference-contexts: Alternatively the instrumentation can be inserted after the executable image has been built. This approach, called binary re-writing, inserts instrumentation into an object file after it has been compiled and assembled. QPT [4], Mtool [33], jprof <ref> [77] </ref>, and the Stardent Postloader [45] all use this style of instrumentation. Binary rewriting has the advantage that it is language independent, and that compilers and libraries do not need to be modified. It is also possible to hand modify an application to insert calls to collect performance data.
Reference: 78. <author> B. Ries, R. Anderson, W. Auld, D. Breazeal, K. Callaghan, E. Richards and W. Smith, </author> <title> The Paragon Performance Monitoring Environment, </title> <address> Supercomputing'93, Portland, OR, Nov 15-19, </address> <year> 1993, </year> <pages> pp. 850-859. </pages>
Reference-contexts: Pablo [75] and AIMS [101] 12 use source to source translation. The techniques described so far all require the instrumentation to be inserted prior to program execution. One system that defers instrumentation until the program has started to execute is the TAM facility <ref> [78] </ref> provided by Intel for the Paragon. TAM uses a fixed set of performance instrumentation profiles (i.e., prof style sampling, or full event tracing) to insert instrumentation into a program after it has been loaded into memory but prior to execution.
Reference: 79. <author> D. A. Schneider and D. J. DeWitt, </author> <title> A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment, </title> <type> Tech. Report 836, </type> <institution> Dept. of Comp. Sci., University of Wisconsin, </institution> <month> April </month> <year> 1989. </year>
Reference-contexts: For the dynamic tests, we ran two parallel applications on the CM-5. The first application does a domain-decomposition method for optimizing large-scale linear models. The second program is a database simulator that implements a parallel Grace Hash join algorithm in a simulated shared-nothing environment <ref> [79] </ref>. Both programs are written in C. We compared the execution time of an un-instrumented application to the execution time of an instrumented application. We measured the overall cost of our instrumentation and the overhead of various 58 components of the instrumentation.
Reference: 80. <author> K. Schwan, R. Ramnath, S. Vasudevan and D. M. Ogle, </author> <title> A language and system for parallel programming, </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> April </month> <year> 1988, </year> <pages> pp. 455-471. </pages>
Reference-contexts: One approach to reduce the amount of data is to filter it and store only interesting events. EDL [5], Segall et. al's receptacles [81], and Jade [46] use predicates and actions to recognize desired events (or sequences of events) and then generate synthetic events. ISSOS <ref> [80] </ref> and Meta [61] also use predicates to detect interesting states in an application, but require the user program to periodically call the filtering system. Filtering greatly reduces the amount of performance data collected.
Reference: 81. <author> Z. Segall, A. Singh, R. T. Snodgrass, A. K. Jones and D. P. Siewiorek, </author> <title> An Integrated Instrumentation Environment for Multiprocessors, </title> <journal> IEEE Transactions on Computers C-32, </journal> <month> 1 (January </month> <year> 1983), </year> <pages> pp. 4-14. </pages>
Reference-contexts: However, tools are often limited by the large amount of performance data they can generate. One approach to reduce the amount of data is to filter it and store only interesting events. EDL [5], Segall et. al's receptacles <ref> [81] </ref>, and Jade [46] use predicates and actions to recognize desired events (or sequences of events) and then generate synthetic events. ISSOS [80] and Meta [61] also use predicates to detect interesting states in an application, but require the user program to periodically call the filtering system.
Reference: 82. <author> Z. Segall and L. Rudolph, PIE: </author> <title> A programming and instrumentation environment for parallel processing, </title> <booktitle> IEEE Software 2, </booktitle> <month> 6 (November </month> <year> 1985), </year> <pages> pp. 22-37. </pages>
Reference-contexts: This visualization is simple, but effective for identifying memory hot spots. A limitation of MAP is that it does not relate memory accesses to the source code. Another tool that provides a single visualization of the resources in a parallel computation is PIE <ref> [55, 82] </ref>. PIE provides a color coded time-line tracking CPU state (e.g., useful work, blocked, spinning) through time. The tool can also show which procedures are executing by assigning a different color to each procedure in the program. <p> A similar approach is to provide a set of instrumented primitives and require the user to write their applications using these primitives. For example, Instant Replay [53] provides primitives to operate on events, queues, shared memory objects, and processes on the BBN Butterfly [3]. Likewise, PIE <ref> [82] </ref> works with a programming model called MPC [94] which provides parallelism via operations on shared memory. Basing instrumentation on a particular set of primitives is appropriate when the performance monitoring system is being developed in conjunction with the programming model.
Reference: 83. <author> J. P. Singh, W. Weber and A. Gupta, </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory, Computer Architecture News 20, </title> <month> 1 (March </month> <year> 1992), </year> <pages> pp. 5-44. </pages>
Reference-contexts: Another advantage of using IPS-2 tracing is that we could compare the amount of performance data generated by our method to existing trace based and sampling approaches. We used our prototype to study three applications: two are from the Splash benchmark suite <ref> [83] </ref> and one is a database application. To validate the guidance supplied by the Performance Consultant, we also studied each application program using the IPS-2 [65] performance tools.
Reference: 84. <author> S. Sistare, D. Allen, R. Bowker, K. Jourdenais, J. Simons and R. </author> <title> Title, Data Visualization and Performance Analysis in the Prism Programming Environment, in Programming Environments for Parallel Computing, </title> <editor> N. Topham, R. Ibbett and T. Bemmerl (eds), </editor> <publisher> North-Holland, </publisher> <year> 1992, </year> <pages> pp. 37-52. </pages>
Reference-contexts: To collect performance information about individual program constructs, such as loops or statements, instrumenting runtime libraries or primitives is not sufficient. Collecting data at this granularity requires instrumentation be interspersed with the statements of the user's program. CXpa [36], AE [52], Prism <ref> [84] </ref>, and MPP Apprentice [98] use a modified compiler to insert instrumentation at the desired location. Compiler based instrumentation affords access to the wealth of information that is available during compilation. For example, information about data and loop dependencies is difficult to gather without compiler information.
Reference: 85. <author> R. L. Sites, ed., </author> <title> Alpha Architecture Reference Manual, </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: This time could be substantially reduced if processors provided a clock that was readable by user level code at register access speed. Currently, few RISC processors, with the exception of the DEC Alpha <ref> [85] </ref>, provide such a clock.
Reference: 86. <author> E. T. Smith, </author> <title> Debugging Techniques for Communicating, Loosely-Coupled Processes, </title> <type> PhD Thesis, </type> <institution> University of Rochester, </institution> <month> December </month> <year> 1981. </year>
Reference-contexts: However, kernel based approaches suffer several limitations: modifying a kernel is time consuming, error prone, and users need to run a modified kernel to use the tool. METRIC [63] is an early example of using a modified kernel for data collection. Smith's Spider debugger <ref> [86] </ref> used kernel based instrumentation to track all inter-process communication, but it only worked on a single machine. The Distributed Program Monitor [64] traces inter-process message passing in a distributed environment. The Mach Kernel Monitor [56] instruments context switches to trace the state of processes through time. <p> Examples of predicates are the first time a selected procedure is called, or when the synchronization wait time is above a selected threshold. This method requires less direct involvement by the user. Existing correctness debuggers (e.g., Spider <ref> [86] </ref>, and TOPSYS [9]) use similar predicates. Also, PMPL [26] uses similar predicates to trigger data recording. A third approach is to have programmers annotate their programs with calls to library routines to indicate major parts of the computation.
Reference: 87. <author> S. Smith and M. G. Williams, </author> <title> The Use of Sound in an Exploratory Visualization Environment, </title> <institution> University of Lowell Computer Science Department Technical Report #R-89-002, </institution> <year> 1989. </year>
Reference-contexts: A problem with algorithm animation is that it adds the burden of creating and debugging animations to the process of writing a parallel application. A variation on algorithm animation is auralization <ref> [28, 87, 89] </ref>. Instead of representing a program graphically, auralization animates it using sound, or a combination of sound and pictures.
Reference: 88. <author> D. Socha, M. L. Baily and D. Notkin, Voyeur: </author> <title> Graphical Views of Parallel Programs, </title> <booktitle> ACM SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <address> Madison, WI, </address> <month> May 5-6, </month> <year> 1988, </year> <pages> pp. 206-215. </pages> <note> appears as SIGPLAN Notices, </note> <month> January </month> <year> 1989. </year>
Reference-contexts: The difficult part is relating events on the physical machine back to an appropriate abstract model. Two approaches have been tried to express these relationships. IVE [29], Voyeur <ref> [88] </ref>, POLKA [90], and Zeus [14] require the programmer to make calls to animation routines from their program. A second approach, used in Belvedere [42, 43], provides an event stream and the user defines reduction and animation operations over it.
Reference: 89. <author> D. H. Sonnenwald, B. Gopinaht, G. O. Haberman, W. M. K. III and J. S. Myers, InfoSound: </author> <title> An Audio Aid to Program Comprehension, </title> <booktitle> 23rd Hawaii International Conferences on System Sciences 11 (1990), </booktitle> <pages> pp. 541-546. </pages>
Reference-contexts: A problem with algorithm animation is that it adds the burden of creating and debugging animations to the process of writing a parallel application. A variation on algorithm animation is auralization <ref> [28, 87, 89] </ref>. Instead of representing a program graphically, auralization animates it using sound, or a combination of sound and pictures.
Reference: 90. <author> J. T. Stasko and E. Kraemer, </author> <title> A Methodology for Building Application-Specific Visualizations of Parallel Programs, </title> <journal> Journal of Parallel and Distributed Computing 18, </journal> <month> 2 (June </month> <year> 1993), </year> <pages> pp. 258-264. </pages>
Reference-contexts: The difficult part is relating events on the physical machine back to an appropriate abstract model. Two approaches have been tried to express these relationships. IVE [29], Voyeur [88], POLKA <ref> [90] </ref>, and Zeus [14] require the programmer to make calls to animation routines from their program. A second approach, used in Belvedere [42, 43], provides an event stream and the user defines reduction and animation operations over it.
Reference: 91. <author> C. B. Stunkel, D. G. Shea, D. G. Grice, P. H. Hochschild and M. Tsao, </author> <title> The SP1 High-Performance Switch, </title> <booktitle> 1994 Scalable High-Performance Computing Conference, </booktitle> <month> May </month> <year> 1994, </year> <pages> pp. 150-157. </pages>
Reference-contexts: On machines that do not provide hardware broadcast, such at the IBM SP/2 <ref> [91] </ref>, we can construct a software message spanning tree. The spanning tree technique achieves logarithmic time, instead of unit time cost.
Reference: 92. <author> S. S. Thakkar, </author> <type> Personal Communication. </type>
Reference-contexts: Passive hardware monitors observe activity on a parallel computer and record results for later analysis. They are often implemented as a set of programmable counters that record events on the bus, in the memory hierarchy, and on the processor. The performance monitors built into the Sequent Symmetry <ref> [92] </ref>, RP3 [12], Power2 (RS-6000)[97] and the PEM Monitor [17] use programmable hardware counters. Advantages of passive monitoring include no perturbation of the observed system, and a simple hardware design. <p> Usually, this information can be read by user processes. Also, a number of machines provide hardware based counters that are a source of useful performance information. For example, the Power2 [97], Cray Y-MP [18], and Sequent Symmetry <ref> [92] </ref> systems provide detailed counters of processor events. We can combine external information with direct instrumentation to get precise information to relate the external events back to specific parts of the program.
Reference: 93. <author> R. </author> <title> Title, Connection Machine Debugging and Performance Analysis: Present and Future, </title> <booktitle> ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <address> Santa Cruz, CA, </address> <month> May 20-21, </month> <year> 1991, </year> <pages> pp. 272-275. </pages>
Reference-contexts: The standard UNIX CPU time profiler, Prof, is an example of such a tool. A logical extension of this technique to parallel applications is to total the CPU time profiles from each process (or thread). The profiling environment on the Connection Machine <ref> [93] </ref> provides this type of metric. Adding up CPU time from each process ignores the fact that not all CPU time is of equal importance to a parallel program's performance.
Reference: 94. <author> D. F. Vrsalovic, </author> <title> Performance Efficient Parallel Programming in MPC, </title> <booktitle> 22nd Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1989. </year> <month> 104 </month>
Reference-contexts: For example, Instant Replay [53] provides primitives to operate on events, queues, shared memory objects, and processes on the BBN Butterfly [3]. Likewise, PIE [82] works with a programming model called MPC <ref> [94] </ref> which provides parallelism via operations on shared memory. Basing instrumentation on a particular set of primitives is appropriate when the performance monitoring system is being developed in conjunction with the programming model.
Reference: 95. <author> R. Wahbe, L. Lucco and S. L. Graham, </author> <title> Practical data breakpoints: </title> <booktitle> design and implementation, ACM SIGPLAN'93 Conf. on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June 23-25, </month> <year> 1993, </year> <pages> pp. 1-12. </pages>
Reference-contexts: Brown [15] developed a debugger for Cray Computers that provided this feature. Kessler at Xerox Parc [47] built a system that used dynamic modification of a program to insert breakpoints. Work has also been done by Wahbe, et. al. on fast data breakpoints <ref> [95] </ref>. They employ sophisticated program analysis techniques to minimize the overhead of instrumentation for data breakpoints. Massalin and Pu [62] have a novel application of code patch-up in their Synthesis operating system. They modify a program to automatically schedule another thread when it is about to block.
Reference: 96. <author> K. Wang, </author> <title> Precise Compile-Time Performance Prediction of Superscalar-Based Computers, </title> <booktitle> ACM SIGPLAN'94 Conf. on Programming Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June 20-24, </month> <year> 1994, </year> <pages> pp. 73-84. </pages>
Reference-contexts: An alternative would be to analyze the instrumentation sequences to develop a more precise estimate of the number of cycles required for each instrumentation block. To do this, we could use Wang's <ref> [96] </ref> framework of modeling instructions by their functional unit requirements to get a more accurate estimate. To account for cache time, we used measured times for the instrumentation primitives. These measurements resulted in approximately once cache miss per clock primitive.
Reference: 97. <author> E. H. Welbon, C. C. Chen-Nui, D. J. Shippy and D. A. Hicks, </author> <title> The POWER2 Performance Monitor, </title> <journal> IBM Journal of Research and Development (submitted), </journal> . 
Reference-contexts: Usually, this information can be read by user processes. Also, a number of machines provide hardware based counters that are a source of useful performance information. For example, the Power2 <ref> [97] </ref>, Cray Y-MP [18], and Sequent Symmetry [92] systems provide detailed counters of processor events. We can combine external information with direct instrumentation to get precise information to relate the external events back to specific parts of the program.
Reference: 98. <author> W. Williams, T. Hoel and D. Pase, </author> <title> The MPP Apprentice Performance Tool: Delivering the Performance of the Cray T3D, in Programming Environments for Massively Parallel Distributed Systems, (ed), </title> <journal> North-Holland, </journal> <note> (To Appear) 1994. </note>
Reference-contexts: Data can been presented at the granularity of subroutines, loops, parallel blocks, or case statements. Recently, Cray Research has released a new tool, the MPP Apprentice <ref> [98] </ref>, that recognizes performance problems for message passing programs on the Cray T3D [19]. The MPP Apprentice's approach is similar to ATExpert, but it works for a more general programming model. <p> To collect performance information about individual program constructs, such as loops or statements, instrumenting runtime libraries or primitives is not sufficient. Collecting data at this granularity requires instrumentation be interspersed with the statements of the user's program. CXpa [36], AE [52], Prism [84], and MPP Apprentice <ref> [98] </ref> use a modified compiler to insert instrumentation at the desired location. Compiler based instrumentation affords access to the wealth of information that is available during compilation. For example, information about data and loop dependencies is difficult to gather without compiler information.

References-found: 98

