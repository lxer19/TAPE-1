URL: http://www.cs.washington.edu/research/smt/papers/tme.ps
Refering-URL: http://www.cs.washington.edu/research/smt/
Root-URL: 
Email: fswallace,calder,tullseng@cs.ucsd.edu  
Title: Threaded Multiple Path Execution  
Author: Steven Wallace Brad Calder Dean M. Tullsen 
Address: San Diego  
Affiliation: Department of Computer Science and Engineering University of California,  
Date: June 1998.  
Note: To appear at the 25th International Symposium on Computer Architecture,  
Abstract: This paper presents Threaded Multi-Path Execution (TME), which exploits existing hardware on a Simultaneous Multi-threading (SMT) processor to speculatively execute multiple paths of execution. When there are fewer threads in an SMT processor than hardware contexts, threaded multi-path execution uses spare contexts to fetch and execute code along the less likely path of hard-to-predict branches. This paper describes the hardware mechanisms needed to enable an SMT processor to efficiently spawn speculative threads for threaded multi-path execution. The Mapping Synchronization Bus is described, which enables the spawning of these multiple paths. Policies are examined for deciding which branches to fork, and for managing competition between primary and alternate path threads for critical resources. Our results show that TME increases the single program performance of an SMT with eight thread contexts by 14%-23% on average, depending on the misprediction penalty, for programs with a high misprediction rate. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Calder, D. Grunwald, and B. Zorn. </author> <title> Quantifying behavioral differences between C and C++ programs. </title> <journal> Journal of Programming Languages, </journal> <volume> 2(4), </volume> <year> 1994. </year>
Reference-contexts: 1 Introduction A primary impediment to high throughput in superscalar processors is the branch problem. Integer programs have on average 4 to 5 instructions between each branch instruction <ref> [1] </ref>. On today's deeply pipelined processors, the CPU typically has many unresolved branches in the machine at once. This compounds the branch problem, particularly early in the pipeline the fetch unit will only be fetching useful instructions if all unresolved branches were predicted correctly.
Reference: [2] <author> T.H. Heil and J.E. Smith. </author> <title> Selective dual path execution. </title> <type> Technical report, </type> <institution> University of Wisconsin - Madison, </institution> <month> November </month> <year> 1996. </year> <note> http://www.ece.wisc.edu/jes/papers/sdpe.ps. </note>
Reference-contexts: As the results in this paper have shown, the SMT architecture allows for an efficient extension for threaded multi-path execution. Our work also draws from the two studies by Heil and Smith <ref> [2] </ref> and Tyson, Lick and Farrens [9] on restricted dual path execution. Identifying candidate branches for forking through different branch confidence prediction schemes was examined in great detail in both of these studies.
Reference: [3] <author> E. Jacobsen, E. Rotenberg, and J.E. Smith. </author> <title> Assigning confidence to conditional branch predictions. </title> <booktitle> In 29th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 142152, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: First, alternate paths are forked only along the primary path. This prevents forking from an alternate path, which simplifies the architecture. Branches along alternate paths are predicted in the normal fashion. Second, to determine which primary-path branches to fork, we added branch confidence prediction (as described in <ref> [3] </ref>) to the SMT architecture. A 2048 entry table of n-bit counters, shared among all hardware contexts, is used to keep track of the predictability of a branch.
Reference: [4] <author> A. Klauser, A. Paithankar, and D. Grunwald. </author> <title> Selective eager execution on the polypath architecture. </title> <booktitle> In 25th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: Each bit represents a unique Id identifying the hardware context or path that executed the speculative store instruction. The Id bits are similar to the idea of context tags in <ref> [4] </ref>, but our implementation and use is different since TME can have multiple processes simultaneously executing. For TME, each hardware context has a Speculative Tag Id (STI) and a Search Mask (SM) to handle speculative stores.
Reference: [5] <author> S. McFarling. </author> <title> Combining branch predictors. </title> <type> Technical Report TN-36, </type> <institution> DEC-WRL, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Branch prediction is provided by a decoupled branch target buffer (BTB) and pattern history table (PHT) scheme. We use a 256-entry, four-way set associative BTB. The 2K x 2-bit PHT is accessed by the XOR of the lower bits of the address and the global history register <ref> [5, 13] </ref>. Return destinations are predicted with a 12-entry return stack (per context). The instruction fetch mechanism we assume is the ICOUNT.2.8 mechanism from [7]. It fetches up to eight instructions from up to two threads.
Reference: [6] <author> D.M. Tullsen. </author> <title> Simulation and modeling of a simultaneous multi-threading processor. </title> <booktitle> In 22nd Annual Computer Measurement Group Conference, </booktitle> <month> December </month> <year> 1996. </year>
Reference-contexts: Our simulator is derived from the betaSMT simulator <ref> [6] </ref>. It uses emulation-based, instruction-level simulation. The simulator executes unmodified Alpha object code and models the execution pipelines, memory hierarchy, TLBs, and the branch prediction logic of the processor described in Section 2. This simulator accurately models execution following a branch misprediction.
Reference: [7] <author> D.M. Tullsen, S.J. Eggers, J.S. Emer, H.M. Levy, J.L. Lo, and R.L. Stamm. </author> <title> Exploiting choice: Instruction fetch and issue on an implementable simultaneous multithreading processor. </title> <booktitle> In 23nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191202, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: However, other branches will never fit into this category because they are data-dependent on relatively random data. For those branches, prediction is not a sufficient solution. This paper examines a new speculative execution technique called Threaded Multi-Path Execution (TME). TME uses resources already available on a simultaneous multithreading <ref> [7, 8] </ref> processor to achieve higher instruction level parallelism when there are only one or a few processes running. <p> Multithreading is effective when multiple threads share the system, but does nothing to improve single-thread performance; TME is used for this purpose. Tullsen, et al. <ref> [7] </ref>, showed that SMT can help relieve the branch problem by making the system more tolerant of branch mispredictions, but only in a multiple-thread scenario. <p> The ability to combine instructions from multiple threads in the same cycle allows simultaneous multithreading to both hide latencies and more fully utilize the issue width of a wide superscalar processor. The baseline simultaneous multithreading architecture we use is the SMT processor proposed by Tullsen et al. <ref> [7] </ref>. This processor has the ability to fetch up to eight instructions from the instruction cache each cycle. Those instructions, after decoding and register renaming, find their way to one of two 32-entry instruction queues. <p> The 2K x 2-bit PHT is accessed by the XOR of the lower bits of the address and the global history register [5, 13]. Return destinations are predicted with a 12-entry return stack (per context). The instruction fetch mechanism we assume is the ICOUNT.2.8 mechanism from <ref> [7] </ref>. It fetches up to eight instructions from up to two threads. As many instructions as possible are fetched from the first thread; the second thread is then allowed to use any remaining slots from the 8-instruction fetch bandwidth. <p> We assume a 9-stage instruction pipeline, which is based on the Alpha 21264 pipeline, but includes extra cycles for accessing a large register file, as in <ref> [7] </ref>. This results in a mis-prediction penalty of 7 cycles plus the number of cycles the branch instruction stalled in the processor pipeline. <p> First, the fetch bandwidth has often been identified as the bottleneck resource both in single-thread and multiple-thread processors. Second, the fetch unit is the gateway to the processor giving a thread priority access to the fetch unit gives it priority access to the entire machine. Tullsen et al. <ref> [7] </ref> showed that assigning fetch priorities for threads led to significant performance gains. With TME, the variances between the usefulness of threads are even greater. In our baseline simultaneous multithreading processor, the ICOUNT [7] fetch scheme gives highest priority to threads with the fewest un-issued instructions in the machine. <p> Tullsen et al. <ref> [7] </ref> showed that assigning fetch priorities for threads led to significant performance gains. With TME, the variances between the usefulness of threads are even greater. In our baseline simultaneous multithreading processor, the ICOUNT [7] fetch scheme gives highest priority to threads with the fewest un-issued instructions in the machine. It uses a counter associated with each thread to determine which two threads get to fetch, and which of those have higher priority.
Reference: [8] <author> D.M. Tullsen, S.J. Eggers, and H.M. Levy. </author> <title> Simultaneous multithread-ing: Maximizing on-chip parallelism. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392403, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: However, other branches will never fit into this category because they are data-dependent on relatively random data. For those branches, prediction is not a sufficient solution. This paper examines a new speculative execution technique called Threaded Multi-Path Execution (TME). TME uses resources already available on a simultaneous multithreading <ref> [7, 8] </ref> processor to achieve higher instruction level parallelism when there are only one or a few processes running.
Reference: [9] <author> G. Tyson, K. Lick, and M. Farrens. </author> <title> Limited dual path execution. </title> <type> Technical Report CSE-TR 346-97, </type> <institution> University of Michigan, </institution> <year> 1997. </year>
Reference-contexts: As the results in this paper have shown, the SMT architecture allows for an efficient extension for threaded multi-path execution. Our work also draws from the two studies by Heil and Smith [2] and Tyson, Lick and Farrens <ref> [9] </ref> on restricted dual path execution. Identifying candidate branches for forking through different branch confidence prediction schemes was examined in great detail in both of these studies.
Reference: [10] <author> A. Uht and V. Sindagi. </author> <title> Disjoint eager execution: An optimal form of speculative execution. </title> <booktitle> In 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 313325. </pages> <publisher> IEEE, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: Figure 9 shows that for the mix of programs we examined, and this repartitioning algorithm, that dynamic allocation did not perform better than the default static partitioning. 6 Related Work This research was motivated by the speculative execution technique called Disjoint Eager Execution (DEE) proposed by Uht et al. <ref> [10] </ref>. Instead of speculatively predicting a single-path, DEE in hardware contains an elaborate structure which allows the processor to speculatively execute down multiple paths in a program.
Reference: [11] <author> S. Wallace, B. Calder, </author> <title> and D.M. Tullsen. Threaded multiple path execution. </title> <type> Technical Report CS97-551, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1997. </year>
Reference-contexts: Determining which stores a load may or may not depend on is complex. We have considered two alternative techniques for handling speculative stores, which are briefly described in the following two paragraphs. For a complete description see <ref> [11] </ref>. The first approach assumes a unified non-speculative store buffer, and separate per-thread speculative store buffers (this division could be physical or virtual). A load on an alternate path would have to search both its own and (part of) its primary path's store buffers for possible dependencies.
Reference: [12] <author> K.C. Yeager. </author> <title> The MIPS R10000 superscalar microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 16(2), </volume> <month> April </month> <year> 1996. </year>
Reference-contexts: This results in a mis-prediction penalty of 7 cycles plus the number of cycles the branch instruction stalled in the processor pipeline. Instruction latencies are also based on the Alpha. 2.1 Register Mapping Architecture For register renaming, the SMT architecture uses a mapping scheme (derived from the MIPS R10000 <ref> [12] </ref>) extended for simultaneous multithreading as shown in Figure 1 (except for the mapping synchronization bus, which is added for TME and described in Section 3.2).
Reference: [13] <author> T.-Y. Yeh and Y. Patt. </author> <title> Alternative implementations of two-level adaptive branch prediction. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 124134, </pages> <month> May </month> <year> 1992. </year> <month> 12 </month>
Reference-contexts: Branch prediction is provided by a decoupled branch target buffer (BTB) and pattern history table (PHT) scheme. We use a 256-entry, four-way set associative BTB. The 2K x 2-bit PHT is accessed by the XOR of the lower bits of the address and the global history register <ref> [5, 13] </ref>. Return destinations are predicted with a 12-entry return stack (per context). The instruction fetch mechanism we assume is the ICOUNT.2.8 mechanism from [7]. It fetches up to eight instructions from up to two threads.
References-found: 13

