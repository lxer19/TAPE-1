URL: http://www.cs.cmu.edu/~bsinger/paper.ps
Refering-URL: http://www.cs.cmu.edu/~bsinger/
Root-URL: 
Email: Email: fbsinger+, mmv+g@cs.cmu.edu  bsinger+@cs.cmu.edu  
Phone: Phone  
Title: Learning State Features from Policies to Bias Exploration in Reinforcement Learning Submitted to The Fifteenth
Author: Bryan Singer and Manuela Veloso 
Keyword: reinforcement learning, state generalization, exploration  
Note: Email address of contact author:  number of contact author: (412) 268-3562  
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract: When successful on a single problem, reinforcement learning converges to an optimal policy that maps each state to the best action to take in that state. Unfortunately, this policy learning does not provide the reason why an action should be chosen at a state. In other words, policy learning in general does not map particular state features to particular actions. Therefore, it is infeasible to directly apply the learned policy to new problems that may have the same state features. However, if we use reinforcement learning to solve many individual problems in a specific domain, we can accumulate many policy functions, which can constitute a valuable source of information about the common state features. In this paper, we present our approach and results exploring this research line. For a particular domain, we gather the policies learned by reinforcement learning in a variety of problems. We characterize each state with a set of features and use the policy values learned from featured-states as training examples to a classifier. The learned classifier represents a generalization over all the seen problems, mapping state features to actions. Policy learning in new complex problems is then improved by using the output of the classifier to bias exploration. We present different bias techniques and show empirical results that validate our approach. We demonstrate that state features are effectively learned from learned policies and that reinforcement learning in new problems explores fewer states by using the learned classifier than if no exploration bias is used. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Justin A. Boyan and Andrew W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 369-376. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: State abstraction or generalization is a common method for reducing the search space in reinforcement learning (e.g., <ref> [1, 3, 4, 5, 9, 14] </ref>). In an equivalent way to state abstraction, our algorithm groups states together with a common property which in our algorithm is having the same set of reasonable actions.
Reference: [2] <author> Peter Dayan and Geoffrey E. Hinton. </author> <title> Feudal reinforcement learning. </title> <editor> In Stephen Jose Hanson, Jack D. Cowan, and C. Lee Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 271-278. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Our algorithm has significant flexibility as to what can be learned and when this learned knowledge can be applied, but requires the reinforcement learner to still find the best action to take in every state. Hierarchical abstraction (e.g., <ref> [2, 6, 10] </ref>) is a method of decomposing a task into smaller and smaller subtasks. While the method is very appealing, several suggested algorithms within the context of reinforcement learning require that the hierarchy be hand designed.
Reference: [3] <author> Thomas Dean and Shieu-Hong Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <year> 1995. </year>
Reference-contexts: State abstraction or generalization is a common method for reducing the search space in reinforcement learning (e.g., <ref> [1, 3, 4, 5, 9, 14] </ref>). In an equivalent way to state abstraction, our algorithm groups states together with a common property which in our algorithm is having the same set of reasonable actions.
Reference: [4] <author> Andrew Kachites McCallum. </author> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> Phd. thesis, </type> <institution> Department of Computer Science, University of Rochester, Rochester, </institution> <address> NY, </address> <year> 1995. </year>
Reference-contexts: State abstraction or generalization is a common method for reducing the search space in reinforcement learning (e.g., <ref> [1, 3, 4, 5, 9, 14] </ref>). In an equivalent way to state abstraction, our algorithm groups states together with a common property which in our algorithm is having the same set of reasonable actions.
Reference: [5] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. </title> <booktitle> Machine Learning, </booktitle> <address> 21:199, </address> <year> 1995. </year>
Reference-contexts: State abstraction or generalization is a common method for reducing the search space in reinforcement learning (e.g., <ref> [1, 3, 4, 5, 9, 14] </ref>). In an equivalent way to state abstraction, our algorithm groups states together with a common property which in our algorithm is having the same set of reasonable actions.
Reference: [6] <author> Ronald Parr and Stuart Russell. </author> <title> Reinforcement learning with hierarchies of machines. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 10. </volume> <publisher> The MIT Press, </publisher> <year> 1998. </year> <note> In press. </note>
Reference-contexts: Our algorithm has significant flexibility as to what can be learned and when this learned knowledge can be applied, but requires the reinforcement learner to still find the best action to take in every state. Hierarchical abstraction (e.g., <ref> [2, 6, 10] </ref>) is a method of decomposing a task into smaller and smaller subtasks. While the method is very appealing, several suggested algorithms within the context of reinforcement learning require that the hierarchy be hand designed.
Reference: [7] <author> Doina Precup, Richard S. Sutton, and Satinder P. Singh. </author> <title> Planning with closed-loop macro actions. </title> <booktitle> In Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous Systems, </booktitle> <year> 1997. </year> <note> In press. </note>
Reference-contexts: So, our algorithm does not gain from a reduction in the number of states that state abstraction usually provides, but our algorithm does not suffer from not being able to choose different actions in states that have been grouped together. Action abstraction and macro-operator methods (e.g., <ref> [7, 12] </ref>) learn a sequence of actions which are applicable as a unit in a set of states.
Reference: [8] <author> Peter Stone and Manuela Veloso. </author> <title> Using learned state features in multi-agent reinforcement learning. </title> <note> In Submitted to ICML-98, 1998. 14 </note>
Reference-contexts: Our work focusses on domains for which there are local features to each state that may correlate directly with the actions to explore independent of the particular problem we are trying to solve. In this way our features are "action-dependent" state features <ref> [8] </ref>. For example, in all problems in the 2 maze domain, there is no usefulness to trying to move into a blocked location since that leaves the agent where it was.
Reference: [9] <author> Richard S. Sutton. </author> <title> Generalization in reinforcement learning: Successful ex-amples using sparse coarse coding. </title> <editor> In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 1038-1044. </pages> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: State abstraction or generalization is a common method for reducing the search space in reinforcement learning (e.g., <ref> [1, 3, 4, 5, 9, 14] </ref>). In an equivalent way to state abstraction, our algorithm groups states together with a common property which in our algorithm is having the same set of reasonable actions.
Reference: [10] <author> Prasad Tadepalli and Thomas G. Dietterich. </author> <title> Hierarchical explanation-based reinforcement learning. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <pages> pages 358-366. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, </address> <year> 1997. </year>
Reference-contexts: Our algorithm has significant flexibility as to what can be learned and when this learned knowledge can be applied, but requires the reinforcement learner to still find the best action to take in every state. Hierarchical abstraction (e.g., <ref> [2, 6, 10] </ref>) is a method of decomposing a task into smaller and smaller subtasks. While the method is very appealing, several suggested algorithms within the context of reinforcement learning require that the hierarchy be hand designed.
Reference: [11] <author> Sebastian Thrun and Lorien Pratt, </author> <title> editors. Learning to Learn. </title> <publisher> Kluwer Academic Publisher, </publisher> <year> 1997. </year>
Reference-contexts: some noise that was also reflected in the decision tree, then a weaker bias may have outperformed the stronger ones since in some situations it may have been necessary to take actions not returned by the decision tree. 5 Discussion In other efforts in the area of life-long learning (e.g., <ref> [11] </ref>) solutions to earlier related tasks are used to make solving the next related task easier. Our algorithm also achieves this same goal.
Reference: [12] <author> Sebastian Thrun and Anton Schwartz. </author> <title> Finding structure in reinforcement learning. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 385-392. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: So, our algorithm does not gain from a reduction in the number of states that state abstraction usually provides, but our algorithm does not suffer from not being able to choose different actions in states that have been grouped together. Action abstraction and macro-operator methods (e.g., <ref> [7, 12] </ref>) learn a sequence of actions which are applicable as a unit in a set of states.
Reference: [13] <author> Paul E. Utgoff, Neil C. Berkman, and Jeffery A. Clouse. </author> <title> Decision tree induction based on efficient tree restructuring. </title> <booktitle> Machine Learning, </booktitle> <address> 29:5, </address> <year> 1997. </year>
Reference-contexts: As an example, our current implementation uses the Iterative Tree Induction (ITI) algorithm <ref> [13] </ref> to build a decision tree. This tree induction algorithm maintains a list of all of the classifications seen at each leaf. The decision tree then provides an easy method for determining the set of reasonable actions for a state in a new problem.
Reference: [14] <author> William T. B. Uther and Manuela M. Veloso. </author> <title> Tree based discretization for continuous state space reinforcement learning. </title> <note> In Submitted to AAAI-98, 1998. 15 </note>
Reference-contexts: State abstraction or generalization is a common method for reducing the search space in reinforcement learning (e.g., <ref> [1, 3, 4, 5, 9, 14] </ref>). In an equivalent way to state abstraction, our algorithm groups states together with a common property which in our algorithm is having the same set of reasonable actions.
References-found: 14

