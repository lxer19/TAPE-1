URL: ftp://ftp.isi.edu/pub/hpcc-papers/touch/jsac.gb_enab_apps.ps.Z
Refering-URL: http://www.isi.edu/isi-technical-reports.html
Root-URL: http://www.isi.edu
Title: Abstract defined in the call-for-papers by a list of characteristics that ensures that significant user
Note: 1: Introduction At the First IEEE Gigabit Networking Workshop (GBN), held in Toronto, prior to IEEE Infocom 94, a number of gigabit applications were presented [10]. The GBN submission criteria for gigabit applications were  
Abstract: The First IEEE Gigabit Networking (GBN) Workshop defined a set of characteristics of interesting high-speed applications. The GBN criteria ensure that the application addresses a significant problem, and that it actually requires a gigabit network. This paper presents five challenges that augment the GBN criteria. These challenges ask whether gigabit applications require new research into different protocols, or can be supported by existing protocols that merely run faster. During the past several years the networking research community has been considering the problem of gigabit protocols, especially how they differ from their slower counterparts [9]. There are two primary issues - increased speed or performance of existing protocols, and domains where existing protocols may not suffice. This paper characterizes the latter by a list of challenges developed to complement the GBN criteria. This paper first summarizes the GBN criteria and justifies the need for additional challenges. The challenges are then presented. Finally, as a challenge unanswered is not 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Banerjee, S., Li, V.O., Wang, C., </author> <title> Distributed Database Systems in High-Speed Wide-Area Networks, </title> <journal> IEEE Journal on Selected Areas in Communications, V. </journal> <volume> 11, </volume> <editor> N. </editor> <volume> 4, </volume> <month> May </month> <year> 1993, </year> <pages> pp. 617-630. </pages>
Reference-contexts: One solution is to relocate everything, i.e., to get rid of the need to communicate in the first place. Caching is one way to relocate, and another is to circulate the data <ref> [1] </ref> [4]. 2.2: The pipe is not kept full Assuming speed exists, a method is needed that keeps the bandwidth-delay product pipe full to achieve high throughput. If the bandwidth-delay product is not larger than in a current WAN, current protocols already suffice. <p> Another way to relocate data is to circulate it among the nodes of a protocol [4]. Variants of this protocol rely on predictive behavior of data reuse to govern caching <ref> [1] </ref>. These are useful techniques, but are protocol extrapolations of previously known methods. 4: WWW interactive applications Although it is useful to eliminate domains where giga-bit protocols are not needed, it begs the question of where they are. <p> Also, the WWW drives the interaction towards first-use, because the clients themselves have caches. Char. #2 - Non-linear Communication The feedback needs to be non-deterministic. Otherwise simple pipelining again works fine, as in the case of sending a very large file or database in total <ref> [1] </ref> [4]. WWWias have a branching control structure with recursion, as indicated by the URL links and the history of the browser (user interface). Large windows or packets help only during the transmission of a branch item.
Reference: [2] <author> Berners-Lee, T.J., Cailliau, R., Groff, J-F, Pollermann, B., </author> <title> World-Wide Web: The Information Universe, Electronic Networking: Research, Applications and Policy, </title> <publisher> Meckler Publishing, </publisher> <address> Connecticut, </address> <month> Spring </month> <year> 1992, </year> <pages> pp. 52-58. </pages>
Reference-contexts: The result is to define a set of criteria that require new protocols to use gigabit networking for real applications. The application that exhibits this goal is the WorldWide Web (WWW) client server system <ref> [2] </ref>. It exhibits gigabit requirements when real-time interactive constraints are added [13]. The WWW is emerging as a dominant (and thus realistic) consumer and business application [2]. <p> The application that exhibits this goal is the WorldWide Web (WWW) client server system <ref> [2] </ref>. It exhibits gigabit requirements when real-time interactive constraints are added [13]. The WWW is emerging as a dominant (and thus realistic) consumer and business application [2]. Although originally developed as an interface to Internet navigation of file transfer client/server systems, its current use is evolving towards a distributed interactive application. As such, the requirements on response time are narrowing. <p> This solution requires knowledge of the state space evolution of the other end of the channel, where the state evolution has moderately-constrained branching properties. The domain was described where source-anticipation would help, specifically distributed hypermedia navigation [12] [13]. This describes the WWW, used as a real-time interactive distributed system <ref> [2] </ref>. The WWW browsers are currently used as client/server interfaces, where response time tolerance is high. Casual users have come to begin to expect a level of real-time interaction that does not match the client/server design of the system.
Reference: [3] <author> Braden, R., </author> <title> Extending TCP for Transactions -- Concepts, </title> <address> RFC-1379, USC/ISI, </address> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Bandwidth Server to browser, browser to server. 7: WWWias Architecture The design for a WWWia architecture augments the existing WWW client/server with a presending pump and browser filter (Figures 1, 2) [13]. The pump and filter are supported by either the existing transport protocols or their more recent extensions <ref> [3] </ref>, or by an augmented transport protocol [12]. The pump and filter implement the Web-equivalent of the Parallel Communication protocol [12]. FIGURE 1. Implementation of the WWW intermediaries called the pump and filter. FIGURE 2.
Reference: [4] <author> Herman, G., Gopal, G., Lee, K., and Weinrib, A., </author> <title> The data-cycle architecture for very high throughput database systems, </title> <booktitle> in Proc. ACM SIGMOD Conf., </booktitle> <year> 1987, </year> <pages> pp. 97-103. </pages>
Reference-contexts: One solution is to relocate everything, i.e., to get rid of the need to communicate in the first place. Caching is one way to relocate, and another is to circulate the data [1] <ref> [4] </ref>. 2.2: The pipe is not kept full Assuming speed exists, a method is needed that keeps the bandwidth-delay product pipe full to achieve high throughput. If the bandwidth-delay product is not larger than in a current WAN, current protocols already suffice. <p> A protocol is not needed if there is no communication, or more precisely, if there is no feedback of state between two separated entities. Another way to relocate data is to circulate it among the nodes of a protocol <ref> [4] </ref>. Variants of this protocol rely on predictive behavior of data reuse to govern caching [1]. <p> Also, the WWW drives the interaction towards first-use, because the clients themselves have caches. Char. #2 - Non-linear Communication The feedback needs to be non-deterministic. Otherwise simple pipelining again works fine, as in the case of sending a very large file or database in total [1] <ref> [4] </ref>. WWWias have a branching control structure with recursion, as indicated by the URL links and the history of the browser (user interface). Large windows or packets help only during the transmission of a branch item.
Reference: [5] <author> Jacobson, V., and Braden, R., </author> <title> TCP Extensions for Long-Delay Paths, </title> <institution> RFC-1072, LBL and USC/Information Sciences Institute, </institution> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: There is nothing about sliding-windows protocols that necessitates a particular window size or window granularity, only the implementation has these properties. Increase the number of bits for the window sequence or count over larger window components <ref> [5] </ref> [6] [7]. Even with large windows, there is not enough stuff Even if the windowing mechanism allows large amounts of pipelining, there may not be enough data with which to fill the pipe. A gigabit WAN has 30-100 Mbits in the pipe - 4-12 Mbytes. <p> This helps fill the pipe only if the application has sufficient data to supply. It addresses an implementation deficiency only. The main difficulties are backward compatibility and acquiring the consensus of standards bodies <ref> [5] </ref> [6] [7]. The window size parameter is also an example of a compile or run-time parameter that is unfortunately treated as a specification constant. There are several such parameters maximum protocol data unit, timeout values, window granularity and range, etc.
Reference: [6] <author> Jacobson, V., Braden, R., and Zhang, L., </author> <title> TCP Extensions for High-Speed Paths, </title> <institution> RFC-1185, LBL and USC/Information Sciences Institute, </institution> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: There is nothing about sliding-windows protocols that necessitates a particular window size or window granularity, only the implementation has these properties. Increase the number of bits for the window sequence or count over larger window components [5] <ref> [6] </ref> [7]. Even with large windows, there is not enough stuff Even if the windowing mechanism allows large amounts of pipelining, there may not be enough data with which to fill the pipe. A gigabit WAN has 30-100 Mbits in the pipe - 4-12 Mbytes. <p> This helps fill the pipe only if the application has sufficient data to supply. It addresses an implementation deficiency only. The main difficulties are backward compatibility and acquiring the consensus of standards bodies [5] <ref> [6] </ref> [7]. The window size parameter is also an example of a compile or run-time parameter that is unfortunately treated as a specification constant. There are several such parameters maximum protocol data unit, timeout values, window granularity and range, etc.
Reference: [7] <author> Jacobson, V., Braden, R., and Borman, D., </author> <title> TCP Extensions for High Performance, </title> <institution> RFC-1323, LBL, USC/Information Sciences Institute, and Cray Research, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: There is nothing about sliding-windows protocols that necessitates a particular window size or window granularity, only the implementation has these properties. Increase the number of bits for the window sequence or count over larger window components [5] [6] <ref> [7] </ref>. Even with large windows, there is not enough stuff Even if the windowing mechanism allows large amounts of pipelining, there may not be enough data with which to fill the pipe. A gigabit WAN has 30-100 Mbits in the pipe - 4-12 Mbytes. <p> This helps fill the pipe only if the application has sufficient data to supply. It addresses an implementation deficiency only. The main difficulties are backward compatibility and acquiring the consensus of standards bodies [5] [6] <ref> [7] </ref>. The window size parameter is also an example of a compile or run-time parameter that is unfortunately treated as a specification constant. There are several such parameters maximum protocol data unit, timeout values, window granularity and range, etc. The constancy of these parameters is a limitation of implementations only.
Reference: [8] <author> Kleinrock, </author> <title> L, The Latency / Bandwidth Tradeoff in Giga-bit Networks, </title> <journal> IEEE Communications Magazine, </journal> <volume> Vol. 30, No. 4, </volume> <month> April </month> <year> 1992, </year> <pages> pp. 36-40. </pages>
Reference-contexts: A gigabit WAN has 30-100 Mbits in the pipe - 4-12 Mbytes. That is more RAM than many systems have, and certainly larger than most messages an application has to transmit. One solution is to use multiplexing to share the channel among user processes <ref> [8] </ref>. This is a parallel of process-swapping in OS - when one process runs out of data to send, another is activated. This works, provided that the process activation is deterministic, i.e., that the other side of the channel knows what the process activation order is [11]. <p> Protocol speedup is a control and feedback issue, sensitive to the bandwidth-delay product only. Challenge #2 - Multiplex (Deterministic) Multiplexing has been proposed as a solution to the do not have enough stuff to send issue, as mentioned before <ref> [8] </ref>. This is equivalent to not having a gigabit application - the gigabits are shared among a set of applications on a workstation. Using multiple hosts, users, or processes are all ways of providing aggregate gigabits only. In the deterministic multiplexer case, this avoids the domain examined here [11].
Reference: [9] <institution> NSF Report 92-109, </institution> <note> Research Priorities in Networking and Communications, </note> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: During the past several years the networking research community has been considering the problem of gigabit protocols, especially how they differ from their slower counterparts <ref> [9] </ref>. There are two primary issues - increased speed or performance of existing protocols, and domains where existing protocols may not suffice. This paper characterizes the latter by a list of challenges developed to complement the GBN criteria.
Reference: [10] <editor> Sterbenz, J., et. al., </editor> <booktitle> Gigabit Networking Workshop 94, </booktitle> <address> &lt;http://info.gte.com/ieee-tcgn/conference/gbn94&gt;. </address>
Reference-contexts: 1: Introduction At the First IEEE Gigabit Networking Workshop (GBN), held in Toronto, prior to IEEE Infocom 94, a number of gigabit applications were presented <ref> [10] </ref>. The GBN submission criteria for gigabit applications were defined in the call-for-papers by a list of characteristics that ensures that significant user bases exist, and that a gigabit network was required. <p> Government. This paper is based on a presentation given at the First IEEE Gigabit Networking Workshop, May 1994, Toronto <ref> [10] </ref>. entirely useful, an application is presented that survives these challenges. <p> This is done to open the door to broader consideration of some unconventional trade-offs that use bandwidth as a resource 2 rather than as a constraint. 1.1: The GBN Criteria - A Review The GBN criteria were described in its call-for-papers <ref> [10] </ref>. They are designed to ensure a significant user base, and that a gigabit network is required.
Reference: [11] <author> Touch, J.D., and Farber, </author> <title> D.,Reducing Latency in Communication, </title> <journal> IEEE Communications Magazine, </journal> <volume> Vol. 31, No. 2, </volume> <month> Feb. </month> <year> 1993, </year> <pages> pp. 8-9. </pages>
Reference-contexts: This is a parallel of process-swapping in OS - when one process runs out of data to send, another is activated. This works, provided that the process activation is deterministic, i.e., that the other side of the channel knows what the process activation order is <ref> [11] </ref>. This is also tantamount to not having a gigabit application protocol - each application does not use a gigabit channel, so bandwidth needs to be aggregated over a set of multiplexed applications. <p> Some believe this problem will disappear, but other investigations indicate it will not <ref> [11] </ref> [12]. The transient environment has already been observed - when files were much larger than the bandwidth-delay product. The current ratios are not transient, but will continue. It is now recognized that bandwidth needs to keep pace with processing and storage evolution. <p> This is equivalent to not having a gigabit application - the gigabits are shared among a set of applications on a workstation. Using multiple hosts, users, or processes are all ways of providing aggregate gigabits only. In the deterministic multiplexer case, this avoids the domain examined here <ref> [11] </ref>. In the nondeterministic case, the problem has just moved down a level in the protocol stack.
Reference: [12] <author> Touch, J.D., </author> <title> Parallel Communication, </title> <booktitle> Proc. IEEE Info-com, </booktitle> <month> Mar. </month> <year> 1993, </year> <pages> pp. 505-512. </pages>
Reference-contexts: Some believe this problem will disappear, but other investigations indicate it will not [11] <ref> [12] </ref>. The transient environment has already been observed - when files were much larger than the bandwidth-delay product. The current ratios are not transient, but will continue. It is now recognized that bandwidth needs to keep pace with processing and storage evolution. <p> Control parallelization works where packets are unrelated - e.g., UDP, but not for TCP-like protocols [14]. In the latter case, regardless of partitioning (per packet, per function, etc.), the parallelism is limited to about 5 processors per connection. The real issue is that of protocol relativity <ref> [12] </ref>. A protocol does not know the clock rate - only the number of bits in transit between components. Protocol speedup is a control and feedback issue, sensitive to the bandwidth-delay product only. <p> This is how sliding windows works - by predicting subsequent states, in a linear manner. If the subsequent state is not predictable, neither is the subsequent data <ref> [12] </ref>. It does not matter whether it is the application state, or the multiplexer state. Nondeterministic multiplexing moves the state prediction problem to the multiplexer-synchronization level, i.e., lower in the protocol stack. <p> The goal is to reduce the perceived latency, to give the illusion of low latency. This work began as Mirage (a model) and continues as Parallel Communication (a protocol based on Mirage) <ref> [12] </ref>. Latency compensation is possible using source-based anticipation (presending). The composition of two pre-sending channels (back-to-back) is the more common receiver-based anticipation, i.e., prefetching. This differs because it is source-based. There are several advantages of presending over prefetching. Presending distributes the computational effort between source and receiver. <p> This solution requires knowledge of the state space evolution of the other end of the channel, where the state evolution has moderately-constrained branching properties. The domain was described where source-anticipation would help, specifically distributed hypermedia navigation <ref> [12] </ref> [13]. This describes the WWW, used as a real-time interactive distributed system [2]. The WWW browsers are currently used as client/server interfaces, where response time tolerance is high. <p> Large windows or packets help only during the transmission of a branch item. The branching structure cannot be accommodated by current sliding-windows protocols, and inhibits use of large linear windows or large packets <ref> [12] </ref>. The combination of feedback and nonlinear communication defines a rich control structure. It is this structure that the source uses to guide its presending. Making the data chunks larger reduces the richness of the control. <p> The pump and filter are supported by either the existing transport protocols or their more recent extensions [3], or by an augmented transport protocol <ref> [12] </ref>. The pump and filter implement the Web-equivalent of the Parallel Communication protocol [12]. FIGURE 1. Implementation of the WWW intermediaries called the pump and filter. FIGURE 2. Design of the Pump and Filter appears to the server and client as if it were a Proxy Cache. <p> The pump and filter are supported by either the existing transport protocols or their more recent extensions [3], or by an augmented transport protocol <ref> [12] </ref>. The pump and filter implement the Web-equivalent of the Parallel Communication protocol [12]. FIGURE 1. Implementation of the WWW intermediaries called the pump and filter. FIGURE 2. Design of the Pump and Filter appears to the server and client as if it were a Proxy Cache. The pump acts as a proxy for the browser at the server.
Reference: [13] <author> Touch, J.D., and Farber, D., </author> <title> An Experiment in Latency Reduction, </title> <booktitle> Proc. IEEE Infocom, </booktitle> <month> May. </month> <year> 1994, </year> <pages> pp. 175-181. </pages>
Reference-contexts: The result is to define a set of criteria that require new protocols to use gigabit networking for real applications. The application that exhibits this goal is the WorldWide Web (WWW) client server system [2]. It exhibits gigabit requirements when real-time interactive constraints are added <ref> [13] </ref>. The WWW is emerging as a dominant (and thus realistic) consumer and business application [2]. Although originally developed as an interface to Internet navigation of file transfer client/server systems, its current use is evolving towards a distributed interactive application. As such, the requirements on response time are narrowing. <p> Nondeterministic multiplexing moves the state prediction problem to the multiplexer-synchronization level, i.e., lower in the protocol stack. Challenge #3 - Use Large Payloads Using large payloads is another way to shut off the protocol, and increase the effective speed of the control proto col <ref> [13] </ref>. Versions of TCP run at 1 Gbps by using 64Kbyte packets, i.e., by this technique (e.g., UltraNet). Using large payloads slows down the control protocol. Header frequency determines the rate of the protocol. The ratio of header to payload determines the stability of the protocol [14]. <p> This solution requires knowledge of the state space evolution of the other end of the channel, where the state evolution has moderately-constrained branching properties. The domain was described where source-anticipation would help, specifically distributed hypermedia navigation [12] <ref> [13] </ref>. This describes the WWW, used as a real-time interactive distributed system [2]. The WWW browsers are currently used as client/server interfaces, where response time tolerance is high. <p> WWWias are well-defined - the server is one side, the browser is the other. 6: An Example that Survives the Challenges This section explains how WWWia survives the challenges (and exhibits the characteristics). Specifically, it describes WWWias modified with server-based preload-ing of the browser cache <ref> [13] </ref>. 6.1: Survival The WWWias survive the five challenges as follows: #1-Increase the Clock Rate Current WWW would just get each hypertext page faster. <p> Bandwidth Server to browser, browser to server. 7: WWWias Architecture The design for a WWWia architecture augments the existing WWW client/server with a presending pump and browser filter (Figures 1, 2) <ref> [13] </ref>. The pump and filter are supported by either the existing transport protocols or their more recent extensions [3], or by an augmented transport protocol [12]. The pump and filter implement the Web-equivalent of the Parallel Communication protocol [12]. FIGURE 1.

References-found: 13

