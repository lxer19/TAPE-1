URL: http://www.iro.umontreal.ca/~marcotte/ARTIPS/newton.ps.gz
Refering-URL: http://www.iro.umontreal.ca/~marcotte/publi.html
Root-URL: http://www.iro.umontreal.ca
Title: A NOTE ON A GLOBALLY CONVERGENT NEWTON METHOD FOR SOLVING MONOTONE VARIATIONAL INEQUALITIES  
Author: Patrice MARCOTTE Jean-Pierre DUSSAULT 
Abstract: Resume. Il est bien connu que la methode de Newton, lorsqu'appliquee a un probleme d'inequation variationnelle fortement monotone, converge localement vers la solution de l'inequation, et que l'ordre de convergence est quadratique. Dans cet article nous mon-trons que la direction de Newton constitue une direction de descente pour un objectif non differentiable et non convexe, et ceci m^eme en l'abscence de l'hypothese de monotonie forte. Ce resultat permet de modifier la methode et de la rendre globalement convergente. De plus, sous l'hypothese de forte monotonie, les deux methodes sont localement equivalentes: il s'ensuit que la methode modifiee herite des proprietes de convergence de la methode de Newton: identification implicite des contraintes actives a la solution (sous l'hypothese de stricte complementarite) et ordre de convergence quadratique. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Auslender, </author> <title> Optimisation; methodes numeriques, </title> <publisher> Masson, </publisher> <address> Paris (1976). </address>
Reference: [2] <author> J.M. Danskin, </author> <title> "The theory of min-max, with applications", </title> <note> SIAM Journal of Applied Mathematics 14 , 641-664 (1966). </note>
Reference-contexts: Lemma. If x k is not solution to V IP , then the direction d k generated by GNEW is a feasible descent direction for g at x k . Proof. We have, by Danskin's rule of differentiation of max-functions (see Dan-skin <ref> [2] </ref>): g 0 (x; d) = max d t r x f (x y) t F (x)g Therefore: g 0 (x k ; d k ) = max (x x k ) t (F (x k ) + F 0 t = (x x k ) t F (x k )
Reference: [3] <author> D.W. Hearn, </author> <title> "The gap function of a convex program", </title> <note> Operations Research Letters 1 , 67-71 (1981). </note>
Reference-contexts: The gap function, though in general nondifferentiable and nonconvex, can be driven to zero in a monotone fashion by specialized algorithms (Marcotte [5], Marcotte and Dussault [6]). The term "gap function" has been used by Hearn <ref> [3] </ref> to denote the same function, although in an optimization framework, i.e. when F is the gradient of some convex function f . Definition 3.
Reference: [4] <author> D.J. Luenberger, </author> <title> Introduction to linear and nonlinear programming, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass. </address> <year> (1973). </year>
Reference: [5] <author> P. Marcotte, </author> <title> "A new algorithm for solving variational inequalities, with application to the traffic assignment problem", </title> <note> Mathematical Programming 33 339-351 (1985). </note>
Reference-contexts: The gap function, though in general nondifferentiable and nonconvex, can be driven to zero in a monotone fashion by specialized algorithms (Marcotte <ref> [5] </ref>, Marcotte and Dussault [6]). The term "gap function" has been used by Hearn [3] to denote the same function, although in an optimization framework, i.e. when F is the gradient of some convex function f . Definition 3.
Reference: [6] <author> P. Marcotte and J.-P. Dussault, </author> <title> "A modified Newton method for solving variational inequalities", </title> <booktitle> Proceedings of the 24 th IEEE Conference on Decision and Control, </booktitle> <address> Fort Lauderdale, </address> <month> December 11-13 </month> <year> (1985). </year>
Reference-contexts: The gap function, though in general nondifferentiable and nonconvex, can be driven to zero in a monotone fashion by specialized algorithms (Marcotte [5], Marcotte and Dussault <ref> [6] </ref>). The term "gap function" has been used by Hearn [3] to denote the same function, although in an optimization framework, i.e. when F is the gradient of some convex function f . Definition 3. <p> Proof. From the proof of proposition 8 in Marcotte and Dussault <ref> [6] </ref> we have that kx fl xk O (g (x)).
Reference: [7] <author> J.S. Pang and D. Chan, </author> <title> "Iterative methods for variational and complementarity problems", </title> <note> Mathematical Programming 24 , 284-313 (1982). </note>
Reference: [8] <author> S.M. Robinson, </author> <title> "Generalized equations", in Mathematical programming: The state of the art, </title> <editor> Bachem, Grotschel, Korte ed., </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin (1983). </address>
Reference-contexts: QED Remark. It is known (see Robinson <ref> [8] </ref>) that Newton's method does not require strong monotonicity for quadratic convergence.
Reference: [9] <author> B. von Hohenbalken, </author> <title> "A finite algorithm to maximize certain pseudo-concave functions", </title> <booktitle> Mathematical Programming 8, </booktitle> <month> 189-196 </month> <year> (1975). </year>
Reference-contexts: Numerical results. The algorithm has been developed while investigating efficient methods for solving large-scale network equilibrium problems, using the restriction strategy of von Hohenbalken <ref> [9] </ref>. At each iteration, a variational inequality problem on the unit simplex is being solved. For the restricted variational inequality, the gap function can be evaluated by inspecting its value at the extreme points defining the current restriction, rather than by solving a linear program over .
Reference: [10] <author> R. Mi*in, </author> <title> "A superlinearly convergent algorithm for one-dimensional minimization with convex functions", </title> <note> Mathematics of Operations Research 8, </note> <month> 189-206 </month> <year> (1983). </year> <title> Aknowledgments. We want to thank Rene Ferland for his careful programming of the test problems, and a referee for suggestions that helped improve the paper. </title> <type> 7 </type>
Reference-contexts: However, in our case, it can be proven that the directional derivative along Newton's direction varies continuously with x, due to the differentiability of the cost mapping F . Hence it is unnecessary to use a more sophisticated linesearch strategy, such as the one proposed by Mi*in <ref> [10] </ref>. On our test examples, Armijo's rule performed very satisfactorily. 6
References-found: 10

