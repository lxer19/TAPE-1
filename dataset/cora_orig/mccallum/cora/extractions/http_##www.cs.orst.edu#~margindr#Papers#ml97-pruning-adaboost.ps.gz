URL: http://www.cs.orst.edu/~margindr/Papers/ml97-pruning-adaboost.ps.gz
Refering-URL: http://www.cs.orst.edu/~margindr/ML_RG/winter97-mlrg.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: margindr@cs.orst.edu  tgd@cs.orst.edu  
Title: Pruning Adaptive Boosting  
Author: Dragos Margineantu Thomas G. Dietterich 
Web: http://www.cs.orst.edu  
Address: Corvallis, OR 97331-3202  
Affiliation: Department of Computer Science Oregon State University,  
Abstract: The boosting algorithm AdaBoost, developed by Freund and Schapire, has outperformed all other learning algorithms on several benchmark problems when using C4.5 as the "weak" algorithm to be "boosted." Like other ensemble learning approaches, AdaBoost constructs a composite hypothesis by voting many individual hypotheses. In practice, the large amount of memory required to store these hypotheses can make ensemble methods hard to deploy in applications. This paper shows that by selecting a subset of the hypotheses, it is possible to obtain nearly the same levels of performance as the entire set. The results also provide some insight into the behavior of AdaBoost. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bakiri, G. </author> <year> (1991). </year> <title> Converting English text to speech: A machine learning approach. </title> <type> Tech. rep. </type> <institution> 91-30-2, Department of Computer Science, Oregon State University, Corvallis, </institution> <address> OR. </address>
Reference-contexts: However, each tree requires 295 Kbytes of memory, so an ensemble of 200 trees requires 59 Mbytes. Similarly, in an application of error-correcting output coding to the NETtalk task <ref> (Bakiri, 1991) </ref>, an ensemble based on 127 decision trees requires 1.3 Mbytes while storing the data set itself requires only 590Kbytes, so the ensemble is much bigger than the data set from which it was constructed.
Reference: <author> Bishop, Y. M. M., Fienberg, S. E., & Holland, P. W. </author> <year> (1975). </year> <title> Discrete multivariate analysis: Theory and practice. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: This is repeated until U contains M classifiers. 3.3 Kappa Pruning Another way of choosing diverse classifiers is to measure how much their classification decisions differ. Statisticians have developed several measures of agreement (or disagreement) between classifiers. The most widely used measure is the Kappa statistic, <ref> (Bishop, Fienberg, & Holland, 1975) </ref>. Here, we employ a slight modification of , which we will call 0 . It is defined as follows.
Reference: <author> Breiman, L. </author> <year> (1996a). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24(2). </volume>
Reference: <author> Breiman, L. </author> <year> (1996b). </year> <title> Bias, variance, and arcing classifiers. </title> <type> Tech. rep., </type> <institution> Department of Statistics, University of California, Berkeley. </institution>
Reference: <author> Cover, T., & Thomas, J. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> J.Wiley and Sons, Inc. </publisher>
Reference: <author> Freund, Y., & Schapire, R. </author> <year> (1996). </year> <title> Experiments with a new boosting algorithm. </title> <booktitle> In Proceedings of the International Conference in Machine Learning San Francisco, </booktitle> <address> CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1995). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <type> Tech. rep., </type> <institution> AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ. </address>
Reference-contexts: 1 Introduction The adaptive boosting algorithm AdaBoost <ref> (Freund & Schapire, 1995) </ref> in combination with the decision-tree algorithm C4.5 (Quinlan, 1993) has been shown to be a very accurate learning procedure (Freund & Schapire, 1996; Quinlan, 1996; Breiman, 1996b).
Reference: <author> Friedman, J. H., & Stuetzle, W. </author> <year> (1981). </year> <title> Projection pursuit regression. </title> <journal> J. American Statistical Association, </journal> <volume> 76 (376), </volume> <pages> 817-823. </pages>
Reference-contexts: Our goal is to choose the set of M classifiers that give the best voted performance on the pruning set. We could use a greedy algorithm to approximate this, but we decided to use a more sophisticated search method called backfitting <ref> (Friedman & Stuetzle, 1981) </ref>. Backfitting proceeds as follows. Like a simple greedy algorithm, it is a procedure for constructing a set U of classifiers by growing U one classifier at a time. The first two steps are identical to the greedy algorithm.
Reference: <author> Murphy, P., & Aha, D. </author> <year> (1994). </year> <title> UCI repository of machine learning databases. </title> <type> Tech. rep., </type> <address> U.C.Irvine, Irvine, CA. </address>
Reference-contexts: Then it takes another greedy step to expand U . This continues until U contains M classifiers. 4 Experiments and Results We tested these five pruning techniques on six data sets (see Table 2). Except for the Expf data set, all were drawn from the Irvine Repository <ref> (Murphy & Aha, 1994) </ref>. Expf is a synthetic data set with only 2 features. Data points are drawn uniformly from the rectangle x 2 [10; +10]; y 2 [10; +10] and labeled according to the decision boundaries shown in Figure 2.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Empirical Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: 1 Introduction The adaptive boosting algorithm AdaBoost (Freund & Schapire, 1995) in combination with the decision-tree algorithm C4.5 <ref> (Quinlan, 1993) </ref> has been shown to be a very accurate learning procedure (Freund & Schapire, 1996; Quinlan, 1996; Breiman, 1996b). Like all ensemble methods, AdaBoost works by generating a set of classifiers and then voting them to classify test examples.
Reference: <author> Quinlan, J. </author> <year> (1996). </year> <title> Bagging, boosting, </title> <booktitle> and C4.5. In Proceedings AAAI-96. </booktitle> <pages> 10 </pages>
References-found: 11

