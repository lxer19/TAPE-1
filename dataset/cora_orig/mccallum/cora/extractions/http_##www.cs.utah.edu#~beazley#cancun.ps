URL: http://www.cs.utah.edu/~beazley/cancun.ps
Refering-URL: http://www.cs.utah.edu/~beazley/publications.html
Root-URL: 
Title: A High Performance Communications and Memory Caching Scheme for Molecular Dynamics on the CM-5  
Author: David M. Beazley, Peter S. Lomdahl, Niels Grtnbech-Jensen, and Pablo Tamayo 
Address: Los Alamos, NM 87545  
Affiliation: Theoretical Division and Advanced Computing Laboratory Los Alamos National Laboratory,  
Abstract: We present several techniques that we have used to optimize the performance of a message-passing C code for molecular dynamics on the CM-5. We describe our use of the CM-5 vector units and a parallel memory caching scheme that we have developed to speed up the code by more than 50%. A modification that decreases our communication time by 35% is also presented along with a discussion of how we have been able to take advantage of the CM-5 hardware without significantly compromising code portability. We have been able to speed up our original code by a factor of ten and we feel that our modifications may be useful in optimizing the performance of other message-passing C applications on the CM-5. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Computer Simulations of Liquids, </institution> <note> M. </note> <editor> P. Allen and D. J. Tildesley. </editor> <publisher> Clarendon Press, </publisher> <address> Oxford (1987). </address>
Reference: [2] <author> A. I. Mel'cuk, R. C. Giles, and H. Gould, </author> <title> Computers in Physics, </title> <note> May/June 1991, p. 311. </note>
Reference-contexts: This substantially reduces the complexity of the problem, but there are still many computational difficulties associated with short-range MD simulations. Due to limitations in computing resources, most MD simulations have been limited to small simulations involving less than a million atoms <ref> [2, 3, 10, 11] </ref>. However, the development of massively parallel supercomputers has generated considerable interest in developing fast parallel algorithms for performing multi-million atom MD simulations [2, 3, 4, 5, 6, 7, 8]. <p> Due to limitations in computing resources, most MD simulations have been limited to small simulations involving less than a million atoms [2, 3, 10, 11]. However, the development of massively parallel supercomputers has generated considerable interest in developing fast parallel algorithms for performing multi-million atom MD simulations <ref> [2, 3, 4, 5, 6, 7, 8] </ref>. On state of the art MPP systems, simulations with more than 100 million (10 8 ) atoms can now be performed [9]. <p> Caching at different stages of force calculation gorithms (particularly on SIMD machines) <ref> [2, 3] </ref>. In a SIMD implementation, each cell might correspond to a different processor and each step of the path would involve communications. However, on the CM-5, each processor can hold many thousands of cells and only a fraction of these lie on the processor boundaries.
Reference: [3] <author> P. Tamayo, J. P. Mesirov, and B. M. </author> <title> Boghosian, </title> <booktitle> Proc. of Supercomputing 91, IEEE Computer Society (1991), </booktitle> <address> p. </address> <month> 462. </month>
Reference-contexts: This substantially reduces the complexity of the problem, but there are still many computational difficulties associated with short-range MD simulations. Due to limitations in computing resources, most MD simulations have been limited to small simulations involving less than a million atoms <ref> [2, 3, 10, 11] </ref>. However, the development of massively parallel supercomputers has generated considerable interest in developing fast parallel algorithms for performing multi-million atom MD simulations [2, 3, 4, 5, 6, 7, 8]. <p> Due to limitations in computing resources, most MD simulations have been limited to small simulations involving less than a million atoms [2, 3, 10, 11]. However, the development of massively parallel supercomputers has generated considerable interest in developing fast parallel algorithms for performing multi-million atom MD simulations <ref> [2, 3, 4, 5, 6, 7, 8] </ref>. On state of the art MPP systems, simulations with more than 100 million (10 8 ) atoms can now be performed [9]. <p> Caching at different stages of force calculation gorithms (particularly on SIMD machines) <ref> [2, 3] </ref>. In a SIMD implementation, each cell might correspond to a different processor and each step of the path would involve communications. However, on the CM-5, each processor can hold many thousands of cells and only a fraction of these lie on the processor boundaries.
Reference: [4] <author> B. L. Holian et al. </author> <note> Phys. Rev. A 43, 2655 (1991). </note>
Reference-contexts: Due to limitations in computing resources, most MD simulations have been limited to small simulations involving less than a million atoms [2, 3, 10, 11]. However, the development of massively parallel supercomputers has generated considerable interest in developing fast parallel algorithms for performing multi-million atom MD simulations <ref> [2, 3, 4, 5, 6, 7, 8] </ref>. On state of the art MPP systems, simulations with more than 100 million (10 8 ) atoms can now be performed [9].
Reference: [5] <author> R. C. Giles and P. Tamayo, </author> <booktitle> Proc. of SHPCC'92, IEEE Computer Society (1992), </booktitle> <address> p. </address> <month> 240. </month>
Reference-contexts: Due to limitations in computing resources, most MD simulations have been limited to small simulations involving less than a million atoms [2, 3, 10, 11]. However, the development of massively parallel supercomputers has generated considerable interest in developing fast parallel algorithms for performing multi-million atom MD simulations <ref> [2, 3, 4, 5, 6, 7, 8] </ref>. On state of the art MPP systems, simulations with more than 100 million (10 8 ) atoms can now be performed [9].
Reference: [6] <author> S. Plimpton and G. Heffelfinger, </author> <booktitle> Proc. of SHPCC'92, IEEE Computer Society (1992), </booktitle> <address> p. </address> <month> 246. </month>
Reference-contexts: Due to limitations in computing resources, most MD simulations have been limited to small simulations involving less than a million atoms [2, 3, 10, 11]. However, the development of massively parallel supercomputers has generated considerable interest in developing fast parallel algorithms for performing multi-million atom MD simulations <ref> [2, 3, 4, 5, 6, 7, 8] </ref>. On state of the art MPP systems, simulations with more than 100 million (10 8 ) atoms can now be performed [9]. <p> Another approach that is commonly used is to gather all of the particle data and communicate it in several large messages. This can be done in as few as 6 message passing calls in some cases <ref> [6, 7] </ref>. However, we feel that our approach offers several advantages. First, our scheme only requires a small buffer for storing one cell of particles. If we were to communicate particle data from all of the boundary cells at once, it would require a tremendous memory overhead. <p> Our best timing is that for 65 million particles on 1024 PNs. The update time of 16.55 seconds corresponds to 250 nano-seconds per particle. To the best of our knowledge, this is the lowest reported timing to date <ref> [6, 7] </ref>. Our optimizations were mainly designed to dramatically improve the speed of simulations using a small 1 We actually do slightly more communications than is necessary.
Reference: [7] <author> S. Plimpton, </author> <title> Fast Parallel Algorithms for Short-Range Molecular Dynamics, </title> <institution> Sandia National Laboratory Report, SAND91-1144, </institution> <month> UC705 </month> <year> (1993). </year>
Reference-contexts: Due to limitations in computing resources, most MD simulations have been limited to small simulations involving less than a million atoms [2, 3, 10, 11]. However, the development of massively parallel supercomputers has generated considerable interest in developing fast parallel algorithms for performing multi-million atom MD simulations <ref> [2, 3, 4, 5, 6, 7, 8] </ref>. On state of the art MPP systems, simulations with more than 100 million (10 8 ) atoms can now be performed [9]. <p> Another approach that is commonly used is to gather all of the particle data and communicate it in several large messages. This can be done in as few as 6 message passing calls in some cases <ref> [6, 7] </ref>. However, we feel that our approach offers several advantages. First, our scheme only requires a small buffer for storing one cell of particles. If we were to communicate particle data from all of the boundary cells at once, it would require a tremendous memory overhead. <p> Our best timing is that for 65 million particles on 1024 PNs. The update time of 16.55 seconds corresponds to 250 nano-seconds per particle. To the best of our knowledge, this is the lowest reported timing to date <ref> [6, 7] </ref>. Our optimizations were mainly designed to dramatically improve the speed of simulations using a small 1 We actually do slightly more communications than is necessary.
Reference: [8] <author> D. M. Beazley and P. S. Lomdahl, </author> <title> MessagePassing Multi-Cell Molecular Dynamics on the Connection Machine 5, Parall. </title> <journal> Comp. </journal> <note> (to appear 1994). </note>
Reference-contexts: Due to limitations in computing resources, most MD simulations have been limited to small simulations involving less than a million atoms [2, 3, 10, 11]. However, the development of massively parallel supercomputers has generated considerable interest in developing fast parallel algorithms for performing multi-million atom MD simulations <ref> [2, 3, 4, 5, 6, 7, 8] </ref>. On state of the art MPP systems, simulations with more than 100 million (10 8 ) atoms can now be performed [9]. <p> Processor layout and force calculation. MD simulations, we should stress that our algorithm can use any short-range pair potential. 2 The Multi-cell MD Algorithm Our algorithm has been described in detail in <ref> [8] </ref>. Here we briefly outline its main features, illustrating the algorithm in 2D, but it extends naturally to 3D. The main problem in all short-range MD simulations is that of determining which particles interact and calculating the resulting forces. <p> We have made these modifications at various stages of code development which has allowed us to track the improvements in performance. Our first version of code did not use vector units and a simulation involving approximately 65 million 3D particles required approximately 87.4 seconds per time step <ref> [8] </ref>. This particular simulation involved a smaller interaction cutoff of r max = 2:0 where each particle had approximately 112 interacting neighbors in addition to a slightly modified LJ- potential. In any case, we can use this to roughly estimate the speedup gained by our modifications.
Reference: [9] <author> P. S. Lomdahl, P. Tamayo, N. Grtnbech-Jensen, and D. M. </author> <title> Beazley, </title> <booktitle> Proc. of Supercomputing 93, IEEE Computer Society (1993), p. </booktitle> <pages> 520-527. </pages>
Reference-contexts: On state of the art MPP systems, simulations with more than 100 million (10 8 ) atoms can now be performed <ref> [9] </ref>. Simulations of this size will be crucial in performing realistic experiments in materials science where it will be necessary to simulate hundreds of millions or even billions of atoms to realistically capture the behavior of dislocations, fracture, and crack propagation. <p> This has improved the time required to perform a single timestep by roughly a factor of ten. In addition, SPaSM was one of the winners in the 1993 IEEE Gordon Bell prize competition for achieving a speed of 50 Gflops on a 1024 processor CM-5 <ref> [9] </ref>. In this paper, we provide a brief overview of our general MD algorithm and focus on the enhancements that have allowed us to achieve high performance on the CM-5. We also present recent timings. <p> However, the modifications have also improved runs using a larger cutoff of r max = 5:0. In early timings using the VUs, a simulation with 65,536,000 particles on 1024 PNs using r max = 5:0 required 92.91 seconds per timestep and ran at 50 Gflops <ref> [9] </ref>. In this simulation, each particle calculated forces with approximately 1750 neighbors where each force calculation required 42 floating point operations. Recently, we were able to repeat this calculation using our latest version of code. The time per timestep dropped to 83.0 seconds, a speedup of 11%.
Reference: [10] <author> W.C. Swope and H.C. </author> <title> Andersen, </title> <journal> Phys. Rev. B, </journal> <volume> 41, </volume> <month> 7042 </month> <year> (1990). </year>
Reference-contexts: This substantially reduces the complexity of the problem, but there are still many computational difficulties associated with short-range MD simulations. Due to limitations in computing resources, most MD simulations have been limited to small simulations involving less than a million atoms <ref> [2, 3, 10, 11] </ref>. However, the development of massively parallel supercomputers has generated considerable interest in developing fast parallel algorithms for performing multi-million atom MD simulations [2, 3, 4, 5, 6, 7, 8].
Reference: [11] <institution> W.G. Hoover et al., Phys. </institution> <note> Rev. A, 42, 5844 (1990). </note>
Reference-contexts: This substantially reduces the complexity of the problem, but there are still many computational difficulties associated with short-range MD simulations. Due to limitations in computing resources, most MD simulations have been limited to small simulations involving less than a million atoms <ref> [2, 3, 10, 11] </ref>. However, the development of massively parallel supercomputers has generated considerable interest in developing fast parallel algorithms for performing multi-million atom MD simulations [2, 3, 4, 5, 6, 7, 8].
Reference: [12] <institution> VU Programmer's Handbook, Thinking Machines Corporation (1993), Cambridge, Massachusetts. </institution>
References-found: 12

