URL: http://www.cs.columbia.edu/~pkc/papers/jpdc91.ps
Refering-URL: http://www.cs.columbia.edu/~pkc/
Root-URL: 
Title: PARULEL: Parallel Rule Processing Using Meta-rules for Redaction  
Author: Salvatore J. Stolfo Ouri Wolfson Philip K. Chan Hasanat M. Dewan Leland Woodbury Jason S. Glazier David A. Ohsie 
Note: This work is partially supported by Citicorp and the New York State Science and Technology Foundation through the Center for Advanced Technology under contract NYSSTFCU01207901, and by NSF grant IRI-90-03341.  
Date: June 25, 1991  
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Afrati, F., and Papadimitriou, C. </author> <title> The parallel complexity of simple chain queries. </title> <booktitle> In Proc. 6th ACM Symposium on PODS (1987), </booktitle> <pages> pp. 210-213. </pages>
Reference-contexts: Many efforts in the database community have been devoted to theoretical characterization of the logic programs that belong to the NC complexity class <ref> [1, 12, 30] </ref>. If a program is in NC, it means that it can be interpreted very fast, given an essentially unbounded number of processors; the processors have to communicate extensively, usually through shared memory.
Reference: [2] <author> Brownston, L., Farrell, R., Kant, E., and Martin, N. </author> <title> Programming Expert Systems in OPS5: An Introduction to Rule-Based Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1985. </year>
Reference-contexts: Executing rule-based systems with massive databases is considerably more expensive; the combinatorics of matching explodes. Fourth, often sequentialities are programmed into the rule base to force desirable sequences of rule firings (see <ref> [2] </ref> for example). These sequentialities are represented by control elements, or secret messages, that serve to activate or deactivate rules as these elements are added to or deleted from the working memory.
Reference: [3] <author> Cheng, F.-C., Chen, H.-H., and Perng, J.-H. </author> <title> Parallel execution on production systems. </title> <booktitle> In Proceedings of the Second IEEE Symposium on Parallel and Distributed Processing (1990), </booktitle> <pages> pp. 463-470. </pages>
Reference-contexts: If two rules do not interfere with each other and are frequently fired simultaneously, they should be allocated to different processors. Ishida [11] proposes a heuristic approach while Cheng <ref> [3] </ref> proposes a probabilistic one. 2.3 Program Transformation We note that various rules in a single rule-based program require different amounts of time to match than others. Some rules are "hot spots" requiring much more processing time than others.
Reference: [4] <author> Cohen, S., and Wolfson, O. </author> <title> Why a single parallelization strategy is not enough in knowledge bases. </title> <booktitle> In Proc. 8th ACM Symposium on PODS (1989), </booktitle> <pages> pp. 200-217. </pages>
Reference-contexts: Some rules are "hot spots" requiring much more processing time than others. Rules that match many working memory elements are costly to match and should be the subject of data-reduction techniques <ref> [4, 31, 33] </ref> to limit the number of working memory elements they test.
Reference: [5] <author> Davis, R. </author> <title> Meta-rules: Reasoning about control. </title> <booktitle> Artificial Intelligence 15 (1980), </booktitle> <pages> 179-222. </pages>
Reference-contexts: Instead, we provide a means of expressing preferences within a set-oriented semantics. The earliest work in this direction was reported over a decade ago in the form of controlled rule languages [7, 25] and meta-control by way of meta-rule formalisms <ref> [5] </ref>. These two early techniques are related in that they provide a means to express the control of execution of an underlying "non-deterministic" 17 formalism. Although parallelism and non-determinism are distinct concepts, they share some intellectual kinship. <p> DATALOG :fl follows a "bottom-up" evaluation strategy akin to forward-chaining production systems, while "top-down" evaluation is akin to backward-chaining in the style of PROLOG and MYCIN. In addition, a well-known idea is employed in PARULEL, that of using meta-rules. Meta-rules were first reported by Davis <ref> [5] </ref> in the context of the MYCIN/TEIREISIAS experiments. However, the adaptation of the meta-rule concept in PARULEL is specifically for the purpose of paralleliza-tion, in addition to specifying control by ordering rules and goals to pursue. <p> While each instance is formed, it is immediately matched against the meta-rules in parallel. This may partially remove the overhead expense of the synchronization points of other proposed parallel approaches. The redaction meta-rule paradigm of PARULEL is similar to MYCIN's <ref> [5] </ref> meta-rules. However, there are important distinctions between the two. <p> Thus, Davis' <ref> [5] </ref> original intent of providing meta-level control through a uniform rule framework can be extended to the case of parallelism. He developed techniques for rule acquisition and maintenance facilities for object-level rule programs that are also applicable to the meta-level. PARULEL programs can enjoy these benefits as well. <p> However, we argue that meta-rules improve "programmability" of rule languages. One can argue two cases. First, we posit that representing the control information directly and explicitly by means of meta-rules is simply easier to understand and code. The reader is referred to Davis' seminal work <ref> [5] </ref> in this area providing many examples and a mechanism to manage meta-rule construction and maintenance. Much of his thesis can be considered a running example and detailed argument supporting this point.
Reference: [6] <author> Forgy, C. L. </author> <title> On the Efficient Implementation of Production Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <year> 1979. </year>
Reference-contexts: This is evident in the ALEXSYS expert system discussed in Sec. 3.2. Lastly, even if the rule programmer is thinking of a parallel solution, rule 4 languages such as OPS5 <ref> [6] </ref> are not equipped to easily express parallelism. Parallelizing a rule-based program written in an inherently sequential language is certainly possible through a number of techniques. <p> Later, we will demonstrate that these techniques, although important, are not sufficient by themselves to maximize parallelism. 2.1 Parallel Match In one much-cited study conducted by Forgy, it is reported that more than 90% of the CPU time in OPS5 is spent in the match phase <ref> [6] </ref>. Much work has been done in an attempt to speed up the match phase by distributing workload among a number of concurrent processors. In one fine-grain approach, the match network is distributed among a set of processors and the conflict set is computed in parallel [8, 14, 26].
Reference: [7] <author> Georgeff, M. P. </author> <title> Procedural control in production systems. </title> <booktitle> Artificial Intelligence 18, </booktitle> <month> 2 (March </month> <year> 1982), </year> <pages> 175-201. </pages>
Reference-contexts: Instead, we provide a means of expressing preferences within a set-oriented semantics. The earliest work in this direction was reported over a decade ago in the form of controlled rule languages <ref> [7, 25] </ref> and meta-control by way of meta-rule formalisms [5]. These two early techniques are related in that they provide a means to express the control of execution of an underlying "non-deterministic" 17 formalism. Although parallelism and non-determinism are distinct concepts, they share some intellectual kinship.
Reference: [8] <author> Gupta, A. </author> <title> Parallelism in Production Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: Much work has been done in an attempt to speed up the match phase by distributing workload among a number of concurrent processors. In one fine-grain approach, the match network is distributed among a set of processors and the conflict set is computed in parallel <ref> [8, 14, 26] </ref>.
Reference: [9] <author> Ishida, T. </author> <title> Methods and effectiveness of parallel rule firing. </title> <booktitle> In Proceedings of the Sixth Conference on Artificial Intelligence Applications (1990), </booktitle> <pages> pp. 116-122. </pages>
Reference-contexts: Most work in this area relies on building data dependency graphs to detect interferences among rules. However, most researchers concentrate on compile-time analyses [11, 23], which are too pessimistic in some cases and hence lead to reduced parallelism [18]. Ishida <ref> [9] </ref> proposes an additional run-time analysis to detect interferences among instantiations, rather than among rules. Similarly, Schmolze [22] proposes serializability as an underlying conflict avoidance scheme among concurrent instantiations. <p> However, it is much more difficult to automatically determine that the need WMEs are also forcing a sequential process. Without the knowledge of the system characteristics not apparent in the rules, even run-time analyses <ref> [9] </ref> that check WME conflicts among instantiations are sometimes too pessimistic. For example, these analyses may specify that a pool WME cannot be used in two instantiations, yet it is perfectly legal to split a two-million pool into two good millions to allocate to two different contracts. <p> Under sequential execution semantics, many researchers have defined compile-time analysis systems, as outlined in Sec. 2.2, to determine which rules may be fired in parallel from the conflict set <ref> [9, 11] </ref>. Clearly, firing all instances may lead to inconsistent databases. Run-time synchronization points are then naturally defined to test those rule instances that may be fired. Such approaches suffer from two problems. First, synchronization and testing of possible rule conflicts is expensive in time and space, decreasing parallel performance. <p> It is interesting to note that meta-rules can be written to detect conflicts between multiple instances of the same rule, which is presently not possible in the current proposed rule-level synchronization schemes. Instance-level schemes <ref> [9, 23] </ref> do succeed at avoiding conflicts, but only from a "data-consistency" point of view. Meta-rules may identify other "semantic" conflicts. When all the meta-rules have been processed, the final conflict set of rule instances is taken to be conflict free, and is fired in parallel.
Reference: [10] <author> Ishida, T., and Stolfo, S. J. </author> <title> Simultaneous firing of production rules on tree-structured ma chines. </title> <type> Tech. Rep. </type> <institution> CUCS-109-84, Department of Computer Science, Columbia University, </institution> <year> 1984. </year>
Reference-contexts: This results in added savings by reducing the amount of overhead in synchronizing the processors to collectively select rules. One of the earliest approaches to multiple rule firing in parallel was reported in <ref> [10, 11] </ref> and later elaborated by Miranker [15], Moldovan [17], and Schmolze [23]. Most work in this area relies on building data dependency graphs to detect interferences among rules.
Reference: [11] <author> Ishida, T., and Stolfo, S. J. </author> <title> Towards the parallel execution of rules in production system programs. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing (1985), IEEE, </booktitle> <pages> pp. 568-575. </pages>
Reference-contexts: This results in added savings by reducing the amount of overhead in synchronizing the processors to collectively select rules. One of the earliest approaches to multiple rule firing in parallel was reported in <ref> [10, 11] </ref> and later elaborated by Miranker [15], Moldovan [17], and Schmolze [23]. Most work in this area relies on building data dependency graphs to detect interferences among rules. <p> Most work in this area relies on building data dependency graphs to detect interferences among rules. However, most researchers concentrate on compile-time analyses <ref> [11, 23] </ref>, which are too pessimistic in some cases and hence lead to reduced parallelism [18]. Ishida [9] proposes an additional run-time analysis to detect interferences among instantiations, rather than among rules. Similarly, Schmolze [22] proposes serializability as an underlying conflict avoidance scheme among concurrent instantiations. <p> Rather, OPS5's sequential semantics, and conflict resolution by recency, are maintained. 7 Besides checking for rule interferences, the allocation of right-hand sides (RHS's) among processors, called the decomposition problem <ref> [11] </ref>, affects the amount of parallelism. If two rules do not interfere with each other and are frequently fired simultaneously, they should be allocated to different processors. Ishida [11] proposes a heuristic approach while Cheng [3] proposes a probabilistic one. 2.3 Program Transformation We note that various rules in a single <p> by recency, are maintained. 7 Besides checking for rule interferences, the allocation of right-hand sides (RHS's) among processors, called the decomposition problem <ref> [11] </ref>, affects the amount of parallelism. If two rules do not interfere with each other and are frequently fired simultaneously, they should be allocated to different processors. Ishida [11] proposes a heuristic approach while Cheng [3] proposes a probabilistic one. 2.3 Program Transformation We note that various rules in a single rule-based program require different amounts of time to match than others. Some rules are "hot spots" requiring much more processing time than others. <p> Furthermore, rules can be rewritten to eliminate interferences and sequentialities detected by the analyses described in the previous section. In this manner, more rules can co-execute and more parallelism can be achieved <ref> [11, 15, 17] </ref>. 3 ALEXSYS A "real" AI expert system is being used in this study, an OPS5 program to solve the mortgage pool allocation problem. <p> Under sequential execution semantics, many researchers have defined compile-time analysis systems, as outlined in Sec. 2.2, to determine which rules may be fired in parallel from the conflict set <ref> [9, 11] </ref>. Clearly, firing all instances may lead to inconsistent databases. Run-time synchronization points are then naturally defined to test those rule instances that may be fired. Such approaches suffer from two problems. First, synchronization and testing of possible rule conflicts is expensive in time and space, decreasing parallel performance. <p> The subset of instances of the same rule found not to be in conflict can thus be fired in parallel. Multiple instances of the rules P 1 and P 2 in Sec. 5.1, for example, would be flagged by Ishida and Stolfo <ref> [11] </ref>, Moldovan [13] and others as not being parallelizable, because they rely on static or dynamic techniques 29 that are too pessimistic. The redaction meta-rule M 1 of Sec. 5.1, however, lets the programmer specify that non-conflicting instances of P 1 and P 2 can indeed be fired in parallel.
Reference: [12] <author> Kanellakis, P. </author> <title> Logic programming and parallel complexity. </title> <booktitle> In Proceedings of the International Conference on Database Theory (1986), </booktitle> <pages> pp. 1-30. </pages>
Reference-contexts: Many efforts in the database community have been devoted to theoretical characterization of the logic programs that belong to the NC complexity class <ref> [1, 12, 30] </ref>. If a program is in NC, it means that it can be interpreted very fast, given an essentially unbounded number of processors; the processors have to communicate extensively, usually through shared memory.
Reference: [13] <author> Kuo, S., Moldovan, D., and Cha, S. </author> <title> Control in production systems with multiple rule fir ings. </title> <type> Tech. Rep. </type> <institution> PKPL-90-10, Department of Electrical Engineering, University of Southern California, </institution> <year> 1990. </year>
Reference-contexts: The subset of instances of the same rule found not to be in conflict can thus be fired in parallel. Multiple instances of the rules P 1 and P 2 in Sec. 5.1, for example, would be flagged by Ishida and Stolfo [11], Moldovan <ref> [13] </ref> and others as not being parallelizable, because they rely on static or dynamic techniques 29 that are too pessimistic. The redaction meta-rule M 1 of Sec. 5.1, however, lets the programmer specify that non-conflicting instances of P 1 and P 2 can indeed be fired in parallel.
Reference: [14] <author> Miranker, D. P. </author> <title> TREAT: A New and Efficient Match Algorithm for AI Production Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> October </month> <year> 1986. </year>
Reference-contexts: The goal is simply to make rule-based systems fast and efficient, obviating the need for reimplementation of solutions in fast and efficient serial, imperative languages. Others have been investigating various means of speeding up rule-based systems in serial environments by improved matching algorithms and compilation techniques <ref> [14, 16] </ref>. The primary advantage of these approaches is to maintain applications in a high-level, declarative-style rule formalism, while enjoying the benefits of fast performance. There are a number of reasons why rule-based systems are slow performers. <p> Much work has been done in an attempt to speed up the match phase by distributing workload among a number of concurrent processors. In one fine-grain approach, the match network is distributed among a set of processors and the conflict set is computed in parallel <ref> [8, 14, 26] </ref>.
Reference: [15] <author> Miranker, D. P., Kuo, C., and Browne, J. </author> <title> Parallelizing transformations for a concurrent rule execution language. </title> <type> Tech. Rep. </type> <institution> TR-89-30, Department of Computer Science, University of Texas at Austin, </institution> <year> 1989. </year>
Reference-contexts: This results in added savings by reducing the amount of overhead in synchronizing the processors to collectively select rules. One of the earliest approaches to multiple rule firing in parallel was reported in [10, 11] and later elaborated by Miranker <ref> [15] </ref>, Moldovan [17], and Schmolze [23]. Most work in this area relies on building data dependency graphs to detect interferences among rules. However, most researchers concentrate on compile-time analyses [11, 23], which are too pessimistic in some cases and hence lead to reduced parallelism [18]. <p> Furthermore, rules can be rewritten to eliminate interferences and sequentialities detected by the analyses described in the previous section. In this manner, more rules can co-execute and more parallelism can be achieved <ref> [11, 15, 17] </ref>. 3 ALEXSYS A "real" AI expert system is being used in this study, an OPS5 program to solve the mortgage pool allocation problem. <p> Thus, we cannot express this form of parallelism in OPS5 without resorting to a complex simulation, detailed in Section 5.2. Furthermore, although the two-pool-combo and three-pool-combo rules are in conflict because of the goal WMEs, it is possible for some compile-time analyses <ref> [15] </ref> to determine automatically how data flow from one rule to another and reveal that the goal WMEs are forcing the program to execute rules serially. However, it is much more difficult to automatically determine that the need WMEs are also forcing a sequential process.
Reference: [16] <author> Miranker, D. P., Lofaso, B., Farmer, G., Chandra, A., and Brant, D. </author> <title> On a TREAT based production system compiler. </title> <booktitle> In Proceedings of the 10th International Conference on Expert Systems (1990), </booktitle> <pages> pp. 617-630. 40 </pages>
Reference-contexts: The goal is simply to make rule-based systems fast and efficient, obviating the need for reimplementation of solutions in fast and efficient serial, imperative languages. Others have been investigating various means of speeding up rule-based systems in serial environments by improved matching algorithms and compilation techniques <ref> [14, 16] </ref>. The primary advantage of these approaches is to maintain applications in a high-level, declarative-style rule formalism, while enjoying the benefits of fast performance. There are a number of reasons why rule-based systems are slow performers. <p> A coarser approach is to distribute the left-hand side (LHS) of rules among the processors and have each processor produce a local conflict set, which constitutes 6 part of the final conflict set [23, 26]. (This scheme was used to port an OPS5-to-C compiler <ref> [16] </ref> to the DADO2 parallel computer [27] and, in current work, to the more recent DADO4.) An even more coarse approach is proposed here whereby the working memory is partitioned into subsets (that are not necessarily disjoint), and these are distributed among a number of parallel processors, each with a copy
Reference: [17] <author> Moldovan, D. RUBIC: </author> <title> A multiprocessor for rule-based systems. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics 19, </journal> <volume> 4 (1989), </volume> <pages> 699-706. </pages>
Reference-contexts: This results in added savings by reducing the amount of overhead in synchronizing the processors to collectively select rules. One of the earliest approaches to multiple rule firing in parallel was reported in [10, 11] and later elaborated by Miranker [15], Moldovan <ref> [17] </ref>, and Schmolze [23]. Most work in this area relies on building data dependency graphs to detect interferences among rules. However, most researchers concentrate on compile-time analyses [11, 23], which are too pessimistic in some cases and hence lead to reduced parallelism [18]. <p> Furthermore, rules can be rewritten to eliminate interferences and sequentialities detected by the analyses described in the previous section. In this manner, more rules can co-execute and more parallelism can be achieved <ref> [11, 15, 17] </ref>. 3 ALEXSYS A "real" AI expert system is being used in this study, an OPS5 program to solve the mortgage pool allocation problem.
Reference: [18] <author> Oshisanwo, A. O., and Daiewicz, P. P. </author> <title> A parallel model and architecture for production systems. </title> <booktitle> In Proceedings of the IEEE International Conference on Parallel Processing (1987), </booktitle> <pages> pp. 147-153. </pages>
Reference-contexts: Most work in this area relies on building data dependency graphs to detect interferences among rules. However, most researchers concentrate on compile-time analyses [11, 23], which are too pessimistic in some cases and hence lead to reduced parallelism <ref> [18] </ref>. Ishida [9] proposes an additional run-time analysis to detect interferences among instantiations, rather than among rules. Similarly, Schmolze [22] proposes serializability as an underlying conflict avoidance scheme among concurrent instantiations.
Reference: [19] <author> Pasik, A. </author> <title> Improving production system performance on parallel architectures by creating con strained copies of culprit rules. </title> <type> Tech. Rep. </type> <institution> CUCS-313-87, Department of Computer Science, Columbia University, </institution> <year> 1987. </year>
Reference-contexts: Rules that match many working memory elements are costly to match and should be the subject of data-reduction techniques [4, 31, 33] to limit the number of working memory elements they test. An equivalent technique, copy-and-constrain, first reported in [24] and later elaborated in <ref> [19, 28] </ref>, creates multiple copies of the same rule, where each copy is constrained to match a subset of the working memory elements matched by the original rule. <p> Care must be taken to ensure that the entire set of copies computes the same conflict set of rule instances as the original hot spot rule. It has also been shown that this approach produces significant speedup <ref> [19, 20] </ref> and can reduce the time of an individual cycle of execution by speeding up the match time. Furthermore, rules can be rewritten to eliminate interferences and sequentialities detected by the analyses described in the previous section. <p> Prior work on "copy-and-constrain" demonstrates a mechanical means for accomplishing this goal, as elaborated in <ref> [19, 28] </ref>. This assumption depends upon a number of parameters including the number of RHS actions, the number of available processing elements (PEs), and inter-PE I/O bandwidths.
Reference: [20] <author> Pasik, A. </author> <title> A Methodology for Programming Production Systems and Its Implications on Par allelism. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Care must be taken to ensure that the entire set of copies computes the same conflict set of rule instances as the original hot spot rule. It has also been shown that this approach produces significant speedup <ref> [19, 20] </ref> and can reduce the time of an individual cycle of execution by speeding up the match time. Furthermore, rules can be rewritten to eliminate interferences and sequentialities detected by the analyses described in the previous section.
Reference: [21] <author> Pasik, A. J., Miranker, D. P., Stolfo, S. J., and Kresnicka, T. </author> <title> User-defined predicates in OPS5: a needed language extension for financial expert systems. </title> <type> Tech. Rep. </type> <institution> CUCS-496-89, Department of Computer Science, Columbia University, </institution> <year> 1989. </year>
Reference-contexts: This version of OPS5 had already been extended to provide user-defined predicates in the LHS of OPS5 rules <ref> [21] </ref>. This implementation encompasses concepts that go beyond the actual syntax of OPS5 and certainly its semantics. More specifically, we have modified the OPS5 interpreter to remove all vestiges of its conflict resolution strategies and its singular rule firing strategy.
Reference: [22] <author> Schmolze, J. G. </author> <title> Guaranteeing serializable results in synchronous parallel production systems. </title> <type> Tech. Rep. </type> <institution> TR-89-5, Department of Computer Science, Tufts University, </institution> <year> 1989. </year>
Reference-contexts: However, most researchers concentrate on compile-time analyses [11, 23], which are too pessimistic in some cases and hence lead to reduced parallelism [18]. Ishida [9] proposes an additional run-time analysis to detect interferences among instantiations, rather than among rules. Similarly, Schmolze <ref> [22] </ref> proposes serializability as an underlying conflict avoidance scheme among concurrent instantiations. However, these latter two efforts do not focus on providing what we may call application specific preferences, which we treat in later sections.
Reference: [23] <author> Schmolze, J. G., and Goel, S. </author> <title> A parallel asynchronous distributed production system. </title> <booktitle> In Proceedings of AAAI-90 (1990), </booktitle> <pages> pp. 65-71. </pages>
Reference-contexts: A coarser approach is to distribute the left-hand side (LHS) of rules among the processors and have each processor produce a local conflict set, which constitutes 6 part of the final conflict set <ref> [23, 26] </ref>. (This scheme was used to port an OPS5-to-C compiler [16] to the DADO2 parallel computer [27] and, in current work, to the more recent DADO4.) An even more coarse approach is proposed here whereby the working memory is partitioned into subsets (that are not necessarily disjoint), and these are <p> This results in added savings by reducing the amount of overhead in synchronizing the processors to collectively select rules. One of the earliest approaches to multiple rule firing in parallel was reported in [10, 11] and later elaborated by Miranker [15], Moldovan [17], and Schmolze <ref> [23] </ref>. Most work in this area relies on building data dependency graphs to detect interferences among rules. However, most researchers concentrate on compile-time analyses [11, 23], which are too pessimistic in some cases and hence lead to reduced parallelism [18]. <p> Most work in this area relies on building data dependency graphs to detect interferences among rules. However, most researchers concentrate on compile-time analyses <ref> [11, 23] </ref>, which are too pessimistic in some cases and hence lead to reduced parallelism [18]. Ishida [9] proposes an additional run-time analysis to detect interferences among instantiations, rather than among rules. Similarly, Schmolze [22] proposes serializability as an underlying conflict avoidance scheme among concurrent instantiations. <p> It is interesting to note that meta-rules can be written to detect conflicts between multiple instances of the same rule, which is presently not possible in the current proposed rule-level synchronization schemes. Instance-level schemes <ref> [9, 23] </ref> do succeed at avoiding conflicts, but only from a "data-consistency" point of view. Meta-rules may identify other "semantic" conflicts. When all the meta-rules have been processed, the final conflict set of rule instances is taken to be conflict free, and is fired in parallel.
Reference: [24] <author> Stolfo, S., Miranker, D., and Mills, R. </author> <title> A simple processing scheme to extract and load balance implicit parallelism in the concurrent match of production rules. </title> <booktitle> In Proc. of the AFIPS Symposium on Fifth Generation Computing (1985). </booktitle>
Reference-contexts: Rules that match many working memory elements are costly to match and should be the subject of data-reduction techniques [4, 31, 33] to limit the number of working memory elements they test. An equivalent technique, copy-and-constrain, first reported in <ref> [24] </ref> and later elaborated in [19, 28], creates multiple copies of the same rule, where each copy is constrained to match a subset of the working memory elements matched by the original rule.
Reference: [25] <author> Stolfo, S. J. </author> <title> Learning meta-rule control of production systems from execution traces. </title> <type> Tech. Rep. </type> <institution> CUCS-10-80, Department of Computer Science, Columbia University, </institution> <month> August </month> <year> 1980. </year> <month> 41 </month>
Reference-contexts: Instead, we provide a means of expressing preferences within a set-oriented semantics. The earliest work in this direction was reported over a decade ago in the form of controlled rule languages <ref> [7, 25] </ref> and meta-control by way of meta-rule formalisms [5]. These two early techniques are related in that they provide a means to express the control of execution of an underlying "non-deterministic" 17 formalism. Although parallelism and non-determinism are distinct concepts, they share some intellectual kinship.
Reference: [26] <author> Stolfo, S. J. </author> <title> Five parallel algorithms for production system execution on the DADO machine. </title> <booktitle> In Proceedings of the National Conference on Artifical Intelligence (1984), </booktitle> <publisher> AAAI. </publisher>
Reference-contexts: Much work has been done in an attempt to speed up the match phase by distributing workload among a number of concurrent processors. In one fine-grain approach, the match network is distributed among a set of processors and the conflict set is computed in parallel <ref> [8, 14, 26] </ref>. <p> A coarser approach is to distribute the left-hand side (LHS) of rules among the processors and have each processor produce a local conflict set, which constitutes 6 part of the final conflict set <ref> [23, 26] </ref>. (This scheme was used to port an OPS5-to-C compiler [16] to the DADO2 parallel computer [27] and, in current work, to the more recent DADO4.) An even more coarse approach is proposed here whereby the working memory is partitioned into subsets (that are not necessarily disjoint), and these are
Reference: [27] <author> Stolfo, S. J., and Miranker, D. </author> <title> The DADO production system machine. </title> <note> Journal of Parallel and Distributed Computing (August 1986). </note>
Reference-contexts: A coarser approach is to distribute the left-hand side (LHS) of rules among the processors and have each processor produce a local conflict set, which constitutes 6 part of the final conflict set [23, 26]. (This scheme was used to port an OPS5-to-C compiler [16] to the DADO2 parallel computer <ref> [27] </ref> and, in current work, to the more recent DADO4.) An even more coarse approach is proposed here whereby the working memory is partitioned into subsets (that are not necessarily disjoint), and these are distributed among a number of parallel processors, each with a copy of the complete rule set.
Reference: [28] <author> Stolfo, S. J., Miranker, D., and Mills, R. </author> <title> More rules may mean faster parallel execution. </title> <booktitle> In Proceedings Workshop on AI and Distributed Problem Solving (1985), </booktitle> <institution> Navy Research Laboratories, Office of Naval Research and National Academy of Sciences, National Academy Press. </institution>
Reference-contexts: Rules that match many working memory elements are costly to match and should be the subject of data-reduction techniques [4, 31, 33] to limit the number of working memory elements they test. An equivalent technique, copy-and-constrain, first reported in [24] and later elaborated in <ref> [19, 28] </ref>, creates multiple copies of the same rule, where each copy is constrained to match a subset of the working memory elements matched by the original rule. <p> Prior work on "copy-and-constrain" demonstrates a mechanical means for accomplishing this goal, as elaborated in <ref> [19, 28] </ref>. This assumption depends upon a number of parameters including the number of RHS actions, the number of available processing elements (PEs), and inter-PE I/O bandwidths.
Reference: [29] <author> Stolfo, S. J., Woodbury, L., Glazier, J., and Chan, P. </author> <title> The ALEXSYS mortgage pool allocation expert system: A case study of speeding up rule-based systems. </title> <booktitle> AI and Business Workshop, AAAI-90, </booktitle> <year> 1990. </year>
Reference-contexts: The result is a rule-based program that solves the problem better than human experts. (However, at present, the rule-based system is slower than desired for very large data sets and has been reimplemented in C++ to achieve acceptable performance.) The system, which we call ALEXSYS <ref> [29] </ref>, serves as our testbed for various studies on parallelism. The mortgage pool allocation problem is faced by financial companies that trade mortgage-backed securities. An individual security is a collection of homeowner mortgages, commonly referred to as a pool. <p> Second, it is a program moderate in size, which makes it easier to analyze. Third, after various optimization techniques have been applied to ALEXSYS, it is still undesirably slow 9 when large amounts of data are presented to the system <ref> [29] </ref>. Lastly, the mortgage pool allocation problem possesses characteristics that challenge some of the traditional parallelization techniques, which forced us to invent ways to tackle these newly-surfaced problems. The organization of ALEXSYS, its inherent sequentialities, and attempts to parallelize the system are discussed in the following subsections. <p> Finally, a set of reporting rules that report the detailed allocation of an individual contract (10 rules). The heuristics we gleaned from the Citicorp allocators can be summarized as follows (more details are provided in <ref> [29] </ref>). First, allocate good millions to contracts with the highest profitability. This strategy tends to allocate the best contracts first, thus failing on the worst contracts if there is insufficient inventory. <p> analysis scheme that allows rewriting ALEXSYS' inherently sequential OPS5 implementation to a more parallel one without a considerably (artificially) intelligent analysis system that is knowledgeable about the application domain. 3.3 Attempts to Parallelize ALEXSYS in OPS5 After a considerable effort to speed up the sequential program as much as possible <ref> [29] </ref>, a series of experiments were attempted to speed up ALEXSYS in a simulated parallel environment, using a number of strategies, while maintaining OPS5's sequential execution semantics. Our empirical results show that moderate speedup was realized or possible under different parallelization schemes.
Reference: [30] <author> Ullman, J., and Gelder, A. V. </author> <title> Parallel complexity of logic programs. </title> <journal> Algorithmica, </journal> <volume> 3 (1988), </volume> <pages> 5-42. </pages>
Reference-contexts: Thus, the proposed language concepts, expressing sequentialities explicitly, may provide a means of measuring the parallelism of an application differently from that offered theoretically by NC-complexity <ref> [30] </ref>, discussed in more detail in Sec. 7. Intuitively, the more sequentialities written, the more sequential the application. 5 Outline of the PARULEL Language In this section, we focus on the execution semantics and structure of PARULEL (PArallel RULE Language). <p> Many efforts in the database community have been devoted to theoretical characterization of the logic programs that belong to the NC complexity class <ref> [1, 12, 30] </ref>. If a program is in NC, it means that it can be interpreted very fast, given an essentially unbounded number of processors; the processors have to communicate extensively, usually through shared memory.
Reference: [31] <author> Wolfson, O. </author> <title> Parallel evaluation of datalog programs by load sharing. </title> <note> Journal of Logic Pro gramming (1989). (To appear). </note>
Reference-contexts: Some rules are "hot spots" requiring much more processing time than others. Rules that match many working memory elements are costly to match and should be the subject of data-reduction techniques <ref> [4, 31, 33] </ref> to limit the number of working memory elements they test. <p> The partition of the rule instantiations under copy-and-constrain is achieved by restricted versions of the original program, such that each restricted version is evaluated at one processor. In general, it is necessary for the processors to communicate intermediate results by message passing. It is shown in <ref> [33, 31] </ref> how the communication among the processors can be minimized. A synchronous and an asynchronous version of the paradigm are discussed in [32], and the applicability of an incremental rule evaluation algorithm to the asynchronous version of the copy-and-constrain paradigm was demonstrated.
Reference: [32] <author> Wolfson, O., Dewan, H., Stolfo, S., and Yemini, Y. </author> <title> Incremental evaluation of rules and its re lationship to parallelism. </title> <booktitle> In Proc. of the ACM-SIGMOD 1991, Intl. Conf. on the Management of Data (1991), </booktitle> <pages> pp. 78-87. </pages>
Reference-contexts: semantics of the rule language should be inherently parallel, rather than sequential; (b) programmers should be able to easily specify control of rule execution in a parallel environment; (c) an incremental update capability should be provided, which allows non-monotonic updates to be efficiently incorporated into the database while infer-encing proceeds <ref> [32] </ref>; and (d) a mapping protocol and load balancing scheme is needed, driven by the architecture of the system to effectively exploit whatever parallelism has been specified. 35 Traditional AI rule languages have not been designed with an eye towards large databases. <p> In general, it is necessary for the processors to communicate intermediate results by message passing. It is shown in [33, 31] how the communication among the processors can be minimized. A synchronous and an asynchronous version of the paradigm are discussed in <ref> [32] </ref>, and the applicability of an incremental rule evaluation algorithm to the asynchronous version of the copy-and-constrain paradigm was demonstrated. <p> In conjunction with the parallel implementation, the PARADISER environment will be developed to provide support for incremental rule evaluation via the maintenance algorithm that we have developed and previously reported <ref> [32] </ref>. As noted in the previous section, we also intend to incorporate the mapping algorithm for a distributed architecture using the copy-and-constrain or data-reduction principle into the PARADISER/PARULEL environment.
Reference: [33] <author> Wolfson, O., and Ozeri, A. </author> <title> A new paradigm for parallel and distributed rule-processing. </title> <booktitle> In Proc. of the ACM-SIGMOD 1990, Intl. Conf. on the Management of Data (1990), </booktitle> <pages> pp. 133-142. </pages>
Reference-contexts: Some rules are "hot spots" requiring much more processing time than others. Rules that match many working memory elements are costly to match and should be the subject of data-reduction techniques <ref> [4, 31, 33] </ref> to limit the number of working memory elements they test. <p> The partition of the rule instantiations under copy-and-constrain is achieved by restricted versions of the original program, such that each restricted version is evaluated at one processor. In general, it is necessary for the processors to communicate intermediate results by message passing. It is shown in <ref> [33, 31] </ref> how the communication among the processors can be minimized. A synchronous and an asynchronous version of the paradigm are discussed in [32], and the applicability of an incremental rule evaluation algorithm to the asynchronous version of the copy-and-constrain paradigm was demonstrated.

References-found: 33

