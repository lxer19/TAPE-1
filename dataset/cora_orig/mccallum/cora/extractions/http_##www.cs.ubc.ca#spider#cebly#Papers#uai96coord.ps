URL: http://www.cs.ubc.ca/spider/cebly/Papers/uai96coord.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: cebly@cs.ubc.ca  
Title: Learning Conventions in Multiagent Stochastic Domains using Likelihood Estimates  
Author: Craig Boutilier 
Address: Vancouver, BC V6T 1Z4  
Affiliation: Dept. of Computer Science University of British Columbia  
Abstract: Fully cooperative multiagent systemsthose in which agents share a joint utility modelis of special interest in AI. A key problem is that of ensuring that the actions of individual agents are coordinated, especially in settings where the agents are autonomous decision makers. We investigate approaches to learning coordinated strategies in stochastic domains where an agent's actions are not directly observable by others. Much recent work in game theory has adopted a Bayesian learning perspective to the more general problem of equilibrium selection, but tends to assume that actions can be observed. We discuss the special problems that arise when actions are not observable, including effects on rates of convergence, and the effect of action failure probabilities and asymmetries. We also use likelihood estimates as a means of generalizing fictitious play learning models in our setting. Finally, we propose the use of maximum likelihood as a means of removing strategies from consideration, with the aim of convergence to a conventional equilibrium, at which point learning and deliberation can cease.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert Axelrod. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1984. </year>
Reference: [2] <author> Craig Boutilier. </author> <title> Planning, learning and coordination in multiagent decision processes. </title> <booktitle> In Proceedings of the Sixth Conference on Theoretical Aspects of Rationality and Knowledge, </booktitle> <address> Amsterdam, </address> <year> 1996. </year> <note> (to appear). </note>
Reference-contexts: How- ever, some of these directions appear promising. In addition, the interaction of these methods in true sequential decision problems consisting of a wide variety of related state games is of considerable interest <ref> [2] </ref>. In this setting, we are ultimately interested in the generalization of learned conventions across similar state games, exploiting structured (Bayes net) representations of games and utility functions, as in [3].
Reference: [3] <author> Craig Boutilier, Richard Dearden, and Moises Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1104-1111, </pages> <address> Montreal, </address> <year> 1995. </year>
Reference-contexts: In this setting, we are ultimately interested in the generalization of learned conventions across similar state games, exploiting structured (Bayes net) representations of games and utility functions, as in <ref> [3] </ref>. Finally, generalizations of this model, especially those where only partial common knowledge of the game structure is assumed, will be required to make the effort more robust and realistic. This will require the use of ideas from reinforcement learning and learning models of dynamical systems.
Reference: [4] <author> Drew Fudenberg and David K. Levine. </author> <title> Steady state learning and nash equilibrium. </title> <journal> Econometrica, </journal> <volume> 61(3) </volume> <pages> 547-573, </pages> <year> 1993. </year>
Reference: [5] <author> John C. Harsanyi and Reinhard Selten. </author> <title> A General Theory of Equilibrium Selection in Games. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference: [6] <author> David Heckerman. </author> <title> A tutorial on learning bayesian net-works. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <address> Redmond, WA, </address> <year> 1995. </year>
Reference: [7] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1960. </year>
Reference: [8] <author> Ehud Kalai and Ehud Lehrer. </author> <title> Rational learning leads to nash equilibrium. </title> <journal> Econometrica, </journal> <volume> 61(5) </volume> <pages> 1019-1045, </pages> <year> 1993. </year>
Reference: [9] <author> David K. Lewis. </author> <title> Conventions, A Philosophical Study. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, </address> <year> 1969. </year>
Reference: [10] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 157-163, </pages> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference: [11] <author> George Mailath, Michihiro Kandori and Rafael Rob. </author> <title> Learning, mutation and long run equilibria in games. </title> <journal> Econometrica, </journal> <volume> 61(1) </volume> <pages> 29-56, </pages> <year> 1993. </year>
Reference: [12] <author> Roger B. Myerson. </author> <title> Game Theory: Analysis of Conflict. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, </address> <year> 1991. </year>
Reference: [13] <author> Guillermo Owen. </author> <title> Game Theory. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference: [14] <author> Martin L. Puterman. </author> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1994. </year>
Reference: [15] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> Emergent conven-tions in multi-agent systems: Initial experimental results and observations. </title> <booktitle> In Third Intl. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 225-231, </pages> <address> Cambridge, </address> <year> 1992. </year>
Reference: [16] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> On the synthesis of useful social laws for artificial agent societies. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 276-281, </pages> <address> San Jose, </address> <year> 1992. </year>
Reference: [17] <author> Gerhard Wei. </author> <title> Learning to coordinate actions in multi-agent systems. </title> <booktitle> In Proceedingsof the Thirteenth International Joint Conferenceon Artificial Intelligence, </booktitle> <pages> pages 311-316, </pages> <address> Chambery, FR, </address> <year> 1993. </year>
Reference: [18] <author> Holly Yanco and Lynn Andrea Stein. </author> <title> An adaptive commu-nication protocol for cooperating mobile robots. </title> <booktitle> Proc. of the Second International Conference on the Simulation of Adaptive Behavior, </booktitle> <pages> pages 478-485. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference: [19] <author> H. Peyton Young. </author> <title> The evolution of conventions. </title> <journal> Econometrica, </journal> <volume> 61(1) </volume> <pages> 57-84, </pages> <year> 1993. </year>
References-found: 19

