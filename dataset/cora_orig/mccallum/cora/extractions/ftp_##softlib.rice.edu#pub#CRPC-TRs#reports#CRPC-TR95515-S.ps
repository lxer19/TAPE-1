URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95515-S.ps
Refering-URL: http://www.cs.rice.edu/~ken/kennedy-vita.html
Root-URL: 
Email: ken@rice.edu sethi@rice.edu  
Title: A Constraint-based Communication Placement Framework  
Author: Ken Kennedy Ajay Sethi 
Address: 6100 S.Main MS 41, Houston, TX 77005  
Affiliation: Department of Computer Science, Rice University  
Note: Center for Research on Parallel Computation  
Abstract: Communication placement and optimization is an important step in the compilation of data-parallel languages. We present a framework that maximizes latency hiding by determining, in two separate phases, the earliest safe placement for sends and the latest balanced placement for the corresponding receives. The compositional structure of our technique allows machine-dependent resource constraints, which can effect the correctness of the placement and restrict the achievable communication and computation overlap, to influence the communication placement. We use constrained buffer size to illustrate constraint analysis and constraint-based communication placement. Finally, we indicate how communication optimizations, like message vectorization, partially redundant communication elimination, message coalescing, and vector message pipelining, can be incorporated into the framework.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Vienna Fortran | A Fortran language extension for distributed memory multiprocessors. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction High Performance Fortran [12], along with its predecessor data-parallel languages like Fortran D [3] and Vienna Fortran <ref> [1] </ref>, allow programmers to write sequential or shared-memory parallel programs annotated with data-decomposition directives for distributed-memory machines. Compilers for these languages are responsible for partitioning the computation and generating the communication necessary to fetch values of non-local data referenced by a processor.
Reference: [2] <author> J. Cocke and R. Miller. </author> <title> Some analysis techniques for optimizing computer programs. </title> <booktitle> In Proceedings of the 2nd Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 143-146, </pages> <year> 1969. </year>
Reference-contexts: Note that a node nested in multiple loops is a member of the Tarjan interval of the header of each enclosing loop. * G is reducible; that is, each loop has a unique header node. The classical node splitting transforma tion <ref> [2] </ref> can be used to obtain a reducible graph. * There are no critical edges, which connect a node with multiple successors to nodes with multiple predecessors.
Reference: [3] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C.-W. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: 1 Introduction High Performance Fortran [12], along with its predecessor data-parallel languages like Fortran D <ref> [3] </ref> and Vienna Fortran [1], allow programmers to write sequential or shared-memory parallel programs annotated with data-decomposition directives for distributed-memory machines. Compilers for these languages are responsible for partitioning the computation and generating the communication necessary to fetch values of non-local data referenced by a processor.
Reference: [4] <author> C. Gong, R. Gupta, and R. Melhem. </author> <title> Compilation techniques for optimizing communication on distributed memory systems. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: There have been several efforts <ref> [5, 4, 6, 7] </ref> to use data-flow analysis to determine communication placement for maximizing latency hiding. In addition, these data-flow frameworks reduce the number of messages by combining fl This work was supported in part by ARPA contract DABT63-92-C-0038 and NSF Cooperative Agreement Number CCR-9120008. <p> They eliminate reads of not-owned variables, in parallelized and vectorized codes, if these variables have already been read or 2 written locally. Gong, Gupta, and Melhem present a data-flow framework to separate sends and receives by placing sends at "the earliest point at which the communication can be performed" <ref> [4] </ref>. Moreover, they show how to incorporate various communication optimizations into the data-flow framework. However, their technique does not eliminate partially redundant communication and handles only singly-nested loops and one-dimensional arrays. <p> These operators, as well as the and operators described in Section 6, can be based on an appropriate array section representation <ref> [8, 4, 6] </ref>. Vector message pipelining optimization moves the Send of a communication set towards the definition of the data.
Reference: [5] <author> E. Granston and A. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: There have been several efforts <ref> [5, 4, 6, 7] </ref> to use data-flow analysis to determine communication placement for maximizing latency hiding. In addition, these data-flow frameworks reduce the number of messages by combining fl This work was supported in part by ARPA contract DABT63-92-C-0038 and NSF Cooperative Agreement Number CCR-9120008. <p> Several researchers have utilized this correspondence to address the problem of communication placement and optimization. Granston and Veidenbaum combine PRE and dependence analysis to eliminate redundant monolithic global-memory accesses across loop nests in the presence of conditionals <ref> [5] </ref>. They eliminate reads of not-owned variables, in parallelized and vectorized codes, if these variables have already been read or 2 written locally.
Reference: [6] <author> M. Gupta, E. Schonberg, and H. Srinivasan. </author> <title> A unified data-flow framework for optimizing communication. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: There have been several efforts <ref> [5, 4, 6, 7] </ref> to use data-flow analysis to determine communication placement for maximizing latency hiding. In addition, these data-flow frameworks reduce the number of messages by combining fl This work was supported in part by ARPA contract DABT63-92-C-0038 and NSF Cooperative Agreement Number CCR-9120008. <p> PRE and LCM avoid unnecessary recomputations by code motion and redundant expression elimination. Since determining the availability of non-local data due to communication is similar to the classical available expression analysis <ref> [6] </ref>, techniques for moving and placing code can be adapted to determine communication placement. Several researchers have utilized this correspondence to address the problem of communication placement and optimization. <p> Gupta, Schonberg, and Srinivasan use PRE and available section descriptors to develop a framework for optimizing communication <ref> [6] </ref>. Their framework obtains the earliest placement of sends and performs communication vectorization, moves communication earlier to hide latency, and eliminates redundant communication. <p> The correctness criteria, together with the optimization criteria imposed by the Give-N-Take framework [7], require the framework to perform message vectorization, communication volume reduction, partially redundant communication elimination, and the earliest placement of Sends for hiding latency. (Gupta, Schonberg, and Srinivasan <ref> [6] </ref> also perform these optimizations.) Besides these, our framework requires the latest placement of Recvs and the balanced placement of Sends and Recvs. 3.2 Interval Flow Graph Let G = (N; E) be the interval flow graph of a structured program, with nodes N and edges E. <p> We assume that the communication set Comm is determined in the initialization phase using dependence analysis and array kill information [10]; however, it can also be computed during the propagation phase with the help of an appropriate array section representation <ref> [8, 6] </ref>. Succs (n) and Preds (n) correspond to the set of successor and predecessor nodes of n. Succs F (n) and Succs E (n) then correspond to the forward and entry edge successors of n. <p> These operators, as well as the and operators described in Section 6, can be based on an appropriate array section representation <ref> [8, 4, 6] </ref>. Vector message pipelining optimization moves the Send of a communication set towards the definition of the data. <p> Previous communication placement frameworks <ref> [7, 6] </ref> cannot capture this optimization since it requires the movement of sends along the back edge.
Reference: [7] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Give-N-Take | A balanced code placement framework. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: There have been several efforts <ref> [5, 4, 6, 7] </ref> to use data-flow analysis to determine communication placement for maximizing latency hiding. In addition, these data-flow frameworks reduce the number of messages by combining fl This work was supported in part by ARPA contract DABT63-92-C-0038 and NSF Cooperative Agreement Number CCR-9120008. <p> The Give-N-Take code placement framework, developed by von Hanxleden and Kennedy, can be used to generate balanced sends and receives using a producer-consumer concept <ref> [7] </ref>. The Give-N-Take framework provides a production region, instead of a single location, for communication placement, which can be used for general latency hiding. <p> our approach, we first present some definitions in the next section. 3 Preliminaries In this section we describe the objectives of our framework, the graph structure, and the predicates used by the framework. 3 3.1 Correctness Criteria Our communication placement framework imposes the same correctness requirements as the Give-N-Take framework <ref> [7] </ref>: 1. Balance: A Send corresponds to exactly one Recv 1 . 2. Safety: Everything communicated is used; that is, there is no unnecessary communication. 3. Sufficiency: Each non-local reference is preceded by an appropriate communication. The correctness criteria, together with the optimization criteria imposed by the Give-N-Take framework [7], require <p> framework <ref> [7] </ref>: 1. Balance: A Send corresponds to exactly one Recv 1 . 2. Safety: Everything communicated is used; that is, there is no unnecessary communication. 3. Sufficiency: Each non-local reference is preceded by an appropriate communication. The correctness criteria, together with the optimization criteria imposed by the Give-N-Take framework [7], require the framework to perform message vectorization, communication volume reduction, partially redundant communication elimination, and the earliest placement of Sends for hiding latency. (Gupta, Schonberg, and Srinivasan [6] also perform these optimizations.) Besides these, our framework requires the latest placement of Recvs and the balanced placement of Sends and Recvs. <p> This can be achieved by adding a post body node to T (h) <ref> [7] </ref>. 1 Send and Recv are used to represent the communication primitives. 4 3.3 Definitions Let Comm be the set of data communicated among the processors. <p> Succs (n) and Preds (n) correspond to the set of successor and predecessor nodes of n. Succs F (n) and Succs E (n) then correspond to the forward and entry edge successors of n. Additionally, the edges induce the following traversal orders over G <ref> [7] </ref>: given a forward edge (m; n), a Forward order visits m before n, and a Backward order visits m after n. <p> On the other hand, hoisting communication out of a loop can only cause over-communication. Therefore, the importance of message vectorization in distributed-memory compilation [9] favors an optimistic handling of loops <ref> [7] </ref>. Under the relaxed safety constraint, we want to hoist communication out of a loop if it can be hoisted across 6 the first child of the loop header node. <p> Previous communication placement frameworks <ref> [7, 6] </ref> cannot capture this optimization since it requires the movement of sends along the back edge.
Reference: [8] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: We assume that the communication set Comm is determined in the initialization phase using dependence analysis and array kill information [10]; however, it can also be computed during the propagation phase with the help of an appropriate array section representation <ref> [8, 6] </ref>. Succs (n) and Preds (n) correspond to the set of successor and predecessor nodes of n. Succs F (n) and Succs E (n) then correspond to the forward and entry edge successors of n. <p> These operators, as well as the and operators described in Section 6, can be based on an appropriate array section representation <ref> [8, 4, 6] </ref>. Vector message pipelining optimization moves the Send of a communication set towards the definition of the data.
Reference: [9] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: On the other hand, hoisting communication out of a loop can only cause over-communication. Therefore, the importance of message vectorization in distributed-memory compilation <ref> [9] </ref> favors an optimistic handling of loops [7]. Under the relaxed safety constraint, we want to hoist communication out of a loop if it can be hoisted across 6 the first child of the loop header node. <p> For true dependences carried by the loop corresponding to the header node h, instead of placing 15 all the corresponding Sends at Succs E (h), vector message pipelining moves them towards the source of the dependence across the back edge and, therefore, across iterations to achieve better overlap <ref> [9] </ref>. Previous communication placement frameworks [7, 6] cannot capture this optimization since it requires the movement of sends along the back edge.
Reference: [10] <author> K. Kennedy and N. Nedeljkovic. </author> <title> Combining dependence and data-flow analyses to optimize communication. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: Recently, Kennedy and Nedeljkovic have proposed some techniques to incorporate dependence information into the framework in order to improve the precision of the framework for regular computations and to perform more extensive communication optimizations <ref> [10] </ref>. However, the complexity of the Give-N-Take framework makes the incorporation of constraints difficult. None of the above-mentioned frameworks take constraints into account. The framework presented in this paper incorporates constraints by allowing them to influence the placement of sends (and, therefore, the placement of receives). <p> Let the negation operator be: :&gt; = ?, :TRUE = ?, and :? = &gt;. We assume that the communication set Comm is determined in the initialization phase using dependence analysis and array kill information <ref> [10] </ref>; however, it can also be computed during the propagation phase with the help of an appropriate array section representation [8, 6]. Succs (n) and Preds (n) correspond to the set of successor and predecessor nodes of n. <p> Array portions can be compared at the initialization phase to avoid array section analysis during the propagation phase <ref> [10] </ref>; that is, it is possible to implement communication placement in the absence of constraints using bit vectors, and this was the approach adopted in this paper. On the other hand, the analysis can be performed by computing and propagating array sections to obtain a more precise communication placement.
Reference: [11] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Optimal code motion: </title> <journal> Theory and practice. ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(4) </volume> <pages> 1117-1155, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Another algorithm for eliminating partial redundancies, the Lazy Code Motion technique (LCM), decomposes the bi-directional structure of PRE into a sequence of uni-directional analyses to avoid unnecessary code motion. LCM places the computation of subexpressions "as early as necessary but as late as possible" <ref> [11] </ref>, and this laziness allows it to minimize register pressure. PRE and LCM avoid unnecessary recomputations by code motion and redundant expression elimination. <p> The classical node splitting transforma tion [2] can be used to obtain a reducible graph. * There are no critical edges, which connect a node with multiple successors to nodes with multiple predecessors. Critical edges can be eliminated by splitting edges as follows <ref> [11] </ref>: every edge leading to a node with more than one predecessor is split by inserting a synthetic node. * For every non-empty interval T (h), there exists an unique g 2 T (h) such that (g; h) 2 E; that is, there is only one back edge out of T <p> We now present the equations that provide the safe and sufficient placement for Send (d), for d 2 Comm. The solution of the following equation, which requires a Backward and Upward traversal of G, gives the set of safe (and sufficient) nodes for Send placement <ref> [11] </ref>: SAFE (n; d) = SAFE (n; d) " [Used (n; d) [ Transp (n; d) " " s2Succs F (n) SAFE (s; d)] where Used , Transp, and Succs F are as defined in Section 3.3. <p> The following equations give the set of nodes that satisfy the earliest property <ref> [11] </ref>: EARLIEST (n; d) = 8 &gt; &gt; : [ p2Preds F (n) [:Transp (p; d) [ :Safe (p; d)] otherwise For loop header node h, communication set d, and n = Succs E (h), n is earliest if Send (d) is not hoisted (b) Placement of Sends without taking constraints <p> Safe and Earliest predicates defines a location for the Send placement, since whenever Safe (n; d) holds there exists a node m, on every path from s to n, satisfying Safe (m; d) and Earliest (m; d) such that the data set d is not modified between m and n <ref> [11] </ref>. corresponding to the non-local reference at node 14 has been hoisted to the left branch of the if (node 3) to eliminate partial redundancy along the right branch. Send y [1:1000] can be initiated at node 2 itself because it is not modified in the program. <p> Also, we can determine the balanced receive placement even if a Send is placed at a node such that the data is not required along every terminating path. These requirements differentiate the equations presented here from that for the latest computation placement of the LCM technique <ref> [11] </ref>. 6 Taking Constraints into Account Let max buffer (1024 bytes) be the maximum size of the machine-dependent buffer that can be used by each processor for storing non-local data. We illustrate our approach by imposing the buffer size constraint on the communication placement.
Reference: [12] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year> <month> 17 </month>
Reference-contexts: 1 Introduction High Performance Fortran <ref> [12] </ref>, along with its predecessor data-parallel languages like Fortran D [3] and Vienna Fortran [1], allow programmers to write sequential or shared-memory parallel programs annotated with data-decomposition directives for distributed-memory machines.
Reference: [13] <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: into the framework and conclude in Section 8 by summarizing our contributions. 2 Related Work Partial Redundancy Elimination (PRE), a bi-directional algorithm for the suppression of partial redundancies, was proposed by Morel and Renvoise to improve the efficiency of a program by avoiding unnecessary recomputations of values at run time <ref> [13] </ref>. Another algorithm for eliminating partial redundancies, the Lazy Code Motion technique (LCM), decomposes the bi-directional structure of PRE into a sequence of uni-directional analyses to avoid unnecessary code motion.
Reference: [14] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceed ings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 62-73, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: In case the loop bounds are unknown, the loop bounds can be assumed to be either really small (and all the messages fits in the buffer) or really large (such that no message fits in the buffer) <ref> [14] </ref>. 6.1.2 Send Placement Each loop header node n with Loop (n) &gt; total buffer corresponds to a loop nest whose buffer requirements exceed the maximum allowed buffer and, thus, inhibits hoisting messages across it.
Reference: [15] <author> R. E. Tarjan. </author> <title> Testing flow graph reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 355-365, </pages> <year> 1974. </year> <month> 18 </month>
Reference-contexts: The interval flow graph, used both for the placement of communication primitives and propagation of constraints, is similar to the graph structure used by the Give-N-Take framework. For the sake of completeness, we summarize the important properties of G: * G is based on Tarjan intervals <ref> [15] </ref>, where a Tarjan interval T (h) is a set of control-flow nodes that correspond to a loop in the program text, entered through a unique header node h, where h 62 T (h).
References-found: 15

