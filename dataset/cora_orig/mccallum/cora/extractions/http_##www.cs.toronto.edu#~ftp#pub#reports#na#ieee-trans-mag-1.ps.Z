URL: http://www.cs.toronto.edu/~ftp/pub/reports/na/ieee-trans-mag-1.ps.Z
Refering-URL: http://www.cs.toronto.edu/NA/reports.html
Root-URL: 
Email: &lt;krj@na.toronto.edu&gt;  
Title: A Survey of Parallel Numerical Methods for Initial Value Problems for Ordinary Differential Equations  
Author: Kenneth R. Jackson 
Address: Toronto, Ontario, Canada M5S 1A4.  
Affiliation: Computer Science Department, University of Toronto,  
Note: c To appear in IEEE Transactions on Magnetics 1  
Abstract: The parallel solution of Initial Value Problems for Ordinary Differential Equations has become an active area of research during the past few years. We briefly survey the recent developments in this area, with particular emphasis on traditional forward-step methods that offer the potential for effective small-scale parallelism on currently existing machines. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Aslam and C. W. Gear, </author> <title> "Asynchronous integration of ordinary differential equations on multiprocessors", </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-89-1525, Comp. Sci. Dept., Univ. of Illinois, Urbana, IL, </institution> <year> 1988. </year>
Reference: [2] <author> L. Bales, O. Karakashian and S. Serbin, </author> <title> "On the A o - acceptability of rational approximations to the exponential function with only real poles", </title> <journal> BIT, </journal> <volume> vol. 28, </volume> <pages> pp. 70-79, </pages> <year> 1988. </year>
Reference-contexts: His technique transforms this system of sm coupled linear equations to s linear systems of m equations. If A can be diagonalized, then these s systems are independent and can therefore be solved simultaneously. Several authors <ref> [2, 27, 24, 25, 50] </ref> have studied these transformations for PaRK schemes, with particular emphasis on the restrictions that this technique places on the choice of coefficients for Fully-Implicit RK formulas. This work has yielded some promising theoretical and numerical results for the solution of stiff IVPs.
Reference: [3] <author> A. Bellen, </author> <title> "Parallelism across the steps for difference and differential equations", in Numerical Methods for Ordinary Differential Equations, </title> <editor> A. Bellen, C. W. Gear and E. Russo (eds.), </editor> <booktitle> Proceedings, L'Aquila, 1987, Lecture Notes in Mathematics #1386, </booktitle> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1989, </year> <pages> pp. 22-35. </pages>
Reference: [4] <author> A. Bellen, R. Vermiglio and M. Zennaro, </author> <title> "Parallel ODE-solvers with stepsize control", </title> <journal> J. Comput. Appl. Math., </journal> <volume> vol. 31, </volume> <pages> pp. 277-293, </pages> <year> 1990. </year>
Reference: [5] <author> L. G. Birta and O. Abou-Rabia, </author> <title> "Parallel block predictor-corrector methods for ode's", </title> <journal> IEEE Trans. Comput., </journal> <volume> vol. C-36, </volume> <pages> pp. 299-311, </pages> <year> 1987. </year>
Reference-contexts: More recently, several other authors <ref> [5, 7, 8, 11, 12, 32, 56, 57, 58, 23, 59] </ref> have considered similar explicit k-block r-value schemes of the form Y n = i=1 k X B i F ni ; (6) where Y n 2 R rm consists of r y-values and F n 2 R rm is f
Reference: [6] <author> A. Bose, I. Nelken and J. Gelfand, </author> <title> "A comparison of several methods of integrating stiff ordinary differential equations on parallel computing architectures", </title> <booktitle> in Proc. of the Third Hypercube Conf., </booktitle> <address> Pasadena, CA, </address> <pages> pp. 1712-1716, </pages> <year> 1988. </year>
Reference-contexts: Furthermore, we emphasize that many of these techniques are complementary: a method might, for example, compute several f evaluations simultaneously while assigning several processors to compute each f . Although exploiting parallelism in the f evaluations and linear and nonlinear algebra within an IVP method can be effective <ref> [6, 42, 61, 62] </ref>, we do not discuss this approach further, since the focus of this paper is on the IVP methods themselves. There are several impediments to parallelism, the seemingly natural forward propagation of information in IVPs being a prime example.
Reference: [7] <author> K. Burrage, </author> <title> "Solving nonstiff IVPs in a transputer environment", </title> <type> manuscript, </type> <institution> CMSR, Univ. of Liverpool, </institution> <note> c fl To appear in IEEE Transactions on Magnetics 5 Liverpool, England, </note> <year> 1989. </year>
Reference-contexts: See also the earlier reviews of Gear [15, 16] and Burrage <ref> [7] </ref>. Although the development of parallel algorithms in this area has lagged that in several other fields, such as linear algebra and partial differential equations, activity in parallel methods for IVPs has recently increased significantly. <p> More recently, several other authors <ref> [5, 7, 8, 11, 12, 32, 56, 57, 58, 23, 59] </ref> have considered similar explicit k-block r-value schemes of the form Y n = i=1 k X B i F ni ; (6) where Y n 2 R rm consists of r y-values and F n 2 R rm is f <p> If, on the other hand, the initial guess Y n depends on previously computed values other than y n , then the resulting PC formula looses the one-step property of the underlying RK formula. Although this adds to the complexity of the scheme, results in <ref> [7, 8, 28] </ref> suggest that this approach may yield very effective parallel methods.
Reference: [8] <author> K. Burrage, </author> <title> "The error behaviour of a general class of predictor-corrector methods", </title> <type> manuscript, </type> <institution> CMSR, University of Liverpool, </institution> <address> Liverpool, England, </address> <year> 1989. </year>
Reference-contexts: More recently, several other authors <ref> [5, 7, 8, 11, 12, 32, 56, 57, 58, 23, 59] </ref> have considered similar explicit k-block r-value schemes of the form Y n = i=1 k X B i F ni ; (6) where Y n 2 R rm consists of r y-values and F n 2 R rm is f <p> If, on the other hand, the initial guess Y n depends on previously computed values other than y n , then the resulting PC formula looses the one-step property of the underlying RK formula. Although this adds to the complexity of the scheme, results in <ref> [7, 8, 28] </ref> suggest that this approach may yield very effective parallel methods. <p> However, Tam's results [56, 57, 58] show that explicit parallel methods afford one more opportunity than sequential methods to obtain a formula that has both a large stability region and small truncation errors. Burrage's analysis <ref> [8] </ref> of truncation errors in PC GLMs contributes to our understanding of the derivation of such schemes. However, much more work in this area is needed.
Reference: [9] <author> J. C. Butcher, </author> <title> "On the implementation of implicit Runge-Kutta methods", </title> <journal> BIT, </journal> <volume> vol. 16, </volume> <pages> pp. 237-240, </pages> <year> 1976. </year>
Reference-contexts: This specific observation concerning the inherent parallelism in (11) and (12) generalizes to the application of iterative methods in many contexts. Butcher <ref> [9] </ref> proposed an effective scheme for solving the linear equations associated with (12). His technique transforms this system of sm coupled linear equations to s linear systems of m equations. If A can be diagonalized, then these s systems are independent and can therefore be solved simultaneously.
Reference: [10] <author> J. C. Butcher, </author> <title> "General linear methods: a survey", </title> <journal> Appl. Numer. Math., </journal> <volume> vol. 1, </volume> <pages> pp. 273-284, </pages> <year> 1985. </year>
Reference-contexts: Although this adds to the complexity of the scheme, results in [7, 8, 28] suggest that this approach may yield very effective parallel methods. Several authors have studied the potential for parallelism in General Linear Methods (GLMs). (GLMs <ref> [10] </ref> form a broad class of methods including all the schemes considered above.) Tam [56] and Skeel and Tam [55] analyze the stability regions of explicit GLMs, and show that in some sense parallelism cannot improve the stability of explicit methods.
Reference: [11] <author> M. T. Chu and H. Hamilton, </author> <title> "Parallel solution of ODEs by multi-block methods", </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> vol. 8, </volume> <pages> pp. 342-353, </pages> <year> 1987. </year>
Reference-contexts: More recently, several other authors <ref> [5, 7, 8, 11, 12, 32, 56, 57, 58, 23, 59] </ref> have considered similar explicit k-block r-value schemes of the form Y n = i=1 k X B i F ni ; (6) where Y n 2 R rm consists of r y-values and F n 2 R rm is f
Reference: [12] <author> M. A. Franklin, </author> <title> "Parallel solution of ordinary differential equations", </title> <journal> IEEE Trans. Comput., </journal> <volume> vol. C-27, </volume> <pages> pp. 413-420, </pages> <year> 1978. </year>
Reference-contexts: More recently, several other authors <ref> [5, 7, 8, 11, 12, 32, 56, 57, 58, 23, 59] </ref> have considered similar explicit k-block r-value schemes of the form Y n = i=1 k X B i F ni ; (6) where Y n 2 R rm consists of r y-values and F n 2 R rm is f
Reference: [13] <author> E. Gallopoulos and Y. Saad, </author> <title> "On the parallel solution of parabolic equations", </title> <booktitle> in Proc. 1989 ACM Int'l Conf. on Supercomputing, </booktitle> <address> Herakleion, Greece, </address> <pages> pp. 17-28, </pages> <year> 1989. </year>
Reference-contexts: The reader interested in large-scale parallelism for IVPs should see [1, 3, 4, 17, 18, 29, 30, 31, 33, 37, 38, 41, 43, 44, 46, 47, 48, 51, 54, 60] and the references therein. For the applications of these methods to partial differential equations, see <ref> [13, 14, 35, 36] </ref>. General Discussion The desire for parallel IVP solvers arises from the need to solve many important problems more rapidly than is currently possible.
Reference: [14] <author> E. Gallopoulos and Y. Saad, </author> <title> "Efficient solution of parabolic equations by polynomial approximation methods", </title> <type> Tech. Rep. 969, </type> <institution> CSRD, Univ. of Illinois, Urbana, IL, </institution> <year> 1990. </year>
Reference-contexts: The reader interested in large-scale parallelism for IVPs should see [1, 3, 4, 17, 18, 29, 30, 31, 33, 37, 38, 41, 43, 44, 46, 47, 48, 51, 54, 60] and the references therein. For the applications of these methods to partial differential equations, see <ref> [13, 14, 35, 36] </ref>. General Discussion The desire for parallel IVP solvers arises from the need to solve many important problems more rapidly than is currently possible.
Reference: [15] <author> C. W. Gear, </author> <title> "The potential for parallelism in ordinary differential equations", </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-86-1246, Comp. Sci. Dept., Univ. of Illinois, Urbana, IL, </institution> <year> 1986. </year>
Reference-contexts: See also the earlier reviews of Gear <ref> [15, 16] </ref> and Burrage [7]. Although the development of parallel algorithms in this area has lagged that in several other fields, such as linear algebra and partial differential equations, activity in parallel methods for IVPs has recently increased significantly. <p> Gear <ref> [15, 16] </ref> classifies the means of achieving paral lelism in IVP solvers into two main categories: * parallelism across the method or equivalently paral lelism across time, and * parallelism across the system or equivalently paral lelism across space, with the second name for each category being motivated by the parallel <p> narrowness of the computation lattice, an extreme case of which is a portion of the computation that must be performed on one processor, and * the need for synchronization, forcing the computation to halt on one processor while waiting for information from another, both of which are discussed by Gear <ref> [15] </ref> in the context of IVP solvers.
Reference: [16] <author> C. W. Gear, </author> <title> "The parallel methods for ordinary differential equations", </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-87-1369, Comp. Sci. Dept., Univ. of Illinois, Urbana, IL, </institution> <year> 1987. </year>
Reference-contexts: See also the earlier reviews of Gear <ref> [15, 16] </ref> and Burrage [7]. Although the development of parallel algorithms in this area has lagged that in several other fields, such as linear algebra and partial differential equations, activity in parallel methods for IVPs has recently increased significantly. <p> Gear <ref> [15, 16] </ref> classifies the means of achieving paral lelism in IVP solvers into two main categories: * parallelism across the method or equivalently paral lelism across time, and * parallelism across the system or equivalently paral lelism across space, with the second name for each category being motivated by the parallel
Reference: [17] <author> C. W. Gear, </author> <title> "Massive parallelism across the method in ODEs", </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-88-1442, Comp. Sci. Dept., Univ. of Illinois, Urbana, IL, </institution> <year> 1988. </year>
Reference-contexts: Included in the first class are algorithms that exploit several concurrent function evaluations within each step, as do the Block and Runge-Kutta methods discussed in the next section, as well as techniques that solve for many steps simultaneously, as do the fast parallel methods for linear recurrence relations <ref> [17, 52] </ref> and some dynamic iteration schemes. <p> There are several impediments to parallelism, the seemingly natural forward propagation of information in IVPs being a prime example. However, this is not always as severe an impediment as one might first think. Gear <ref> [17] </ref> notes that the problems y 0 = f (x), y (x 0 ) = y 0 , and 0 = f (x; y) | in some sense two extreme limiting cases of the IVP (1) | are "embarrassingly" parallel. <p> He also observes that fast methods for the solution of linear IVPs can be constructed from fast algorithms for the parallel solution of linear recurrence relations <ref> [17, 52] </ref>. Further study is needed to determine the character and extent of this impediment.
Reference: [18] <author> C. W. Gear and D. Wang, </author> <title> "Explicit stiff stability via splitting and the parallel solution of ODEs", </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-87-1328, Comp. Sci. Dept., Univ. of Illinois, Urbana, IL, </institution> <year> 1987. </year>
Reference: [19] <author> J. L. Gustavson, </author> <title> "Reevaluating Amdahl's law", </title> <journal> Comm. ACM, </journal> <volume> vol. 31, </volume> <pages> pp. 532-533, </pages> <year> 1988. </year>
Reference-contexts: In discussing Amdahl's law, Gustavson <ref> [19] </ref> notes that the narrowness of the computation graph is often not as serious an impediment to parallelism as it first seems, since, as the problem size grows, the time to execute the narrow sections frequently remains constant, hence requiring a decreasing portion of the total execution time.
Reference: [20] <author> P. J. van der Houwen and B. P. Sommeijer, </author> <title> "Variable step iteration of high-order Runge-Kutta methods on parallel computers", </title> <type> Tech. Rep. </type> <institution> NM-R8817, Dept. of Numer. Math., Centre for Math. and Comp. Sci., </institution> <address> Am-sterdam, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: Promising results for RK PC methods based on (11) and (13) are reported in <ref> [20, 21, 22, 28] </ref>. If, on the other hand, the initial guess Y n depends on previously computed values other than y n , then the resulting PC formula looses the one-step property of the underlying RK formula.
Reference: [21] <author> P. J. van der Houwen and B. P. Sommeijer, </author> <title> "Parallel iteration of high-order Runge-Kutta methods with stepsize control", </title> <journal> J. Comput. Appl. Math., </journal> <volume> vol. 29, </volume> <pages> pp. 111-127, </pages> <year> 1990. </year>
Reference-contexts: Thus, all Y n;i and the associated F n;i in the k th block can be computed simultaneously by solving independent systems of m equations each. Consequently, the ERK and DIRK variants, respectively, retain their characteristic explicit or diagonally-implicit property. Regrettably, these ERK schemes offer little potential for parallelism <ref> [21, 27] </ref>, as the order of a p-block ERK formula cannot exceed p and, if the order is p, the stability region is fz : j P p i=0 z i =i!j 1g, which is not large. <p> The construction of parallel ERK formulas and some minor advantages of these schemes are discussed in <ref> [21, 27] </ref>. However, a predictor-corrector variant discussed below seems more promising. On the other hand, parallel DIRK formulas offer some advantage. <p> Promising results for RK PC methods based on (11) and (13) are reported in <ref> [20, 21, 22, 28] </ref>. If, on the other hand, the initial guess Y n depends on previously computed values other than y n , then the resulting PC formula looses the one-step property of the underlying RK formula.
Reference: [22] <author> P. J. van der Houwen, B. P. Sommeijer and W. Couzy, </author> <title> "Embedded diagonally implicit Runge-Kutta algorithms on parallel computers", </title> <type> Tech. Rep. </type> <institution> NM-R8912, Dept. of Numer. Math., Centre for Math. and Comp. Sci., </institution> <address> Amsterdam, The Netherlands, </address> <year> 1989. </year>
Reference-contexts: Other formulas of this type are given in <ref> [22, 26, 27, 34] </ref>, where restrictions on the attainable order and stability for this class of formulas are considered. <p> This work has yielded some promising theoretical and numerical results for the solution of stiff IVPs. The inherent parallelism in both (11) and (12) can be exploited in Predictor-Corrector (PC) variants of RK formulas. In this context, we introduce an extension of (11) proposed in <ref> [22] </ref>: Y (k+1) n + h (DI m )F (k+1) (13) where D is a diagonal matrix. Since Y (k+1) n is an argument of F n , we must solve for Y (k+1) n in (13). <p> Promising results for RK PC methods based on (11) and (13) are reported in <ref> [20, 21, 22, 28] </ref>. If, on the other hand, the initial guess Y n depends on previously computed values other than y n , then the resulting PC formula looses the one-step property of the underlying RK formula.
Reference: [23] <author> P. J. van der Houwen, B. P. Sommeijer and P. A. van Mourik, </author> <title> "Note on explicit parallel multistep Runge-Kutta methods", </title> <journal> J. Comput. Appl. Math., </journal> <volume> vol. 27, </volume> <pages> pp. 411-420, </pages> <year> 1989. </year>
Reference-contexts: More recently, several other authors <ref> [5, 7, 8, 11, 12, 32, 56, 57, 58, 23, 59] </ref> have considered similar explicit k-block r-value schemes of the form Y n = i=1 k X B i F ni ; (6) where Y n 2 R rm consists of r y-values and F n 2 R rm is f
Reference: [24] <author> O. A. Karakashian and W. Rust, </author> <title> "On the parallel implementation of implicit Runge-Kutta methods", </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> vol. 9, </volume> <pages> pp. 1985-1090, </pages> <year> 1988. </year>
Reference-contexts: His technique transforms this system of sm coupled linear equations to s linear systems of m equations. If A can be diagonalized, then these s systems are independent and can therefore be solved simultaneously. Several authors <ref> [2, 27, 24, 25, 50] </ref> have studied these transformations for PaRK schemes, with particular emphasis on the restrictions that this technique places on the choice of coefficients for Fully-Implicit RK formulas. This work has yielded some promising theoretical and numerical results for the solution of stiff IVPs.
Reference: [25] <author> S. L. Keeling, </author> <title> "On implicit Runge-Kutta methods with a stability function having distinct real poles", </title> <journal> BIT, </journal> <volume> vol. 29, </volume> <pages> pp. 91-109, </pages> <year> 1989. </year>
Reference-contexts: His technique transforms this system of sm coupled linear equations to s linear systems of m equations. If A can be diagonalized, then these s systems are independent and can therefore be solved simultaneously. Several authors <ref> [2, 27, 24, 25, 50] </ref> have studied these transformations for PaRK schemes, with particular emphasis on the restrictions that this technique places on the choice of coefficients for Fully-Implicit RK formulas. This work has yielded some promising theoretical and numerical results for the solution of stiff IVPs.
Reference: [26] <author> A. Iserles and S. P. Ntrsett, </author> <title> "On the theory of parallel Runge-Kutta methods", </title> <note> to appear in IMA J. Numer. Anal., </note> <year> 1990. </year>
Reference-contexts: The construction of parallel ERK formulas and some minor advantages of these schemes are discussed in [21, 27]. However, a predictor-corrector variant discussed below seems more promising. On the other hand, parallel DIRK formulas offer some advantage. For example, Iserles and Ntrsett <ref> [26] </ref> derive a family of 4-stage, 4 th -order 2-parallel DIRK formulas, which includes the L-stable (but not B-stable) formula 1/2 1/2 0 0 0 1/2 -5/2 5/2 1/2 0 -1 3/2 -1 3/2 for which Y n;1 and Y n;2 can be computed simultaneously, after which Y n;3 and Y <p> Other formulas of this type are given in <ref> [22, 26, 27, 34] </ref>, where restrictions on the attainable order and stability for this class of formulas are considered.
Reference: [27] <author> K. R. Jackson and S. P. </author> <title> Ntrsett "The potential for parallelism in Runge-Kutta methods. Part 1: RK formulas in standard form", </title> <type> Tech. Rep. </type> <institution> No. 239/90 Comp. Sci. Dept., Univ. of Toronto, Toronto, Canada, </institution> <note> 1990; submitted to Math. Comp. </note>
Reference-contexts: Thus, all Y n;i and the associated F n;i in the k th block can be computed simultaneously by solving independent systems of m equations each. Consequently, the ERK and DIRK variants, respectively, retain their characteristic explicit or diagonally-implicit property. Regrettably, these ERK schemes offer little potential for parallelism <ref> [21, 27] </ref>, as the order of a p-block ERK formula cannot exceed p and, if the order is p, the stability region is fz : j P p i=0 z i =i!j 1g, which is not large. <p> The construction of parallel ERK formulas and some minor advantages of these schemes are discussed in <ref> [21, 27] </ref>. However, a predictor-corrector variant discussed below seems more promising. On the other hand, parallel DIRK formulas offer some advantage. <p> Other formulas of this type are given in <ref> [22, 26, 27, 34] </ref>, where restrictions on the attainable order and stability for this class of formulas are considered. <p> His technique transforms this system of sm coupled linear equations to s linear systems of m equations. If A can be diagonalized, then these s systems are independent and can therefore be solved simultaneously. Several authors <ref> [2, 27, 24, 25, 50] </ref> have studied these transformations for PaRK schemes, with particular emphasis on the restrictions that this technique places on the choice of coefficients for Fully-Implicit RK formulas. This work has yielded some promising theoretical and numerical results for the solution of stiff IVPs.
Reference: [28] <author> K. R. Jackson and S. P. </author> <title> Ntrsett "The potential for parallelism in Runge-Kutta methods. Part 2: RK predictor-corrector formulas", </title> <note> in preparation to be submitted to Math. Comp. </note>
Reference-contexts: Promising results for RK PC methods based on (11) and (13) are reported in <ref> [20, 21, 22, 28] </ref>. If, on the other hand, the initial guess Y n depends on previously computed values other than y n , then the resulting PC formula looses the one-step property of the underlying RK formula. <p> If, on the other hand, the initial guess Y n depends on previously computed values other than y n , then the resulting PC formula looses the one-step property of the underlying RK formula. Although this adds to the complexity of the scheme, results in <ref> [7, 8, 28] </ref> suggest that this approach may yield very effective parallel methods.
Reference: [29] <author> F.-L. Juang, </author> <title> "Accuracy increase in waveform relaxation", </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-88-1466, Comp. Sci. Dept., Univ. of Illinois at Urbana-Champaign, </institution> <address> Ur-bana, IL, 61801, </address> <year> 1988. </year>
Reference: [30] <author> F.-L. Juang, </author> <title> "Waveform methods for ordinary differential equations", </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-90-1563, Comp. Sci. Dept., Univ. of Illinois, Urbana, IL, </institution> <year> 1990. </year>
Reference: [31] <author> F.-L. Juang and C. W. Gear, </author> <title> "Accuracy increase in waveform Gauss Seidel", </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-89-1518, Comp. Sci. Dept., Univ. of Illinois, Urbana, IL, </institution> <year> 1988. </year>
Reference: [32] <author> I. N. Katz, M. A. Franklin and A. Sen, </author> <title> "Optimally stable parallel predictors for Adams-Moulton correctors", </title> <journal> Comp. and Maths. with Appls., </journal> <volume> vol. 3, </volume> <pages> pp. 217-233, </pages> <year> 1977. </year>
Reference-contexts: More recently, several other authors <ref> [5, 7, 8, 11, 12, 32, 56, 57, 58, 23, 59] </ref> have considered similar explicit k-block r-value schemes of the form Y n = i=1 k X B i F ni ; (6) where Y n 2 R rm consists of r y-values and F n 2 R rm is f
Reference: [33] <author> E. Lelarasmee, A. Ruehli and A. L. Sangiovanni-Vincentelli, </author> <title> "The waveform relaxation method for time-domain analysis of large scale integrated circuits", </title> <journal> IEEE Trans. Computer-Aided Design, </journal> <volume> vol. CAD-1, </volume> <pages> pp. 131-145, </pages> <year> 1982. </year>
Reference: [34] <author> I. </author> <title> Lie, "Some aspects of parallel Runge-Kutta methods", Math. and Comp. </title> <type> Rep. 3/87, Numer. </type> <institution> Math. Dept., Norwegian Inst. of Tech., Trondheim, Norway, </institution> <year> 1987. </year>
Reference-contexts: Other formulas of this type are given in <ref> [22, 26, 27, 34] </ref>, where restrictions on the attainable order and stability for this class of formulas are considered.
Reference: [35] <author> I. Lie and R. Sk-alin, </author> <title> "Relaxation-based integration by Runge-Kutta methods and its application to moving finite element methods", </title> <type> manuscript, </type> <institution> NDRE, Electronics Div., </institution> <address> P. O. Box 25, N-2007, Kjeller, Norway, </address> <year> 1989. </year>
Reference-contexts: The reader interested in large-scale parallelism for IVPs should see [1, 3, 4, 17, 18, 29, 30, 31, 33, 37, 38, 41, 43, 44, 46, 47, 48, 51, 54, 60] and the references therein. For the applications of these methods to partial differential equations, see <ref> [13, 14, 35, 36] </ref>. General Discussion The desire for parallel IVP solvers arises from the need to solve many important problems more rapidly than is currently possible.
Reference: [36] <author> Ch. Lubich and A. Ostermann, </author> <title> "Multi-grid dynamic iteration for parabolic equations", </title> <journal> BIT, </journal> <volume> vol. 27, </volume> <pages> pp. 216-234, </pages> <year> 1987. </year> <note> c fl To appear in IEEE Transactions on Magnetics 6 </note>
Reference-contexts: The reader interested in large-scale parallelism for IVPs should see [1, 3, 4, 17, 18, 29, 30, 31, 33, 37, 38, 41, 43, 44, 46, 47, 48, 51, 54, 60] and the references therein. For the applications of these methods to partial differential equations, see <ref> [13, 14, 35, 36] </ref>. General Discussion The desire for parallel IVP solvers arises from the need to solve many important problems more rapidly than is currently possible.
Reference: [37] <author> U. Miekkala and O. Nevanlinna, </author> <title> "Convergence of dynamic iteration methods for initial value problems", </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> vol. 8, </volume> <pages> pp. 459-482, </pages> <year> 1987. </year>
Reference: [38] <author> U. Miekkala and O. Nevanlinna, </author> <title> "Sets of convergence and stability regions", </title> <journal> BIT, </journal> <volume> vol. 27, </volume> <pages> pp. 554-584, </pages> <year> 1987. </year>
Reference: [39] <author> W. L. Miranker and W. Liniger, </author> <title> "Parallel methods for the numerical integration of ordinary differential equations", </title> <journal> Math. Comp., </journal> <volume> vol. 21, </volume> <pages> pp. 303-320, </pages> <year> 1967. </year>
Reference-contexts: The dynamic iteration methods, not reviewed in this paper, can effectively use many more processors with slower inter-processor communication speeds, characteristic of message-passing machines. Small-Scale Parallelism in Traditional IVP Methods Two of the earliest papers on parallel methods for IVPs were by Miranker and Liniger <ref> [39] </ref> and Miranker [40]. <p> n and F n = f (x n+1 ; y n+1 ) n ) ; we can rewrite the predictor-corrector pair (4)-(3) as an explicit one-step block method : Y n = AY n1 + hBF n1 (5) where A = 0 I 1 2 I : Miranker and Liniger <ref> [39] </ref> go on to develop a theory for more general multistep block methods of this type and propose several schemes suitable for machines with a few processors (e.g., 2 to 4). <p> Simonsen [53] exploits the natural parallelism of this type inherent in extrapolation schemes to construct an effective parallel method. The paradigm can also be applied to Runge-Kutta (RK) methods. Miranker and Lin-iger <ref> [39] </ref> derived a class of parallel RK (PaRK) methods, but their schemes are ineffective because of poor stability. We briefly review below other classes of PaRK methods, both as instructive examples of variants of the general paradigm noted above and also because PaRK schemes are a promising class of methods.
Reference: [40] <author> W. L. Miranker, </author> <title> "A survey of parallelism in numerical analysis", </title> <journal> SIAM Review, </journal> <volume> vol. 13, </volume> <pages> pp. 524-547, </pages> <year> 1971. </year>
Reference-contexts: The dynamic iteration methods, not reviewed in this paper, can effectively use many more processors with slower inter-processor communication speeds, characteristic of message-passing machines. Small-Scale Parallelism in Traditional IVP Methods Two of the earliest papers on parallel methods for IVPs were by Miranker and Liniger [39] and Miranker <ref> [40] </ref>.
Reference: [41] <author> D. Mitra, </author> <title> "Asynchronous relaxation for the numerical solution of differential equations by parallel computer", </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> vol. 8, </volume> <pages> pp. </pages> <address> s43-s56, </address> <year> 1987. </year>
Reference: [42] <author> I. Nelken, T. S. Ho, H. Rabitz and J. Gelfand, </author> <title> "Parallel methods in sensitivity analysis", </title> <booktitle> in Proc. of the Fourth Hypercube Conf., </booktitle> <address> Monterey, CA, </address> <year> 1989. </year>
Reference-contexts: Furthermore, we emphasize that many of these techniques are complementary: a method might, for example, compute several f evaluations simultaneously while assigning several processors to compute each f . Although exploiting parallelism in the f evaluations and linear and nonlinear algebra within an IVP method can be effective <ref> [6, 42, 61, 62] </ref>, we do not discuss this approach further, since the focus of this paper is on the IVP methods themselves. There are several impediments to parallelism, the seemingly natural forward propagation of information in IVPs being a prime example.
Reference: [43] <author> O. Nevanlinna, </author> <title> "Remarks on Picard-Lindelof iteration: Part I", </title> <journal> BIT, </journal> <volume> vol. 29, </volume> <pages> pp. 328-346, </pages> <year> 1989. </year>
Reference: [44] <author> O. Nevanlinna, </author> <title> "Remarks on Picard-Lindelof iteration: Part II", </title> <journal> BIT, </journal> <volume> vol. 29, </volume> <pages> pp. 535-562, </pages> <year> 1989. </year>
Reference: [45] <author> O. Nevanlinna, </author> <title> "A Note on Picard-Lindelof iteration", in Numerical Methods for Ordinary Differential Equations, </title> <editor> A. Bellen, C. W. Gear and E. Russo (eds.), </editor> <booktitle> Proceedings, L'Aquila, 1987, Lecture Notes in Mathematics #1386, </booktitle> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1989, </year> <pages> pp. 97-102. </pages>
Reference: [46] <author> O. Nevanlinna and F. Odeh, </author> <title> "Remarks on the convergence of waveform relaxation method", </title> <journal> Numer. Funct. Anal. and Optimiz., </journal> <volume> vol. 9, </volume> <pages> pp. 435-445, </pages> <year> 1987. </year>
Reference: [47] <author> A. R. Newton and A. L. Sangiovanni-Vincentelli, </author> <title> "Relaxation-based electrical simulation", </title> <journal> IEEE Trans. on Computer-Aided Design, </journal> <volume> vol. CAD-3, </volume> <pages> pp. 308-330, </pages> <year> 1984. </year>
Reference: [48] <author> J. Nievergelt, </author> <title> "Parallel methods for integrating ordinary differential equations", </title> <journal> Comm. ACM, </journal> <volume> vol. 7, </volume> <pages> pp. 731-733, </pages> <year> 1964. </year>
Reference: [49] <author> S. P. Ntrsett and H. H. Simonsen, </author> <title> "Aspects of parallel Runge-Kutta methods", in Numerical Methods for Ordinary Differential Equations, </title> <editor> A. Bellen, C. W. Gear and E. Russo (eds.), </editor> <booktitle> Proceedings, L'Aquila, 1987, Lecture Notes in Mathematics #1386, </booktitle> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1989, </year> <pages> pp. 103-117. </pages>
Reference: [50] <author> B. </author> <title> Orel, Real pole approximations to the exponential function, </title> <type> Tech. Rep. 1/90, </type> <institution> Math. Sci. Div., Norwegian Inst. of Tech., Trondheim, Norway, </institution> <note> 1989; to appear in BIT. </note>
Reference-contexts: His technique transforms this system of sm coupled linear equations to s linear systems of m equations. If A can be diagonalized, then these s systems are independent and can therefore be solved simultaneously. Several authors <ref> [2, 27, 24, 25, 50] </ref> have studied these transformations for PaRK schemes, with particular emphasis on the restrictions that this technique places on the choice of coefficients for Fully-Implicit RK formulas. This work has yielded some promising theoretical and numerical results for the solution of stiff IVPs.
Reference: [51] <author> J. Sand and S. Skelboe, </author> <title> "Stability of backward Euler multirate methods and the convergence of waveform relaxation", </title> <note> submitted to BIT, </note> <year> 1990. </year>
Reference: [52] <editor> U. Schendel, </editor> <title> Introduction to Numerical Methods for Parallel Computers, </title> <address> New York: </address> <publisher> Ellis Horwood, </publisher> <year> 1984. </year>
Reference-contexts: Included in the first class are algorithms that exploit several concurrent function evaluations within each step, as do the Block and Runge-Kutta methods discussed in the next section, as well as techniques that solve for many steps simultaneously, as do the fast parallel methods for linear recurrence relations <ref> [17, 52] </ref> and some dynamic iteration schemes. <p> He also observes that fast methods for the solution of linear IVPs can be constructed from fast algorithms for the parallel solution of linear recurrence relations <ref> [17, 52] </ref>. Further study is needed to determine the character and extent of this impediment.
Reference: [53] <author> H. H. Simonsen, </author> <title> "Extrapolation methods for ODE's: continuous approximations, a parallel approach", </title> <type> Ph. D. thesis, </type> <institution> Math. Sci. Div., Norwegian Inst. of Tech., Trondheim, Norway, </institution> <year> 1990. </year>
Reference-contexts: The challenge to computational scientists is to exploit this potential to solve problems more efficiently and/or reliably than they could in the past, or to solve problems that were previously intractable. As discussed more fully in <ref> [53] </ref>, the IVP (1) can be time consuming to solve if * f is expensive to evaluate, as might be the case if each f evaluation requires the solution of an auxiliary problem, * the number of equations, m, in the system is large, a property characteristic of spatially-discretized PDEs or <p> The example above suggests a simple general paradigm for achieving parallelism across time in traditional forward-step methods: group the stages of a method into blocks for which all function evaluations associated with each block can be performed simultaneously. Simonsen <ref> [53] </ref> exploits the natural parallelism of this type inherent in extrapolation schemes to construct an effective parallel method. The paradigm can also be applied to Runge-Kutta (RK) methods. Miranker and Lin-iger [39] derived a class of parallel RK (PaRK) methods, but their schemes are ineffective because of poor stability.
Reference: [54] <author> R. D. Skeel, </author> <title> "Waveform iteration and the shifted Pi-card splitting", </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> vol. 10, </volume> <pages> pp. 756-776, </pages> <year> 1989. </year>
Reference: [55] <author> R. D. Skeel and H. W. Tam, </author> <title> "Potential for parallelism in explicit linear methods", </title> <type> manuscript, </type> <institution> Comp. Sci. Dept., Univ. of Illinois, Urbana, IL, </institution> <year> 1989. </year>
Reference-contexts: Several authors have studied the potential for parallelism in General Linear Methods (GLMs). (GLMs [10] form a broad class of methods including all the schemes considered above.) Tam [56] and Skeel and Tam <ref> [55] </ref> analyze the stability regions of explicit GLMs, and show that in some sense parallelism cannot improve the stability of explicit methods.
Reference: [56] <author> H. W. Tam, </author> <title> "Parallel methods for the numerical solution of ordinary differential equations", </title> <type> Ph. D. thesis, Tech. Rep. </type> <institution> UIUCDCS-R-89-1516, Comp. Sci. Dept., Univ. of Illinois, Urbana, IL, </institution> <year> 1989. </year>
Reference-contexts: More recently, several other authors <ref> [5, 7, 8, 11, 12, 32, 56, 57, 58, 23, 59] </ref> have considered similar explicit k-block r-value schemes of the form Y n = i=1 k X B i F ni ; (6) where Y n 2 R rm consists of r y-values and F n 2 R rm is f <p> Several authors have studied the potential for parallelism in General Linear Methods (GLMs). (GLMs [10] form a broad class of methods including all the schemes considered above.) Tam <ref> [56] </ref> and Skeel and Tam [55] analyze the stability regions of explicit GLMs, and show that in some sense parallelism cannot improve the stability of explicit methods. <p> However, Tam's results <ref> [56, 57, 58] </ref> show that explicit parallel methods afford one more opportunity than sequential methods to obtain a formula that has both a large stability region and small truncation errors. Burrage's analysis [8] of truncation errors in PC GLMs contributes to our understanding of the derivation of such schemes.
Reference: [57] <author> H. W. Tam, </author> <title> "One-stage parallel methods for the numerical solution of ordinary differential equations", </title> <note> submitted to SIAM J. </note> <institution> Sci. Stat. Comput., </institution> <year> 1989. </year>
Reference-contexts: More recently, several other authors <ref> [5, 7, 8, 11, 12, 32, 56, 57, 58, 23, 59] </ref> have considered similar explicit k-block r-value schemes of the form Y n = i=1 k X B i F ni ; (6) where Y n 2 R rm consists of r y-values and F n 2 R rm is f <p> However, Tam's results <ref> [56, 57, 58] </ref> show that explicit parallel methods afford one more opportunity than sequential methods to obtain a formula that has both a large stability region and small truncation errors. Burrage's analysis [8] of truncation errors in PC GLMs contributes to our understanding of the derivation of such schemes.
Reference: [58] <author> H. W. Tam, </author> <title> "Two-stage parallel methods for the numerical solution of ordinary differential equations", </title> <note> submitted to SIAM J. </note> <institution> Sci. Stat. Comput., </institution> <year> 1989. </year>
Reference-contexts: More recently, several other authors <ref> [5, 7, 8, 11, 12, 32, 56, 57, 58, 23, 59] </ref> have considered similar explicit k-block r-value schemes of the form Y n = i=1 k X B i F ni ; (6) where Y n 2 R rm consists of r y-values and F n 2 R rm is f <p> However, Tam's results <ref> [56, 57, 58] </ref> show that explicit parallel methods afford one more opportunity than sequential methods to obtain a formula that has both a large stability region and small truncation errors. Burrage's analysis [8] of truncation errors in PC GLMs contributes to our understanding of the derivation of such schemes.
Reference: [59] <author> P. B. Worland, </author> <title> "Parallel methods for the numerical solution of ordinary differential equations", </title> <journal> IEEE Trans. Comput., </journal> <volume> vol. C-25, </volume> <pages> pp. 1045-1048, </pages> <year> 1976. </year>
Reference-contexts: More recently, several other authors <ref> [5, 7, 8, 11, 12, 32, 56, 57, 58, 23, 59] </ref> have considered similar explicit k-block r-value schemes of the form Y n = i=1 k X B i F ni ; (6) where Y n 2 R rm consists of r y-values and F n 2 R rm is f
Reference: [60] <author> Xu Xuhai and C. W. Gear, </author> <title> "Potential performance of methods for parallelism across time in ODEs", </title> <type> Tech. Rep. </type> <institution> UIUCDCS-R-90-1587, Comp. Sci. Dept., Univ. of Illinois, Urbana, IL, </institution> <year> 1990. </year>
Reference: [61] <author> G-C. Yang, </author> <title> "Paraspice: a parallel circuit simulator for shared-memory multiprocessors", </title> <booktitle> in 27th ACM/IEEE Design Automation Conf., </booktitle> <pages> pp. 400-405, </pages> <year> 1990. </year>
Reference-contexts: Furthermore, we emphasize that many of these techniques are complementary: a method might, for example, compute several f evaluations simultaneously while assigning several processors to compute each f . Although exploiting parallelism in the f evaluations and linear and nonlinear algebra within an IVP method can be effective <ref> [6, 42, 61, 62] </ref>, we do not discuss this approach further, since the focus of this paper is on the IVP methods themselves. There are several impediments to parallelism, the seemingly natural forward propagation of information in IVPs being a prime example.
Reference: [62] <author> G-C. Yang, </author> <title> "Paraspice: a parallel direct circuit simulator for shared-memory multiprocessors", </title> <type> Ph. D. thesis, </type> <institution> Comp. Sci. Dept., Univ. of Illinois, Urbana, IL, </institution> <year> 1990. </year>
Reference-contexts: Furthermore, we emphasize that many of these techniques are complementary: a method might, for example, compute several f evaluations simultaneously while assigning several processors to compute each f . Although exploiting parallelism in the f evaluations and linear and nonlinear algebra within an IVP method can be effective <ref> [6, 42, 61, 62] </ref>, we do not discuss this approach further, since the focus of this paper is on the IVP methods themselves. There are several impediments to parallelism, the seemingly natural forward propagation of information in IVPs being a prime example.
References-found: 62

