URL: http://www.ai.mit.edu/projects/darpa/vsam/vsam-cvpr98.ps
Refering-URL: http://www.ai.mit.edu/projects/darpa/vsam/
Root-URL: 
Email: lleeg@ai.mit.edu  
Title: Using adaptive tracking to classify and monitor activities in a site  
Author: W.E.L. Grimson C. Stauffer R. Romano L. Lee fwelg, stauffer, romano, 
Affiliation: Artificial Intelligence Laboratory Massachusetts Institute of Technology  
Abstract: We describe a vision system that monitors activity in a site over extended periods of time. The system uses a distributed set of sensors to cover the site, and an adaptive tracker detects multiple moving objects in the sensors. Our hypothesis is that motion tracking is sufficient to support a range of computations about site activities. We demonstrate using the tracked motion data: to calibrate the distributed sensors, to construct rough site models, to classify detected objects, to learn common patterns of activity for different object classes, and to detect unusual activities. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Faugeras, </author> <title> O.D., Three-Dimensional Computer Vision: A Geometric Viewpoint, </title> <publisher> The MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: In such scenes, moving objects are typically cars or people moving on the ground plane, and therefore the objects' motion is often planar. Corresponding image points of tracked objects in a camera pair are then related by a projective linear transformation or homography <ref> [1] </ref>. We have tested the use of dynamic point correspondences for estimating the homographies between images of the scene's ground plane in a laboratory setting in which three cameras view a scene containing a single moving object.
Reference: [2] <author> Friedman, N., and S. Russell, </author> <title> Image segmentation in video sequences: A probabilistic approach Proc. </title> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <year> 1997. </year>
Reference-contexts: Simple implementations of background-ing just subtract consecutive images and threshold the resulting difference image to determine pixels that may correspond to motion. More robust methods use time averages of images <ref> [2] </ref>, adaptive Gaussian estimation [12], or Kalman filtering [7] to derive the background image to be subtracted. While such methods often run in real time, they are generally not robust. <p> It includes linear prediction in x, y and size parameters. An example of one frame of the tracker is shown in remove shadows, and a method such as that used in <ref> [2] </ref> could be included to handle this case.
Reference: [3] <author> Johnson, N., and, D. C., </author> <title> Learning the distribution of object trajectories for event recognition in: Pycock, D (editors) British Machine Vision Conference 1995, </title> <type> vol.2, </type> <institution> pp.583-592. BMVA, </institution> <year> 1995. </year>
Reference-contexts: By clustering the tracks on the basis of common attributes, we can automatically deduce lanes of vehicular traffic, and pedestrian paths, and we can automatically correlate volumes of such traffic with time of day. Our approach of using motion information to categorize activities is similar in spirit to <ref> [3] </ref>, although we differ in several key details. Once we have extracted clusters representing common patterns of activity, we can cue our system to look for unusual events. These are outliers in the clustered distributions.
Reference: [4] <author> Horswill, I. and M. Yamamoto, </author> <title> A $1000 Active Stereo Vision System, </title> <booktitle> Proc., IEEE/IAP Workshop on Visual Behaviors, </booktitle> <address> Seattle, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Each sensor unit is a compact packaging of camera, onboard computational power, local memory, communication capability and possibly locational instrumentation (e.g., GPS). Example systems exist <ref> [4, 5, 8] </ref>, and more powerful systems will emerge as technology in sensor design, DSP processing, and communications evolves.
Reference: [5] <author> Horswill, I. </author> <title> Visual routines and visual search: A real-time implementation and automata-theoretic analysis, </title> <booktitle> Proc. IJCAI, </booktitle> <year> 1995. </year>
Reference-contexts: Each sensor unit is a compact packaging of camera, onboard computational power, local memory, communication capability and possibly locational instrumentation (e.g., GPS). Example systems exist <ref> [4, 5, 8] </ref>, and more powerful systems will emerge as technology in sensor design, DSP processing, and communications evolves.
Reference: [6] <author> Johnson, N. and D. Hogg. </author> <title> Learning the distribution of object trajectories for even recognition, </title> <journal> Image and Computing, </journal> <volume> vol. 14(8), </volume> <month> August </month> <year> 1996, </year> <pages> pp. 609-615. </pages>
Reference-contexts: Vector quantization can also be used <ref> [6] </ref> to reduce the continuous state space to a discrete space. We calculate accumulated co-occurrence statistics of the labels over all sequences using each sequence of labels as an equivalence class. <p> Unfortunately, in our case, neither of the above assumptions are completely valid, so it is not possible to determine the exact underlying processes. Rather than clustering the sequences into N different activity clusters, as in <ref> [6] </ref>, we determine a compact, hierarchical representation. Our goal is to determine relatively independent sets of state approximators at each level of our representation. This problem fits rather well into a formalism of graph bi-partitioning.
Reference: [7] <author> Koller, D., J. Weber, T. Huang, J. Malik, G. Ogasawara, B. Rao and S. Russell, </author> <title> Towards robust automatic traffic scene analysis in real-time Proc. </title> <address> ICPR, Israel, </address> <year> 1994. </year>
Reference-contexts: Simple implementations of background-ing just subtract consecutive images and threshold the resulting difference image to determine pixels that may correspond to motion. More robust methods use time averages of images [2], adaptive Gaussian estimation [12], or Kalman filtering <ref> [7] </ref> to derive the background image to be subtracted. While such methods often run in real time, they are generally not robust.
Reference: [8] <author> Konolige, K., </author> <title> Small vision systems: Hardware and Implementation, </title> <booktitle> Eighth International Symposium on Robotics Research, </booktitle> <address> Hayama, Japan, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: Each sensor unit is a compact packaging of camera, onboard computational power, local memory, communication capability and possibly locational instrumentation (e.g., GPS). Example systems exist <ref> [4, 5, 8] </ref>, and more powerful systems will emerge as technology in sensor design, DSP processing, and communications evolves.
Reference: [9] <author> Stauffer, C., </author> <title> Adaptive background mixture models, </title> <note> in preparation. </note>
Reference-contexts: and return an accurate description of the observed object, both its motion parameters and its intrinsic parameters such as size and shape; and methods that can use such tracking data to accomplish the tasks listed above. 2 A robust adaptive tracker In this section, we describe a novel tracking system <ref> [9] </ref>, based on the standard notion of background subtraction. Simple implementations of background-ing just subtract consecutive images and threshold the resulting difference image to determine pixels that may correspond to motion.
Reference: [10] <author> Stauffer, C., </author> <title> Scene Reconstruction Using Accumluated Line-of-Sight, </title> <type> SM Thesis, </type> <institution> MIT, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: In our approach <ref> [10] </ref>, we initially consider the site, as viewed from the camera, as completely filled. Now suppose that we observe 4 model. The left image shows a background image, an image containing a moving object, the extracted moving object and the current rough depth map, with intensity encoding distance.
Reference: [11] <author> Wallace, R., </author> <title> Finding Natural Clusters through Entropy Minimization PhD thesis, </title> <address> CMU, CMU-CS-89-183, </address> <year> 1989. </year>
Reference-contexts: We have considered two approaches for classifying actions. In the first method, we cluster the tracker output using an entropy minimization algorithm by Wallace <ref> [11] </ref>, the numeric iterative hierarchical cluster (NIHC) algorithm. The NIHC algorithm starts with randomly assigning data to clusters in a B-tree structure (a binary tree in which all internal nodes have two children) and iteratively reduces the total Gaussian entropy of the tree.
Reference: [12] <author> Wren, C., A. Azarbayejani, T. Darrell, A. Pentland. Pfinder: </author> <title> Real-Time Tracking of the Human Body, </title> <journal> IEEE Trans. PAMI, </journal> <volume> 19(7) </volume> <pages> 780-785, </pages> <month> July </month> <year> 1997. </year> <title> extended period of time. Each row shows an example image and the track patterns for the previous hour, with times ranging from 7am to 4pm. </title> <type> 8 </type>
Reference-contexts: Simple implementations of background-ing just subtract consecutive images and threshold the resulting difference image to determine pixels that may correspond to motion. More robust methods use time averages of images [2], adaptive Gaussian estimation <ref> [12] </ref>, or Kalman filtering [7] to derive the background image to be subtracted. While such methods often run in real time, they are generally not robust. <p> This reflects the expectation that samples of the same scene point are likely to display normal noise distributions, and the expectation that more than one process may be observed over time. This is in contrast to Pfinder <ref> [12] </ref> which fits a single Gaussian to a background pixel's history and requires a model of the moving object.
References-found: 12

