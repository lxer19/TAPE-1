URL: ftp://seldi.uniba.it/pub/papers/seke95.ps
Refering-URL: http://www.cs.umd.edu/~lanubile/public.html
Root-URL: 
Email: email: visaggio@seldi.uniba.it  
Title: Comparing models for identifying fault-prone software components major problem in predicting software reliability using the
Author: F. Lanubile, A. Lonigro, G. Visaggio 
Date: 40%  
Note: 1: Introduction A  is the This work has been partially supported by national funds MURST  This paper compares the following modeling  which has been often  models [5], [6], [17]. Discriminant analysis, which has been previously  which has been included in  [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], [23], [24], [30], or the  [18] or reusability metrics [4].  
Address: Italy  
Affiliation: Dipartimento di Informatica University of Bari,  
Abstract: We present an empirical investigation of the modeling techniques for identifying fault-prone software components early in the software life cycle. Using software complexity measures, the techniques build models which classify components as likely to contain faults or not. The modeling techniques applied in this study cover the main classification paradigms, including principal component analysis, discriminant analysis, logistic regression, logical classification models, layered neural networks, and holographic networks. Experimental results are obtained from 27 academic software projects. We evaluate the models with respect to four criteria: predictive validity, misclassification rate, achieved quality, and verification cost. A surprising result is that no model is able to discriminate between components with faults and components without faults. Software complexity metrics are often used as indirect metrics of reliability since they can be obtained relatively early in the software development life cycle. Using complexity metrics to identify components which likely contain faults allows software engineers to focus the verification effort on them, thus achieving a reliable product at a lower cost. In this paper, we assume that there exists a relationship between the measures of software complexity and the faults found during testing and operation phases. The foundation of this assumption comes from past research which has given empirical evidence of the existence of this relationship [11], [13], [20]. high skewed distribution of faults, because the majority of components have no faults or very few faults [22]. Instead to estimate the number of potential faults in a software component we will indicate whether a component is likely to be fault-prone or not. In this case, the direct metric of reliability is the class to which the software component belongs (high-risk or low-risk), and the prediction model is reduced to a classification model. Classification problems have traditionally been solved by various methods, which originate from different problemsolving paradigms: statistical analysis, machine learning, and neural networks. Statistical methods usually try to find an explicit numerical formula, which determines completely how classification is performed. Machine learning methods try to deduce exact if-then-else rules that can be used in the classification process. The neural network paradigm, instead to produce formulas or rules, trains a neural network to reproduce a given set of correct classification examples. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. </author> <title> Agresti, </title> <publisher> Categorical Data Analysis , John Wiley & Sons, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: The first one, applying discriminant analysis directly on the original 11 complexity metrics. The second one, using as input the domain metrics obtained from the principal component analysis. 3.3: Logistic regression Logistic regression refers to an analysis which computes the probability of class membership according to the following equation <ref> [1] </ref>: log p 1- 0 i n where p is the probability of a software component to be high-risk, and the independent variables x i complexity metrics. The regression coefficients c i computed through a maximum-likelihood estimation. Like discriminant analysis, two regression models were built. <p> The networks input and output is bounded between 0 and 1. Thus, we reduced input data using a direct scaling. When testing the network, we increased the error tolerance to 0.5 so that low-risk components correspond to observations with an output value in the first half of <ref> [0, 1] </ref>, and high-risk components to observations with an output value in the second half. 3.6: Holographic networks With holographic networks information is encoded inside holographic neurons rather than in the connection weights between neurons [32]. A holographic neuron holds a correlation matrix which enables memorizing stimulus-response associations.
Reference: [2] <author> V. R. Basili, and B. T. Perricone, </author> <title> Software errors and complexity: an empirical investigation, </title> <journal> Communications of the ACM , vol.27, </journal> <volume> no.1, </volume> <month> January </month> <year> 1984, </year> <month> pp.42-52. </month>
Reference-contexts: Applying discriminant analysis with principal components, at a probability level of 80 percent, resulted in recognizing 79 percent of the modules with a total misclassification rate of 5 percent. Our result is closer with the investigation performed by Basili and Perricone <ref> [2] </ref>, where the unexpected result was that module size and cyclomatic complexity had no relationship with the number of faults, although there was a negative relationship with the fault density.
Reference: [3] <author> V. R. Basili, and H. D. Rombach, </author> <title> The TAME project: towards improvement-oriented software environments, </title> <journal> IEEE Transactions on Software Engineering , vol.14, </journal> <volume> no.6, </volume> <month> June </month> <year> 1988, </year> <month> pp.758-773. </month>
Reference-contexts: From our study, we argue that the relationship between software complexity measures and software quality cannot be considered an assumption which holds for any data set and project, but should be validated in each specific environment <ref> [3] </ref>, [28]. A predictive model, from the simplest to the most complex, is worthwhile only if there is a local process to select metrics which are valid as predictors.
Reference: [4] <author> G. Boetticher, K. Srinivas, and D. Eichmann, </author> <title> A neural net-based approach to software metrics, </title> <booktitle> in Proceedings of the 5th International Conference on Software Engineering and Knowledge Engineering , San Francisco, </booktitle> <address> California, </address> <month> June </month> <year> 1993, </year> <month> pp.271-274. </month>
Reference-contexts: Layered neural networks, which have been already applied in software engineering applications to build reliability growth models [15], [16], predict the gross change [18] or reusability metrics <ref> [4] </ref>. Holographic networks, a nonconnectionist kind of neural network, which have been proposed for evaluating software quality [19]. Empirical investigations have been already performed in other fields, including financing [31] and manufacturing [14]. In Section 2, we describe the software environment and the data used in the empirical study.
Reference: [5] <author> L. C. Briand, W. M. Thomas, and C. J. Hetmanski, </author> <title> Modeling and managing risk early in software development, </title> <booktitle> in Proceedings of the 15th International Conference on Software Engineering , Baltimore, </booktitle> <address> Maryland, </address> <month> May </month> <year> 1993, </year> <month> pp.55-65. </month>
Reference-contexts: This paper compares the following modeling techniques covering all the three classification paradigms: Principal component analysis, which has been often used in the software engineering field to improve the accuracy of discriminant models [22] or regression models <ref> [5] </ref>, [6], [17]. Discriminant analysis, which has been previously applied to detect fault-prone programs [22]. Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], <p> which has been often used in the software engineering field to improve the accuracy of discriminant models [22] or regression models <ref> [5] </ref>, [6], [17]. Discriminant analysis, which has been previously applied to detect fault-prone programs [22]. Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], [23], [24], [30], or the detection of reusable software components [9]. <p> <ref> [5] </ref>, [6], [17]. Discriminant analysis, which has been previously applied to detect fault-prone programs [22]. Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], [23], [24], [30], or the detection of reusable software components [9]. Layered neural networks, which have been already applied in software engineering applications to build reliability growth models [15], [16], predict the gross change [18] or reusability metrics [4]. <p> We suppose that the verification will be so exhaustive to find the faults of all the components which are actually high-risk. We measure this criterion using the completeness measure <ref> [5] </ref>, which is the percentage of faulty components that have been actually classified as such by the model.
Reference: [6] <author> L. C. Briand, V. R. Basili, and C. J. Hetmanski, </author> <title> Developing interpretable models with optimized set reduction for identifying high-risk software components, </title> <journal> IEEE Transactions on Software Engineering , vol.19, </journal> <volume> no.11, </volume> <month> November </month> <year> 1993, </year> <month> pp.1028-1044. </month>
Reference-contexts: This paper compares the following modeling techniques covering all the three classification paradigms: Principal component analysis, which has been often used in the software engineering field to improve the accuracy of discriminant models [22] or regression models [5], <ref> [6] </ref>, [17]. Discriminant analysis, which has been previously applied to detect fault-prone programs [22]. Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], <p> has been often used in the software engineering field to improve the accuracy of discriminant models [22] or regression models [5], <ref> [6] </ref>, [17]. Discriminant analysis, which has been previously applied to detect fault-prone programs [22]. Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], [23], [24], [30], or the detection of reusable software components [9]. <p> <ref> [6] </ref>, [17]. Discriminant analysis, which has been previously applied to detect fault-prone programs [22]. Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], [23], [24], [30], or the detection of reusable software components [9]. Layered neural networks, which have been already applied in software engineering applications to build reliability growth models [15], [16], predict the gross change [18] or reusability metrics [4]. <p> This result is in contrast with various papers which report successful results in recognizing fault-prone components from analogous sets of complexity measures. Briand et al. <ref> [6] </ref> presented an experiment for predicting high-risk components using two logical classification models (Optimized Set Reduction and classification tree) and two logistic regression models (with and without principal components). Design and code metrics were collected from 146 components of a 260 KLOC system.
Reference: [7] <author> W. J. Conover, </author> <title> Practical Nonparametric Statistics , Wiley, </title> <address> New York, </address> <year> 1971. </year>
Reference-contexts: In this case, the predictive model is not able to discriminate low-risk components from high-risk components. The alternative hypothesis is one of general association. A chisquare ( c 2 ) statistic <ref> [7] </ref> with a distribution of one degree of freedom is applied to test the null hypothesis. 4.2: Misclassification rate For our predictive models, which classify components as either low-risk or high-risk, two misclassification errors are possible.
Reference: [8] <author> W. R. Dillon, and M. Goldstein, </author> <title> Multivariate Analysis: </title> <publisher> Methods and Applications , John Wiley & Sons, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: In this cases, principal component analysis can be used to reduce the dimensions of the metric space and obtain a smaller number of orthogonal domain metrics <ref> [8] </ref>. In our experiment, the principal component analysis applied on the 11 complexity metrics revealed three distinct complexity domains, having eigenvalues greater than 0.9. In Table 1, each column shows the degree of relationship between the 11 complexity metrics and the three orthogonal domains. <p> Rotated factor pattern 3.2: Discriminant analysis Discriminant analysis develops a discriminant function or classification criterion to place each observation into one of mutually exclusive groups <ref> [8] </ref>. It requires that there exists a prior knowledge of the classes, in our case low-risk and high-risk components. To develop the classification criterion, we used a parametric method which uses a measure of generalized square distance and is based on the pooled covariance matrix.
Reference: [9] <author> J. C. Esteva, and R. G. Reynolds, </author> <title> Identifying reusable software components by induction, </title> <journal> International Journal of Software Engineering and Knowledge Engineering , vol.1, </journal> <volume> no.3, </volume> <year> 1991, </year> <month> pp.271-292. </month>
Reference-contexts: Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], [23], [24], [30], or the detection of reusable software components <ref> [9] </ref>. Layered neural networks, which have been already applied in software engineering applications to build reliability growth models [15], [16], predict the gross change [18] or reusability metrics [4]. Holographic networks, a nonconnectionist kind of neural network, which have been proposed for evaluating software quality [19].
Reference: [10] <author> N. E. Fenton, </author> <title> Software Metrics: A Rigorous Approach , Chapman & Hall, </title> <address> London, </address> <year> 1991. </year>
Reference-contexts: McCabes cyclomatic complexity ( v (G) ), where v (G) = e - n + 2 for a flowchart with e edges and n nodes [21] 2. Number of unique operands ( h 2 3. Total number of operands ( N 2 4. Number of lines of code (LOC) <ref> [10] </ref> 5. Number of noncomment lines of code (NCLOC), excluding comments and blank lines [10] 6. Halsteads program length ( N ), where N = N 1 2 and N is the total number of operators [12] 7. <p> Number of unique operands ( h 2 3. Total number of operands ( N 2 4. Number of lines of code (LOC) <ref> [10] </ref> 5. Number of noncomment lines of code (NCLOC), excluding comments and blank lines [10] 6. Halsteads program length ( N ), where N = N 1 2 and N is the total number of operators [12] 7. Halsteads volume ( V ), where V = N * log 2 h , where h = h + h is the program vocabulary [12] 8. <p> Henry&Kafuras information flow (IF), where IF = (fanin*fanout) 2 [13] 11. Density of comments (DC), where DC = CLOC / LOC , and CLOC is the number of comment lines of program text <ref> [10] </ref> The metrics have been selected so as to measure both design and implementation attributes of the components, such as control flow structure (metric 1), data structure (metrics 2-3), size (metrics 4-7), coupling (metrics 8-10), and documentation (metric 11). 0 20 40 60 Number of faults The set of 118 observations
Reference: [11] <author> A. Fitzsimmons, and T. Love, </author> <title> A review and evaluation of software science, </title> <journal> Computing Surveys , vol.10, no.1, March1978, pp.3-18. </journal>
Reference-contexts: In this paper, we assume that there exists a relationship between the measures of software complexity and the faults found during testing and operation phases. The foundation of this assumption comes from past research which has given empirical evidence of the existence of this relationship <ref> [11] </ref>, [13], [20]. A major problem in predicting software reliability using the number of faults in software components is the * This work has been partially supported by national funds MURST 40% high skewed distribution of faults, because the majority of components have no faults or very few faults [22].
Reference: [12] <author> M. H. </author> <title> Halstead, </title> <publisher> Elements of Software Science , Elsevier North-Holland, </publisher> <address> New York, </address> <year> 1977. </year>
Reference-contexts: Number of lines of code (LOC) [10] 5. Number of noncomment lines of code (NCLOC), excluding comments and blank lines [10] 6. Halsteads program length ( N ), where N = N 1 2 and N is the total number of operators <ref> [12] </ref> 7. Halsteads volume ( V ), where V = N * log 2 h , where h = h + h is the program vocabulary [12] 8. <p> Halsteads program length ( N ), where N = N 1 2 and N is the total number of operators <ref> [12] </ref> 7. Halsteads volume ( V ), where V = N * log 2 h , where h = h + h is the program vocabulary [12] 8. Henry&Kafuras fan-in (fanin), where the fan-in of a module M is the number of local flows that terminate at M, plus the number of data structures from which information is retrieved by M [13] 9.
Reference: [13] <author> S. Henry, and D. Kafura, </author> <title> Software structure metrics based on information flow, </title> <journal> IEEE Transactions on Software Engineering , vol.SE-7, </journal> <volume> no.5, </volume> <month> September </month> <year> 1981, </year> <month> pp.510-518. </month>
Reference-contexts: In this paper, we assume that there exists a relationship between the measures of software complexity and the faults found during testing and operation phases. The foundation of this assumption comes from past research which has given empirical evidence of the existence of this relationship [11], <ref> [13] </ref>, [20]. A major problem in predicting software reliability using the number of faults in software components is the * This work has been partially supported by national funds MURST 40% high skewed distribution of faults, because the majority of components have no faults or very few faults [22]. <p> Henry&Kafuras fan-in (fanin), where the fan-in of a module M is the number of local flows that terminate at M, plus the number of data structures from which information is retrieved by M <ref> [13] </ref> 9. Henry&Kafuras fanout (fanout), where the fanout of a module M is the number of local flows that emanate from M, plus the number of data structures that are updated by M [13] 10. Henry&Kafuras information flow (IF), where IF = (fanin*fanout) 2 [13] 11. <p> that terminate at M, plus the number of data structures from which information is retrieved by M <ref> [13] </ref> 9. Henry&Kafuras fanout (fanout), where the fanout of a module M is the number of local flows that emanate from M, plus the number of data structures that are updated by M [13] 10. Henry&Kafuras information flow (IF), where IF = (fanin*fanout) 2 [13] 11. <p> which information is retrieved by M <ref> [13] </ref> 9. Henry&Kafuras fanout (fanout), where the fanout of a module M is the number of local flows that emanate from M, plus the number of data structures that are updated by M [13] 10. Henry&Kafuras information flow (IF), where IF = (fanin*fanout) 2 [13] 11.
Reference: [14] <author> G. Jensen, </author> <title> Quality control in manufacturing based on fuzzy classification, in Frontier Decision Support Concepts , John Wiley & Sons, </title> <address> New York, </address> <year> 1994, </year> <month> pp.107-118. </month>
Reference-contexts: Holographic networks, a nonconnectionist kind of neural network, which have been proposed for evaluating software quality [19]. Empirical investigations have been already performed in other fields, including financing [31] and manufacturing <ref> [14] </ref>. In Section 2, we describe the software environment and the data used in the empirical study. In Section 3, we present how the modeling techniques where used to build the predictive models. Section 4 gives the criteria which we use to validate and compare the models.
Reference: [15] <author> N. Karunanithi, D. Whitley, and Y. K. Malaiya, </author> <title> Prediction of software reliability using connectionists models, </title> <journal> IEEE Transactions on Software Engineering , vol.18, </journal> <volume> no.7, </volume> <month> July </month> <year> 1992, </year> <month> pp.563-573. </month>
Reference-contexts: Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], [23], [24], [30], or the detection of reusable software components [9]. Layered neural networks, which have been already applied in software engineering applications to build reliability growth models <ref> [15] </ref>, [16], predict the gross change [18] or reusability metrics [4]. Holographic networks, a nonconnectionist kind of neural network, which have been proposed for evaluating software quality [19]. Empirical investigations have been already performed in other fields, including financing [31] and manufacturing [14].
Reference: [16] <author> N. Karunanithi, D. Whitley, and Y. K. Malaiya, </author> <title> Using neural networks in reliability prediction, </title> <note> IEEE Software , July 1992, pp.53-59 </note>
Reference-contexts: Layered neural networks, which have been already applied in software engineering applications to build reliability growth models [15], <ref> [16] </ref>, predict the gross change [18] or reusability metrics [4]. Holographic networks, a nonconnectionist kind of neural network, which have been proposed for evaluating software quality [19]. Empirical investigations have been already performed in other fields, including financing [31] and manufacturing [14].
Reference: [17] <author> T. M. Khoshgoftaar, D. L. Lanning, and J. C. Munson, </author> <title> A comparative study of predictive models for program changes during system testing and maintenance, </title> <booktitle> in Proceedings of the Conference on Software Maintenance , Montreal, </booktitle> <address> Canada, </address> <month> September </month> <year> 1993, </year> <month> pp.72-79. </month>
Reference-contexts: This paper compares the following modeling techniques covering all the three classification paradigms: Principal component analysis, which has been often used in the software engineering field to improve the accuracy of discriminant models [22] or regression models [5], [6], <ref> [17] </ref>. Discriminant analysis, which has been previously applied to detect fault-prone programs [22]. Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6].
Reference: [18] <author> T. M. Khoshgoftaar, and R. M. Szabo, </author> <title> Improving code churn prediction during the system test and maintenance phases, </title> <booktitle> in Proceedings of the International Conference on Software Maintenance , Victoria, </booktitle> <address> British Columbia, Canada, </address> <month> September </month> <year> 1994, </year> <month> pp.58-67. </month>
Reference-contexts: Layered neural networks, which have been already applied in software engineering applications to build reliability growth models [15], [16], predict the gross change <ref> [18] </ref> or reusability metrics [4]. Holographic networks, a nonconnectionist kind of neural network, which have been proposed for evaluating software quality [19]. Empirical investigations have been already performed in other fields, including financing [31] and manufacturing [14].
Reference: [19] <author> F. Lanubile, and G. Visaggio, </author> <title> Quality evaluation on software reengineering based on fuzzy classification, in Frontier Decision Support Concepts , John Wiley & Sons, </title> <address> New York, </address> <year> 1994, </year> <month> pp.119-134. </month>
Reference-contexts: Layered neural networks, which have been already applied in software engineering applications to build reliability growth models [15], [16], predict the gross change [18] or reusability metrics [4]. Holographic networks, a nonconnectionist kind of neural network, which have been proposed for evaluating software quality <ref> [19] </ref>. Empirical investigations have been already performed in other fields, including financing [31] and manufacturing [14]. In Section 2, we describe the software environment and the data used in the empirical study. In Section 3, we present how the modeling techniques where used to build the predictive models.
Reference: [20] <author> J. Lewis, amd S. Henry, </author> <title> A methodology for integrating maintainability using software metrics, </title> <booktitle> in Proceedings of the Conference on Software Maintenance , Miami, </booktitle> <address> FL, </address> <month> October </month> <year> 1989, </year> <month> pp.32-39. </month>
Reference-contexts: In this paper, we assume that there exists a relationship between the measures of software complexity and the faults found during testing and operation phases. The foundation of this assumption comes from past research which has given empirical evidence of the existence of this relationship [11], [13], <ref> [20] </ref>. A major problem in predicting software reliability using the number of faults in software components is the * This work has been partially supported by national funds MURST 40% high skewed distribution of faults, because the majority of components have no faults or very few faults [22].
Reference: [21] <author> T. J. McCabe, </author> <title> A complexity measure, </title> <journal> IEEE Transactions on Software Engineering , vol.SE-2, </journal> <volume> no.4, </volume> <month> December </month> <year> 1976, </year> <month> pp.308-320. </month>
Reference-contexts: The metric data collected were as follows: 1. McCabes cyclomatic complexity ( v (G) ), where v (G) = e - n + 2 for a flowchart with e edges and n nodes <ref> [21] </ref> 2. Number of unique operands ( h 2 3. Total number of operands ( N 2 4. Number of lines of code (LOC) [10] 5. Number of noncomment lines of code (NCLOC), excluding comments and blank lines [10] 6.
Reference: [22] <author> J. C. Munson, and T. M. Khoshgoftaar, </author> <title> The detection of fault-prone programs, </title> <journal> IEEE Transactions on Software Engineering , vol.18, </journal> <volume> no.5, </volume> <month> May </month> <year> 1992, </year> <month> pp.423-433. </month>
Reference-contexts: A major problem in predicting software reliability using the number of faults in software components is the * This work has been partially supported by national funds MURST 40% high skewed distribution of faults, because the majority of components have no faults or very few faults <ref> [22] </ref>. Instead to estimate the number of potential faults in a software component we will indicate whether a component is likely to be fault-prone or not. <p> This paper compares the following modeling techniques covering all the three classification paradigms: Principal component analysis, which has been often used in the software engineering field to improve the accuracy of discriminant models <ref> [22] </ref> or regression models [5], [6], [17]. Discriminant analysis, which has been previously applied to detect fault-prone programs [22]. Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. <p> paper compares the following modeling techniques covering all the three classification paradigms: Principal component analysis, which has been often used in the software engineering field to improve the accuracy of discriminant models <ref> [22] </ref> or regression models [5], [6], [17]. Discriminant analysis, which has been previously applied to detect fault-prone programs [22]. Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], [23], [24], [30], or the detection of reusable software components [9]. <p> He measured the mean accuracy across all tree applications according to completeness (82 percent) and to the percentage of components whose target class membership is correctly identified (72 percent), that is the complement of the Proportion of Type 1 and Type 2 error. Munson and Koshgoftaar <ref> [22] </ref> detected faulty components by applying principal component analysis and discriminant analysis to discriminate between programs with less than five faults and programs having 5 or more faults. The data set included 327 program modules from two distinct Ada projects of a command and control communication system.
Reference: [23] <author> A. A. Porter, and R. W. Selby, </author> <title> Empirically guided software development using metric-based classification trees, </title> <journal> IEEE Software , March 1990, pp.46-54. </journal>
Reference-contexts: Discriminant analysis, which has been previously applied to detect fault-prone programs [22]. Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], <ref> [23] </ref>, [24], [30], or the detection of reusable software components [9]. Layered neural networks, which have been already applied in software engineering applications to build reliability growth models [15], [16], predict the gross change [18] or reusability metrics [4].
Reference: [24] <author> A. A. Porter, </author> <title> Developing and analyzing classification rules for predicting faulty software components, </title> <booktitle> in Proceedings of the 5th International Conference on Software Engineering and Knowledge Engineering , San Francisco, </booktitle> <address> California, </address> <month> June </month> <year> 1993, </year> <month> pp.453-461. </month>
Reference-contexts: Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], [23], <ref> [24] </ref>, [30], or the detection of reusable software components [9]. Layered neural networks, which have been already applied in software engineering applications to build reliability growth models [15], [16], predict the gross change [18] or reusability metrics [4]. <p> The classification tree was more complete (82 percent) and correct (83 percent) than logistic regression models. The use of principal components improved the accuracy of logistic regression, from 67 to 71 percent of completeness and from 77 to 80 percent of correctness. Porter <ref> [24] </ref> presented an application of classification trees to data collected from 1400 components of six FORTRAN projects in NASA environment. For each component, 19 attributes were measured, capturing information spanning from design specifications to implementation.
Reference: [25] <author> J. R. Quinlan, </author> <title> Induction of decision trees, Machine Learning , vol.1, </title> <address> no.1, </address> <year> 1986, </year> <month> pp.81-106. </month>
Reference-contexts: They are generated through a recursive algorithm that selects metrics which best discriminate between components within a target class and those outside it. We used the C4.5 system [26] which is a variation on the ID3 system <ref> [25] </ref> to automatically build the classification model. C4.5 partitions continuous attributes finding the best threshold among the set of training cases. The recursive partition method continues to subdivide the training set until each subset in the partition contains cases of a single class, or until no test offers any improvement.
Reference: [26] <author> J. R. Quinlan, C4.5: </author> <title> Programs for Machine Learning , Morgan Kauffman Publishers, </title> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: They are generated through a recursive algorithm that selects metrics which best discriminate between components within a target class and those outside it. We used the C4.5 system <ref> [26] </ref> which is a variation on the ID3 system [25] to automatically build the classification model. C4.5 partitions continuous attributes finding the best threshold among the set of training cases.
Reference: [27] <author> D. Rumelhart, G. Hinton, and R. Williams, </author> <title> Learning internal representations by error propagation, </title> <booktitle> in Parallel Distribuited Processing , vol.I, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986, </year> <month> pp.318-362. </month>
Reference-contexts: The Halsteads metrics ( h 2 2 V ) did not appeared neither in the decision tree nor in the production rules. 3.5: Layered neural networks We used a typical feed-forward neural network <ref> [27] </ref>, characterized in our experiment by one input layer of 11 neurons, each connected to a complexity metric, one output layer of only 1 neuron that sends externally the predicted risk, and one layer of 50 hidden neurons.
Reference: [28] <author> N. F. Schneidewind, </author> <title> Methodology for validating software metrics, </title> <journal> IEEE Transactions on Software Engineering , vol.18, </journal> <volume> no.5, </volume> <month> May </month> <year> 1992, </year> <month> pp.410-422. </month>
Reference-contexts: From our study, we argue that the relationship between software complexity measures and software quality cannot be considered an assumption which holds for any data set and project, but should be validated in each specific environment [3], <ref> [28] </ref>. A predictive model, from the simplest to the most complex, is worthwhile only if there is a local process to select metrics which are valid as predictors.
Reference: [29] <author> N. F. Schneidewind, </author> <title> Validating metrics for ensuring Space Shuttle Flight software quality, </title> <note> Computer , August 1994, pp.50-57. </note>
Reference-contexts: This would cause a waste of effort. In the contingency table, the number of Type 1 and Type 2 errors is given, respectively, by n 21 12 adopt from <ref> [29] </ref> the following measures of misclassification: Proportion of Type 1: P 1 21 Proportion of Type 2: P 2 12 Proportion of Type 1 + Type 2: P 12 21 12 4.3: Quality achieved We are interested in measuring how effective are the predictive models in terms of the quality achieved <p> Completeness: C = n 22 2 4.4: Verification cost Quality is achieved by increasing the cost of verification due to an extra effort in inspection and testing for the components which have been flagged as high-risk. We measure the verification cost by using two indicators. The former, inspection <ref> [29] </ref>, measures the overall cost by considering the percentage of components which should be verified. The latter, wasted inspection, is the percentage of verified components which do not contain faults because they have been incorrectly classified.
Reference: [30] <author> R. W. Selby, and A. A. Porter, </author> <title> Learning from examples: generation and evaluation of decision trees for software resource analysis, </title> <journal> IEEE Transactions on Software Engineering , vol.14, </journal> <volume> no.12, </volume> <month> December </month> <year> 1988, </year> <month> pp.1743-1757. </month>
Reference-contexts: Logistic regression, which has been included in empirical comparisons between models identifying high-risk components [5], [6]. Logical classification models, which have been extensively used in software engineering issues, such as the identification of high-risk modules [5], [6], [23], [24], <ref> [30] </ref>, or the detection of reusable software components [9]. Layered neural networks, which have been already applied in software engineering applications to build reliability growth models [15], [16], predict the gross change [18] or reusability metrics [4].
Reference: [31] <author> B. Soucek, J. Sutherland, and G. Visaggio, </author> <title> Holographic decision support system: credit scoring based on quality metrics, in Frontier Decision Support Concepts , John Wiley & Sons, </title> <address> New York, </address> <year> 1994, </year> <month> pp.171-182. </month>
Reference-contexts: Holographic networks, a nonconnectionist kind of neural network, which have been proposed for evaluating software quality [19]. Empirical investigations have been already performed in other fields, including financing <ref> [31] </ref> and manufacturing [14]. In Section 2, we describe the software environment and the data used in the empirical study. In Section 3, we present how the modeling techniques where used to build the predictive models. Section 4 gives the criteria which we use to validate and compare the models.
Reference: [32] <author> J. Sutherland, </author> <title> A holographic model of memory, learning and expression, </title> <journal> International Journal of Neural Systems , vol.1, </journal> <volume> no.3, </volume> <year> 1990, </year> <month> pp.259-267. </month>
Reference-contexts: that low-risk components correspond to observations with an output value in the first half of [0, 1], and high-risk components to observations with an output value in the second half. 3.6: Holographic networks With holographic networks information is encoded inside holographic neurons rather than in the connection weights between neurons <ref> [32] </ref>. A holographic neuron holds a correlation matrix which enables memorizing stimulus-response associations. Individual associations are learned deterministically on one non-iterative transformation.
References-found: 32

