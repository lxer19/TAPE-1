URL: http://www.cs.tu-berlin.de/~mki/report/part6.ps
Refering-URL: http://www.cs.tu-berlin.de/~mki/report/tr.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Reinforcement Learning of Multiple Real-Valued Control Actions in a Dynamical System  
Author: M. Stauch, A. Rother, K. Neuenhofen, H. Kampe and T. Scheffer 
Abstract: This paper presents an approach to learning how to control a dynamical system that requires several real-valued control actions. The reinforcement theory has been implemented and applied to the simulation of an autonomous flying robot. Experiments have shown good learning results. 
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> Brandenburg, W., Finke, M., Hanisch, D., Musial, M. & Stenzel, R. </author> <year> (1995). </year> <title> Tubrob An autonomously flying robot. </title> <booktitle> Proc. Symposium of the Association for Unmanned Vehicle Systems: </booktitle> <pages> pp. 32-42. </pages>
Reference-contexts: We applied our implemented system to the control of the aerial robot Tubrob (see Fig. 1), that was constructed by the real time systems group at the Technische Universitat Berlin <ref> [ 1 ] </ref> . It is based on a helium baloon of five meters length and propelled by electric motors. The mounted frame comprises an ultrasonic positioning system, a compass and a microcomputer. <p> At the same time a Fermi function will normalize the change of Q to produce a value of r in the interval <ref> [1; 1] </ref>. In the course of the learning a useful value for is to be determined for every box in every box-structure. At the same time, as approaches an optimal value should be reduced resulting in a higher probability of values close to being output by the box. <p> They both expect a real number input in the interval <ref> [1; 1] </ref>. Thus our RLS contained two box-structures. <p> At the same time, the used state variables could be weighted to ensure a meaningful quality criterion calculation. An illustration is shown in Fig. 5. Using a probing mechanism, the parameters of the Fermi function that is used to limit the change of quality (i.e. reinforcement) to an interval <ref> [1; 1] </ref> were es tablished previous to the actual learning.
Reference: [ 2 ] <author> Barto, A. B., Sutton, R. S. & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike Adaptive Elements that can Solve Difficult Learning Control Problems. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics. </journal> <volume> 13(5). </volume>
Reference-contexts: 1 Introduction Traditionally control problems have been solved for dynamical systems with discrete control actions, and several solutions to the well-known cart-pole system have been presented in the past, notably the Boxes algorithm [ 5 ] and the ASE-ACE approach <ref> [ 2 ] </ref> . A number of new methods of reinforcement learning have been presented since [ 7 ] , the most popular of which is Q-learning [ 6 ] .
Reference: [ 3 ] <author> Dean, D., Basye K. and Shewchuk J. </author> <year> (1993). </year> <title> Reinforcement Learning for Planning and Control. In Minton (Ed.), Machine Learning Methods for Planning. </title> <address> San Mateo. </address>
Reference: [ 4 ] <author> Gullappalli, V. </author> <year> (1990). </year> <title> A Stochastic Reinforcement Learning Algorithm. </title> <booktitle> Neural Networks. </booktitle> <volume> 3: </volume> <pages> 671-692. </pages>
Reference-contexts: In the cart-pole system however the learned control actions are simple: either the cart is pushed to the left, or it is pushed to the right; the cart-pole algorithms provide no facility for controlling real-valued actions. Gullap-palli <ref> [ 4 ] </ref> presented a stochastical reinforcement learning algorithm that handles continuous signals, but his approach was restricted to learning functions and since it did not address the credit assignment problem, was not applicable to control problems.
Reference: [ 5 ] <author> Michie, D. and Chambers, R. </author> <year> (1968). </year> <title> Boxes: An Experiment in Adaptive Control. </title> <editor> In Dale, E. and Michie, D. (Eds.), </editor> <booktitle> Machine Intelligence. </booktitle> <publisher> Edinburgh: Oliver and Boyd. </publisher>
Reference-contexts: 1 Introduction Traditionally control problems have been solved for dynamical systems with discrete control actions, and several solutions to the well-known cart-pole system have been presented in the past, notably the Boxes algorithm <ref> [ 5 ] </ref> and the ASE-ACE approach [ 2 ] . A number of new methods of reinforcement learning have been presented since [ 7 ] , the most popular of which is Q-learning [ 6 ] .
Reference: [ 6 ] <author> Watkins, C.J.C.H., Dayan, P. </author> <year> (1992). </year> <title> Q-Learning. </title> <journal> Machine Learning. </journal> <volume> 8: </volume> <pages> 279-292. </pages>
Reference-contexts: A number of new methods of reinforcement learning have been presented since [ 7 ] , the most popular of which is Q-learning <ref> [ 6 ] </ref> . In the cart-pole system however the learned control actions are simple: either the cart is pushed to the left, or it is pushed to the right; the cart-pole algorithms provide no facility for controlling real-valued actions.
Reference: [ 7 ] <author> (1992). </author> <title> Special Issue on Reinforcement Learning. </title> <booktitle> Machine Learning. </booktitle> <pages> 8(3-4). </pages>
Reference-contexts: A number of new methods of reinforcement learning have been presented since <ref> [ 7 ] </ref> , the most popular of which is Q-learning [ 6 ] .
References-found: 7

