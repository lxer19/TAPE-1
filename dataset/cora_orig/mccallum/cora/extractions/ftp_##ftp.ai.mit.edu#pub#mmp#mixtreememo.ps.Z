URL: ftp://ftp.ai.mit.edu/pub/mmp/mixtreememo.ps.Z
Refering-URL: http://www.ai.mit.edu/people/mmp/mmp.html
Root-URL: 
Email: fmmp, jordang@ai.mit.edu  
Title: Estimating Dependency Structure as a Hidden Variable  
Author: Marina Meila and Michael I. Jordan 
Address: 45 Carleton St. E25-201 Cambridge, MA 02142  
Affiliation: Center for Biological Computational Learning Massachusetts Institute of Technology  
Abstract: This paper introduces a probability model, the mixture of trees that can account for sparse, dynamically changing dependence relationships. We present a family of efficient algorithms that use EM and the Minimum Spanning Tree algorithm to find the ML and MAP mixture of trees for a variety of priors, including the Dirichlet and the MDL priors.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. K. Chow and C. N. Liu. </author> <title> Approximating discrete probability distributions with dependence trees. </title> <journal> "IEEE Transactions on Information Theory", </journal> <volume> IT-14(3):462-467, </volume> <month> MAy </month> <year> 1968. </year>
Reference-contexts: Work on fitting a tree to a distribution in a Maximum-Likelihood (ML) framework has been pioneered by Chow and Liu <ref> [1] </ref> and was extended to polytrees by Pearl [8] and to mixtures of trees with observed structure variable by Geiger [4]. This work presents efficient algorithms for learning mixture of trees models with unknown or hidden structure variable. <p> To obtain the new distributions T k , we have to maximize for each k the expression that is the negative of the crossentrop between P k and T k . i=1 This problem can be solved exactly as shown in <ref> [1] </ref>. Here we will give a brief description of the procedure.
Reference: [2] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: a belief network, but most of the results here owe to the belief network perspective. 3 THE BASIC ALGORITHM: ML FITTING OF MIXTURES OF TREES This section will show how a mixture of trees can be fit to an observed dataset in the Maximum Likelihood paradigm via the EM algorithm <ref> [2] </ref>. The observations are denoted by fx 1 ; x 2 ; : : : ; x N g; the corresponding values of the structure variable are fz i ; i = 1; : : : N g.
Reference: [3] <author> B. J. Frey, G. E. Hinton, and P. Dayan. </author> <title> Does the wakw-sleep algorithm produce good density estimators? In D. </title> <editor> Touretsky, M. Mozer, and M. Hasselmo, editors, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <volume> number 8, </volume> <pages> pages 661-667. </pages> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: One dataset contained images of single digits in 64 dimensions, the second contained 128 dimensional vectors representing randomly paired digit images. The training, validation and test set contained 6000, 2000, and 5000 exemplars respectively. The data sets, the training conditions and the algorithms we compared with are described in <ref> [3] </ref>. We tried mixtures of 16, 32, 64 and 128 trees, fitted by the basic algorithm. We showed only the best performance in the results table 2.
Reference: [4] <author> D. Geiger. </author> <title> An entropy-based learning algorithm of bayesian conditional trees. </title> <booktitle> In Proceedings of the 8th Conference on Uncertainty in AI, </booktitle> <pages> pages 92-97. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Work on fitting a tree to a distribution in a Maximum-Likelihood (ML) framework has been pioneered by Chow and Liu [1] and was extended to polytrees by Pearl [8] and to mixtures of trees with observed structure variable by Geiger <ref> [4] </ref>. This work presents efficient algorithms for learning mixture of trees models with unknown or hidden structure variable. The following section introduces the model; then, section 3 develops the basic algorithm for its estimation from data in the ML framework.
Reference: [5] <author> D. Heckerman, D. Geiger, and D. M. Chickering. </author> <title> Learning Bayesian networks: the combination of knowledge and statistical data. </title> <journal> Machine Learining, </journal> <volume> 20(3) </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: A Dirichlet prior over a tree can be represented as a table of fictitious marginal probabilities P 0k uv for each pair u; v of variables plus an equivalent sample size N 0 that gives the strength of the prior <ref> [5] </ref>.
Reference: [6] <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Constructing bayesian finite mixture models by the EM algorithm. </title> <type> Technical Report C-1996-9, </type> <institution> Univeristy of Helsinky, Department of Computer Science, </institution> <year> 1996. </year>
Reference-contexts: Mixture models have been extensively used in the statistics and neural network literature. Of relevance to the present work are the mixtures of Gaussians, whose distribution space, in the case of continuous variables overlaps with the space of mixtures of trees (see and the recent work of <ref> [6] </ref> on mixtures of factorial distributions, a subclass of tree distributions. <p> MST is mixtures of spanning trees, MF is a mixture of factorial distributions. Algorithm Classification #runs Algorithm Classification #runs performance performance MST m=30 fi =0 *.82 10 MF m=35 .73 2 MST m=30 fi =.1 .70 1 MF m=48 *.80 8 MST m=30 fi =1. .67 2 MF from <ref> [6] </ref> *.84 MST m=35 fi =0 .71 3 Flexible Bayes from [6] .66 MST m=40 fi =.1 .73 3 C4 from [6] .66 MST m=25 fi =0 .77 14 1Rw from [6] .66 This method and several variations thereof are discussed in [7]. <p> Algorithm Classification #runs Algorithm Classification #runs performance performance MST m=30 fi =0 *.82 10 MF m=35 .73 2 MST m=30 fi =.1 .70 1 MF m=48 *.80 8 MST m=30 fi =1. .67 2 MF from <ref> [6] </ref> *.84 MST m=35 fi =0 .71 3 Flexible Bayes from [6] .66 MST m=40 fi =.1 .73 3 C4 from [6] .66 MST m=25 fi =0 .77 14 1Rw from [6] .66 This method and several variations thereof are discussed in [7]. <p> performance MST m=30 fi =0 *.82 10 MF m=35 .73 2 MST m=30 fi =.1 .70 1 MF m=48 *.80 8 MST m=30 fi =1. .67 2 MF from <ref> [6] </ref> *.84 MST m=35 fi =0 .71 3 Flexible Bayes from [6] .66 MST m=40 fi =.1 .73 3 C4 from [6] .66 MST m=25 fi =0 .77 14 1Rw from [6] .66 This method and several variations thereof are discussed in [7]. Its effect is to give a small probability weight to unseen instances and to draw the components closer to each other, thereby reducing the effective value of m. <p> 2 MST m=30 fi =.1 .70 1 MF m=48 *.80 8 MST m=30 fi =1. .67 2 MF from <ref> [6] </ref> *.84 MST m=35 fi =0 .71 3 Flexible Bayes from [6] .66 MST m=40 fi =.1 .73 3 C4 from [6] .66 MST m=25 fi =0 .77 14 1Rw from [6] .66 This method and several variations thereof are discussed in [7]. Its effect is to give a small probability weight to unseen instances and to draw the components closer to each other, thereby reducing the effective value of m. <p> For comparison we tried also mixtures of factorial distributions of different sizes. One seventh of the data, picked randomly at each trial, was used for testing and the rest for training. We replicate for comparison results obtained and cited in <ref> [6] </ref> on training/test sets of the same size. Table 1 shows a selection of the results we obtained. Smoothing with marginals proved to be bad for classification; therefore those results are not shown. The effect of edge pruning seems not to be significant on classification.
Reference: [7] <author> H. Ney, U. Essen, and R. Kneser. </author> <title> On structuring probabilistic dependences in stochastic language modelling. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 8 </volume> <pages> 1-38, </pages> <year> 1994. </year>
Reference-contexts: MST m=30 fi =1. .67 2 MF from [6] *.84 MST m=35 fi =0 .71 3 Flexible Bayes from [6] .66 MST m=40 fi =.1 .73 3 C4 from [6] .66 MST m=25 fi =0 .77 14 1Rw from [6] .66 This method and several variations thereof are discussed in <ref> [7] </ref>. Its effect is to give a small probability weight to unseen instances and to draw the components closer to each other, thereby reducing the effective value of m.
Reference: [8] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kauf-man Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1988. </year>
Reference-contexts: Work on fitting a tree to a distribution in a Maximum-Likelihood (ML) framework has been pioneered by Chow and Liu [1] and was extended to polytrees by Pearl <ref> [8] </ref> and to mixtures of trees with observed structure variable by Geiger [4]. This work presents efficient algorithms for learning mixture of trees models with unknown or hidden structure variable.
References-found: 8

