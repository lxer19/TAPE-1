URL: http://www.robotics.stanford.edu/~birch/publications/CS-TR-96-1573.ps.gz
Refering-URL: http://www.robotics.stanford.edu/~birch/p2p/
Root-URL: http://www.robotics.stanford.edu
Email: [birchfield, tomasi]@cs.stanford.edu  
Title: Depth Discontinuities by Pixel-to-Pixel Stereo  
Author: Stan Birchfield Carlo Tomasi 
Note: This research was supported by the National Science Foundation under a Graduate Research Fellowship and under contract IRI-9506064, and by the Department of Defense under MURI contract DAAH04-96-1-0007 monitored by ARO and under a subcontract of STTR contract F49620-95-C-0078 monitored by AFOSR.  
Address: Stanford, California 94305  
Affiliation: Computer Science Department Stanford University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> H. H. Baker and T. O. Binford. </author> <title> Depth from edge and intensity based stereo. </title> <booktitle> In Proc. 7th IJCAI , pp. </booktitle> <pages> 631-636, </pages> <month> August </month> <year> 1981. </year>
Reference-contexts: In contrast to the computationally expensive solution of matching at subpixel resolution or over windows, our dissimilarity measure solves the sampling problem with only a negligible increase in computation. Most stereo algorithms require texture 1 throughout the image (but see [6], [8], and <ref> [1] </ref>), the assumption presumably being that untextured regions have no information for matching. However, images quite often contain large untextured regions, particularly in indoor settings. Our algorithm is able to handle these regions by making the simple but powerful assumption that depth discontinuities are always accompanied by intensity gradients. <p> Therefore, there must be some j &gt; i such that (x j ; y i+1 ) is a match. 17 18 Chapter 4 Searching for the Optimal Match Sequence Thanks to the structure of the cost function (Equation 3.1), the technique of dynamic programming (also used in <ref> [1] </ref>, [12], [2], and [7]), can be used to find the optimal match sequence by searching over all the match sequences that satisfy the constraints given in Section 3.2. the number of pixels in each scanline and is the maximum disparity allowed.) Because of constraint C3, many of the cells in <p> In the extension from 1D to 2D, it is not uncommon for the computing time to increase by 600% or more [2, 12]. As a result, some approaches avoid the extension altogether [7, 8]. Moreover, we cannot rely on techniques that look only at pairs <ref> [1] </ref> or triplets [2] of adjacent scanlines at a time, since our initial disparity maps contain errors over large regions.
Reference: [2] <author> P. N. Belhumeur and D. Mumford. </author> <title> A Bayesian treatment of the stereo correspondence problem using half-occluded regions. </title> <booktitle> In CVPR, </booktitle> <pages> pp. 506-512, </pages> <year> 1992. </year>
Reference-contexts: Because our algorithm establishes full correspondence, it is a stereo system in its own right, despite its specialized purpose. As such, it is similar to other stereo algorithms that incorporate depth discontinuities and occluded regions <ref> [2, 7, 6, 11, 8, 9] </ref>. Three aspects of the algorithm are worth noting. First, it uses a measure of pixel dissimilarity that looks at the linearly interpolated intensity functions surrounding the two pixels, and thus is provably insensitive to sampling effects. <p> Therefore, the depths of the objects can be determined by triangulation. Just how restrictive are these assumptions? Overall, we have found A1 to be fairly reasonable (similar to the observation made in <ref> [2] </ref>), even though real-world objects are often not Lambertian [13]. Specular reflections cause little problem in our images since the baseline is small (but see [4] for one approach to handling them). Our experiments suggest that the assumption becomes less valid as the baseline is increased. <p> The cost function and hard constraints are the subjects of the next two sections. 3.1 Cost function Instead of deriving a maximum a posteriori cost function from a Bayesian formulation (as is done in <ref> [2] </ref>, [7], and [11]), we propose a simple cost function based mainly on intuition and justified solely by empirical evidence. <p> In fact, this difference in intensities varies depending upon how much the intensity of the image is changing in the vicinity. We know of no one who has explicitly addressed this problem. Typically, the problem is alleviated either by working at subpixel resolution <ref> [2, 11] </ref> or by adding robustness through window-based matching [7, 9, 6]. Instead, we propose to use the linearly interpolated intensity functions surrounding two pixels to measure their dissimilarity, in a method that is provably insensitive to sampling. <p> Therefore, there must be some j &gt; i such that (x j ; y i+1 ) is a match. 17 18 Chapter 4 Searching for the Optimal Match Sequence Thanks to the structure of the cost function (Equation 3.1), the technique of dynamic programming (also used in [1], [12], <ref> [2] </ref>, and [7]), can be used to find the optimal match sequence by searching over all the match sequences that satisfy the constraints given in Section 3.2. the number of pixels in each scanline and is the maximum disparity allowed.) Because of constraint C3, many of the cells in the grid <p> However, minimizing such a function in a computationally efficient manner is not a straightforward task. In the extension from 1D to 2D, it is not uncommon for the computing time to increase by 600% or more <ref> [2, 12] </ref>. As a result, some approaches avoid the extension altogether [7, 8]. Moreover, we cannot rely on techniques that look only at pairs [1] or triplets [2] of adjacent scanlines at a time, since our initial disparity maps contain errors over large regions. <p> In the extension from 1D to 2D, it is not uncommon for the computing time to increase by 600% or more [2, 12]. As a result, some approaches avoid the extension altogether [7, 8]. Moreover, we cannot rely on techniques that look only at pairs [1] or triplets <ref> [2] </ref> of adjacent scanlines at a time, since our initial disparity maps contain errors over large regions.
Reference: [3] <author> P. N. Belhumeur. </author> <title> A binocular stereo algorithm for reconstructing sloping, creased, and broken surfaces in the presence of half-occlusion. </title> <booktitle> In Proc. 4th ICCV, </booktitle> <pages> pp. 431-438, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The algorithm also works well on the Graebel box, which straddles two disparities. Although depth discontinuities are falsely declared within the box, they are declared independently for each scanline and therefore exhibit no coherence between scanlines. (For a more principled approach to handling sloping surfaces, see <ref> [3] </ref>.) A3. Intensity gradients accompany depth discontinuities. In general, this assumption holds. One exception occurs in Figure 5.3 along the right edge of the left Clorox bottle, just above the lettering. The algorithm assumes that the triangular wedge belongs to the Simulink box instead.
Reference: [4] <author> D. N. Bhat and S. K. Nayar. </author> <title> Stereo in the presence of specular reflection. </title> <booktitle> In Proc. 5th ICCV, </booktitle> <pages> pp. 1086 - 1092, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Just how restrictive are these assumptions? Overall, we have found A1 to be fairly reasonable (similar to the observation made in [2]), even though real-world objects are often not Lambertian [13]. Specular reflections cause little problem in our images since the baseline is small (but see <ref> [4] </ref> for one approach to handling them). Our experiments suggest that the assumption becomes less valid as the baseline is increased. To remove the effects of different imaging parameters, we obtained our stereo images by translating a single camera in a static scene; using two different cameras may require calibration.
Reference: [5] <author> O. Faugeras. </author> <title> Three-Dimensional Computer Vision. </title> <publisher> The MIT Press: </publisher> <address> Cam-bridge, Massachussetts, </address> <year> 1993, </year> <pages> pp. 178-181. </pages>
Reference-contexts: Assumption A5 seems reasonable, since it is violated only when a thin object close to the viewer occludes a distant surface. In addition, it is worth noting that it can be proved to hold wherever the scene contains a unique surface of nonzero thickness <ref> [5] </ref>.
Reference: [6] <author> P. Fua. </author> <title> Combining stereo and monocular information to compute dense depth maps that preserve depth discontinuities. </title> <booktitle> In Proc. 12th IJCAI, </booktitle> <pages> pp. 1292-1298, </pages> <year> 1991. </year>
Reference-contexts: Because our algorithm establishes full correspondence, it is a stereo system in its own right, despite its specialized purpose. As such, it is similar to other stereo algorithms that incorporate depth discontinuities and occluded regions <ref> [2, 7, 6, 11, 8, 9] </ref>. Three aspects of the algorithm are worth noting. First, it uses a measure of pixel dissimilarity that looks at the linearly interpolated intensity functions surrounding the two pixels, and thus is provably insensitive to sampling effects. <p> In contrast to the computationally expensive solution of matching at subpixel resolution or over windows, our dissimilarity measure solves the sampling problem with only a negligible increase in computation. Most stereo algorithms require texture 1 throughout the image (but see <ref> [6] </ref>, [8], and [1]), the assumption presumably being that untextured regions have no information for matching. However, images quite often contain large untextured regions, particularly in indoor settings. <p> A3. At every depth discontinuity, there is an intensity gradient between the near object and the far object. This assumption (which is similar to the one made in <ref> [6] </ref>) allows us to prohibit depth discontinuities in regions of nearly constant intensity (i.e., untextured regions). A4. Within each scanline, every object contains at least a modest amount of intensity variation. Therefore, we are not hindered by ambiguity in separating two adjacent objects. <p> We know of no one who has explicitly addressed this problem. Typically, the problem is alleviated either by working at subpixel resolution [2, 11] or by adding robustness through window-based matching <ref> [7, 9, 6] </ref>. Instead, we propose to use the linearly interpolated intensity functions surrounding two pixels to measure their dissimilarity, in a method that is provably insensitive to sampling.
Reference: [7] <author> D. Geiger, B. Ladendorf, and A. Yuille. </author> <title> Occlusions and binocular stereo. </title> <journal> IJCV, </journal> <volume> 14(3): </volume> <pages> 211-226, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Because our algorithm establishes full correspondence, it is a stereo system in its own right, despite its specialized purpose. As such, it is similar to other stereo algorithms that incorporate depth discontinuities and occluded regions <ref> [2, 7, 6, 11, 8, 9] </ref>. Three aspects of the algorithm are worth noting. First, it uses a measure of pixel dissimilarity that looks at the linearly interpolated intensity functions surrounding the two pixels, and thus is provably insensitive to sampling effects. <p> Although our formulation could easily be extended to subpixel resolution, pixel resolution is sufficient to compute a rough depth map and to detect most depth discontinuities. Other pixel-resolution stereo algorithms are described in <ref> [7] </ref>, [9], [10], and [8]. Correspondence is encoded in a match sequence, which is a sequence of matches. Each match is an ordered pair (x; y) of pixels signifying that the intensities I L (x) and I R (y) are images of the same scene point. <p> The cost function and hard constraints are the subjects of the next two sections. 3.1 Cost function Instead of deriving a maximum a posteriori cost function from a Bayesian formulation (as is done in [2], <ref> [7] </ref>, and [11]), we propose a simple cost function based mainly on intuition and justified solely by empirical evidence. <p> Other researchers have encountered difficulty in using a cost function that adds only a constant occlusion penalty to pixel dissimilarities [8]. Second, as explained in <ref> [7] </ref>, the function must satisfy eff (l 1 ) + eff (l 2 ) eff (l 1 + l 2 ) for all lengths l 1 and l 2 . <p> We know of no one who has explicitly addressed this problem. Typically, the problem is alleviated either by working at subpixel resolution [2, 11] or by adding robustness through window-based matching <ref> [7, 9, 6] </ref>. Instead, we propose to use the linearly interpolated intensity functions surrounding two pixels to measure their dissimilarity, in a method that is provably insensitive to sampling. <p> there must be some j &gt; i such that (x j ; y i+1 ) is a match. 17 18 Chapter 4 Searching for the Optimal Match Sequence Thanks to the structure of the cost function (Equation 3.1), the technique of dynamic programming (also used in [1], [12], [2], and <ref> [7] </ref>), can be used to find the optimal match sequence by searching over all the match sequences that satisfy the constraints given in Section 3.2. the number of pixels in each scanline and is the maximum disparity allowed.) Because of constraint C3, many of the cells in the grid are disallowed; <p> However, minimizing such a function in a computationally efficient manner is not a straightforward task. In the extension from 1D to 2D, it is not uncommon for the computing time to increase by 600% or more [2, 12]. As a result, some approaches avoid the extension altogether <ref> [7, 8] </ref>. Moreover, we cannot rely on techniques that look only at pairs [1] or triplets [2] of adjacent scanlines at a time, since our initial disparity maps contain errors over large regions.
Reference: [8] <author> S. S. Intille and A. F. Bobick. </author> <title> Disparity-space images and large occlusion stereo. </title> <booktitle> In ECCV, </booktitle> <pages> pp. 179-186, </pages> <year> 1994. </year>
Reference-contexts: Because our algorithm establishes full correspondence, it is a stereo system in its own right, despite its specialized purpose. As such, it is similar to other stereo algorithms that incorporate depth discontinuities and occluded regions <ref> [2, 7, 6, 11, 8, 9] </ref>. Three aspects of the algorithm are worth noting. First, it uses a measure of pixel dissimilarity that looks at the linearly interpolated intensity functions surrounding the two pixels, and thus is provably insensitive to sampling effects. <p> In contrast to the computationally expensive solution of matching at subpixel resolution or over windows, our dissimilarity measure solves the sampling problem with only a negligible increase in computation. Most stereo algorithms require texture 1 throughout the image (but see [6], <ref> [8] </ref>, and [1]), the assumption presumably being that untextured regions have no information for matching. However, images quite often contain large untextured regions, particularly in indoor settings. <p> Although our formulation could easily be extended to subpixel resolution, pixel resolution is sufficient to compute a rough depth map and to detect most depth discontinuities. Other pixel-resolution stereo algorithms are described in [7], [9], [10], and <ref> [8] </ref>. Correspondence is encoded in a match sequence, which is a sequence of matches. Each match is an ordered pair (x; y) of pixels signifying that the intensities I L (x) and I R (y) are images of the same scene point. <p> Other researchers have encountered difficulty in using a cost function that adds only a constant occlusion penalty to pixel dissimilarities <ref> [8] </ref>. Second, as explained in [7], the function must satisfy eff (l 1 ) + eff (l 2 ) eff (l 1 + l 2 ) for all lengths l 1 and l 2 . <p> However, minimizing such a function in a computationally efficient manner is not a straightforward task. In the extension from 1D to 2D, it is not uncommon for the computing time to increase by 600% or more [2, 12]. As a result, some approaches avoid the extension altogether <ref> [7, 8] </ref>. Moreover, we cannot rely on techniques that look only at pairs [1] or triplets [2] of adjacent scanlines at a time, since our initial disparity maps contain errors over large regions.
Reference: [9] <author> D. G. Jones and J. Malik. </author> <title> Computational framework for determining stereo correspondence from a set of linear spatial filters. </title> <journal> Image and Vision Computing, </journal> <volume> 10(10): </volume> <pages> 699-708, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Because our algorithm establishes full correspondence, it is a stereo system in its own right, despite its specialized purpose. As such, it is similar to other stereo algorithms that incorporate depth discontinuities and occluded regions <ref> [2, 7, 6, 11, 8, 9] </ref>. Three aspects of the algorithm are worth noting. First, it uses a measure of pixel dissimilarity that looks at the linearly interpolated intensity functions surrounding the two pixels, and thus is provably insensitive to sampling effects. <p> Although our formulation could easily be extended to subpixel resolution, pixel resolution is sufficient to compute a rough depth map and to detect most depth discontinuities. Other pixel-resolution stereo algorithms are described in [7], <ref> [9] </ref>, [10], and [8]. Correspondence is encoded in a match sequence, which is a sequence of matches. Each match is an ordered pair (x; y) of pixels signifying that the intensities I L (x) and I R (y) are images of the same scene point. <p> We know of no one who has explicitly addressed this problem. Typically, the problem is alleviated either by working at subpixel resolution [2, 11] or by adding robustness through window-based matching <ref> [7, 9, 6] </ref>. Instead, we propose to use the linearly interpolated intensity functions surrounding two pixels to measure their dissimilarity, in a method that is provably insensitive to sampling.
Reference: [10] <author> J. J. Little and W. E. Gillett. </author> <title> Direct evidence for occlusion in stereo and motion. </title> <journal> Image and Vision Computing, </journal> <volume> 8(4): </volume> <pages> 328-340, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Although our formulation could easily be extended to subpixel resolution, pixel resolution is sufficient to compute a rough depth map and to detect most depth discontinuities. Other pixel-resolution stereo algorithms are described in [7], [9], <ref> [10] </ref>, and [8]. Correspondence is encoded in a match sequence, which is a sequence of matches. Each match is an ordered pair (x; y) of pixels signifying that the intensities I L (x) and I R (y) are images of the same scene point.
Reference: [11] <author> A. Luo and H. Burkhardt. </author> <title> An intensity-based cooperative bidirectional stereo matching with simultaneous detection of discontinuities and occlusions. </title> <journal> IJCV, </journal> <volume> 15(3) </volume> <pages> 171-188, </pages> <month> July </month> <year> 1995. </year> <month> 51 </month>
Reference-contexts: Because our algorithm establishes full correspondence, it is a stereo system in its own right, despite its specialized purpose. As such, it is similar to other stereo algorithms that incorporate depth discontinuities and occluded regions <ref> [2, 7, 6, 11, 8, 9] </ref>. Three aspects of the algorithm are worth noting. First, it uses a measure of pixel dissimilarity that looks at the linearly interpolated intensity functions surrounding the two pixels, and thus is provably insensitive to sampling effects. <p> The cost function and hard constraints are the subjects of the next two sections. 3.1 Cost function Instead of deriving a maximum a posteriori cost function from a Bayesian formulation (as is done in [2], [7], and <ref> [11] </ref>), we propose a simple cost function based mainly on intuition and justified solely by empirical evidence. <p> In fact, this difference in intensities varies depending upon how much the intensity of the image is changing in the vicinity. We know of no one who has explicitly addressed this problem. Typically, the problem is alleviated either by working at subpixel resolution <ref> [2, 11] </ref> or by adding robustness through window-based matching [7, 9, 6]. Instead, we propose to use the linearly interpolated intensity functions surrounding two pixels to measure their dissimilarity, in a method that is provably insensitive to sampling.
Reference: [12] <author> Y. Ohta and T. Kanade. </author> <title> Stereo by intra- and inter-scanline search using dynamic programming. </title> <journal> IEEE Trans. PAMI, </journal> <volume> PAMI-7(2): </volume> <pages> 139-154, </pages> <month> March </month> <year> 1985. </year>
Reference-contexts: Therefore, there must be some j &gt; i such that (x j ; y i+1 ) is a match. 17 18 Chapter 4 Searching for the Optimal Match Sequence Thanks to the structure of the cost function (Equation 3.1), the technique of dynamic programming (also used in [1], <ref> [12] </ref>, [2], and [7]), can be used to find the optimal match sequence by searching over all the match sequences that satisfy the constraints given in Section 3.2. the number of pixels in each scanline and is the maximum disparity allowed.) Because of constraint C3, many of the cells in the <p> However, minimizing such a function in a computationally efficient manner is not a straightforward task. In the extension from 1D to 2D, it is not uncommon for the computing time to increase by 600% or more <ref> [2, 12] </ref>. As a result, some approaches avoid the extension altogether [7, 8]. Moreover, we cannot rely on techniques that look only at pairs [1] or triplets [2] of adjacent scanlines at a time, since our initial disparity maps contain errors over large regions.
Reference: [13] <author> M. Oren and S. K. Nayar. </author> <title> Generalization of the Lambertian model and implications for machine vision. </title> <journal> IJCV, </journal> <volume> 14(3): </volume> <pages> 227-252, </pages> <month> April </month> <year> 1995. </year> <month> 52 </month>
Reference-contexts: Therefore, the depths of the objects can be determined by triangulation. Just how restrictive are these assumptions? Overall, we have found A1 to be fairly reasonable (similar to the observation made in [2]), even though real-world objects are often not Lambertian <ref> [13] </ref>. Specular reflections cause little problem in our images since the baseline is small (but see [4] for one approach to handling them). Our experiments suggest that the assumption becomes less valid as the baseline is increased.
References-found: 13

