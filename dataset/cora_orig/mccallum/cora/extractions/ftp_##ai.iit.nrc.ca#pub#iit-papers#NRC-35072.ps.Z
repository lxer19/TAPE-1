URL: ftp://ai.iit.nrc.ca/pub/iit-papers/NRC-35072.ps.Z
Refering-URL: http://ai.iit.nrc.ca/cgi-bin/ftpsearch/?turney
Root-URL: 
Email: peter@ai.iit.nrc.ca  
Title: A Theory of Cross-Validation Error A Theory of Cross-Validation Error Running Head: A Theory of
Author: Peter Turney 
Address: Ottawa, Ontario, Canada K1A 0R6  
Affiliation: Knowledge Systems Laboratory Institute for Information Technology National Research Council Canada  
Date: June 17, 1993 1  
Note: Submitted to the Journal of Experimental and Theoretical Artificial Intelligence  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aha, D.W., Kibler, D. </author> <title> (1989) Noise-tolerant instance-based learning algorithms, </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> 794-799. </pages>
Reference-contexts: Thus the weights will be nearly identical. This suggests that we will usually be closer to the lower bound . It is common practice in instance-based learning to reduce the size of the model by eliminating redundant instances <ref> (Aha & Kibler, 1989) </ref>. Let us consider how this practice affects the stability of the model. For simplicity, let us consider a variation on the basic algorithm , rather than a variation on the more complex algorithm . Let us use to refer to this new algorithm.
Reference: <author> Aha, D.W., Kibler, D., & Albert, </author> <title> M.K. (1991) Instance-based learning algorithms, </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66. </pages>
Reference-contexts: The prediction for the output is , the element of that corresponds to the row vector (Kibler et al., 1989). This describes instance-based learning for predicting real-valued attributes. It is easy to see how the same procedure could be used for predicting discrete, symbolic attributes <ref> (Aha et al., 1991) </ref>. Instance-based learning is a form of nearest neighbor pattern recognition (Dasarathy, 1991). There are many ways that one might choose to measure the similarity between two vectors. Let us assume only that we are using a reasonable measure of similarity.
Reference: <author> Akaike, H. </author> <title> (1970) Statistical predictor identification, </title> <journal> Annals of the Institute of Statistical Mathematics, </journal> <volume> 22 </volume> <pages> 203-217. </pages>
Reference: <author> Akaike, H. </author> <title> (1973) Information theory and an extension of the maximum likelihood principle, </title> <booktitle> Second International Symposium on Information Theory, edited by B.N. </booktitle>
Reference: <editor> Petrov and F. </editor> <address> Csaki (Budapest: </address> <publisher> Akademia Kiado). </publisher>
Reference: <author> Akaike, H. </author> <title> (1974) A new look at the statistical model identification, </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-19: </volume> <pages> 716-723. </pages>
Reference: <author> Barron, </author> <title> A.R. (1984) Predicted squared error: a criterion for automatic model selection, in Self-organizing Methods in Modeling: GMDH Type Algorithms, edited by S.J. </title> <address> Farlow (New York: </address> <publisher> Marcel Dekker). </publisher>
Reference: <author> Dasarathy, </author> <title> B.V. (1991) Nearest Neighbor Pattern Classification Techniques, Edited collection (California: </title> <publisher> IEEE Press). </publisher>
Reference-contexts: This describes instance-based learning for predicting real-valued attributes. It is easy to see how the same procedure could be used for predicting discrete, symbolic attributes (Aha et al., 1991). Instance-based learning is a form of nearest neighbor pattern recognition <ref> (Dasarathy, 1991) </ref>. There are many ways that one might choose to measure the similarity between two vectors. Let us assume only that we are using a reasonable measure of similarity. <p> Let be weights such that: (125) Let be the k elements in that correspond to the k row vectors . The predicted output is: (126) Clearly equals when . The parameter k is called the size of the neighborhood in nearest neighbor pattern recognition <ref> (Dasarathy, 1991) </ref>. <p> This makes particularly suitable for instance-based learning, which is a type of nonparametric analysis <ref> (Dasarathy, 1991) </ref>. In general, machine learning algorithms tend to be nonparametric (in the statistical sense of parametric), thus this theory of cross-validation error is particularly suitable for machine learning.
Reference: <author> Draper, N.R. & Smith, H. </author> <title> (1981) Applied Regression Analysis, </title> <address> Second Edition (New York: </address> <publisher> John Wiley & Sons). </publisher>
Reference-contexts: The error on the training set is often taken to be a r 1+ r 1+ r r A Theory of Cross-Validation Error Submitted to the Journal of Experimental and Theoretical Artificial Intelligence June 17, 1993 3 measure of the accuracy of the algorithm <ref> (Draper and Smith, 1981) </ref>. The second component is the instability of the algorithm (Turney, 1990). The instability of the algorithm is the sensitivity of the algorithm to noise in the data. Instability is closely related to our intuitive notion of complexity (Turney, 1990). <p> This theorem is justification for the intuition that good models should be both simple and accurate. It is assumed that a good model is a model that minimizes cross-validation error. After examining cross-validation error in general, Section 3 looks at the accuracy and stability of linear regression <ref> (Draper and Smith, 1981) </ref>, and Section 4 considers instance-based learning (Kibler et al., 1989; Aha et al., 1991). In both cases, it turns out that there is a conict between accuracy and stability. <p> Although our model m must be linear, we can introduce an unlimited number of new inputs, by treating various functions of the given inputs as if they were themselves inputs. The parameter r is called the number of terms in the linear model <ref> (Draper and Smith, 1981) </ref>. Draper and Smith (1981) describe the process of selecting the inputs as follows: Suppose we wish to establish a linear regression equation for a particular response Y in terms of the basic independent or predictor variables . <p> In linear regression, is called the residual <ref> (Draper and Smith, 1981) </ref>. It is well-known that the residual is orthogonal to the column space of X (Strang, 1976). It is clear that lies in the column space of X. Therefore and are orthogonal.
Reference: <author> Ein-Dor, P. & Feldmesser, J. </author> <title> (1987) Attributes of the performance of central processing units: a relative performance prediction model, </title> <journal> Communications of the ACM, </journal> <volume> 30 </volume> <pages> 308-317. </pages>
Reference: <author> Eubank, </author> <title> R.L. (1988) Spline Smoothing and Nonparametric Regression (New York: </title> <publisher> Marcel Dekker). </publisher>
Reference: <author> Fraser, D.A.S. </author> <title> (1976) Probability and Statistics: Theory and Applications (Massachusetts: </title> <publisher> Duxbury Press). </publisher>
Reference-contexts: to the Journal of Experimental and Theoretical Artificial Intelligence June 17, 1993 6 (10) With each repetition k of the n experiments, the n outputs change, because the random noise has changed: (11) Consider the average output of the first m repetitions: (12) By the Weak Law of Large Numbers <ref> (Fraser, 1976) </ref>, as , converges to . This follows from the fact that the mean of the noise is zero. <p> set is: (15) The error on the testing set, the cross-validation error , is: (16) Let us assume that our goal is to minimize the expected length of the cross-validation error vector: (17) T is the matrix transpose operation, is vector length, and is the expectation operator of probability theory <ref> (Fraser, 1976) </ref>. If is a function of a random variable x, where x is a sample from a probability distribution with density , then the expected value of is defined as follows (Fraser, 1976): (18) The expected value of is its mean or average value. <p> (17) T is the matrix transpose operation, is vector length, and is the expectation operator of probability theory <ref> (Fraser, 1976) </ref>. If is a function of a random variable x, where x is a sample from a probability distribution with density , then the expected value of is defined as follows (Fraser, 1976): (18) The expected value of is its mean or average value. Note that the integration in is over both and . It is assumed that the inputs X are the same in the training set and the testing set. <p> Now, consider the squared length of the cross-validation error: (38) Since is a sequence of independent samples from a standardized distribution (mean 0, variance 1), it follows that <ref> (Fraser, 1976) </ref>: (39) Finally, applying Lemma 1 (which is proven immediately after this theorem): (40) Lemma 1: Assume that we have a function of a random variable x, and a constant , such that and . If then . <p> ( ) E t x ( )( )-( ) 2 ( ) 0= 2 2 A Theory of Cross-Validation Error Submitted to the Journal of Experimental and Theoretical Artificial Intelligence June 17, 1993 12 (43) (45) Combining these results, we get: (46) Thus: (47) Lemma 1 is a well-known result <ref> (Fraser, 1976) </ref>. <p> Thus . Since and are sequences of independent samples from a standardized distribution (mean 0, variance 1), it follows that has mean 0 and variance 2 <ref> (Fraser, 1976) </ref>. <p> Since X has r linearly independent column vectors, W has r orthonormal column vectors: (89) The following is a useful property of W <ref> (Fraser, 1976) </ref>: (90) We can express the projection matrix P in terms of W: (91) P has the following properties (Strang, 1976): (92) Consider the term : (93) Now consider the squared length of this term: (94) Hold i fixed for a moment and consider the squared term: (96) We can <p> y i ,( ) E e t ( ) y i X y 1 ,( ) E e t ( ) E e s ( ) s A Theory of Cross-Validation Error Submitted to the Journal of Experimental and Theoretical Artificial Intelligence June 17, 1993 32 apply statistical estimators of <ref> (Fraser, 1976) </ref>. Of course, some combination of these approaches may be fruitful. When we are using a modeling technique, such as neural networks, where we do not (yet) have theorems like those of Sections 3 and 4 to guide us, it may be more difficult to estimate . <p> We assume that the random noise is independent of . The best that we can do is to guess the mean of the noise. This follows from the Cramr-Rao ine quality <ref> (Fraser, 1976) </ref>. 3. The theorems here often mention both the expected squared length (e.g. ) and the expected length (e.g. ). This is because the relation between expected squared length and expected length is not simple, as we can see from Lemma 1.
Reference: <author> Geman, S., Bienenstock, E., & Doursat, R. </author> <title> (1992) Neural networks and the bias/variance dilemma, </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58. </pages>
Reference-contexts: More generally, the work here is related to the view that selecting a model involves a trade-off between bias and variance (Barron, 1984; Eubank, 1988; Geman et al., 1992; Moody, 1991, 1992). Bias and variance are defined as follows <ref> (Geman et al., 1992) </ref>: (203) The bias of the model for the value is the square of the difference between the expected output of the model and the expected value of , given . <p> It is possible to prove that the expected value of the square of the difference between the output of the model and the expected value of given is the sum of the bias and the variance for <ref> (Geman et al., 1992) </ref>: (206) This is analogous to the relationship between expected cross-validation error, expected training set error, and expected instability (Theorems 7 and 12): (207) Expected training set error is related to bias and expected instability is related to variance. <p> variance for <ref> (Geman et al., 1992) </ref>: (206) This is analogous to the relationship between expected cross-validation error, expected training set error, and expected instability (Theorems 7 and 12): (207) Expected training set error is related to bias and expected instability is related to variance. Bias and variance, as defined by (Geman et al., 1992), are difficult to use in practice, for several reasons: 1. To calculate the expectation requires a probability distribution over and . In most applications, the probability distribution will not be known.
Reference: <author> Kibler, D., Aha, D.W., & Albert, </author> <title> M.K. (1989) Instance-based prediction of real-valued attributes, </title> <journal> Computational Intelligence, </journal> <volume> 5 </volume> <pages> 51-57. </pages>
Reference-contexts: The theoretical discussion is followed by Section 5, which considers the practical application of the theory. Section 5 presents techniques for estimating accuracy and stability. The theory is then applied to an empirical comparison of instance-based learning and linear regression <ref> (Kibler et al. 1989) </ref>. It is concluded that the domain will determine whether instance-based learning or linear regression is superior. Section 6 presents an example of fitting data with linear regression and instance-based learning. <p> The prediction for the output is , the element of that corresponds to the row vector <ref> (Kibler et al., 1989) </ref>. This describes instance-based learning for predicting real-valued attributes. It is easy to see how the same procedure could be used for predicting discrete, symbolic attributes (Aha et al., 1991). Instance-based learning is a form of nearest neighbor pattern recognition (Dasarathy, 1991). <p> Thus the weights will be nearly identical. This suggests that we will usually be closer to the lower bound . It is common practice in instance-based learning to reduce the size of the model by eliminating redundant instances <ref> (Aha & Kibler, 1989) </ref>. Let us consider how this practice affects the stability of the model. For simplicity, let us consider a variation on the basic algorithm , rather than a variation on the more complex algorithm . Let us use to refer to this new algorithm. <p> Kibler et al. (1989) report the results of an empirical comparison of linear regression and instance-based learning. The form of instance-based learning that they used was essentially what is called here . Let us consider their Experiment 1. In Experiment 1, , (Ein-Dor and Feldmesser, 1987), and <ref> (Kibler et al., 1989) </ref>. Therefore . From Theorems 6 and 10, we see that linear regression will be more stable, for these values of n, r, and k, than instance-based learning. Kibler et al. (1989) did not use cross-validation testing, but they did report the error on the training set. <p> Let us now apply instance-based learning. Let us consider four different instance-based learning models, . All four models use the same inputs X: The model is essentially the model of Section 4, with . The four instance-based learning models use the following measure of similarity <ref> (Kibler et al., 1989) </ref>: P i X i X i X i ( ) X i = i 1 4, ,= m LRi i 1 r i= x-axis 0.0 0.4 0.8 y-axis r = 1 r = 3 m IBL1 m IBL2 m IBL3 m IBL4 , , , x 1
Reference: <author> Moody, </author> <title> J.E. (1991) Note on generalization, regularization, and architecture selection in nonlinear learning systems, </title> <booktitle> First IEEE-SP Workshop on Neural Networks for Signal Processing (California: </booktitle> <publisher> IEEE Press). </publisher>
References-found: 15

