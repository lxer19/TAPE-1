URL: http://polaris.cs.uiuc.edu/reports/1178.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Toward a Methodology of Optimizing Programs for High-Performance Computers  
Author: Rudolf Eigenmann 
Address: USA  
Affiliation: for Supercomputing Research and Development (CSRD) University of Illinois at Urbana-Champaign,  
Abstract: This report describes experiences in porting the Perfect Benchmarks R fl programs to the Alliant FX/8 and Cedar machines. Although there is much to learn before we can really say we know how to optimize programs successfully for parallel computers in general and for hierarchical shared-memory architectures in particular, it is hoped that this report will be helpful to those who are beginning to port and transform programs for parallel machines. Perhaps it is even an interesting reference for experts. In addition, this paper summarizes our experience with the programming tools we used and discusses tools that could have helped to increase the productivity of the optimization process. This information is believed to be a useful basis for future tool development projects. 
Abstract-found: 1
Intro-found: 1
Reference: [BE92] <author> William Blume and Rudolf Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks T M Programs. </title> <journal> IEEE Transactions of Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> Novem-ber </month> <year> 1992. </year>
Reference-contexts: Jay Hoeflinger, Greg Jaxon, Zhiyuan Li, and David Padua participated in the program optimization effort, which is described in [EHLP92, EHJ + 91]. Bill Blume wrote the original version of the instrumentation tool, optimized the ADM code, and provided many facts about available compilers <ref> [BE92] </ref>. Jay Hoeflinger wrote a library that reduces the traces at runtime, recording minimum, average, maximum, and accumulated timings. Patrick McClaughry provided tools that really helped our effort [EM93].
Reference: [CKPK90] <author> George Cybenko, Lyle Kipp, Lynn Pointer, and David Kuck. </author> <title> Supercomputer Performance Evaluation and the Perfect Benchmarks T M . Proceedings of ICS, </title> <address> Am-sterdam, Netherlands, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: How much potential for performance improvement is there when optimizing at the program level? Table 1 shows the Perfect Benchmarks R fl programs and their execution times from automatic parallelization, program-level optimization, and the best time in the Perfect Benchmarks reports <ref> [CKPK90] </ref> for the Alliant FX/80 machine as well as for the Cedar machine. For most programs, program-level optimization achieved a significant speed improvement over the automatically paral-lelized version.
Reference: [CY91] <author> Ding-Kai Chen and Pen-Chung Yew. </author> <title> An Empirical Study of DOACROSS Loops. </title> <booktitle> Proceedings of Supercomputing'91, </booktitle> <address> Albuquerque, NM, </address> <pages> pages 630-632, </pages> <month> November 18-, </month> <year> 1991. </year>
Reference-contexts: These transformations were reported in [EHLP92, EHJP92]. Section 2.2.4 outlines the most important of these transformations. Is is useful to know the maximum possible speed of a given loop, and tools giving this information are being developed at CSRD <ref> [CY91, PP93] </ref>. They do not directly give the potential that is exploitable by a compiler; rather they indicate the parallelism observed at runtime by performing a thorough data-flow analysis. 3 Nevertheless this information is valuable as a rough es-timate of an upper speed limit.
Reference: [DSB89] <author> J. Dongarra, D. Sorensen, and O. Brewer. </author> <title> Tools and methodology for programming parallel processors. </title> <editor> In M. H. Wright, editor, </editor> <booktitle> Aspects of Computation on Asynchronous Parallel Processors, </booktitle> <pages> pages 125-137. </pages> <publisher> El-sevier Science Publishers B. V. (North-Holland), </publisher> <year> 1989. </year>
Reference-contexts: Usually this is called porting, optimizing, or paralleliz-ing programs. Methods and tools to achieve this objective are complementary to those applied for writing new parallel programs <ref> [DSB89] </ref>. It is not uncommon to port code by studying in detail the problem being solved and then finding parallelism that can be exploited by the machine. In contrast to this application-level approach to porting programs, the method considered in this paper is termed optimization at the program level.
Reference: [EHJ + 91] <author> Rudolf Eigenmann, Jay Hoeflinger, Greg Jaxon, Zhiyuan Li, and David Padua. </author> <title> Restructuring Fortran Programs for Cedar. </title> <booktitle> Proceedings of ICPP'91, </booktitle> <address> St. Charles, IL, </address> <month> 1 </month> <pages> 57-66, </pages> <month> August 12-16, </month> <year> 1991. </year>
Reference-contexts: Acknowledgment This report is based on research and development efforts from many people. Jay Hoeflinger, Greg Jaxon, Zhiyuan Li, and David Padua participated in the program optimization effort, which is described in <ref> [EHLP92, EHJ + 91] </ref>. Bill Blume wrote the original version of the instrumentation tool, optimized the ADM code, and provided many facts about available compilers [BE92]. Jay Hoeflinger wrote a library that reduces the traces at runtime, recording minimum, average, maximum, and accumulated timings.
Reference: [EHJP92] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, and D. Padua. </author> <title> The Cedar Fortran Project. </title> <type> Technical Report 1262, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Su-percomp. R&D, </institution> <year> 1992. </year>
Reference-contexts: The numbers show performance improvements over the serial program execution. (See <ref> [EHJP92] </ref> for details on used compilers and applied program transformations.) This paper is based on experiments that originate from a research effort to identify program transformations that can be automated in parallelizing compilers. The intent was to apply and measure manual changes to the Perfect Benchmarks suite. <p> The intent was to apply and measure manual changes to the Perfect Benchmarks suite. These experiments have been described more detailed in <ref> [EHLP92, EHJP92] </ref>. The effort led beyond the original goals of the project in that for some of these programs the best performance reported in the Perfect Club was achieved, as shown in Table 1. <p> The provision of more detailed compilation reports of applied and failed transformations is another desirable feature of future restructurers. The catalog of transformation techniques that have been successful in our optimization efforts, is one of the most helpful sources of information. These transformations were reported in <ref> [EHLP92, EHJP92] </ref>. Section 2.2.4 outlines the most important of these transformations. Is is useful to know the maximum possible speed of a given loop, and tools giving this information are being developed at CSRD [CY91, PP93].
Reference: [EHLP92] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the Automatic Paral-lelization of Four Perfect-Benchmarks Programs. </title> <booktitle> Lecture Notes in Computer Science 589, </booktitle> <publisher> Springer Verlag, NY, </publisher> <pages> pages 65-83, </pages> <year> 1992. </year>
Reference-contexts: The intent was to apply and measure manual changes to the Perfect Benchmarks suite. These experiments have been described more detailed in <ref> [EHLP92, EHJP92] </ref>. The effort led beyond the original goals of the project in that for some of these programs the best performance reported in the Perfect Club was achieved, as shown in Table 1. <p> The provision of more detailed compilation reports of applied and failed transformations is another desirable feature of future restructurers. The catalog of transformation techniques that have been successful in our optimization efforts, is one of the most helpful sources of information. These transformations were reported in <ref> [EHLP92, EHJP92] </ref>. Section 2.2.4 outlines the most important of these transformations. Is is useful to know the maximum possible speed of a given loop, and tools giving this information are being developed at CSRD [CY91, PP93]. <p> Acknowledgment This report is based on research and development efforts from many people. Jay Hoeflinger, Greg Jaxon, Zhiyuan Li, and David Padua participated in the program optimization effort, which is described in <ref> [EHLP92, EHJ + 91] </ref>. Bill Blume wrote the original version of the instrumentation tool, optimized the ADM code, and provided many facts about available compilers [BE92]. Jay Hoeflinger wrote a library that reduces the traces at runtime, recording minimum, average, maximum, and accumulated timings.
Reference: [EM91] <author> Perry Emrath and Bret Marsolf. </author> <title> mdb Xylem Parallel Debugger User's Guide. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> December </month> <year> 1991. </year> <note> CSRD Report No. 1180. </note>
Reference-contexts: As can be expected, conventional debugging methods were used also. Sometimes the simple and time-consuming way of inserting print statements had to be chosen. A low-level debugger built into the runtime system <ref> [EM91] </ref> was of further help. It could indicate the error address which, together with some utilities, pointed to the Fortran source line.
Reference: [EM93] <author> Rudolf Eigenmann and Patrick McClaughry. </author> <title> Practical Tools for Optimizing Parallel Programs. </title> <booktitle> Presented at the 1993 SCS Multiconference, </booktitle> <address> Arlington, VA, March 27 - April 1, </address> <year> 1993. </year>
Reference-contexts: In fact, an effort to address the tool questions raised has already been initiated and is described in <ref> [EM93] </ref>. The experience from early tool prototypes of this project is included in the present report. <p> Bill Blume wrote the original version of the instrumentation tool, optimized the ADM code, and provided many facts about available compilers [BE92]. Jay Hoeflinger wrote a library that reduces the traces at runtime, recording minimum, average, maximum, and accumulated timings. Patrick McClaughry provided tools that really helped our effort <ref> [EM93] </ref>.
Reference: [GJT + 91] <author> K. Gallivan, W. Jalby, S. Turner, A. Veidenbaum, and H. Wijshoff. </author> <title> Preliminary Basic Performance Analysis of the Cedar Multiprocessor Memory Systems. </title> <booktitle> Proceedings of ICPP'91, </booktitle> <address> St. Charles, IL, I:71-75, </address> <month> August 12-16, </month> <year> 1991. </year>
Reference-contexts: Several compiler and hardware details are useful to know in this context: Prefetched data accesses from global memory can be as fast as from cluster cache under ideal circumstances, such as long vectors and best stride. In <ref> [GJT + 91] </ref>, best performance was measured for a stride of 16. Prefetch lengths of 512 are ideal. Up to 10% performance loss was measured for prefetch lengths of 32, which is what the compiler generates. There are circumstances that may stop a prefetch operation at runtime.
Reference: [Hoe91] <author> Jay Hoeflinger. </author> <title> Cedar Fortran Programmer's Handbook. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> October </month> <year> 1991. </year> <note> CSRD Report No. 1157. </note>
Reference-contexts: The language used in the project described here is Cedar Fortran <ref> [Hoe91] </ref>, a parallel Fortran dialect developed at CSRD. <p> In the one-cluster optimization the outer loop was likely to be the concurrent loop, which performed less well than the second loop. So now, when the one-cluster execution of the cross-cluster variant is 4. sdoall,xdoall,cdoall, and cdoacross are keywords of the Cedar Fortran language <ref> [Hoe91] </ref> 8 compared with the one-cluster optimized version, there is a resulting speed improvement. Negative spreading overhead can also appear as a result of optimizations applied to the cross-cluster program.
Reference: [ME91] <author> Ulrike Meier and Rudolf Eigenmann. </author> <title> Parallelization and Performance of Conjugate Gradient Algorithms on the Cedar Hierarchical-Memory Multiprocessor. </title> <booktitle> Proceedings of the 3rd ACM Sigplan Symp. on Principles and Practice of Parallel Programming, Williams-burg, VA, </booktitle> <pages> pages 178-188, </pages> <month> April 21-24, </month> <year> 1991. </year>
Reference-contexts: In addition to localizations that are limited to the scope of one loop, there may be opportunities for placing data in cluster memory across the execution of multiple loops. This is a very important transformation; however, there are no general recipes available. <ref> [ME91] </ref> describes the data localization techniques applied to the Conjugate Gradient algorithm. Strategies for data partitioning and distribution are currently being investigated in many related research groups. Spreading Overhead Discussion. XO values can range up to a few 100%.
Reference: [Poi90] <author> Lynn Pointer. </author> <title> Perfect: Performance Evaluation for Cost-Effective Transformations Report 2. </title> <type> Technical report, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Res & Dev, </institution> <month> March </month> <year> 1990. </year> <note> CSRD Report No. 964. </note>
Reference-contexts: This optimization effort, made at CSRD, led to significant speed improvements as shown in Figure 1. For many codes the gained performance is the best reported in the Perfect database <ref> [Poi90] </ref> for the Alliant FX/80 machine. The intent of this paper is to exploit this success for two purposes.
Reference: [PP93] <author> Paul M. Petersen and David A. Padua. </author> <title> Static and Dynamic Evaluation of Data Dependence Analysis. </title> <booktitle> In Proc. of ICS'93, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: These transformations were reported in [EHLP92, EHJP92]. Section 2.2.4 outlines the most important of these transformations. Is is useful to know the maximum possible speed of a given loop, and tools giving this information are being developed at CSRD <ref> [CY91, PP93] </ref>. They do not directly give the potential that is exploitable by a compiler; rather they indicate the parallelism observed at runtime by performing a thorough data-flow analysis. 3 Nevertheless this information is valuable as a rough es-timate of an upper speed limit.
Reference: [SBSS + 92] <author> S. Sharma, R. Bramley, P. Sinvahl-Sharma, J. Bruner, and G. Cybenko. P3S: </author> <title> Portable, Parallel Program Performance Evaluation System. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> September </month> <year> 1992. </year> <note> CSRD Report No. 1170. </note>
Reference-contexts: It provides the user with a subroutine-level timing profile. Since this profile is too coarse for loop-level analysis, it motivated the design of 2 the new tool mentioned above. Compatibility between related tools is certainly very desirable. Notable complementary tool efforts have also been made at CSRD <ref> [TJC91, SBSS + 92] </ref>. 2.2 Manual Restructuring 2.2.1 Alternative ways of improving program performance There are three different ways of restructuring a program.
Reference: [Sta91] <author> CSRD Staff. </author> <title> The Cedar Project. </title> <booktitle> Proceedings of ICPP'91, </booktitle> <address> St. Charles, IL, </address> <month> August 12-16, </month> <year> 1991. </year>
Reference-contexts: 1 Introduction This paper describes the process of optimizing the Perfect Benchmarks R fl programs for the Alliant FX/8 and the Cedar multiprocessor <ref> [Sta91] </ref>. This optimization effort, made at CSRD, led to significant speed improvements as shown in Figure 1. For many codes the gained performance is the best reported in the Perfect database [Poi90] for the Alliant FX/80 machine. The intent of this paper is to exploit this success for two purposes.
Reference: [TJC91] <author> Allan Tuchman, David Jablonowski, and George Cy-benko. </author> <title> Run-Time Visualization of Program Data. </title> <booktitle> Proceedings of Visualization '91, </booktitle> <address> San Diego, CA, Oc-tober -25, </address> <year> 1991. </year> <month> 10 </month>
Reference-contexts: It provides the user with a subroutine-level timing profile. Since this profile is too coarse for loop-level analysis, it motivated the design of 2 the new tool mentioned above. Compatibility between related tools is certainly very desirable. Notable complementary tool efforts have also been made at CSRD <ref> [TJC91, SBSS + 92] </ref>. 2.2 Manual Restructuring 2.2.1 Alternative ways of improving program performance There are three different ways of restructuring a program.
References-found: 17

