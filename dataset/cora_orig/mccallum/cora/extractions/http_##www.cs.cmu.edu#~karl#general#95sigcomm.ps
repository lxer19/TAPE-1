URL: http://www.cs.cmu.edu/~karl/general/95sigcomm.ps
Refering-URL: http://www.cs.cmu.edu/~karl/
Root-URL: 
Title: Software Support for Outboard Buffering and Checksumming  
Author: Karl Kleinpaste, Peter Steenkiste, Brian Zill 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Data copying and checksumming are the most expensive operations when doing high-bandwidth network IO over a high-speed network. Under some conditions, outboard buffering and checksumming can eliminate accesses to the data, thus making communication less expensive and faster. One of the scenarios in which outboard buffering pays off is the common case of applications accessing the network using the Berkeley sockets interface and the Internet protocol stack. In this paper we describe the changes that were made to a BSD protocol stack to make use of a network adaptor that supports outboard buffering and checksumming. Our goal is not only to achieve single copy communication for application that use sockets, but to also have efficient communication for in-kernel applications and for applications using other networks. Performance measurements show that for large reads and writes the single-copy path through the stack is significantly more efficient than the original implementation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Borman, R. Braden, and V. Jacobson. </author> <title> Tcp extensions for high performance. Request for Comments 1323, </title> <month> May </month> <year> 1992. </year>
Reference-contexts: The OSF/1 protocol stack is based on Net2 BSD and also supports TCP window scaling <ref> [1] </ref>. The network device used is the CAB [20] and the Maximum Transmission Unit (MTU) is 32 KBytes. For all tests, the TCP window size is 512 KBytes.
Reference: [2] <author> Jose Brustoloni. </author> <title> Exposed buffering and subdatagram ow control for ATM LANs. </title> <booktitle> In Proceedings of the 19th Conference on Local Computer Networks, </booktitle> <pages> pages 324334. </pages> <publisher> IEEE, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support [19], and several projects have proposed or implemented network adaptors that include these features. [11, 20, 5, 8]. Alternatively, it is possible to use APIs with share semantics <ref> [6, 7, 2] </ref>. Many applications use sockets for communication, and as a result it is worthwhile to look at how they can be supported efficiently. The adaptor hardware and the host software support needed for a single-copy host interface for sockets have been widely described in the literature.
Reference: [3] <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen. </author> <title> An analysis of tcp processing overhead. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6):2329, </volume> <month> June </month> <year> 1989. </year>
Reference-contexts: 1 Introduction For bulk data transfer over high-speed networks, the sending and receiving hosts typically form the bottleneck, and it is important to minimize the communication overhead to achieve high application-level throughput. The communication cost can be broken up in per-packet and per-byte costs. The per-packet cost can be optimized <ref> [3, 17] </ref>, and for large packets, this overhead is amortized over a lot of data. However, the per-byte cost is not reduced by increasing the packet size.
Reference: [4] <author> Jim Crapuchettes. </author> <title> TURBOchannel Interface ASIC Functional Specification. TRI/ADD Program, </title> <note> DEC, revision 0.6, preliminary edition, </note> <year> 1992. </year>
Reference-contexts: These restrictions stem both from features of the TcIA chip <ref> [4] </ref> that is used as the interface to the Turbochannel, and from the architecture of network memory, which was designed for high-bandwidth streaming of data. Most of these restrictions can be worked around by the host and CAB software, and only have an impact on the efficiency of the DMA. <p> While the CAB hardware is designed for bandwidths up to 300 Mbit/second, the microcode currently limits throughput to less than half of that. The bottleneck is the transfer of data across the Turbochannel, which is managed by the TcIA chip <ref> [4] </ref>. The TcIA architecture and the way the chip is used on the CAB make it very hard to pipeline the DMA engines and to use large burst sizes (larger than 8 words), both 8 .
Reference: [5] <author> Chris Dalton, Greg Watson, David Banks, Costas Calamvokis, Aled Edwards, and John Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network Magazine, </journal> <volume> 7(4):3643, </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support [19], and several projects have proposed or implemented network adaptors that include these features. <ref> [11, 20, 5, 8] </ref>. Alternatively, it is possible to use APIs with share semantics [6, 7, 2]. Many applications use sockets for communication, and as a result it is worthwhile to look at how they can be supported efficiently.
Reference: [6] <author> Peter Druschel and Larry Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proceedings of the Fourteenth Symposium on Operating System Principles, </booktitle> <pages> pages 189202. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1993. </year>
Reference-contexts: For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support [19], and several projects have proposed or implemented network adaptors that include these features. [11, 20, 5, 8]. Alternatively, it is possible to use APIs with share semantics <ref> [6, 7, 2] </ref>. Many applications use sockets for communication, and as a result it is worthwhile to look at how they can be supported efficiently. The adaptor hardware and the host software support needed for a single-copy host interface for sockets have been widely described in the literature. <p> Even though the API still has copy semantics, performance will be best if a limited set of buffers are used for communication, e.g. the usage of the API has share semantics (e.g <ref> [6] </ref>). 4.4.2 Synchronization The copy semantics of the socket interfaces requires that the application is only allowed to continue after a copy has been made of the data (transmit), or after the incoming data is available (receive).
Reference: [7] <author> Peter Druschel, Larry Peterson, and Bruce Davie. </author> <title> Experience with a high-speed network adaptor: A software perspective. </title> <booktitle> In Proceedings of the SIGCOMM 94 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 213. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support [19], and several projects have proposed or implemented network adaptors that include these features. [11, 20, 5, 8]. Alternatively, it is possible to use APIs with share semantics <ref> [6, 7, 2] </ref>. Many applications use sockets for communication, and as a result it is worthwhile to look at how they can be supported efficiently. The adaptor hardware and the host software support needed for a single-copy host interface for sockets have been widely described in the literature.
Reference: [8] <author> Aled Edwards, Greg Watson, John Lumley, David Banks, Costas Calamvokis, and Chris Dalton. </author> <title> User-space protocols deliver high performance to application on a low-cost Gb/s LAN. </title> <booktitle> In Proceedings of the SIG-COMM 94 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 1423. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1994. </year>
Reference-contexts: For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support [19], and several projects have proposed or implemented network adaptors that include these features. <ref> [11, 20, 5, 8] </ref>. Alternatively, it is possible to use APIs with share semantics [6, 7, 2]. Many applications use sockets for communication, and as a result it is worthwhile to look at how they can be supported efficiently.
Reference: [9] <author> Ken Hardwick. </author> <title> Hippi world the switch is the network. </title> <booktitle> In Thirty Seventh IEEE Computer Society International Conference, </booktitle> <pages> pages 234238. </pages> <publisher> IEEE, </publisher> <month> February </month> <year> 1992. </year>
Reference-contexts: Media access control is performed by hardware on the CAB, under control of the host. This component of the CAB is network-specific. Our implementation is for HIPPI <ref> [9] </ref>, which has a line rate of 100 MByte/second. The simplest MAC algorithm for a switch-based network is to send packets in FIFO order.
Reference: [10] <author> M. G. Hluchyj and M.J. Karol. </author> <title> Queueing in high-performance packet switching. </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <volume> 6(9):15871597, </volume> <month> De-cember </month> <year> 1988. </year>
Reference-contexts: Analysis shows that one can utilize at most 58% of the network bandwidth, assuming random traffic <ref> [10] </ref>.
Reference: [11] <author> Van Jacobson. </author> <title> Efficient protocol implementation. </title> <booktitle> ACM 90 SIGCOMM tutorial, </booktitle> <month> September </month> <year> 1990. </year>
Reference-contexts: For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support [19], and several projects have proposed or implemented network adaptors that include these features. <ref> [11, 20, 5, 8] </ref>. Alternatively, it is possible to use APIs with share semantics [6, 7, 2]. Many applications use sockets for communication, and as a result it is worthwhile to look at how they can be supported efficiently.
Reference: [12] <author> Van Jacobson. pbufs. </author> <type> Personal communication, </type> <month> October </month> <year> 1992. </year>
Reference-contexts: However, this would have required more substantial changes to the code, and the data structure would have ended up being fairly similar to external mbufs. In some ways, our UIO mbufs are similar to pbufs <ref> [12] </ref>.
Reference: [13] <author> Samuel J. Lefer, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1989. </year>
Reference-contexts: This model is different from that found in Berke-ley Unix operating systems, where data is channeled through the systems network buffer pool <ref> [13] </ref>. The difference in the models, together with the restriction that data in CAB memory should be formatted into complete packets, means that decisions about partitioning of user data into packets must be made before the data is transferred out of user space.
Reference: [14] <author> J. Postel. </author> <title> Transmission control protocol. Request for Comments 793, </title> <month> September </month> <year> 1981. </year>
Reference-contexts: The host sets the value of S to the length of all the headers (HIPPI and IP), i.e. the hardware calculates the checksum over the user data, and the host is responsible for the fields in the header (the TCP header and pseudo-header <ref> [14] </ref>). While there are many other choices for S, this selection has the advantage that it works correctly when retransmitting data. Specifically, when retransmitting, the host provides a new header, with a new checksum seed.
Reference: [15] <author> K.K. Ramakrishnan. </author> <title> Performance considerations in designing network interfaces. </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <volume> 11(2):203219, </volume> <month> Febru-ary </month> <year> 1993. </year>
Reference-contexts: This means that when a read or write operation is interrupted (say because of an alarm signal), the process will only be allowed to restart after all outstanding DMA operations have completed. 4.4.3 Optimization based on packet size The tradeoff between programmed IO and DMA are well understood (e.g. <ref> [15] </ref>). PIO has typically a higher per-byte overhead while DMA has a higher per-transfer overhead; as a result PIO is typically more efficient for short transfers, and DMA for longer transfers. Since the CAB only supports DMA, this is not an issue.
Reference: [16] <author> Richard F. Rashid, Robert V. Baron, A. Forin, David B. Golub, Michael Jones, Daniel Julin, D. Orr, and R. Sanzi. </author> <title> Mach: A foundation for open systems. </title> <booktitle> In Proceedings of the Second IEEE Workshop on Workstation Operating Systems, </booktitle> <pages> pages 109113, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: In contrast, when the DMA is to or from an application address space, the driver (or kernel) has to perform VM operations on pages in a different address space. Unfortunately, operating systems do not support this in a uniform way. We briey describe implementations for Mach <ref> [16] </ref> and DEC OSF/1, and compare them in terms of complexity and performance. In Mach, the required VM operations can be performed on a different address by specifying the appropriate Mach port [21] for the target address space. <p> In Mach, the required VM operations can be performed on a different address by specifying the appropriate Mach port [21] for the target address space. The invoking process can be the kernel, or in the case of a Mach 3.0 microkernel <ref> [16] </ref>, the Unix server. Using these features, it is possible to do address translation and pinning of pages in the application address space in the driver, i.e. DMA support is localized.
Reference: [17] <author> Peter Steenkiste. </author> <title> Analyzing communication latency using the nectar communication processor. </title> <booktitle> In Proceedings of the SIGCOMM 92 Symposium on Communications Architectures and Protocols, pages 199209, </booktitle> <address> Baltimore, </address> <month> August </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: 1 Introduction For bulk data transfer over high-speed networks, the sending and receiving hosts typically form the bottleneck, and it is important to minimize the communication overhead to achieve high application-level throughput. The communication cost can be broken up in per-packet and per-byte costs. The per-packet cost can be optimized <ref> [3, 17] </ref>, and for large packets, this overhead is amortized over a lot of data. However, the per-byte cost is not reduced by increasing the packet size.
Reference: [18] <author> Peter Steenkiste, Michael Hemy, Todd Mummert, and Brian Zill. </author> <title> Architecture and evaluation of a high-speed networking subsystem for distributed-memory systems. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture. IEEE, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Similar mechanisms are needed for the single-copy and copy+read scenarios for the other adaptor architectures, although the lack of outboard buffering and an API with sharing semantics will simplify the implementation. Another example of a single-copy interface is the Gigabit Nectar HIPPI interface <ref> [18] </ref>, which has hardware support for protocol processing similar to that of the provided by the CAB.
Reference: [19] <author> Peter A. Steenkiste. </author> <title> A systematic approach to host interface design for high-speed networks. </title> <journal> IEEE Computer, </journal> <volume> 26(3):4757, </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: In contrast, most host interfaces in use today copy the data two or three times before it reaches the network. For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support <ref> [19] </ref>, and several projects have proposed or implemented network adaptors that include these features. [11, 20, 5, 8]. Alternatively, it is possible to use APIs with share semantics [6, 7, 2]. <p> a strict requirement (IP does not guarantee in order delivery), frequent reordering of packets could confuse clients or reduce their per formance, for example by triggering retransmits. 6 Applicability to other systems An important question is how closely the single-path optimizations are tied to the details of the CAB architecture. <ref> [19] </ref> presents a taxonomy of host interfaces as a function of 7 PIO programmed IO (with checksum) DMA direct memory access (with checksum) Read_C checksum calculation two copy architecture copy plus checksum single copy architecture Copy_C copy with checksum other API Copy Copy Shared Shared Checksum Header Trailer Header Trailer PIO
Reference: [20] <author> Peter A. Steenkiste, Brian D. Zill, H.T. Kung, Steven J. Schlick, Jim Hughes, Bob Kowalski, and John Mul-laney. </author> <title> A host interface architecture for high-speed networks. </title> <booktitle> In Proceedings of the 4th IFIP Conference on High Performance Networks, </booktitle> <pages> pages A3 116, </pages> <address> Liege, Belgium, </address> <month> December </month> <year> 1992. </year> <title> IFIP, </title> <publisher> Elsevier. </publisher>
Reference-contexts: For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support [19], and several projects have proposed or implemented network adaptors that include these features. <ref> [11, 20, 5, 8] </ref>. Alternatively, it is possible to use APIs with share semantics [6, 7, 2]. Many applications use sockets for communication, and as a result it is worthwhile to look at how they can be supported efficiently. <p> Analysis shows that one can utilize at most 58% of the network bandwidth, assuming random traffic [10]. The CAB uses multiple logical channels, queues of packets with different destinations, to get around this problem <ref> [20] </ref>. 2.2 Host view From the viewpoint of the host system software, the CAB is a large bank of memory accompanied by a means for transferring data into and out of that memory. <p> The OSF/1 protocol stack is based on Net2 BSD and also supports TCP window scaling [1]. The network device used is the CAB <ref> [20] </ref> and the Maximum Transmission Unit (MTU) is 32 KBytes. For all tests, the TCP window size is 512 KBytes. The implementation of the single-copy stack currently supports user-level and in-kernel applications communicating through Ethernet (Section 5) and user-level applications communicating through the CAB.
Reference: [21] <author> Linda Walmer and Mary Thompson. </author> <title> A Programmers Guide to the Mach System Calls. </title> <institution> Carnegie Mellon University, </institution> <year> 1989. </year> <month> 12 </month>
Reference-contexts: Unfortunately, operating systems do not support this in a uniform way. We briey describe implementations for Mach [16] and DEC OSF/1, and compare them in terms of complexity and performance. In Mach, the required VM operations can be performed on a different address by specifying the appropriate Mach port <ref> [21] </ref> for the target address space. The invoking process can be the kernel, or in the case of a Mach 3.0 microkernel [16], the Unix server. Using these features, it is possible to do address translation and pinning of pages in the application address space in the driver, i.e.
References-found: 21

