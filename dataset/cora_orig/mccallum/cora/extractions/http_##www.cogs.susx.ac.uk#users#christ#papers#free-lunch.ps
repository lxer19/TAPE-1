URL: http://www.cogs.susx.ac.uk/users/christ/papers/free-lunch.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)1273 678856  
Title: There is No Free Lunch but the Starter is Cheap: Generalisation from First Principles  
Author: Chris Thornton 
Date: March 17, 1998  
Web: WWW: http://www.cogs.susx.ac.uk/users/cjt  
Address: Brighton BN1 9QH  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: According to Wolpert's no-free-lunch (NFL) theorems [1, 2], gener-alisation in the absence of domain knowledge is necessarily a zero-sum enterprise. Good generalisation performance in one situation is always offset by bad performance in another. Wolpert notes that the theorems do not demonstrate that effective generalisation is a logical impossibility but merely that a learner's bias (or assumption set) is of key importance 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> Wolpert, D. </author> <year> (1996). </year> <title> The existence of a priori distinctions between learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 8, No. </volume> <pages> 7. </pages>
Reference-contexts: 1 Introduction There has been lively controversy over Wolpert's no-free-lunch theorems <ref> [1; 2; 3; 4; 5; 6] </ref> and Schaffer's closely related conservation law [7]. These results show that there is no guaranteed correct way of performing generalisation.
Reference: [3] <author> Wolpert, D. </author> <year> (1995). </year> <title> On overfitting avoidance as bias. </title> <institution> SFI-TR-92-03-5001, Santa Fe Institute. </institution>
Reference-contexts: 1 Introduction There has been lively controversy over Wolpert's no-free-lunch theorems <ref> [1; 2; 3; 4; 5; 6] </ref> and Schaffer's closely related conservation law [7]. These results show that there is no guaranteed correct way of performing generalisation.
Reference: [4] <author> Wolpert, D. </author> <year> (1992). </year> <title> On the connection between in-sample testing and generalization error. </title> <journal> Complex Systems, </journal> <pages> 6 (pp. 47-94). </pages>
Reference-contexts: 1 Introduction There has been lively controversy over Wolpert's no-free-lunch theorems <ref> [1; 2; 3; 4; 5; 6] </ref> and Schaffer's closely related conservation law [7]. These results show that there is no guaranteed correct way of performing generalisation.
Reference: [5] <author> Wolpert, D. </author> <year> (1995). </year> <title> The relationship between PAC, the statistical physics framework, the bayesian framework, and the VC framework. </title> <editor> In D. Wolpert (Ed.), </editor> <title> The Mathematics of Generalization. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: 1 Introduction There has been lively controversy over Wolpert's no-free-lunch theorems <ref> [1; 2; 3; 4; 5; 6] </ref> and Schaffer's closely related conservation law [7]. These results show that there is no guaranteed correct way of performing generalisation.
Reference: [6] <author> Wolpert, D. and Macready, W. </author> <year> (1995). </year> <title> No Free Lunch Theorems for Search. </title> <journal> Unpublished MS. </journal>
Reference-contexts: 1 Introduction There has been lively controversy over Wolpert's no-free-lunch theorems <ref> [1; 2; 3; 4; 5; 6] </ref> and Schaffer's closely related conservation law [7]. These results show that there is no guaranteed correct way of performing generalisation.
Reference: [7] <author> Schaffer, C. </author> <year> (1994). </year> <title> Conservation law for generalization performance. </title> <booktitle> Proceedings of the International Conference on Machine Learning (pp. </booktitle> <pages> 259-265). </pages> <month> July 10th-13th, </month> <institution> Rutgers University, </institution> <address> New Brunswick, New Jersey. </address>
Reference-contexts: 1 Introduction There has been lively controversy over Wolpert's no-free-lunch theorems [1; 2; 3; 4; 5; 6] and Schaffer's closely related conservation law <ref> [7] </ref>. These results show that there is no guaranteed correct way of performing generalisation.
Reference: [8] <author> Hume, D. </author> <title> (1740). A Treatise of Human Nature (second edition). </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: They thus affirm Hume's claim to the effect that the observation of `the frequent conjunction of objects' does not permit the drawing of any particular inference concerning `any object beyond those of which we have had experience' <ref> [8] </ref>. The underlying idea behind these results is easily stated. Let's say we have a particular learning method and we would like to know how well it will generalise on the problems from a specific domain.
Reference: [9] <author> Thrun, S., Bala, J., Bloedorn, E., Bratko, I., Cestnik, B., Cheng, J., De Jong, K., Dzeroski, S., Fisher, D., Fahlman, S., Hamann, R., Kaufman, K., Keller, S., Kononenko, I., Kreuziger, J., Michalski, R., Mitchell, T., Pa-chowicz, P., Reich, Y., Vafaie, H., Van de Welde, W., Wenzel, W., Wnek, J. and Zhang, J. </author> <year> (1991). </year> <title> The MONK's problems a performance comparison of different learning algorithms. </title> <institution> CMU-CS-91-197, School of Computer Science, Carnegie-Mellon University. </institution> <month> 21 </month>
Reference-contexts: He notes that `for many algorithms, no one has even tried to write down that set of [problems] for which their algorithm works well.' [1]. However, it is clear that generalisation methods are capable of performing well in practice across a wide variety of situations <ref> [9] </ref>.
Reference: [10] <author> Holte, R. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine learning, </booktitle> <pages> 3 (pp. 63-91). </pages>
Reference-contexts: In fact, in a recent article 1 Wolpert specifically mentions the requirement to prove that `the non-uniformity in [the problem domain] is well-matched to your ... learning algorithm.' [1, p. 19] 2 From a posting to the `connectionists' mail list. 3 Holte <ref> [10] </ref> has shown that even rather trivial methods may perform well on a wide variety of real-world generalisation problems. In practice, then, it seems as if generalisation methods are often able to `get away with' not being mindful of their biases. <p> Only if the geometric seperability for a particular task is high is this strategy likely to be effective. The geometric seperability values for 16 of the most frequently used Machine Learning datasets <ref> [10] </ref> is tabulated in Table 1. As we expect, in all cases the values are well above zero. The average geometric seperability value is, in fact, 0.85.
Reference: [11] <author> Clark, A. and Thornton, C. </author> <year> (1997). </year> <title> Trading spaces: computation, representation and the limits of uninformed learning. </title> <journal> Behaviour and Brain Sciences, </journal> <pages> 20 (pp. 57-90). </pages> <publisher> Cambridge University Press. </publisher>
Reference-contexts: The present paper aims to respond to this challenge by producing precisely the proof the Wolpert believes is required, i.e., a proof of generic non-uniformity based exclusively on first principles. The proof will use a logical task-analysis of the process of generalisation introduced in <ref> [11, 12] </ref>. This analysis will be reviewed in section 2. Section 3 will investigate the caveats that have to be applied when the analysis is applied to realistic scenarios. Section 4 of the paper will show how the analysis justifies certain a priori assumption regarding generic non-uniformities. <p> Researchers have been familiar with this distinction in the space of possible learning tasks for many years (cf. <ref> [13, 11] </ref>). It is, in fact, common practice to refer to methods specifically intended for use on relational tasks as relational learning methods (cf. [14, 15]).
Reference: [12] <author> Thornton, C. and Clark, A. </author> <title> (Forthcoming). Reading the generalizer's mind. Behaviour and Brain Sciences, </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: The present paper aims to respond to this challenge by producing precisely the proof the Wolpert believes is required, i.e., a proof of generic non-uniformity based exclusively on first principles. The proof will use a logical task-analysis of the process of generalisation introduced in <ref> [11, 12] </ref>. This analysis will be reviewed in section 2. Section 3 will investigate the caveats that have to be applied when the analysis is applied to realistic scenarios. Section 4 of the paper will show how the analysis justifies certain a priori assumption regarding generic non-uniformities.
Reference: [13] <author> Dietterich, T. and Michalski, R. </author> <year> (1983). </year> <title> A comparative review of selected methods for learning from examples. </title> <editor> In R. Michalski, J. Carbonell and T. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Palo Alto: </address> <publisher> Tioga. </publisher>
Reference-contexts: Researchers have been familiar with this distinction in the space of possible learning tasks for many years (cf. <ref> [13, 11] </ref>). It is, in fact, common practice to refer to methods specifically intended for use on relational tasks as relational learning methods (cf. [14, 15]).
Reference: [14] <author> Muggleton, S. (Ed.) </author> <year> (1992). </year> <title> Inductive Logic Programming. </title> <publisher> Academic Press. </publisher>
Reference-contexts: Researchers have been familiar with this distinction in the space of possible learning tasks for many years (cf. [13, 11]). It is, in fact, common practice to refer to methods specifically intended for use on relational tasks as relational learning methods (cf. <ref> [14, 15] </ref>).
Reference: [15] <author> Mitchell, T. </author> <year> (1997). </year> <title> Machine Learning. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: Researchers have been familiar with this distinction in the space of possible learning tasks for many years (cf. [13, 11]). It is, in fact, common practice to refer to methods specifically intended for use on relational tasks as relational learning methods (cf. <ref> [14, 15] </ref>).
Reference: [16] <author> Thornton, C. </author> <year> (1997). </year> <title> Separability is a learner's best friend. </title> <editor> In J.A. Bulli-naria, D.W. Glasspool and G. Houghton (Eds.), </editor> <booktitle> Proceedings of the Fourth Neural Computation and Psychology Workshop: Connectionist Representations (pp. </booktitle> <pages> 40-47). </pages> <address> London: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: We have already noted that the solving of a non-relationally-learnable problem involves exploitation of data clustering. Thus, we expect any such problem to exhibit reasonably well clustered data. One way to measure the degree of clustering in a particular dataset is to compute its geometric seperability <ref> [16] </ref> which is just the proportion of datapoints whose nearest neighbours share the same output classification. geometric seperability (f ) = P n 0 n Here, f is a binary target function, x is the data set, x 0 i is the nearest neighbour of x i and n is the
Reference: [17] <author> Minsky, M. and Papert, S. </author> <year> (1988). </year> <title> Perceptrons: An Introduction to Computational Geometry (expanded edn). </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Geometric seperability is a measure of the degree to which datapoints with the same action cluster together. In some sense, it is a generalisation of the linear-separability concept <ref> [17] </ref>. Although not a boolean measure (i.e., a predicate), geometric seperability can be viewed, like the linear-separability concept, as differentiating tasks which are appropriate for a particular learning strategy. 5 The strategy in this case is non-relational (i.e., similarity-based) learning.
Reference: [18] <author> Breiman, L., Friedman, J., Olshen, R. and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth. </publisher>
Reference: [19] <author> Rumelhart, D. and Zipser, D. </author> <year> (1986). </year> <title> Feature discovery by competitive learning. </title> <editor> In D. Rumelhart, J. </editor> <booktitle> McClelland and the PDP Research Group 22 (Eds.), Parallel Distributed Processing: Explorations in the Microstructures of Cognition. </booktitle> <volume> Vol I (pp. </volume> <pages> 151-193). </pages> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference: [20] <author> Kohonen, T. </author> <year> (1984). </year> <title> Self-organization and Associative Memory. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference: [21] <author> Diday, E. and Simon, J. </author> <year> (1980). </year> <title> Clustering analysis. </title> <editor> In K. Fu (Ed.), </editor> <title> Digital Pattern Recognition. </title> <journal> Communications and Cybernetics, </journal> <volume> No. </volume> <pages> 10 (pp. 47-92). </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: [22] <author> Langley, P. </author> <year> (1977). </year> <title> Rediscovering physics with bacon-3. </title> <booktitle> Proceedings of the Fifth International Joint Conference on Artificial Intelligence: Vol I. </booktitle>
Reference: [23] <author> Langley, P. </author> <year> (1978). </year> <title> BACON.1: a general discovery system. </title> <booktitle> Proceedings of the Second National Conference of the Canadian Society for Computational Studies in Intelligence (pp. </booktitle> <pages> 173-180). </pages> <address> Toronto. </address>
Reference: [24] <author> Langley, P., Bradshaw, G. and Simon, H. </author> <year> (1983). </year> <title> Rediscovering chemistry with the BACON system. </title> <editor> In R. Michalski, J. Carbonell and T. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (pp. </booktitle> <pages> 307-329). </pages> <address> Palo Alto: </address> <publisher> Tioga. </publisher>
Reference: [25] <author> Langley, P., Simon, H., Bradshaw, G. and Zytkow, J. </author> <year> (1987). </year> <title> Scientific Discovery: Computational Explorations of the Creative Processes. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference: [26] <author> Wolff, J. </author> <year> (1978). </year> <title> Grammar discovery as data compression. </title> <booktitle> Proceedings of the AISB/GI conference on Artificial Intelligence (pp. </booktitle> <pages> 375-379). </pages> <address> Hamburg. </address>
Reference: [27] <author> Wolff, J. </author> <year> (1980). </year> <title> Data compression, generalisation and overgeneralisation in an evolving theory of language development. </title> <booktitle> Proceedings of the AISB-80 conference on Artificial Intelligence. </booktitle> <address> Amsterdam. </address> <month> 23 </month>
Reference: [28] <author> Lenat, D. </author> <year> (1982). </year> <title> AM: discovery in mathematics as heuristic search. </title> <editor> In R. Davis and D.B. Lenat (Eds.), </editor> <booktitle> Knowledge-Based Systems in Artificial Intelligence (pp. </booktitle> <pages> 1-225). </pages> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: [29] <author> Wnek, J. and Michalski, R. </author> <year> (1994). </year> <title> Hypothesis-driven constructive induction in AQ17-HCI: a method and experiments. </title> <booktitle> Machine Learning, </booktitle> <address> 14 (p. 139). Boston: </address> <publisher> Kluwer Academic Publishers. </publisher> <pages> 24 </pages>
References-found: 28

