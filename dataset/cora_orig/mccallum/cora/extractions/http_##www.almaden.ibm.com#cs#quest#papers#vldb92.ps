URL: http://www.almaden.ibm.com/cs/quest/papers/vldb92.ps
Refering-URL: http://www.almaden.ibm.com/cs/quest/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: An Interval Classifier for Database Mining Applications  
Author: Rakesh Agrawal Sakti Ghosh Tomasz Imielinski Bala Iyer Arun Swami 
Date: 1992  
Note: Current address:  Proceedings of the 18th VLDB Conference  
Address: 650 Harry Road, San Jose, CA 95120  NJ 08903  Canada  
Affiliation: IBM Almaden Research Center  Computer Science Department, Rutgers University,  Vancouver, British Columbia,  
Abstract: We are given a large population database that contains information about population instances. The population is known to comprise of m groups, but the population instances are not labeled with the group identification. Also given is a population sample (much smaller than the population but representative of it) in which the group labels of the instances are known. We present an interval classifier (IC) which generates a classification function for each group that can be used to efficiently retrieve all instances of the specified group from the population database. To allow IC to be embedded in interactive loops to answer adhoc queries about attributes with missing values, IC has been designed to be efficient in the generation of classification functions. Preliminary experimental results indicate that IC not only has retrieval and classifier generation efficiency advantages, but also compares favorably in the classification accuracy with current tree classifiers, such as ID3, which were primarily designed for minimizing classification errors. We also describe some new applications that arise from encapsulating the classification capability in database systems and discuss extensions to IC for it to be used in these new application domains. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dina Bitton, David J. DeWitt, and Carolyn Tur-byfill, </author> <title> "Benchmarking Database Systems: A Systematic Approach", </title> <booktitle> VLDB 83 , Florence, </booktitle> <address> Italy, </address> <year> 1983. </year>
Reference-contexts: Since elevel is a categorical attribute, each of its values is considered to be an interval and we have the following intervals: Attribute: elevel Interval Winner Strength [0] Group A Strong <ref> [1] </ref> Group A Strong [2] Group B Strong [3] Group B Strong [4] Group B Strong Since all the intervals are strong, the tree is not grown further. If all or some of the intervals were weak, the algorithm would develop the tree further for those intervals. <p> Our approach and the benchmarks we are developing allows one to systematically explore various operating regions. Our hope is that these benchmarks will serve the same role in classifier performance evaluation as the Wisconsin Benchmarks <ref> [1] </ref> played in the evaluation of relational query processing strategies. The work reported in this paper has been done in the context of the Quest project at the IBM Almaden Research Center. In Quest, we are exploring the various aspects of the database mining problem.
Reference: [2] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, </author> <title> Classification and Regression Trees, </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: As long as two or more E i 's are non-empty, each E i is smaller than E, and since E is finite, this procedure will terminate. ID3 (and its variants such as C4.5) [13] [14] and CART <ref> [2] </ref> are the best-known examples of tree classifiers. These decision trees usually have a branch for every value of a non-numeric attribute at a decision node. A numeric attribute is handled by repeated binary decomposition of its range of values. <p> An imperfect, smaller decision tree, rather than one that perfectly classifies all the known objects, usually is more accurate in classifying new objects because a decision tree that is perfect for the known objects may be overly sensitive to statistical idiosyncrasies of the given data set <ref> [2] </ref> [15]. To avoid overfitting the data, both ID3 and CART first obtain a large decision tree for the training set and then prune the tree (usually a large portion of it) starting from the leaves [2] [14] [15]. <p> known objects may be overly sensitive to statistical idiosyncrasies of the given data set <ref> [2] </ref> [15]. To avoid overfitting the data, both ID3 and CART first obtain a large decision tree for the training set and then prune the tree (usually a large portion of it) starting from the leaves [2] [14] [15]. Developing the full tree and then pruning it leads to more accurate trees, but makes classifier generation expensive. The interval classifier (IC) we propose is also a tree classifier. <p> Other possibilities for the goodness function include the cost of evaluating a predicate. The resubstitution error rate <ref> [2] </ref> for an attribute is computed as 1 v winner freq (v)=total freq where winner freq (v) is the frequency of the winning group for the attribute value v, and total freq is the total frequency of all groups over all values of this attribute in histograms H. <p> Since elevel is a categorical attribute, each of its values is considered to be an interval and we have the following intervals: Attribute: elevel Interval Winner Strength [0] Group A Strong [1] Group A Strong <ref> [2] </ref> Group B Strong [3] Group B Strong [4] Group B Strong Since all the intervals are strong, the tree is not grown further. If all or some of the intervals were weak, the algorithm would develop the tree further for those intervals.
Reference: [3] <author> Wray Buntine, </author> <title> About the IND Tree Package, </title> <institution> NASA Ames Research Center, Moffett Field, Cal-ifornia, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Since elevel is a categorical attribute, each of its values is considered to be an interval and we have the following intervals: Attribute: elevel Interval Winner Strength [0] Group A Strong [1] Group A Strong [2] Group B Strong <ref> [3] </ref> Group B Strong [4] Group B Strong Since all the intervals are strong, the tree is not grown further. If all or some of the intervals were weak, the algorithm would develop the tree further for those intervals. Figure 2 shows the decision tree generated by IC. <p> from the observed error rate to arrive at the error rate due to the classifier, some of the tuples that would have been reported as misclassified end up being in the intrinsic error pool, bringing the effective error rate down. 3.4 Comparison with ID3 We obtained the IND tree package <ref> [3] </ref> from the NASA Ames Research Center and ported it to IBM RISC System/6000 to compare the performance of IC with other classifiers. Experiments have been performed by the IND designers to ensure that IND reimplements C4 reasonably well.
Reference: [4] <editor> Wray Buntine and Matha Del Alto (Editors), </editor> <title> Collected Notes on the Workshop for Pattern Discovery in Large Databases, </title> <type> Technical Report FIA-91-07, </type> <institution> NASA Ames Research Center, Moffett Field, California, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: One such application domain that is likely to acquire considerable significance in the near future is database mining <ref> [4] </ref> [10] [12] [16] [18] [17] [20]. Several organizations have created ultra large data bases, running into several gigabytes and more. The databases relate to various aspects of their business and are information mines that they would like to exploit to improve the quality of their decision making. <p> Since elevel is a categorical attribute, each of its values is considered to be an interval and we have the following intervals: Attribute: elevel Interval Winner Strength [0] Group A Strong [1] Group A Strong [2] Group B Strong [3] Group B Strong <ref> [4] </ref> Group B Strong Since all the intervals are strong, the tree is not grown further. If all or some of the intervals were weak, the algorithm would develop the tree further for those intervals. Figure 2 shows the decision tree generated by IC.
Reference: [5] <author> John M. Chambers, William S. Cleaveland, Beat Kleiner, and Paul A. Tukey, </author> <title> Graphical Methods of Data Analysis, </title> <publisher> Wadsworth International Group (Duxbury Press), </publisher> <year> 1983. </year>
Reference-contexts: Given a histogram f (v i ; f i )g of values of a non-categorical attribute, where f i is the frequency of the attribute for value v i , we need a way to interpolate for frequency for values not present in the histogram. Following the technique in <ref> [5] </ref>, the frequency f for a value v is determined by considering the contribution of all values that occur in the histogram within an interval of length h centered at v.
Reference: [6] <author> Philip Andrew Chou, </author> <title> "Application of Information Theory to Pattern Recognition and the Design of Decision Trees and Trellises", </title> <type> Ph.D. Thesis, </type> <institution> Stan-ford University, California, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: We also refer to the examples set E as the training set and the database D as the test data set . This problem has been investigated in the AI and Statistics literature under the topic of supervised learning (see, for example, <ref> [6] </ref> [11] [12] [17]) 1 . We put the following additional requirements, not considered in the classical treatment of the problem, on the classification functions: 1. <p> Neural nets learn classification functions by multiple passes over the training set till the net converges, and have poor generation efficiency. Neural nets also do not handle non-numerical data well. Another important family of classifiers is based on decision trees (see [7] <ref> [6] </ref> [12] for an overview). The basic idea behind tree classifiers is as follows [13]. Let E be a finite collection of objects. If E contains only objects of one group, the decision tree is just a leaf labeled with that group.
Reference: [7] <author> G. R. Dattatreya and L. N. Kanal, </author> <title> "Decision Trees in Pattern Recognition", In Progress in Pattern Recognition 2 , L. </title> <editor> N. Kanal and A. Rosen-feld (Editors), </editor> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <year> 1985. </year>
Reference-contexts: Neural nets learn classification functions by multiple passes over the training set till the net converges, and have poor generation efficiency. Neural nets also do not handle non-numerical data well. Another important family of classifiers is based on decision trees (see <ref> [7] </ref> [6] [12] for an overview). The basic idea behind tree classifiers is as follows [13]. Let E be a finite collection of objects. If E contains only objects of one group, the decision tree is just a leaf labeled with that group.
Reference: [8] <author> L. Hayafil and R. L. Rivest, </author> <title> "Constructing Optimal Binary Decision Trees is NP-Complete", </title> <journal> Information Processing Letters, </journal> <volume> 5, 1, </volume> <year> 1976, </year> <pages> 15-17. </pages>
Reference-contexts: weighting if it is desired to bias the selection in favor of some specific groups. next attr: The function next attr (H) is greedy, and selects the next branching attribute by considering one attribute at a time. (The problem of computing optimal decision trees has been shown to be NP-complete <ref> [8] </ref>.) We consider two goodness functions: one min imizes the resubstitution error rate, the other maximizes the information gain ratio. Other possibilities for the goodness function include the cost of evaluating a predicate.
Reference: [9] <author> A. K. Jain and R. C. Dube, </author> <title> Algorithms for Clustering Data, </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: Generation Efficiency: The algorithm for generating the classification functions should be efficient. 1 The other major topic in classification is unsupervised learning. In unsupervised classification methods, clusters are first located in the feature space, and then the user decides which clusters represent which groups. See <ref> [9] </ref> for an overview of clustering algorithms. The emphasis in the current classifiers has been on minimizing the classification error and generation efficiency has not been an important design consideration. This has been the case because usually the classifier is generated once and then is used over and over again.
Reference: [10] <author> Ravi Krishnamurthy and Tomasz Imielinski, </author> <title> "Practitioner Problems in Need of Database Research: Research Directions in Knowledge Discovery", </title> <booktitle> SIGMOD Record , Vol. </booktitle> <volume> 20, No. 3, </volume> <month> Sept. </month> <year> 1991, </year> <pages> 76-78. </pages>
Reference-contexts: One such application domain that is likely to acquire considerable significance in the near future is database mining [4] <ref> [10] </ref> [12] [16] [18] [17] [20]. Several organizations have created ultra large data bases, running into several gigabytes and more. The databases relate to various aspects of their business and are information mines that they would like to exploit to improve the quality of their decision making.
Reference: [11] <author> Richard P. Lippmann, </author> <title> "An Introduction to Computing with Neural Nets", </title> <journal> IEEE ASSP Magazine, </journal> <month> April </month> <year> 1987, </year> <pages> 4-22. </pages>
Reference-contexts: We also refer to the examples set E as the training set and the database D as the test data set . This problem has been investigated in the AI and Statistics literature under the topic of supervised learning (see, for example, [6] <ref> [11] </ref> [12] [17]) 1 . We put the following additional requirements, not considered in the classical treatment of the problem, on the classification functions: 1. <p> Due to the requirement for retrieval efficiency, a classifier requiring objects to be retrieved one at a time into memory from the database before the classification function can be applied to them is not appropriate for database mining applications. Neural nets (see <ref> [11] </ref> for a survey) fit in this category. A neural net is a fixed sized data structure with the output of one node feeding into one or many other nodes. The classification functions generated by neural nets are buried in the weights on the inter-node links.
Reference: [12] <author> David J. Lubinsky, </author> <title> "Discovery from Databases: A Review of AI and Statistical Techniques", </title> <institution> AT&T Bell Laboratories, </institution> <address> Holmdel, New Jersey, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: One such application domain that is likely to acquire considerable significance in the near future is database mining [4] [10] <ref> [12] </ref> [16] [18] [17] [20]. Several organizations have created ultra large data bases, running into several gigabytes and more. The databases relate to various aspects of their business and are information mines that they would like to exploit to improve the quality of their decision making. <p> We also refer to the examples set E as the training set and the database D as the test data set . This problem has been investigated in the AI and Statistics literature under the topic of supervised learning (see, for example, [6] [11] <ref> [12] </ref> [17]) 1 . We put the following additional requirements, not considered in the classical treatment of the problem, on the classification functions: 1. <p> Neural nets learn classification functions by multiple passes over the training set till the net converges, and have poor generation efficiency. Neural nets also do not handle non-numerical data well. Another important family of classifiers is based on decision trees (see [7] [6] <ref> [12] </ref> for an overview). The basic idea behind tree classifiers is as follows [13]. Let E be a finite collection of objects. If E contains only objects of one group, the decision tree is just a leaf labeled with that group.
Reference: [13] <author> J. Ross Quinlan, </author> <title> "Induction of Decision Trees", </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <year> 1986, </year> <pages> 81-106. </pages>
Reference-contexts: Neural nets also do not handle non-numerical data well. Another important family of classifiers is based on decision trees (see [7] [6] [12] for an overview). The basic idea behind tree classifiers is as follows <ref> [13] </ref>. Let E be a finite collection of objects. If E contains only objects of one group, the decision tree is just a leaf labeled with that group. <p> As long as two or more E i 's are non-empty, each E i is smaller than E, and since E is finite, this procedure will terminate. ID3 (and its variants such as C4.5) <ref> [13] </ref> [14] and CART [2] are the best-known examples of tree classifiers. These decision trees usually have a branch for every value of a non-numeric attribute at a decision node. A numeric attribute is handled by repeated binary decomposition of its range of values. <p> However, it has the disadvantage that it can lead to large decision trees, with unrelated attribute values being grouped together and with multiple tests for the same attribute <ref> [13] </ref>. Mor-ever, binary decomposition may cause large increase in computation, since an attribute with w values has a computational requirement similar to 2 w1 1 binary attributes [13]. <p> that it can lead to large decision trees, with unrelated attribute values being grouped together and with multiple tests for the same attribute <ref> [13] </ref>. Mor-ever, binary decomposition may cause large increase in computation, since an attribute with w values has a computational requirement similar to 2 w1 1 binary attributes [13]. The essence of classification is to construct a decision tree that correctly classifies not only objects in the training set but also the unseen objects in the test data set [13]. <p> in computation, since an attribute with w values has a computational requirement similar to 2 w1 1 binary attributes <ref> [13] </ref>. The essence of classification is to construct a decision tree that correctly classifies not only objects in the training set but also the unseen objects in the test data set [13]. An imperfect, smaller decision tree, rather than one that perfectly classifies all the known objects, usually is more accurate in classifying new objects because a decision tree that is perfect for the known objects may be overly sensitive to statistical idiosyncrasies of the given data set [2] [15]. <p> The information gain ratio is an information theoretic measure proposed in <ref> [13] </ref>. Let the example set E of e objects contain e k objects of group G k .
Reference: [14] <author> J. Ross Quinlan, </author> <title> "Simplifying Decision Trees", </title> <journal> Int. J. Man-Machine Studies, </journal> <volume> 27, </volume> <year> 1987, </year> <pages> 221-234. </pages>
Reference-contexts: As long as two or more E i 's are non-empty, each E i is smaller than E, and since E is finite, this procedure will terminate. ID3 (and its variants such as C4.5) [13] <ref> [14] </ref> and CART [2] are the best-known examples of tree classifiers. These decision trees usually have a branch for every value of a non-numeric attribute at a decision node. A numeric attribute is handled by repeated binary decomposition of its range of values. <p> To avoid overfitting the data, both ID3 and CART first obtain a large decision tree for the training set and then prune the tree (usually a large portion of it) starting from the leaves [2] <ref> [14] </ref> [15]. Developing the full tree and then pruning it leads to more accurate trees, but makes classifier generation expensive. The interval classifier (IC) we propose is also a tree classifier. <p> IC also provides a dynamic tree pruning criteria. For each node, an expansion merit is computed, and a node is expanded only if this merit is above an acceptable level. The expansion merit is based on the ideas in <ref> [14] </ref>. Suppose a tree T has been generated using N cases in the training set. Let some leaf account for K of these cases with J of them misclassified.
Reference: [15] <author> J. Ross Quinlan and Ronald L. Rivest, </author> <title> "Inferring Decision Trees Using the Minimum Description Length Principle", </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <year> 1989, </year> <pages> 227-248. </pages>
Reference-contexts: An imperfect, smaller decision tree, rather than one that perfectly classifies all the known objects, usually is more accurate in classifying new objects because a decision tree that is perfect for the known objects may be overly sensitive to statistical idiosyncrasies of the given data set [2] <ref> [15] </ref>. To avoid overfitting the data, both ID3 and CART first obtain a large decision tree for the training set and then prune the tree (usually a large portion of it) starting from the leaves [2] [14] [15]. <p> may be overly sensitive to statistical idiosyncrasies of the given data set [2] <ref> [15] </ref>. To avoid overfitting the data, both ID3 and CART first obtain a large decision tree for the training set and then prune the tree (usually a large portion of it) starting from the leaves [2] [14] [15]. Developing the full tree and then pruning it leads to more accurate trees, but makes classifier generation expensive. The interval classifier (IC) we propose is also a tree classifier.
Reference: [16] <editor> G. Piatetsky-Shapiro and W. Frawley (Editors), </editor> <booktitle> Proceedings of IJCAI-89 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Detroit, Michigan, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: One such application domain that is likely to acquire considerable significance in the near future is database mining [4] [10] [12] <ref> [16] </ref> [18] [17] [20]. Several organizations have created ultra large data bases, running into several gigabytes and more. The databases relate to various aspects of their business and are information mines that they would like to exploit to improve the quality of their decision making.
Reference: [17] <editor> G. Piatetsky-Shapiro (Editor), </editor> <title> Knowledge Discovery in Databases, </title> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: One such application domain that is likely to acquire considerable significance in the near future is database mining [4] [10] [12] [16] [18] <ref> [17] </ref> [20]. Several organizations have created ultra large data bases, running into several gigabytes and more. The databases relate to various aspects of their business and are information mines that they would like to exploit to improve the quality of their decision making. <p> We also refer to the examples set E as the training set and the database D as the test data set . This problem has been investigated in the AI and Statistics literature under the topic of supervised learning (see, for example, [6] [11] [12] <ref> [17] </ref>) 1 . We put the following additional requirements, not considered in the classical treatment of the problem, on the classification functions: 1.
Reference: [18] <editor> G. Piatetsky-Shapiro (Editor), </editor> <booktitle> Proceedings of AAAI-91 Workshop on Knowledge Discovery in Databases, </booktitle> <address> Anaheim, California, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: One such application domain that is likely to acquire considerable significance in the near future is database mining [4] [10] [12] [16] <ref> [18] </ref> [17] [20]. Several organizations have created ultra large data bases, running into several gigabytes and more. The databases relate to various aspects of their business and are information mines that they would like to exploit to improve the quality of their decision making.
Reference: [19] <author> G. W. Snedecor and W.G. Cochran, </author> <title> Statistical Methods, 7th Edition, </title> <institution> Iowa State University Press, </institution> <year> 1980. </year>
Reference-contexts: The ratio J=K does not provide a reliable estimate of the error rate of the leaf for unseen cases, since the tree has been tailored to the training set. A more realistic error rate is obtained by applying continuity correction for the binomial distribution <ref> [19] </ref> in which J is repalaced by J +1=2. Let S be a subtree of T containing L (S) leaves and let J and K be the corresponding sums over the leaves of S. S will misclassify P of K unseen cases.
Reference: [20] <author> Shalom Tsur, </author> <title> "Data Dredging", </title> <journal> IEEE Data Engineering Bulletin, </journal> <volume> 13, 4, </volume> <month> December </month> <year> 1990, </year> <pages> 58-63. </pages>
Reference-contexts: One such application domain that is likely to acquire considerable significance in the near future is database mining [4] [10] [12] [16] [18] [17] <ref> [20] </ref>. Several organizations have created ultra large data bases, running into several gigabytes and more. The databases relate to various aspects of their business and are information mines that they would like to exploit to improve the quality of their decision making.
References-found: 20

