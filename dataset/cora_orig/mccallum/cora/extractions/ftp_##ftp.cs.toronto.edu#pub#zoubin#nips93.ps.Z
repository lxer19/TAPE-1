URL: ftp://ftp.cs.toronto.edu/pub/zoubin/nips93.ps.Z
Refering-URL: http://www.cs.toronto.edu/~zoubin/zoubin/nips93.abstract.html
Root-URL: 
Email: zoubin@psyche.mit.edu.  
Title: Supervised learning from incomplete data via an EM approach  
Author: Zoubin Ghahramani and Michael I. Jordan In Cowan, J.D., Tesauro, G., and Alspector, J. (eds.). 
Note: Advances in Neural Information Processing Systems 6. Morgan Kaufmann Publishers,  1994. Email to:  
Address: Cambridge, MA 02139  San Francisco, CA,  
Affiliation: Department of Brain Cognitive Sciences Massachusetts Institute of Technology  
Abstract: Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data sets. We use mixture models for the density estimates and make two distinct appeals to the Expectation-Maximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm|EM is used both for the estimation of mixture components and for coping with missing data. The resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems. Results from a classification benchmark|the iris data set|are presented.
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA. </address>
Reference-contexts: The LSE estimator on a Gaussian mixture has interesting relations to algorithms such as CART <ref> (Breiman et al., 1984) </ref>, MARS (Friedman, 1991), and mixtures of experts (Jacobs et al., 1991; Jordan and Jacobs, 1994), in that the mixture of Gaussians competitively partitions the input space, and learns a linear regression surface on each partition.
Reference: <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference-contexts: This is because the problem of estimating mixture densities can itself be viewed as a missing data problem (the "labels" for the component densities are missing) and an Expectation-Maximization (EM) algorithm <ref> (Dempster et al., 1977) </ref> can be developed to handle both kinds of missing data. 2 Density estimation using EM This section outlines the basic learning algorithm for finding the maximum likelihood parameters of a mixture model (Dempster et al., 1977; Duda and Hart, 1973; Nowlan, 1991). <p> Since z is unknown l c cannot be utilized directly, so we instead work with its expectation, denoted by Q (j k ). As shown by <ref> (Dempster et al., 1977) </ref>, l (jX ) can be maximized by iterating the following two steps: E step: Q (j k ) = E [l c (jX ; Z)jX ; k ] M step: k+1 = arg max The E (Expectation) step computes the expected complete data log likelihood and the
Reference: <author> Duda, R. O. and Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Friedman, J. H. </author> <year> (1991). </year> <title> Multivariate adaptive regression splines. </title> <journal> The Annals of Statistics, </journal> <volume> 19 </volume> <pages> 1-141. </pages>
Reference-contexts: The LSE estimator on a Gaussian mixture has interesting relations to algorithms such as CART (Breiman et al., 1984), MARS <ref> (Friedman, 1991) </ref>, and mixtures of experts (Jacobs et al., 1991; Jordan and Jacobs, 1994), in that the mixture of Gaussians competitively partitions the input space, and learns a linear regression surface on each partition. This similarity has also been noted by Tresp et al. (1993) .
Reference: <author> Ghahramani, Z. </author> <year> (1994). </year> <title> Solving inverse problems using an EM approach to density estimation. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School. </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: Second, estimating the density explicitly enables us to represent any relation be-tween the variables. Density estimation is fundamentally more general than function approximation and this generality is needed for a large class of learning problems arising from inverting causal systems <ref> (Ghahramani, 1994) </ref>. These problems cannot be solved easily by traditional function approximation techniques since the data is not generated from noisy samples of a function, but rather of a relation. Acknowledgements Thanks to D. M. Titterington and David Cohn for helpful comments.
Reference: <author> Jacobs, R., Jordan, M., Nowlan, S., and Hinton, G. </author> <year> (1991). </year> <title> Adaptive mixture of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87. </pages>
Reference: <author> Jordan, M. and Jacobs, R. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214. </pages>
Reference: <author> Little, R. J. A. and Rubin, D. B. </author> <year> (1987). </year> <title> Statistical Analysis with Missing Data. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> McLachlan, G. and Basford, K. </author> <year> (1988). </year> <title> Mixture models: Inference and applications to clustering. </title> <publisher> Marcel Dekker. </publisher>
Reference-contexts: A possible disadvantage of parametric methods is their lack of flexibility when compared with nonparamet-ric methods. This problem, however, can be largely circumvented by the use of mixture models <ref> (McLachlan and Basford, 1988) </ref>. Mixture models combine much of the flexibility of nonparametric methods with certain of the analytic advantages of parametric methods. Mixture models have been utilized recently for supervised learning problems in the form of the "mixtures of experts" architecture (Jacobs et al., 1991; Jordan and Jacobs, 1994).
Reference: <author> Nowlan, S. J. </author> <year> (1991). </year> <title> Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures. </title> <institution> CMU-CS-91-126, School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference: <author> Specht, D. F. </author> <year> (1991). </year> <title> A general regression neural network. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 2(6) </volume> <pages> 568-576. </pages>
Reference: <author> Tresp, V., Hollatz, J., and Ahmad, S. </author> <year> (1993). </year> <title> Network structuring and training using rule-based knowledge. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <publisher> Morgan Kauf-man Publishers, </publisher> <address> San Mateo, CA. </address>
References-found: 12

