URL: ftp://ftp.cs.washington.edu/tr/1997/03/UW-CSE-97-03-02.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Note: c Copyright 1996 Suzanne Bunton  
Abstract-found: 0
Intro-found: 1
Reference: [Ash65] <author> R. Ash. </author> <title> Information Theory. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, New York, </address> <year> 1965. </year>
Reference-contexts: The more restricted the assumed class, the weaker the result. Three interesting subfamilies of stochastic FSM sources from the literature are Markov sources (Section 2.2), FSMX sources [Ris86a], and finite-order FSM sources <ref> [Ash65] </ref>, which are also called finite-context automata [BM89]. The containment relationships between these model classes are shown within the Chomsky Hierarchy [HU79] in Figure 2.3. * In Markov sources, the next state is completely determined by the previous 26 state and the current source symbol. <p> Another common assumption made for Markovian and non-Markovian sources is that they are unifilar. A unifilar Markovian FSM has exactly one transition for each source symbol of non-zero probability leaving each state <ref> [Ash65] </ref>. <p> Below, we give a new proof of that containment, which also shows that the characterization of Theorem 4.4.1, C k () is finite-order, as it should be. 66 Intuitively, DMC models belong to the class of finite-order Markov sources <ref> [Ash65] </ref>, also known as Finite-Context Automata [BM89], because only a finite suffix of a given source string is required to determine the state to which the string will carry a given model.
Reference: [BB92] <author> S. Bunton and G. Borriello. </author> <title> Practical dictionary management for hardware data compression. </title> <journal> Communications of the ACM, </journal> <volume> 35(1) </volume> <pages> 95-104, </pages> <year> 1992. </year>
Reference-contexts: Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94]. Other techniques, namely any variant of the elegant Ziv-Lempel techniques <ref> [ZL77, ZL78, MW85, Wel84, FG89, BB92] </ref>, trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85]. <p> to design|within the next few months|an implementation that fit on a single chip, had good compression performance, required trivial timing and buffering overhead, and was an order of magnitude faster than the state of the art, she would abandon the techniques presented here, just as she did seven years ago <ref> [BB92, Bun92] </ref>. 6 Indeed, this has already happened in the case of arithmetic coding [RM89, CKW91, HV92, FGC93, MSWB93]. 6 be that it is simply not worthwhile to improve the compression performance further. Even so, there remains one other very important motivation for this research.
Reference: [BCW90] <author> T. C. Bell, J. G. Cleary, and I. H. Witten. </author> <title> Text Compression. Advanced Reference Series. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1990. </year>
Reference-contexts: On-line models, which are incrementally reconstructed by the decoder from the decoded data sequence, require no specific transmission. However, the prediction inaccuracies endured early in the source stream by on-line techniques increase the overall length of the encoded sequence, just as a static-model preamble would. It has been shown <ref> [BCW90, CW84a, Ris89] </ref> that we can approach the MDL with online models, and that there exist circumstances where an off-line model will perform arbitrarily worse than any on-line model. In practice, on-line models do work better than off-line models. <p> Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique <ref> [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94] </ref>. Other techniques, namely any variant of the elegant Ziv-Lempel techniques [ZL77, ZL78, MW85, Wel84, FG89, BB92], trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85]. <p> Roughly speaking, these estimators "blend" statistics from different order models in a weighted average. "Blending" techniques have been tuned empirically over the years, primarily using the Calgary Corpus <ref> [BCW90] </ref>. Apparently, no empirical studies of the Context algorithm, its variants, or the WLZ algorithm, have been published, and it is not known how these algorithms perform on actual data. <p> For example, if blending method PPMC [Mof90] were used, the frequency on the suffix () edge should always equal the number of distinct symbols which have been seen when in a given state, and the application of such optimizations as exclusion and update exclusion <ref> [BCW90] </ref> is straightforward. DMC's probability estimation technique, and some efficient variations, are de scribed in Chapter 6. 4.7 Summary This chapter's analytical study of DMC completes our study of model structure. Chapter 3 showed how to construct suffix tree models using only the "splitting" of string-labeled transitions. <p> In <ref> [BCW90] </ref> this approach is called "full blending." However, computing such weighted averages is expensive, and computations cannot be reused between visits to a given set of excited states. Furthermore, there was no published evidence prior to this work that it produces better probability estimates|no published on-line algorithms use it. <p> The next chapter uses that cross product implementation to evaluate different solutions to the component subprob-lems, including the new solutions we introduced earlier. Chapter 9 PERFORMANCE MEASUREMENTS In this chapter, we measure the predictive ability of our models by running them on the files of the Calgary Corpus <ref> [BCW90] </ref>. All probability estimates are coded using a floating-point m-ary arithmetic coder that we based upon the popular integer coder implemented in [WNC87, MSWB93].
Reference: [Bel86] <author> T. C. Bell. </author> <title> A Unifying Theory and Improvements for Existing Approaches to Text Compression. </title> <type> PhD thesis, </type> <institution> University of Canterbury, </institution> <year> 1986. </year>
Reference-contexts: Though only a few practical data compression meth 5 ods use explicit stochastic models, researchers have systematically proved that the other known practical methods can be exactly and efficiently emulated with adaptive stochastic models <ref> [Ris83, Lan83, Bel86] </ref>. Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94]. <p> Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique <ref> [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94] </ref>. Other techniques, namely any variant of the elegant Ziv-Lempel techniques [ZL77, ZL78, MW85, Wel84, FG89, BB92], trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85].
Reference: [BM89] <author> T. C. Bell and A. Moffat. </author> <title> A note on the DMC data compression scheme. </title> <journal> The British Computer Journal, </journal> <volume> 32(1) </volume> <pages> 16-20, </pages> <year> 1989. </year>
Reference-contexts: The more restricted the assumed class, the weaker the result. Three interesting subfamilies of stochastic FSM sources from the literature are Markov sources (Section 2.2), FSMX sources [Ris86a], and finite-order FSM sources [Ash65], which are also called finite-context automata <ref> [BM89] </ref>. The containment relationships between these model classes are shown within the Chomsky Hierarchy [HU79] in Figure 2.3. * In Markov sources, the next state is completely determined by the previous 26 state and the current source symbol. <p> DMC is the only published stochastic data model for which a characterization of its states, in terms of conditioning contexts, is unknown. Up until now, all that was certain about DMC was that a finite-context characterization exists, which was proved in <ref> [BM89] </ref> using a finiteness argument. Here, we present and prove the first finite-context characterization of the states of DMC's data model. The impact of our characterization is threefold. 1. <p> The closure of the set of children of state s i equals the set of descendants of s i , and is denoted extns fl k (s i ). Additionally, two observations from <ref> [BM89] </ref> are used repeatedly in the proofs of the lemmas and theorems to follow, and they can be restated using the suffix function above. <p> However, this is a loose characterization, for it was shown in <ref> [BM89] </ref> the languages recognizable by DMC models are contained, possibly properly, in the class of languages expressible as A fl F [ G (where F and G are finite sets), that is, finite-order languages. <p> Below, we give a new proof of that containment, which also shows that the characterization of Theorem 4.4.1, C k () is finite-order, as it should be. 66 Intuitively, DMC models belong to the class of finite-order Markov sources [Ash65], also known as Finite-Context Automata <ref> [BM89] </ref>, because only a finite suffix of a given source string is required to determine the state to which the string will carry a given model. This means, for one thing, that DMC FSMs cannot recognize infinitely repeating patterns, a common capability of more general FSMs.
Reference: [Bun92] <author> S. Bunton. </author> <title> Data structure management tagging system. </title> <type> U.S. Patent 5,151,697, </type> <month> September </month> <year> 1992. </year>
Reference-contexts: to design|within the next few months|an implementation that fit on a single chip, had good compression performance, required trivial timing and buffering overhead, and was an order of magnitude faster than the state of the art, she would abandon the techniques presented here, just as she did seven years ago <ref> [BB92, Bun92] </ref>. 6 Indeed, this has already happened in the case of arithmetic coding [RM89, CKW91, HV92, FGC93, MSWB93]. 6 be that it is simply not worthwhile to improve the compression performance further. Even so, there remains one other very important motivation for this research.
Reference: [Bun94] <author> S. Bunton. </author> <title> A characterization of the `Dynamic Markov Compression' FSM with finite conditioning contexts. </title> <type> UW-CSE Technical Report UW-CSE-94-11-03, </type> <institution> The University of Washington, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: We develop techniques that generalize and improve the algorithms as a group. These algorithms were not originally introduced as having a common structure, and none were originally implemented or described with suffix trees. Yet, since several promising asymptotic techniques from the information-theoretic literature are based upon 6 In <ref> [Bun94] </ref>, I independently proposed a very similar solution, which I called "lazy cloning" and "LazyDMC," that leads to slightly better performance. <p> Other authors [TR93, Ton93, Whi94, Yu93] have independently generalized DMC to larger alphabets using variations of what we call lazy cloning , which copies outgoing transitions only as needed. However, only the lazy-evaluation solutions introduced in <ref> [Bun94] </ref> and in [TR93] successfully reduce DMC's memory requirements without eliminating DMC's advantages. (Sections 6.5 and 9.5 explain and refine the ideas from [Bun94] and Section 9.5 compares their performance with the technique of [TR93].) The others all had a similar approach: when the required outgoing transition was absent from the <p> However, only the lazy-evaluation solutions introduced in <ref> [Bun94] </ref> and in [TR93] successfully reduce DMC's memory requirements without eliminating DMC's advantages. (Sections 6.5 and 9.5 explain and refine the ideas from [Bun94] and Section 9.5 compares their performance with the technique of [TR93].) The others all had a similar approach: when the required outgoing transition was absent from the current state, the needed transition was copied from a state that was essentially a zeroth or first-order state. <p> symbol transitions (edges) or string transitions (edge fl s), are implementation-dependent. 137 9.5 Improvements to DMC Variants 9.5.1 GDMC and LazyDMC We began our attempts to create a better-performing variant of DMC [CH87] with two starting points: the GDMC algorithm [TR93] and our own lazy-cloning DMC variant, LazyDMC, proposed in <ref> [Bun94] </ref>. GDMC uses a mixture with weight function A (thus nothing gets added to a state's escape count after it is initially set to the redirected prefix edge's count) and inheritance evaluation time M 3 .
Reference: [Bun96] <author> S. Bunton. </author> <title> On-Line Stochastic Processes in Data Compression. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: This flexibility is not a feature of all Markov models with underlying suffix-tree structure. We have proved that there exist useful finite-context Markov models that are not FSMX <ref> [Bun96, Chapter 4] </ref>. Those models allow arbitrarily long extensions to state contexts and include Dynamic Markov Compression (DMC) [CH87] models as a special case. <p> variants, and thus these techniques, along with other variants of mixtures, are interchangeable. 6.1 Recursive Mixtures We are concerned with estimating a probability P e (a i ja 1 a 2 a i1 ) using the frequencies stored at the excited states of a suffix-tree FSM (see Chapter 2 of <ref> [Bun96] </ref>), where the excited states are those states of the FSM whose associated conditioning context partitions contain the sequence a 1 a 2 a i1 2 A fl . <p> The DMC algorithm, which originally used a binary alphabet, adds each new state to its model by "cloning" an eligible parent state. Each clone receives a scaled copy of the parent state's frequency distribution the moment it is added. Since, as we proved 87 in Chapter 4 of <ref> [Bun96] </ref>, the conditioning context relationship among clones and parent states is equivalent to that among suffixes in other suffix-tree models, inherit at state creation corresponds to the numerical aspects of cloning. <p> First, update exclusion improves performance of mixtures (which we propose to combine with state selection). Second, update exclusion correctly handles the incomplete frontiers that result from lazily evaluating refinements to suffix-tree states <ref> [Bun96, see Section 7.6] </ref>. With mixtures, the probability estimate is defined recursively in terms of ancestor nodes. <p> that is represented by s's children is incomplete because L (s) t:suffix (t)=s L (t) 6= fg: To complete s's context partition element, we maintain a "shadow child" of s that 107 maintains a next-symbol frequency distribution that is conditioned by L (s) t:suffix (t)=s L (t): As explained in <ref> [Bun96] </ref>, Chapter 5, this is the same frequency distribution that is formed by the update-excluded frequency counts count [; s; 1] if maximum-order updates are globally enabled for the modeling algorithm.
Reference: [Bun97a] <author> S. Bunton. </author> <title> A percolating state selector for suffix-tree context models. </title> <booktitle> In Proceedings Data Compression Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> March </month> <year> 1997. </year>
Reference-contexts: a i in a 1 a 2 a n , where ^s i is a state that is specially selected as the starting node for the recursive mixture computation using the states excited by a 1 a 2 a i1 . (State selection is the topic of the companion paper <ref> [Bun97a] </ref>.) For the present discussion, assume that ^s i is the maximum-order excited state at time i). <p> In this case, the selected order, which we tally for our experiments, is counted as the order of the maximum-order excited state plus one. heuristic, S 2 : Select the min-order excited state with one out-edge [CTW95]. percolating, S 3 : See companion paper <ref> [Bun97a] </ref>. The construction of [WLZ92] (WLZ) uses an order-bounded state selector that produces the same results as our percolating state selector does in a model that has the same structural order bound, assuming that both models use the same selection threshold.
Reference: [Bun97b] <author> S. Bunton. </author> <title> Semantically motivated improvements for PPM variants. </title> <journal> The British Computer Journal, Special Data Compression Issue, </journal> <note> 1997. (invited paper, to appear June 1997). </note>
Reference-contexts: However, quite often the abstract differences among apparently distinct approaches can be expressed in a single technique as different values of parameters that have simple connotations. 1 The seven algorithms are DMC [CH87], GDMC [TR93], PPM [Mof90], PPM* [CTW95], WLZ [WLZ92], plus BestFSMX <ref> [Bun97b] </ref> and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far ([Bun96, Chapter 9]).
Reference: [BW94] <author> M. Burrows and D. J. Wheeler. </author> <title> A block-sorting lossless data compression algorithm. </title> <note> Research Report 124, DEC SRC, </note> <month> May </month> <year> 1994. </year> <month> 151 </month>
Reference-contexts: To treat these finitely represented quantities as members of an alphabet, as we do with computer data, such as text or object code, is to grossly overgeneralize the problem, and thereby 1 One notable exception is the off-line lossless transform method published by Burroughs and Wheeler <ref> [BW94] </ref>, which is competitive with the best on-line stochastic techniques. 3 throw away useful a priori knowledge. Adjacent samples in a signal data sequence are usually highly correlated, whereas adjacent symbols in alphabet-oriented computer data are not.
Reference: [BWC89] <author> T. C. Bell, I. H. Witten, and J. G. Cleary. </author> <title> Modeling for text compression. </title> <journal> ACM Computer Surveys, </journal> <volume> 24(4) </volume> <pages> 555-591, </pages> <year> 1989. </year>
Reference-contexts: Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique <ref> [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94] </ref>. Other techniques, namely any variant of the elegant Ziv-Lempel techniques [ZL77, ZL78, MW85, Wel84, FG89, BB92], trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85].
Reference: [CH87] <author> G. V. Cormack and R. N. S. Horspool. </author> <title> Data compression using dynamic Markov modelling. </title> <journal> The Computer Journal, </journal> <volume> 30(6) </volume> <pages> 541-550, </pages> <year> 1987. </year>
Reference-contexts: This flexibility is not a feature of all Markov models with underlying suffix-tree structure. We have proved that there exist useful finite-context Markov models that are not FSMX [Bun96, Chapter 4]. Those models allow arbitrarily long extensions to state contexts and include Dynamic Markov Compression (DMC) <ref> [CH87] </ref> models as a special case. They require explicit destination pointers because their conditioning contexts cannot be described by a single string. 2.6.3 Model Semantics I: Conditioning Context Partitions The suffix-tree structure of the models discussed in this work may have a number of mathematical interpretations. <p> PPM's reign as the best-compressing modeling algorithm has stood unchallenged for the 12 years preceding this thesis. PPM's success is due to its ad hoc "blending" technique for computing probability estimates, and the careful engineering behind Moffat's 1990 imple mentation, PPMC [Mof90]. * DMC <ref> [CH87] </ref>, or Dynamic Markov Modeling, grows an unstructured binary-alphabet finite-state machine by redirecting any sufficiently popular transition to a new state in a process called "cloning." Miraculously, the maiden implementation of this simple idea delivered performance competitive with carefully engineered and tuned implementations of PPM. <p> Thus, this work is preliminary to a taxonomical and empirical study that shows how DMC's unique abstract features can be combined with the best features from other influential algorithms to produce truly novel modeling techniques with superior performance. 4.1 Introduction The popular Dynamic Markov Compression Algorithm (DMC) <ref> [CH87] </ref> is a member of the family of data compression algorithms that combine an on-line stochastic data model with an arithmetic coder. DMC's distinguishing feature is an elegant but ad 50 hoc modeling technique that provides state-of-the-art compression performance and matchless conceptual simplicity. <p> Cloning <ref> [CH87] </ref> simply redirects the current transition, k (s p ; a), to a newly added state s k+1 . <p> Cloning is performed whenever E (M k ; wa) = T , where 1 Throughout, we shall denote the concatenation of strings w and y as wy or as w y. 2 Other possible initial models are given in <ref> [CH87] </ref>. 52 is created incrementally by cloning the destination of the current transition, if it is determined to be eligible. The bold (presumably eligible) transition in each model M k is redirected to a newly added state s k+1 to form model M k+1 . <p> The number of proper prefixes wa of the given input sequence for which the criterion E (M k ; wa) holds determines the value of m. The eligibility criterion given in <ref> [CH87] </ref> is based upon the popularity of the current transition relative to the popularity of its destination. <p> Blending can be viewed as a bottom-up recursive procedure for computing a mixture, barring one missing term for each level of the recursion, where a mixture is basically a weighted average of several probability estimates. We shall show by decomposition that mixtures generalize the techniques used in DMC variants <ref> [CH87, TR93] </ref>, as well as PPM variants, and thus these techniques, along with other variants of mixtures, are interchangeable. 6.1 Recursive Mixtures We are concerned with estimating a probability P e (a i ja 1 a 2 a i1 ) using the frequencies stored at the excited states of a suffix-tree <p> For example, PPM's blending is a forgetful type of mixture that lazily evaluates its inheritances as novel events occur, while DMC's "cloning" <ref> [CH87] </ref> produces a mixture that evaluates its inheritances when new states are added, but which also subtracts the inherited frequency from the parent distribution. <p> However, quite often the abstract differences among apparently distinct approaches can be expressed in a single technique as different values of parameters that have simple connotations. 1 The seven algorithms are DMC <ref> [CH87] </ref>, GDMC [TR93], PPM [Mof90], PPM* [CTW95], WLZ [WLZ92], plus BestFSMX [Bun97b] and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far ([Bun96, Chapter 9]). <p> The sizes of model states (nodes), and symbol transitions (edges) or string transitions (edge fl s), are implementation-dependent. 137 9.5 Improvements to DMC Variants 9.5.1 GDMC and LazyDMC We began our attempts to create a better-performing variant of DMC <ref> [CH87] </ref> with two starting points: the GDMC algorithm [TR93] and our own lazy-cloning DMC variant, LazyDMC, proposed in [Bun94].
Reference: [Cho88] <author> P. A. Chou. </author> <title> Applications of Information Theory to Pattern Recognition and the Design of Decision Trees and Trellises. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1988. </year>
Reference-contexts: All data compressors use some sort 2 This is a topic of rate distortion theory, which is rigorously presented in [CT91], and applied to tree-structured pattern classification in <ref> [Cho88] </ref>. <p> All data compressors use some sort 2 This is a topic of rate distortion theory, which is rigorously presented in [CT91], and applied to tree-structured pattern classification in [Cho88]. An application to image compression, pruned tree-structured vector quantization <ref> [Cho88, Chapter 5] </ref>, is further developed in [Ris90]. 3 Discrete signal data sequences can easily be decorrelated, and thereafter treated as alphabet sequences suitable for modeling with alphabet-oriented techniques, by first recoding each successive sample as the difference between it and the preceding sample. 4 The information-theoretic literature employs the term
Reference: [CKW91] <author> D. Chevion, E. D. Karnin, and E. Wallach. </author> <title> High efficiency, multiplication-free approximation of arithmetic coding. </title> <booktitle> In Proceedings Data Compression Conference, </booktitle> <pages> pages 43-52. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> March </month> <year> 1991. </year>
Reference-contexts: good compression performance, required trivial timing and buffering overhead, and was an order of magnitude faster than the state of the art, she would abandon the techniques presented here, just as she did seven years ago [BB92, Bun92]. 6 Indeed, this has already happened in the case of arithmetic coding <ref> [RM89, CKW91, HV92, FGC93, MSWB93] </ref>. 6 be that it is simply not worthwhile to improve the compression performance further. Even so, there remains one other very important motivation for this research. These same stochastic modeling techniques are applicable to other computing disciplines in a straightforward manner. <p> Implementations of arithmetic coding have been honed to near perfection over the years. There are now several fast and extremely accurate approximations which eliminate the divisions and multiplications that are required by straightforward implementations <ref> [RM89, CKW91, FGC93] </ref>. 2.4 A General Strategy for Optimal Source Modeling Assume the string was generated by a stochastic FSM. Given a string to compress, we first assume that it was generated by a finite stochastic source belonging to a tractable but useful class of models.
Reference: [CT91] <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, New York, </address> <year> 1991. </year>
Reference-contexts: All data compressors use some sort 2 This is a topic of rate distortion theory, which is rigorously presented in <ref> [CT91] </ref>, and applied to tree-structured pattern classification in [Cho88]. <p> An ergodic source is the most general dependent source 2 for which the strong law of large numbers holds <ref> [CT91, Chapter 15.7] </ref>. That is, if an ergodic source is allowed to emit symbols indefinitely, the relative frequency of the symbols, given the state the source is in when it emits the symbols, will converge to the transition probabilities of the source. <p> Conversely, given a set of code lengths that satisfy this inequality, it is possible to construct a uniquely decodable code with their lengths <ref> [CT91, chapter 5] </ref>.
Reference: [CTW95] <author> J. G. Cleary, W. J. Teahan, and I. H. Witten. </author> <title> Unbounded length contexts for PPM. </title> <booktitle> In Proceedings Data Compression Conference, </booktitle> <month> March </month> <year> 1995. </year>
Reference-contexts: Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique <ref> [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94] </ref>. Other techniques, namely any variant of the elegant Ziv-Lempel techniques [ZL77, ZL78, MW85, Wel84, FG89, BB92], trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85]. <p> GDMC does not, however, outperform PPM. * PPM* <ref> [CTW95] </ref> does away with PPM's global order-bound in a linear-space implementation. However, despite its unbounded order, PPM* does not out perform higher-order implementations of PPMC. <p> PPM constructs a bounded-order Markov model such that the maximum order of the excited states is allowed to vary within the order bound. Recently, a new variant, PPM*, was developed that eliminated the (arbitrary) order bound in PPM's model structure <ref> [CTW95] </ref>. Both PPM and PPM* were developed without explicit description of their underlying suffix-tree structure, although several authors have since transformed PPM to suffix-tree-based implementations. <p> Note that it is not necessary to represent the suffix relationship explicitly by storing a suffix pointer at each state <ref> [Mof90, CTW95] </ref>. The conditioning context partition of any PPM variant is induced by the function context : S ! A fl . <p> Unfortunately, the straightforward procedure described above will construct a suffix tree requiring super-linear space for PPM* models. The authors of PPM* <ref> [CTW95] </ref> gave a linear-space solution based upon "patricia trees" but it was not clear how this solution related to suffix trees, and it did not use the minimal number of nodes. <p> To do this, we assume that there exists a virtual state between every two adjacent symbols on every string transition. The virtual states that correspond to the so-called deterministic states in the original PPM fl implementation <ref> [CTW95] </ref> are shown in Figure 3.2. Since virtual states do not exist in storage, we must deduce the frequency distribution that is conditioned by the conditioning contexts at excited virtual states, in order to use them for probability 43 estimation. <p> The model's excited states for the sequence abracadabra are denoted by numbered arrows, to illustrate their one-to-one correspondence with the states pointed to by the "context list" pictured in the original PPM* illustration <ref> [CTW95] </ref>. The number on each arrow indicates the Markov order of the indicated state. Note that only the excited virtual states actually exist in memory at any given time. <p> Both PPM fl model structures, ours and the original in <ref> [CTW95] </ref>, represent every subsequence of the already-scanned portion of the input. The original PPM and this section's space-saving implementation of it represent every subsequence of the processed input that is no longer than the order bound M . <p> To see why this is true, consider the "forward tree" that is induced by the (actual) suffix-tree FSM, and which corresponds to the original PPM* implementation <ref> [CTW95] </ref>: * the forward-child relationship is equal to the next-state relationship, * the set of nodes include the actual nodes in the FSM plus an added node for each (string-labeled) pointer into the history buffer, * the leaf nodes of the forward tree are the added nodes, and * the internal <p> The end position is given by the pointer into the history buffer, while the beginning position is given by subtracting the length of the node's context from the end position. The beginning position of the sub-buffer pointed to by each forward-tree leaf is unique <ref> [CTW95] </ref>. Therefore, there are no more forward-tree leaves than there are input symbols. Since all internal nodes have at least two forward-children (actual nodes have at least two string-labeled outgoing transitions), there are no more internal nodes (or equivalently, actual nodes), than there are input symbols. <p> It illuminates principled solutions for curbing counterproductive model growth. 3. It provides a sufficiently general on-line Markov model that can be parameterized to exactly emulate many other influential algorithms from the literature, including many popular FSMX models <ref> [CTW95, CW84b, Fur91, RC89, Ris83, Ris86a, WLZ92] </ref>. <p> This means, for one thing, that DMC FSMs cannot recognize infinitely repeating patterns, a common capability of more general FSMs. Neither, however, can any of the popular FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref>, all of which can determine the current model state by locating the state whose single associated finite context is a maximal suffix of the given input string. <p> This contradicts the conventional wisdom that FSMX models "generalize the class of Markov Models" [Ris86a]. 4.5.4 Structural Comparison The context () of a DMC state is analogous to the finite contexts of the states in FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> vis a vis the terms of the definition of C k (), and the fact that every string that takes a DMC model to a given state s i ends in context (s i ). <p> A more important distinction is DMC's capacity for variable-length minimal extensions of a context. This capacity alone sets DMC's (Markovian) data model apart from all FSMX models. Techniques that build FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> all grow models with single-character minimal extensions. <p> In this case, the selected order, which we tally for our experiments, is counted as the order of the maximum-order excited state plus one. heuristic, S 2 : Select the min-order excited state with one out-edge <ref> [CTW95] </ref>. percolating, S 3 : See companion paper [Bun97a]. The construction of [WLZ92] (WLZ) uses an order-bounded state selector that produces the same results as our percolating state selector does in a model that has the same structural order bound, assuming that both models use the same selection threshold. <p> However, quite often the abstract differences among apparently distinct approaches can be expressed in a single technique as different values of parameters that have simple connotations. 1 The seven algorithms are DMC [CH87], GDMC [TR93], PPM [Mof90], PPM* <ref> [CTW95] </ref>, WLZ [WLZ92], plus BestFSMX [Bun97b] and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far ([Bun96, Chapter 9]). <p> Moffat's 1990 implementation, PPMC [Mof90], of Cleary and Witten's 1984 PPM algorithm [CW84b], remained unchallenged until Cleary, et al. did away with PPM's order bound to produce PPM* in 1995 <ref> [CTW95] </ref>. The authors claimed that PPM* outperformed PPMC in their paper. However, PPMC was known to achieve superior compression performance as the order bound increased up to 5 [Mof90], after which its performance starts to decline. <p> The transformation describes an on-line linear-space suffix-tree construction algorithm. We started the work late in 1994, upon receiving a preprint of <ref> [CTW95] </ref>. However, Ukkonen published an on-line suffix-tree construction algorithm [Ukk95] first, in 1995, while we were combining ours with state selection and mixtures in the implementation of our taxonomy.
Reference: [CW84a] <author> J. G. Cleary and I. H. Witten. </author> <title> A comparison of enumerative and adaptive codes. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 30(2) </volume> <pages> 306-315, </pages> <year> 1984. </year>
Reference-contexts: On-line models, which are incrementally reconstructed by the decoder from the decoded data sequence, require no specific transmission. However, the prediction inaccuracies endured early in the source stream by on-line techniques increase the overall length of the encoded sequence, just as a static-model preamble would. It has been shown <ref> [BCW90, CW84a, Ris89] </ref> that we can approach the MDL with online models, and that there exist circumstances where an off-line model will perform arbitrarily worse than any on-line model. In practice, on-line models do work better than off-line models. <p> is not explicitly coded: the deterministic algorithm of the encoder is emulated by the decoder to deduce the state that was selected for coding without any side-information about the actual model. * In on-line modeling, the coding penalty is incorporated into the inaccurate probability estimates from early in the sequence <ref> [CW84a] </ref>.
Reference: [CW84b] <author> J. G. Cleary and I. H. Witten. </author> <title> Data compression using adaptive coding and partial string matching. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 32(4) </volume> <pages> 396-402, </pages> <year> 1984. </year>
Reference-contexts: Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique <ref> [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94] </ref>. Other techniques, namely any variant of the elegant Ziv-Lempel techniques [ZL77, ZL78, MW85, Wel84, FG89, BB92], trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85]. <p> This compares favorably with the the expected 2:1 compression of Unix compress, or the expected 2.5:1 compression of gzip, which are based upon [ZL78] and [ZL77], respectively. * PPM <ref> [CW84b] </ref>, or Prediction by Partial Matching, grows an m-ary alphabet FSMX model aggressively and on-demand, using lazy evaluation of state refinements and a fixed global order bound. PPM's reign as the best-compressing modeling algorithm has stood unchallenged for the 12 years preceding this thesis. <p> Once we have transformed these popular techniques to a unified (suffix-tree) structure, we systematically improve their performance. Chapter 3 A MINIMAL SUFFIX-TREE IMPLEMENTATION OF PPM AND PPM* Prediction By Partial Matching (PPM) has set the standard in lossless data compression research since its introduction in 1984 <ref> [CW84b] </ref>. PPM constructs a bounded-order Markov model such that the maximum order of the excited states is allowed to vary within the order bound. Recently, a new variant, PPM*, was developed that eliminated the (arbitrary) order bound in PPM's model structure [CTW95]. <p> It illuminates principled solutions for curbing counterproductive model growth. 3. It provides a sufficiently general on-line Markov model that can be parameterized to exactly emulate many other influential algorithms from the literature, including many popular FSMX models <ref> [CTW95, CW84b, Fur91, RC89, Ris83, Ris86a, WLZ92] </ref>. <p> This means, for one thing, that DMC FSMs cannot recognize infinitely repeating patterns, a common capability of more general FSMs. Neither, however, can any of the popular FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref>, all of which can determine the current model state by locating the state whose single associated finite context is a maximal suffix of the given input string. <p> This contradicts the conventional wisdom that FSMX models "generalize the class of Markov Models" [Ris86a]. 4.5.4 Structural Comparison The context () of a DMC state is analogous to the finite contexts of the states in FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> vis a vis the terms of the definition of C k (), and the fact that every string that takes a DMC model to a given state s i ends in context (s i ). <p> A more important distinction is DMC's capacity for variable-length minimal extensions of a context. This capacity alone sets DMC's (Markovian) data model apart from all FSMX models. Techniques that build FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> all grow models with single-character minimal extensions. <p> This way, larger input alphabets can be economically accommodated, since only conditioning contexts that have been seen before will be represented by the model. Probability estimation can proceed using the recursive blending technique of PPM <ref> [CW84b] </ref>, or by lazy evaluation of DMC's initial frequency distribution assumption. <p> The best-performing method in the data compression literature for computing probability estimates of sequences on-line using a suffix-tree model is the blending technique used by PPM <ref> [CW84b, Mof90] </ref>. Blending can be viewed as a bottom-up recursive procedure for computing a mixture, barring one missing term for each level of the recursion, where a mixture is basically a weighted average of several probability estimates. <p> That is, count (s) = X a : a is not excluded count [a; s; u (s)] &gt; 0 count (a; s): 91 Unless stated otherwise, we shall assume that exclusions are enabled in all computations described from here on. 6.4.2 Blending's Missing Term Blending <ref> [CW84b] </ref> evaluates the ancestor likelihood P e (ajsuffix (s)) before novel event updates, but at all subsequent occurrences of any string in the set L (s) a. <p> Moffat's 1990 implementation, PPMC [Mof90], of Cleary and Witten's 1984 PPM algorithm <ref> [CW84b] </ref>, remained unchallenged until Cleary, et al. did away with PPM's order bound to produce PPM* in 1995 [CTW95]. The authors claimed that PPM* outperformed PPMC in their paper.
Reference: [FG89] <author> E. Fiala and D. Greene. </author> <title> Data compression with finite windows. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 490-505, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94]. Other techniques, namely any variant of the elegant Ziv-Lempel techniques <ref> [ZL77, ZL78, MW85, Wel84, FG89, BB92] </ref>, trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85].
Reference: [FGC93] <author> G. Feygin, P. G. Gulak, and P. Chow. </author> <title> Minimizing error and VLSI complexity in the multiplication-free approximation of arithmetic coding. </title> <booktitle> In Proceedings Data Compression Conference, </booktitle> <pages> pages 118-124. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> March </month> <year> 1993. </year>
Reference-contexts: good compression performance, required trivial timing and buffering overhead, and was an order of magnitude faster than the state of the art, she would abandon the techniques presented here, just as she did seven years ago [BB92, Bun92]. 6 Indeed, this has already happened in the case of arithmetic coding <ref> [RM89, CKW91, HV92, FGC93, MSWB93] </ref>. 6 be that it is simply not worthwhile to improve the compression performance further. Even so, there remains one other very important motivation for this research. These same stochastic modeling techniques are applicable to other computing disciplines in a straightforward manner. <p> Implementations of arithmetic coding have been honed to near perfection over the years. There are now several fast and extremely accurate approximations which eliminate the divisions and multiplications that are required by straightforward implementations <ref> [RM89, CKW91, FGC93] </ref>. 2.4 A General Strategy for Optimal Source Modeling Assume the string was generated by a stochastic FSM. Given a string to compress, we first assume that it was generated by a finite stochastic source belonging to a tractable but useful class of models.
Reference: [Fur91] <author> G. Furlan. </author> <title> An enhancement to universal modeling algorithm `context' for real-time applications to image compression. </title> <booktitle> In IEEE Transactions on Acoustics Speech and Signal Processing, </booktitle> <pages> pages 2777-2780, </pages> <year> 1991. </year> <month> 152 </month>
Reference-contexts: model structure used for computing probability estimates was based upon model entropies was on the right track. 35 * There are later variants of Context that handle arbitrarily-sized m-ary alphabets using lazy evaluation of state refinements (e.g., [Ris86b]) and/or flawed 4 but efficient MDL-based 5 implementations of Rissanen's top-down pruning <ref> [Fur91] </ref>. <p> It illuminates principled solutions for curbing counterproductive model growth. 3. It provides a sufficiently general on-line Markov model that can be parameterized to exactly emulate many other influential algorithms from the literature, including many popular FSMX models <ref> [CTW95, CW84b, Fur91, RC89, Ris83, Ris86a, WLZ92] </ref>. <p> This means, for one thing, that DMC FSMs cannot recognize infinitely repeating patterns, a common capability of more general FSMs. Neither, however, can any of the popular FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref>, all of which can determine the current model state by locating the state whose single associated finite context is a maximal suffix of the given input string. <p> This contradicts the conventional wisdom that FSMX models "generalize the class of Markov Models" [Ris86a]. 4.5.4 Structural Comparison The context () of a DMC state is analogous to the finite contexts of the states in FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> vis a vis the terms of the definition of C k (), and the fact that every string that takes a DMC model to a given state s i ends in context (s i ). <p> A more important distinction is DMC's capacity for variable-length minimal extensions of a context. This capacity alone sets DMC's (Markovian) data model apart from all FSMX models. Techniques that build FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> all grow models with single-character minimal extensions. <p> The frequency of a particular edge does say something about the probability of the state it leads to. Thus the original E () crudely approximates the contribution of that destination state to the entropy of the model. The other approximation extreme is exemplified by Rissanen [Ris86a] and Furlan <ref> [Fur91] </ref>. These authors closely approximate the relative entropies between states and their minimal 75 extension states using counters which keep track of the code-length differences. 2. The refinement eligibility criterion E () should prevent transitions from being redirected more than once, under most circumstances. 5 3. <p> This means that a node must always be considered for selection along with its siblings. To select, for example, the excited state with the lowest expected codelength, or the minimum-order excited state whose expected codelength is better than that of its excited child <ref> [Fur91] </ref>, is incorrect, not merely suboptimal. This is because the children of a state s (i.e., those nodes whose contexts correspond to minimal extensions of the state's context) may have better performance than the state s, even while the currently excited child of s has worse performance than s does.
Reference: [Gua80] <author> M. Guazzo. </author> <title> A general minimum-redundancy source-coding algorithm. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 26(1) </volume> <pages> 15-25, </pages> <month> January </month> <year> 1980. </year>
Reference-contexts: The symbol-wise probabilities required for on-line coding are obtained in the modeling process, which in one pass constructs an adaptive finite-state model. Arithmetic coding <ref> [Pas76, Ris76, RL79, Rub79, Gua80] </ref> codes each symbol with a number of bits that can be made arbitrarily close to the negative logarithm of the symbol's probability, by increasing the size of the registers used to do the arithmetic.
Reference: [How93] <author> P. G. Howard. </author> <title> The Design and Analysis of Efficient Lossless Data Compression Systems. </title> <type> PhD thesis, </type> <institution> Brown University, </institution> <year> 1993. </year>
Reference-contexts: Alternatively, at regular intervals, all the frequencies in the model can be scaled by a small constant, which would implement an exponential decay function <ref> [How93] </ref>. Or, the same process could be carried on locally, on a per-state basis, when the state's total frequency exceeded a threshold [Mof90]. <p> The authors claimed that PPM* outperformed PPMC in their paper. However, PPMC was known to achieve superior compression performance as the order bound increased up to 5 [Mof90], after which its performance starts to decline. In 1993, Howard published a simple change to PPMC's escape mechanism, called PPMD <ref> [How93] </ref>: add .5 instead of 1.0 to the escape count and scanned event count, whenever a novel event is seen. PPMD gets even better performance than PPMC.
Reference: [HU79] <author> J. E. Hopcroft and J. D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1979. </year>
Reference-contexts: Three interesting subfamilies of stochastic FSM sources from the literature are Markov sources (Section 2.2), FSMX sources [Ris86a], and finite-order FSM sources [Ash65], which are also called finite-context automata [BM89]. The containment relationships between these model classes are shown within the Chomsky Hierarchy <ref> [HU79] </ref> in Figure 2.3. * In Markov sources, the next state is completely determined by the previous 26 state and the current source symbol.
Reference: [HV92] <author> P. G. Howard and J. S. Vitter. </author> <title> Practical implementations of arithmetic coding. </title> <editor> In J. A. Storer, editor, </editor> <booktitle> Image and Text Compression, </booktitle> <pages> pages 85-112. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Norwell Massachusetts, </address> <year> 1992. </year>
Reference-contexts: good compression performance, required trivial timing and buffering overhead, and was an order of magnitude faster than the state of the art, she would abandon the techniques presented here, just as she did seven years ago [BB92, Bun92]. 6 Indeed, this has already happened in the case of arithmetic coding <ref> [RM89, CKW91, HV92, FGC93, MSWB93] </ref>. 6 be that it is simply not worthwhile to improve the compression performance further. Even so, there remains one other very important motivation for this research. These same stochastic modeling techniques are applicable to other computing disciplines in a straightforward manner.
Reference: [Lan83] <author> G. G. Langdon. </author> <title> A note on the Ziv-Lempel model for compressing individual sequences. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 29(2) </volume> <pages> 284-287, </pages> <year> 1983. </year>
Reference-contexts: Though only a few practical data compression meth 5 ods use explicit stochastic models, researchers have systematically proved that the other known practical methods can be exactly and efficiently emulated with adaptive stochastic models <ref> [Ris83, Lan83, Bel86] </ref>. Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94].
Reference: [Lar96] <author> N. J. Larsson. </author> <title> Extended application of suffix trees to data compression. </title> <booktitle> In Proceedings Data Compression Conference, </booktitle> <pages> pages 190-199, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: The independently derived algorithms are quite similar|even down to the use of the term "splitting." Both algorithms are functionally related to McCreight's suffix-tree construction algorithm [McC76]. Additionally, Larsson <ref> [Lar96] </ref> applied Ukkonen's construction to PPM* and described how to make the suffix-tree represent a sliding window of input history. Thus, our on-line suffix-tree construction algorithm is not the first, nor is our application of on-line suffix-tree construction algorithms to implementing PPM*.
Reference: [LR83] <author> G. G. Langdon and J. J. Rissanen. </author> <title> A double-adaptive file compression algorithm. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 31(11) </volume> <pages> 1253-1255, </pages> <year> 1983. </year>
Reference-contexts: Therefore, universal on-line models are doubly adaptive; both the structure and the parameters change to reflect the properties of the data. The separation of modeling from coding, and the use of doubly adaptive stochastic models is the modern data compression paradigm, 1 pioneered by Glen Langdon and Jorma Rissanen <ref> [RL81, LR83, Ris83] </ref>, and further developed in [Ris86a, MGZ89]. By "paradigm," I mean an ideal and general blueprint for other solutions to follow, and Rissanen's and Langdon's certainly fits this strict interpretation. <p> The first community, represented in information and coding theory literature, has produced the following constructions: * The granddaddy of the suffix-tree stochastic modeling algorithms is the Doubly Adaptive File Compression algorithm, or DAFC <ref> [LR83] </ref>. DAFC introduced the now sacred notions of adapting model structure and frequency data simultaneously, and of separating coding from modeling in on-line data compression algorithms.
Reference: [McC76] <author> E. McCreight. </author> <title> A space-economical suffix tree construction algorithm. </title> <journal> Journal of the ACM, </journal> <volume> 23(2) </volume> <pages> 262-272, </pages> <month> April </month> <year> 1976. </year>
Reference-contexts: The independently derived algorithms are quite similar|even down to the use of the term "splitting." Both algorithms are functionally related to McCreight's suffix-tree construction algorithm <ref> [McC76] </ref>. Additionally, Larsson [Lar96] applied Ukkonen's construction to PPM* and described how to make the suffix-tree represent a sliding window of input history. Thus, our on-line suffix-tree construction algorithm is not the first, nor is our application of on-line suffix-tree construction algorithms to implementing PPM*.
Reference: [MGZ89] <author> N. Merhav, M. Gutman, and J. Ziv. </author> <title> On the estimation of the order of a Markov chain and universal data compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 35(5) </volume> <pages> 1014-1019, </pages> <year> 1989. </year>
Reference-contexts: The separation of modeling from coding, and the use of doubly adaptive stochastic models is the modern data compression paradigm, 1 pioneered by Glen Langdon and Jorma Rissanen [RL81, LR83, Ris83], and further developed in <ref> [Ris86a, MGZ89] </ref>. By "paradigm," I mean an ideal and general blueprint for other solutions to follow, and Rissanen's and Langdon's certainly fits this strict interpretation.
Reference: [Mof90] <author> A. Moffat. </author> <title> Implementing the PPM data compression scheme. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(11) </volume> <pages> 1917-1921, </pages> <year> 1990. </year>
Reference-contexts: Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique <ref> [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94] </ref>. Other techniques, namely any variant of the elegant Ziv-Lempel techniques [ZL77, ZL78, MW85, Wel84, FG89, BB92], trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85]. <p> For example, in this thesis, we introduce an algorithm that gets an average codelenth of 2.177 bits per character (bpc) on the files of the Calgary Corpus. The benchmark PPM implementation <ref> [Mof90] </ref>, gets 2.48 bpc on the same data. These codelengths correspond to expected probability estimates of 0.1792 and 0.2211 per character, respectively. Therefore, our algorithm improves upon PPM's compression performance by 12.2%, while improving the probability estimates by 23.4%. <p> PPM's reign as the best-compressing modeling algorithm has stood unchallenged for the 12 years preceding this thesis. PPM's success is due to its ad hoc "blending" technique for computing probability estimates, and the careful engineering behind Moffat's 1990 imple mentation, PPMC <ref> [Mof90] </ref>. * DMC [CH87], or Dynamic Markov Modeling, grows an unstructured binary-alphabet finite-state machine by redirecting any sufficiently popular transition to a new state in a process called "cloning." Miraculously, the maiden implementation of this simple idea delivered performance competitive with carefully engineered and tuned implementations of PPM. <p> Note that it is not necessary to represent the suffix relationship explicitly by storing a suffix pointer at each state <ref> [Mof90, CTW95] </ref>. The conditioning context partition of any PPM variant is induced by the function context : S ! A fl . <p> Probability estimation can proceed using the recursive blending technique of PPM [CW84b], or by lazy evaluation of DMC's initial frequency distribution assumption. For example, if blending method PPMC <ref> [Mof90] </ref> were used, the frequency on the suffix () edge should always equal the number of distinct symbols which have been seen when in a given state, and the application of such optimizations as exclusion and update exclusion [BCW90] is straightforward. <p> We shall refer to this technique as full updates. A different approach, called update exclusion <ref> [Mof90] </ref>, so effectively and consistently improves the probability estimates produced by PPM that its application and combination with state selection in all suffix-tree models, including PPM fl , warrants careful consideration and, as we explain, some modification for combination with state selection. 5.1 Model Semantics II: Update Exclusion vs. <p> Full Updates Generally speaking, full updates correctly update the frequencies at every FSM model that is simulated by the suffix-tree, while update exclusion correctly updates the frequencies at the single FSM model that corresponds to the maximum suffix-tree frontier. As defined in <ref> [Mof90] </ref>, update exclusion is a frequency-update technique that increments the counts of event ajs when s is excited and either symbol a is novel at state s, s is the parent of a state where a is novel, or s is the maximum-order excited state. <p> The best-performing method in the data compression literature for computing probability estimates of sequences on-line using a suffix-tree model is the blending technique used by PPM <ref> [CW84b, Mof90] </ref>. Blending can be viewed as a bottom-up recursive procedure for computing a mixture, barring one missing term for each level of the recursion, where a mixture is basically a weighted average of several probability estimates. <p> Alternatively, at regular intervals, all the frequencies in the model can be scaled by a small constant, which would implement an exponential decay function [How93]. Or, the same process could be carried on locally, on a per-state basis, when the state's total frequency exceeded a threshold <ref> [Mof90] </ref>. However, regardless of whatever merit direct techniques for recency-weighting stored frequencies may have (none has been shown to consistently improve predictions of blended techniques), these approaches each add an additional feature to the model. <p> This is best accomplished with a technique known as exclusion (not to be confused with update exclusion), which was developed for PPM <ref> [Mof90] </ref>. The basic idea is this: when exclusion is enabled for the model, only consider the frequencies of event ajsuffix (s) if the higher-order descendant s is currently excited and event ajs has not occurred before. <p> However, quite often the abstract differences among apparently distinct approaches can be expressed in a single technique as different values of parameters that have simple connotations. 1 The seven algorithms are DMC [CH87], GDMC [TR93], PPM <ref> [Mof90] </ref>, PPM* [CTW95], WLZ [WLZ92], plus BestFSMX [Bun97b] and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far ([Bun96, Chapter 9]). <p> Moffat's 1990 implementation, PPMC <ref> [Mof90] </ref>, of Cleary and Witten's 1984 PPM algorithm [CW84b], remained unchallenged until Cleary, et al. did away with PPM's order bound to produce PPM* in 1995 [CTW95]. The authors claimed that PPM* outperformed PPMC in their paper. <p> The authors claimed that PPM* outperformed PPMC in their paper. However, PPMC was known to achieve superior compression performance as the order bound increased up to 5 <ref> [Mof90] </ref>, after which its performance starts to decline. In 1993, Howard published a simple change to PPMC's escape mechanism, called PPMD [How93]: add .5 instead of 1.0 to the escape count and scanned event count, whenever a novel event is seen. PPMD gets even better performance than PPMC. <p> Given a shorthand version of our nomenclature, the standard reference version of PPM <ref> [Mof90] </ref> becomes PPMC3X if it is implemented with symbol transitions, and PPMC*3X if implemented with string transitions. The prior state-of-the-art PPM variant, which has order 5, update exclusion, and uses escape mechanism D, is PPMD5X or PPMD*5X, depending on the suffix-tree implementation. <p> In addition, we undertook improving the performance and memory consumption of three baseline data compression algorithms: * We improved PPM*'s performance by 7% (Table 9.9). * Our string-transition suffix-tree implementation reduced PPM*'s model size by half (Section 3.5). * We improved PPM's performance by 12% over the standard reference <ref> [Mof90] </ref>, and by 5% over the best PPM variant consisting of technologies available prior to this thesis (Tables 9.2 and 9.8). 147 * Our order-bounded string-transition suffix-tree implementation reduced PPM's model size extensively for higher order models, which were previously impossible to execute (Table 9.11). * Our lazy cloning implementation, LazyDMC,
Reference: [MSWB93] <author> A. Moffat, N. Sharman, I. H. Witten, and T. C. Bell. </author> <title> An empirical evaluation of coding methods for multi-symbol alphabets. </title> <booktitle> In Proceedings Data Compression Conference, </booktitle> <pages> pages 108-117, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: good compression performance, required trivial timing and buffering overhead, and was an order of magnitude faster than the state of the art, she would abandon the techniques presented here, just as she did seven years ago [BB92, Bun92]. 6 Indeed, this has already happened in the case of arithmetic coding <ref> [RM89, CKW91, HV92, FGC93, MSWB93] </ref>. 6 be that it is simply not worthwhile to improve the compression performance further. Even so, there remains one other very important motivation for this research. These same stochastic modeling techniques are applicable to other computing disciplines in a straightforward manner. <p> Chapter 9 PERFORMANCE MEASUREMENTS In this chapter, we measure the predictive ability of our models by running them on the files of the Calgary Corpus [BCW90]. All probability estimates are coded using a floating-point m-ary arithmetic coder that we based upon the popular integer coder implemented in <ref> [WNC87, MSWB93] </ref>. Arithmetically coded probability estimates are an excellent measure of model performance because they compute a tight, infinite-precision upper bound on the log likelihood of the model, given the input sequence.
Reference: [MW85] <author> V. S. Miller and M. N. Wegman. </author> <title> Variations on a theme by Ziv and Lempel. </title> <editor> In A. Apostolico and Z. Galil, editors, </editor> <booktitle> Combinatorial Algorithms 153 on Words, </booktitle> <pages> pages 131-140. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, New, York, </address> <year> 1985. </year>
Reference-contexts: Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94]. Other techniques, namely any variant of the elegant Ziv-Lempel techniques <ref> [ZL77, ZL78, MW85, Wel84, FG89, BB92] </ref>, trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85].
Reference: [Pas76] <author> R. </author> <title> Pasco. Source Coding Algorithms for Fast Data Compression. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1976. </year>
Reference-contexts: The symbol-wise probabilities required for on-line coding are obtained in the modeling process, which in one pass constructs an adaptive finite-state model. Arithmetic coding <ref> [Pas76, Ris76, RL79, Rub79, Gua80] </ref> codes each symbol with a number of bits that can be made arbitrarily close to the negative logarithm of the symbol's probability, by increasing the size of the registers used to do the arithmetic.
Reference: [Paz71] <author> A. Paz. </author> <title> Introduction to Probabilistic Automata. </title> <publisher> Academic Press, </publisher> <address> 111 Fifth Avenue, New York, New York 10003, </address> <year> 1971. </year>
Reference-contexts: A formal definition of ergodic is beyond the present scope, as is most formal probability theory. However, the ergodicity of an FSM source has been proved to be a decidable graph-theoretic property <ref> [Paz71, Sha48] </ref>. An FSM is ergodic if the graph consisting of its states and transitions that have non-zero probability meets both of the following conditions: 1. The graph is strongly connected, i.e., every state is reachable from every other. 2.
Reference: [Rab89] <author> L. R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <journal> Proc. IEEE, </journal> <volume> 77(2) </volume> <pages> 257-286, </pages> <year> 1989. </year>
Reference-contexts: For example, the lattice-structured Hidden Markov Models (HMMs) <ref> [Rab89] </ref> that are commonly used in speech and genomics model position-related data dependencies explicitly, but not context-related dependencies. Thus, speech and genomic sequences are currently modeled with zero-order statistics.
Reference: [RC89] <author> T. V. Ramabadran and D. L. Cohn. </author> <title> An adaptive algorithm for the compression of computer data. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 37(4) </volume> <pages> 317-324, </pages> <year> 1989. </year>
Reference-contexts: Context's dynamic pruning technique, or "state-selection", was companion to some fundamental asymptotic results in sequential coding theory. * The CRAM algorithm (Cohn and Ramabadran's Acronym Mystery?) <ref> [RC89] </ref> grows a binary FSMX model more cautiously. It employs a suboptimal state-entropy-based heuristic that adds minimal extensions to any state whose entropy is greater than some threshold value. <p> It illuminates principled solutions for curbing counterproductive model growth. 3. It provides a sufficiently general on-line Markov model that can be parameterized to exactly emulate many other influential algorithms from the literature, including many popular FSMX models <ref> [CTW95, CW84b, Fur91, RC89, Ris83, Ris86a, WLZ92] </ref>. <p> This means, for one thing, that DMC FSMs cannot recognize infinitely repeating patterns, a common capability of more general FSMs. Neither, however, can any of the popular FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref>, all of which can determine the current model state by locating the state whose single associated finite context is a maximal suffix of the given input string. <p> This contradicts the conventional wisdom that FSMX models "generalize the class of Markov Models" [Ris86a]. 4.5.4 Structural Comparison The context () of a DMC state is analogous to the finite contexts of the states in FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> vis a vis the terms of the definition of C k (), and the fact that every string that takes a DMC model to a given state s i ends in context (s i ). <p> A more important distinction is DMC's capacity for variable-length minimal extensions of a context. This capacity alone sets DMC's (Markovian) data model apart from all FSMX models. Techniques that build FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> all grow models with single-character minimal extensions.
Reference: [Ris76] <author> J. J. Rissanen. </author> <title> Generalized Kraft inequality and arithmetic coding. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 20 </volume> <pages> 198-203, </pages> <month> May </month> <year> 1976. </year>
Reference-contexts: The symbol-wise probabilities required for on-line coding are obtained in the modeling process, which in one pass constructs an adaptive finite-state model. Arithmetic coding <ref> [Pas76, Ris76, RL79, Rub79, Gua80] </ref> codes each symbol with a number of bits that can be made arbitrarily close to the negative logarithm of the symbol's probability, by increasing the size of the registers used to do the arithmetic.
Reference: [Ris83] <author> J. J. Rissanen. </author> <title> A universal data compression system. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 29(5) </volume> <pages> 656-664, </pages> <year> 1983. </year>
Reference-contexts: Though only a few practical data compression meth 5 ods use explicit stochastic models, researchers have systematically proved that the other known practical methods can be exactly and efficiently emulated with adaptive stochastic models <ref> [Ris83, Lan83, Bel86] </ref>. Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94]. <p> Therefore, universal on-line models are doubly adaptive; both the structure and the parameters change to reflect the properties of the data. The separation of modeling from coding, and the use of doubly adaptive stochastic models is the modern data compression paradigm, 1 pioneered by Glen Langdon and Jorma Rissanen <ref> [RL81, LR83, Ris83] </ref>, and further developed in [Ris86a, MGZ89]. By "paradigm," I mean an ideal and general blueprint for other solutions to follow, and Rissanen's and Langdon's certainly fits this strict interpretation. <p> Developed in the days of expensive memory, DAFC starts with a complete order-1 m-ary suffix tree, then dynamically adds and prunes a fixed number of order-2 states as needed. DAFC handles run-length subsequences separately. * Rissanen's algorithm, Context <ref> [Ris83] </ref>, aggressively grows an arbitrarily large binary-alphabet FSMX model. It adds all possible (there are only 2) refining children to a state as soon as the state's conditioning context occurs in the input sequence a second time, and then uses a top-down neighborhood-entropy-based heuristic for dynamically pruning the model. <p> It illuminates principled solutions for curbing counterproductive model growth. 3. It provides a sufficiently general on-line Markov model that can be parameterized to exactly emulate many other influential algorithms from the literature, including many popular FSMX models <ref> [CTW95, CW84b, Fur91, RC89, Ris83, Ris86a, WLZ92] </ref>. <p> This means, for one thing, that DMC FSMs cannot recognize infinitely repeating patterns, a common capability of more general FSMs. Neither, however, can any of the popular FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref>, all of which can determine the current model state by locating the state whose single associated finite context is a maximal suffix of the given input string. <p> In that case, C k (s i ) = A fl context (s i ) [ s d 2extns k (s i ) A fl context (s d ); which is logically equivalent to the closed-form transition function of FSMX models. FSMX models, defined by Rissanen <ref> [Ris83] </ref>, are characterized by their state-set and transition function: * The states satisfy a suffix property, where states mapped to unique finite strings, or conditioning contexts, and for every state with context x, there exist states for each proper suffix of x. <p> This contradicts the conventional wisdom that FSMX models "generalize the class of Markov Models" [Ris86a]. 4.5.4 Structural Comparison The context () of a DMC state is analogous to the finite contexts of the states in FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> vis a vis the terms of the definition of C k (), and the fact that every string that takes a DMC model to a given state s i ends in context (s i ). <p> A more important distinction is DMC's capacity for variable-length minimal extensions of a context. This capacity alone sets DMC's (Markovian) data model apart from all FSMX models. Techniques that build FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> all grow models with single-character minimal extensions. <p> Note that stochastic complexity assigns a coding penalty to each model state. The penalty is a lower bound on the number of bits required to encode that state. Most other treatments of the state-selection approach to on-line modeling (e.g., <ref> [Ris83, WLZ92] </ref>) require that a refinement (e.g., the children or deeper descendants) of a state be selected in preference to that state if the performance of that refinement improves the performance of the model frontier containing the original state by an amount 101 exceeding the cost of encoding the states comprising <p> Top-down, select the minimum order excited state for which no complete fron tier of its subtree improves its expected codelength. The first approach was introduced, using entropies instead of MDLs <ref> [Ris83] </ref>, and then later using MDLs [Ris86a]. This technique systematically under-estimates the local order of the model [WLZ92]; it is a hill-climbing technique that can get stuck in local minima. The second approach is introduced here as an obvious complement to the top-down hill-climbing approach, to complete the taxonomy. <p> The remaining two command lines approximate the original binary-alphabet Context algorithm <ref> [Ris83] </ref> and a 256-ary variant [Ris86b]. The emulations are only approximate because the true Context model is a non-Markovian FSMX finite state machine, which cannot be represented state-for-state with an FSM that has explicit transitions.
Reference: [Ris86a] <author> J. J. Rissanen. </author> <title> Complexity of strings in the class of Markov sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 32(4) </volume> <pages> 526-532, </pages> <year> 1986. </year>
Reference-contexts: The separation of modeling from coding, and the use of doubly adaptive stochastic models is the modern data compression paradigm, 1 pioneered by Glen Langdon and Jorma Rissanen [RL81, LR83, Ris83], and further developed in <ref> [Ris86a, MGZ89] </ref>. By "paradigm," I mean an ideal and general blueprint for other solutions to follow, and Rissanen's and Langdon's certainly fits this strict interpretation. <p> The more restricted the assumed class, the weaker the result. Three interesting subfamilies of stochastic FSM sources from the literature are Markov sources (Section 2.2), FSMX sources <ref> [Ris86a] </ref>, and finite-order FSM sources [Ash65], which are also called finite-context automata [BM89]. The containment relationships between these model classes are shown within the Chomsky Hierarchy [HU79] in Figure 2.3. * In Markov sources, the next state is completely determined by the previous 26 state and the current source symbol. <p> The modeling algorithms plotted here are explained in Section 2.7. 28 FSMX sources. For example, the authority responsible for defining the FSMX class introduces the distinction in <ref> [Ris86a] </ref> as "a useful subclass of FSM sources which includes the Markov sources," and later claims that "This class, which we denote FSMX, generalizes the class of Markov sources." Recently, the following description appeared in [WST95]: "These [FSMX] sources form a subclass of the set of sources defined by finite state <p> The fact that the improvements developed and tested later in this work apply to FSMX models is important because FSMX models are ubiquitous in the information theoretic literature. FSMX models <ref> [Ris86a] </ref>, are suffix-tree context models with single-symbol minimal extensions such that the next state given by the transition function on a given state and input symbol can have Markov order that differs arbitrarily from the order of the given state. <p> The impact of our characterization is threefold. 1. It proves that although DMC constructs a unifilar finite-order Markov FSM, DMC states cannot be uniquely characterized by single conditioning contexts, and therefore DMC models do not belong to the class of FSMX automata <ref> [Ris86a] </ref>, which purportedly contain all finite order Markovian FSMs. 2. It illuminates principled solutions for curbing counterproductive model growth. 3. <p> It illuminates principled solutions for curbing counterproductive model growth. 3. It provides a sufficiently general on-line Markov model that can be parameterized to exactly emulate many other influential algorithms from the literature, including many popular FSMX models <ref> [CTW95, CW84b, Fur91, RC89, Ris83, Ris86a, WLZ92] </ref>. <p> Thus, the context () of a state in a DMC model is analogous to the context of a state in an FSMX model <ref> [Ris86a] </ref>. However, there are differences. For one thing, context (prefix (s i )) is exactly one symbol shorter than context (s i ), while context (s i ) may be arbitrarily longer than context (suffix (s i )). <p> This means, for one thing, that DMC FSMs cannot recognize infinitely repeating patterns, a common capability of more general FSMs. Neither, however, can any of the popular FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref>, all of which can determine the current model state by locating the state whose single associated finite context is a maximal suffix of the given input string. <p> This contradicts the conventional wisdom that FSMX models "generalize the class of Markov Models" <ref> [Ris86a] </ref>. 4.5.4 Structural Comparison The context () of a DMC state is analogous to the finite contexts of the states in FSMX models [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] vis a vis the terms of the definition of C k (), and the fact that every string that takes <p> This contradicts the conventional wisdom that FSMX models "generalize the class of Markov Models" [Ris86a]. 4.5.4 Structural Comparison The context () of a DMC state is analogous to the finite contexts of the states in FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> vis a vis the terms of the definition of C k (), and the fact that every string that takes a DMC model to a given state s i ends in context (s i ). <p> A more important distinction is DMC's capacity for variable-length minimal extensions of a context. This capacity alone sets DMC's (Markovian) data model apart from all FSMX models. Techniques that build FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> all grow models with single-character minimal extensions. <p> The frequency of a particular edge does say something about the probability of the state it leads to. Thus the original E () crudely approximates the contribution of that destination state to the entropy of the model. The other approximation extreme is exemplified by Rissanen <ref> [Ris86a] </ref> and Furlan [Fur91]. These authors closely approximate the relative entropies between states and their minimal 75 extension states using counters which keep track of the code-length differences. 2. The refinement eligibility criterion E () should prevent transitions from being redirected more than once, under most circumstances. 5 3. <p> Top-down, select the minimum order excited state for which no complete fron tier of its subtree improves its expected codelength. The first approach was introduced, using entropies instead of MDLs [Ris83], and then later using MDLs <ref> [Ris86a] </ref>. This technique systematically under-estimates the local order of the model [WLZ92]; it is a hill-climbing technique that can get stuck in local minima. The second approach is introduced here as an obvious complement to the top-down hill-climbing approach, to complete the taxonomy. It systematically over-estimates local order.
Reference: [Ris86b] <author> J. J. Rissanen. </author> <title> An image compression system. </title> <booktitle> In Proceedings HILCOM 86, </booktitle> <year> 1986. </year>
Reference-contexts: However, the idea of generalizing DAFC models so that the model structure used for computing probability estimates was based upon model entropies was on the right track. 35 * There are later variants of Context that handle arbitrarily-sized m-ary alphabets using lazy evaluation of state refinements (e.g., <ref> [Ris86b] </ref>) and/or flawed 4 but efficient MDL-based 5 implementations of Rissanen's top-down pruning [Fur91]. <p> The remaining two command lines approximate the original binary-alphabet Context algorithm [Ris83] and a 256-ary variant <ref> [Ris86b] </ref>. The emulations are only approximate because the true Context model is a non-Markovian FSMX finite state machine, which cannot be represented state-for-state with an FSM that has explicit transitions.
Reference: [Ris89] <author> J. J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference-contexts: Current research that seeks to improve upon the compression performance of the best-performing techniques known relies upon explicit, sophisticated data models for probability estimation. Furthermore, the problem of finding a means for minimally describing a sequence is extremely well-motivated <ref> [Ris89] </ref>, and has application to other fields such as pattern recognition, language processing, and cryptography. However, there is a limit to the compressibility of any sequence, and new techniques necessarily work harder than their predecessors to effect ever-decreasing improvements in compression. <p> With off-line modeling, the model is built in a preprocessing pass, and the model is said to be trained off-line. A description of an off-line model must be transmitted in addition to the encoded source message: the shortest such description is known as the Minimal Description Length (MDL) <ref> [Ris89] </ref> of the sequence. On-line models, which are incrementally reconstructed by the decoder from the decoded data sequence, require no specific transmission. However, the prediction inaccuracies endured early in the source stream by on-line techniques increase the overall length of the encoded sequence, just as a static-model preamble would. <p> On-line models, which are incrementally reconstructed by the decoder from the decoded data sequence, require no specific transmission. However, the prediction inaccuracies endured early in the source stream by on-line techniques increase the overall length of the encoded sequence, just as a static-model preamble would. It has been shown <ref> [BCW90, CW84a, Ris89] </ref> that we can approach the MDL with online models, and that there exist circumstances where an off-line model will perform arbitrarily worse than any on-line model. In practice, on-line models do work better than off-line models. <p> However, its acceptance as the standard benchmark has led to the questionable practice of tuning allegedly "universal" algorithms a priori to the test data. The upside 4 See Section 7.3 for an explanation of the flaw. 5 Rissanen's work in coding theory led to his development of Stochastic Complexity <ref> [Ris89] </ref>, which presents the Minimum Description Length or MDL principle, and which has since revolutionized the field of statistical inference. <p> The set of state-selection techniques presented in this chapter combine orthogonally with the other sets of design options covered so far. 7.1 Stochastic Complexity The stochastic complexity of a string is the length of its optimal off-line encoding, that is, its Minimum Description Length, or MDL <ref> [Ris89] </ref>. A string's MDL is the sum of the lengths of an encoding of a model plus the encoding of the string with respect to that model such that the total encoding length is minimal over all possible models within an assumed model class.
Reference: [Ris90] <author> E. A. Riskin. </author> <title> Variable-Rate Vector Quantization of Images. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1990. </year>
Reference-contexts: All data compressors use some sort 2 This is a topic of rate distortion theory, which is rigorously presented in [CT91], and applied to tree-structured pattern classification in [Cho88]. An application to image compression, pruned tree-structured vector quantization [Cho88, Chapter 5], is further developed in <ref> [Ris90] </ref>. 3 Discrete signal data sequences can easily be decorrelated, and thereafter treated as alphabet sequences suitable for modeling with alphabet-oriented techniques, by first recoding each successive sample as the difference between it and the preceding sample. 4 The information-theoretic literature employs the term sequential. 4 of data model, either explicitly
Reference: [RL79] <author> J. J. Rissanen and G. G. Langdon. </author> <title> Arithmetic coding. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 23 </volume> <pages> 146-162, </pages> <month> March </month> <year> 1979. </year>
Reference-contexts: The symbol-wise probabilities required for on-line coding are obtained in the modeling process, which in one pass constructs an adaptive finite-state model. Arithmetic coding <ref> [Pas76, Ris76, RL79, Rub79, Gua80] </ref> codes each symbol with a number of bits that can be made arbitrarily close to the negative logarithm of the symbol's probability, by increasing the size of the registers used to do the arithmetic. <p> Arithmetic coding, and how it interfaces with models, is notoriously difficult to grasp. The following presentation and its illustration in particular are a departure from the usual descriptions, exemplified by <ref> [RL79] </ref> and [WNC87]. We first give an operational description that steps through how a stochastic model and an arithmetic 17 coder/decoder work together to encode and then decode each symbol of a source message. <p> Here, resetting the unstable bits counter inserts an extra 0 into (the untransmitted and unstable portion of) the bit stream, and is called "bit stuffing" <ref> [RL79] </ref>. An alternative to bit stuffing can be found in [WNC87]. In summary, a fixed-precision arithmetic coder incrementally transmits an approximate codepoint that is larger than the exact codepoint. There are two determinants of coder accuracy.
Reference: [RL81] <author> J. J. Rissanen and G. G. Langdon. </author> <title> Universal modeling and coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 27(1) </volume> <pages> 12-23, </pages> <year> 1981. </year>
Reference-contexts: Therefore, universal on-line models are doubly adaptive; both the structure and the parameters change to reflect the properties of the data. The separation of modeling from coding, and the use of doubly adaptive stochastic models is the modern data compression paradigm, 1 pioneered by Glen Langdon and Jorma Rissanen <ref> [RL81, LR83, Ris83] </ref>, and further developed in [Ris86a, MGZ89]. By "paradigm," I mean an ideal and general blueprint for other solutions to follow, and Rissanen's and Langdon's certainly fits this strict interpretation.
Reference: [RM89] <author> J. J. Rissanen and K. M. Mohiuddin. </author> <title> A multiplication-free multialphabet arithmetic code. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 37 </volume> <pages> 93-98, </pages> <year> 1989. </year>
Reference-contexts: good compression performance, required trivial timing and buffering overhead, and was an order of magnitude faster than the state of the art, she would abandon the techniques presented here, just as she did seven years ago [BB92, Bun92]. 6 Indeed, this has already happened in the case of arithmetic coding <ref> [RM89, CKW91, HV92, FGC93, MSWB93] </ref>. 6 be that it is simply not worthwhile to improve the compression performance further. Even so, there remains one other very important motivation for this research. These same stochastic modeling techniques are applicable to other computing disciplines in a straightforward manner. <p> Implementations of arithmetic coding have been honed to near perfection over the years. There are now several fast and extremely accurate approximations which eliminate the divisions and multiplications that are required by straightforward implementations <ref> [RM89, CKW91, FGC93] </ref>. 2.4 A General Strategy for Optimal Source Modeling Assume the string was generated by a stochastic FSM. Given a string to compress, we first assume that it was generated by a finite stochastic source belonging to a tractable but useful class of models.
Reference: [Rub79] <author> F. Rubin. </author> <title> Arithmetic stream coding using fixed precision registers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 25 </volume> <pages> 672-675, </pages> <month> November </month> <year> 1979. </year> <month> 154 </month>
Reference-contexts: The symbol-wise probabilities required for on-line coding are obtained in the modeling process, which in one pass constructs an adaptive finite-state model. Arithmetic coding <ref> [Pas76, Ris76, RL79, Rub79, Gua80] </ref> codes each symbol with a number of bits that can be made arbitrarily close to the negative logarithm of the symbol's probability, by increasing the size of the registers used to do the arithmetic.
Reference: [Sha48] <author> C. E. Shannon. </author> <title> A mathematical theory of communication. </title> <journal> Bell System Technical Journal, </journal> <volume> 27(3) </volume> <pages> 379-423, </pages> <month> March </month> <year> 1948. </year>
Reference-contexts: If the stochastic process used to model the input sequence is identical to the machine which emitted the sequence, the number of bits output by the combined model and arithmetic coder will approach the entropy of the source message, which is the theoretical optimum <ref> [Sha48] </ref>. The relative generality and power of stochastic techniques justify the exclusion of data compression methods that do not separate the problem into modeling for probability estimation and coding. <p> A formal definition of ergodic is beyond the present scope, as is most formal probability theory. However, the ergodicity of an FSM source has been proved to be a decidable graph-theoretic property <ref> [Paz71, Sha48] </ref>. An FSM is ergodic if the graph consisting of its states and transitions that have non-zero probability meets both of the following conditions: 1. The graph is strongly connected, i.e., every state is reachable from every other. 2. <p> That is, for all i and k, the infinitely long sequence a i a i+1 has the same statistical properties as the sequence a i+k a i+k+1 , if they are both emitted by the same stationary source. In his "noiseless source coding theorem" <ref> [Sha48] </ref>, Shannon proves that assuming a finite, stationary, and ergodic source, members of the set of messages that can be emitted by that source cannot be uniquely encoded with fewer output symbols per source character than is given by the source's entropy.
Reference: [Sto85] <author> J. Storer. </author> <title> Textual substitution techniques for data compression. </title> <editor> In A. Apostolico and Z. Galil, editors, </editor> <booktitle> Combinatorial Algorithms on Words, </booktitle> <pages> pages 111-129. </pages> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: Other techniques, namely any variant of the elegant Ziv-Lempel techniques [ZL77, ZL78, MW85, Wel84, FG89, BB92], trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques <ref> [Sto85] </ref>. Textual substitution techniques are much easier than stochastic techniques to understand and implement, and therefore save another costly resource|design time. 5 1.1.4 The Importance of Stochastic Techniques It is undeniable that some of the stochastic methods we discuss, in their current state of development, are only minimally practical.
Reference: [Ton93] <author> T. Y. Tong. </author> <title> Data Compression with Arithmetic Coding and Source Modeling. </title> <type> PhD thesis, </type> <institution> University of Waterloo, </institution> <year> 1993. </year>
Reference-contexts: DMC's distinguishing feature is an elegant but ad 50 hoc modeling technique that provides state-of-the-art compression performance and matchless conceptual simplicity. In practice, however, the cost of DMC's simplicity and performance is often outrageous memory consumption. Several known attempts at reducing DMC's unwieldy model growth (e.g., <ref> [Ton93, Yu93] </ref>) have rendered DMC's compression performance uncompetitive. One reason why DMC's model growth problem has resisted solution is that the algorithm is poorly understood. DMC is the only published stochastic data model for which a characterization of its states, in terms of conditioning contexts, is unknown. <p> However, as originally presented, DMC is only feasible with a binary alphabet, since jAj outgoing transitions are created for every new state. Other authors <ref> [TR93, Ton93, Whi94, Yu93] </ref> have independently generalized DMC to larger alphabets using variations of what we call lazy cloning , which copies outgoing transitions only as needed.
Reference: [TR93] <author> J. Teuhola and T. Raita. </author> <title> Application of a finite-state model to text compression. </title> <journal> The Computer Journal, </journal> <volume> 36(7) </volume> <pages> 607-614, </pages> <year> 1993. </year>
Reference-contexts: The well-organized empirical studies proved, above all else, that it will take more than bright ideas and thoroughness to outperform PPMC. * GDMC <ref> [TR93] </ref>, or Generalized DMC, is the first published 6 generalization of DMC to an m-ary alphabet that does not sacrifice compression performance or create an astronomically large model. GDMC does not, however, outperform PPM. * PPM* [CTW95] does away with PPM's global order-bound in a linear-space implementation. <p> However, as originally presented, DMC is only feasible with a binary alphabet, since jAj outgoing transitions are created for every new state. Other authors <ref> [TR93, Ton93, Whi94, Yu93] </ref> have independently generalized DMC to larger alphabets using variations of what we call lazy cloning , which copies outgoing transitions only as needed. <p> Other authors [TR93, Ton93, Whi94, Yu93] have independently generalized DMC to larger alphabets using variations of what we call lazy cloning , which copies outgoing transitions only as needed. However, only the lazy-evaluation solutions introduced in [Bun94] and in <ref> [TR93] </ref> successfully reduce DMC's memory requirements without eliminating DMC's advantages. (Sections 6.5 and 9.5 explain and refine the ideas from [Bun94] and Section 9.5 compares their performance with the technique of [TR93].) The others all had a similar approach: when the required outgoing transition was absent from the current state, the <p> However, only the lazy-evaluation solutions introduced in [Bun94] and in <ref> [TR93] </ref> successfully reduce DMC's memory requirements without eliminating DMC's advantages. (Sections 6.5 and 9.5 explain and refine the ideas from [Bun94] and Section 9.5 compares their performance with the technique of [TR93].) The others all had a similar approach: when the required outgoing transition was absent from the current state, the needed transition was copied from a state that was essentially a zeroth or first-order state. <p> J. Teuhola and T. Raita report [personal correspondence] that prediction quality degrades when this restriction is combined with lazy cloning. In <ref> [TR93] </ref> they based E () only upon the difference between a candidate transition's frequency and the frequency of the transition's destination state. <p> Preliminary experiments with max-order updates supported our conclusion that update exclusion approximates the max-order update context partition, however, there was no performance advantage. Max-order updates are not considered further in this thesis, except as an option required in our taxonomy to exactly emulate the DMC variant GDMC <ref> [TR93] </ref>. 5.3 Summary The principal update schemes given here, full updates, update exclusion, and max-order updates, form a single set of interchangeable options for the executable taxonomy and are empirically evaluated in Chapter 9. <p> Blending can be viewed as a bottom-up recursive procedure for computing a mixture, barring one missing term for each level of the recursion, where a mixture is basically a weighted average of several probability estimates. We shall show by decomposition that mixtures generalize the techniques used in DMC variants <ref> [CH87, TR93] </ref>, as well as PPM variants, and thus these techniques, along with other variants of mixtures, are interchangeable. 6.1 Recursive Mixtures We are concerned with estimating a probability P e (a i ja 1 a 2 a i1 ) using the frequencies stored at the excited states of a suffix-tree <p> However, quite often the abstract differences among apparently distinct approaches can be expressed in a single technique as different values of parameters that have simple connotations. 1 The seven algorithms are DMC [CH87], GDMC <ref> [TR93] </ref>, PPM [Mof90], PPM* [CTW95], WLZ [WLZ92], plus BestFSMX [Bun97b] and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far ([Bun96, Chapter 9]). <p> The sizes of model states (nodes), and symbol transitions (edges) or string transitions (edge fl s), are implementation-dependent. 137 9.5 Improvements to DMC Variants 9.5.1 GDMC and LazyDMC We began our attempts to create a better-performing variant of DMC [CH87] with two starting points: the GDMC algorithm <ref> [TR93] </ref> and our own lazy-cloning DMC variant, LazyDMC, proposed in [Bun94]. GDMC uses a mixture with weight function A (thus nothing gets added to a state's escape count after it is initially set to the redirected prefix edge's count) and inheritance evaluation time M 3 . <p> in Chapter 9. * The unifying framework enables meaningful, component-wise comparison among theoretical and practical techniques. 1 I only claim "independent invention" of lazy cloning, and do so with the explicit agreement of the other inventors, Jukka Teuhola and Timo Raita, who published "GDMC" (which implements both concepts) first in <ref> [TR93] </ref>. In August 92, I reported the same ideas by mail to one of DMC's authors, Nigel Horspool, who invited me to present it in person at the University of Victoria in March 1993.
Reference: [Ukk95] <author> E. Ukkonen. </author> <title> On-line construction of suffix trees. </title> <journal> Algorithmica, </journal> <volume> 14(3) </volume> <pages> 249-260, </pages> <year> 1995. </year>
Reference-contexts: The transformation describes an on-line linear-space suffix-tree construction algorithm. We started the work late in 1994, upon receiving a preprint of [CTW95]. However, Ukkonen published an on-line suffix-tree construction algorithm <ref> [Ukk95] </ref> first, in 1995, while we were combining ours with state selection and mixtures in the implementation of our taxonomy. The independently derived algorithms are quite similar|even down to the use of the term "splitting." Both algorithms are functionally related to McCreight's suffix-tree construction algorithm [McC76].
Reference: [vL90] <author> J. van Leeuwen. </author> <title> Handbook of Theoretical Computer Science, Volume B, Formal Models and Semantics. Advanced Reference Series. </title> <publisher> The MIT Press, </publisher> <address> 55 Hayward Street, Cambridge MA 02142, </address> <year> 1990. </year>
Reference-contexts: Star-free regular languages are regular sets that can be described using a finite number of set concatenations, unions, and complements, <ref> [vL90, Chapter 1] </ref>. To see that C k (s i ) is star-free for any s i , simply replace set subtraction with intersection of the absolute complement of the subtrahend, and A fl with fg.
Reference: [WB91] <author> I. H. Witten and T. C. Bell. </author> <title> The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(4) </volume> <pages> 1085-1094, </pages> <year> 1991. </year>
Reference-contexts: Thus the choice of weighting function reduces to a solution to an ancient problem|the "zero-frequency problem," or how to assign a likelihood to an event that has never occurred before|for which it is widely agreed that no principled solution exists, in the absence of a priori knowledge <ref> [WB91] </ref>. Therefore, the merit of any weighting function for a universal model is determined analytically by how the assumptions it imposes interact with other assumptions made in the model, and empirically by its performance on actual data. <p> Several approaches to solving the zero frequency problem, known as "escape" mechanisms, have been used successfully with PPM implementations. Four of the simplest and best-performing escape mechanisms are known in the literature as `A,' `B,' `C,' and `D' <ref> [WB91] </ref>. In this section, we shall show how these simple escape mechanisms correspond to different weighting functions W (s). <p> That is, the greater than 1% performance improvement of D over C in simpler variants is more than halved in XSM variants. Since D, C, and other known solutions to the "zero frequency problem" are known to have no principled basis <ref> [WB91] </ref>, our optimizations increase the universality of PPM variants by reducing the relative effects of the necessarily ad hoc solutions to the zero frequency (i.e., mixture weighting) problem.
Reference: [Wel84] <author> T. Welch. </author> <title> A technique for high-performance data compression. </title> <journal> IEEE Computer, </journal> <volume> 17(6) </volume> <pages> 8-19, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94]. Other techniques, namely any variant of the elegant Ziv-Lempel techniques <ref> [ZL77, ZL78, MW85, Wel84, FG89, BB92] </ref>, trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85]. <p> 0.816 0.808 progc 39,611 2.977 2.666 2.499 2.401 2.411 2.377 progl 71,646 2.168 1.826 1.904 1.671 1.729 1.693 progp 49,379 2.222 1.905 1.843 1.624 1.753 1.719 trans 93,695 2.114 1.734 1.772 1.447 1.539 1.495 Average 224,402 2.721 2.524 2.460 2.339 2.324 2.294 124 which is based upon Welch's popular implementation <ref> [Wel84] </ref> of the Ziv and Lempel's second major string-matching algorithm [ZL78], and `gzip,' which is based upon Ziv and Lempel's first major string-matching construction [ZL77].
Reference: [Whi94] <author> R. F. Whitehead. </author> <title> An exploration of dynamic Markov compression. </title> <type> Master's thesis, </type> <institution> University of Canterbury, </institution> <year> 1994. </year>
Reference-contexts: However, as originally presented, DMC is only feasible with a binary alphabet, since jAj outgoing transitions are created for every new state. Other authors <ref> [TR93, Ton93, Whi94, Yu93] </ref> have independently generalized DMC to larger alphabets using variations of what we call lazy cloning , which copies outgoing transitions only as needed.
Reference: [Wil91] <author> R. N. Williams. </author> <title> Adaptive Data Compression. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Massachusetts, </address> <year> 1991. </year>
Reference-contexts: If the dependence between successive random variables is limited to the preceding random variable, the sequence is Markovian. 1 This is Ross Williams' terminology <ref> [Wil91] </ref>. 13 A Markovian FSM information source is a finite-state machine (FSM) representation of a Markovian stochastic process such that each state corresponds to a random variable and for any state there is at most one transition for each output symbol, or source symbol in the FSM alphabet, and each transition <p> For years, practical data compression researchers have asked: Can DMC, given proper care and feeding, outperform PPMC? What does the context partition of DMC look like, anyway? 37 Are there hidden, abstract feature of DMC that could be useful in improving PPMC? * SAKDC, or Swiss Army Knife Data Compression <ref> [Wil91] </ref>, is a reimplementation of PPM with a suffix-tree and a vast number of justifiable but ad hoc features, none of which significantly improved PPM's performance. <p> This means, for one thing, that DMC FSMs cannot recognize infinitely repeating patterns, a common capability of more general FSMs. Neither, however, can any of the popular FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref>, all of which can determine the current model state by locating the state whose single associated finite context is a maximal suffix of the given input string. <p> This contradicts the conventional wisdom that FSMX models "generalize the class of Markov Models" [Ris86a]. 4.5.4 Structural Comparison The context () of a DMC state is analogous to the finite contexts of the states in FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> vis a vis the terms of the definition of C k (), and the fact that every string that takes a DMC model to a given state s i ends in context (s i ). <p> A more important distinction is DMC's capacity for variable-length minimal extensions of a context. This capacity alone sets DMC's (Markovian) data model apart from all FSMX models. Techniques that build FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> all grow models with single-character minimal extensions. <p> For example, a sliding window of input history can be kept, and as sequence symbols that have passed through the buffer pass out of the buffer, the event frequencies originally incremented by these symbols can be decremented <ref> [Wil91] </ref>. Alternatively, at regular intervals, all the frequencies in the model can be scaled by a small constant, which would implement an exponential decay function [How93]. Or, the same process could be carried on locally, on a per-state basis, when the state's total frequency exceeded a threshold [Mof90].
Reference: [WLZ92] <author> M. J. Weinberger, A. Lempel, and J. Ziv. </author> <title> A sequential algorithm for the universal coding of finite memory sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(3) </volume> <pages> 1002-1014, </pages> <year> 1992. </year>
Reference-contexts: "This class, which we denote FSMX, generalizes the class of Markov sources." Recently, the following description appeared in [WST95]: "These [FSMX] sources form a subclass of the set of sources defined by finite state machines (`FSMs') and an extension (`X') of the class of finite-order Markov sources." The authors of <ref> [WLZ92] </ref>, give a definition of a class of sources that corresponds to the above definition of finite-order FSM sources and claims that, with the additional restrictions of unifilarity and ergodicity, the resulting class equals the class of FSMX models. <p> In lazy evaluation of state refinements (i.e., children), only the refinement that is presently represented by the input sequence is added to the model, rather than adding all m possible refinements. * A recent asymptotic result by Weinberger, Lempel, and Ziv (WLZ) <ref> [WLZ92] </ref> provides the first published construction with provably optimal performance, assuming an infinite input sequence and assuming that the FSM that presumably emitted the input sequence conformed to a known order bound. The authors make the key observation that Context systematically underestimates the local order. <p> It illuminates principled solutions for curbing counterproductive model growth. 3. It provides a sufficiently general on-line Markov model that can be parameterized to exactly emulate many other influential algorithms from the literature, including many popular FSMX models <ref> [CTW95, CW84b, Fur91, RC89, Ris83, Ris86a, WLZ92] </ref>. <p> This means, for one thing, that DMC FSMs cannot recognize infinitely repeating patterns, a common capability of more general FSMs. Neither, however, can any of the popular FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref>, all of which can determine the current model state by locating the state whose single associated finite context is a maximal suffix of the given input string. <p> This contradicts the conventional wisdom that FSMX models "generalize the class of Markov Models" [Ris86a]. 4.5.4 Structural Comparison The context () of a DMC state is analogous to the finite contexts of the states in FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> vis a vis the terms of the definition of C k (), and the fact that every string that takes a DMC model to a given state s i ends in context (s i ). <p> A more important distinction is DMC's capacity for variable-length minimal extensions of a context. This capacity alone sets DMC's (Markovian) data model apart from all FSMX models. Techniques that build FSMX models <ref> [CW84b, CTW95, Fur91, RC89, Ris83, Ris86a, WLZ92, Wil91] </ref> all grow models with single-character minimal extensions. <p> Note that stochastic complexity assigns a coding penalty to each model state. The penalty is a lower bound on the number of bits required to encode that state. Most other treatments of the state-selection approach to on-line modeling (e.g., <ref> [Ris83, WLZ92] </ref>) require that a refinement (e.g., the children or deeper descendants) of a state be selected in preference to that state if the performance of that refinement improves the performance of the model frontier containing the original state by an amount 101 exceeding the cost of encoding the states comprising <p> Top-down, select the minimum order excited state for which no complete fron tier of its subtree improves its expected codelength. The first approach was introduced, using entropies instead of MDLs [Ris83], and then later using MDLs [Ris86a]. This technique systematically under-estimates the local order of the model <ref> [WLZ92] </ref>; it is a hill-climbing technique that can get stuck in local minima. The second approach is introduced here as an obvious complement to the top-down hill-climbing approach, to complete the taxonomy. It systematically over-estimates local order. <p> Weinberger, et al present a formal, asymptotically convergent solution to the third approach that requires an order bound <ref> [WLZ92] </ref>. Below, we describe our own solution, which requires no order bound and which allows efficient implementation. <p> In this case, the selected order, which we tally for our experiments, is counted as the order of the maximum-order excited state plus one. heuristic, S 2 : Select the min-order excited state with one out-edge [CTW95]. percolating, S 3 : See companion paper [Bun97a]. The construction of <ref> [WLZ92] </ref> (WLZ) uses an order-bounded state selector that produces the same results as our percolating state selector does in a model that has the same structural order bound, assuming that both models use the same selection threshold. <p> However, quite often the abstract differences among apparently distinct approaches can be expressed in a single technique as different values of parameters that have simple connotations. 1 The seven algorithms are DMC [CH87], GDMC [TR93], PPM [Mof90], PPM* [CTW95], WLZ <ref> [WLZ92] </ref>, plus BestFSMX [Bun97b] and BestDMC, which are the best-performing FSMX and DMC variants tested with this taxonomy so far ([Bun96, Chapter 9]).
Reference: [WMB94] <author> I. H. Witten, A. Moffat, and T. C. Bell. </author> <title> Managing Gigabytes: Compressing and Indexing Documents and Images. </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, New York, </address> <year> 1994. </year> <month> 155 </month>
Reference-contexts: As a consequence, on-line statistical data compression techniques, which are relatively computationally intensive, have risen from "laboratory curiosities" into practice <ref> [WMB94, Chapter 2] </ref>. Members of this family of techniques have out-performed almost every other type of compressor for the past thirteen years. 1 Moreover, the information-theoretical underpinnings of statistical techniques enable rigorous analysis. <p> Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique <ref> [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94] </ref>. Other techniques, namely any variant of the elegant Ziv-Lempel techniques [ZL77, ZL78, MW85, Wel84, FG89, BB92], trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85].
Reference: [WNC87] <author> I. H. Witten, R. M. Neal, and J. G. Cleary. </author> <title> Arithmetic coding for data compression. </title> <journal> Communications of the ACM, </journal> <volume> 30(6) </volume> <pages> 520-540, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Arithmetic coding, and how it interfaces with models, is notoriously difficult to grasp. The following presentation and its illustration in particular are a departure from the usual descriptions, exemplified by [RL79] and <ref> [WNC87] </ref>. We first give an operational description that steps through how a stochastic model and an arithmetic 17 coder/decoder work together to encode and then decode each symbol of a source message. <p> Here, resetting the unstable bits counter inserts an extra 0 into (the untransmitted and unstable portion of) the bit stream, and is called "bit stuffing" [RL79]. An alternative to bit stuffing can be found in <ref> [WNC87] </ref>. In summary, a fixed-precision arithmetic coder incrementally transmits an approximate codepoint that is larger than the exact codepoint. There are two determinants of coder accuracy. It is most affected by the number of bits p used to represent the width of the renormalized coding interval, W renormalized . <p> Chapter 9 PERFORMANCE MEASUREMENTS In this chapter, we measure the predictive ability of our models by running them on the files of the Calgary Corpus [BCW90]. All probability estimates are coded using a floating-point m-ary arithmetic coder that we based upon the popular integer coder implemented in <ref> [WNC87, MSWB93] </ref>. Arithmetically coded probability estimates are an excellent measure of model performance because they compute a tight, infinite-precision upper bound on the log likelihood of the model, given the input sequence.
Reference: [WST95] <author> F. M. J. Willems, Y. M. Shtarkov, and T. J. Tjalkens. </author> <title> The context tree weighting method: Basic properties. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 41(3) </volume> <pages> 653-664, </pages> <year> 1995. </year>
Reference-contexts: For example, the authority responsible for defining the FSMX class introduces the distinction in [Ris86a] as "a useful subclass of FSM sources which includes the Markov sources," and later claims that "This class, which we denote FSMX, generalizes the class of Markov sources." Recently, the following description appeared in <ref> [WST95] </ref>: "These [FSMX] sources form a subclass of the set of sources defined by finite state machines (`FSMs') and an extension (`X') of the class of finite-order Markov sources." The authors of [WLZ92], give a definition of a class of sources that corresponds to the above definition of finite-order FSM sources <p> The resulting FSM is a variable-order HMM. Extend the taxonomy: * Recently, a new theoretical on-line modeling technique called "The Context-Tree Weighting Method" was published in <ref> [WST95] </ref>. It explicitly constructs binary FSMX models, but it very explicitly does not perform state selection.
Reference: [Yu93] <author> T. L. Yu. </author> <title> Hybrid dynamic Markov modeling. </title> <booktitle> In Proceedings Data Compression Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> March </month> <year> 1993. </year>
Reference-contexts: DMC's distinguishing feature is an elegant but ad 50 hoc modeling technique that provides state-of-the-art compression performance and matchless conceptual simplicity. In practice, however, the cost of DMC's simplicity and performance is often outrageous memory consumption. Several known attempts at reducing DMC's unwieldy model growth (e.g., <ref> [Ton93, Yu93] </ref>) have rendered DMC's compression performance uncompetitive. One reason why DMC's model growth problem has resisted solution is that the algorithm is poorly understood. DMC is the only published stochastic data model for which a characterization of its states, in terms of conditioning contexts, is unknown. <p> However, as originally presented, DMC is only feasible with a binary alphabet, since jAj outgoing transitions are created for every new state. Other authors <ref> [TR93, Ton93, Whi94, Yu93] </ref> have independently generalized DMC to larger alphabets using variations of what we call lazy cloning , which copies outgoing transitions only as needed.
Reference: [ZL77] <author> J. Ziv and A. Lempel. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 23(3) </volume> <pages> 337-343, </pages> <year> 1977. </year>
Reference-contexts: Furthermore, the most effective practical data compression method for the past thirteen years, namely PPM, is a stochastic technique [Bel86, BWC89, BCW90, CTW95, CW84b, Mof90, WMB94]. Other techniques, namely any variant of the elegant Ziv-Lempel techniques <ref> [ZL77, ZL78, MW85, Wel84, FG89, BB92] </ref>, trade compression performance for speed and memory conservation more appropriately for today's technology. These techniques are examples of the more general textual substitution techniques [Sto85]. <p> The algorithms below all grow very large suffix-trees and are not used in practice yet, because of their high memory and computation requirements. Today's data compression practice is dominated instead by fast string-matching techniques based upon Ziv-Lempel algorithms <ref> [ZL77, ZL78] </ref>. However, the trend in computer system tradeoffs continues toward increasingly cheaper computation and memory relative to secondary storage and communication bandwidth. The algorithms below represent the state of the art with respect to compression performance. <p> This compares favorably with the the expected 2:1 compression of Unix compress, or the expected 2.5:1 compression of gzip, which are based upon [ZL78] and <ref> [ZL77] </ref>, respectively. * PPM [CW84b], or Prediction by Partial Matching, grows an m-ary alphabet FSMX model aggressively and on-demand, using lazy evaluation of state refinements and a fixed global order bound. PPM's reign as the best-compressing modeling algorithm has stood unchallenged for the 12 years preceding this thesis. <p> 1.753 1.719 trans 93,695 2.114 1.734 1.772 1.447 1.539 1.495 Average 224,402 2.721 2.524 2.460 2.339 2.324 2.294 124 which is based upon Welch's popular implementation [Wel84] of the Ziv and Lempel's second major string-matching algorithm [ZL78], and `gzip,' which is based upon Ziv and Lempel's first major string-matching construction <ref> [ZL77] </ref>. Moffat's 1990 implementation, PPMC [Mof90], of Cleary and Witten's 1984 PPM algorithm [CW84b], remained unchallenged until Cleary, et al. did away with PPM's order bound to produce PPM* in 1995 [CTW95]. The authors claimed that PPM* outperformed PPMC in their paper.

References-found: 64

