URL: http://ai.eecs.umich.edu/ifor/papers/aaaipl94.ps
Refering-URL: http://ai.eecs.umich.edu/ifor/papers/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: frjones,wrayre,vanlent,lairdg@eecs.umich.edu  
Title: Planning in the Tactical Air Domain intelligent agents for use in tactical flight training simulators.
Author: Randolph M. Jones, Robert Wray, Michael van Lent, and John E. Laird 
Note: TacAir-Soar (Jones et al. 1993; Rosenbloom et al. 1994) implements artificial,  
Address: 1101 Beal Avenue Ann Arbor, MI 48109-2110  
Affiliation: Artificial Intelligence Laboratory University of Michigan  
Abstract: TacAir-Soar is a reactive system that uses recognition-driven problem solving to plan and generate behavior in the domain of tactical air combat simulation. Our current research efforts focus on integrating more deliberative planning and learning mechanisms into the system. This paper discusses characteristics of the domain that influence potential planning solutions, together with our approach for integrating reactive and deliberative planning. In order to accomplish this task, we need not only to acquire and encode a large amount of complex knowledge, but also to address a number of core research issues within artificial intelligence. Not the least of these issues is the ability for the agent to plan its activities appropriately, and to acquire efficient and effective new behaviors as a consequence of planning. We are investigating the hypothesis that a variety of appropriate behaviors can arise from a system with a small, organized set of cognitive mechanisms as it interacts with a complex environment. Thus, the primary thrust of our research relies on integration in a number of different forms. Reactive behavior generation fl Thanks to Paul Rosenbloom, Lewis Johnson, Soowon Lee, Frank Koss, and the reviewers for their comments on earlier drafts of this paper. In addition, this research has benefited enormously from the combined efforts of the Soar-IFOR group. The research is supported by contract N00014-02-K-2015 from the Advanced Systems Technology Office of the Advanced Research Projects Agency and the Naval Research Laboratory. must be integrated with goal-directed reasoning and planning. These in turn must be integrated with other cognitive capabilities, such as situation interpretation, natural language understanding and generation, plan recognition, planning, etc. Rather than combining distinct modules for execution, planning, and learning, we are attempting to integrate all of these capabilities within a single control scheme. Thus, planning becomes simply another form of execution, which must interact with other knowledge in order to generate appropriate behavior. Learning occurs as a side effect of execution, manifesting itself in different ways depending on the particular tasks being executed. Because of the incremental, dynamic, and complex nature of behavior generation in the tactical air domain, learning must also be incremental, fast, and able to capture the complexities of goals and actions. The current version of TacAir-Soar combines reactive and goal-driven reasoning to create what we call recognition-driven problem solving (Tambe et al. 1994). The system contains a large set of rules that fire as soon as their conditions are met, without search or conflict resolution. Some of these rules respond to immediate changes in sensory inputs, while others respond to higher-level interpretations of those changes and goals that the system posts for itself. As an example, TacAir-Soar may observe a series of readings about a contact on its radar, and conclude that the contact is an aggressive enemy aircraft. Thus, the system posts a goal of intercepting the aircraft, which involves maintaining a collision course. The actual heading of TacAir-Soar's aircraft will change every time the collision course changes. This paradigm for behavior generation is similar to reactive planning in the spirit of Firby's (1987) RAP planners. That is, the system does not perform any search to determine the best course of action, and it does not plan in terms of predicting future states of the environment. 1 It also 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E., and Chapman, D. </author> <year> 1987. </year> <title> Pengi: An implementation of a theory of action. </title> <booktitle> In Proceedings of AAAI-87, </booktitle> <pages> 123-154. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Bimson, K.; Marsden, C.; McKenzie, F.; and Paz, N. </author> <year> 1994. </year> <title> Knowledge-based tactical decision making in the CCTT SAF prototype. </title> <booktitle> In Proceedings of the Fourth Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> 293-303. </pages>
Reference-contexts: One approach to this type of domain has been to attempt to capture every possible situation that an agent may encounter in a recognition rule <ref> (e.g., Bimson et al. 1994) </ref>. However, even if such an approach is possible, it would require extensive work on the knowledge base every time the domain changes a bit (for example, if new aircraft or missiles are developed).
Reference: <author> Chapman, D. </author> <year> 1987. </year> <title> Planning for conjunctive goals. </title> <booktitle> Artificial Intelligence 32 </booktitle> <pages> 333-377. </pages>
Reference: <author> Cohen, P. R.; Greenberg, M. L.; Hart, D. M.; and Howe, A. E. </author> <year> 1989. </year> <title> Understanding the design requirements for agents in complex environments. </title> <journal> AI Magazine 10(3) </journal> <pages> 32-48. </pages>
Reference: <author> Covrigaru, A. </author> <year> 1992. </year> <title> Emergence of meta-level control in multi-tasking autonomous agents. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Electrical Engineering and Computer Science, University of Michigan. </institution>
Reference: <author> Firby, R. J. </author> <year> 1987. </year> <title> An investigation into reactive planning in complex domains. </title> <booktitle> In Proceedings of AAAI-87, </booktitle> <pages> 202-206. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Georgeff, M. </author> <year> 1984. </year> <title> A theory of action for multiagent planning. </title> <booktitle> In Proceedings of AAAI-84, </booktitle> <pages> 121-125. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Planning in the Face of Uncertainty A key feature of the tactical air domain is that there is generally a large number of participants in any given scenario. Some research <ref> (e.g., Georgeff 1984) </ref> has focused on this problem, and it naturally will have a strong effect on how TacAir-Soar can interpret and predict the consequences of its actions while planning.
Reference: <author> Gervasio, M. T., and DeJong, G. F. </author> <year> 1994. </year> <title> An incremental learning approach for completable planning. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh National Conference, </booktitle> <pages> 78-86. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Predicting the future actions of competing agents is somewhat more difficult, and relies in part on recognizing the plans and goals of those agents (Tambe & Rosenbloom 1994; Wilensky 1981). Given the unpredictable nature of modeling other agents, it is most appropriate for TacAir-Soar to create completable plans <ref> (Gervasio & DeJong 1994) </ref>, in order to react appropriately to future actions by other agents. Contingency plans (Warren 1976) might also be useful, but these are generally expensive to generate.
Reference: <author> Johnson, W. L. </author> <year> 1994a. </year> <title> Agents that explain their own actions. </title> <booktitle> In Proceedings of the Fourth Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> 87-95. </pages>
Reference: <author> Johnson, W. L. </author> <year> 1994b. </year> <title> Agents that learn to explain themselves. </title> <booktitle> In Proceedings of AAAI-94, </booktitle> <pages> 1257-1263. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Jones, R. M.; Tambe, M.; Laird, J. E.; and Rosen-bloom, P. S. </author> <year> 1993. </year> <title> Intelligent automated agents for flight training simulators. </title> <booktitle> In Proceedings of the Third Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> 33-42. </pages>
Reference: <author> Kaelbling, L. P. </author> <year> 1986. </year> <title> An architecture for intelligent reactive systems. </title> <booktitle> In Proceedings of the Workshop on Planning and Reasoning about Action, </booktitle> <pages> 235-250. </pages>
Reference: <author> Laird, J. E., and Rosenbloom, P. S. </author> <year> 1990. </year> <title> Integrating execution, planning, and learning in Soar for external environments. </title> <booktitle> In Proceedings of AAAI-90, </booktitle> <pages> 1022-1029. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Integrated Planning, Learning, and Execution Our commitment to an integrated system began with our selection of the Soar architecture (Laird, Newell, & Rosenbloom 1987) as the platform for development. Soar provides an ideal basis for recognition-driven problem solving, and naturally supports the integration of execution, planning, and learning <ref> (Laird & Rosenbloom 1990) </ref>. Readers familiar with Soar will recall that all reasoning and behavior generation takes place in problem spaces, through the deliberate selection of operators.
Reference: <author> Laird, J. E.; Newell, A.; and Rosenbloom, P. S. </author> <year> 1987. </year> <title> Soar: An architecture for general intelligence. </title> <booktitle> Artificial Intelligence 33 </booktitle> <pages> 1-64. </pages>
Reference-contexts: It begins with a discussion of the overall integrated framework, and then describes specific ideas for each of the planning issues. Integrated Planning, Learning, and Execution Our commitment to an integrated system began with our selection of the Soar architecture <ref> (Laird, Newell, & Rosenbloom 1987) </ref> as the platform for development. Soar provides an ideal basis for recognition-driven problem solving, and naturally supports the integration of execution, planning, and learning (Laird & Rosenbloom 1990).
Reference: <author> Lee, S. </author> <year> 1994. </year> <title> Multi-Method Planning. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Southern California. </institution>
Reference: <author> Rosenbloom, P. S.; Johnson, W. L.; Jones, R. M.; Koss, F.; Laird, J. E.; Lehman, J. F.; Rubinoff, R.; Schwamb, K. B.; and Tambe, M. </author> <year> 1994. </year> <title> Intelligent automated agents for tactical air simulation: A progress report. </title> <booktitle> In Proceedings of the Fourth Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> 69-78. </pages>
Reference: <author> Rosenbloom, P. S.; Lee, S.; and Unruh, A. </author> <year> 1992. </year> <title> Bias in planning and explanation-based learning. </title> <editor> In Chip-man, S., and Meyrowitz, A., eds., </editor> <title> Machine Learning: Induction, analogy and discovery. </title> <publisher> Norwell, </publisher> <address> MA: </address> <publisher> Kluwer. </publisher>
Reference: <author> Tambe, M., and Rosenbloom, P. S. </author> <year> 1994. </year> <title> Event tracking in complex multi-agent environments. </title> <booktitle> In Proceedings of the Fourth Conference on Computer Generated Forces and Behavioral Representation, </booktitle> <pages> 473-484. </pages>
Reference-contexts: The current version of TacAir-Soar combines reactive and goal-driven reasoning to create what we call recognition-driven problem solving <ref> (Tambe et al. 1994) </ref>. The system contains a large set of rules that fire as soon as their conditions are met, without search or conflict resolution.
Reference: <author> Tambe, M.; Jones, R. M.; Laird, J. E.; Rosenbloom, P. S.; Schwamb, K.; and Koss, F. </author> <year> 1994. </year> <title> Intelligent agents for interactive simulation environments. </title> <note> Manuscript in preparation. </note>
Reference-contexts: The current version of TacAir-Soar combines reactive and goal-driven reasoning to create what we call recognition-driven problem solving <ref> (Tambe et al. 1994) </ref>. The system contains a large set of rules that fire as soon as their conditions are met, without search or conflict resolution.
Reference: <author> VanLehn, K.; Jones, R. M.; and Chi, M. T. H. </author> <year> 1992. </year> <title> A model of the self-explanation effect. </title> <journal> Journal of the Learning Sciences 2 </journal> <pages> 1-59. </pages>
Reference-contexts: By going back over each step of the scenario, the pilot can identify successes and failures, consider alternative courses of action, and take more time to evaluate various possible outcomes. Automated agents have also been demonstrated to benefit from such self-explanations <ref> (VanLehn, Jones, & Chi 1992) </ref>. In addition, Johnson (1994a; 1994b) has presented a debriefing facility, in which TacAir-Soar agents can explain their actions after a scenario, and consider some hypothetical alternatives. The deliberative planning mechanism should expand on this approach and allow the system to learn from the debriefing experience.
Reference: <author> Veloso, M. M. </author> <year> 1989. </year> <title> Nonlinear problem solving using intelligent casual commitment. </title> <type> Technical Report CMU-CS-89-210, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference: <author> Warren, D. H. D. </author> <year> 1976. </year> <title> Generating conditional plans and programs. </title> <booktitle> In Proceedings of the AISB Conference, </booktitle> <pages> 344-354. </pages>
Reference-contexts: Given the unpredictable nature of modeling other agents, it is most appropriate for TacAir-Soar to create completable plans (Gervasio & DeJong 1994), in order to react appropriately to future actions by other agents. Contingency plans <ref> (Warren 1976) </ref> might also be useful, but these are generally expensive to generate. In a sense, TacAir-Soar's current knowledge base consists of a large completable plan, and such planning is consistent with our desire to integrate the current recognition-driven problem-solving structure with deliberative planning.
Reference: <author> Wilensky, R. </author> <year> 1981. </year> <title> Inside computer understanding: Five programs plus miniatures. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum. </publisher>
References-found: 23

