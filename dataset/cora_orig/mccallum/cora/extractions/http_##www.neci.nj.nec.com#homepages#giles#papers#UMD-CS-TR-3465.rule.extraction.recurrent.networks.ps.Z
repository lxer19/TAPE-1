URL: http://www.neci.nj.nec.com/homepages/giles/papers/UMD-CS-TR-3465.rule.extraction.recurrent.networks.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: http://www.neci.nj.nec.com
Email: E-mail: fomlinc,gilesg@research.nj.nec.com  
Phone: Phone: (609) 951-f2691,2642g  
Title: Extraction of Rules from Discrete-Time Recurrent Neural Networks  
Author: Christian W. Omlin, C. Lee Giles 
Address: 4 Independence Way Princeton, N.J. 08540 USA  
Affiliation: NEC Research Institute  
Abstract: The extraction of symbolic knowledge from trained neural networks and the direct encoding of (partial) knowledge into networks prior to training are important issues. They allow the exchange of information between symbolic and connectionist knowledge representations. The focus of this paper is on the quality of the rules that are extracted from recurrent neural networks. Discrete-time recurrent neural networks can be trained to correctly classify strings of a regular language. Rules defining the learned grammar can be extracted from networks in the form of deterministic finite-state automata (DFA's) by applying clustering algorithms in the output space of recurrent state neurons. Our algorithm can extract different finite-state automata that are consistent with a training set from the same network. We compare the generalization performances of these different models and the trained network and we introduce a heuristic that permits us to choose among the consistent DFA's the model which best approximates the learned regular grammar. Keywords: Recurrent Neural Networks, Grammatical Inference, Regular Languages, Deterministic Finite-State Automata, Rule Extraction, Generalization Performance, Model Selection, Occam's Razor. fl Technical Report CS-TR-3465 and UMIACS-TR-95-54, University of Maryland, College Park, MD 20742. Ac cepted for publication in Neural Networks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. & Smith, C.H. </author> <year> (1983). </year> <title> Inductive Inference: Theory and Methods. </title> <journal> ACM Computing Surveys. </journal> <volume> 15. </volume> <pages> 237-269. </pages>
Reference: <author> Bengio, Y., Simard, P., Frasconi, P. </author> <year> (1994). </year> <title> Learning Long-Term Dependencies with Gradient Descent is Difficult. </title> <journal> IEEE Transactions on Neural Networks. </journal> <volume> 1. </volume> <pages> 157-166. </pages>
Reference-contexts: The working set is expanded until a network correctly classifies all the strings of the training set. This incremental training heuristic aims at overcoming the problems that arise from training on long strings with a gradient descent learning algorithm <ref> (Bengio et al. 1994) </ref>.
Reference: <author> Cleeremans, A., Servan-Schreiber, D. & McClelland, J. </author> <year> (1989). </year> <title> Finite State Automata and Simple Recurrent Recurrent Networks. </title> <journal> Neural Computation. </journal> <volume> 1, </volume> <pages> 372-381. </pages>
Reference: <editor> Clouse, D.S., Giles, C.L., Horne, B.G. & Cottrell, G.W. </editor> <year> (1994). </year> <title> Learning Large DeBruijn Automata with Feed-Forward Neural Networks. </title> <type> Technical Report CS94-398, </type> <institution> Department of Computer Science and Engineering, University of California at San Diego, La Jolla. </institution> <note> Neural Network Rule Extraction 14 Crutchfield, </note> <author> J.P. & Young, K. </author> <year> (1991). </year> <title> Computation at the Onset of Chaos. In W.H. </title> <editor> Zurek (Ed.), </editor> <booktitle> Proceedings of the 1988 Workshop on Complexity, Entropy and the Physics of Information, </booktitle> <address> (pp.223-269). Redwood City: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Das, S. & Mozer, M.C. </author> <year> (1994). </year> <title> A Unified Gradient-Descent/Clustering Architecture for Finite State Machine Induction. </title> <editor> In Cowan, J.W., Tesauro, G. & Alspector J. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> (pp. 19-26). </pages> <address> San Mateo: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Elman, J.L. </author> <year> (1990). </year> <title> Finding Structure in Time. </title> <journal> Cognitive Science. </journal> <volume> 14. </volume> <pages> 179-211. </pages>
Reference: <author> Frasconi, P., Gori, M., Maggini, M. & Soda, G. </author> <year> (1991). </year> <title> A Unified Approach for Integrating Explicit Knowledge and Learning by Example in Recurrent Networks. </title> <booktitle> In 1991 IEEE INNS International Joint Conference on Neural Networks, </booktitle> <address> Seattle , (pp.811-816). Piscataway: </address> <publisher> IEEE Press. </publisher>
Reference: <author> Frasconi, P., Gori, M., Maggini, M. & Soda, G. </author> <year> (1995). </year> <title> Representation of Finite State Automata in Recurrent Radial Basis Function Networks. Machine Learning. </title> <publisher> In press. </publisher>
Reference: <author> Fu, L. </author> <year> (1994). </year> <title> Rule Generation from Neural Networks. </title> <journal> IEEE Transactions on Neural Networks. </journal> <volume> 24. </volume> <pages> 1114-1124. </pages>
Reference: <author> Giles, C.L., Sun, G.Z., Chen, H.H., Lee, Y.C., & Chen, D. </author> <year> (1990). </year> <title> Higher Order Recurrent Networks & Grammatical Inference. </title> <editor> In D.S. Touretzky (Ed), </editor> <booktitle> Advances in Neural Information Systems 2 (pp. </booktitle> <pages> 380-387). </pages> <address> San Mateo: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Giles, C.L., Miller, C.B., Chen, D., Chen, H.H., Sun, G.Z. & Lee, Y.C. </author> <year> (1992). </year> <title> Learning and Extracting Finite State Automata with Second-Order Recurrent Neural Network. </title> <journal> Neural Computation, </journal> <volume> 4. </volume> <pages> 393-405. </pages>
Reference-contexts: how well do extracted DFAs generalize compared to the network's generalization performance; we also compare the performance of DFAs that are extracted with different quantization levels from the same network. 4 RULE EXTRACTION 4.1 Algorithm We describe our heuristic for extracting rules from recurrent networks in the form of DFAs <ref> (Giles et al. 1992) </ref>. Different approaches are described in Cleeremans (1989) and in Watrous & Kuhn (1992). The heuristic is not restricted to second-order networks and can be applied to any discrete-time neural network. <p> Our results raise the issue of could improved performance be obtained from some recursive combination of rule extraction and rule insertion. We have developed a simple algorithm which maps a (partial) description of a DFA into a second-order recurrent neural network by programming some of the weights accordingly <ref> (Omlin & Giles 1992) </ref>. The architecture of the network is defined by the amount of prior information about the otherwise unknown DFA. Significant learning time improvements were achieved by training networks with prior knowledge.
Reference: <author> Giles, C.L., Omlin, C.W. </author> <year> (1992). </year> <title> Inserting Rules into Recurrent Neural Networks. </title> <editor> In S. Kung, </editor> <publisher> F. </publisher>
Reference-contexts: how well do extracted DFAs generalize compared to the network's generalization performance; we also compare the performance of DFAs that are extracted with different quantization levels from the same network. 4 RULE EXTRACTION 4.1 Algorithm We describe our heuristic for extracting rules from recurrent networks in the form of DFAs <ref> (Giles et al. 1992) </ref>. Different approaches are described in Cleeremans (1989) and in Watrous & Kuhn (1992). The heuristic is not restricted to second-order networks and can be applied to any discrete-time neural network. <p> Our results raise the issue of could improved performance be obtained from some recursive combination of rule extraction and rule insertion. We have developed a simple algorithm which maps a (partial) description of a DFA into a second-order recurrent neural network by programming some of the weights accordingly <ref> (Omlin & Giles 1992) </ref>. The architecture of the network is defined by the amount of prior information about the otherwise unknown DFA. Significant learning time improvements were achieved by training networks with prior knowledge.
Reference: <author> Fallside, J.A. Sorenson & C. Kamm (Eds.), </author> <title> Neural Networks for Signal Processing II, </title> <booktitle> Proceedings of The 1992 IEEE Workshop, </booktitle> <address> (pp.13-22). Piscataway: </address> <publisher> IEEE Press. </publisher>
Reference: <author> Giles, C.L., Horne, B.G & Lin, T. </author> <year> (1994). </year> <title> Learning a Class of Large Finite State Machines with Neural Network Rule Extraction 15 a Recurrent Neural Network. </title> <type> Technical Report UMIACS-TR-94-94, </type> <institution> Institute for Advanced Computer Studies, University of Maryland, College Park. </institution>
Reference-contexts: Recurrent networks have recently been shown to be computationally quite powerful, i.e. Turing equivalent (Siegelmann & Sontag 1992). The representational limitations of Neural Network Rule Extraction 3 recurrent network models have been explored by showing their relationships to automata <ref> (Goudreau et. al. 1994, Giles et. al. 1995) </ref>. Thus, it is natural to see how well recurrent networks learn what they can be proven to represent. This being said, it is not evident that recurrent networks are the best tools for learning regular grammars.
Reference: <author> Giles, C.L. & Omlin, C.W. </author> <year> (1994). </year> <title> Pruning Recurrent Neural Networks for Improved Generalization Performance. </title> <journal> IEEE Transactions on Neural Networks. </journal> <volume> 5(5). </volume> <pages> 848-851. </pages>
Reference-contexts: Recurrent networks have recently been shown to be computationally quite powerful, i.e. Turing equivalent (Siegelmann & Sontag 1992). The representational limitations of Neural Network Rule Extraction 3 recurrent network models have been explored by showing their relationships to automata <ref> (Goudreau et. al. 1994, Giles et. al. 1995) </ref>. Thus, it is natural to see how well recurrent networks learn what they can be proven to represent. This being said, it is not evident that recurrent networks are the best tools for learning regular grammars.
Reference: <author> Giles, C.L., Chen, D., Sun, G.Z., Chen, H.H., Lee, Y.C. & Goudreau, M.W. </author> <year> (1995). </year> <title> Constructive Learning of Recurrent Neural Networks: Limitations of Recurrent Casade Correlation and a Simple Solution. </title> <journal> IEEE Transactions on Neural Networks. </journal> <note> In press. </note>
Reference: <author> Gold, E.M. </author> <year> (1978). </year> <title> Complexity of Automaton Identification from Given Data. </title> <journal> Information and Control, </journal> <volume> 37. </volume> <pages> 302-320. </pages>
Reference: <author> Goudreau, M.W., Giles, C.L., Chakradhar, S.T. & Chen, D. </author> <year> (1994). </year> <title> First-Order Vs. Second-Order Single Layer Recurrent Networks. </title> <journal> IEEE Transactions on Neural Networks. </journal> <volume> 5(3). </volume> <pages> 511-513. </pages>
Reference-contexts: Recurrent networks have recently been shown to be computationally quite powerful, i.e. Turing equivalent (Siegelmann & Sontag 1992). The representational limitations of Neural Network Rule Extraction 3 recurrent network models have been explored by showing their relationships to automata <ref> (Goudreau et. al. 1994, Giles et. al. 1995) </ref>. Thus, it is natural to see how well recurrent networks learn what they can be proven to represent. This being said, it is not evident that recurrent networks are the best tools for learning regular grammars.
Reference: <author> Hanson, S. & Burr, D. </author> <year> (1991). </year> <title> What Connectionist Models Learn: Learning and Representation in Connectionist Networks. </title> <editor> In R. Mammone & Y.Zeevi (Eds.), </editor> <booktitle> Neural Networks: Theory and Applications. </booktitle> <address> Boston: </address> <publisher> Academic Press. </publisher>
Reference: <author> Hertz, J., Krogh, A. & Palmer, R.G. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <address> Redwood City: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: the DFA corresponds to a non-terminal symbol and the DFA transitions correspond to production rules. 3 RECURRENT NEURAL NETWORK 3.1 Architecture Recurrent neural networks have been shown to have powerful capabilities for modeling many computational structures; an excellent discussion of recurrent neural network models and references can be found in <ref> (Hertz, Krogh & Palmer 1991) </ref>. To learn grammars, we use a second-order recurrent neural network (Lee et al. 1986; Giles et al. 1990; Sun et al. 1990; Pollack 1991). The network architecture is illustrated in figure 1.
Reference: <author> Hopcroft, J.E. & Ullman, J.F. </author> <year> (1979). </year> <title> Introduction to Automata Theory, </title> <booktitle> Languages and Computation, </booktitle> <address> Reading: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: A summary of the empirical results, followed by a short discussion of related work and possible future research directions conclude this paper. 2 REGULAR LANGUAGES 2.1 Definitions Regular languages represent the smallest class of formal languages in the Chomsky hierarchy <ref> (Hopcroft & Ullman 1979) </ref>. Regular languages are generated by regular grammars. <p> Other methods such as hierarchical clustering or unsupervised learning algorithms could be used instead. Although different DFAs may be extracted for different values of the quantization level q, a standard minimization algorithm <ref> (Hopcroft & Ullman 1979) </ref> can be applied yielding unique, minimal representations of the DFAs. Furthermore, these DFAs may be identical over ranges of the quantization level q. We have observed that different DFAs of minimal representation can be extracted from a trained network depending on the parameter q.
Reference: <author> Jim, K., Horne, B.G. & Giles, C.L. </author> <year> (1995). </year> <title> Effects of Noise on Convergence and Generalization in Recurrent Networks. </title> <editor> In Tesauro, G., Touretzky, D. & Leen, T. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> In press. </publisher>
Reference: <author> Lee, Y.C., Doolen, G., Chen, H.H., Sun, G.Z., Maxwell, T., Lee, H.Y. & Giles, C.L. </author> <year> (1986). </year> <title> Machine Learning Using a Higher Order Correlational Network. </title> <journal> Physica D, </journal> <volume> 22. </volume> <month> 276-306. </month> <title> Neural Network Rule Extraction 16 Manolios, </title> <editor> P. & Fanelli, R. </editor> <year> (1994). </year> <title> First Order Recurrent Neural Networks and Deterministic Finite State Automata. </title> <journal> Neural Computation. </journal> <volume> 6(6). </volume> <pages> 1154-1172. </pages>
Reference: <author> Omlin, C.W. & Giles, C.L., </author> <year> (1992). </year> <title> Training Second-Order Recurrent Neural Networks using Hints. </title> <editor> In Sleeman, D. & Edwards, P. (Eds.), </editor> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> (pp. 363-368). </pages> <address> San Mateo: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Our results raise the issue of could improved performance be obtained from some recursive combination of rule extraction and rule insertion. We have developed a simple algorithm which maps a (partial) description of a DFA into a second-order recurrent neural network by programming some of the weights accordingly <ref> (Omlin & Giles 1992) </ref>. The architecture of the network is defined by the amount of prior information about the otherwise unknown DFA. Significant learning time improvements were achieved by training networks with prior knowledge.
Reference: <author> Pollack, J.B. </author> <year> (1991). </year> <title> The Induction of Dynamical Recognizers. </title> <journal> Machine Learning. </journal> <volume> 7. </volume> <pages> 227-252. </pages>
Reference: <editor> Siegelmann, H.T. & Sontag, E.D. </editor> <booktitle> (1992). On the Computational Power of Neural Nets. In Proceedings of the Fifth ACM Workshop on Computational Learning Theory (pp.440-449). </booktitle> <address> New York: </address> <publisher> ACM. </publisher>
Reference-contexts: Grammars generate the languages that automata recognize. The computational power of recurrent networks has been proven by relating these networks to automata. Recurrent networks have recently been shown to be computationally quite powerful, i.e. Turing equivalent <ref> (Siegelmann & Sontag 1992) </ref>. The representational limitations of Neural Network Rule Extraction 3 recurrent network models have been explored by showing their relationships to automata (Goudreau et. al. 1994, Giles et. al. 1995).
Reference: <author> Sun, G.Z., Chen, H.H., Giles, C.L., Lee, Y.C. & Chen, D. </author> <year> (1990). </year> <title> Connectionist Pushdown Automata that Learn Context-Free Grammars. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks (pp. </booktitle> <pages> 577-580). </pages> <address> Hillsdale: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> Towell, G.G., Craven, M.W. & Shavlik, J.W., </author> <year> (1990). </year> <title> Constructive Induction using Knowledge-Based Neural Networks. In L.A. </title> <editor> Birnbaum, G.C. Collins (Eds.), </editor> <booktitle> Proceedings of the Eighth International Machine Learning Workshop (p.213). </booktitle> <address> San Mateo: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Watrous, R.L. & Kuhn, G.M. </author> <year> (1992). </year> <title> Induction of Finite-State Languages Using Second-Order Recurrent Networks. </title> <journal> Neural Computation. </journal> <volume> 4. </volume> <pages> 406-414. </pages>
Reference: <author> Williams, R.J. & Zipser, D. </author> <year> (1989). </year> <title> A Learning Algorithm for Continually Running Fully Recurrent Neural Networks. </title> <journal> Neural Computation. </journal> <volume> 1. </volume> <pages> 270-280. </pages>

References-found: 30

