URL: ftp://cse.ogi.edu/pub/tech-reports/1997/97-002.ps.gz
Refering-URL: ftp://cse.ogi.edu/pub/tech-reports/README.html
Root-URL: http://www.cse.ogi.edu
Title: Exploiting Non-Determinism in Set Iterators to Reduce I/O Latency  
Author: David C. Steere 
Date: March 26, 1997  
Abstract: A key goal of distributed systems is to provide prompt access to shared information repositories. The high latency of remote access is a serious impediment to this goal. We propose a new file system abstraction called dynamic sets that allows the system to transparently reduce I/O latency without relying on reference locality, without modifying DFS servers and protocols, and without unduly complicating the programming model. We present this abstraction, and describe an implementation of it that runs on local and distributed file systems, as well as the World Wide Web. Substantial performance gains are demonstrated up to 50% savings in runtime for search on NFS, and up to 90% reduction in I/O latency for Web searches. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> BACH, M. J. </author> <title> The Design of the Unix Operating System. </title> <publisher> Prentice Hall, Inc. A division of Simon & Schuster, </publisher> <address> En-glewood Cliffs, New Jersey 07632, </address> <year> 1986. </year> <title> Chapter 3: The Buffer Cache. </title>
Reference-contexts: One study found a 20x slow down in one case when prefetching data from disk on a parallel computer [15]. However, prefetching can produce substantial improvement if the access pattern is sufficiently regular and easily detected, such as Unix's one-block read-ahead mechanism <ref> [1, 27] </ref>. One way to avoid the problem of inaccurate predictions is to expose asynchronous I/O directly to applications, and let applications manage their I/O explicitly.
Reference: [2] <author> BAKER, M. G., HARTMAN, J. H., KUPFER, M. D., SHIRRIFF, K., AND OUSTERHOUT, J. K. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles (Octo-ber 1991). </booktitle>
Reference-contexts: Now consider how grep might use dynamic sets. First, grep would create a dynamic set to hold the files named 2 Main loop of grep while (*argv) f fd = open (argv++); execute (fd); close (fd); g Main loop using dynamic sets s = setOpen (argv <ref> [2] </ref>); while (fd = setIterate (s)) f execute (fd); close (fd); g setClose (s); The two sections of code reflect how grep can be modified to use dynamic sets. The code on the left is the main loop of grep. <p> Fortunately, the range of sizes under which dynamic sets offer greatest performance improvements covers most files in a typical Unix environment. Studies have shown median file sizes between 10KB and 16KB, and 80% to 90% of files are less than 50KB in size <ref> [2, 21, 25] </ref>. The second result is that SETS is able to exploit parallelism between servers to virtually eliminate latency, even for large files.
Reference: [3] <author> BOWMAN, M., SPASOJEVIC, M., AND SPECTOR, A. </author> <title> File system support for search. </title> <type> Transarc white paper, </type> <year> 1994. </year>
Reference-contexts: Executable specifications name programs that act as filters over a portion of the system's name space, returning the names of satisfactory files to SETS. Note that interpreted specifications use existing search engines such as SQL databases to provide functionality similar to that provided by search-enhanced file systems <ref> [7, 18, 3] </ref>. 5 4.2 SETS Prefetching Engine The prefetching engine consists of a number of worker threads, and is responsible for evaluating specifications and prefetching set members. The API layer generates requests on behalf of applications and queues them for workers.

Reference: [5] <author> CAO, P., FELTEN, E. W., AND LI, K. </author> <title> Implementation and performance of application-controlled file caching. </title> <booktitle> In Proceedingsof the First USENIX Symposium on Operating Systems Design and Implementation (November 1994). </booktitle>
Reference-contexts: The system can safely prefetch based on these hints, and the application is not complicated by the need to control prefetching or manage system resources. Recent studies by Patterson et al [23], Cao et al <ref> [4, 5] </ref>, and Kimbrel et al [13] have found significant speedups from informed prefetching in local file systems, particularly when reading data from multiple disks in parallel. These systems require application programmers to manually augment their code to pass hints of future block accesses to the file system.
Reference: [6] <author> CUREWITZ, K. M., KRISHNAN, P., AND VITTER, J. S. </author> <title> Practical prefetching via data compression. </title> <booktitle> In Proceedings of the 1993 ACM Conf. on Management of Data (SIGMOD) (May 1993). </booktitle>
Reference-contexts: The drawbacks of prefetching are that one must somehow predict future data accesses in order to prefetch them, and inaccurate predictions increase the load on the I/O subsystem, and can lead to thrashing. Systems that infer future accesses based on past history <ref> [16, 6, 31, 22, 9] </ref> are most susceptible to this problem. One study found a 20x slow down in one case when prefetching data from disk on a parallel computer [15].
Reference: [7] <author> GIFFORD, D. K., JOUVELOT, P., SHELDON, M. A., AND O'TOOLE, JR., J. W. </author> <title> Semantic file systems. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles (October 1991). </booktitle>
Reference-contexts: Executable specifications name programs that act as filters over a portion of the system's name space, returning the names of satisfactory files to SETS. Note that interpreted specifications use existing search engines such as SQL databases to provide functionality similar to that provided by search-enhanced file systems <ref> [7, 18, 3] </ref>. 5 4.2 SETS Prefetching Engine The prefetching engine consists of a number of worker threads, and is responsible for evaluating specifications and prefetching set members. The API layer generates requests on behalf of applications and queues them for workers.
Reference: [8] <author> GLASSMAN, S. </author> <title> A caching relay for the world wide web. </title> <booktitle> Computer Networks and ISDN Systems 27, </booktitle> <address> 2 (Nov. </address> <year> 1994). </year> <note> Special Issue: selected papers from the First International WWW Conference. </note>
Reference-contexts: For example, one study of a World Wide Web (Web) caching proxy saw only a 33% hit rate, even though the cache had unlimited size and served all external references from employees of a large computer company <ref> [8] </ref>.
Reference: [9] <author> GRIFFIOEN, J., AND APPLETON, R. </author> <title> The design, implementation, and evaluation of a predictive caching file system. </title> <type> Tech. Rep. </type> <institution> CS-264-96, Department of Computer Science, University of Kentucky, </institution> <month> June </month> <year> 1996. </year>
Reference-contexts: The drawbacks of prefetching are that one must somehow predict future data accesses in order to prefetch them, and inaccurate predictions increase the load on the I/O subsystem, and can lead to thrashing. Systems that infer future accesses based on past history <ref> [16, 6, 31, 22, 9] </ref> are most susceptible to this problem. One study found a 20x slow down in one case when prefetching data from disk on a parallel computer [15].
Reference: [10] <author> HOWARD, J., KAZAR, M., MENEES, S., NICHOLS, D., SATYANARAYANAN, M., SIDEBOTHAM, R., AND WEST, M. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Trans. Comput. Syst. </journal> <volume> 6, </volume> <month> 1 (Feb. </month> <year> 1988). </year>
Reference-contexts: Caching is widely used, and is nearly ubiquitous in distributed file systems <ref> [10, 24, 20] </ref> in which accessing remote data incurs high latency. However, caching is effective only if applications exhibit locality of reference. Prefetching does not rely on locality and so is more suited to applications with poor locality like search.
Reference: [11] <author> JOSEPH, A. D., DELESPINASSE, A. F., TAUBER, J. A., GIFFORD, D. K., AND KAASHOEK, M. F. </author> <title> Rover: A toolkit for mobile information access. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles (December 1995). </booktitle>
Reference-contexts: In addition, applications that manage I/O themselves are highly sensitive to changes in CPU or I/O speed, and are thus difficult to port or maintain. An example of explicit prefetching is the Queued RPC mechanism of the Rover toolkit <ref> [11] </ref>, which exposes asynchrony to application programmers and users. Although this can result in more efficient I/O, it requires the application programmer to poll to determine when an operation has completed and to maintain the operation's context until the operation terminates.
Reference: [12] <author> JOY, W. </author> <title> An introduction to the C shell. In Unix User's Manual, Supplementary Documents, </title> <editor> M. J. Karels and S. J. Leffler, Eds. </editor> <booktitle> Computer Science Division, </booktitle> <institution> Department of Electrical Engineering and Computer Science, University of California, </institution> <year> 1980. </year>
Reference-contexts: When the process exits, open sets are automatically destroyed and their resources freed. When a set is created, the creator supplies a specification which SETS evaluates to produce a list of the names of the set members. The specification language used by SETS extends the csh wildcard set notation <ref> [12] </ref> to supports three types of specifications. Figure 3 gives examples of each. Explicit specifications use standard csh wildcard notation to indicate the names of the members of the set. Interpreted specifications contain strings in some query language, such as SQL, delimited by n.
Reference: [13] <author> KIMBREL, T., TOKMINS, A., PATTERSON, R. H., BER-SHAD, B., CAO, P., FELTEN, E. W., GIBSON, G. A., KARLIN, A. R., AND LI, K. </author> <title> A trace-driven comparison of algorithms for parallel prefetching and caching. </title> <booktitle> In Proceedings of the Second USENIX Symposium on Operating Systems Design and Implementation (Oct. </booktitle> <year> 1996). </year>
Reference-contexts: The system can safely prefetch based on these hints, and the application is not complicated by the need to control prefetching or manage system resources. Recent studies by Patterson et al [23], Cao et al [4, 5], and Kimbrel et al <ref> [13] </ref> have found significant speedups from informed prefetching in local file systems, particularly when reading data from multiple disks in parallel. These systems require application programmers to manually augment their code to pass hints of future block accesses to the file system.
Reference: [14] <author> KLEIMAN, S. Vnodes: </author> <title> An architecture for multiple file system types in Sun UNIX. </title> <booktitle> In Summer USENIX Conference Proceedings (Atlanta, </booktitle> <year> 1986). </year>
Reference-contexts: Wardens can run in the kernel, such as the NFS warden which is based on an in-kernel NFS client, or in user-level processes. User-level wardens communicate with SETS using an existing upcall mechanism [29] which passes VFS file system operations <ref> [14] </ref> to user-level DFS clients, caching data in the kernel to avoid upcalls where 6 possible. We extended this mechanism with operations to prefetch an object, open a cursor for an interpreted specification, expand the cursor to retrieve the resulting filenames, and close the cursor.
Reference: [15] <author> KOTZ, D., AND ELLIS, C. </author> <title> Practical prefetching techniques for parallel file systems. </title> <booktitle> In Proceedings of the 1st International Conference on Parallel and Distributed Information Systems (Miami Beach, </booktitle> <address> Florida, </address> <month> Dec. </month> <year> 1992). </year>
Reference-contexts: Systems that infer future accesses based on past history [16, 6, 31, 22, 9] are most susceptible to this problem. One study found a 20x slow down in one case when prefetching data from disk on a parallel computer <ref> [15] </ref>. However, prefetching can produce substantial improvement if the access pattern is sufficiently regular and easily detected, such as Unix's one-block read-ahead mechanism [1, 27]. One way to avoid the problem of inaccurate predictions is to expose asynchronous I/O directly to applications, and let applications manage their I/O explicitly.
Reference: [16] <author> KUENNING, G. H. </author> <title> The design of the SEER predictive caching system. </title> <booktitle> In Proceedings of the Workshop on Mo 14 bile Computing Systems and Applications (Santa Cruz, </booktitle> <address> CA, </address> <month> Dec. </month> <year> 1994). </year>
Reference-contexts: The drawbacks of prefetching are that one must somehow predict future data accesses in order to prefetch them, and inaccurate predictions increase the load on the I/O subsystem, and can lead to thrashing. Systems that infer future accesses based on past history <ref> [16, 6, 31, 22, 9] </ref> are most susceptible to this problem. One study found a 20x slow down in one case when prefetching data from disk on a parallel computer [15].
Reference: [17] <author> LISKOV, B., AND GUTTAG, J. </author> <title> Abstraction and Specification in Program Development. The MIT EECS Series. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA ; McGraw-Hill, New York, </address> <year> 1986. </year>
Reference-contexts: Second, iterators are a convenient mechanism for processing groups of objects, as attested by the widespread use of iterator-like constructs such as cursors in SQL; foreach loops in shells like perl, tcl, and sh; and iterators in higher level languages like Alphard [26] and CLU <ref> [17] </ref>. Third, the use of iterators on sets of objects could allow a system to transparently reduce the aggregate I/O latency of accessing the set members if the iterator was visible to the system.
Reference: [18] <author> MANBER, U., AND WU, S. Glimpse: </author> <title> A tool to search through entire file systems. </title> <note> In Winter USENIX Conference Proceedings (1994). Also available as The University of Arizona Department of Computer Science Technical Report TR 93-34. </note>
Reference-contexts: The warden that interprets the query is not necessarily responsible for the objects named by the query, for instance a GLIMPSE <ref> [18] </ref> warden could reference NFS objects. The second example in Figure 3 would cause SETS to send the SQL query to a database mounted at /staff. <p> Executable specifications name programs that act as filters over a portion of the system's name space, returning the names of satisfactory files to SETS. Note that interpreted specifications use existing search engines such as SQL databases to provide functionality similar to that provided by search-enhanced file systems <ref> [7, 18, 3] </ref>. 5 4.2 SETS Prefetching Engine The prefetching engine consists of a number of worker threads, and is responsible for evaluating specifications and prefetching set members. The API layer generates requests on behalf of applications and queues them for workers.
Reference: [19] <author> MOWRY, T. C., DEMKE, A. K., AND KRIEGER, O. </author> <title> Automatic compiler-inserted I/O prefetching for out-of-core applications. </title> <booktitle> In Proceedings of the Second USENIX Symposium on Operating Systems Design and Implementation (Oct. </booktitle> <year> 1996). </year>
Reference-contexts: These systems require application programmers to manually augment their code to pass hints of future block accesses to the file system. Mowry et al <ref> [19] </ref> describe a similar approach which uses compiler generated hints to pre-page in a virtual memory system [19]. Their compiler generates prefetch requests by analyzing program loops to determine near-future data accesses in virtual memory. Similar analysis allows the compiler to insert hints to release pages as well. <p> These systems require application programmers to manually augment their code to pass hints of future block accesses to the file system. Mowry et al <ref> [19] </ref> describe a similar approach which uses compiler generated hints to pre-page in a virtual memory system [19]. Their compiler generates prefetch requests by analyzing program loops to determine near-future data accesses in virtual memory. Similar analysis allows the compiler to insert hints to release pages as well. The work described here also uses informed prefetching to reduce latency, but differs in several respects.
Reference: [20] <author> NELSON, M., WELCH, B., AND OUSTERHOUT, J. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Trans. Comput. Syst. </journal> <volume> 6, </volume> <month> 1 (Feb. </month> <year> 1988). </year>
Reference-contexts: Caching is widely used, and is nearly ubiquitous in distributed file systems <ref> [10, 24, 20] </ref> in which accessing remote data incurs high latency. However, caching is effective only if applications exhibit locality of reference. Prefetching does not rely on locality and so is more suited to applications with poor locality like search.
Reference: [21] <author> OUSTERHOUT, J. K., DA COSTA, H., HARRISON, D., KUNZE, J. A., KUPFER, M., AND THOMPSON, J. G. </author> <title> A trace-driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating Systems Principles (December 1985). </booktitle>
Reference-contexts: Fortunately, the range of sizes under which dynamic sets offer greatest performance improvements covers most files in a typical Unix environment. Studies have shown median file sizes between 10KB and 16KB, and 80% to 90% of files are less than 50KB in size <ref> [2, 21, 25] </ref>. The second result is that SETS is able to exploit parallelism between servers to virtually eliminate latency, even for large files.
Reference: [22] <author> PADMANABHAN, V. N., AND MOGUL, J. C. </author> <title> Using predictive prefetching to improve world wide web latency. </title> <journal> ACM SIGCOMM Computer Communication Review 26, </journal> <month> 3 (July </month> <year> 1996). </year>
Reference-contexts: The drawbacks of prefetching are that one must somehow predict future data accesses in order to prefetch them, and inaccurate predictions increase the load on the I/O subsystem, and can lead to thrashing. Systems that infer future accesses based on past history <ref> [16, 6, 31, 22, 9] </ref> are most susceptible to this problem. One study found a 20x slow down in one case when prefetching data from disk on a parallel computer [15].
Reference: [23] <author> PATTERSON, R. H., GIBSON, G. A., GINTING, E., STODOLSKY, D., AND ZELENKA, J. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating System Principles (Dec. </booktitle> <year> 1995). </year>
Reference-contexts: The system can safely prefetch based on these hints, and the application is not complicated by the need to control prefetching or manage system resources. Recent studies by Patterson et al <ref> [23] </ref>, Cao et al [4, 5], and Kimbrel et al [13] have found significant speedups from informed prefetching in local file systems, particularly when reading data from multiple disks in parallel. <p> However, controlling data layout in this manner is not practical in a real-world setting. An alternate strategy that avoided concurrently reading more than one file from the same disk should not suffer this problem. In addition, we could easily extend SETS to use a system like TIP2 <ref> [23] </ref> to manage local disk prefetching, if it were available on the same platform. 6 Dynamic Sets and the Web We now turn to the question of whether search on the World Wide Web could benefit from dynamic sets.
Reference: [24] <author> SANDBERG, R., GOLDBERG, D., KLEIMAN, S., WALSH, D., AND LYON, B. </author> <title> Design and implementation of the Sun Network File System. </title> <booktitle> In Summer USENIX Conference Proceedings, </booktitle> <address> Portland (1985). </address>
Reference-contexts: Caching is widely used, and is nearly ubiquitous in distributed file systems <ref> [10, 24, 20] </ref> in which accessing remote data incurs high latency. However, caching is effective only if applications exhibit locality of reference. Prefetching does not rely on locality and so is more suited to applications with poor locality like search.
Reference: [25] <author> SATYANARAYANAN, M. </author> <title> A study of file sizes and functional lifetimes. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Operating Systems Principles (December 1981). </booktitle>
Reference-contexts: Fortunately, the range of sizes under which dynamic sets offer greatest performance improvements covers most files in a typical Unix environment. Studies have shown median file sizes between 10KB and 16KB, and 80% to 90% of files are less than 50KB in size <ref> [2, 21, 25] </ref>. The second result is that SETS is able to exploit parallelism between servers to virtually eliminate latency, even for large files.
Reference: [26] <author> SHAW, M., WULF, W. A., AND LONDON, R. L. </author> <title> Abstraction and verification in Alphard: Defining and specifying iteration and generators. </title> <journal> Commun. ACM 20, </journal> <month> 8 (Mar. </month> <year> 1977). </year> <title> Reprinted in Tutorial: Programming Language Design, text for IEEE Tutorial by Anthony I. </title> <type> Wasserman, </type> <year> 1980, </year> <pages> pp. 145-155. </pages>
Reference-contexts: Second, iterators are a convenient mechanism for processing groups of objects, as attested by the widespread use of iterator-like constructs such as cursors in SQL; foreach loops in shells like perl, tcl, and sh; and iterators in higher level languages like Alphard <ref> [26] </ref> and CLU [17]. Third, the use of iterators on sets of objects could allow a system to transparently reduce the aggregate I/O latency of accessing the set members if the iterator was visible to the system.
Reference: [27] <author> SMITH, A. J. </author> <title> Disk cache miss ratio analysis and design considerations. </title> <journal> ACM Trans. Comput. Syst. </journal> <volume> 3, </volume> <month> 3 (Aug. </month> <year> 1985). </year>
Reference-contexts: One study found a 20x slow down in one case when prefetching data from disk on a parallel computer [15]. However, prefetching can produce substantial improvement if the access pattern is sufficiently regular and easily detected, such as Unix's one-block read-ahead mechanism <ref> [1, 27] </ref>. One way to avoid the problem of inaccurate predictions is to expose asynchronous I/O directly to applications, and let applications manage their I/O explicitly.
Reference: [28] <author> SPASOJEVIC, M., AND SATYANARAYANAN, M. </author> <title> A usage profile and evaluation of a wide-area distributed file system. </title> <booktitle> In Winter Usenix Conference Proceedings (San Fran-cisco, </booktitle> <address> CA, </address> <year> 1994). </year>
Reference-contexts: are handled in a similar manner, but start a new process in which to run the command instead of contacting a warden. 4.2.1 Prefetching Policy We designed the SETS's prefetching policy to work in an environment where remote access incurs a high latency, such as a wide-area DFS like AFS <ref> [28] </ref> or a mobile client connected over a low-bandwidth link. The policy has to balance conflicting goals: aggressive prefetching results in lower latencies, but may overwhelm disks, networks, or servers, resulting in thrashing and loss of performance.
Reference: [29] <author> STEERE, D., KISTLER, J., AND SATYANARAYANAN, M. </author> <title> Efficient user-level file cache management on the Sun vn-ode interface. </title> <booktitle> In Summer USENIX Conference Proceedings (Anaheim, </booktitle> <address> CA, </address> <year> 1990). </year>
Reference-contexts: Wardens can run in the kernel, such as the NFS warden which is based on an in-kernel NFS client, or in user-level processes. User-level wardens communicate with SETS using an existing upcall mechanism <ref> [29] </ref> which passes VFS file system operations [14] to user-level DFS clients, caching data in the kernel to avoid upcalls where 6 possible.
Reference: [30] <author> STEERE, D. C. </author> <title> Using Dynamic Sets to Reduce the Aggregate Latency of Data Access. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1997. </year> <note> Available as technical report CMU-CS-94-215. </note>
Reference-contexts: In ad dition, we ran two experiments to examine the effect of reordering and the benefits of dynamic sets for search on a local file system. A more complete set of experiments, including low bandwidth and interactive search tests, is described elsewhere <ref> [30] </ref>. 5.0.1 Test Methodology Our experiments use a benchmark program called synth-Grep to generate a workload for the system. <p> The traces were replayed during peak hours (afternoon EST) for greatest realism; other experiments that replay the traces on weekends, without loading inlined images, and over a phone line see vastly different latencies but similar benefits to those shown here <ref> [30] </ref>. broken out by search task and averaged over 5 runs. Each bar consists of three parts: the user think time captured in the trace, CPU to fetch and display the images, and the latency seen by Mosaic. <p> As a result, the system can prefetch without accurate predictions of latency by fetching a small number of objects concurrently and opportunistically yielding the first to return. 9 Acknowledgements This paper describes work originally presented in my thesis <ref> [30] </ref> performed while I was a Ph.D. student in the School of Computer Science at Carnegie Mellon University. My advisor, M. Satyanarayanan, made significant contributions to the work, as did my thesis committee - Garth Gibson, Jeannette Wing, and Hector Garcia-Molina.
Reference: [31] <author> TAIT, C. D., AND DUCHAMP, D. </author> <title> Detection and exploitation of file working sets. </title> <booktitle> In Proceedingsof the 11th International Conference on Distributed Com puting Systems (Ar-lington, </booktitle> <address> TX, </address> <year> 1991). </year>
Reference-contexts: The drawbacks of prefetching are that one must somehow predict future data accesses in order to prefetch them, and inaccurate predictions increase the load on the I/O subsystem, and can lead to thrashing. Systems that infer future accesses based on past history <ref> [16, 6, 31, 22, 9] </ref> are most susceptible to this problem. One study found a 20x slow down in one case when prefetching data from disk on a parallel computer [15].
Reference: [32] <institution> WEBCOMPASS 1.0. Quarterdeck, Corp. Marina del Ray, CA. </institution> <note> (800) 683-6696. Additional information is available at http://www.quarterdeck.com. </note>
Reference-contexts: If a user decides that she might wish to visit some number of the links on a page, she could create a set by selecting these links and then iterate on the set to view the members. There exist tools to prefetch sets of objects (such as WebCompass <ref> [32] </ref>), but these tools use predefined sets and prefetch set members well in advance of a search.
Reference: [33] <author> WING, J., AND STEERE, D. </author> <title> Specifying weak sets. </title> <booktitle> In Proceedings of the International Conference on Distributed Computer Systems (Vancouver, </booktitle> <month> June </month> <year> 1995). </year> <note> Also available as Carnegie Mellon University School of Computer Science technical report CMU-CS-94-194. 15 </note>
Reference-contexts: However, we leave the design of this more general abstraction to future work. * Loosely consistent Ideally, membership would be evaluated atomically and have perfect precision and recall (no false positives or negatives). However, it can be expensive in system complexity and performance to provide these properties <ref> [33] </ref>. Further, dynamic sets are layered on top of existing systems for simplicity, and as such cannot provide a stronger consistency model than the underlying system. Fortunately, many searches on DFS are satisfied without strong consistency guarantees, as the widespread use of these systems can attest.
References-found: 32

