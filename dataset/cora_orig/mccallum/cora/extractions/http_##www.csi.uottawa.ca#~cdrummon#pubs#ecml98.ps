URL: http://www.csi.uottawa.ca/~cdrummon/pubs/ecml98.ps
Refering-URL: http://www.csi.uottawa.ca/~cdrummon/pubs.html
Root-URL: 
Email: cdrummon@csi.uottawa.ca  
Title: Composing Functions to Speed up Reinforcement Learning in a Changing World  
Author: Chris Drummond 
Address: Ottawa, Ontario, Canada, K1N 6N5  
Affiliation: Department of Computer Science, University of Ottawa  
Abstract: This paper presents a system that transfers the results of prior learning to speed up reinforcement learning in a changing world. Often, even when the change to the world is relatively small an extensive relearning effort is required. The new system exploits strong features in the multi-dimensional function produced by reinforcement learning. The features generate a partitioning of the state space. The partition is represented as a graph. This is used to index and compose functions stored in a case base to form a close approximation to the solution of the new task. The experimental results investigate one important example of a changing world, a new goal position. In this situation, there is close to a two orders of magnitude increase in learning rate over using a basic reinforcement learning algorithm.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. D. Christiansen. </author> <title> Learning to predict in uncertain continuous tasks. </title> <type> ICML pp 72-81, </type> <year> 1992. </year>
Reference-contexts: The efficacy of this approach is due to the composition occurring at a sufficiently abstract level where much of the uncertainty has been removed. Each function acts much like a funnel operator <ref> [1] </ref>, so although individual actions may be highly uncertain the overall result is largely predictable. The central intuition of this work is that there are strong features in the multi-dimensional function learnt using reinforcement learning.
Reference: 2. <author> L. D. Cohen and Isaac Cohen. </author> <title> Finite element methods for active contour models and balloons for 2-d and 3-d images. </title> <type> PAMI 15(11) </type> <pages> 1131-1147, </pages> <month> Nov </month> <year> 1993. </year>
Reference-contexts: Fig. 5. Fitting the Snake The dark lines in figure 5 are the snake. Starting from the initial position, the smallest rectangle, the snake is expanded by a ballooning force <ref> [2] </ref> until it reaches the base of the hills. Now to simplify the exposition, we can imagine that the snake consists of a number of individual hill climbers spread out along the line representing the snake, indicated by the small circles. <p> When the snake reaches the top of the ridge, the largest rectangle in figure 5, it will tend to oscillate around an equilibrium position. By limiting the step size the process can be brought into a stationary state. A more detailed mathematical treatment of this approach is given in <ref> [2] </ref>. Looking at the gradient plot, the doorways are regions with a small differential between the ridges. The position of the doorways can be determined from the magnitude of the gradient along the snake.
Reference: 3. <author> E. W. Dijkstra. </author> <title> A note on two problems in connection with graphs. </title> <journal> Numer. Math. </journal> <volume> 1 </volume> <pages> 269-271, </pages> <year> 1959. </year>
Reference-contexts: The subgraph selected is a compromise between the best fit and the least scaling, particularly asymmetric scaling. The corresponding function from the case base is then modified using the same transform. Fig. 6. Using Dijkstra's Algorithm Function composition uses Dijkstra's algorithm <ref> [3] </ref> to traverse the edges between doorway nodes. Figure 6 shows the composite graph for the new task. To begin the process, the subgraph which contains the goal is selected and the best matching isomorphic subgraph is found.
Reference: 4. <author> C. Drummond. </author> <title> Using a case-base of surfaces to speed-up reinforcement learning. </title> <booktitle> LNAI volume 1266, </booktitle> <pages> pp 435-444, </pages> <year> 1997. </year>
Reference-contexts: In addition, the system only detects a change in goal position. Other changes such as the opening and closing of doors are also being investigated. Previous work by this author <ref> [4] </ref> investigated using the features discussed in this paper to recognise if a similar task had been solved previously. Functions stored in a case base were then used to initialise and thus speed up the initial learning process. These two ideas will combined in future work.
Reference: 5. <author> K. J. Hammond. </author> <title> Case-based planning: A framework for planning from experience. </title> <journal> Journal of Cognitive Science 14(3) </journal> <pages> 85-443, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: Unlike this other research, in the work presented here the case is not an example of the value function during learning. Rather it is a function representing a macro action and the principle should be complementary to these other approaches. This work is also related to case based planning <ref> [5, 17] </ref>, firstly through the general connection of reinforcement learning and planning. But it is analogous in other ways.
Reference: 6. <author> A. MacDonald. </author> <title> Graphs: </title> <booktitle> Notes on symetries, </booktitle> <institution> imbeddings, decompositions. Elec. Eng. Dept. TR-92-10-AJM, Brunel University, Uxbridge, </institution> <note> Middx, </note> <author> U. K., </author> <month> Oct </month> <year> 1992. </year>
Reference-contexts: The system finds all subgraphs in the case base isomorphic to a subgraph extracted using the snake and all possible isomorphic mappings between their nodes, using a labelling algorithm <ref> [6] </ref>. Associated with each node of a subgraph is an (x; y) coordinate. A similarity transform is applied to each of the isomorphic subgraphs to minimise the squared distance between the coordinates of the mapped nodes. The transform permits translation, rotation and independent scaling in each dimension.
Reference: 7. <author> S. Mahadevan and J. Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <booktitle> Artificial Intelligence 55 </booktitle> <pages> 311-365, </pages> <year> 1992. </year>
Reference-contexts: Thrun's research [16] identifies macro actions used in multiple tasks. But unlike the research presented here, no mapping of such actions to new tasks is proposed. Mahadevan and Connell <ref> [7] </ref> use reinforcement learning in behaviour based robot control. Although in a much simpler domain, the work presented here does not require rewards for individual macro actions. Rather the macro actions are identified and extracted from the solution of a compound task.
Reference: 8. <author> J. Peng. </author> <title> Efficient memory-based dynamic programming. </title> <note> ICML pp 438-439 1995. </note>
Reference-contexts: Rather the macro actions are identified and extracted from the solution of a compound task. Previous work that combines instance based or case based learning with reinforcement learning has principally addressed the economical representation of the state space. Peng <ref> [8] </ref> and Tadepalli [15] use learnt instances combined with linear regression over a set of neighbouring points. Sheppard and Salzberg [10] also use learnt instances but they are carefully selected by a genetic algorithm.
Reference: 9. <author> D. Precup and R. S. Sutton. </author> <title> Multi-time models for temporally abstract planning. </title> <booktitle> NIPS 10 1997. </booktitle>
Reference-contexts: When a new task is being learnt, the system will progressively build up a solution by function composition, as different features become apparent. 6 Related Work The most strongly related work is that investigating macro actions in reinforcement learning. Precup and Sutton <ref> [9] </ref> propose a possible semantics for macro actions within the framework of normal reinforcement learning. Singh [11] uses policies learnt to solve low level problems as primitives for reinforcement learning at a higher level.
Reference: 10. <author> J. W. Sheppard and S. L. Salzberg. </author> <title> A teaching strategy for memory-based control. </title> <journal> Artificial Intelligence Review 11 </journal> <pages> 343-370, </pages> <year> 1997. </year>
Reference-contexts: Previous work that combines instance based or case based learning with reinforcement learning has principally addressed the economical representation of the state space. Peng [8] and Tadepalli [15] use learnt instances combined with linear regression over a set of neighbouring points. Sheppard and Salzberg <ref> [10] </ref> also use learnt instances but they are carefully selected by a genetic algorithm. Unlike this other research, in the work presented here the case is not an example of the value function during learning.
Reference: 11. <author> S. P. Singh. </author> <title> Reinforcement learning with a hierarchy of abstract models. </title> <note> AAAI pp 202-207, </note> <year> 1992. </year>
Reference-contexts: Precup and Sutton [9] propose a possible semantics for macro actions within the framework of normal reinforcement learning. Singh <ref> [11] </ref> uses policies learnt to solve low level problems as primitives for reinforcement learning at a higher level. The work presented here gives one way macro actions can be extracted from the systems interaction with its environment without external help. Thrun's research [16] identifies macro actions used in multiple tasks.
Reference: 12. <author> P. Suetens, P. Fua, and A. Hanson. </author> <title> Computational strategies for object recogni tion. </title> <journal> Computing surveys 4(1) </journal> <pages> 5-61, </pages> <year> 1992. </year>
Reference-contexts: The important aspect of these features, for the purposes of this paper, is that they largely dictate the shape of this function. A popular technique in object recognition, the snake <ref> [12] </ref>, is used to locate and characterise these features. The snake is then converted into a discrete graph. The graph and its constituent subgraphs act as an index into a case base of previously learnt functions. When a new task is being learnt planning occurs at the graphical level.
Reference: 13. <author> R. S. Sutton. </author> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <type> NIPS 8 pp 1038-1044, </type> <year> 1996. </year>
Reference-contexts: The function discussed in previous paragraphs and shown in the figures represents this maximum value. The action selected in each state is usually the one with the highest score. But to encourage exploration of the state space this paper uses an * greedy policy <ref> [13] </ref> which chooses a random action a fraction * of the time.
Reference: 14. <author> R. S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <type> ICML pp 216-224, </type> <year> 1990. </year>
Reference-contexts: In figure 1 the robot would move to the state 10 steps from goal. If this process is repeated the robot will take the shortest path to goal. In practice we must, of course, learn such values. This can be done using some type of reinforcement learning <ref> [18, 14] </ref> which progressively improves estimates of the distance to goal from each state until they converge to the correct values. This paper is not primarily concerned with learning in a radically new environment, the system incorporates a reinforcement learning algorithm for that purpose. <p> This is an increase in learning rate of between times 60 and 100. 5 Limitations and Future Work Future work will address limitations in the present experimental validation. A more thorough comparison to other approaches is needed. An alternative straw man such Dyna-Q+ <ref> [14] </ref>, specifically designed to deal with changing worlds, would unquestionably reduce the speed-up experimentally obtained. Also, experiments using a random positioning of the goal would be more realistic. The system exploits symmetries in the domain.
Reference: 15. <author> P. Tadepalli and D. </author> <title> Ok. Scaling up average reward reinforcement learning by ap proximating learning by approximating. </title> <type> ICML pp 471-479, </type> <year> 1996. </year>
Reference-contexts: Rather the macro actions are identified and extracted from the solution of a compound task. Previous work that combines instance based or case based learning with reinforcement learning has principally addressed the economical representation of the state space. Peng [8] and Tadepalli <ref> [15] </ref> use learnt instances combined with linear regression over a set of neighbouring points. Sheppard and Salzberg [10] also use learnt instances but they are carefully selected by a genetic algorithm.
Reference: 16. <author> S. Thrun and A. Schwartz. </author> <title> Finding structure in reinforcement learning. </title> <note> NIPS 7 pp 385-392 1994. </note>
Reference-contexts: Singh [11] uses policies learnt to solve low level problems as primitives for reinforcement learning at a higher level. The work presented here gives one way macro actions can be extracted from the systems interaction with its environment without external help. Thrun's research <ref> [16] </ref> identifies macro actions used in multiple tasks. But unlike the research presented here, no mapping of such actions to new tasks is proposed. Mahadevan and Connell [7] use reinforcement learning in behaviour based robot control.
Reference: 17. <author> M. M. Veloso and J. G. Carbonell. </author> <title> Derivational analogy in prodigy: Automating case acquisition, storage and utilization. </title> <journal> Machine Learning, </journal> <volume> 10(3) </volume> <pages> 249-278, </pages> <year> 1993. </year>
Reference-contexts: Unlike this other research, in the work presented here the case is not an example of the value function during learning. Rather it is a function representing a macro action and the principle should be complementary to these other approaches. This work is also related to case based planning <ref> [5, 17] </ref>, firstly through the general connection of reinforcement learning and planning. But it is analogous in other ways.
Reference: 18. <author> C. J. C. H. Watkins and P. Dayan. </author> <note> Technical note: Q-learning. Machine Learning, 8(3-4):279-292, </note> <month> May </month> <year> 1992. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: In figure 1 the robot would move to the state 10 steps from goal. If this process is repeated the robot will take the shortest path to goal. In practice we must, of course, learn such values. This can be done using some type of reinforcement learning <ref> [18, 14] </ref> which progressively improves estimates of the distance to goal from each state until they converge to the correct values. This paper is not primarily concerned with learning in a radically new environment, the system incorporates a reinforcement learning algorithm for that purpose. <p> The use of an expectation here allows the actions to be stochastic, so when the robot takes a step in a particular direction from a particular state, the state reached is not always the same. To do the reinforcement learning this research uses the Q-learning algorithm <ref> [18] </ref>. This algorithm assumes the world is a discrete Markov process thus both states and actions actions are discrete. <p> But to encourage exploration of the state space this paper uses an * greedy policy [13] which chooses a random action a fraction * of the time. Q t+1 s;a + ff (r + flmax a 0 Q t s 0 ;a 0 ) (1) Watkins and Dayan <ref> [18] </ref> proved that this will converge to the optimal value with certain constraints on the reward and the learning rate ff. The optimal solution is to take the action with the greatest value in any state.
References-found: 18

