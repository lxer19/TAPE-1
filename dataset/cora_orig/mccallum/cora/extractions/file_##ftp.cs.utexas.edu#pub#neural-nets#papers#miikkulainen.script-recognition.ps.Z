URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/miikkulainen.script-recognition.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Email: risto@cs.ucla.edu  
Title: SCRIPT RECOGNITION WITH HIERARCHICAL FEATURE MAPS  
Author: Risto Miikkulainen 
Address: Los Angeles, CA 90024  
Affiliation: Artificial Intelligence Laboratory Computer Science Department University of California,  
Abstract: The hierarchical feature map system recognizes an input story as an instance of a particular script by classifying it at three levels: scripts, tracks and role bindings. The recognition taxonomy, i.e. the breakdown of each script into the tracks and roles, is extracted automatically and independently for each script from examples of script instantiations in an unsupervised self-organizing process. The process resembles human learning in that the differentiation of the most frequently encountered scripts become gradually the most detailed. The resulting structure is a hierachical pyramid of feature maps. The hierarchy visualizes the taxonomy and the maps lay out the topology of each level. The number of input lines and the self-organization time are considerably reduced compared to the ordinary single-level feature mapping. The system can recognize incomplete stories and recover the missing events. The taxonomy also serves as memory organization for script-based episodic memory. The maps assign a unique memory location for each script instantiation. The most salient parts of the input data are separated and most resources are concentrated on representing them accurately. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bower, G. H., Black, J. B., and Turner, T. J. </author> <year> (1979). </year> <title> Scripts in memory for text. </title> <journal> Cognitive Psychology, </journal> <volume> 11 </volume> <pages> 177-220. </pages>
Reference-contexts: For example, there is a fancy-restaurant track, fast-food track and a coffee-shop track for the restaurant script, with slightly different events. Script theory is attractive because it separates a fairly well defined, managable part of cognition. Scripts are intuitively plausible and well supported by experimental evidence <ref> (Bower et al., 1979) </ref>. They also provide a computational theory, similar in spirit to the frame theory of representation in artificial intelligence (Minsky, 1981). The basic idea behind both is schematic knowledge: people organize knowledge about familiar objects, events and situations in terms of prototypes, or schemata.
Reference: <author> Cullingford, R. E. </author> <year> (1978). </year> <title> Script Application: Computer Understanding of Newspaper Stories. </title> <type> PhD thesis, </type> <institution> New Haven, CT: Department of Computer Science, Yale University. </institution> <type> Technical Report 116. </type>
Reference: <author> DeJong, G. F. </author> <year> (1979). </year> <title> Skimming Stories in Real Time: An Experiment in Integrated Understanding. </title> <type> PhD thesis, </type> <institution> New Haven, CT: Department of Computer Science, Yale University. </institution> <type> Technical Report 158. </type>
Reference: <author> Dolan, C. P. and Dyer, M. G. </author> <year> (1987). </year> <title> Symbolic schemata, role binding, and the evolution of structure in connectionist memories. </title> <booktitle> In Proceedings of the IEEE First International Conference on Neural Networks. </booktitle> <address> Piscataway, NJ: </address> <publisher> IEEE. </publisher>
Reference-contexts: In these models, the recognition of the appropriate script, its instantiation and inferences automatically emerge from the input story representation in a distributed fashion. Connectionist mechanisms for processing causal sequences (Golden, 1986), connecting multiple scripts (Sharkey et al., 1986), role binding <ref> (Dolan and Dyer, 1987) </ref>, and learning inferences from examples (Miikkulainen and Dyer, 1989b; Miikkulainen, 1990) have been presented. However, the script representations are still coded in beforehand and remain fixed in these models. 2 between 0 and 1.
Reference: <author> Dyer, M. G. </author> <title> (in press). Symbolic NeuroEngineering for natural language processing: A multilevel research approach. </title> <editor> In Barnden, J. and Pollack, J., editors, </editor> <booktitle> Advances in Connectionist and Neural Computation Theory, </booktitle> <volume> Vol. 1: </volume> <booktitle> High-Level Connectionist Models. </booktitle> <address> Norwood, NJ: </address> <publisher> Ablex. </publisher>
Reference: <author> Dyer, M. G., Cullingford, R. E., and Alvarado, S. </author> <year> (1987). </year> <title> Scripts. </title> <editor> In Shapiro, S. C., editor, </editor> <booktitle> Encyclopedia of Artificial Intelligence. </booktitle> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: In these models, the recognition of the appropriate script, its instantiation and inferences automatically emerge from the input story representation in a distributed fashion. Connectionist mechanisms for processing causal sequences (Golden, 1986), connecting multiple scripts (Sharkey et al., 1986), role binding <ref> (Dolan and Dyer, 1987) </ref>, and learning inferences from examples (Miikkulainen and Dyer, 1989b; Miikkulainen, 1990) have been presented. However, the script representations are still coded in beforehand and remain fixed in these models. 2 between 0 and 1.
Reference: <author> Fillmore, C. J. </author> <year> (1968). </year> <title> The case for case. In Bach, </title> <editor> E. and Harms, R. T., editors, </editor> <booktitle> Universals in Linguistic Theory. </booktitle> <address> New York: </address> <publisher> Holt, Rinehart and Winston. 14 Golden, </publisher> <editor> R. M. </editor> <year> (1986). </year> <title> Representing causal schemata in connectionist systems. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Hinton, G. E. </author> <year> (1981). </year> <title> Implementing semantic networks in parallel hardware. </title> <editor> In Hinton, G. E. and Anderson, J. A., editors, </editor> <booktitle> Parallel Models of Associative Memory. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Hopfield, J. J. </author> <year> (1982). </year> <title> Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> In Proceedings of the National Academy of Sciences, USA, </booktitle> <pages> pages 2554-2558. </pages>
Reference: <author> Kohonen, T. </author> <year> (1982a). </year> <title> Analysis of a simple self-organizing process. </title> <journal> Biological Cybernetics, </journal> <volume> 44 </volume> <pages> 135-140. </pages>
Reference: <author> Kohonen, T. </author> <year> (1982b). </year> <title> Clustering, taxonomy, and topological maps of patterns. </title> <booktitle> In Proceedings of the Sixth International Conference on Pattern Recognition. </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: If the data is hierarchical, the map reflects this through topological order. For instance, the map of taxonomical data essentially displays the minimal spanning tree of the data items, curved to fill in the whole area of the map <ref> (Kohonen, 1982b) </ref>. Or, if we form a mapping of script-based stories, different variations of the scripts are mapped near each other (figure 3). Knowing what the hierarchical taxonomy of the data is, it is easy to see that the spatial layout of the map reflects the taxonomy.
Reference: <author> Kohonen, T. </author> <year> (1982c). </year> <title> Self-organized formation of topologically correct feature maps. </title> <journal> Biological Cybernetics, </journal> <volume> 43 </volume> <pages> 59-69. </pages>
Reference: <author> Kohonen, T. </author> <year> (1984). </year> <title> Self-Organization and Associative Memory. </title> <address> Berlin; Heidelberg; New York: </address> <publisher> Springer. </publisher>
Reference: <author> Kolodner, J. L. </author> <year> (1983a). </year> <title> Maintaining organization in a dynamic long-term memory. </title> <journal> Cognitive Science, </journal> <volume> 7 </volume> <pages> 243-280. </pages>
Reference: <author> Kolodner, J. L. </author> <year> (1983b). </year> <title> Reconstructive memory: A computer model. </title> <journal> Cognitive Science, </journal> <volume> 7 </volume> <pages> 281-328. </pages>
Reference: <author> McClelland, J. L. and Kawamoto, A. H. </author> <year> (1986). </year> <title> Mechanisms of sentence processing: Assigning roles to constituents. </title> <editor> In McClelland, J. L. and Rumelhart, D. E., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 2: Psychological and Biological Models. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Miikkulainen, R. </author> <year> (1987). </year> <title> Self-organizing process based on lateral inhibition and weight redistribution. </title> <type> Technical Report UCLA-AI-87-16, </type> <institution> Los Angeles: Computer Science Department, University of California, </institution> <address> Los Angeles. </address>
Reference-contexts: The relative differences between the items are greater, making the self-organization easier. In a single-level map all units need to receive the complete representations of all input items (figure 2). Modifying the weights is costly, because the initial neighborhood must cover almost the entire map <ref> (Miikkulainen, 1987) </ref>. In a hierarchical map system, the maps at lower levels receive only small subsets of the original input lines, and the neighborhoods are always small because the maps are small at all levels (figure 5). This reduces the training time considerably. <p> The self-organizing process resembles a human learner in building the script taxonomy. The process begins by establishing a gross ordering of the input data, dividing the input into a few large categories <ref> (Miikkulainen, 1987) </ref>. The most prominent and regular event sequences are recognized as the first rudimentary scripts. For example, the restaurant and shopping stories are first grouped together, separate from the travel stories. These categories become gradually more refined, as more attention is paid on the details.
Reference: <author> Miikkulainen, R. </author> <year> (1990). </year> <title> A neural network model of script processing and memory. </title> <type> Technical Report UCLA-AI-90-03, </type> <institution> Los Angeles: Computer Science Department, University of California, </institution> <address> Los Angeles. </address>
Reference-contexts: The script recognition system is not dependent on this particular type of story representation. The concatenation of event representations is perhaps the most straightforward approach. Another possibility would be to compose the vector from assemblies specifying the script name, track name, and the role bindings <ref> (Miikkulainen, 1990) </ref>. It is essential, though, that the front end forms a stationary representation of the event sequence, which can then be input to the feature maps. <p> The taxonomy can also be used to organize an episodic memory of these stories <ref> (Miikkulainen, 1990) </ref>. Stories are read in and memorized one at a time and can be retrieved with a partial cue. 11 An ordinary feature map is a classifier, mapping an input vector to a location on the map. A trace feature map (Miikkulainen, 1990), in addition, creates a memory trace on <p> to organize an episodic memory of these stories <ref> (Miikkulainen, 1990) </ref>. Stories are read in and memorized one at a time and can be retrieved with a partial cue. 11 An ordinary feature map is a classifier, mapping an input vector to a location on the map. A trace feature map (Miikkulainen, 1990), in addition, creates a memory trace on that location. The map remembers that at some point it received an input item that was classified at that location. Trace feature map mechanism is used at the bottom level of the map hierarchy to create a trace of the story.
Reference: <author> Miikkulainen, R. and Dyer, M. G. </author> <year> (1989a). </year> <title> Encoding input/output representations in connectionist cognitive systems. </title> <editor> In Touretzky, D. S., Hinton, G. E., and Sejnowski, T. J., editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The network also develops distributed representations for the words while it is learning the case-role assignment task. The resulting representations reflect the regularities in the use of the words, and can be claimed to code the meanings of the words as well <ref> (Miikkulainen and Dyer, 1989a) </ref>. 3 For each role, only a generic filler representation (e.g. PERSON, FANCY-FOOD) was developed by the sentence parser. Representations for the different filler instances (e.g.
Reference: <author> Miikkulainen, R. and Dyer, M. G. </author> <year> (1989b). </year> <title> A modular neural network architecture for sequential paraphrasing of script-based stories. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks. </booktitle> <address> Piscataway, NJ: </address> <publisher> IEEE. </publisher>
Reference: <author> Minsky, M. </author> <year> (1981). </year> <title> A framework for representing knowledge. </title> <editor> In Haugeland, J., editor, </editor> <booktitle> Mind Design: Philosophy, Psychology, Artificial Intelligence. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Script theory is attractive because it separates a fairly well defined, managable part of cognition. Scripts are intuitively plausible and well supported by experimental evidence (Bower et al., 1979). They also provide a computational theory, similar in spirit to the frame theory of representation in artificial intelligence <ref> (Minsky, 1981) </ref>. The basic idea behind both is schematic knowledge: people organize knowledge about familiar objects, events and situations in terms of prototypes, or schemata. A schema contains representation for common knowledge, shared by all instances, and a number of slots which take different values for different instances.
Reference: <author> Ritter, H. J. and Schulten, K. J. </author> <year> (1988). </year> <title> Convergency properties of Kohonen's topology conserving maps: Fluctuations, stability and dimension selection. </title> <journal> Biological Cybernetics, </journal> <volume> 60 </volume> <pages> 59-71. </pages>
Reference: <author> Schank, R. and Abelson, R. </author> <year> (1977). </year> <title> Scripts, Plans, Goals, and Understanding An Inquiry into Human Knowledge Structures. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: 1 Introduction According to script theory, people organize knowledge of everyday routines in the form of scripts <ref> (Schank and Abelson, 1977) </ref>. Scripts are schemata of often-encountered, stereotypical sequences of events. Common knowledge of this kind makes it possible to efficiently perform social tasks such as visiting a restaurant, visiting a doctor, shopping at a supermarket, traveling by airplane, attending a meeting, etc. <p> The maps assign different areas of the memory to different items, making it possible to store them one-shot. As was pointed out in <ref> (Schank and Abelson, 1977) </ref>, people seem to make scripts out of all regularities in everyday experiences, and in several levels of complexity, abstraction and extent.
Reference: <author> Sharkey, N. E., Sutcliffe, R. F. E., and Wobcke, W. R. </author> <year> (1986). </year> <title> Mixing binary and continuous connection schemes for knowledge access. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 15 </pages>
Reference-contexts: A number of connectionist models of script application have also been proposed. In these models, the recognition of the appropriate script, its instantiation and inferences automatically emerge from the input story representation in a distributed fashion. Connectionist mechanisms for processing causal sequences (Golden, 1986), connecting multiple scripts <ref> (Sharkey et al., 1986) </ref>, role binding (Dolan and Dyer, 1987), and learning inferences from examples (Miikkulainen and Dyer, 1989b; Miikkulainen, 1990) have been presented. However, the script representations are still coded in beforehand and remain fixed in these models. 2 between 0 and 1.
References-found: 24

