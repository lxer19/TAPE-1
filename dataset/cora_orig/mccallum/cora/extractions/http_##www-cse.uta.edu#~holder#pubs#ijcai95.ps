URL: http://www-cse.uta.edu/~holder/pubs/ijcai95.ps
Refering-URL: http://www-cse.uta.edu/~holder/pubs.html
Root-URL: 
Title: Intermediate Decision Trees  
Author: Lawrence B. Holder 
Address: Box 19015, Arlington, TX 76019-0015  
Affiliation: Department of Computer Science and Engineering University of Texas at Arlington  
Abstract: Intermediate decision trees are the subtrees of the full (unpruned) decision tree generated in a breadth-first order. An extensive empirical investigation evaluates the classification error of intermediate decision trees and compares their performance to full and pruned trees. Empirical results were generated using C4.5 with 66 databases from the UCI machine learning database repository. Results show that when attempting to minimize the error of the pruned tree produced by C4.5, the best intermediate tree performs significantly better in 46 of the 66 databases. These and other results question the effectiveness of decision tree pruning strategies and suggest further consideration of the full tree and its intermediates. Also, the results reveal specific properties satisfied by databases in which the intermediate full tree performs best. Such relationships improve guidelines for selecting appropriate inductive strategies based on domain properties.
Abstract-found: 1
Intro-found: 1
Reference: [ Elomaa, 1994 ] <author> T. Elomaa. </author> <title> In defense of c4.5: Notes on learning one-level decision trees. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 62-69, </pages> <year> 1994. </year>
Reference: [ Holder and Chaudhry, 1993 ] <author> L. B. Holder and A. Chaudhry. </author> <title> Simple selection of utile control rules in speedup learning. </title> <booktitle> In Proceedings of the Third International Workshop on Knowledge Compilation and Speedup Learning, </booktitle> <pages> pages 77-82, </pages> <year> 1993. </year>
Reference-contexts: Previous results have shown that the intermediate decision tree compares favorably to other pruning methods [ Holder, 1992a ] , and intermediate learned concepts in general address overfitting issues in both inductive and speedup learning <ref> [ Holder, 1990; 1992b; Holder and Chaudhry, 1993 ] </ref> . The main result of this paper is that the best intermediate tree of a full decision tree is often better (less error) than the pruned tree.
Reference: [ Holder, 1990 ] <author> L. B. Holder. </author> <title> The general utility problem in machine learning. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 402-410, </pages> <year> 1990. </year>
Reference-contexts: Previous results have shown that the intermediate decision tree compares favorably to other pruning methods [ Holder, 1992a ] , and intermediate learned concepts in general address overfitting issues in both inductive and speedup learning <ref> [ Holder, 1990; 1992b; Holder and Chaudhry, 1993 ] </ref> . The main result of this paper is that the best intermediate tree of a full decision tree is often better (less error) than the pruned tree.
Reference: [ Holder, 1992a ] <author> L. B. Holder. </author> <title> Empirical analysis of the general utility problem in machine learning. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 249-254, </pages> <year> 1992. </year>
Reference-contexts: Previous results have shown that the intermediate decision tree compares favorably to other pruning methods <ref> [ Holder, 1992a ] </ref> , and intermediate learned concepts in general address overfitting issues in both inductive and speedup learning [ Holder, 1990; 1992b; Holder and Chaudhry, 1993 ] .
Reference: [ Holder, 1992b ] <author> L. B. Holder. </author> <title> Unifying empirical and explanation-based learning by modeling the utility of learned knowledge. </title> <booktitle> In Proceedings of the ML92 Workshop on Knowledge Compilation and Speedup Learning, </booktitle> <year> 1992. </year>
Reference: [ Holte, 1993 ] <author> R. C. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 63-90, </pages> <year> 1993. </year>
Reference: [ Mangasarian and Wolberg, 1990 ] <author> O. L. Mangasarian and W. H. Wolberg. </author> <title> Cancer diagnosis via linear programming. </title> <journal> SIAM News, </journal> <volume> 23(5) </volume> <pages> 1-18, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Attributes 7-28 were typed continuous, and case 65's illegal value of SI TIPO for attribute 10 was set to 1 TIPO. The Wisconsin breast cancer database (BCW) was originally obtained from the University of Wisconsin Hospitals in Madison thanks to Dr. William H. Wol-berg <ref> [ Mangasarian and Wolberg, 1990 ] </ref> . The breast cancer (BCR), lymphography (LYM) and primary tumor (PRI) databases were originally obtained from the University Medical Centre, Institute of Oncology, Ljubljana, Yugoslavia thanks to M. Zwitter and M. Soklic.
Reference: [ Mingers, 1989 ] <author> J. Mingers. </author> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4(2) </volume> <pages> 227-243, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Numerous decision tree pruning methods have been developed to reduce decision tree error due to overfitting on the part of the decision tree induction method <ref> [ Mingers, 1989 ] </ref> . Of course, as Schaffer [ 1993; 1994 ] emphasizes, no one pruning method can improve the performance of decision tree induction on all domains. However, certain pruning methods can improve performance on certain domains.
Reference: [ Murphy and Aha, 1994 ] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI repository of machine learning databases: Machine-readable data repository. </title> <institution> University of Cali-fornia, Department of Information and Computer Science, </institution> <address> Irvine, CA, </address> <year> 1994. </year>
Reference-contexts: Section 3 discusses the intermediate decision tree in detail. Section 4 presents the experimental results based on a large sampling of the University of California, Irvine Machine Learning Database Repository <ref> [ Murphy and Aha, 1994 ] </ref> . The appendices describe these databases and show a sample of the experimental results. <p> DNF2 domain in [ Pagallo and Haussler, 1990 ] . 4 Experiments In order to evaluate the performance of intermediate trees, an extensive empirical investigation was performed using the decision tree induction program C4.5 Release 6.0 [ Quinlan, 1992 ] on 66 databases from the UCI Machine Learning Databases Repository <ref> [ Murphy and Aha, 1994 ] </ref> . Appendix A describes the databases. Four different trees were considered for comparison: the full tree FT (no post-pruning), the pruned tree PT, the best (lowest error) intermediate tree of the full tree IFT, and the best intermediate tree of the pruned tree IPT. <p> However, continued derivation and verification of relationships between database properties and induction strategies will lead to improved guidelines for selecting the proper strategy. A Databases All databases used in the experiments were taken from the UCI Repository of Machine Learning Databases <ref> [ Murphy and Aha, 1994 ] </ref> . Table 1 lists the 66 databases and the corresponding label used for the database throughout the paper.
Reference: [ Murphy and Pazzani, 1994 ] <author> P. M. Murphy and M. J. Pazzani. </author> <title> Exploring the decision forest: An empirical investigation of occam's razor in decision tree induction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 257-275, </pages> <year> 1994. </year>
Reference: [ Pagallo and Haussler, 1990 ] <author> G. Pagallo and D. Haus-sler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-100, </pages> <year> 1990. </year>
Reference-contexts: This behavior of the error curves is typical for many domains. The specific behavior in which the minimum of the breadth-first traversal is less than the final error is the motivation for our interest in intermediate decision trees. 1 These particular curves come from the DNF2 domain in <ref> [ Pagallo and Haussler, 1990 ] </ref> . 4 Experiments In order to evaluate the performance of intermediate trees, an extensive empirical investigation was performed using the decision tree induction program C4.5 Release 6.0 [ Quinlan, 1992 ] on 66 databases from the UCI Machine Learning Databases Repository [ Murphy and Aha,
Reference: [ Quinlan, 1992 ] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: for our interest in intermediate decision trees. 1 These particular curves come from the DNF2 domain in [ Pagallo and Haussler, 1990 ] . 4 Experiments In order to evaluate the performance of intermediate trees, an extensive empirical investigation was performed using the decision tree induction program C4.5 Release 6.0 <ref> [ Quinlan, 1992 ] </ref> on 66 databases from the UCI Machine Learning Databases Repository [ Murphy and Aha, 1994 ] . Appendix A describes the databases. <p> Specifically, databases in which IFT had significantly less error than PT opt were labeled as positive examples; the remaining databases were labeled negative. Using these examples, the C4.5 rule generator <ref> [ Quinlan, 1992 ] </ref> produced the following rules (the numbers to the right of each rule indicate the correct/incorrect classifications): (Classes &gt; 3) & (Cont = 0) ! - [3/0] (Size 23) ! - [5/1] (Classes 2) & (Cont = 0) & (A2 2) ! - [4/1] (Size &gt; 3197) !
Reference: [ Schaffer, 1993 ] <author> C. Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10(2) </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference: [ Schaffer, 1994 ] <author> C. Schaffer. </author> <title> A conservation law for generalization performance. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 259-265, </pages> <year> 1994. </year>

References-found: 14

