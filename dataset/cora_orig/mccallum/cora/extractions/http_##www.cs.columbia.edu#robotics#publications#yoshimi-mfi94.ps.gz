URL: http://www.cs.columbia.edu/robotics/publications/yoshimi-mfi94.ps.gz
Refering-URL: http://www.cs.columbia.edu/robotics/publications/publications.html
Root-URL: http://www.cs.columbia.edu
Title: Visual Control of Grasping and Manipulation Tasks  
Author: Billibon H. Yoshimi and Peter K. Allen 
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University,  
Abstract: This paper discusses the problem of visual control of grasping. We have implemented an object tracking system that can be used to provide visual feedback for locating the positions of fingers and objects to be manipulated, as well as the relative relationships of them. This visual analysis can be used to control open loop grasping systems in a number of manipulation tasks where finger contact, object movement, and task completion need to be monitored and controlled. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Abrams and P. Allen. </author> <title> Dynamic sensor plan ning. </title> <booktitle> In IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 2-605 - 2-610, </pages> <month> May 2-7 </month> <year> 1993. </year>
Reference-contexts: The problem of visual occlusion may be alleviated through the use of active vision and multi-camera techniques, in which the vision system moves to keep designated features and objects in view <ref> [1] </ref>. Additionally, we believe it may be possible to estimate grasping forces from vision alone, given an appropriate spring model of force/displacement and knowledge of the actuator forces. Visual control of grasping is not a panacea. <p> The method described here needs to be extended in a number of ways. First, we are currently extending these results to 3-D analysis by use of stereo (2 mon itoring cameras) and active techniques with a single camera <ref> [1] </ref>. Second, the experimental results shown here can track the finger-bolt-finger snakes at about 1/7 Hertz.; however, we have optimized this method to achieve a speed of approximately 1 Hertz by intelligent buffering of images and transfers to the host.
Reference: [2] <author> A. Blake. </author> <title> Computational modelling of hand-eye coordination. In Active Perception. </title> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <year> 1993. </year>
Reference: [3] <author> R. Cipolla and A. Blake. </author> <title> Surface orientation and time to contact from image divergence and deformation. </title> <booktitle> In ECCV 92, </booktitle> <pages> pages 187-202, </pages> <year> 1992. </year>
Reference-contexts: They have been used in many different image-processing applications ranging from tracking of medical imagery to facial expression modeling. The work most directly related to ours involves the real-time computation of snakes and includes Williams and Shah [11], Cipolla and Blake <ref> [3] </ref>, Curwen and Blake [4] and Hollinghurst and Cipolla [5].
Reference: [4] <author> R. Curwen and A. Blake. </author> <title> Dynamic contours: Real-time active splines. </title> <booktitle> In Active Vision, </booktitle> <pages> pages 39-57. </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: They have been used in many different image-processing applications ranging from tracking of medical imagery to facial expression modeling. The work most directly related to ours involves the real-time computation of snakes and includes Williams and Shah [11], Cipolla and Blake [3], Curwen and Blake <ref> [4] </ref> and Hollinghurst and Cipolla [5].
Reference: [5] <author> N. Hollinghurst and R. Cipolla. </author> <title> Uncalibrated stereo hand-eye coordination. </title> <type> Technical Report CUED/F-INFENG/TR126, </type> <institution> Department of En gineering, University of Cambridge, </institution> <year> 1993. </year>
Reference-contexts: This method can be used to track multiple fingers and objects, alleviating some of the problems discussed above. The manipulation tasks we have experimented with are primarily 2-D in nature; however, by using a variety of methods, including stereo <ref> [5] </ref>, these results can be applied to 3-D scenes. The problems of occlusion and force estimation are subjects of our ongoing research, but they will not be discussed in detail in this paper. <p> The work most directly related to ours involves the real-time computation of snakes and includes Williams and Shah [11], Cipolla and Blake [3], Curwen and Blake [4] and Hollinghurst and Cipolla <ref> [5] </ref>.
Reference: [6] <author> M. Kass, A. Witkin, and D. Terzopoulos. Snakes: </author> <title> Active contour models. </title> <booktitle> In Proceedings of First International Conference on Computer Vision, </booktitle> <pages> pages 259-269, </pages> <year> 1987. </year>
Reference-contexts: Depending on the complexity of the experiment, this can range from tracking a nut, to tracking the fingers of a robotic hand, to tracking a complete robot. A useful tracking primitive is a snake. Kass et al. <ref> [6] </ref> originally developed snakes, a model for representing image contours which allows them to be easily manipulated by higher level processes. The central idea behind a snake is that it is a deformable contour that moves under a variety of image constraints (which tend to be local) and object-model constraints.
Reference: [7] <author> T. Murphy, D. Lyons, and A. Hendriks. </author> <title> Visu ally guided multi-fingered robot hand grasping as defined by schemas and a reactive system. </title> <booktitle> In USC Workshop on Neural Architectures and Distributed AI: From Schema Assemblages to Neural Networks, </booktitle> <pages> pages 1-5, </pages> <year> 1993. </year>
Reference: [8] <author> R. Sharma, J. Herve, and P. Cucka. </author> <title> Analy sis of dynamic hand positioning tasks using visual feedback. </title> <type> Technical Report CAR-TR-574, </type> <institution> Center for Automation Research, University of Maryland, </institution> <year> 1991. </year>
Reference: [9] <author> T. M. Sobh and R. </author> <title> Bajcsy. Autonomous ob servation under uncertainty. </title> <booktitle> In Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 1792-1798, </pages> <month> May </month> <year> 1992. </year>
Reference: [10] <author> K. Suzumori, S. Iikura, and H. Tanaka. </author> <title> De velopment of a flexible microactuator and its application to robotic mechanisms. </title> <booktitle> In IEEE International Conference of Robotics and Automation, </booktitle> <pages> pages 1622-1627, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: C Experimental Procedure Our experimental system is shown in figure 2. A static monocular camera system is used to monitor and control a multi-fingered robotic gripper. The gripper for this experiment is the FMA (Flexible Micro Actuator) gripper <ref> [10] </ref>, provided by Toshiba Corporation and pictured in figure 3). Each finger of the FMA gripper is controlled by three servo valves which either inflate or vent air to three chambers in the finger.
Reference: [11] <author> D. Williams and M. Shah. </author> <title> A fast algorithm for active contours and curvature estimation. </title> <booktitle> In CVGIP: Image Understanding, </booktitle> <pages> pages 14-26, </pages> <year> 1992. </year>
Reference-contexts: Several researchers have investigated the use of snakes for tracking contours. They have been used in many different image-processing applications ranging from tracking of medical imagery to facial expression modeling. The work most directly related to ours involves the real-time computation of snakes and includes Williams and Shah <ref> [11] </ref>, Cipolla and Blake [3], Curwen and Blake [4] and Hollinghurst and Cipolla [5].
References-found: 11

