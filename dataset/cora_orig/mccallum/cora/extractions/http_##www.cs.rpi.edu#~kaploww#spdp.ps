URL: http://www.cs.rpi.edu/~kaploww/spdp.ps
Refering-URL: http://www.cs.rpi.edu/~kaploww/research.html
Root-URL: http://www.cs.rpi.edu
Email: fkaploww,szymanskg@cs.rpi.edu  
Title: Tiling for Parallel Execution Optimizing Node Cache Performance  
Author: Wesley K. Kaplow and Boleslaw K. Szymanski 
Keyword: Cache, Performance Estimation, Loop Optimization, Tiling.  
Address: Troy, N.Y. 12180-3590, USA  
Affiliation: Department of Computer Science Rensselaer Polytechnic Institute,  
Note: Parallel Processing Letters c World Scientific Publishing Company  Received (received date) Revised (revised date) Communicated by (Name of Editor)  
Abstract: Tiling has been used by parallelizing compilers to define fine-grain parallel tasks and to optimize cache performance. In this paper we present a novel compile-time technique, called miss-driven cache simulation, for determining tile size that achieves the highest cache hit-rate. The widening disparity between the processor's peak instruction rate and the main memory access time in modern processor makes this kind of optimization increasingly important for overall program efficiency. Our technique identifies potential cache misses through compile-time analysis of a loop nest and then processes them on an architecturally accurate cache model. Processing only a small portion of the memory reference trace of a program yields simulation speeds in the millions memory references per second on workstations, with statistics of misses per reference and inter-reference interference counts gathered at the same time. We discuss the results of applying this method to guide loop tiling for such commonly used computational kernels as matrix multiplication and Jacobi iteration for various cache parameters. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> W. Abu-Safah, D. J. Kuck, and D. H. Lawrie. </author> <title> Automatic program transformations for virtual memory computers. </title> <booktitle> In Proceedings of the 1979 National Computer Conference, </booktitle> <pages> pages 969-974, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: Originally explored as a technique to improve the virtual memory performance <ref> [1] </ref> of uniprocessors, the technique has also been applied to explore fine-grained parallelism exposed by loop skewing and wavefront transformations [11,12] for parallel machines. Designers of modern multi-processor machines have focused on architectures with the high-speed interconnection of moderate number of fast uniprocessors. <p> Figure 2 shows an example of the input program. 1 A.range <ref> [1] </ref> = A.range [2] = B.range [1] = B.range [2] = 2048 2 A.base = 10 3 B.base = 200000 4 for k = 1, 1024 6 A [k,i]=B [k+1,i]+B [i-1,j-1]+B [i+1,j]+B [i,j+1] 7 for r = 1, 256; s = 1, 256 9 end Fig. 2: Sample Simulation Source File <p> Figure 2 shows an example of the input program. 1 A.range <ref> [1] </ref> = A.range [2] = B.range [1] = B.range [2] = 2048 2 A.base = 10 3 B.base = 200000 4 for k = 1, 1024 6 A [k,i]=B [k+1,i]+B [i-1,j-1]+B [i+1,j]+B [i,j+1] 7 for r = 1, 256; s = 1, 256 9 end Fig. 2: Sample Simulation Source File The simulation source language is similar
Reference: 2. <author> S. Carr, K. McKinley, and C W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In ACM Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> San Jose, CA, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Figure 2 shows an example of the input program. 1 A.range [1] = A.range <ref> [2] </ref> = B.range [1] = B.range [2] = 2048 2 A.base = 10 3 B.base = 200000 4 for k = 1, 1024 6 A [k,i]=B [k+1,i]+B [i-1,j-1]+B [i+1,j]+B [i,j+1] 7 for r = 1, 256; s = 1, 256 9 end Fig. 2: Sample Simulation Source File The simulation source <p> Figure 2 shows an example of the input program. 1 A.range [1] = A.range <ref> [2] </ref> = B.range [1] = B.range [2] = 2048 2 A.base = 10 3 B.base = 200000 4 for k = 1, 1024 6 A [k,i]=B [k+1,i]+B [i-1,j-1]+B [i+1,j]+B [i,j+1] 7 for r = 1, 256; s = 1, 256 9 end Fig. 2: Sample Simulation Source File The simulation source language is similar to most imperative
Reference: 3. <author> T. Fahringer. </author> <title> Automatic Cache Performance Prediction in a Parallelizing Compiler. </title> <booktitle> In Proceeding of AICA 1993, </booktitle> <address> Lecce/Italy, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: In many parallel programs, an estimate of the total number of cache lines accessed by the program is essential for predicting the run-time performance <ref> [3] </ref>. Consequently, compile-time optimizations that improve memory reference locality are relevant to parallelizing compilers. In scientific programs the various loop nests operating on multi-dimensional ar-rays are the prime candidates for improvement via compilation optimization. The goal of these optimizations is to change loop nest characteristics to improve memory reference locality.
Reference: 4. <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for Cache and Local Memory Management by Global Program Transformation . Journal of Parallel and Distributed Computing, </title> <month> October </month> <year> 1988. </year>
Reference: 5. <author> A. Goldberg and J. Hennessy. </author> <title> Performance debugging shared-memory multiprocessor programs with mtool. </title> <booktitle> In Processings of Supercomputing 91, </booktitle> <year> 1991. </year>
Reference: 6. <author> A. Gupta, M. Martonosi, and T. Anderson. Memspy: </author> <title> Analyzing memory system bottlencks in programs. </title> <journal> Performance Analysis Review, </journal> <volume> 20(1), </volume> <year> 1992. </year>
Reference: 7. <author> W. K. Kaplow and B. K. Szymanski. </author> <title> Program optimization based on compile-time cache performance prediction. </title> <journal> Parallel Processing Letters, </journal> <volume> 6(1) </volume> <pages> 173-184, </pages> <year> 1996. </year>
Reference-contexts: The symbolic methods includes mainly analytic approaches, with the recent addition of compile-time simulation <ref> [7] </ref>. The execution-driven methods measure the run-time of a compiled program to determine the effect of optimization choices, e.g., tile size selection. <p> Their accuracy is limited by the difficulty in accounting for such cache attributes as the line-replacement algorithm, set-associativity, and virtual to physical address mapping. Moreover, these methods cannot accurately link cache miss counts and types with source program components. In <ref> [7] </ref>, we introduced a novel method called compile-time cache performance analysis. It uses the parse tree of a program to generate a trace of memory accesses that the compiled program would generate if executed on a target. This trace is then fed to an architecturally accurate cache model. <p> The contribution of this paper is a novel miss-driven cache simulation model in which events are potential cache misses. This model leads to a significantly faster simulation than the ones which processes all array accesses <ref> [7] </ref>. A cache miss causes an entire line to be read into the cache, so while processing it the simulator can predict what is the next set of indices for the same data structure reference that will access the data beyond the cache line just loaded. <p> The parser of a source language can be extended to produce an expression that can be used to generate the symbolic trace (see <ref> [7] </ref>). The miss-driven cache simulation method processes only the reference identifiers representing potential cache misses. Each time a reference identifier is processed, the simulation determines its effect on the state of the cache model and determines the next candidate miss event.
Reference: 8. <author> M. S. Lam, E. E. Rothberg, and M. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proc. ACM ASPLOS, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 63-74. </pages> <publisher> ACM, </publisher> <address> NY, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Tiling is such a method applicable to loop nests. It requires that the loops are restructured to create the blocked iteration scheme with carefully selected tile sizes. In <ref> [8] </ref> the authors show that changing tile size can significantly affect the program performance and that the optimum tile size is dependent on both the cache organization and program characteristics. <p> The left graph in Figure 11 presents the cache performance for various tile sizes for two different matrix multiply problem sizes. As shown, for the same cache architecture and algorithm the tile size can depend on small changes in problem size (as shown in <ref> [8] </ref>). The right graph in Figure 11 gives miss-rates for matrix-multiply on a 32K cache for three different cache organizations.
Reference: 9. <author> Kathryn S. McKinley, Steve Carr, and Chau-Wen Tseng. </author> <title> Improving data locality with loop transformations. </title> <journal> ACM Transactions on Programming Lanaguages and Systems, </journal> <volume> 18(4) </volume> <pages> 424-453, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: Each symbolic reference is assigned the number of times this references cause a miss, as well as the number of times this reference displaced others in the cache. This information can be used to direct other loop optimizations such as loop permutation, fusion, and distribution <ref> [9] </ref>. Moreover, per reference based information is critical to whole program cache optimization [10]. Acknowledgments The authors would like to thank their colleagues from Rensselaer Polytechnic Institute: Peter Tannenbaum for his help in implementing the parser and Charles Norton for help in the C++ implementation of the simulator.
Reference: 10. <author> Kathryn S. McKinley and Oliver Temam. </author> <title> A quantatative analysis of loop nest locality. </title> <booktitle> In ASPLOS-VII. ACM, </booktitle> <year> 1996. </year>
Reference-contexts: This information can be used to direct other loop optimizations such as loop permutation, fusion, and distribution [9]. Moreover, per reference based information is critical to whole program cache optimization <ref> [10] </ref>. Acknowledgments The authors would like to thank their colleagues from Rensselaer Polytechnic Institute: Peter Tannenbaum for his help in implementing the parser and Charles Norton for help in the C++ implementation of the simulator.
Reference: 11. <author> Balram Sinharoy and Boleslaw Szymanski. </author> <title> Finding optimum wavefront of parallel computation. </title> <journal> Journal of Parallel Algorithms and Applications, </journal> <volume> 2(1) </volume> <pages> 5-26, </pages> <year> 1994. </year>
Reference: 12. <author> M. E. Wolf and M. S. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> 3(10) </volume> <pages> 452-471, </pages> <year> 1991. </year>
References-found: 12

