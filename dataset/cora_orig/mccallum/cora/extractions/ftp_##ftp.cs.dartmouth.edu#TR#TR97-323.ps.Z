URL: ftp://ftp.cs.dartmouth.edu/TR/TR97-323.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR97-323/
Root-URL: http://www.cs.dartmouth.edu
Title: ViC*: A Compiler for Virtual-Memory C*  
Author: Alex Colvin Thomas H. Cormen 
Affiliation: Dartmouth College Department of Computer Science  
Date: November 1997  
Note: PCS-TR97-323  
Abstract: Dartmouth College Computer Science Technical Report Abstract This paper describes the functionality of ViC*, a compiler for a variant of the data-parallel language C* with support for out-of-core data. The compiler translates C* programs with shapes declared outofcore, which describe parallel data stored on disk. The compiler output is a SPMD-style program in standard C with I/O and library calls added to efficiently access out-of-core parallel data. The ViC* compiler also applies several program transformations to improve out-of-core data layout and access.
Abstract-found: 1
Intro-found: 1
Reference: [AP94] <author> Alok Aggarwal and C. Greg Plaxton. </author> <title> Optimal parallel sorting in multi-level storage. </title> <booktitle> In Proceedings of the 5th Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 659-668, </pages> <month> January </month> <year> 1994. </year>
Reference: [Arg95] <author> Lars Arge. </author> <title> The buffer tree: A new technique for optimal I/O-algorithms. </title> <booktitle> In 4th International Workshop on Algorithms and Data Structures (WADS), </booktitle> <pages> pages 334-345, </pages> <month> August </month> <year> 1995. </year>
Reference: [AVV95] <author> Lars Arge, Darren Erik Vengroff, and Jeffrey Scott Vitter. </author> <title> External-memory algorithms for processing line segments in geographic information systems. </title> <editor> In Paul Spirakis, editor, </editor> <booktitle> Proceedings of the Third Annual European Symposium on Algorithms (ESA '95), volume 979 of Lecture Notes in Computer Science, </booktitle> <pages> pages 295-310. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1995. </year>
Reference: [BCT92] <author> Preston Briggs, Keith D. Cooper, and Linda Torczon. </author> <title> Rematerialization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 311-321, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Variable where_1 is traversed twice. Table 2 shows the resulting page I/O counts. 9 loop k where 1 harm total 1 0 1024K 128K 1024K 2176K program 1024K 256K 2048K 3328K Table 2: Page I/O counts for the program schema in Figure 3 after loop fusion. Rematerialization Rematerialization <ref> [BCT92] </ref> is a transformation that recomputes a variable instead of using its stored value|the inverse of common subexpression elimination. In other compilers, rematerialization typically reduces the number of memory accesses. ViC* uses rematerial-ization to reduce the number of out-of-core reads at the cost of additional computation.
Reference: [BGV97] <author> Rakesh D. Barve, Edward F. Grove, and Jeffrey Scott Vitter. </author> <title> Simple randomized mergesort on parallel disks. </title> <booktitle> Parallel Computing, </booktitle> <address> 23(4-5):601-631, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: How good would a demand paging system with a restructuring compiler, parallel disk system, and prefetching/post-writing be? If all computations made sequential passes over the data, it would be quite good. However, some asymptotically optimal out-of-core algorithms for the Parallel Disk Model (e.g., those for sorting <ref> [BGV97, NV93, NV95, VS94] </ref>, structured permutations [CSW94, Wis96], and FFTs [Cor97, CN96, CWN97]) do not access out-of-core data in a simple, sequential fashion. They read and write whole disk blocks, but the blocks may be scattered throughout the parallel disk system.
Reference: [CC94] <author> Thomas H. Cormen and Alex Colvin. </author> <title> ViC*: A preprocessor for virtual-memory C*. </title> <type> Technical Report PCS-TR94-243, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Throughout the history of electronic computing, no matter how big and fast the top fl Author's email address: Alex.Colvin@dartmouth.edu. y Supported by the National Science Foundation under Grant CCR-9625894. Author's email address: thc@cs.dartmouth.edu. This paper is a major update of <ref> [CC94] </ref>. 1 machines have been, there have always been applications that needed them to be bigger and faster, and it remains true today. Over thirty years ago, computer architects devised virtual memory to solve this problem for sequential machines [Den70].
Reference: [CGG + 95] <author> Yi-Jen Chiang, Michael T. Goodrich, Edward F. Grove, Roberto Tamassia, Darren Erik Vengroff, and Jeffrey Scott Vitter. </author> <title> External-memory graph algorithms. </title> <booktitle> In Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 139-149, </pages> <month> January </month> <year> 1995. </year>
Reference: [CH97] <author> Thomas H. Cormen and Melissa Hirschl. </author> <title> Early experiences in evaluating the Parallel Disk Model with the ViC* implementation. </title> <booktitle> Parallel Computing, </booktitle> <address> 23(4-5):571-600, </address> <month> June </month> <year> 1997. </year> <month> 19 </month>
Reference-contexts: Even with the highly unusual configuration of a parallel disk system for swap space, only the transfer rate improves. I/O latency for demand paging does not improve. I/O latency can be hidden in many out-of-core computations by prefetching and post-writing (see <ref> [CH97] </ref> for an example). How good would a demand paging system with a restructuring compiler, parallel disk system, and prefetching/post-writing be? If all computations made sequential passes over the data, it would be quite good.
Reference: [CN96] <author> Thomas H. Cormen and David M. Nicol. </author> <title> Performing out-of-core FFTs on par-allel disk systems. </title> <type> Technical Report PCS-TR96-294, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> August </month> <year> 1996. </year> <note> To appear in Parallel Computing. </note>
Reference-contexts: However, some asymptotically optimal out-of-core algorithms for the Parallel Disk Model (e.g., those for sorting [BGV97, NV93, NV95, VS94], structured permutations [CSW94, Wis96], and FFTs <ref> [Cor97, CN96, CWN97] </ref>) do not access out-of-core data in a simple, sequential fashion. They read and write whole disk blocks, but the blocks may be scattered throughout the parallel disk system. These algorithms require the ability to independently access individual disks. <p> These algorithms take advantage of an independent I/O interface to a parallel disk system. Such algorithms offer large speedups over conventional in-core algorithms under demand paging. In one case, an explicit out-of-core FFT algorithm was over 144 times faster than a demand-paged version of the traditional in-core Cooley-Tukey method <ref> [CN96] </ref>. 3 Background concepts and overview of ViC* This section introduces the parallel programming model and the language features of C* and ViC* that implement it. More information about the C* language appears in [TMC93].
Reference: [Cor93] <author> Thomas H. Cormen. </author> <title> Fast permuting in disk arrays. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 17(12) </volume> <pages> 41-57, </pages> <month> January and February </month> <year> 1993. </year>
Reference: [Cor97] <author> Thomas H. Cormen. </author> <title> Determining an out-of-core FFT decomposition strategy for parallel disks by dynamic programming. </title> <type> Technical Report PCS-TR97-322, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1997. </year> <note> To appear in [IMA96]. </note>
Reference-contexts: However, some asymptotically optimal out-of-core algorithms for the Parallel Disk Model (e.g., those for sorting [BGV97, NV93, NV95, VS94], structured permutations [CSW94, Wis96], and FFTs <ref> [Cor97, CN96, CWN97] </ref>) do not access out-of-core data in a simple, sequential fashion. They read and write whole disk blocks, but the blocks may be scattered throughout the parallel disk system. These algorithms require the ability to independently access individual disks.
Reference: [CSW94] <author> Thomas H. Cormen, Thomas Sundquist, and Leonard F. Wisniewski. </author> <title> Asymptotically tight bounds for performing BMMC permutations on parallel disk systems. </title> <type> Technical Report PCSTR94-223, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> July </month> <year> 1994. </year> <note> Preliminary version appeared in Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures. Revised version to appear in SIAM Journal on Computing. </note>
Reference-contexts: However, some asymptotically optimal out-of-core algorithms for the Parallel Disk Model (e.g., those for sorting [BGV97, NV93, NV95, VS94], structured permutations <ref> [CSW94, Wis96] </ref>, and FFTs [Cor97, CN96, CWN97]) do not access out-of-core data in a simple, sequential fashion. They read and write whole disk blocks, but the blocks may be scattered throughout the parallel disk system. These algorithms require the ability to independently access individual disks.
Reference: [CWN97] <author> Thomas H. Cormen, Jake Wegmann, and David M. Nicol. </author> <title> Multiprocessor out-of-core FFTs with distributed memory and parallel disks. </title> <booktitle> In Proceedings of the Fifth Workshop on I/O in Parallel and Distributed Systems (IOPADS '97), </booktitle> <pages> pages 68-78, </pages> <month> November </month> <year> 1997. </year> <institution> Also Dartmouth College Computer Science Technical Report PCS-TR97-303. </institution>
Reference-contexts: However, some asymptotically optimal out-of-core algorithms for the Parallel Disk Model (e.g., those for sorting [BGV97, NV93, NV95, VS94], structured permutations [CSW94, Wis96], and FFTs <ref> [Cor97, CN96, CWN97] </ref>) do not access out-of-core data in a simple, sequential fashion. They read and write whole disk blocks, but the blocks may be scattered throughout the parallel disk system. These algorithms require the ability to independently access individual disks.
Reference: [dDEF + 97] <author> B. D. de Dinechin, G. Elsesser, G. Fischer, B.H. Johnson, T. MacDonald, R. W. Numrich, and Jon L. Steidel. </author> <title> Definition of the F extension to Fortran 90. </title> <booktitle> In Proceedings of the 10th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: C* also faces issues of pointer aliasing not permitted in HPF. Many of the issues raised in ViC* implementation are not particular to C*, but are common to compiled data-parallel languages. For example, the language F <ref> [dDEF + 97] </ref> bears many similarities to C*.
Reference: [Den70] <author> Peter J. Denning. </author> <title> Virtual memory. </title> <journal> ACM Computing Surveys, </journal> <volume> 2(3) </volume> <pages> 153-189, </pages> <month> September </month> <year> 1970. </year>
Reference-contexts: This paper is a major update of [CC94]. 1 machines have been, there have always been applications that needed them to be bigger and faster, and it remains true today. Over thirty years ago, computer architects devised virtual memory to solve this problem for sequential machines <ref> [Den70] </ref>. Today's parallel machines typically run traditional sequential virtual memory on the individual nodes. <p> Section 4 discusses program transformations to improve access to out-of-core data, and Section 5 describes parallel data layout. Section 6 describes the runtime interface used to access out-of-core data. Section 7 presents performance measurements. We conclude in Section 8. 2 Virtual Memory As described by Denning <ref> [Den70] </ref> in 1970, virtual memory presents the programmer "the illusion that he has a very large main memory at his disposal, even though the computer actually has a relatively small main memory." Demand paging is a common implementation of virtual memory, but, as we are about to see, for large data
Reference: [GTVV93] <author> Michael T. Goodrich, Jyh-Jong Tsay, Darren E. Vengroff, and Jeffrey Scott Vit-ter. </author> <title> External-memory computational geometry. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 714-723, </pages> <month> Novem-ber </month> <year> 1993. </year>
Reference: [IMA96] <institution> Proceedings of the Workshop on Algorithms for Parallel Machines, 1996-97 Special Year on Mathematics of High Performance Computing, Institute for Mathematics and Its Applications, University of Minnesota, Minneapolis, </institution> <month> September </month> <year> 1996. </year>
Reference: [LH92] <author> Anthony J. Lapadula and Kathleen P. Herold. </author> <title> A retargetable C* compiler and run-time library for mesh-connected MIMD multicomputers. </title> <type> Technical 20 Report TR 92-15, </type> <institution> University of New Hampshire, </institution> <year> 1992. </year> <note> Revised by Phil Hatcher, </note> <month> October </month> <year> 1993. </year>
Reference-contexts: Finally, although many people think of C* solely as a bygone product of Thinking Machines Corporation, there is an active project under the direction of Phil Hatcher at the University of New Hampshire that has produced a C* compiler and runtime system for a distributed-memory model (see <ref> [LH92] </ref> and http://www.cs.unh.edu/pjh/cstar/cstar.html). The remainder of this paper is organized as follows. Section 2 describes virtual memory and its implementation on parallel disk systems. Section 3 presents a brief overview of the C* language and the ViC* extensions. <p> Loop fusion reduces loop overhead and, more significantly for out-of-core data, improves data locality when the loops access the same data. which every parallel operation becomes a loop. A more sophisticated C* compiler <ref> [LH92] </ref> would fuse these loops, as illustrated in Figure 3. The first three loops of Figure 2 have been fused into a single PASS. The remaining loop cannot be fused because of data dependencies: the sum depends on any modifications to elements of harm in the call to filter ().
Reference: [NV93] <author> Mark H. Nodine and Jeffrey Scott Vitter. </author> <title> Deterministic distribution sort in shared and distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 120-129, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: How good would a demand paging system with a restructuring compiler, parallel disk system, and prefetching/post-writing be? If all computations made sequential passes over the data, it would be quite good. However, some asymptotically optimal out-of-core algorithms for the Parallel Disk Model (e.g., those for sorting <ref> [BGV97, NV93, NV95, VS94] </ref>, structured permutations [CSW94, Wis96], and FFTs [Cor97, CN96, CWN97]) do not access out-of-core data in a simple, sequential fashion. They read and write whole disk blocks, but the blocks may be scattered throughout the parallel disk system.
Reference: [NV95] <author> Mark H. Nodine and Jeffrey Scott Vitter. </author> <title> Greed sort: Optimal deterministic sorting on parallel disks. </title> <journal> Journal of the ACM, </journal> <volume> 42(4) </volume> <pages> 919-933, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: How good would a demand paging system with a restructuring compiler, parallel disk system, and prefetching/post-writing be? If all computations made sequential passes over the data, it would be quite good. However, some asymptotically optimal out-of-core algorithms for the Parallel Disk Model (e.g., those for sorting <ref> [BGV97, NV93, NV95, VS94] </ref>, structured permutations [CSW94, Wis96], and FFTs [Cor97, CN96, CWN97]) do not access out-of-core data in a simple, sequential fashion. They read and write whole disk blocks, but the blocks may be scattered throughout the parallel disk system.
Reference: [TMC93] <institution> Thinking Machines Corporation. </institution> <note> C* Programming Guide, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Programmers do not need specialized knowledge of PDM algorithms in order to avoid huge performance penalties. To be more specific, the ViC* system is based on using a data-parallel language, in particular C* <ref> [TMC93] </ref>. The ViC* (Virtual-memory C*) compiler transforms a C* program with parallel variables so large that they must reside on disk into a C program with I/O and library calls to access out-of-core data on a parallel disk system. <p> More information about the C* language appears in <ref> [TMC93] </ref>. C*, and hence ViC*, supports data-parallel programming, in which a sequential program operates on parallel data distributed among a set of positions. A virtual processor operates on parallel data at each position. The underlying computer multiplexes a set of physical processors among the virtual processors.
Reference: [VS94] <author> Jeffrey Scott Vitter and Elizabeth A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <journal> Algorithmica, </journal> 12(2/3):110-147, August and Septem-ber 1994. 
Reference-contexts: This approach is the focus of this paper. The other way to reduce the number of disk accesses is to design algorithms that explicitly work with out-of-core data on parallel disks. Since the introduction of the Parallel Disk Model (PDM) by Vitter and Shriver in 1990 <ref> [VS94] </ref>, there have been significant technical advances on how to carefully plan parallel disk accesses for common data-parallel operations and algorithms [AP94, Arg95, AVV95, BGV97, CGG + 95, Cor93, Cor97, CN96, CWN97, CSW94, GTVV93, NV93, NV95, Wis96, WGWR93, VS94]. <p> How good would a demand paging system with a restructuring compiler, parallel disk system, and prefetching/post-writing be? If all computations made sequential passes over the data, it would be quite good. However, some asymptotically optimal out-of-core algorithms for the Parallel Disk Model (e.g., those for sorting <ref> [BGV97, NV93, NV95, VS94] </ref>, structured permutations [CSW94, Wis96], and FFTs [Cor97, CN96, CWN97]) do not access out-of-core data in a simple, sequential fashion. They read and write whole disk blocks, but the blocks may be scattered throughout the parallel disk system.
Reference: [WGWR93] <author> David Womble, David Greenberg, Stephen Wheat, and Rolf Riesen. </author> <title> Beyond core: Making parallel computer I/O practical. </title> <booktitle> In DAGS '93, </booktitle> <month> June </month> <year> 1993. </year>
Reference: [Wis96] <author> Leonard F. Wisniewski. </author> <title> Efficient Design and Implementation of Permutation Algorithms on the Memory Hierarchy. </title> <type> PhD thesis, </type> <institution> Dartmouth College Department of Computer Science, </institution> <month> March </month> <year> 1996. </year> <month> 21 </month>
Reference-contexts: However, some asymptotically optimal out-of-core algorithms for the Parallel Disk Model (e.g., those for sorting [BGV97, NV93, NV95, VS94], structured permutations <ref> [CSW94, Wis96] </ref>, and FFTs [Cor97, CN96, CWN97]) do not access out-of-core data in a simple, sequential fashion. They read and write whole disk blocks, but the blocks may be scattered throughout the parallel disk system. These algorithms require the ability to independently access individual disks.
References-found: 24

