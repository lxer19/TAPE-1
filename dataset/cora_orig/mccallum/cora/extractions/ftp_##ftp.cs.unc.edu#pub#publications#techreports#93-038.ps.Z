URL: ftp://ftp.cs.unc.edu/pub/publications/techreports/93-038.ps.Z
Refering-URL: http://www.cs.unc.edu/Research/graphics/pubs.html
Root-URL: http://www.cs.unc.edu
Keyword: 1.2. Redistribution 1.3. Mesh Networks  
Abstract: into n regions and assigns each node a separate region to render. This parallel algorithm does not specify what rendering method is used by each node to render its region. By considering parallel algorithms and rendering methods independently, the performance ramifications of each issue are separately more clearly. Communication costs are an important issue for parallel system and software designers to consider. The selection of a parallel algorithm has a major impact on the communication requirement between nodes. Unless all nodes have a local copy of the data, or viewing positions are severely restricted, a parallel volume rendering algorithm intrinsically requires communication between compute nodes. The transfer between nodes of volume or image data necessitated by a parallel algorithm is defined here as redistribution. Redistribution costs are measured as the quantity of data transferred (redistribution size) and the time consumed by moving it over the network (redistribution time). Replication of data at each node is wasteful for large numbers of nodes and impossible when data size exceeds local memory size. Restricting the viewing positions limits one's ability to explore the data. Therefore, in most practical cases, redistribution must occur. The upper bound of redistribution size is independent of the rendering method. The choice of rendering method may reduce the actual requirement. For example, nodes that render by ray casting may adaptively terminate rays and therefore not access portions of the data that would otherwise be needed. Such efficiencies are data dependent but often significant. In this analysis, the peak communication requirement is derived as an upper bound with the understanding that rendering efficiencies may reduce this by some factor. Communication between nodes in multicomputers is frequently through two and three-dimensional mesh-connected networks. (E.g.: Stanford Dash, Intel Delta and Paragon, MIT J-Machine, Caltech Mosaic.) The performance of these communication networks with parallel volume rendering algorithms is one focus of this paper. Mesh networks scale easily so they are a practical choice for Abstract This paper examines the many ways to structure parallel volume rendering algorithms and analyzes the communication costs associated with them. Parallel volume rendering algorithms are enumerated through a taxonomy which sorts them into two main classes that exhibit similar communication costs: image and object partitions. The intrinsic communication costs for algorithms in these classes are analyzed independent of an implementation. Given a network model for a target system, an algorithm's intrinsic communication cost can be used to estimate the time consumed by communication and the effect upon communication time as the system size and data size are varied. Communication cost and time are measured on the Intel Touchstone Delta to verify the predicted scaling behavior. The results show that, for a fixed screen size, systems with mesh networks scale well for object partition algorithms - the time required for communication decreases as the data and system sizes increase. 
Abstract-found: 1
Intro-found: 1
Reference: [Cama93] <author> Emilio Camahort and Indranil Chakravarty. </author> <title> "Integrating Volume Rendering on Distributed Memory Architectures." </title> <booktitle> 1993 Parallel Rendering Symposium, </booktitle> <pages> 89-96, </pages> <month> October </month> <year> 1993, </year> <booktitle> ACM Proceedings. </booktitle>
Reference-contexts: Object Partition In object partition algorithms (Fig. 3) nodes compute images of their local data subset and redistribute the local images among themselves to combine them into a final image [Hsu93] [Ma93] <ref> [Cama93] </ref> [Chal91] [Yoo91]. The view point and aspect ratio of the data subsets affect the redistribution size. Slabs, shafts, and blocks vary from highly unbalanced aspect ratios to perfectly balanced ratios.
Reference: [Chal91] <author> Judy Challinger. </author> <title> "Parallel Volume Rendering on a Shared-Memory Multiprocessor." </title> <institution> Computer and Information Sciences, UC Santa Cruz, Tech Report CRL-91-23, </institution> <month> Revised March </month> <year> 1992. </year>
Reference-contexts: Image Partition with Static Data Distribution In this class of algorithms, nodes are assigned one or more subsets of image lattice points to compute. Often shafts subsets are used which equates to assigning screen regions to nodes <ref> [Chal91] </ref> [Corr92] [Mont92] [Nieh92] [Vzi92] [Yoo91]. Data subsets are distributed among the nodes in a static distribution a specified data point is always stored in the same node's local memory. <p> Object Partition In object partition algorithms (Fig. 3) nodes compute images of their local data subset and redistribute the local images among themselves to combine them into a final image [Hsu93] [Ma93] [Cama93] <ref> [Chal91] </ref> [Yoo91]. The view point and aspect ratio of the data subsets affect the redistribution size. Slabs, shafts, and blocks vary from highly unbalanced aspect ratios to perfectly balanced ratios.
Reference: [Corr92] <author> Brian Corrie and Paul Mackerras. </author> <title> "Parallel Volume Rendering and Data Coherence on the Fujitsu AP1000." </title> <institution> Department of Computer Science, The Australian National University, Tech Report TR-CS-92-11, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: Image Partition with Static Data Distribution In this class of algorithms, nodes are assigned one or more subsets of image lattice points to compute. Often shafts subsets are used which equates to assigning screen regions to nodes [Chal91] <ref> [Corr92] </ref> [Mont92] [Nieh92] [Vzi92] [Yoo91]. Data subsets are distributed among the nodes in a static distribution a specified data point is always stored in the same node's local memory. To render their region (s), nodes access remote or local data as necessary (Fig. 2) based on the current view transformation. <p> Alternatively, a faster (more expensive) network must be provided as the system size is increased. Section 1 described how rendering efficiencies can reduce redistribution size. It may also be lowered by using large caches to take advantage of image and temporal coherence <ref> [Corr92] </ref>. Cached values from the previous frame are likely to be needed for the current frame. With caches the redistribution costs in Eqs. 5 - 7 are modified by setting r = nh, where h is the average hit ratio of the caches. 4.2.
Reference: [Hsu93] <author> William M. Hsu. </author> <title> "Segmented Ray Casting for Data Parallel Volume Rendering." </title> <booktitle> 1993 Parallel Rendering Symposium, </booktitle> <pages> 7-14, </pages> <month> October </month> <year> 1993, </year> <booktitle> ACM Proceedings. </booktitle>
Reference-contexts: Object Partition In object partition algorithms (Fig. 3) nodes compute images of their local data subset and redistribute the local images among themselves to combine them into a final image <ref> [Hsu93] </ref> [Ma93] [Cama93] [Chal91] [Yoo91]. The view point and aspect ratio of the data subsets affect the redistribution size. Slabs, shafts, and blocks vary from highly unbalanced aspect ratios to perfectly balanced ratios.
Reference: [Ma93] <author> Kwan-Liu Ma, James S. Painter, Charles D. Hansen, Micheal F. Krogh. </author> <title> "A Data Distributed Algorithm for Ray-Traced Volume Rendering." </title> <booktitle> 1993 Parallel Rendering Symposium, </booktitle> <pages> 15-22, </pages> <month> October </month> <year> 1993, </year> <booktitle> ACM Proceedings. </booktitle>
Reference-contexts: Object Partition In object partition algorithms (Fig. 3) nodes compute images of their local data subset and redistribute the local images among themselves to combine them into a final image [Hsu93] <ref> [Ma93] </ref> [Cama93] [Chal91] [Yoo91]. The view point and aspect ratio of the data subsets affect the redistribution size. Slabs, shafts, and blocks vary from highly unbalanced aspect ratios to perfectly balanced ratios. <p> Scaling Performance scaling for this and other object partition implementations <ref> [Ma93] </ref> is lower than one might expect when nodes are added for a constant data size. This is partially due to the loss of rendering efficiency obtained from adaptive ray termination.
Reference: [Mont92] <author> C. Montani, R. Perego, and R. Scopigno. </author> <title> "Parallel Volume Visualization on a Hypercube Architecture." </title> <booktitle> 1992 Workshop on Volume Visualization, </booktitle> <pages> 9-16, </pages> <month> October </month> <year> 1992. </year> <booktitle> Workshop Proceedings. </booktitle>
Reference-contexts: Image Partition with Static Data Distribution In this class of algorithms, nodes are assigned one or more subsets of image lattice points to compute. Often shafts subsets are used which equates to assigning screen regions to nodes [Chal91] [Corr92] <ref> [Mont92] </ref> [Nieh92] [Vzi92] [Yoo91]. Data subsets are distributed among the nodes in a static distribution a specified data point is always stored in the same node's local memory. To render their region (s), nodes access remote or local data as necessary (Fig. 2) based on the current view transformation.
Reference: [Neum93] <author> Ulrich Neumann. </author> <title> "Volume Reconstruction and Parallel Rendering Algorithms: A Comparative Analysis." </title> <institution> Department of Computer Science, UNC at Chapel Hill, Tech Report TR93-017, </institution> <month> May </month> <year> 1993. </year> <type> Ph.D. Dissertation. </type>
Reference-contexts: Redistribution occurs as local images are moved to facilitate their combining into a complete image. Member algorithms in each class differ in the shapes of the data and image subsets, the subset's static or dynamic nature over time, and the spatial relationship of the subsets to each other <ref> [Neum93] </ref>. A taxonomy (Fig. 4) enumerates the possible algorithms graphically. Note that the choice of image or object order rendering methods is also a variable. 2.1. Lattice Subsets Subsets of the object or image lattices may be distributed among nodes in three shapes: slabs, shafts, and blocks (Fig. 5). <p> In all cases the screen size is 256 2 pixels. The data is analytically generated from Gaussian point and line sources sampled at different densities. The reader is referred to <ref> [Neum93] </ref> for further details about the isosurface shading and load balancing used in this implementation. The Delta provides access to a frame buffer through an I/O node that feeds a HIPPI channel.
Reference: [Ngai89] <author> John Y. Ngai. </author> <title> "A Framework for Adaptive Routing in Multi computer Networks." </title> <institution> Department of Computer Science, California Institute of Technology, Tech Report CS-TR-89-09, </institution> <month> May </month> <year> 1989. </year> <type> Ph.D. Dissertation. </type>
Reference-contexts: A partially-routed blocked message occupies paths that may in turn block other messages. John Ngai <ref> [Ngai89] </ref> characterized these networks while proposing adaptive enhancements. Some of Ngai's test results for 2D and 3D mesh and torus topologies are reproduced in figure 6. <p> 100 140 180 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 3D Mesh , 512 Nodes 2D Mesh , 256 Nodes 3D Torus , 512 Nodes 2D Torus , 256 Nodes Normalized Throughput A e a e a e c Fig. 6 - Average latency vs. normalized throughput (adapted from <ref> [Ngai89] </ref>) Glossary of abbreviations: a dimension of a mesh network b bisection bandwidth c channel bandwidth for one link in a network d volume data size - number of samples h average cache hit ratio k edge length of a network m redistribution size - amount of data moved per frame
Reference: [Nieh92] <author> Jason Nieh and Marc Levoy. </author> <title> "Volume Rendering on Scalable Shared-Memory MIMD Architectures." </title> <booktitle> 1992 Workshop on Volume Visualization, </booktitle> <pages> 17-24, </pages> <month> October </month> <year> 1992. </year> <booktitle> Workshop Proceedings. </booktitle>
Reference-contexts: A fine-grain randomly interleaved block data distribution achieves this and makes the redistribution size view-independent <ref> [Nieh92] </ref>. This data distribution is the context for the remainder of section 4. 4.1.1 Redistribution Costs Redistribution size is affected by replication of the data s e t . <p> Image Partition with Static Data Distribution In this class of algorithms, nodes are assigned one or more subsets of image lattice points to compute. Often shafts subsets are used which equates to assigning screen regions to nodes [Chal91] [Corr92] [Mont92] <ref> [Nieh92] </ref> [Vzi92] [Yoo91]. Data subsets are distributed among the nodes in a static distribution a specified data point is always stored in the same node's local memory. To render their region (s), nodes access remote or local data as necessary (Fig. 2) based on the current view transformation.
Reference: [Shu91] <author> Renben Shu and Alan Liu. </author> <title> "A Fast Ray Casting Algorithm Using Isotriangular Subdivision." </title> <journal> IEEE Visualization'91, </journal> <pages> 232-237, </pages> <month> October </month> <year> 1991. </year> <booktitle> Conference Proceedings. </booktitle>
Reference: [Vzi92] <author> Guy Vzina, Peter A. Fletcher, and Philip K. Robertson. </author> <title> "Vol ume Rendering on the MasPar MP-1." </title> <booktitle> 1992 Workshop on Volume Visualization, </booktitle> <pages> 3-8, </pages> <month> October </month> <year> 1992. </year> <booktitle> Workshop Proceedings. </booktitle>
Reference-contexts: Image Partition with Static Data Distribution In this class of algorithms, nodes are assigned one or more subsets of image lattice points to compute. Often shafts subsets are used which equates to assigning screen regions to nodes [Chal91] [Corr92] [Mont92] [Nieh92] <ref> [Vzi92] </ref> [Yoo91]. Data subsets are distributed among the nodes in a static distribution a specified data point is always stored in the same node's local memory. To render their region (s), nodes access remote or local data as necessary (Fig. 2) based on the current view transformation.

References-found: 11

