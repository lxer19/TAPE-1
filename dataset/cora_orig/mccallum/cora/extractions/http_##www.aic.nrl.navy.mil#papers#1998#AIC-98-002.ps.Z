URL: http://www.aic.nrl.navy.mil/papers/1998/AIC-98-002.ps.Z
Refering-URL: http://www.aic.nrl.navy.mil/~aha/pub-details.html
Root-URL: 
Email: aha@aic.nrl.navy.mil  
Title: The Omnipresence of Case-Based Reasoning in Science and Application  
Author: David W. Aha 
Web: http://www.aic.nrl.navy.mil/~aha  
Address: Code 5510 Washington, DC 20375 USA  
Affiliation: Navy Center for Applied Research in Artificial Intelligence Naval Research Laboratory,  
Abstract: A surprisingly large number of research disciplines have contributed towards the development of knowledge on lazy problem solving, which is characterized by its storage of ground cases and its demand driven response to queries. Case-based reasoning (CBR) is an alternative, increasingly popular approach for designing expert systems that implements this approach. This paper lists pointers to some contributions in some related disciplines that offer insights for CBR research. We then outline a small number of Navy applications based on this approach that demonstrate its breadth of applicability. Finally, we list a few successful and failed attempts to apply CBR, and list some predictions on the future roles of CBR in applications.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aamodt, A., & Plaza, E. </author> <year> (1994). </year> <title> Case-based reasoning: Foundational issues, methodological variations, and system approaches. </title> <journal> AI Communications, </journal> <volume> 7, </volume> <pages> 39-59. </pages>
Reference-contexts: For example, consider its interpretation by the following three groups: * Cognitive Scientists: CBR is a plausible high-level model for cognitive processing (Kolodner, 1993a). * Artificial Intelligence Researchers: CBR is a computational paradigm for problem solving <ref> (Aamodt & Plaza, 1994) </ref>. * Expert System Practitioners: CBR is a design model for expert systems that can be used in either stand alone or embedded architectures (Watson, 1997). To introduce CBR requires identifying it in a particular context.
Reference: <author> Aha, D. W. </author> <year> (1992). </year> <title> Generalizing from case studies: A case study. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 1-10). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Disjunctive Solution Spaces: Lazy approaches are often most appropriate for tasks whose solution spaces are complex, making them less appropriate for approaches that replace data with abstractions <ref> (Aha, 1992) </ref>. 5. Precedent Explanations: By virtue of storing rather than discarding case data, lazy approaches can generate precedent explanations (i.e., based on the retrieved cases). <p> For example, research on natural language learning (e.g., Daelemans et al., 1997) often involves working with data sets whose concepts are highly disjunctive. In such circumstances, it is well known that lazy approaches have benefits vs. eager approaches <ref> (e.g., Aha, 1992) </ref>. This suggests further study of lazy realizations of eager approaches for natural language learning tasks. Another good application area concerns robotic control. See (Atkeson et al., 1997b) for a survey on selected lazy approaches for control tasks.
Reference: <author> Aha, D. W. (Ed.) </author> <title> Lazy Learning. </title> <publisher> Norwell, </publisher> <address> MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Aha, D. W., & Bankert, R. L. </author> <year> (1997). </year> <title> Cloud classification using error-correcting output codes. </title> <booktitle> Artificial Intelligence Applications: Natural Science, Agriculture, and Environmental Science, </booktitle> <volume> 11(1), </volume> <pages> 13-28. </pages>
Reference-contexts: However, perhaps the most compelling reason for using a lazy problem solving approach is that it is highly intuitive; experts often relate their problem solving behavior in ways that suggest a form of case-based reasoning. We discuss these and other benefits of lazy approaches throughout this section. See <ref> (Aha, 1997) </ref> for a collection of articles focussed on lazy learning. Lazy problem solving is a distinguishing characteristic of CBR approaches: they perform demand-driven reasoning from a stored library of cases. <p> This eliminated any hope of using an expensive eager approach. More recently, we examined the use of an alternative output representation for these tasks <ref> (Aha & Bankert, 1997) </ref>. In particular, we found that error-correcting output codes (ECOCs), a type of distributed representation, can further enhance accuracy, though at the cost of speed and storage. <p> In particular, some systems have several volumes dedicated to troubleshooting scenarios. These are difficult to master, particularly when short billets dictate that personnel have little time to master the machinery they work with. Conversational CBR approaches <ref> (Aha & Breslow, 1997) </ref> have been used to assist naval personnel with troubleshooting complex systems. One application works as follows: Onboard personnel contact one of the four Fleet Technical Service Centers with troubleshooting questions. <p> Perhaps the most significant concern regarding CCBR systems is that they demand a learning curve for authoring case libraries. Our group's research focuses on simplifying the case authoring task for CCBR systems. We created a system, named NaCoDAE (Navy Conversational Decision Aids Environment), for this purpose <ref> (Breslow & Aha, 1997) </ref>. We are pursuing specific methods for simplifying case authoring: 1. Library Revision: Aha and Breslow (1997) describe Clire, a component of NaCoDAE that enforces case authoring guidelines for a given case library. <p> Given the current problem description (i.e., the user's text and their 12 answered questions), these models can be used by a relational database retrieval engine to answer other questions in the case library. We are currently examining how Parka can support this process <ref> (Aha & Maney, 1997) </ref>. More information on our CBR research activities can be viewed at www.aic.nrl.navy.mil/~aha/cbr/practical-advances.html. 3.1.2 Recommenders Firefly (www.firefly.com) is one of the first companies to begin marketing software development tools that use a CBR component to make recommendations to clients.
Reference: <author> Aha, D. W., & Breslow, L. A. </author> <year> (1997). </year> <title> Refining conversational case libraries. </title> <booktitle> Proceedings of the Second International Conference on Case-Based Reasoning (pp. </booktitle> <pages> 267-278). </pages> <address> Providence, RI: </address> <publisher> Springer. </publisher>
Reference-contexts: However, perhaps the most compelling reason for using a lazy problem solving approach is that it is highly intuitive; experts often relate their problem solving behavior in ways that suggest a form of case-based reasoning. We discuss these and other benefits of lazy approaches throughout this section. See <ref> (Aha, 1997) </ref> for a collection of articles focussed on lazy learning. Lazy problem solving is a distinguishing characteristic of CBR approaches: they perform demand-driven reasoning from a stored library of cases. <p> This eliminated any hope of using an expensive eager approach. More recently, we examined the use of an alternative output representation for these tasks <ref> (Aha & Bankert, 1997) </ref>. In particular, we found that error-correcting output codes (ECOCs), a type of distributed representation, can further enhance accuracy, though at the cost of speed and storage. <p> In particular, some systems have several volumes dedicated to troubleshooting scenarios. These are difficult to master, particularly when short billets dictate that personnel have little time to master the machinery they work with. Conversational CBR approaches <ref> (Aha & Breslow, 1997) </ref> have been used to assist naval personnel with troubleshooting complex systems. One application works as follows: Onboard personnel contact one of the four Fleet Technical Service Centers with troubleshooting questions. <p> Perhaps the most significant concern regarding CCBR systems is that they demand a learning curve for authoring case libraries. Our group's research focuses on simplifying the case authoring task for CCBR systems. We created a system, named NaCoDAE (Navy Conversational Decision Aids Environment), for this purpose <ref> (Breslow & Aha, 1997) </ref>. We are pursuing specific methods for simplifying case authoring: 1. Library Revision: Aha and Breslow (1997) describe Clire, a component of NaCoDAE that enforces case authoring guidelines for a given case library. <p> Given the current problem description (i.e., the user's text and their 12 answered questions), these models can be used by a relational database retrieval engine to answer other questions in the case library. We are currently examining how Parka can support this process <ref> (Aha & Maney, 1997) </ref>. More information on our CBR research activities can be viewed at www.aic.nrl.navy.mil/~aha/cbr/practical-advances.html. 3.1.2 Recommenders Firefly (www.firefly.com) is one of the first companies to begin marketing software development tools that use a CBR component to make recommendations to clients.
Reference: <author> Aha, D. W., & Kibler, D. </author> <year> (1989). </year> <title> Noise-tolerant instance-based learning algorithms. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 794-799). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference: <author> Aha, D. W., & Maney, T. </author> <year> (1997). </year> <title> A model-based approach for supporting dialogue inferencing in a conversational case-based reasoner (Technical Report AIC-97-023). </title> <address> Washington, DC: </address> <institution> Naval Research Laboratory, </institution> <note> Navy Center for Applied Research in Artificial Intelligence. </note>
Reference-contexts: However, perhaps the most compelling reason for using a lazy problem solving approach is that it is highly intuitive; experts often relate their problem solving behavior in ways that suggest a form of case-based reasoning. We discuss these and other benefits of lazy approaches throughout this section. See <ref> (Aha, 1997) </ref> for a collection of articles focussed on lazy learning. Lazy problem solving is a distinguishing characteristic of CBR approaches: they perform demand-driven reasoning from a stored library of cases. <p> This eliminated any hope of using an expensive eager approach. More recently, we examined the use of an alternative output representation for these tasks <ref> (Aha & Bankert, 1997) </ref>. In particular, we found that error-correcting output codes (ECOCs), a type of distributed representation, can further enhance accuracy, though at the cost of speed and storage. <p> In particular, some systems have several volumes dedicated to troubleshooting scenarios. These are difficult to master, particularly when short billets dictate that personnel have little time to master the machinery they work with. Conversational CBR approaches <ref> (Aha & Breslow, 1997) </ref> have been used to assist naval personnel with troubleshooting complex systems. One application works as follows: Onboard personnel contact one of the four Fleet Technical Service Centers with troubleshooting questions. <p> Perhaps the most significant concern regarding CCBR systems is that they demand a learning curve for authoring case libraries. Our group's research focuses on simplifying the case authoring task for CCBR systems. We created a system, named NaCoDAE (Navy Conversational Decision Aids Environment), for this purpose <ref> (Breslow & Aha, 1997) </ref>. We are pursuing specific methods for simplifying case authoring: 1. Library Revision: Aha and Breslow (1997) describe Clire, a component of NaCoDAE that enforces case authoring guidelines for a given case library. <p> Given the current problem description (i.e., the user's text and their 12 answered questions), these models can be used by a relational database retrieval engine to answer other questions in the case library. We are currently examining how Parka can support this process <ref> (Aha & Maney, 1997) </ref>. More information on our CBR research activities can be viewed at www.aic.nrl.navy.mil/~aha/cbr/practical-advances.html. 3.1.2 Recommenders Firefly (www.firefly.com) is one of the first companies to begin marketing software development tools that use a CBR component to make recommendations to clients.
Reference: <author> Albert, M. K., & Aha, D. W. </author> <year> (1991). </year> <title> Analyses of instance-based learning algorithms. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 553-558). </pages> <address> Anaheim, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Alpaydin, E. </author> <year> (1997). </year> <title> Voting over multiple condensed nearest neighbors. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11, </volume> <pages> 115-132. </pages>
Reference: <author> Atkeson, C., Moore, A., & Schaal, S. </author> <year> (1997a). </year> <title> Locally weighted learning. </title> <journal> Artificial Intelligence Review, </journal> <pages> 11(1-5), 11-73. </pages>
Reference-contexts: A significant amount of ML research on lazy algorithms has focused on robotic control tasks (e.g., Connell & Utgoff, 1987; Maes & Brooks, 1990; Moore, 1990). See <ref> (Atkeson et al., 1997a) </ref> for an excellent survey on this subject, and its relationship to statistics. They strongly advocate a fairly pure lazy approach where an impressive amount of parameter tuning is performed during prediction tasks. <p> For example, Friedman (1994), Smyth and Cunningham (1994), and Friedman et al. (1996) have described the benefits of using a lazy approach for decision tree induction. The primary advantage is that the trees (really, paths) created by these algorithms are query-specific <ref> (Atkeson et al., 1997a) </ref>; the only features in the tree are those pertinent to the query and missing values are never considered. Other 7 benefits include increased classification accuracy. Webb (1996), among others, are now investigating a similar idea for rule-induction algorithms.
Reference: <author> Atkeson, C., Moore, A., & Schaal, S. </author> <year> (1997b). </year> <title> Locally weighted learning for control. </title> <journal> Artificial Intelligence Review, </journal> <pages> 11(1-5), 75-113. </pages>
Reference-contexts: In such circumstances, it is well known that lazy approaches have benefits vs. eager approaches (e.g., Aha, 1992). This suggests further study of lazy realizations of eager approaches for natural language learning tasks. Another good application area concerns robotic control. See <ref> (Atkeson et al., 1997b) </ref> for a survey on selected lazy approaches for control tasks. Given this discussion of related research, several lessons suggest how to decide whether to use case-base vs. alternative reasoning approaches.
Reference: <author> Bankert, R. L. </author> <year> (1994). </year> <title> Cloud classification of AVHRR imagery in maritime regions using a probabilistic neural network. </title> <journal> Journal of Applied Meteorology, </journal> <volume> 33, </volume> <pages> 909-918. </pages>
Reference: <author> Bankert, R. L., & Aha, D. W. </author> <year> (1996). </year> <title> Improvement to a neural network cloud classifier. </title> <journal> Applied Meteorology, </journal> <volume> 35, </volume> <pages> 2036-2039. </pages>
Reference: <author> Barsalou, L. W. </author> <year> (1989). </year> <title> On the indistinguishability of exemplar memory and abstraction in category representation. </title> <editor> In T. K. Srull & R. S. Wyer (Eds.), </editor> <booktitle> Advances in social cognition. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> Biberman, Y. </author> <year> (1994). </year> <title> A context similarity measure. </title> <booktitle> In Proceedings of the European Conference on Machine Learning (pp. </booktitle> <pages> 49-63). </pages> <address> Catania, Italy: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Borrajo, D., & Veloso, M. </author> <year> (1997). </year> <title> Lazy incremental learning of control knowledge for efficiently obtaining quality plans. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11, </volume> <pages> 371-405. </pages>
Reference: <author> Bradshaw, G. L. </author> <year> (1986). </year> <title> Learning by disjunctive spanning. </title> <editor> In T. M. Mitchell, J. G. Carbonell, & R. S. Michalski (Eds.), </editor> <title> Machine learning: A guide to current research. </title> <address> Boston, MA: </address> <publisher> Kluwer. </publisher>
Reference: <author> Bradshaw, G. </author> <year> (1987). </year> <title> Learning about speech sounds: The NEXUS project. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 1-11). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Branting, L. K., & Broos, P. </author> <year> (1994). </year> <title> Automated acquisition of user preferences. </title> <journal> International Journal of Human-Computer Studies, </journal> <volume> 46, </volume> <pages> 55-77. </pages>
Reference-contexts: 1997; Maron & Moore, 1997; Wettschereck et al., 1997; Howe & Cardie, 1997), * information theory (Lee, 1994; Cleary & Trigg, 1995; Wettschereck & Dietterich, 1995), * noise (Stanfill, 1987; Aha & Kibler, 1989; Aha et al., 1991; Ting, 1997), * parallel implementations (Stanfill & Waltz, 1986), * preference learning <ref> (Branting & Broos, 1994) </ref>, * speedup techniques (Deng & Moore, 1995; Grolimund & Ganascia, 1996; Daelemans et al., 1997), * storage reduction (Zhang, 1992; Cameron-Jones, 1992; Skalak, 1994; Wilson & Martinez, 1997b), * symbolic features (Cost & Salzberg, 1993; Biberman, 1994), and * voting techniques (Alpaydin, 1997; Skalak, 1997; Ricci &
Reference: <author> Breese, J. S., & Heckerman, D. </author> <year> (1995). </year> <title> Decision-theoretic case-based reasoning. </title> <booktitle> Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics (pp. </booktitle> <pages> 56-63). </pages> <address> Ft. Lauderdale, FL: </address> <note> Unpublished. 16 Breslow, </note> <author> L., & Aha, D. W. </author> <year> (1997). </year> <title> NaCoDAE: Navy Conversational Decision Aids Environment (Technical Report AIC-97-018). </title> <address> Washington, DC: </address> <institution> Naval Research Laboratory, </institution> <note> Navy Center for Applied Research in Artificial Intelligence. </note>
Reference: <author> Broomhead, D. S., & Lowe, D. </author> <year> (1988). </year> <title> Multivariable functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> 2, </volume> <pages> 321-355. </pages>
Reference: <author> Cain, T., Pazzani, M. J., & Silverstein, G. </author> <year> (1991). </year> <title> Using domain knowledge to influence similarity judgement. </title> <booktitle> In Proceedings of the Case-Based Reasoning Workshop (pp. </booktitle> <pages> 191-202). </pages> <address> Washington, DC: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Callan, J. P., Lu, Z., & Croft, W. B. </author> <year> (1995). </year> <title> Searching distributed collections with inference networks. </title> <booktitle> Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. </booktitle> <pages> 21-28). </pages> <address> Seattle, WA: </address> <note> Unpublished. </note>
Reference-contexts: For example, closely related research has been pursued in cognitive science on computational analogy (Gentner & Forbus, 1991), in information retrieval on inference networks <ref> (Callan et al., 1995) </ref>, in physics on provably correct retrieval, similarity, and adaptation functions for a set of domains (Rudolph & Hertkorn, 1997), in software engineering on software reuse, and in statistics on recursive partitioning (Friedman, 1994) and tangent distance functions (e.g., Hastie and Tibshirani, 1994).
Reference: <author> Cameron-Jones, R. M. </author> <year> (1992). </year> <title> Minimum description length instance-based learning. </title> <booktitle> In Proceedings of the Fifth Australian Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 368-373). </pages> <address> Hobart, Australia: </address> <publisher> World Scientific. </publisher>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 25-32). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cardie, C., & Howe, N. </author> <year> (1997). </year> <title> Improving minority-class prediction using case-specific feature weights. </title> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning (pp. </booktitle> <pages> 57-65). </pages> <address> Nashville, TN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Researchers have studied several lazy learning topics, including * case selection (Zhang et al., 1997), * concept distribution skew <ref> (Cardie & Howe, 1997) </ref>, * concept shift (Salganicoff, 1997), * cost-sensitive learning (Tan & Schlimmer, 1990; Turney, 1993), * discretization (Ting, 1997; Wilson & Martinez, 1997a), 1 Several synonyms have been used to describe these algorithms. For example, these include fcase,exemplar,instance,memoryg-based flearning,reasoningg.
Reference: <author> Clark, P., & Holte, R. </author> <year> (1992). </year> <title> Lazy partial evaluation: An integration of explanation-based generalisation and partial evaluation. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 82-91). </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, the tradeoff often exists that lazy approaches require more work to answer information queries, although smart caching schemes can be used to decrease this workload <ref> (e.g., Clark & Holte, 1992) </ref>. 4. Disjunctive Solution Spaces: Lazy approaches are often most appropriate for tasks whose solution spaces are complex, making them less appropriate for approaches that replace data with abstractions (Aha, 1992). 5.
Reference: <author> Cleary, J. G., & Trigg, L. E. </author> <year> (1995). </year> <title> K*: An instance-based learner using an entropic distance measure. </title> <booktitle> In Proceedings of the Twelfth International Machine Learning Conference (pp.108-114). </booktitle> <address> Tahoe City, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Connell, M. E., & Utgoff, P. E. </author> <year> (1987). </year> <title> Learning to control a dynamic physical system. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 456-460). </pages> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cost, S., & Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 57-78. </pages>
Reference: <author> Daelemans, W., van den Bosch, A., & Weijters, G. </author> <year> (1997). </year> <title> IGTree: Using trees for compression and classification in lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11, </volume> <pages> 407-423. </pages>
Reference-contexts: An enormous amount of research on reasoning from cases has appeared in research published in many disciplines. A few application areas are particularly good matches for the biases of lazy approaches. For example, research on natural language learning <ref> (e.g., Daelemans et al., 1997) </ref> often involves working with data sets whose concepts are highly disjunctive. In such circumstances, it is well known that lazy approaches have benefits vs. eager approaches (e.g., Aha, 1992). This suggests further study of lazy realizations of eager approaches for natural language learning tasks.
Reference: <author> Dasarathy, B. V. </author> <year> (1980). </year> <title> Nosing around the neighborhood: A new system structure and classification rule for recognition in partially exposed environments. </title> <journal> Pattern Analysis and Machine Intelligence, </journal> <volume> 2, </volume> <pages> 67-71. </pages>
Reference: <author> Dasarathy, B. V. (Ed.). </author> <year> (1991). </year> <title> Nearest neighbor(NN) norms: NN pattern classification techniques. </title> <publisher> Los Alamitos, </publisher> <address> CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Deng, K., & Moore, A. W. </author> <year> (1995). </year> <title> Multiresolution instance-based learning. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1233-1239). </pages> <address> Montreal: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Domingos, P. </author> <year> (1995). </year> <title> Rule induction and instance-based learning: A unified approach. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 1226-1232). </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher> <address> 17 Domingos, P. </address> <year> (1997). </year> <title> Context-sensitive feature selection for lazy learners. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11, </volume> <pages> 227-253. </pages>
Reference: <author> Elliot, T., & Scott, P. D. </author> <year> (1991). </year> <title> Instance-based and generalization-based learning procedures applied to solving integration problems. </title> <booktitle> In Proceedings of the Eighth Conference of the Society for the Study of Artificial Intelligence (pp. </booktitle> <pages> 256-265). </pages> <address> Leeds, England: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Emde, W. & Wettschereck, D. </author> <year> (1996). </year> <title> Relational instance-based learning. </title> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning (pp. </booktitle> <pages> 122-130). </pages> <address> Bari, Italy: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Epstein, S. L., & Shih, J. </author> <year> (1997). </year> <title> Learning from sequential examples: initial results with instance-based learning. </title> <editor> In D. Wettschereck & D. W. </editor> <title> Aha Working Notes for Case-Based Learning: Beyond Classification of Feature Vectors (Technical Report AIC-97-005). </title> <address> Washington, DC: </address> <institution> Naval Research Laboratory, </institution> <note> Navy Center for Applied Research in Artificial Intelligence. </note>
Reference-contexts: For example, in highly disjunctive spaces (e.g., several natural language processing tasks), eager approaches often combine disjuncts repre senting different solutions, and thereby will reduce task performance. * Sequence-based reasoning: Temporal reasoning from sequences has proven effective for such diverse tasks as selecting moves in two-person games <ref> (Epstein & Shih, 1997) </ref> and for disambiguating states (McCallum, 1995). The potential of CBR for other types of sequential tasks is worth investigating. * Query-specific reasoning: Lazy approaches provide the enormous benefit of using local information to characterize states and generate predictions (e.g., Friedman et al., 1996).
Reference: <author> Fisher, D. H. </author> <year> (1989). </year> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 139-172. </pages>
Reference-contexts: Quinlan (1993b) examined how decision tree predictions could be used to modify the prediction of a lazy learner. Utgoff (1989), among others, has examined how decision trees can be efficiently incrementally updated by storing cases at its leaves. This is reminiscent of how unsupervised algorithms, such as COBWEB <ref> (Fisher, 1989) </ref>, use evaluation functions to update their hierarchies. Similar approaches might prove useful for guiding the incremental modifications of indexing structures for case retrieval.
Reference: <author> Fix, E., & Hodges, J. L., Jr. </author> <year> (1951). </year> <title> Discriminatory analysis, nonparametric discrimination, </title> <type> consistency properties (Technical Report 4). </type> <institution> Randolph Field, TX: United States Air Force, School of Aviation Medicine. </institution>
Reference: <author> Friedman, J. H. </author> <year> (1994). </year> <title> Flexible metric nearest neighbor classification. </title> <note> Unpublished manuscript available by anonymous FTP from playfair.stanford.edu (see pub/friedman/README). </note>
Reference-contexts: cognitive science on computational analogy (Gentner & Forbus, 1991), in information retrieval on inference networks (Callan et al., 1995), in physics on provably correct retrieval, similarity, and adaptation functions for a set of domains (Rudolph & Hertkorn, 1997), in software engineering on software reuse, and in statistics on recursive partitioning <ref> (Friedman, 1994) </ref> and tangent distance functions (e.g., Hastie and Tibshirani, 1994). An enormous amount of research on reasoning from cases has appeared in research published in many disciplines. A few application areas are particularly good matches for the biases of lazy approaches.
Reference: <author> Friedman, J. H., Kohavi, R., & Yun, Y. </author> <year> (1996). </year> <title> Lazy decision trees. </title> <booktitle> Proceedings of the Thirteenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 717-724). </pages> <address> Portland, OR: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The potential of CBR for other types of sequential tasks is worth investigating. * Query-specific reasoning: Lazy approaches provide the enormous benefit of using local information to characterize states and generate predictions <ref> (e.g., Friedman et al., 1996) </ref>. Because abstraction approaches attempt to fit all cases simultaneously, they may lose information that is crucial for specific queries. * Training speed: Case-based approaches are usually much cheaper to use when storing information.
Reference: <author> Gates, G. W. </author> <year> (1972). </year> <title> The reduced nearest neighbor rule. </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory, </journal> <volume> 18, </volume> <pages> 431-433. </pages>
Reference: <author> Gebhardt, F., Voss, A., Graether, W., & Schmidt-Belz, B. </author> <year> (1997). </year> <title> Reasoning with Complex Cases. </title> <address> Boston: </address> <publisher> Kluwer. </publisher>
Reference: <author> Gentner, D., & Forbus, K. </author> <year> (1991). </year> <title> MAC/FAC: A model of similarity-based retrieval. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 504-509). </pages> <address> Chicago, IL: </address> <publisher> Lawrence Earlbaum. </publisher>
Reference-contexts: For example, closely related research has been pursued in cognitive science on computational analogy <ref> (Gentner & Forbus, 1991) </ref>, in information retrieval on inference networks (Callan et al., 1995), in physics on provably correct retrieval, similarity, and adaptation functions for a set of domains (Rudolph & Hertkorn, 1997), in software engineering on software reuse, and in statistics on recursive partitioning (Friedman, 1994) and tangent distance functions
Reference: <author> Goodman, M. </author> <year> (1994). </year> <title> Results on controlling action with projective visualization. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 1245-1250). </pages> <address> Seattle, WA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Grefenstette, J. J. </author> <year> (1996). </year> <title> Genetic learning for adaptation in autonomous robots. </title> <editor> In M. Jamshidi, F. Pin, & D. Dauchez (Eds.), </editor> <booktitle> Robotics and Manufacturing: Recent Trends in Research and Applications. </booktitle> <address> New York: </address> <publisher> ASME Press. </publisher>
Reference: <author> Griffiths, A. D. </author> <year> (1996). </year> <title> Inductive generalisation in case-based reasoning systems (Technical Report YCST 97/02). </title> <address> Heslington, England: </address> <institution> University of York, Department of Computer Science. </institution>
Reference: <author> Grolimund, S., & Ganascia, J.-L. </author> <year> (1995). </year> <title> Integrating case-based reasoning and tabu search for solving optimisation problems. </title> <booktitle> Proceedings of the First International Conference on Case-Based Reasoning (pp. </booktitle> <pages> 451-460). </pages> <address> Sesimbra, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Hart, P. E. </author> <year> (1968). </year> <title> The condensed nearest neighbor rule. </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory, </journal> <volume> 14, </volume> <pages> 515-516. </pages>
Reference-contexts: Both of these algorithms were, not surprisingly, highly similar to earlier research on pattern recognition. In particular, disjunctive spanning is closely realted to Sebestyen's (1962) algorithm (i.e., but without pre-determined thresholds), and IB2 is a simplification of CNN <ref> (Hart, 1968) </ref>. 2 Since these early efforts, lazy algorithms have undergone dramatic design enhancements.
Reference: <author> Hastie, T., & Tibshirani, R. </author> <year> (1994). </year> <title> Discriminant adaptive nearest neighbor classification. </title> <type> Unpublished manuscript. </type> <note> 18 Hintzman, </note> <author> D. L., & Ludlam, G. </author> <year> (1984). </year> <title> Differential forgetting of prototypes and old instances: Simulation by an exemplar-based classification model. </title> <journal> Memory & Cognition, </journal> <volume> 8, </volume> <pages> 378-382. </pages>
Reference-contexts: & Forbus, 1991), in information retrieval on inference networks (Callan et al., 1995), in physics on provably correct retrieval, similarity, and adaptation functions for a set of domains (Rudolph & Hertkorn, 1997), in software engineering on software reuse, and in statistics on recursive partitioning (Friedman, 1994) and tangent distance functions <ref> (e.g., Hastie and Tibshirani, 1994) </ref>. An enormous amount of research on reasoning from cases has appeared in research published in many disciplines. A few application areas are particularly good matches for the biases of lazy approaches.
Reference: <author> Howe, N, & Cardie, C. </author> <year> (1997). </year> <title> Examining locally varying weights for nearest neighbor algorithms. </title> <booktitle> Proceedings of the Second International Conference on Case-Based Reasoning (pp. </booktitle> <pages> 455-466). </pages> <address> Providence, RI: </address> <publisher> Springer. </publisher>
Reference-contexts: Researchers have studied several lazy learning topics, including * case selection (Zhang et al., 1997), * concept distribution skew <ref> (Cardie & Howe, 1997) </ref>, * concept shift (Salganicoff, 1997), * cost-sensitive learning (Tan & Schlimmer, 1990; Turney, 1993), * discretization (Ting, 1997; Wilson & Martinez, 1997a), 1 Several synonyms have been used to describe these algorithms. For example, these include fcase,exemplar,instance,memoryg-based flearning,reasoningg.
Reference: <author> Indurkhya, N., & Weiss, S. M. </author> <year> (1995). </year> <title> Using case data to improve on rule-based function approximation. </title> <booktitle> Proceedings of the First International Conference on Case-Based Reasoning (pp. </booktitle> <pages> 217-228). </pages> <address> Sesimbra, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> John, G., Kohavi, R., & Pfleger, K. </author> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference (pp. </booktitle> <pages> 121-129). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: He used a clustering algorithm to evaluate the quality of a feature subset, along with a neural network classifier. Although he reported what seemed to be reasonable performance, we found that a wrapper approach <ref> (John et al., 1994) </ref>, in which the same algorithm was used for both the feature subset evaluator and the classifier, was preferable. In particular, we used a case-based classifier for both components, and it significantly increased classification accuracies.
Reference: <author> Kelly, J. D., Jr., & Davis, L. </author> <year> (1991). </year> <title> A hybrid genetic algorithm for classification. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 645-650). </pages> <address> Sydney, Australia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kibler, D., & Aha, D. W. </author> <year> (1987). </year> <title> Learning representative exemplars of concepts: An initial case study. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 24-30). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kibler, D., & Aha, D. W. </author> <year> (1988). </year> <title> Comparing instance-averaging with instance-filtering learning algorithms. </title> <booktitle> In Proceedings of the Third European Working Session on Learning (pp. </booktitle> <pages> 63-80). </pages> <address> Glasgow, Scotland: </address> <publisher> Pitman. </publisher>
Reference-contexts: He focused on the task of recognizing spoken letters, and introduced an instance-averaging algorithm named disjunctive spanning that attempted to locate centroids of disjuncts. Kibler and Aha (1987) later introduced IB2, a similar algorithm that performs instance filtering (i.e., by retaining only correctly classified cases) rather than instance averaging <ref> (Kibler & Aha, 1988) </ref>. Both of these algorithms were, not surprisingly, highly similar to earlier research on pattern recognition.
Reference: <author> Kitano, H., Shibata, A., Shimazu, H., Kajihara, J., & Sato, A. </author> <year> (1992). </year> <title> Building large-scale and corporate-wide case-based systems: Integration of the organizational and machine executable algorithms. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 843-849). </pages> <address> San Jose, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Kohavi, R., Langley, P., & Yun, Y. </author> <year> (1997). </year> <title> The utility of feature weighting in nearest neighbor algorithms. </title> <editor> In M. van Someren & G. Widmer (Eds.) </editor> <booktitle> Poster Papers: Ninth European Conference on Machine Learning. Unpublished manuscript. </booktitle>
Reference: <author> Kolodner, J. </author> <year> (1993a). </year> <title> Case-based reasoning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is difficult to find consensus on more detailed definitions of CBR because it means different things to different groups of people. For example, consider its interpretation by the following three groups: * Cognitive Scientists: CBR is a plausible high-level model for cognitive processing <ref> (Kolodner, 1993a) </ref>. * Artificial Intelligence Researchers: CBR is a computational paradigm for problem solving (Aamodt & Plaza, 1994). * Expert System Practitioners: CBR is a design model for expert systems that can be used in either stand alone or embedded architectures (Watson, 1997).
Reference: <author> Kolodner, J. </author> <year> (1993b). </year> <title> Case-based learning. </title> <address> Boston: </address> <publisher> Kluwer. </publisher>
Reference: <author> Koplowitz, J., & Brown, T. A. </author> <year> (1981). </year> <title> On the relation of performance to editing in nearest neighbor rules. </title> <journal> Pattern Recognition, </journal> <volume> 13, </volume> <pages> 251-255. </pages>
Reference: <author> Kruschke, J. K. </author> <year> (1992). </year> <title> ALCOVE: An exemplar-based connectionist model of category learning. </title> <journal> Psychological Review, </journal> <volume> 99, </volume> <pages> 22-44. </pages>
Reference: <author> Kurtzberg, J. M. </author> <year> (1987). </year> <title> Feature analysis for symbol recognition by elastic matching. </title> <journal> International Business Machines Journal of Research and Development, </journal> <volume> 31, </volume> <pages> 91-95. </pages>
Reference-contexts: Alternatively, Hart (1968) popularized the study of algorithms that deleted correctly classified cases, arguing that the atypical cases provided more information on class boundaries. This line of research has been more empirically inclined, with a focus on thresholding (Sebestyen, 1962), iterative deletion (Gates, 1973), and domain-specific feature selection <ref> (Kurtzberg, 1987) </ref>. Naturally, a combination of these two strategies has also been investigated in detail (e.g., Voisin & Devijer, 1987). Dasarathy's (1991) collection contains several classic papers related to this research. Interest in the AI community on case deletion is strong. <p> For example, these include fcase,exemplar,instance,memoryg-based flearning,reasoningg. These names reflect a focus on representation to the exclusion of processing. This is one reason why we prefer the phrase lazy learning; it implies both. 2 CNN was also rediscovered elsewhere. For example, see <ref> (Kurtzberg, 1987) </ref>. 6 * feature selection and weighting (Kelly & Davis, 1991; Cain et al., 1991; Cardie, 1993; Skalak, 1994; Ricci & Avesani, 1995; Kohavi et al., 1997; Domingos, 1997; Ling & Wang, 1997; Maron & Moore, 1997; Wettschereck et al., 1997; Howe & Cardie, 1997), * information theory (Lee, 1994;
Reference: <author> Langley, P., & Iba, W. </author> <year> (1993). </year> <title> Average-case analysis of a nearest neighbor algorithm. </title> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 889-894). </pages> <address> Chambery, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Langley, P., Pfleger, K., & Sahami, M. </author> <year> (1997). </year> <title> Lazy acquisition of place knowledge. </title> <journal> Artificial Intelligence Review, </journal> <pages> 11(1-5), 315-342. </pages>
Reference: <author> Leake, D. B. (Ed.) </author> <title> Case-Based Reasoning: Experiences, Lessons, and Future Directions. </title> <publisher> AAAI Press/MIT Press. 19 Leake, </publisher> <editor> D. B., & Plaza, E. (Eds.) </editor> <year> (1997). </year> <title> Case Based Reasoning Research and Development: </title> <booktitle> Proceedings of the First International Conference. </booktitle> <address> Providence, RI: </address> <publisher> Springer. </publisher>
Reference: <author> Lee, C. </author> <year> (1994). </year> <title> An instance-based learning method for databases: An information theoretic approach. </title> <booktitle> In Proceedings of the European Conference on Machine Learning (pp. </booktitle> <pages> 387-390). </pages> <address> Catania, Italy: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Lewis, L. </author> <year> (1995). </year> <title> Managing Computer Networks: A Case-Based Reasoning Approach. </title> <address> Norwood, MA: </address> <publisher> Artech House. </publisher>
Reference: <author> Lin, J.-H., & Vitter, J. S. </author> <year> (1994). </year> <title> A theory for memory-based learning. </title> <journal> Machine Learning, </journal> <volume> 17, </volume> <pages> 143-168. </pages>
Reference: <author> Ling, X. C., & Wang, H. </author> <year> (1997). </year> <title> Towards optimal weights setting for the 1-nearest neighbour learning algorithm. </title> <journal> Artificial Intelligence Review, </journal> <pages> 11(1-5), 255-272. </pages>
Reference: <author> Maes, P., & Brooks, R. </author> <year> (1990). </year> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 796-802). </pages> <address> Boston, MA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Maher, M. L., Balachandran, M. B., & Zhang, D. M. </author> <year> (1995). </year> <title> Case-Based Reasoning in Design. </title> <address> Mahwah, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Maher, M. L. and Pu, P. (Eds.) </author> <year> (1997). </year> <title> Issues and Applications of Case-Based Reasoning to Design. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Maron, O., & Moore, A. W. </author> <year> (1997). </year> <title> The racing algorithm: Model selection for lazy learners. </title> <journal> Artificial Intelligence Review, </journal> <pages> 11(1-5), 193-225. </pages>
Reference: <author> Matheus, C. J. </author> <year> (1987). </year> <title> Conceptual purpose: Implications for representation and learning in machines and humans (Technical Report 87-1370). </title> <institution> Urbana, IL: University of Illinois, Department of Computer Science. </institution>
Reference-contexts: Given this discussion of related research, several lessons suggest how to decide whether to use case-base vs. alternative reasoning approaches. For example: * Incremental learning: Case-based approaches are suited for incremental learning tasks because they prevent the premature selection of summary abstractions. For more information, see <ref> (Matheus, 1987) </ref>. * Highly disjunctive spaces: Case-based approaches are particularly suited for tasks where abstractions tend to yield over-generalizations.
Reference: <author> McCallum, R. A. </author> <year> (1995). </year> <title> Instance-based utile distinctions for reinforcement learning. </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <address> Lake Tahoe, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Sequential Problem Solving: Sequential tasks often benefit from the storage of a history in the form of the states that lead to the current state. Lazy approaches are used to store this information, which can then be used, for example, to disambiguate states <ref> (e.g., McCallum, 1995) </ref>. However, perhaps the most compelling reason for using a lazy problem solving approach is that it is highly intuitive; experts often relate their problem solving behavior in ways that suggest a form of case-based reasoning. We discuss these and other benefits of lazy approaches throughout this section. <p> spaces (e.g., several natural language processing tasks), eager approaches often combine disjuncts repre senting different solutions, and thereby will reduce task performance. * Sequence-based reasoning: Temporal reasoning from sequences has proven effective for such diverse tasks as selecting moves in two-person games (Epstein & Shih, 1997) and for disambiguating states <ref> (McCallum, 1995) </ref>. The potential of CBR for other types of sequential tasks is worth investigating. * Query-specific reasoning: Lazy approaches provide the enormous benefit of using local information to characterize states and generate predictions (e.g., Friedman et al., 1996).
Reference: <author> Medin, D. L., & Schaffer, M. M. </author> <year> (1978). </year> <title> Context theory of classification learning. </title> <journal> Psychological Review, </journal> <volume> 85, </volume> <pages> 207-238. </pages>
Reference: <author> Moody, J., & Darken, C. </author> <year> (1988). </year> <title> Learning with localized receptive fields. </title> <booktitle> In Proceedings of the 1988 Connectionist Models Summer School (pp. </booktitle> <pages> 133-143). </pages> <address> Pittsburgh, PA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moore, A. W. </author> <year> (1990). </year> <title> Acquisition of dynamic control knowledge for a robotic manipulator. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 244-252). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moore, A. W. & Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 103-130. </pages>
Reference: <author> Nosofsky, R. M. </author> <year> (1986). </year> <title> Attention, similarity, and the identification-categorization relationship. </title> <journal> Journal of Experimental Psychology: General, </journal> <volume> 15, </volume> <pages> 39-57. </pages>
Reference: <author> Okamoto, S., & Yugami, N. </author> <year> (1997). </year> <title> Theoretical analysis of a case retrieval method based on neighborhood of a new problem. </title> <booktitle> Proceedings of the Second International Conference on Case-Based Reasoning (pp. </booktitle> <pages> 349-358). </pages> <address> Providence, RI: </address> <publisher> Springer. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1993a). </year> <title> C4.5: Programs for machine learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. 20 Quinlan, </publisher> <editor> J. R. </editor> <year> (1993b). </year> <title> Combining instance-based learning and model-based learning. </title> <booktitle> In Proceed--ings of the Tenth International Conference on Machine Learning (pp. </booktitle> <pages> 236-243). </pages> <address> Amherst, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, in the context of supervised learning algorithms, the k-nearest neighbor classifier is a lazy algorithm, while algorithms that greedily induce decision trees (e.g., C4.5 <ref> (Quinlan, 1993a) </ref>) are eager. This lazy/eager distinction exhibits many interesting tradeoffs. For example, the following benefits of lazy problem solving approaches exist: 1.
Reference: <author> Ricci, F., & Aha, D. W. </author> <year> (1997). </year> <title> Extending local learners with error-correcting output codes (Technical Report AIC-97-001). </title> <address> Washington, DC: </address> <institution> Naval Research Laboratory, </institution> <note> Navy Center for Applied Research in Artificial Intelligence. </note>
Reference: <author> Ricci, F., & Avesani, P. </author> <year> (1995). </year> <title> Learning a local similarity metric for case-based reasoning. </title> <booktitle> In Proceedings of the First International Conference on Case-Based Reasoning (pp. </booktitle> <pages> 301-312). </pages> <address> Sesimbra, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Riesbeck, C. K., & Schank, R. C. </author> <year> (1989). </year> <title> Inside Case-Based Reasoning. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Rudolph, S. </author> <year> (1997). </year> <title> On the foundations and applications of similarity theory to case-based reasoning. </title> <booktitle> Proceedings of the Twelvth International Conference for Applications of Artificial Intelligence in Engineering. </booktitle> <address> Capri, Italy. </address>
Reference-contexts: For example, closely related research has been pursued in cognitive science on computational analogy (Gentner & Forbus, 1991), in information retrieval on inference networks (Callan et al., 1995), in physics on provably correct retrieval, similarity, and adaptation functions for a set of domains <ref> (Rudolph & Hertkorn, 1997) </ref>, in software engineering on software reuse, and in statistics on recursive partitioning (Friedman, 1994) and tangent distance functions (e.g., Hastie and Tibshirani, 1994). An enormous amount of research on reasoning from cases has appeared in research published in many disciplines.
Reference: <author> Salganicoff, M. </author> <year> (1997). </year> <title> Tolerating concept and sampling shift in lazy learning using prediction error context switching. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11, </volume> <pages> 133-155. </pages>
Reference-contexts: Researchers have studied several lazy learning topics, including * case selection (Zhang et al., 1997), * concept distribution skew (Cardie & Howe, 1997), * concept shift <ref> (Salganicoff, 1997) </ref>, * cost-sensitive learning (Tan & Schlimmer, 1990; Turney, 1993), * discretization (Ting, 1997; Wilson & Martinez, 1997a), 1 Several synonyms have been used to describe these algorithms. For example, these include fcase,exemplar,instance,memoryg-based flearning,reasoningg. These names reflect a focus on representation to the exclusion of processing.
Reference: <author> Salzberg, S. L. </author> <year> (1991). </year> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 251-276. </pages>
Reference: <author> Schank, R. C., & Abelson, R. P. </author> <year> (1977). </year> <title> Scripts, plans, goals, and understanding. </title> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: <editor> Schank, R. C., Kass, A., & Riesbeck, C. K. (Eds.) </editor> <year> (1994). </year> <title> Inside Case-Based Explanation. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Sebestyen, G. S. </author> <year> (1962). </year> <title> Decision-making processes in pattern recognition. </title> <address> New York, NY: Macmil-lan. </address>
Reference-contexts: Alternatively, Hart (1968) popularized the study of algorithms that deleted correctly classified cases, arguing that the atypical cases provided more information on class boundaries. This line of research has been more empirically inclined, with a focus on thresholding <ref> (Sebestyen, 1962) </ref>, iterative deletion (Gates, 1973), and domain-specific feature selection (Kurtzberg, 1987). Naturally, a combination of these two strategies has also been investigated in detail (e.g., Voisin & Devijer, 1987). Dasarathy's (1991) collection contains several classic papers related to this research.
Reference: <author> Shepard, R. N. </author> <year> (1987). </year> <title> Toward a universal law of generalization for psychological science. </title> <journal> Science, </journal> <volume> 237, </volume> <pages> 1317-1323. </pages>
Reference: <author> Sheppard, J. W., & Salzberg, S. L. </author> <year> (1997). </year> <title> A teaching strategy for memory-based control. </title> <journal> Artificial Intelligence Review, </journal> <pages> 11(1-5), 343-370. </pages>
Reference: <author> Skalak, D. </author> <year> (1994). </year> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference (pp. </booktitle> <pages> 293-301). </pages> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Skalak, D. </author> <year> (1997). </year> <title> Prototype selection for composite nearest neighbor classifiers (Technical Report 96-89). </title> <address> Amherst, MA: </address> <institution> University of Massachusetts, Department of Computer Science. </institution>
Reference: <author> Smith, E. E., & Medin, D. L. </author> <year> (1981). </year> <title> Categories and concepts. </title> <address> Cambridge, MA: </address> <publisher> Harvard University Press. </publisher>
Reference: <author> Smyth, B., & Cunningham, P. </author> <year> (1994). </year> <title> A comparison of incremental CBR and inductive learning. </title>
Reference: <editor> In M. Keane, J. P. Haton, & M. Manago (Eds.) </editor> <booktitle> Working Papers of the Second European Workshop on Case-Based Reasoning. </booktitle> <address> Chantilly, France: </address> <note> Unpublished. </note>
Reference: <author> Smyth, B. & Keane, M. T. </author> <year> (1995). </year> <title> Remembering to forget: A competence-preserving deletion policy for case-based systems. </title> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 377-383). </pages> <address> Montreal, Canada: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Dasarathy's (1991) collection contains several classic papers related to this research. Interest in the AI community on case deletion is strong. For example, the 1995 IJCAI best paper award was given to a paper on this topic <ref> (Smyth & Keane, 1995) </ref>. Thus, it is probable that 5 several of the advances in pattern recognition on this topic are of interest to CBR researchers. Furthermore, work by Dasarathy (1980) and others on learning when to not make predictions from stored cases should also be of interest.
Reference: <author> Stanfill, C. </author> <year> (1987). </year> <title> Memory-based reasoning applied to English pronunciation. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 577-581). </pages> <address> Seattle, WA: </address> <publisher> Morgan Kaufmann. </publisher> <address> 21 Stanfill, C., & Waltz, D. </address> <year> (1986). </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the Asso--ciation for Computing Machinery, </journal> <volume> 29, </volume> <pages> 1213-1228. </pages>
Reference: <author> Stewart, T. A. </author> <year> (1997). </year> <title> Intellectual Capital: The New Wealth of Organizations. </title> <address> New York: Dou-bleday/Currency. </address>
Reference-contexts: Knowledge Management: A primary concern of corporate entities is in recording and reusing the knowledge of their employees. The motivation is clear; employee turnover means loss of knowledge, and thus loss of intellectual capital <ref> (Stewart, 1997) </ref>. This is an excellent applica tions area for case-based reasoning technology that is only now beginning to be pursued.
Reference: <author> Tadepalli, P. </author> <year> (1989). </year> <title> Lazy explanation-based learning: A solution to the intractable theory problem. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 694-700). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Tan, M., & Schlimmer, J. C. </author> <year> (1989). </year> <title> Cost-sensitive concept learning of sensor use in approach and recognition. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 392-395). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ting, K. M. </author> <year> (1994). </year> <title> The problem of small disjuncts: Its remedy in decision trees. </title> <booktitle> In Proceedings of the Tenth Canadian Conference on Artificial Intelligence (pp. </booktitle> <pages> 91-97). </pages>
Reference-contexts: 1997; Maron & Moore, 1997; Wettschereck et al., 1997; Howe & Cardie, 1997), * information theory (Lee, 1994; Cleary & Trigg, 1995; Wettschereck & Dietterich, 1995), * noise (Stanfill, 1987; Aha & Kibler, 1989; Aha et al., 1991; Ting, 1997), * parallel implementations (Stanfill & Waltz, 1986), * preference learning <ref> (Branting & Broos, 1994) </ref>, * speedup techniques (Deng & Moore, 1995; Grolimund & Ganascia, 1996; Daelemans et al., 1997), * storage reduction (Zhang, 1992; Cameron-Jones, 1992; Skalak, 1994; Wilson & Martinez, 1997b), * symbolic features (Cost & Salzberg, 1993; Biberman, 1994), and * voting techniques (Alpaydin, 1997; Skalak, 1997; Ricci &
Reference: <author> Ting, K. M. </author> <year> (1997). </year> <title> Discretization in lazy learning algorithms. </title> <editor> In D. W. Aha (Ed.), </editor> <title> Lazy Learning. </title> <publisher> Norwell, </publisher> <address> MA: </address> <publisher> Kluwer. </publisher>
Reference: <author> Tirri, H., Kontkanen, P., & Myllmaki, P. </author> <year> (1996). </year> <title> A Bayesian framework for case-based reasoning. </title> <booktitle> Proceedings of the Third European Workshop on Case-Based Reasoning (pp. </booktitle> <pages> 413-427). </pages> <address> Lausanne, Switzerland: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Torgo, L. </author> <year> (1997). </year> <title> Kernel regression trees. </title> <editor> In M. van Someren & G. Widmer (Eds.) </editor> <booktitle> Poster Papers: Ninth European Conference on Machine Learning. Unpublished manuscript. </booktitle>
Reference: <author> Turney, P. D. </author> <year> (1993). </year> <title> Exploiting context when learning to classify. </title> <booktitle> In Proceedings of the European Conference on Machine Learning (pp. </booktitle> <pages> 402-407). </pages> <address> Vienna, Austria: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Turney, P. D. </author> <year> (1994). </year> <title> Theoretical analyses of cross-validation error and voting in instance-based learning. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 6, </volume> <pages> 331-360. </pages>
Reference: <author> Utgoff, P. E. </author> <year> (1989). </year> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 161-186. </pages>
Reference: <editor> Veloso, M., & Aamodt, A. (Eds.) </editor> <year> (1995). </year> <title> Case Based Reasoning Research and Development: </title> <booktitle> Proceedings of the First International Conference. </booktitle> <address> Sesimbra, Portugal: </address> <publisher> Springer. </publisher>
Reference: <author> Voisin, J., & Devijver, P. A. </author> <year> (1987). </year> <title> An application of the Multiedit-Condensing technique to the reference selection problem in a print recognition system. </title> <journal> Pattern Recognition, </journal> <volume> 5, </volume> <pages> 465-474. </pages>
Reference-contexts: This line of research has been more empirically inclined, with a focus on thresholding (Sebestyen, 1962), iterative deletion (Gates, 1973), and domain-specific feature selection (Kurtzberg, 1987). Naturally, a combination of these two strategies has also been investigated in detail <ref> (e.g., Voisin & Devijer, 1987) </ref>. Dasarathy's (1991) collection contains several classic papers related to this research. Interest in the AI community on case deletion is strong. For example, the 1995 IJCAI best paper award was given to a paper on this topic (Smyth & Keane, 1995).
Reference: <author> Volper, D. J., & Hampson, S. E. </author> <year> (1987). </year> <title> Learning and using specific instances. </title> <journal> Biological Cybernetics, </journal> <volume> 57, </volume> <pages> 57-71. </pages>
Reference: <author> Wagner, T. J. </author> <year> (1973). </year> <title> Convergence of the edited nearest neighbor. </title> <journal> Institute of Electrical and Electronic Engineers Transactions on Information Theory, </journal> <volume> 19, </volume> <pages> 696-697. </pages>
Reference: <author> Watson, I. </author> <year> (1997). </year> <title> Applying Case-Based Reasoning. </title> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: CBR is a plausible high-level model for cognitive processing (Kolodner, 1993a). * Artificial Intelligence Researchers: CBR is a computational paradigm for problem solving (Aamodt & Plaza, 1994). * Expert System Practitioners: CBR is a design model for expert systems that can be used in either stand alone or embedded architectures <ref> (Watson, 1997) </ref>. To introduce CBR requires identifying it in a particular context. For example, variants of Aamodt and Plaza's (1994) problem-solving cycle are frequently used when introducing CBR to AI researchers. <p> to cognitive models (e.g., Schank & Kass, 1994; Leake, 1996), most recent books related to CBR have stressed issues in computer science, including design (Maher et al., 1995; Gebhardt et al., 1997; Maher & Pu, 1997), expert systems (Lewis, 1995; Watson, 1997), machine learning (Kolodner, 1993b; Aha, 1997), and applications <ref> (Watson, 1997) </ref>. Several other research disciplines (e.g., machine learning, cognitive psychology, process planning, statistics) have also contributed to a growing body of knowledge on reasoning from cases. However, few CBR publications relate these other contributions to CBR research. <p> We strongly encourage that practitioners use a comparative analysis of the pros and cons for whether a CBR approach is well-suited for their task. 2.2 Omnipresence in Application Case-base approaches have been applied to a broad spectrum of problems in various roles. Readers interested in details should consult <ref> (Watson, 1997) </ref> or certain CBR-related WWW pages (e.g., AI-CBR, U. Kaiserslautern's page, or our own). <p> For each category, we mention a company whose products focus on these tasks. For more information on other companies who are pursuing these and other interesting tasks (e.g., AcknoSoft), and on other successful CBR ventures, please see <ref> (Watson, 1997) </ref>. 3.1.1 Interactive Troubleshooting By far the most extensive application of CBR technology has been in the area of interactive troubleshooting. In particular, Inference Corporation (www.inference.com) has over 650 corporate contracts with organizations who are using Inference's CBR Content Navigator products for help desk and WWW self-help applications.
Reference: <author> Webb, G. </author> <year> (1996). </year> <title> A heuristic covering algorithm outperforms learning all rules. </title> <booktitle> Proceedings of Information, Statistics, and Induction in Science (pp. </booktitle> <pages> 20-30). </pages> <address> Melbourne: </address> <publisher> World Scientific. </publisher>
Reference: <author> Wettschereck, D., Aha, D. W. & Mohri, T. </author> <year> (1997). </year> <title> A review and empirical comparison of feature weighting methods for a class of lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <pages> 11(1-5), 273-314. </pages>
Reference: <author> Wettschereck, D., & Dietterich, T. G. </author> <year> (1995). </year> <title> An experimental comparison of the nearest neighbor and nearest hyperrectangle algorithms. </title> <journal> Machine Learning, </journal> <volume> 19, </volume> <pages> 5-28. </pages>
Reference: <author> Widmer, G. </author> <year> (1993). </year> <title> Combining knowledge-based and instance-based learning to exploit qualitative knowledge. </title> <journal> Informatica, </journal> <volume> 17, </volume> <pages> 371-385. </pages> <address> 22 Wilson, D. </address> <year> (1972). </year> <title> Asymptotic properties of nearest neighbor rules using edited data. </title> <journal> Institute of Electrical and Electronic Engineers Transactions on Systems, Man and Cybernetics, </journal> <volume> 2, </volume> <pages> 408-421. </pages>
Reference: <author> Wilson, D. R., & Martinez, T. R. </author> <year> (1996). </year> <title> Instance-based learning with genetically derived attribute weights. </title> <booktitle> Proceedings of the International Conference on Artificial Intelligence, Expert Systems, and Neural Networks (pp. </booktitle> <pages> 11-14). </pages> <note> Unpublished manuscript. </note>
Reference: <author> Wilson, D. R., & Martinez, T. R. </author> <year> (1997a). </year> <title> Improved heterogeneous distance functions. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 6, </volume> <pages> 1-34. </pages>
Reference: <author> Wilson, D. R., & Martinez, T. R. </author> <year> (1997b). </year> <title> Instance pruning techniques. </title> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning (pp. </booktitle> <pages> 403-411). </pages> <address> Nashville, TN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Zhang, J. </author> <year> (1992). </year> <title> Selecting typical instances in instance-based learning. </title> <booktitle> In Proceedings of the Ninth International Machine Learning Conference (pp. </booktitle> <pages> 470-479). </pages> <address> Aberdeen, Scotland: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Zhang, J., & Michalski, R. S. </author> <year> (1995). </year> <title> An integration of rule induction and exemplar-based learning for graded concepts. </title> <journal> Machine Learning, </journal> <volume> 21(3), </volume> <pages> 235-268. </pages>
Reference: <author> Zhang, J., Yim, Y.-S., & Yang, J. </author> <year> (1997). </year> <title> Intelligent selection of instances for prediction functions in lazy learning algorithms. </title> <journal> Artificial Intelligence Review, </journal> <pages> 11(1-5), 175-192. 23 </pages>
Reference-contexts: In particular, disjunctive spanning is closely realted to Sebestyen's (1962) algorithm (i.e., but without pre-determined thresholds), and IB2 is a simplification of CNN (Hart, 1968). 2 Since these early efforts, lazy algorithms have undergone dramatic design enhancements. Researchers have studied several lazy learning topics, including * case selection <ref> (Zhang et al., 1997) </ref>, * concept distribution skew (Cardie & Howe, 1997), * concept shift (Salganicoff, 1997), * cost-sensitive learning (Tan & Schlimmer, 1990; Turney, 1993), * discretization (Ting, 1997; Wilson & Martinez, 1997a), 1 Several synonyms have been used to describe these algorithms. For example, these include fcase,exemplar,instance,memoryg-based flearning,reasoningg.
References-found: 128

