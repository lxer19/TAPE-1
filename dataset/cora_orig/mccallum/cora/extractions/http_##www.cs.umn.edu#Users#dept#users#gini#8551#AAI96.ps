URL: http://www.cs.umn.edu/Users/dept/users/gini/8551/AAI96.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/gini/8551/
Root-URL: http://www.cs.umn.edu
Email: pstone@cs.cmu.edu veloso@cs.cmu.edu  
Phone: (412) 268-7123 (412) 268-8464  
Title: A Layered Approach to Learning Client Behaviors in the RoboCup Soccer Server  
Author: Peter Stone Manuela Veloso 
Date: April 1, 1997  
Address: Pittsburgh PA 15213-3890  
Affiliation: Computer Science Department Carnegie Mellon University  
Web: http://www.cs.cmu.edu/pstone http://www.cs.cmu.edu/mmv  
Abstract: In the past few years, Multiagent Systems (MAS) has emerged as an active subfield of Artificial Intelligence (AI). Because of the inherent complexity of MAS, there is much interest in using Machine Learning (ML) techniques to help build multiagent systems. Robotic soccer is a particularly good domain for studying MAS and Multiagent Learning. Our approach to using ML as a tool for building Soccer Server clients involves layering increasingly complex learned behaviors. In this article, we describe two levels of learned behaviors. First, the clients learn a low-level individual skill that allows them to control the ball effectively. Then, using this learned skill, they learn a higher-level skill that involves multiple players. For both skills, we describe the learning method in detail and report on our extensive empirical testing. We also verify empirically that the learned skills are applicable to game situations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> AAAI. </author> <title> Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. Sandip SenChair. </note>
Reference-contexts: Focussing on how AI agents' behaviors can and do interact, MAS applies to a variety of frameworks ranging from information agents to real robots. Because of the inherent complexity of MAS, there is much interest in using Machine Learning (ML) techniques to help deal with this complexity <ref> [1, 26] </ref>. Robotic soccer is a particularly good domain for studying MAS. It has been gaining popularity in recent years, with international competitions, namely RoboCup and MIROSOT, planned for the near future [22, 9].
Reference: [2] <author> M. Asada, S. Noda, and K. Hosoda. </author> <title> Action-based sensor space categorization for robot learning. </title> <booktitle> In Proc. of IEEE/RSJ International Conference on Intelligent Robots and Systems 1996 (IROS '96), </booktitle> <pages> pages 15021509, </pages> <year> 1996. </year>
Reference-contexts: Furthermore, it is in keeping with our goal of building up learning clients by layering learned behaviors on top of each other. 1 It was similarly noticed in <ref> [2] </ref> that the ball's angle is more useful for learning than is the ball's distance. 10 Defender Behavior Saves (%) Goals (%) Saves/(Goals+Saves) (%) NN 86 9 91 Lookup Table 83 8 91 Analytic 82 13 86 Table 1: The defender's performance when using a NN, a one-dimensional lookup table, and
Reference: [3] <author> M. Asada, S. Noda, S. Tawaratumida, and K. Hosoda. </author> <title> Purposive behavior acquisition for a real robot by vision-based reinforcement learning. </title> <booktitle> Machine Learning, </booktitle> <address> 23:279303, </address> <year> 1996. </year>
Reference-contexts: We also hope to do minimal hard-wiring, instead learning behaviors from the bottom up. The Robotic Soccer system being developed in Asada's lab is very different from both the Dynamo system and from our own <ref> [3, 25] </ref>. Asada's robots are larger and are equipped with on-board sensing capabilities. They have been used to develop some low-level behaviors such as shooting and avoiding as well as a RL technique for combining behaviors [3, 25]. <p> Asada's lab is very different from both the Dynamo system and from our own <ref> [3, 25] </ref>. Asada's robots are larger and are equipped with on-board sensing capabilities. They have been used to develop some low-level behaviors such as shooting and avoiding as well as a RL technique for combining behaviors [3, 25]. While the goals of this research are very similar to our own, the approach is different.
Reference: [4] <author> Rodney A. Brooks. </author> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-2:1423, </volume> <year> 1986. </year>
Reference-contexts: The work described in this article uses Neural Networks and Decision Trees to learn different behaviors in the RoboCup Soccer Server. A wide variety of MAS research is related to the layered learning approach espoused in this paper. Most significantly, Mataric uses Brooks' Subsumption Architecture <ref> [4] </ref> to build multiagent behaviors on top of a set of learned basis behaviors [10]. Mataric's basis behaviors are chosen to be necessary and sufficient for the learning task, while remaining as simple and robust as possible.
Reference: [5] <author> Roger Ford, Craig Boutilier, and Keiji Kanazawa. </author> <title> Exploiting natural structure in reinforcement learning: Experience in robot soccer-playing, 1994. </title> <type> Unpublished Manuscript. </type>
Reference-contexts: Several researchers have previously used simulated robotic soccer to study ML applications. Using the Dynasim soccer simulator [16, 17], Ford et al. used a Reinforcement Learning (RL) approach with sensory predicates to learn to choose among low-level behaviors <ref> [5] </ref>. Using a simulator based closely upon the Dynasim system, Stone and Veloso used Memory-based Learning to allow a player to learn when to shoot and when to pass the ball [19]. They then used Neural Networks to teach a player to shoot a moving ball into the goal [21].
Reference: [6] <author> John Grefenstette and Robert Daley. </author> <title> Methods for competitive and cooperative co-evolution. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 4550, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: First, if the adversaries are allowed to continually adjust to each other, they may evolve increasingly complex behaviors with no net advantage to either side. This potential stumbling block in competitive coevolution has been identified and addressed by several researchers who work with genetic algorithms <ref> [6, 7, 15] </ref>. Second, since a robotic soccer team must be able to play against many different opponents, often for only a single match, it must be able to adapt quickly to opponent behaviors without permanently harming performance against other opponents.
Reference: [7] <author> Thomas Haynes and Sandip Sen. </author> <title> Evolving behavioral strategies in predators and prey. </title> <editor> In Ger-hard Weiand Sandip Sen, editors, </editor> <booktitle> Adaptation and Learning in Multiagent Systems, </booktitle> <pages> pages 113126. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: First, if the adversaries are allowed to continually adjust to each other, they may evolve increasingly complex behaviors with no net advantage to either side. This potential stumbling block in competitive coevolution has been identified and addressed by several researchers who work with genetic algorithms <ref> [6, 7, 15] </ref>. Second, since a robotic soccer team must be able to play against many different opponents, often for only a single match, it must be able to adapt quickly to opponent behaviors without permanently harming performance against other opponents.
Reference: [8] <author> Marcus J. Huber and Edmund H. Durfee. </author> <title> Deciding when to commit to action during observation-based coordination. </title> <booktitle> In Proceedings of the First International Conference on Multi-Agent Systems (ICMAS-95), </booktitle> <pages> pages 163170, </pages> <address> Menlo Park, California, June 1995. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This work enhances previous work that aims at having agents deduce other agents' intentions through observation <ref> [8] </ref>. 4 The Simulator Extensive experimentation of the type described in this article is not feasible with physical robotic systems. Consequently, to conduct meaningful research in simulation that might apply to the real world, a well-designed simulator is needed.
Reference: [9] <author> Hiroaki Kitano, Yasuo Kuniyoshi, Itsuki Noda, Minoru Asada, Hitoshi Matsubara, and Ei-Ichi Osawa. </author> <title> Robocup: A challenge problem for ai. </title> <journal> AI Magazine, </journal> <volume> 18(1):7385, </volume> <month> Spring </month> <year> 1997. </year>
Reference-contexts: Robotic soccer is a particularly good domain for studying MAS. It has been gaining popularity in recent years, with international competitions, namely RoboCup and MIROSOT, planned for the near future <ref> [22, 9] </ref>. Robotic soccer can be used as a standard testbed to evaluate different MAS techniques in a straightforward manner: teams implemented with different techniques can play against each other. <p> We believe that both approaches are valuable for advancing the state of the art of robotic soccer research. Although real robotic systems, such as those mentioned above and the many new ones being built for robotic soccer tournaments <ref> [22, 9] </ref>, are needed for studying certain robotic issues, it is often possible to conduct research more efficiently in a well-designed simulator. Several researchers have previously used simulated robotic soccer to study ML applications.
Reference: [10] <editor> Maja J. Mataric. </editor> <title> Designing and understanding adaptive group behavior. Adaptive Behavior, </title> <type> 4(1), </type> <month> December </month> <year> 1995. </year>
Reference-contexts: A wide variety of MAS research is related to the layered learning approach espoused in this paper. Most significantly, Mataric uses Brooks' Subsumption Architecture [4] to build multiagent behaviors on top of a set of learned basis behaviors <ref> [10] </ref>. Mataric's basis behaviors are chosen to be necessary and sufficient for the learning task, while remaining as simple and robust as possible.
Reference: [11] <author> Hitoshi Matsubara, Itsuki Noda, and Kazuo Hiraki. </author> <title> Learning of cooperative actions in multi-agent systems: a case study of pass play in soccer. In Adaptation, Coevolution and Learning in Multiagent 18 Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 6367, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: They then used Neural Networks to teach a player to shoot a moving ball into the goal [21]. In the RoboCup Soccer Server Matsubar et al. used a Neural Network to allow a player to learn when to shoot and when to pass <ref> [11] </ref> (as opposed to the Memory-based technique used by Stone and Veloso for a simlar task). The work described in this article uses Neural Networks and Decision Trees to learn different behaviors in the RoboCup Soccer Server.
Reference: [12] <editor> Itsuki Noda and Hitoshi Matsubara. </editor> <booktitle> Soccer server and researches on multi-agent systems. In Proceedings of the IROS-96 Workshop on RoboCup, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: A wide variety of MAS issues can be studied in robotic soccer [20]. In this article, we focus on the multiagent learning opportunities that arise in Noda's Soccer Server <ref> [12] </ref>. The properties of simulated robotic soccer that make it a good testbed for MAS include: fl This research is sponsored in part by the DARPA/RL Knowledge Based Planning and Scheduling Initiative under grant number F30602-95-1-0018. <p> Consequently, to conduct meaningful research in simulation that might apply to the real world, a well-designed simulator is needed. Though not directly based upon any single robotic system, Noda's Soccer Server <ref> [12] </ref>, pictured in Figure 3, captures enough real-world complexities to be an indispensable tool.
Reference: [13] <author> M V Nagendra Prasad, Victor R. Lesser, and Susan E. Lander. </author> <title> Learning organizational roles in a heterogeneous multi-agent system. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pages 7277, </pages> <address> Menlo Park,CA, </address> <month> March </month> <year> 1996. </year> <note> AAAI Press. AAAI Technical Report SS-96-01. </note>
Reference-contexts: Tambe discusses a framework in which agents can take over the roles of other teammates in a helicopter-combat domain [23]. In the learning context, Prasad et al. have created design agents that can learn which role to fill <ref> [13] </ref>. We plan to combine role learning with dynamic role assumption as we progress to higher levels of learned behaviors (see Section 7). In addition to reasoning about roles of teammates, Tambe's combat agents can also reason about the roles that opponents are playing in team behaviors [24].
Reference: [14] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The goal of learning is to use these attributes to predict whether a pass to the given receiver will lead to a SUCCESS, a FAILURE, or a MISS. For training, we used standard off-the-shelf C4.5 code with all of the default parameters <ref> [14] </ref>. We gathered a total of 5000 training examples, 51% of which were successes, 42% of which were failures, and 7% of which were misses. Training on this data produced a pruned tree with 87 nodes giving 26% error on the training set. The tree is shown in Figure 14.
Reference: [15] <author> Christopher D. Rosin and Richard K. Belew. </author> <title> Methods for competitive co-evolution: Finding opponents worth beating. </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 373380, </pages> <address> San Mateo,CA, July 1995. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: First, if the adversaries are allowed to continually adjust to each other, they may evolve increasingly complex behaviors with no net advantage to either side. This potential stumbling block in competitive coevolution has been identified and addressed by several researchers who work with genetic algorithms <ref> [6, 7, 15] </ref>. Second, since a robotic soccer team must be able to play against many different opponents, often for only a single match, it must be able to adapt quickly to opponent behaviors without permanently harming performance against other opponents.
Reference: [16] <author> Michael Sahota. </author> <title> Dynasim user guide. </title> <note> Available at http://www.cs.ubc.ca/nest/lci/soccer, January 1996. </note>
Reference-contexts: Several researchers have previously used simulated robotic soccer to study ML applications. Using the Dynasim soccer simulator <ref> [16, 17] </ref>, Ford et al. used a Reinforcement Learning (RL) approach with sensory predicates to learn to choose among low-level behaviors [5].
Reference: [17] <author> Michael K. Sahota. </author> <title> Real-time intelligent behaviour in dynamic environments: Soccer-playing robots. </title> <type> Master's thesis, </type> <institution> University of British Columbia, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: This system was designed to be capable of supporting several robots per team, but most work has been done in a 1 vs. 1 scenario. Sahota used this system to introduce a decision making strategy called reactive deliberation which was used to choose from among seven hard-wired behaviors <ref> [17] </ref>. <p> Several researchers have previously used simulated robotic soccer to study ML applications. Using the Dynasim soccer simulator <ref> [16, 17] </ref>, Ford et al. used a Reinforcement Learning (RL) approach with sensory predicates to learn to choose among low-level behaviors [5].
Reference: [18] <author> Michael K. Sahota, Alan K. Mackworth, Rod A. Barman, and Stewart J. Kingdon. </author> <title> Real-time control of soccer-playing robots using off-board vision: the dynamite testbed. </title> <booktitle> In IEEE International Conference on Systems, Man, and Cybernetics, </booktitle> <pages> pages 36903663, </pages> <year> 1995. </year>
Reference-contexts: 2 (b)) that we hope to enhance with the aid of the simulator client behaviors described in this article. 3 Related Work A ground-breaking system for Robotic Soccer, and the one that served as the inspiration for our work, is the Dynamo System developed at the University of British Columbia <ref> [18] </ref>. This system was designed to be capable of supporting several robots per team, but most work has been done in a 1 vs. 1 scenario. Sahota used this system to introduce a decision making strategy called reactive deliberation which was used to choose from among seven hard-wired behaviors [17].
Reference: [19] <author> Peter Stone and Manuela Veloso. </author> <title> Beating a defender in robotic soccer: Memory-based learning of a continuous function. </title> <editor> In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 896902, </pages> <address> Cambridge, MA, 1996. </address> <publisher> MIT press. </publisher>
Reference-contexts: Using a simulator based closely upon the Dynasim system, Stone and Veloso used Memory-based Learning to allow a player to learn when to shoot and when to pass the ball <ref> [19] </ref>. They then used Neural Networks to teach a player to shoot a moving ball into the goal [21].
Reference: [20] <author> Peter Stone and Manuela Veloso. </author> <title> Multiagent systems: A survey from a machine learning perspective. </title> <note> Submitted to Journal of Artificial Intelligence Research (JAIR), </note> <month> February </month> <year> 1997. </year>
Reference-contexts: 1 Introduction In the past few years, Multiagent Systems (MAS) has emerged as an active subfield of Artificial Intelligence (AI) <ref> [20] </ref>. Focussing on how AI agents' behaviors can and do interact, MAS applies to a variety of frameworks ranging from information agents to real robots. Because of the inherent complexity of MAS, there is much interest in using Machine Learning (ML) techniques to help deal with this complexity [1, 26]. <p> The main goal of any testbed is to facilitate the trial and evaluation of ideas that have promise in the real world. A wide variety of MAS issues can be studied in robotic soccer <ref> [20] </ref>. In this article, we focus on the multiagent learning opportunities that arise in Noda's Soccer Server [12]. <p> S. Government. 1 * Enough complexity to be realistic; * Easy accessibility to researchers worldwide; * Embodiment of most MAS issues, including <ref> [20] </ref>; Ability to support reactive or deliberative agents Need for agents to model other agents Need for agents to affect each other Room for both cooperative and competitive agents Possibility for stable or evolving agents Need for resource management (stamina) Need for social conventions Opportunity for agents to fill different roles
Reference: [21] <author> Peter Stone and Manuela M. Veloso. </author> <title> Towards collaborative and adversarial learning: A case study in robotic soccer. </title> <note> To appear in International Journal of Human-Computer Systems (IJHCS), </note> <year> 1997. </year>
Reference-contexts: Using a simulator based closely upon the Dynasim system, Stone and Veloso used Memory-based Learning to allow a player to learn when to shoot and when to pass the ball [19]. They then used Neural Networks to teach a player to shoot a moving ball into the goal <ref> [21] </ref>. In the RoboCup Soccer Server Matsubar et al. used a Neural Network to allow a player to learn when to shoot and when to pass [11] (as opposed to the Memory-based technique used by Stone and Veloso for a simlar task). <p> The low-level skill we identified as being most essential to our Soccer Server clients was the ability to intercept a moving ball. This skill is ubiquitous in all soccer-type frameworks as indicated by the fact that we taught clients a similar skill in a different simulator <ref> [21] </ref>. Intercepting a moving ball is considerably more difficult than moving to a stationary ball both because of the ball's unpredictable movement (due to simulator noise) and because the client may need to turn and move in such a direction that it cannot see the ball (see Figure 5).
Reference: [22] <author> Peter Stone, Manuela M. Veloso, and Sorin Achim. </author> <title> Collaboration and learning in robotic soccer. </title> <booktitle> In Proceedings of the Micro-Robot World Cup Soccer Tournament, </booktitle> <address> Taejon, Korea, </address> <month> November </month> <year> 1996. </year> <journal> IEEE Robotics and Automation Society. </journal>
Reference-contexts: Robotic soccer is a particularly good domain for studying MAS. It has been gaining popularity in recent years, with international competitions, namely RoboCup and MIROSOT, planned for the near future <ref> [22, 9] </ref>. Robotic soccer can be used as a standard testbed to evaluate different MAS techniques in a straightforward manner: teams implemented with different techniques can play against each other. <p> We believe that both approaches are valuable for advancing the state of the art of robotic soccer research. Although real robotic systems, such as those mentioned above and the many new ones being built for robotic soccer tournaments <ref> [22, 9] </ref>, are needed for studying certain robotic issues, it is often possible to conduct research more efficiently in a well-designed simulator. Several researchers have previously used simulated robotic soccer to study ML applications.
Reference: [23] <author> Milind Tambe. </author> <title> Teamwork in real-world, dynamic environments. </title> <booktitle> In Proceedings of the Second International Conference on Multi-Agent Systems (ICMAS-96), </booktitle> <address> Menlo Park, California, </address> <month> December </month> <year> 1996. </year> <note> AAAI Press. </note>
Reference-contexts: Although the roles are fixed in the current implementation, the players will eventually need to change roles as a match progresses. Tambe discusses a framework in which agents can take over the roles of other teammates in a helicopter-combat domain <ref> [23] </ref>. In the learning context, Prasad et al. have created design agents that can learn which role to fill [13]. We plan to combine role learning with dynamic role assumption as we progress to higher levels of learned behaviors (see Section 7).
Reference: [24] <author> Milind Tambe. </author> <title> Tracking dynamic team activity. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> Menlo Park, California, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: We plan to combine role learning with dynamic role assumption as we progress to higher levels of learned behaviors (see Section 7). In addition to reasoning about roles of teammates, Tambe's combat agents can also reason about the roles that opponents are playing in team behaviors <ref> [24] </ref>. By recognizing an opponent's action as a part of a larger team action, an agent is able to more easily make sense of the individual opponent's behavior with the goal of being able to predict the opponent's future actions.
Reference: [25] <author> Eiji Uchibe, Minoru Asada, and Koh Hosoda. </author> <title> Behavior coordination for a mobile robot using modular reinforcement learning. </title> <booktitle> In Proc. of IEEE/RSJ International Conference on Intelligent Robots and Systems 1996 (IROS '96), </booktitle> <pages> pages 13291336, </pages> <year> 1996. </year>
Reference-contexts: We also hope to do minimal hard-wiring, instead learning behaviors from the bottom up. The Robotic Soccer system being developed in Asada's lab is very different from both the Dynamo system and from our own <ref> [3, 25] </ref>. Asada's robots are larger and are equipped with on-board sensing capabilities. They have been used to develop some low-level behaviors such as shooting and avoiding as well as a RL technique for combining behaviors [3, 25]. <p> Asada's lab is very different from both the Dynamo system and from our own <ref> [3, 25] </ref>. Asada's robots are larger and are equipped with on-board sensing capabilities. They have been used to develop some low-level behaviors such as shooting and avoiding as well as a RL technique for combining behaviors [3, 25]. While the goals of this research are very similar to our own, the approach is different.
Reference: [26] <author> Gerhard Wei and Sandip Sen, </author> <title> editors. Adaptation and Learning in Multiagent Systems. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year> <month> 19 </month>
Reference-contexts: Focussing on how AI agents' behaviors can and do interact, MAS applies to a variety of frameworks ranging from information agents to real robots. Because of the inherent complexity of MAS, there is much interest in using Machine Learning (ML) techniques to help deal with this complexity <ref> [1, 26] </ref>. Robotic soccer is a particularly good domain for studying MAS. It has been gaining popularity in recent years, with international competitions, namely RoboCup and MIROSOT, planned for the near future [22, 9].
References-found: 26

