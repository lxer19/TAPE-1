URL: http://www.cs.rice.edu:80/~sethi/hipc96.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~sethi/publications.html
Root-URL: 
Email: ken@rice.edu sethi@rice.edu  
Title: A Communication Placement Framework with Unified Dependence and Data-flow Analysis  
Author: Ken Kennedy Ajay Sethi 
Address: Houston, TX 77005  
Affiliation: Department of Computer Science Rice University,  
Abstract: Communication placement analysis is an important step in the compilation of data-parallel programs for multiprocessor systems. This paper presents a communication placement framework that minimizes frequency of communication, eliminates redundant communication, and maximizes communication latency hiding. The paper shows how data-dependence information can be combined with data-flow analysis to devise simpler and cleaner data-flow problems. It shows how to develop equations for balanced communication placement using a set of uni-directional analyses - with an independent equation system for each placement criterion. This structure allows the framework to support vector message pipelining an important optimization for programs with loop-carried dependences but, that was not supported by any previous data-flow framework. The paper also describes how other optimizations, such as partially redundant communication elimination and message coalescing, are supported by the framework. Finally, the paper presents experimental results to prove the efficacy of our placement analysis. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: Section 8 compares our framework with previous work and Section 9 concludes by summarizing our framework. 2 Preliminaries This section briefly summarizes the graph structure and the predicates used by our framework. 2.1 Interval-flow graph Our communication placement framework (Section 3) is based on interval analysis <ref> [1] </ref>. Interval analysis incorporates program structure into data-flow equations to enable their non-iterative and efficient solution. However, unlike classical interval analysis, we do not construct a sequence of interval graphs by recursively collapsing intervals into single nodes. <p> As shown in Figure 5, the movement of Send z [i-1] from node 6 to node 7 in Figure 3 inserts Send z <ref> [1] </ref> at node 4 and Recv z [256] at node 9. It should be stressed that though in Figure 5, we achieve only fine-grain pipelining, pipelining can often be combined with message vectorization.
Reference: [2] <author> S. Amarasinghe and M. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The problem of data-flow based communication place-ment has been addressed by several researchers. Amaras-inghe and Lam use a last write tree to optimize communication placement and support VMP within a single loop nest <ref> [2] </ref>. Moreover, they don't allow loops within conditionals. Granston and Veidenbaum combine PRE and dependence analysis to eliminate redundant monolithic global-memory accesses across loop nests in the presence of conditionals [4].
Reference: [3] <author> C. Gong, R. Gupta, and R. Melhem. </author> <title> Compilation techniques for optimizing communication on distributed-memory systems. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Gong, Gupta, and Melhem present a data-flow framework to separate sends and receives by placing sends at "the earliest point at which the communication can be performed" <ref> [3] </ref>. However, their technique does not eliminate partially redundant communication, handles only singly-nested loops and one-dimensional arrays, and does not provide balanced communication placement. The unified communication optimization framework developed by Gupta, Schonberg, and Srinivasan adapts PRE for communication placement [5].
Reference: [4] <author> E. Granston and A. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Moreover, they don't allow loops within conditionals. Granston and Veidenbaum combine PRE and dependence analysis to eliminate redundant monolithic global-memory accesses across loop nests in the presence of conditionals <ref> [4] </ref>. Gong, Gupta, and Melhem present a data-flow framework to separate sends and receives by placing sends at "the earliest point at which the communication can be performed" [3].
Reference: [5] <author> M. Gupta, E. Schonberg, and H. Srinivasan. </author> <title> A unified data-flow framework for optimizing communication. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The important contributions of this paper are as follows. First, it presents a framework that incorporates data-dependence information in the placement analysis to simplify the data-flow equations. This approach differs from previous global data-flow techniques <ref> [5, 6, 8] </ref>, which use dependence information only to evaluate if different references have data-dependent (overlapping) communication sets. Second, the paper shows that the communication placement can be determined by a sequence of simple unidirectional analyses. <p> Also, the framework has independent equations to address separate concerns such as safety, balance, etc. Finally, the paper describes how our framework combines dependence distance information with data-flow analysis to perform vector message pipelining [7] efficiently. None of the previous data-flow placement frameworks <ref> [5, 6] </ref> supported this optimization. The paper also describes how communication optimizations such as message coalescing are supported in our framework. Unlike dependence-based placement techniques [7], our framework performs communication optimizations across loop nests. The paper is organized as follows. <p> Traditional dependence-based communication placement techniques performed VMP by placing Send immediately after the source of the dependence. But clearly, in the presence of conditionals, balance cannot be guaranteed with dependence-based placement. On the other hand, none of the previous data-flow based communication placement frameworks <ref> [5, 6] </ref> provided support for VMP. We exploit the knowledge about the graph structure and combine it with the equations for Send placement to support VMP. <p> However, their technique does not eliminate partially redundant communication, handles only singly-nested loops and one-dimensional arrays, and does not provide balanced communication placement. The unified communication optimization framework developed by Gupta, Schonberg, and Srinivasan adapts PRE for communication placement <ref> [5] </ref>. Their framework does not perform any analysis to ensure balanced placement of Send and (blocking) Recv primitives. To ensure correctness, it initiates non-blocking receives immediately after the sends and uses a wait primitive before every non-local reference to block the statement from executing till the data are received.
Reference: [6] <author> R. v. Hanxleden and K. Kennedy. </author> <title> Give-N-Take | A balanced code placement framework. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Moreover, the framework ensures that every program execution path contains matching Send and Recv pairs; that is, it ensures that Sends and Recvs are balanced <ref> [6] </ref>. The important contributions of this paper are as follows. First, it presents a framework that incorporates data-dependence information in the placement analysis to simplify the data-flow equations. <p> The important contributions of this paper are as follows. First, it presents a framework that incorporates data-dependence information in the placement analysis to simplify the data-flow equations. This approach differs from previous global data-flow techniques <ref> [5, 6, 8] </ref>, which use dependence information only to evaluate if different references have data-dependent (overlapping) communication sets. Second, the paper shows that the communication placement can be determined by a sequence of simple unidirectional analyses. <p> Also, the framework has independent equations to address separate concerns such as safety, balance, etc. Finally, the paper describes how our framework combines dependence distance information with data-flow analysis to perform vector message pipelining [7] efficiently. None of the previous data-flow placement frameworks <ref> [5, 6] </ref> supported this optimization. The paper also describes how communication optimizations such as message coalescing are supported in our framework. Unlike dependence-based placement techniques [7], our framework performs communication optimizations across loop nests. The paper is organized as follows. <p> An interval is just a collection of nodes; it is defined formally below. While solving data-flow equations, we summarize appropriate information for an interval and record it in the interval header. Our interval-flow graph is similar to that used by the Give-N-Take framework <ref> [6] </ref>; it differs in the way critical edges are eliminated (see below). The important properties of the interval-flow graph, G = (N; E), are as follows. First, G is reducible; that is, each loop has a unique header node. <p> Finally, for every non-empty interval T (h), there exists a unique g 2 T (h) such that (g; h) 2 E; that is, there is only one back edge out of T (h). This is achieved by adding a post-body node to T (h) <ref> [6] </ref>. We will use the interval-flow graph shown in Figure 1 to illustrate our communication placement framework. Though not shown in the figure, the graph is implicitly partitioned into intervals; each loop body in Figure 1 corresponds to an interval. <p> Let Succs (n) and Preds (n) correspond to the set of successor and predecessor nodes of n. Succs F (n) and Succs E (n) then correspond to the forward and entry edge successors of n. Additionally, the edges induce the following traversal orders over G <ref> [6] </ref>: given a forward edge (m; n), a Forward order visits m before n, and a Backward order visits m after n. <p> Therefore, the complexity of our algorithm is O (E). It has been noted by several researchers that for typical programs, the average out-degree of graph nodes and the maximal loop nesting depth can be assumed to be bounded by a small constant independent of the size of the program <ref> [13, 6] </ref>. Under the assumption that O (E) is the same as the order of the program size, our algorithm has linear time complexity. The placement analysis is performed on the interval-flow graph of the original sequential program. Before performing the analysis, the interval-flow graph is canonicalized in two ways. <p> Traditionally, hoisting computation out of a loop with unknown bounds is considered unsafe because if the loop body is not executed, it introduces a new value on some execution path (s) of the program. However, since hoisting communication out of a loop can only cause overcommunication <ref> [6] </ref>, we relax the safety criterion to hoist communication out of potentially zero-trip loops as follows. For header node h, not only the communication placed at the forward edge successor (s) of h can be hoisted across T (h), the communication can also be hoisted out of T (h). <p> Traditional dependence-based communication placement techniques performed VMP by placing Send immediately after the source of the dependence. But clearly, in the presence of conditionals, balance cannot be guaranteed with dependence-based placement. On the other hand, none of the previous data-flow based communication placement frameworks <ref> [5, 6] </ref> provided support for VMP. We exploit the knowledge about the graph structure and combine it with the equations for Send placement to support VMP. <p> The Lazy Code Motion (LCM) technique [10] and the Give-N-Take framework <ref> [6] </ref> improved partial redundancy elimination optimization by excluding unnecessary code motion and by providing non-atomic placement regions, respectively. Our framework is based on both these frameworks.
Reference: [7] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albu-querque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: Also, the framework has independent equations to address separate concerns such as safety, balance, etc. Finally, the paper describes how our framework combines dependence distance information with data-flow analysis to perform vector message pipelining <ref> [7] </ref> efficiently. None of the previous data-flow placement frameworks [5, 6] supported this optimization. The paper also describes how communication optimizations such as message coalescing are supported in our framework. Unlike dependence-based placement techniques [7], our framework performs communication optimizations across loop nests. The paper is organized as follows. <p> describes how our framework combines dependence distance information with data-flow analysis to perform vector message pipelining <ref> [7] </ref> efficiently. None of the previous data-flow placement frameworks [5, 6] supported this optimization. The paper also describes how communication optimizations such as message coalescing are supported in our framework. Unlike dependence-based placement techniques [7], our framework performs communication optimizations across loop nests. The paper is organized as follows. Section 2 introduces the graph structure and predicates used by our framework. Section 3 presents an overview of our framework. Sections 4 and 5 present equations for Send and Recv placement. <p> The message coalescing optimization avoids redundant communication by combining communication primitives with partly or completely overlapping data sections <ref> [7] </ref>. We now describe how these optimizations are incorporated in our framework. As explained in Section 2.1, Send x [i] is partially redundant along the right branch. <p> The optimization that hoists communication to the outermost placement level is known as message vectorization <ref> [7] </ref>. The placement that combines message vectorization with message pipelining is known as vector message pipelining (VMP) [7]. Traditional dependence-based communication placement techniques performed VMP by placing Send immediately after the source of the dependence. But clearly, in the presence of conditionals, balance cannot be guaranteed with dependence-based placement. <p> The optimization that hoists communication to the outermost placement level is known as message vectorization <ref> [7] </ref>. The placement that combines message vectorization with message pipelining is known as vector message pipelining (VMP) [7]. Traditional dependence-based communication placement techniques performed VMP by placing Send immediately after the source of the dependence. But clearly, in the presence of conditionals, balance cannot be guaranteed with dependence-based placement. <p> No overlap Overlap Diff. 32 138.82 130.46 6.02 8 365.04 344.89 5.52 128 x 128 x 128 16 143.14 135.94 5.03 32 - 8 58.860 57.432 2.42 Table 1: Timings in milliseconds for Erlebacher. before <ref> [7] </ref>, we restrict our analysis to the performance gains due to latency hiding achieved by non-atomic placement of Sends and Recvs. For Erlebacher, we used the Fortran D95 compiler to translate it into SPMD node code with the communication placement that achieves latency hiding.
Reference: [8] <author> K. Kennedy and N. Nedeljkovic. </author> <title> Combining dependence and data-flow analyses to optimize communication. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <address> Santa Barbara, CA, </address> <month> April </month> <year> 1995. </year>
Reference-contexts: The important contributions of this paper are as follows. First, it presents a framework that incorporates data-dependence information in the placement analysis to simplify the data-flow equations. This approach differs from previous global data-flow techniques <ref> [5, 6, 8] </ref>, which use dependence information only to evaluate if different references have data-dependent (overlapping) communication sets. Second, the paper shows that the communication placement can be determined by a sequence of simple unidirectional analyses.
Reference: [9] <author> K. Kennedy and A. Sethi. </author> <title> Resource-based communication placement analysis. </title> <booktitle> In Proceedings of the Ninth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> San Jose, CA, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Moreover, independent Send and Recv phases allow us to support VMP. Finally, the compositional structure of our analysis allows us to incorporate machine-dependent resource constraints such as cache size and buffer size into the placement analysis <ref> [9] </ref>. The problem of data-flow based communication place-ment has been addressed by several researchers. Amaras-inghe and Lam use a last write tree to optimize communication placement and support VMP within a single loop nest [2]. Moreover, they don't allow loops within conditionals. <p> Two independent phases simplify the implementation of message coalescing optimization. Moreover, and perhaps more significantly, the compositional structure of our placement analysis allows us to take machine-dependent resource constraints such as cache size and buffer size into account <ref> [9] </ref>. It allows us to perform resource analysis between Send and Recv placement phases as well as to use resource constraints to influence the Send placement.
Reference: [10] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Optimal code motion: </title> <journal> Theory and practice. ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(4) </volume> <pages> 1117-1155, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: A Tarjan interval has a unique header node h, where h 62 T (h). Third, there are no critical edges. A critical edge connects a node with multiple successors to a node with multiple predecessors <ref> [10] </ref>. Critical edges are eliminated by splitting edges as follows: every edge leading to a node with more than one predecessor is split by inserting a synthetic node. <p> Thus, the initialization causes communication to be placed inside the loop, as required. In traditional code motion techniques, placement of computation at a node is considered safe if the computed value is used along all terminating paths from the node <ref> [13, 10] </ref>. <p> it either (a) contains an use of the communicated data (that is, Used (n; d) = &gt;), or (b) does not modify the data being sent (that is, Transp (n; d) = &gt;) and all its successors are safe (that is, " s2Succs F (n) SAFE (s; d) = &gt;) <ref> [10] </ref>. In terms 1 The framework places send primitives only at the node start. of data-flow equation: SAFE (n; d) = SAFE (n; d) " [Used (n; d) [ (Transp (n; d) " " s2Succs F (n) SAFE (s; d))] Succs F is as defined in Section 2.2. <p> The Lazy Code Motion (LCM) technique <ref> [10] </ref> and the Give-N-Take framework [6] improved partial redundancy elimination optimization by excluding unnecessary code motion and by providing non-atomic placement regions, respectively. Our framework is based on both these frameworks.
Reference: [11] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: In addition, the overhead can be reduced by eliminating redundant communication and, for the communication that cannot be eliminated, by hiding communication latency. This paper describes a placement framework that minimizes communication overhead. The framework has been implemented in the Rice Fortran D95 compiler (a High Performance Fortran <ref> [11] </ref> compiler) where it is used to determine the placement of communication primitives (Send and Recv) for distributed-memory systems. The framework decreases communication frequency by hoisting communication to the outermost placement location. Latency is hidden by determining a non-atomic placement of Send and Recv primitives.
Reference: [12] <author> U. Kremer and M. </author> <title> Rame. Compositional oil reservoir simulation in Fortran D: A feasibility study on Intel iPSC/860. </title> <journal> International Journal of Supercomputer Applications and High Performance Computing, </journal> <volume> 8(2), </volume> <month> Summer </month> <year> 1994. </year>
Reference-contexts: Disper is a 1500 line stencil computation from the UT-COMP reservoir simulator <ref> [12] </ref>. Unlike Erlebacher, Dis-per is highly data-parallel program. However, it has a lot of conditionals nested inside loop nests. Thus, it requires data-flow analysis to ensure that every program execution path contains balanced communication placement. <p> Thus, it requires data-flow analysis to ensure that every program execution path contains balanced communication placement. In order to derive any benefits, communication needs to be moved out of the conditionals possibly resulting in over-communication; but this allows communication to be hoisted out of loops <ref> [12] </ref>. In other words, Disper requires the safety criterion presented in Section 4 to be relaxed. However, since the safety has an independent data-flow equation, the equation can be changed easily.
Reference: [13] <author> E. Morel and C. </author> <title> Renvoise. Global optimization by suppression of partial redundancies. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 96-103, </pages> <month> February </month> <year> 1979. </year>
Reference-contexts: Therefore, the complexity of our algorithm is O (E). It has been noted by several researchers that for typical programs, the average out-degree of graph nodes and the maximal loop nesting depth can be assumed to be bounded by a small constant independent of the size of the program <ref> [13, 6] </ref>. Under the assumption that O (E) is the same as the order of the program size, our algorithm has linear time complexity. The placement analysis is performed on the interval-flow graph of the original sequential program. Before performing the analysis, the interval-flow graph is canonicalized in two ways. <p> Thus, the initialization causes communication to be placed inside the loop, as required. In traditional code motion techniques, placement of computation at a node is considered safe if the computed value is used along all terminating paths from the node <ref> [13, 10] </ref>. <p> The benchmark programs presented in this section require communication placement that hides latency (either with Send/Recv separation or with VMP in presence of conditionals) and, thus, demonstrate the usefulness of our framework. 8 Related work Code motion and placement to eliminate partially redundant computations was introduced by the PRE algorithm <ref> [13] </ref>. The Lazy Code Motion (LCM) technique [10] and the Give-N-Take framework [6] improved partial redundancy elimination optimization by excluding unnecessary code motion and by providing non-atomic placement regions, respectively. Our framework is based on both these frameworks.
Reference: [14] <author> R. E. Tarjan. </author> <title> Testing flow graph reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 355-365, </pages> <year> 1974. </year>
Reference-contexts: First, G is reducible; that is, each loop has a unique header node. In this paper we assume that G corresponds to a structured program; however, the placement framework can be extended to handle jumps out of loops. Second, G uses Tarjan intervals <ref> [14] </ref>, where a Tarjan interval T (h) is a set of control-flow nodes that correspond to a loop body in the program text. A Tarjan interval has a unique header node h, where h 62 T (h). Third, there are no critical edges.
Reference: [15] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Remote access in hierarchical multiprocessor systems is orders of magnitude slower than access to a processor's local memory. Thus, remote accesses add communication overhead to the total execution time. As the number of processors increase, communication overhead begins to play a crucial role in the total execution time <ref> [15] </ref>. Communication overhead can be reduced by decreasing the frequency of communication. In addition, the overhead can be reduced by eliminating redundant communication and, for the communication that cannot be eliminated, by hiding communication latency. This paper describes a placement framework that minimizes communication overhead.
References-found: 15

