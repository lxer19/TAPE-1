URL: ftp://ftp.cs.brown.edu/pub/techreports/94/cs94-23.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-94-23.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. B. Adams III, D. P. Agrawal and H. J. Seigel, </author> <title> A Survey and Comparison of Fault-tolerant Multistage Interconnection Networks, </title> <note> IEEE Computer 20 6 (1987) 14-29. </note>
Reference-contexts: A combining interconnection network that is well suited for implementing synchronous concurrent reads and writes is studied in [23] (the combining properties are used in their simplest form only to implement concurrent access to memory). The network can be made more reliable by employing redundancy <ref> [1] </ref>. 2.2. Measures of Efficiency The complexity measure used throughout this work is the available processor steps of [18]. It generalizes the fault-free Parallel-time fiProcessors product and accounts for all steps performed by the active processors. Definition 2.1. <p> We use the notation W CR=W [s] to indicate that input elements are organized into clusters of size s. For the case s = 1 we use the shorthand W CR=W for W CR=W <ref> [1] </ref>. The following theorem provides bounds on the performance of this special case: Theorem 3.6.
Reference: [2] <author> R. Anderson and H. Woll, </author> <title> Wait-Free Parallel Algorithms for the Union-Find Problem, </title> <booktitle> in: Proc. 23rd ACM STOC (1991) 370-380. </booktitle>
Reference-contexts: More general processor asynchrony has been examined in [2, 3, 4, 7, 8, 9, 11, 14, 20, 21, 22, 25, 26, 27]. Many of these analyses involve average processor behavior and use randomization. Some (e.g., <ref> [2, 7, 22] </ref>) deal with deterministic asynchronous computation (i.e., removing assumption (2) without using randomness). In this case the strongest lower bounds are in [22] and the tightest upper bounds are in [2]. A key primitive in much of the above mentioned work is the Write-All operation of [18]. <p> Many of these analyses involve average processor behavior and use randomization. Some (e.g., [2, 7, 22]) deal with deterministic asynchronous computation (i.e., removing assumption (2) without using randomness). In this case the strongest lower bounds are in [22] and the tightest upper bounds are in <ref> [2] </ref>. A key primitive in much of the above mentioned work is the Write-All operation of [18]. The main technical insight in this paper is an efficient, deterministic fault-tolerant Write-All algorithm that significantly reduces memory access concurrency.
Reference: [3] <author> Y. Aumann and M. O. Rabin, </author> <title> Clock Construction in Fully Asynchronous Parallel Systems and PRAM Simulation, </title> <booktitle> in: Proc. 34th IEEE FOCS (1992) 147-156. </booktitle>
Reference: [4] <author> Y. Aumann, Z. M. Kedem, K. V. Palem and M. O. Rabin, </author> <title> Highly Efficient Asynchronous Execution of Large-Grained Parallel Programs, </title> <booktitle> in: Proc. 35th IEEE FOCS (1993) 271-280. </booktitle>
Reference: [5] <author> P. Beame, M. Kik and M. Kutylowski, </author> <title> Information broadcasting by Exclusive Read PRAMs, </title> <type> manuscript 1992. </type>
Reference-contexts: For P = N , this solution is optimal (based on a result of <ref> [5] </ref>. For general simulations of PRAM algorithms our solution introduces a fi (P 0 log P ) additive overhead. It also handles static memory faults at no additional overhead. In summary, the static case has a simple optimal solution which eliminates memory access concurrency. <p> Theorem 5.1. The Write-All problem with initial processor and memory faults can be solved in place with S = O (N + P 0 log P ) on an EREW PRAM, where 1 P N and P P 0 is the number of initial faults. With the result of <ref> [5] </ref> it can be shown that this algorithm is optimal. Algorithm E can be used for simulations of PRAM algorithms on fail-stop PRAMs that are subject to static initial processor and memory faults. Simulations are much simpler for this case as compared to the dynamic failures case.
Reference: [6] <author> R. P. Brent, </author> <title> The parallel evaluation of general arithmetic expressions, </title> <journal> J. </journal> <note> ACM 21 (1974) 201-206. </note>
Reference-contexts: In Section 3.6 we describe how this algorithm can be used as a building block for efficient simulations of arbitrary PRAMs. This is a dynamic version of Brent's lemma <ref> [6] </ref>. In Section 4 we examine the worst-case concurrency per step. This is a different measure than the overall concurrency of Section 3. We present a variation of algorithm W whose maximum read/write concurrency per step is N=polylog (N ) instead of N . <p> In particular, only the enumeration phase E1 of algorithm E is needed. At the beginning of the simulation we use the enumeration algorithm to determine the number P 0 of live processors. We then use Brent's Lemma <ref> [6] </ref> to simulate the P processors of the original algorithm on the P 0 available processors. The time overhead of the simulation in this case is just O (log P ) for the enumeration and the work overhead is O (P 0 log P ). Both overheads are additive.
Reference: [7] <author> J. Buss, P.C. Kanellakis, P. Ragde and A. A. Shvartsman, </author> <title> Parallel Algorithms with Processor Failures and Delays, </title> <institution> Brown University TR CS-91-54, </institution> <year> 1991. </year> <note> (Prel. version appears as P. </note> <author> C. Kanellakis and A. A. Shvartsman, </author> <title> Efficient Parallel Algorithms On Restartable Fail-Stop Processors, </title> <booktitle> in: Proc. 10th ACM PODC (1991) 23-36.) </booktitle> <pages> 31 </pages>
Reference-contexts: More general processor asynchrony has been examined in [2, 3, 4, 7, 8, 9, 11, 14, 20, 21, 22, 25, 26, 27]. Many of these analyses involve average processor behavior and use randomization. Some (e.g., <ref> [2, 7, 22] </ref>) deal with deterministic asynchronous computation (i.e., removing assumption (2) without using randomness). In this case the strongest lower bounds are in [22] and the tightest upper bounds are in [2]. A key primitive in much of the above mentioned work is the Write-All operation of [18].
Reference: [8] <author> R. Cole and O. Zajicek, </author> <title> The APRAM: Incorporating Asynchrony into the PRAM Model, </title> <booktitle> in: Proc. 1st ACM Symposium on Parallel Algorithms and Architectures (1989) 170-178. </booktitle>
Reference-contexts: Static Initial Memory Errors The Write-All algorithms and simulations, e.g., [18, 20, 22, 31], or the algorithms that can serve as Write-All solutions, e.g., the algorithms in <ref> [8, 26] </ref>, invariably assume that a linear portion of shared memory is either cleared or is initialized to known values. Starting with a non-contaminated portion of memory, these algorithms perform their computation by "using up" the clear memory, and concurrently or subsequently clearing segments of memory needed for future iterations.
Reference: [9] <author> R. Cole and O. Zajicek, </author> <title> The Expected Advantage of Asynchrony, </title> <booktitle> in: Proc. 2nd ACM Symposium on Parallel Algorithms and Architectures (1990) 85-94. </booktitle>
Reference: [10] <author> F. Cristian, </author> <title> Understanding Fault-Tolerant Distributed Systems, </title> <note> Communications of the ACM 3 2 (1991) 56-78. </note>
Reference-contexts: Fault-tolerant technologies, such as those in surveys <ref> [10, 15, 16] </ref>, all contribute towards concrete realizations of its components. a. There are P fail-stop processors (see [29]), each with a unique address and some local memory. b. There are Q shared memory cells, the input of size N Q is stored in shared memory.
Reference: [11] <author> C. Dwork, J. Halpern and O. Waarts, </author> <title> Performing work efficiently in the presence of faults, </title> <booktitle> Proc. 11th ACM PODC (1992) 91-102. </booktitle>
Reference: [12] <author> D. Eppstein and Z. Galil, </author> <title> Parallel Techniques for Combinatorial Computation, </title> <note> Annual Computer Science Review 3 (1988) 233-283. </note>
Reference-contexts: The Model and Base Algorithms 2.1. Fail-Stop PRAM The parallel random access machine (PRAM) of Fortune and Wyllie [13] combines the simplicity of RAM with the power of parallelism, and a wealth of efficient algorithms exist for it; see surveys <ref> [12, 19] </ref> for the rationale behind this model and the fundamental algorithms. The main features of this model are as follows: 1. There are P initial processors with unique identifiers (PID) in the range 1; : : : ; P .
Reference: [13] <author> S. Fortune and J. Wyllie, </author> <title> Parallelism in Random Access Machines, </title> <booktitle> in: Proc. 10th ACM STOC (1978) 114-118. </booktitle>
Reference-contexts: In summary, the static case has a simple optimal solution which eliminates memory access concurrency. We conclude with some open problems in Section 6. 2. The Model and Base Algorithms 2.1. Fail-Stop PRAM The parallel random access machine (PRAM) of Fortune and Wyllie <ref> [13] </ref> combines the simplicity of RAM with the power of parallelism, and a wealth of efficient algorithms exist for it; see surveys [12, 19] for the rationale behind this model and the fundamental algorithms. The main features of this model are as follows: 1.
Reference: [14] <author> P. Gibbons, </author> <title> A More Practical PRAM Model, </title> <booktitle> in: Proc. 1st ACM Symposium on Parallel Algorithms and Architectures (1989) 158-168. </booktitle>
Reference: [15] <institution> Fault-Tolerant Computing, </institution> <note> IEEE Computer special issue 17 8 (1984). </note>
Reference-contexts: Fault-tolerant technologies, such as those in surveys <ref> [10, 15, 16] </ref>, all contribute towards concrete realizations of its components. a. There are P fail-stop processors (see [29]), each with a unique address and some local memory. b. There are Q shared memory cells, the input of size N Q is stored in shared memory.
Reference: [16] <institution> Fault-Tolerant Systems, </institution> <note> IEEE Computer special issue 23 7 (1990). </note>
Reference-contexts: Fault-tolerant technologies, such as those in surveys <ref> [10, 15, 16] </ref>, all contribute towards concrete realizations of its components. a. There are P fail-stop processors (see [29]), each with a unique address and some local memory. b. There are Q shared memory cells, the input of size N Q is stored in shared memory.
Reference: [17] <author> P. C. Kanellakis, D. Michailidis and A. A. Shvartsman, </author> <title> Controlling Memory Access Con-currency in Efficient Fault-Tolerant Parallel Algorithms, </title> <booktitle> Proc. 7th WDAG, Lecture Notes in Computer Science vol. </booktitle> <month> 725 </month> <year> (1993) </year> <month> 99-114. </month>
Reference-contexts: To see that there is a trade-off consider that reliability usually requires adding redundancy to the computation in order to fl Research supported by ONR grant N00014-91-J-1613. A preliminary version appeared in <ref> [17] </ref>. y Department of Computer Science, Brown University, Box 1910, Providence, RI 02912-1910, USA. z Digital Equipment Corporation, 30 Porter Road, LJ02/I11, Littleton, MA 01460-1446, USA. 1 detect errors and reassign resources, whereas gaining efficiency by massively parallel com-puting requires removing redundancy from the computation to fully utilize each processor.
Reference: [18] <author> P. C. Kanellakis and A. A. Shvartsman, </author> <title> Efficient Parallel Algorithms Can Be Made Robust, </title> <note> Distributed Computing 5 (1992) 201-217. (Prel. version in Proc. 8th ACM PODC (1989) 138-148.) </note>
Reference-contexts: Even allowing for some abstraction in the model of parallel computation, it is not obvious that there are any non-trivial fault models that allow near-linear speed-ups. So it was somewhat surprising when in <ref> [18] </ref> we demonstrated that it is possible to combine efficiency and fault-tolerance for many basic algorithms expressed as concurrent-read concurrent-write parallel random access machines (CRCW PRAMs). The [18] fault model allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. <p> So it was somewhat surprising when in <ref> [18] </ref> we demonstrated that it is possible to combine efficiency and fault-tolerance for many basic algorithms expressed as concurrent-read concurrent-write parallel random access machines (CRCW PRAMs). The [18] fault model allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. Interestingly, our technique can be extended to all CRCW PRAMs. <p> The fault model was extended in [32] to include arbitrary static memory faults , i.e., arbitrary memory initialization. The CRCW PRAM computation model is a widely used abstraction of real massively parallel machines. As shown in <ref> [18] </ref>, it suffices to consider COMMON CRCW PRAMs (all concurrent writes are identical) in which the atomically written words need only contain a constant number of bits. All the above-mentioned algorithmic work makes two somewhat unrealistic, but critical, assumptions. <p> In this case the strongest lower bounds are in [22] and the tightest upper bounds are in [2]. A key primitive in much of the above mentioned work is the Write-All operation of <ref> [18] </ref>. The main technical insight in this paper is an efficient, deterministic fault-tolerant Write-All algorithm that significantly reduces memory access concurrency. The Write-All problem is: using P processors write 1s into all locations of an array of size N , where P N . <p> Under dynamic failures, efficient deterministic solutions to Write-All, i.e., increasing the fault-free O (N ) work by small polylog (N ) factors, are non-obvious. The first such 2 solution was algorithm W of <ref> [18] </ref> which has (to date) the best worst-case work bound O (N + P log 2 N= log log N ) for 1 P N (this bound was shown in [22] for a variant of algorithm W and in [24] the same bound was shown for algorithm W). <p> To make the exposition self contained we present in Section 2.3 an outline of the main algorithmic ideas of algorithm W and sketch how it is utilized in PRAM simulation and memory clearing. Memory access concurrency is the main topic of this paper. In <ref> [18] </ref> it is shown that read and write concurrency are necessary for deterministic efficient solutions under dynamic processor faults. All Write-All solutions that have efficient work for any dynamic processor failures require concurrent-read concurrent-write (CRCW) PRAMs. <p> atomic with respect to failures: failures can occur before or after a shared write of fi (log maxfN; P g)-bit words, but not during the write. (This non-trivial assumption is made only for simplicity of presentation; algorithms that make it can be converted to use only single bit atomic writes <ref> [18] </ref>.) We also handle arbitrary initialization of the shared memory, which is assumed clear in the fault-free PRAM model. (This extends 3 above.) The abstract model that we are studying can be realized in the architecture of Fig. 1. <p> The network can be made more reliable by employing redundancy [1]. 2.2. Measures of Efficiency The complexity measure used throughout this work is the available processor steps of <ref> [18] </ref>. It generalizes the fault-free Parallel-time fiProcessors product and accounts for all steps performed by the active processors. Definition 2.1. <p> This does not imply that the algorithm itself is an EREW or CREW algorithm. However, an algorithm is EREW if its worst case and ! are 0 and it is CREW if its worst case ! is 0. 2.3. Background Algorithms 2.3.1. Algorithm W Algorithm W of <ref> [18] </ref> is a robust Write-All solution. It uses two complete binary trees as data structures: the processor enumeration and the progress trees. A high level view of algorithm W is in Fig. 2. <p> Algorithm W performs best when the cluster size s is chosen to be log N , resulting in a progress tree of N= log N leaves. A complete description of the algorithm can be found in <ref> [18] </ref>. Using the above lemma for U = N= log N , a tight bound can be obtained for algorithm W [log N ]: Theorem 2.2 ([18, 24]). <p> If the Parallel-time fiProcessors of an original N -processor algorithm is t N , then the available steps of the fault-tolerant simulation will be O (t S w (N; P )). 2.3.3. Static Initial Memory Errors The Write-All algorithms and simulations, e.g., <ref> [18, 20, 22, 31] </ref>, or the algorithms that can serve as Write-All solutions, e.g., the algorithms in [8, 26], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values. <p> The same problem is shared by all robust deterministic Write-All algorithms that have been proposed to date. As shown in <ref> [18] </ref>, concurrent memory writes cannot be eliminated altogether. Any Write-All algorithm that does not use concurrent writes, e.g. CREW PRAM, has worst case work (N 2 ) and thus is not robust. 3.1. <p> A timestamp in this context is just a sequence number that need not exceed N (see <ref> [18] </ref>). <p> The resulting algorithm W CR=W [log N log P ] turns out to be nonoptimal, so a further modification is needed. To obtain optimality we make use of the perfect allocation property of algorithm W (see <ref> [18] </ref>). This property guarantees that available processors are allocated evenly to the unvisited leaves in phase W2. Let U i and P i be the numbers of unvisited leaves and available processors, respectively, at the beginning of the ith iteration of the while loop of algorithm 21 W CR=W . <p> Specifically, we show that there is no robust algorithm for the Write-All problem with concurrency ! jF j " for 0 " &lt; 1. For this we consider a simpler Write-One problem <ref> [18] </ref> which involves P processors and a single memory location. The objective is to write 1 into the memory location subject to faults determined by an on-line adversary.
Reference: [19] <author> R. M. Karp and V. Ramachandran, </author> <title> A Survey of Parallel Algorithms for Shared-Memory Machines, </title> <editor> in: J. van Leeuwen, ed., </editor> <booktitle> Handbook of Theoretical Computer Science (North-Holland, </booktitle> <year> 1990) </year> <month> 869-941. </month>
Reference-contexts: The Model and Base Algorithms 2.1. Fail-Stop PRAM The parallel random access machine (PRAM) of Fortune and Wyllie [13] combines the simplicity of RAM with the power of parallelism, and a wealth of efficient algorithms exist for it; see surveys <ref> [12, 19] </ref> for the rationale behind this model and the fundamental algorithms. The main features of this model are as follows: 1. There are P initial processors with unique identifiers (PID) in the range 1; : : : ; P . <p> Alternatively, we can maintain the same concurrency bounds at the expense of increasing the work by a logarithmic factor by first converting the original algorithm into an equivalent EREW algorithm <ref> [19] </ref>. Similar results can be derived for contaminated initial memory using algorithm Z and Theorem 3.4 to initialize an appropriate amount of auxiliary shared memory required by the simulation.
Reference: [20] <author> Z. M. Kedem, K. V. Palem and P. Spirakis, </author> <title> Efficient Robust Parallel Computations, </title> <booktitle> in: Proc. 22nd ACM STOC (1990) 138-148. </booktitle>
Reference-contexts: The [18] fault model allows any pattern of dynamic fail-stop no restart processor errors, as long as one processor remains alive. Interestingly, our technique can be extended to all CRCW PRAMs. In <ref> [20] </ref> it is shown how to simulate any fault-free PRAM on a CRCW PRAM that executes in a fail-stop no-restart environment with comparable efficiency and in [31] it is shown how the simulation can be made optimal using processor slackness. <p> Algorithm W was extended to handle arbitrary initial memory in [32] and served as a building block for transforming fault-free PRAMs into fault-prone PRAMs of comparable efficiency in <ref> [20, 31] </ref>. We say that Write-All completes at the global clock tick at which all the processors that have not fail-stopped share the knowledge that 1's have been written into all N array locations. <p> Requiring completion of a Write-All algorithm is critical if one wishes to iterate it, as pointed out in <ref> [20] </ref> which uses a certification bit to separate the various iterations of (Certified) Write-All. Note that the Write-All completes when all processors halt in algorithm W. This is also the case for the algorithms presented here. In Section 2.1 we present our fault and computation models. <p> PRAM Simulations The original motivation for studying the Write-All problem was that it intuitively captures the essential nature of a single synchronous PRAM step. This intuition was made concrete when it was shown in <ref> [20, 31] </ref> how to use solutions for the Write-All problem in implementing general PRAM simulations. 8 01 forall processors PID=1..P parbegin Use P processors to clear N memory locations 02 Clear the initial block of N 0 = G 0 elements sequentially using P processors 03 i := 0 Iteration counter <p> This is done using two generations of shared memory, "current" and "future", and by executing each of these cycles in the Write-All style, e.g., using algorithm W (for details, see <ref> [20, 31] </ref>). Using such algorithm simulation techniques it was shown in [20, 31] that if S w (N; P ) denotes the available steps of solving a Write-All instance of size N using P processors, and if a linear in N amount of clear memory is available, then any N -processor <p> This is done using two generations of shared memory, "current" and "future", and by executing each of these cycles in the Write-All style, e.g., using algorithm W (for details, see <ref> [20, 31] </ref>). Using such algorithm simulation techniques it was shown in [20, 31] that if S w (N; P ) denotes the available steps of solving a Write-All instance of size N using P processors, and if a linear in N amount of clear memory is available, then any N -processor PRAM step can be deterministically simulated using P fail-stop processors and <p> If the Parallel-time fiProcessors of an original N -processor algorithm is t N , then the available steps of the fault-tolerant simulation will be O (t S w (N; P )). 2.3.3. Static Initial Memory Errors The Write-All algorithms and simulations, e.g., <ref> [18, 20, 22, 31] </ref>, or the algorithms that can serve as Write-All solutions, e.g., the algorithms in [8, 26], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values.
Reference: [21] <author> Z. M. Kedem, K. V. Palem, M. O. Rabin and A. Raghunathan, </author> <title> Efficient Program Transformations for Resilient Parallel Computation via Randomization, </title> <booktitle> in: Proc. 24th ACM STOC (1992) 306-318. </booktitle>
Reference: [22] <author> Z. M. Kedem, K. V. Palem, A. Raghunathan and P. Spirakis, </author> <title> Combining Tentative and Definite Executions for Dependable Parallel Computing, </title> <booktitle> in: Proc. 23rd ACM STOC (1991) 381-390. </booktitle>
Reference-contexts: More general processor asynchrony has been examined in [2, 3, 4, 7, 8, 9, 11, 14, 20, 21, 22, 25, 26, 27]. Many of these analyses involve average processor behavior and use randomization. Some (e.g., <ref> [2, 7, 22] </ref>) deal with deterministic asynchronous computation (i.e., removing assumption (2) without using randomness). In this case the strongest lower bounds are in [22] and the tightest upper bounds are in [2]. A key primitive in much of the above mentioned work is the Write-All operation of [18]. <p> Many of these analyses involve average processor behavior and use randomization. Some (e.g., [2, 7, 22]) deal with deterministic asynchronous computation (i.e., removing assumption (2) without using randomness). In this case the strongest lower bounds are in <ref> [22] </ref> and the tightest upper bounds are in [2]. A key primitive in much of the above mentioned work is the Write-All operation of [18]. The main technical insight in this paper is an efficient, deterministic fault-tolerant Write-All algorithm that significantly reduces memory access concurrency. <p> The first such 2 solution was algorithm W of [18] which has (to date) the best worst-case work bound O (N + P log 2 N= log log N ) for 1 P N (this bound was shown in <ref> [22] </ref> for a variant of algorithm W and in [24] the same bound was shown for algorithm W). Algorithm W was extended to handle arbitrary initial memory in [32] and served as a building block for transforming fault-free PRAMs into fault-prone PRAMs of comparable efficiency in [20, 31]. <p> If the Parallel-time fiProcessors of an original N -processor algorithm is t N , then the available steps of the fault-tolerant simulation will be O (t S w (N; P )). 2.3.3. Static Initial Memory Errors The Write-All algorithms and simulations, e.g., <ref> [18, 20, 22, 31] </ref>, or the algorithms that can serve as Write-All solutions, e.g., the algorithms in [8, 26], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values. <p> A second open problem is whether there is a faster algorithm than W for the Write-All and whether the methods presented here for controlling memory accesses can be applied to it. In <ref> [22] </ref> an (N log N ) lower bound was shown for the Write-All problem but no known deterministic algorithm attains it. A third open problem is whether the lower bound of [22] applies to the static case. <p> In <ref> [22] </ref> an (N log N ) lower bound was shown for the Write-All problem but no known deterministic algorithm attains it. A third open problem is whether the lower bound of [22] applies to the static case. It is interesting to consider whether there is a CRCW algorithm for the static Write-All that requires o (N log N ) work.
Reference: [23] <author> C. P. Kruskal, L. Rudolph, M. Snir, </author> <title> Efficient Synchronization on Multiprocessors with Shared Memory, </title> <journal> ACM TOPLAS 10 (1988) pp. </journal> <pages> 579-601. </pages>
Reference-contexts: Processors and memory are interconnected via a synchronous network (e.g., as in the Ultracomputer [30]). A combining interconnection network that is well suited for implementing synchronous concurrent reads and writes is studied in <ref> [23] </ref> (the combining properties are used in their simplest form only to implement concurrent access to memory). The network can be made more reliable by employing redundancy [1]. 2.2. Measures of Efficiency The complexity measure used throughout this work is the available processor steps of [18].
Reference: [24] <author> C. </author> <title> Martel, </title> <type> personal communication, </type> <month> March, </month> <year> 1991. </year>
Reference-contexts: The first such 2 solution was algorithm W of [18] which has (to date) the best worst-case work bound O (N + P log 2 N= log log N ) for 1 P N (this bound was shown in [22] for a variant of algorithm W and in <ref> [24] </ref> the same bound was shown for algorithm W). Algorithm W was extended to handle arbitrary initial memory in [32] and served as a building block for transforming fault-free PRAMs into fault-prone PRAMs of comparable efficiency in [20, 31]. <p> Simulations using Write-All primitive and two generations of shared memory. The following lemma of <ref> [24] </ref> provides a bound on the total number of block steps executed by all processors (where a block step is an execution by one processor of the body of the while-loop in Fig. 2). It will be used in our analyses in Section 3. Lemma 2.1 ([24]).
Reference: [25] <author> C. Martel, A. Park and R. Subramonian, </author> <title> Work-optimal Asynchronous Algorithms for Shared Memory Parallel Computers, </title> <note> SIAM Journal on Computing 21 (1992) 1070-1099. </note>
Reference: [26] <author> C. Martel, R. Subramonian and A. Park, </author> <title> Asynchronous PRAMs are (Almost) as Good as Synchronous PRAMs, </title> <booktitle> in: Proc. 32nd IEEE FOCS (1990) 590-599. </booktitle>
Reference-contexts: Static Initial Memory Errors The Write-All algorithms and simulations, e.g., [18, 20, 22, 31], or the algorithms that can serve as Write-All solutions, e.g., the algorithms in <ref> [8, 26] </ref>, invariably assume that a linear portion of shared memory is either cleared or is initialized to known values. Starting with a non-contaminated portion of memory, these algorithms perform their computation by "using up" the clear memory, and concurrently or subsequently clearing segments of memory needed for future iterations.
Reference: [27] <author> N. Nishimura, </author> <title> Asynchronous Shared Memory Parallel Computation, </title> <booktitle> in: Proc. 2nd ACM Symposium on Parallel Algorithms and Architectures (1990) 76-84. </booktitle> <pages> 32 </pages>
Reference: [28] <author> D. B. Sarrazin and M. Malek, </author> <title> Fault-Tolerant Semiconductor Memories, </title> <note> IEEE Computer 17 8 (1984) 49-56. </note>
Reference-contexts: There are Q shared memory cells, the input of size N Q is stored in shared memory. These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques without appreciably degrading performance <ref> [28] </ref>. c. Processors and memory are interconnected via a synchronous network (e.g., as in the Ultracomputer [30]).
Reference: [29] <author> R. D. Schlichting and F. B. Schneider, </author> <title> Fail-Stop Processors: an Approach to Designing Fault-tolerant Computing Systems, </title> <journal> ACM Trans. on Comp. Sys. </journal> <month> 1 </month> <year> (1983) </year> <month> 222-238. </month>
Reference-contexts: Fault-tolerant technologies, such as those in surveys [10, 15, 16], all contribute towards concrete realizations of its components. a. There are P fail-stop processors (see <ref> [29] </ref>), each with a unique address and some local memory. b. There are Q shared memory cells, the input of size N Q is stored in shared memory. These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques without appreciably degrading performance [28]. c.
Reference: [30] <author> J. T. Schwartz, </author> <note> Ultracomputers, ACM TOPLAS 2 (1980) 484-521. </note>
Reference-contexts: These semiconductor memories can be manufactured with built-in fault tolerance using replication and coding techniques without appreciably degrading performance [28]. c. Processors and memory are interconnected via a synchronous network (e.g., as in the Ultracomputer <ref> [30] </ref>). A combining interconnection network that is well suited for implementing synchronous concurrent reads and writes is studied in [23] (the combining properties are used in their simplest form only to implement concurrent access to memory). The network can be made more reliable by employing redundancy [1]. 2.2.
Reference: [31] <author> A. A. Shvartsman, </author> <title> Achieving Optimal CRCW PRAM Fault-Tolerance, </title> <note> Information Processing Letters 39 (1991) 59-66. </note>
Reference-contexts: Interestingly, our technique can be extended to all CRCW PRAMs. In [20] it is shown how to simulate any fault-free PRAM on a CRCW PRAM that executes in a fail-stop no-restart environment with comparable efficiency and in <ref> [31] </ref> it is shown how the simulation can be made optimal using processor slackness. The fault model was extended in [32] to include arbitrary static memory faults , i.e., arbitrary memory initialization. The CRCW PRAM computation model is a widely used abstraction of real massively parallel machines. <p> Algorithm W was extended to handle arbitrary initial memory in [32] and served as a building block for transforming fault-free PRAMs into fault-prone PRAMs of comparable efficiency in <ref> [20, 31] </ref>. We say that Write-All completes at the global clock tick at which all the processors that have not fail-stopped share the knowledge that 1's have been written into all N array locations. <p> PRAM Simulations The original motivation for studying the Write-All problem was that it intuitively captures the essential nature of a single synchronous PRAM step. This intuition was made concrete when it was shown in <ref> [20, 31] </ref> how to use solutions for the Write-All problem in implementing general PRAM simulations. 8 01 forall processors PID=1..P parbegin Use P processors to clear N memory locations 02 Clear the initial block of N 0 = G 0 elements sequentially using P processors 03 i := 0 Iteration counter <p> This is done using two generations of shared memory, "current" and "future", and by executing each of these cycles in the Write-All style, e.g., using algorithm W (for details, see <ref> [20, 31] </ref>). Using such algorithm simulation techniques it was shown in [20, 31] that if S w (N; P ) denotes the available steps of solving a Write-All instance of size N using P processors, and if a linear in N amount of clear memory is available, then any N -processor <p> This is done using two generations of shared memory, "current" and "future", and by executing each of these cycles in the Write-All style, e.g., using algorithm W (for details, see <ref> [20, 31] </ref>). Using such algorithm simulation techniques it was shown in [20, 31] that if S w (N; P ) denotes the available steps of solving a Write-All instance of size N using P processors, and if a linear in N amount of clear memory is available, then any N -processor PRAM step can be deterministically simulated using P fail-stop processors and <p> If the Parallel-time fiProcessors of an original N -processor algorithm is t N , then the available steps of the fault-tolerant simulation will be O (t S w (N; P )). 2.3.3. Static Initial Memory Errors The Write-All algorithms and simulations, e.g., <ref> [18, 20, 22, 31] </ref>, or the algorithms that can serve as Write-All solutions, e.g., the algorithms in [8, 26], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values.
Reference: [32] <author> A. A. Shvartsman, </author> <title> An Efficient Write-All Algorithm for Fail-Stop PRAM without Initialized Memory, </title> <note> Information Processing Letters 44 (1992) 223-231. 33 </note>
Reference-contexts: In [20] it is shown how to simulate any fault-free PRAM on a CRCW PRAM that executes in a fail-stop no-restart environment with comparable efficiency and in [31] it is shown how the simulation can be made optimal using processor slackness. The fault model was extended in <ref> [32] </ref> to include arbitrary static memory faults , i.e., arbitrary memory initialization. The CRCW PRAM computation model is a widely used abstraction of real massively parallel machines. <p> Algorithm W was extended to handle arbitrary initial memory in <ref> [32] </ref> and served as a building block for transforming fault-free PRAMs into fault-prone PRAMs of comparable efficiency in [20, 31]. <p> Starting with a non-contaminated portion of memory, these algorithms perform their computation by "using up" the clear memory, and concurrently or subsequently clearing segments of memory needed for future iterations. An efficient Write-All solution that requires no clear shared memory has recently been defined using a bootstrap approach <ref> [32] </ref>. The bootstrapping proceeds in stages: In stage 1 all P processors clear an initial segment of N 0 locations in the auxiliary memory. <p> If N i+1 &gt; N i and N 0 1, then the required N memory location will be cleared in at most N stages. The efficiency of the resulting algorithm depends on the particular Write-All solution (s) used and the parameters N i and G i . In <ref> [32] </ref>, a solution for Write-All (algorithm Z, see Fig. 4) is presented that for any failure pattern F (jF j &lt; P ) has work O (N +P log 3 N=(log log N ) 2 ) without any initialization assumptions. <p> The bound on is obtained as in Theorem 3.6. 2 3.4. Handling Unitialized Memory The algorithms considered so far require that memory be initially clear. Algorithm Z of <ref> [32] </ref> extends algorithm W to handle unitialized memory (see Section 2.3.3). It is possible to incorporate CR/W into Z to obtain a Write-All algorithm with limited concurrency that dispenses with the requirement that memory be clear. <p> This analysis is very similar to the analysis of <ref> [32] </ref> and the work bound follows by the same method; the only difference is the different value for the G i 's and that the work of each iteration is given by Theorem 3.7.
References-found: 32

