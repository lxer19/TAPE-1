URL: http://ftp.ics.uci.edu/pub/smyth/papers/hmmpin.ps.Z
Refering-URL: http://www.ics.uci.edu/~mlearn/MLPapers.html
Root-URL: 
Email: [smyth@ics.uci.edu]  [heckerma@microsoft.com]  [jordan@psyche.mit.edu]  
Title: Probabilistic Independence Networks for Hidden Markov Probability Models  
Author: Padhraic Smyth David Heckerman Michael I. Jordan 
Date: May 1, 1996  
Address: CA 92717-3425.  Building 9S/1 Redmond, WA 98052-6399  Cambridge, MA 02139  
Affiliation: Department of Information and Computer Science University of California, Irvine  Microsoft Research,  Department of Brain and Cognitive Sciences MIT,  
Abstract: Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper contains a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach.
Abstract-found: 1
Intro-found: 1
Reference: <author> Baum, L. E., and Petrie, T. </author> <year> 1966. </year> <title> Statistical inference for probabilistic functions of finite state Markov chains. </title> <journal> Ann. Math. Stat., </journal> <volume> v.37, </volume> <pages> 1554-1563. </pages>
Reference: <author> Bishop, Y.M.M., Fienberg, S.E. and Holland, P.W. </author> <year> 1973. </year> <title> Discrete Multivariate Analysis: Theory and Practice. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: Terms used in the literature to described UPINs of one form or another include Markov random fields (Isham 1981, Ge-man and Geman 1984), Markov networks (Pearl 1988), Boltzmann machines (Hinton and Sejnowski 1986), and log-linear models <ref> (Bishop, Fienberg, & Holland 1973) </ref>. 3.1.1 Conditional Independence Semantics of UPIN Structures Let A, B, and S be any disjoint subsets of nodes in an undirected graph (UG) G. <p> We wish to convert such a local specification into a globally consistent joint probability distribution, i.e., a marginal representation. An algorithm known as Iterative Proportional Fitting (IPF) is available to perform this conversion. Classically, IPF proceeds as follows <ref> (Bishop, Fienberg, & Holland, 1973) </ref>. Suppose for simplicity that all of the random variables are discrete (a Gaussian version of IPF is also available (Whittaker 1990)) such that the joint distribution can be represented as a table. The table is initialized with equal values in all of the cells.
Reference: <author> Buntine, W. </author> <year> 1994. </year> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research. </journal> <pages> 2 159-225. </pages>
Reference: <author> Buntine, W. </author> <title> in press. A guide to the literature on learning probabilistic networks from data. </title> <journal> IEEE Transactions on Knowledge and Data Engineering. </journal>
Reference: <author> Dawid, A. P. </author> <year> 1992. </year> <title> Applications of a general propagation algorithm for probabilistic expert systems. </title> <journal> Statistics and Computing. </journal> <pages> 2 25-36. </pages>
Reference: <author> DeGroot, M. </author> <year> 1970. </year> <title> Optimal Statistical Decisions. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference: <author> Dempster, A., Laird, N., Rubin, D.1977. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B. </journal> <volume> 39, </volume> <pages> 1-38. </pages>
Reference-contexts: MAP and ML estimates can be found using traditional techniques such as gradient descent and expectation-maximization (EM) <ref> (Dempster et al., 1977) </ref>. The EM algorithm can be applied efficiently whenever the likelihood function has sufficient statistics that are of fixed dimension for any data set.
Reference: <author> Elliott, R. J., Aggoun, L., Moore, J. B. </author> <year> 1995. </year> <title> Hidden Markov models: Estimation and Control. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Frasconi, P. and Bengio, Y. </author> <year> 1994. </year> <title> An EM approach to grammatical inference: input/output HMMs. </title> <booktitle> Proceedings of the 12th IAPR Intl. Conf. on Pattern Recognition, </booktitle> <publisher> IEEE Computer Society Press. </publisher> <pages> 289-294. </pages> <note> 34 Fung, </note> <author> R. M. and Crawford, S. L. </author> <year> 1990. </year> <title> A system for induction of probabilistic models. </title> <booktitle> Eighth National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA: </address> <publisher> AAAI, </publisher> <pages> 762-779. </pages>
Reference: <author> Gauvain, J., Lee, C. </author> <year> 1994. </year> <title> Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains. </title> <journal> IEEE Trans. Sig. Audio Proc.. </journal> <volume> 2, </volume> <pages> 291-298. </pages>
Reference-contexts: Heckerman and Geiger (1995) describe a simple method for assessing these priors. These priors have also been used for learning parameters in standard HMMs <ref> (e.g., Gauvain and Lee, 1994) </ref>. Parameter independence is usually not assumed in general for HMM structures.
Reference: <author> Geman, S. and Geman, D. </author> <year> 1984. </year> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell. </journal> <volume> 6, </volume> <pages> 721-741. </pages>
Reference: <author> Ghahramani, Z., and Jordan, M. I. </author> <year> 1996. </year> <title> Factorial Hidden Markov models. </title> <editor> In D. </editor> <publisher> S. </publisher>
Reference: <editor> Touretzky, M. C. Mozer, & M. E. Hasselmo (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Huang, X. D., Ariki, Y., Jack, M. A. </author> <year> 1990. </year> <title> Hidden Markov Models for Speech Recognition. </title>
Reference: <institution> Edinburgh, </institution> <address> U.K.: </address> <publisher> Edinburgh University Press. </publisher>
Reference: <author> Heckerman, D., and Geiger, D. </author> <year> 1995. </year> <title> Likelihoods and priors for Bayesian networks. </title> <publisher> MSR-TR-95-54, Microsoft, </publisher> <address> Redmond, WA. </address>
Reference: <author> Heckerman, D., Geiger, D., and Chickering, D. </author> <year> 1995. </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning. </journal> <volume> 20, </volume> <pages> 197-243. </pages>
Reference: <author> Hinton, G. E. and Sejnowski, T. J. </author> <year> 1986. </year> <title> Learning and relearning in Boltzmann machines. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <editor> Rumelhart D.E., McClelland J. L., and the PDP Research Group, editors. </editor> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <address> v.1, ch. </address> <month> 7. </month>
Reference-contexts: UPIN parameters consist of numerical specifications of a particular probability model consistent with the UPIN structure. Terms used in the literature to described UPINs of one form or another include Markov random fields (Isham 1981, Ge-man and Geman 1984), Markov networks (Pearl 1988), Boltzmann machines <ref> (Hinton and Sejnowski 1986) </ref>, and log-linear models (Bishop, Fienberg, & Holland 1973). 3.1.1 Conditional Independence Semantics of UPIN Structures Let A, B, and S be any disjoint subsets of nodes in an undirected graph (UG) G. <p> A UPIN is equivalent to a Markov random field (Isham 1981). In the Markov random field literature the clique functions are generally referred to as "potential functions." A 5 X 2 X 3 X 6 X 1 related terminology, used in the context of the Boltzmann machine <ref> (Hinton & Sejnowski, 1986) </ref>, is that of "energy function." The exponential of the negative energy of a configuration is a "Boltzmann factor." Scaling each Boltzmann factor by the sum across Boltzmann factors (the partition function) yields a factorization of the joint density (the Boltzmann distribution), i.e., a product of clique functions.
Reference: <author> Isham, V. </author> <year> 1981. </year> <title> An introduction to spatial point processes and Markov random fields. </title> <journal> International Statistical Review. </journal> <volume> 49, </volume> <pages> 21-43. </pages>
Reference-contexts: UPIN parameters consist of numerical specifications of a particular probability model consistent with the UPIN structure. Terms used in the literature to described UPINs of one form or another include Markov random fields <ref> (Isham 1981, Ge-man and Geman 1984) </ref>, Markov networks (Pearl 1988), Boltzmann machines (Hinton and Sejnowski 1986), and log-linear models (Bishop, Fienberg, & Holland 1973). 3.1.1 Conditional Independence Semantics of UPIN Structures Let A, B, and S be any disjoint subsets of nodes in an undirected graph (UG) G. <p> The set of clique functions associated with a UPIN structure provides the numerical parameterization of the UPIN. A UPIN is equivalent to a Markov random field <ref> (Isham 1981) </ref>.
Reference: <author> Jensen, F. V., Lauritzen, S. L. and Olesen, K. G., </author> <year> 1990. </year> <title> Bayesian updating in recursive graphical models by local computations. </title> <journal> Computational Statistical Quarterly. </journal> <volume> 4, </volume> <pages> 269-282. </pages>
Reference-contexts: When the schedule of flows is complete one gets a new representation K fl f such that the local potential on each clique is f fl (x C ) = p (x h C ; e), i.e., the joint probability of the local unobserved clique variables and the observed evidence <ref> (Jensen et al. 1990) </ref> (similarly for the separator potential functions). If one marginalizes at the clique over the unobserved local clique variables, X C C ; e) = p (e); (16) one gets the probability of the observed evidence directly. <p> Problems of finding optimally small junction trees (e.g., finding the junction tree with the smallest maximal clique) are NP-hard. Nonetheless, the heuristic algorithm for triangulation described earlier has been found to work well in practice <ref> (Jensen et al. 1990) </ref>. 6 Inference and MAP Calculations in HMM (1,1) 6.1 The F-B Algorithm for HMM (1,1) is a Special Case of the JLO Al gorithm HMM (1,1) junction tree structure to obtain a particular inference algorithm for HMM (1,1).
Reference: <author> Jirousek, R. and Preucil, S. </author> <year> 1995. </year> <title> On the effective implementation of the iterative proportional fitting procedure. </title> <journal> Computational Statistics and Data Analysis. </journal> <volume> 19, </volume> <pages> 177-189. </pages>
Reference: <author> Kass, R., Tierney, L. and Kadane, J., </author> <year> 1988. </year> <title> Asymptotics in Bayesian computation. In Bayesian Statistics 3, </title> <editor> J. Bernardo and M. DeGroot and D. Lindley and A. Smith (editors), </editor> <publisher> Oxford, UK: Oxford University Press, </publisher> <pages> 261-278. </pages>
Reference-contexts: An approximation that is less accurate but more efficient is one based on the observation that, under certain conditions, the quantity p ( s jS) p (Dj s ; S) converges to a multivariate Gaussian distribution as the sample size increases <ref> (see, e.g., Kass et al., 1988, and MacKay, 1992ab) </ref>. Less accurate but more efficient approximations are based on the observation that the Gaussian distribution converges to a delta function centered at the maximum-a-posteriori (MAP) and eventually the maximum-likelihood (ML) value of s .
Reference: <author> Kent, R. D. & Minifie, F. D. </author> <year> 1977. </year> <title> Coarticulation in recent speech production models. </title> <journal> Journal of Phonetics. </journal> <volume> 5, </volume> <pages> 115-117. </pages>
Reference-contexts: This coupled physical process is not well modeled by the unstructured state transition matrix of HMM (1,1). Moreover, the first-order Markov properties of HMM (1,1) are not well suited to modeling the ubiquitous coarticulation effects that occur in speech, particularly coarticulatory effects that extend across several phonemes <ref> (cf. Kent & Minifie, 1977) </ref>. A variety of techniques have been developed to surmount these basic weaknesses of the HMM (1,1) model, including mixture modeling of emission probabilities, triphone modeling, and discriminative training.
Reference: <author> Lauritzen, S. L. and Spiegelhalter D. J. </author> <year> 1988. </year> <title> Local computations with probabilities on graphical structures and their application to expert systems (with discussion). </title> <journal> J. Roy. Statist. Soc. Ser. </journal> <volume> B. </volume> <pages> 50 157-224. </pages>
Reference: <author> Lauritzen, S., and Wermuth, N. </author> <year> 1989. </year> <title> Graphical models for associations between variables, some of which are qualitative and some quantitative. </title> <journal> Annals of Statistics. </journal> <volume> 17, </volume> <pages> 31-57. </pages>
Reference-contexts: The JLO algorithm may also be used when some parameters are equal and when the likelihoods of some variables are Gaussian or Gaussian-mixture distributions <ref> (Lauritzen and Wermuth, 1989) </ref>. In the M step, we use the expected sufficient statistics as if they were actual sufficient statistics, and set the new values of s to be the MAP or ML values given these statistics.
Reference: <author> Lauritzen, S. L., Dawid, A. P., Larsen, B. N., and Leimer, H. </author> <title> G 1990. Independence properties of directed Markov fields. </title> <journal> Networks. </journal> <volume> 20, </volume> <pages> 491-505. </pages> <address> 35 Lindblom, </address> <publisher> B. </publisher> <year> 1990. </year> <title> Explaining phonetic variation: A sketch of the H&H theory. In Speech Production and Speech Modeling, </title> <editor> W.J. Hardcastle and A. Marchal, (Eds.). </editor> <publisher> Kluwer: Dordrecht. </publisher>
Reference-contexts: the same definition as for a UPIN structure except that separation has a more complex interpretation in the directed context: S separates A from B in a directed graph if S separates A from B in the moral (undirected) graph of the smallest ancestral set containing A, B, and S <ref> (Lauritzen et al. 1990) </ref>. It can be shown that this definition of a DPIN structure is equivalent to the more intuitive statement that, given the values of its parents, a variable X i is independent of all other nodes in the directed graph except for its descendants.
Reference: <author> Lucke, H. </author> <year> 1995. </year> <title> Bayesian Belief Networks as a tool for stochastic parsing. </title> <journal> Speech Communication. </journal> <volume> 16, </volume> <pages> 89-118. </pages>
Reference: <author> MacKay, D. J. C., </author> <year> 1992a. </year> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 415-447. </pages>
Reference: <author> MacKay, D. J. C., </author> <year> 1992b. </year> <title> A practical Bayesian framework for backpropagation networks. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 448-472. </pages>
Reference: <author> Madigan, D. and Raftery, A. E., </author> <year> 1994. </year> <title> Model selection and accounting for model uncertainty in graphical models using Occam's window. </title> <journal> J. Am. Stat. Assoc., </journal> <volume> 89, </volume> <pages> 1535-1546. </pages>
Reference: <author> Neal, R. </author> <year> 1993. </year> <title> Probabilistic inference using Markov chain Monte Carlo methods. </title> <institution> CRG-TR-93-1, Department of Computer Science, University of Toronto. </institution>
Reference-contexts: When data are missing, the exact evaluation of the posterior p ( s jD; S) is typically intractable, so we turn to approximations. Accurate but slow approximations are based on Monte-Carlo sampling <ref> (e.g., Neal, 1993) </ref>.
Reference: <author> Pearl, J. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: UPIN parameters consist of numerical specifications of a particular probability model consistent with the UPIN structure. Terms used in the literature to described UPINs of one form or another include Markov random fields (Isham 1981, Ge-man and Geman 1984), Markov networks <ref> (Pearl 1988) </ref>, Boltzmann machines (Hinton and Sejnowski 1986), and log-linear models (Bishop, Fienberg, & Holland 1973). 3.1.1 Conditional Independence Semantics of UPIN Structures Let A, B, and S be any disjoint subsets of nodes in an undirected graph (UG) G. <p> Associated with each edge in the junction tree is a separator S, such that S contains the variables in the intersection of the two cliques which it links. Given a junction tree representation, one can factorize p (U) as the product of clique marginals over separator marginals <ref> (Pearl 1988) </ref>: p (u) = C2V C p (x C ) S2V S p (x S ) where p (x C ) and p (x S ) are the marginal (joint) distributions for the variables in clique C and separator S respectively and V C and V S are the set <p> once the ^ K f representation is obtained, one can locally identify the values of X h C which maximize the full joint probability as ^x h C 18 In the probabilistic expert systems literature this procedure is known as generating the "most probable explanation" (MPE) given the observed evidence <ref> (Pearl 1988) </ref>.
Reference: <author> Pearl, J., Geiger, D., and Verma, T. </author> <year> 1990. </year> <title> The logic of influence diagrams. Influence Diagrams, Belief Nets, and Decision Analysis. Oliver, </title> <editor> R. M. and Smith, J. Q. </editor> <publisher> (eds.). </publisher>
Reference: <editor> Chichester, </editor> <publisher> U.K.: John Wiley and Sons. </publisher> <pages> 67-83. </pages>
Reference: <author> Perkell, J. S., Matthies, M. L., Svirsky, M. A., and Jordan, M. I. </author> <year> 1993. </year> <title> Trading relations between tongue-body raising and lip rounding in production of the vowel /u/: A pilot motor equivalence study. </title> <journal> Journal of the Acoustical Society of America. </journal> <volume> 93, </volume> <pages> 2948-2961. </pages>
Reference: <author> Poritz, A. M. </author> <year> 1988. </year> <title> Hidden Markov models: a guided tour. </title> <booktitle> Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. </booktitle> <address> New York: </address> <publisher> IEEE Press. vol.1, </publisher> <pages> 7-13. </pages>
Reference: <author> Rabiner, L., </author> <year> 1989. </year> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77, </volume> <pages> 257-285. </pages>
Reference: <author> Raftery, A. </author> <year> 1995. </year> <title> Bayesian model selection in social research. </title> <editor> In Marsden, P., </editor> <booktitle> Sociological Methodology. </booktitle> <address> Blackwells, Cambridge, MA. </address>
Reference-contexts: The BIC score is the additive inverse of Rissanen's (1987) minimum description length (MDL). Other scores, which can be viewed as approximations to the marginal likelihood, are hypothesis testing <ref> (Raftery 1995) </ref> and cross validation (Fung and Crawford 1990).
Reference: <author> Rissanen, J. </author> <year> 1987. </year> <title> Stochastic complexity (with discussion). </title> <journal> Journal of the Royal Statistical Society, Series B. </journal> <volume> 49, </volume> <pages> 223-239 and 253-265. </pages>
Reference: <author> Saul, L. K., and Jordan, M. I. </author> <year> 1995. </year> <title> Boltzmann chains and hidden Markov models. </title> <editor> In G. Tesauro, D. S. Touretzky & T. K. Leen, (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: H 1 H 3 H 2 O 3 O 2 O 1 O 4 O 5 H 4 H 3 H 5 O 2 O 3 O 1 of this UPIN structure. 21 The first model that we consider can be viewed as a coupling of two HMM (1,1) chains <ref> (Saul & Jordan, 1995) </ref>. Such a model can be useful in general sensor fusion problems, for example in the fusion of an audio signal with a video signal in lipreading.
Reference: <author> Saul, L. K., and Jordan, M. I. </author> <year> 1996. </year> <title> Exploiting tractable substructures in intractable networks. </title> <editor> In D. S. Touretzky, M. C. Mozer, & M. E. Hasselmo (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address> <note> 36 Shachter, </note> <author> R. D., Anderson, S. K. and Szolovits, P. </author> <year> 1994. </year> <title> Global conditioning for praba--bilistic inference in belief networks. </title> <booktitle> Proceedings of the Uncertainty in AI Conference 1994, </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 514-522. </pages>
Reference: <author> Schwarz, G. </author> <year> 1978. </year> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics. </journal> <volume> 6, </volume> <pages> 461-464. </pages>
Reference: <author> Spiegelhalter, D. J., Dawid, A. P., Hutchinson, T. A., and Cowell, R. G. </author> <year> 1991. </year> <title> Probabilistic expert systems and graphical modelling: a case study in drug safety. </title> <journal> Phil. Trans. R. Soc. Lond. A. </journal> <volume> 337, </volume> <pages> 387-405. </pages>
Reference-contexts: DPINs have found application in causal modelling in applied statistics and artificial intelligence. Their popularity in these fields stems from the fact that the joint probability model can be specified directly via Equation 3, i.e., via the specification of con 9 ditional probability tables or functions <ref> (Spiegelhalter et al. 1991) </ref>. In contrast, UPINs must be specified in terms of clique functions (as in Equation 1) which may not be as easy to work with (cf.
Reference: <author> Spiegelhalter, D. J. and Lauritzen, S. L. </author> <year> 1990. </year> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20, </volume> <pages> 579-605. </pages>
Reference: <author> Spirtes, P. and Meek, C. </author> <year> 1995. </year> <title> Learning Bayesian networks with discrete variables from data. </title> <booktitle> In Proceedings of First International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press, </publisher> <pages> 294-299. </pages>
Reference: <author> Stolorz, P. </author> <year> 1994. </year> <title> Recursive approaches to the statistical physics of lattice proteins. In L. </title>
Reference-contexts: Saul and Jordan (1996) have proposed a second extension of the HMM (1,1) model which is motivated by the desire to provide a more effective model of coarticulation <ref> (see also Stolorz, 1994) </ref>. In this model, shown in Figure 10, coarticulatory influences are modeled via additional links between output variables and states along an HMM (1,1) backbone.
Reference: <editor> Hunter, ed. </editor> <booktitle> Proc. 27th Hawaii Intl. Conf. on System Sciences, V, </booktitle> <pages> 316-325. </pages>
Reference: <author> Tao, C., </author> <year> 1992. </year> <title> A generalization of the discrete hidden Markov model and of the Viterbi algorithm. </title> <journal> Pattern Recognition, </journal> <volume> 25(11), </volume> <pages> 1381-1387. </pages>
Reference: <author> B. Thiesson, </author> <year> 1995. </year> <title> Score and information for recursive exponential models with incomplete data. </title> <institution> Institute of Electronic Systems, Aalborg University, Aalborg, Denmark, </institution> <type> technical report, </type> <month> October </month> <year> 1995. </year>
Reference-contexts: Fortunately, parameter equalities such as these are easily handled in the framework above <ref> (see Thiesson, 1995, for a detailed discussion) </ref>. In addition the assumption that patterns are complete is clearly inappropriate for HMM structures in general, where some of the variables are hidden from observation.
Reference: <author> Vandermeulen, D., Verbeeck, R., Berben, L. Delaere, D., Suetens, P. and Marchal, G. </author> <year> 1994. </year> <title> Continuous voxel classification by stochastic relaxation: theory and application to MR imaging and MR angiography. </title> <booktitle> Image and Vision Computing. </booktitle> <pages> 12(9) 559-572. </pages>
Reference: <author> Whittaker, J. </author> <year> 1990. </year> <title> Graphical Models in Applied Multivariate Statistics, </title> <address> Chichester, UK: </address> <publisher> John Wiley and Sons. </publisher>
Reference-contexts: Conditional independence is symmetric. Note also that marginal independence (no conditioning) does not in general imply conditional independence, nor does conditional independence in general imply marginal independence <ref> (Whittaker 1990) </ref>. With any set of random variables U we can associate a graph G defined as G = (V; E). <p> Two important classes of graphs for modeling probability distributions that we consider in this paper are UGs and acyclic directed graphs (ADGs)|directed graphs having no directed cycles. We note in passing that there exists a theory for graphical independence models involving both directed and undirected edges <ref> (chain graphs, Whittaker 1990) </ref>, but these are not discussed here. For an UG G, a subset of nodes C separates two other subsets of nodes A and B if every path joining every pair of nodes i 2 A and j 2 B contains at least one node from C. <p> An algorithm known as Iterative Proportional Fitting (IPF) is available to perform this conversion. Classically, IPF proceeds as follows (Bishop, Fienberg, & Holland, 1973). Suppose for simplicity that all of the random variables are discrete (a Gaussian version of IPF is also available <ref> (Whittaker 1990) </ref>) such that the joint distribution can be represented as a table. The table is initialized with equal values in all of the cells.
Reference: <author> Williams, C., and Hinton, G. E. </author> <year> 1990. </year> <title> Mean field networks that learn to discriminate temporally distorted strings. </title> <booktitle> Proc. Connectionist Models Summer School, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 18-22. </pages>
Reference: <author> Modestino, J. and Zhang, J. </author> <year> 1992. </year> <title> A Markov random field model-based approach to image segmentation. </title> <journal> IEEE Trans. Patt. Anal. Mach. Int. </journal> <volume> 14(6), </volume> <pages> 606-615. 37 </pages>
References-found: 53

