URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/bengioy_TR1019.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/other.html
Root-URL: http://www.iro.umontreal.ca
Email: bengioy@iro.umontreal.ca  
Title: Using a Financial Training Criterion Rather than a Prediction Criterion  
Author: Yoshua Bengio 
Date: #1019, February 1996  
Address: Montreal, Qc, H3C 3J7 CANADA  
Affiliation: Dept. IRO Universite de Montreal  
Pubnum: Technical Report  
Abstract: The application of this work is to decision taking with financial time-series, using learning algorithms. The traditional approach is to train a model using a prediction criterion, such as minimizing the squared error between predictions and actual values of a dependent variable, or maximizing the likelihood of a conditional model of the dependent variable. We find here with noisy time-series that better results can be obtained when the model is directly trained in order to optimize the financial criterion of interest. Experiments were performed on portfolio selection with 35 Canadian stocks. 
Abstract-found: 1
Intro-found: 1
Reference: [Ben96] <author> Y. Bengio. </author> <title> Neural Networks for Speech and Sequence Recognition. </title> <publisher> International Thompson Computer Press, </publisher> <address> London, UK, </address> <year> 1996. </year>
Reference-contexts: The latter work is also related to several proposals to build modular systems that are trained cooperatively in order to optimize a common objective function (see [BG91] and <ref> [Ben96] </ref>, Chapter 5). Consider the following situation. We have two modules in series, M 1 , and M 2 , with the output of M 1 feeding the input of M 2 . Module M 1 computes y (x; 1 ), with input x and parameters 1 .
Reference: [BG91] <author> L. Bottou and P. Gallinari. </author> <title> A framework for the cooperation of learning algorithms. </title> <editor> In R. P. Lippman, R. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 781-788, </pages> <address> Denver, CO, </address> <year> 1991. </year>
Reference-contexts: The latter work is also related to several proposals to build modular systems that are trained cooperatively in order to optimize a common objective function (see <ref> [BG91] </ref> and [Ben96], Chapter 5). Consider the following situation. We have two modules in series, M 1 , and M 2 , with the output of M 1 feeding the input of M 2 . Module M 1 computes y (x; 1 ), with input x and parameters 1 .
Reference: [DBG91] <author> X. Driancourt, L. Bottou, and P. Gallinari. </author> <title> Learning vector quantization, multi-layer perceptron and dynamic programming: Comparison and cooperation. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 815-819, </pages> <year> 1991. </year>
Reference-contexts: parameters, several training schemes have been proposed that are closer to that objective than a prediction or likelihood criterion: see for example the work on the Classification Figure of Merit [HW90], as well as the work on training neural networks through a post-processor based on dynamic programming for speech recognition <ref> [DBG91] </ref> (in which the objective is to correctly recognize and segment sequences of phonemes, rather than individual phonemes). The latter work is also related to several proposals to build modular systems that are trained cooperatively in order to optimize a common objective function (see [BG91] and [Ben96], Chapter 5).
Reference: [HK92] <author> J. B. Hampshire, II and B. V. K. Vijaya Kumar. </author> <title> Shooting craps in search of an optimal strategy for training connectionist pattern classifiers. </title> <editor> In J. Moody, S. Hanson, and R. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 1125-1132, </pages> <address> Denver, CO, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: However, this incorrect assumption may be hurtful, especially when the training data is not abundant (or non-stationary, for time-series), and noisy. In particular, it has been theoretically shown <ref> [HK92] </ref> for classification tasks that this strategy is less optimal than one based on training the model with respect to the decision surfaces, which may be determined by a discriminant function associated to each class (e.g., one output of a neural network for each class).
Reference: [HW90] <author> John B. Hampshire and Alexander H. Waibel. </author> <title> A novel objective function for improved phoneme recognition using time-delay neural networks. </title> <journal> IEEE Transactions of Neural Networks, </journal> <volume> 1(2) </volume> <pages> 216-228, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Since the number of classification errors is a discrete function of the parameters, several training schemes have been proposed that are closer to that objective than a prediction or likelihood criterion: see for example the work on the Classification Figure of Merit <ref> [HW90] </ref>, as well as the work on training neural networks through a post-processor based on dynamic programming for speech recognition [DBG91] (in which the objective is to correctly recognize and segment sequences of phonemes, rather than individual phonemes).
Reference: [RL91] <author> Michael D. Richard and Richard P. Lippmann. </author> <title> Neural network classifiers estimate Bayesian a-posteriori probabilities. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 461-483, </pages> <year> 1991. </year>
Reference-contexts: to perform a statistically meaningful task: for example, with the mean squared error criterion in order to estimate the expected value of output variables given input variables, or with cross-entropy or maximum likelihood, in order to build a model of the conditional distribution of discrete output variables, given input variables <ref> [Whi89, RL91] </ref>. However, in many applications of learning algorithms, the ultimate objective is not to build a model of the distribution or of the expected value of the output variables, but rather to use the trained system in order to take the best decisions, according to some criterion.
Reference: [Whi89] <author> H. White. </author> <title> Learning in artificial neural networks: A statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1(4) </volume> <pages> 425-464, </pages> <year> 1989. </year> <month> 14 </month>
Reference-contexts: to perform a statistically meaningful task: for example, with the mean squared error criterion in order to estimate the expected value of output variables given input variables, or with cross-entropy or maximum likelihood, in order to build a model of the conditional distribution of discrete output variables, given input variables <ref> [Whi89, RL91] </ref>. However, in many applications of learning algorithms, the ultimate objective is not to build a model of the distribution or of the expected value of the output variables, but rather to use the trained system in order to take the best decisions, according to some criterion.
References-found: 7

