URL: ftp://ilk.kub.nl/pub/antalb/naic97.ps.gz
Refering-URL: http://ilk.kub.nl/papers.html
Root-URL: 
Email: fweijters,antal,herikg@cs.unimaas.nl  
Title: Intelligible Neural Networks with BP-SOM  
Author: Ton Weijters, Antal van den Bosch, H. Jaap van den Herik 
Address: P.O. Box 616, NL-6200 MD Maastricht The Netherlands  
Affiliation: Department of Computer Science Universiteit Maastricht  
Abstract: Back-propagation learning (BP) is known for its serious limitations in generalising knowledge from certain types of learning material. BP-SOM is an extension of BP which overcomes some of these limitations. BP-SOM is a combination of a multi-layered feed-forward network (MFN) trained with BP, and Kohonen's self-organising maps (SOMs). In earlier reports, it has been shown that BP-SOM improved the generalisation performance whereas it decreased simultaneously the number of necessary hidden units without loss of generalisation performance. These are only two effects of the use of SOM learning during training of MFNs. In this paper we focus on two additional effects. First, we show that after BP-SOM training, activations of hidden units of MFNs tend to oscillate among a limited number of discrete values. Second, we identify SOM elements as adequate organisers of instances of the task at hand. We visualise both effects, and argue that they lead to intelligible neural networks and can be employed as a basis for automatic rule extraction.
Abstract-found: 1
Intro-found: 1
Reference: [Hin86] <author> Hinton, G. E. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> 1-12. </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: BP, and BP augmented with weight decay <ref> [Hin86] </ref>. Henceforth, the latter is referred to as BPWD. The reported experiments show that (i) BP-SOM learning results in MFNs with a better generalisation performance, as compared to MFNs trained with BP and BPWD, and (ii) an increased amount of hidden units can be pruned without loss of generalisation performance.
Reference: [Koh89] <author> Kohonen, T. </author> <year> (1989). </year> <title> Self-organisation and Associative Memory. </title> <publisher> Berlin: Springer Verlag. </publisher>
Reference-contexts: For details we refer to [Wei95,Wei96,WVVP97]. The aim of the BP-SOM learning algorithm is to establish a cooperation between BP-learning and SOM-learning in order to find adequate hidden-layer representations for learning classification tasks. To achieve this aim, the traditional MFN architecture [RHW86] is combined with self-organising maps (SOMs) <ref> [Koh89] </ref>: each hidden layer of the MFN is associated with one SOM (see Figure 1). During training of the weights in the MFN, the corresponding SOM is trained on the hidden-unit activation patterns. <p> The BP learning rate was set to 0.15 and the momentum to 0.4. In all SOMs a decreasing interaction strength from 0.15 to 0.05, and a decreasing neighbourhood-updating context from a square with maximally 25 units to only 1 unit (the winner) was used <ref> [Koh89] </ref>. BP-SOM was trained for a fixed number of cycles m = 2000; class labelling was performed at each 5th cycle (n = 5).
Reference: [Nor89] <author> Norris, D. </author> <year> (1989). </year> <title> How to build a connectionist idiot (savant). </title> <journal> Cognition, </journal> <volume> 35, </volume> <pages> 277-291. </pages>
Reference: [Pre94] <author> Prechelt, L. </author> <year> (1994). </year> <title> Proben1: A set of neural network benchmark problems and benchmarking rules. </title> <type> Technical Report 24/94, </type> <institution> Fakult at f ur Informatik, Universit at Karlsruhe, Germany. </institution>
Reference-contexts: Early stopping, a common method to prevent overfitting, was used in all experiments: the performance of a trained network was calculated in percentages of incorrectly-processed test instances at the cycle where the classification error on validation material was minimal <ref> [Pre94] </ref>. For further details on the experiments, we refer to [Wei96]. 3.1 Parity-12 BP, BPWD, and BP-SOM have been applied to the parity-12 task, i.e., the task to determine whether a bit string of 0's and 1's of length 12 contains an even number of 1's. <p> Within subsets, all instances are associated with the same class assuming a reliability of the SOM element of 1.0. This automatic division into homogeneous subsets can be a useful step in automatic rule extraction. As an example, we trained BP-SOM networks on the monks-1 task <ref> [Pre94] </ref>, a well-known benchmark problem for rule extraction. The task is to classify an instance (a1, a2, a3, a4, a5, a6) with six attributes.
Reference: [RHW86] <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1: </volume> <pages> Foundations (pp. 318-362). </pages> <address> Cambridge, MA: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: 1 Background Back-propagation learning <ref> [RHW86] </ref> (henceforth referred to as BP) is known for its serious limitations in (i) generalising knowledge from certain types of training material [Nor89,WK91] and (ii) inducing intelligible models [Nor89,SL97], In earlier publications [Wei95,Wei96,WVVP97] experimental results are reported in which the generalisation performances of BP-SOM are compared to two other learning algorithms <p> For details we refer to [Wei95,Wei96,WVVP97]. The aim of the BP-SOM learning algorithm is to establish a cooperation between BP-learning and SOM-learning in order to find adequate hidden-layer representations for learning classification tasks. To achieve this aim, the traditional MFN architecture <ref> [RHW86] </ref> is combined with self-organising maps (SOMs) [Koh89]: each hidden layer of the MFN is associated with one SOM (see Figure 1). During training of the weights in the MFN, the corresponding SOM is trained on the hidden-unit activation patterns. <p> In Figure 1, we see four class labels A, four class labels B, and one element unlabelled. The self-organisation of the SOM is used as an addition to the standard BP learning rule <ref> [RHW86] </ref>. Classification and reliability information from the SOMs is included when updating the connection weights of the MFN. The error of a hidden-layer vector is a accumulation of the error computed by the BP learning rule, and a SOM error.
Reference: [Sch93] <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 153-178. </pages>
Reference: [SL97] <author> Setiono, R. and Liu, H. </author> <year> (1997). </year> <title> NeuroLinear: A system for extracting oblique decision rules from neural networks. </title> <editor> In M. Van Someren and G. Widmer (Eds.), </editor> <booktitle> Proceedings of the Ninth European Conference on Machine Learning. Lecture Notes in Computer Science 1224. </booktitle> <address> Berlin: </address> <publisher> Springer Verlag, </publisher> <pages> 221-233. </pages>
Reference-contexts: The apparent discretisation of activations offers a potentially interesting basis for rule extraction from MFNs: In order to extract rules from (MFNs), it is better that (hidden-unit) activations be grouped into a small number of clusters while at the same time preserving the accuracy of the network <ref> [SL97] </ref>. Current approaches to rule extraction from MFNs have to rely on a procedure which can only be applied after having trained the MFN [SL97]. In contrast, BP-SOM yields the discretisation automatically during learning; no additional discretisation method need to be performed. <p> (MFNs), it is better that (hidden-unit) activations be grouped into a small number of clusters while at the same time preserving the accuracy of the network <ref> [SL97] </ref>. Current approaches to rule extraction from MFNs have to rely on a procedure which can only be applied after having trained the MFN [SL97]. In contrast, BP-SOM yields the discretisation automatically during learning; no additional discretisation method need to be performed.
Reference: [Tho95] <author> Thornton, C. </author> <year> (1995). </year> <title> Measuring the difficulty of specific learning problems. </title> <journal> Connection Science, </journal> <volume> 7, </volume> <pages> 81-92. </pages>
Reference: [Thr91] <author> Thrun, S. B., Bala, J., Bloedorn, E., Bratko, I., Cestnik, B., Cheng, J., De Jong, K., D zeroski, S., Fahlman, S. E., Fisher, D., Hamann, R., Kauf-man, K., Keller, S., Kononenko, I., Kreuziger, J., Michalski, R. S., Mitchell, T., Pachowicz, P., Reich, Y., Vafaie, H., Van de Velde, W., Wenzel, W., Wnek, J., and Zhang, J. </author> <year> (1991). </year> <title> The MONK's Problems: a performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference-contexts: The parity-12 task is hard to learn for many learning algorithms, since it often leads to overfitting [Sch93,Tho95]. The reason is that the output classification depends on the values of all input features. The monks-1 is a well-known benchmark task for automatic rule extraction <ref> [Thr91] </ref>. For all experiments reported in this paper we have used a fixed set of parameters for the learning algorithms. The BP learning rate was set to 0.15 and the momentum to 0.4.
Reference: [Wei95] <author> Weijters, A. </author> <year> (1995). </year> <title> The BP-SOM architecture and learning rule. </title> <journal> Neural Processing Letters, </journal> <volume> 2, </volume> <pages> 13-16. </pages>
Reference: [Wei96] <author> Weijters, A. </author> <year> (1996). </year> <title> BP-SOM: A profitable cooperation. </title> <editor> In J.-J. Ch. Meyer and L. C. Van der Gaag (Eds.), </editor> <booktitle> Proceedings of the Eighth Dutch Conference on Artificial Intelligence, NAIC'96, </booktitle> <pages> 381-391. </pages>
Reference-contexts: Early stopping, a common method to prevent overfitting, was used in all experiments: the performance of a trained network was calculated in percentages of incorrectly-processed test instances at the cycle where the classification error on validation material was minimal [Pre94]. For further details on the experiments, we refer to <ref> [Wei96] </ref>. 3.1 Parity-12 BP, BPWD, and BP-SOM have been applied to the parity-12 task, i.e., the task to determine whether a bit string of 0's and 1's of length 12 contains an even number of 1's. <p> SOM of the BP-SOM network is considerably higher than that of the SOM of the BP-trained and BPWD-trained MFNs. Simplified hidden-unit activations By including the SOM in BP-SOM learning, clusters of hidden-unit activation patterns associated with the same class tend to become more similar <ref> [Wei96] </ref>. When analysing the hidden-unit activations in BP-SOM networks, we observed a valuable additional effect, viz. that hidden-unit activations culminated in a stable activity with a very low variance or that they resulted in oscillating between a limited number of discrete values.
Reference: [WVVP97] <author> Weijters, A., Van den Herik, H. J., Van den Bosch, A., and Postma, E. O. </author> <year> (1997). </year> <title> Avoiding overfitting with BP-SOM. </title> <note> To appear in Proceedings of IJCAI'97. </note>
Reference: [WK91] <author> Weiss, S. and Kulikowski, C. </author> <year> (1991). </year> <title> Computer Systems that Learn. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 13

