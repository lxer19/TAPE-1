URL: http://www.cs.orst.edu/~margindr/ML_RG/ray1.ps
Refering-URL: http://www.cs.orst.edu/~margindr/ML_RG/winter97-mlrg.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: lierer@research.cs.orst.edu tadepall@research.cs.orst.edu  
Title: Active Learning with Committees for Text Categorization  
Author: Ray Liere Prasad Tadepalli 
Keyword: Content areas: machine learning, information retrieval Word count: 5000 Tracking number: A292  
Note: This research was partially supported by the National Science Foundation under grant number IRI-9520243.  
Address: Dearborn Hall 303, Corvallis, OR 97331-3202, USA  
Affiliation: Department of Computer Science, Oregon State University,  
Abstract: In many real-world domains like text categorization, supervised learning requires a large number of training examples. In this paper we describe an active learning method that uses a committee of learners to reduce the number of training examples required for learning. Our approach is similar to the Query by Committee framework, where disagreement among the committee members on the predicted label for the input part of the example is used to signal the need for knowing the actual value of the label. Our experiments in text categorization using a committee of Winnow-based learners demonstrate that this approach can reduce the number of labeled training examples required over that used by a single Winnow learner by 1-2 orders of magnitude. This paper is not under review or accepted for publication in another conference or journal. Acknowledgements: The availability of the Reuters-22173 corpus [Reuters] and of the | STAT Data Manipulation and Analysis Programs [Perlman] has greatly assisted in our research to date. 
Abstract-found: 1
Intro-found: 1
Reference: [Angluin88] <author> Dana Angluin, </author> <title> Queries and Concept Learn ing, </title> <booktitle> Machine Learning 2 </booktitle> <pages> 319-342, </pages> <year> 1988 </year>
Reference-contexts: One active learning approach is the membership query paradigm, in which the learner can construct new sets of inputs and request that the teacher provide their labels <ref> [Angluin88] </ref>. In this paper, we are specifically considering the type of active learning in which there exists a set of examples, and the learner chooses which of these it will use for learning. Typically, the cycle proceeds as follows.
Reference: [Apte94] <author> Chidanand Apt, Fred Damerau, Sholom M. Weiss, </author> <title> Automated Learning of Decision Rules for Text Categorization, </title> <booktitle> ACM TOIS 12(2) </booktitle> <pages> 233-251, </pages> <month> July </month> <year> 1994 </year>
Reference-contexts: --- 21,334 unique tokens (maximum --- the actual number depends on the tokenizing method used) --- 679 categories (which in turn are divided into 6 subcate gories: topics, places, people, organizations, exchanges, companies) 4.3 Repeated Trials A variety of approaches have been utilized in previous research using the Reuters corpus <ref> [Hayes90, Lewis91, Apte94] </ref>. There are some differences among researchers as to which articles in the corpus are used, and also there are differences in how the corpus is split into training and test sets.
Reference: [Board87] <author> Raymond A. Board, Leonard Pitt, </author> <title> Semi-Supervised Learning, </title> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <note> Report No. UIUCDCS-R-87-1372, </note> <month> September </month> <year> 1987 </year>
Reference-contexts: Since the method was developed for text categorization, it is able to handle noise as well as a large numbers of features [Lewis94]. While approaches and results vary, these and other studies have concluded that active learning greatly improves learning efficiency by reducing the number of labeled examples used <ref> [Board87, Freund92, Dagan95] </ref>. 2.2 Query by Committee (QBC) Query by Committee (QBC) is a learning method which uses a committee of hypotheses to decide (in a particular manner) for which examples the labels will be requested.
Reference: [Breiman96] <author> Leo Breiman, </author> <title> Bagging Predictors, </title> <booktitle> Machine Learning 24(2) </booktitle> <pages> 123-140, </pages> <month> August </month> <year> 1996 </year>
Reference-contexts: The idea behind using a committee to make predictions is that a committee of several members might be able to outperform a single member <ref> [Freund92, Seung92, Breiman96] </ref>. Each member predicts a label, and these votes are then combined to form a prediction by the committee as a whole. There are several methods for determining the committee's prediction based on the predictions of the individual members.
Reference: [Cohn94] <author> David Cohn, Les Atlas, Richard Ladner, </author> <title> Improving Generalization with Active Learning, </title> <booktitle> Machine Learning 15(2) </booktitle> <pages> 201-221, </pages> <month> May </month> <year> 1994 </year>
Reference-contexts: Active learning refers to machine learning methods that allow the learning program to exert some control over the examples on which it learns <ref> [Cohn94] </ref>. Query by Committee (QBC) is one specific type of active learning which starts with a committee of all possible hypotheses. Each feature vector is presented to the committee. <p> There have been some promising results in the active learning area. Cohn, Atlas, and Ladner developed the theory for an active learning method called selective sampling and then applied it to some small to moderate sized problems as a demonstration of the viability of this new approach <ref> [Cohn94] </ref>. Lewis and Gale developed a method called uncertainty sampling, which is similar conceptually to selective sampling, but which is specifically meant for use in text categorization.
Reference: [Croft95] <author> W. Bruce Croft, </author> <title> Effective Text Retrieval Based on Combining Evidence from the Corpus and Users, </title> <booktitle> IEEE Expert 10(6) </booktitle> <pages> 59-63, </pages> <month> December </month> <year> 1995 </year>
Reference-contexts: Information retrieval in which one allows queries to be phrased using unrestricted text is very difficult because of the complexity of natural language syntax and semantics. It has been found that user input in the form of relevance feedback significantly increases the retrieval efficiency <ref> [Croft95] </ref>. We can think of relevance feedback as allowing the system to learn the user's intentions by asking for the labels for selected examples, and use the active learning with committees paradigm. 7.
Reference: [Dagan95] <author> Ido Dagan, Sean P. Engelson, </author> <title> Committee-Based Sampling for Training Probabilistic Classifiers, </title> <booktitle> in Proceedings: ML95, 1995, p. </booktitle> <pages> 150-157 </pages>
Reference-contexts: Since the method was developed for text categorization, it is able to handle noise as well as a large numbers of features [Lewis94]. While approaches and results vary, these and other studies have concluded that active learning greatly improves learning efficiency by reducing the number of labeled examples used <ref> [Board87, Freund92, Dagan95] </ref>. 2.2 Query by Committee (QBC) Query by Committee (QBC) is a learning method which uses a committee of hypotheses to decide (in a particular manner) for which examples the labels will be requested. <p> Dagan and Engelson proposed a similar method, termed committee-based sampling, for selecting examples to be labeled <ref> [Dagan95] </ref>. The informativeness of an example (and so the desirability of having it labeled) is indicated by the entropy of the predictions of the various hypotheses in the committee. 3.
Reference: [Dumais95] <author> Susan T. Dumais, </author> <title> Latent Semantic Indexing (LSI): TREC-3 Report, in Overview of the Third Text REtrieval Conference (TREC-3), </title> <editor> D. K. </editor> <title> Harman, editor, </title> <institution> National Institute of Standards and Technology, </institution> <note> NIST Special Publication 500-225, </note> <month> April </month> <year> 1995, </year> <pages> p. 219-230 </pages>
Reference-contexts: Such preprocessing is typically quite expensive in both time and space, and some feel that its benefits are questionable in certain situations <ref> [Dumais95] </ref>. Actually, "Winnow" refers to a quite large family of algorithms [Littlestone89]. We hav e thus far used one of the more general (and simpler) Winnow algorithms --- WINNOW2 in [Littlestone88], with some modifications from [Littlestone91]. We will hereafter refer to the algorithm that we use as simply "Winnow".
Reference: [Freund92] <author> Yoav Freund, H. Sebastian Seung, Eli Shamir, Naftali Tishby, </author> <title> Information, Prediction, and Query by Committee, </title> <booktitle> NIPS92, p. </booktitle> <pages> 483-490 </pages>
Reference-contexts: The label is then used to remove all hypotheses from the committee that do not predict the actual label <ref> [Freund92, Seung92, Freund95] </ref>. The learning methods which we have inv estigated are similar to QBC, in that they use disagreement among the committee members as to the value of the predicted label to determine the need for requesting the actual value of that example's label from the teacher. <p> Since the method was developed for text categorization, it is able to handle noise as well as a large numbers of features [Lewis94]. While approaches and results vary, these and other studies have concluded that active learning greatly improves learning efficiency by reducing the number of labeled examples used <ref> [Board87, Freund92, Dagan95] </ref>. 2.2 Query by Committee (QBC) Query by Committee (QBC) is a learning method which uses a committee of hypotheses to decide (in a particular manner) for which examples the labels will be requested. <p> If their predictions form a tie, then the example is assumed to be maximally informative, the algorithm requests the actual label from the teacher, and the algorithm updates the version space <ref> [Freund92, Seung92, Freund95] </ref>. Freund, Seung, Shamir, and Tishby analyzed QBC in detail and showed that the number of examples required in this learning situation is logarithmic in the number of examples required for random example selection learning [Freund92]. <p> Freund, Seung, Shamir, and Tishby analyzed QBC in detail and showed that the number of examples required in this learning situation is logarithmic in the number of examples required for random example selection learning <ref> [Freund92] </ref>. Dagan and Engelson proposed a similar method, termed committee-based sampling, for selecting examples to be labeled [Dagan95]. The informativeness of an example (and so the desirability of having it labeled) is indicated by the entropy of the predictions of the various hypotheses in the committee. 3. <p> The idea behind using a committee to make predictions is that a committee of several members might be able to outperform a single member <ref> [Freund92, Seung92, Breiman96] </ref>. Each member predicts a label, and these votes are then combined to form a prediction by the committee as a whole. There are several methods for determining the committee's prediction based on the predictions of the individual members.
Reference: [Freund95] <author> Yoav Freund, H. Sebastian Seung, Eli Shamir, Naftali Tishby, </author> <title> Selective Sampling Using the Query by Committee Algorithm, </title> <month> July </month> <year> 1995 </year>
Reference-contexts: The label is then used to remove all hypotheses from the committee that do not predict the actual label <ref> [Freund92, Seung92, Freund95] </ref>. The learning methods which we have inv estigated are similar to QBC, in that they use disagreement among the committee members as to the value of the predicted label to determine the need for requesting the actual value of that example's label from the teacher. <p> If their predictions form a tie, then the example is assumed to be maximally informative, the algorithm requests the actual label from the teacher, and the algorithm updates the version space <ref> [Freund92, Seung92, Freund95] </ref>. Freund, Seung, Shamir, and Tishby analyzed QBC in detail and showed that the number of examples required in this learning situation is logarithmic in the number of examples required for random example selection learning [Freund92]. <p> We use QBC with k = 1. One may wonder at the choice of k = 1. Others have found that making k larger does not actually result in much improvement <ref> [Freund95] </ref>. That is also borne out by some of our earlier tests. 3.2 Updating the Hypotheses After the label is seen, the learners adjust the hypotheses in the committee. Typically, each member of the committee learns individually. We chose Winnow as the learning algorithm [Littlestone88].
Reference: [Furnas88] <author> George W. Furnas, Scott Deerwester, Susan T. Dumais, Thomas K. Landauer, Richard A. Harshman, Lynn A. Streeter, Karen E. Lochbaum, </author> <title> Information Retrieval Using a Singular Value Decomposition Model of Latent Semantic Structure, </title> <booktitle> in SIGIR88, p. </booktitle> <pages> 465-480 </pages>
Reference-contexts: The use of Winnow sav es us from a preprocessing step (dimensionality reduction) that is used by many text categorization systems in order to reduce the number of attributes and often to also reduce the percentage of features that are irrelevant <ref> [Furnas88, Yang94] </ref>. Such preprocessing is typically quite expensive in both time and space, and some feel that its benefits are questionable in certain situations [Dumais95]. Actually, "Winnow" refers to a quite large family of algorithms [Littlestone89].
Reference: [Hayes90] <author> Phillip J. Hayes, Peggy M. Andersen, Irene B. Nirenburg, Linda M. Schmandt, </author> <title> TCS: A Shell for Content-Based Text Categorization, </title> <booktitle> in Proceedings of the 6th IEEE CAIA (Conference on Artificial Intelligence for Applications), 1990, IEEE, p. </booktitle> <pages> 320-326 </pages>
Reference-contexts: --- 21,334 unique tokens (maximum --- the actual number depends on the tokenizing method used) --- 679 categories (which in turn are divided into 6 subcate gories: topics, places, people, organizations, exchanges, companies) 4.3 Repeated Trials A variety of approaches have been utilized in previous research using the Reuters corpus <ref> [Hayes90, Lewis91, Apte94] </ref>. There are some differences among researchers as to which articles in the corpus are used, and also there are differences in how the corpus is split into training and test sets.
Reference: [Lewis91] <author> David D. Lewis, </author> <title> Representation and Learning in Information Retrieval, </title> <type> Ph.D. Thesis, </type> <institution> University of Massachusetts at Amherst, </institution> <type> COINS Technical Report 91-93, </type> <month> December </month> <year> 1991 </year>
Reference-contexts: --- 21,334 unique tokens (maximum --- the actual number depends on the tokenizing method used) --- 679 categories (which in turn are divided into 6 subcate gories: topics, places, people, organizations, exchanges, companies) 4.3 Repeated Trials A variety of approaches have been utilized in previous research using the Reuters corpus <ref> [Hayes90, Lewis91, Apte94] </ref>. There are some differences among researchers as to which articles in the corpus are used, and also there are differences in how the corpus is split into training and test sets. <p> We are mainly interested at this point in comparisons among our learning systems. In particular, we want to compare their performance for categories most likely to have ample training data. We used the 10 most frequently occurring topic categories, as listed in <ref> [Lewis91] </ref>, for our experiments. We performed repeated trials for each category, using randomly chosen training-test splits. By using enough repeated random splits, the Central Limit Theorem provides us with assurances that our results are, with a high degree of confidence, a reection of the actual underlying population.
Reference: [Lewis94] <author> David D. Lewis, William A. Gale, </author> <title> A Sequential Algorithm for Training Text Classifiers, </title> <editor> in Proceed ings: SIGIR'94, p. </editor> <month> 3-12 </month>
Reference-contexts: Their method selects for labeling those examples whose membership is most unclear by using an approximation based on Bayes Rule, certain independence assumptions, and logistic regression. Since the method was developed for text categorization, it is able to handle noise as well as a large numbers of features <ref> [Lewis94] </ref>.
Reference: [Littlestone88] <author> Nick Littlestone, </author> <title> Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm, </title> <booktitle> Machine Learning 2(4) </booktitle> <pages> 285-318, </pages> <year> 1988 </year>
Reference-contexts: That is also borne out by some of our earlier tests. 3.2 Updating the Hypotheses After the label is seen, the learners adjust the hypotheses in the committee. Typically, each member of the committee learns individually. We chose Winnow as the learning algorithm <ref> [Littlestone88] </ref>. Winnow is especially suited to large attribute spaces and to situations in which there is a large percentage of irrelevant features. Winnow also has a relatively low space and time complexity and is easy to implement. And, Winnow has been used successfully in other noisy text-based applications [Roth96]. <p> Actually, "Winnow" refers to a quite large family of algorithms [Littlestone89]. We hav e thus far used one of the more general (and simpler) Winnow algorithms --- WINNOW2 in <ref> [Littlestone88] </ref>, with some modifications from [Littlestone91]. We will hereafter refer to the algorithm that we use as simply "Winnow". Conceptually, think of each document as being represented by a data point in some feature space.
Reference: [Littlestone89] <author> Nicholas Littlestone, </author> <title> Mistake Bounds and Logarithmic Linear-Threshold Learning Algorithms, </title> <type> Doctoral Thesis, </type> <institution> Baskin Center for Computer Engineering and Information Sciences, University of Cali fornia at Santa Cruz, UCSC-CRL-89-11, </institution> <month> March </month> <year> 1989 </year>
Reference-contexts: Such preprocessing is typically quite expensive in both time and space, and some feel that its benefits are questionable in certain situations [Dumais95]. Actually, "Winnow" refers to a quite large family of algorithms <ref> [Littlestone89] </ref>. We hav e thus far used one of the more general (and simpler) Winnow algorithms --- WINNOW2 in [Littlestone88], with some modifications from [Littlestone91]. We will hereafter refer to the algorithm that we use as simply "Winnow".
Reference: [Littlestone91] <author> Nick Littlestone, </author> <title> Redundant Noisy Attributes, Attribute Errors, and Linear-Threshold Learning Using Winnow, </title> <booktitle> in Proceedings of the Fourth Annual Workshop on Computational Learning Theory (COLT'91), 1991, p. </booktitle> <pages> 147-156 </pages>
Reference-contexts: Actually, "Winnow" refers to a quite large family of algorithms [Littlestone89]. We hav e thus far used one of the more general (and simpler) Winnow algorithms --- WINNOW2 in [Littlestone88], with some modifications from <ref> [Littlestone91] </ref>. We will hereafter refer to the algorithm that we use as simply "Winnow". Conceptually, think of each document as being represented by a data point in some feature space.
Reference: [Perlman] <author> Gary Perlman, </author> | <note> STAT version 5.4, software and documentation, is described in: http://www.acm.- org/~perlman/statinfo.html and is available from: ftp:/archive.cis.ohio-state.edu/- pub/stat/ </note>
Reference-contexts: Acknowledgements The availability of the Reuters-22173 corpus [Reuters] and of the | STAT Data Manipulation and Analysis Programs <ref> [Perlman] </ref> has greatly assisted in our research to date. This research was partially supported by the National Science Foundation under grant number IRI-9520243.
Reference: [Reuters] <author> Reuters-22173 corpus, </author> <title> a collection of 22,173 indexed documents appearing on the Reuters newswire in 1987; Reuters Ltd, </title> <institution> Carnegie Group, David Lewis, </institution> <note> Information Retrieval Laboratory at the University of Massachusetts; available via ftp from: ciir-ftp.- cs.umass.edu:/pub/reuters1/corpus.tar.Z </note>
Reference-contexts: Prediction is by that same Winnow. This can be thought of as the "base case" --- a single supervised learner and predictor. 4.2 Test Bed All of our experiments were conducted using the titles of newspaper articles from the Reuters-22173 corpus <ref> [Reuters] </ref>, hereafter "Reuters". The Reuters corpus is a collection of 22,173 Reuters newswire articles ("documents") from 1987. It is a 25Mb full text corpus. Each article has been assigned to categories by human indexers. An article may be assigned to any number of categories, including none. <p> We can think of relevance feedback as allowing the system to learn the user's intentions by asking for the labels for selected examples, and use the active learning with committees paradigm. 7. Acknowledgements The availability of the Reuters-22173 corpus <ref> [Reuters] </ref> and of the | STAT Data Manipulation and Analysis Programs [Perlman] has greatly assisted in our research to date. This research was partially supported by the National Science Foundation under grant number IRI-9520243.
Reference: [Roth96] <author> Dan Roth, </author> <title> Applying Winnow to Context Sensitive Spelling Correction, </title> <booktitle> ML96, p. </booktitle> <pages> 182-190 </pages>
Reference-contexts: Winnow is especially suited to large attribute spaces and to situations in which there is a large percentage of irrelevant features. Winnow also has a relatively low space and time complexity and is easy to implement. And, Winnow has been used successfully in other noisy text-based applications <ref> [Roth96] </ref>. The use of Winnow sav es us from a preprocessing step (dimensionality reduction) that is used by many text categorization systems in order to reduce the number of attributes and often to also reduce the percentage of features that are irrelevant [Furnas88, Yang94].
Reference: [Seung92] <author> H. S. Seung, M. Opper, H. Sompolinsky, </author> <title> Query by Committee, </title> <booktitle> in Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (COLT92), </booktitle> <institution> Association for Computing Machin ery:New York, </institution> <address> New York, 1992, p. </address> <pages> 287-294 </pages>
Reference-contexts: The label is then used to remove all hypotheses from the committee that do not predict the actual label <ref> [Freund92, Seung92, Freund95] </ref>. The learning methods which we have inv estigated are similar to QBC, in that they use disagreement among the committee members as to the value of the predicted label to determine the need for requesting the actual value of that example's label from the teacher. <p> If their predictions form a tie, then the example is assumed to be maximally informative, the algorithm requests the actual label from the teacher, and the algorithm updates the version space <ref> [Freund92, Seung92, Freund95] </ref>. Freund, Seung, Shamir, and Tishby analyzed QBC in detail and showed that the number of examples required in this learning situation is logarithmic in the number of examples required for random example selection learning [Freund92]. <p> However, QBC needs to maintain all possible hypotheses consistent with the training data --- the version space --- in some form <ref> [Seung92] </ref>. This is the committee. When data is noisy, this will not be possible. When there is a very large number of candidate hypotheses, this will not be practical. In text categorization, we have data that is noisy. <p> The idea behind using a committee to make predictions is that a committee of several members might be able to outperform a single member <ref> [Freund92, Seung92, Breiman96] </ref>. Each member predicts a label, and these votes are then combined to form a prediction by the committee as a whole. There are several methods for determining the committee's prediction based on the predictions of the individual members.
Reference: [Yang94] <author> Yiming Yang, Christopher G. Chute, </author> <title> An Example-Based Mapping Method for Text Categorization and Retrieval, </title> <journal> ACM Transactions on Information Sys tems 12(3) </journal> <pages> 252-277, </pages> <month> July </month> <year> 1994 </year>
Reference-contexts: The use of Winnow sav es us from a preprocessing step (dimensionality reduction) that is used by many text categorization systems in order to reduce the number of attributes and often to also reduce the percentage of features that are irrelevant <ref> [Furnas88, Yang94] </ref>. Such preprocessing is typically quite expensive in both time and space, and some feel that its benefits are questionable in certain situations [Dumais95]. Actually, "Winnow" refers to a quite large family of algorithms [Littlestone89].
References-found: 22

