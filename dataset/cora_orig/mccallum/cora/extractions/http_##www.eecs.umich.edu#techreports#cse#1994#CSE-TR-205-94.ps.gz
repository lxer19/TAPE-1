URL: http://www.eecs.umich.edu/techreports/cse/1994/CSE-TR-205-94.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse94.html
Root-URL: http://www.eecs.umich.edu
Email: fsga,patregg@eecs.umich.edu  
Title: Reduction of Cache Interference Misses through Selective Bit-permutation Mapping  
Author: Santosh G. Abraham and Henky Agusleo 
Keyword: Key Words: cache interference, cache conflicts, bit-selection mapping, cache modeling, exploiting reuse  
Address: 1301 Beal Avenue, Ann Arbor, MI 48109-2122  
Affiliation: Dept. of EECS, Univ. of Michigan  
Abstract: Cache miss rates have a large and increasing impact on overall performance. In this report, we address the problem of cache interference in regular numerical programs dominated by strided memory access patterns. In our scheme, the interfering strides in each region of memory may be annotated by the programmer, detected at compile time, or even at run time. The algorithm developed in this report uses this stride information to compute a permutation of address bits for each memory region. During page placement, the operating system ensures that some low-order bits of the virtual page number are included in the permutation that forms the physical page number. Simulation results show that this scheme can reduce overall miss rates by nearly a factor of three for the fftpde benchmark from the NAS suite. In an extension of this scheme, the hardware provides support for selecting the address bits used for the set and line fields by permuting the address bits in the desired manner before each cache access. Simulation results show that the selective bit-permutation scheme yields a factor of four improvement in overall miss rates compared to the standard bit-selection scheme for the blocked matrix-multiplication and LU-decomposition programs on typical first-level cache configurations. The tomcatv and swm256 benchmarks have regular strides and the highest miss rates on typical first-level data cache configurations in the SPEC92 suite. Selective bit-permutation achieves a factor of two improvement in overall miss rates on these two benchmarks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> Analysis of Cache Performance for Operating Systems and Multiprogramming. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1988. </year> <note> Available as Technical Report CSL-TR-87-332. </note>
Reference-contexts: We describe in greater detail hardware and software strategies that have been developed to reduce conflict misses. Thiebaut and Stone [14] [19], Agarwal <ref> [1] </ref>, Hill [6] [5] and others have proposed models which can explain or classify misses. These models are useful both for gaining the insight required to develop caching strategies and for evaluating these strategies.
Reference: [2] <author> A. Agarwal and S. D. Pudar. </author> <title> Column-associative caches: A technique for reducing the miss rate of direct-mapped caches. </title> <booktitle> In Proc. of 20th Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 179-190, </pages> <year> 1993. </year>
Reference-contexts: In the victim cache approach, the size of the victim cache limits the number of conflicts that can be handled. In column-associative caches, a conflicting line uses a different hashing function to map into another set in the cache <ref> [2] </ref>. With this organization, the cache usually has the fast access time of a direct-mapped cache but a miss ratio close to a two-way set-associative cache. In the column-associative cache, the number of conflicts that can be handled is proportional to the cache size. <p> Temam [18] develops techniques to analyze data cache interference misses in a class of regular numerical programs. As described briefly in the introductory section, victim caches [8], column-associative caches <ref> [2] </ref> and skewed-associative caches [13] are cache organization schemes that reduce cache interference misses. As illustrated in the example program described in the introductory section, the number of conflicting lines in numerical programs can be much larger than can be handled by any of the above schemes.
Reference: [3] <author> C. Eisenbeis, W. Jalby, D. Windheiser, and F. Bodin. </author> <title> A strategy for array management in local memory. </title> <booktitle> In Proc. 3rd Wkshp. Prog. Lang. & Compilers for Par. Comp., </booktitle> <year> 1990. </year>
Reference-contexts: Capacity misses can be reduced by reordering the access stream so that the reuse distance (i.e. the number of distinct accesses between two references to the same address) is reduced to less than 4 the cache size. Eisenbeis et al <ref> [3] </ref> and Wolf and Lam [21] have developed data locality optimizing algorithms that automatically block a class of regular numerical programs.
Reference: [4] <author> J.D. Gee, M.D. Hill, D.N. Pnevmatikos, and A.J. Smith. </author> <title> Cache performance of the SPEC92 benchmark suite. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 17-27, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: In addition, MM and LU contain loops that are typical in many engineering and scientific applications, whereas tomcatv and swm256 are two of the SPEC92 benchmark programs that exhibit the largest data cache miss rates <ref> [4] </ref>. fftpde manipulates matrices that are larger than those present in the SPEC92 suite and thus is suitable for illustrating misses in bigger caches.
Reference: [5] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture | A Quantitive Approach. </title> <publisher> Morgan Kaufmann Publishers Inc., </publisher> <year> 1990. </year>
Reference-contexts: The latency of cache misses will be higher in such future processors with higher clock rates and wider issue widths. Therefore, it is important to develop techniques to reduce cache miss rates. Caches are characterized by the following three major parameters: cache size, line size and associativity <ref> [5] </ref>. In the standard bit-selection mapping, the binary representation of an address is divided into three contiguous fields: tag, set, and line fields. The set field is used to index into one of the sets of the cache. <p> We describe in greater detail hardware and software strategies that have been developed to reduce conflict misses. Thiebaut and Stone [14] [19], Agarwal [1], Hill [6] <ref> [5] </ref> and others have proposed models which can explain or classify misses. These models are useful both for gaining the insight required to develop caching strategies and for evaluating these strategies.
Reference: [6] <author> M. D. Hill. </author> <title> Aspects of Cache Memory and Instruction Buffer Performance. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1987. </year> <note> Available as Technical Report UCB/CSD 87/381. </note>
Reference-contexts: We describe in greater detail hardware and software strategies that have been developed to reduce conflict misses. Thiebaut and Stone [14] [19], Agarwal [1], Hill <ref> [6] </ref> [5] and others have proposed models which can explain or classify misses. These models are useful both for gaining the insight required to develop caching strategies and for evaluating these strategies.
Reference: [7] <author> W. W. Hwu and P. P. Chang. </author> <title> Achieving high instruction cache performance with an optimizing compiler. </title> <booktitle> In Proc. of 16th Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 242-251, </pages> <year> 1989. </year>
Reference-contexts: Several hardware and software methods have been proposed for reducing mapping (or conflict) misses in instruction and data caches. McFarling [12] and Hwu and Chang <ref> [7] </ref> statically remap basic blocks in memory to eliminate instruction cache mapping misses on frequently executed basic blocks. Temam [18] develops techniques to analyze data cache interference misses in a class of regular numerical programs.
Reference: [8] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proc. of 17th Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <year> 1990. </year>
Reference-contexts: In the victim cache organization, a small fully-associative cache maintains a set of lines that have been evicted due to conflicts in the main set-associative cache <ref> [8] </ref>. In the victim cache approach, the size of the victim cache limits the number of conflicts that can be handled. In column-associative caches, a conflicting line uses a different hashing function to map into another set in the cache [2]. <p> Temam [18] develops techniques to analyze data cache interference misses in a class of regular numerical programs. As described briefly in the introductory section, victim caches <ref> [8] </ref>, column-associative caches [2] and skewed-associative caches [13] are cache organization schemes that reduce cache interference misses. As illustrated in the example program described in the introductory section, the number of conflicting lines in numerical programs can be much larger than can be handled by any of the above schemes.
Reference: [9] <author> N. P. Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> In Proc. of 20th Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 191-202, </pages> <year> 1993. </year>
Reference-contexts: Though conflict misses can be reduced by increasing cache associativity, a larger associativity can influence other parameters of interest unfavorably, such as silicon area (or number of chips) and cache access time. For instance, Jouppi describes why direct-mapped caches are desirable in first-level CPU caches <ref> [9] </ref>. Conflict misses can account for a large fraction of overall cache misses, especially in direct-mapped caches. 1 Recently, several cache organizations have been proposed for reducing cache conflict misses.
Reference: [10] <author> R. E. Kessler and M. D. Hill. </author> <title> Page placement algorithms for large real-indexed caches. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 10(4) </volume> <pages> 338-359, </pages> <year> 1992. </year>
Reference-contexts: We also present three schemes to implement the algorithm. In large physically-indexed caches, the set field overlaps with the page frame number and these bits are affected by the virtual-to-physical translation. These bits can be controlled during the placement of pages by the operating system <ref> [10] </ref>. Thus, in the first scheme called the Limited Selective Bit-Permutation (LSelBiP ), the application provides the operating system with the desired permutation of the address bits (excluding the bits that are part of the page offset). The operating system uses this information in the mapping process. <p> Our approach uses the stride information to change the mapping of addresses to sets and remove the conflicts. Even when there are a large number of conflicting lines under the standard bit-selection mapping, there are few mapping conflicts in the reorganized mapping. Kessler and Hill <ref> [10] </ref> have investigated several careful mapping algorithms to be used by an operating system to place memory pages with the objective of reducing cache conflicts in large physically-indexed caches. <p> The results therefore represent the improvement of our scheme over the page-coloring scheme <ref> [10] </ref> when under each scheme the page mapper can successfully find an available physical page frame with the appropriate characteristics. Even when permutation is restricted to the bits in a page number, the LSelBiP scheme manages to achieve substantial reduction of misses.
Reference: [11] <author> M. Lam, E. E. Rothberg, and M. E. Wolf. </author> <title> The cache performance of blocked algorithms. </title> <booktitle> In Proc. of ASPLOS IV, </booktitle> <year> 1991. </year>
Reference-contexts: Eisenbeis et al [3] and Wolf and Lam [21] have developed data locality optimizing algorithms that automatically block a class of regular numerical programs. Subsequently, Lam et al <ref> [11] </ref> and Temam et al [17] show that cache interference effects in blocked programs can greatly reduce the benefit of blocking using these data locality optimization techniques.
Reference: [12] <author> S. McFarling. </author> <title> Program optimization for instruction caches. </title> <booktitle> In Proc. of ASPLOS III, </booktitle> <year> 1989. </year>
Reference-contexts: Several hardware and software methods have been proposed for reducing mapping (or conflict) misses in instruction and data caches. McFarling <ref> [12] </ref> and Hwu and Chang [7] statically remap basic blocks in memory to eliminate instruction cache mapping misses on frequently executed basic blocks. Temam [18] develops techniques to analyze data cache interference misses in a class of regular numerical programs.
Reference: [13] <author> A. Seznec. </author> <title> A case for two-way skewed-associative caches. </title> <booktitle> In Proc. of 20th Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 169-178, </pages> <year> 1993. </year>
Reference-contexts: In the column-associative cache, the number of conflicts that can be handled is proportional to the cache size. The skewed-asociative cache also uses a secondary hash if the primary hash fails <ref> [13] </ref>. In regular numerical programs typical of many engineering and scientific applications, the access streams have regular strides. Traditionally, numerical programs have been optimized for vector machines. Recently, there has been considerable interest in migrating this workload to workstation type systems because of their greater perceived cost-effectiveness. <p> Temam [18] develops techniques to analyze data cache interference misses in a class of regular numerical programs. As described briefly in the introductory section, victim caches [8], column-associative caches [2] and skewed-associative caches <ref> [13] </ref> are cache organization schemes that reduce cache interference misses. As illustrated in the example program described in the introductory section, the number of conflicting lines in numerical programs can be much larger than can be handled by any of the above schemes.
Reference: [14] <author> H. S. Stone. </author> <title> High-Performance Computer Architecture. </title> <publisher> Addison-Wesley, </publisher> <address> 2 nd edition, </address> <year> 1987. </year>
Reference-contexts: We describe in greater detail hardware and software strategies that have been developed to reduce conflict misses. Thiebaut and Stone <ref> [14] </ref> [19], Agarwal [1], Hill [6] [5] and others have proposed models which can explain or classify misses. These models are useful both for gaining the insight required to develop caching strategies and for evaluating these strategies.
Reference: [15] <author> R. A. Sugumar and S. G. Abraham. </author> <title> Efficient simulation of caches under optimal replacement with applications to miss characterization. </title> <booktitle> In Proc. ACM SIGMETRICS Conf., </booktitle> <pages> pages 24-35, </pages> <year> 1993. </year>
Reference-contexts: In Hill's three C's model, misses are classified into three componenets: compulsory misses misses that occur on first time reference to lines; capacity misses additional misses in a fully-associative LRU cache; and conflict misses additional misses due to the constraints of limited associativity. In the OPT model <ref> [15] </ref>, capacity misses are redefined as the non-compulsory misses from a fully-associative cache with OPT replacement rather than LRU replacement. <p> Total miss rate is the ratio of the number of misses to the total number of references. Using the OPT model, the total miss rate can be broken down into 4 components: compulsory, capacity, mapping, and replacement misses <ref> [15] </ref>. Mapping misses are of particular interest to us, since they arise from the set-mapping strategy and are affected by the choice of set-mapping strategy (e.g. our bit-permutation mapping instead of the usual bit-selection scheme). <p> it excludes such miss components as compulsory and capacity misses that any cache configuration must suffer. 5.2 Environment and Trace Description We compiled six Fortran programs on a DECstation 5000/120, and then used the Mips Pixie tool to generate address traces to feed directly to a modified Cheetah cache simulator <ref> [15] </ref> that simulated the configurations. The programs are given in Table 1, and the trace of each is run to completion. 6 We selected the programs because each has regular strided accesses.
Reference: [16] <author> G. Taylor, P. Davies, and M. Farmwald. </author> <title> The TLB slice-a low cost high-speed address translation mechanism. </title> <booktitle> In Proc. of 17th Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 355-363, </pages> <year> 1990. </year>
Reference-contexts: A cache selects a set for an incoming reference by extracting bits from the set field of the reference's address. When the address is virtual (not translated), the cache is said to be virtually-indexed; when it is physical, the cache is physically-indexed. Taylor et al <ref> [16] </ref> and Wang et al [20] elaborate on why physically-indexed caches are desirable, especially for cases where the needed address translation does not seriously impair access time.
Reference: [17] <author> O. Temam, C. Fricker, and W. Jalby. </author> <title> Impact of cache interference on usual numerical dense loop nests. </title> <journal> Proc. IEEE, </journal> <volume> 81(8) </volume> <pages> 1103-1115, </pages> <month> Aug </month> <year> 1993. </year>
Reference-contexts: Eisenbeis et al [3] and Wolf and Lam [21] have developed data locality optimizing algorithms that automatically block a class of regular numerical programs. Subsequently, Lam et al [11] and Temam et al <ref> [17] </ref> show that cache interference effects in blocked programs can greatly reduce the benefit of blocking using these data locality optimization techniques.
Reference: [18] <author> Olivier Temam. </author> <title> Study and optimization of numerical codes cache behavior. </title> <type> PhD thesis, </type> <institution> University of Rennes, </institution> <year> 1993. </year>
Reference-contexts: As a result, the latency of cache misses in processor cycles is increasing rapidly. Even a 6% first-level cache miss rate can halve the performance of the DEC Alpha processor compared to a system with an ideal memory system <ref> [18] </ref>. Another trend in computer architecture is toward processors that issue more than one instruction/operation per cycle, such as VLIW or superscalar processors. The latency of cache misses will be higher in such future processors with higher clock rates and wider issue widths. <p> Several hardware and software methods have been proposed for reducing mapping (or conflict) misses in instruction and data caches. McFarling [12] and Hwu and Chang [7] statically remap basic blocks in memory to eliminate instruction cache mapping misses on frequently executed basic blocks. Temam <ref> [18] </ref> develops techniques to analyze data cache interference misses in a class of regular numerical programs. As described briefly in the introductory section, victim caches [8], column-associative caches [2] and skewed-associative caches [13] are cache organization schemes that reduce cache interference misses. <p> We will use the blocked matrix multiplication program as an example as we go along. In this example, the strides of accesses are determined by the programmer. Some of the concepts and terminology described here are based on [21] and <ref> [18] </ref>. Particularly, we will employ the terms self- and cross-interference for our subsequent discussion. In addition, we define some new terms relevant to the development of the algorithm. Reuse occurs when the same location in memory is accessed more than once.
Reference: [19] <author> D. Thiebaut. </author> <title> On the fractal dimension of computer programs and its application to the prediction of the cache miss ratio. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 38(7) </volume> <pages> 1012-1026, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: We describe in greater detail hardware and software strategies that have been developed to reduce conflict misses. Thiebaut and Stone [14] <ref> [19] </ref>, Agarwal [1], Hill [6] [5] and others have proposed models which can explain or classify misses. These models are useful both for gaining the insight required to develop caching strategies and for evaluating these strategies.
Reference: [20] <author> W. Wang, J. Baer, and H. M. Levy. </author> <title> Organization and performance of a two-level virtual-real cache hierarchy. </title> <booktitle> In Proc. of 16th Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 140-148, </pages> <year> 1989. </year>
Reference-contexts: When the address is virtual (not translated), the cache is said to be virtually-indexed; when it is physical, the cache is physically-indexed. Taylor et al [16] and Wang et al <ref> [20] </ref> elaborate on why physically-indexed caches are desirable, especially for cases where the needed address translation does not seriously impair access time.
Reference: [21] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proc. of the SIGPLAN '91 Conf. on Prog. Lang. Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <year> 1991. </year>
Reference-contexts: Capacity misses can be reduced by reordering the access stream so that the reuse distance (i.e. the number of distinct accesses between two references to the same address) is reduced to less than 4 the cache size. Eisenbeis et al [3] and Wolf and Lam <ref> [21] </ref> have developed data locality optimizing algorithms that automatically block a class of regular numerical programs. Subsequently, Lam et al [11] and Temam et al [17] show that cache interference effects in blocked programs can greatly reduce the benefit of blocking using these data locality optimization techniques. <p> We will use the blocked matrix multiplication program as an example as we go along. In this example, the strides of accesses are determined by the programmer. Some of the concepts and terminology described here are based on <ref> [21] </ref> and [18]. Particularly, we will employ the terms self- and cross-interference for our subsequent discussion. In addition, we define some new terms relevant to the development of the algorithm. Reuse occurs when the same location in memory is accessed more than once. <p> The blocking factors for blocked programs are determined from the formulas n m 2 + 3m and n m 2 respectively where n is the cache size and m the desired blocking factor <ref> [21] </ref>. 6 Except fftpde, whose run is simulated up to 50 million data references. 17 Name Description MM Blocked multiplication of 2 128x128 matrices; blocking factor = 43 MMbig Blocked multiplication of 2 256x256 matrices; blocking factor = 254 LU Blocked LU-decomposition of a 128x128 matrix; blocking factor = 45 tomcatv
References-found: 21

