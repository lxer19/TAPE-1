URL: ftp://ftp.cs.wisc.edu/wwt/sigmetrics93_wwt.ps
Refering-URL: http://www.cs.wisc.edu/~markhill/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: wwt@cs.wisc.edu  
Title: The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers  
Author: Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood 
Address: 1210 West Dayton Street Madison, WI 53706 USA  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Date: May 1993.  
Note: Appears in: "Proceedings of the 1993 ACM SIGMETRICS Conference,"  Reprinted by permission of ACM.  
Abstract: We have developed a new technique for evaluating cache coherent, shared-memory computers. The Wis-consin Wind Tunnel (WWT) runs a parallel shared-memory program on a parallel computer (CM-5) and uses execution-driven, distributed, discrete-event simulation to accurately calculate program execution time. WWT is a virtual prototype that exploits similarities between the system under design (the target) and an existing evaluation platform (the host). The host directly executes all target program instructions and memory references that hit in the target cache. WWT's shared memory uses the CM-5 memory's error-correcting code (ECC) as valid bits for a fine-grained extension of shared virtual memory. Only memory references that miss in the target cache trap to WWT, which simulates a cache-coherence protocol. WWT correctly interleaves target machine events and calculates target program execution time. WWT runs on parallel computers with greater speed and memory capacity than uniprocessors. WWT's simulation time decreases as target system size increases for fixed-size problems and holds roughly constant as the target system and problem scale. fl This work is supported in part by NSF PYI Awards CCR-9157366 and MIPS-8957278, NSF Grant CCR-9101035, Univ. of Wisconsin Graduate School Grant, Wisconsin Alumni Research Foundation Fellowship and donations from A.T.&T. Bell Laboratories and Digital Equipment Corporation. Our Thinking Machines CM-5 was purchased through NSF Institutional Infrastructure Grant No. CDA-9024618 with matching funding from the Univ. of Wisconsin Graduate School. c fl 1993 ACM. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and that notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Richard Simoni, Mark Horowitz, and John Hennessy. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference-contexts: A processor node handles a cache miss by sending a message to the referenced block's home node and receiving the block's contents in a reply message. Processors use a directory protocol to keep cache and memory blocks coherent <ref> [1] </ref>. Typically, a directory is distributed so a directory entry and mem ory block reside on the same node. The directory protocol determines the contents of a directory entry and the messages used to maintain coherence. We assume that the execution time for each instruction is fixed. <p> WWT facilitates these studies, because it clearly separates the modules that specify a target machine's cache coherence protocol and the rest of WWT. To date, WWT runs DASH [20], Dir i NB for i = 1 : : : N <ref> [1] </ref>, Dir i B for i = 0 : : : N [1], and Dir 1 SW variants [15, 34]. 8 Conclusions This paper describes the Wisconsin Wind Tunnel (WWT)|a system for evaluating cache-coherent, shared-memory computers on a Thinking Machines CM-5. <p> To date, WWT runs DASH [20], Dir i NB for i = 1 : : : N <ref> [1] </ref>, Dir i B for i = 0 : : : N [1], and Dir 1 SW variants [15, 34]. 8 Conclusions This paper describes the Wisconsin Wind Tunnel (WWT)|a system for evaluating cache-coherent, shared-memory computers on a Thinking Machines CM-5. WWT runs parallel shared-memory binaries and concurrently calculates the programs' execution times on the target hardware with a distributed, discrete-event simulation.
Reference: [2] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera Computer System. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: WWT could run on machines besides a CM-5. The key CM-5 features exploited by WWT are precise interrupts on ECC errors, fast user-level messages, and efficient reductions. Without the ECC mechanism, fine-grained shared virtual memory could be implemented with memory tag bits (e.g., Tera <ref> [2] </ref>). On a machine with neither mechanism, qpt can add additional code to a program to directly test if a shared-memory reference will miss, as in Tango/Dixie.
Reference: [3] <author> Rassul Ayani. </author> <title> A Parallel Simulation Scheme Based on the Distance Between Objects. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 113-118, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: WWT uses the CM-5's fast reductions to determine when all messages are delivered. WWT currently processes requests on a target directory at the request's arrival time rather than en-queuing the request until its service time. This optimization, advocated by Ayani <ref> [3] </ref>, is possible because the directory uses first-come-first-serve queuing in which an event is unaffected by events that arrive after it. The optimization is important because directory service and completion events need not be enqueued and the running program is interrupted less often. <p> Second, it facilitates successive refinement of software and hardware by providing a much faster response time. Other researchers <ref> [12, 16, 18, 24, 25, 3, 28, 27] </ref> have studied simulation systems for parallel hosts. These systems perform a discrete-event simulation of a parallel target using a discrete-event workload. These systems and WWT exploit the parallel host by avoiding a fine-grained global simulation clock that advances in lock-step. <p> WWT's method for coordinating nodes for the exe 10 cution time calculation is a conservative, synchronous method. Synchronous algorithms use the target times of some or all neighboring nodes to ensure causality <ref> [3, 24, 28, 27] </ref>. The difference between the target times of two nodes is called lag and the minimum target time for one node to affect another is called distance. Synchronous algorithms ensure the lag between each pair of nodes is always less than the distance.
Reference: [4] <author> Thomas Ball and James R. Larus. </author> <title> Optimally Profiling and Tracing Programs. </title> <booktitle> In Conference Record of the Nineteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 59-70, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: WWT ensures that the target program always returns before the next event by using a modified version of qpt <ref> [4] </ref> to add quick tests to the target program's binary executable. Before resuming the target program, WWT sets a counter in a global register to permitted . Conceptually after each instruction, the instrumented target program decrements the counter by the instruction's static cycle cost.
Reference: [5] <author> Bob Boothe. </author> <title> Fast Accurate Simulation of Large Shared Memory Multiprocessors. </title> <type> Technical Report CSD 92/682, </type> <institution> Computer Science Division (EECS), University of Califor-nia at Berkeley, </institution> <month> January </month> <year> 1992. </year> <month> 12 </month>
Reference-contexts: In WWT, the host directly executes all target instructions and memory references that hit in the target cache. Direct execution means that the host hardware executes target program operations|for example, a floating-point multiply instruction runs as a floating-point multiply <ref> [5, 6, 9, 11] </ref>. Simulation is only necessary for target operations that a host machine does not support. Direct execution runs orders of magnitude faster than software simulation [9]. A unique aspect of WWT is that it directly executes memory references that hit in a target cache. <p> Of course, even academic hardware prototypes are not completely realistic, because of constraints on infrastructure, money, and availability of experienced designers. An alternative to hardware prototyping is software simulation. MIT Proteus [6], Berkeley FAST <ref> [5] </ref>, Rice Parallel Processing Testbed [9], and Stanford Tango [11] are simulation systems that simulate parallel machines by running actual programs (as opposed to distribution-driven or trace-driven workloads). All use direct execution and run on a uniprocessor host.
Reference: [6] <author> Eric A. Brewer, Chrysanthos N Dellarocas, Adrian Colbrook, and William Weihl. PROTEUS: </author> <title> A High-Performance Parallel-Architecture Simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: In WWT, the host directly executes all target instructions and memory references that hit in the target cache. Direct execution means that the host hardware executes target program operations|for example, a floating-point multiply instruction runs as a floating-point multiply <ref> [5, 6, 9, 11] </ref>. Simulation is only necessary for target operations that a host machine does not support. Direct execution runs orders of magnitude faster than software simulation [9]. A unique aspect of WWT is that it directly executes memory references that hit in a target cache. <p> Of course, even academic hardware prototypes are not completely realistic, because of constraints on infrastructure, money, and availability of experienced designers. An alternative to hardware prototyping is software simulation. MIT Proteus <ref> [6] </ref>, Berkeley FAST [5], Rice Parallel Processing Testbed [9], and Stanford Tango [11] are simulation systems that simulate parallel machines by running actual programs (as opposed to distribution-driven or trace-driven workloads). All use direct execution and run on a uniprocessor host.
Reference: [7] <author> David Chaiken, John Kubiatowics, and Anant Agar-wal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The two closely-related methods for studying large-scale parallel computers are hardware prototyping and software simulation. The methods are not mutually-exclusive as simulation usually precedes prototyping. Stanford DASH [20], MIT Alewife <ref> [7] </ref>, and MIT J-Machine [10] are projects that built hardware prototypes. WWT offers at least four advantages over hardware prototyping. First, WWT ran programs after two months, ran large applications four months later (instead of multiple years), and produced accurate cycle counts in two more months.
Reference: [8] <author> Robert F. Cmelik, Shing I. Kong, David R. Ditzel, and Edmund J. Kelly. </author> <title> An Analysis of MIPS and SPARC Instruction Set Utilization on the SPEC Benchmarks. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 290-302, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The estimated cycle counts|target execution times|are not quite as close. Some deviation is unavoidable because Tango/Dixie runs on a MIPS, while WWT runs on a SPARC. Cmelik et al. showed that programs on these architectures typically perform within 10% for similar compilers <ref> [8] </ref>. To minimize instruction set differences, we assumed all instructions execute in a single cycle. A problem, however, is that Tango/Dixie counts instructions in compiler-generated assembly code. Because the MIPS assembler expands pseudo-instructions, assembly code differs from executed binary code.
Reference: [9] <author> R.C. Covington, S. Madala, V. Mehta, J.R. Jump, and J.B. Sinclair. </author> <title> The Rice Parallel Processing Testbed. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Paper studies, analytic models, and simulations of abstract workloads can uncover architectural flaws. Nevertheless, running real applications and system software can expose software and hardware problems that other techniques cannot find and encourages improvements through successive refinement <ref> [9] </ref>. The two well-known methods for evaluating parallel computer architectures with real workloads are execution-driven simulation and hardware prototyp-ing. Call the computer under study the target machine and the existing computer used to perform the evaluation the host machine. <p> With execution-driven simulation, researchers write a host machine program, called a simulator , that interprets a target machine program, mimics the operation of the target machine, and estimates its performance <ref> [9] </ref>. By contrast, with hardware prototyping, researchers build a copy (or a few copies) of the target system, execute target programs, and measure their performance [20]. Execution-driven simulation and hardware proto-typing offer largely complementary advantages and disadvantages. <p> In WWT, the host directly executes all target instructions and memory references that hit in the target cache. Direct execution means that the host hardware executes target program operations|for example, a floating-point multiply instruction runs as a floating-point multiply <ref> [5, 6, 9, 11] </ref>. Simulation is only necessary for target operations that a host machine does not support. Direct execution runs orders of magnitude faster than software simulation [9]. A unique aspect of WWT is that it directly executes memory references that hit in a target cache. <p> Direct execution means that the host hardware executes target program operations|for example, a floating-point multiply instruction runs as a floating-point multiply [5, 6, 9, 11]. Simulation is only necessary for target operations that a host machine does not support. Direct execution runs orders of magnitude faster than software simulation <ref> [9] </ref>. A unique aspect of WWT is that it directly executes memory references that hit in a target cache. Other direct-execution simulators test for a cache hit before every memory reference. <p> Of course, even academic hardware prototypes are not completely realistic, because of constraints on infrastructure, money, and availability of experienced designers. An alternative to hardware prototyping is software simulation. MIT Proteus [6], Berkeley FAST [5], Rice Parallel Processing Testbed <ref> [9] </ref>, and Stanford Tango [11] are simulation systems that simulate parallel machines by running actual programs (as opposed to distribution-driven or trace-driven workloads). All use direct execution and run on a uniprocessor host. <p> These systems and WWT exploit the parallel host by avoiding a fine-grained global simulation clock that advances in lock-step. The key advance of WWT over these systems is that WWT supports dynamic execution of target program binaries. As discussed by Covington, et al. <ref> [9] </ref>, execution-driven simulation of computer systems produces more detailed results than either distribution-or trace-driven simulation. To the best of our knowledge, all other distributed, discrete-event simulation systems simulate queuing networks or computer systems with stochastic workload models, not real programs.
Reference: [10] <author> William J. Dally, Andrew Chien, Stuart Fiske, Waldemar Horwat, John Keen, Michael Larivee, Rich Nuth, Scott Wills, Paul Carrick, and Greg Flyer. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <editor> In G. X. Ritter, editor, </editor> <booktitle> Proc. Information Processing 89. </booktitle> <publisher> Elsevier North-Holland, Inc., </publisher> <year> 1989. </year>
Reference-contexts: The two closely-related methods for studying large-scale parallel computers are hardware prototyping and software simulation. The methods are not mutually-exclusive as simulation usually precedes prototyping. Stanford DASH [20], MIT Alewife [7], and MIT J-Machine <ref> [10] </ref> are projects that built hardware prototypes. WWT offers at least four advantages over hardware prototyping. First, WWT ran programs after two months, ran large applications four months later (instead of multiple years), and produced accurate cycle counts in two more months.
Reference: [11] <author> Helen Davis, Stephen R. Goldschmidt, and John Hen-nessy. </author> <title> Multiprocessor Simulation and Tracing Using Tango. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing (Vol. II Software), </booktitle> <pages> pages II99-107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: In WWT, the host directly executes all target instructions and memory references that hit in the target cache. Direct execution means that the host hardware executes target program operations|for example, a floating-point multiply instruction runs as a floating-point multiply <ref> [5, 6, 9, 11] </ref>. Simulation is only necessary for target operations that a host machine does not support. Direct execution runs orders of magnitude faster than software simulation [9]. A unique aspect of WWT is that it directly executes memory references that hit in a target cache. <p> Of course, even academic hardware prototypes are not completely realistic, because of constraints on infrastructure, money, and availability of experienced designers. An alternative to hardware prototyping is software simulation. MIT Proteus [6], Berkeley FAST [5], Rice Parallel Processing Testbed [9], and Stanford Tango <ref> [11] </ref> are simulation systems that simulate parallel machines by running actual programs (as opposed to distribution-driven or trace-driven workloads). All use direct execution and run on a uniprocessor host.
Reference: [12] <author> Richard M. Fujimoto. </author> <title> Parallel Discrete Event Simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> Octo-ber </month> <year> 1990. </year>
Reference-contexts: WWT does not use the vector units. 2.3 Distributed, Discrete-Event Simu lation In discrete-event simulation, the target system (sometimes called the physical system) is modeled by a set of state variables that make discrete transitions in response to events <ref> [12] </ref>. A uniprocessor host computer performs discrete-event simulation by removing the first event from an event list (ordered by target time), processing the event to change the system state, and scheduling zero or more events. <p> WWT adds logical clocks to target processes, protocol messages, directory hardware, etc. to enable WWT to model latencies, dependencies, and queuing. The result is an event-driven simulation of a parallel target machine that runs on a parallel host machine. WWT differs from other distributed, discrete-event simulators <ref> [12] </ref> because its workload is an executing program. Driving a simulation from an executing target program requires new techniques because of the frequency with which the program modifies the target machine's state. By contrast, a queuing network simulation only modifies target state on job arrival and departure. <p> Second, it facilitates successive refinement of software and hardware by providing a much faster response time. Other researchers <ref> [12, 16, 18, 24, 25, 3, 28, 27] </ref> have studied simulation systems for parallel hosts. These systems perform a discrete-event simulation of a parallel target using a discrete-event workload. These systems and WWT exploit the parallel host by avoiding a fine-grained global simulation clock that advances in lock-step.
Reference: [13] <author> James R. Goodman. </author> <title> Coherency for Multiprocessor Virtual Address Caches. </title> <booktitle> In Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS II), </booktitle> <pages> pages 408-419, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Finally, WWT allocates cache page frames in 4K-byte blocks, but usually uses smaller blocks ( 32 bytes) for data transfers and coherence. The IBM 360/85 cache [23] used different block sizes for allocation and transfer. Goodman <ref> [13] </ref> discusses the use of different block sizes for allocation, transfer, and coherence. These ideas are also used more recently by Tamir [32] and Kendall Square [17]. 7 Extensions and Future Work Currently, WWT is a novel simulator of cache-coherent, shared-memory computers that runs on a Thinking Machines CM-5.
Reference: [14] <author> John L. Gustafson. </author> <title> Reevaluating Amdahl's Law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Since Tango/Dixie simulates all processors serially, its execution time triples from 4 to 16 processors. WWT's elapsed time again decreases. The advantage of WWT is more striking for scaled speedup. Gustafson <ref> [14] </ref> and others have argued that parallel speedup is uninteresting: give scientists a larger machine and they will solve larger problems in the same amount of time. This argument calls for scaling a problem's size approximately linearly with the number of processor nodes.
Reference: [15] <author> Mark D. Hill, James R. Larus, Steven K. Reinhardt, and David A. Wood. </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <pages> pages 262-273, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: To date, WWT runs DASH [20], Dir i NB for i = 1 : : : N [1], Dir i B for i = 0 : : : N [1], and Dir 1 SW variants <ref> [15, 34] </ref>. 8 Conclusions This paper describes the Wisconsin Wind Tunnel (WWT)|a system for evaluating cache-coherent, shared-memory computers on a Thinking Machines CM-5. WWT runs parallel shared-memory binaries and concurrently calculates the programs' execution times on the target hardware with a distributed, discrete-event simulation.
Reference: [16] <author> David R. Jefferson. </author> <title> Virtual Time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Second, it facilitates successive refinement of software and hardware by providing a much faster response time. Other researchers <ref> [12, 16, 18, 24, 25, 3, 28, 27] </ref> have studied simulation systems for parallel hosts. These systems perform a discrete-event simulation of a parallel target using a discrete-event workload. These systems and WWT exploit the parallel host by avoiding a fine-grained global simulation clock that advances in lock-step.
Reference: [17] <institution> Kendall Square Research. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: The IBM 360/85 cache [23] used different block sizes for allocation and transfer. Goodman [13] discusses the use of different block sizes for allocation, transfer, and coherence. These ideas are also used more recently by Tamir [32] and Kendall Square <ref> [17] </ref>. 7 Extensions and Future Work Currently, WWT is a novel simulator of cache-coherent, shared-memory computers that runs on a Thinking Machines CM-5. We believe that virtual prototyping is very general concept and can be used to model any target system and to extend WWT to run on other hosts.
Reference: [18] <author> Pavlos Konas and Pen-Chung Yew. </author> <title> Parallel Discrete Event Simulation on Shared-Memory Multiprocessors. </title> <booktitle> In Proc. of the 24th Annual Simulation Symposium, </booktitle> <pages> pages 134-148, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Second, it facilitates successive refinement of software and hardware by providing a much faster response time. Other researchers <ref> [12, 16, 18, 24, 25, 3, 28, 27] </ref> have studied simulation systems for parallel hosts. These systems perform a discrete-event simulation of a parallel target using a discrete-event workload. These systems and WWT exploit the parallel host by avoiding a fine-grained global simulation clock that advances in lock-step. <p> After each barrier, nodes cooperate to determine the next barrier time. WWT eliminates this phase by using a fixed time between barriers. Konas and Yew and Lin et al. perform parallel simulations of multiprocessors, but neither system directly executes target programs. Konas and Yew <ref> [18, 19] </ref> use distribution-driven workloads and simulation algorithms that rely on a shared-memory host. Lin et al. [22] use trace-driven workloads in which the trace and interactions among processors are unaffected by a memory-reference miss (as if a cache hit and miss take the same time).
Reference: [19] <author> Pavlos Konas and Pen-Chung Yew. </author> <title> Synchronous Parallel Discrete Event Simulation on Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of 6th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 12-21, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: After each barrier, nodes cooperate to determine the next barrier time. WWT eliminates this phase by using a fixed time between barriers. Konas and Yew and Lin et al. perform parallel simulations of multiprocessors, but neither system directly executes target programs. Konas and Yew <ref> [18, 19] </ref> use distribution-driven workloads and simulation algorithms that rely on a shared-memory host. Lin et al. [22] use trace-driven workloads in which the trace and interactions among processors are unaffected by a memory-reference miss (as if a cache hit and miss take the same time).
Reference: [20] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: By contrast, with hardware prototyping, researchers build a copy (or a few copies) of the target system, execute target programs, and measure their performance <ref> [20] </ref>. Execution-driven simulation and hardware proto-typing offer largely complementary advantages and disadvantages. Simulators can be constructed relatively quickly, can be modified easily, and can compare radically different alternatives. However, simulators often are too slow and run on machines without enough memory to simulate realistic workloads and system parameters. <p> Tango is a direct-execution simulation system that runs on a uniprocessor MIPS-based workstation. Tango instruments a target program's assembly code with additional code that computes instruction execution times and calls the simulator before loads and stores. The Dixie memory system simulation models the DASH prototype hardware <ref> [20] </ref>. Together, Dixie and Tango execute parallel shared-memory programs and estimate system performance. Our original intent was to perform a black box comparison by writing our own DASH simulator for WWT (WWT/DASH) from only published details and reproducing DASH performance results. <p> The two closely-related methods for studying large-scale parallel computers are hardware prototyping and software simulation. The methods are not mutually-exclusive as simulation usually precedes prototyping. Stanford DASH <ref> [20] </ref>, MIT Alewife [7], and MIT J-Machine [10] are projects that built hardware prototypes. WWT offers at least four advantages over hardware prototyping. First, WWT ran programs after two months, ran large applications four months later (instead of multiple years), and produced accurate cycle counts in two more months. <p> We are using WWT to compare existing and new cache-coherence protocols. WWT facilitates these studies, because it clearly separates the modules that specify a target machine's cache coherence protocol and the rest of WWT. To date, WWT runs DASH <ref> [20] </ref>, Dir i NB for i = 1 : : : N [1], Dir i B for i = 0 : : : N [1], and Dir 1 SW variants [15, 34]. 8 Conclusions This paper describes the Wisconsin Wind Tunnel (WWT)|a system for evaluating cache-coherent, shared-memory computers on a Thinking
Reference: [21] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Other direct-execution simulators test for a cache hit before every memory reference. WWT uses the CM-5 memory's error-correcting code (ECC) as valid bits to build a fine-grained extension of Li's shared virtual memory <ref> [21] </ref>. Only the memory references that miss in the target cache trap to WWT, which simulates a cache coherence protocol by explicitly sending messages. WWT, like uniprocessor execution-driven simulators, correctly interleaves target machine events and calculates target program execution times. <p> Our approach is a fine-grained extension of Li's shared virtual memory. With this technique, WWT directly executes all instructions and all memory references that hit in the target cache. Only target cache misses require simulation. Li's shared virtual memory (SVM) <ref> [21] </ref> implements shared memory on a distributed-memory system. Pages local to a processor are mapped into the processor's virtual address space. A remote page is left unmapped so a reference to it causes a page fault, which invokes system software to obtain the page. <p> WWT regains control on all cache misses so it can simulate the target cache. Assume for a moment that the target cache's block size equals the host page size (4K bytes). In this case, WWT handles a cache miss in a similar manner as shared virtual memory (SVM) <ref> [21] </ref>. Upon a cache miss, WWT allocates a page from the pool of CPFs and changes the target's page tables so the faulting address maps to the CPF. WWT then simulates the target cache-coherence protocol by sending messages to other processors to obtain the referenced block and update directory information.
Reference: [22] <author> Y.-B. Lin, J.-L. Baer, and E. D. Lazowska. </author> <title> Tailoring a Parallel Trace-Driven Simulation Technique to Specific Multiprocessor Cache Coherence Protocols. </title> <type> Technical Report 88-01-02, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: Konas and Yew and Lin et al. perform parallel simulations of multiprocessors, but neither system directly executes target programs. Konas and Yew [18, 19] use distribution-driven workloads and simulation algorithms that rely on a shared-memory host. Lin et al. <ref> [22] </ref> use trace-driven workloads in which the trace and interactions among processors are unaffected by a memory-reference miss (as if a cache hit and miss take the same time). WWT models reference times more accurately to compute target execution time and allows memory system behavior to affect program execution.
Reference: [23] <author> J. S. Liptay. </author> <title> Structural Aspects of the System/360 Model 85, Part II: The Cache. </title> <journal> IBM Systems Journal, </journal> <volume> 7(1) </volume> <pages> 15-21, </pages> <year> 1968. </year>
Reference-contexts: WWT models reference times more accurately to compute target execution time and allows memory system behavior to affect program execution. Finally, WWT allocates cache page frames in 4K-byte blocks, but usually uses smaller blocks ( 32 bytes) for data transfers and coherence. The IBM 360/85 cache <ref> [23] </ref> used different block sizes for allocation and transfer. Goodman [13] discusses the use of different block sizes for allocation, transfer, and coherence.
Reference: [24] <author> Boris D. Lubachevsky. </author> <title> Efficient Distributed Event-Driven Simulations of Multiple-Loop Networks. </title> <journal> Communications of the ACM, </journal> <volume> 32(2) </volume> <pages> 111-123, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Second, it facilitates successive refinement of software and hardware by providing a much faster response time. Other researchers <ref> [12, 16, 18, 24, 25, 3, 28, 27] </ref> have studied simulation systems for parallel hosts. These systems perform a discrete-event simulation of a parallel target using a discrete-event workload. These systems and WWT exploit the parallel host by avoiding a fine-grained global simulation clock that advances in lock-step. <p> WWT's method for coordinating nodes for the exe 10 cution time calculation is a conservative, synchronous method. Synchronous algorithms use the target times of some or all neighboring nodes to ensure causality <ref> [3, 24, 28, 27] </ref>. The difference between the target times of two nodes is called lag and the minimum target time for one node to affect another is called distance. Synchronous algorithms ensure the lag between each pair of nodes is always less than the distance. <p> Our approach is closely related to the methods of Ayani, Lubachevsky and Nicol. Ayani uses barriers, but runs on a shared-memory host machine and simulates at most one event per node per barrier. Lubachevsky <ref> [24] </ref> repeatedly broadcasts the time of the slowest node (to maintain a bounded lag), but allows the lag to be greater than the distance between some nodes. Nicol [27] uses barriers at variable intervals. After each barrier, nodes cooperate to determine the next barrier time.
Reference: [25] <author> Jayadev Misra. </author> <title> Distributed-Discrete Event Simulation. </title> <journal> ACM Computing Surveys, </journal> <volume> 18(1) </volume> <pages> 39-65, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Second, it facilitates successive refinement of software and hardware by providing a much faster response time. Other researchers <ref> [12, 16, 18, 24, 25, 3, 28, 27] </ref> have studied simulation systems for parallel hosts. These systems perform a discrete-event simulation of a parallel target using a discrete-event workload. These systems and WWT exploit the parallel host by avoiding a fine-grained global simulation clock that advances in lock-step.
Reference: [26] <author> Todd Mowry and Anoop Gupta. </author> <title> Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: We could not simulate these libraries with Tango, because we did not have MIPS math library sources. not demonstrate that either system correctly modeled a real computer. We compared WWT against Dixie, a Tango-based simulation of the DASH multiprocessor <ref> [26] </ref>. Tango is a direct-execution simulation system that runs on a uniprocessor MIPS-based workstation. Tango instruments a target program's assembly code with additional code that computes instruction execution times and calls the simulator before loads and stores. The Dixie memory system simulation models the DASH prototype hardware [20].
Reference: [27] <author> David Nicol. </author> <title> Conservative Parallel Simulation of Priority Class Queueing Networks. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(3) </volume> <pages> 398-412, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Second, it facilitates successive refinement of software and hardware by providing a much faster response time. Other researchers <ref> [12, 16, 18, 24, 25, 3, 28, 27] </ref> have studied simulation systems for parallel hosts. These systems perform a discrete-event simulation of a parallel target using a discrete-event workload. These systems and WWT exploit the parallel host by avoiding a fine-grained global simulation clock that advances in lock-step. <p> WWT's method for coordinating nodes for the exe 10 cution time calculation is a conservative, synchronous method. Synchronous algorithms use the target times of some or all neighboring nodes to ensure causality <ref> [3, 24, 28, 27] </ref>. The difference between the target times of two nodes is called lag and the minimum target time for one node to affect another is called distance. Synchronous algorithms ensure the lag between each pair of nodes is always less than the distance. <p> Ayani uses barriers, but runs on a shared-memory host machine and simulates at most one event per node per barrier. Lubachevsky [24] repeatedly broadcasts the time of the slowest node (to maintain a bounded lag), but allows the lag to be greater than the distance between some nodes. Nicol <ref> [27] </ref> uses barriers at variable intervals. After each barrier, nodes cooperate to determine the next barrier time. WWT eliminates this phase by using a fixed time between barriers. Konas and Yew and Lin et al. perform parallel simulations of multiprocessors, but neither system directly executes target programs.
Reference: [28] <author> David M. Nicol. </author> <title> Performance Bounds on Parallel Self-Initiating Discrete-Event Simulations. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 1(1) </volume> <pages> 24-50, </pages> <month> Jan-uary </month> <year> 1991. </year>
Reference-contexts: Second, it facilitates successive refinement of software and hardware by providing a much faster response time. Other researchers <ref> [12, 16, 18, 24, 25, 3, 28, 27] </ref> have studied simulation systems for parallel hosts. These systems perform a discrete-event simulation of a parallel target using a discrete-event workload. These systems and WWT exploit the parallel host by avoiding a fine-grained global simulation clock that advances in lock-step. <p> WWT's method for coordinating nodes for the exe 10 cution time calculation is a conservative, synchronous method. Synchronous algorithms use the target times of some or all neighboring nodes to ensure causality <ref> [3, 24, 28, 27] </ref>. The difference between the target times of two nodes is called lag and the minimum target time for one node to affect another is called distance. Synchronous algorithms ensure the lag between each pair of nodes is always less than the distance.
Reference: [29] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: These three benchmarks, two from the SPLASH benchmark suite <ref> [29] </ref> and a parallelized version of a SPEC benchmark [31], were chosen because they make little use of the math libraries. We could not simulate these libraries with Tango, because we did not have MIPS math library sources. not demonstrate that either system correctly modeled a real computer. <p> We would also like to acknowledge the invaluable assistance provided by the Stanford DASH project. Helen Davis, Kourosh Gharachorloo, Stephen Gold-schmidt, Anoop Gupta, John Hennessy, and Todd Mowry wrote and generously provided Tango and Dixie. Singh et al. <ref> [29] </ref> wrote and distributed the SPLASH benchmarks.
Reference: [30] <author> Richard L. Sites, Anton Chernoff, Matthew B. Kirk, Mau-rice P. Marks, and Scott G. Robinson. </author> <title> Binary Translation. </title> <journal> Communications of the ACM, </journal> <volume> 36(2) </volume> <pages> 69-81, </pages> <month> Febru-ary </month> <year> 1993. </year>
Reference-contexts: Instruction set extensions|e.g., a compare-and-swap instruction|are implemented by existing instructions (either in-line, in a call, or with a simulator trap). qpt models the instruction's performance by charging an assumed cycle count. qpt can also form the basis of a binary-to-binary translator that permits non-SPARC instruction sets <ref> [30] </ref>. This technique could model a dynamic pipeline of arbitrary complexity. At some point, however, the complexity of translating an instruction set outweights the benefits of direct execution and an instruction simulator is a better alternative. Fine-grained shared-virtual memory efficiently simulates caches that use pseudo-random replacement.
Reference: [31] <author> SPEC. </author> <title> SPEC Benchmark Suite Release 1.0, </title> <month> Winter </month> <year> 1990. </year>
Reference-contexts: These three benchmarks, two from the SPLASH benchmark suite [29] and a parallelized version of a SPEC benchmark <ref> [31] </ref>, were chosen because they make little use of the math libraries. We could not simulate these libraries with Tango, because we did not have MIPS math library sources. not demonstrate that either system correctly modeled a real computer.
Reference: [32] <author> Yuval Tamir and G. Janakiraman. </author> <title> Hierarchical Coherency Management for Shared Virtual Memory Multicomput-ers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 408-419, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The IBM 360/85 cache [23] used different block sizes for allocation and transfer. Goodman [13] discusses the use of different block sizes for allocation, transfer, and coherence. These ideas are also used more recently by Tamir <ref> [32] </ref> and Kendall Square [17]. 7 Extensions and Future Work Currently, WWT is a novel simulator of cache-coherent, shared-memory computers that runs on a Thinking Machines CM-5.
Reference: [33] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <year> 1991. </year>
Reference-contexts: Queuing delay is included in the cache miss cost. Network topology and contention are ignored, and the latency of all messages is fixed at T cycles. 2.2 TMC CM-5 Host The host platform for WWT is a Thinking Machines CM-5 <ref> [33] </ref>. The architecture supports from 32 to 16,384 processing nodes. Each node contains a SPARC processor, cache, memory management unit (MMU), custom vector/memory controller units, up to 128 MB of memory, and a custom network interface. Nodes are connected by two user-accessible networks.
Reference: [34] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt. </author> <title> Mechanisms for Cooperative Shared Memory. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-168, </pages> <month> May </month> <year> 1993. </year> <month> 13 </month>
Reference-contexts: To date, WWT runs DASH [20], Dir i NB for i = 1 : : : N [1], Dir i B for i = 0 : : : N [1], and Dir 1 SW variants <ref> [15, 34] </ref>. 8 Conclusions This paper describes the Wisconsin Wind Tunnel (WWT)|a system for evaluating cache-coherent, shared-memory computers on a Thinking Machines CM-5. WWT runs parallel shared-memory binaries and concurrently calculates the programs' execution times on the target hardware with a distributed, discrete-event simulation.
References-found: 34

