URL: ftp://www.cs.rutgers.edu/pub/technical-reports/dcs-tr-344.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: iftode@cs.rutgers.edu  jps@cs.princeton.edu  
Title: Shared Virtual Memory: Progress and Challenges  
Author: Liviu Iftode Jaswinder Pal Singh 
Address: Piscataway, NJ 08855  Princeton, NJ 08544  
Affiliation: Computer Science Department Rutgers University  Computer Science Department Princeton University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. V. Adve, A. L. Cox, S. Dwarkadas, R. Rajamony, and W. Zwaenepoel. </author> <title> A Comparison of Entry Consistency and Lazy Release Consistency Implementation. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: It has been shown to perform slightly better than diffs for migratory sharing patterns. However, the only comparison in the literature shows that overall a TreadMarks-style LRC protocol with diffs performs better than one with software dirty bits due to the cost of instrumentation <ref> [1] </ref>.
Reference: [2] <author> C. Amza, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> Software DSM Protocols that Adapt between Single Writer and Multiple Writer. </title> <booktitle> In The 3rd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <year> 1997. </year>
Reference-contexts: Progress was slow until relaxed memory consistency models breathed new life into the approach in the 1990s, starting with research at Rice University [10, 24]. Software shared memory became a very active research area, with many groups designing new protocols, consistency models and systems <ref> [10, 24, 4, 33, 37, 18, 2, 32, 26] </ref>. These groups inspired and motivated one another, building on one another's ideas to push performance higher (Figure 1). <p> And at least on the Paragon platform, the home-based software multiple-writer protocol seems to often substantially outperform the original, non home-based one. A simulation-based comparison of home-based with architectural support versus non-home-based LRC can be found in [17]. Two recent papers <ref> [2, 21] </ref> independently propose schemes to improve migratory sharing (the third problem above) by recognizing this sharing pattern and treat it differently. For pages that exhibit migratory sharing and not write-write false sharing within a synchronization interval, there is clearly no need for a multiple writer protocol. <p> Benchmarks LRC A-LRC IS 1.2 2.1 SOR 6.7 6.8 Water 3.4 3.5 TSP 5.8 5.2 Shallow 5.1 5.6 Barnes 3.8 3.9 Ilink 5.1 5.1 Table 6: Speedups for LRC vs adaptive LRC (Rice, non home-based). 8 processors <ref> [2] </ref>. Finally, to eliminate the overhead of diffs we can use other methods to track and merge modifications in multiple-writer protocols. One alternative is to maintain per-word dirty bits in software, as introduced in the Midway system [4]. This requires instrumenting memory operations in the program, incurring some runtime overhead.
Reference: [3] <author> A.R. Lebeck B. Falsafi, S.K. Reinhart, I. Schoinas, M.D. Hill, J.R. Larus, A. Rogers, and D.A. Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Supercomputing '94, </booktitle> <year> 1994. </year>
Reference-contexts: Understanding application-protocol interactions more deeply can help in two ways: improving protocols to match the applications better, or (ii) restructuring applications to interact better with the protocols. In the former area, early work done in application-specific protocols for fine-grained software shared memory showed fairly dramatic performance gains <ref> [3] </ref>. In the SVM context, research has been done in having the compiler detect mismatches between application sharing patterns and protocols and adjust the protocol accordingly (or the user may indicate the sharing patterns to the system) [12].
Reference: [4] <author> B.N. Bershad and M.J. Zekauskas. Midway: </author> <title> Shared Memory Parallel Programming with Entry Consistency for Distributed Memory Multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Progress was slow until relaxed memory consistency models breathed new life into the approach in the 1990s, starting with research at Rice University [10, 24]. Software shared memory became a very active research area, with many groups designing new protocols, consistency models and systems <ref> [10, 24, 4, 33, 37, 18, 2, 32, 26] </ref>. These groups inspired and motivated one another, building on one another's ideas to push performance higher (Figure 1). <p> Finally, to eliminate the overhead of diffs we can use other methods to track and merge modifications in multiple-writer protocols. One alternative is to maintain per-word dirty bits in software, as introduced in the Midway system <ref> [4] </ref>. This requires instrumenting memory operations in the program, incurring some runtime overhead. It has been shown to perform slightly better than diffs for migratory sharing patterns. <p> Protocols and coherence state become too complicated to be implemented in hardware. Software solutions too may require additional hardware or compiler support. Worse still, programming complexity also increases. While early results did not indicate significant performance benefits, recent results are somewhat more encouraging. Entry Consistency <ref> [4] </ref> proposed the idea of binding data to synchronization variables, and at a synchronization event making only the data bound to that variable coherent. However, the programmer had to provide the binding, which was a major burden.
Reference: [5] <author> R. Bianchini, L.I Kontothanassis, R. Pinto, M. De Maria, M. Abud, and C.L. Amorim. </author> <title> Hiding Communication Latency and Coherence Overhead in Software DSMs. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The other alternative to incurring software diffing overhead is to use hardware support for either computing and applying diffs <ref> [5] </ref> or for propagating writes to the home in hardware in home-based protocols, thus eliminating diffs altogether [16, 27] (see Sec 2.4). 2.3.2 Laziness in Diffing As with coherence propagation, there many degrees of laziness are possible in diff propagation and application.
Reference: [6] <author> A. Bilas, L. Iftode, D. Martin, and J.P. Singh. </author> <title> Shared Virtual Memory Across SMP Nodes Using Automatic Update: Protocols and Performance. </title> <type> Technical Report TR-517-96, </type> <institution> Princeton, NJ, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: A study investigating automatic update support with commodity network interfaces like Myrinet finds that the performance benefits of AU support are smaller than with customized network interfaces <ref> [6] </ref>. The DEC Memory Channel also offers support for remote writes, but via explicit operations on the I/O bus [15] so writes need to be instrumented for this purpose. <p> This means that the differences which would have occurred in terms of bus and network contention between these two approaches are not exposed. At the same time, simulation studies indicate that while there is usually a benefit to using multiprocessor rather than uniprocessor nodes <ref> [20, 6] </ref>, the benefit is usually small when node size is small relative to overall system size. 3.4 Beyond RC Beyond release consistency the consistency model performance game is much more difficult to play. Protocols and coherence state become too complicated to be implemented in hardware.
Reference: [7] <author> Angelos Bilas and Jaswinder Pal Singh. </author> <title> The Effects of communication Parameters on End Performance of Shared Virtual Memory Clusters. </title> <booktitle> In Proceedings of Supercomputing'97, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: In a typical SVM implementation (without a dedicated protocol processor), incoming requests interrupt the compute processor and are handled by it. The interrupt overhead is the most significant factor in the performance and scalability of a SVM protocol <ref> [7] </ref>. Using polling (e.g. at back-edges) as an alternative to interrupts in handling remote requests can improve performance or not depending on the ratio of interrupt cost vs. polling overhead on a particular platform.
Reference: [8] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Support for AU was provided through a snooping device on the memory bus connected to the SHRIMP network interface <ref> [8] </ref>. Preliminary results of an home-based LRC implementation with AU support (AURC) on a 16-node SHRIMP multicomputer indicate that AURC typically outperforms all-software home-based LRC as long as the traffic is low or the automatic updates are performed to consecutive addresses and can be combined by the network interface.
Reference: [9] <author> J. B. Carter, J. K. Bennett, and Willy Zwaenepoel. </author> <title> Techniques for Reducing Consistency-Related Communication in Distributed Shared-Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-244, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Benchmarks SC ERC MULT 14.5 14.6 DIFF 8.4 12.3 TSP-C 11.3 12.6 QSORT 4.1 8.9 FFT 0.1 8.2 GAUSS 5.1 8.6 Table 1: Speedups for SC vs. ERC, an early implementation of release consistency for SVM in the Munin system. 16 processors <ref> [9] </ref>. and data. Greater laziness typically implies greater complexity and protocol state, but fewer communication and protocol operations. For example, hardware-coherent systems that use relaxed consistency tend to propagate coherence operations immediately, thus simplifying the data structures that need to be maintained for coherence (e.g. directories).
Reference: [10] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Progress was slow until relaxed memory consistency models breathed new life into the approach in the 1990s, starting with research at Rice University <ref> [10, 24] </ref>. Software shared memory became a very active research area, with many groups designing new protocols, consistency models and systems [10, 24, 4, 33, 37, 18, 2, 32, 26]. These groups inspired and motivated one another, building on one another's ideas to push performance higher (Figure 1). <p> Progress was slow until relaxed memory consistency models breathed new life into the approach in the 1990s, starting with research at Rice University [10, 24]. Software shared memory became a very active research area, with many groups designing new protocols, consistency models and systems <ref> [10, 24, 4, 33, 37, 18, 2, 32, 26] </ref>. These groups inspired and motivated one another, building on one another's ideas to push performance higher (Figure 1). <p> Among the many relaxed consistency models proposed for hardware-coherent systems, release consistency [14] which separates acquire from release synchronization was the first to inspire a major breakthrough in the performance of software shared memory systems <ref> [10] </ref> (see Table 1 1 ). Under release consistency and models inspired by it, processors can thus continue to share a page without any communication of data or coherence information until a synchronization point. A memory consistency model specifies when coherence operations and data need to become visible. <p> One important type of laziness in a protocol is measured by how much it delays the propagation of coherence operations (e.g. invalidations). In eager release consistency (ERC) <ref> [10] </ref>, invalidations are propagated to all sharing processors (and in fact applied there) before the issuing processor completes its next release operation. <p> In lazy release consistency (LRC) [24], the invalidation corresponding to a write by processor A is conveyed to a processor B on demand upon B's next acquire synchronization operation, if that acquire is causally after the release by the writing processor A 2 . Munin <ref> [10] </ref> and TreadMarks [22] from Rice University were the first SVM systems to propose and implement ERC and LRC protocols, respectively. Laziness has implications for the amount and lifetime of the buffered coherence state that must be maintained.
Reference: [11] <author> M. Dubois, J.C. Wang, L.A. Barroso, K. Lee, and Y-S Chen. </author> <title> Delayed Consistency and Its Effects on the Miss Rate of Parallel Programs. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 197-206, </pages> <year> 1991. </year>
Reference-contexts: These are often glossed over in comparing eager propagation protocols with LRC, but they can have significant performance effects. Delayed consistency (DC) <ref> [11] </ref> is an example of eager propagation but lazy application. 3 In this case, incoming invalidations are not applied right away but are queued at the destinations until they perform their next acquire operation (whether causally related to that release or not). <p> The queue mechanisms needed by DC can be used in hardware protocols as well as in software DC protocols <ref> [11, 38] </ref>.
Reference: [12] <author> S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. </author> <title> An Integrated Compile-Time/Run-Time Software Distributed Shared Memory Systems. </title> <booktitle> In ASPLOS-VII, </booktitle> <year> 1996. </year>
Reference-contexts: In the SVM context, research has been done in having the compiler detect mismatches between application sharing patterns and protocols and adjust the protocol accordingly (or the user may indicate the sharing patterns to the system) <ref> [12] </ref>. For example, the compiler may detect write-only pages and prevent the SVM protocol from invalidating them at synchronization points, or it can indicate to the protocol which data should be eagerly propagated at a barrier in order to reduce the following read miss latency.
Reference: [13] <author> A. Erlichson, N. Nuckolls, G. Chesson, and J. Hennessy. Soft-FLASH: </author> <title> Analyzing the Performance of Clustered Distributed Virtual Shared Memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: One of the first implementations indicates that shooting down the TLBs inside the multiprocessor can be expensive particularly if it occurs frequently <ref> [13] </ref>. An alternative scheme called incoming diffs was suggested in [35], but found to not help very much.
Reference: [14] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Relaxed memory consistency models allow the propagation and application of coherence operations (e.g. invalidations) to be postponed to synchronization points, greatly reducing the impact of false sharing and the frequency of coherence operations. Among the many relaxed consistency models proposed for hardware-coherent systems, release consistency <ref> [14] </ref> which separates acquire from release synchronization was the first to inspire a major breakthrough in the performance of software shared memory systems [10] (see Table 1 1 ).
Reference: [15] <author> Richard Gillett. </author> <title> Memory Channel Network for PCI. </title> <booktitle> In Proceedings of Hot Interconnects '95 Symposium, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: A study investigating automatic update support with commodity network interfaces like Myrinet finds that the performance benefits of AU support are smaller than with customized network interfaces [6]. The DEC Memory Channel also offers support for remote writes, but via explicit operations on the I/O bus <ref> [15] </ref> so writes need to be instrumented for this purpose.
Reference: [16] <author> L. Iftode, C. Dubnicki, E. W. Felten, and Kai Li. </author> <title> Improving Release-Consistent Shared Virtual Memory using Automatic Update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: The other alternative to incurring software diffing overhead is to use hardware support for either computing and applying diffs [5] or for propagating writes to the home in hardware in home-based protocols, thus eliminating diffs altogether <ref> [16, 27] </ref> (see Sec 2.4). 2.3.2 Laziness in Diffing As with coherence propagation, there many degrees of laziness are possible in diff propagation and application. These must be clearly understood in interpreting performance results that compare protocols, since they too can influence the results significantly. <p> For example, home-based protocols were first proposed assuming hardware support for propagation of fine-grained writes <ref> [16, 27] </ref>. One such mechanism is called automatic update. With automatic update, writes to pre-mapped shared pages are snooped and transparently propagated to the home copy of the page if it is remote.
Reference: [17] <author> L. Iftode, J. P. Singh, and Kai Li. </author> <title> Understanding Application Performance on Shared Virtual Memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: And at least on the Paragon platform, the home-based software multiple-writer protocol seems to often substantially outperform the original, non home-based one. A simulation-based comparison of home-based with architectural support versus non-home-based LRC can be found in <ref> [17] </ref>. Two recent papers [2, 21] independently propose schemes to improve migratory sharing (the third problem above) by recognizing this sharing pattern and treat it differently. <p> Recall that other studies support the advantage of home-based protocols both with and without AU support over original TreadMarks LRC <ref> [37, 17] </ref>. 2.4.3 Communication Parameters In addition to the above forms of support, comparisons between protocols are also affected greatly by the communication mechanisms and parameter values used, and studies should examine these effects as well. <p> Performance evaluations in software shared memory should use a wider range of applications in different classes with regard to sharing patterns at page-grain <ref> [17] </ref>. Overall, we believe that while some of the open high-level and lower-level protocol questions discussed above are important to answer, even effort on improving protocols should now be spent primarily based on bottlenecks discovered in applications or insights into exploiting architectural support. <p> The gap between hardware cache coherence and software shared memory is still quite large (e.g. <ref> [17] </ref>), even at relatively small scale. <p> The time is now ripe to focus research attention primarily on the gap between the performance of software shared memory and hardware-coherent shared memory <ref> [17] </ref>, and how it might be further alleviated.
Reference: [18] <author> L. Iftode, J.P. Singh, and K. Li. </author> <title> Scope Consistency: a Bridge Between Release Consistency and Entry Consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: Progress was slow until relaxed memory consistency models breathed new life into the approach in the 1990s, starting with research at Rice University [10, 24]. Software shared memory became a very active research area, with many groups designing new protocols, consistency models and systems <ref> [10, 24, 4, 33, 37, 18, 2, 32, 26] </ref>. These groups inspired and motivated one another, building on one another's ideas to push performance higher (Figure 1). <p> Entry Consistency [4] proposed the idea of binding data to synchronization variables, and at a synchronization event making only the data bound to that variable coherent. However, the programmer had to provide the binding, which was a major burden. Scope consistency (ScC) <ref> [18] </ref> shows that the effect of the synchronization to data binding can be achieved without including the binding requirement in the definition. Instead, the data to lock (scope) association is achieved dynamically when a write access occurs inside a scope. <p> By still being page-grained, unlike EC ScC is able to reduce false sharing while still preserving the prefetching effect of pages, and while keeping the state overhead required low. ScC was initially evaluated through simulation with a home-based protocol using automatic update support <ref> [18] </ref>, but in the same paper an all-software home based ScC protocol is also proposed. The model has more recently been implemented for a non-home based multiple-writer scheme similar to that used in TreadMarks, in the Brazos protoype [34].
Reference: [19] <author> Dongming Jiang, Hongzhang Shan, and Jaswinder Pal Singh. </author> <title> Application Restructuring and Performance Portability Across Shared Virtual Memory and Hardware-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the 6th ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Even when "standard" applications have been used to evaluate SVM tradeoffs, the versions used have been those that were written for hardware coherence. Structuring the applications or data structures more appropriately for SVM might change the performance dramatically <ref> [19] </ref>. Simultaneous research in applications and systems will help us understand whether SVM can indeed work well for a wide range of applications, how applications might be structured to perform portably well across different shared memory systems, and what types of further protocol optimizations and architectural support might be useful. <p> For restructuring applications, a study was performed to examine application restructuring for SVM systems and the performance portability of applications across SVM and hardware-coherent systems <ref> [19] </ref>. The study found that in most cases the applications can indeed be restructured to deliver good performance on moderate-scale SVM systems, the improvements in performance being quite dramatic.
Reference: [20] <author> M. Karlsson and P. Stenstrom. </author> <title> Performance Evaluation of Cluster-Based Multiprocessor Built from ATM Switches and Bus-Based Multiprocessor Servers. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: An alternative scheme called incoming diffs was suggested in [35], but found to not help very much. For protocol handling, there are questions about whether to use a dedicated protocol processor within the node, and if not, how to distribute protocol processing in response to incoming messages across processors <ref> [20] </ref>. Systems have also been developed to implement SVM across hardware DSM rather than bus-based machines [36]. <p> This means that the differences which would have occurred in terms of bus and network contention between these two approaches are not exposed. At the same time, simulation studies indicate that while there is usually a benefit to using multiprocessor rather than uniprocessor nodes <ref> [20, 6] </ref>, the benefit is usually small when node size is small relative to overall system size. 3.4 Beyond RC Beyond release consistency the consistency model performance game is much more difficult to play. Protocols and coherence state become too complicated to be implemented in hardware.
Reference: [21] <author> P. Keleher. </author> <title> Is There No Place Like Home. </title> <note> In Submitted for publication, </note> <year> 1997. </year>
Reference-contexts: And at least on the Paragon platform, the home-based software multiple-writer protocol seems to often substantially outperform the original, non home-based one. A simulation-based comparison of home-based with architectural support versus non-home-based LRC can be found in [17]. Two recent papers <ref> [2, 21] </ref> independently propose schemes to improve migratory sharing (the third problem above) by recognizing this sharing pattern and treat it differently. For pages that exhibit migratory sharing and not write-write false sharing within a synchronization interval, there is clearly no need for a multiple writer protocol. <p> An ERC protocol is even more constrained: Diffs are computed and propagated at a release together with invalidations. Otherwise, retrieving them upon a page fault may unnecessarily increase the read miss penalty since they can be retrieved only from the originating 4 <ref> [21] </ref> reportedly contains such results for one type of home-based protocol, but the final version of the paper is not yet available writers and not from the home.
Reference: [22] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: In lazy release consistency (LRC) [24], the invalidation corresponding to a write by processor A is conveyed to a processor B on demand upon B's next acquire synchronization operation, if that acquire is causally after the release by the writing processor A 2 . Munin [10] and TreadMarks <ref> [22] </ref> from Rice University were the first SVM systems to propose and implement ERC and LRC protocols, respectively. Laziness has implications for the amount and lifetime of the buffered coherence state that must be maintained. <p> transitive propagation of the coherence information requires an LRC protocol to store much more complex state for much longer: timestamp vectors to keep track of the causal dependencies among synchronization events, and write-notices for the coherence information received from other nodes during past synchronizations (for possible transmission in the future) <ref> [22] </ref>.
Reference: [23] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepole. </author> <title> An Evaluation of Software-Based Release Consistent Protocols. </title>
Reference-contexts: Keeping track of the transitive closure is also more expansive in terms of protocol and memory overhead than in ERC. Table 2 reports the results of a comparison of a particular ERC protocol similar to Munin and the TreadMarks LRC protocol <ref> [23] </ref>. The comparison is not purely about this issue, however, since there are other differences: This ERC protocol uses a hybrid update-invalidation approach discussed later, while the LRC protocol is invalidation-based (both are also multiple-writer protocols, which we will discuss in Section 2.3). <p> Benchmarks ERC LRC SOR 7.2 7.4 Water 3.8 4.6 Barnes 2.6 3.2 FFT 4.2 3.6 ILINK 5.9 5.8 MIP 4.2 5.6 Table 2: Speedups for particular ERC vs. LRC protocols. 8 processors <ref> [23] </ref> 2.2 Laziness in Applying Coherence Informa tion While a protocol may propagate coherence information (e.g. invalidations) eagerly at a release, as in ERC above, various degrees of laziness can be used for actually applying or performing the coherence operations and in processing acknowledgments. <p> Early ERC protocols like in Munin use a hybrid update-invalidate scheme in which diffs are eagerly propagated on a release to all active copies in the sharing list, and inactive copies are invalidated. In <ref> [23] </ref> Keleher describes a different hybrid scheme for ERC and uses it to compare with TreadMarks LRC (this was the comparison shown for ERC versus LRC in Table 2: On a release, invalidations are sent eagerly to sharers as usual; on receiving invalidations, the sharers that have also modified their copies
Reference: [24] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Progress was slow until relaxed memory consistency models breathed new life into the approach in the 1990s, starting with research at Rice University <ref> [10, 24] </ref>. Software shared memory became a very active research area, with many groups designing new protocols, consistency models and systems [10, 24, 4, 33, 37, 18, 2, 32, 26]. These groups inspired and motivated one another, building on one another's ideas to push performance higher (Figure 1). <p> Progress was slow until relaxed memory consistency models breathed new life into the approach in the 1990s, starting with research at Rice University [10, 24]. Software shared memory became a very active research area, with many groups designing new protocols, consistency models and systems <ref> [10, 24, 4, 33, 37, 18, 2, 32, 26] </ref>. These groups inspired and motivated one another, building on one another's ideas to push performance higher (Figure 1). <p> In eager release consistency (ERC) [10], invalidations are propagated to all sharing processors (and in fact applied there) before the issuing processor completes its next release operation. In lazy release consistency (LRC) <ref> [24] </ref>, the invalidation corresponding to a write by processor A is conveyed to a processor B on demand upon B's next acquire synchronization operation, if that acquire is causally after the release by the writing processor A 2 .
Reference: [25] <author> P.J. Keleher. </author> <title> The Relative Importance of Concurrent Writers and Weak Consistency Models. </title> <booktitle> In Proceedings of the IEEE COMPCON '96 Conference, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: been compared with SC on a number of SPLASH-2 applications that cover most of the basic sharing patterns [38] One is an eager-propagation but lazy-application delayed consistency protocol as described above, and the other is a single-writer LRC protocol (i.e. lazy propagation and application) similar to the one proposed in <ref> [25] </ref>. Table 3 shows some of the speedup comparisons, obtained on the Wisconsin Typhoon-zero platform. The lazier application and propagation of the LRC protocol have some advantages, especially in complex irregular applications that use substantial lock synchronization. <p> Multiple-writer protocols are clearly very advantageous for irregular applications that tend to exhibit write-write false sharing and lock-based synchronization, and of course not advantageous for applications whose patterns of page access are mostly single-writer <ref> [25] </ref>. And at least on the Paragon platform, the home-based software multiple-writer protocol seems to often substantially outperform the original, non home-based one. A simulation-based comparison of home-based with architectural support versus non-home-based LRC can be found in [17].
Reference: [26] <author> L. Kontothanassis, G. Hunt, R. Stets, N. Hardavellas, M. Cier-nak, S. Parthasarathy, W. Meira Jr., S. Dwarkadas, and M. Scott. </author> <title> VM-based Shared Memory on Low-Latency, Remote-Memory-Access Networks. </title> <booktitle> In ISCA24, </booktitle> <year> 1997. </year>
Reference-contexts: Progress was slow until relaxed memory consistency models breathed new life into the approach in the 1990s, starting with research at Rice University [10, 24]. Software shared memory became a very active research area, with many groups designing new protocols, consistency models and systems <ref> [10, 24, 4, 33, 37, 18, 2, 32, 26] </ref>. These groups inspired and motivated one another, building on one another's ideas to push performance higher (Figure 1). <p> Eager protocols may then outperform lazy protocols. For example, the protocols in the Cashmere home-based ERC system use the multicast support provided by the DEC Memory Channel interconnect to handle directory updates and synchronization <ref> [26] </ref>. 2.4.2 Fine-grain Remote Operations Support for fine-grain remote operations in the network interface may also affect some protocol design decisions. For example, home-based protocols were first proposed assuming hardware support for propagation of fine-grained writes [16, 27]. One such mechanism is called automatic update. <p> ERC (Cashmere). 32 processors <ref> [26] </ref> The Cashmere protocol uses eager coherence propagation with directories, so the design had also relied heavily on hardware support for remote reads (e.g. to access directory information efficiently), which was not available in their Memory Channel interface.
Reference: [27] <author> L. I. Kontothanassis and M. L. Scott. </author> <title> Using Memory-Mapped Network Interfaces to Improve the Performance of Distributed Shared Memory. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: Release latency is large due to large round-trip message cost and remote interrupt overhead, both of which are on the critical path. The Cashmere system from Rochester <ref> [27] </ref> proposes a lazy-application, eager-propagation protocol that queues invalidations but still waits for acknowledgments at release time. <p> The other alternative to incurring software diffing overhead is to use hardware support for either computing and applying diffs [5] or for propagating writes to the home in hardware in home-based protocols, thus eliminating diffs altogether <ref> [16, 27] </ref> (see Sec 2.4). 2.3.2 Laziness in Diffing As with coherence propagation, there many degrees of laziness are possible in diff propagation and application. These must be clearly understood in interpreting performance results that compare protocols, since they too can influence the results significantly. <p> For example, home-based protocols were first proposed assuming hardware support for propagation of fine-grained writes <ref> [16, 27] </ref>. One such mechanism is called automatic update. With automatic update, writes to pre-mapped shared pages are snooped and transparently propagated to the home copy of the page if it is remote.
Reference: [28] <author> L. Lamport. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocessor Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <year> 1979. </year>
Reference-contexts: It influences both programming complexity and performance. A strict consistency model like sequential consistency <ref> [28] </ref> is intuitive for a programmer.
Reference: [29] <author> K. Li. </author> <title> Shared Virtual Memory on Loosely-coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> October </month> <year> 1986. </year> <note> Tech Report YALEU-RR-492. </note>
Reference-contexts: 1 Introduction When the idea of implementing a shared address space in software at page granularity across a network of computers|i.e. the shared virtual memory or SVM approach|was first introduced <ref> [29] </ref>, few would have predicted that a decade would not be enough to exhaust its research potential or even understand its performance capabilities. The initial approach used a sequential memory consistency model, which together with the large granularity of coherence and the high software overhead of communication greatly limited performance.
Reference: [30] <author> R. Samanta, L. Iftode, J.P. Singh, and Kai Li. </author> <title> Home-based SVM Protocols for SMPs. </title> <type> Technical report tr-535-96, </type> <institution> Princeton University, Princeton, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: With respect to laziness, the question is how quickly does the coherence information from outside propagate within the multiprocessor. For example, when one processor has to invalidate a page, should the page also be invalidated for other processors in that node <ref> [30, 35] </ref>? There is a tension between the need for some software involvement even within a node to provide laziness, and the need to reduce software eliminate software involvement to exploit hardware sharing within a node.
Reference: [31] <author> D. Scales and K. Gharachorloo. </author> <title> Towards Transparent and Efficient Software Distributed Shared Memory. </title> <booktitle> In SOSP-16, </booktitle> <year> 1997. </year>
Reference-contexts: In the only two published papers describing real software DSM systems built across SMP clusters <ref> [31, 35] </ref> the comparison is made between the real DSM across multiprocessor clusters and a DSM for uniprocessor nodes emulated on the same hardware. This means that the differences which would have occurred in terms of bus and network contention between these two approaches are not exposed.
Reference: [32] <author> D.J. Scales, K. Gharachorloo, and C.A. Thekkath. </author> <title> Shasta: A Low Overhead, Software-Only Approach for Supporting Fine-Grain Shared Memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Progress was slow until relaxed memory consistency models breathed new life into the approach in the 1990s, starting with research at Rice University [10, 24]. Software shared memory became a very active research area, with many groups designing new protocols, consistency models and systems <ref> [10, 24, 4, 33, 37, 18, 2, 32, 26] </ref>. These groups inspired and motivated one another, building on one another's ideas to push performance higher (Figure 1). <p> While Blizzard-S [33] was the first all-software fine-grain DSM, the recent Shasta system breathed new life into the approach by demonstrate quite good performance (Table 8) on a high-performance configuration (a cluster of 275 MHz DEC Alpha systems interconnected with Memory Channel) <ref> [32] </ref>. This was possible due to a number of subtle optimizations in instrumentation, some of which are specific to RISC (and even to Alpha) architectures. Among protocol-level optimizations, support for multiple coherence granularities on a per data structure basis was found to be particularly useful. <p> It helps optimize the prefetching effect of large granuarlity communication without inducing false sharing, though it does rely on some support from the programmer. Benchmarks Shasta LU-Cont 4.8 Water-Nsq 4.2 LU 3.8 Barnes 3.4 Volrend 2.8 Water-Spatial 2.0 Table 8: Shasta 8 processors <ref> [32] </ref> Fine-grained approaches and SVM are two very different approaches to software shared memory, but there is no reported comparison between them.
Reference: [33] <author> I. Schoinas, B. Falsafi, A.R. Lebeck, S.K. Reinhardt, J.R. Larus, and D.A. Wood. </author> <title> Fine-grain Access for Distributed Shared Memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 297-306, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Progress was slow until relaxed memory consistency models breathed new life into the approach in the 1990s, starting with research at Rice University [10, 24]. Software shared memory became a very active research area, with many groups designing new protocols, consistency models and systems <ref> [10, 24, 4, 33, 37, 18, 2, 32, 26] </ref>. These groups inspired and motivated one another, building on one another's ideas to push performance higher (Figure 1). <p> The alternative approach is to build software DSM systems with coherence performed at fine (or variable) grain. In these cases, access control and coherence are provided by instrumenting memory operations in the code rather than through the virtual memory mechanism <ref> [33] </ref>. The advantage of the fine-grained approach is simplicity, since it can use a sequential consistency model without suffering much from false sharing and communication fragmentation (and hence can also keep synchronization protocol-free and much less expensive). <p> However, code instrumentation is not always portable and adds overhead on each load and store, and since communication may be more frequent the approach relies on low-latency messaging. While Blizzard-S <ref> [33] </ref> was the first all-software fine-grain DSM, the recent Shasta system breathed new life into the approach by demonstrate quite good performance (Table 8) on a high-performance configuration (a cluster of 275 MHz DEC Alpha systems interconnected with Memory Channel) [32].
Reference: [34] <author> E. Speight and J.K. Bennett. Brazos: </author> <title> A Third Generation DSM Systems. </title> <booktitle> In USENIX Workshop on Windows-NT, </booktitle> <year> 1997. </year>
Reference-contexts: The model has more recently been implemented for a non-home based multiple-writer scheme similar to that used in TreadMarks, in the Brazos protoype <ref> [34] </ref>. This implementation relies heavily on the broadcast support of the interconnect (100 Mbps Ethernet) for a hybrid update-invalidate protocol, but it corroborates the promise of the earlier simulation results (Table 10). <p> Benchmarks LRC ScC SOR 5.5 5.61 ILINK 2.3 3.4 Barnes 2.2 4 Raytrace 0.5 1.8 Water 3.8 3.9 Table 10: Speedups for LRC vs ScC. 8 processors <ref> [34] </ref> 4 Conclusions There has been a lot of progress in shared virtual memory over the last several years. Protocols and their implementations have been improved steadily, fueled by relaxed memory consistency models, and are now at a stage of relative maturity.
Reference: [35] <author> R. Stets, S. Dwarkadas, N. Hardavellas, G. Hunt, L. Kon-tothanassis, S. Parthasarathy, and M. Scott. Cashmere-2L: </author> <title> Software Coherent Shared Memory on a Clustered Remote-Write Network. </title> <booktitle> In SOSP-16, </booktitle> <year> 1997. </year>
Reference-contexts: With respect to laziness, the question is how quickly does the coherence information from outside propagate within the multiprocessor. For example, when one processor has to invalidate a page, should the page also be invalidated for other processors in that node <ref> [30, 35] </ref>? There is a tension between the need for some software involvement even within a node to provide laziness, and the need to reduce software eliminate software involvement to exploit hardware sharing within a node. <p> One of the first implementations indicates that shooting down the TLBs inside the multiprocessor can be expensive particularly if it occurs frequently [13]. An alternative scheme called incoming diffs was suggested in <ref> [35] </ref>, but found to not help very much. For protocol handling, there are questions about whether to use a dedicated protocol processor within the node, and if not, how to distribute protocol processing in response to incoming messages across processors [20]. <p> In the only two published papers describing real software DSM systems built across SMP clusters <ref> [31, 35] </ref> the comparison is made between the real DSM across multiprocessor clusters and a DSM for uniprocessor nodes emulated on the same hardware. This means that the differences which would have occurred in terms of bus and network contention between these two approaches are not exposed.
Reference: [36] <author> D. Yeung, J. Kubiatowicz, and A. Agarwal. MGS: </author> <title> A Multigrain Shared Memory System. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: For protocol handling, there are questions about whether to use a dedicated protocol processor within the node, and if not, how to distribute protocol processing in response to incoming messages across processors [20]. Systems have also been developed to implement SVM across hardware DSM rather than bus-based machines <ref> [36] </ref>. The performance gains in switching from uniprocessor to multiprocessor clusters are not yet clearly understood, and may be affected by the performance of the communication support within and across nodes, in addition to the degree of laziness implemented in the protocol.
Reference: [37] <author> Y. Zhou, L. Iftode, and K. Li. </author> <title> Performance Evaluation of Two Home-Based Lazy Release Consistency Protocols for Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Progress was slow until relaxed memory consistency models breathed new life into the approach in the 1990s, starting with research at Rice University [10, 24]. Software shared memory became a very active research area, with many groups designing new protocols, consistency models and systems <ref> [10, 24, 4, 33, 37, 18, 2, 32, 26] </ref>. These groups inspired and motivated one another, building on one another's ideas to push performance higher (Figure 1). <p> Second, the need to keep diffs around at the writers can generate very large memory consumption (often larger than the application data set size itself <ref> [37] </ref>), thus greatly limiting scalability; periodic garbage collection is needed, which adds more overhead. <p> In an all-software home-based LRC protocol (HLRC), evaluated in <ref> [37] </ref>, a home node is selected for each shared page (usually the first writer of that page). Diffs are eagerly propagated to the home at a release, and then discarded at the writer. At the home, diffs are immediately applied as soon as they are received, then discarded there too. <p> Benchmarks LRC HLRC LU 11.5 13.9 Water-Nsq 11.7 18.9 Water-Spatial 14 20 Raytrace 10.6 26.8 Table 5: Speedups for non home-based multiple-writer LRC vs home-based multiple-writer LRC on Intel Paragon. 32 processors <ref> [37] </ref>. (HLRC) with a single-writer LRC and with an original non-home LRC, respectively. Multiple-writer protocols are clearly very advantageous for irregular applications that tend to exhibit write-write false sharing and lock-based synchronization, and of course not advantageous for applications whose patterns of page access are mostly single-writer [25]. <p> Recall that other studies support the advantage of home-based protocols both with and without AU support over original TreadMarks LRC <ref> [37, 17] </ref>. 2.4.3 Communication Parameters In addition to the above forms of support, comparisons between protocols are also affected greatly by the communication mechanisms and parameter values used, and studies should examine these effects as well.
Reference: [38] <author> Y. Zhou, L. Iftode, J.P. Singh, B.R. Toonen, I.Schoinas, M.D. Hill, and D.A. Wood. </author> <title> Relaxed Consistency and Coherence Granularity in DSM Systems: A Performance Evaluation. </title> <booktitle> In ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1997. </year>
Reference-contexts: The queue mechanisms needed by DC can be used in hardware protocols as well as in software DC protocols <ref> [11, 38] </ref>. <p> Two RC-based protocols that allow only a single writer to a page at a time (see next subsection) but propagate and apply invalidations at different times have been compared with SC on a number of SPLASH-2 applications that cover most of the basic sharing patterns <ref> [38] </ref> One is an eager-propagation but lazy-application delayed consistency protocol as described above, and the other is a single-writer LRC protocol (i.e. lazy propagation and application) similar to the one proposed in [25]. Table 3 shows some of the speedup comparisons, obtained on the Wisconsin Typhoon-zero platform. <p> Benchmarks SC DC LRC LU 8.6 8.6 8.4 Ocean 2.7 3.8 5.7 Water-Nsquared 11 11.3 11.3 Volrend 0.8 1.7 2.9 Water-Spatial 4.9 5.8 7.3 Raytrace 6.6 8 9 Barnes 0.9 1.9 2.2 Table 3: Speedups for Single Writer Protocols: SC vs. DC. vs LRC. 16 processors <ref> [38] </ref>. In contrast, the Munin system discussed earlier implements an eager-propagation, eager-application (ERC) protocol, in which invalidations or updates are performed as soon as they arrive (i.e. it is not DC) and a releaser waits for acknowledgments before proceeding. <p> Tables 4 and 5 compare the home-based LRC Benchmarks SW-LRC HLRC LU 8.0 7.9 Ocean 6 8.7 Water-Nsquared 11.6 11.8 Volrend 3 9 Water-Spatial 7 12 Raytrace 9 13 Barnes 3 6 Table 4: Speedups for Single Writer vs Multiple Writer Home-based LRC on Typhoon-zero. 16 processors <ref> [38] </ref>. Benchmarks LRC HLRC LU 11.5 13.9 Water-Nsq 11.7 18.9 Water-Spatial 14 20 Raytrace 10.6 26.8 Table 5: Speedups for non home-based multiple-writer LRC vs home-based multiple-writer LRC on Intel Paragon. 32 processors [37]. (HLRC) with a single-writer LRC and with an original non-home LRC, respectively. <p> However, a group of researchers compared the two approaches on a fairly large and varied set of applications using the Typhoon-0 platform, which allows protocols to be run in software but uses a uniform hardware access control mechanism for both approaches <ref> [38] </ref>. <p> Benchmarks FG- SC HLRC LU 6.0 7.9 Ocean 6.1 8.7 Water-Nsquared 12.6 11.8 Volrend 6 9 Water-Spatial 12.7 12 Raytrace 14 13 Barnes 7 6 Table 9: Fine-grain SC vs HLRC. 16 processors <ref> [38] </ref> 3.2 Applications and Application-driven Re search Another major area enabled by SVM progress is research in applications for these systems. Even when "standard" applications have been used to evaluate SVM tradeoffs, the versions used have been those that were written for hardware coherence.
References-found: 38

