URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1994/TR01.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Email: Email: basak,panda@cis.ohio-state.edu  
Phone: Tel: (614)-292-5199, Fax: (614)-292-2911  
Title: Designing Large Hierarchical Multiprocessor Systems under Processor, Interconnection, and Packaging  
Author: Advancements Debashis Basak and Dhabaleswar K. Panda 
Keyword: packaging constraints, hierarchical systems, clustered systems, pinout constraint, interconnection network, k-ary n-cube. systems.  
Address: Columbus, OH 43210-1277  
Affiliation: Department of Computer and Information Science Ohio State University  
Abstract: In this paper we present a general framework for architectural design of large hierarchical multiprocessor systems under rapidly changing packaging, processor, and interconnection technologies. Processor boards with larger area (A) and greater pinouts are becoming feasible. Board interconnection technology has advanced from only peripheral connections O( A) to elastomeric surface connections O(A). As processor and interconnection technology is growing, there is a varying demand on the interconnection network of the system. The proposed framework is capable of taking into account all these changing technologies. Each technology is captured through one or more parameter(s) which reflects its level of advancement. Depending on a given set of values of these parameters, the framework guides us to choose the most optimum topology. The framework is illustrated by considering the design problem of the currently popular k-ary n-cube cluster-c scalable architecture. These architectures combine the scalability of k-ary n-cube wormhole-routed networks with the cost-effectiveness of processor cluster designs. Each cluster consists of c processors interconnected through a bus/MIN/direct network. Our results indicate that under surface pinout technology an increase in cluster size(c) is associated with a growth in bisection bandwidth, whereas in periphery pinout it leads to a fall in bisection bandwidth. For systems supporting 16-bit links, cluster sizes of 2 to 3 processors with 3D/4D k-ary n-cube interconnections are optimal. Similarly systems with 32-bit links can support larger cluster sizes(c=7,8) and 3D/4D interconnections. p
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agrawal. </author> <title> Limits on Interconnection Network Performance. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 2(4), </volume> <month> Oct </month> <year> 1991. </year>
Reference-contexts: The equivalence holds good because clusters can be treated as supernodes connected through a flat network (internet). Such an analysis has been done by Agrawal in <ref> [1] </ref>, where it was shown that for arbitrary length messages and reasonable switch delays a 3D topology will slightly outperform the 4D topology. Hence we conclude that 5-ary 3-cube cluster-8 configuration will be the best.
Reference: [2] <author> D. Basak and D. K. Panda. </author> <title> Scalable Architectures with k-ary n-cube cluster-c organization. </title> <booktitle> In Proc. of the Symposium of Parallel and Distributed Processing, </booktitle> <year> 1993. </year> <month> 19 </month>
Reference-contexts: Moreover the techniques which work for two-level hierarchies can be easily extended to design more levels. Examples of previous work in this area include two-level systems based on hypercube and other network topologies [5, 15, 18], MINs and n-hop networks [18], two-level systems with k-ary n-cube topologies <ref> [2] </ref>, and combination of bus and mesh/hypercube networks [10]. However, most of these analyses either did not consider packaging and interconnecting constraints, or provided guidelines based only on fixed board sizes with fixed pinouts, not considering changes in board sizes and alternate pinout technologies. <p> This is done on the basis of relative performance and cost of these systems. Simulation modeling may be employed to decide among very close candidates. The paper is organized as follows. In section 2 we discuss the two-level k-ary n-cube cluster-c architecture <ref> [2] </ref> used in this paper to illustrate the working of the design framework. In section 3 we discuss the trends in growth of processor board sizes, alternate pinout technologies, channel width technology, and derive the design-feasible configurations 2 and their characterestics. <p> In Section 5 we show how to derive the good configurations and discuss important considerations in choosing the best configuration. Finally, concluding remarks and future work are presented. 2 Architectural model and alternatives In this section we discuss the class of k-ary n-cube cluster-c systems <ref> [2, 7] </ref>. Such architectures are currently the trend in building large scalable systems. Examples are Intel Paragon [12], Stanford DASH [6] and Cray T3D [4]. A k-ary n-cube cluster-c system consists of two levels. <p> We represent demanded bisection bandwidth in high-level network as B and demanded bisection bandwidth in each low-level cluster network as B 1 . It can be shown <ref> [2, 18] </ref> that B = N D (1 p)=2 and B 1 = cD (2 p)=2. To support larger clusters, the cluster bisection bandwidth B 1 should scale linearly with cluster size c.
Reference: [3] <author> C. Chen, D. P. Agrawal, and J. R. Burke. dBcube: </author> <title> A new class of Hierarchical Multi--processor Interconnection Network with area efficient layout. </title> <note> to appear in IEEE Trans. on Parallel and Distributed Systems, </note> <month> Dec </month> <year> 1993. </year>
Reference-contexts: The k n cluster interfaces are interconnected through a k-ary n-cube network. 3 Though in this paper we work with k-ary n-cube internets, our design framework is more general and can work for other internet topologies like DeBruijn graphs and n-hop networks <ref> [3, 18] </ref>. Without loss of generality, in this paper, we emphasize on k-ary n-cube because it is a currently popular trend which we believe will continue in the near future also.
Reference: [4] <author> Cray Reasearch Inc. </author> <title> Cray T3D System Architecture Overview, </title> <year> 1993. </year>
Reference-contexts: Finally, concluding remarks and future work are presented. 2 Architectural model and alternatives In this section we discuss the class of k-ary n-cube cluster-c systems [2, 7]. Such architectures are currently the trend in building large scalable systems. Examples are Intel Paragon [12], Stanford DASH [6] and Cray T3D <ref> [4] </ref>. A k-ary n-cube cluster-c system consists of two levels. The lower level consists of k n clusters of processors interconnected by a higher level direct k-ary n-cube network (also referred to as inter-cluster network or internet). Each cluster consists of c processors interconnected through buses/MINs/direct networks. <p> Without loss of generality, in this paper we use values of p1 in the range of 64-32 and those of p2 in the range of 256-128 . The higher limits are derived from the current design trends of CRAY T3D <ref> [4] </ref> and J-machine [14]. 6 3.3 Parameterized representation of inter-cluster bisection band- width and channel width In this section we develop a general scheme to represent architectural characteristics (bisection bandwidth and channel width) of the inter-cluster network. <p> B) Inter-cluster channel width W : Wires in a channel can be divided into two categories: wires that carry data and those that carry channel control signals. For example in Cray T3D <ref> [4] </ref>, a channel width is 16 (data part) + 8 (control part) = 24 wires. Actually the 16-bit data part in channels can be observed as a trend in most current generation machines [4, 7, 12]. <p> For example in Cray T3D [4], a channel width is 16 (data part) + 8 (control part) = 24 wires. Actually the 16-bit data part in channels can be observed as a trend in most current generation machines <ref> [4, 7, 12] </ref>. Many factors like path-width inside routers, connector technology etc. restrict channel widths from being arbitrarily large. However it is expected that technologies in near future would allow channels to carry 32-bit or 64-bit data [16], corresponding to W 40 and W 72, respectively.
Reference: [5] <author> S. Dandamudi and D. Eager. </author> <title> Hierarchical Interconnection Networks for Multicomputer Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-39(6), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: Moreover the techniques which work for two-level hierarchies can be easily extended to design more levels. Examples of previous work in this area include two-level systems based on hypercube and other network topologies <ref> [5, 15, 18] </ref>, MINs and n-hop networks [18], two-level systems with k-ary n-cube topologies [2], and combination of bus and mesh/hypercube networks [10].
Reference: [6] <author> D. Lenoski et al. </author> <title> The Stanford DASH Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <year> 1990. </year>
Reference-contexts: Finally, concluding remarks and future work are presented. 2 Architectural model and alternatives In this section we discuss the class of k-ary n-cube cluster-c systems [2, 7]. Such architectures are currently the trend in building large scalable systems. Examples are Intel Paragon [12], Stanford DASH <ref> [6] </ref> and Cray T3D [4]. A k-ary n-cube cluster-c system consists of two levels. The lower level consists of k n clusters of processors interconnected by a higher level direct k-ary n-cube network (also referred to as inter-cluster network or internet). <p> B) Localized traffic 11 Under this assumption, a request by a processor has higher probability of being sat-isfied by closer neighbors than by distant ones. This is supposed to be more representative for distributed shared memory programming with memory hierarchies and caches <ref> [6, 13] </ref>. A location often referred by a processor is attracted close to the processor [9], thus ensuring locality.
Reference: [7] <author> D. Lenoski et al. </author> <title> The DASH Prototype: Implementation and Performance. </title> <booktitle> In Proc. of the Int'l Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <year> 1992. </year>
Reference-contexts: In Section 5 we show how to derive the good configurations and discuss important considerations in choosing the best configuration. Finally, concluding remarks and future work are presented. 2 Architectural model and alternatives In this section we discuss the class of k-ary n-cube cluster-c systems <ref> [2, 7] </ref>. Such architectures are currently the trend in building large scalable systems. Examples are Intel Paragon [12], Stanford DASH [6] and Cray T3D [4]. A k-ary n-cube cluster-c system consists of two levels. <p> For example in Cray T3D [4], a channel width is 16 (data part) + 8 (control part) = 24 wires. Actually the 16-bit data part in channels can be observed as a trend in most current generation machines <ref> [4, 7, 12] </ref>. Many factors like path-width inside routers, connector technology etc. restrict channel widths from being arbitrarily large. However it is expected that technologies in near future would allow channels to carry 32-bit or 64-bit data [16], corresponding to W 40 and W 72, respectively.
Reference: [8] <author> D. Lenoski et al. </author> <title> The DASH Prototype Logic Overhead and Performance. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(1) </volume> <pages> 41-61, </pages> <month> Jan </month> <year> 1993. </year>
Reference-contexts: The changes in processor and interconnection technology and its relation to packaging technology in designing systems was also not studied. Some of the works suggested guidelines based only on uniform traffic patterns. However, localized traffic patterns are more common in distributed shared memory (DSM) systems with caches <ref> [8] </ref> or cache-only memories [9, 13]. Localized traffic patterns put lesser load on the higher hierarchies, allowing system designs with low-bandwidth networks to be considered which may have been rejected outright under uniform traffic assumption. <p> Their size is restricted by electrical, mechanical, and board fabrication constraints. In our system we assume each board to contain c processors chips, intra-cluster interconnection network, local memories and the cluster interface (consisting of part of the internet router logic and cache-coherence logic if it is a DSM system <ref> [8] </ref>). The maximum size of processor board restricts the number of 4 processors (c) that can be put on a board and imposes a natural limit on the pinout. It has been shown in [8] that the area of processor and memory chips dominate the space requirements. <p> of part of the internet router logic and cache-coherence logic if it is a DSM system <ref> [8] </ref>). The maximum size of processor board restricts the number of 4 processors (c) that can be put on a board and imposes a natural limit on the pinout. It has been shown in [8] that the area of processor and memory chips dominate the space requirements. Let us define the area required by a processor chip, its local memory, and a per-processor fraction of the network and other logic, to be a. <p> We discuss this briefly in the next section. 4.2 Traffic Patterns Traffic patterns depend on disparate factors like application characteristics, underlying architecture, mapping efficiency and programming model, etc. A convincing way to represent them is to actually generate them through application implementation <ref> [8] </ref>. However, for comparing a large number of design alternatives this approach is not practical. Researchers therefore rely on two synthetic traffic patterns [11, 18] to give them quick feedback on relative performance of systems.
Reference: [9] <author> Erik Hagersten and Seif Haridi. </author> <title> A quantitative comparison of efficiency for large shared memory architectures. </title> <booktitle> In Third Workshop on Scalable Shared Memory Multiprocessors. International Symposium on Computer Architecure, </booktitle> <year> 1993. </year>
Reference-contexts: Some of the works suggested guidelines based only on uniform traffic patterns. However, localized traffic patterns are more common in distributed shared memory (DSM) systems with caches [8] or cache-only memories <ref> [9, 13] </ref>. Localized traffic patterns put lesser load on the higher hierarchies, allowing system designs with low-bandwidth networks to be considered which may have been rejected outright under uniform traffic assumption. In this paper we present a general framework to provide design guidelines for building large hierarchical multiprocessor systems. <p> This is supposed to be more representative for distributed shared memory programming with memory hierarchies and caches [6, 13]. A location often referred by a processor is attracted close to the processor <ref> [9] </ref>, thus ensuring locality.
Reference: [10] <author> W. Hsu and P. C. Yew. </author> <title> The Performance of Hierarchical Systems with wiring constraints. </title> <booktitle> In Proc. of the Int. Conf. on Parallel Processing, </booktitle> <month> Aug </month> <year> 1991. </year>
Reference-contexts: Examples of previous work in this area include two-level systems based on hypercube and other network topologies [5, 15, 18], MINs and n-hop networks [18], two-level systems with k-ary n-cube topologies [2], and combination of bus and mesh/hypercube networks <ref> [10] </ref>. However, most of these analyses either did not consider packaging and interconnecting constraints, or provided guidelines based only on fixed board sizes with fixed pinouts, not considering changes in board sizes and alternate pinout technologies.
Reference: [11] <author> W. Hsu and P. C. Yew. </author> <title> The Impact of Wiring Constraints on Hierarchical Network Performance. </title> <booktitle> In Proc. of the Int. Parallel Processing Symposium, </booktitle> <month> Mar </month> <year> 1992. </year>
Reference-contexts: A convincing way to represent them is to actually generate them through application implementation [8]. However, for comparing a large number of design alternatives this approach is not practical. Researchers therefore rely on two synthetic traffic patterns <ref> [11, 18] </ref> to give them quick feedback on relative performance of systems. A) Uniform traffic Under this assumption, a message or a request generated by a processor is equally probable to go to any other processor in the system.
Reference: [12] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: Finally, concluding remarks and future work are presented. 2 Architectural model and alternatives In this section we discuss the class of k-ary n-cube cluster-c systems [2, 7]. Such architectures are currently the trend in building large scalable systems. Examples are Intel Paragon <ref> [12] </ref>, Stanford DASH [6] and Cray T3D [4]. A k-ary n-cube cluster-c system consists of two levels. The lower level consists of k n clusters of processors interconnected by a higher level direct k-ary n-cube network (also referred to as inter-cluster network or internet). <p> For example in Cray T3D [4], a channel width is 16 (data part) + 8 (control part) = 24 wires. Actually the 16-bit data part in channels can be observed as a trend in most current generation machines <ref> [4, 7, 12] </ref>. Many factors like path-width inside routers, connector technology etc. restrict channel widths from being arbitrarily large. However it is expected that technologies in near future would allow channels to carry 32-bit or 64-bit data [16], corresponding to W 40 and W 72, respectively.
Reference: [13] <institution> Kendall Square Research. </institution> <type> KSR Technical Summary, </type> <year> 1992. </year>
Reference-contexts: Some of the works suggested guidelines based only on uniform traffic patterns. However, localized traffic patterns are more common in distributed shared memory (DSM) systems with caches [8] or cache-only memories <ref> [9, 13] </ref>. Localized traffic patterns put lesser load on the higher hierarchies, allowing system designs with low-bandwidth networks to be considered which may have been rejected outright under uniform traffic assumption. In this paper we present a general framework to provide design guidelines for building large hierarchical multiprocessor systems. <p> B) Localized traffic 11 Under this assumption, a request by a processor has higher probability of being sat-isfied by closer neighbors than by distant ones. This is supposed to be more representative for distributed shared memory programming with memory hierarchies and caches <ref> [6, 13] </ref>. A location often referred by a processor is attracted close to the processor [9], thus ensuring locality.
Reference: [14] <author> Michael D. Noakes, Deborah A. Wallach, and William J. Dally. </author> <title> The j-machine multi-computer: An architectural evaluation. </title> <booktitle> In Proc. of the Int. Symposium on Computer Architecture, </booktitle> <pages> pages 224-235, </pages> <year> 1993. </year>
Reference-contexts: In recent years, board sizes have grown in physical dimensions. Progressive improvements in the processor-memory integration has also resulted in a becoming smaller leading to more processors on the same board. We now have contemporary prototypes like the MIT J-machine <ref> [14] </ref> which has 64 processors on a board. The technology at a given time determines the largest board size (m:a), which can fit at most m processors. The maximum board size determines the largest cluster size in our system design. <p> Currently two different types of technologies are being employed by the computer industry: A) Surface pinouts: In this technology the surface of the board is utilized for external connections using elastomeric connectors <ref> [14] </ref>. Under this technology the pin-count from a board of area A is O (A). Let us define parameter p1 to be the pin-count that can be supported on a given board area a using surface pinout technology. <p> Without loss of generality, in this paper we use values of p1 in the range of 64-32 and those of p2 in the range of 256-128 . The higher limits are derived from the current design trends of CRAY T3D [4] and J-machine <ref> [14] </ref>. 6 3.3 Parameterized representation of inter-cluster bisection band- width and channel width In this section we develop a general scheme to represent architectural characteristics (bisection bandwidth and channel width) of the inter-cluster network. The representation is parameterized with respect to the two pinout technologies, under fixed total board area.
Reference: [15] <author> K. Padmanabhan. </author> <title> Effective Architectures for Data Access in a Shared Memory Hierarchy. </title> <journal> Jour. of Parallel and Distributed Computing, </journal> <volume> 11, </volume> <year> 1991. </year>
Reference-contexts: Moreover the techniques which work for two-level hierarchies can be easily extended to design more levels. Examples of previous work in this area include two-level systems based on hypercube and other network topologies <ref> [5, 15, 18] </ref>, MINs and n-hop networks [18], two-level systems with k-ary n-cube topologies [2], and combination of bus and mesh/hypercube networks [10].
Reference: [16] <author> David A. Patterson. </author> <title> Observations in Massive Parallelism Trends and Predictions for 1995 to 2000. </title> <type> Technical Report 93-87, </type> <institution> DIMACS, </institution> <month> Sept </month> <year> 1993. </year>
Reference-contexts: Many factors like path-width inside routers, connector technology etc. restrict channel widths from being arbitrarily large. However it is expected that technologies in near future would allow channels to carry 32-bit or 64-bit data <ref> [16] </ref>, corresponding to W 40 and W 72, respectively. Table 2 gives the channel widths of the internets for various topologies with N = 1024 processors under the two pinout technologies. <p> p A) Technology. 9 of the table shows the restrictions imposed on system design by three different channel width constraints: 1) W 24 corresponds to current generation machines, 2) W 40 corresponds to near future generation, and 3) W 72 corresponds to machines predicted at the turn of this century <ref> [16] </ref>. It is to be noted that these constraints restrict allowable cluster sizes to small ranges. In O ( p A) pinout technology: we observe that W 24 restricts cluster sizes to 1-3, W 40 restricts cluster sizes to 2-7, and W 72 restricts cluster sizes to 2-22. <p> Table 4: Demanded bisection bandwidth B as a function of for both traffic models under various system configurations with N = 1024. To get representative values of we choose various combinations of processor and network speeds and summarize these in Table 5. Most of the values are taken from <ref> [16] </ref>, and are predictions for the near future.
Reference: [17] <author> M. T. Raghunath. </author> <title> Interconnection Network Design Based on Packaging Considerations. </title> <type> PhD thesis, </type> <address> U. C. Berkeley, </address> <month> Nov </month> <year> 1993. </year>
Reference-contexts: We consider a packaging technology with two levels: processors in the parallel computer are organized into a number of boards (clusters) and the boards are then connected through inter-board wires. Though the precise characteristics of existing and emerging packaging technologies are hard to model, Raghunath <ref> [17] </ref> has shown that a common characteristic of most packaging technologies is a progressive increase in costs and decrease in capacities as we go up the packaging hierarchy. For instance, it is clear that as we proceed up the hierarchy, wires usually get longer with larger inter-wire spacings. <p> this pattern is 1) the application is dynamic with no communication locality, or 2) not enough compiler or human effort is expended on efficient mapping on underlying interconnection, or 3) some high-level programming constructs purposely hide details of the architecture from the user, encouraging simplicity at the cost of performance <ref> [17] </ref>. B) Localized traffic 11 Under this assumption, a request by a processor has higher probability of being sat-isfied by closer neighbors than by distant ones. This is supposed to be more representative for distributed shared memory programming with memory hierarchies and caches [6, 13]. <p> Among all the good configurations the choice which provides best performance with cheapest cost is defined as the best configuration <ref> [17] </ref>. We take an example to show how to decide the set of good configurations. The search for the best configuration, however, is not a very clear-cut process. Unknown and inestimable factors like exact link costs, connector costs, and other packaging costs make this decision extremely hard.
Reference: [18] <author> M. T. Raghunath and Abhiram Ranade. </author> <title> Designing interconnection networks for multilevel packaging. </title> <booktitle> In Proc. of the Supercomputing, </booktitle> <pages> pages 772-781, </pages> <year> 1993. </year> <month> 20 </month>
Reference-contexts: Moreover the techniques which work for two-level hierarchies can be easily extended to design more levels. Examples of previous work in this area include two-level systems based on hypercube and other network topologies <ref> [5, 15, 18] </ref>, MINs and n-hop networks [18], two-level systems with k-ary n-cube topologies [2], and combination of bus and mesh/hypercube networks [10]. <p> Moreover the techniques which work for two-level hierarchies can be easily extended to design more levels. Examples of previous work in this area include two-level systems based on hypercube and other network topologies [5, 15, 18], MINs and n-hop networks <ref> [18] </ref>, two-level systems with k-ary n-cube topologies [2], and combination of bus and mesh/hypercube networks [10]. <p> The k n cluster interfaces are interconnected through a k-ary n-cube network. 3 Though in this paper we work with k-ary n-cube internets, our design framework is more general and can work for other internet topologies like DeBruijn graphs and n-hop networks <ref> [3, 18] </ref>. Without loss of generality, in this paper, we emphasize on k-ary n-cube because it is a currently popular trend which we believe will continue in the near future also. <p> The advantage of under-populating is that the remaining area of the board may be used to support larger number of inter-board links which may improve system performance. However, assuming that these links are expensive <ref> [18] </ref>, this performance gain might not be cost-worthy. We therefore choose to avoid under-populating and develop design guidelines assuming a board area is packed to its capacity. Thus we assume that for building a system with N processors we are given exactly (N:a) total board space. <p> In other words the sum total of areas of all boards in the system is (N:a). 3.2 Two pinout technologies The pin-count P out of a board has a direct influence on the data rate that can flow in/out of a given processor cluster. It has been shown in <ref> [18] </ref> that for any given inter-board network topology (3D mesh, hypercube), P also directly determines the bisection bandwidth of the inter-board network, and hence the peak performance of the system. <p> The representation is parameterized with respect to the two pinout technologies, under fixed total board area. Ideally we should consider similar characterization of the lower-level cluster-networks. However as shown in <ref> [18] </ref>, lower levels have more flexibility in packaging, allowing system designer to choose channel widths and bisection bandwidths with more freedom. It is the design of the higher level inter-cluster network, under more rigid packaging constraints, that is critical in determining overall system performance. <p> We represent demanded bisection bandwidth in high-level network as B and demanded bisection bandwidth in each low-level cluster network as B 1 . It can be shown <ref> [2, 18] </ref> that B = N D (1 p)=2 and B 1 = cD (2 p)=2. To support larger clusters, the cluster bisection bandwidth B 1 should scale linearly with cluster size c. <p> To support larger clusters, the cluster bisection bandwidth B 1 should scale linearly with cluster size c. In this paper, we assume this to be the case, since packaging constraints are less rigid in lower hierarchies allowing thicker channel/buses to be used to scale up network bandwidth <ref> [18] </ref>. We focus on the more critical upper level inter-cluster network bandwidth B. Our goal is to parameterize this B as a function of processor and network speeds and then compare it with actual offered bisection bandwidth by the packaging technology (derived in sec. 3.3). <p> A convincing way to represent them is to actually generate them through application implementation [8]. However, for comparing a large number of design alternatives this approach is not practical. Researchers therefore rely on two synthetic traffic patterns <ref> [11, 18] </ref> to give them quick feedback on relative performance of systems. A) Uniform traffic Under this assumption, a message or a request generated by a processor is equally probable to go to any other processor in the system.
Reference: [19] <author> E. Rothberg, J. P. Singh, and Anoop Gupta. </author> <title> Working Sets, Cache Sizes, and Node Granularity Issues for Large-Scale Multiprocessors. </title> <booktitle> In Proc. of the Int. Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <year> 1993. </year> <month> 21 </month>
Reference-contexts: To get an estimate of demand on network bandwidth we need to know the desired sustainable computation to communication ratio in future systems. For this paper we choose this ratio as 1FLOP/byte as suggested in <ref> [19] </ref> (however, our analytical framework works for any computation to communication ratio). This implies that a 25MFLOPS processor demands 25MB/s of network bandwidth. In a hierarchical system not all the traffic reaches all networks. We first explain how to compute the demanded bisection bandwidth in each network in the hierarchy.
References-found: 19

