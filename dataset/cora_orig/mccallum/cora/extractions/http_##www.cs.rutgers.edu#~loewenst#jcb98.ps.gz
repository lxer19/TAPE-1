URL: http://www.cs.rutgers.edu/~loewenst/jcb98.ps.gz
Refering-URL: http://www.cs.rutgers.edu/~loewenst/
Root-URL: http://www.cs.rutgers.edu
Email: Email: davel@research.nj.nec.com  Email: pny@research.nj.nec.com  
Phone: Phone: 609-951-2798 Fax: 609-951-2483  
Title: Significantly Lower Entropy Estimates for Natural DNA Sequences  
Author: David Loewenstern* and Peter N. Yianilos and 
Date: Submitted: November 10, 1996 Revised: August 14, 1998  
Address: 4 Independence Way, Princeton, NJ 08540  4 Independence Way, Princeton, NJ 08540  Princeton, New Jersey 08544  
Affiliation: NEC Research Institute,  NEC Research Institute,  Department of Computer Science Princeton University,  
Abstract-found: 0
Intro-found: 1
Reference: [Baum, 1972] <author> Baum, L. E. </author> <year> (1972). </year> <title> An inequality and associated maximization technique in statistical estimatation of probabilistic functions of Markov processes. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8. </pages>
Reference: [Baum and Eagon, 1967] <author> Baum, L. E. and Eagon, J. E. </author> <year> (1967). </year> <title> An inequality with application to statistical estimation for probabalistic functions of a Markov process and to models for ecology. </title> <journal> Bull. AMS, </journal> <volume> 73 </volume> <pages> 360-363. </pages>
Reference: [Baum et al., 1970] <author> Baum, L. E., Petrie, T., Soules, G., and Weiss, N. </author> <year> (1970). </year> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Ann. Math Stat., </journal> <volume> 41 </volume> <pages> 164-171. </pages>
Reference: [Bell et al., 1990] <author> Bell, T. C., Cleary, J. G., and Witten, I. H. </author> <year> (1990). </year> <title> Text Compression. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: Assuming each character (nucleotide) is drawn uniformly at random from the alphabet, and that all positions in the string are independent, we know from elementary information theory <ref> [Cover and Thomas, 1991, Bell et al., 1990] </ref> that an optimal code will devote 2 bits to representing each character. This is the maximum entropy case. <p> Longer contexts reach farther into the past, and might reasonably be expected to result in stronger models. However as context length increases, it becomes increasingly unlikely that a given context has ever been seen. This problem has led to the development of variable length context language models <ref> [Bell et al., 1990] </ref>, which use long contexts when enough earlier observations exists, and otherwise use shorter ones.
Reference: [Cardon and Stormo, 1992] <author> Cardon, L. and Stormo, G. </author> <year> (1992). </year> <title> Expectation maximization algorithm for identifying protein-binding sites with variable lengths from unaligned DNA fragments. </title> <journal> JMB, </journal> <volume> 223 </volume> <pages> 159-170. </pages>
Reference: [Cosmi et al., 1990] <author> Cosmi, C., Cuomo, V., Ragosta, M., and Macchiato, M. </author> <year> (1990). </year> <title> Characterization of nucleotidic sequences using maximum entropy techniques. </title> <journal> J. Theor. Biol, </journal> (147):423-432. 
Reference: [Cover and Thomas, 1991] <author> Cover, T. M. and Thomas, J. A. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> Wiley. </publisher>
Reference-contexts: Assuming each character (nucleotide) is drawn uniformly at random from the alphabet, and that all positions in the string are independent, we know from elementary information theory <ref> [Cover and Thomas, 1991, Bell et al., 1990] </ref> that an optimal code will devote 2 bits to representing each character. This is the maximum entropy case. <p> The entropy rate of a stochastic process fX t g is defined as: lim 1 H (X 1 ; X 2 ; : : : ; X t ) (1) where this limit need not in general exist, and H denotes the information theoretic entropy function (see <ref> [Cover and Thomas, 1991] </ref>). Given full knowledge of the process, and the limit's existence, the entropy rate is a well-defined attribute of the process. But we know very little of the process underlying the generation of natural DNA, and can merely observe the outcome, i.e., the nucleotide sequence.
Reference: [Dempster et al., 1977] <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Ser. B (methodological), </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference-contexts: To effect this optimization we apply the Baum-Welch algorithm for Hidden Markov Models [Baum and Eagon, 1967, Baum et al., 1970, Baum, 1972, Poritz, 1988], which may be viewed as an instance of Expectation Maximization (EM) | a later rediscovery <ref> [Dempster et al., 1977, Redner and Walker, 1984] </ref> of essentially the same algorithm and underlying information theoretic inequality. This approach requires a starting point (in our experiments uniform distributions) and climbs to a local maximum.
Reference: [Dujon et al., 1994] <author> Dujon, B., Alexandraki, D., Andre, B., Ansorge, W., and et al., V. B. </author> <year> (1994). </year> <title> Complete DNA sequence of yeast chromosome XI. </title> <booktitle> Nature, </booktitle> <pages> 369. </pages>
Reference-contexts: In this case of panmtpacga, nearly all of the redundancy is explained by the unigraph statistics H 1 = 1:88. The observed relationship between %(C + G) and gene density <ref> [Dujon et al., 1994] </ref> in yeast is reflected in the discrepancy in H 1 between the coding and non-coding region of Yeast chromosome III. Observe that the H 6 estimate is rarely better than H 4 , and in some cases is markedly worse (a consequence of limited sequence length).
Reference: [Farach et al., 1994] <author> Farach, M., Noordewier, M., Savari, S., Shepp, L., Wyner, A., and Ziv, J. </author> <year> (1994). </year> <title> On the entropy of DNA: Algorithms and measurements based on memory and rapid convergence. </title> <booktitle> In Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms. </booktitle>
Reference-contexts: Thus the maximum entropy level per nucleotide for coding regions is not log 2 20=3 1:44 bits, but rather is log 2 61=3 1:977 bits. In fact, it has been observed by several authors that coding regions are less compressible than non-coding regions (e.g., <ref> [Salamon and Konopka, 1992, Farach et al., 1994] </ref>). So it is clear that two sequences that code for the same polypeptide may nevertheless have large Hamming distance.
Reference: [Gatlin, 1972] <author> Gatlin, L. L. </author> <year> (1972). </year> <title> Information Theory and the Living System. </title> <publisher> Columbia University Press, </publisher> <address> New York. </address>
Reference-contexts: It is entirely possible that very different results will be obtained, particularly for coding regions, when much more DNA is available for analysis. It should be noted that H 1 entropy estimates include the known effect of %(C +G) <ref> [Gatlin, 1972] </ref> on entropy estimation, and biocompress-2 includes the also known effect of long exact repeats and exact complement repeats [Herzel et al., 1994]. cdna's generally superior performance indicates that DNA possesses more structure which may be exploited.
Reference: [Grumbach and Tahi, 1994] <author> Grumbach, S. and Tahi, F. </author> <year> (1994). </year> <title> A new challenge for compression algorithms: genetic sequences. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 30(6) </volume> <pages> 875-886. </pages>
Reference-contexts: The resulting sequence contains 484; 483 bases and is referred to as our non-redundant data set. 5 Experimental Results Our model's performance on the sequences described in Section 4 is summarized in Table 2. In some cases our results may be compared directly with estimates from <ref> [Grumbach and Tahi, 1994] </ref>, which are included in the table. Our values for H 4 (the 4-symbol entropy) may be compared with the redundancy estimates of [Mantegna et al., 1993] and are in agreement. <p> We have grouped our results by general type (i.e., mammalian, prokaryote, etc.). 9 The H 1 ; H 4 ; H 6 columns contain conventional multigram entropy estimates. The cdna col-umn reports our model's cross-validation entropy estimates. Compressive estimates from the biocompress-2 program of <ref> [Grumbach and Tahi, 1994] </ref> are contained in the following column. Our model's compressive estimates are given in the table's final column, cdna-compress. The compressive estimates are generated by partitioning the sequence into 20 equal segments: s 1 ; s 2 ; : : : ; s 20 .
Reference: [Herzel, 1988] <author> Herzel, H. </author> <year> (1988). </year> <title> Complexity of symbol sequences. </title> <journal> Syst. Anal. Modl. Simul., </journal> <volume> 5(5) </volume> <pages> 435-444. </pages>
Reference-contexts: the next nucleotide, and generative stochastic modeling in which one imagines the data to emanate from a particular process. 4 3 Algorithms In this section we further motivate our model and then describe it in formal terms. 3.1 Motivation That natural DNA includes near repeats is well known, and in <ref> [Herzel, 1988] </ref> the statistics of their occurrence are discussed.
Reference: [Herzel et al., 1994] <author> Herzel, H., Ebeling, W., and Schmitt, A. </author> <year> (1994). </year> <title> Entropies of biosequences: The role of repeats. </title> <journal> Physical Review E, </journal> <volume> 50(6) </volume> <pages> 5061-5071. </pages>
Reference-contexts: It should be noted that H 1 entropy estimates include the known effect of %(C +G) [Gatlin, 1972] on entropy estimation, and biocompress-2 includes the also known effect of long exact repeats and exact complement repeats <ref> [Herzel et al., 1994] </ref>. cdna's generally superior performance indicates that DNA possesses more structure which may be exploited. Several notions of distance were evaluated and the best performance resulted from considering both Hamming distance to reversed and complemented targets, as well as standard Hamming distance, then selecting the minimum.
Reference: [Krogh et al., 1994] <author> Krogh, A., Mian, I., and Haussler, D. </author> <year> (1994). </year> <title> A hidden Markov model that finds genes in Escheria Coli DNA. </title> <journal> Nucleic Acids Research supplement, </journal> <volume> 22 </volume> <pages> 4768-4778. 21 </pages>
Reference: [Laplace, 1825] <author> Laplace, P.-S. </author> <title> (1825). Philosophical Essay on Probabilities. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <note> 1995 translation by A.I. Dale from the fifth French edition. </note>
Reference-contexts: 8 were matched to contexts in the subsequent 7 8 of the gene. 5 p w;h (bjS; T ) = L + jmatch (suffix (S; w) : b; T; h)j P 2 L + jmatch (suffix (S; w) : ; T; h)j (3) where L is a Laplace-style flattening constant <ref> [Laplace, 1825] </ref>, and is equal to one for the purposes of this paper. Strings S and T are thought of as time series, and the p w;h provide predictions for next symbol in S and correspond to the experts discussed earlier.
Reference: [Lauc et al., 1992] <author> Lauc, G., Ilic, I., and Heffer-Lauc, H. </author> <year> (1992). </year> <title> Entropies of coding and noncoding sequences of DNA and proteins. </title> <journal> Biophysical Chemistry, </journal> (42):7-11. 
Reference: [Loewenstern et al., 1997] <author> Loewenstern, D., Berman, H., and Hirsh, H. </author> <year> (1997). </year> <title> Maximum a posteriori classification of DNA structure from sequence information. </title> <type> Technical Report DCS-TR-331, </type> <institution> Dept. of Computer Science, Rutgers University. </institution>
Reference-contexts: Similarly L (y : z) L (y) corresponds to log 2 P (zjy). At this point one might simply compare to affect classification, or exponentiate yielding probabilities. These can then be combined along with prior class probabilities, resulting in Bayesian classifier built from data compressors. Later work <ref> [Loewenstern et al., 1997] </ref> applied this general idea to classify short DNA sequences by their x-ray crystallographic structure. 8 Conclusion and Future Work We have shown that the near repeats in natural DNA sequences may be incorporated into a statistical model resulting in significantly lower entropy estimates.
Reference: [Loewenstern et al., 1995] <author> Loewenstern, D., Hirsh, H., Yianilos, P. N., and Noordewier, M. </author> <year> (1995). </year> <title> DNA sequence classification using compression-based induction. </title> <type> Technical Report TR 95-087, </type> <institution> DIMACS. </institution>
Reference-contexts: A data compressor may be used as a classifier, and the result interpreted in probabilistic terms. This idea was introduced in the context of bioinformatics by <ref> [Loewenstern et al., 1995] </ref> where compressive classification was applied to several problems including promoter recognition. To illustrate, suppose one has two long sequences x; y of DNA, the first of some type A and the other of type B. The task is to classify a third strand z.
Reference: [Mantegna et al., 1993] <author> Mantegna, R., Buldyrev, S., Goldberger, A., Havlin, S., Peng, C.-K., Si-mons, M., and Stanley, H. </author> <year> (1993). </year> <title> Linguistic features of noncoding DNA sequences. </title> <journal> Physical Review Letters, </journal> <volume> 73(23) </volume> <pages> 3169-3172. </pages>
Reference-contexts: A logical next step taken by several investigators focuses instead on higher order entropy estimates arising from measurements of the frequencies of longer sequences. For natural languages (e.g., English) this step typically leads to significantly lower entropy estimates. But the best resulting estimate for humretblas is roughly 1:90 bits <ref> [Mantegna et al., 1993] </ref>, still not impressively different from our 2-bit random starting point. This may be something of a surprise, since such models reflect such known DNA structure as %(C + G) composition and CG suppression. <p> In some cases our results may be compared directly with estimates from [Grumbach and Tahi, 1994], which are included in the table. Our values for H 4 (the 4-symbol entropy) may be compared with the redundancy estimates of <ref> [Mantegna et al., 1993] </ref> and are in agreement. We have grouped our results by general type (i.e., mammalian, prokaryote, etc.). 9 The H 1 ; H 4 ; H 6 columns contain conventional multigram entropy estimates. The cdna col-umn reports our model's cross-validation entropy estimates.
Reference: [Noordewier, 1996] <author> Noordewier, M. </author> <year> (1996). </year> <title> Private Communication. </title> <note> Available at http://paul.rutgers.edu/~loewenst/cdna.html. </note>
Reference-contexts: To gather a larger body of coding regions, we obtained a data set of 490 complete human genes. This data set was screened to remove any partial genes, pseudogenes, mutants, copies, or variants of the same gene <ref> [Noordewier, 1996] </ref>. The resulting sequence contains 484; 483 bases and is referred to as our non-redundant data set. 5 Experimental Results Our model's performance on the sequences described in Section 4 is summarized in Table 2.
Reference: [Poritz, 1988] <author> Poritz, A. B. </author> <year> (1988). </year> <title> Hidden Markov models: a guided tour. </title> <booktitle> In Proc. ICASSP-88, </booktitle> <pages> pages 7-13. </pages>
Reference: [Redner and Walker, 1984] <author> Redner, R. A. and Walker, H. F. </author> <year> (1984). </year> <title> Mixture densities, maximum likelihood, and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26 </volume> <pages> 195-239. </pages>
Reference-contexts: To effect this optimization we apply the Baum-Welch algorithm for Hidden Markov Models [Baum and Eagon, 1967, Baum et al., 1970, Baum, 1972, Poritz, 1988], which may be viewed as an instance of Expectation Maximization (EM) | a later rediscovery <ref> [Dempster et al., 1977, Redner and Walker, 1984] </ref> of essentially the same algorithm and underlying information theoretic inequality. This approach requires a starting point (in our experiments uniform distributions) and climbs to a local maximum.
Reference: [Salamon and Konopka, 1992] <author> Salamon, P. and Konopka, A. K. </author> <year> (1992). </year> <title> A maximum entropy principle for the distribution of local complexity in naturally occurring nucleotide sequences. </title> <journal> Computers Chem., </journal> <volume> 16(2) </volume> <pages> 117-124. </pages>
Reference-contexts: Thus the maximum entropy level per nucleotide for coding regions is not log 2 20=3 1:44 bits, but rather is log 2 61=3 1:977 bits. In fact, it has been observed by several authors that coding regions are less compressible than non-coding regions (e.g., <ref> [Salamon and Konopka, 1992, Farach et al., 1994] </ref>). So it is clear that two sequences that code for the same polypeptide may nevertheless have large Hamming distance.
Reference: [Yianilos, 1997] <author> Yianilos, P. N. </author> <year> (1997). </year> <title> Topics in Computational Hidden State Modeling. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Princeton University. </institution>
Reference-contexts: This approach requires a starting point (in our experiments uniform distributions) and climbs to a local maximum. We have not considered the complexity of the cdna global optimization problem but remark that the general problem of globally optimizing directed acyclic graph-based models is known to be NP-complete (see <ref> [Yianilos, 1997, p. 32] </ref>). Despite our model's simple graphical structure it does exhibit multiple local maxima | but our experience based on trying many random starting points 7 is that these are very close to one another.
Reference: [Ziv and Lempel, 1977] <author> Ziv, J. and Lempel, A. </author> <year> (1977). </year> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Transactions on Information Theory, IT-23(3). </journal> <volume> 22 </volume>
Reference-contexts: The 1:90 bit result is then even more surprising since it implies that knowledge of the immediate past reduces the entropy estimate by a mere 0:05 bits. 1 Data compression techniques such as Lempel-Ziv (LZ) coding <ref> [Ziv and Lempel, 1977] </ref> may be viewed as entropy estimators, with LZ corresponding to a model that predicts based on a historical context of variable length. It "compresses" humretblas to 2:14 bits per character 2 , which is actually worse than the flat random model we started with.
References-found: 26

