URL: http://www.isle.org/~langley/papers/traffic.tr.ps
Refering-URL: http://www.isle.org/~langley/pubs.html
Root-URL: 
Email: moriarty, langley@rtna.daimlerbenz.com  
Title: Distributed Learning of Lane-Selection Strategies for Traffic Management  
Author: David E. Moriarty Pat Langley 
Keyword: Running head: Distributed Learning for Traffic Management Key words: reinforcement learning, distributed control, traffic management, lane selection  
Note: Contact author: Pat Langley; langley@rtna.daimlerbenz.com; (650) 845-2532  
Address: 1510 Page Mill Road, Palo Alto, CA 94304  
Affiliation: Intelligent Systems Laboratory Daimler-Benz Research and Technology Center  
Abstract: This paper explores a novel approach to traffic management that relies on a distributed scheme in which vehicles themselves select lanes. After formulating the problem of distributed traffic control, we describe an initial system that uses reinforcement learning to acquire lane selection strategies from experience with a traffic simulator. In addition, we report experimental studies of this method which demonstrate that the learned strategies let drivers more closely match their desired speeds than do handcrafted strategies and that it also reduces the number of lane changes. The experiments also suggest that the learned behaviors generalize to different traffic densities, different numbers of lanes, situations involving blocked lanes, and even the presence of selfish drivers. In addition, we report lesion studies that reveal the contributions of the different learning modules. In closing, we discuss related work on traffic control and consider some directions for future research. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Carrara, M., & Morello, E. </author> <title> Advanced control strategies and methods for motorway of the future. In The Drive Project DOMINC: New Concepts and Research Under Way. </title>
Reference: <author> Dietterich, T. G. </author> <year> (1990). </year> <title> Exploratory research in machine learning. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 5-9. </pages>
Reference: <author> Eskafi, F. </author> <year> (1996). </year> <title> Modeling and Simulation of the Automated Highway System. </title> <type> Ph.D. thesis, </type> <institution> Department of EECS, The University of California at Berkeley. </institution>
Reference-contexts: Removing these assumptions unnecessarily complicates the model, which creates unacceptable run times for exploratory research. In future work, we will expand our experiments to a more realistic simulator such as SmartPATH <ref> (Eskafi, 1996) </ref>. During training, the learning system uses the traffic simulator to evaluate candidate lane-selection strategies. Each evaluation or trial lasts 400 simulated seconds and begins with a random dispersement of 200 cars over three lanes on the 13.3 mile roadway.
Reference: <author> Forbes, J., Huang, T., Kanazawa, K., & Russell, S. </author> <year> (1995). </year> <title> The BATmobile: Toards a bayesian automated taxi. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95) Montreal, </booktitle> <address> CA. </address>
Reference-contexts: However, there are enough common aims that we could adapt their learning technique to our task and vice versa. The Bayesian Automated Taxi (BAT) project aims to build a fully automated vehicle that can drive in normal traffic <ref> (Forbes, Huang, Kanazawa, & Russell, 1995) </ref>. The system includes a hierarchy of control modules that range from trip planning to automated driving, including a module for lane selection at the middle level. However, the current system makes lane changes only to maintain the target speed of the individual car.
Reference: <author> Gilmore, J. F., Elibiary, K. J., & Forbes, H. C. </author> <year> (1994). </year> <title> Knowledge-based advanced traffic management system. </title> <booktitle> In Proceedings of IVHS America Atlanta, </booktitle> <address> GA. </address>
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address> <note> Distributed Learning for Traffic Management 22 Grefenstette, </note> <author> J. J., Ramsey, C. L., & Schultz, A. C. </author> <year> (1990). </year> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 355-381. </pages>
Reference-contexts: The system maintains a population of possible strategies, evaluates the goodness of each from its performance in the domain, and uses an evolutionary algorithm to generate new strategies. The evolutionary algorithm modifies the pool of strategies through common genetic operators like selection, crossover, and mutation <ref> (Goldberg, 1989) </ref>. SANE represents its decision strategies as artificial neural networks that form a direct mapping from sensors to decisions and provide effective generalization over the state space.
Reference: <author> Hall, R. W. </author> <year> (1995). </year> <title> Longitudinal and lateral throughput on an idealized highway. </title> <journal> Transportation Science, </journal> <volume> 29, </volume> <pages> 118-127. </pages>
Reference: <author> Holland, J. H., & Reitman, J. S. </author> <year> (1978). </year> <title> Cognitive systems based on adaptive algorithms. In Pattern-directed Inference Systems. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Kaelbling, L. P., Littman, M. L., & Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 237-285. </pages>
Reference: <author> Kagolanu, K., Fink, R., Smartt, H., Powell, R., & Larson, E. </author> <year> (1995). </year> <title> An intelligent traffic controller. </title> <booktitle> In Proceedings of the Second World Congress on Intelligent Transport Systems, </booktitle> <pages> pp. </pages> <address> 259-264 Yokohama, Japan. </address>
Reference: <author> McCallum, A. K. </author> <year> (1996). </year> <title> Learning to use selective attention and short-term memory in sequential tasks. </title> <booktitle> In Proceedings of Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pp. </pages> <address> 315-324 Cape Cod, MA. </address>
Reference: <author> Moriarty, D. E. </author> <year> (1997). </year> <title> Symbiotic Evolution of Neural Networks in Sequential Decision Tasks. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Sciences, The University of Texas at Austin. </institution>
Reference-contexts: This feature helps ensure that the solution space will be explored efficiently throughout the learning process. Thus, SANE is more resilient to suboptimal convergence and more adaptive to domain changes than standard evolutionary algorithms <ref> (Moriarty, 1997) </ref>. SANE represents each lane-selection strategy as a neural network that maps a car's sensory input into a specific lane-selection decision. Figure 4 shows the input and output of the lane-selection networks. Each network consists of 16 input units, 12 hidden units, and 3 output units.
Reference: <author> Moriarty, D. E., & Miikkulainen, R. </author> <year> (1996a). </year> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22, </volume> <pages> 11-32. </pages>
Reference: <author> Moriarty, D. E., & Miikkulainen, R. </author> <year> (1996b). </year> <title> Evolving obstacle avoidance behavior in a robot arm. </title> <booktitle> In From Animals to Animats: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior (SAB-96), </booktitle> <pages> pp. </pages> <address> 468-475 Cape Cod, MA. </address>
Reference-contexts: Experience in other domains such as robotics indicates that SANE is very good at finding good areas of the solution space quickly, but that it can have trouble pinpointing the best solutions within that space <ref> (Moriarty & Miikkulainen, 1996b) </ref>. The local learning module does aid in this refinement, but there remains a quickly diminishing return as simulation continues.
Reference: <author> Pomerleau, D. </author> <year> (1995). </year> <title> Ralph: Rapidly adapting lateral position handler. </title> <booktitle> In Proceedings of the 1995 IEEE Symposium on Intelligent Vehicles Detroit, </booktitle> <address> MI. </address>
Reference: <author> Pomerleau, D. A. </author> <year> (1992). </year> <title> Neural Network Perception for Mobile Robot Guidance. </title> <type> Ph.D. thesis, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Pooran, F. J., Tarnoff, P. J., & Kalaputapu, R. </author> <year> (1996). </year> <title> RT-TRACS: </title> <booktitle> Development of the real-time control logic. In Proceedings of the 1996 Annual Meeting of ITS America, </booktitle> <pages> pp. </pages> <address> 422-430 Houston, Tx. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference: <author> Ramaswamy, D., Medanic, J. V., Perkins, W. R., & Benekohal, R. F. </author> <year> (1997). </year> <title> Lane assignment on automated highway systems. </title> <journal> IEEE Transactions on Vehicular Technology, </journal> <volume> 46, </volume> <pages> 755-769. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> pp. 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: These pairs denote examples of good behavior that can be fed to a supervised learning method to form initial strategies. Since SANE's strategies are represented as neural networks, the population seeder employs the backpropagation algorithm <ref> (Rumelhart et al., 1986) </ref> to train the networks over the training examples. To maintain diversity within the initial population of neural networks and not overly bias SANE toward the rules of thumb, only a subset of the networks are seeded using the default knowledge.
Reference: <author> Sammut, C., Hurst, S., Kedzier, D., & Michie, D. </author> <year> (1992). </year> <title> Learning to fly. </title> <booktitle> In Machine Learning: Proceedings of the Ninth International Workshop, </booktitle> <pages> pp. 385-393. </pages> <editor> Morgan Kaufmann. </editor> <title> Distributed Learning for Traffic Management 23 Schmidhuber, </title> <editor> J. </editor> <year> (1996). </year> <title> A general method for multi-agent reinforcement learning in unrestricted environments. In Adaptation, Coevolution, and Learning: </title> <booktitle> Papers from the AAAI Spring Symposium, </booktitle> <pages> pp. 84-97. </pages> <publisher> AAAI Press. </publisher>
Reference: <author> Sukthankar, R., Baluja, S., & Hancock, J. </author> <year> (1997). </year> <title> Evolving an intelligent vehicle for tactical reasoning in traffic. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation. </booktitle>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temproal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: A negative training signal in the previous example, would generate a target output of (1.0, 0.0, 1.0), and the resulting network would be less likely to choose change left in similar situations. The learning strategy is somewhat similar to temporal-difference methods <ref> (Sutton, 1988) </ref> for reinforcement learning, in that updates are based on the performance differences over successive time periods. However, temporal difference methods treat performance differences as prediction errors from which they can learn to predict future rewards.
Reference: <author> Varaiya, P. </author> <year> (1993). </year> <title> Smart cars on smart roads: Problems ofcontrol. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 38, </volume> <pages> 195-207. </pages>
Reference: <author> Varaiya, P., & Shladover, S. </author> <year> (1991). </year> <title> Sketch of an ivhs systems architecture. </title> <type> Tech. rep. </type> <note> PATH Research Report UCB-ITS-PRR-91-03, </note> <institution> Instution for Transportation Studies, The University of California at Berkeley. </institution>
Reference: <author> Watkins, C. J. C. H., & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 (3), </volume> <pages> 279-292. </pages>
Reference: <author> Whitley, D., Dominic, S., Das, R., & Anderson, C. W. </author> <year> (1993). </year> <title> Genetic reinforcement learning for neurocontrol problems. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 259-284. </pages>
Reference: <author> Wilson, S. W. </author> <year> (1994). </year> <title> ZCS: A zeroth level classifier system. </title> <journal> Evolutionary Computation, </journal> <volume> 2 (1), </volume> <pages> 1-18. </pages>
References-found: 28

