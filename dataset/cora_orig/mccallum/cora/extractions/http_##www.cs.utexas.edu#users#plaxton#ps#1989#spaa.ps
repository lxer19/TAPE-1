URL: http://www.cs.utexas.edu/users/plaxton/ps/1989/spaa.ps
Refering-URL: http://www.cs.utexas.edu/users/plaxton/html/abc.html
Root-URL: 
Title: Load Balancing, Selection and Sorting on the Hypercube  
Author: C. Greg Plaxton 
Address: Stanford, CA 94305  
Affiliation: Department of Computer Science Stanford University  
Abstract: This paper presents novel load balancing, selection and sorting algorithms for the hypercube with 1-port communication. The main result is an algorithm for sorting n values on p processors, SmoothSort, that runs asymptotically faster (in the worst case) than any previously known algorithm over a wide range of the ratio n=p. The load balancing and selection algorithms upon which SmoothSort is based are expected to be of independent interest. Although the analysis of our algorithms is limited to obtaining asymptotic bounds, the constant factors being ignored are quite small.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal and M.-D. A. Huang. </author> <title> Network complexity of sorting and graph problems and simulating CRCW PRAMs by interconnection networks. </title> <editor> In J. H. Reif, editor, </editor> <booktitle> Lecture Notes in Computer Science: VLSI Algorithms and Architectures (AWOC 88), </booktitle> <volume> vol. 319, </volume> <pages> pages 339-350. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: One may verify that BitonicSort provides optimal speedup over sequential sorting only if p = O (2 log n ). Two recent algorithms, which we refer to as CubeSort (Cypher and Sanz, [5]) and ColumnSort (Aggarwal and Huang, <ref> [1] </ref>), have improved this result significantly. Both of these algorithms are optimal if n exceeds p by a polynomial factor, that is, if n = p 1+* for any constant * &gt; 0. <p> Therefore, 2 Algorithm Running Time Range BitonicSort [2][3][9] O ((n=p) log 2 p) n = (p) MergeSort [13] O (log 2 p= log (p=n)) n = O (p) ColumnSort <ref> [1] </ref> O ((n log n)=p) n = (p 1+* ), * &gt; 0 CubeSort [5] O ((n=p) log 2 p= log (n=p)) essentially n = (p) Table 1: Previous sorting algorithms for the weak model. the running time of Balance must be at least (n dn=medn=pe)=(f (H d ; dn=me)). <p> Consider the function f (x) = x log 1=2 (n=x), where we allow x to take on real values over the range <ref> [1; n=2] </ref>. One may easily verify that f 00 (x) &lt; 0 in this range. In other words, f (x) is a concave function.
Reference: [2] <author> K. E. Batcher. </author> <title> Sorting networks and their applications. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <volume> vol. 32, </volume> <pages> pages 307-314, </pages> <year> 1968. </year>
Reference-contexts: For the case n = p, the best known sorting algorithm for the weak model is Batcher's bitonic sort, which runs in O (log 2 p) time <ref> [2] </ref>. For n 6= p, a number of other algorithms have been proposed. The running time and range of applicability of each of these algorithms is summarized in Table 1.
Reference: [3] <author> G. Baudet and D. Stevenson. </author> <title> Optimal sorting algorithms for parallel computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27:84-87, </volume> <year> 1978. </year>
Reference-contexts: For n 6= p, a number of other algorithms have been proposed. The running time and range of applicability of each of these algorithms is summarized in Table 1. Note that BitonicSort refers to the straightforward split-and-merge generalization of bitonic sort, due independently to Baudet and Stevenson <ref> [3] </ref> and Johns-son [9]. Also, it should be emphasized that we have restricted attention to deterministic, worst-case complexity algorithms running on the weak model. For examples of results based on other assumptions, we refer the reader to [15], [16] and [18].
Reference: [4] <author> A. Borodin, L. J. Guibas, N. A. Lynch, and A. C. Yao. </author> <title> Efficient searching using partial ordering. </title> <journal> Information Processing Letters, </journal> <volume> 12 </volume> <pages> 71-75, </pages> <year> 1981. </year>
Reference-contexts: This sequential trade-off between preprocessing time and search time is well understood, see <ref> [4] </ref> and [10]. 3. Find that m 2 M with rank in S closest to j (if there is a tie, break it arbitrarily) and broadcast it to all processors. We compute m in time O (s + log 2 p) as follows.
Reference: [5] <author> R. E. Cypher and J. L. C. Sanz. Cubesort: </author> <title> An optimal sorting algorithm for feasible parallel computers. </title> <editor> In J. H. Reif, editor, </editor> <booktitle> Lecture Notes in Computer Science: VLSI Algorithms and Architectures (AWOC 88), </booktitle> <volume> vol. 319, </volume> <pages> pages 456-464. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: For examples of results based on other assumptions, we refer the reader to [15], [16] and [18]. One may verify that BitonicSort provides optimal speedup over sequential sorting only if p = O (2 log n ). Two recent algorithms, which we refer to as CubeSort (Cypher and Sanz, <ref> [5] </ref>) and ColumnSort (Aggarwal and Huang, [1]), have improved this result significantly. Both of these algorithms are optimal if n exceeds p by a polynomial factor, that is, if n = p 1+* for any constant * &gt; 0. <p> Therefore, 2 Algorithm Running Time Range BitonicSort [2][3][9] O ((n=p) log 2 p) n = (p) MergeSort [13] O (log 2 p= log (p=n)) n = O (p) ColumnSort [1] O ((n log n)=p) n = (p 1+* ), * &gt; 0 CubeSort <ref> [5] </ref> O ((n=p) log 2 p= log (n=p)) essentially n = (p) Table 1: Previous sorting algorithms for the weak model. the running time of Balance must be at least (n dn=medn=pe)=(f (H d ; dn=me)).
Reference: [6] <author> P. Frankl and Z. Furedi. </author> <title> A short proof for a theorem of Harper about Hamming spheres. </title> <journal> Discrete Mathematics, </journal> <volume> 34 </volume> <pages> 311-313, </pages> <year> 1981. </year>
Reference-contexts: The correctness of this intuition is borne out by the following theorem due to Harper [7]. Note that Frankl and Furedi have given a simpler proof of the same result <ref> [6] </ref>. Theorem 1 There exists a ball B 2 B (x) such that f (H d ; x) = jF (H d ; B)j, 0 x p. We need the following estimate of the "volume-to-surface" ratio of a Hamming ball.
Reference: [7] <author> L. Harper. </author> <title> Optimal numberings and isoperimet-ric problems on graphs. </title> <journal> J. Combinatorial Theory, </journal> <volume> 1 </volume> <pages> 385-393, </pages> <year> 1966. </year> <month> 10 </month>
Reference-contexts: Intuitively, one might expect that the value of f (H d ; x) is determined by a Hamming ball configuration. The correctness of this intuition is borne out by the following theorem due to Harper <ref> [7] </ref>. Note that Frankl and Furedi have given a simpler proof of the same result [6]. Theorem 1 There exists a ball B 2 B (x) such that f (H d ; x) = jF (H d ; B)j, 0 x p.
Reference: [8] <author> C.-T. Ho and S. L. Johnsson. </author> <title> Distributed rout-ing algorithms for broadcasting and personalized communication in hypercubes. </title> <booktitle> In Proceedings of the 1986 IEEE International Conference on Parallel Processing, </booktitle> <pages> pages 640-648, </pages> <year> 1986. </year>
Reference-contexts: If l = 0, return. 3. Compute ffi i , 0 i &lt; l. This involves performing l independent sums over the entire subcube. These sum operations can be pipelined to run in a total of O (l) time <ref> [8] </ref>. 4. Determine the dimension i fl with least associated discrepancy ffi i fl . This takes O (l) time. 5. Recursively balance the high and low subcubes with respect to dimension i fl , using steps 2 to 6. 6. <p> If l = 0, return. 3. Compute ffi M i , 0 i &lt; l. This involves performing l independent sums over the entire subcube for each of the g groups. Each set of l sum operations can be pipelined to run in O (l) time <ref> [8] </ref>, so the total running time is O (gl). 4. Determine the dimension i fl with least associated multi-discrepancy ffi M i fl . This takes O (l) time. 5.
Reference: [9] <author> S. L. Johnsson. </author> <title> Combining parallel and sequential sorting on a Boolean n-cube. </title> <booktitle> In Proceedings of the 1984 IEEE International Conference on Parallel Processing, </booktitle> <pages> pages 444-448, </pages> <year> 1984. </year>
Reference-contexts: The running time and range of applicability of each of these algorithms is summarized in Table 1. Note that BitonicSort refers to the straightforward split-and-merge generalization of bitonic sort, due independently to Baudet and Stevenson [3] and Johns-son <ref> [9] </ref>. Also, it should be emphasized that we have restricted attention to deterministic, worst-case complexity algorithms running on the weak model. For examples of results based on other assumptions, we refer the reader to [15], [16] and [18].
Reference: [10] <author> R. M. Karp, R. Motwani, and P. Raghavan. </author> <title> Deferred data structuring. </title> <journal> SIAM J. Comput., </journal> <volume> 17 </volume> <pages> 883-902, </pages> <year> 1988. </year>
Reference-contexts: This sequential trade-off between preprocessing time and search time is well understood, see [4] and <ref> [10] </ref>. 3. Find that m 2 M with rank in S closest to j (if there is a tie, break it arbitrarily) and broadcast it to all processors. We compute m in time O (s + log 2 p) as follows.
Reference: [11] <author> F. T. Leighton. </author> <type> Personal communication. </type>
Reference-contexts: Note that we can view the post-processing task as an instance of Balance with m = log p. An algorithm due to Leighton gives an O (m log p) implementation of Balance with minimum error <ref> [11] </ref>. The algorithm runs in m phases, and each phase takes care of one token from every processor for which the supply of tokens has not yet been exhausted. In a phase, the designated tokens are routed to a contiguous block (with respect to processor id modulo p) of processors.
Reference: [12] <author> F. T. Leighton. </author> <title> Tight bounds on the complexity of parallel sorting. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34:344-354, </volume> <year> 1985. </year>
Reference-contexts: Both of these algorithms are optimal if n exceeds p by a polynomial factor, that is, if n = p 1+* for any constant * &gt; 0. ColumnSort is based on Leighton's technique for sorting n values by performing a constant number of smaller sorts <ref> [12] </ref>. Note that Table 1 does not indicate the running time of ColumnSort when * is allowed to vary.
Reference: [13] <author> D. Nassimi and S. Sahni. </author> <title> Parallel permutation and sorting algorithms and a new generalized connection network. </title> <journal> JACM, </journal> <volume> 29 </volume> <pages> 642-667, </pages> <year> 1982. </year>
Reference-contexts: Only processors located at the fringe of S, those in F (H d ; S), can send tokens out of S, and these can only transmit one token per time step. Therefore, 2 Algorithm Running Time Range BitonicSort [2][3][9] O ((n=p) log 2 p) n = (p) MergeSort <ref> [13] </ref> O (log 2 p= log (p=n)) n = O (p) ColumnSort [1] O ((n log n)=p) n = (p 1+* ), * &gt; 0 CubeSort [5] O ((n=p) log 2 p= log (n=p)) essentially n = (p) Table 1: Previous sorting algorithms for the weak model. the running time of <p> In this manner, the set of tokens will be distributed as evenly as possible. A single phase of Leighton's algorithm is implemented by performing a prefix sum over the designated tokens followed by a concentration route as defined by Nas-simi and Sahni <ref> [13] </ref>. The prefix sum gives the offset of each token within the contiguous block of destination processors. The size of the block is broadcast so that all processors can compute the start of the next block.
Reference: [14] <author> D. Peleg and E. Upfal. </author> <title> The token distribution problem. </title> <journal> SIAM J. Comput., </journal> <volume> 18 </volume> <pages> 229-243, </pages> <year> 1989. </year>
Reference-contexts: The problem is to redistribute the tokens so that each processor has either bn=pc or dn=pe tokens, that is, so that the load is distributed as evenly as possible. Peleg and Upfal have exhibited tight bounds for this operation on a certain class of expander networks <ref> [14] </ref>. In many applications, it is not necessary to balance the population of tokens exactly. If the difference between the maximum number of tokens at any processor and the minimum number of tokens at any processor is ~, we will say that the tokens have been balanced with error ~. <p> The communication component of algorithm Select can be expressed in terms of the primitive operations: sort p values, broadcast and sum. Thus, it adapts easily to a variety of networks. However, simulating Vishkin's algorithm yields a faster selection algorithm on any network family (eg. certain expander graphs, see <ref> [14] </ref>) that can implement the Balance operation of Section 2 in o (m log log p) time. 5 Sorting The Sort operation is defined as follows.
Reference: [15] <author> J. H. Reif and L. G. Valiant. </author> <title> A logarithmic time sort for linear size networks. </title> <journal> JACM, </journal> <volume> 34 </volume> <pages> 60-76, </pages> <year> 1987. </year>
Reference-contexts: Also, it should be emphasized that we have restricted attention to deterministic, worst-case complexity algorithms running on the weak model. For examples of results based on other assumptions, we refer the reader to <ref> [15] </ref>, [16] and [18]. One may verify that BitonicSort provides optimal speedup over sequential sorting only if p = O (2 log n ). Two recent algorithms, which we refer to as CubeSort (Cypher and Sanz, [5]) and ColumnSort (Aggarwal and Huang, [1]), have improved this result significantly.
Reference: [16] <author> P. Varman and K. Doshi. </author> <title> Sorting with linear speedup on a pipelined hypercube. </title> <type> Technical Report TR-8802, </type> <institution> Rice University, Department of Electrical and Computer Engineering, </institution> <month> Febru-ary </month> <year> 1988. </year>
Reference-contexts: Also, it should be emphasized that we have restricted attention to deterministic, worst-case complexity algorithms running on the weak model. For examples of results based on other assumptions, we refer the reader to [15], <ref> [16] </ref> and [18]. One may verify that BitonicSort provides optimal speedup over sequential sorting only if p = O (2 log n ). Two recent algorithms, which we refer to as CubeSort (Cypher and Sanz, [5]) and ColumnSort (Aggarwal and Huang, [1]), have improved this result significantly. <p> It should be mentioned that Leighton's algorithm is best suited for a particular pipelined model of the hypercube. For a definition of this model, see the work of Varman and Doshi <ref> [16] </ref>. The prefix sum and broadcast operations can be pipelined on the weak model but the concentration routes cannot (provably). The pipelined model of the hypercube allows concentration routes to be pipelined as well, so that Balance can be implemented to run in O (m+log p) time.
Reference: [17] <author> U. Vishkin. </author> <title> An optimal parallel algorithm for selection. </title> <type> Technical Report 106, </type> <institution> Courant Institute of Mathematical Sciences, Department of Computer Science, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: Our selection algorithm obtains efficient performance by eliminating from consideration a constant fraction of the values at every processor in each iteration. Interestingly, this is accomplished without redistributing the values, in contrast to the best known PRAM algorithm due to Vishkin <ref> [17] </ref>. Algorithm Select 1. At every processor, mark each of the O (n=p) lo cal values "live". This takes O (n=p) time. 2. Let S i denote the set of live items located at processor i, and let m i denote the median of S i .
Reference: [18] <author> B. Wagar. Hyperquicksort: </author> <title> A fast sorting algorithm for hypercubes. </title> <booktitle> In Proceedings of the Second Conference on Hypercube Multiprocessors, </booktitle> <pages> pages 292-299, </pages> <year> 1986. </year> <month> 11 </month>
Reference-contexts: Also, it should be emphasized that we have restricted attention to deterministic, worst-case complexity algorithms running on the weak model. For examples of results based on other assumptions, we refer the reader to [15], [16] and <ref> [18] </ref>. One may verify that BitonicSort provides optimal speedup over sequential sorting only if p = O (2 log n ). Two recent algorithms, which we refer to as CubeSort (Cypher and Sanz, [5]) and ColumnSort (Aggarwal and Huang, [1]), have improved this result significantly. <p> By simultaneously guaranteeing good worst-case performance, SmoothSort avoids the potential pitfalls of a simpler scheme such as HyperQuickSort <ref> [18] </ref>. For solving the related problem of permutation routing, SmoothSort is even more practical because the cost of performing selections goes away. On the perfect shu*e, SmoothSort performs permutation routing in O ((n=p) log 2 p= log (n=p)) time.
References-found: 18

