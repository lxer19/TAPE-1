URL: http://www.cs.arizona.edu/people/debray/papers/cfwd.ps
Refering-URL: http://www.cs.arizona.edu/jc/
Root-URL: http://www.cs.arizona.edu
Title: Call Forwarding: A Simple Interprocedural Optimization Technique for Dynamically Typed Languages  
Author: Koen De Bosschere; Saumya Debray; David Gudeman; Sampath Kannan 
Address: B-9000 Gent, Belgium  Tucson, AZ 85721, USA  
Affiliation: Department of Electronics Universiteit Gent  Department of Computer Science The University of Arizona  
Abstract: This paper discusses call forwarding, a simple interpro-cedural optimization technique for dynamically typed languages. The basic idea behind the optimization is straightforward: find an ordering for the "entry actions" of a procedure, and generate multiple entry points for the procedure, so as to maximize the savings realized from different call sites bypassing different sets of entry actions. We show that the problem of computing optimal solutions to arbitrary call forwarding problems is NP-complete, and describe an efficient greedy algorithm for the problem. Experimental results indicate that (i) this algorithm is effective, in that the solutions produced are generally close to optimal; and (ii) the resulting optimization leads to significant performance improvements for a number of benchmarks tested. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Appel, </author> <title> Compiling with Continuations, </title> <publisher> Cam-bridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: optimization is not limited, a priori, to dynamically typed languages: it is also applicable, in principle, to idempotent entry actions, such as initialization and array bound checks, in statically typed languages, and some optimizations used in statically typed languages, such as inverse eta-reduction/uncurrying/argument flattening in Standard ML of New Jersey <ref> [1] </ref>, can also be thought of as instances of call forwarding (see Section 6). 2 The Call Forwarding Problem As discussed in the previous section, the code generated for a procedure consists of a set of entry actions, which can be carried out in a number of different legal orders, followed <p> Standard ML of New Jersey uses a combination of three transformations|inverse eta-reduction, uncurrying, and argument flattening|to optimize functions where all of the known call sites pass tuples of the same size as arguments, but where the function may "escape," i.e., not all of call sites are known at compile time <ref> [1] </ref>. The idea is to have the known call sites pass arguments in registers instead of constructing and deconstructing tuples on the heap, while call sites that are unknown at compile time execute additional code to correctly deconstruct the tuples they pass.
Reference: [2] <author> A. V. Aho, R. Sethi and J. D. Ullman, </author> <booktitle> Compilers - Principles, Techniques and Tools, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: cells, while Janus is a single assignment language where the program allocates new cons cells at each iteration|its performance can be attributed at least in part to the benefits of call forwarding. 6 Related Work The optimizations described here can be seen as generalizing some optimizations for traditional imperative languages <ref> [2] </ref>. In the special case of a (conditional or unconditional) jump whose target is a (conditional or unconditional) jump instruction, call forwarding generalizes the flow-of-control optimization that collapses chains of jump instructions. <p> able to deal with conditional jumps to conditional jumps (this turns out to be an important source of performance improvement in practice), while traditional compilers for imperative languages such as C and Fortran typically deal only with jump chains where there is at most one conditional jump (see, for example, <ref> [2] </ref>, p. 556). <p> From the conventional definition of a "loop" in a flow graph (see, for example, <ref> [2] </ref>), there is one loop in the flow graph of this function that includes both the tail recursive call sites for f ().
Reference: [3] <author> T. Ball and J. Larus, </author> <title> "Optimally Profiling and Tracing Programs", </title> <booktitle> Proc. 19th. ACM Symp. on Principles of Programming Languages, </booktitle> <address> Albu-querque, NM, </address> <month> Jan. </month> <year> 1992, </year> <pages> pp. 59-70. </pages>
Reference-contexts: Moreover, each call site has associated with it an estimate of its execution frequency: such estimates can be obtained from profile information, or from the structure of the call graph of the program (see, for example, <ref> [3, 19] </ref>).
Reference: [4] <author> M. Carlsson and J. Widen, </author> <title> SICStus Prolog User's Manual, </title> <institution> Swedish Institute of Computer Science, </institution> <month> Oct. </month> <year> 1988. </year>
Reference-contexts: make many assumptions about the source language, except that a call to a procedure typically involves executing a set of idempotent "entry actions." This covers a wide variety of dynamically typed languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [15]), logic programming languages such as Prolog <ref> [4] </ref>, GHC [17] and Janus [11, 13], imperative languages such as SETL [14], and object-oriented languages such as Smalltalk [10] and SELF [6].
Reference: [5] <author> C. Chambers and D. Ungar, </author> <title> "Iterative Type Analysis and Extended Message Splitting: Optimizing Dynamically Typed Object-Oriented Programs", </title> <booktitle> Proc. SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990, </year> <pages> pp. </pages> <month> 150-164. </month> <journal> SIG-PLAN Notices vol. </journal> <volume> 25 no. </volume> <pages> 6. </pages>
Reference-contexts: into a jump instruction, and noticing that the recursive call does not need to test the types of its second and third arguments, the target of this jump 1 In reality, the generated code would distinguish between the numeric types int and float, e.g., using "message splitting" techniques as in <ref> [5, 6] </ref>|the distinction is not important here, and we assume a single numeric type for simplicity of exposition. 1 would be chosen to bypass these tests. <p> Figure 2 (a) gives the intermediate code that might be generated in a straightforward way. (In reality, the generated code would distinguish between the numeric types int and float, e.g., using "message splitting" techniques as in <ref> [5, 6] </ref>|the distinction is not important here, and we assume a single numeric type for simplicity of exposition.) The first six instructions of ave are entry actions that can be executed in any order where the derefer-encing of a register precedes its use. <p> Chambers and Ungar consider compile-time optimization techniques to reduce runtime type checking in dynamically typed object-oriented languages <ref> [5, 6] </ref>. Their approach uses type analysis to generate multiple copies of program fragments, in particular loop bodies, where each copy is specialized to a particular type and therefore can omit some type tests.
Reference: [6] <author> C. Chambers, D. Ungar and E. Lee, </author> <title> "An Efficient Implementation of SELF, A Dynamically Typed Object-Oriented Language Based on Prototypes", </title> <booktitle> Proc. OOPSLA '89, </booktitle> <address> New Orleans, LA, </address> <year> 1989, </year> <pages> pp. 49-70. </pages>
Reference-contexts: into a jump instruction, and noticing that the recursive call does not need to test the types of its second and third arguments, the target of this jump 1 In reality, the generated code would distinguish between the numeric types int and float, e.g., using "message splitting" techniques as in <ref> [5, 6] </ref>|the distinction is not important here, and we assume a single numeric type for simplicity of exposition. 1 would be chosen to bypass these tests. <p> actions." This covers a wide variety of dynamically typed languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [15]), logic programming languages such as Prolog [4], GHC [17] and Janus [11, 13], imperative languages such as SETL [14], and object-oriented languages such as Smalltalk [10] and SELF <ref> [6] </ref>. The optimization we discuss is likely to be most beneficial for languages and programs where procedure calls are common, and which are therefore liable to benefit significantly from reducing the cost of procedure calls. <p> Figure 2 (a) gives the intermediate code that might be generated in a straightforward way. (In reality, the generated code would distinguish between the numeric types int and float, e.g., using "message splitting" techniques as in <ref> [5, 6] </ref>|the distinction is not important here, and we assume a single numeric type for simplicity of exposition.) The first six instructions of ave are entry actions that can be executed in any order where the derefer-encing of a register precedes its use. <p> Chambers and Ungar consider compile-time optimization techniques to reduce runtime type checking in dynamically typed object-oriented languages <ref> [5, 6] </ref>. Their approach uses type analysis to generate multiple copies of program fragments, in particular loop bodies, where each copy is specialized to a particular type and therefore can omit some type tests.
Reference: [7] <author> S. K. Debray, </author> <title> "A Simple Code Improvement Scheme for Prolog", </title> <journal> J. Logic Programming, </journal> <volume> vol. 13 no. 1, </volume> <month> May </month> <year> 1992, </year> <pages> pp. 57-88. </pages>
Reference-contexts: A number of authors have shown that significant performance improvements are possible if the lengths of these pointer chains can be predicted via compile-time analysis, so that unnecessary dereferenc-ing code can be deleted <ref> [7, 12, 16] </ref>; however, the analyses involved are fairly complex. Here we show how, in many cases, unnecessary dereference operations can be elim-inated using call forwarding.
Reference: [8] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness, </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: It remains NP-complete even if all entry actions have equal size. Proof By reduction from the Optimal Linear Arrangement problem, which is known to be NP-complete <ref> [8, 9] </ref>. See the Appendix for details. This result might very well be of only academic interest if the number of entry actions encountered in typical programs could be guaranteed to be small. However, our experience has been that this is not the case in many actual applications.
Reference: [9] <author> M. R. Garey, D. S. Johnson, and L. Stockmeyer, </author> <title> "Some Simplified NP-complete Graph Problems", </title> <journal> Theoretical Computer Science vol. </journal> <volume> 1, </volume> <pages> pp. 237-267, </pages> <year> 1976. </year>
Reference-contexts: It remains NP-complete even if all entry actions have equal size. Proof By reduction from the Optimal Linear Arrangement problem, which is known to be NP-complete <ref> [8, 9] </ref>. See the Appendix for details. This result might very well be of only academic interest if the number of entry actions encountered in typical programs could be guaranteed to be small. However, our experience has been that this is not the case in many actual applications.
Reference: [10] <author> A. Goldberg and D. Robson, </author> <title> Smalltalk-80: The Language and its Implementation, </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: of idempotent "entry actions." This covers a wide variety of dynamically typed languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [15]), logic programming languages such as Prolog [4], GHC [17] and Janus [11, 13], imperative languages such as SETL [14], and object-oriented languages such as Smalltalk <ref> [10] </ref> and SELF [6]. The optimization we discuss is likely to be most beneficial for languages and programs where procedure calls are common, and which are therefore liable to benefit significantly from reducing the cost of procedure calls.
Reference: [11] <author> D. Gudeman, K. De Bosschere, and S. K. Debray, </author> <title> "jc : An Efficient and Portable Implementation of Janus", </title> <booktitle> Proc. Joint International Conference and Symposium on Logic Programming, </booktitle> <address> Washing-ton DC, Nov. 1992. </address> <publisher> MIT Press. </publisher>
Reference-contexts: source language, except that a call to a procedure typically involves executing a set of idempotent "entry actions." This covers a wide variety of dynamically typed languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [15]), logic programming languages such as Prolog [4], GHC [17] and Janus <ref> [11, 13] </ref>, imperative languages such as SETL [14], and object-oriented languages such as Smalltalk [10] and SELF [6]. <p> The numbers presented reflect the performance of jc <ref> [11] </ref>, an implementation of a logic programming language called Janus [13] on a Sparcstation-1. 3 This system is currently available by anonymous FTP from cs.arizona.edu.
Reference: [12] <author> A. Marien, G. Janssens, A. Mulkers, and M. Bruynooghe, </author> <title> "The Impact of Abstract Interpretation: An Experiment in Code Generation", </title> <booktitle> Proc. Sixth International Conference on Logic Programming, </booktitle> <address> Lisbon, </address> <month> June </month> <year> 1989, </year> <pages> pp. 33-47. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: A number of authors have shown that significant performance improvements are possible if the lengths of these pointer chains can be predicted via compile-time analysis, so that unnecessary dereferenc-ing code can be deleted <ref> [7, 12, 16] </ref>; however, the analyses involved are fairly complex. Here we show how, in many cases, unnecessary dereference operations can be elim-inated using call forwarding.
Reference: [13] <author> V. Saraswat, K. Kahn, and J. Levy, </author> <title> "Janus: A step towards distributed constraint programming", </title> <booktitle> in Proc. 1990 North American Conference on Logic Programming, </booktitle> <address> Austin, TX, </address> <month> Oct. </month> <year> 1990, </year> <pages> pp. 431-446. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: source language, except that a call to a procedure typically involves executing a set of idempotent "entry actions." This covers a wide variety of dynamically typed languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [15]), logic programming languages such as Prolog [4], GHC [17] and Janus <ref> [11, 13] </ref>, imperative languages such as SETL [14], and object-oriented languages such as Smalltalk [10] and SELF [6]. <p> The numbers presented reflect the performance of jc [11], an implementation of a logic programming language called Janus <ref> [13] </ref> on a Sparcstation-1. 3 This system is currently available by anonymous FTP from cs.arizona.edu.
Reference: [14] <author> J. T. Schwartz, R. B. K. Dewar, E. Dubinsky, and E. Schonberg, </author> <title> Programming with Sets: An Introduction to SETL, </title> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: a procedure typically involves executing a set of idempotent "entry actions." This covers a wide variety of dynamically typed languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [15]), logic programming languages such as Prolog [4], GHC [17] and Janus [11, 13], imperative languages such as SETL <ref> [14] </ref>, and object-oriented languages such as Smalltalk [10] and SELF [6]. The optimization we discuss is likely to be most beneficial for languages and programs where procedure calls are common, and which are therefore liable to benefit significantly from reducing the cost of procedure calls.
Reference: [15] <author> G. L. Steele Jr., </author> <title> Common Lisp: The Language, </title> <publisher> Digital Press, </publisher> <year> 1984. </year>
Reference-contexts: level: for this reason, we do not make many assumptions about the source language, except that a call to a procedure typically involves executing a set of idempotent "entry actions." This covers a wide variety of dynamically typed languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see <ref> [15] </ref>), logic programming languages such as Prolog [4], GHC [17] and Janus [11, 13], imperative languages such as SETL [14], and object-oriented languages such as Smalltalk [10] and SELF [6].
Reference: [16] <author> A. Taylor, </author> <title> "Removal of Dereferencing and Trailing in Prolog Compilation", </title> <booktitle> Proc. Sixth International Conference on Logic Programming, </booktitle> <address> Lisbon, </address> <month> June </month> <year> 1989, </year> <pages> pp. 48-60. </pages> <publisher> MIT Press. </publisher>
Reference-contexts: A number of authors have shown that significant performance improvements are possible if the lengths of these pointer chains can be predicted via compile-time analysis, so that unnecessary dereferenc-ing code can be deleted <ref> [7, 12, 16] </ref>; however, the analyses involved are fairly complex. Here we show how, in many cases, unnecessary dereference operations can be elim-inated using call forwarding.
Reference: [17] <author> K. Ueda, </author> <title> "Guarded Horn Clauses", in Concurrent Prolog: </title> <booktitle> Collected Papers, </booktitle> <volume> vol. 1, </volume> <editor> ed. E. </editor> <booktitle> Shapiro, </booktitle> <pages> pp. 140-156, </pages> <address> 1987. </address> <publisher> MIT Press. </publisher>
Reference-contexts: assumptions about the source language, except that a call to a procedure typically involves executing a set of idempotent "entry actions." This covers a wide variety of dynamically typed languages, e.g., functional programming languages such as Lisp and Scheme (e.g., see [15]), logic programming languages such as Prolog [4], GHC <ref> [17] </ref> and Janus [11, 13], imperative languages such as SETL [14], and object-oriented languages such as Smalltalk [10] and SELF [6].
Reference: [18] <author> P. Van Roy, </author> <title> Can Logic Programming Execute as Fast as Imperative Programming?, </title> <type> PhD Dissertation, </type> <institution> University of California, Berkeley, </institution> <month> Nov. </month> <year> 1990. </year>
Reference-contexts: The idea of compiling functions with multiple entry points is not new: many Lisp systems do this, Standard ML of New Jersey and Yale Haskell generate dual entry points for functions, and Aquarius Prolog generates multiple entry points for primitive operations <ref> [18] </ref>. However, we do not know of any system that attempts to order the entry actions carefully in order to maximize the savings from bypassing entry actions. Some optimizations used in statically typed languages can also be thought of in terms of call forwarding.
Reference: [19] <author> D. W. Wall, </author> <title> "Predicting Program Behavior Using Real or Estimated Profiles", </title> <booktitle> Proc. SIGPLAN-91 Conf. on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1991, </year> <pages> pp. 59-70. </pages>
Reference-contexts: Moreover, each call site has associated with it an estimate of its execution frequency: such estimates can be obtained from profile information, or from the structure of the call graph of the program (see, for example, <ref> [3, 19] </ref>).
References-found: 19

