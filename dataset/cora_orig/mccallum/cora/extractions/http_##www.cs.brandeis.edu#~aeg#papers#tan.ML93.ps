URL: http://www.cs.brandeis.edu/~aeg/papers/tan.ML93.ps
Refering-URL: http://www.cs.brandeis.edu/~aeg/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: tan@gte.com  
Title: Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents  
Author: Ming Tan 
Address: 40 Sylvan Road Waltham, MA 02254  
Affiliation: GTE Laboratories Incorporated  
Abstract: Intelligent human agents exist in a cooperative social environment that facilitates learning. They learn not only by trial-and-error, but also through cooperation by sharing instantaneous information, episodic experience, and learned knowledge. The key investigations of this paper are, "Given the same number of reinforcement learning agents, will cooperative agents outperform independent agents who do not communicate during learning?" and "What is the price for such cooperation?" Using independent agents as a benchmark, cooperative agents are studied in following ways: (1) sharing sensation, (2) sharing episodes, and (3) sharing learned policies. This paper shows that (a) additional sensation from another agent is beneficial if it can be used efficiently, (b) sharing learned policies or episodes among agents speeds up learning at the cost of communication, and (c) for joint tasks, agents engaging in partnership can significantly outperform independent agents although they may learn slowly in the beginning. These tradeoffs are not just limited to multi-agent reinforcement learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bellman, R. E. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference-contexts: Unlike DAI, this work does not deal with issues such as communication language, agent beliefs, resource constraint, and negotiation. It also mainly focus on homogeneous agents. 3 REINFORCEMENT LEARNING Reinforcement learning is an on-line technique that approximates the conventional optimal control technique known as dynamic programming <ref> (Bellman 1957) </ref>. The external world is modeled as a discrete-time, finite state, Markov decision process. Each action is associated with a reward. The task of reinforcement learning is to maximize the long-term discounted reward per action. In this study, each reinforcement-learning agent uses the one-step Q-learning algorithm (Watkins 1989).
Reference: <author> Chan, P. K. & Stolfo, J. S. </author> <year> (1993). </year> <title> Toward parallel and distributed learning by meta-learning, </title> <note> Proceedings of AAAI Workshop on Knowledge Discovery in Databases, To appear. </note>
Reference: <author> Durfee, E. H. </author> <year> (1988). </year> <title> Coordination of Distributed Problem Solvers, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference-contexts: In the terms of DAI, my case studies 1 and 2 explore reinforcement learning in collaborative reasoning systems (Pope et. al 1992) which are concerned with coordinating intelligent behavior across multiple self-sufficient agents, and my case study 3 studies reinforcement learning in distributed problem-solving systems <ref> (Durfee 1988, Tan & Weihmayer 1992) </ref> in which a particular problem is divided among agents that cooperate and interact to develop a solution. Unlike DAI, this work does not deal with issues such as communication language, agent beliefs, resource constraint, and negotiation.
Reference: <author> Gasser, L. & Huhns, M. </author> <year> (1989). </year> <journal> Distributed Artificial Intelligence, </journal> <volume> 2, </volume> <editor> (eds.) </editor> <publisher> Pitman, London. </publisher>
Reference-contexts: His main theorem is that n reinforcement-learning agents who can observe everything about each other can decrease the required learning time at a rate that is (1=n). Recent work in the field of Distributed Artificial Intelligence (DAI) <ref> (Gasser & Huhns 1989) </ref> has addressed the issues of organization, coordination, and cooperation among agents, but not for multi-agent learning.
Reference: <author> Lin, L. J. </author> <year> (1991). </year> <title> Programming robots using reinforcement learning and teaching. </title> <booktitle> In Proceedings of AAAI-91. </booktitle> <pages> (pp. 781-786). </pages>
Reference-contexts: Examples are common in non-human societies as well. For example, ants are known to communicate about the locations of food, and to move objects collectively. In this paper, I use reinforcement learning to study intelligent agents <ref> (Mahadevan & Connel 1991, Lin 1991, Tan 1991) </ref>. Each reinforcement-learning agent can incrementally learn an efficient decision policy over a state space by trial-and-error, where the only input from an environment is a delayed scalar reward. The task of each agent is to maximize the long-term discounted reward per action. <p> Note that an expert hunter could be just another hunter who has already learned hunting skills. This result demonstrates another benefit of learning in a cooperative society where novices can learn quickly from experts by examples <ref> (Lin 1991, Whitehead 1991) </ref>. case study. Generally speaking, during the early phase of training, cooperative learning outperforms independent learning, and learning from an expert outperforms both. Their differences in performance are statistically significant according to t-tests.
Reference: <author> Mahadevan, S. & Connel, J. </author> <year> (1991). </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <booktitle> In Proceedings of AAAI-91. </booktitle> <pages> (pp. 768-773). </pages>
Reference-contexts: Examples are common in non-human societies as well. For example, ants are known to communicate about the locations of food, and to move objects collectively. In this paper, I use reinforcement learning to study intelligent agents <ref> (Mahadevan & Connel 1991, Lin 1991, Tan 1991) </ref>. Each reinforcement-learning agent can incrementally learn an efficient decision policy over a state space by trial-and-error, where the only input from an environment is a delayed scalar reward. The task of each agent is to maximize the long-term discounted reward per action.
Reference: <author> Pope, R., Conry, S., & Meyer, R. </author> <year> (1992). </year> <title> Distributing the planning process in a dynamic environment. </title> <booktitle> Proceedings of the 11th International Workshop on Distributed AI, </booktitle> <address> Glen Arbor, MI. </address>
Reference-contexts: Recent work in the field of Distributed Artificial Intelligence (DAI) (Gasser & Huhns 1989) has addressed the issues of organization, coordination, and cooperation among agents, but not for multi-agent learning. In the terms of DAI, my case studies 1 and 2 explore reinforcement learning in collaborative reasoning systems <ref> (Pope et. al 1992) </ref> which are concerned with coordinating intelligent behavior across multiple self-sufficient agents, and my case study 3 studies reinforcement learning in distributed problem-solving systems (Durfee 1988, Tan & Weihmayer 1992) in which a particular problem is divided among agents that cooperate and interact to develop a solution.
Reference: <author> Shaw, M. J. & Sikora, R. </author> <year> (1990). </year> <title> A distributed problem-solving approach to rule induction: learning in distributed artificial intelligence systems. </title> <type> Technical Report, </type> <institution> CMU-RI-TR-90-28, The Robotics Institute, Carnegie Mellon University. </institution>
Reference-contexts: GTE's ILS system (Silver et. al 1990) integrates heterogeneous (inductive, search-based, and knowledge-based) learning agents by a central controller through which the agents critique each other's proposals. The MALE system (Sian 1991) uses an interaction board (similar to a blackboard) to coordinate different learning agents. DLS <ref> (Shaw & Sikora 1990) </ref> adopts a distributed problem-solving approach to rule induction by dividing data among inductive learning agents. Recently, Chan and Stolfo (1993) advocate meta-learning for distributed learning. Most of these systems deal with inductive learning from examples, rather than autonomous learning agents that involve perception and action.
Reference: <author> Sian, S. S. </author> <year> (1991). </year> <title> Extending learning to multiple agents: issues and a model for multi-agent machine learning. </title> <editor> In Y. Kodratoff (Ed.), </editor> <booktitle> Machine Learning - EWSL 91. </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pp. 440-456. </pages>
Reference-contexts: GTE's ILS system (Silver et. al 1990) integrates heterogeneous (inductive, search-based, and knowledge-based) learning agents by a central controller through which the agents critique each other's proposals. The MALE system <ref> (Sian 1991) </ref> uses an interaction board (similar to a blackboard) to coordinate different learning agents. DLS (Shaw & Sikora 1990) adopts a distributed problem-solving approach to rule induction by dividing data among inductive learning agents. Recently, Chan and Stolfo (1993) advocate meta-learning for distributed learning.
Reference: <author> Silver, B., Frawely, W., Iba, G., Vittal, J., & Bradford, K. </author> <year> (1990). </year> <title> A framework for multi-paradigmatic learning. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> 348-358. </pages> <address> Austin, Texas. </address>
Reference-contexts: Ideally, intelligent agents would learn when to cooperate and which cooperative method to use to achieve maximum gain. This paper is a starting point for the examination of these fundamental open questions. 2 RELATED WORK Several multi-agent learning systems have been developed for speed and/or accuracy. GTE's ILS system <ref> (Silver et. al 1990) </ref> integrates heterogeneous (inductive, search-based, and knowledge-based) learning agents by a central controller through which the agents critique each other's proposals. The MALE system (Sian 1991) uses an interaction board (similar to a blackboard) to coordinate different learning agents.
Reference: <author> Tan, M. </author> <year> (1991). </year> <title> Cost-sensitive reinforcement learning for adaptive classification and control. </title> <booktitle> In Proceedings of AAAI-91. </booktitle> <pages> (pp. 774-780). </pages>
Reference-contexts: Examples are common in non-human societies as well. For example, ants are known to communicate about the locations of food, and to move objects collectively. In this paper, I use reinforcement learning to study intelligent agents <ref> (Mahadevan & Connel 1991, Lin 1991, Tan 1991) </ref>. Each reinforcement-learning agent can incrementally learn an efficient decision policy over a state space by trial-and-error, where the only input from an environment is a delayed scalar reward. The task of each agent is to maximize the long-term discounted reward per action.
Reference: <author> Tan, M. & Weihmayer, R. </author> <year> (1992). </year> <title> Integrating agent-oriented programming and planning for cooperative problem solving. </title> <booktitle> Proceedings of the AAAI-92's Workshop on Cooperation among Heterogeneous Intelligent Agents, </booktitle> <address> San Jose, CA, </address> <note> Watkins, </note> <author> C. J. C. H. </author> <year> (1989). </year> <title> Learning With Delayed Rewards. </title> <type> Ph.D. thesis, </type> <institution> Cambridge University Psychology Department. </institution>
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <title> (1992) Technical Note: </title> <journal> Q-Learning. Machine Learning, </journal> <volume> 8(3/4), </volume> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Whitehead, S. D. </author> <year> (1991). </year> <title> A complexity analysis of cooperative mechanisms in reinforcement learning. </title> <booktitle> In Proceedings of AAAI-91. </booktitle> <pages> (pp. 607-613) </pages>
Reference-contexts: Note that an expert hunter could be just another hunter who has already learned hunting skills. This result demonstrates another benefit of learning in a cooperative society where novices can learn quickly from experts by examples <ref> (Lin 1991, Whitehead 1991) </ref>. case study. Generally speaking, during the early phase of training, cooperative learning outperforms independent learning, and learning from an expert outperforms both. Their differences in performance are statistically significant according to t-tests.
References-found: 14

