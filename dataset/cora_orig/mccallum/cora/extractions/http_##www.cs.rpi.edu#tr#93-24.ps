URL: http://www.cs.rpi.edu/tr/93-24.ps
Refering-URL: http://www.cs.rpi.edu/tr/
Root-URL: 
Email: stewart@cs.rpi.edu  
Title: A New Robust Operator for Computer Vision: Application to Range and Intensity Images  
Author: Charles V. Stewart 
Date: February 7, 1994  
Address: Troy, New York 12180-3590  
Affiliation: Department of Computer Science Rensselaer Polytechnic Institute  
Abstract: MINPRAN accurately reconstructs range data taken from scenes that produce: (1) large numbers of outliers, (2) points from multiple surfaces interspersed over large image regions, and (3) extended regions containing only bad data. MINPRAN uses random sampling to search for fits that minimize a criterion function which models the probability that a fit could be due to random bad points. In a previous paper [24], we showed that MINPRAN finds accurate fits, even when more than half of the data are bad; and it avoids hallucinating fits when all of the data are bad. However, we also showed that MINPRAN often favors a single fit bridging multiple surfaces when the data in an image region arise from one or more surfaces. In this paper, we extend MINPRAN in two ways to handle the problem of multiple fits in a region: (1) it now searches for and compares two disjoint fits to the single best fit in a region, and (2) it reconstructs the best values of each data point, re-applies the criterion function to each fit using only the inliers that remain consistent with that fit, and eliminates fits that appear random. The new version of the algorithm, called MINPRAN2, produces extremely good results on difficult range images and intensity images that have been corrupted with a large number of random bad values. fl The author would like to acknowledge the financial support of the National Science Foundation under grant IRI-9217195, and the valuable assistance of Robin Flatland and James Miller for their detailed comments on earlier drafts of this paper and for their work in preparing the results for presentation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Abramowitz and I. A. Stegum. </author> <title> Handbook of Mathematical Functions. </title> <publisher> Dover Publications, </publisher> <year> 1968. </year>
Reference-contexts: Given N points, MINPRAN searches for the fit and the values of r and k that minimize F . F (r; k; N) is based on the well known binomial distribution, which is equivalent to the incomplete beta function (see <ref> [1] </ref>): F (r; k; N) = i=k N ! N! Z r=Z 0 t k1 (1 t) Nk dt (2) Several important properties of F follow from this equation and are proved in [24] 1. <p> It fits parameterized function models of the form f (~x; ~a) = 0 to the data, where ~x is the coordinate vector and ~a is the parameter vector. (For example, for planes of the form a 0 + a 1 x + a 2 y z = 0, ~x = <ref> [1; x; y; z] </ref> T and ~a = [a 0 ; a 1 ; a 2 ; 1] T . <p> f (~x; ~a) = 0 to the data, where ~x is the coordinate vector and ~a is the parameter vector. (For example, for planes of the form a 0 + a 1 x + a 2 y z = 0, ~x = [1; x; y; z] T and ~a = <ref> [a 0 ; a 1 ; a 2 ; 1] </ref> T .
Reference: [2] <author> P. J. Besl, J. B. Birch, and L. T. Watson. </author> <title> Robust window operators. </title> <booktitle> In Proceedings of the IEEE International Conference on Computer Vision, </booktitle> <pages> pages 591-600, </pages> <year> 1988. </year>
Reference-contexts: Subsequently, if j = j j j, then w j = &lt; 1 if j - j otherwise This produces a larger estimate of -2 than either a Minimax scheme [9] or redescending M-estimators (see discussion in <ref> [2] </ref>), making the overall technique more conservative in rejecting fits. 5 Experimental Results The final version of MINPRAN, called MINPRAN2, includes the split-search technique, the final randomness test, and the technique to correct for underestimates in .
Reference: [3] <author> P. J. Besl and R. C. Jain. </author> <title> Segmentation through variable-order surface fitting. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10 </volume> <pages> 167-192, </pages> <year> 1988. </year>
Reference-contexts: Within the large reconstruction literature there is growing interest in techniques that handle overlapping and transparent surfaces. Broadly speaking, these techniques either: (1) grow surfaces from seed patches <ref> [3, 5, 18] </ref> or (2) solve a global optimization model that includes clustering points on different, overlapping surfaces as part of the model [14]. MINPRAN2 offers two potential advantages if it is used as part of a surface growing technique.
Reference: [4] <author> R. C. Bolles and M. A. Fischler. </author> <title> A Ransac-based approach to model fitting and its application to finding cylinders in range data. </title> <booktitle> In Proceedings Seventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 637-643, </pages> <year> 1981. </year>
Reference-contexts: Combining these gives the result. 2 4 2.1 The MINPRAN Algorithm Given N points, MINPRAN uses random sampling to search for the fit that minimizes F . Like other random sampling techniques in both computer vision and robust statistics <ref> [4, 6, 7, 11, 12, 16, 20, 21, 23] </ref>, MINPRAN repeatedly selects small subsets of p data points, generates a fit to each subset, and computes the (absolute) residuals of the remaining N p points relative to each fit.
Reference: [5] <author> T. Boult and M. Lerner. </author> <title> Energy-based segmentation of very sparse range surfaces. </title> <booktitle> In Proceedings IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 232-237, </pages> <year> 1990. </year>
Reference-contexts: Within the large reconstruction literature there is growing interest in techniques that handle overlapping and transparent surfaces. Broadly speaking, these techniques either: (1) grow surfaces from seed patches <ref> [3, 5, 18] </ref> or (2) solve a global optimization model that includes clustering points on different, overlapping surfaces as part of the model [14]. MINPRAN2 offers two potential advantages if it is used as part of a surface growing technique.
Reference: [6] <author> M. A. Fischler and R. C. Bolles. </author> <title> Random Sample Consensus: A paradigm for model fitting with applications to image analysis and automated cartography. </title> <journal> Communications of the ACM, </journal> <volume> 24 </volume> <pages> 381-395, </pages> <year> 1981. </year>
Reference-contexts: In addition to assuming a known bound on the distance between a point and its correct surface (or curve), Hough transforms and Ransac <ref> [6, 10, 20] </ref> implicitly assume the outliers are drawn from a uniform distribution in order to distinguish good fits from random fits [6, 8, 20]. In a recent paper [24], we introduced a new robust operator that tolerates more than 50% outliers only by assuming the outliers are uniformly distributed. <p> In addition to assuming a known bound on the distance between a point and its correct surface (or curve), Hough transforms and Ransac [6, 10, 20] implicitly assume the outliers are drawn from a uniform distribution in order to distinguish good fits from random fits <ref> [6, 8, 20] </ref>. In a recent paper [24], we introduced a new robust operator that tolerates more than 50% outliers only by assuming the outliers are uniformly distributed. <p> Combining these gives the result. 2 4 2.1 The MINPRAN Algorithm Given N points, MINPRAN uses random sampling to search for the fit that minimizes F . Like other random sampling techniques in both computer vision and robust statistics <ref> [4, 6, 7, 11, 12, 16, 20, 21, 23] </ref>, MINPRAN repeatedly selects small subsets of p data points, generates a fit to each subset, and computes the (absolute) residuals of the remaining N p points relative to each fit.
Reference: [7] <author> M. A. Fischler and O. Firschein. </author> <title> Parallel guessing: A strategy for high-speed computation. </title> <journal> Pattern Recognition, </journal> <volume> 20 </volume> <pages> 257-263, </pages> <year> 1987. </year>
Reference-contexts: Combining these gives the result. 2 4 2.1 The MINPRAN Algorithm Given N points, MINPRAN uses random sampling to search for the fit that minimizes F . Like other random sampling techniques in both computer vision and robust statistics <ref> [4, 6, 7, 11, 12, 16, 20, 21, 23] </ref>, MINPRAN repeatedly selects small subsets of p data points, generates a fit to each subset, and computes the (absolute) residuals of the remaining N p points relative to each fit.
Reference: [8] <author> W. E. L. Grimson and D. Huttenlocher. </author> <title> On the sensitivity of the Hough transform for object recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 255-274, </pages> <year> 1990. </year>
Reference-contexts: In addition to assuming a known bound on the distance between a point and its correct surface (or curve), Hough transforms and Ransac [6, 10, 20] implicitly assume the outliers are drawn from a uniform distribution in order to distinguish good fits from random fits <ref> [6, 8, 20] </ref>. In a recent paper [24], we introduced a new robust operator that tolerates more than 50% outliers only by assuming the outliers are uniformly distributed.
Reference: [9] <author> P. J. Huber. </author> <title> Robust Statistics. </title> <publisher> John Wiley & Sons, </publisher> <year> 1980. </year>
Reference-contexts: Subsequently, if j = j j j, then w j = &lt; 1 if j - j otherwise This produces a larger estimate of -2 than either a Minimax scheme <ref> [9] </ref> or redescending M-estimators (see discussion in [2]), making the overall technique more conservative in rejecting fits. 5 Experimental Results The final version of MINPRAN, called MINPRAN2, includes the split-search technique, the final randomness test, and the technique to correct for underestimates in .
Reference: [10] <author> J. Illingworth and J. Kittler. </author> <title> A survey of the Hough transform. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 44 </volume> <pages> 87-116, </pages> <year> 1988. </year> <month> 24 </month>
Reference-contexts: In addition to assuming a known bound on the distance between a point and its correct surface (or curve), Hough transforms and Ransac <ref> [6, 10, 20] </ref> implicitly assume the outliers are drawn from a uniform distribution in order to distinguish good fits from random fits [6, 8, 20]. In a recent paper [24], we introduced a new robust operator that tolerates more than 50% outliers only by assuming the outliers are uniformly distributed.
Reference: [11] <author> J.-M. Jolion, P. Meer, and S. Bataouche. </author> <title> Robust clustering with applications in com-puter vision. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13 </volume> <pages> 791-802, </pages> <year> 1991. </year>
Reference-contexts: Combining these gives the result. 2 4 2.1 The MINPRAN Algorithm Given N points, MINPRAN uses random sampling to search for the fit that minimizes F . Like other random sampling techniques in both computer vision and robust statistics <ref> [4, 6, 7, 11, 12, 16, 20, 21, 23] </ref>, MINPRAN repeatedly selects small subsets of p data points, generates a fit to each subset, and computes the (absolute) residuals of the remaining N p points relative to each fit.
Reference: [12] <author> R. Kumar and A. R. Hanson. </author> <title> Analysis of different robust methods for pose refinement. </title> <booktitle> In Proceedings of the International Workshop on Robust Computer Vision, </booktitle> <pages> pages 167-182, </pages> <year> 1990. </year>
Reference-contexts: Combining these gives the result. 2 4 2.1 The MINPRAN Algorithm Given N points, MINPRAN uses random sampling to search for the fit that minimizes F . Like other random sampling techniques in both computer vision and robust statistics <ref> [4, 6, 7, 11, 12, 16, 20, 21, 23] </ref>, MINPRAN repeatedly selects small subsets of p data points, generates a fit to each subset, and computes the (absolute) residuals of the remaining N p points relative to each fit.
Reference: [13] <author> A. Leon-Garcia. </author> <title> Probability and Random Processes for Electrical Engineering. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: As such, it has mean N P and variance 2 (N P ) <ref> [13] </ref>.
Reference: [14] <author> S. Madarasmi, D. Kersten, and T. C. Pong. </author> <title> Multi-layer surface segmentation using energy minimization. </title> <booktitle> In Proceedings IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 774-775, </pages> <year> 1993. </year>
Reference-contexts: Broadly speaking, these techniques either: (1) grow surfaces from seed patches [3, 5, 18] or (2) solve a global optimization model that includes clustering points on different, overlapping surfaces as part of the model <ref> [14] </ref>. MINPRAN2 offers two potential advantages if it is used as part of a surface growing technique. First, it can identify seed patches for data where less robust techniques may fail.
Reference: [15] <author> P. Meer, D. Mintz, and A. Rosenfeld. </author> <title> Least median of squares based robust analysis of image structure. </title> <booktitle> In Proceedings of the DARPA Image Understanding Workshop, </booktitle> <pages> pages 231-254, </pages> <year> 1990. </year>
Reference-contexts: In one way or another this error is common to most fitting techniques, including least squares and least median of squares (see <ref> [15, 17] </ref> for detailed discussion).
Reference: [16] <author> P. Meer, D. Mintz, A. Rosenfeld, and D. Y. Kim. </author> <title> Robust regression methods for computer vision: A review. </title> <journal> International Journal of Computer Vision, </journal> <volume> 6 </volume> <pages> 59-70, </pages> <year> 1991. </year>
Reference-contexts: Combining these gives the result. 2 4 2.1 The MINPRAN Algorithm Given N points, MINPRAN uses random sampling to search for the fit that minimizes F . Like other random sampling techniques in both computer vision and robust statistics <ref> [4, 6, 7, 11, 12, 16, 20, 21, 23] </ref>, MINPRAN repeatedly selects small subsets of p data points, generates a fit to each subset, and computes the (absolute) residuals of the remaining N p points relative to each fit.
Reference: [17] <author> D. Mintz. </author> <title> Robustness by consensus. </title> <type> Technical Report CAR-TR-576, </type> <institution> University of Maryland | Center for Automation Research, </institution> <year> 1991. </year>
Reference-contexts: In one way or another this error is common to most fitting techniques, including least squares and least median of squares (see <ref> [15, 17] </ref> for detailed discussion).
Reference: [18] <author> M. J. Mirza and K. L. Boyer. </author> <title> The Robust Sequential Estimator: A computationally efficient algorithm for estimating surface curvature. </title> <type> Technical Report SAMPL-91-02, </type> <institution> The Ohio State University, </institution> <year> 1991. </year>
Reference-contexts: Within the large reconstruction literature there is growing interest in techniques that handle overlapping and transparent surfaces. Broadly speaking, these techniques either: (1) grow surfaces from seed patches <ref> [3, 5, 18] </ref> or (2) solve a global optimization model that includes clustering points on different, overlapping surfaces as part of the model [14]. MINPRAN2 offers two potential advantages if it is used as part of a surface growing technique.
Reference: [19] <author> D. Nitzan. </author> <title> Three-dimensional vision structure for robot applications. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10 </volume> <pages> 291-309, </pages> <year> 1988. </year>
Reference-contexts: 1 Introduction Range images of complex scenes place strong demands on the robustness of reconstruction techniques. First, when there are many depth discontinuities, range sensors can produce a large number of outliers near these discontinuities <ref> [19] </ref>. These "true outliers" arise in part because both the foreground and background project to the same sensor pixel, and in part because along discontinuities the normal to the foreground surface can be nearly orthogonal to the line of sight. These points can arbitrarily corrupt the reconstructed surfaces.
Reference: [20] <author> G. Roth and M. D. Levine. </author> <title> Extracting geometric primitives. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 58 </volume> <pages> 1-22, </pages> <year> 1993. </year>
Reference-contexts: In addition to assuming a known bound on the distance between a point and its correct surface (or curve), Hough transforms and Ransac <ref> [6, 10, 20] </ref> implicitly assume the outliers are drawn from a uniform distribution in order to distinguish good fits from random fits [6, 8, 20]. In a recent paper [24], we introduced a new robust operator that tolerates more than 50% outliers only by assuming the outliers are uniformly distributed. <p> In addition to assuming a known bound on the distance between a point and its correct surface (or curve), Hough transforms and Ransac [6, 10, 20] implicitly assume the outliers are drawn from a uniform distribution in order to distinguish good fits from random fits <ref> [6, 8, 20] </ref>. In a recent paper [24], we introduced a new robust operator that tolerates more than 50% outliers only by assuming the outliers are uniformly distributed. <p> Combining these gives the result. 2 4 2.1 The MINPRAN Algorithm Given N points, MINPRAN uses random sampling to search for the fit that minimizes F . Like other random sampling techniques in both computer vision and robust statistics <ref> [4, 6, 7, 11, 12, 16, 20, 21, 23] </ref>, MINPRAN repeatedly selects small subsets of p data points, generates a fit to each subset, and computes the (absolute) residuals of the remaining N p points relative to each fit.
Reference: [21] <author> P. J. Rousseeuw and A. M. Leroy. </author> <title> Robust Regression and Outlier Detection. </title> <publisher> John Wiley & Sons, </publisher> <year> 1987. </year>
Reference-contexts: From a theoretical viewpoint, these problems require a reconstruction technique to fit surfaces when less than 50% of the data lie on one surface. Since 50% outliers is the highest percentage that can be tolerated <ref> [21] </ref> without any a priori knowledge, we must introduce some assumptions about the distribution of outliers. <p> Combining these gives the result. 2 4 2.1 The MINPRAN Algorithm Given N points, MINPRAN uses random sampling to search for the fit that minimizes F . Like other random sampling techniques in both computer vision and robust statistics <ref> [4, 6, 7, 11, 12, 16, 20, 21, 23] </ref>, MINPRAN repeatedly selects small subsets of p data points, generates a fit to each subset, and computes the (absolute) residuals of the remaining N p points relative to each fit.
Reference: [22] <author> K. Sato and S. </author> <title> Inokuchi. Range-imaging system utilizing nematic liquid crystal mask. </title> <booktitle> In Proceedings of the IEEE International Conference on Computer Vision, </booktitle> <pages> pages 657-661, </pages> <year> 1987. </year>
Reference-contexts: The plot shows the results for the original MINPRAN algorithm ("Basic") and for the split-search ("S-S") technique. of the surface patches change smoothly to approximate the curvature of the spheres. 5.2 Structured Light Range Data Our range data of real scenes are taken from a structured light range finder <ref> [22] </ref>. The sensor is inconsistent with MINPRAN's randomness model in two ways. First, MINPRAN2 assumes the noise in the data is continuous, but since there are a finite number of light patterns, the possible depth values are discrete.
Reference: [23] <author> S. S. Sinha and B. G. Schunck. </author> <title> A two-stage algorithm for discontinuity-preserving surface reconstruction. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 36-55, </pages> <year> 1992. </year>
Reference-contexts: Combining these gives the result. 2 4 2.1 The MINPRAN Algorithm Given N points, MINPRAN uses random sampling to search for the fit that minimizes F . Like other random sampling techniques in both computer vision and robust statistics <ref> [4, 6, 7, 11, 12, 16, 20, 21, 23] </ref>, MINPRAN repeatedly selects small subsets of p data points, generates a fit to each subset, and computes the (absolute) residuals of the remaining N p points relative to each fit.
Reference: [24] <author> C. V. Stewart. </author> <title> A new robust operator for computer vision: Theoretical analysis. </title> <type> Technical Report 93-21, </type> <institution> Department of Computer Science, Rensselaer Polytechnic Institute, </institution> <month> August, </month> <year> 1993. </year> <month> 25 </month>
Reference-contexts: In a recent paper <ref> [24] </ref>, we introduced a new robust operator that tolerates more than 50% outliers only by assuming the outliers are uniformly distributed. The bounds on this distribution are determined by the dynamic range of a "sensor" (either a physical sensor or the output of a lower-level computation). <p> For more details on the analysis, see <ref> [24] </ref>. MINPRAN searches for fits that minimize a criterion function, which is based on the assumption that the bad data are uniformly distributed within the dynamic range of the sensor 1 . Consider a fit (x) and a band of r around as shown in Figure 1. <p> is based on the well known binomial distribution, which is equivalent to the incomplete beta function (see [1]): F (r; k; N) = i=k N ! N! Z r=Z 0 t k1 (1 t) Nk dt (2) Several important properties of F follow from this equation and are proved in <ref> [24] </ref> 1. Given k inliers, F (r; k; N ) increases monotonically with the inlier bound r. 1 This assumption is discussed further in Sections 5 and 6. 3 that at least k points are within r of (x) is given by a binomial sum. 2. <p> The MINPRAN algorithm is based on the following theorem, which depends on these properties. (We repeat the proof from <ref> [24] </ref> because it is crucial to understanding the algorithm.) Theorem 1 Given N data points, the minimum of F over S fits to these points can be found in time O (SN log N + N 2 ). <p> See <ref> [24] </ref> for discussion.) Therefore, the probability that at least one sample is good in S samples 5 gather the N points in the image region into a list; calculate S, the initial number of samples required; calculate F 0 , the cut-off between good and random fits ; N 0 = <p> On-line, for particular values of N and P 0 , this table is searched to find the appropriate value of F 0 . See <ref> [24] </ref> for details. 2.2 Analysis We have analyzed the criterion function to predict MINPRAN's performance [24]. In particular, we showed the following results, which have been confirmed experimentally: * The expected minimum of the criterion function for the correct fit typically occurs within 2% of the correct number of inliers. <p> On-line, for particular values of N and P 0 , this table is searched to find the appropriate value of F 0 . See <ref> [24] </ref> for details. 2.2 Analysis We have analyzed the criterion function to predict MINPRAN's performance [24]. In particular, we showed the following results, which have been confirmed experimentally: * The expected minimum of the criterion function for the correct fit typically occurs within 2% of the correct number of inliers. <p> However, since MINPRAN's underestimate is typically no more than 6% of the standard deviation <ref> [24] </ref>, we can recompute the inlier bound based on the estimated standard deviation, find the inliers based on this bound, and recompute the fit parameters. <p> Experimentally, we have found that on average, the final estimate of the standard deviation is within 2:5% of the correct value, with no significant bias. As discussed in <ref> [24] </ref>, MINPRAN only accepts random fits when the dynamic range of the sensor is overestimated by 10% or more. In this case, the accepted random fits tend to include nearly all random points as inliers, so their variance estimates are extremely large. <p> It outputs reconstructed range data and local planar surface patches that could be used to build extended, higher-order surface models. MINPRAN2's ability to tolerate outliers and avoid hallucinating surfaces depends on the basic MINPRAN algorithm introduced in <ref> [24] </ref>. Its ability to reconstruct overlapping surfaces within the same region requires the split-search technique and the final randomness test introduced here. We have demonstrated the effectiveness of these techniques on range images, and have showed that MINPRAN2 effectively reconstructs intensity images corrupted by high outlier percentages. <p> We have demonstrated the effectiveness of these techniques on range images, and have showed that MINPRAN2 effectively reconstructs intensity images corrupted by high outlier percentages. Also, although MINPRAN2 fits planar patches, these patches give good approximations of curved surfaces (see Figure 10). Comparing MINPRAN2 to other techniques, in <ref> [24] </ref> we place the basic MINPRAN model in the context of other robust methods, while here we consider MINPRAN2 in relation to surface reconstruction techniques. Within the large reconstruction literature there is growing interest in techniques that handle overlapping and transparent surfaces.

References-found: 24

