URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/139.ps.gz
Refering-URL: http://www.cs.wisc.edu/~fischer/course/html/submissions/
Root-URL: http://www.cs.wisc.edu
Email: knoop@fmi.uni-passau.de  
Title: Partial Dead Code Elimination for Parallel Programs  
Author: Jens Knoop 
Keyword: Parallelism, interleaving semantics, shared memory, synchronization, compiler, program optimization, data flow analysis, bitvector problems, code motion, partial redundancy elimination, assignment motion, dead code elimination.  
Address: Passau  
Affiliation: Universitat  
Abstract: The elimination of partially dead code (PDE) has proved to be a powerful run-time optimization technique for sequential programs. In this paper we show how this technique can be adapted to parallel programs on the basis of a recently presented framework for efficient and precise bitvec-tor analyses for parallel programs. Whereas this framework allows a straightforward adaptation of the required data flow analyses to the parallel case, the transformation part of the optimization requires some special care in order to preserve parallelism. This preservation is an absolute must in order to guarantee that the optimization does never impair efficiency. The introduction of a corresponding natural side condition suffices to lift even the optimality result known from the sequential setting to the parallel setting. 
Abstract-found: 1
Intro-found: 1
Reference: [CC] <author> Cousot, P., and Cousot, R. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Conf. Record of the 4 th Internat. Symposium on Principles of Programming Languages (POPL'77), </booktitle> <address> Los Angeles, CA, </address> <year> 1977, </year> <pages> 238 - 252. </pages>
Reference-contexts: Theoretically well-founded are DFAs that are based on abstract interpretation (cf. <ref> [CC, Ma] </ref>). The point of this approach is to replace the "full" semantics by a simpler more abstract version, which is tailored to deal with a specific problem.
Reference: [CH] <author> Chow, J.-H., and Harrison, W. L. </author> <title> State Space Reduction in Abstract Interpretation of Parallel Programs. </title> <booktitle> In Proceedings of the International Conference on Computer Languages, </booktitle> <address> (ICCL'94), Toulouse, France, </address> <month> May 16-19, </month> <year> 1994, </year> <pages> 277-288. Page 11 </pages>
Reference-contexts: This setup already introduces the phenomena of interference and synchronization, and allows us to concentrate on the central features of our approach, which, however, is not limited to this setting. For example, a replicator statement allowing a dynamical process creation can be integrated along the lines of <ref> [CH, Vo] </ref>. 2.1 Parallel Flow Graphs Similarly to [SHW] and [GS], we represent a parallel program by a nondeterministic parallel flow graph G fl = (N fl ; E fl ; s fl ; e fl ) with node set N fl and edge set E fl as illustrated in Figure
Reference: [DC] <author> Dwyer, M. B., and Clarke, L. A. </author> <title> Data flow analysis for verifying properties of concurrent programs. </title> <booktitle> In Proceedings of the 2 nd ACM SIGSOFT'94 Symposium on Foundations of Software Engineering (SIG-SOFT'94), </booktitle> <address> New Orleans, </address> <booktitle> Software Engineering Notes 19 , 5 (1994), </booktitle> <volume> 62 - 75. </volume>
Reference-contexts: Thus, the current situation is characterized by approaches aiming at frameworks for the static analysis of parallel programs (cf. [GS, KY, SS, SW, WS]) and by approaches concerned with analysis problems that are specific to parallel programs like dead lock or mutual exclusion (cf. <ref> [DC] </ref>). 2 The Parallel Setting We consider a parallel imperative programming language with interleaving semantics. Parallelism is syntactically expressed by means of a par statement whose components are assumed to be executed in parallel on a shared memory.
Reference: [FKCX] <author> Feigen, L., Klappholz, D., Casazza, R., and Xue, X. </author> <title> The revival transformation. </title> <booktitle> In Conference Record of the 21 st ACM SIGPLAN Symposium on Principles of Programming Languages (POPL'94), Port-land, Oregon, 1994, </booktitle> <volume> 421 - 434. </volume>
Reference-contexts: Section 4, finally, contains our conclusions, and the Appendix recalls the generic algorithm for computing the PMFP -solution. Related Work: There are two major algorithms addressing the problem of PDE in the sequential setting. First, the algorithm of Feigen et al. <ref> [FKCX] </ref>, and second the algorithm of Knoop et al. [KRS3] underlying the approach in this paper. Both algorithms are incomparable as the algorithm of [FKCX] considers complex program statements as motion candidates, but it is not capable of moving statements out of or across loops. <p> Related Work: There are two major algorithms addressing the problem of PDE in the sequential setting. First, the algorithm of Feigen et al. <ref> [FKCX] </ref>, and second the algorithm of Knoop et al. [KRS3] underlying the approach in this paper. Both algorithms are incomparable as the algorithm of [FKCX] considers complex program statements as motion candidates, but it is not capable of moving statements out of or across loops. For parallel programs, the problem of PDE has Page 3 not yet been addressed to the knowledge of the author.
Reference: [GS] <author> Grunwald, D., and Srinivasan, H. </author> <title> Data flow equations for explicitely parallel programs. </title> <booktitle> In Proc. of the ACM SIGPLAN Symposium on Principles of Parallel Programming (PPOPP'93), SIGPLAN Notices 28 , 7 (1993). </booktitle>
Reference-contexts: Thus, the current situation is characterized by approaches aiming at frameworks for the static analysis of parallel programs (cf. <ref> [GS, KY, SS, SW, WS] </ref>) and by approaches concerned with analysis problems that are specific to parallel programs like dead lock or mutual exclusion (cf. [DC]). 2 The Parallel Setting We consider a parallel imperative programming language with interleaving semantics. <p> For example, a replicator statement allowing a dynamical process creation can be integrated along the lines of [CH, Vo]. 2.1 Parallel Flow Graphs Similarly to [SHW] and <ref> [GS] </ref>, we represent a parallel program by a nondeterministic parallel flow graph G fl = (N fl ; E fl ; s fl ; e fl ) with node set N fl and edge set E fl as illustrated in Figure 3.
Reference: [He] <author> Hecht, M. S. </author> <title> Flow analysis of computer programs. </title> <publisher> Elsevier, North-Holland, </publisher> <year> 1977. </year>
Reference-contexts: As a consequence, all the well-known bitvector algorithms for liveness, availability, very business, or reaching definitions (cf. <ref> [He] </ref>) can easily be adapted for parallel programs at almost no cost on the runtime and the implementation side. This is important as assignment sinking and dead code elimination only requires bitvector analyses of this type. <p> Except for subgraphs representing par statements a parallel flow graph is a nondeterministic flow graph as for the representation of a sequential program (cf. <ref> [He] </ref>). <p> Note that G seq is free of nested parallel statements: all components of parallel statements are standard nondeterministic sequential flow graphs (cf. <ref> [He] </ref>). Interleaving Predecessors: For a sequential flow graph G, the set of nodes that might dynamically precede a node n is precisely given by the set of its static predecessors pred G (n). <p> m to a predecessor of n by PP G fl [m; n] and PP G fl [m; n [, respectively. 4 2.2 Data Flow Analysis For imperative programming languages, data flow analysis (DFA) provides information about the program states that may occur at some given program points during execution (cf. <ref> [He, MJ] </ref>). Theoretically well-founded are DFAs that are based on abstract interpretation (cf. [CC, Ma]). The point of this approach is to replace the "full" semantics by a simpler more abstract version, which is tailored to deal with a specific problem. <p> Despite their simplicity, unidirectional bitvec-tor problems are highly relevant in practice because of their broad scope of applications ranging from simple analyses like liveness, availability, very business, reaching definitions, and definition-use chains (cf. <ref> [He] </ref>) to more sophisticated and powerful program optimizations like code motion, partial dead code elimination, assignment motion, and strength reduction. Next we are going to show, how to optimize the effort for computing the PMOP -solution of bitvector problems. <p> Thus, eliminating dead assignments requires to compute for every program point the set of variables that are never used on a program continuation without a preceding redefinition. This is a classical bitvector problem (cf. <ref> [He] </ref>).
Reference: [KRS1] <author> Knoop, J., Ruthing, O., and Steffen, B. </author> <title> Lazy code motion. </title> <booktitle> In Proceedings of the ACM SIGPLAN'92 Conference on Programming Language Design and Implementation (PLDI'92), </booktitle> <address> San Francisco, CA, </address> <booktitle> SIGPLAN Notices 27 , 7 (1992), </booktitle> <volume> 224 - 234. </volume>
Reference-contexts: In the following we assume that all edges starting in a node outside N fl N with more than one successor have been split by inserting a synthetic node. Edge splitting is typical for code motion transformations (cf. <ref> [KRS1, KRS2] </ref>) in order to avoid the blocking of the code motion process by critical edges as illustrated in Appendix A. We are now going to develop our optimal PDE algorithm for parallel programs starting with a `naive' one.
Reference: [KRS2] <author> Knoop, J., Ruthing, O., and Steffen, B. </author> <title> Optimal code motion: </title> <journal> Theory and practice. Trans. on Programming Languages and Systems 16 , 4 (1994), </journal> <volume> 1117 - 1155. </volume>
Reference-contexts: In the following we assume that all edges starting in a node outside N fl N with more than one successor have been split by inserting a synthetic node. Edge splitting is typical for code motion transformations (cf. <ref> [KRS1, KRS2] </ref>) in order to avoid the blocking of the code motion process by critical edges as illustrated in Appendix A. We are now going to develop our optimal PDE algorithm for parallel programs starting with a `naive' one.
Reference: [KRS3] <author> Knoop, J., Ruthing, O., and Steffen, B. </author> <title> Partial dead code elimination. </title> <booktitle> In Proceedings of the ACM SIGPLAN'94 Conference on Programming Language Design and Implementation (PLDI'94), </booktitle> <address> Orlando, Florida, </address> <booktitle> SIGPLAN Notices 29 , 6 (1994), </booktitle> <volume> 147 - 158. </volume>
Reference-contexts: Obviously, placing assignments as-late-as-possible maximizes the potential of dead code, which can then be eliminated. Repeating this process until the program stabilizes captures all second order effects and guarantees the following optimality result <ref> [KRS3] </ref>: * Partially dead code remaining in the resulting program cannot be eliminated without changing the branching structure or the semantics of the program, or without impair ing some program executions. <p> This is important as assignment sinking and dead code elimination only requires bitvector analyses of this type. The power of the resulting algorithm, which works for irreducible control flow structures and has the same worst case time complexity as its sequential counterpart of <ref> [KRS3] </ref>, is illustrated in optimization result of Figure 3 (b). for Parallel Programs In Figure 3 (a) the assignment to y at node 1 is partially dead with respect to the assignment to y in node 7. <p> Related Work: There are two major algorithms addressing the problem of PDE in the sequential setting. First, the algorithm of Feigen et al. [FKCX], and second the algorithm of Knoop et al. <ref> [KRS3] </ref> underlying the approach in this paper. Both algorithms are incomparable as the algorithm of [FKCX] considers complex program statements as motion candidates, but it is not capable of moving statements out of or across loops. <p> Central is the admissibility constraint of assignment sinking. For the `naive' algorithm this is the straightforward extension of its sequential counterpart (cf. <ref> [KRS3] </ref>).
Reference: [KRS4] <author> Knoop, J., Ruthing, O., and Steffen, B. </author> <title> The power of assignment motion. </title> <booktitle> In Proc. of the ACM SIGPLAN'95 Conference on Programming Language Design and Implementation (PLDI'95), </booktitle> <address> La Jolla, CA, </address> <note> SIG-PLAN Notices 30 , 6 (1995), 233 - 245. </note>
Reference-contexts: The algorithm is currently implemented on the fixpoint-analysis machine of [SCKKM]. Moreover, we are investigating how to adapt the approach of this paper to the essentially dual technique of assig-ment motion <ref> [KRS4] </ref>, which, however, is more intricate as it uniformly combines the elimination of partially redundant assignments and expressions.
Reference: [KS] <author> Knoop, J., and Steffen, B. </author> <title> The interpro-cedural coincidence theorem. </title> <booktitle> In Proceedings of the 4 th International Conference on Compiler Construction (CC'92), </booktitle> <address> Pader-born, Germany, </address> <note> Springer-Verlag, LNCS 641 (1992), 125 - 140. </note>
Reference-contexts: The information that is necessary to model this effect can be computed by a hierarchical algorithm that only considers purely sequential programs. The central idea coincides with that of interprocedural analysis <ref> [KS] </ref>: we need to compute the effect of complete subgraphs, or in this case of complete parallel components. This information is computed in an `innermost' fashion and then propagated to the next surrounding parallel statement. The complete three-step procedure P is given below: 1.
Reference: [KSV1] <author> Knoop, J., Steffen, B., and Vollmer, J. </author> <title> Parallelism for free: Efficient and optimal bitvector analyses for parallel programs. </title> <journal> Accepted for ACM Transactions on Programming Languages and Systems. </journal>
Reference-contexts: Restricting assignment sinkings to sinkings respecting the Profitability Constraint (PC), we can lift the optimality result known from the sequential setting to the parallel one (cf. Theorem 3.3). Fundamental for the construction of the new algorithm is the framework of <ref> [KSV1, KSV2] </ref>, where it was recently shown how to construct for unidirectional bitvector problems analysis algorithms for parallel programs with shared memory and interleaving semantics that 1. precisely cover the phenomenon of interfer ence 2. are as efficient as their sequential counter parts and 3. easy to implement. <p> Thus, the execution of this assignment may be for free, if the execution of the right component of the parallel statement takes longer. Structure of the Paper: After considering the related work, we recall the framework of <ref> [KSV1] </ref> for unidirectional bitvector analyses of parallel programs in Section 2. <p> The MOP -approach (in the parallel setting the PMOP -approach) directly mimics possible program executions in that it "meets" (intersects) all informations belonging to a program path reaching the program point under consideration. 4 In <ref> [KSV1] </ref> an alternative characterization of parallel paths is given, which in spirit follows the definition of in-terprocedural program paths as proposed by Sharir and Pnueli [SP]. 5 In the following C will always denote a complete lattice. <p> Thus, 6 Note that the local semantic functional [[ ]] 0 of G 0 is known, whenever this step is executed. 7 See <ref> [KSV1] </ref> for details. Page 7 the effect of each complete path through a parallel statement is already given by some path through one of the parallel components (the one containing the vital statement). <p> Thus, in order to model the effect (or PMOP -solution) of a parallel statement, it is sufficient to combine the effects of all paths local to the components. By means of this fact, which is formalized in Lemma 2.3, we can prove (cf. <ref> [KSV1] </ref>): Theorem 2.4 (Hierarchical Coincidence Th.) Let G 2 G P (G fl ) be a parallel flow graph, and [[ ]] : N fl ! F B a local semantic functional. <p> Moreover, it coincides with the desired PMOP - solution (cf. <ref> [KSV1] </ref>). Theorem 2.6 (Parallel BV-Coincidence Th.) Let G fl = (N fl ; E fl ; s fl ; e fl ) be a parallel flow graph, and [[ ]] : N fl ! F B a local semantic functional. <p> Thus, we have lifted the optimality result of the sequential setting to the parallel one. 4 Conclusions We have presented an algorithm for eliminating partially dead code in a parallel program with shared memory and interleaving semantics. Based on the framework of <ref> [KSV1] </ref> the DFAs required could straightforward be adapted from their sequential counterparts. However, the transformation itself required special care as a straightforward adaption of the optimal sequential algorithm may destroy the explicit parallelism of the argument program, and thus drastically impair the execution time.
Reference: [KSV2] <author> Knoop, J., Steffen, B., and Vollmer, J. </author> <title> Parallelism for free: </title> <booktitle> Bitvector Analyses ) No state explosion! In Proc. of the International Workshop on Tools and Algorithms for the Construction and Analysis of Systems (TACAS'95), </booktitle> <address> Aarhus, DK, </address> <note> Springer-Verlag, LNCS 1019 (1995), 264 - 289. </note>
Reference-contexts: Restricting assignment sinkings to sinkings respecting the Profitability Constraint (PC), we can lift the optimality result known from the sequential setting to the parallel one (cf. Theorem 3.3). Fundamental for the construction of the new algorithm is the framework of <ref> [KSV1, KSV2] </ref>, where it was recently shown how to construct for unidirectional bitvector problems analysis algorithms for parallel programs with shared memory and interleaving semantics that 1. precisely cover the phenomenon of interfer ence 2. are as efficient as their sequential counter parts and 3. easy to implement.
Reference: [KU] <author> Kam, J. B., and Ullman, J. D. </author> <title> Monotone data flow analysis frameworks. </title> <journal> Acta Infor-matica 7 , (1977), </journal> <volume> 309 - 317. </volume>
Reference-contexts: This extension is the key for defining the solution of the parallel version of the meet over all paths (MOP ) approach in the sense of Kam and Ullman <ref> [KU] </ref>, which specifies the intuitively desired solution of a DFA-problem. <p> Con tinue with step 1. In essence, this three-step algorithm is a straightforward hierarchical adaptation of the functional version of the maximal fixed point (MFP ) approach in the sense of Kam and Ullman <ref> [KU] </ref> to the parallel setting (cf. [SP]). 7 Here we only consider the second step realizing the synchronization at nodes in N fl X in more detail. Central is the fol lowing lemma, which can be proved by means of Main Lemma 2.2. <p> if n = s fl fl ffi [[[ start (pfg (n)) ]]]u Const NonDestructed (n) if n 2 N fl X uf [[ m ]] ffi [[[ m ]]] j m 2 pred G fl (n)gu Const NonDestructed (n) otherwise In analogy to the MFP -solution of Kam and Ull-man <ref> [KU] </ref> for the sequential setting, we can now define the PMFP BV -solution of unidirectional bitvec-tor problems for the parallel setting: The PMFP BV -Solution: PMFP BV (G fl ;[[ ]]) : N fl ! F B defined by 8 n 2 N fl 8 b 2 B: PMFP BV (G
Reference: [KY] <author> Krishnamurthy, A., and Yelick, K. </author> <title> Optimizing parallel programs with explicit synchronization. </title> <booktitle> In Proceedings of the ACM SIGPLAN'95 Conference on Programming Language Design and Implementation (PLDI'95), </booktitle> <address> La Jolla, California, </address> <booktitle> SIGPLAN Notices 30 , 6 (1995), </booktitle> <volume> 196 - 204. </volume>
Reference-contexts: Thus, the current situation is characterized by approaches aiming at frameworks for the static analysis of parallel programs (cf. <ref> [GS, KY, SS, SW, WS] </ref>) and by approaches concerned with analysis problems that are specific to parallel programs like dead lock or mutual exclusion (cf. [DC]). 2 The Parallel Setting We consider a parallel imperative programming language with interleaving semantics.
Reference: [Ma] <author> Marriot, K. </author> <title> Frameworks for abstract interpretation. </title> <journal> Acta Informatica 30 , (1993), </journal> <volume> 103 - 129. </volume>
Reference-contexts: Theoretically well-founded are DFAs that are based on abstract interpretation (cf. <ref> [CC, Ma] </ref>). The point of this approach is to replace the "full" semantics by a simpler more abstract version, which is tailored to deal with a specific problem.
Reference: [MJ] <editor> Muchnick, S. S., and Jones, N. D. (Eds.). </editor> <title> Program flow analysis: Theory and applications. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: m to a predecessor of n by PP G fl [m; n] and PP G fl [m; n [, respectively. 4 2.2 Data Flow Analysis For imperative programming languages, data flow analysis (DFA) provides information about the program states that may occur at some given program points during execution (cf. <ref> [He, MJ] </ref>). Theoretically well-founded are DFAs that are based on abstract interpretation (cf. [CC, Ma]). The point of this approach is to replace the "full" semantics by a simpler more abstract version, which is tailored to deal with a specific problem.
Reference: [MP] <author> Midkiff, S. P., and Padua, D. A. </author> <title> Issues in the optimization of parallel programs. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, Volume II , St. </booktitle> <address> Charles, Illinois, </address> <year> (1990), </year> <pages> 105 - 113. </pages>
Reference-contexts: In fact, despite the growing importance of parallel languages, which are more and more supported by modern hardware environments, there is currently very little work on classical optimizations for parallel programs. This may be due to the fact that naive adaptations of the sequential optimization methods fail <ref> [MP] </ref>, and their straightforward correct adaptations have unacceptable costs caused by the interleavings which manifest the possible executions of a parallel program.
Reference: [Ru] <author> Ruthing, O. </author> <title> The complexity of exhaustive motion-elimination frameworks. </title> <note> Submitted to ACM SIGPLAN PLDI'96. </note>
Reference: [SCKKM] <author> Steffen, B., Claen, A., Klein, M., Knoop, J., and Margaria, T. </author> <booktitle> The fixpoint-analysis machine. In Proceedings of the 6 th International Conference on Concurrency Theory (CONCUR'95), </booktitle> <address> Philadelphia, </address> <note> Springer-Verlag, LNCS 962 (1995), 72 - 87. </note>
Reference-contexts: By means of a natural and sufficient side condition this defect could be cured and the optimality result known from the sequential setting be lifted to the parallel setting. The algorithm is currently implemented on the fixpoint-analysis machine of <ref> [SCKKM] </ref>. Moreover, we are investigating how to adapt the approach of this paper to the essentially dual technique of assig-ment motion [KRS4], which, however, is more intricate as it uniformly combines the elimination of partially redundant assignments and expressions.
Reference: [SHW] <author> Srinivasan, H., Hook, J., and Wolfe, M. </author> <title> Static single assignment form for explicitly parallel programs. </title> <booktitle> In Conference Record of the 20 th ACM SIGPLAN Symposium on Principles of Programming Languages (POPL'93), </booktitle> <address> Charleston, South Carolina, </address> <year> 1993, </year> <pages> 260 - 272. </pages>
Reference-contexts: For example, a replicator statement allowing a dynamical process creation can be integrated along the lines of [CH, Vo]. 2.1 Parallel Flow Graphs Similarly to <ref> [SHW] </ref> and [GS], we represent a parallel program by a nondeterministic parallel flow graph G fl = (N fl ; E fl ; s fl ; e fl ) with node set N fl and edge set E fl as illustrated in Figure 3.
Reference: [SS] <author> Shasha, D. and Snir, M. </author> <title> Efficient and correct execution of parallel programs that share memory. </title> <journal> Trans. on Progr. Lang. and Systems 10 , 2 (1988), </journal> <volume> 282 - 312. </volume>
Reference-contexts: Thus, the current situation is characterized by approaches aiming at frameworks for the static analysis of parallel programs (cf. <ref> [GS, KY, SS, SW, WS] </ref>) and by approaches concerned with analysis problems that are specific to parallel programs like dead lock or mutual exclusion (cf. [DC]). 2 The Parallel Setting We consider a parallel imperative programming language with interleaving semantics.
Reference: [SP] <author> Sharir, M., and Pnueli, A. </author> <title> Two approaches to interprocedural data flow analysis. </title> <booktitle> In [MJ], 1981, </booktitle> <volume> 189 - 233. </volume>
Reference-contexts: directly mimics possible program executions in that it "meets" (intersects) all informations belonging to a program path reaching the program point under consideration. 4 In [KSV1] an alternative characterization of parallel paths is given, which in spirit follows the definition of in-terprocedural program paths as proposed by Sharir and Pnueli <ref> [SP] </ref>. 5 In the following C will always denote a complete lattice. <p> Con tinue with step 1. In essence, this three-step algorithm is a straightforward hierarchical adaptation of the functional version of the maximal fixed point (MFP ) approach in the sense of Kam and Ullman [KU] to the parallel setting (cf. <ref> [SP] </ref>). 7 Here we only consider the second step realizing the synchronization at nodes in N fl X in more detail. Central is the fol lowing lemma, which can be proved by means of Main Lemma 2.2.
Reference: [SW] <author> Srinivasan, H., and Wolfe, M. </author> <title> Analyzing programs with explicit parallelism. </title> <booktitle> In Proc. of the 4 th International Conference on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <note> Springer-Verlag, LNCS 589 (1991), 405 - 419. </note>
Reference-contexts: Thus, the current situation is characterized by approaches aiming at frameworks for the static analysis of parallel programs (cf. <ref> [GS, KY, SS, SW, WS] </ref>) and by approaches concerned with analysis problems that are specific to parallel programs like dead lock or mutual exclusion (cf. [DC]). 2 The Parallel Setting We consider a parallel imperative programming language with interleaving semantics.
Reference: [Vo] <author> Vollmer, J. </author> <title> Data flow analysis of parallel programs. </title> <booktitle> In Proc. of the IFIP WG 10.3 Working Conf. on Parallel Architectures and Compilation Techniques (PACT'95), Limassol, Cyprus, 1995, </booktitle> <volume> 168 - 177. </volume> <pages> Page 12 </pages>
Reference-contexts: This setup already introduces the phenomena of interference and synchronization, and allows us to concentrate on the central features of our approach, which, however, is not limited to this setting. For example, a replicator statement allowing a dynamical process creation can be integrated along the lines of <ref> [CH, Vo] </ref>. 2.1 Parallel Flow Graphs Similarly to [SHW] and [GS], we represent a parallel program by a nondeterministic parallel flow graph G fl = (N fl ; E fl ; s fl ; e fl ) with node set N fl and edge set E fl as illustrated in Figure
Reference: [WS] <author> Wolfe, M, and Srinivasan, H. </author> <title> Data struc-tures for optimizing programs with explicit parallelism. </title> <booktitle> In Proc. of the 1 st Internat. Conf. of the Austrian Center for Parallel Computation, </booktitle> <address> Salzburg, Austria, </address> <note> Springer-Verlag, LNCS 591 (1991), 139 - 156. </note>
Reference-contexts: Thus, the current situation is characterized by approaches aiming at frameworks for the static analysis of parallel programs (cf. <ref> [GS, KY, SS, SW, WS] </ref>) and by approaches concerned with analysis problems that are specific to parallel programs like dead lock or mutual exclusion (cf. [DC]). 2 The Parallel Setting We consider a parallel imperative programming language with interleaving semantics.
References-found: 26

