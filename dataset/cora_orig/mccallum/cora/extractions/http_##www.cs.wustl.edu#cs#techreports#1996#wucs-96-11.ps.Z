URL: http://www.cs.wustl.edu/cs/techreports/1996/wucs-96-11.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Email: gopal@dworkin.wustl.edu guru@flora.wustl.edu  
Title: Efficient User space Protocol Implementations with QoS Guarantees using Real-time Upcalls  
Author: R. Gopalakrishnan Gurudatta M. Parulkar 
Address: in St.Louis  
Affiliation: Department of Computer Science Washington University  
Abstract: Real-time upcalls (rtus) are an operating systems mechanism to provide quality-of-service (QoS) guarantees to network applications, and to efficiently implement protocols in user space with (QoS) guarantees. Traditionally, threads (and real-time extensions to threads) have been used to structure concurrent activities in user space protocol implementations. However, preemptive scheduling required for real-time threads leads to excessive context switching, and introduces the need for expensive concurrency control mechanisms such as locking. The rtu mechanism exploits the iterative nature of protocol processing to eliminate the need for locking, and reduce asynchronous preemption, while ensuring real-time operation. In addition to efficiency, eliminating the need for concurrency control considerably simplifies protocol code. rtus have been implemented in the NetBSD OS on the Sparc and Pentium platforms. We used the rtu mechanism to implement a sender and a receiver program that communicate using udp sockets over 155Mbps atm, and compared the throughput with that obtained when rtus are not used. Our results show that the rtu based programs maintain the same throughput regardless of other system activities. Without the rtu mechanism, background load reduces throughput by as much as 80%. Using the rtu mechanism, total network bandwidth can be partitioned among different udp streams, and delivered to user programs even with background system load. We have also implemented the tcp protocol in user space using the rtu mechanism. For each tcp connection, rtus are setup for performing tcp output, input, and timer processing functions. The use of rtus, in conjunction with shared memory between kernel and user processes for data movement, and the ability of the atm adaptor driver to separate headers and data, makes our user level tcp implementation the most efficient one that we know of for providing QoS guarantees within the endsystem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Author(s), </author> <title> "Efficient Quality of Service Support in Multimedia Computer Operating Systems," </title> <type> Technical Report XXCS-94-26, </type> <institution> Dept. of Computer Science, XXX University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Finally we present our conclusions and plan of future work. 2 The QoS Framework We have developed a QoS framework <ref> [1] </ref> from the point of view of application processes that run on the endsystem. We identify four dimensions to the problem of providing QoS guarantees. These are QoS Specification Specification of QoS requirements is essential to provide performance guarantees. <p> We have worked out details of the mapping operation for the resources mentioned above <ref> [1] </ref>. 3 QoS Enforcement The mapping operation as mentioned above derives resource requirements, that are allocated by the operating system to each application during the setup phase. During the data transfer phase, the operating system implements the QoS enforcement function, which involves scheduling various shared resources to satisfy these allocations. <p> Batching is useful because, if protocol sessions are active in different processes, it is expensive to switch between processes after each processed pdu [21]. Other trade-offs involved in the choice of batch size are discussed in <ref> [1] </ref>. Depending on the host platform, and the amount of processing involved, the computation time C required to process B pdus can be determined. Thus, we arrive at a periodic processing model as shown in Figure 2. The period is T, and the requested computation time is C.
Reference: [2] <author> Author(s), </author> <title> "Real-time Upcalls: A Mechanism to Provide Real-time Processing Guarantees," </title> <type> Technical Report XXCS-95-06, </type> <institution> Dept. of Computer Science, XXX University, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: This is because the rtu handlers run at the lowest priority level just as any other user process, and therefore cannot block interrupts. More details are given in <ref> [2] </ref>. 7 5 Experiments Using RTUs Our initial implementation of the rtu facility was in the Netbsd kernel on a Sun 4/360 Sparc-1 machine. We subsequently ported it to the Pentium platform. The Pentium machines are equipped with 155 Mbps atm interfaces on the pci bus.
Reference: [3] <author> Author(s), </author> <title> "Bringing Real-time Scheduling Theory and Practice Closer for Multimedia Computing," </title> <booktitle> Accepted at ACM SIGMETRICS, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: An event for an rtu is the start of a new period 1 and occurs in the clock interrupt. There is an admission control operation (schedulability test) to ensure that each rtu gets its requested time to run in each period <ref> [3] </ref>. If admitted, each rtu handler is invoked periodically in real-time. The overall organization of the rtu facility is shown in Figure 3. The rtu facility is layered on top of the normal unix process scheduling mechanism. No modifications to the existing unix scheduler, or its policies are needed.
Reference: [4] <author> Author(s), </author> <title> "Quality of Service Support for Protocol Processing Within Endsystems," High Speed Networking for Multimedia Applications, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1995. </year>
Reference-contexts: provide efficient OS mechanisms to improve the efficiency of protocol implementations by reducing the overhead associated with data movement and context switching operations that have been shown to dominate protocol processing costs [7]. 2.1 CPU Requirements for Protocol Processing QoS specification and mapping steps are explained in greater detail in <ref> [4] </ref>. This paper mainly deals with QoS enforcement and protocol implementation. However, for the purpose of this paper, it is necessary to understand how processing requirements are expressed for each connection.
Reference: [5] <author> Chen, J.B., et. al., </author> <title> "The Measured Performance of Personal Computer Operating Systems," </title> <booktitle> 15 th ACM Symposium on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Locking mechanisms for real-time threads must prevent unbounded priority inversions, and have to be implemented in the kernel. Therefore real-time threads have to make system calls to manipulate locks. For a Pentium machine running Netbsd the system call overhead is about 770 processor cycles (or 280 instructions) <ref> [5] </ref>. Since pdu processing itself takes only a few hundred instructions, getting and releasing a lock per pdu increases pdu processing time manyfold.
Reference: [6] <author> Clark, </author> <title> D.D., "The Structuring of Systems using Upcalls," </title> <booktitle> ACM Symposium on Operating Systems Principles, </booktitle> <year> 1985, </year> <pages> pp. 171-180. </pages>
Reference-contexts: An upcall is a well known mechanism <ref> [6] </ref> to structure layered protocol code. Protocol code in a user process can register an upcall with the kernel and associate a handler function with each upcall. The kernel can then arrange to have the handler invoked when an event occurs for the upcall.
Reference: [7] <author> Clark, D.D., Jacobsen, V., Romkey, J., Salwen, H., </author> <title> "An Analysis of TCP Processing Overhead," </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6), </volume> <year> 1989, </year> <note> pp.23-29. 16 </note>
Reference-contexts: The rtu approach is based on the following arguments. * It is a well recognized fact that the upcall mechanism allows efficient protocol implementations <ref> [7] </ref>. Our experience strongly suggests that upcalls are the right abstraction to implement all concurrent activities in protocol processing. Our real-time extension to the upcall mechanism maintains all these advantages, and in addition provides QoS guarantees to network applications at lesser run-time cost than other mechanisms such as real-time threads. <p> An important objective of the model is to provide efficient OS mechanisms to improve the efficiency of protocol implementations by reducing the overhead associated with data movement and context switching operations that have been shown to dominate protocol processing costs <ref> [7] </ref>. 2.1 CPU Requirements for Protocol Processing QoS specification and mapping steps are explained in greater detail in [4]. This paper mainly deals with QoS enforcement and protocol implementation. However, for the purpose of this paper, it is necessary to understand how processing requirements are expressed for each connection. <p> Although a completely non-preemptive scheduling scheme would also eliminate the need for locking, it is not a viable option since it can provide processing guarantees only at very low utilizations. * Context switching costs are known to dominate pdu processing costs <ref> [7] </ref>. Therefore, the number of context switches must be kept to a minimum. However, a fully preemptive real-time scheduling policy can lead to excessive context switching compared to quantum based time sharing schemes, thereby reducing effective utilization.
Reference: [8] <author> Clark, D.D., Shenker, S., Zhang, L., </author> <title> "Supporting Real-time Applications in an Integrated Services Packet Network: Architecture and Mechanism," </title> <booktitle> ACM SIGCOMM, </booktitle> <month> Aug, </month> <year> 1992, </year> <month> pp.14-26. </month>
Reference-contexts: This requirement is also referred to as the need to provide "real-time" guarantees for data transfer and processing. Emerging networks such as atm and the proposed integrated services Internet <ref> [8] </ref> with reservation protocols such as rsvp [23] can provide guarantees for data transfer by managing network resources appropriately. Similarly, the operating system (os) must manage endsystem resources so that processing needs implied by the bandwidth and delay requirements of each connection are satisfied.
Reference: [9] <author> Dannenberg, R.B., et. al., </author> <title> "Performance Measurements of the Multimedia Testbed in RT-Mach" Tech. </title> <type> Rep. </type> <institution> CMU-CS-94-141, School of Comp. Sc., Carnegie mellon University. </institution>
Reference-contexts: When a real-time thread locks a shared resource, priority inheritance [20] is required to prevent unbounded priority inversions. Both rt-mach <ref> [9] </ref> and Solaris [14] address this problem, but it comes at a high cost. Each lock operation requires a system call to setup state in the kernel, so that priority can subsequently be inherited (if required) [14].
Reference: [10] <author> Druschel, P., Peterson, L.L., Davie, B.S., </author> <title> "Experiences with a High-Speed Network Adaptor: A Software Perspective," </title> <booktitle> ACM SIGCOMM, </booktitle> <month> Aug. </month> <year> 1994, </year> <pages> pp. 2-13. </pages>
Reference-contexts: Shared memory is used in place of system calls to initiate network i/o. The buffer management facility uses "lock free" <ref> [10] </ref> data structures to eliminate the need for synchronization between the user and the kernel during i/o operations. <p> as 10 msec, the upper bound on the number of of wired pages in the system is within 1 MB. 7.2.1 Lock Free Buffer Operations All mbuf network i/o and memory management operations that require synchronization between the user protocol and the kernel are implemented using lock free data structures <ref> [10] </ref>. The basic data structure is a one-reader-one-writer fifo queue to pass buffers between the user protocol and the kernel. <p> A queue consists of an array, with a read pointer and a write pointer that are indices into this array. The operation of the queue is explained in <ref> [10] </ref>. One important feature 13 of our implementation is that we store indices of mbufs rather than their addresses in the array. Since the car is contiguous in memory, these indices can be converted into either user addresses or kernel addresses without the need for any vm operations.
Reference: [11] <author> Edwards, A., Muir. S., </author> <title> "Experiences Implementing a High Performance TCP in User Space" Proc. </title> <booktitle> ACM SIGCOMM, </booktitle> <year> 1995, </year> <month> pp.196-205. </month>
Reference-contexts: This scheme eliminates the locking operations (system calls) during protocol processing. It also avoids situations in which a thread is switched in, only to yield immediately because a lock is held by a lower priority thread <ref> [11] </ref>. Eliminating concurrency control also simplifies protocol code and speeds up development and testing. * Although user space protocols have been shown to deliver good performance, we believe that further innovations are required to handle the additional dimension of QoS guarantees. <p> In addition, locking combined with preemptive priority scheduling causes unnecessary context switches due to lock contention. This occurs when a thread preempts a running (low priority) thread, only to find that a lock that it requires is unavailable <ref> [11] </ref>. Locking is used in most protocol implementations. For example in the bsd tcp implementation, the output processing, input processing, and timer processing routines lock the protocol control block (pcb) for a tcp connection before they process each segment. <p> The most compelling reason is that user space implementations can improve efficiency of protocol processing by taking advantage of the ability to move data directly between user processes and the network interface bypassing the os buffers. Mechanisms such as packet filters [19], bqis [21], and the atm vci <ref> [11] </ref> are used to achieve this. Since rtu handlers run in user space, they facilitate application level (or user space) protocol (alp) implementations. The rtu mechanism allows application processing to be also scheduled in real-time, and therefore has the potential to provide end-to-end guarantees. <p> The data transfer protocol processing that largely determines QoS for a session executes as part of the application process. This processing is structured using os mechanisms such as threads [21], processes <ref> [11] </ref> or in our case rtus. The alp model is well suited to native mode atm implementations where pdus of each transport connection are carried on a separate network connection with appropriate QoS parameters that meet the requirements of the session. <p> Thus, some mechanism for concurrency is required to implement protocols. With concurrency, arises the need for a concurrency control mechanism for synchronization between the upper and lower halves. 6.1 Related Work There have been several user space protocol implementations <ref> [17, 21, 11, 12] </ref>. Most of them share common features such as using a connection server for connection setup and teardown, avoiding copying of data by using memory "pools" to locate network data, and demultiplexing packets at the adaptor level. Most previous efforts have focused on the tcp/ip protocol stack. <p> Moreover, we expect that if rt-threads in rt-mach are used in place of the original C-threads, in order to provide QoS guarantees, the cost of concurrency control will be very high and make the implementation perform even worse. Implementations of tcp in hp-ux <ref> [11] </ref> and of u-net in sunos [12] are representative user space implementations in unix. <p> The hp implementation used real-time priorities to speedup protocol processing, although it was not intended to provide bandwidth guarantees. u-net does not deal with concurrency issues in protocol processing and is at present a "proof of concept" implementation. We will focus on the hp implementation <ref> [11] </ref> to illustrate the deficiencies with current user space protocol implementations. <p> In a kernel implementation of tcp, this is not a problem since the tcp state is in the kernel. The hp implementation uses a separate process to achieve this <ref> [11] </ref>. Other implementations transfer state to the kernel when the process exits [17]. The first solution is awkward, and the second solution requires the kernel to know the termination procedure for each user space protocol. <p> The shared memory region is called the communication area (car) of a process. The car is very similar to the pool model used in <ref> [11] </ref> and to the communication segments in u-net [12]. However it does not need any support from the network adaptor hardware. The car is of fixed size and is allocated in a contiguous portion of the process address space. <p> For example, a process with a user space tcp connection should be prevented from terminating before it closes its connection. In <ref> [11] </ref> a separate process is required to do this for each connection and in [17] connection state is copied to the kernel on process exit. In our framework, the kernel uses its knowledge of active rtus to decide when to deallocate a process.
Reference: [12] <author> Eicken, </author> <title> T.V., </title> <editor> et. al., "U-Net: </editor> <title> A user Level Network Interface for Parallel and Distributed Computing," </title> <booktitle> 15 th ACM Symposium on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Another benefit of using atm is the simple demultiplexing scheme that is based on identifying each connection by its vci. Thus, both incoming and outgoing data can move directly between user space and the vci without having to be moved into os buffers <ref> [12] </ref>. Thus the os is involved mainly in scheduling the processing of network data in the different processes. <p> Thus, some mechanism for concurrency is required to implement protocols. With concurrency, arises the need for a concurrency control mechanism for synchronization between the upper and lower halves. 6.1 Related Work There have been several user space protocol implementations <ref> [17, 21, 11, 12] </ref>. Most of them share common features such as using a connection server for connection setup and teardown, avoiding copying of data by using memory "pools" to locate network data, and demultiplexing packets at the adaptor level. Most previous efforts have focused on the tcp/ip protocol stack. <p> Moreover, we expect that if rt-threads in rt-mach are used in place of the original C-threads, in order to provide QoS guarantees, the cost of concurrency control will be very high and make the implementation perform even worse. Implementations of tcp in hp-ux [11] and of u-net in sunos <ref> [12] </ref> are representative user space implementations in unix. The hp implementation used real-time priorities to speedup protocol processing, although it was not intended to provide bandwidth guarantees. u-net does not deal with concurrency issues in protocol processing and is at present a "proof of concept" implementation. <p> The shared memory region is called the communication area (car) of a process. The car is very similar to the pool model used in [11] and to the communication segments in u-net <ref> [12] </ref>. However it does not need any support from the network adaptor hardware. The car is of fixed size and is allocated in a contiguous portion of the process address space.
Reference: [13] <author> Govindan, R., Anderson, </author> <title> D.P., "Scheduling and IPC Mechanisms for Continuous Media," </title> <booktitle> 13 th ACM Symp. on Operating Systems Principles, </booktitle> <year> 1991. </year>
Reference-contexts: The count of pdus that remain to be processed for the current period is stored in the shared memory (not shown), and when the handler is resumed, it needs to process only these many pdus. Other systems <ref> [13, 18] </ref> have used shared memory to allow the user level scheduler and the kernel level scheduler to communicate, but their objectives and the algorithms used are different.
Reference: [14] <author> Khanna, S., et. al., </author> <title> "Realtime Scheduling in SunOS5.0," </title> <booktitle> USENIX, Winter 1992, </booktitle> <address> pp.375-390. </address>
Reference-contexts: This is because the unix scheduler is designed for time sharing rather than real-time operation. We therefore need the functionality of a real-time thread mechanism provided by operating systems such as rt-Mach [22] or Solaris <ref> [14] </ref>. However, there are several drawbacks to using existing real-time thread mechanisms for protocol processing. * The real-time nature of threads, coupled with the preemptive scheduling mechanism used in most systems, introduces additional complexity to support locking of shared variables. <p> When a real-time thread locks a shared resource, priority inheritance [20] is required to prevent unbounded priority inversions. Both rt-mach [9] and Solaris <ref> [14] </ref> address this problem, but it comes at a high cost. Each lock operation requires a system call to setup state in the kernel, so that priority can subsequently be inherited (if required) [14]. <p> Both rt-mach [9] and Solaris <ref> [14] </ref> address this problem, but it comes at a high cost. Each lock operation requires a system call to setup state in the kernel, so that priority can subsequently be inherited (if required) [14]. Locking is inherently pessimistic, since it protects against the unlikely eventuality that access to shared resources overlap in time. It therefore runs counter to the "success oriented" strategies used by protocol implementations. In addition, locking combined with preemptive priority scheduling causes unnecessary context switches due to lock contention.
Reference: [15] <editor> Le*er, S.J., et. al., </editor> <title> "The Design and Implementation of the 4.3BSD UNIX Operating System," </title> <publisher> Addison Wesley Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Figure 8 shows two processes with their cars that are also mapped to kernel address space. The management page of the car contains information about transmit and receive queues for each vci in the process. Most of the tcp/ip protocol implementations use the Mbuf facility <ref> [15] </ref>. To use existing tcp/ip code with minimal modifications, we ported the mbuf facility to run in user space, and use the memory in the car to allocate the buffers used by mbufs. Accordingly, pages in the car are allocated among normal mbufs and cluster mbufs [15] where our cluster is <p> use the Mbuf facility <ref> [15] </ref>. To use existing tcp/ip code with minimal modifications, we ported the mbuf facility to run in user space, and use the memory in the car to allocate the buffers used by mbufs. Accordingly, pages in the car are allocated among normal mbufs and cluster mbufs [15] where our cluster is equal to a page size (4KB). Since rtus are guaranteed to run every period, the size of the car required to avoid overflows can be kept small. This advantage is not present in other user space protocol implementations.
Reference: [16] <author> Liu, C.L. and Layland, J.W., </author> <title> "Scheduling Algorithms for Multiprogramming in a Hard-Real-Time Environment," </title> <journal> JACM, </journal> <volume> Vol. 20,No. 1, </volume> <month> January </month> <year> 1973, </year> <pages> pp. 46-61. </pages>
Reference-contexts: In the next section, we describe how the rmdp policy takes advantage of the iterative nature of protocol processing to eliminate the need for concurrency control and reduce number of preemptions. 4.1 The RMDP Scheduling Policy The rmdp policy is a modification of the rate monotonic scheduling policy <ref> [16] </ref>. In the basic rm policy, the priority of a thread is inversely proportional to its period (or directly proportional to its rate). The rm policy requires the highest priority thread in the run queue to be running at any instant. <p> We report one of these experiments to show that rtus meet deadlines and provide cpu utilization that is guaranteed by theoretical results <ref> [16] </ref>. We created 6 rtus in each of two processes with periods ranging from 40 msecs to 90 msecs in increments of 10 msec. We varied the total cpu utilization consumed by the 12 rtu handlers and measured how close each invocation came to its deadline.
Reference: [17] <author> Maeda, C., Bershad, </author> <title> B.N., "Protocol Service Decomposition for High Performance Networking," </title> <booktitle> 14 th ACM Symp. on Operating Systems Principles, </booktitle> <month> Dec </month> <year> 1993, </year> <pages> pp. 244-55. </pages>
Reference-contexts: sharing provided by the rtu mechanism is preserved whereas the throughput for the non-rtu case drops even lower relative to its value in the no load case. 9 6 User Space Protocol Implementation-Issues and Related Work Most of the important arguments for user space protocols have been brought out in <ref> [21, 17] </ref>. These include ease of prototyping and maintenance, co-existence of multiple protocols, and ability to exploit application specific knowledge to improve performance. <p> Thus, some mechanism for concurrency is required to implement protocols. With concurrency, arises the need for a concurrency control mechanism for synchronization between the upper and lower halves. 6.1 Related Work There have been several user space protocol implementations <ref> [17, 21, 11, 12] </ref>. Most of them share common features such as using a connection server for connection setup and teardown, avoiding copying of data by using memory "pools" to locate network data, and demultiplexing packets at the adaptor level. Most previous efforts have focused on the tcp/ip protocol stack. <p> In a kernel implementation of tcp, this is not a problem since the tcp state is in the kernel. The hp implementation uses a separate process to achieve this [11]. Other implementations transfer state to the kernel when the process exits <ref> [17] </ref>. The first solution is awkward, and the second solution requires the kernel to know the termination procedure for each user space protocol. <p> For example, a process with a user space tcp connection should be prevented from terminating before it closes its connection. In [11] a separate process is required to do this for each connection and in <ref> [17] </ref> connection state is copied to the kernel on process exit. In our framework, the kernel uses its knowledge of active rtus to decide when to deallocate a process.
Reference: [18] <author> Marsh, B.D., et. al., </author> <title> "First Class User Level Threads," </title> <booktitle> ACM Symposium on Operating Systems Principles, </booktitle> <month> Oct. </month> <year> 1991, </year> <month> pp.110-21. </month>
Reference-contexts: The count of pdus that remain to be processed for the current period is stored in the shared memory (not shown), and when the handler is resumed, it needs to process only these many pdus. Other systems <ref> [13, 18] </ref> have used shared memory to allow the user level scheduler and the kernel level scheduler to communicate, but their objectives and the algorithms used are different.
Reference: [19] <author> Mogul, J.C., et. al., </author> <title> "The Packet Filter: An Efficient Mechanism for User Level Network Code," </title> <booktitle> 11 th ACM Symposium on Operating System Principles, </booktitle> <address> Nov. 87, pp.39-51, </address>
Reference-contexts: The most compelling reason is that user space implementations can improve efficiency of protocol processing by taking advantage of the ability to move data directly between user processes and the network interface bypassing the os buffers. Mechanisms such as packet filters <ref> [19] </ref>, bqis [21], and the atm vci [11] are used to achieve this. Since rtu handlers run in user space, they facilitate application level (or user space) protocol (alp) implementations.
Reference: [20] <author> Sha, L. et. al., </author> <title> "Priority Inheritance Protocols: An Approach to Real-Time Synchronization," </title> <journal> IEEE Transactions on Computers, Vol.39, </journal> <volume> No.9, </volume> <month> Sep </month> <year> 1990, </year> <month> pp.1175-1185. </month>
Reference-contexts: However, there are several drawbacks to using existing real-time thread mechanisms for protocol processing. * The real-time nature of threads, coupled with the preemptive scheduling mechanism used in most systems, introduces additional complexity to support locking of shared variables. When a real-time thread locks a shared resource, priority inheritance <ref> [20] </ref> is required to prevent unbounded priority inversions. Both rt-mach [9] and Solaris [14] address this problem, but it comes at a high cost. Each lock operation requires a system call to setup state in the kernel, so that priority can subsequently be inherited (if required) [14].
Reference: [21] <author> Thekkath, </author> <title> C.A., et.al., "Implementing Network Protocols at the User Level," </title> <booktitle> ACM SIGCOMM, </booktitle> <month> Sep. </month> <year> 1993, </year> <pages> pp. 64-72. </pages>
Reference-contexts: Equivalently, we could require that a batch of B pdus be generated in every interval of length T, that is B times the original duration. Batching is useful because, if protocol sessions are active in different processes, it is expensive to switch between processes after each processed pdu <ref> [21] </ref>. Other trade-offs involved in the choice of batch size are discussed in [1]. Depending on the host platform, and the amount of processing involved, the computation time C required to process B pdus can be determined. Thus, we arrive at a periodic processing model as shown in Figure 2. <p> sharing provided by the rtu mechanism is preserved whereas the throughput for the non-rtu case drops even lower relative to its value in the no load case. 9 6 User Space Protocol Implementation-Issues and Related Work Most of the important arguments for user space protocols have been brought out in <ref> [21, 17] </ref>. These include ease of prototyping and maintenance, co-existence of multiple protocols, and ability to exploit application specific knowledge to improve performance. <p> The most compelling reason is that user space implementations can improve efficiency of protocol processing by taking advantage of the ability to move data directly between user processes and the network interface bypassing the os buffers. Mechanisms such as packet filters [19], bqis <ref> [21] </ref>, and the atm vci [11] are used to achieve this. Since rtu handlers run in user space, they facilitate application level (or user space) protocol (alp) implementations. The rtu mechanism allows application processing to be also scheduled in real-time, and therefore has the potential to provide end-to-end guarantees. <p> The data transfer protocol processing that largely determines QoS for a session executes as part of the application process. This processing is structured using os mechanisms such as threads <ref> [21] </ref>, processes [11] or in our case rtus. The alp model is well suited to native mode atm implementations where pdus of each transport connection are carried on a separate network connection with appropriate QoS parameters that meet the requirements of the session. <p> Thus, some mechanism for concurrency is required to implement protocols. With concurrency, arises the need for a concurrency control mechanism for synchronization between the upper and lower halves. 6.1 Related Work There have been several user space protocol implementations <ref> [17, 21, 11, 12] </ref>. Most of them share common features such as using a connection server for connection setup and teardown, avoiding copying of data by using memory "pools" to locate network data, and demultiplexing packets at the adaptor level. Most previous efforts have focused on the tcp/ip protocol stack. <p> Most previous efforts have focused on the tcp/ip protocol stack. The first implementation was for the mach os using mach threads for concurrency and mach ipc for data movement <ref> [21] </ref>. This implementation was shown to perform better than the standard tcp implementation in mach. However, it did not perform as well as a monolithic kernel implementation.
Reference: [22] <author> Tokuda, H., Nakajima, T., Rao, P., </author> <title> "Real-Time Mach: Towards Predictable Real-time Systems," </title> <booktitle> USENIX Mach Workshop, </booktitle> <month> Oct </month> <year> 1990. </year>
Reference-contexts: Implementing protocols using the components provided in the framework This paper reports on a mechanism called the real-time upcall (rtu) that can be used as a QoS enforcement mechanism in the endsystem, and to efficiently implement protocols in user space with QoS guarantees. rtus are an alternative to real-time threads <ref> [22] </ref> for protocol processing. The most important benefits of the rtu mechanism are an improvement in runtime efficiency and simplification of protocol code. and claims-Arguments for the RTU Approach and Claims. <p> Thus, we arrive at a periodic processing model as shown in Figure 2. The period is T, and the requested computation time is C. Note that such a periodic model is widely used in the real-time scheduling domain to express processing requirements <ref> [22] </ref>. In addition, it closely mirrors the periodic nature of continuous media data. The main difference is that the rtu mechanism uses the number of pdus processed per period, rather than time, as a measure of computation requirement. <p> This is because the unix scheduler is designed for time sharing rather than real-time operation. We therefore need the functionality of a real-time thread mechanism provided by operating systems such as rt-Mach <ref> [22] </ref> or Solaris [14]. However, there are several drawbacks to using existing real-time thread mechanisms for protocol processing. * The real-time nature of threads, coupled with the preemptive scheduling mechanism used in most systems, introduces additional complexity to support locking of shared variables.
Reference: [23] <author> Zhang, L., et. al., "RSVP: </author> <title> A New Resource Reservation Protocol," </title> <journal> IEEE Network, </journal> <month> Sep. </month> <year> 1993, </year> <note> pp.8-18. 17 </note>
Reference-contexts: This requirement is also referred to as the need to provide "real-time" guarantees for data transfer and processing. Emerging networks such as atm and the proposed integrated services Internet [8] with reservation protocols such as rsvp <ref> [23] </ref> can provide guarantees for data transfer by managing network resources appropriately. Similarly, the operating system (os) must manage endsystem resources so that processing needs implied by the bandwidth and delay requirements of each connection are satisfied.
References-found: 23

