URL: http://polaris.cs.uiuc.edu/reports/1314.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Symbolic Analysis: A Basis for Parallelization, Optimization, and Scheduling of Programs  
Author: Mohammad R. Haghighat Constantine D. Polychronopoulos 
Address: Urbana, IL 61801-2932  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: This paper presents an abstract interpretation framework for parallelizing compilers. Within this framework, symbolic analysis is used to solve various flow analysis problems in a unified way. Symbolic analysis also serves as a basis for code generation optimizations and a tool for derivation of computation cost estimates. A loop scheduling strategy that utilizes symbolic timing information is also presented.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers : Principles, Techniques and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <month> March </month> <year> 1986. </year>
Reference-contexts: In the general case, finding an optimal solution of code generation has been proven to be NP-Complete. However, for the cases where the DAG representation of expressions is a tree, efficient algorithms exist that find an optimal solution for machines that have a single instruction stream <ref> [1] </ref>. In many cases that the DAG representation of expressions is not a tree, symbolic values of expressions may be used to find an equivalent tree of the DAG, and thus generate the optimal code. the useful expressions.
Reference: [2] <author> A. V. Aho and J. D. Ullman. </author> <title> The Theory of Parsing, Translation, and Compiling, volume II : Compiling. </title> <publisher> Prentice-Hall, </publisher> <year> 1973. </year>
Reference-contexts: This precision of compile time analysis is the result of intelligent manipulation of symbolic values by the abstract interpreter. 2.2 Generalized Induction Variables Traditionally, induction variables of a loop have been defined as variables whose values at successive iterations of the loop form an arithmetic progression <ref> [2] </ref>. These variables are often defined by recurrence relations which introduce dependences between successive iterations of a loop, and thus prohibit loop parallelization. However, the compiler can find solutions of the recurrences in terms of the loop index variables, and substitute the solutions for the uses of the induction variables.
Reference: [3] <author> F. E. Allen, J. Cocke, and K. Kennedy. </author> <title> Reduction of operator strength. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 79-101. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: computational structures. 3 Program Optimization Symbolic analysis can be used as a basis for a wide range of program optimizations such as strength reduction, restructuring of arithmetic computations, and elimination of redundant computations. 3.1 Generalized Strength Reduction Strength reduction is a compiler technique that replaces expensive operations by fast instructions <ref> [3] </ref>. The classical strength reduction algorithm is based on induction variable detection. A pattern matching approach is used to remove multiplications of induction variables by region constants or other induction variables. Any arbitrary polynomial of induction variables can be reduced by multiple applications of the algorithm.
Reference: [4] <author> I. Barani and Z. Furedi. </author> <title> Computing the volume is difficult. </title> <booktitle> In 18th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 442-447, </pages> <address> Berkeley, California, </address> <year> 1986. </year> <month> 21 </month>
Reference-contexts: Computing the volume of convex sets, in the general case, is proven to be an exponential problem in terms of the dimension of the convex set <ref> [4] </ref>. However, the convex polyhedra that we consider, have a nice property that makes computation of their volumes much easier. In the mathematical sense, these polyhedra are cylinders.
Reference: [5] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the perfect benchmarks programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Empirical results indicate that existing parallelizing compilers cause insignificant improvements on the performance of many real application programs <ref> [9, 5] </ref>. The speedups obtained by manual transformation of these applications [9] show the potential for significantly advancing paral-lelizing compiler technology. The poor performance of current restructuring compilers can be attributed to two causes: imprecise analysis and inappropriate performance-wise transformations.
Reference: [6] <author> T. E. Cheatham, JR., H. Holloway, G., and J. A. Townley. </author> <title> Symbolic evaluation and the analysis of programs. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(4):402-417, </volume> <month> July </month> <year> 1979. </year>
Reference-contexts: The preliminary results indicate that symbolic analysis supports compilation of very high precision, and is necessary for sophisticated parallelizing compilers and programming environments. 1 2 Symbolic Analysis Symbolic analysis is a program analysis method in which values of program expressions are represented by symbolic expressions <ref> [6, 7, 16] </ref>. Using the concrete symbolic algebra kernel of Parafrase-2 and our abstract interpretation method, functional behavior of programs is derived.
Reference: [7] <author> L. A. Clarke and D. J. Richardson. </author> <title> Applications of symbolic evaluation. </title> <journal> Journal of Systems and Software, </journal> <volume> 5(1) </volume> <pages> 15-35, </pages> <year> 1985. </year>
Reference-contexts: The preliminary results indicate that symbolic analysis supports compilation of very high precision, and is necessary for sophisticated parallelizing compilers and programming environments. 1 2 Symbolic Analysis Symbolic analysis is a program analysis method in which values of program expressions are represented by symbolic expressions <ref> [6, 7, 16] </ref>. Using the concrete symbolic algebra kernel of Parafrase-2 and our abstract interpretation method, functional behavior of programs is derived.
Reference: [8] <author> P. Cousot and R. Cousot. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Proceedings of the 4th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <address> Los Angeles, CA, </address> <month> January </month> <year> 1977. </year>
Reference-contexts: In the abstract interpretation method <ref> [8] </ref>, for each operation in the concrete domain there is a corresponding operation in the abstract domain. Function and subroutine calls are not exceptions to the rule; they are modeled by their effect on the program variables. Symbolic analysis attempts to abstract the program loops by closed form expressions.
Reference: [9] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic paral-lelization of four perfect-benchmark programs. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 65-82. </pages> <publisher> Springer-Verlag, LNCS 589, </publisher> <month> August 7-9 </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Empirical results indicate that existing parallelizing compilers cause insignificant improvements on the performance of many real application programs <ref> [9, 5] </ref>. The speedups obtained by manual transformation of these applications [9] show the potential for significantly advancing paral-lelizing compiler technology. The poor performance of current restructuring compilers can be attributed to two causes: imprecise analysis and inappropriate performance-wise transformations. <p> 1 Introduction Empirical results indicate that existing parallelizing compilers cause insignificant improvements on the performance of many real application programs [9, 5]. The speedups obtained by manual transformation of these applications <ref> [9] </ref> show the potential for significantly advancing paral-lelizing compiler technology. The poor performance of current restructuring compilers can be attributed to two causes: imprecise analysis and inappropriate performance-wise transformations. The causes are not completely independent; namely, imprecise information results in inappropriate transformations. <p> This class of variables are called generalized induction variables <ref> [9, 16, 31] </ref>. Programs ADM, MDG, MG3D, OCEAN, SPEC77, and TRFD from Perfect Benchmarks rfl are but some examples that contain such cases. <p> Manual transformation of a generalized induction variable in a loop that performs 40% of the computations of the whole OCEAN program results in a speedup of 8:1 on the Cedar multiprocessor <ref> [9] </ref>. Automatic handling of such cases is beyond the power of existing parallelizing compilers. Elimination of the dependences introduced by the assignments to the variable mijkl requires sophisticated symbolic analysis and global forward substitution. To our knowledge, Parafrase-2 is the only existing parallelizing compiler capable of handling this case.
Reference: [10] <author> L. E. Flynn and S. Flynn Hummel. </author> <title> Scheduling variable-length parallel subtasks. </title> <type> Technical Report RC15492, T.J. </type> <institution> Watson Research Center, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: Guided self-scheduling [22], factoring <ref> [10, 11] </ref>, and trapezoidal self-scheduling [27] are examples of such schemes. All the above schemes can benefit the cost estimates provided by the symbolic analysis. We demonstrate this by modifying the chunk-scheduling scheme to utilize the symbolic execution time information. We call this new scheme balanced chunk-scheduling (or BCS).
Reference: [11] <author> S. Flynn Hummel, E. Schonberg, and L. E. Flynn. </author> <title> Factoring: A method for scheduling parallel loops. </title> <journal> CACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Guided self-scheduling [22], factoring <ref> [10, 11] </ref>, and trapezoidal self-scheduling [27] are examples of such schemes. All the above schemes can benefit the cost estimates provided by the symbolic analysis. We demonstrate this by modifying the chunk-scheduling scheme to utilize the symbolic execution time information. We call this new scheme balanced chunk-scheduling (or BCS). <p> In the case of coarse-grained iterations with variable execution times self-scheduling performs much better than chunk-scheduling, while fine-grained iterations with constant execution time favor chunk-scheduling. To compare the performance of balanced chunk-scheduling against the other schemes, we have selected two examples from <ref> [11] </ref> that cover a wide spectrum of characteristics. The benchmark program shown in Figure 7 is an adjoint-convolution. The granularity of the parallel work is large with a great deal of variance among the parallel iterations.
Reference: [12] <author> M. B. Girkar. </author> <title> Functional Parallelism : Theoretical Foundations and Implementation. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: In the case of dynamic scheduling, the compiler can use the generalized strength reduction scheme of Section 3 to instrument the code in such a way that each task of the program HTG <ref> [12] </ref> has its own metric computed in an efficient way.
Reference: [13] <author> T. Gonzalez and J. Ja'Ja'. </author> <title> Evaluation of arithmetic expressions with algebraic identities. </title> <journal> SIAM J. of Computing, </journal> <volume> 11(4) </volume> <pages> 633-662, </pages> <month> November 82. </month>
Reference-contexts: such a way to get the computations done with minimal amount of work. 3.2 Restructuring of Arithmetic Computations To evaluate an arithmetic expression E under a set of algebraic laws, the compiler can generate code for any equivalent expression E 0 , obtained by successive applications of the algebraic laws <ref> [13] </ref>. An optimality criterion can be defined based on the cost of computations. In the general case, finding an optimal solution of code generation has been proven to be NP-Complete.
Reference: [14] <author> M. R. Haghighat. </author> <title> Symbolic Analysis for High Performance Parallelizing Compilers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: Proof. See <ref> [14] </ref>. In the code segment of Figure 4, let the value of the subscript expression of the array xijkl at the iteration (mi; mj; mk; ml) be denoted by e (mi; mj; mk; ml).
Reference: [15] <author> M. R. Haghighat and C. D. Polychronopoulos. </author> <title> Symbolic dependence analysis for high-performance parallelizing compilers. </title> <editor> In A. Nicolau, Gelernter D., T. Gross, and Padua D., editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 310-330. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: The attained solution space of the above problems is much larger than that handled by existing compiler technology. The collected information about the relationships between program expressions supports a very precise symbolic dependence analysis scheme <ref> [15] </ref>. The power of a parallelizing compiler critically depends on its ability to analyze program loops for extraction and exploitation of the available parallelism. Analysis of a loop involves solving the system of recurrence relations defined by the loop body. <p> Our proposed symbolic analysis framework supports a very accurate dependence analysis scheme in the presence of unknown symbolic terms <ref> [15] </ref>.
Reference: [16] <author> M. R. Haghighat and C. D. Polychronopoulos. </author> <title> Symbolic Program Analysis and Optimization for Parallelizing Compilers. </title> <booktitle> 5th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August 3-5 </month> <year> 1992. </year>
Reference-contexts: The preliminary results indicate that symbolic analysis supports compilation of very high precision, and is necessary for sophisticated parallelizing compilers and programming environments. 1 2 Symbolic Analysis Symbolic analysis is a program analysis method in which values of program expressions are represented by symbolic expressions <ref> [6, 7, 16] </ref>. Using the concrete symbolic algebra kernel of Parafrase-2 and our abstract interpretation method, functional behavior of programs is derived. <p> Analysis of a loop involves solving the system of recurrence relations defined by the loop body. Parafrase-2 employs mathematical methods such as finite differences to solve the corresponding recurrences symbolically <ref> [16] </ref>. In the following sections we discuss a number of known optimization problems cast in the framework of symbolic analysis. <p> This class of variables are called generalized induction variables <ref> [9, 16, 31] </ref>. Programs ADM, MDG, MG3D, OCEAN, SPEC77, and TRFD from Perfect Benchmarks rfl are but some examples that contain such cases.
Reference: [17] <author> T. Hickey and J. Cohen. </author> <title> Automating program analysis. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35(1) </volume> <pages> 185-220, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Symbolic analysis can even provide statistical information such as minimum, maximum, average, and standard deviation of the execution time of iterations of program loops. Such a metric for analysis of algorithms is proposed by Knuth [18] and used by Wegbreit [28, 29], Ramshaw [23], and Hickey and Cohen <ref> [17] </ref>. Abstract interpretation can employ symbolic analysis to support an accurate metric for program performance, which also takes into account memory management issues such as type of memory accesses in computer architectures with hierarchical memories. Compilers can choose the appropriate choice of transformations using this performance measure.
Reference: [18] <author> D. E. Knuth. </author> <booktitle> The Art of Computer Programming, Vol. 1 / Fundamental Algorithms. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <note> second edition, </note> <year> 1973. </year>
Reference-contexts: Symbolic analysis can even provide statistical information such as minimum, maximum, average, and standard deviation of the execution time of iterations of program loops. Such a metric for analysis of algorithms is proposed by Knuth <ref> [18] </ref> and used by Wegbreit [28, 29], Ramshaw [23], and Hickey and Cohen [17]. Abstract interpretation can employ symbolic analysis to support an accurate metric for program performance, which also takes into account memory management issues such as type of memory accesses in computer architectures with hierarchical memories.
Reference: [19] <author> C. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(10), </volume> <month> October </month> <year> 1985. </year> <month> 22 </month>
Reference-contexts: However, the self-scheduling is not a good scheme if the overhead involved with each dispatch is comparable to the average execution time of the iterations. At the other extreme, another scheme called chunk-scheduling <ref> [19] </ref> allocates a fixed number of iterations (a chunk) to each idle processor. This method reduces the scheduling overhead but may result in a poor performance when there is a considerable variation between execution time of loop iterations.
Reference: [20] <author> J. E. Moreira. </author> <title> Auto-scheduling Compilers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana--Champaign, Urbana, Illinois, </institution> <month> May </month> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: We call this new scheme balanced chunk-scheduling (or BCS). The performance of balanced chunk-scheduling will be compared with that of chunk-scheduling and self-scheduling. The reported performance is the result of compiling and running our examples on a system developed by Jose Moreira <ref> [20] </ref>. It is the characteristics of loop iterations which make a scheduling scheme perform better than the other. In the case of coarse-grained iterations with variable execution times self-scheduling performs much better than chunk-scheduling, while fine-grained iterations with constant execution time favor chunk-scheduling.
Reference: [21] <author> C. D. Polychronopoulos, M. B. Girkar, M. R. Haghighat, C. L. Lee, B. P. Leung, and D. A. Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing and scheduling programs on multiprocessors. </title> <booktitle> In Proceedings of the 1989 ICPP, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1989. </year> <institution> Penn State. </institution>
Reference-contexts: The causes are not completely independent; namely, imprecise information results in inappropriate transformations. The main problem of compile time analysis is the difficulty of collecting sufficient information in an efficient way to utilize the underlying architecture. We have built a framework for Parafrase-2 <ref> [21] </ref> that supports a very precise compile time analysis of programs. Within this framework, symbolic analysis is used as an abstract interpretation technique to solve many of the program flow analysis problems in a unified way.
Reference: [22] <author> C. D. Polychronopoulos and D. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Guided self-scheduling <ref> [22] </ref>, factoring [10, 11], and trapezoidal self-scheduling [27] are examples of such schemes. All the above schemes can benefit the cost estimates provided by the symbolic analysis. We demonstrate this by modifying the chunk-scheduling scheme to utilize the symbolic execution time information.
Reference: [23] <author> L. H. Ramshaw. </author> <title> Formalizing the analysis of algorithms. </title> <type> Technical Report SL-79-5, </type> <institution> Xerox Palo Alto Research Center, </institution> <address> Palo Alto, CA, </address> <year> 1979. </year>
Reference-contexts: Symbolic analysis can even provide statistical information such as minimum, maximum, average, and standard deviation of the execution time of iterations of program loops. Such a metric for analysis of algorithms is proposed by Knuth [18] and used by Wegbreit [28, 29], Ramshaw <ref> [23] </ref>, and Hickey and Cohen [17]. Abstract interpretation can employ symbolic analysis to support an accurate metric for program performance, which also takes into account memory management issues such as type of memory accesses in computer architectures with hierarchical memories.
Reference: [24] <author> V. Sarkar. </author> <title> Determining average program execution times and their variance. </title> <booktitle> In Proceedings of the ACM SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 298-312, </pages> <address> Portland, Oregon, </address> <month> June 21-23 </month> <year> 1989. </year>
Reference-contexts: This indicates that symbolic cost estimates can be used to design robust scheduling strategies that perform very well on a wide range of cases. Sarkar has studied partitioning of parallel programs using cost estimates computed from the profile information <ref> [24, 25] </ref>. 12 INTEGER iter (p+1) np1 = n + 1 iter (1) = 1 work = n * n DO i = 1, n load = work iter (pp1) = work + 1 work = work - n start = 1 procs = p DO ix = 2, p this
Reference: [25] <author> V. Sarkar. </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: This indicates that symbolic cost estimates can be used to design robust scheduling strategies that perform very well on a wide range of cases. Sarkar has studied partitioning of parallel programs using cost estimates computed from the profile information <ref> [24, 25] </ref>. 12 INTEGER iter (p+1) np1 = n + 1 iter (1) = 1 work = n * n DO i = 1, n load = work iter (pp1) = work + 1 work = work - n start = 1 procs = p DO ix = 2, p this
Reference: [26] <author> P. Tang and P. C. Yew. </author> <title> Processor self-scheduling for multiple-nested parallel loops. </title> <booktitle> In Proceedings of the 1986 ICPP, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: At one extreme, a strategy called self-scheduling <ref> [26] </ref> schedules loop iterations one at a time, and thus achieves the best possible load balancing. However, the self-scheduling is not a good scheme if the overhead involved with each dispatch is comparable to the average execution time of the iterations.
Reference: [27] <author> T. H. Tzen and L. M. Ni. </author> <title> Dynamic loop scheduling for shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1991 ICPP, </booktitle> <volume> volume 2, </volume> <month> August </month> <year> 1991. </year>
Reference-contexts: Guided self-scheduling [22], factoring [10, 11], and trapezoidal self-scheduling <ref> [27] </ref> are examples of such schemes. All the above schemes can benefit the cost estimates provided by the symbolic analysis. We demonstrate this by modifying the chunk-scheduling scheme to utilize the symbolic execution time information. We call this new scheme balanced chunk-scheduling (or BCS).
Reference: [28] <author> B. Wegbreit. </author> <title> Mechanical program analysis. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 18(9) </volume> <pages> 528-539, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: Symbolic analysis can even provide statistical information such as minimum, maximum, average, and standard deviation of the execution time of iterations of program loops. Such a metric for analysis of algorithms is proposed by Knuth [18] and used by Wegbreit <ref> [28, 29] </ref>, Ramshaw [23], and Hickey and Cohen [17]. Abstract interpretation can employ symbolic analysis to support an accurate metric for program performance, which also takes into account memory management issues such as type of memory accesses in computer architectures with hierarchical memories.
Reference: [29] <author> B. Wegbreit. </author> <title> Verifying program performance. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 23(4) </volume> <pages> 691-699, </pages> <month> October </month> <year> 1976. </year>
Reference-contexts: Symbolic analysis can even provide statistical information such as minimum, maximum, average, and standard deviation of the execution time of iterations of program loops. Such a metric for analysis of algorithms is proposed by Knuth [18] and used by Wegbreit <ref> [28, 29] </ref>, Ramshaw [23], and Hickey and Cohen [17]. Abstract interpretation can employ symbolic analysis to support an accurate metric for program performance, which also takes into account memory management issues such as type of memory accesses in computer architectures with hierarchical memories.
Reference: [30] <author> M. J. Wolfe. </author> <title> Techniques for improving the inherent parallelism in programs. </title> <type> Technical Report 78-929, </type> <institution> Department of Computer Science, </institution> <month> July </month> <year> 1978. </year>
Reference-contexts: However, the compiler can find solutions of the recurrences in terms of the loop index variables, and substitute the solutions for the uses of the induction variables. This transformation, called induction variable substitution has been used in paral-lelizing compilers for a number of years <ref> [30] </ref>, with varying degrees of effectiveness.
Reference: [31] <author> M. J. Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <address> San Francisco, California, </address> <month> June 17-19 </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: This class of variables are called generalized induction variables <ref> [9, 16, 31] </ref>. Programs ADM, MDG, MG3D, OCEAN, SPEC77, and TRFD from Perfect Benchmarks rfl are but some examples that contain such cases.
References-found: 31

