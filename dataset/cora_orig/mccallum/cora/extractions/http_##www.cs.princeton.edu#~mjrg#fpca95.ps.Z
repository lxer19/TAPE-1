URL: http://www.cs.princeton.edu/~mjrg/fpca95.ps.Z
Refering-URL: http://www.cs.princeton.edu/~mjrg/
Root-URL: http://www.cs.princeton.edu
Email: fmjrg,appelg@cs.princeton.edu  
Title: Cache Performance of Fast-Allocating Programs  
Author: Marcelo J. R. Gonc~alves and Andrew W. Appel 
Keyword: Topics: 4 benchmarks, performance analysis; 21 hardware design, measurements; 17 garbage collection, storage allocation; 46 runtime systems.  
Date: June 1995.  
Note: To appear in the Proceedings of the 7th Conference on Functional Programming Languages and Computer Architecture,  
Affiliation: Princeton University Department of Computer Science  
Abstract: We study the cache performance of a set of ML programs, compiled by the Standard ML of New Jersey compiler. We find that more than half of the reads are for objects that have just been allocated. We also consider the effects of varying software (garbage collection frequency) and hardware (cache) parameters. Confirming results of related experiments, we found that ML programs can have good cache performance when there is no penalty for allocation. Even on caches that have an allocation penalty, we found that ML programs can have lower miss ratios than the C and Fortran SPEC92 benchmarks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Andrew W. Appel. </author> <title> Simple generational garbage collection and fast allocation. </title> <journal> Software|Practice and Experience, </journal> <volume> 19(2) </volume> <pages> 171-83, </pages> <year> 1989. </year>
Reference-contexts: SML/NJ allocates so frequently because it allocates function activation records (closures) on the heap, instead of using a stack as do most language implementations [2]. In order to make heap allocation efficient it is essential that both allocation and deallocation be very fast <ref> [1] </ref>. Efficient allocation can be achieved by allocating sequentially from a large contiguous memory area, the allocation space. We only need to keep two pointers, one to the next free address, the allocation pointer, and one to the last usable address, the limit pointer. <p> Thus, prefetching avoids secondary cache misses in our measurement, and only avoids primary cache misses in Necula's. * We use (for this measurement) SML/NJ 0.93 with a two-generation garbage collector <ref> [1] </ref>; Necula uses a more recent version of the compiler with improved instruction schedul ing and the multi-generation collector. * We use a DEC 3000/400, he uses a DEC 3000/600. The DEC Alpha 21164 allows up to six outstanding memory references at a time (including cache misses) without stalling.
Reference: [2] <author> Andrew W. Appel. </author> <title> Compiling with Continuations. </title> <publisher> Cam-bridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: SML/NJ allocates so frequently because it allocates function activation records (closures) on the heap, instead of using a stack as do most language implementations <ref> [2] </ref>. In order to make heap allocation efficient it is essential that both allocation and deallocation be very fast [1]. Efficient allocation can be achieved by allocating sequentially from a large contiguous memory area, the allocation space. <p> Version 1.05a also uses the newest version of the multigenerational collector, which uses a mark-and-sweep collector for large objects. The benchmark programs are described in Table 2. Some or all of these programs have been used as benchmark programs in other works on the performance of SML/NJ programs <ref> [2, 34, 14, 36] </ref>. Each of these programs is run in its entirety by the simulator. Four of the programs|Barnes-Hut, Mandelbrot, Ray, and Simple|use floating-point intensively; the other six use only integer instructions.
Reference: [3] <author> Andrew W. Appel and David B. MacQueen. </author> <title> Standard ML of New Jersey. </title> <editor> In Martin Wirsing, editor, </editor> <booktitle> Third Int'l Symp. on Prog. Lang. Implementation and Logic Programming, </booktitle> <pages> pages 1-13, </pages> <address> New York, </address> <month> August </month> <year> 1991. </year> <note> Springer-Verlag. </note>
Reference-contexts: This work studies the cache performance of programs with intensive heap allocation and generational garbage collection, such as ML programs compiled by the Standard ML of New Jersey (SML/NJ) compiler <ref> [3] </ref>. Dynamic heap allocation is used in the implementation of many programming languages, but the SML/NJ implementation is characterized by a much higher allocation rate than most other implementations: a typical program allocates one word for every six machine instructions.
Reference: [4] <author> Andrew W. Appel, James S. Mattson, and David R. Tarditi. </author> <title> A lexical analyzer. Distributed with Standard ML of New Jersey, </title> <month> December </month> <year> 1989. </year>
Reference-contexts: Reppy. Boyer 910 An ML implementation of the Boyer benchmark [19]. Knuth-Bendix 580 An implementation by Gerard Huet of the Knuth-Bendix completion algorithm translated into ML by Xavier Leroy. Lexgen 1178 A lexical-analyzer generator, written by James S. Mattson and David R. Tarditi <ref> [4] </ref>, processing the lexical description of Standard ML. Life 140 Reade's implementation of the game of Life [30]. Mandelbrot 60 A program to generate Mandelbrot sets. MLYACC 7422 A LALR (1) parser generator, written by David Tarditi [37], processing the grammar of Standard ML.
Reference: [5] <author> Joshua Barnes and Piet Hut. </author> <title> Error analysis of a tree code. </title> <journal> Astrophysical Journal Supplement, </journal> <volume> 389(70). </volume>
Reference-contexts: WV = Explicit alloc not needed with write-validate policy or with 1-word cache line. Table 2: General information about the benchmark programs. Program Lines Description Barnes-Hut 1060 The Barnes-Hut N-body simulation program <ref> [6, 5] </ref>, translated into ML by John H. Reppy. Boyer 910 An ML implementation of the Boyer benchmark [19]. Knuth-Bendix 580 An implementation by Gerard Huet of the Knuth-Bendix completion algorithm translated into ML by Xavier Leroy. Lexgen 1178 A lexical-analyzer generator, written by James S. Mattson and David R.
Reference: [6] <author> Joshua Barnes and Piet Hut. </author> <title> A hierarchical O(N log N) force calculation algorithm. </title> <booktitle> Nature, </booktitle> <pages> 446(324). </pages>
Reference-contexts: WV = Explicit alloc not needed with write-validate policy or with 1-word cache line. Table 2: General information about the benchmark programs. Program Lines Description Barnes-Hut 1060 The Barnes-Hut N-body simulation program <ref> [6, 5] </ref>, translated into ML by John H. Reppy. Boyer 910 An ML implementation of the Boyer benchmark [19]. Knuth-Bendix 580 An implementation by Gerard Huet of the Knuth-Bendix completion algorithm translated into ML by Xavier Leroy. Lexgen 1178 A lexical-analyzer generator, written by James S. Mattson and David R.
Reference: [7] <author> M. C. Becker, M. S. Allen, C. R. Moore, J. S. Muhich, and D. P. Tuttle. </author> <title> The PowerPC 601 microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 13(5) </volume> <pages> 54-68, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: DEC3000/500 1992 8K I (L1) 32 d-m Uses an Alpha 21064 CPU. [15] 8K D (L1) 32 d-m no no" y 512K (L2) 32 d-m yes yes] y ]Fetch-on-write. PowerPC 601 1993 32K 64 8-way yes nox yesz On-chip first level cache. LRU replacement. zCache-line-allocate-and-zero instruc tion. <ref> [7] </ref> xNon-blocking fetch-on-write [23]. PowerPC 603 1993 8K I 8K D 32 2-way yes nox yesz On-chip first level cache. LRU replacement. [8] PowerPC 604 1994 16K I 16K D 32 4-way yes nox yesz On-chip first level cache.
Reference: [8] <author> Brad Burgess, Nasr Ullah, Peter van Overen, and Deene ogden. </author> <title> The PowerPC 603 microprocessor. </title> <journal> Communications of the ACM, </journal> <volume> 37(6) </volume> <pages> 34-42, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: PowerPC 601 1993 32K 64 8-way yes nox yesz On-chip first level cache. LRU replacement. zCache-line-allocate-and-zero instruc tion. [7] xNon-blocking fetch-on-write [23]. PowerPC 603 1993 8K I 8K D 32 2-way yes nox yesz On-chip first level cache. LRU replacement. <ref> [8] </ref> PowerPC 604 1994 16K I 16K D 32 4-way yes nox yesz On-chip first level cache. LRU replace ment. [35] Pentium 1993 8K I 8K D 32 2-way no no no On-chip first level cache.
Reference: [9] <author> Marcelo J. R. Conc~alves. </author> <title> Cache Performance of Programs with Intensive Heap Allocation and Generational Garbage Collection. </title> <type> PhD thesis, </type> <institution> Princeton University, Princeton, NJ, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: On machines where these features have an impact on performance, the actual break-even miss penalties is likely to be a little higher than the values shown here. In this paper we have discussed only split I/D caches, but Gonc~alves's Ph.D. thesis <ref> [9] </ref> analyzes both split and unified caches. We have shown measurements for many sizes of single-level cache. Most modern machines have two or three levels of cache. We believe that most of our results are applicable to these machines, taken one cache at a time.
Reference: [10] <author> Intel Corporation. </author> <title> Pentium Processor User's Manual, Volume 2: 82496 Cache Controller and 82491 Cache SRAM Data Book. Intel Literature Sales, Mt. </title> <type> Prospect, </type> <institution> Illinois, </institution> <year> 1993. </year>
Reference-contexts: LRU replacement. [8] PowerPC 604 1994 16K I 16K D 32 4-way yes nox yesz On-chip first level cache. LRU replace ment. [35] Pentium 1993 8K I 8K D 32 2-way no no no On-chip first level cache. Pentium systems usu ally have a 256K second level cache. <ref> [10] </ref> Intel P6 1995 8K I (L1) 8K D (L1) 32 2-way 4-way ? ? fetch- L2 cache on a separate die, packaged with the CPU.
Reference: [11] <author> W. Crowley, C. Hendrickson, and T. Rudy. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID 17715, </type> <institution> Lawrence Livermore Laboratory, Livermore, </institution> <address> CA, </address> <month> February </month> <year> 1978. </year>
Reference-contexts: Mandelbrot 60 A program to generate Mandelbrot sets. MLYACC 7422 A LALR (1) parser generator, written by David Tarditi [37], processing the grammar of Standard ML. Ray 423 A ray tracer, written by Don Mitchell, and translated into ML by John H. Reppy. Simple 906 A spherical fluid-dynamics program <ref> [11, 16] </ref>, translated into ML by Lal George. VLIW 3571 A VLIW instruction scheduler written by John Danskin. 3 Table 3: Total, user, garbage collection, and system times in sec-onds for the benchmark programs, measured on a DEC5000/240 workstation, with an allocation space of 512K bytes.
Reference: [12] <institution> Digital Equipment Corporation, Maynard, </institution> <month> Massachussets. </month> <title> DECchip 21064 | AA Microprocessor Hardware Reference Manual, </title> <booktitle> first edition, </booktitle> <month> October </month> <year> 1992. </year> <title> Order number EC-N0079-72. </title>
Reference-contexts: D 4 d-m yes no WV Fetches 8 blocks (32 bytes) on a read miss. [13] Alpha 21064 1992 8K I 8K D 32 d-m no no" y On-chip first level cache, with on-chip control for second level cache. yLoad instruction (for explicit cache line allocation by prefetch) "semi-stalls." "Write-around. <ref> [12] </ref> Alpha 21164 1994 8K I (L1) 8K D (L1) 32 32/64 d-m no no" fetch On-chip control for a third level direct-mapped cache. Load instruction, usable for explicit cache line allocation by prefetch, is non blocking.
Reference: [13] <institution> Digital Equipment Corporation, Palo Alto, California. </institution> <note> DEC--station and DECsystem 5000 Model 240 Technical Overview, version 2 edition, </note> <month> February </month> <year> 1992. </year> <title> Order number EC-N0194-51. </title>
Reference-contexts: Machine/ Year Cache Block Assoc. Write Write Expli- Comments/references CPU size size alloc. Miss cit (bytes) penalty Alloc. DEC5000/240 1990 64K-byte I 64K D 4 d-m yes no WV Fetches 8 blocks (32 bytes) on a read miss. <ref> [13] </ref> Alpha 21064 1992 8K I 8K D 32 d-m no no" y On-chip first level cache, with on-chip control for second level cache. yLoad instruction (for explicit cache line allocation by prefetch) "semi-stalls." "Write-around. [12] Alpha 21164 1994 8K I (L1) 8K D (L1) 32 32/64 d-m no no" fetch
Reference: [14] <author> Amer Diwan, David Tarditi, and Eliot Moss. </author> <title> Memory subsystem performance of programs with copying garbage collection. </title> <booktitle> In Proceedings of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Language, </booktitle> <pages> pages 1-13, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: This means that heap allocated closures can have good locality; on suitable cache architectures, stack allocation should not be preferred over heap allocation just for reasons of locality. Diwan et al. <ref> [14] </ref> suggest that ML programs tend to read objects soon after they have been written, based on indirect evidence from cache behavior on different architectures, and we have confirmed this by direct experiment. * Varying minor garbage collection frequency (i.e., changing the size of allocation space) may have an impact on <p> More recent work, by Koopman et al [27], Diwan et al. <ref> [14] </ref> and Reinhold [31, 32] has shown that some cache design features, already available on current machines, can eliminate all of the allocation misses. Moreover, Reinhold shows that sequential allocation, due to the fact that it tends to spread memory references uniformly across memory, is naturally suited to direct-mapped caches. <p> Version 1.05a also uses the newest version of the multigenerational collector, which uses a mark-and-sweep collector for large objects. The benchmark programs are described in Table 2. Some or all of these programs have been used as benchmark programs in other works on the performance of SML/NJ programs <ref> [2, 34, 14, 36] </ref>. Each of these programs is run in its entirety by the simulator. Four of the programs|Barnes-Hut, Mandelbrot, Ray, and Simple|use floating-point intensively; the other six use only integer instructions. <p> This curve is shown to give an idea of the fraction of misses that are eliminated by write-validate and write-around caches. Write-validate is the best organization for fast-allocating programs, a result already shown by Koopman et al. [27] and Diwan et al. <ref> [14] </ref>. It eliminates all of the write misses without adding many read misses, as can been seen from the comparison with the read miss ratio of fetch-on-write caches. On the write-around caches, on the other hand, most write misses of the fetch-on-write cache simply become read misses.
Reference: [15] <author> Todd A. Dutton, Daniel Eiref, Hugh R. Kurth, James J. Reis-ert, and Robin L. Stewart. </author> <title> The design of the DEC3000 AXP systems, two high-performance workstations. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4), </volume> <year> 1992. </year>
Reference-contexts: Load instruction, usable for explicit cache line allocation by prefetch, is non blocking. DEC3000/500 1992 8K I (L1) 32 d-m Uses an Alpha 21064 CPU. <ref> [15] </ref> 8K D (L1) 32 d-m no no" y 512K (L2) 32 d-m yes yes] y ]Fetch-on-write. PowerPC 601 1993 32K 64 8-way yes nox yesz On-chip first level cache. LRU replacement. zCache-line-allocate-and-zero instruc tion. [7] xNon-blocking fetch-on-write [23]. <p> Only in a contrived configuration (fetch-on-write 500-cycle-miss 8k primary cache, write-validate secondary cache) would our data give contradictory recommendations (keep allocation space in primary cache but not secondary). 7 A case study: the DEC3000 Alpha workstation The DEC3000/500 Alpha workstation <ref> [15] </ref> uses a DEC Alpha 21064 microprocessor that has 8-Kbyte on-chip instruction and data caches, and support for a second level cache. The first level data cache is write-around, and write misses are non-blocking. <p> We computed the probability of a write buffer being flushed because of a read to a partially filled block. For the benchmark programs, on average, 74% of the write buffers would be flushed. Considering a penalty of 27 cycles to fetch a block from memory <ref> [15] </ref>, this should add a cost of 0.43 CPUI (cycles per user instruction). 4 Keeping the allocation space in the cache can eliminate the memory fetch penalty, but it increases garbage collection time. On average, the increased garbage-collection time causes a penalty of about 4% in performance.
Reference: [16] <author> K. Ekanadham and Arvind. </author> <title> SIMPLE: An exercise in future scientific programming. Technical Report Computation Structures Group Memo 273, </title> <publisher> MIT, </publisher> <address> Cambridge, MA, </address> <month> July </month> <year> 1987. </year> <note> Simultaneously published as IBM/T.J. </note> <institution> Watson Research Center Research Report 12686, Yorktown Heights, NY. </institution>
Reference-contexts: Mandelbrot 60 A program to generate Mandelbrot sets. MLYACC 7422 A LALR (1) parser generator, written by David Tarditi [37], processing the grammar of Standard ML. Ray 423 A ray tracer, written by Don Mitchell, and translated into ML by John H. Reppy. Simple 906 A spherical fluid-dynamics program <ref> [11, 16] </ref>, translated into ML by Lal George. VLIW 3571 A VLIW instruction scheduler written by John Danskin. 3 Table 3: Total, user, garbage collection, and system times in sec-onds for the benchmark programs, measured on a DEC5000/240 workstation, with an allocation space of 512K bytes.
Reference: [17] <author> Jean-Marc Frailong et al. </author> <title> The next generation SPARC multiprocessing system architecture. </title> <booktitle> In Proceedings of COMP-CON, </booktitle> <pages> pages 475-480, </pages> <address> San Francisco, California, February 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: For prefetch, 4 concurrent L2-cache ac cesses. [22] SuperSPARC 1993 20K I (L1) 16K D (L1) 4 4-way yes no WV On-chip first level cache. LRU replace ment. <ref> [17] </ref> HPPA-RISC 1992 4K-1M I 4K-4M D 32 d-m yes no WV On-chip support to dual off-chip caches. Cache size is system dependent. [18] I = instruction cache; D = data cache; d-m = direct-mapped cache; L1/L2 = first/second-level cache; ? = unknown.
Reference: [18] <author> Tom Asprey et el. </author> <title> Performance features of the PA7100 microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 22-35, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: LRU replace ment. [17] HPPA-RISC 1992 4K-1M I 4K-4M D 32 d-m yes no WV On-chip support to dual off-chip caches. Cache size is system dependent. <ref> [18] </ref> I = instruction cache; D = data cache; d-m = direct-mapped cache; L1/L2 = first/second-level cache; ? = unknown. WV = Explicit alloc not needed with write-validate policy or with 1-word cache line. Table 2: General information about the benchmark programs.
Reference: [19] <author> Richard P. Gabriel. </author> <title> Performance and Evaluation of Lisp Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: Table 2: General information about the benchmark programs. Program Lines Description Barnes-Hut 1060 The Barnes-Hut N-body simulation program [6, 5], translated into ML by John H. Reppy. Boyer 910 An ML implementation of the Boyer benchmark <ref> [19] </ref>. Knuth-Bendix 580 An implementation by Gerard Huet of the Knuth-Bendix completion algorithm translated into ML by Xavier Leroy. Lexgen 1178 A lexical-analyzer generator, written by James S. Mattson and David R. Tarditi [4], processing the lexical description of Standard ML.
Reference: [20] <author> Jeffrey D. Gee, Mark D. Hill, Dionisios N. Pnevmatikatos, and Alan J. Smith. </author> <title> Cache performance of the SPEC92 benchmark suite. </title> <journal> IEEE Micro, </journal> <volume> 13(4) </volume> <pages> 17-27, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Table 3 shows the run time of the programs, measured on a DECstation 5000/240, and time breakdowns by user code, garbage collection (GC) time, and system time. We ran each program a number of times, and took the run with minimum total time (as recommended by the SPEC consortium <ref> [20] </ref>). Table 4 shows instruction counts and number of reads and writes for user and garbage collection code. Garbage collection time and instruction counts can vary considerably if the size of the allocation space is changed.
Reference: [21] <author> Marcelo J. R. Gonc~alves. </author> <title> Cache Performance of Programs with Intensive Allocation and Generational Garbage Colec-tion. </title> <type> PhD thesis, </type> <institution> Princeton University, Department of Computer Science, </institution> <year> 1995. </year> <note> (In preparation). </note>
Reference-contexts: 20.06 33.95 1.57 Knuth-Bendix 59.50 36.13 4.09 0.28 Lexgen 35.34 45.47 7.55 11.64 Life 45.17 42.84 11.70 0.29 Mandelbrot 30.64 30.87 38.32 0.17 MLYACC 52.49 31.88 11.77 3.86 Ray 23.00 50.60 23.40 3.00 Simple 42.86 30.51 26.39 0.24 VLIW 51.15 27.94 15.86 5.05 Average 41.50 36.87 18.71 2.91 data structures <ref> [21, 36] </ref>. All objects are preceded by a one-word descriptor, which is included in the size of the objects. In most cases, descriptors are only used by the garbage collector, and since most objects do not survive to a garbage collection, most descriptors are never used. <p> We report read and write miss ratios separately. Due to space limitations, we report only the weighted arithmetic mean of miss ratios of the 10 benchmark programs 2 . The complete set of miss ratios is available in <ref> [21] </ref>. caches, with varying block sizes. The allocation space is 512K bytes, which explains the sharp drop in write miss ratios for caches of 512K bytes or more.
Reference: [22] <author> Tom R. </author> <title> Halfhill. </title> <journal> Intel's P6. Byte, </journal> <volume> 20(4) </volume> <pages> 42-58, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Pentium systems usu ally have a 256K second level cache. [10] Intel P6 1995 8K I (L1) 8K D (L1) 32 2-way 4-way ? ? fetch- L2 cache on a separate die, packaged with the CPU. For prefetch, 4 concurrent L2-cache ac cesses. <ref> [22] </ref> SuperSPARC 1993 20K I (L1) 16K D (L1) 4 4-way yes no WV On-chip first level cache. LRU replace ment. [17] HPPA-RISC 1992 4K-1M I 4K-4M D 32 d-m yes no WV On-chip support to dual off-chip caches.
Reference: [23] <author> William R. Hardell, Dwain A. Hicks, Lawrence C. How-ell, Warren E. Maule, Robert Montoye, and David P. Tuttle. </author> <title> Data cache and storage control units. </title> <booktitle> In IBM RISC System/6000 Technology, </booktitle> <pages> pages 44-50. </pages> <institution> IBM, </institution> <year> 1990. </year>
Reference-contexts: PowerPC 601 1993 32K 64 8-way yes nox yesz On-chip first level cache. LRU replacement. zCache-line-allocate-and-zero instruc tion. [7] xNon-blocking fetch-on-write <ref> [23] </ref>. PowerPC 603 1993 8K I 8K D 32 2-way yes nox yesz On-chip first level cache. LRU replacement. [8] PowerPC 604 1994 16K I 16K D 32 4-way yes nox yesz On-chip first level cache. <p> Some machines (such as the IBM R/S 6000 <ref> [23] </ref>) have an instruction to allocate (and zero) a specified cache line. If this is done in advance of writing to the line, no allocate miss will occur. <p> Full-line write-allocate does not guarantee sequential consistency, and neither does write-around. Obtaining sequential consistency is difficult in high-performance protocols, so many machines (such as the Alpha) do not guarantee sequential consistency unless a "memory barrier" instruction is executed. The IBM RS/6000 <ref> [23] </ref> (and perhaps Power-PC) has a "cache reload buffer" that (among other things) serves as a write buffer, satisfies read requests directly, implements non-blocking fetch-on-write, and puts full lines into the cache.
Reference: [24] <author> Norman P. Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 191-201, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Moreover, Reinhold shows that sequential allocation, due to the fact that it tends to spread memory references uniformly across memory, is naturally suited to direct-mapped caches. Following Jouppi <ref> [24] </ref> we classify cache architectures as follows. A fetch-on-write cache is one that allocates a block and fetches the contents of the block from memory on a write miss. <p> If block sizes are not too small, miss ratios of ML programs can be lower than that of SPEC programs. 5.1 Write-validate and write-around caches Write-validate caches always, and write-around caches usually, outperform fetch-on-write caches <ref> [24] </ref>. Write-validate and write-around avoid fetching data on write misses, but they may have to fetch data later on a read. <p> Even for "conventional" C programs, it is well known <ref> [24] </ref> that write-validate gives significantly better performance than write-around.
Reference: [25] <author> Alan H. Karp and Rajiv Gupta. </author> <title> Hardware support for data merging. </title> <booktitle> In Proceedings Intl. Parallel Proc. Symp., </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: One way to solve the problem is to have the cache-coherence controller merge the valid portions of lines in P 's and Q's caches. Merging has been demonstrated at the virtual-memory page level in software [26], and at the cache level in hardware <ref> [25] </ref>. If merging can be done cheaply, then we regard it as a good solution|any technique that allows write-validate is acceptable. But we now propose another solution that may be easier to implement than merging.
Reference: [26] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: One way to solve the problem is to have the cache-coherence controller merge the valid portions of lines in P 's and Q's caches. Merging has been demonstrated at the virtual-memory page level in software <ref> [26] </ref>, and at the cache level in hardware [25]. If merging can be done cheaply, then we regard it as a good solution|any technique that allows write-validate is acceptable. But we now propose another solution that may be easier to implement than merging.
Reference: [27] <author> Philip J. Koopman, Jr., Peter Lee, and Daniel P. Siewiorek. </author> <title> Cache behavior of combinator graph reduction. </title> <journal> ACM TOPLAS, </journal> <volume> 14(2) </volume> <pages> 265-297, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: In essence, the main conclusion of these papers is that programs with dynamic heap allocation tend to have bad cache performance|and either hardware techniques, or software techniques, or a combination of both, must be used in order to improve cache performance. More recent work, by Koopman et al <ref> [27] </ref>, Diwan et al. [14] and Reinhold [31, 32] has shown that some cache design features, already available on current machines, can eliminate all of the allocation misses. <p> This curve is shown to give an idea of the fraction of misses that are eliminated by write-validate and write-around caches. Write-validate is the best organization for fast-allocating programs, a result already shown by Koopman et al. <ref> [27] </ref> and Diwan et al. [14]. It eliminates all of the write misses without adding many read misses, as can been seen from the comparison with the read miss ratio of fetch-on-write caches. <p> Koopman <ref> [27] </ref> found that this improved the performance of combinator graph reduction by "up to 20%" on the VAX 8800. We have implemented this technique on the DEC Alpha 21064.
Reference: [28] <author> George Necula. </author> <type> Personal communication, </type> <year> 1995. </year>
Reference-contexts: Using the improved scheduler, with and without prefetching, prefetching causes a 1.5% increase in instructions executed, and a 4.5% decrease in total cycles on a DEC 3000/600 <ref> [28] </ref>. The difference between Necula's 4.5% and our 18% can be explained as follows: * Our measurement is for an allocation space much larger than the cache, Necula's allocation space is the same size as the secondary cache (512k) as recommended in earlier sections of this paper.
Reference: [29] <author> Chih-Jui Peng and Gurindar S. Sohi. </author> <title> Cache memory design considerations to support languages with dynamic heap allocation. </title> <type> Technical Report 860, </type> <institution> Computer Science Department, University of Wisconsin-Madison, </institution> <year> 1989. </year>
Reference-contexts: We found that a large number of active generations can increase the number of conflict misses. The problem of the cache performance of programs with sequential allocation has been addressed previously. Peng and Sohi <ref> [29] </ref> studied the cache performance of a set of Lisp programs. They show that conventional cache memories were inadequate for these programs and proposed the use of an allocate instruction to eliminate allocation misses.
Reference: [30] <author> Chris Reade. </author> <title> Elements of Functional Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Knuth-Bendix 580 An implementation by Gerard Huet of the Knuth-Bendix completion algorithm translated into ML by Xavier Leroy. Lexgen 1178 A lexical-analyzer generator, written by James S. Mattson and David R. Tarditi [4], processing the lexical description of Standard ML. Life 140 Reade's implementation of the game of Life <ref> [30] </ref>. Mandelbrot 60 A program to generate Mandelbrot sets. MLYACC 7422 A LALR (1) parser generator, written by David Tarditi [37], processing the grammar of Standard ML. Ray 423 A ray tracer, written by Don Mitchell, and translated into ML by John H. Reppy.
Reference: [31] <author> Mark B. Reinhold. </author> <title> Cache performance of garbage-collected programming languages. </title> <type> Technical Report 581, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: More recent work, by Koopman et al [27], Diwan et al. [14] and Reinhold <ref> [31, 32] </ref> has shown that some cache design features, already available on current machines, can eliminate all of the allocation misses. Moreover, Reinhold shows that sequential allocation, due to the fact that it tends to spread memory references uniformly across memory, is naturally suited to direct-mapped caches.
Reference: [32] <author> Mark B. Reinhold. </author> <title> Cache performance of garbage-collected programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN'94 Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: More recent work, by Koopman et al [27], Diwan et al. [14] and Reinhold <ref> [31, 32] </ref> has shown that some cache design features, already available on current machines, can eliminate all of the allocation misses. Moreover, Reinhold shows that sequential allocation, due to the fact that it tends to spread memory references uniformly across memory, is naturally suited to direct-mapped caches.
Reference: [33] <author> John H. Reppy. </author> <title> A high-performance garbage collector for Standard ML. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <year> 1994. </year>
Reference-contexts: Table 1 describes the basic features of the caches of some current machines. 2 The SML/NJ Runtime System The SML/NJ runtime system uses a generational garbage collector. The two-generation collector used up to version 0.93 proved inadequate on long-running programs, so John Reppy has implemented a multi-generation collector <ref> [33] </ref>. Our measurements reported here are of an early prototype of Reppy's collector, which will be distributed with future versions of SML/NJ. The heap is divided into an allocation space and from one to seven generations. <p> A kth-generation major collection promotes the older objects of each generation i into generation i + 1, for 1 i k; a fixed number of kth-generation collections occur for each (k + 1)th-generation collection. The early prototype we measured lacks certain features of the collector Reppy describes <ref> [33] </ref>, particularly the use of a mark-and-sweep collector for machine code and other big objects.
Reference: [34] <author> Zhong Shao and Andrew Appel. </author> <title> Space-efficient closure representations. </title> <booktitle> In Proceedings of the 1994 ACM Conference on Lisp and Functional Programming, </booktitle> <year> 1994. </year>
Reference-contexts: We also ran some experiments on an DEC3000 Alpha workstation. For these experiments we used SML/NJ version 1.05a, since version 0.93 was not ported to the Alpha architecture. The main difference in version 1.05 from version 0.93 is the use of more efficient closure representations <ref> [34] </ref>, which reduces the amount of closure allocation. Version 1.05a also uses the newest version of the multigenerational collector, which uses a mark-and-sweep collector for large objects. The benchmark programs are described in Table 2. <p> Version 1.05a also uses the newest version of the multigenerational collector, which uses a mark-and-sweep collector for large objects. The benchmark programs are described in Table 2. Some or all of these programs have been used as benchmark programs in other works on the performance of SML/NJ programs <ref> [2, 34, 14, 36] </ref>. Each of these programs is run in its entirety by the simulator. Four of the programs|Barnes-Hut, Mandelbrot, Ray, and Simple|use floating-point intensively; the other six use only integer instructions.
Reference: [35] <author> S. Peter Song, Marvin Denman, and Joe Chang. </author> <title> The Pow-erPC 604 risc microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 14(5) </volume> <pages> 8-17, </pages> <month> Oc-tober </month> <year> 1994. </year>
Reference-contexts: LRU replacement. zCache-line-allocate-and-zero instruc tion. [7] xNon-blocking fetch-on-write [23]. PowerPC 603 1993 8K I 8K D 32 2-way yes nox yesz On-chip first level cache. LRU replacement. [8] PowerPC 604 1994 16K I 16K D 32 4-way yes nox yesz On-chip first level cache. LRU replace ment. <ref> [35] </ref> Pentium 1993 8K I 8K D 32 2-way no no no On-chip first level cache.
Reference: [36] <author> Darko Stefanovic and J. E. B. Moss. </author> <title> Characterization of object behaviour in Standard ML of New Jersey. </title> <booktitle> In Proceedings of the 1994 ACM Conference on Lisp and Functional Programming, </booktitle> <address> Orlando, Florida, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Version 1.05a also uses the newest version of the multigenerational collector, which uses a mark-and-sweep collector for large objects. The benchmark programs are described in Table 2. Some or all of these programs have been used as benchmark programs in other works on the performance of SML/NJ programs <ref> [2, 34, 14, 36] </ref>. Each of these programs is run in its entirety by the simulator. Four of the programs|Barnes-Hut, Mandelbrot, Ray, and Simple|use floating-point intensively; the other six use only integer instructions. <p> 20.06 33.95 1.57 Knuth-Bendix 59.50 36.13 4.09 0.28 Lexgen 35.34 45.47 7.55 11.64 Life 45.17 42.84 11.70 0.29 Mandelbrot 30.64 30.87 38.32 0.17 MLYACC 52.49 31.88 11.77 3.86 Ray 23.00 50.60 23.40 3.00 Simple 42.86 30.51 26.39 0.24 VLIW 51.15 27.94 15.86 5.05 Average 41.50 36.87 18.71 2.91 data structures <ref> [21, 36] </ref>. All objects are preceded by a one-word descriptor, which is included in the size of the objects. In most cases, descriptors are only used by the garbage collector, and since most objects do not survive to a garbage collection, most descriptors are never used.
Reference: [37] <author> David R. Tarditi and Andrew W. Appel. ML-Yacc, </author> <title> version 2.0. Distributed with Standard ML of New Jersey, </title> <month> April </month> <year> 1990. </year>
Reference-contexts: Mattson and David R. Tarditi [4], processing the lexical description of Standard ML. Life 140 Reade's implementation of the game of Life [30]. Mandelbrot 60 A program to generate Mandelbrot sets. MLYACC 7422 A LALR (1) parser generator, written by David Tarditi <ref> [37] </ref>, processing the grammar of Standard ML. Ray 423 A ray tracer, written by Don Mitchell, and translated into ML by John H. Reppy. Simple 906 A spherical fluid-dynamics program [11, 16], translated into ML by Lal George.
Reference: [38] <author> Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. </author> <title> Caching considerations for generational garbage collection. </title> <booktitle> In Proceedings of the 1992 Conference on Lisp and Functional Programming, </booktitle> <pages> pages 32-42, </pages> <address> San Francisco, Califor-nia, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: They also show that the LRU replacement policy commonly used in associative caches is bad for this class of programs and proposed an alternative replacement policy. Finally they proposed the use of a control bit to avoid writing back cache blocks filled with garbage. Wilson et al. <ref> [38] </ref> and Zorn [40, 39] proposed fitting the allocation space in the cache as an alternative, software-based method for eliminating allocation misses on large caches. Wilson et al. also show that modestly set-associative caches can achieve a significant performance improvement over direct mapped caches. <p> Associativity has unusual results for the case where the cache matches allocation space size (512K bytes). At this point there is an inversion, with higher miss ratios for higher associativity. Wilson et al. observed something similar <ref> [38] </ref> in their study of Lisp programs. The problem is that the LRU replacement policy of the associative caches is not good for cyclic reference patterns. For a 512K-byte cache there should be no allocation misses, because the allocation space is also 512K bytes.
Reference: [39] <author> Benjamin Zorn. </author> <title> Comparative Performance Evaluation of Garbage Collection Algorithms. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, EECS Department, </institution> <year> 1989. </year> <note> Technical Report UCB/CSD 89/544. </note>
Reference-contexts: Finally they proposed the use of a control bit to avoid writing back cache blocks filled with garbage. Wilson et al. [38] and Zorn <ref> [40, 39] </ref> proposed fitting the allocation space in the cache as an alternative, software-based method for eliminating allocation misses on large caches. Wilson et al. also show that modestly set-associative caches can achieve a significant performance improvement over direct mapped caches.
Reference: [40] <author> Benjamin Zorn. </author> <title> The effect of garbage collection on cache performance. </title> <type> Technical Report CU-CS-528-91, </type> <institution> Department of Computer Science, University of Colorado, Boulder, Boulder, Colorado, </institution> <month> May </month> <year> 1991. </year> <month> 13 </month>
Reference-contexts: Finally they proposed the use of a control bit to avoid writing back cache blocks filled with garbage. Wilson et al. [38] and Zorn <ref> [40, 39] </ref> proposed fitting the allocation space in the cache as an alternative, software-based method for eliminating allocation misses on large caches. Wilson et al. also show that modestly set-associative caches can achieve a significant performance improvement over direct mapped caches.
References-found: 40

