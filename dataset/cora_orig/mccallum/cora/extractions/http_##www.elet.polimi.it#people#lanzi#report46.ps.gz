URL: http://www.elet.polimi.it/people/lanzi/report46.ps.gz
Refering-URL: http://www.elet.polimi.it/people/lanzi/listpub.html
Root-URL: 
Title: Model of the Environment to Avoid Local Learning  
Note: A  
Address: Piazza Leonardo da Vinci 32 I-20133 Milano Italia  
Affiliation: Dipartimento di Elettronica e Informazione Politecnico di Milano  
Abstract: Pier Luca Lanzi Technical Report N. 97.46 December 20 th , 1997 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> P.V.C. Caironi and Marco Dorigo. </editor> <title> Training and delayed reinforcements in q-learning. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 12(10), </volume> <year> 1997. </year>
Reference-contexts: Wilson proposes a different solution in which random exploration, usually employed in XCS, is replaced with biased exploration (also pseudorandom exploration according to <ref> [1] </ref>), which works as follows. When in exploration, the animat decides with a certain probability (P s ) whether to select the action randomly or to select the action which predicts the maximum payoff.
Reference: [2] <author> Dave Cliff and Susi Ross. </author> <title> Adding memory to ZCS. Adaptive Behaviour, </title> <booktitle> 3(2) </booktitle> <pages> 101-150, </pages> <year> 1994. </year>
Reference-contexts: First we extend the results presented in [5] comparing the two solutions proposed to counterbalance generalization mechanism: Specify and biased exploration. The comparison is done in two new environments, Maze5 and Maze6, and subsequently in Woods14, the well-known ribbon problem which was firstly introduced by Cliff & Ross in <ref> [2] </ref>. These comparisons are not intended to show the best strategies to solve the proposed problems, accordingly, all the experiments employ standard parameter settings and not specific ones because, even if these could show better performance, they surely would lack in generality. <p> We now extend the results presented in the previous section, analyzing the performance of XCS in an environment which require long sequence of actions for reaching the goal state. For this purpose, we apply different versions of XCS in the Woods14 environment <ref> [2] </ref>. Woods14 (Figure 6) is a very simple environment, which consists of a linear path of 18 blank cells to a food cell and has an expected optimal path to food of 9 steps. the food cell.
Reference: [3] <author> Tim Kovacs. </author> <title> Evolving optimal populations with XCS classifier systems. </title> <institution> Technical Report CSR-96-17 and CSRP-96-17, School of Computer Science, University of Birmingham, Birm-ingham, U.K., </institution> <year> 1996. </year> <note> Avaiable from the technical report archive at http://www/system/tech-reports/tr.html. </note>
Reference: [4] <author> Tim Kovacs. </author> <title> Evolving optimal populations with XCS classifier systems. </title> <institution> Technical Report CSR-96-17 and CSRP-96-17, School of Computer Science, University of Birmingham, Birm-ingham, U.K., </institution> <year> 1996. </year> <note> Available from the technical report archive at http://www/system/tech-reports/tr.html. </note>
Reference-contexts: 1 Introduction Generalization is the most interesting feature of XCS, the classifier system introduced by Wilson [9]. XCS has in fact been proved to evolve near-minimal populations of classifiers that are accurate and maximally general. Recently, Kovacs <ref> [4] </ref> has proposed an optimality hypothesis for XCS and showed experimental evidence of such hypothesis with respect to the function representation problem involving multiplexers. Conversely, for animat problems, [5] presented experimental results showing that XCS may fail to converge to an optimal solution. <p> Nevertheless, as we show in the first part of this paper, it does not guarantee the convergence to an optimal solution in more difficult environments. 1 The analysis of XCS's behavior has always been presented without considering the relation between XCS's performance and the environment structure <ref> [4, 5] </ref>; specifically, it is not clear why an environment is easy to solve while a very similar one can be much more difficult. <p> Finally, Section 10 ends the papers, drawing some conclusions and the directions for future work. 2 Description of XCS We now give a brief review of XCS in its most recent version [10]. We refer the interested reader to [9] for the original XCS description or to Kovacs's report <ref> [4] </ref> for a more detailed discussion for 3 implementors. Classifiers in XCS have three main parameters: the prediction p j , the prediction error " j and the fitness F j . Prediction p j gives an estimate of what is the reward that the classifier is expected to gain. <p> Recently, Kovacs <ref> [4] </ref> has proposed an optimality hypothesis which states that XCS tends to evolve the minimal population with respect to the boolean function representation problem.
Reference: [5] <author> Pier Luca Lanzi. </author> <title> A Study on the Generalization Capabilities of XCS. </title> <booktitle> In Proceedings of the Seventh International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year>
Reference-contexts: XCS has in fact been proved to evolve near-minimal populations of classifiers that are accurate and maximally general. Recently, Kovacs [4] has proposed an optimality hypothesis for XCS and showed experimental evidence of such hypothesis with respect to the function representation problem involving multiplexers. Conversely, for animat problems, <ref> [5] </ref> presented experimental results showing that XCS may fail to converge to an optimal solution. The author observed that, due to a strong genetic pressure, XCS could be unable to recover dangerous situations in which overgeneral classifiers are likely to corrupt the population. [5] relates this phenomenon with the structure of <p> Conversely, for animat problems, <ref> [5] </ref> presented experimental results showing that XCS may fail to converge to an optimal solution. The author observed that, due to a strong genetic pressure, XCS could be unable to recover dangerous situations in which overgeneral classifiers are likely to corrupt the population. [5] relates this phenomenon with the structure of the environment, specifically with amount of generalizations that the problem allows, and suggests it is more likely to happen in environments which allows few generalizations. However, [5] did not discuss the concept of environment which allows few/many generalizations and keep it intuitive. <p> be unable to recover dangerous situations in which overgeneral classifiers are likely to corrupt the population. <ref> [5] </ref> relates this phenomenon with the structure of the environment, specifically with amount of generalizations that the problem allows, and suggests it is more likely to happen in environments which allows few generalizations. However, [5] did not discuss the concept of environment which allows few/many generalizations and keep it intuitive. In order to help XCS to recover from the presence of overgeneral classifiers, [5] introduced a new operator named Specify that counterbalances the effect of generalization when such situations occur. <p> However, <ref> [5] </ref> did not discuss the concept of environment which allows few/many generalizations and keep it intuitive. In order to help XCS to recover from the presence of overgeneral classifiers, [5] introduced a new operator named Specify that counterbalances the effect of generalization when such situations occur. Experimental results confirmed that Specify successfully adapts overgeneral classifiers when the system can be prevented to converge. Wilson [12] observed that the behavior discussed in [5] depends on the amount of random exploration the <p> to recover from the presence of overgeneral classifiers, <ref> [5] </ref> introduced a new operator named Specify that counterbalances the effect of generalization when such situations occur. Experimental results confirmed that Specify successfully adapts overgeneral classifiers when the system can be prevented to converge. Wilson [12] observed that the behavior discussed in [5] depends on the amount of random exploration the agent does; specifically, if the agent wanders around too much in between arrivals at the goal it can fail to reach optimal solution. <p> Biased exploration reduces the amount of random exploration that the animat performs, therefore the animat concentrates on best policies which will reproduce more. Experimental results, not presented here, report that XCS with biased exploration successfully solves the simple problem proposed in <ref> [5] </ref>. <p> Nevertheless, as we show in the first part of this paper, it does not guarantee the convergence to an optimal solution in more difficult environments. 1 The analysis of XCS's behavior has always been presented without considering the relation between XCS's performance and the environment structure <ref> [4, 5] </ref>; specifically, it is not clear why an environment is easy to solve while a very similar one can be much more difficult. <p> The aim of this paper is to answer to such questions in order to get a better understanding of the generalization mechanism of XCS, while giving a unified view of what observed in <ref> [5] </ref> and [12]. First we extend the results presented in [5] comparing the two solutions proposed to counterbalance generalization mechanism: Specify and biased exploration. <p> The aim of this paper is to answer to such questions in order to get a better understanding of the generalization mechanism of XCS, while giving a unified view of what observed in <ref> [5] </ref> and [12]. First we extend the results presented in [5] comparing the two solutions proposed to counterbalance generalization mechanism: Specify and biased exploration. The comparison is done in two new environments, Maze5 and Maze6, and subsequently in Woods14, the well-known ribbon problem which was firstly introduced by Cliff & Ross in [2]. <p> Macroclassifiers are essentially a programming technique that speeds up the learning process reducing the number of real, macro, classifiers XCS has to deal with. Since XCS was presented, two genetic operators have been proposed as extensions to the original system: Subsumption deletion [10] and Specify <ref> [5] </ref>. Subsumption deletion has been introduced to improve generalization capabilities of XCS. Sub-sumption deletion acts when classifiers created by the genetic component have to be inserted in the population. <p> Every statistic presented in this paper is averaged on ten experiments. 4 XCS in Maze5 and Maze6 A first analysis of the generalization capabilities of XCS for animat problems has been presented in <ref> [5] </ref> where the authors observed that XCS may fail to converge to an optimal policy, even for simple problems, when, due to a strong genetic pressure, the generalization mechanism is not able to recover from dangerous situations, in which overgeneral classifiers can corrupt the population. <p> The Specify operator was thus introduced in order to help the generalization mechanism in recovering from overgeneral classifiers. Wilson [12] observed that the behavior discussed in <ref> [5] </ref> also depends on the amount of random exploration the agent does: If the agent wanders around too much in between arrivals at the goal it can fail to converge to optimal performance. <p> We compare four algorithms for each environment: (i) XCS according to the original definition [9] that is, without subsumption deletion; (ii) XCS without don't care symbols (# are not introduced in the initial population, neither in covering or mutation); (iii) XCS with Specify <ref> [5] </ref>, called XCSS; (iv) XCS with biased exploration [12]. The performances of algorithms (i) and (ii) are two important references. <p> Specifically, XCS with biased exploration (algorithm (iv)) converges slower than XCSS (algorithm (iii)). Moreover, experimental results, not reported here, show that sometimes XCS with biased exploration fails to converge even to a suboptimal solution, while XCSS always reaches an optimal solution that is also very stable. <ref> [5] </ref> observed that XCSS is very stable with respect to the population size parameter; we thus apply XCS with biased exploration and XCSS to Maze5 using 800 classifiers only. Results in in a stable way. <p> of XCS in some applications. 2 The scale of the ordinate axis in Figure 4 represent values up to 500 steps, while in Figure 2 its values are limited upto 100 steps. 9 4.3 The Specify Operator and Biased Exploration Results presented in this section confirm the analysis presented in <ref> [5] </ref>. Specify successfully helps the generalization mechanism to recover dangerous situations which could corrupt the population. On the other hand, biased exploration is successful in simple environments, such as Maze5, but it is unfeasible in more complex environments. <p> On the other hand, biased exploration is successful in simple environments, such as Maze5, but it is unfeasible in more complex environments. In our opinion, this happens because biased exploration is a global solution to the problem of balancing the generalization mechanism, while Specify is a local solution. <ref> [5] </ref> in fact observed that XCS's generalization mechanism acts in environmental niches, and these should be considered a sort of fundamental element for operators in XCS. <p> being accurate. 14 In the next section, we are going to analyze the generalization mechanism in details, in order to show why it may fail under certain conditions. 6.2 Are Overgeneral Classifiers Inaccurate? The generalization mechanism is sound and thus it is not clear why it fails in certain environments. <ref> [5] </ref> observes that generalization in XCS is achieved through evolution; hence in case there is a strong genetic pressure, the generalization mechanism can be too slow to delete overgeneral classifiers, and these have enough time to corrupt the population.
Reference: [6] <author> L.-J. Lin. </author> <title> Reinforcement learning for robots using neural networks. </title> <type> Technical Report CMU-CS-93-103, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1993. </year>
Reference-contexts: Moreover, when neural networks are employed, all the areas of the environment have to be explored with the same frequence, otherwise the neural network may overfit locally. Solutions to this kind of problems for reinforcement algorithms have already been proposed in literature by Sutton [7] and by Lin <ref> [6] </ref>. Sutton proposed the Dyna architecture which integrates the learning algorithm with a model of the environment that is built by experience. The model is then employed to simulate exploration in other areas of the environment or for planning. Another solution is the one proposed by [6] that introduces the idea <p> [7] and by Lin <ref> [6] </ref>. Sutton proposed the Dyna architecture which integrates the learning algorithm with a model of the environment that is built by experience. The model is then employed to simulate exploration in other areas of the environment or for planning. Another solution is the one proposed by [6] that introduces the idea of experience replay: Past experienced trajectories to goal states are memorized and subsequently used to reproduce past experience in order to avoid local overfitting. 8.2 Dyna Architecture for XCS Teletransportation can be implemented for real problems integrating XCS with a model of the environment that is
Reference: [7] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <address> Austin, TX, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Moreover, when neural networks are employed, all the areas of the environment have to be explored with the same frequence, otherwise the neural network may overfit locally. Solutions to this kind of problems for reinforcement algorithms have already been proposed in literature by Sutton <ref> [7] </ref> and by Lin [6]. Sutton proposed the Dyna architecture which integrates the learning algorithm with a model of the environment that is built by experience. The model is then employed to simulate exploration in other areas of the environment or for planning. <p> Therefore, the analysis of the generalization mechanism of XCS, we present in this paper, motivates why it may be useful to integrate XCS with a model of the environment, developed by experience. The model can be employed as in <ref> [7] </ref> to simulate exploration in other areas of the environment, while the agent is actually exploring one specific 23 environmental niche, or otherwise for planning. In the following, we give a brief overview of a very simple version of dyna architecture we integrated in XCS that we are currently experimenting.
Reference: [8] <author> B. Widrow and M. Hoff. </author> <title> Adaptive switching circuits. </title> <booktitle> In Western Electronic Show and Convention, </booktitle> <volume> volume 4, </volume> <pages> pages 96-104. </pages> <booktitle> Institute of Radio Engineers (now IEEE), </booktitle> <year> 1960. </year>
Reference-contexts: First, the Q-learning-like payoff P is computed as the sum of the reward received at the previous time step and the maximum system prediction discounted by a factor fl (0 fl &lt; 1). P is used to update the prediction p j by the Widrow-Hoff delta rule <ref> [8] </ref> with learning rate fi (0 fi &lt; 1): p j p j + fi (P p j ). Likewise, the prediction error " j is adjusted with the formula: " j " j + fi (jP p j j " j ). Fitness update is slightly more complex.
Reference: [9] <author> Stewart W. Wilson. </author> <title> Classifier fitness based on accuracy. </title> <journal> Evolutionary Computation, </journal> <volume> 3(2) </volume> <pages> 149-175, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Generalization is the most interesting feature of XCS, the classifier system introduced by Wilson <ref> [9] </ref>. XCS has in fact been proved to evolve near-minimal populations of classifiers that are accurate and maximally general. Recently, Kovacs [4] has proposed an optimality hypothesis for XCS and showed experimental evidence of such hypothesis with respect to the function representation problem involving multiplexers. <p> Although these results are interesting, they only report experimental evidence and do not explain XCS behavior, that is our major goal. We then try to explain XCS's behavior analyzing the assumptions which underlie generalization in XCS and Wilson's generalization hypothesis <ref> [9] </ref>; we study XCS's generalization mechanism in depth and formulate a specific hypothesis. In short, we hypothesize that XCS fails in learning optimal behavior in those environments where some assumptions on which the generalization mechanism is based do not hold. <p> The paper is organized as follows. Section 2 briefly overviews XCS according to <ref> [9] </ref>, while Section 3 presents the design of experiments we employed in the paper. XCS with Specify, called XCSS, and XCS with biased exploration are compared in Section 4 using two new environments, Maze5 and Maze6; in Section 5 the same comparison is done using the Woods14 environment. <p> Finally, Section 10 ends the papers, drawing some conclusions and the directions for future work. 2 Description of XCS We now give a brief review of XCS in its most recent version [10]. We refer the interested reader to <ref> [9] </ref> for the original XCS description or to Kovacs's report [4] for a more detailed discussion for 3 implementors. Classifiers in XCS have three main parameters: the prediction p j , the prediction error " j and the fitness F j . <p> These environments are very similar since they differ in few positions only, nevertheless Maze6 is much more difficult to solve than Maze5, as our experiments will confirm. We compare four algorithms for each environment: (i) XCS according to the original definition <ref> [9] </ref> that is, without subsumption deletion; (ii) XCS without don't care symbols (# are not introduced in the initial population, neither in covering or mutation); (iii) XCS with Specify [5], called XCSS; (iv) XCS with biased exploration [12]. The performances of algorithms (i) and (ii) are two important references. <p> Although these results are interesting, they do not help us to explain the real causes which underlie the results we observed. In order to understand XCS's behavior, we need to study the generalization mechanism of XCS and Wilson's Generalization Hypothesis <ref> [9] </ref>; this is the subject of the next section where we discuss generalization capabilities of XCS and formulate an hypothesis to explain XCS's performance. 6 Generalization with XCS in Animat Problems 6.1 The Generalization Mechanism of XCS Experimental results show that not all grid worlds are equivalent for XCS: There are <p> section where we discuss generalization capabilities of XCS and formulate an hypothesis to explain XCS's performance. 6 Generalization with XCS in Animat Problems 6.1 The Generalization Mechanism of XCS Experimental results show that not all grid worlds are equivalent for XCS: There are environments such as Woods2 and Woods1 (see <ref> [9] </ref>) in which XCS easily produces optimal solutions; others, such as Maze5, Maze6 and Woods14, require special exploration policies and/or special operators. We now analyze the generalization mechanism of XCS in order to understand which factors influence the performance of the system. <p> In XCS this situation can be easily detected since the number of macroclassifier and the number of (micro)classifiers in the action sets will tend to be 8 This phenomenon was already noticed by Wilson in <ref> [9] </ref> where, discussing the generalization produced by XCS in Woods2, Wilson evidenced that XCS produced classifiers which matched the same niches but contained different amount of # symbols 25 the same.
Reference: [10] <author> Stewart W. Wilson. </author> <title> Generalization in XCS. Unpublished contribution to the ICML '96 Workshop on Evolutionary Computing and Machine Learning. </title> <note> Avaiable at http://netq.rowland.org/sw/swhp.html, 1996. 30 </note>
Reference-contexts: Finally, Section 10 ends the papers, drawing some conclusions and the directions for future work. 2 Description of XCS We now give a brief review of XCS in its most recent version <ref> [10] </ref>. We refer the interested reader to [9] for the original XCS description or to Kovacs's report [4] for a more detailed discussion for 3 implementors. Classifiers in XCS have three main parameters: the prediction p j , the prediction error " j and the fitness F j . <p> Macroclassifiers are essentially a programming technique that speeds up the learning process reducing the number of real, macro, classifiers XCS has to deal with. Since XCS was presented, two genetic operators have been proposed as extensions to the original system: Subsumption deletion <ref> [10] </ref> and Specify [5]. Subsumption deletion has been introduced to improve generalization capabilities of XCS. Sub-sumption deletion acts when classifiers created by the genetic component have to be inserted in the population.
Reference: [11] <author> Stewart W. Wilson. </author> <title> Generalisation in evolutionary learning. </title> <booktitle> In Proc. Fourth European Conf. on Artificial Life (ECAL97), </booktitle> <year> 1997. </year>
Reference-contexts: Most important, in such situations XCS may evolve a redundant representation of the task. Accordingly, we discuss the importance of subsumption deletion <ref> [11] </ref> in order to evolve a compact representation. The analysis, we present in this paper, does not concern XCS in general, instead it discusses the generalization mechanism of XCS when animats are applied in grid-worlds. <p> Another important aspect of generalization in XCS concerns the capability of XCS to evolve a compact representation of a learned task. 9.1 Generalization and Task Representation in XCS Results reported in the literature show that XCS can evolve near minimal populations of accurate and maximally general classifiers <ref> [11] </ref>. Recently, Kovacs [4] has proposed an optimality hypothesis which states that XCS tends to evolve the minimal population with respect to the boolean function representation problem.
Reference: [12] <author> Stewart W. Wilson. </author> <type> Personal communication. </type> <year> 1997. </year> <month> 31 </month>
Reference-contexts: In order to help XCS to recover from the presence of overgeneral classifiers, [5] introduced a new operator named Specify that counterbalances the effect of generalization when such situations occur. Experimental results confirmed that Specify successfully adapts overgeneral classifiers when the system can be prevented to converge. Wilson <ref> [12] </ref> observed that the behavior discussed in [5] depends on the amount of random exploration the agent does; specifically, if the agent wanders around too much in between arrivals at the goal it can fail to reach optimal solution. <p> The aim of this paper is to answer to such questions in order to get a better understanding of the generalization mechanism of XCS, while giving a unified view of what observed in [5] and <ref> [12] </ref>. First we extend the results presented in [5] comparing the two solutions proposed to counterbalance generalization mechanism: Specify and biased exploration. <p> The Specify operator was thus introduced in order to help the generalization mechanism in recovering from overgeneral classifiers. Wilson <ref> [12] </ref> observed that the behavior discussed in [5] also depends on the amount of random exploration the agent does: If the agent wanders around too much in between arrivals at the goal it can fail to converge to optimal performance. <p> compare four algorithms for each environment: (i) XCS according to the original definition [9] that is, without subsumption deletion; (ii) XCS without don't care symbols (# are not introduced in the initial population, neither in covering or mutation); (iii) XCS with Specify [5], called XCSS; (iv) XCS with biased exploration <ref> [12] </ref>. The performances of algorithms (i) and (ii) are two important references. <p> XCSS general parameters are set as for previous experiments, while for the Specify we set: N Sp =20 and P Sp =0.90. Figure 9 reports the 3 Due to the significant difference of the results is not possible to use the same scale on both plots. 4 Wilson <ref> [12] </ref> reports successful convergence for Woods14 employing a version of XCS specifically developed for solving Woods14. <p> On the other, hand Wilson <ref> [12] </ref> observed that XCS performance can be severely compromised if overgeneral classifiers appear in an empty environmental niche. Both statements above are correct, nevertheless they do not explain the phenomenon that has been observed in previous sections. <p> The two types of generalizations might seem equivalent but it is worth noticing that the former tends to produce a population which is very likely to be overgeneral in case the environment is extended, as for example if a new area of the environment is discovered <ref> [12] </ref>. This happens because, with an "a priori" generalization, the system tends to produce classifiers which apply in much more conditions than the agent experimented. <p> According to the classification of generalization types we present, the subsumption deletion operator compacts the representation of a task learned according to an "a priori" generalization. Hence, the system can produce solutions that are likely to result overgeneral when new areas are discovered in the environment. Wilson <ref> [12] </ref> suggests that in such cases the Specify operator can be a useful way to recover from classifiers that are overgeneral in the new areas. <p> We developed a series of experiments in which XCS with biased exploration was applied to the environments previously presented. Results, not reported here, show that the performance of the system is highly decreased when subsumption deletion is used. The same kind of result was reported by Wilson <ref> [12] </ref>. After we introduced teletransportation we repeated the set of experiments in order to test whether the decrease of performance when subsumption was used, depended on the presence of overgeneral classifiers that were evaluated as accurate.
References-found: 12

