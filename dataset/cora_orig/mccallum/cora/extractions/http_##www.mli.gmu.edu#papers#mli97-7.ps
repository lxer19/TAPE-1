URL: http://www.mli.gmu.edu/papers/mli97-7.ps
Refering-URL: http://www.mli.gmu.edu/kpubs.html
Root-URL: 
Email: michalski, iimam-@aic.gmu.edu  
Title: To the special memory of Cecylia Rauszer an outstanding scientist, a magnanimous human being, and
Author: Ryszard S. Michalski* and Ibrahim F. Imam 
Keyword: Key words: machine learning, inductive learning, decision structures, decision trees, decision rules, attribute selection, knowledge acquisition, data mining, knowledge discovery.  
Note: *Also with the  
Address: Fairfax, VA. 22030  Warsaw, Poland  
Affiliation: George Mason University  Institute of Computer Science, Polish Academy of Sciences,  
Abstract: A decision structure is a simple and powerful tool for organizing decision processes. It differs from a conventional decision tree in that its nodes are assigned tests that can be functions of the attributes, rather than single attributes; the branches stemming from a node can be assigned a subset of attribute values rather than a single attribute value (test outcome); and the leaves can be assigned one or more alternative decisions. This paper describes a methodology for learning decision structures from declarative knowledge expressed in the form of decision rules. The decision rules are generated by an expert, or by an AQ-type inductive learning program (with or without constructive induction). From a given set of rules, one can generate many different decision structures. The proposed methodology generates the one that is most suitable for the given decision-making situation, according to a multicriterion cost function. Experiments with a program implementing the methodology have indicated many advantages of the proposed methodology. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bergadano, F., Matwin, S., Michalski R. S. and Zhang, J., </author> <title> "Learning Two-tiered Descriptions of Flexible Concepts: The POSEIDON System," </title> <journal> Machine Learning, </journal> <volume> Vol. 8, No. 1, </volume> <pages> pp. 5-43, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Rules with a very small u-weight (the number of examples covered by a given rule and no previous rule in the set) point out to rare, special cases, or errors in the data. The study presented in <ref> (Bergadano et al., 1992) </ref> demonstrates that one can obtain a very high predictive accuracy by selecting only the strongest rule from each class (and ignoring the remaining rules), and then using it with a flexible matching procedure when determining the class membership of a new example. <p> This example shows that the rule truncation may significantly simplify a decision structure, without significantly affecting the prediction accuracy. In fact, in some experiments, such a rule truncation not only simplifies decision rules (and thus leads a simpler decision structure), but also improves their prediction accuracy <ref> (Bergadano et al., 1992) </ref>. 6. Applying AQDT-2 to Congressional Voting 81 Problem To test AQDT-2 on a real-world problem, it was applied to learning patterns in the U.S. congressional voting (1981 Congress Voting Record). There are two decision classes: Democratic Voting Pattern and Republican Voting Pattern.
Reference: <author> Bloedorn, E. and R. S. Michalski, </author> <title> DATA-DRIVEN CONSTRUCTIVE INDUCTION: A Methodology and Experiments, Reports of Machine Learning and Inference Laboratory, </title> <institution> George Mason University, </institution> <note> 1997 (to appear). </note>
Reference: <author> Bloedorn, E., Wnek, J., Michalski, R.S., and Kaufman, K., "AQ17: </author> <title> A Multistrategy Learning System: The Method and Users Guide, Report of Machine Learning and Inference Laboratory, </title> <institution> MIL-93-12, George Mason University, </institution> <year> 1993. </year>
Reference: <author> Bohanec, M. and Bratko, I., </author> <title> Trading Accuracy for Simplicity in Decision Trees, </title> <journal> Machine Learning Journal, </journal> <volume> Vol. 15, No. 3, </volume> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference: <author> Breiman, L., Friedman, J.H., Olshen, R.A. and Stone, C.J., </author> <title> Classification and Regression Structures, </title> <type> Belmont, </type> <institution> California: Wadsworth Int. Group, </institution> <year> 1984. </year>
Reference-contexts: The essential characteristic methods for learning decision trees is an attribute selection criterion employed for choosing attributes to be assigned to the nodes of the decision tree being built. Such criteria include entropy reduction (e.g., Quinlan 1979 and 1983), the Gini index of diversity <ref> (Breiman et al, 1984) </ref>, and others (e.g., Mingers, 1989; Chestnik and Bratko, 1991; Chestnik and Karalic, 1991). A decision tree/structure be an effective tool for describing a decision process if the required tests can be measured, and the class of decision-making situations it was designed for remains constant.
Reference: <author> Cestnik, B. & Bratko, I., </author> <title> On Estimating Probabilities in Structure Pruning, </title> <booktitle> Proceeding of EWSL 91, </booktitle> <pages> (pp. 138-150) Porto, </pages> <address> Portugal, March 6-8, </address> <year> 1991. </year>
Reference: <author> Cestnik, B. & Karalic, A., </author> <title> The Estimation of Probabilities in Attribute Selection Measures for Decision Structure Induction in Proceeding of the European Summer School on Machine Learning, </title> <address> July 22-31, Priory Corsendonk, Belgium, </address> <year> 1991. </year> <title> 24 Imam, I.F. and Michalski, R.S., "Learning Decision Structures from Decision Rules: A method and initial results from a comparative study", </title> <journal> in Journal of Intelligent Information Systems JIIS, </journal> <volume> Vol. 2, No. 3, </volume> <pages> pp. 279-304, </pages> <editor> Kerschberg, L., Ras, Z., & Zemankova, M. (Eds.), </editor> <publisher> Kluwer Academic Pub., </publisher> <address> MA, </address> <year> 1993. </year>
Reference: <editor> Imam, I.F., Michalski, R.S. and Kerschberg, L., </editor> <title> Discovering Attribute Dependence in Databases by Integrating Symbolic Learning and Statistical Analysis Techniques", </title> <booktitle> Proceeding of the First International Workshop on Knowledge Discovery in Database, </booktitle> <address> Washington, D.C., </address> <month> July, </month> <pages> 11-12, </pages> <year> 1993. </year>
Reference: <author> Michalski, </author> <title> R.S., AQVAL/1-Computer Implementation of a Variable-Valued Logic System VL1 and Examples of Its Application to Pattern Recognition, </title> <booktitle> Proceeding of the First International Joint Conference on Pattern Recognition, </booktitle> <pages> (pp. 3-17), </pages> <address> Washington, DC, </address> <month> October 30- November 1, </month> <year> 1973. </year>
Reference: <author> Michalski, </author> <title> R.S., "A Theory and Methodology of Inductive Learning," Chapter in Machine Learning: An Artificial Intelligence Approach, </title> <editor> R. S. Michalski, J. Carbonell and T. Mitchell (Eds.), </editor> <publisher> TIOGA Publishing Co., Palo Alto, </publisher> <pages> pp. 83-134, </pages> <year> 1983. </year> <title> Michalski, R.S, Designing Extended Entry Decision Tables and Optimal Decision Trees Using Decision Diagrams, </title> <type> Technical Report No. 898, </type> <institution> Urbana: University of Illinois, </institution> <month> March </month> <year> 1978. </year>
Reference-contexts: selection criteria based on decision rules, a new method for combining attribute selection criteria, the ability to generate unknown nodes in situations in which there is insufficient information for generating a 5 complete decision structure, the ability to learn decision structures from discriminant decision rules as well as characteristic rules <ref> (Michalski, 1983) </ref>, and finally, the ability to provide the most probably correct decision when the decision process stops due to the inability to measure an attribute associated with some node. <p> Experiments have shown that this criterion is especially useful when discriminant decision rules <ref> (Michalski, 1983) </ref> are used as the source rules. Dominance: This criterion measures the number of rules involving a test.
Reference: <author> Michalski, R.S., Mozetic, I., Hong, J. & Lavrac, N., </author> <title> The MultiPurpose Incremental Learning System AQ15 and its Testing Application to Three Dedical Domains, </title> <booktitle> Proceedings of AAAI-86, </booktitle> <pages> (pp. 1041-1045), </pages> <address> Philadelphia, PA, </address> <year> 1986. </year>
Reference-contexts: Flexible matching is done by computing a degree of match between an example and rules of candidate classes, and selecting decision corresponding tot he best match <ref> (Michalski et al., 1986) </ref>. The default setting of AQDT-2 requires pruning decision rules with a support level of 3% or less.
Reference: <author> Mingers, J., </author> <title> An Empirical Comparison of Selection Measures for DecisionStructure Induction, </title> <journal> Machine Learning, </journal> <volume> Vol. 3, No. 3, </volume> <pages> (pp. 319-342), </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989a. </year>
Reference: <author> Quinlan, J.R., </author> <title> Discovering Rules by Induction from Large Collections of Examples, </title> <editor> in D. Michie (Edr), </editor> <booktitle> Expert Systems in the Microelectronic Age, </booktitle> <publisher> Edinburgh University Press, </publisher> <year> 1979. </year>
Reference-contexts: Decision trees are typically learned from a set of examples of decisions. The essential characteristic methods for learning decision trees is an attribute selection criterion employed for choosing attributes to be assigned to the nodes of the decision tree being built. Such criteria include entropy reduction <ref> (e.g., Quinlan 1979 and 1983) </ref>, the Gini index of diversity (Breiman et al, 1984), and others (e.g., Mingers, 1989; Chestnik and Bratko, 1991; Chestnik and Karalic, 1991).
Reference: <author> Quinlan, J.R., </author> <title> Learning Efficient Classification Procedures and Their Application To Chess End Games, in R.S. </title> <editor> Michalski, J.G. Carbonell and T.M. Mitchell, (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <address> Los Altos: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference: <author> Quinlan, J. R. </author> <title> Probabilistic Decision Structures, </title> <editor> in Y. Kodratoff and R.S. Michalski (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Vol. III, </booktitle> <address> San Mateo, CA, </address> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> (pp. 63-111), </pages> <month> June, </month> <year> 1990. </year>
Reference-contexts: For comparison, the well known C4.5 program for decision trees was also applied to the same problem <ref> (Quinlan, 1990) </ref>. C4.5 was used with the default window setting (maximum of 20% the number of examples and twice the square root the number of examples), and the number of trials was set to one.
Reference: <author> Smyth, P., Goodman, R.M., and Higgins, C., </author> <title> A Hybrid Rule-based/Bayesian Classifier, </title> <booktitle> Proceedings of ECAI-90, </booktitle> <address> Stockholm, </address> <year> 1990. </year>
Reference-contexts: In such cases, if no more information can be obtained, but a decision must be made at an indecision node, it would be useful to know an estimate of the probability distribution of different candidate decisions <ref> (Smyth, Goodman & Higgins, 1990) </ref>. We will present here a method for computing a databased estimate of such probabilities.
References-found: 16

