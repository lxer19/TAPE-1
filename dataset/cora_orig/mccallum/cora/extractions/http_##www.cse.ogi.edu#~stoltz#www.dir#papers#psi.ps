URL: http://www.cse.ogi.edu/~stoltz/www.dir/papers/psi.ps
Refering-URL: http://www.cse.ogi.edu/~stoltz/
Root-URL: http://www.cse.ogi.edu
Email: fstoltz,hook,mwolfeg@cse.ogi.edu harini@cs.colorado.edu  
Phone: (503) 690-1121 ext. 7404  
Title: Static Single Assignment Form for Explicitly Parallel Programs: Theory and Practice  
Author: Eric Stoltz Harini Srinivasan zx James Hook Michael Wolfe 
Keyword: Optimizing compilers, use-def chains, parallel languages, explicit parallel sec tions, Static Single Assignment.  
Note: Corresponding author. Supported in part by NSF grant CCR-9113885 and a grant from Intel Corporation and the  Supported in part by an IBM Graduate Fellowship. Supported in part by NSF grant CCR-9101721.  
Address: P.O. Box 91000 Portland, OR 97291-1000  Boulder CO  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science Technology  Oregon Advanced Computing Institute. University of Colorado,  
Abstract: To sensibly reason about parallel programs, a coherent intermediate form needs to be developed. We describe and prove correctness and safety of algorithms to convert programs that use the Parallel Computing Forum Parallel Sections construct into a parallel Static Single Assignment (SSA) form. We define what the concept of dominator and dominance frontier mean in parallel programs. How to extend the SSA form to handle parallel updates and still preserve the SSA properties is described by introducing a new parallel merge operator, the -function. The minimal placement points for -functions are identified and proved correct by introducing the meet of two nodes in a Parallel Precedence Graph (PPG), which is the dual concept of join in a sequential control flow graph (CFG). The resulting intermediate form allows compilers to apply classical scalar optimization algorithms to explicitly parallel programs. We also discuss issues encountered while implementing these constructs in Nascent, our restructuring research compiler. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing Static Single Assignment form and the control dependence graph. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year> <month> 65 </month>
Reference-contexts: The interplay between the language model, the compiler model and the architectural model is a subject for another paper. 3 The Static Single Assignment Form This section reviews some concepts of SSA construction introduced by Cytron et al. <ref> [1] </ref>. The algorithm to convert a sequential program into SSA form uses the Control Flow Graph (CFG) of a procedure. <p> Further details on the construction of SSA form may be found in the original paper <ref> [1] </ref>. 6 4 Flow Graphs for Parallel Constructs As illustrated in the programs in Figure 1, traditional analysis methods on Control Flow Graphs will not easily apply to explicitly parallel programs. It is not possible to analyze parallel merge points using the techniques for sequential merge points like endif statements. <p> each of these sections, then the algorithm takes O ( ^ P fi ( ^ N 2 + ^ E) + ^ P fi ^ N) time, which asymptotically 13 PFRONT ( G main ) Procedure PFRONT ( PCFG G ) /* Compute dominance frontier (DF) using the algorithm in <ref> [1] </ref> */ for each node X in G do compute DF (X) enddo for each node X in G do PPF (X) DF (X) if X Exit X then PPF (X) PPF (X) [ PPF (P X ) endif if X is a parallel block, P X , then for each <p> These nodes are precisely the nodes in the set J + (S) (iterated join set), where S is the set of nodes containing assignments to the variable. Cytron et al. <ref> [1] </ref> have proved the equivalence between J + (S) and DF + (S) for the sequential case. <p> Proof: The proof of this theorem proceeds by considering the different cases in the definition of J p and showing the equivalence between the two sets for each case. Details of the proof can be found in previous work [12]. 2 We know for the sequential case <ref> [1] </ref> that the iterated join set J + (S) is equal to the iterated dominance frontier DF + (S). <p> Therefore, [ J p (fX; Y g) = 8X;Y 2S^X6=Y This equivalence can be extended to the iterated joins, i.e. J + u (S) (4) From the paper by Cytron et al. <ref> [1] </ref>, we know that J + (S) = DF + (S). This equivalence again is unaffected by taking the union over all factored CFGs, i.e. J + From equations 4 and 5, J + p (S) = PDF + (S). <p> The proof of the theorem follows from Theorems 5.1 and 5.2, i.e. J + p (S) = PPF + (S). 2 6.3 Algorithm to place -functions The algorithm to place -functions is a worklist algorithm, involving several modifications to the algorithm for placing -functions in sequential programs <ref> [1] </ref>. Essentially a recursive call to the algorithm is made at each supernode, retaining the reaching definitions from outside the supernode, while propagating reaching definitions from within the supernode to the outer program block. <p> We also define M + (S) as the limit of increasing sequences analogous to that used for join and dominance frontier: M 1 (S) = M (S) M i+1 (S) = M (S [ M i (S)) The definition of join, the basis of work to place -functions <ref> [1] </ref>, is a well-known concept. Although in a CFG J + (S) = J (S) [13], the dual definition of join for PPGs, meet, does not possess this property. Consider Figure 7 (a). Let S= fX,Yg. <p> Within our implementation we have added to each PCFG an extra edge added from Entry to Exit, called the slice edge. This edge is necessary for the proofs in the original SSA transformation algorithms dealing with control dependence <ref> [1] </ref>, and they are also needed for the construction we provide to support the SSA extensions for explicit parallelism. Conceptually, this edge represents a conditional within each Entry node as to whether the CFG will be executed. <p> After placing -functions a technique known as renaming transforms each variable definition into a unique name and each use into the name of its unique reaching definition <ref> [1] </ref>. The method employed to perform this renaming is depth-first, in that it recursively traverses the dominator tree in a depth-first order, keeping a stack of current definitions for each variable. <p> The key property that this renaming scheme satisfies is that at each node the correct "current" definition (an original definition or -function) of each variable is the most recent definition on the depth-first path to this node from Entry, i.e., the definition on top of the definition stack <ref> [1, Lemma 10] </ref>. In fact, a depth-first traversal of any spanning tree of the CFG will also satisfy this property. 42 Unfortunately, a depth-first traversal of the nodes of a PPG will not satisfy this key property with merge operators at M + (S). <p> Next, we show that the iterated dominance frontier is a superset of the iterated reaching frontier on all graphs. Theorem 9.2 DF + (S) RF + (S) Proof: It has been shown <ref> [1, Lemma 4] </ref> that for any node Z that X reaches, some node Y 2 fX [ DF + (X)g dominates Z. <p> requirements for placing -functions within the PPG, we can use the space consumed by -function placement as an upper bound, since M + (S) DF + (S). 48 While the worst case scenario could be O (N 2 ), in practice most programs exhibit linear space requirements when placing -functions <ref> [1, 16] </ref>. 9.5 The Complete Algorithms The complete transformation of an intermediate representation into parallel SSA form is accomplished in two main phases: function placement and renaming. <p> In the former case, we have a node with no nodes in its dominance frontier, and in the latter we have a node with exactly one predecessor. The original renaming algorithm <ref> [1, Figure 5] </ref> performs a depth-first traversal of the dominator tree of the CFG. We modify this algorithm for parallel constructs as follows: 1. <p> Proof: Consider any -function f for variable v. If f is within a local PCFG, then Algorithm 2 works as originally presented <ref> [1] </ref>. We need only consider the case where the PCFG contains a supernode, P, and (i) f is within P, or (ii) f is at a point reached by P. case (i). Either f 2 DF + of A (v), or not. <p> for computing the running time for Definition 5.4, and where ^ V is the number of variables in the program, we calculate the running time of our algorithms as follows: the first phase, and -function placement, takes worst case O ( ^ N 2 61 + ^ E) per section <ref> [1] </ref>, thus over all sequential sections it will take O ( ^ P fi ( ^ N 2 + ^ E) ) time.
Reference: [2] <author> B. Alpern, M.N. Wegman, and F.K. Zadeck. </author> <title> Detecting equality of variables in programs. </title> <booktitle> In Conf. Record 15th Annual ACM Symp. Principles of Programming Languages [31], </booktitle> <pages> pages 1-11. </pages>
Reference-contexts: For sequential programs, the Static Single Assignment (SSA)[1] form has proven to be an efficient and practical basis for many code optimization algorithms, including constant propagation, redundancy elimination, and induction variable analysis <ref> [2, 3, 4, 5] </ref>. Indeed, it has already found its way into modern commercial compilers. In this paper we extend the algorithms developed by Cytron et al. that convert a program into SSA form to handle explicitly parallel programs.
Reference: [3] <author> B.K. Rosen, M.N. Wegman, and F.K. Zadeck. </author> <title> Global value numbers and redundant computations. </title> <booktitle> In Conf. Record 15th Annual ACM Symp. Principles of Programming Languages [31], </booktitle> <pages> pages 12-27. </pages>
Reference-contexts: For sequential programs, the Static Single Assignment (SSA)[1] form has proven to be an efficient and practical basis for many code optimization algorithms, including constant propagation, redundancy elimination, and induction variable analysis <ref> [2, 3, 4, 5] </ref>. Indeed, it has already found its way into modern commercial compilers. In this paper we extend the algorithms developed by Cytron et al. that convert a program into SSA form to handle explicitly parallel programs.
Reference: [4] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: For sequential programs, the Static Single Assignment (SSA)[1] form has proven to be an efficient and practical basis for many code optimization algorithms, including constant propagation, redundancy elimination, and induction variable analysis <ref> [2, 3, 4, 5] </ref>. Indeed, it has already found its way into modern commercial compilers. In this paper we extend the algorithms developed by Cytron et al. that convert a program into SSA form to handle explicitly parallel programs.
Reference: [5] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: For sequential programs, the Static Single Assignment (SSA)[1] form has proven to be an efficient and practical basis for many code optimization algorithms, including constant propagation, redundancy elimination, and induction variable analysis <ref> [2, 3, 4, 5] </ref>. Indeed, it has already found its way into modern commercial compilers. In this paper we extend the algorithms developed by Cytron et al. that convert a program into SSA form to handle explicitly parallel programs. <p> Consider the sequential and parallel programs in Figure 1; even though the conditional branching behavior of the sequential program resembles the parallel fork structure of the parallel program, these two programs are quite different. Variable j is a linear induction variable in the parallel program <ref> [5] </ref>, but not in the sequential program. Recognition of j as an induction variable is important for data dependence analysis and strength reduction.
Reference: [6] <author> Parallel Computing Forum. </author> <title> PCF Parallel Fortran extensions. </title> <journal> Fortran Forum, </journal> <volume> 10(3), </volume> <month> September </month> <year> 1991. </year> <note> (special issue). </note>
Reference-contexts: Other issues that affect parallel programs, such as multiple parallel updates and the concept of parallel precedence, are also considered. We chose to start with the Parallel Computing Forum Parallel Fortran extensions since this is the basis of the ANSI committee X3H5 standardization effort <ref> [6] </ref>. Because of the industrial participation and commitment to this effort, these particular parallel language extensions may have widespread impact. <p> Section 10 contrasts this work with related research, suggesting some continuing directions of interest. Section 11 provides conclusions. 2 Parallel Section Semantics The Parallel Sections construct <ref> [6] </ref> is similar to the cobegin/coend of Brinch Hansen [7] or the IBM Parallel Cases statement [8]. It is a block structured construct used to specify parallel execution of identified sections of code. The parallel sections may also be nested. <p> The parallel sections may also be nested. The sections of code must be data independent, except where an appropriate synchronization mechanism is used. Transfer of control into or out of a parallel section is not supported <ref> [6] </ref>. Here we consider only structured synchronization expressed as Wait clauses, i.e. DAG parallelism [9]. An example parallel program with Wait clauses is given in Figure 2. The Wait clause in section D specifies that this section can start executing only after both sections A and B have completed execution.
Reference: [7] <author> Per Brinch Hansen. </author> <title> Operating Systems Principles. Automatic Computation. </title> <publisher> Prentice-Hall, </publisher> <year> 1973. </year>
Reference-contexts: Section 10 contrasts this work with related research, suggesting some continuing directions of interest. Section 11 provides conclusions. 2 Parallel Section Semantics The Parallel Sections construct [6] is similar to the cobegin/coend of Brinch Hansen <ref> [7] </ref> or the IBM Parallel Cases statement [8]. It is a block structured construct used to specify parallel execution of identified sections of code. The parallel sections may also be nested. The sections of code must be data independent, except where an appropriate synchronization mechanism is used.
Reference: [8] <author> IBM Corporation, Kingston, </author> <title> NY. Parallel FORTRAN Language and Library Reference, </title> <year> 1988. </year>
Reference-contexts: Section 10 contrasts this work with related research, suggesting some continuing directions of interest. Section 11 provides conclusions. 2 Parallel Section Semantics The Parallel Sections construct [6] is similar to the cobegin/coend of Brinch Hansen [7] or the IBM Parallel Cases statement <ref> [8] </ref>. It is a block structured construct used to specify parallel execution of identified sections of code. The parallel sections may also be nested. The sections of code must be data independent, except where an appropriate synchronization mechanism is used.
Reference: [9] <author> Ron Cytron, Michael Hind, and Wilson Hsieh. </author> <title> Automatic generation of DAG parallelism. </title> <booktitle> In Proc. ACM SIGPLAN '89 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 54-68, </pages> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The sections of code must be data independent, except where an appropriate synchronization mechanism is used. Transfer of control into or out of a parallel section is not supported [6]. Here we consider only structured synchronization expressed as Wait clauses, i.e. DAG parallelism <ref> [9] </ref>. An example parallel program with Wait clauses is given in Figure 2. The Wait clause in section D specifies that this section can start executing only after both sections A and B have completed execution.
Reference: [10] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: DAG parallelism [9]. An example parallel program with Wait clauses is given in Figure 2. The Wait clause in section D specifies that this section can start executing only after both sections A and B have completed execution. Note that Wait clauses do not affect control dependence relations <ref> [10] </ref>; all unconditional code in the Parallel Sections construct is identically control dependent on the same predicate that controls execution of the Parallel Sections statement itself. Once this predicate evaluates to true all the parallel sections will execute.
Reference: [11] <author> Harini Srinivasan and Michael Wolfe. </author> <title> Analyzing programs with explicit parallelism. </title> <editor> In Utpal Banerjee, David Gelernter, Alexandru Nicolau, and David A. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, number 589 in Lecture Notes in Computer Science, </booktitle> <pages> pages 403-419. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: fl a Section D, Wait (A, B) (y) c = a fl b + c * f End Parallel Sections (n) d = d + f else (q) d = 23 (r) endif (r) e = a + b fl c fl d 4 to local copies of the variable <ref> [11] </ref>. When the parallel block is complete, the global state is updated with any modifications made within any section. This gives a well-defined program without volatile variables, and allows optimization within a parallel section independent of code in other sections.
Reference: [12] <author> Harini Srinivasan. </author> <title> Analyzing programs with explicit parallelism. M.S. </title> <type> thesis 91-TH-006, </type> <institution> Oregon Graduate Institute, Dept. of Computer Science and Engineering, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Proof: The proof of this theorem proceeds by considering the different cases in the definition of J p and showing the equivalence between the two sets for each case. Details of the proof can be found in previous work <ref> [12] </ref>. 2 We know for the sequential case [1] that the iterated join set J + (S) is equal to the iterated dominance frontier DF + (S).
Reference: [13] <author> Michael Wolfe. </author> <title> J + = J. </title> <journal> ACM Sigplan Notices, </journal> <volume> 29(7) </volume> <pages> 51-53, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Although in a CFG J + (S) = J (S) <ref> [13] </ref>, the dual definition of join for PPGs, meet, does not possess this property. Consider Figure 7 (a). Let S= fX,Yg. Then M (S) = fW,Zg; in fact, A 62 M (S), but A 2 M (S [ M (S)) = M (X,Y,W,Z) = fW,Z,Ag.
Reference: [14] <author> Harini Srinivasan, James Hook, and Michael Wolfe. </author> <title> Static single assignment for explicitly parallel programs. </title> <booktitle> In Conf. Record 20th Annual ACM Symp. Principles of Programming Lan guages, </booktitle> <pages> pages 260-272, </pages> <address> Charleston, SC, </address> <month> January </month> <year> 1993. </year> <month> 66 </month>
Reference-contexts: We will first prove that it is sufficient to place -functions at M + (S). The concept of iterated meet is a refinement of the -function placement method suggested earlier <ref> [14] </ref>, in that the iterated meet is smaller and, in fact, the minimal set. 31 Theorem 7.3 In a PPG, with -functions for v placed at M + (S), all uses of v within node N will be reached (in the sense of Definition 7.1) by exactly one definition (including -functions)
Reference: [15] <author> Michael Wolfe, Michael P. Gerlek, and Eric Stoltz. Nascent: </author> <title> A Next-Generation, High Performance Compiler. </title> <institution> Oregon Graduate Institute of Science & Technology unpublished, </institution> <year> 1993. </year>
Reference-contexts: Next, we present the complete algorithm to insert and -functions, and correctly create and generate the proper reaching definitions as arguments. We also demonstrate the correctness and safety of these algorithms, which have been successfully implemented in our restructuring compiler, Nascent <ref> [15] </ref>. We close this section with a few reflections on the implementation. 39 9.1 An Introduction to Implementation In our implementation, the compiler finds for each variable the set of nodes in each PCFG or PPG where the variable is assigned.
Reference: [16] <author> Paul Havlak. </author> <title> Interprocedural Symbolic Analysis. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <year> 1994. </year>
Reference-contexts: requirements for placing -functions within the PPG, we can use the space consumed by -function placement as an upper bound, since M + (S) DF + (S). 48 While the worst case scenario could be O (N 2 ), in practice most programs exhibit linear space requirements when placing -functions <ref> [1, 16] </ref>. 9.5 The Complete Algorithms The complete transformation of an intermediate representation into parallel SSA form is accomplished in two main phases: function placement and renaming.
Reference: [17] <author> Eric Stoltz, Michael P. Gerlek, and Michael Wolfe. </author> <title> Extended SSA with factored use-def chains to support optimization and parallelism. </title> <booktitle> In 1994 ACM Conf. Proceedings Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: In this way, every use has exactly one reaching definition. However, for real implementations, such as our Fortran restructuring compiler, Nascent, one does not seriously entertain the notion of a symbol table explosion to insure this property <ref> [17] </ref>. Instead, a use-def ssalink pointer field is assigned to each use (fetch in the intermediate representation) and all arguments of and -functions [17]. The example program transformed into SSA form (Figure 18) demonstrates the unique reaching definition semantics, not the syntax of implementation. <p> However, for real implementations, such as our Fortran restructuring compiler, Nascent, one does not seriously entertain the notion of a symbol table explosion to insure this property <ref> [17] </ref>. Instead, a use-def ssalink pointer field is assigned to each use (fetch in the intermediate representation) and all arguments of and -functions [17]. The example program transformed into SSA form (Figure 18) demonstrates the unique reaching definition semantics, not the syntax of implementation. We retain the designation "renaming" for historical purposes. <p> In Nascent, we have accomplished this by supplying a sparse form of use-def links. This is in contrast to most other implementations of which we are aware, which provide def-use links. We believe our approach has numerous advantages, including efficient space utilization and effective solutions to data-flow problems <ref> [17] </ref>. 62 * Removing duplicate -arguments. We have seen how duplicate arguments can occur. At first glance, it may appear that in order to remove duplicates, the arguments would need to be sorted, taking N log N time.
Reference: [18] <author> Robert Sedgewick. </author> <title> Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: We prove that Algorithm 4 in Figure 19 accomplishes the desired task, which is called by the routine traversePPG (). 60 Claim 9.1 Popping T will visit the nodes of G in topological order. This result is well-known <ref> [18] </ref>. Claim 9.2 E is a spanning tree of G. Proof : Choose any node N of G. We know N is visited (Claim 9.1), and visited only once, since it is marked when visited the first time, and will not be revisited once so marked.
Reference: [19] <author> David Callahan, Ken Kennedy, and Jaspal Subhlok. </author> <title> Analysis of event synchronization in a parallel programming tool. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming [32], </booktitle> <pages> pages 21-30. </pages>
Reference-contexts: Data flow analysis of these programs for compile time optimizations has become a topic of interest only recently. To our knowledge, none of the earlier work on race detection <ref> [19, 20, 21, 22] </ref> uses the SSA form of a program. We have shown that an outcome of translating explicitly parallel programs to their SSA form is static detection of write-write races in these programs fl . However, race detection is not the main focus of this research work.
Reference: [20] <author> Anne Dinning and Edith Schonberg. </author> <title> An empirical comparison of monitoring algorithms for access anomaly detection. </title> <booktitle> In Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming [32], </booktitle> <pages> pages 1-10. </pages>
Reference-contexts: Data flow analysis of these programs for compile time optimizations has become a topic of interest only recently. To our knowledge, none of the earlier work on race detection <ref> [19, 20, 21, 22] </ref> uses the SSA form of a program. We have shown that an outcome of translating explicitly parallel programs to their SSA form is static detection of write-write races in these programs fl . However, race detection is not the main focus of this research work.
Reference: [21] <author> Vasanth Balasundaram and Ken Kennedy. </author> <title> Compile-time detection of race conditions in a parallel program. </title> <booktitle> In Proc. 3rd International Conference on Supercomputing, </booktitle> <pages> pages 175-185, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Data flow analysis of these programs for compile time optimizations has become a topic of interest only recently. To our knowledge, none of the earlier work on race detection <ref> [19, 20, 21, 22] </ref> uses the SSA form of a program. We have shown that an outcome of translating explicitly parallel programs to their SSA form is static detection of write-write races in these programs fl . However, race detection is not the main focus of this research work.
Reference: [22] <author> D. Callahan and J. Subhlok. </author> <title> Static Analysis of low-level synchronization. </title> <booktitle> In Proc. of the ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 100-111, </pages> <address> Madison, WA, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Data flow analysis of these programs for compile time optimizations has become a topic of interest only recently. To our knowledge, none of the earlier work on race detection <ref> [19, 20, 21, 22] </ref> uses the SSA form of a program. We have shown that an outcome of translating explicitly parallel programs to their SSA form is static detection of write-write races in these programs fl . However, race detection is not the main focus of this research work.
Reference: [23] <author> Jyh-Herng Chow and Williams Ludwell Harrisson. </author> <title> Compile time analysis of parallel programs that share memory. </title> <booktitle> In Conf. Record 19th Annual ACM Symp. Principles of Programming Languages, </booktitle> <pages> pages 130-141, </pages> <year> 1992. </year>
Reference-contexts: However, race detection is not the main focus of this research work. More recently there has been considerable interest in data-flow analysis of explicitly parallel programs. Chow and Harrison <ref> [23] </ref> have presented a general framework for analyzing parallel programs using abstract interpretation. They do not consider synchronization and assume a strong memory consistency model.
Reference: [24] <author> Dirk Grunwald and Harini Srinivasan. </author> <title> Data flow equations for explicitly parallel programs. </title> <booktitle> In Conf. Record 4th ACM Symp. Principles and Practices of Parallel Programming, </booktitle> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: As mentioned earlier, we believe this is overly restrictive if fl Note that since we assume copy-in/copy-out semantics, we never report a write-read race in the parallel program. 63 the purpose of the data-flow analysis framework is code optimizations. Grunwald and Srinivasan <ref> [24] </ref> present a data-flow framework to compute reaching definitions information in explicitly parallel programs with post/wait synchronization. This work also does not focus on SSA form. Sarkar and Simons [25, 26] define a representation for explicitly parallel programs, called the Parallel Program Graph.
Reference: [25] <author> Vivek Sarkar. </author> <title> A Concurrent Execution Semantics for Parallel Program Graphs and Program Dependence Graphs. </title> <booktitle> In Conf. Record 5th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 13-20, </pages> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Grunwald and Srinivasan [24] present a data-flow framework to compute reaching definitions information in explicitly parallel programs with post/wait synchronization. This work also does not focus on SSA form. Sarkar and Simons <ref> [25, 26] </ref> define a representation for explicitly parallel programs, called the Parallel Program Graph. They describe a way of specifying concurrent execution semantics for a Parallel Program Graph and also suggest the use of this representation for parallel program analysis.
Reference: [26] <author> Vivek Sarkar and Barbara Simons. </author> <title> Parallel Program Graphs and Their Classification. </title> <booktitle> In Proc. of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 633-655, </pages> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Grunwald and Srinivasan [24] present a data-flow framework to compute reaching definitions information in explicitly parallel programs with post/wait synchronization. This work also does not focus on SSA form. Sarkar and Simons <ref> [25, 26] </ref> define a representation for explicitly parallel programs, called the Parallel Program Graph. They describe a way of specifying concurrent execution semantics for a Parallel Program Graph and also suggest the use of this representation for parallel program analysis.
Reference: [27] <author> Harini Srinivasan and Dirk Grunwald. </author> <title> An Efficient Construction of Parallel Static Single Assignment Form for Structured Parallel Programs. </title> <type> Technical Report CU-CS-564-91, </type> <institution> University of Colorado at Boulder, </institution> <month> December </month> <year> 1991. </year> <month> 67 </month>
Reference-contexts: They describe a way of specifying concurrent execution semantics for a Parallel Program Graph and also suggest the use of this representation for parallel program analysis. Some work has also been done by Grunwald and Srinivasan <ref> [27] </ref> in translating structured parallel programs to their SSA form. Their algorithm uses the Parallel Flow Graph representation which is not a hierarchical data structure like the Extended Flow Graph.
Reference: [28] <author> Jong-Deok Choi, Ron Cytron, and Jeanne Ferrante. </author> <title> Automatic construction of sparse data flow evaluation graphs. </title> <booktitle> In Conf. Record 18th Annual ACM Symp. Principles of Programming Languages, </booktitle> <address> Orlando, Florida, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: SSA form gives the reaching definition for every use of a variable in the program. As future work, we would like to extend the algorithms to derive Sparse Evaluation Graphs <ref> [28] </ref>, used to solve several data-flow problems including reaching definitions in sequential programs, to handle the parallel and synchronization constructs mentioned in this paper. The concepts of dominance relation and dominance frontiers have been used in deriving Sparse Evaluation Graphs as well [28]. <p> extend the algorithms to derive Sparse Evaluation Graphs <ref> [28] </ref>, used to solve several data-flow problems including reaching definitions in sequential programs, to handle the parallel and synchronization constructs mentioned in this paper. The concepts of dominance relation and dominance frontiers have been used in deriving Sparse Evaluation Graphs as well [28].
Reference: [29] <author> Samuel P. Midkiff and David A. Padua. </author> <title> Issues in the optimization of parallel programs. </title> <booktitle> In Proc. 1990 International Conf. on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 105-113, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year> <institution> Penn State Press. </institution>
Reference-contexts: Previous work on applying scalar optimizations to parallel programs focused on stricter language semantics and the problems this caused for the compiler <ref> [29, 30] </ref>. Our language model is more appropriate for application-level code, and allows more aggressive optimizations. Acknowledgements We would like to gratefully acknowledge the reviewers, whose thorough reading and helpful comments definitely improved the quality of this paper.
Reference: [30] <author> Samuel P. Midkiff, David A. Padua, and Ron Cytron. </author> <title> Compiling programs with user parallelism. </title> <editor> In David Gelernter, Alexandru Nicolau, and David A. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Research Monographs in Parallel and Distributed Computing, </booktitle> <pages> pages 402-422. </pages> <publisher> MIT Press, </publisher> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: Previous work on applying scalar optimizations to parallel programs focused on stricter language semantics and the problems this caused for the compiler <ref> [29, 30] </ref>. Our language model is more appropriate for application-level code, and allows more aggressive optimizations. Acknowledgements We would like to gratefully acknowledge the reviewers, whose thorough reading and helpful comments definitely improved the quality of this paper.
Reference: [31] <editor> Conf. </editor> <booktitle> Record 15th Annual ACM Symp. Principles of Programming Languages, </booktitle> <address> San Diego, CA, </address> <month> January </month> <year> 1988. </year>
Reference: [32] <institution> Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </institution> <address> Seattle, Washington, March 1990. </address> <publisher> ACM Press. </publisher> <pages> 68 </pages>
References-found: 32

