URL: ftp://ftpipr.ira.uka.de/pub/papers/1996/eg96sm.ps.gz
Refering-URL: ftp://ftpipr.ira.uka.de/.public_html/papersna.html
Root-URL: 
Title: Intelligent Control for Haptic Displays  
Author: Stefan Munch and Martin Stangenberg 
Keyword: Multimodal interaction; haptic output; user modelling; human-computer interaction  
Address: Kaiserstr. 12, D-76128 Karlsruhe, Germany  
Affiliation: Institute for Real-Time Computer Systems Robotics, University of Karlsruhe  
Date: August 26-30, 1996)  
Note: In: COMPUTER GRAPHICS forum, Vol. 15, No. 3, pp. 217-226, Conference Issue (Eurographics '96, Poitiers, France,  1996 Copyright Eurographics Association  
Abstract: Usually, a mouse is used for input activities only, whereas output from the computer is sent via the monitor and one or two loudspeakers. But why not use the mouse for output, too? For instance, if it would be possible to predict the next interaction object the user wants to click on, a mouse with a mechanical brake could stop the cursor movement at the desired position. This kind of aid is especially attractive for small targets like resize handles of windows or small buttons. In this paper, we present an approach for the integration of haptic feedback in everyday graphical user interfaces. We use a specialized mouse which is able to apply simple haptic information to the user's hand and index finger. A multi-agent system has been designed which 'observes' the user in order to predict the next interaction object and launch haptic feedback, thus supporting positioning actions with the mouse. Although primarily designed in order to provide 'intelligent' haptic feedback, the system can be combined with other output modalities as well, due to its modular and flexible architecture. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Y. Fukui and M. Shimojo. </author> <title> Edge Tracing of Virtual Shape Using Input Device with Force Feedback. </title> <journal> Systems and Computers in Japan, </journal> <volume> 23(5) </volume> <pages> 94-104, </pages> <year> 1992. </year>
Reference-contexts: Although tactile and force feedback alone have been addressed by a number of researchers (see, e. g., <ref> [1, 2, 3, 4, 5] </ref>), their effect in standard interaction tasks has not been investigated in depth, and most evaluations are based on very small data sets only.
Reference: 2. <author> F. P. Brooks, Jr. et al. </author> <title> Project GROPE - Haptic Displays for Scientific Visualization. </title> <editor> In F. Baskett, editor, </editor> <booktitle> Computer Graphics (SIGGRAPH '90 Proceedings), </booktitle> <volume> volume 24, </volume> <pages> pages 177-185, </pages> <address> Dallas, TX, </address> <month> August </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: Although tactile and force feedback alone have been addressed by a number of researchers (see, e. g., <ref> [1, 2, 3, 4, 5] </ref>), their effect in standard interaction tasks has not been investigated in depth, and most evaluations are based on very small data sets only.
Reference: 3. <author> M. Akamatsu and S. Sato. </author> <title> A multi-modal mouse with tactile and force feedback. </title> <journal> Int. Journal of Human Computer Studies, </journal> <volume> 40 </volume> <pages> 443-453, </pages> <year> 1994. </year>
Reference-contexts: Although tactile and force feedback alone have been addressed by a number of researchers (see, e. g., <ref> [1, 2, 3, 4, 5] </ref>), their effect in standard interaction tasks has not been investigated in depth, and most evaluations are based on very small data sets only. <p> In contrast to these devices, and in order to study the effect of haptic feedback in simple interaction tasks, we have equipped a standard mouse with a pin in one button and two magnets in its base, see Figure 2. This socalled ForceMouse, based on an idea by <ref> [3] </ref>, can not only generate the usual input parameters but also provide haptic output to the user. The term haptic output (or feedback) comprises both, the human tactile and kinesthetic senses.
Reference: 4. <author> R. Balakrishnan, C. Ware, and T. Smith. </author> <title> Virtual Hand Tool with Force Feedback. </title> <editor> In C. Plaison, editor, </editor> <booktitle> Human Factors in Computing Systems, CHI'94. Conference Proceedings, Conference Companion, </booktitle> <pages> pages 83-84, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1994. </year> <title> ACM/SIGCHI, </title> <publisher> ACM Press. </publisher>
Reference-contexts: Although tactile and force feedback alone have been addressed by a number of researchers (see, e. g., <ref> [1, 2, 3, 4, 5] </ref>), their effect in standard interaction tasks has not been investigated in depth, and most evaluations are based on very small data sets only.
Reference: 5. <author> W. Kerstner, G. Pigel, and M. Tscheligi. </author> <title> The FeelMouse: Making Computer Screens Feelable. </title> <editor> In W. Zagler et al., editors, </editor> <booktitle> Computers for Handicapped Persons. Proc. of the ICCHP'94. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Although tactile and force feedback alone have been addressed by a number of researchers (see, e. g., <ref> [1, 2, 3, 4, 5] </ref>), their effect in standard interaction tasks has not been investigated in depth, and most evaluations are based on very small data sets only.
Reference: 6. <author> M. Bevan. Force-feedback technology. </author> <title> VR NEWS Virtual Reality Worldwide, </title> <booktitle> 4(6) </booktitle> <pages> 23-29, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: In addition, haptic displays are usually not available for the common user: "Whereas synthetic visual and audio images are omnipresent nowadays, opportunities to reach out and feel non-existent objects possessing texture, shape and inertia are, to put it mildly, not." <ref> [6] </ref> 3. The ForceMouse Most discussions on multimodal and multimedia user interface systems focus mainly on the visual and auditive modality [7]. In contrast, only little effort is made in integrating haptic information into the human-computer interaction process.
Reference: 7. <editor> M. M. Blattner and R. Dannenberg, editors. </editor> <booktitle> Multimedia Interface Design. ACM Press/Addison-Wesley, </booktitle> <year> 1992. </year>
Reference-contexts: The ForceMouse Most discussions on multimodal and multimedia user interface systems focus mainly on the visual and auditive modality <ref> [7] </ref>. In contrast, only little effort is made in integrating haptic information into the human-computer interaction process. Nearly all of the haptic displays commercially available are developed for special applications like telemanipulation or virtual reality (VR). Additionally, they often need permanent installation, are bulky, and very expensive.
Reference: 8. <author> R. Nikolai and B. Unger. </author> <title> Experimentelle Analyse bimodaler Interaktionsformen. </title> <type> Master's thesis, </type> <institution> Univer sitat Karlsruhe (TH), Institut fur Prozerechentechnik und Robotik, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: In: COMPUTER GRAPHICS forum, Vol. 15, No. 3, pp. 217-226, Conference Issue (Eurographics '96, Poitiers, France, August 26-30, 1996) 1996 - Copyright Eurographics Association Comprehensive tests with the ForceMouse have revealed two important results <ref> [8] </ref>: 1. Even the relatively simple haptic feedback provided by the ForceMouse makes interactions more com fortable and reduces positioning times significantly (by about 10%). 2. A more sophisticated control mechanism is needed to support interactions.
Reference: 9. <author> M. Stangenberg. </author> <title> Entwicklung eines Agenten fur haptische Ruckkopplungen in Standardbenutzungsober flachen. </title> <type> Master's thesis, </type> <institution> Universitat Karlsruhe (TH), Institut fur Prozerechentechnik und Robotik, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: Requirements and Concept The experiments' results outlined in section 3 helped us to identify the drawbacks of a simple and the requirements of a good controller for the ForceMouse <ref> [9] </ref>: Reaction time: In order to present the user a consistent and coherent feedback, the delay between entering a widget and launching the feedback should be minimized. Prediction: The best result regarding the reaction time can be achieved when the next widget is known in advance (a priori ). <p> When a sufficient amount of data has been collected and analyzed, it constitutes the basis for the prediction of future actions. In order to increase the performance of the prediction component and to model the user's behavior as closely as possible, two different approaches|trajectory-based and dialog-based prediction|have been combined <ref> [9] </ref>. 4.2. Prediction of user's actions The basic problem of predicting the user's actions can be described as follows: Given a set of widgets (potential targets) and the user's observable behavior (a trajectory), find the widget which is most likely to be used next.
Reference: 10. <author> F. Jelinek. </author> <title> Self-Organized Language Modelling for Speech Recognition. </title> <editor> In A. Waibel and K.-F. Lee, editors, </editor> <booktitle> Readings in Speech Recognition. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: P j (posj ~m i ) &lt; j ) widget ! j will not be used, no haptic feedback will be launched. 4.4. Dialog-based prediction The second method to predict the next user action has been adapted from speech recognition <ref> [10] </ref>. Instead of using Equation 2, socalled trigram probabilities approximate the a priori -probability: P (W ) j The idea is to model the semantics of the man-machine-dialog. Usually, some widgets will be used more frequently right after a specific other widget has been used.
Reference: 11. <editor> M. J. Wooldridge and N. R. Jennings, editors. </editor> <booktitle> Intelligent Agents | Theories, Architectures, and Lan guages, volume 890 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <month> January </month> <year> 1995. </year>
Reference-contexts: When the modules are completely independent, realized as separate processes and fulfilling certain criteria like autonomy, social ability, reactivity, etc., they are said to be agents <ref> [11] </ref>, and the architecture is called a multi-agent system, see Figure 5. By using PVM (Parallel Virtual Machine, [12]) as the underlying software package for interprocess communication, such a system is relatively easy to realize and might be extended with other components with minimal effort.
Reference: 12. <author> A. Geist et al. </author> <title> PVM: Parallel Virtual Machine|A User's Guide and Tutorial for Networked Parallel Computing. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, London, </address> <year> 1994. </year> <note> Also available via anonymous ftp from netlib2.cs-.utk.edu as pvm3/book/pvm-book.ps; or via WWW at http://www.netlib.org./pvm3/book/pvm-book.-html. </note>
Reference-contexts: When the modules are completely independent, realized as separate processes and fulfilling certain criteria like autonomy, social ability, reactivity, etc., they are said to be agents [11], and the architecture is called a multi-agent system, see Figure 5. By using PVM (Parallel Virtual Machine, <ref> [12] </ref>) as the underlying software package for interprocess communication, such a system is relatively easy to realize and might be extended with other components with minimal effort. The system is composed of four main components: The control tool ForceAgent is the heart of the system.
Reference: 13. <author> J. K. Ousterhout. </author> <title> Tcl and the Tk Toolkit. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1994. </year>
Reference-contexts: The Spy As mentioned above, the system has been developed to work with applications written in Tcl/Tk. Tcl is a simple scripting language, and Tk a toolkit for the X Window System (X) which provides its own widget set <ref> [13] </ref>. Besides its many features, Tk provides two mechanisms which are especially interesting: event bindings and the send command. An event binding allows to associate a script or a program with an event, e. g. the entering of a widget. Whenever the event is detected, the script will be processed. <p> Whenever the event is detected, the script will be processed. With the send command, this can be done in 'foreign' applications as well: "With send, any Tk application can invoke arbitrary Tcl scripts in any other Tk application on the display; [: : : ]" <ref> [13] </ref>. Although of great power, send is processed synchronously, i. e. the calling process has to wait for a reply.
References-found: 13

