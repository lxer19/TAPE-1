URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P706.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts98.htm
Root-URL: http://www.mcs.anl.gov
Email: E-mail: anitescu@mcs.anl.gov  E-mail: rserban@ccad.uiowa.edu  
Title: A Sparse Superlinearly Convergent SQP with Applications to Two-dimensional Shape Optimization  
Author: Mihai Anitescu Radu Serban 
Note: Address all correspondence to this author. The work of this author was supported by the Mathematical, Information and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38.  
Date: February 12, 1998  
Address: 9700 South Cass Avenue, Argonne, Illinois 60439  ERF 229, Iowa City, Iowa 52242  
Affiliation: Mathematics and Computer Science Division, Argonne National Laboratory  Department of Mechanical Engineering, The University of Iowa  
Abstract: Discretization of optimal shape design problems leads to very large nonlinear optimization problems. For attaining maximum computational efficiency, a sequential quadratic programming (SQP) algorithm should achieve superlinear convergence while preserving sparsity and convexity of the resulting quadratic programs. Most classical SQP approaches violate at least one of the requirements. We show that, for a very large class of optimization problems, one can design SQP algorithms that satisfy all these three requirements. The improvements in computational efficiency are demonstrated for a cam design problem. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Betts, J. T., and Frank P. D., </author> <year> 1994, </year> <title> A Sparse Nonlinear Optimization Algorithm, </title> <journal> Journal of Nonlinear Optimization Theory and Applications, </journal> <volume> Vol. 82, No. 3, </volume> <pages> pp. 519-541. </pages>
Reference: <author> Bertsekas, D. P., </author> <year> 1982, </year> <title> Constrained Optimization and Lagrange Multi 10 plier Methods, </title> <publisher> Academic Press, </publisher> <address> N.Y. </address>
Reference-contexts: At each step of this iterative procedure, a constrained quadratic program is solved. The desirable features of such an algorithm would be to achieve fast convergence, to generate easy-to-solve quadratic programs, and to preserve sparsity for fast linear algebra resolution. Fast convergence is usually associated with superlinear convergence <ref> (Bertsekas 1982) </ref>. A "not difficult" quadratic program is one that has a positive semidefinite matrix in the objective function because the resulting problem is convex. <p> As a variant of this method one can solve an additional quadratic program of the same form as (7) and do an arc search instead of a line search, in order to guarantee superlinear convergence. For details, see <ref> (Bertsekas 1982) </ref>. However, the most important factor in securing superlinear convergence is the choice of the H k matrix. Let L (x fl ; fl ) be the Lagrangian function associated with the program (4). <p> penalty function and to ensure that the sequence H k be uniformly bounded, positive definite on the column span of Z fl and satisfy lim [H k r 2 Here Z fl is matrix whose columns are a base for the nullspace of the Jacobian matrix of the active constraints <ref> (Bertsekas, Prop 4.32, 1982) </ref>. Since x fl is regular, the Jacobian of the active and equality constraints has full row rank. By Assumption E, it follows that this Jacobian is square and invertible. Therefore, its nullspace is 0, or Z fl = 0. <p> It is immediate that any constant sequence A satisfies all the requirements on H k for superlinear convergence of x k . rrr One way to ensure that the step length is unity for all k sufficiently big is to solve an additional QP and to do an arc search <ref> (Bertsekas 1982) </ref>. Since 6 this is not the focus of our investigation, we simply assume the stepsize to be unity for all sufficiently big k. An interesting conclusion is that sequential linear programming (A=0) will actually achieve superlinear convergence under these conditions.
Reference: <author> Braibant, V. and Fleury, C., </author> <year> 1984, </year> <title> Shape Optimal Design Using B-splines, </title> <booktitle> Computer Methods in Applied Mechanics and Engineering, </booktitle> <volume> Vol. 44, </volume> <pages> pp. 247-267. </pages>
Reference-contexts: 1 Introduction Within the class of potentially very large scale problems, shape optimization occupies an important place, being an essential part of the design of structures and mechanisms. The reduction of the continuous problem to a finite one via discretization or spline function approximations <ref> (Braibant and Fleury 1984) </ref> can lead to a very large nonlinear constrained optimization problem (NLP). Because of its origin, however, this NLP can be extremely sparse, as measured by the fill-in that appears in the rows of the Jacobian of the resulting constraints.
Reference: <author> Fletcher, R., </author> <year> 1987, </year> <title> Practical Methods of Optimization, </title> <publisher> John Willey & Sons, </publisher> <address> N.Y. </address>
Reference: <author> Gill, P. E., Murray, W., and Wright, M. H., </author> <year> 1981, </year> <title> Practical Optimization, </title> <publisher> Academic Press, </publisher> <address> N.Y. </address>
Reference: <author> Powell, M. J. D., </author> <year> 1978, </year> <title> The Convergence of Variable Metric Methods for Nonlinearly Constrained Optimization Calculations, Nonlinear Programming 3 (O. </title> <editor> L. Mangasarian, R. Meyer and S. Robinson, </editor> <booktitle> eds.), </booktitle> <pages> pp. 27-63, </pages> <publisher> Academic Press, </publisher> <address> N.Y. </address>
Reference-contexts: Although an acceleration can be observed by adaptively reducing the diagonal perturbation to zero, it is not clear whether superlinear convergence and convexity of the QP can be guaranteed under the usual assumptions. Finally, Powell's method <ref> (Powell 1978) </ref>, based on the BFGS formula, will result in a convex QP and will exhibit 2 superlinear convergence (with some line search modifications; see Bertsekas 1982) but it will destroy the sparse pattern of the problem.
Reference: <author> Vanderbei, R. J., </author> <year> 1994, </year> <title> LOQO: An Interior-Point Code for Quadratic Programming, </title> <type> Technical Report SOR-94-21, </type> <institution> Statistics and Operations Research, Princeton University, Princeton. </institution> - , <year> 1997, </year> <note> LOQO User's Manual Version 3.10, Technical Report SOR-97-08, </note> <institution> Statistics and Operations Research, Princeton University, Princeton. </institution>
Reference-contexts: A constant, sparse positive definite matrix will be enough to ensure the convexity and sparsity of the resulting QP. An SQP algorithm, implemented in Matlab, that solves the QP based on an interior-point technique <ref> (Vanderbei 1994) </ref> with sparsity support is used for the NLP that results from the discretization of a cam design problem. <p> The first one is provided within Matlab and uses an active set strategy, similar to the one described by Gill et. al. (1981). The second one, Loqo <ref> (Vanderbei 1994 and 1997) </ref>, is an interior-point algorithm that uses a one-phase primal-dual path-following method. Table 1 presents the evolution of the norm of the Newton direction, using the interior-point algorithm, with m = 101.
Reference: <author> Wright, S. J., </author> <year> 1997, </year> <title> Primal-Dual Interior Point Methods, </title> <publisher> SIAM, Philadel-phia. </publisher>
Reference-contexts: Fast convergence is usually associated with superlinear convergence (Bertsekas 1982). A "not difficult" quadratic program is one that has a positive semidefinite matrix in the objective function because the resulting problem is convex. Results from the past decade show that such quadratic programs have only polynomial complexity <ref> (Wright 1997) </ref>, and are easy to solve from the viewpoint of complexity theory. Unfortunately, traditional SQP approaches do not achieve, at the same time, superlinear convergence and convexity and sparsity of the resulting quadratic program (QP).

References-found: 8

