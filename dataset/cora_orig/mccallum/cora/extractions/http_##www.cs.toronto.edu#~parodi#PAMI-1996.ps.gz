URL: http://www.cs.toronto.edu/~parodi/PAMI-1996.ps.gz
Refering-URL: http://www.cs.toronto.edu/~parodi/abstract_pami.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D.A.Huffman. </author> <title> Impossible objects as nonsense sentences. </title> <journal> Machine Intelligence, </journal> <volume> 6 </volume> <pages> 295-323, </pages> <year> 1971. </year>
Reference-contexts: The key step for the interpretation of line drawings is thought to be the consistent labeling of its segments as the projection of convex ("+"), concave ("") or contour ("!") edges <ref> [1, 2, 3] </ref>. Labelability is a necessary condition for a line drawing to be realizable as the projection of a polyhedral scene. <p> The reader is referred to any of the papers <ref> [1, 2, 3, 14] </ref> for the labeling step. The 3D elements of the scene will be called vertices, edges, and planar faces, whereas the corresponding 2D elements of the line drawing will be called junctions, segments, and polygons. Junctions can have an arbitrary number of incident segments. <p> It is also assumed that every visible planar panel (except for the background) has at least two nonparallel visible edges, which is usually verified in nonpathological cases. Following <ref> [1, 2, 15] </ref>, segments are assigned a label describing some 3D physical properties.
Reference: [2] <author> M.B. Clowes. </author> <title> On seeing things. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 79-116, </pages> <year> 1971. </year>
Reference-contexts: The key step for the interpretation of line drawings is thought to be the consistent labeling of its segments as the projection of convex ("+"), concave ("") or contour ("!") edges <ref> [1, 2, 3] </ref>. Labelability is a necessary condition for a line drawing to be realizable as the projection of a polyhedral scene. <p> The reader is referred to any of the papers <ref> [1, 2, 3, 14] </ref> for the labeling step. The 3D elements of the scene will be called vertices, edges, and planar faces, whereas the corresponding 2D elements of the line drawing will be called junctions, segments, and polygons. Junctions can have an arbitrary number of incident segments. <p> It is also assumed that every visible planar panel (except for the background) has at least two nonparallel visible edges, which is usually verified in nonpathological cases. Following <ref> [1, 2, 15] </ref>, segments are assigned a label describing some 3D physical properties.
Reference: [3] <author> D. Waltz. </author> <title> Understanding line-drawings of scenes with shadows. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 79-116, </pages> <year> 1971. </year>
Reference-contexts: The key step for the interpretation of line drawings is thought to be the consistent labeling of its segments as the projection of convex ("+"), concave ("") or contour ("!") edges <ref> [1, 2, 3] </ref>. Labelability is a necessary condition for a line drawing to be realizable as the projection of a polyhedral scene. <p> The reader is referred to any of the papers <ref> [1, 2, 3, 14] </ref> for the labeling step. The 3D elements of the scene will be called vertices, edges, and planar faces, whereas the corresponding 2D elements of the line drawing will be called junctions, segments, and polygons. Junctions can have an arbitrary number of incident segments.
Reference: [4] <author> K. Sugihara. </author> <title> A necessary and sufficient condition for a picture to represent a polyhedral scene. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6(5) </volume> <pages> 578-586, </pages> <year> 1984. </year>
Reference-contexts: Labelability is a necessary condition for a line drawing to be realizable as the projection of a polyhedral scene. Sugihara <ref> [4] </ref> found a necessary and sufficient condition for a labeled line drawing to be realizable as the orthographic/perspective projection of a polyhedral scene, and presented an algorithm which actually found a 3D reconstruction (if possible). The method was by reduction to Linear Programming, and it therefore took polynomial time. <p> Junctions can have an arbitrary number of incident segments. The scene is assumed to be composed by a finite number of planar panels, which are to be regarded, as in <ref> [4] </ref>, as opaque 2D polygonal areas with possible polygonal holes. This assumption includes, as a special case, polyhedral scenes (in that they can always be simulated as scenes of planar panels.) It is assumed that no planar panel lies on a planar surface containing the optical center. <p> A detailed treatment of the construction of the spatial structure can be found in <ref> [4] </ref>. The general idea is given in Section 4 of this paper. The concept of components of the incidence structure can be introduced. Each component can be seen as a different object (or a set of physically connected objects) 4 . <p> The problem of labeling a line drawing of a trihedral scene (the case examined in [7]) can be shown to be N P-complete even for this kind of labeling. 3 The following definition slightly differs from the one given in <ref> [4] </ref>. 4 This means no abuse, as if a line drawing L is realizable as the perspective projection of a polyhedral scene, there is always a realization of L in which all the components of the incidence structure correspond to physically distinct objects (or sets of touching objects). 3 3 Using <p> The main steps are the following. 1. Extracting the spatial structure and finding the components The spatial structure is extracted similarly to <ref> [4] </ref>, the main difference being that the information on vanishing points allows to put all depth relations as relations between vertices (and not between vertices and panels). The general idea is the following. A panel f is created for every polygon (f ), except for the background. <p> whether a p-tuple (t 1 ; : : : t p ) of scale factors (one for each component) can be found such that the resulting 3D locations of all vertices simultaneously satisfy all the incidence and depth relations specified in the sets R and T of the spatial structure <ref> [4] </ref>. The realizability check can be subdivided into the three following independent tests, which are to be performed in sequence. <p> Table 3 near here 6 Conclusion The a priori information on vanishing points allows to solve some of the problems of the usual approach to the reconstruction problem <ref> [4] </ref>. Let us take a closer look at the advantages of this approach. Error tolerance. According to [4], the incidence structure I = (V; F; R) is said to be position-free if realizability of the line drawing is not affected by slightly moving any of its junctions. <p> Table 3 near here 6 Conclusion The a priori information on vanishing points allows to solve some of the problems of the usual approach to the reconstruction problem <ref> [4] </ref>. Let us take a closer look at the advantages of this approach. Error tolerance. According to [4], the incidence structure I = (V; F; R) is said to be position-free if realizability of the line drawing is not affected by slightly moving any of its junctions. <p> Algebraically, this means that the system of linear equations associated, as in <ref> [4] </ref>, with (B), has redundant equations. The straightforward application of Linear Programming to (B) is therefore meaningless. The solution proposed in [4] is the introduction of a computationally costly mechanism of junction-position correction. The approach of this paper is different. <p> Algebraically, this means that the system of linear equations associated, as in <ref> [4] </ref>, with (B), has redundant equations. The straightforward application of Linear Programming to (B) is therefore meaningless. The solution proposed in [4] is the introduction of a computationally costly mechanism of junction-position correction. The approach of this paper is different.
Reference: [5] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1992. </year>
Reference-contexts: of the scene is not unique; (ii) the slightest error in the line drawing can make the reconstruction impossible, and a procedure to correct the location of junctions is necessary; (iii) Linear Programming is P-complete, which means that it is presumably impossible to find efficient parallel algorithms that solve it <ref> [5, 6] </ref>. This paper studies an interesting special case of the reconstruction problem, in which the location of the vanishing points is known.
Reference: [6] <author> R. M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In J. Van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science: Algorithms and Complexity, chapter 17, </booktitle> <pages> pages 869-942. </pages> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: of the scene is not unique; (ii) the slightest error in the line drawing can make the reconstruction impossible, and a procedure to correct the location of junctions is necessary; (iii) Linear Programming is P-complete, which means that it is presumably impossible to find efficient parallel algorithms that solve it <ref> [5, 6] </ref>. This paper studies an interesting special case of the reconstruction problem, in which the location of the vanishing points is known.
Reference: [7] <author> L.M.Kirousis and C.H.Papadimitriou. </author> <title> The complexity of recognizing polyhedral scenes. </title> <journal> Journal of Computer and System Sciences, </journal> (37):14-38, 1988. 
Reference-contexts: It is an extension to perspective projections of general scenes with planar surfaces and to an arbitrary number of admissible directions of a previous result by Kirousis & Papadimitriou <ref> [7] </ref> on the realizability of Manhattan (or Legoland) scenes, that is scenes where edges have only three admissible orthogonal directions. <p> The problem of labeling a line drawing of a trihedral scene (the case examined in <ref> [7] </ref>) can be shown to be N P-complete even for this kind of labeling. 3 The following definition slightly differs from the one given in [4]. 4 This means no abuse, as if a line drawing L is realizable as the perspective projection of a polyhedral scene, there is always a
Reference: [8] <author> M.Straforini, C.Coelho, M.Campani, and V.Torre. </author> <title> The recovery and understanding of a line drawing from indoor scenes. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14(2) </volume> <pages> 298-303, </pages> <year> 1992. </year>
Reference-contexts: The case treated in this paper seems relevant in practice, as the vanishing points can often be obtained from the image itself by standard techniques (see <ref> [8, 9, 10, 11, 12, 13] </ref>) 1 . <p> If some special assumption about the scene are satisfied, O (N ) methods can be used <ref> [8] </ref>. In this work, we have used whenever it was possible a combination of the algorithm [8] with a method [14] which allows to find all the vanishing points of the line drawing from a small set of known vanishing points. <p> If some special assumption about the scene are satisfied, O (N ) methods can be used <ref> [8] </ref>. In this work, we have used whenever it was possible a combination of the algorithm [8] with a method [14] which allows to find all the vanishing points of the line drawing from a small set of known vanishing points. The resulting algorithm takes time O (N ). 1 of planar panels from a labeled line drawing. <p> After some low-level processing of the image 10 the vanishing points were extracted as described in [20]. Junctions were recovered by exploiting the information on vanishing points as in <ref> [8] </ref>, thus obtaining the line drawing of Fig. 4 (B). The line drawing has many missing and spurious segments; yet, the labeling shown in Fig. 4 (B) allows correctly to recover the main features of the 3D scene. <p> of Fig. 5 and 4; the error on junction locations has been considered to be equal to the threshold under which the end-points of two segments are considered to belong to the same junction (8 pxl in the case of Fig. 4, 10 pxl in the case of Fig. 5) <ref> [8, 21] </ref>. 5.2 Line drawings extracted by model-based techniques In most images of outdoor and indoor scenes, it is not possible to extract a line drawing in a bottom-up automatic way. <p> In the case above, three vertices must be created (one for each occluding segment). The edges projected in segments marked as "fl" are assumed to be parallel to f 1 . Fig. 4: (A) A picture of a laboratory. (B) The labeled line drawing extracted by the algorithm in <ref> [8] </ref>, along with the 3D vertices produced by the reconstruction algorithm. Fig. 5: (A) A photograph of a set of boxes on a table. (B) The line drawing extracted by the algorithm in [21], labeled, and the 3D vertices extracted by the reconstruction algorithm.
Reference: [9] <author> B. Brillault and O'Mahony. </author> <title> New method for vanishing points detection. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 54(2) </volume> <pages> 289-300, </pages> <year> 1991. </year>
Reference-contexts: The case treated in this paper seems relevant in practice, as the vanishing points can often be obtained from the image itself by standard techniques (see <ref> [8, 9, 10, 11, 12, 13] </ref>) 1 .
Reference: [10] <author> H. Nakatani and T. Kitahashi. </author> <title> Determination of vanishing point in outdoor scene. </title> <journal> Trans. IEICE, </journal> <volume> 64(5) </volume> <pages> 357-358, </pages> <year> 1981. </year>
Reference-contexts: The case treated in this paper seems relevant in practice, as the vanishing points can often be obtained from the image itself by standard techniques (see <ref> [8, 9, 10, 11, 12, 13] </ref>) 1 .
Reference: [11] <author> S. T. Barnard. </author> <title> Interpreting prospective images. </title> <journal> A.I. Journal, </journal> <volume> 21(4) </volume> <pages> 435-462, </pages> <year> 1983. </year>
Reference-contexts: The case treated in this paper seems relevant in practice, as the vanishing points can often be obtained from the image itself by standard techniques (see <ref> [8, 9, 10, 11, 12, 13] </ref>) 1 .
Reference: [12] <author> Quan and R. Mohr. </author> <title> Determining perspective structures using hierarchical hough transformation. </title> <journal> Pattern Recognition Letters, </journal> <volume> 9 </volume> <pages> 279-286, </pages> <year> 1989. </year>
Reference-contexts: The case treated in this paper seems relevant in practice, as the vanishing points can often be obtained from the image itself by standard techniques (see <ref> [8, 9, 10, 11, 12, 13] </ref>) 1 .
Reference: [13] <author> M. J. Magee and J. K. Aggarwal. </author> <title> Determining vanishing points form perspective images. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 26 </volume> <pages> 256-267, </pages> <year> 1984. </year>
Reference-contexts: The case treated in this paper seems relevant in practice, as the vanishing points can often be obtained from the image itself by standard techniques (see <ref> [8, 9, 10, 11, 12, 13] </ref>) 1 .
Reference: [14] <author> P. Parodi and V. Torre. </author> <title> On the complexity of labeling line drawings of polyhedral scenes. </title> <journal> Artificial Intelligence, </journal> <volume> 70 </volume> <pages> 239-276, </pages> <year> 1994. </year>
Reference-contexts: The case treated in this paper seems relevant in practice, as the vanishing points can often be obtained from the image itself by standard techniques (see [8, 9, 10, 11, 12, 13]) 1 . This analysis is further motivated by the fact that in <ref> [14] </ref> it was recently shown that once the location of the vanishing points of the line drawing (of a trihedral, solid scene) is known, the labeling problem has polynomial complexity, and it is important to exploit the constraints provided by the vanishing points in the quantitative reconstruction stage as well. <p> If some special assumption about the scene are satisfied, O (N ) methods can be used [8]. In this work, we have used whenever it was possible a combination of the algorithm [8] with a method <ref> [14] </ref> which allows to find all the vanishing points of the line drawing from a small set of known vanishing points. The resulting algorithm takes time O (N ). 1 of planar panels from a labeled line drawing. <p> so that no correction mechanism is necessary; (iii) the complexity of the algorithm is lower than the Linear Programming method by Sugihara, and the algorithm is conceptually simpler; (iv) the algorithm can be easily parallelized; (v) in the case of trihedral scenes, a labeling can be obtained in polynomial time <ref> [14] </ref>. <p> The reader is referred to any of the papers <ref> [1, 2, 3, 14] </ref> for the labeling step. The 3D elements of the scene will be called vertices, edges, and planar faces, whereas the corresponding 2D elements of the line drawing will be called junctions, segments, and polygons. Junctions can have an arbitrary number of incident segments. <p> It is not possible to apply linear time algorithms as the one described in <ref> [14] </ref>, and this is one of the reasons for which the resort to model-based reconstruction as in Section 5.2 is more profitable. 9 shown in Fig. 4 and 5. <p> The line drawing in Fig. 6 (B) was obtained from the output of the low-level algorithms as described in [24], and then labeled by the algorithm described in <ref> [14] </ref> (same figure). A calibration sequence has been used to calibrate the camera, thus obtaining a focal length of 595 5pxl. The vanishing points were automatically extracted from the image as described in [20, 14]. <p> A calibration sequence has been used to calibrate the camera, thus obtaining a focal length of 595 5pxl. The vanishing points were automatically extracted from the image as described in <ref> [20, 14] </ref>. As an output of the application of the metrical reconstruction algorithm, the 3D 11 positions of all 61 vertices, marked in Fig. 6 (B) by their identification number, have been obtained.
Reference: [15] <author> T. Kanade. </author> <title> A theory of Origami world. </title> <journal> Artificial Intelligence, </journal> <volume> 13 </volume> <pages> 279-311, </pages> <year> 1980. </year>
Reference-contexts: It is also assumed that every visible planar panel (except for the background) has at least two nonparallel visible edges, which is usually verified in nonpathological cases. Following <ref> [1, 2, 15] </ref>, segments are assigned a label describing some 3D physical properties. <p> The labeling shown in (B) corresponds to an interpretation of (A) as an Origami scene (the rear part of the van is viewed as a hollow box) <ref> [15, 25, 26] </ref>. Since the line drawing depicted in Fig 7 (A) has four meaningful components (five, if considering the `window' through which the scene is viewed), four scale factors need to be fixed in order to have an absolute 3D reconstruction of the scene.
Reference: [16] <author> J. Malik. </author> <title> Interpreting line drawings of curved objects. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 73-103, </pages> <year> 1987. </year>
Reference-contexts: Other examples of collapsed labelings can be found in Mackworth (Ph. D. thesis) and <ref> [16] </ref> 2 . The labeled line drawing already contains much information on the incidence and depth relations between the geometrical elements of the scene (vertices, edges, panels). Such relations are made explicit in the incidence structure and in the spatial structure associated with the line drawing.
Reference: [17] <author> A.K.Mackworth. </author> <title> Interpreting pictures of polyhedral scenes. </title> <journal> Artificial Intelligence, </journal> (4):121-137, 1984. 
Reference-contexts: Two examples of the application of our algorithm to line drawings automatically extracted are 9 A reasonable labeling of segments as plain, arrow and dashed can be achieved by Constraint Satisfaction methods <ref> [17] </ref>, along with some heuristic rules for pruning the set of possible labelings.
Reference: [18] <author> J. Canny. </author> <title> A computational approach to edge detection. </title> <journal> pami, </journal> (8):199-213, 1986. 
Reference-contexts: Table 1 reports the size of the three boxes as computed by the location of the 3D vertices. Table 1 near here 10 This and all the other real-world images shown in this paper have undergone the following elaborations: edge extraction by a Canny <ref> [18] </ref> edge detector; chain following; polygonal approximation by Pavlidis' algorithm [19].
Reference: [19] <author> T. Pavlidis. </author> <title> Algorithms for Graphics and Image Processing. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1982. </year>
Reference-contexts: Table 1 near here 10 This and all the other real-world images shown in this paper have undergone the following elaborations: edge extraction by a Canny [18] edge detector; chain following; polygonal approximation by Pavlidis' algorithm <ref> [19] </ref>.
Reference: [20] <author> M. Straforini, C. Coelho, and M. Campani. </author> <title> Extraction of vanishing points of images of indoor and outdoor scenes. </title> <journal> Image Vision Computing, </journal> <volume> 11(2) </volume> <pages> 91-100, </pages> <year> 1993. </year>
Reference-contexts: Fig. 4 (A) depicts a rather complicated Legoland scene (that is, a scene with only three vanishing points corresponding to orthogonal 3D directions). After some low-level processing of the image 10 the vanishing points were extracted as described in <ref> [20] </ref>. Junctions were recovered by exploiting the information on vanishing points as in [8], thus obtaining the line drawing of Fig. 4 (B). <p> A calibration sequence has been used to calibrate the camera, thus obtaining a focal length of 595 5pxl. The vanishing points were automatically extracted from the image as described in <ref> [20, 14] </ref>. As an output of the application of the metrical reconstruction algorithm, the 3D 11 positions of all 61 vertices, marked in Fig. 6 (B) by their identification number, have been obtained.
Reference: [21] <author> M. Gatti, P. Olivieri, M. Straforini, and V. Torre. </author> <title> Robust recovery of the 3D structure of indoor scenes. </title> <booktitle> In 2nd Int. Workshop on Robust Computer Vision, </booktitle> <pages> pages 401-412, </pages> <address> Bonn, Germany, </address> <year> 1992. </year> <month> 16 </month>
Reference-contexts: Fig. 5 (A) is the picture of a set of boxes laid upon a table. It is not a Legoland scene; in these conditions the extraction of the line drawing can be best performed by a general algorithm as the one described in in <ref> [21] </ref>. The result is shown in Fig 5 (B). Observe that this line drawing is rather faithful, in that no segment is missing and there are only two spurious segments, close to the two visible table vertices. <p> of Fig. 5 and 4; the error on junction locations has been considered to be equal to the threshold under which the end-points of two segments are considered to belong to the same junction (8 pxl in the case of Fig. 4, 10 pxl in the case of Fig. 5) <ref> [8, 21] </ref>. 5.2 Line drawings extracted by model-based techniques In most images of outdoor and indoor scenes, it is not possible to extract a line drawing in a bottom-up automatic way. <p> Fig. 5: (A) A photograph of a set of boxes on a table. (B) The line drawing extracted by the algorithm in <ref> [21] </ref>, labeled, and the 3D vertices extracted by the reconstruction algorithm. Fig. 6: (A) A frame from a sequence acquired by a camere set on a moving vehicle. (B) The labeled line drawing extracted as described in [24], and the 3D vertices extracted by the reconstruction algorithm.
Reference: [22] <author> B. Caprile and V.Torre. </author> <title> Using vanishing points for TV camera calibration. </title> <journal> Int. J. Computer Vision, </journal> <year> 1990. </year>
Reference-contexts: The different high-level elaborations are specified in the text. 10 In both examples, the focal length was unknown and has been computed according to the calibration algorithm shown in <ref> [22] </ref> which gave two focal lengths, f = 918 20 pxl and f = 481 10 pxl respectively for the images of Fig. 5 and 4; the error on junction locations has been considered to be equal to the threshold under which the end-points of two segments are considered to belong
Reference: [23] <author> K. Kadono, M. Asada, and Y. Shirai. </author> <title> Context-constrained matching of hierarchical cad-based models for outdoor scene interpretation. </title> <booktitle> In IEEE Workshop, </booktitle> <pages> pages 186-195, </pages> <address> Hawaii, USA, </address> <year> 1991. </year>
Reference-contexts: There are, however, methods by which is possible to extract a line drawing by model-based feature extraction schemes <ref> [23, 24] </ref>: the extraction is guided by prior knowledge on the environment. This seems the most promising field of application of our algorithm. It is useful to spend a few words about these schemes. <p> This seems the most promising field of application of our algorithm. It is useful to spend a few words about these schemes. In <ref> [23] </ref> a context constrained matching of hierarchical CAD based models for outdoor scene interpretation is proposed. The system can deal with outdoor scenes with artificial objects which may have various shapes and sizes, and these are not in finite number but vary according to a number of parameters.
Reference: [24] <author> M. Campani, M. Cappello, G. Piccioli, E. Reggio, M. Straforini, and V. Torre. </author> <title> Visual routines for outdoor navigation. </title> <booktitle> In Proc. of Intelligent Vehicles Symposium, </booktitle> <pages> pages 107-112, </pages> <address> Tokyo, </address> <year> 1993. </year>
Reference-contexts: There are, however, methods by which is possible to extract a line drawing by model-based feature extraction schemes <ref> [23, 24] </ref>: the extraction is guided by prior knowledge on the environment. This seems the most promising field of application of our algorithm. It is useful to spend a few words about these schemes. <p> Context information (e.g., a car always lies on the ground plane) is used to constrain the search space and reduce the number of parameters. A similar approach is adopted in <ref> [24] </ref>, where the main features of the scene (road, cars, buildings, trees...) are identified by independent, ad hoc criteria and their parameters (width, length, height...) are computed after the identification process. As an example, Fig. 6 (A) shows an image of a typical road scene. <p> The line drawing in Fig. 6 (B) was obtained from the output of the low-level algorithms as described in <ref> [24] </ref>, and then labeled by the algorithm described in [14] (same figure). A calibration sequence has been used to calibrate the camera, thus obtaining a focal length of 595 5pxl. The vanishing points were automatically extracted from the image as described in [20, 14]. <p> Fig. 6: (A) A frame from a sequence acquired by a camere set on a moving vehicle. (B) The labeled line drawing extracted as described in <ref> [24] </ref>, and the 3D vertices extracted by the reconstruction algorithm. Fig. 7: (A) An image of a typical urban scene. (B) The labeled line drawing, extracted in a semiautomatic fashion, and the 3D vertices extracted by the reconstruction algorithm. There are four meaningful components.
Reference: [25] <author> P. Parodi. </author> <title> Labeling perspective projections of origami scenes. </title> <booktitle> In Asian Conf. of Comp. Vision, </booktitle> <volume> ACCV '93, </volume> <pages> pages 446-449, </pages> <address> Osaka, Japan, </address> <year> 1993. </year>
Reference-contexts: The labeling shown in (B) corresponds to an interpretation of (A) as an Origami scene (the rear part of the van is viewed as a hollow box) <ref> [15, 25, 26] </ref>. Since the line drawing depicted in Fig 7 (A) has four meaningful components (five, if considering the `window' through which the scene is viewed), four scale factors need to be fixed in order to have an absolute 3D reconstruction of the scene.
Reference: [26] <author> P. Parodi. </author> <title> The complexity of understanding of origami scenes. </title> <journal> International Journal of Computer Vision, </journal> <note> 1994. Accepted. </note>
Reference-contexts: The labeling shown in (B) corresponds to an interpretation of (A) as an Origami scene (the rear part of the van is viewed as a hollow box) <ref> [15, 25, 26] </ref>. Since the line drawing depicted in Fig 7 (A) has four meaningful components (five, if considering the `window' through which the scene is viewed), four scale factors need to be fixed in order to have an absolute 3D reconstruction of the scene.
Reference: [27] <author> T. Kanade. </author> <title> From a real chair to a negative chair. </title> <journal> Artificial Intelligence, </journal> <volume> 59(1-2):95-101, </volume> <year> 1993. </year>
Reference-contexts: Unique reconstruction of 3-D geometry. There are in general many possible 3D realizations associated with the same labeled line drawing. This is in contrast with the fact that human observers usually understand a picture in only one way, which is considered somehow 13 more natural <ref> [27] </ref>.
Reference: [28] <author> D. Dobkin, R. J. Lipton, and S. Reiss. </author> <title> Linear programming is log-space hard for p. </title> <journal> Inf. Process. Lett., </journal> (9):96-97, 1979. 
Reference-contexts: If a scale factor is available for every component, the computational complexity becomes O (N ). Parallelizability. Linear Programming is among the most difficult problems in P (the set of polynomially solvable problems), in the sense that it is P -complete <ref> [28] </ref>. This means that, presumably, it is not efficiently parallelizable (a problem of input size n is said to be efficiently parallelizable if it can be solved in O (log O (1) n) steps by O (n O (1) ) processors).

References-found: 28

