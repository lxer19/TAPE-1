URL: http://www.cs.cmu.edu/afs/cs/user/will/www/papers/Uther97a.ps
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: futher,velosog@cs.cmu.edu  
Title: Adversarial Reinforcement Learning  
Author: William Uther and Manuela Veloso 
Date: April 24, 1997  
Address: Pittsburgh, PA 15213  
Affiliation: Computer Science Department Carnegie Mellon University  
Abstract: Reinforcement Learning has been used for a number of years in single agent environments. This article reports on our investigation of Reinforcement Learning techniques in a multi-agent and adversarial environment with continuous observable state information. We introduce a new framework, two-player hexagonal grid soccer, in which to evaluate algorithms. We then compare the performance of several single-agent Reinforcement Learning techniques in that environment. These are further compared to a previously developed adversarial Reinforcement Learning algorithm designed for Markov games. Building upon these efforts, we introduce new algorithms to handle the multi-agent, the adversarial, and the continuous-valued aspects of the domain. We introduce a technique for modelling the opponent in an adversarial game. We introduce an extension to Prioritized Sweeping that allows generalization of learnt knowledge over neighboring states in the domain; and we introduce an extension to the U Tree generalizing algorithm that allows the handling of continuous state spaces. Extensive empirical evaluation is conducted in the grid soccer domain. 
Abstract-found: 1
Intro-found: 1
Reference: [Baird, 1995] <author> Leemon Baird. </author> <title> Residual algorithms: Reinforcement learning with function approximation. </title> <editor> In A Prieditis and S Russell, editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 3037, </pages> <address> San Francisco, C.A., 1995. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: This has been shown to be unstable in some cases [Boyan and Moore, 1995] never converging to a solution although some good results have been obtained with this method [Tesauro, 1995]. Methods that are stable do exist for using either Neural Nets <ref> [Baird, 1995] </ref> or Decision Trees [McCallum, 1995]. First we introduce a stable method which uses both a table and a generalizing function approximator.
Reference: [Boyan and Moore, 1995] <author> J. A. Boyan and A. W. Moore. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Tesauro, D. S. Touretzky, and T. K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 7, </booktitle> <address> Cambridge, MA, 1995. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: One naive generalization method is to take a simple model-free algorithm like Q Learning and replace the table of values with a generalizing function approximator (e.g. a Neural Net). This has been shown to be unstable in some cases <ref> [Boyan and Moore, 1995] </ref> never converging to a solution although some good results have been obtained with this method [Tesauro, 1995]. Methods that are stable do exist for using either Neural Nets [Baird, 1995] or Decision Trees [McCallum, 1995].
Reference: [Boyan and Moore, 1996] <author> Justin A. Boyan and Andrew W. Moore. </author> <title> Learning evaluation functions for large acyclic domains. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: First we introduce a stable method which uses both a table and a generalizing function approximator. Then [McCallum, 1995]'s method using Decision Trees is described and we extend it to handle continuous state values. 4.1 Fitted Prioritized Sweeping As pointed out in <ref> [Boyan and Moore, 1996] </ref>, generalizing function approximators are not a problem if the approximation is not iterated. That is, if an approximation is not used to update itself. Fitted Prioritized Sweeping makes use of this result by using standard Prioritized Sweeping as a base, and then doing the generalization afterwards.
Reference: [Kaelbling et al., 1996] <author> Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4:237285, </volume> <year> 1996. </year>
Reference-contexts: Indeed, one of the first areas to be studied in Artificial Intelligence was game playing. For example, the pioneering checkers playing algorithm by [Samuel, 1959] used both search and machine learning strategies. Interestingly, his approach is similar to modern Reinforcement Learning techniques <ref> [Kaelbling et al., 1996] </ref>. An evaluation function that guides the selection of moves is represented as a parameterized weighted sum of game features. Parameters are incrementally refined as a function of the game playing performance. <p> This work has been repeated in other domains, but again, without the same success as in the checkers domain (in <ref> [Kaelbling et al., 1996] </ref>). [Littman, 1994] took standard Q Learning, [Watkins and Dayan, 1992], and modified it to work with Markov games. He replaced the simple min update used in standard Q Learning with a mixed strategy (probabilistic) minimax update. <p> Similarly to the rest of the experiments, these 1,000 games were split into the first 500 games and the second 500 games to measure learning speed and final learned ability. 3 State-Specific Learning Algorithms In the machine learning formulation of Reinforcement Learning <ref> [Kaelbling et al., 1996] </ref> there are a discrete set of states, s, and actions, a. The agent can detect its current state, and in each state can choose to perform an action which will in turn move it to its next state. <p> One might expect G = P 1 t=1 R t we just sum the reinforcement signal for every timestep to get some measure of how we are doing. Unfortunately, this sum diverges. The standard solution, although others have been tried (see <ref> [Kaelbling et al., 1996] </ref>), is to discount future rewards. A discount factor, fl, 0 &lt; fl &lt; 1, is added to the sum giving: G = P 1 t=1 fl t R t .
Reference: [Littman, 1994] <author> Michael L. Littman. </author> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> Machine Learning, </booktitle> <address> 11:157163, </address> <year> 1994. </year> <month> 17 </month>
Reference-contexts: This work has been repeated in other domains, but again, without the same success as in the checkers domain (in [Kaelbling et al., 1996]). <ref> [Littman, 1994] </ref> took standard Q Learning, [Watkins and Dayan, 1992], and modified it to work with Markov games. He replaced the simple min update used in standard Q Learning with a mixed strategy (probabilistic) minimax update. <p> He replaced the simple min update used in standard Q Learning with a mixed strategy (probabilistic) minimax update. He then evaluated this by playing against both standard Q Learning and random players in a simple game. The game used in <ref> [Littman, 1994] </ref> is a small two player grid soccer game designed to be able to be solved quickly by traditional Q Learning techniques. He trained 4 different players for his game. Two players used his algorithm, two used normal Q Learning. <p> His results showed that his algorithms, which learned a probabilistic strategy, performed better under these conditions than Q Learning, which learned a deterministic strategy, or his hand coded, but again deterministic, strategy. We use a similar environment to that used by <ref> [Littman, 1994] </ref> to investigate Markov games. Our environment is larger, both in number of states and number of actions per state, to more effectively test the generalization capabilities of our algorithms. We conduct tests where both players are learning as they play. <p> Finally, we look at what can be learned by looking at the world from your opponent's point of view. 2 2 Hexcer: The Adversarial Learning Environment As a substrate to our investigation, we introduce a hexagonal grid based soccer simulation, Hexcer, which is similar to the game framework used by <ref> [Littman, 1994] </ref> to test Minimax Q Learning. Hexcer consists of a board with a hexagonal grid, two players and a ball (See The two players start in fixed positions on the board, as shown. The game then proceeds in rounds. <p> As stated above, we used a fixed learning rate. We trading guaranteed convergence for the ability to learn the non-stationary policy we need to play against an opponent that is itself learning. 5 3.2 Minimax Q Learning Minimax Q Learning <ref> [Littman, 1994] </ref> is a modification of Q Learning to work with Markov games. Instead of treating the opponent as part of the environment, Minimax Q Learning records a function not just from state/action pairs to values, but from state/action/action triples to values, Q (s; a m ; a o ). <p> The result is that we should choose move a 0 m with 6 probability 0.47, and move a 1 m with probability 0.53. The expected sum of discounted reward is 292. This the value, V (s), of this state. As <ref> [Littman, 1994] </ref> points out, this is a pessimistic value. Whatever the opponent actually does, the expected return value generated, and probability distribution over our moves, will be that for playing an optimal opponent. <p> Minimax Q Learning The reason for this is that the stochastic updates in normal Q Learning result in a distribution of moves that approximates the optimal mixed strategy. As long as these updates continue an explicit mixed strategy is not necessary. As <ref> [Littman, 1994] </ref> points out though, if you stop learning then the explicit mixed strategy is an advantage against an opponent that is learning. 3.4 Opponent Modelling Q Learning Minimax Q Learning is a pessimistic algorithm. <p> Opponent Modelling It should be noted that the opponent modelling strategy is not a mixed (probabilistic) strategy. If we stopped performing updates as <ref> [Littman, 1994] </ref> did then it would fail the same way Q Learning does against a learning opponent.
Reference: [McCallum, 1995] <author> Andrew Kachites McCallum. </author> <title> Reinforcement Learning with Selective Per--ception and Hidden State. </title> <type> Phd. thesis, </type> <institution> University of Rochester, </institution> <year> 1995. </year>
Reference-contexts: All of these techniques rely on a table of values and actions and do not generalize between similar or equivalent states. The learned tables are state-specific. We introduce Fitted Prioritized Sweeping and a modification of the U Tree algorithm <ref> [McCallum, 1995] </ref>, Continuous U Tree, as examples of algorithms that generalize over multiple states. <p> This has been shown to be unstable in some cases [Boyan and Moore, 1995] never converging to a solution although some good results have been obtained with this method [Tesauro, 1995]. Methods that are stable do exist for using either Neural Nets [Baird, 1995] or Decision Trees <ref> [McCallum, 1995] </ref>. First we introduce a stable method which uses both a table and a generalizing function approximator. Then [McCallum, 1995]'s method using Decision Trees is described and we extend it to handle continuous state values. 4.1 Fitted Prioritized Sweeping As pointed out in [Boyan and Moore, 1996], generalizing function approximators <p> Methods that are stable do exist for using either Neural Nets [Baird, 1995] or Decision Trees <ref> [McCallum, 1995] </ref>. First we introduce a stable method which uses both a table and a generalizing function approximator. Then [McCallum, 1995]'s method using Decision Trees is described and we extend it to handle continuous state values. 4.1 Fitted Prioritized Sweeping As pointed out in [Boyan and Moore, 1996], generalizing function approximators are not a problem if the approximation is not iterated. <p> As can be seen in Table 9 it learns significantly faster than standard Prioritized Sweeping. While effective, this algorithm still requires a prior discretization of the world. It cannot handle continuous state spaces. 4.3 Continuous U Tree The U Tree algorithm <ref> [McCallum, 1995] </ref> is a method for using a decision tree [Quinlan, 1986] instead of a table of values in the Prioritized Sweeping algorithm. In the original U Tree 13 Prioritized Sweeping vs.
Reference: [Moore and Atkeson, 1993] <author> A. Moore and C. G. Atkeson. </author> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <year> 1993. </year>
Reference-contexts: The 9 change in opponent behavior is detected directly and can cause changes in backed up value much faster than the normal learning rate would allow. 3.6 Prioritized Sweeping Prioritized Sweeping <ref> [Moore and Atkeson, 1993] </ref> is significantly different from the preceeding methods. Instead of relying on sampling to model the state transition probability distribution, P (s;a) (s 0 ), implicitly, Prioritized Sweeping proceeds by building a model of the world explicitly.
Reference: [Owen, 1995] <author> Guillermo Owen. </author> <title> Game Theory. </title> <publisher> Academic Press, </publisher> <address> San Diego, California, 3 edition, </address> <year> 1995. </year> <note> ISBN: 0-12-531151-6. </note>
Reference-contexts: The value of this action is the value, V (s), for that state. This method of picking an action is known in the game theory literature as `solution by fictitious play' <ref> [Owen, 1995] </ref>. It has been shown that two players using this method against each other repeatedly, with a single state and known move values, will converge.
Reference: [Press et al., 1992] <author> William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. </author> <title> Numerical Recipies in C: </title> <booktitle> the art of scientific computing. </booktitle> <publisher> Cambridge University Press, </publisher> <address> 2nd edition, </address> <year> 1992. </year>
Reference-contexts: Projecting this point down onto the x axis gives the required distribution over our actions. The y coordinate of this point is the expected reward for moving with that probability distribution. Finding this point is a classical linear programming problem. (See <ref> [Press et al., 1992] </ref> for more detail.) In the example above, the maximum expected reward is at the intersection of the lines generated from opponent moves a 1 o and a 2 o . <p> This is considered to be the final state for the transition. Using the expected reward for that state, V (s 0 ), and the recorded reward for the 1 see <ref> [Press et al., 1992] </ref> for more information on these sorts of tests. 14 transition, r, we can assign a value to the initial sensory input/action pair. q (I; a) = r + flV (s 0 ) (6) Having calculated values for the datapoints and used that to discretize the world we
Reference: [Quinlan, 1986] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <address> 1:81106, </address> <year> 1986. </year>
Reference-contexts: While effective, this algorithm still requires a prior discretization of the world. It cannot handle continuous state spaces. 4.3 Continuous U Tree The U Tree algorithm [McCallum, 1995] is a method for using a decision tree <ref> [Quinlan, 1986] </ref> instead of a table of values in the Prioritized Sweeping algorithm. In the original U Tree 13 Prioritized Sweeping vs.
Reference: [Samuel, 1959] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Research Journal, </journal> <volume> 3(3), </volume> <year> 1959. </year> <note> Reprinted in 'Readings in Machine Learning' by Shavlik and Dietterich. </note>
Reference-contexts: 1 Introduction Multi-agent adversarial environments have traditionally been addressed as game playing situations. Indeed, one of the first areas to be studied in Artificial Intelligence was game playing. For example, the pioneering checkers playing algorithm by <ref> [Samuel, 1959] </ref> used both search and machine learning strategies. Interestingly, his approach is similar to modern Reinforcement Learning techniques [Kaelbling et al., 1996]. An evaluation function that guides the selection of moves is represented as a parameterized weighted sum of game features.
Reference: [Tesauro, 1995] <author> G Tesauro. </author> <title> Temporal difference learning and td-gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38(3):5867, </volume> <year> 1995. </year>
Reference-contexts: This is a similar method to classical Reinforcement Learning which also provides for incremental update of an evaluation function, although in this case it is represented as a table of values. Since Samuel's work however, Reinforcement Learning techniques were not used again in an adversarial setting until quite recently. <ref> [Tesauro, 1995, Thrun, 1995] </ref> have both used neural nets in a Reinforcement Learning paradigm. [Tesauro, 1995]'s work in the game of checkers 1 was successful, but required hand tuned features being fed to the algorithm for high quality play. [Thrun, 1995] was moderately successful in using similar techniques in chess, but <p> Since Samuel's work however, Reinforcement Learning techniques were not used again in an adversarial setting until quite recently. [Tesauro, 1995, Thrun, 1995] have both used neural nets in a Reinforcement Learning paradigm. <ref> [Tesauro, 1995] </ref>'s work in the game of checkers 1 was successful, but required hand tuned features being fed to the algorithm for high quality play. [Thrun, 1995] was moderately successful in using similar techniques in chess, but these techniques were not as successful as they had been in the checkers domain. <p> This has been shown to be unstable in some cases [Boyan and Moore, 1995] never converging to a solution although some good results have been obtained with this method <ref> [Tesauro, 1995] </ref>. Methods that are stable do exist for using either Neural Nets [Baird, 1995] or Decision Trees [McCallum, 1995]. First we introduce a stable method which uses both a table and a generalizing function approximator.
Reference: [Thrun, 1995] <author> Sebastian Thrun. </author> <title> Learning to play the game of chess. </title> <editor> In G. Tesauro and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 7, </booktitle> <address> Cam-bridge, MA, 1995. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: This is a similar method to classical Reinforcement Learning which also provides for incremental update of an evaluation function, although in this case it is represented as a table of values. Since Samuel's work however, Reinforcement Learning techniques were not used again in an adversarial setting until quite recently. <ref> [Tesauro, 1995, Thrun, 1995] </ref> have both used neural nets in a Reinforcement Learning paradigm. [Tesauro, 1995]'s work in the game of checkers 1 was successful, but required hand tuned features being fed to the algorithm for high quality play. [Thrun, 1995] was moderately successful in using similar techniques in chess, but <p> were not used again in an adversarial setting until quite recently. [Tesauro, 1995, Thrun, 1995] have both used neural nets in a Reinforcement Learning paradigm. [Tesauro, 1995]'s work in the game of checkers 1 was successful, but required hand tuned features being fed to the algorithm for high quality play. <ref> [Thrun, 1995] </ref> was moderately successful in using similar techniques in chess, but these techniques were not as successful as they had been in the checkers domain.
Reference: [Watkins and Dayan, 1992] <author> Christopher J. C. H. Watkins and Peter Dayan. Q-learning. </author> <title> Machine Learning, </title> <address> 8(3):279292, </address> <year> 1992. </year> <month> 18 </month>
Reference-contexts: This work has been repeated in other domains, but again, without the same success as in the checkers domain (in [Kaelbling et al., 1996]). [Littman, 1994] took standard Q Learning, <ref> [Watkins and Dayan, 1992] </ref>, and modified it to work with Markov games. He replaced the simple min update used in standard Q Learning with a mixed strategy (probabilistic) minimax update. He then evaluated this by playing against both standard Q Learning and random players in a simple game. <p> It corresponds to there being a chance, probability (1 fl), that the world ends between each move. It can also be seen as an interest rate, or just as a trick to make the sum bounded. In our experiments, fl = 0:95. 3.1 Q Learning Q Learning <ref> [Watkins and Dayan, 1992] </ref> is a method of building a table of values that can be used to decide how to act so as to maximize the agent's discounted reward over time.
References-found: 14

