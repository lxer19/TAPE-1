URL: http://polaris.cs.uiuc.edu/reports/1270.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: AUTOMATIC DETECTION OF NONDETERMINACY, AND SCALAR OPTIMIZATIONS IN PARALLEL PROGRAMS  
Author: BY SANJOY GHOSH 
Degree: B.Engg., Birla Institute of Technology and Science, 1982 M.Tech., Indian Institute of Technology, 1986 THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Address: 1992 Urbana, Illinois  
Affiliation: University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [AgHo90] <author> Hiralal Agrawal, and Joseph R. Horgan. </author> <title> Dynamic Program Slicing, </title> <booktitle> Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation. </booktitle> <pages> pp. 246-256, </pages> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The slice is also very useful in distinguishing the different forms of nondeterminacy. Informally, the slice of an occurrence of a variable in the program is the set of statements that may affect the value of the variable at that occurrence <ref> [AgHo90] </ref>. There are two kinds of slices depending on whether it is computed at compile time or execution time. The static slice is computed at compile time, and consists of statements that may affect the value.
Reference: [ADSp91] <author> Hiralal Agrawal, Richard A. DeMillo, and Eugene H. Spafford. </author> <title> An Execution-Backtracking Approach to Debugging, </title> <journal> IEEE Software, </journal> <pages> pp. 21-26, </pages> <month> May </month> <year> 1991. </year>
Reference: [AlPa87] <author> Todd R. Allen, and David A. Padua. </author> <title> Debugging Fortran on a Shared Memory Machine, </title> <booktitle> Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pp. 721-727, </pages> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: The program may take different paths on different executions depending on the order in which nondeterminate operations execute. In particular, as pointed out by Allen and Padua <ref> [AlPa87] </ref>, this could hide other races from detection by dynamic analysis if the execution took a particular path on the given execution. For example, consider the following program. <p> The cost of building this graph can be prohibitive for practical programs. 132 8.2 Trace analysis Trace analysis attempts to obtain the two pieces of information needed to check Bernstein's conditions by examining an execution trace of the program, either during execution or later. Allen and Padua <ref> [AlPa87] </ref> consider the problem of detecting races in parallel programs with synchronization very similar to advance/await. Their solution is similar to ours. Miller and Choi [MiCh88] describe a parallel program debugger that has a builtin race detector. They consider programs with cobegin/coend and semaphores.
Reference: [ApMc88] <author> William F. Appelbe, and Charles E. McDowell. </author> <title> Integrating Tools for Debugging and Developing Multitasking Programs, </title> <booktitle> Proceedings of the SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. 78-88, </pages> <address> Madison, WI, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: The principal drawback of this approach is that the concurrency history graph is combina-torially explosive. It is exponential in the size of the program. However, this was the starting point for subsequent work in this area. 131 Appelbe and McDowell <ref> [ApMc88] </ref> have used a similar approach to analyzing programs in a parallel dialect of Fortran. They use the information obtained, to provide inputs to a system that checks for races at execution time. In addition, they use the information to detect deadlocks.
Reference: [ASUl86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools, </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference: [BaKe89] <author> Vasanth Balasundaram, and Ken Kennedy. </author> <title> Compile-time Detection of Race Conditions in a Parallel Program, </title> <booktitle> Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pp. 175-185, </pages> <address> Crete, Greece, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: In case of multiple posts possibly triggering a wait, they compute the common ancestors of the posts. This framework fails to account for the ordering within loop nests, and also can not deal with clears. Balasundaram and Kennedy <ref> [BaKe89] </ref> propose a technique to detect deadlock in restricted programs, that can also be used to detect races. They construct a graph that explicitly represents the operations that execute concurrently.
Reference: [BaKe89] <author> Vasanth Balasundaram, and Ken Kennedy. </author> <title> A Technique for Summarizing Data Access and its use in Parallelism Enhancing Transformations, </title> <booktitle> Proceedings of SIG-PLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 41-53, </pages> <address> Portland, OR, </address> <year> 1989. </year>
Reference-contexts: In case of multiple posts possibly triggering a wait, they compute the common ancestors of the posts. This framework fails to account for the ordering within loop nests, and also can not deal with clears. Balasundaram and Kennedy <ref> [BaKe89] </ref> propose a technique to detect deadlock in restricted programs, that can also be used to detect races. They construct a graph that explicitly represents the operations that execute concurrently.
Reference: [Bala89] <author> Vasanth Balasundaram. </author> <title> Interactive Parallelization of Numerical Scientific Programs, </title> <type> Ph.D. Thesis, </type> <institution> Rice University Computer Science Technical Report TR89-95, </institution> <month> July </month> <year> 1989. </year>
Reference: [Bane88] <author> Utpal Banerjee. </author> <title> Dependence Analysis for Supercomputers, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: Each has its strengths and weaknesses. Static analysis is the most efficient, and is applicable to every input data set for the program. However, it is often not accurate enough for the same reasons as dependence analysis <ref> [Bane88] </ref>. Dependence analysis tries to determine when the result of an operation depends on another by virtue of both accessing the same memory location. The second operation is said to depend on the first. <p> However, if the algorithm is accurate enough to determine, for instance by some dataflow analysis technique, that the values of i and j are guaranteed to be different or guaranteed to be the same, then the potential race is resolved. Therefore, we need accurate techniques for dependence analysis <ref> [Bane88] </ref>. Interprocedural analysis can increase the accuracy of the algorithm. Another way would be to accept assertions from the programmer. A possible problem with this is that false assertions could be introduced. The compiler could be extended to optionally generate code to verify the assertions. <p> This would imply lack of conflict between the accesses, which would prove the absence of races, at compile-time. However, due to the presence of vector notation we do not use the standard dependence analysis techniques described in <ref> [Bane88] </ref>. Those techniques are applicable, but not sufficiently aggressive. Instead we use pattern-matching techniques to prove independence between accesses, at compile-time. This consists of showing that as a result of the loop structure, accesses to an array must access disjoint regions of the array on different iterations of the loop.
Reference: [Bern66] <author> A. J. Bernstein. </author> <title> Analysis of Programs for Parallel Processing, </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> EC-15(5), </volume> <pages> pp. 757-763, </pages> <month> October </month> <year> 1966. </year>
Reference-contexts: is more than 1 processor executing it, by reducing the amount of sequential code. * For scheduling purposes, if we have 3 or fewer processors, it would be more efficient to assign the first and third branches of the cobegin to the same processor. 129 Chapter 8 Related Work Bernstein <ref> [Bern66] </ref> was the first to study the problem of nondeterminacy in parallel programs. He analyzed the problem in terms of sets of variables read and written by concurrent threads within the program.
Reference: [CaSu88] <author> David Callahan, and Jaspal Subhlok. </author> <title> Static Analysis of Low-level Synchronization, </title> <booktitle> Proceedings of the SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. 100-111, </pages> <address> Madison, WI, </address> <month> May </month> <year> 1988. </year> <month> 139 </month>
Reference-contexts: In other words, instances of statements that might access the same element of an array but cannot be proved to be ordered, at compile time, are reported as races. It has been proved by others [Tayl83a], <ref> [CaSu88] </ref>, [NeMi90], that computing the ordering that is guaranteed between points in the program for most of the commonly used synchronization mechanisms is co-NP-hard. Therefore, most algorithms to analyze synchronization are polynomial approximations, that tradeoff accuracy for efficiency. <p> Here, we implicitly assume that the iteration vector is within the iteration space of the block being considered. In particular, the universal quantifier actually extends over the iteration space of the 67 corresponding block. Unfortunately, these sets are NP-hard to compute, even for straightline code <ref> [CaSu88] </ref>. <p> This was a very important result and subsequent researchers have proved similar results for a variety of other languages and synchronization mechanisms. Most practical techniques for race detection therefore use polynomial algorithms to compute approximations to the ordering. Callahan and Subhlok <ref> [CaSu88] </ref> proved that computing guaranteed ordering is Co-NP-Hard even for programs with only straightline code, and no clears, but only posts and waits. They formulated a set of dataflow equations whose solution would provide information about which parts of the program could execute concurrently.
Reference: [CKSu90] <author> David Callahan, Ken Kennedy, and Jaspal Subhlok. </author> <title> Analysis of Event Synchroniza--tion in A Parallel Programming Tool, </title> <booktitle> Proceedings of the Second ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pp. 21-30, </pages> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: In particular, for the language that we are considering, it means programs which use advance/await, or single posts without clears, to synchronize. The other, called the static common ancestor algorithm is an extension of an algorithm devised by Callahan, Kennedy, and Subhlok <ref> [CKSu90] </ref>. This algorithm allows static analysis to be performed for programs that have anonymous synchronization. This means that different points in the program may synchronize on different executions. In particular, this means that programs can have more than one post on the same event. Clears are still not allowed. <p> And at least one of the ordering paths connecting the two conflicting nodes is annotated with a special mark. The presence of a special mark on an ordering path indicates that we do not precisely know which instances are ordered. 4.2 Static common ancestor algorithm Callahan, Kennedy, and Subhlok <ref> [CKSu90] </ref> present a technique to perform static analysis of programs with posts and waits only. They use a directed graph to represent the program. And annotated edges to represent control flow as well as synchronization, in a manner similar to order/conflict graphs. <p> They formulated a set of dataflow equations whose solution would provide information about which parts of the program could execute concurrently. Since they considered only straightline code, identifying the variables accessed was not a problem. In Callahan, Kennedy, and Subhlok <ref> [CKSu90] </ref> they extended this framework to handle loops. They used distances similar to dependence distances to indicate instances that access the same element of an array, as well as posts that post the same element of the event array that a wait statement waits upon.
Reference: [CMNe91] <author> Jong-Deok Choi, Barton P. Miller, and Robert H. B. Netzer. </author> <title> Techniques for Debugging Parallel Programs with Flowback Analysis, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 13, No. 4, </volume> <year> 1991. </year>
Reference-contexts: The race on y is automatically eliminated. However, if S2 executes after S3, then the race on y will be exposed, detected and eliminated. Completely nondeterminate races are not amenable to automatic analysis. However, if a flowback debugger of the kind proposed by Choi, Miller, and Netzer <ref> [CMNe91] </ref> is used, then the user can discover what may be hidden, by checking the conditional statement for each of the values that the completely nondeterminate races might define.
Reference: [ChMi91] <author> Jong-Deok Choi, and Sang Lyul Min. </author> <title> Race Frontier: Reproducing Data Races in Parallel Program Debugging, </title> <booktitle> Proceedings of the Third ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 145-154, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference: [Cybe91] <author> George Cybenko. </author> <title> Supercomputer Performance Trends and the Perfect Benchmarks, </title> <booktitle> Supercomputing Review, </booktitle> <pages> pp. 53-60, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This is satisfied by the advance (1) operation in iteration 5. The accesses a 5 and a 7 are guaranteed to be ordered, and are not a race. The following table, taken from Cybenko <ref> [Cybe91] </ref>, lists the names of the programs used, their respective application area, the dominant algorithms used by the program, and the number of lines of source code.
Reference: [DiSc90a] <author> Anne Dinning, and Edith Schonberg. </author> <title> The Task Recycling Technique for Detecting Access Anomalies On-The-Fly, </title> <type> IBM Technical Report RC15385, </type> <month> January </month> <year> 1990. </year>
Reference-contexts: Any unordered access is reported as a race. Whenever an access a is recorded, the value of local (node after (a)) is also included. We now discuss ways to safely discard accesses from the read and write lists. The next two lemmas were stated in <ref> [DiSc90a] </ref>. Lemma 3: If a read r 1 is guaranteed to execute before another read r 2 on the same variable, then it is safe to discard r 1 . Clearly, the above lemma also holds for write accesses that are ordered.
Reference: [DiSc90b] <author> Anne Dinning, and Edith Schonberg. </author> <title> An Empirical Comparison of Monitoring Algorithms for Access Anomaly Detection, </title> <booktitle> Proceedings of the Second ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 1-10, </pages> <address> Seattle, WA, </address> <month> march </month> <year> 1990. </year>
Reference-contexts: They considered only programs that had cobegin/coend constructs, and no other synchronization mechanism. For such programs they were able to provide an efficient algorithm that encoded the concurrency information as well as the accesses made by the threads. Dinning and Schonberg <ref> [DiSc90b] </ref> describe techniques that are improvements to those proposed by Rudoph and Nudler, to detect races on-the-fly. Their technique works well for synchronization mechanisms which perform synchronous coordination.
Reference: [Dinn90] <author> Anne Dinning. </author> <title> Detecting Nondeterminism in Shared Memory Parallel Programs, </title> <type> Ph.D. Thesis, </type> <institution> Computer Science Technical Report 526, </institution> <address> New York University, </address> <month> November </month> <year> 1990. </year>
Reference-contexts: It is important to trim these records in order to save space, and to make checking for races quicker. Dinning <ref> [Dinn90] </ref> shows that for write accesses it is sufficient to record only the latest. 92 But unfortunately, in her algorithm all mutually concurrent read accesses have to be recorded.
Reference: [DiSc91] <author> Anne Dinning, and Edith Schonberg. </author> <title> Detecting Access Anomalies in Programs with Critical Sections, </title> <booktitle> Proceedings of the ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. 79-90, </pages> <address> Santa Cruz, CA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: On-the-fly analysis avoids these difficulties by not storing any information on disk, and discarding a substantial fraction of the trace information as the execution proceeds. Unfortunately, conventional on-the-fly analysis requires that the synchronizations be fixed on every execution. Dinning and Schonberg <ref> [DiSc91] </ref> call it the single input, single execution (SISE) property, which they define as the condition that "A single execution instance is sufficient to determine the existence of an anomaly for a given input". <p> They also describe situations where accesses that have already occurred can be discarded from the lists that are maintained to check for races. Their technique is not directly applicable to events which do not have the property of synchronous coordination. They <ref> [DiSc91] </ref> also present a technique to analyze programs with critical sections. Although they are able to discard all write accesses to a variable except for the latest, they have to retain all mutually concurrent read accesses.
Reference: [EmPa88] <author> Perry A. Emrath, and David A. Padua. </author> <title> Automatic Detection of Nondeterminacy in Parallel Programs, </title> <booktitle> Proceedings of the SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. 89-99, </pages> <address> Madison, WI, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: The same program may belong to different categories for different input values. For this reason it is assumed that both the code as well as the input data is held constant for the purpose of classification. We distinguish four categories of determinacy in a parallel program <ref> [EmPa88] </ref>. * Internal Determinacy. This is when the sequence of instructions executed by each thread, as well the values of the operands of each instruction, is determinate. * External Determinacy. This is when the program is not internally determinate but its output is determinate. * Associative Nondeterminacy.
Reference: [EGPa89] <author> Perry A. Emrath, Sanjoy Ghosh, and David A. Padua. </author> <title> Event Synchronization Analysis for Debugging Parallel Programs, </title> <booktitle> Proceedings of Supercomputing '89, </booktitle> <pages> pp. 580-588, </pages> <address> Reno, NV, </address> <month> Nov. </month> <year> 1989. </year>
Reference: [EGPa92] <author> Perry A. Emrath, Sanjoy Ghosh, and David A. Padua. </author> <title> Detecting Non-Determinacy in Parallel Programs, </title> <journal> IEEE Software, </journal> <month> January </month> <year> 1992. </year>
Reference: [FOWa87] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The Program Dependence Graph and its use in Optimization, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 9, No. 3, </volume> <pages> pp. 319-349, </pages> <year> 1987. </year>
Reference-contexts: The race is then input-conditional on the variables in this set of definitions. For each access it is easy to compute the conditional statements that the access is control dependent upon, at compile time. The Program Dependence Graph (PDG) <ref> [FOWa87] </ref> provides an adequate data structure for this purpose. Suppose that we want to know whether the statement, S, containing an access involved in a race is input-conditional. We check the PDG to identify all the conditional statements that S is control dependent upon.
Reference: [GPHL88] <author> M. D. Guzzi, D. A. Padua, J. P. Hoeflinger, and D. H. Lawrie. </author> <title> Cedar Fortran and Other Vector and Parallel Fortran Dialects, </title> <booktitle> Proceedings of Supercomputing '88, </booktitle> <pages> pp. 114-121, </pages> <address> Orlando, FL, </address> <month> November </month> <year> 1988. </year> <month> 140 </month>
Reference-contexts: We exclude distributed memory machines from our discussion although message buffers might be regarded as shared locations. 1.1 Organization of thesis Chapter 2 sets the stage for the thesis by describing the parallel constructs and synchronization mechanisms that we deal with. These are mostly taken from Cedar Fortran <ref> [GPHL88] </ref>, as well as another construct that is commonly used. This chapter also discusses the various ways in which nondeterminacy manifests itself. It classifies parallel programs by the degree of determinacy they exhibit. This helps in debugging as well as finding the best way to eliminate the nondeterminacy. <p> Each processor can also have private storage. We have excluded distributed memory machines that communicate through message passing, although some of the issues discussed here are also pertinent to them. The language model that we consider was suggested by Cedar Fortran <ref> [GPHL88] </ref>. The algorithms that we describe are applicable to other parallel dialects of Fortran 77, [PCF88], [IBM88] that are based on a shared address space, which makes them semantically similar. Cedar Fortran provides parallelism at the coarse, medium, and fine levels of granularity.
Reference: [HMWa89] <author> David P. Helmbold, Charles E. McDowell, and Jian-Zhong Wang. </author> <title> Analyzing Traces with Anonymous Synchronization, </title> <booktitle> Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pp. 70-77, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: This is similar to the timestamps of <ref> [HMWa89] </ref>. A timestamp at a node is an efficient representation of all the other nodes that have paths to the node in question. They are vectors of length T, where T is the total number of threads in the program.
Reference: [HKMe90] <author> Robert Hood, Ken Kennedy, and John Mellor-Crummey. </author> <title> Parallel Program Debugging with On-the-fly Anomaly Detection, </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <pages> pp. 74-81, </pages> <address> New York, NY, </address> <month> November </month> <year> 1990. </year>
Reference: [IBM88] <institution> IBM Parallel FORTRAN Language and Library Reference, IBM Corporation, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: The language model that we consider was suggested by Cedar Fortran [GPHL88]. The algorithms that we describe are applicable to other parallel dialects of Fortran 77, [PCF88], <ref> [IBM88] </ref> that are based on a shared address space, which makes them semantically similar. Cedar Fortran provides parallelism at the coarse, medium, and fine levels of granularity. Coarse grain parallelism is provided by subroutines that execute concurrently.
Reference: [KDLS86] <author> D. Kuck, E. Davidson, D. Lawrie, and A. Sameh. </author> <title> Parallel supercomputing today and the Cedar approach, </title> <journal> Science, </journal> <volume> vol. 231, </volume> <pages> pp. 967-974, </pages> <month> February </month> <year> 1986. </year>
Reference-contexts: Conditional constant propagation is constant propagation that takes into account conditional statements that can be evaluated at compile time. Although the discussion is targeted at the Cedar shared memory multiprocessor, developed at the University of Illinois <ref> [KDLS86] </ref>, the principles discussed apply to other parallel dialects of Fortran, such as PCF Fortran [PCF88], that are directed at shared memory machines.
Reference: [Lamp79] <author> Leslie Lamport. </author> <title> How to make a Multiprocessor Computer that correctly executes Multiprocess Programs, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-28, No. 9, </volume> <pages> pp. 690-691, </pages> <month> September </month> <year> 1979. </year>
Reference: [LeMe87] <author> Thomas J. LeBlanc, and John M. Mellor-Crummey. </author> <title> Debugging Parallel Programs with Instant Replay, </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-36, No. 4, </volume> <pages> pp. 471-482, </pages> <month> April </month> <year> 1987. </year>
Reference: [McAp87] <author> Charles E. McDowell, and William F. Appelbe. </author> <title> Minimizing the Complexity of Static Analysis of Parallel Programs, </title> <booktitle> Proceedings of the 20th Annual Hawaii International Conference on System Science, </booktitle> <pages> pp. 171-176, </pages> <year> 1987. </year>
Reference: [McHe91] <author> Charles E. McDowell, and David P. Helmbold. </author> <title> Computing Reachable States of Parallel Programs, </title> <booktitle> Proceedings of the ACM/ONR Workshop on Parallel and Distributed Debugging, </booktitle> <address> Santa Cruz, CA, </address> <month> May </month> <year> 1991. </year>
Reference: [Mell89] <author> John M. Mellor-Crummey. </author> <title> Debugging and Analysis of Large-Scale Parallel Programs, </title> <type> Ph.D. Thesis, </type> <institution> Computer Science Technical Report, 312, University of Rochester, </institution> <month> September </month> <year> 1989. </year>
Reference: [Mell91] <author> John M. Mellor-Crummey. </author> <title> On-the-fly Detection of Data Races for Programs with Nested Fork-Join Parallelism, </title> <booktitle> Proceedings of Supercomputing '91, </booktitle> <pages> pp. 24-33, </pages> <address> Albu-querque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: They [DiSc91] also present a technique to analyze programs with critical sections. Although they are able to discard all write accesses to a variable except for the latest, they have to retain all mutually concurrent read accesses. Mellor-Crummey <ref> [Mell91] </ref> presents a technique to perform on-the-fly race detection on programs that have perfectly nested parallelism such as parallel loops and cobegin/coend constructs and no other synchronization mechanism.
Reference: [MiCh88] <author> Barton P. Miller, and Jong-Deok Choi. </author> <title> A Mechanism for Efficient Debugging of Parallel Programs, </title> <booktitle> Proceedings of the SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. 141-150, </pages> <address> Madison, WI, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Allen and Padua [AlPa87] consider the problem of detecting races in parallel programs with synchronization very similar to advance/await. Their solution is similar to ours. Miller and Choi <ref> [MiCh88] </ref> describe a parallel program debugger that has a builtin race detector. They consider programs with cobegin/coend and semaphores. Although their system provides very powerful facilities for debugging, such as checkpointing, it does not compute the guaranteed ordering accurately, making the race detection also not very reliable.
Reference: [MiCh91] <author> Sang Lyul Min, and Jong-Deok Choi. </author> <title> An Efficient Cache-based Access Anomaly Detection Scheme, </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 235-244, </pages> <address> Palo Alto, CA, </address> <month> April </month> <year> 1991. </year>
Reference: [MiPa90] <author> Samuel P. Midkiff, and David A. Padua. </author> <title> Issues in the Compile-Time Optimization of Parallel Programs, </title> <booktitle> Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 105-113, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year> <month> 141 </month>
Reference-contexts: Midkiff, Padua, and Cytron [MPCy90] have identified some of the problems in dataflow analysis of parallel programs, and proposed some solutions. But that work too deals with array references, and attempts to extend the dependence analysis of sequential programs in order to further parallelize parallel programs. Midkiff and Padua <ref> [MiPa90] </ref> discuss some of the problems in compiling parallel programs. Taylor and Osterweil [TaOs80] discuss dataflow analysis of parallel programs to detect races at compile-time. However, their technique can be adapted to perform optimizations.
Reference: [MPCy90] <author> Samuel P. Midkiff, David A. Padua, and Ron Cytron. </author> <title> Compiling Programs with User Parallelism, </title> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pp. 402-422, </pages> <publisher> MIT Press, </publisher> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: One reason for this omission is that, until recently, there have not been many explicitly parallel programs in existence. But with the rapid proliferation of parallel hardware that is changing. Midkiff, Padua, and Cytron <ref> [MPCy90] </ref> have identified some of the problems in dataflow analysis of parallel programs, and proposed some solutions. But that work too deals with array references, and attempts to extend the dependence analysis of sequential programs in order to further parallelize parallel programs.
Reference: [NeMi90] <author> Robert H. B. Netzer, and Barton P. Miller. </author> <title> On the Complexity of Event Ordering for Shared-Memory Parallel Program Executions, </title> <booktitle> Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 93-97, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: In other words, instances of statements that might access the same element of an array but cannot be proved to be ordered, at compile time, are reported as races. It has been proved by others [Tayl83a], [CaSu88], <ref> [NeMi90] </ref>, that computing the ordering that is guaranteed between points in the program for most of the commonly used synchronization mechanisms is co-NP-hard. Therefore, most algorithms to analyze synchronization are polynomial approximations, that tradeoff accuracy for efficiency. <p> They, therefore, constitute a race which is reported to the user. 3.2.1.4 Example where common ancestor algorithm misses orderings The closest common ancestor algorithm has polynomial time complexity. Netzer and Miller <ref> [NeMi90] </ref> have proved that the complexity of computing guaranteed ordering in a program with posts, waits, and clears is Co-NP-hard. This implies that the closest common ancestor algorithm computes an approximation to the guaranteed ordering. Which means that it may fail 38 to compute all the guaranteed ordering. <p> Although their system provides very powerful facilities for debugging, such as checkpointing, it does not compute the guaranteed ordering accurately, making the race detection also not very reliable. They compute ordering as observed on that particular execution, and do not attempt to compute guaranteed ordering. Netzer and Miller <ref> [NeMi90] </ref> present a formal model that represents parallel program executions. This model can be used to characterize actual, observed, and potential execution behaviours of the program. The actual behaviour is an exact representation of the execution that may not be experimentally observable.
Reference: [NeMi91] <author> Robert H. B. Netzer, and Barton P. Miller. </author> <title> Improving the Accuracy of Data Race Detection, </title> <booktitle> Proceedings of the 3rd ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 133-144, </pages> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: They can also check what other accesses might be hidden in the other possible directions. Another phenomenon similar to hiding was pointed out by Netzer and Miller <ref> [NeMi91] </ref>. A conditional may also imply ordering. <p> They also distinguish between general races and data races to distinguish situations where nondeterminacy is required, from those where only mutual exclusion is needed. This is reflected in the different techniques needed to detect the two kinds of races. Netzer and Miller <ref> [NeMi91] </ref> also present a technique to distinguish races that are genuine bugs from those that arise as a result of other bugs. This helps to keep the user focused on the important races, which cause others, instead of those that are artifacts of the primary races.
Reference: [NeMi90] <author> Robert H. B. Netzer, and Barton P. Miller. </author> <title> Detecting Data Races in Parallel Program Executions, </title> <booktitle> Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pp. 109-129, </pages> <year> 1991. </year>
Reference-contexts: In other words, instances of statements that might access the same element of an array but cannot be proved to be ordered, at compile time, are reported as races. It has been proved by others [Tayl83a], [CaSu88], <ref> [NeMi90] </ref>, that computing the ordering that is guaranteed between points in the program for most of the commonly used synchronization mechanisms is co-NP-hard. Therefore, most algorithms to analyze synchronization are polynomial approximations, that tradeoff accuracy for efficiency. <p> They, therefore, constitute a race which is reported to the user. 3.2.1.4 Example where common ancestor algorithm misses orderings The closest common ancestor algorithm has polynomial time complexity. Netzer and Miller <ref> [NeMi90] </ref> have proved that the complexity of computing guaranteed ordering in a program with posts, waits, and clears is Co-NP-hard. This implies that the closest common ancestor algorithm computes an approximation to the guaranteed ordering. Which means that it may fail 38 to compute all the guaranteed ordering. <p> Although their system provides very powerful facilities for debugging, such as checkpointing, it does not compute the guaranteed ordering accurately, making the race detection also not very reliable. They compute ordering as observed on that particular execution, and do not attempt to compute guaranteed ordering. Netzer and Miller <ref> [NeMi90] </ref> present a formal model that represents parallel program executions. This model can be used to characterize actual, observed, and potential execution behaviours of the program. The actual behaviour is an exact representation of the execution that may not be experimentally observable.
Reference: [NuRo88] <author> Itzhak Nudler, and Larry Rudolph. </author> <title> Tools for the Efficient Development of Efficient Parallel Programs, </title> <booktitle> Proceedings of the 1st Israeli Conference on Computer System Engineering, </booktitle> <year> 1988. </year>
Reference-contexts: This helps to keep the user focused on the important races, which cause others, instead of those that are artifacts of the primary races. Nudler and Rudolph <ref> [NuRo88] </ref> were the first to describe on-the-fly race detection techniques. They considered only programs that had cobegin/coend constructs, and no other synchronization mechanism. For such programs they were able to provide an efficient algorithm that encoded the concurrency information as well as the accesses made by the threads.
Reference: [OlOs90] <author> Kurt M. Olender, and Leon J. Osterweil. Cecil: </author> <title> A Sequencing Constraint Language for Automatic Static Analysis Generation, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 16, No. 3, </volume> <pages> pp. 268-280, </pages> <month> March </month> <year> 1990. </year>
Reference: [PaLi88] <author> Douglas Z. Pan, and Mark A. Linton. </author> <title> Supporting Reverse Execution of Parallel Programs, </title> <booktitle> Proceedings of the SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pp. 124-129, </pages> <address> Madison, WI, </address> <month> May </month> <year> 1988. </year>
Reference: [PCF88] <author> PCF Fortran: </author> <title> Language Definition, </title> <booktitle> The Parallel Computing Forum, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: Although the discussion is targeted at the Cedar shared memory multiprocessor, developed at the University of Illinois [KDLS86], the principles discussed apply to other parallel dialects of Fortran, such as PCF Fortran <ref> [PCF88] </ref>, that are directed at shared memory machines. We exclude distributed memory machines from our discussion although message buffers might be regarded as shared locations. 1.1 Organization of thesis Chapter 2 sets the stage for the thesis by describing the parallel constructs and synchronization mechanisms that we deal with. <p> We have excluded distributed memory machines that communicate through message passing, although some of the issues discussed here are also pertinent to them. The language model that we consider was suggested by Cedar Fortran [GPHL88]. The algorithms that we describe are applicable to other parallel dialects of Fortran 77, <ref> [PCF88] </ref>, [IBM88] that are based on a shared address space, which makes them semantically similar. Cedar Fortran provides parallelism at the coarse, medium, and fine levels of granularity. Coarse grain parallelism is provided by subroutines that execute concurrently.
Reference: [Scho89] <author> Edith Schonberg. </author> <title> On-The-Fly Detection of Access Anomalies, </title> <booktitle> Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 285-297, </pages> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference: [SrWo91] <author> Harini Srinivasan, and Michael Wolfe. </author> <title> Analyzing Programs with Explicit Parallelism, </title> <booktitle> Proceedings of the Fourth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pp. 405-419, </pages> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference: [Stee90] <author> Guy L. Steele. </author> <title> making Asynchronous Parallelism Safe for the World, </title> <booktitle> Proceedings of the 17th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 218-223, </pages> <address> San Francisco, CA, </address> <month> January </month> <year> 1990. </year>
Reference: [TaOs80] <author> Richard N. Taylor, and Leon J, Osterweil. </author> <title> Anomaly Detection in Concurrent Software by Static Data Flow Analysis, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-6, No. 3, </volume> <pages> pp. 265-277, </pages> <month> May </month> <year> 1980. </year>
Reference-contexts: But that work too deals with array references, and attempts to extend the dependence analysis of sequential programs in order to further parallelize parallel programs. Midkiff and Padua [MiPa90] discuss some of the problems in compiling parallel programs. Taylor and Osterweil <ref> [TaOs80] </ref> discuss dataflow analysis of parallel programs to detect races at compile-time. However, their technique can be adapted to perform optimizations. There has been little work on conventional dataflow analysis of parallel programs to perform conventional scalar optimizations. <p> techniques can be categorized into those that attempt to perform these tasks at compile time by analyzing the source code, and those that do it at execution time by examining one execution of the program. 8.1 Static analysis The earliest work in this area was done by Taylor and Osterweil <ref> [TaOs80] </ref>. They developed techniques to detect races in Ada programs in which rendezvousing is the most interesting synchronization mechanism. They solved the problem of determining concurrently executing instructions by building a concurrency history graph for the program.
Reference: [Tayl80] <author> Richard N. Taylor. </author> <title> Static Analysis of the Synchronization Structure of Concurrent Programs, </title> <type> Ph.D. Thesis, </type> <institution> University of Colorado at Coulder, </institution> <year> 1980. </year>
Reference: [Tayl83a] <author> Richard N. Taylor. </author> <title> Complexity of Analyzing the Synchronization Structure of Concurrent Programs, </title> <journal> Acta Informatica, </journal> <volume> 19, </volume> <pages> pp. 57-84, </pages> <year> 1983. </year> <month> 142 </month>
Reference-contexts: In other words, instances of statements that might access the same element of an array but cannot be proved to be ordered, at compile time, are reported as races. It has been proved by others <ref> [Tayl83a] </ref>, [CaSu88], [NeMi90], that computing the ordering that is guaranteed between points in the program for most of the commonly used synchronization mechanisms is co-NP-hard. Therefore, most algorithms to analyze synchronization are polynomial approximations, that tradeoff accuracy for efficiency. <p> This works well for parallel loops where threads representing individual iterations behave in identical ways. Nevertheless, the technique is not very practical for large programs. Taylor <ref> [Tayl83a] </ref> also proved that the complexity of computing ordering in Ada programs is NP-hard. This was a very important result and subsequent researchers have proved similar results for a variety of other languages and synchronization mechanisms.
Reference: [Tayl83b] <author> Richard N. Taylor. </author> <title> A general-purpose Algorithm for Analyzing Concurrent Pro--grams, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 26, No. 5, </volume> <pages> pp. 362-376, </pages> <month> May </month> <year> 1983. </year>
Reference: [Wegb75] <author> B. Wegbreit. </author> <title> Property Extraction in well-founded Property Sets, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-1, No. 3, </volume> <pages> pp. 270-285, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: However, it is easy to combine this with the detection of conditional branches whose condition can be evaluated at compile time. This information can be used to make the analysis more accurate, and to eliminate unreachable code, a form of deadcode. This problem was initially studied by Wegbreit <ref> [Wegb75] </ref> for sequential programs. It was improved upon by Wegman and Zadeck [WeZa91]. We extend their approach to explicitly parallel programs. The principal difference between their technique and ours is that whereas they mark edges between nodes in the CFG as either executable or non-executable, we use the nodes instead.
Reference: [WeZa91] <author> Mark N. Wegman, and Kenneth F. Zadeck. </author> <title> Constant Propagation with Conditional Branches, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 13, No. 2. </volume> <pages> pp. 181-210, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: This information can be used to make the analysis more accurate, and to eliminate unreachable code, a form of deadcode. This problem was initially studied by Wegbreit [Wegb75] for sequential programs. It was improved upon by Wegman and Zadeck <ref> [WeZa91] </ref>. We extend their approach to explicitly parallel programs. The principal difference between their technique and ours is that whereas they mark edges between nodes in the CFG as either executable or non-executable, we use the nodes instead.
Reference: [YoTa88] <author> Michael Young, and Richard N. Taylor. </author> <title> Combining Static Concurrency Analysis with Symbolic Execution, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 14, No. 10, </volume> <pages> pp. 1499-1511, </pages> <month> October </month> <year> 1988. </year> <month> 143 </month>
References-found: 55

