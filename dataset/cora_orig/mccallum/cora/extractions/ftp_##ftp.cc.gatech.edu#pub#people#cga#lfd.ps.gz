URL: ftp://ftp.cc.gatech.edu/pub/people/cga/lfd.ps.gz
Refering-URL: http://www.cs.gatech.edu/~saho/cs7100/project03/CS7100_p3.html
Root-URL: 
Email: fcga,sschaalg@cc.gatech.edu,  
Title: Robot Learning From Demonstration  
Author: Christopher G. Atkeson and Stefan Schaal 
Web: http://www.cc.gatech.edu/fac/fChris.Atkeson,Stefan.Schaalg  
Address: Atlanta, GA 30332-0280, USA  2-2 Hikaridai, Seiko-cho, Soraku-gun, 619-02 Kyoto, Japan  
Affiliation: College of Computing, Georgia Institute of Technology,  ATR Human Information Processing,  
Abstract: The goal of robot learning from demonstration is to have a robot learn from watching a demonstration of the task to be performed. In our approach to learning from demonstration the robot learns a reward function from the demonstration and a task model from repeated attempts to perform the task. A policy is computed based on the learned reward function and task model. Lessons learned from an implementation on an anthropomorphic robot arm using a pendulum swing up task include 1) simply mimicking demonstrated motions is not adequate to perform this task, 2) a task planner can use a learned model and reward function to compute an appropriate policy, 3) this model-based planning process supports rapid learning, 4) both parametric and nonparametric models can be learned and used, and 5) incorporating a task level direct learning component, which is non-model-based, in addition to the model-based planner, is useful in compensating for structural modeling errors and slow model learning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aboaf, E. W., Drucker, S. M., and Atkeson, C. G. </author> <year> (1989). </year> <title> Task-level robot learning: Juggling a tennis ball more accurately. </title> <booktitle> In Proceedings of the 1989 IEEE International Conference on Robotics and Automation. </booktitle>
Reference-contexts: Our goal is task level learning from demonstration, rather than matching patterns of human arm movements <ref> (Aboaf et al., 1989) </ref>. 3 THE ROBOT IMPLEMENTATION OF THE TASK We implemented learning from demonstration on a hydraulic seven degree of freedom anthropomorphic robot arm (SARCOS Dextrous Arm located at ATR, with the same stereo vision system that was used to observe the human demonstrations. <p> In the case of the nonparametric model the inputs may not include all relevant factors (such as the state of the robot arm, which affects the robot trajectory following errors). We introduce direct task-level learning to overcome the occasional failure of purely model-based learning <ref> (Aboaf et al., 1989) </ref>. For fast learning, the planner can compensate for deficiencies in the modeling approach that block or slow down further performance improvements by directly adjusting task parameters. For the swing up task we many natural ways to do this.
Reference: <author> An, C. H., Atkeson, C. G., and Hollerbach, J. M. </author> <year> (1988). </year> <title> Model-Based Control of a Robot Manipulator. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The pendulum axis is aligned with the fingers and with the forearm in this arm config uration. for several demonstration swing ups by a human. The pendulum starts at = and a successful swing up moves the pendulum to = 0. learning. two pumping motions were used. <ref> (An et al., 1988) </ref> to allow the robot to follow desired hand motions. In general we are interested in building complex motions out of simple components (motion primitives), as we feel this approach accelerates learning of new tasks.
Reference: <author> Atkeson, C. G., Moore, A. W., and Schaal, S. </author> <year> (1996a). </year> <title> Locally weighted learning. </title> <journal> Artificial Intelligence Review. </journal> <note> in press. </note>
Reference-contexts: Nonparametric models were constructed using locally weighted learning as described in <ref> (Atkeson et al., 1996a) </ref>. <p> Why can't nonparametric modeling techniques eliminate the need for the task level direct learning component?: We applied locally weighted regression <ref> (Atkeson et al., 1996a) </ref> in an attempt to avoid the structural modeling errors of the idealized parametric models during swing up, and also to see if a priori knowledge of the structure of the task dynamics was necessary to learn the task.
Reference: <author> Atkeson, C. G., Moore, A. W., and Schaal, S. </author> <year> (1996b). </year> <title> Locally weighted learning for control. </title> <journal> Artificial Intelligence Review. </journal> <note> in press. </note>
Reference: <author> Atkeson, C. G. and Schaal, S. </author> <year> (1997). </year> <title> Learning tasks from a single demonstration. </title> <booktitle> In Proceedings of the 1997 IEEE International Conference on Robotics and Automation. </booktitle>
Reference: <author> Bakker, P. and Kuniyoshi, Y. </author> <year> (1996). </year> <title> Robot see, robot do: An overview of robot imitation. </title> <booktitle> In AISB96 Workshop on Learning in Robots and Animals, </booktitle> <pages> pages 3-11. </pages>
Reference: <author> Cohen, M. F. </author> <year> (1992). </year> <title> Interactive spacetime control for animation. </title> <journal> Computer Graphics, </journal> <volume> 26(2) </volume> <pages> 293-302. </pages>
Reference: <author> Delson, N. and West, H. </author> <year> (1996). </year> <title> Robot programming by human demonstration: Adaptation and inconsistency in constrained motion. </title> <booktitle> In Proceedings of the 1996 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 30|36. </pages>
Reference: <author> Dillmann, R., Friedrich, H., Kaiser, M., and Ude, A. </author> <year> (1996). </year> <title> Integration of symbolic and subsymbolic learning to support robot programming by human demonstration. </title> <editor> In Giralt, G. and Hirzinger, G., editors, </editor> <booktitle> Robotics Research: The Seventh International Symposium, </booktitle> <pages> pages 296|307. </pages> <publisher> Springer, </publisher> <address> NY. </address>
Reference: <author> Friedrich, H. and Dillmann, R. </author> <year> (1995). </year> <title> Robot programming using user intentions and a single demonstration. </title> <editor> In Kaiser, M., editor, </editor> <booktitle> Proceedings of the 3rd European Workshop on Learning Robots (EWLR-3), </booktitle> <address> Heraklion, Crete, Greece. </address>
Reference: <author> Grudic, G. Z. and Lawrence, P. D. </author> <year> (1996). </year> <title> Human-to-robot skill transfer using the SPORE approximation. </title> <booktitle> In Proceedings of the 1996 IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 2962|2967. </pages>
Reference: <author> Hirzinger, G. </author> <year> (1996). </year> <title> Learning and skill acquisition. </title> <editor> In Giralt, G. and Hirzinger, G., editors, </editor> <booktitle> Robotics Research: The Seventh International Symposium, </booktitle> <pages> pages 277|278. </pages> <publisher> Springer, </publisher> <address> NY. </address>
Reference: <author> Ikeuchi, K., Miura, J., Suehiro, T., and Conanto, S. </author> <year> (1996). </year> <title> Designing skills with visual feedback for APO. </title> <editor> In Giralt, G. and Hirzinger, G., editors, </editor> <booktitle> Robotics Research: The Seventh International Symposium, </booktitle> <pages> pages 308|320. </pages> <publisher> Springer, </publisher> <address> NY. </address>
Reference: <author> Kawato, M., Gandolfo, F., Gomi, H., and Wada, Y. </author> <year> (1994). </year> <title> Teaching by showing in kendama based on optimization principle. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks (ICANN'94), </booktitle> <volume> volume 1, </volume> <pages> pages 601|606. </pages>
Reference: <author> Miyamoto, H., Schaal, S., Gandolfo, F., Gomi, H., Koike, Y., Osu, R., Nakano, E., Wada, Y., and Kawato, M. </author> <year> (1996). </year> <title> A kendama learning robot based on bi-directional theory. </title> <booktitle> Neural Networks, </booktitle> <volume> 9(8) </volume> <pages> 1281-1302. </pages>
Reference: <author> Nechyba, M. C. and Xu, Y. </author> <year> (1995). </year> <title> Human skill transfer: Neural networks as learners and teachers. </title> <booktitle> In Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <volume> volume 1, </volume> <pages> pages 314|319. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference: <author> Pomerleau, D. A. </author> <year> (1991). </year> <title> Efficient training of artificial neural networks for autonomous navigation. Neural Computation, </title> <publisher> 3:88|97. </publisher>
Reference: <author> Schaal, S. </author> <year> (1997). </year> <title> Learning from demonstration. </title> <editor> In Mozer, M. C., Jordan, M., and Petsche, T., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <note> NIPS96 proceedings, in press. </note>
Reference-contexts: approach was particularly useful in producing a local linear model of the inverted pendulum dynamics (Schaal and Atke-son, 1996; Atkeson et al., 1996b), for example: _ k+1 = 0:47 k +0:997 _ k +0:0051x k +0:0058 _x k +0:052x k Based on previous work on learning pole balancing from demonstration <ref> (Schaal, 1997) </ref> the following reward function was minimized: r (x; u) = 1200 2 + 25 _ 2 + 125x 2 + 50 _x 2 + 1:5x 2 (4) Based on the learned model and this reward function, a balancing policy can be computed for these types of regulation tasks using
Reference: <author> Schaal, S. and Atkeson, C. G. </author> <year> (1996). </year> <title> From isolation to cooperation: An alternative view of a system of experts. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Has-selmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Spong, M. W. </author> <year> (1995). </year> <title> The swing up control problem for the acrobot. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 15(1) </volume> <pages> 49-55. </pages>
Reference-contexts: Atkeson and Schaal (1997) report on a preliminary implementation of learning from demonstration for the pendulum swing up task. 2 THE PENDULUM SWING UP TASK The pendulum swing up task is a more complex version of the pole or broom balancing task <ref> (Spong, 1995) </ref>. The hand holds the axis of the pendulum, and the pendulum rotates about this hinge in an angular movement (Figure 1).
Reference: <author> Tung, C. P. and Kak, A. C. </author> <year> (1995). </year> <title> Automatic learning of assembly tasks using a dataglove system. </title> <booktitle> In Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <volume> volume 1, </volume> <pages> pages 1|8. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference: <author> Widrow, B. and Smith, F. W. </author> <year> (1964). </year> <title> Pattern recognizing control systems. </title> <booktitle> In 1963 Computer and Information Sciences (COINS) Symposium Proceedings, </booktitle> <pages> pages 288|317. </pages> <publisher> Spartan, </publisher> <address> Washington DC. </address>
References-found: 22

