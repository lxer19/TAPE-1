URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P571.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts96.htm
Root-URL: http://www.mcs.anl.gov
Title: Hierarchical Approaches to Automatic Differentiation  
Author: Christian H. Bischof Mohammad R. Haghighat 
Note: Preprint ANL/MCS-P571-0396  
Abstract: A mathematical function, specified by a computer program, can be differentiated efficiently through the exploitation of its program structure. The important properties of a program for an efficient derivative code are the asymmetries between the number of inputs and outputs of program components at various levels of abstraction and the mathematical complexity of the involved operators. Automatic generation of efficient derivative codes thus requires analysis of programs for detection of such properties and systematic methods for their exploitation in composing the derivative codes. We suggest a hierarchical approach based on a partitioning of the computational or program graph as a means to deduce workable solutions to this hard problem. Each partition corresponds to a localized scope for derivative computation, and hierarchical partitions provide a mechanism for exploiting program structure at various levels. As a particular example, we discuss dynamic programming approaches for finding good one-dimensional partitions and generalizations to arbitrary directed acyclic graphs that, by recycling substructure information, allow one to determine the optimal elimination ordering for a graph with n nodes with complexity O(2 n ), as compared with the O(n!) complexity of a naive search. Lastly, we give a concrete example illustrating the hierarchical approach on the driven cavity problem from the MINPACK-2 optimization test set collection. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aho, J. Hopcroft, and J. Ullman, </author> <title> The Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1974. </year>
Reference-contexts: The total number of different elimination orderings is n! (n=e) n , the enumeration of which is clearly impractical unless n is small. However, as illustrated in Figure 3, a hierarchical view of the program may well lead to small graphs. For linear DAGs, dynamic programming <ref> [1, pp. 67-69] </ref> arrives at an O (n 3 ) algorithm for finding the minimum-cost node elimination ordering by realizing that the optimal solution is composed of optimal solutions to subproblems.
Reference: [2] <author> B. M. Averick, R. G. Carter, J. J. More, and G. L. Xue, </author> <title> The MINPACK-2 test problem collection, </title> <type> Tech. Rep. ANL/MCS-TM-150, Rev. 1, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: To this end, we employ the driven cavity problem in the MINPACK-2 test set collection <ref> [2] </ref>. Its main computational kernel (ignoring boundary conditions) is the loop shown in Figure 7, which computes an n = nx * ny vector fvec from an n-vector x.
Reference: [3] <author> C. Bischof, A. Bouaricha, P. Khademi, and J. </author> <title> More, Computing gradients in large-scale optimization using automatic differentiation, </title> <type> Preprint MCS-P488-0195, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year>
Reference-contexts: Illustration of information reuse in node elimination * the degree of nonlinearity in the computation, and * the degree of derivative sparsity (see, for example, <ref> [7, 3] </ref>). Data flow and dependence analysis tools, static performance analyzers, runtime tools for gathering program statistics, and user interaction may all contribute to this phase.
Reference: [4] <author> C. Bischof, A. Carle, G. Corliss, A. Griewank, and P. Hovland, ADIFOR: </author> <title> Generating derivative codes from Fortran programs, </title> <booktitle> Scientific Programming, 1 (1992), </booktitle> <pages> pp. 11-29. </pages>
Reference-contexts: Recently, automatic differentiation has been approached as a source transformation problem in the ADIFOR <ref> [4, 5] </ref>, ADIC [8], AMC [12], and Odyssee [25] tools.
Reference: [5] <author> C. Bischof, A. Carle, P. Khademi, and A. Mauer, </author> <title> The ADIFOR 2.0 system for the automatic differentiation of Fortran 77 programs, 1994. </title> <type> Preprint MCS-P481-1194, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, and CRPC-TR94491, Center for Research on Parallel Computation, Rice University. </institution> <note> To appear in IEEE Computational Science & Engineering. </note>
Reference-contexts: Recently, automatic differentiation has been approached as a source transformation problem in the ADIFOR <ref> [4, 5] </ref>, ADIC [8], AMC [12], and Odyssee [25] tools. <p> Let us assume that we are interested in computing D := d fvec=d x fl S, where S is an n fi p matrix with p n matrix. If we apply a mainly forward-mode tool such as ADIFOR Hierarchical Automatic Differentiation 9 2.0 <ref> [5] </ref> to compute D, the derivative cost will be linear in p. Applying a reverse-mode tool such as Odyssee [25] overall does not make sense, since its complexity would be linear in n p.
Reference: [6] <author> C. Bischof, A. Carle, P. Khademi, A. Mauer, and P. Hovland, </author> <note> ADIFOR 2.0 user's guide, Technical Memorandum ANL/MCS-TM-192, </note> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1994. </year> <note> CRPC Technical Report CRPC-95516-S. </note>
Reference-contexts: The variable p corresponds to the number of columns of the global seed matrix S. The number 13 in the call to g loopbody is the number of derivatives we wish to compute in this call. For more details on the use of ADIFOR-generated code, see <ref> [6] </ref>. On the other hand, the code inside loopbody is a perfect candidate for the use of the reverse mode. We are interested only in the derivatives with respect to one dependent variable, and no loops or branches complicate the generation of the reverse mode.
Reference: [7] <author> C. Bischof, P. Khademi, A. Bouaricha, and A. Carle, </author> <title> Computation of gradients and Jacobians by transparent exploitation of sparsity in automatic differentiation, 1995. </title> <type> Preprint MCS-P519-0595, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, and CRPC-TR95583, Center for Research on Parallel Computation, Rice University. </institution> <note> Accepted for publication in Optimization Methods and Software. </note>
Reference-contexts: Illustration of information reuse in node elimination * the degree of nonlinearity in the computation, and * the degree of derivative sparsity (see, for example, <ref> [7, 3] </ref>). Data flow and dependence analysis tools, static performance analyzers, runtime tools for gathering program statistics, and user interaction may all contribute to this phase.
Reference: [8] <author> C. Bischof, L. Roh, and A. Mauer, </author> <title> unpublished information, </title> <institution> Argonne National Laboratory, </institution> <year> 1996. </year> <title> Hierarchical Automatic Differentiation 13 </title>
Reference-contexts: Recently, automatic differentiation has been approached as a source transformation problem in the ADIFOR [4, 5], ADIC <ref> [8] </ref>, AMC [12], and Odyssee [25] tools.
Reference: [9] <author> C. H. Bischof, </author> <title> Issues in parallel automatic differentiation, in Automatic Differentiation of Algorithms, </title> <editor> A. Griewank and G. Corliss, eds., </editor> <address> Philadelphia, PA, 1991, </address> <publisher> SIAM, </publisher> <pages> pp. 100-113. </pages>
Reference-contexts: case to the linear DAG case by ordering DAG nodes in a topological fashion and considering all nodes at a given level as a "supernode." Since nodes in a supernode are independent, they can actually be eliminated in parallel, an approach that was explored at the elementary operation level in <ref> [9] </ref>. 4 A Case Study We now provide a concrete example illustrating that hierarchical approaches to derivative generation can do considerably better than either of the "monolithic" approaches largely underlying current AD tools. To this end, we employ the driven cavity problem in the MINPACK-2 test set collection [2].
Reference: [10] <author> B. Mohammadi, J.-M. Male, and N. Rostaing-Schmidt, </author> <title> Automatic Differentiation in Direct and Reverse Modes: Application to Optimum Shape Design in Fluid Mechanics, in Computational Differentiation: Techniques, Applications, and Tools, </title> <editor> M. Berz, C. Bischof, G. Corliss, and A. Griewank, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: This approach is the default used for Jacobian generation in Odyssee <ref> [10] </ref>. To reiterate, these examples achieved improvements by defining a portion of the program (a "differentiation partition"), computing the derivatives of this program fragment in a fashion that was oblivious of context, and applying the chain rule at this level of granularity.
Reference: [11] <author> L. C. W. Dixon, </author> <title> Use of automatic differentiation for calculating Hessians and Newton steps, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. F. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991, </year> <pages> pp. 114 - 125. </pages>
Reference-contexts: 1 Introduction Traditionally, automatic differentiation of computer programs has been strongly influenced by the view of the program as a computational graph or Kantorovich graph (see, for example, <ref> [11, 14, 16, 20] </ref>). An example of a computer program and its corresponding computational graph is shown in Figure 1.
Reference: [12] <author> R. Giering, </author> <title> Adjoint model compiler, manual version 0.2, AMC version 2.04, </title> <type> tech. rep., </type> <institution> Max-Planck Institut fur Meteorologie, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Recently, automatic differentiation has been approached as a source transformation problem in the ADIFOR [4, 5], ADIC [8], AMC <ref> [12] </ref>, and Odyssee [25] tools.
Reference: [13] <author> V. V. Goldman, J. Molenkamp, and J. A. van Hulzen, </author> <title> Efficient numerical program generation and computer algebra environments, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. F. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991, </year> <pages> pp. 74-83. </pages>
Reference-contexts: If we eliminate the node computing b first, we obtain @ fl=@ ff = 2 (M b) T y 2 R and @ fl=@ y = 2 (M b) T A 2 R 1fin . We note that an approach at the matrix level was also used in <ref> [13] </ref> to optimize derivatives in the context of a symbolic manipulation system.
Reference: [14] <author> A. Griewank, </author> <title> On automatic differentiation, </title> <booktitle> in Mathematical Programming: Recent Developments and Applications, </booktitle> <address> Amsterdam, 1989, </address> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 83-108. </pages>
Reference-contexts: 1 Introduction Traditionally, automatic differentiation of computer programs has been strongly influenced by the view of the program as a computational graph or Kantorovich graph (see, for example, <ref> [11, 14, 16, 20] </ref>). An example of a computer program and its corresponding computational graph is shown in Figure 1.
Reference: [15] <author> A. Griewank, D. Juedes, and J. Srinivas. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++, </title> <type> Preprint MCS-P180-1190, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1990. </year>
Reference-contexts: Hence, there is no need to actually build the computational graph. In contrast, the reverse mode of automatic differentiation eliminates the nodes by starting at the output variables of the program, thus requiring storage of the computational or linearized graph in some form. For example, ADOL-C <ref> [15] </ref> generates a "tape," encoding the operands and operations in the order in which they were encountered during a program execution; postprocessing utilities can then compute various varieties of derivatives offline. In contrast, JAKEF [18] stores the linearized graph directly.
Reference: [16] <author> A. Griewank and S. Reese, </author> <title> On the calculation of Jacobian matrices by the Markowitz rule, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. F. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1991, </year> <pages> pp. 126-135. </pages>
Reference-contexts: 1 Introduction Traditionally, automatic differentiation of computer programs has been strongly influenced by the view of the program as a computational graph or Kantorovich graph (see, for example, <ref> [11, 14, 16, 20] </ref>). An example of a computer program and its corresponding computational graph is shown in Figure 1. <p> The linearized graph induced by the computational graph in Figure 1 is shown on the left side of Figure 2. Since all operations have been linearized, the operands associated with the nodes can be omitted. As described by Griewank and Reese <ref> [16] </ref>, the final Jacobian can then be computed by a graph elimination approach. <p> Linearized graph induced by computation shown in Figure 1 (left) and example of node elimination rule for linearized graph (right) a graph representation has been built, however, other elimination orderings are possible, such as the Markowitz rule described in <ref> [16] </ref>. The drawback of this fine-grained graph view of automatic differentiation is that it does not scale.
Reference: [17] <author> M. R. Haghighat, </author> <title> Symbolic Analysis for Parallelizing Compilers, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, </address> <year> 1995. </year>
Reference-contexts: Data flow and dependence analysis techniques, developed in the context of parallelizing compilers (see, for example, <ref> [17, 23] </ref>), provide a strong foundation from which such information can be derived.
Reference: [18] <author> K. E. Hillstrom, </author> <title> JAKEF a portable symbolic differentiator of functions given by algorithms, </title> <type> Technical Report ANL/82-48, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1982. </year>
Reference-contexts: For example, ADOL-C [15] generates a "tape," encoding the operands and operations in the order in which they were encountered during a program execution; postprocessing utilities can then compute various varieties of derivatives offline. In contrast, JAKEF <ref> [18] </ref> stores the linearized graph directly. Note that each of these representations captures only a particular execution path through the program. Once such Hierarchical Automatic Differentiation 3 2.00 0.50 1.00 2.00 0.67 1 1 2 b c*a b+d*a Fig. 2.
Reference: [19] <author> P. Hovland, C. Bischof, D. Spiegelman, and M. Casella, </author> <title> Efficient derivative codes through automatic differentiation and interface contraction: An application in biostatistics, </title> <type> Preprint MCS-P491-0195, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1995. </year> <note> To appear in SIAM J. Scientific Computing. </note>
Reference-contexts: Iri [20] referred to such a situation as a "vertex cut"; Hovland et al. <ref> [19] </ref> called it an "interface contraction." This small example illustrates three important principles. Graph Partitioning: The identification of program sections whose derivatives with respect to input and output variables should be computed "out of context," that is, ignoring the surrounding computations. <p> A concrete example of stencil-level partitioning is given in the next section. 4. Moving up further in the program structure, we observe that subroutine boundaries may be likely places for interface contraction. For example, in the application described in <ref> [19] </ref>, a subroutine that computed a scalar output value from two scalar input values was called in a loop.
Reference: [20] <author> M. Iri, </author> <title> History of automatic differentiation and rounding estimation, in Automatic Differentiation of Algorithms: Theory, Implementation, and Application, </title> <editor> A. Griewank and G. F. Corliss, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, Penn., </address> <year> 1991, </year> <pages> pp. 1-16. </pages>
Reference-contexts: 1 Introduction Traditionally, automatic differentiation of computer programs has been strongly influenced by the view of the program as a computational graph or Kantorovich graph (see, for example, <ref> [11, 14, 16, 20] </ref>). An example of a computer program and its corresponding computational graph is shown in Figure 1. <p> Iri <ref> [20] </ref> referred to such a situation as a "vertex cut"; Hovland et al. [19] called it an "interface contraction." This small example illustrates three important principles.
Reference: [21] <author> A. John and J. C. Browne, </author> <title> A constraint-based parallel programming language, </title> <type> Tech. Rep. </type> <institution> TR95-42, Department of Computer Science, University of Texas at Austin, </institution> <year> 1995. </year>
Reference-contexts: One way to circumvent this difficulty is to increase the granularity of the elementary operators in the underlying language. For example, the CODE system <ref> [21] </ref>, as well as the class definitions underlying the weather model described in [24], employs 3-D data structures as elementary objects. Vectors are employed as base types in [26], and the elementary objects are supported by an algebraic manipulation system in [22].
Reference: [22] <author> M. Monagan, </author> <title> An implementation of automatic differentiation in Maple, 1995. Personal Communication. Software available in the MAPLE Share library. </title>
Reference-contexts: For example, the CODE system [21], as well as the class definitions underlying the weather model described in [24], employs 3-D data structures as elementary objects. Vectors are employed as base types in [26], and the elementary objects are supported by an algebraic manipulation system in <ref> [22] </ref>. The reduction in complexity of the computational graph can be dramatic, as illustrated in Figure 3, which expresses computation at the matrix and vector levels (the matrix M is considered constant with respect to differentiation). Here, up to O (n 2 ) elementary operations are represented by one node.
Reference: [23] <author> C. Polychronopoulos, </author> <title> Parallel Programming and Compilers, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Massachusetts, </address> <year> 1988. </year>
Reference-contexts: Data flow and dependence analysis techniques, developed in the context of parallelizing compilers (see, for example, <ref> [17, 23] </ref>), provide a strong foundation from which such information can be derived.
Reference: [24] <author> A. Rhodin, U. Callies, and D. Eppel, </author> <title> GESIMA 90 An object-oriented approach to program a mesoscale model and its adjoint, 1994. Talk presented at the General Assembly of the European Geophysical Society at Grenoble, </title> <month> April 25-29. </month>
Reference-contexts: One way to circumvent this difficulty is to increase the granularity of the elementary operators in the underlying language. For example, the CODE system [21], as well as the class definitions underlying the weather model described in <ref> [24] </ref>, employs 3-D data structures as elementary objects. Vectors are employed as base types in [26], and the elementary objects are supported by an algebraic manipulation system in [22].
Reference: [25] <author> N. Rostaing, S. Dalmas, and A. Galligo, </author> <title> Automatic differentiation in Odyssee, </title> <booktitle> Tellus, 45a (1993), </booktitle> <pages> pp. 558-568. </pages>
Reference-contexts: Recently, automatic differentiation has been approached as a source transformation problem in the ADIFOR [4, 5], ADIC [8], AMC [12], and Odyssee <ref> [25] </ref> tools. <p> If we apply a mainly forward-mode tool such as ADIFOR Hierarchical Automatic Differentiation 9 2.0 [5] to compute D, the derivative cost will be linear in p. Applying a reverse-mode tool such as Odyssee <ref> [25] </ref> overall does not make sense, since its complexity would be linear in n p. To do better, we realize that the loop body can be viewed as a mapping of 13 distinct elements of the array x to fvec (k).
Reference: [26] <author> D. Shiriaev, </author> <title> Fast automatic differentiation for vector processors and reduction of the spatial complexity in a source translation environment, </title> <type> PhD thesis, </type> <institution> Department of Mathematics, Universitat Karlsruhe, </institution> <year> 1993. </year>
Reference-contexts: For example, the CODE system [21], as well as the class definitions underlying the weather model described in [24], employs 3-D data structures as elementary objects. Vectors are employed as base types in <ref> [26] </ref>, and the elementary objects are supported by an algebraic manipulation system in [22]. The reduction in complexity of the computational graph can be dramatic, as illustrated in Figure 3, which expresses computation at the matrix and vector levels (the matrix M is considered constant with respect to differentiation).
References-found: 26

