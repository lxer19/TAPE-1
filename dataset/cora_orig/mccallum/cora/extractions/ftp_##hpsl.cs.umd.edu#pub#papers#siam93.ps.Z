URL: ftp://hpsl.cs.umd.edu/pub/papers/siam93.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/papers.brandnew/LocalResources/tech-10-23.htm
Root-URL: 
Title: Parallelizing Molecular Dynamics Codes using Parti Software Primitives  
Author: R. Das J. Saltz 
Abstract: This paper is concerned with the implementation of the molecular dynamics code, CHARMM, on massively parallel distributed-memory computer architectures using a data-parallel approach. The implementation is carried out by creating a set of software tools, which provide an interface between the parallelization issues and the sequential code. Large practical MD problems is solved on the Intel iPSC/860 hypercube. The overall solution efficiency is compared with that obtained when implementation is done using data-replication.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. C. G. A. I. Mel'cuk and H. Gould, </author> <title> Molecular dynamics simulation of liquids on the connection machine, </title> <booktitle> Computers in Physics, (May/Jun 1991), </booktitle> <pages> pp. 311-318. </pages>
Reference-contexts: The first is the development of efficient and inherently parallelizable algorithms to do the inter-particle force calculations, which happens to consume bulk of the computation time in these codes. A number of parallel algorithms has been designed and implemented for both SIMD and MIMD architectures <ref> [7, 1, 8, 9] </ref>. The second is an implementation issue. Most often, the programmer is required to explicitly distribute large arrays over multiple local processor memories, and keep track of which portions of each array reside on which processors.
Reference: [2] <author> H. Berryman, J. Saltz, and J. Scroggs, </author> <title> Execution time support for adaptive scientific algorithms on distributed memory architectures, </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3 (1991), </volume> <pages> pp. 159-178. </pages>
Reference-contexts: Some of the important simulation routines are the dynamic analysis, the trajectory manipulations, energy calculations and minimization, and vibrational analysis. It also performs statistical analysis, time series and correlation function analysis and spectral analysis. 3 Parti Primitives The PARTI primitives (Parallel Automated Runtime Toolkit at ICASE) <ref> [2] </ref> are designed to ease the implementation of computational problems on parallel architecture machines by relieving the user of the low-level machine specific issues. The primitives enable the distribution and retrieval of globally indexed but irregularly distributed data-sets over the numerous local processor memories.
Reference: [3] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus, Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations, </title> <journal> Journal of Computational Chemistry, </journal> <note> 4 (1983), p. 187. </note>
Reference-contexts: Given the relatively slow increases in large mainframe supercomputer capabilities, it is now generally acknowledged that the most feasible and economical means of solving extremely large computational problems in the future will be with highly parallel distributed memory architectures. The large molecular dynamics (MD) codes like CHARMM <ref> [3] </ref>, GROMOS [10], AMBER [11], etc. have parts which are extremely computationally intensive. Imple-mentationg these codes on massively parallel machines not only reduces the total solution time but allows one to solve very large problems. <p> The pairlist generation is not performed every iteration but after every n iterations where n is a variable that can be fixed by the user. The MD code that we used was CHARMM (Chemistry at HARvard Macromolecular Mechanics) <ref> [3] </ref> which was developed at Harvard University for biomolecular simulations. It is a relatively efficient program which uses empirical energy functions to model molecular systems. The code is written in Fortran and is about 110,000 lines. The program is capable of performing a wide range of analyses.
Reference: [4] <author> B. R. Brooks and M. Hodoscek, </author> <title> Parallelization of charmm for mimd machines, </title> <journal> Chemical Design Automation News, </journal> <note> 7 (1992), p. 16. </note>
Reference-contexts: This savings would require a further compromise in the sequential operation ordering, but at least one group has found that this modification does not have an adverse effect on the computational results <ref> [4] </ref>. These results can be compared with the ones presented by Brooks in [4]. Their implementation replicates all the data structure on all the processors. <p> This savings would require a further compromise in the sequential operation ordering, but at least one group has found that this modification does not have an adverse effect on the computational results <ref> [4] </ref>. These results can be compared with the ones presented by Brooks in [4]. Their implementation replicates all the data structure on all the processors.
Reference: [5] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis, </author> <title> Distributed memory compiler methods for irregular problems data copy reuse and runtime partitioning, in Compilers and Runtime Software for Scalable Multiprocessors, </title> <editor> J. Saltz and P. Mehrotra Editors, </editor> <address> Amsterdam, The Netherlands, 1992, </address> <publisher> Elsevier. </publisher>
Reference-contexts: The parallelized code looks very similar to the sequential code except that it has a number of Parti primitive calls embeded in it. Initially, all parallel irregular loops were transformed into inspector/executor pairs. Then we performed optimizations associated with reusing copies of off-processor data and with vectorizing communications <ref> [5] </ref>. The entire energy calculation portion of CHARMM has been parallelized. This involves both the internal (bond, angle etc.) energy calculation and the external (nonbonded) energy calculations. We present some of the results that we have obtained on the Intel iPSC/860. <p> In the larger carboxy-myoglobin simulation, pairlist generation proved to be extremely expensive. The reasons for the high cost do not appear to be fundamental, these costs in part are prompting us to redesign the address translation mechanisms in our software. We utilize incremental scheduling <ref> [5] </ref> to reduce the volume of data to be communicated by reusing live data that has been fetched for previous loops. Since we use incremental scheduling we can fetch all the required data i.e. the off processor atom coordinates that will be referenced.
Reference: [6] <author> R. Mirchandaney, J. H. Saltz, R. M. Smith, D. M. Nicol, and K. Crowley, </author> <title> Principles of runtime support for parallel processors, </title> <booktitle> in Proceedings of the 1988 ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1988, </year> <pages> pp. 140-152. </pages>
Reference-contexts: In irregular problems, such as calculating inter-particle forces and sparse matrix algorithms, it is not possible to predict at compile time what data must be prefetched. This lack of information is dealt with by transforming the original parallel loop into two constructs called an inspector and executor <ref> [6] </ref>. The PARTI primitives can be used directly by programmers to generate inspector/executor pairs. Each inspector produces a communications schedule, which is essentially a pattern of communication for gathering or scattering data. The executor has embedded PARTI primitives to gather or scatter data.
Reference: [7] <author> J. P. M. P. Tamayo and B. M. Boghosian, </author> <title> Parallel approaches to short range molecular dynamics simulations, </title> <booktitle> in Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: The first is the development of efficient and inherently parallelizable algorithms to do the inter-particle force calculations, which happens to consume bulk of the computation time in these codes. A number of parallel algorithms has been designed and implemented for both SIMD and MIMD architectures <ref> [7, 1, 8, 9] </ref>. The second is an implementation issue. Most often, the programmer is required to explicitly distribute large arrays over multiple local processor memories, and keep track of which portions of each array reside on which processors.
Reference: [8] <author> S. Plimpton and G. Heffelfinger, </author> <title> Scalable parallel molecular dynamics on mimd supercomputers, </title> <booktitle> in SHPCC92, </booktitle> <address> Williamsburg, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: The first is the development of efficient and inherently parallelizable algorithms to do the inter-particle force calculations, which happens to consume bulk of the computation time in these codes. A number of parallel algorithms has been designed and implemented for both SIMD and MIMD architectures <ref> [7, 1, 8, 9] </ref>. The second is an implementation issue. Most often, the programmer is required to explicitly distribute large arrays over multiple local processor memories, and keep track of which portions of each array reside on which processors.
Reference: [9] <author> J. A. M. Terry W. Clark and L. R. Scott, </author> <title> Parallel molecular dynamics, </title> <booktitle> in Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Houston, TX., </address> <month> March </month> <year> 1991. </year>
Reference-contexts: The first is the development of efficient and inherently parallelizable algorithms to do the inter-particle force calculations, which happens to consume bulk of the computation time in these codes. A number of parallel algorithms has been designed and implemented for both SIMD and MIMD architectures <ref> [7, 1, 8, 9] </ref>. The second is an implementation issue. Most often, the programmer is required to explicitly distribute large arrays over multiple local processor memories, and keep track of which portions of each array reside on which processors.
Reference: [10] <author> W. F. van Gunsteren and H. J. C. Berendsen, Gromos: </author> <title> Groningen molecular simulation software., </title> <type> technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: Given the relatively slow increases in large mainframe supercomputer capabilities, it is now generally acknowledged that the most feasible and economical means of solving extremely large computational problems in the future will be with highly parallel distributed memory architectures. The large molecular dynamics (MD) codes like CHARMM [3], GROMOS <ref> [10] </ref>, AMBER [11], etc. have parts which are extremely computationally intensive. Imple-mentationg these codes on massively parallel machines not only reduces the total solution time but allows one to solve very large problems.
Reference: [11] <author> P. K. Weiner and P. A. Kollman, </author> <title> Amber:assisted model building with energy refinement. a general program for modeling molecules and their interactions, </title> <journal> Journal of Computational Chemistry, </journal> <note> 2 (1981), p. 287. </note>
Reference-contexts: The large molecular dynamics (MD) codes like CHARMM [3], GROMOS [10], AMBER <ref> [11] </ref>, etc. have parts which are extremely computationally intensive. Imple-mentationg these codes on massively parallel machines not only reduces the total solution time but allows one to solve very large problems. There are two software issues that needs to be addressed when one considers to effectively parallelize MD codes.
References-found: 11

