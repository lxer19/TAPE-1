URL: http://www.eecs.umich.edu/~bnoble/papers/cmg95.ps
Refering-URL: http://www.eecs.umich.edu/~bnoble/papers/papers.html
Root-URL: http://www.eecs.umich.edu
Email: bnoble@cs.cmu.edu  
Title: The Structure and Evolution of a Distributed Measurement Framework  
Author: Brian D. Noble 
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Distributed system are becoming increasingly important in day to day computing tasks. While building such systems is a well understood process, measuring and monitoring such systems is not. This paper describes the difficulties inherent in measuring distributed systems, and enumerates four goals for such measurement: longevity, flexibility, fault-tolerance, and unintrusiveness. It then describes the Coda File System, a research system that has been deployed in a moderately-sized user community for four years, and actively measured for three and a half years. The architecture of this measurement framework is described in detail, with an eye toward examining how well it meets the four goals. The paper concludes with the lessons to be taken from this experience, both those that were foreseen as well as those that were learned along the way. In an effort to help teach these lessons, we have made the Coda source code, along with the measurement framework, freely available for any purpose. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> BAKER, M. G., HARTMAN, J. H., KUPFER, M. D., SHIRRIFF, K. W., AND OUSTERHOUT, J. K. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating System Principles (Pacific Grove, </booktitle> <address> CA, </address> <month> October </month> <year> 1991). </year>
Reference-contexts: For example, if a file is created, renamed, and later deleted, none of the records need be kept in the log; it is as if they never happened. Such outdated records are removed from the log to manage its size. The locality of update observed in typical Unix workloads <ref> [1, 15] </ref> predicts that such optimizations will be very effective. When Venus reestablishes contact with one or more servers, it replays the logged updates through a process called reintegration. 3.3 Current Status Coda was first deployed for use in the Fall of 1991. It has evolved considerably over that time.
Reference: [2] <author> BHIDE, A., ELNOZAHY, E. N., AND MORGAN, S. P. </author> <title> A Highly Available Network File Server. </title> <booktitle> In Winter Usenix Conference Proceedings (Dallas, </booktitle> <address> TX, </address> <month> January </month> <year> 1991). </year>
Reference-contexts: Unfortunately, we have not done nearly so good a job in measuring these systems. Even more unfortunately, just catching up to fault tolerance will still leave us behind. An extremely active area of research in the academic community is the design and construction of highly-available distributed systems <ref> [2, 3, 7, 16, 18, 22] </ref>. In a highly-available system, not only must the system not produce incorrect results in the face of failures, but it must continue correct operation. In some systems, these failures may span many days.
Reference: [3] <author> COVA, L. </author> <title> Resource Management in Federated Computing Environments. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: Unfortunately, we have not done nearly so good a job in measuring these systems. Even more unfortunately, just catching up to fault tolerance will still leave us behind. An extremely active area of research in the academic community is the design and construction of highly-available distributed systems <ref> [2, 3, 7, 16, 18, 22] </ref>. In a highly-available system, not only must the system not produce incorrect results in the face of failures, but it must continue correct operation. In some systems, these failures may span many days.
Reference: [4] <author> FLOYD, R. </author> <title> Directory Reference Patterns in a Unix Environment. </title> <type> Tech. Rep. </type> <institution> TR-179, Department of Computer Science, University of Rochester, </institution> <year> 1986. </year>
Reference-contexts: A prototype of the current system was first implemented in 1990. A full implementation was built in 1992 to incorporate all of the features described here. That version's focus was on understanding disconnected operation as well as the more traditional file system metrics reported in similar studies <ref> [24, 23, 15, 4, 5] </ref>. During 1993, measurement of server replication was added, and it is this system that was used to provide data for [14] and [12]. Within the past year, the system has been further extended by three researchers concurrently.
Reference: [5] <author> FLOYD, R. </author> <title> Short-Term File Reference Patterns in a Unix Environment. </title> <type> Tech. Rep. </type> <institution> TR-177, Department of Computer Science, University of Rochester, </institution> <year> 1986. </year>
Reference-contexts: A prototype of the current system was first implemented in 1990. A full implementation was built in 1992 to incorporate all of the features described here. That version's focus was on understanding disconnected operation as well as the more traditional file system metrics reported in similar studies <ref> [24, 23, 15, 4, 5] </ref>. During 1993, measurement of server replication was added, and it is this system that was used to provide data for [14] and [12]. Within the past year, the system has been further extended by three researchers concurrently.
Reference: [6] <author> GRAY, J., AND REUTER, A. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufmann Publiishers, </publisher> <address> San Francisco, CA, </address> <year> 1993. </year>
Reference-contexts: On the face of it, it seems unlikely that a failure-free system can be produced; even if it were possible it is estimated to cost a full order of magnitude more than current systems <ref> [6] </ref>. Even if that cost can be met, the system can never control external factors, both human and environmental. 2.3 Measurement Requirements What makes for good measurement of distributed systems? There are four key goals a distributed measurement framework must meet: longevity, flexibility, fault-tolerance, and unintrusiveness.
Reference: [7] <author> HISGEN, A., BIRRELL, A., MANN, T., SCHROEDER, M., AND SWART, G. </author> <title> Availability and Consistency Tradeoffs in the Echo Distributed File System. </title> <booktitle> In Proceedings of the Second Workshop on Workstation Operating Systems (Pacific Grove, </booktitle> <address> CA, </address> <month> September </month> <year> 1989). </year>
Reference-contexts: Unfortunately, we have not done nearly so good a job in measuring these systems. Even more unfortunately, just catching up to fault tolerance will still leave us behind. An extremely active area of research in the academic community is the design and construction of highly-available distributed systems <ref> [2, 3, 7, 16, 18, 22] </ref>. In a highly-available system, not only must the system not produce incorrect results in the face of failures, but it must continue correct operation. In some systems, these failures may span many days.
Reference: [8] <author> HOWARD, J. H., KAZAR, M. L., MENEES, S. G., NICHOLS, D. A., SATYANARAYANAN, M., SIDEBOTHAM, R. N., AND WEST, M. J. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems 6, </journal> <month> 1 (February </month> <year> 1988). </year>
Reference-contexts: Managing the growth of data over time is a particular concern when measuring large distributed systems. 3 Coda: A Concrete Example This section describes the Coda File System, which exhibits all of the characteristics outlined in Section 2.1. Coda, a descendant of AFS <ref> [8] </ref>, provides highly available file access to clients in the face of network and server failures. The current system can support file access while disconnected, or at any network speed from 10 Mb/s ethernet down to 9600 baud modem.
Reference: [9] <author> JAIN, R. </author> <title> The Art of Computer Systems Performance. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: There is no complete set of principles which can make measurement an engineering discipline rather than a science, though some good texts do exist. One of the best of these is Jain <ref> [9] </ref>. However, much work remains to take the mystery out of measurement. 6 Obtaining Source Code Coda source code is available for unrestricted including commercial use. There is no fee or royalty. However, the code is not completely unencumbered; obtaining it requires the signing of a license.
Reference: [10] <author> KISTLER, J. J. </author> <title> Disconnected Operation in a Distributed File System. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1993. </year>
Reference-contexts: Our current release policy promotes alpha to beta and beta to production every month. Coda remains a very active platform for research; two Ph.D. theses have been published <ref> [10, 12] </ref>, and three other students are currently finishing their doctoral research within Coda. Needless to say, the system is undergoing active development. 3.4 Why Coda Was Measured When first proposed, Coda was met with some skepticism by the academic community, which raised several pointed questions.
Reference: [11] <author> KISTLER, J. J., AND SATYANARAYANAN, M. </author> <title> Disconnected Operation in the Coda File System. </title> <journal> ACM Transactions on Computer Systems 10, </journal> <month> 1 (February </month> <year> 1992). </year>
Reference-contexts: These failures are exacerbated in mobile systems, where failures the loss of connectivity between machines are not rare events simply to be tolerated but an expected and interesting part of life. The Coda File System <ref> [11, 18] </ref> is a distributed system explicitly designed to provide service in the presence of network and server This research was supported by the Air Force Materiel Command (AFMC) and ARPA under contract number F196828-93-C-0193.
Reference: [12] <author> KUMAR, P. </author> <title> Mitgating the Effects of Optimistic Replication in a Distributed File System. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> December </month> <year> 1994. </year> <note> Technical report number CMU-CS-94-215. </note>
Reference-contexts: Our current release policy promotes alpha to beta and beta to production every month. Coda remains a very active platform for research; two Ph.D. theses have been published <ref> [10, 12] </ref>, and three other students are currently finishing their doctoral research within Coda. Needless to say, the system is undergoing active development. 3.4 Why Coda Was Measured When first proposed, Coda was met with some skepticism by the academic community, which raised several pointed questions. <p> The original questions, and their answers, appear elsewhere [14]; the architecture that collected this data has remained largely unchanged since that time, and has also been used to further explore server replication in <ref> [12] </ref>. Since its original deployment, the system has evolved considerably. The first major project, which has been completed, extended Coda clients to operate with only slow and intermittent networks. This substantially changed the logic of reintegration as well as the underlying RPC transport mechanism. <p> That version's focus was on understanding disconnected operation as well as the more traditional file system metrics reported in similar studies [24, 23, 15, 4, 5]. During 1993, measurement of server replication was added, and it is this system that was used to provide data for [14] and <ref> [12] </ref>. Within the past year, the system has been further extended by three researchers concurrently. Measurements for weakly-connected operation have been reported in [13]. Measurements for both cache mangement improvements and semantic extensions to the file system interface are currently being collected and analyzed; their results are forthcoming.
Reference: [13] <author> MUMMERT, L. B., EBLING, M. R., AND SATYANARAYANAN, M. </author> <title> Exploiting Weak Connectivity for Mobile File Access. </title> <booktitle> In Proceedings of the 15th Symposium on Operating System Prinicples (Copper Mountain, </booktitle> <publisher> CO, </publisher> <month> December </month> <year> 1995). </year>
Reference-contexts: The current system can support file access while disconnected, or at any network speed from 10 Mb/s ethernet down to 9600 baud modem. Experiments have shown that typical usage over this diverse range of connectivity performs astonishingly close to that of the best case at all times <ref> [13] </ref>. In the sections to follow, the basic operation of Coda is described, as are its mechanisms for high-availability. <p> Having over three years of data on Coda performance has proven to be immensely useful. We've been able to find several details of performance and behavior that were not predicted in the design [14], and have observed how functionality improvements have impacted performance <ref> [13] </ref>. The database software arbitrates between different researchers' analyses and data insertion quite effectively. 4.4 Flexibility in the Monitor Framework Discounting the versioning layer between Coda software and the monitor daemon, we have not explored how the collection architecture was designed for flexibility. <p> During 1993, measurement of server replication was added, and it is this system that was used to provide data for [14] and [12]. Within the past year, the system has been further extended by three researchers concurrently. Measurements for weakly-connected operation have been reported in <ref> [13] </ref>. Measurements for both cache mangement improvements and semantic extensions to the file system interface are currently being collected and analyzed; their results are forthcoming. All of the researchers who have instrumented Coda have pointed to the instrumentation of the Coda software itself was the most difficult task they faced.
Reference: [14] <author> NOBLE, B. D., AND SATYANARAYANAN, M. </author> <title> An Empirical Study of a Highly Available Filesystem. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modelling of Computer Systems (Nashville, </booktitle> <address> TN, </address> <month> May </month> <year> 1994). </year>
Reference-contexts: The measurement framework was initially designed and built to answer those questions, but also to aid in answering future questions as they arose. The original questions, and their answers, appear elsewhere <ref> [14] </ref>; the architecture that collected this data has remained largely unchanged since that time, and has also been used to further explore server replication in [12]. Since its original deployment, the system has evolved considerably. <p> Once data is entered into the database, individual researchers are able to run arbitrary offline analyses. Having over three years of data on Coda performance has proven to be immensely useful. We've been able to find several details of performance and behavior that were not predicted in the design <ref> [14] </ref>, and have observed how functionality improvements have impacted performance [13]. <p> That version's focus was on understanding disconnected operation as well as the more traditional file system metrics reported in similar studies [24, 23, 15, 4, 5]. During 1993, measurement of server replication was added, and it is this system that was used to provide data for <ref> [14] </ref> and [12]. Within the past year, the system has been further extended by three researchers concurrently. Measurements for weakly-connected operation have been reported in [13]. Measurements for both cache mangement improvements and semantic extensions to the file system interface are currently being collected and analyzed; their results are forthcoming.
Reference: [15] <author> OUSTERHOUT, J. K., DACOSTA, H., HARRISON, D., KUNZE, J. A., KUPFER, M., AND THOMPSON, J. G. </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System. </title> <booktitle> In Proceedings of the 10th ACM Symposium on Operating System Principles (Orcas Island, </booktitle> <address> WA, </address> <month> December </month> <year> 1985). </year>
Reference-contexts: For example, if a file is created, renamed, and later deleted, none of the records need be kept in the log; it is as if they never happened. Such outdated records are removed from the log to manage its size. The locality of update observed in typical Unix workloads <ref> [1, 15] </ref> predicts that such optimizations will be very effective. When Venus reestablishes contact with one or more servers, it replays the logged updates through a process called reintegration. 3.3 Current Status Coda was first deployed for use in the Fall of 1991. It has evolved considerably over that time. <p> A prototype of the current system was first implemented in 1990. A full implementation was built in 1992 to incorporate all of the features described here. That version's focus was on understanding disconnected operation as well as the more traditional file system metrics reported in similar studies <ref> [24, 23, 15, 4, 5] </ref>. During 1993, measurement of server replication was added, and it is this system that was used to provide data for [14] and [12]. Within the past year, the system has been further extended by three researchers concurrently.
Reference: [16] <author> POPEK, G. J., GUY, R. G., PAGE, T. W., AND HEIDEMANN, J. S. </author> <title> Replication in Ficus Distributed File Systems. </title> <booktitle> In Proceedings of the Workshop on Management of Replicated Data (Houston, </booktitle> <address> TX, </address> <month> November </month> <year> 1990). </year>
Reference-contexts: Unfortunately, we have not done nearly so good a job in measuring these systems. Even more unfortunately, just catching up to fault tolerance will still leave us behind. An extremely active area of research in the academic community is the design and construction of highly-available distributed systems <ref> [2, 3, 7, 16, 18, 22] </ref>. In a highly-available system, not only must the system not produce incorrect results in the face of failures, but it must continue correct operation. In some systems, these failures may span many days.
Reference: [17] <author> RODEHEFFER, T. L., AND SCHOREDER, M. D. </author> <title> Automatic Reconfiguration in Autonet. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles (Pacific Grove, </booktitle> <address> CA, </address> <month> October </month> <year> 1991). </year>
Reference-contexts: Some good examples of measuring and monitoring these systems do exist. Two that come to mind are the measurements of the Sprite distributed file system by Baker et. al.[1] and the monitoring of the Autonet local area network reported by Rodeheffer and Schroeder <ref> [17] </ref>. Unfortunately, even these have shortcomings. The former was a short term snapshot; it was not designed as a long-term monitoring system. The latter, while it continuously monitors the system's health as it runs, does not log the failures that it detects for later analysis.
Reference: [18] <author> SATYANARAYANAN, M., KISTLER, J. J., KUMAR, P., OKASAKI, M. E., SIEGEL, E. H., AND STEERE, D. C. Coda: </author> <title> A Highly Available File System for a Distributed Workstation Environment. </title> <journal> IEEE Transactions on Computers 39, </journal> <month> 4 (April </month> <year> 1990). </year>
Reference-contexts: These failures are exacerbated in mobile systems, where failures the loss of connectivity between machines are not rare events simply to be tolerated but an expected and interesting part of life. The Coda File System <ref> [11, 18] </ref> is a distributed system explicitly designed to provide service in the presence of network and server This research was supported by the Air Force Materiel Command (AFMC) and ARPA under contract number F196828-93-C-0193. <p> Unfortunately, we have not done nearly so good a job in measuring these systems. Even more unfortunately, just catching up to fault tolerance will still leave us behind. An extremely active area of research in the academic community is the design and construction of highly-available distributed systems <ref> [2, 3, 7, 16, 18, 22] </ref>. In a highly-available system, not only must the system not produce incorrect results in the face of failures, but it must continue correct operation. In some systems, these failures may span many days.
Reference: [19] <author> SATYANARAYANAN, M., MASHBURN, H. H., KUMAR, P., STEERE, D. C., AND KISTLER, J. J. </author> <title> Lightweight Recoverable Virtual Memory. </title> <journal> ACM Transactions on Computer Systems 12, </journal> <volume> 1 (Februrary 1994), </volume> <pages> 33-57. Corrigendum: </pages> <month> May </month> <year> 1994, </year> <journal> Vol. </journal> <volume> 12, No. 2, </volume> <pages> pp. 165-172. </pages>
Reference-contexts: All Coda software is multi-threaded, and communication between clients and servers is handled by RPC2 [20], a remote procedure call mechanism first used in AFS. Clients and servers keep metadata and some logging information in RVM <ref> [19] </ref>, recoverable virtual memory. RVM provides applications with virtual memory having the transactional properties of durability and atomicity. An application using RVM first maps a range of bytes from a disk file into memory. Updates made to this range of memory are grouped in transactions.
Reference: [20] <author> SATYANARAYANAN, M., AND SIEGEL, E. H. </author> <title> Parallel Communication in a Large Distributed Environment. </title> <journal> IEEE Transactions on Computers 39, </journal> <month> 3 (March </month> <year> 1990). </year>
Reference-contexts: Coda relies on several software packages: lightweight threads, remote procedure call, and a persistent memory package. All Coda software is multi-threaded, and communication between clients and servers is handled by RPC2 <ref> [20] </ref>, a remote procedure call mechanism first used in AFS. Clients and servers keep metadata and some logging information in RVM [19], recoverable virtual memory. RVM provides applications with virtual memory having the transactional properties of durability and atomicity.
Reference: [21] <author> SIDEBOTHAM, R. </author> <title> Volumes: The Andrew File System Data Structuring Primitive. In European Unix User Group Conference Proceedings (August 1986). </title> <note> Also available as Tech. Rep. </note> <institution> CMU-ITC-053, Carnegie Mellon University, Information Technology Center. </institution>
Reference-contexts: We then turn briefly to Coda's current status, and conclude with our motivations for measuring Coda. 3.1 Basic Operation Coda presents all of its clients with a single, global namespace comprised of volumes <ref> [21] </ref>, subtrees of the namespace. During normal operation, Coda's client cache manager, Venus, caches files in their entirety on the client's local disk and then services read and write requests through the cached copy.
Reference: [22] <author> SIEGEL, A., BIRMAN, K., AND MARZULLO, K. Deceit: </author> <title> A Flexible Distributed File System. </title> <type> Tech. Rep. TR 89-1042, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1989. </year>
Reference-contexts: Unfortunately, we have not done nearly so good a job in measuring these systems. Even more unfortunately, just catching up to fault tolerance will still leave us behind. An extremely active area of research in the academic community is the design and construction of highly-available distributed systems <ref> [2, 3, 7, 16, 18, 22] </ref>. In a highly-available system, not only must the system not produce incorrect results in the face of failures, but it must continue correct operation. In some systems, these failures may span many days.
Reference: [23] <author> SMITH, A. J. </author> <title> Analysis of Long Term File Reference Patterns for Application to File Migration Algorithms. </title> <journal> IEEE Transactions on Software Engineering 7, </journal> <month> 4 (July </month> <year> 1981). </year>
Reference-contexts: A prototype of the current system was first implemented in 1990. A full implementation was built in 1992 to incorporate all of the features described here. That version's focus was on understanding disconnected operation as well as the more traditional file system metrics reported in similar studies <ref> [24, 23, 15, 4, 5] </ref>. During 1993, measurement of server replication was added, and it is this system that was used to provide data for [14] and [12]. Within the past year, the system has been further extended by three researchers concurrently.
Reference: [24] <author> STRITTER, E. P. </author> <title> File Migration. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1977. </year>
Reference-contexts: A prototype of the current system was first implemented in 1990. A full implementation was built in 1992 to incorporate all of the features described here. That version's focus was on understanding disconnected operation as well as the more traditional file system metrics reported in similar studies <ref> [24, 23, 15, 4, 5] </ref>. During 1993, measurement of server replication was added, and it is this system that was used to provide data for [14] and [12]. Within the past year, the system has been further extended by three researchers concurrently.
References-found: 24

