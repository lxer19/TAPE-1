URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95551.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: (augusta@rice.edu).  (sorensen@rice.edu).  
Title: A New Matrix-Free Algorithm for the Large-Scale Trust-Region Subproblem  
Author: Sandra A. Santos Danny C. Sorensen 
Address: P.O. BOX 1892, Houston, TX 77251-1892, USA  P.O. BOX 1892, Houston, TX 77251-1892, USA  
Affiliation: Department of Computational and Applied Mathematics, Rice University,  Dept. of Computational and Applied Mathematics, Rice University,  
Note: AMS classification: Primary 65F15; Secondary 65G05 Visiting member of the Center for Research on Parallel Computation and the  This author was supported by FAPESP (93/4907-5).  This author was supported in part by NSF cooperative agreement CCR-9120008, and by ARPA contract number DAAL03-91-C-0047 (administered by the U.S. Army Research Office).  
Date: June 30, 1995  
Abstract: The trust-region subproblem arises frequently in linear algebra and optimization applications. Recently, matrix-free methods have been introduced to solve large-scale trust-region subproblems. These methods only require a matrix-vector product and do not rely on matrix factorizations [4, 7]. These approaches recast the trust-region subproblem in terms of a parameterized eigenvalue problem and then adjust the parameter to find the optimal solution from the eigenvector corresponding to the smallest eigenvalue of the parameterized eigenvalue problem. This paper presents a new matrix-free algorithm for the large-scale trust-region subproblem. The new algorithm improves upon the previous algorithms by introducing a unified iteration that naturally includes the so called hard case. The new iteration is shown to be superlinearly convergent in all cases. Computational results are presented to illustrate convergence properties and robustness of the method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Lehoucq, </author> <title> D.C. Sorensen & P.A. Vu, ARPACK: An implementation of the Implicitly Restarted Arnoldi Iteration that computes some of the eigenvalues and eigenvectors of a large sparse matrix. </title> <note> Available from netlib@ornl.gov under the directory scalapack. </note>
Reference-contexts: We coded Algorithm 1 in MATLAB (version 4.1) and through an interface between MATLAB and FORTRAN we used the Implicitly Restarted Lanczos Method (IRLM) implemented in the package ARPACK <ref> [6, 1] </ref>. All the six sets of tests were run in a SUN SPARC station IPX. The floating point arithmetic is IEEE standard double precision with machine precision 2 52 2:2204 10 16 . The first four set of tests are quite similar to the experiments presented in [7].
Reference: [2] <author> J.J. Mor e & D.C. Sorensen, </author> <title> Computing a Trust Region Step, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 4, </volume> <pages> 553-572, </pages> <year> 1983. </year>
Reference-contexts: If positive-definite matrices of the form A + I can be decomposed into a Cholesky factorization, then the method proposed by More and Sorensen (cf. <ref> [2] </ref>) can be used to solve the problem. In several important applications, factoring or even forming these matrices is out of the question. This has motivated the recent development of conjugate-gradient style matrix-free methods that only require matrix-vector products. <p> Applying Newton's method to solve 1 = 0 has a number of computationally attractive features (cf. <ref> [2] </ref>) and this is the preferred approach when the Cholesky factorization of A I is tractable. When the cost or storage requirements of a Cholesky factorization become prohibitive, a new approach is required. <p> However, this is equivalent to the so-called hard case, analyzed in <ref> [2] </ref> for problems where the Cholesky factorization for A I is affordable and discussed in [4, 7] in the large-scale context. Figure 1.b shows in presence of the hard case, where one can see that ffi 1 is not a pole of . <p> s.t. kyk : In other words, a solution f fl ; y fl g satisfying (T fl I)y fl = V T g, fl (ky fl k) = 0, T fl I positive semidefinite, fl 0, ky fl k and fl ffi 1 is obtained applying the algorithm proposed in <ref> [2] </ref>, based on Cholesky factorization of the tridiagonal matrix T I, &lt; ffi 1 . Then, the initial value to be used is ff = fl g T V y fl . The hot start for ff may improve the convergence by reducing the number of iterations. <p> no restarts are used, the choice V e 1 = g=kgk provides a tridiagonal matrix on the right side of (16). 3.4 Stopping Criteria Before introducing the criteria for declaring convergence in the algorithm, we include for completion the following lemma, which is a restatement of a result given in <ref> [2] </ref>, was presented in [7] and provides a stopping criterion for the hard case. Lemma 5: Let " 2 (0; 1) be given and suppose (A I)p = g, 0 with A I positive semidefinite.
Reference: [3] <author> B.N. Parlett, </author> <title> The Symmetric Eigenvalue Problem, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs NY, </address> <year> 1980. </year>
Reference-contexts: The eigenvalues of B ff shall be denoted by f j (ff)g with 1 (ff) 2 (ff) n+1 (ff). 6 S.A. Santos and D.C. Sorensen As a consequence of Cauchy's interlace theorem (cf. <ref> [3] </ref>, p. 186), the eigenvalues of the matrix A interlace the eigenvalues of the bordered matrix B ff . This can also be seen directly from equation (5). Therefore, the smallest eigenvalue 1 (ff) of B ff satisfies 1 (ff) ffi 1 . <p> Thus, for a fixed tolerance " R 2 (0; 1), the stopping condition in ARPACK is (kf kje T j yj) 2 " R jjG (j), which has to hold for the j smallest Ritz pairs f; yg and where G (j) is the usual gap separation (cf. <ref> [3] </ref> pp.206, 222). For this set of experiments, j = 9 6 = 3.
Reference: [4] <author> F. Rendl & H. Wolkowicz, </author> <title> A Semidefinite Framework to Trust Region Sub-problems with Applications to Large Scale Minimization, </title> <type> CORR Report 94-32, </type> <institution> Department of Combinatorics and Optimization, University of Waterloo, </institution> <year> 1994. </year>
Reference-contexts: In several important applications, factoring or even forming these matrices is out of the question. This has motivated the recent development of conjugate-gradient style matrix-free methods that only require matrix-vector products. The recent works of Sorensen [7] and Rendl & Wolkowicz <ref> [4] </ref> provide such algorithms. Both approaches recast the trust-region subproblem in terms of a parameterized eigenvalue problem and rely upon matrix-vector products. <p> The purpose of this work is to present a new matrix-free algorithm for solving the large-scale trust-region subproblem. Our algorithm is similar to those proposed in <ref> [4, 7] </ref> in the sense that the trust-region subproblem is solved through a parametric eigenvalue problem. However, our algorithm is able to cope with the hard case naturally in the same basic iteration. <p> The iteration we propose is based upon a two-point interpolating scheme that is different from [7]. We show this new iteration is also superlin-early convergent. Moreover, our convergence results include the hard case naturally, since no special iterations are performed. Such a unified approach is not achieved in either <ref> [4] </ref> or [7]. This work is organized as follows. In x2 we analyze the structure of the problem and related results. There we give a complete characterization of the hard case with respect to the parameterized bordered eigenproblems. The detailed algorithm is presented in x3. 4 S.A. Santos and D.C. <p> However, this is equivalent to the so-called hard case, analyzed in [2] for problems where the Cholesky factorization for A I is affordable and discussed in <ref> [4, 7] </ref> in the large-scale context. Figure 1.b shows in presence of the hard case, where one can see that ffi 1 is not a pole of . <p> if q 2 S 1 then B 1 1 A = q T p = q T (A ffi 1 I) y g since (A ffi 1 I)q = 0. 2 The result in Lemma 3 was also stated in [7] and the idea behind Lemma 4 was presented in <ref> [4] </ref>. These results are the heart of the algorithm developed in the next section since they provide the necessary tools for handling the hard case in the same iteration designed for the general case. The next corollary summarizes the main results from Lemmas 3 and 4. <p> Large-Scale Trust-Region Subproblem 13 3.2 Safeguarding Safeguarding must be introduced to assure global convergence of the iteration. In <ref> [4] </ref> bounds are presented for the optimal parameter ff fl = fl g T x fl : ffi 1 However, computing a good approximation to ffi 1 can be as nearly as expensive as solving the given trust-region subproblem. <p> Finally, in the sixth set we provide a comparison between our algorithm and the approach proposed by Rendl & Wolkowicz <ref> [4] </ref>. 28 S.A. Santos and D.C. Sorensen 5.1 Different Tolerances In the first experiment the matrix A on problem (1) is A = L5I, where L is the standard 2-D discrete Laplacian on the unit square based upon a 5-point stencil with equally-spaced mesh points. <p> Moreover, in practice this seemed to occur often and greatly detracted from the performance. Our computational results show this new approach overcomes these difficulties while retaining the good performance of the original algorithm for the standard case. We also compared our approach to the one of Rendl & Wolkowicz <ref> [4] </ref>. In that comparison we used the EIG function from MATLAB to supply the eigenvalues so that both methods were getting the same level of accuracy in the eigenvalue calculation.
Reference: [5] <author> D.C. Sorensen, </author> <title> Newton's Method with a Model Trust Region Modification, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16, </volume> <pages> 409-426, </pages> <year> 1982. </year>
Reference-contexts: Proof: See <ref> [5] </ref>. 2 The optimality conditions of (1) are computationally attractive because they provide a means to reduce the given n-dimensional constrained optimization problem into a zero-finding problem in a single scalar variable. <p> If ffi 1 0 and kpk &lt; , then the solutions to (1) consist of the set fx j x = p + z ; z 2 S 1 ; kxk = g. Proof: See <ref> [5] </ref>. 2 When g is orthogonal to S 1 , there is a potential hard-case situation. This condition has an intriguing consequence. In this case it may be impossible to suitably normalize the eigenvector of interest.
Reference: [6] <author> D.C. Sorensen, </author> <title> Implicit Application of Polynomial Filters in a K-Step Arnoldi Method, </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 13, </volume> <pages> 357-385, </pages> <year> 1992. </year>
Reference-contexts: For the hard case, Sorensen's algorithm is linearly convergent. The recommended technique used in [7] to find the smallest eigenvalue and corresponding eigenvector of the parameterized problem is the Implicitly Restarted Lanczos Method (cf. <ref> [6] </ref>), which meets the requirements of limited storage and reliance only on matrix-vector products. Rendl & Wolkowicz present a primal-dual semidefinite framework for the trust-region subproblem, where a dual simplex-type method is used in the general iteration and a primal simplex-type method provides steps for the hard-case iteration. <p> fl satisfying the correct slope 0 ( fl ) = 2 is very ill-conditioned, i.e. a small change in data produces a large change in the solution. 3 The Algorithm Keeping in mind the availability of a well-suited variant of the Lanczos method, namely the Implicitly Restarted Lanczos Method (cf. <ref> [6] </ref>), we will develop a rapidly convergent iteration to adjust ff based on this process. <p> We coded Algorithm 1 in MATLAB (version 4.1) and through an interface between MATLAB and FORTRAN we used the Implicitly Restarted Lanczos Method (IRLM) implemented in the package ARPACK <ref> [6, 1] </ref>. All the six sets of tests were run in a SUN SPARC station IPX. The floating point arithmetic is IEEE standard double precision with machine precision 2 52 2:2204 10 16 . The first four set of tests are quite similar to the experiments presented in [7].
Reference: [7] <author> D.C. Sorensen, </author> <title> Minimization of a Large Scale Quadratic Function Subject to an Ellipsoidal Constraint, </title> <institution> TR94-27, Department of Computational & Applied Mathematics, Rice University, </institution> <year> 1994. </year>
Reference-contexts: In several important applications, factoring or even forming these matrices is out of the question. This has motivated the recent development of conjugate-gradient style matrix-free methods that only require matrix-vector products. The recent works of Sorensen <ref> [7] </ref> and Rendl & Wolkowicz [4] provide such algorithms. Both approaches recast the trust-region subproblem in terms of a parameterized eigenvalue problem and rely upon matrix-vector products. <p> For the hard case, Sorensen's algorithm is linearly convergent. The recommended technique used in <ref> [7] </ref> to find the smallest eigenvalue and corresponding eigenvector of the parameterized problem is the Implicitly Restarted Lanczos Method (cf. [6]), which meets the requirements of limited storage and reliance only on matrix-vector products. <p> The purpose of this work is to present a new matrix-free algorithm for solving the large-scale trust-region subproblem. Our algorithm is similar to those proposed in <ref> [4, 7] </ref> in the sense that the trust-region subproblem is solved through a parametric eigenvalue problem. However, our algorithm is able to cope with the hard case naturally in the same basic iteration. <p> This does not increase the work or storage required in any substantial way over the method proposed in <ref> [7] </ref>. The iteration we propose is based upon a two-point interpolating scheme that is different from [7]. We show this new iteration is also superlin-early convergent. Moreover, our convergence results include the hard case naturally, since no special iterations are performed. <p> This does not increase the work or storage required in any substantial way over the method proposed in <ref> [7] </ref>. The iteration we propose is based upon a two-point interpolating scheme that is different from [7]. We show this new iteration is also superlin-early convergent. Moreover, our convergence results include the hard case naturally, since no special iterations are performed. Such a unified approach is not achieved in either [4] or [7]. This work is organized as follows. <p> iteration we propose is based upon a two-point interpolating scheme that is different from <ref> [7] </ref>. We show this new iteration is also superlin-early convergent. Moreover, our convergence results include the hard case naturally, since no special iterations are performed. Such a unified approach is not achieved in either [4] or [7]. This work is organized as follows. In x2 we analyze the structure of the problem and related results. There we give a complete characterization of the hard case with respect to the parameterized bordered eigenproblems. The detailed algorithm is presented in x3. 4 S.A. Santos and D.C. <p> The problem we are interested in solving is min 1 s.t. kxk : Due to the structure of (1), its optimality conditions are both necessary and sufficient, as stated in the next lemma, where we follow <ref> [7] </ref> in the non-standard but more convenient use of a non-positive multiplier. <p> The conversion amounts to embedding the given problem into a parameterized bordered matrix eigenvalue problem. To begin, we observe that ff + (x) = 2 1 x T 0 @ g A C 0 @ x C where (x) 1 x T Ax + g T x. As in <ref> [7] </ref>, we denote the bordered matrix appearing on the right of (2) as B ff . <p> However, this is equivalent to the so-called hard case, analyzed in [2] for problems where the Cholesky factorization for A I is affordable and discussed in <ref> [4, 7] </ref> in the large-scale context. Figure 1.b shows in presence of the hard case, where one can see that ffi 1 is not a pole of . <p> Now, if q 2 S 1 then B 1 1 A = q T p = q T (A ffi 1 I) y g since (A ffi 1 I)q = 0. 2 The result in Lemma 3 was also stated in <ref> [7] </ref> and the idea behind Lemma 4 was presented in [4]. These results are the heart of the algorithm developed in the next section since they provide the necessary tools for handling the hard case in the same iteration designed for the general case. <p> Santos and D.C. Sorensen The approach of this work is similar to the one in <ref> [7] </ref> in the following sense. We compute a function b which interpolates and 0 at two properly chosen points. Then, from the interpolating function b we determine b satisfying b 0 ( b ) = 2 . <p> As in <ref> [7] </ref>, we use an interpolating function of the form b () = ffi Let f 0 ; x 0 g denote the point corresponding to the initial ff 0 so that ff 0 = g T x 0 with (A 0 I)x 0 = g : Requiring b ( 0 ) <p> Therefore, ffi is not strongly attached to ffi 1 , as in <ref> [7] </ref>, but can move to the right towards ffi `+1 (see (6)), depending on the occurrence or not of a potential exact or near hard case. <p> the choice V e 1 = g=kgk provides a tridiagonal matrix on the right side of (16). 3.4 Stopping Criteria Before introducing the criteria for declaring convergence in the algorithm, we include for completion the following lemma, which is a restatement of a result given in [2], was presented in <ref> [7] </ref> and provides a stopping criterion for the hard case. Lemma 5: Let " 2 (0; 1) be given and suppose (A I)p = g, 0 with A I positive semidefinite. <p> All the six sets of tests were run in a SUN SPARC station IPX. The floating point arithmetic is IEEE standard double precision with machine precision 2 52 2:2204 10 16 . The first four set of tests are quite similar to the experiments presented in <ref> [7] </ref>. To put the performance of our algorithm in a context, we include the number of matrix-vector products required by conjugate gradients to solve the systems (A I)x = g, for known . <p> We solved a sequence of twenty related problems, differing only by the vector g, randomly generated with entries uniformly distributed on (0; 1). Each of these problems was solved three times, with the tolerances " = 10 4 , 10 6 and 10 8 . As in <ref> [7] </ref>, we relaxed the accuracy requirement of the eigenvalue solution computed by the IRLM. The number of Lanczos basis vectors was limited to nine and six shifts (i.e. six matrix-vector products) were applied on each implicit restart. <p> Here the behavior of the results presented in <ref> [7] </ref> is reproduced: a trust region solution requires fewer than twice as many matrix-vector products on average than the number needed to solve a single linear system to the same accuracy, using conjugate gradients. 5.2 Different Sizes for the Trust-Region Radius In the second experiment the matrix A on problem (1) <p> The same comments made in <ref> [7] </ref> are in order: the conjugate-gradient method has a much easier time for smaller values of than for larger ones. 5.3 Superlinear Convergence In the third experiment the matrix A is again set to A = L 5I with L the 2-D discrete Laplacian on the unit square, but now n <p> The initial values of ff and ffi S were respectively minf0; ff U g and 4:5. We compared the performance of Algorithm 1 with the the algorithms proposed in <ref> [7] </ref>, using the same parameters specified above in the code. In Table 4.a we summarize the average results of a sequence of ten problems, generated with different seeds. We also generated problems with near hard case by adding a noise vector * to g of norm 10 2 . <p> We also generated problems with near hard case by adding a noise vector * to g of norm 10 2 . The comparative results are reported in Table 4.b. ` IT TRMV kg + (A fl I)x fl k (Alg.1, <ref> [7] </ref>) (Alg.1, [7]) (Alg.1, [7]) 5 (11.0, 7.2) (2940.2, 2221.8) (10 4 , 10 3 ) (a) ` IT TRMV kg + (A fl I)x fl k (Alg.1, [7]) (Alg.1, [7]) (Alg.1, [7]) 5 (12.3, 31.8) (3257.4, 9550.8) (10 4 , 10 4 ) (b) Table 4: Behavior for `-dimensional eigenspace <p> We also generated problems with near hard case by adding a noise vector * to g of norm 10 2 . The comparative results are reported in Table 4.b. ` IT TRMV kg + (A fl I)x fl k (Alg.1, <ref> [7] </ref>) (Alg.1, [7]) (Alg.1, [7]) 5 (11.0, 7.2) (2940.2, 2221.8) (10 4 , 10 3 ) (a) ` IT TRMV kg + (A fl I)x fl k (Alg.1, [7]) (Alg.1, [7]) (Alg.1, [7]) 5 (12.3, 31.8) (3257.4, 9550.8) (10 4 , 10 4 ) (b) Table 4: Behavior for `-dimensional eigenspace S 1 <p> We also generated problems with near hard case by adding a noise vector * to g of norm 10 2 . The comparative results are reported in Table 4.b. ` IT TRMV kg + (A fl I)x fl k (Alg.1, <ref> [7] </ref>) (Alg.1, [7]) (Alg.1, [7]) 5 (11.0, 7.2) (2940.2, 2221.8) (10 4 , 10 3 ) (a) ` IT TRMV kg + (A fl I)x fl k (Alg.1, [7]) (Alg.1, [7]) (Alg.1, [7]) 5 (12.3, 31.8) (3257.4, 9550.8) (10 4 , 10 4 ) (b) Table 4: Behavior for `-dimensional eigenspace S 1 : (a) <p> The comparative results are reported in Table 4.b. ` IT TRMV kg + (A fl I)x fl k (Alg.1, <ref> [7] </ref>) (Alg.1, [7]) (Alg.1, [7]) 5 (11.0, 7.2) (2940.2, 2221.8) (10 4 , 10 3 ) (a) ` IT TRMV kg + (A fl I)x fl k (Alg.1, [7]) (Alg.1, [7]) (Alg.1, [7]) 5 (12.3, 31.8) (3257.4, 9550.8) (10 4 , 10 4 ) (b) Table 4: Behavior for `-dimensional eigenspace S 1 : (a) hard case and (b) near hard case Large-Scale Trust-Region Subproblem 33 5.5 Initializing ff In the fifth experiment the matrix A on problem (1) <p> The comparative results are reported in Table 4.b. ` IT TRMV kg + (A fl I)x fl k (Alg.1, <ref> [7] </ref>) (Alg.1, [7]) (Alg.1, [7]) 5 (11.0, 7.2) (2940.2, 2221.8) (10 4 , 10 3 ) (a) ` IT TRMV kg + (A fl I)x fl k (Alg.1, [7]) (Alg.1, [7]) (Alg.1, [7]) 5 (12.3, 31.8) (3257.4, 9550.8) (10 4 , 10 4 ) (b) Table 4: Behavior for `-dimensional eigenspace S 1 : (a) hard case and (b) near hard case Large-Scale Trust-Region Subproblem 33 5.5 Initializing ff In the fifth experiment the matrix A on problem (1) is again <p> comparative results are reported in Table 4.b. ` IT TRMV kg + (A fl I)x fl k (Alg.1, <ref> [7] </ref>) (Alg.1, [7]) (Alg.1, [7]) 5 (11.0, 7.2) (2940.2, 2221.8) (10 4 , 10 3 ) (a) ` IT TRMV kg + (A fl I)x fl k (Alg.1, [7]) (Alg.1, [7]) (Alg.1, [7]) 5 (12.3, 31.8) (3257.4, 9550.8) (10 4 , 10 4 ) (b) Table 4: Behavior for `-dimensional eigenspace S 1 : (a) hard case and (b) near hard case Large-Scale Trust-Region Subproblem 33 5.5 Initializing ff In the fifth experiment the matrix A on problem (1) is again A = <p> Large-Scale Trust-Region Subproblem 35 6 Conclusions We have presented a new variant of an algorithm for the large-scale trust-region subprob-lem. The algorithm is based upon embedding the trust-region problem into a family of parameterized eigenvalue problems as developed in <ref> [7] </ref>. The main contribution of this paper has been to give a better understanding of the hard-case condition and to utilize this understanding to develop a better treatment of this case. The result has been a unified algorithm that naturally incorporates both the standard and hard-case problems. <p> The result has been a unified algorithm that naturally incorporates both the standard and hard-case problems. Superlinear convergence has been proved for this new algorithm and demonstrated computationally for both the standard and hard cases. This represents a major improvement over the performance of the method originally presented in <ref> [7] </ref>. In that approach, a different iteration was devised for the hard case that was not superlinearly convergent. Moreover, in practice this seemed to occur often and greatly detracted from the performance.
Reference: [8] <author> W.W. Symes, </author> <title> A Differential Semblance Criterion for Inversion of Multioffset Seismic Reflection Data, </title> <journal> Journal of Geophysical Research, </journal> <volume> 98, </volume> <pages> 2061-2073, </pages> <year> 1993. </year> <title> Large-Scale Trust-Region Subproblem 37 (dashdotted). The three smallest eigenvalues of A are 2, 0:5 and 2. (a) general case with the slope at fl also plotted; (b) exact hard case; (c) near hard case; (d) detail of the box indicated in (c). </title>
Reference-contexts: Finally, our approach seems to be better suited to obtaining accuracy in the final solution to (A I)x = g. Future work in this area should include a study of this approach for the regularization of ill-posed problems such as those arising in seismic inversion <ref> [8] </ref>. We feel that a further refinement of this approach is likely to be needed for this class of problems. In particular, near hard-case conditions seem to be associated with these problems. 36 S.A. Santos and D.C.
References-found: 8

