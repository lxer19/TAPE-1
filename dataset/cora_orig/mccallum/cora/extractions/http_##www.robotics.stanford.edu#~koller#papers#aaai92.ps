URL: http://www.robotics.stanford.edu/~koller/papers/aaai92.ps
Refering-URL: http://www.robotics.stanford.edu/~koller/papers/aaai92.html
Root-URL: http://www.robotics.stanford.edu
Email: fbacchus@logos.waterloo.edu  grove@cs.stanford.edu  halpern@almaden.ibm.com  daphne@cs.stanford.edu  
Title: From Statistics to Beliefs  
Author: Fahiem Bacchus Adam Grove Joseph Y. Halpern Daphne Koller 
Address: Waterloo, Ontario Canada, N2L 3G1  Stanford, CA 943005  650 Harry Road San Jose, CA 95120-6099  Stanford, CA 943005  
Affiliation: Computer Science Dept. University of Waterloo  Computer Science Dept. Stanford University  IBM Almaden Research Center  Computer Science Dept. Stanford University  
Abstract: An intelligent agent uses known facts, including statistical knowledge, to assign degrees of belief to assertions it is uncertain about. We investigate three principled techniques for doing this. All three are applications of the principle of indifference, because they assign equal degree of belief to all basic "situations" consistent with the knowledge base. They differ because there are competing intuitions about what the basic situations are. Various natural patterns of reasoning, such as the preference for the most specific statistical data available, turn out to follow from some or all of the techniques. This is an improvement over earlier theories, such as work on direct inference and reference classes, which arbitrarily postulate these patterns without offering any deeper explanations or guarantees of consistency. The three methods we investigate have surprising characterizations: there are connections to the principle of maximum entropy, a principle of maximal independence, and a "center of mass" principle. There are also unexpected connections between the three, that help us understand why the specific language chosen (for the knowledge base) is much more critical in inductive reasoning of the sort we consider than it is in traditional deductive reasoning. 
Abstract-found: 1
Intro-found: 1
Reference: [ Bac90 ] <author> F. Bacchus. </author> <title> Representing and Reasoning with Probabilistic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference: [ Car50 ] <author> R. Carnap. </author> <title> Logical Foundations of Probability. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1950. </year>
Reference-contexts: We remark that two of our three methods|the random-worlds method and the random-structures method|are not original to us. They essentially date back to Laplace [ Lap20 ] , and were investigated in some detail by Johnson [ Joh32 ] and Carnap <ref> [ Car50; Car52 ] </ref> . (These two methods correspond to Carnap's state-description and structure-description techniques, respectively.) We believe that the random-propensities method is new; as we shall show, it has some quite attractive properties.
Reference: [ Car52 ] <author> R. Carnap. </author> <title> The Continuum of Inductive Methods. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, </address> <year> 1952. </year>
Reference-contexts: So, according to random-worlds, it is much more likely that the number of individuals satisfying P is bN=2c than that 3 Note that we only consider the predicate denotations when deciding on a world's equivalence class, and ignore the denotations of constants. This is consistent with Carnap's approach <ref> [ Car52 ] </ref> , and is crucial for our results. See the full paper for further discussion of this point. it is 1, whereas for random-structures these two possi-bilities are equally likely. <p> We remark that two of our three methods|the random-worlds method and the random-structures method|are not original to us. They essentially date back to Laplace [ Lap20 ] , and were investigated in some detail by Johnson [ Joh32 ] and Carnap <ref> [ Car50; Car52 ] </ref> . (These two methods correspond to Carnap's state-description and structure-description techniques, respectively.) We believe that the random-propensities method is new; as we shall show, it has some quite attractive properties.
Reference: [ GHK92a ] <author> A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Asymptotic conditional probabilities for first-order logic. </title> <booktitle> In Proc. 24th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 294-305, </pages> <year> 1992. </year>
Reference-contexts: This change has no impact on the random-worlds and the random-propensities method; we still get the same answers as for the smaller vocabulary. In general, the degree of belief in ' given KB does not depend on the vocabulary for these two methods. As shown in <ref> [ GHK92a ] </ref> , this is not true in the case of the random-structures method. We return to this point in the next section. 6 The conditions required vary. <p> If the elements are distinguishable, random-worlds may be a more appropriate model. We remark that this issue of distinguishability is of crucial importance in statistical physics and quantum mechanics. However, there are situations where it is not as critical. In particular, we show in <ref> [ GHK92a ] </ref> and in the full paper that, as long as there are "enough" predicates in the vocabulary, the random-worlds method and the random-structures method are essentially equivalent. "Enough" here means "sufficient" to distinguish the elements in the domain; in a domain of size N , it turns out that
Reference: [ GHK92b ] <author> A. J. Grove, J. Y. Halpern, and D. Koller. </author> <title> Random worlds and maximum entropy. </title> <booktitle> In Proc. 7th IEEE Symp. on Logic in Computer Science, </booktitle> <pages> pages 22-33, </pages> <year> 1992. </year>
Reference-contexts: We can often find a single point in S (KB) that will characterize the degrees of belief generated by our dif 5 There is no guarantee that these limits exist; in complex cases, they may not. As our examples suggest, in typical cases they do (see <ref> [ GHK92b ] </ref> ). ferent methods. In the random-worlds method this is the maximum entropy point of S (KB) (see [ GHK92b; PV89 ] ). In the random-structures method, the characteristic point is the center of mass of S (KB). <p> As our examples suggest, in typical cases they do (see [ GHK92b ] ). ferent methods. In the random-worlds method this is the maximum entropy point of S (KB) (see <ref> [ GHK92b; PV89 ] </ref> ). In the random-structures method, the characteristic point is the center of mass of S (KB). Finally, in the random-propensities method, the characteristic point maximizes the statistical independence of the predicates in the vocabulary. <p> We formalize these latter two characterizations and describe the conditions under which they hold in the full paper. 6 When applicable, the characteristic point determines the degree of belief in ' given KB; we construct a particular probability structure (described also in <ref> [ GHK92b ] </ref> ) whose proportions are exactly those defined by the characteristic point. The probability of ' given KB is exactly the probability of ' given KB in this particular structure.
Reference: [ Goo55 ] <author> N. Goodman. </author> <title> Fact, fiction, and forecast, chapter III. </title> <publisher> Harvard University Press, </publisher> <year> 1955. </year>
Reference-contexts: The random-propensities method gives the language an even more central role. It assumes that there is information implicit in the choice of predicates. To illustrate this phenomenon, consider the well-known "grue/bleen" paradox <ref> [ Goo55 ] </ref> .
Reference: [ Goo92 ] <author> S. D. Goodwin. </author> <title> Second order direct inference: A reference class selection policy. </title> <journal> International Journal of Expert Systems: Research and Applications, </journal> <volume> 5(3) </volume> <pages> 185-210, </pages> <year> 1992. </year>
Reference: [ Hal90 ] <author> J. Y. Halpern. </author> <title> An analysis of first-order logics of probability. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 311-350, </pages> <year> 1990. </year>
Reference: [ Jay78 ] <author> E. T. Jaynes. </author> <title> Where do we stand on maximum entropy? In R. </title> <editor> D. Levine and M. Tribus, editors, </editor> <booktitle> The Maximum Entropy Formalism, </booktitle> <pages> pages 15-118. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1978. </year>
Reference: [ Joh32 ] <author> W. E. Johnson. </author> <title> Probability: The deductive and inductive problems. </title> <journal> Mind, </journal> <volume> 41(164) </volume> <pages> 409-423, </pages> <year> 1932. </year>
Reference-contexts: We remark that two of our three methods|the random-worlds method and the random-structures method|are not original to us. They essentially date back to Laplace [ Lap20 ] , and were investigated in some detail by Johnson <ref> [ Joh32 ] </ref> and Carnap [ Car50; Car52 ] . (These two methods correspond to Carnap's state-description and structure-description techniques, respectively.) We believe that the random-propensities method is new; as we shall show, it has some quite attractive properties.
Reference: [ Key21 ] <author> J. M. </author> <title> Keynes. A Treatise on Probability. </title> <publisher> Macmillan, </publisher> <address> London, </address> <year> 1921. </year>
Reference: [ Kri86 ] <author> J. von Kries. </author> <title> Die Principien der Wahrscheinlichkeitsrechnung und Rational Expectation. </title> <type> Freiburg, 1886. </type>
Reference: [ Kyb61 ] <author> H. E Kyburg, Jr. </author> <title> Probability and the Logic of Rational Belief. </title> <publisher> Wesleyan University Press, </publisher> <address> Middletown, Connecticut, </address> <year> 1961. </year>
Reference: [ Kyb74 ] <author> H. E. Kyburg, Jr. </author> <title> The Logical Foundations of Statistical Inference. </title> <publisher> Reidel, </publisher> <address> Dordrecht, Netherlands, </address> <year> 1974. </year>
Reference: [ Kyb83 ] <author> H. E. Kyburg, Jr. </author> <title> The reference class. </title> <journal> Philosophy of Science, </journal> <volume> 50(3) </volume> <pages> 374-397, </pages> <year> 1983. </year>
Reference: [ Lap20 ] <author> P. S. </author> <title> de Laplace. Essai Philosophique sur les Probabilites. 1820. English translation is Philosophical Essay on Probabilities, </title> <publisher> Dover Publications, </publisher> <address> New York, </address> <year> 1951. </year>
Reference-contexts: We remark that two of our three methods|the random-worlds method and the random-structures method|are not original to us. They essentially date back to Laplace <ref> [ Lap20 ] </ref> , and were investigated in some detail by Johnson [ Joh32 ] and Carnap [ Car50; Car52 ] . (These two methods correspond to Carnap's state-description and structure-description techniques, respectively.) We believe that the random-propensities method is new; as we shall show, it has some quite attractive properties.
Reference: [ Lev80 ] <author> I. Levi. </author> <title> The Enterprise of Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1980. </year>
Reference: [ Pol90 ] <author> J. L. Pollock. </author> <title> Nomic Probabilities and the Foundations of Induction. </title> <publisher> Oxford University Press, Oxford, </publisher> <address> U.K., </address> <year> 1990. </year>
Reference: [ PV89 ] <author> J. B. Paris and A. Vencovska. </author> <title> On the applicability of maximum entropy to inexact reasoning. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 3 </volume> <pages> 1-34, </pages> <year> 1989. </year>
Reference-contexts: As our examples suggest, in typical cases they do (see [ GHK92b ] ). ferent methods. In the random-worlds method this is the maximum entropy point of S (KB) (see <ref> [ GHK92b; PV89 ] </ref> ). In the random-structures method, the characteristic point is the center of mass of S (KB). Finally, in the random-propensities method, the characteristic point maximizes the statistical independence of the predicates in the vocabulary.
Reference: [ PV92 ] <author> J. B. Paris and A. Vencovska. </author> <title> A method for updating justifying minimum cross entropy. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 1-2:1-18, </volume> <year> 1992. </year>
Reference: [ Rei49 ] <author> H. Reichenbach. </author> <title> Theory of Probability. </title> <institution> University of California Press, Berkeley, </institution> <year> 1949. </year>
Reference: [ Sal71 ] <author> W. Salmon. </author> <title> Statistical Explanation and Statistical Relevance. </title> <institution> University of Pittsburgh Press, Pittsburgh, </institution> <year> 1971. </year>
References-found: 22

