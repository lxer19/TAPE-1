URL: http://www.cs.utexas.edu/users/skumar/reports/games-report.ps
Refering-URL: http://www.cs.utexas.edu/users/skumar/reports.html
Root-URL: 
Title: Generalized associative mixture of experts  
Author: Shailesh Kumar and Joydeep Ghosh 
Address: Austin, TX 78712  
Affiliation: Department of Electrical and Compueter Engineering University of Texas at Austin.  
Abstract: Modular learning, inspired by divide and conquer, learns a large number of localized simple concepts (classifiers or function approximators) as against single complex global concept. As a result, modular learning systems are efficient in learning and effective in generalization. In this work, a general model for modular learning systems is proposed whereby, specialization and localization is induced in the modules by associating them with training patterns. A case study of this model leading to a Generalized Associative Mixture of Experts (GAMES) is undertaken. For the supervised learning task of function approximation, GAMES has shown significant improvements over single layer and hierarchical mixture of experts, both in training efficiency and generalization performance, for various artificial training set.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Christopher M. Bishop, </author> <title> "Neural Networks and Pattern Recognition".,Clarendoin Press, </title> <year> 1995 </year>
Reference: [2] <author> Samuel Haykin, </author> <title> "Neural Networks, </title> <publisher> a comprehensive Foundation",Macmillan College Publishing Company, </publisher> <address> New York </address>
Reference: [3] <editor> Sharkey, </editor> <booktitle> On Combining Artificial Neural Networks, Connectionist Science, </booktitle> <volume> 8(3/4), </volume> <year> 1996. </year>
Reference: [4] <author> V. Ramamurti and J. Ghosh. </author> <title> Structural adaptation in mixture of experts. </title> <booktitle> In Proc. ICPR, </booktitle> <pages> pages II:704-708, </pages> <year> 1996. </year>
Reference: [5] <author> V. Ramamurti and J. Ghosh. </author> <title> An Alternative Method to Train the Mixture of Experts Network. </title> <type> unpublished 7 </type>
References-found: 5

