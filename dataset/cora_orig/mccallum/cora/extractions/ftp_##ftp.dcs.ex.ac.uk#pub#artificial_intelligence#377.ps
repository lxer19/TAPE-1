URL: ftp://ftp.dcs.ex.ac.uk/pub/artificial_intelligence/377.ps
Refering-URL: http://www.dcs.ex.ac.uk/reports/reports.html
Root-URL: http://www.dcs.ex.ac.uk
Title: A comparison between symbolic and nonsymbolic data mining techniques  
Author: Radu Greab and Ajit Narayanan 
Date: August 17, 1998  
Address: Exeter EX4 4PT United Kingdom  
Affiliation: Department of Computer Science University of Exeter  
Abstract: This paper presents the results of comparing symbolic and neural network data mining methods on three different data sets from the Machine Learning Database Repository at UC Irvine. The rule induction software CN2 was used for symbolic data mining, and simple feed-forward, back-propagation artificial neural networks available with the Stuttgart Neural Network Simulator (SNNS) package were used for nonsymbolic data mining. We have found in our tests that, although it is often claimed that neural networks generally outperform symbolic learning systems, the performance of the symbolic system used, CN2, compared favourably with the performance of simple feed-forward, back-propagation neural networks. Both in noise-free data sets and in domains with noisy input data, the accuracy of CN2 was very close to the accuracy of the neural networks. The main differences began to emerge only when the data became highly noisy, and even then the nonsymbolic data mining approach outperformed the symbolic data mining approach only in parts. It must be stressed that our results are based on a very limited comparison of symbolic and nonsymbolic data mining tools. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C.J. Merz and P.M. </author> <note> Murphy (1998). UCI Repository of machine learning databases http://www.ics.uci.edu/ mlearn/MLRepository.html. </note> <institution> Irvine, CA: University of Cali-fornia, Department of Information and Computer Science. </institution>
Reference-contexts: 1 Introduction This paper presents the results of comparing symbolic and connectionist data mining methods on three different data sets from the Machine Learning Database Repository at UC Irvine <ref> [1] </ref>. We used the rule induction software CN2 [2] and simple feed-forward, back-propagation artificial neural networks deployed with the Stuttgart Neural Network Simulator (SNNS) package [3]. The goal of this work was to analyse the performances of symbolic systems and artificial neural networks used in data mining.
Reference: [2] <author> P. Clark and T. Niblett. </author> <title> The CN2 Induction Algorithm. </title> <journal> In Machine Learning Journal, </journal> <volume> 3(4), </volume> <pages> pages 261|283, </pages> <address> Netherlands, </address> <publisher> Kluwer, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction This paper presents the results of comparing symbolic and connectionist data mining methods on three different data sets from the Machine Learning Database Repository at UC Irvine [1]. We used the rule induction software CN2 <ref> [2] </ref> and simple feed-forward, back-propagation artificial neural networks deployed with the Stuttgart Neural Network Simulator (SNNS) package [3]. The goal of this work was to analyse the performances of symbolic systems and artificial neural networks used in data mining. <p> Different tools implements different strategies to overcome these problems. 2.1 CN2 rule induction algorithm The CN2 system, by Clark and Niblett (see <ref> [2] </ref>), is a symbolic data mining tool designed to efficiently induce simple and comprehensible rules in domains where noisy data may be present. The input of CN2 consists usually of a file describing the attributes and their types and a file containing the examples.
Reference: [3] <institution> University of Stuttgart, Institute of Parallel and Distributed High-Performance Systems. </institution> <note> Stuttgart Neural Network Simulator version 4.1, http://www.informatik.uni-stuttgart.de/ipvr/bv/projekte/snns/snns.html. </note>
Reference-contexts: We used the rule induction software CN2 [2] and simple feed-forward, back-propagation artificial neural networks deployed with the Stuttgart Neural Network Simulator (SNNS) package <ref> [3] </ref>. The goal of this work was to analyse the performances of symbolic systems and artificial neural networks used in data mining. We wanted to find out what were the strengths and the weaknesses of symbolic and nonsymbolic data mining methods when tackling the same datasets. <p> Knowledge is distributed throughout the network and is stored in the structure of the topology and the weights of the links. The SNNS tool offers a large range of activation functions, output functions, topologies, learning algorithms (see <ref> [3] </ref>). We used in our tests feed-forward networks, with one or zero hidden layers, and we trained them using the back propagation algorithm for supervised learning. A simple feed-forward neural network is presented in the Figure 3.
Reference: [4] <author> M. Holsheimer and A. Siebes. </author> <title> Data Mining: The Search for Knowledge in Databases. </title> <type> Report CS-R9406, ISSN 0169-118X, </type> <institution> CWI, </institution> <address> Amsterdam, Netherlands. </address> <note> Online at http://www.cwi.nl/. </note>
Reference-contexts: data as well as produce intelligible output. 1 2 Symbolic and connectionist data mining systems Data mining, also called the search for information and/or knowledge in databases, is the process of extracting relationships and global patterns that exist in large databases but are `hidden' among the vast amount of data <ref> [4] </ref>. Data mining systems have a large set of practical applications. They are used for mining in medical databases, commercial companies' databases and scientific databases. <p> Artificial neural networks are inspired by the neural networks of the brain. In Figure 3, adapted from <ref> [4] </ref>, a neuron and a feed-forward neural network are shown. To the input of a neuron, or a unit of the network, is presented an activation vector, (x 0 ; x 1 ; ; x n 1 ).
Reference: [5] <author> P. Clark and R. Boswell. </author> <title> Rule induction with CN2: some recent improvements. </title> <booktitle> In Machine Learning Proceedings of the Fifth European Conference (EWSL-91), </booktitle> <pages> pages 151-163, </pages> <editor> Ed. Yves Kodratoff, </editor> <publisher> Berlin, Springer-Verlag, </publisher> <year> 1991. </year> <note> Also available online at http://www.cs.utexas.edu/users/pclark/papers/newcn.ps. </note>
Reference-contexts: The first version of CN2 used an information theoretic entropy measure to compute the accuracy of a complex. A new version, by Clark and Boswell, uses the Laplacian error estimate, as the heuristic of the search, significantly improving the algorithm's performance (see <ref> [5] </ref>). CN2 consists of two main procedures: a search procedure and a control procedure.
Reference: [6] <author> P. H. Winston. </author> <booktitle> Artificial Intelligence (3rd Edition). </booktitle> <publisher> Addison Wesley, </publisher> <year> 1992. </year> <month> 14 </month>
Reference-contexts: This statistic provides a measure of indicating significance | the lower the score, the more likely that the regularity have occurred by chance. 2.1.1 Exemplifying the CN2 algorithm We will use a simple data set to exemplify the CN2 algorithm. This data set is used in <ref> [6] </ref> to present Identification Trees (ID3), another symbolic approach to data mining.
References-found: 6

