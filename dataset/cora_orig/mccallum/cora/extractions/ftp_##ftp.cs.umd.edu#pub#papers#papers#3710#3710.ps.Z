URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3710/3710.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/authors/Anurag_Acharya-no-abs.html
Root-URL: 
Email: facha,edjlali,saltzg@cs.umd.edu  
Title: The Utility of Exploiting Idle Workstations for Parallel Computation  
Author: Anurag Acharya, Guy Edjlali, Joel Saltz 
Keyword: CFD application.  
Address: College Park 20742  
Affiliation: UMIACS and Department of Computer Science University of Maryland,  
Abstract: In this paper, we examine the utility of exploiting idle workstations for parallel computation. We attempt to answer the following questions. First, given a workstation pool, for what fraction of time can we expect to find a cluster of k workstations available? This provides an estimate of the opportunity for parallel computation. Second, how stable is a cluster of free machines and how does the stability vary with the size of the cluster? This indicates how frequently a parallel computation might have to stop for adapting to changes in processor availability. Third, what is the distribution of workstation idle-times? This information is useful for selecting workstations to place computation on. Fourth, how much benefit can a user expect? To state this in concrete terms, if I have a pool of size S, how big a parallel machine should I expect to get for free by harvesting idle machines. Finally, how much benefit can be achieved on a real machine and how hard does a parallel programmer have to work to make this happen? To answer the workstation-availability questions, we have analyzed 14-day traces from three workstation pools. To determine the equivalent parallel machine, we have simulated the execution of a group of well-known parallel programs on these workstation pools. To gain an understanding of the practical problems, we have developed the system support required for adaptive parallel programs as well as an adaptive parallel 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R.C. Agarwal, F.G. Gustavson, and M. Zubair. </author> <title> An efficient algorithm for the 3-D FFT NAS parallel benchmark. </title> <booktitle> In Proceedings of SHPCC'94 (Scalable High-Performance Computing Conference), </booktitle> <pages> pages 129-33, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Finally, we present the results. 3.1 Benchmarks All programs in this suite are programmed in the SPMD model. Figure 5 shows the speedups for the benchmarks running on dedicated parallel machines. These numbers have been obtained from publications <ref> [1, 3, 22, 23, 26] </ref>. The programs themselves are described below. We used class B datasets for all the NAS benchmarks. * nas-bt: this program uses an approach based on block-tridiagonal matrices to solve the Navier-Stokes equations [22]. <p> The running time on one processor of the IBM SP-2 is 228 seconds and the total memory requirement is 461 MB. This program runs on configurations with powers-of-two processors. * nas-fftpde: this program solves a Poisson partial differential equation using the 3-D FFT algorithm <ref> [1] </ref>. The running time on sixteen processors of the IBM SP-1 is 286 seconds and the total memory requirement is 1.75 GB. This program runs on configurations with powers-of-two processors. * dsmc3d: is a Monte-Carlo simulation used to study the flow of gas molecules in three dimensions [23].
Reference: [2] <author> R.H. Arpaci, A.D. Dusseau, A.M. Vahdat, L.T. Liu, T.E. Anderson, and D.A. Patterson. </author> <title> The interaction of parallel and sequential workloads on a network of workstations. </title> <booktitle> In Proceedings of the 1995 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 267-78, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: There are two problems with this approach. First, most parallel programs are not written in a master-slave style. Second, rewriting existing parallel programs as master-slave programs would greatly increase the total communication volume and would require very large amounts of memory on the master processor. Arpaci et al <ref> [2] </ref> study the suitability of dedicated and non-dedicated workstation pools for executing parallel programs. They take a trace-based-analysis approach and base their study on a workstation availability trace, a job arrival trace for a 32-node CM-5 partition and a suite of five data-parallel programs. <p> This trace covers a 46-day period between 02/15/94 and 03/31/96. The trace we received had the busy and availability periods marked in for each workstation. This trace was used by Arpaci et al in <ref> [2] </ref>. We extracted the 14-day segment which had the largest number of traced workstations. We refer to this trace as the ucb trace. The second trace is from the Condor workstation pool at the University of Wisconsin and contains data for about 300 workstations. <p> In each graph, the horizontal line labeled avg shows the average fraction of the pool that is available. These results indicate that, on the average, 60% to 80% of the workstations in a pool are available. This agrees with previous results <ref> [2, 9, 17] </ref>. 2.3 Distribution of workstation idle-time In this section, we try to answer the question what is the probability that an idle workstation will be idle for longer than time t? This question has been previously looked at by several researchers [2, 9]. <p> This agrees with previous results [2, 9, 17]. 2.3 Distribution of workstation idle-time In this section, we try to answer the question what is the probability that an idle workstation will be idle for longer than time t? This question has been previously looked at by several researchers <ref> [2, 9] </ref>. The common experience has been that machines that have been idle for a short time are more likely to be reclaimed than machines that have been idle for a relative long period. <p> Douglis&Ousterhout [9] mention that for their cluster, machines that were idle for 30 seconds were likely to be idle for an average of 26 minutes; Arpaci et al <ref> [2] </ref> mention that, in their study, a recruitment threshold of 3 minutes provided the best throughput. Given the relative plenty in terms of workstation availability, we did not focus on the issue of recruitment. Instead, we looked at distribution of relatively long idle periods (tens of minutes to several hours). <p> We used a settling period of two seconds. Since idle workstations are relatively plentiful, our goal was to use as simple a scheduling strategy as possible. In their study, Arpaci et al <ref> [2] </ref> focus on the interactive performance of parallel jobs and assume a time-sliced scheduling policy. They deduce the need for interactive response from the presence of a large number of short-lived parallel jobs in the CM-5 job arrival trace. Most parallel machines, however, run in a batch mode. <p> From these results, we conclude that harvesting idle workstations from these pools can provide the equivalent of 29 (College Park), 25 (Berkeley) and 92/109 (Madison) dedicated processors. The measures for the Berkeley pool match the 1:2 rule of thumb that Arpaci et al <ref> [2] </ref> suggest for the parallel machine equivalent to a non-dedicated workstation pool. However, the rule does not match the results for the other two clusters.
Reference: [3] <author> D. Bannerjee, T. Tysinger, and W. Smith. </author> <title> A scalable high-performance environment for fluid-flow analysis on unstructured grids. </title> <booktitle> In Proceedings of Supercomputing'94, </booktitle> <pages> pages 8-17, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Finally, we present the results. 3.1 Benchmarks All programs in this suite are programmed in the SPMD model. Figure 5 shows the speedups for the benchmarks running on dedicated parallel machines. These numbers have been obtained from publications <ref> [1, 3, 22, 23, 26] </ref>. The programs themselves are described below. We used class B datasets for all the NAS benchmarks. * nas-bt: this program uses an approach based on block-tridiagonal matrices to solve the Navier-Stokes equations [22]. <p> The running time on one processor of the iPSC/860 is 4876 seconds and the total memory requirement is 30 MB. * unstructured: this is a flow solver capable of solving the Navier-Stokes equations about complex geometries through the use of unstructured grids <ref> [3] </ref>. Its running time on one processor of the Intel Paragon is 68814 seconds and the total memory required is 134 MB. We have data for this program running on 1,2,3,4,5,10,25 and 50 processors. * hydro3d: this is a parallel implementation of 3 + 1-dimensional relativistic hydrodynamics [26].
Reference: [4] <author> N. Carriero, D. Gelernter, M. Jourdenais, and D. Kaminsky. </author> <title> Piranha scheduling: strategies and their implementation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 23(1) </volume> <pages> 5-33, </pages> <month> Feb </month> <year> 1995. </year>
Reference-contexts: Their study is based on simple synthetic models of both workstation availability and parallel program behavior. It is difficult to draw conclusions about behavior of real parallel programs on real workstation pools from their work. Carreiro et al <ref> [4] </ref> and Pruyne&Livny [21] propose schemes based on a master-slave approach. If the workstation on which a task is being executed is reclaimed, the task is killed and is reassigned by the master to a different workstation. There are two problems with this approach.
Reference: [5] <institution> The Condor status monitor. </institution> <address> http://www.cs.wisc.edu/cgi-bin/condor status/-server, </address> <year> 1996. </year>
Reference-contexts: Condor uses several criteria, including user preferences, to decide if a workstation is available for batch jobs. We collected this trace by sampling the Condor status information once every three minutes using the web interface provided by the Condor project <ref> [5] </ref>. We refer to this as the wisc trace. The third trace is from the public workstation cluster of the Department of Computer Science, University of Maryland. This trace contains data for about 40 workstations and covers a 14-day period from 09/24/96 to 10/07/96.
Reference: [6] <institution> The Condor summary status monitor. </institution> <address> http://www.cs.wisc.edu/cgi-bin/condor status/-server+-tot, </address> <year> 1996. </year>
Reference-contexts: Batch-processing systems that utilize idle workstations for running sequential jobs have been in production use for many years. A well-known example is Condor [15], which has been has been in operation at the University of Wisconsin for about 8 years and which currently manages about 300 workstations <ref> [6] </ref>. The utility of harvesting idle workstations for parallel computation is less clear.
Reference: [7] <author> M. Dahlin, R. Wang, T. Anderson, and D. Patterson. </author> <title> Cooperative caching: using remote memory to improve file system performance. </title> <booktitle> In Proceedings of the first Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 267-80, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: With the current growth in the number and the size of data-intensive tasks, exploiting idle workstations for their memory could be an attractive option. Dahlin et al <ref> [7] </ref> study the feasibility of using idle memory to increase the effective file cache size. Feely et al [11] describe a low-level global memory management system that uses idle memory to back up not just file pages but all of virtual memory as well.
Reference: [8] <author> F. Douglis. </author> <title> Transparent Process Migration in the Sprite Operating System. </title> <type> PhD thesis, </type> <institution> Computer Science Division, Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, </institution> <month> Sep </month> <year> 1990. </year>
Reference-contexts: Given the large value of the idleness-cutoff, simple strategies (such as LIFO, FIFO, random etc) for selecting between available workstations should suffice. We note that all of these values are significantly higher than the 26 minutes reported by Douglis <ref> [8] </ref> in 1990 for the Sprite workstations. 3 How much benefit can a user expect? To estimate the benefit that parallel programs might achieve in shared workstation environments, we simulated the execution of a group of well-known parallel programs on all three pools. <p> restart it elsewhere (as in Condor [24]) or it could stop the process and copy it over from memory to memory (as in Locus [20]), or it could copy the stack and a small number of pages over and fault the rest in lazily (as in Accent [28] and Sprite <ref> [8] </ref>). All of these schemes involve moving an executing process. Since SPMD applications run multiple copies of the same program which are usually in (loose) synchrony, there is another, possibly cheaper, alternative. Just the program data for the process can be moved; scratch data and text need not be moved.
Reference: [9] <author> Fred Douglis and John Ousterhout. </author> <title> Transparent process migration: Design alternatives and the Sprite implementation. </title> <journal> Software Practice and Experience, </journal> <volume> 21(8) </volume> <pages> 757-85, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Exploiting idle workstations has been a popular research area. This popularity has been fueled partly by studies which have indicated that a large fraction of workstations are unused for a large fraction of time <ref> [9, 17, 19, 25] </ref> and partly by the rapid growth in the power of workstations. Batch-processing systems that utilize idle workstations for running sequential jobs have been in production use for many years. <p> A well-known example is Condor [15], which has been has been in operation at the University of Wisconsin for about 8 years and which currently manages about 300 workstations [6]. The utility of harvesting idle workstations for parallel computation is less clear. First, the workstation-availability results <ref> [9, 17, 19, 25] </ref> that have held out the promise of free cycles assume, at least implicitly, that progress of execution on one workstation, or the lack thereof, has no effect on the progress of execution on other workstations. This assumption does not hold for most parallel computation. <p> In each graph, the horizontal line labeled avg shows the average fraction of the pool that is available. These results indicate that, on the average, 60% to 80% of the workstations in a pool are available. This agrees with previous results <ref> [2, 9, 17] </ref>. 2.3 Distribution of workstation idle-time In this section, we try to answer the question what is the probability that an idle workstation will be idle for longer than time t? This question has been previously looked at by several researchers [2, 9]. <p> This agrees with previous results [2, 9, 17]. 2.3 Distribution of workstation idle-time In this section, we try to answer the question what is the probability that an idle workstation will be idle for longer than time t? This question has been previously looked at by several researchers <ref> [2, 9] </ref>. The common experience has been that machines that have been idle for a short time are more likely to be reclaimed than machines that have been idle for a relative long period. <p> The common experience has been that machines that have been idle for a short time are more likely to be reclaimed than machines that have been idle for a relative long period. Douglis&Ousterhout <ref> [9] </ref> mention that for their cluster, machines that were idle for 30 seconds were likely to be idle for an average of 26 minutes; Arpaci et al [2] mention that, in their study, a recruitment threshold of 3 minutes provided the best throughput.
Reference: [10] <author> G. Edjlali, G. Agrawal, A. Sussman, and J. Saltz. </author> <title> Data parallel programming in an adaptive environment. </title> <booktitle> In Proceedings of the ninth International Parallel Processing Symposium, </booktitle> <pages> pages 827-32, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: If a reclamation request is received when the program is at any other point, eviction is delayed till all processes reach this point. As described later in this section, the additional delay introduced, at least for this program, is quite small. We used the Adaptive Multiblock PARTI library <ref> [10] </ref> from the University of Maryland for parallelizing the application. This library performs the data partitioning for normal execution as well as the repartitioning for adaptation. It also manages the normal data communication as well as the data motion needed for eviction.
Reference: [11] <author> M. Feely, W. Morgan, F. Pighin, A. Karlin, H. Levy, and C. Thekkath. </author> <title> Implementing global memory management in a workstation cluster. </title> <booktitle> In Proceedings of the fifteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 201-12, </pages> <month> Dec </month> <year> 1995. </year>
Reference-contexts: With the current growth in the number and the size of data-intensive tasks, exploiting idle workstations for their memory could be an attractive option. Dahlin et al [7] study the feasibility of using idle memory to increase the effective file cache size. Feely et al <ref> [11] </ref> describe a low-level global memory management system that uses idle memory to back up not just file pages but all of virtual memory as well.
Reference: [12] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Global memory management in client-server DBMS architectures. </title> <booktitle> In Proceedings of the eighteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 596-609, </pages> <month> Aug </month> <year> 1992. </year> <month> 14 </month>
Reference-contexts: They show that this scheme is able to use idle memory to improve the performance of a suite of sequential data-intensive tasks by a factor between 1.5 and 3.5. Franklin et al <ref> [12] </ref> describe a unified memory management scheme for the servers and all the clients in a client-server database system. Their goal was to avoid replication of pages between the buffer pools of all the clients as well as the buffer pools of the servers.
Reference: [13] <author> L. Iftode, K. Li, and K. Petersen. </author> <title> Memory servers for multicomputers. </title> <booktitle> In COMPCON Spring'93 Digest of Papers, </booktitle> <pages> pages 538-47, </pages> <month> Feb </month> <year> 1993. </year>
Reference-contexts: f P r o c e s s o r s U s e d Number of Parallel Programs Parallel Program 4 Parallel Program 3 Parallel Program 2 Parallel Program 1 dataset and the graph on the right is for the second dataset. by Narten&Yavagkar [18] and Iftode et al <ref> [13] </ref>. Narten&Yavagkar describe a memory server similar in spirit to the Condor central manager. It keeps track of the idle memory available and ships memory objects to the corresponding machines as needed. Iftode et al propose extending the memory hierarchy of multicomputers by introducing a remote memory server layer.
Reference: [14] <author> S. Leutenegger and X.-H. Sun. </author> <title> Distributed computing feasibility in a non-dedicated homogeneous distributed system. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <pages> pages 143-52, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Previous research into using idle workstations for parallel computation has taken one of three approaches. Leutenegger and Sun <ref> [14] </ref> use an analytic-model-based approach to study the feasibility of running parallel applications on non-dedicated workstation pool. Their study is based on simple synthetic models of both workstation availability and parallel program behavior.
Reference: [15] <author> M. Litzkow and M. Livny. </author> <title> Experiences with the Condor distributed batch system. </title> <booktitle> In Proceedings of the IEEE Workshop on Experimental Distributed Systems, </booktitle> <pages> pages 97-101, </pages> <month> Oct </month> <year> 1990. </year>
Reference-contexts: Batch-processing systems that utilize idle workstations for running sequential jobs have been in production use for many years. A well-known example is Condor <ref> [15] </ref>, which has been has been in operation at the University of Wisconsin for about 8 years and which currently manages about 300 workstations [6]. The utility of harvesting idle workstations for parallel computation is less clear.
Reference: [16] <author> J. Moreira, V. Naik, and R. Konuru. </author> <title> A programming environment for dynamic resource allocation and data distribution. </title> <type> Technical Report RC 20239, </type> <institution> IBM Research, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: To achieve efficient communication, this library pre-computes communication schedules. Changing the number or the identity of its processors requires recomputation of the schedule. Adaptive Multiblock PARTI is not unique in providing these services. The DRMS system <ref> [16] </ref> from IBM Research provides similar functionality. The point we would like to make is that this support does not have to be implemented by a parallel programmer. We needed to make four changes to the program to allow it to run in an adaptive fashion.
Reference: [17] <author> Matt Mutka and Miron Livny. </author> <title> The available capacity of a privately owned workstation environment. Performance Evaluation, </title> <booktitle> 12(4) </booktitle> <pages> 269-84, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Exploiting idle workstations has been a popular research area. This popularity has been fueled partly by studies which have indicated that a large fraction of workstations are unused for a large fraction of time <ref> [9, 17, 19, 25] </ref> and partly by the rapid growth in the power of workstations. Batch-processing systems that utilize idle workstations for running sequential jobs have been in production use for many years. <p> A well-known example is Condor [15], which has been has been in operation at the University of Wisconsin for about 8 years and which currently manages about 300 workstations [6]. The utility of harvesting idle workstations for parallel computation is less clear. First, the workstation-availability results <ref> [9, 17, 19, 25] </ref> that have held out the promise of free cycles assume, at least implicitly, that progress of execution on one workstation, or the lack thereof, has no effect on the progress of execution on other workstations. This assumption does not hold for most parallel computation. <p> In each graph, the horizontal line labeled avg shows the average fraction of the pool that is available. These results indicate that, on the average, 60% to 80% of the workstations in a pool are available. This agrees with previous results <ref> [2, 9, 17] </ref>. 2.3 Distribution of workstation idle-time In this section, we try to answer the question what is the probability that an idle workstation will be idle for longer than time t? This question has been previously looked at by several researchers [2, 9].
Reference: [18] <author> T. Narten and R. Yavagkar. </author> <title> Remote memory as a resource in distributed systems. </title> <booktitle> In Proceedings of the third Workshop on Workstation Operating Systems, </booktitle> <pages> pages 132-6, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: m b e r o f P r o c e s s o r s U s e d Number of Parallel Programs Parallel Program 4 Parallel Program 3 Parallel Program 2 Parallel Program 1 dataset and the graph on the right is for the second dataset. by Narten&Yavagkar <ref> [18] </ref> and Iftode et al [13]. Narten&Yavagkar describe a memory server similar in spirit to the Condor central manager. It keeps track of the idle memory available and ships memory objects to the corresponding machines as needed.
Reference: [19] <author> David Nichols. </author> <title> Using idle workstations in a shared computing environment. </title> <booktitle> In Proceedings of the Eleventh ACM Symposium on Operating Systems, </booktitle> <pages> pages 5-12, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Exploiting idle workstations has been a popular research area. This popularity has been fueled partly by studies which have indicated that a large fraction of workstations are unused for a large fraction of time <ref> [9, 17, 19, 25] </ref> and partly by the rapid growth in the power of workstations. Batch-processing systems that utilize idle workstations for running sequential jobs have been in production use for many years. <p> A well-known example is Condor [15], which has been has been in operation at the University of Wisconsin for about 8 years and which currently manages about 300 workstations [6]. The utility of harvesting idle workstations for parallel computation is less clear. First, the workstation-availability results <ref> [9, 17, 19, 25] </ref> that have held out the promise of free cycles assume, at least implicitly, that progress of execution on one workstation, or the lack thereof, has no effect on the progress of execution on other workstations. This assumption does not hold for most parallel computation.
Reference: [20] <author> G. Popek and B. Walker. </author> <title> The LOCUS Distributed System Architecture. </title> <publisher> The MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: For example, it could checkpoint the evicted process to disk and restart it elsewhere (as in Condor [24]) or it could stop the process and copy it over from memory to memory (as in Locus <ref> [20] </ref>), or it could copy the stack and a small number of pages over and fault the rest in lazily (as in Accent [28] and Sprite [8]). All of these schemes involve moving an executing process.
Reference: [21] <author> J. Pruyne and M. Livny. </author> <title> Parallel processing on dynamic resources with CARMI. </title> <booktitle> In Proceedings of the Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pages 259-78, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Their study is based on simple synthetic models of both workstation availability and parallel program behavior. It is difficult to draw conclusions about behavior of real parallel programs on real workstation pools from their work. Carreiro et al [4] and Pruyne&Livny <ref> [21] </ref> propose schemes based on a master-slave approach. If the workstation on which a task is being executed is reclaimed, the task is killed and is reassigned by the master to a different workstation. There are two problems with this approach.
Reference: [22] <author> W. Saphir, A. Woo, and M. Yarrow. </author> <title> The NAS Parallel Benchmarks 2.1 Results. </title> <type> Technical Report NAS-96-010, </type> <institution> NASA Ames Research Center, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: We selected a suite of eight programs which includes the NAS parallel benchmarks <ref> [22] </ref> and three programs that have been studied by one or more research groups working on parallel processing. We simulated two scenarios: (1) repeated execution of individual applications without gaps; (2) repeated execution of the entire set of applications, also without gaps. <p> Finally, we present the results. 3.1 Benchmarks All programs in this suite are programmed in the SPMD model. Figure 5 shows the speedups for the benchmarks running on dedicated parallel machines. These numbers have been obtained from publications <ref> [1, 3, 22, 23, 26] </ref>. The programs themselves are described below. We used class B datasets for all the NAS benchmarks. * nas-bt: this program uses an approach based on block-tridiagonal matrices to solve the Navier-Stokes equations [22]. <p> These numbers have been obtained from publications [1, 3, 22, 23, 26]. The programs themselves are described below. We used class B datasets for all the NAS benchmarks. * nas-bt: this program uses an approach based on block-tridiagonal matrices to solve the Navier-Stokes equations <ref> [22] </ref>. The running time on one processor of the IBM SP-2 is 10942 seconds and the total memory requirement is 1.3 GB. This program runs on configurations with square number of processors. * nas-sp: this program uses a pentadiagonal matrix-based algorithm for the Navier-Stokes equations [22]. <p> to solve the Navier-Stokes equations <ref> [22] </ref>. The running time on one processor of the IBM SP-2 is 10942 seconds and the total memory requirement is 1.3 GB. This program runs on configurations with square number of processors. * nas-sp: this program uses a pentadiagonal matrix-based algorithm for the Navier-Stokes equations [22]. The running time on one processor of the IBM SP-2 is 7955 seconds and the total memory requirement is 325.8 MB. This program runs on configurations with square number of processors. * nas-lu: this program uses a block-lower-triangular block-upper-triangular approximate factorization to solve the Navier-Stokes equations. <p> The running time on one processor of the IBM SP-2 is 8312 seconds and the total memory requirement is 174.8 MB. This program runs on configurations with powers-of two processors. * nas-mg: this implements a multigrid algorithm to solve the scalar discrete Poisson equation <ref> [22] </ref>. The running time on one processor of the IBM SP-2 is 228 seconds and the total memory requirement is 461 MB. This program runs on configurations with powers-of-two processors. * nas-fftpde: this program solves a Poisson partial differential equation using the 3-D FFT algorithm [1].
Reference: [23] <author> S. Sharma, R. Ponnuswami, B. Moon, Y-S Hwang, R. Das, and J. Saltz. </author> <title> Runtime and compile-time support for adaptive irregular problems. </title> <booktitle> In Proceedings of Supercomputing'94, </booktitle> <pages> pages 97-108, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Finally, we present the results. 3.1 Benchmarks All programs in this suite are programmed in the SPMD model. Figure 5 shows the speedups for the benchmarks running on dedicated parallel machines. These numbers have been obtained from publications <ref> [1, 3, 22, 23, 26] </ref>. The programs themselves are described below. We used class B datasets for all the NAS benchmarks. * nas-bt: this program uses an approach based on block-tridiagonal matrices to solve the Navier-Stokes equations [22]. <p> The running time on sixteen processors of the IBM SP-1 is 286 seconds and the total memory requirement is 1.75 GB. This program runs on configurations with powers-of-two processors. * dsmc3d: is a Monte-Carlo simulation used to study the flow of gas molecules in three dimensions <ref> [23] </ref>. The running time on one processor of the iPSC/860 is 4876 seconds and the total memory requirement is 30 MB. * unstructured: this is a flow solver capable of solving the Navier-Stokes equations about complex geometries through the use of unstructured grids [3].
Reference: [24] <author> T. Tannenbaum and M. Litzkow. </author> <title> The Condor distributed processing system. </title> <journal> Dr. Dobbs' Journal, </journal> <volume> 20(2) </volume> <pages> 42-4, </pages> <month> Feb </month> <year> 1995. </year>
Reference-contexts: There are many ways in which an SPMD program can adapt to a change in the number of available processors. For example, it could checkpoint the evicted process to disk and restart it elsewhere (as in Condor <ref> [24] </ref>) or it could stop the process and copy it over from memory to memory (as in Locus [20]), or it could copy the stack and a small number of pages over and fault the rest in lazily (as in Accent [28] and Sprite [8]). <p> Our system (called Finch) uses a central coordinator to keep track of the workstation availability and a per-application manager process which keeps track of the progress of the application. The central coordinator resembles the Condor central manager <ref> [24] </ref> and runs on a central machine. The application manager is created when the job is submitted and lives for the duration of the job. It runs on the submitting machine.
Reference: [25] <author> Marvin Theimer and Keith Lantz. </author> <title> Finding idle machines in a workstation-based distributed system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(11) </volume> <pages> 1444-57, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Exploiting idle workstations has been a popular research area. This popularity has been fueled partly by studies which have indicated that a large fraction of workstations are unused for a large fraction of time <ref> [9, 17, 19, 25] </ref> and partly by the rapid growth in the power of workstations. Batch-processing systems that utilize idle workstations for running sequential jobs have been in production use for many years. <p> A well-known example is Condor [15], which has been has been in operation at the University of Wisconsin for about 8 years and which currently manages about 300 workstations [6]. The utility of harvesting idle workstations for parallel computation is less clear. First, the workstation-availability results <ref> [9, 17, 19, 25] </ref> that have held out the promise of free cycles assume, at least implicitly, that progress of execution on one workstation, or the lack thereof, has no effect on the progress of execution on other workstations. This assumption does not hold for most parallel computation.
Reference: [26] <author> A.S. Umar, D.J. Dean, C. Bottcher, </author> <title> and M.R. Strayer. Spline methods for hydrodynamic equations: </title> <booktitle> parallel implementation. In Proceedings of the Sixth SIAM conference on parallel processing for scientific computing, </booktitle> <pages> pages 26-30, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Finally, we present the results. 3.1 Benchmarks All programs in this suite are programmed in the SPMD model. Figure 5 shows the speedups for the benchmarks running on dedicated parallel machines. These numbers have been obtained from publications <ref> [1, 3, 22, 23, 26] </ref>. The programs themselves are described below. We used class B datasets for all the NAS benchmarks. * nas-bt: this program uses an approach based on block-tridiagonal matrices to solve the Navier-Stokes equations [22]. <p> Its running time on one processor of the Intel Paragon is 68814 seconds and the total memory required is 134 MB. We have data for this program running on 1,2,3,4,5,10,25 and 50 processors. * hydro3d: this is a parallel implementation of 3 + 1-dimensional relativistic hydrodynamics <ref> [26] </ref>. Its running time on one processor of the Intel Delta is 406000 seconds and the total memory required is 89.2 MB.
Reference: [27] <author> V.N. Vatsa, M.D. Sanetrik, and E.B. Parlette. </author> <title> Development of a flexible and efficient multigrid-based multiblock flow solver; AIAA-93-0677. </title> <booktitle> In Proceedings of the 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: Finch is portable across Unix environments. Currently, it runs on Suns, Alphas and RS/6000s. For this study, we used a template extracted from a multiblock computational fluid dynamics application that solves the thin-layer Navier-Stokes equations over a 3D surface (multiblock TLNS3D <ref> [27] </ref>). This is an iterative SPMD program, each iteration corresponds to a different timestep. We chose the top of the time-step loop as the safe point for eviction.
Reference: [28] <author> E. Zayas. </author> <title> The Use of Copy-on-Reference in a Process Migration System. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh PA, </address> <month> April </month> <year> 1987. </year> <month> 15 </month>
Reference-contexts: to disk and restart it elsewhere (as in Condor [24]) or it could stop the process and copy it over from memory to memory (as in Locus [20]), or it could copy the stack and a small number of pages over and fault the rest in lazily (as in Accent <ref> [28] </ref> and Sprite [8]). All of these schemes involve moving an executing process. Since SPMD applications run multiple copies of the same program which are usually in (loose) synchrony, there is another, possibly cheaper, alternative.
References-found: 28

