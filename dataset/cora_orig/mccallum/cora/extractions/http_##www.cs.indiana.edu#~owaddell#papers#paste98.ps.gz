URL: http://www.cs.indiana.edu/~owaddell/papers/paste98.ps.gz
Refering-URL: http://www.cs.indiana.edu/hyplan/owaddell.html
Root-URL: http://www.cs.indiana.edu
Email: owaddell@cs.indiana.edu  jashley@eecs.ukans.edu  
Title: Visualizing the Performance of Higher-Order Programs  
Author: Oscar Waddell J. Michael Ashley 
Note: This material is based on work supported in part by the National Science Foundation under grants CDA-9312614, CDA-9401021, and CCR-9623753.  
Affiliation: Indiana University  University of Kansas  
Abstract: Profiling can provide the information needed to identify performance bottlenecks in a program, but the programmer must understand its relation to the program source in order to use this information. This is difficult due to the tremendous volume of data collected. Moreover, program transformations such as macro expansion and procedure inlining can obscure the relationship between the source and object code. Higher-order programs present additional challenges due to complex control flow and because they often consist of many small, often anonymous, procedures whose individual performance properties may be less interesting than their characteristics as a group. To address these challenges we have implemented a profiler and interactive profile visualizer and integrated them into an optimizing Scheme compiler. The profiler instruments target code and maintains correlation with the original source despite compiler optimizations that can eliminate, duplicate, or move code. The visualizer operates as a source-code browser with features to examine execution counts and execution times from several perspectives. It supports the programmer in identifying program hot spots as well as code regions responsible for or affected by those hotspots. It also supports profile differencing which permits the programmer to study program behavior in different execution contexts. Our experience suggests that visualization tools can help to present raw profile data in a meaningful way. The tool can synthesize a high-level picture of program performance while still giving the programmer the ability to explore the details in interesting regions of code. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Vikram S. Adve, John Mellor-Crummey, Mark Anderson, Ken Kennedy, Jhy-Chun Wang, and Daniel A. Reed. </author> <title> Integrating compilation and performance analysis for data-parallel programs. </title> <editor> In M. L. Simmons, A. H. Hayes, D. A. Reed, and J. Brown, editors, </editor> <booktitle> Proceedings of the Workshop on Debugging and Performance Tuning for Parallel Computing Systems. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> January </month> <year> 1996. </year>
Reference-contexts: In such environments correlating profile information with the original source is trivial unless the compiler performs aggressive code optimizations. One such prototype environment is a combination of the Pablo performance environment with the Rice University Fortran D compiler <ref> [1] </ref>. The Fortran D compiler is an aggressive, automatically parallelizing compiler. As in our compiler, the Fortran D compiler must maintain enough source-level information so that instrumentation code can generate information that is ultimately reported to the programmer at source level.
Reference: [2] <author> Glenn Ammons, Thomas Ball, and James R. Lau-rus. </author> <title> Exploiting hardware performance counters with flow and context sensitive profiling. </title> <booktitle> In Proceedings of the ACM SIGPLAN 1997 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 85-96, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: For example, the programmer might turn the dial to set execution counts on error cases to zero. Another problem is the misattribution of execution time. We are considering using call context trees (CCTs) <ref> [2] </ref> to build more precise control path contexts. A CCT is constructed at runtime and more closely approximates the dynamic call tree than the control-flow graph computed by our analysis.
Reference: [3] <author> Andrew W. Appel, Bruce F. Duba, David B. Mac-Queen, and Andrew P. Tolmach. </author> <title> Profiling in the presence of optimization and garbage collection. </title> <type> Technical Report CS-TR-197-88, </type> <institution> Princeton University, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: Instead, counters are placed only where necessary, and compile-time heuristics are used to minimize counter placement [5, 6]. A counter increment requires a load, a register increment, and a write. Minimizing counter placement reduces perturbations in code size and execution time. Execution time is collected using cost centers <ref> [3, 11] </ref> to correlate the source and object code. The current cost center is maintained in a dedicated memory location and its count incremented by a timer interrupt handler. A cost center is associated with the code for each procedure in the program. <p> Our environment is similar to theirs in that both require information be collected in terms of macro-expanded code yet be correlated with the original, unexpanded source. There has been some work on profiling higher-order languages. Appel et al. <ref> [3] </ref> describe a profiler for ML. Our approach to instrumenting code is similar to theirs. They introduce a counter and cost center for each procedure. The counter is incremented on procedure entry, and the cost center is reset upon return from nontail calls.
Reference: [4] <author> J. Michael Ashley. </author> <title> A practical and flexible flow analysis for higher-order languages. </title> <booktitle> In Proceedings of the 23rd Annual ACM SIGPLAN/SIGACT Symposium on Principles of Programming Languages, </booktitle> <pages> pages 184-194, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Because procedure bodies in higher-order languages are often small, aggregate execution time and interprocedural call counts are useful metrics to report to the programmer. Some form of control-flow graph is needed in order to compute interprocedural call counts and aggregate execution times. Our tool uses a compile-time flow analysis <ref> [4] </ref> to compute a control-flow graph. The graph contains nodes for procedures and call sites. There is complexity.
Reference: [5] <author> Thomas Ball and James Larus. </author> <title> Optimally profiling and tracing programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(4) </volume> <pages> 1319-1360, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Execution counts are collected for each basic block, but counters are not placed on each edge in the intrapro-cedural flow graph. Instead, counters are placed only where necessary, and compile-time heuristics are used to minimize counter placement <ref> [5, 6] </ref>. A counter increment requires a load, a register increment, and a write. Minimizing counter placement reduces perturbations in code size and execution time. Execution time is collected using cost centers [3, 11] to correlate the source and object code.
Reference: [6] <author> Robert G. Burger. </author> <title> Efficient Compilation and Profile-Driven Dynamic Recompilation in Scheme. </title> <type> PhD thesis, </type> <institution> Indiana University, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: Execution counts are collected for each basic block, but counters are not placed on each edge in the intrapro-cedural flow graph. Instead, counters are placed only where necessary, and compile-time heuristics are used to minimize counter placement <ref> [5, 6] </ref>. A counter increment requires a load, a register increment, and a write. Minimizing counter placement reduces perturbations in code size and execution time. Execution time is collected using cost centers [3, 11] to correlate the source and object code.
Reference: [7] <author> R. Kent Dybvig, Robert Hieb, and Carl Brugge-man. </author> <title> Syntactic abstraction in Scheme. </title> <journal> Lisp and Symbolic Computation, </journal> <volume> 5(4) </volume> <pages> 295-326, </pages> <year> 1993. </year>
Reference-contexts: To assist the programmer, a compiler may instrument an application for profiling. Correlating the collected information back to the programmer is a nontrivial task, however. In languages with sophisticated hygienic macro systems <ref> [7] </ref>, code is subject to significant source-to-source rewrites before it even reaches the compiler. Subsequent compiler optimization may further transform the code, making its relationship to the original source even less clear. <p> Profile data collected at runtime is correlated with the original source program. The compiler arranges this by associating with each parse tree node the file name and position corresponding to the original source expression. These source records are preserved through macro expansion <ref> [7] </ref> and the rest of compilation.
Reference: [8] <author> Cormac Flanagan, Matthew Flatt, Shriram Krish-namurthi, and Stephanie Weirich. </author> <title> Catching bugs in the web of program invariants. </title> <booktitle> In Proceedings of the ACM SIGPLAN 1996 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 23-32, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: For source-level browsing we are aware of only Mr. Spidey, an interactive tool for using static dataflow information to debug Scheme programs <ref> [8] </ref>. Our environment is similar to theirs in that both require information be collected in terms of macro-expanded code yet be correlated with the original, unexpanded source. There has been some work on profiling higher-order languages. Appel et al. [3] describe a profiler for ML.
Reference: [9] <author> Susan L. Graham, Peter B. Kessler, and Mar-shall K. McKusick. </author> <title> gprof: a call graph execution profiler. </title> <booktitle> In Proceedings of the ACM SIGPLAN 1982 Symposium on Compiler Construction, </booktitle> <pages> pages 120-126, </pages> <year> 1982. </year>
Reference-contexts: Also, no attempt is made to correlate the profile information with the original source, Our algorithm for aggregating execution times is similar to the one used by gprof <ref> [9] </ref>. Like gprof, our algorithm can misattribute time because the static control-flow graph must attribute time to callees based on execution counts only.
Reference: [10] <author> Thomas Reps, Thomas Ball, Manuvir Des, and James Larus. </author> <title> The use of program profiling for software maintainence with applications to the year 2000 problem. </title> <booktitle> In Proceedings of ESEC/FSE 97, volume 1301 of Lecture Notes in Computer Science, </booktitle> <pages> pages 423-449. </pages> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: For example, by comparing execution profiles collected for different datasets we may discover which parts of a program are affected by the differences in the inputs. This technique was recently developed to help identify portions of legacy code that might be vulnerable to Year 2000 problems <ref> [10] </ref>. It may also prove interesting to compare profiles generated on different architectures or with different compiler optimizations enabled. 6 Conclusions Performance tuning is a crucial component of the software lifecycle.
Reference: [11] <author> Patrick M. Sansom and Simon L. Peyton Jones. </author> <title> Formally based profiling for higher-order functional languages. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 19(2) </volume> <pages> 334-385, </pages> <month> March </month> <year> 1997. </year>
Reference-contexts: Instead, counters are placed only where necessary, and compile-time heuristics are used to minimize counter placement [5, 6]. A counter increment requires a load, a register increment, and a write. Minimizing counter placement reduces perturbations in code size and execution time. Execution time is collected using cost centers <ref> [3, 11] </ref> to correlate the source and object code. The current cost center is maintained in a dedicated memory location and its count incremented by a timer interrupt handler. A cost center is associated with the code for each procedure in the program. <p> Instead, the programmer must manually uninstrument interesting program points in order to charge metrics to callers. Finally, information collected by their profiler is displayed in tabular form, and no attempt is made to correlate the information with the original source. Sansom and Peyton-Jones <ref> [11] </ref> describe a time and space profiler for Haskell. Their profiling methodology requires the programmer to manually instrument the code with named cost centers. Execution times attributed to each cost center are then presented in tabular form.
References-found: 11

