URL: http://www.cs.cmu.edu/~knigam/papers/emcat-mlj99.ps
Refering-URL: http://www.cs.cmu.edu/~knigam/
Root-URL: http://www.cs.cmu.edu/~jr6b
Email: knigam@cs.cmu.edu  mccallum@justresearch.com  thrun@cs.cmu.edu  tom.mitchell@cmu.edu  Editor:  
Title: Text Classification from Labeled and Unlabeled Documents using EM  
Author: KAMAL NIGAM ANDREW KACHITES MCCALLUM zy SEBASTIAN THRUN TOM MITCHELL 
Keyword: text classification, Expectation-Maximization, integrating supervised and unsupervised learning, combining labeled and unlabeled data, Bayesian learning  
Address: Pittsburgh, PA 15213  Street, Pittsburgh, PA 15213  
Affiliation: School of Computer Science, Carnegie Mellon University,  
Note: Machine Learning, 1-34 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  Just Research, 4616 Henry  
Abstract: This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available. We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Blum, A., & Mitchell, T. </author> <year> (1998). </year> <title> Combining labeled and unlabeled data with co-training. </title> <booktitle> In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT '98), </booktitle> <pages> pp. 92-100. </pages>
Reference: <author> Castelli, V., & Cover, T. M. </author> <year> (1995). </year> <title> On the exponential value of labeled samples. </title> <journal> Pattern Recognition Letters, </journal> <volume> 16 (1), </volume> <pages> 105-111. </pages>
Reference-contexts: Argument for the Value of Unlabeled Data How are unlabeled data useful when learning classification? Unlabeled data alone are generally insufficient to yield better-than-random classification because there is no information about the class label <ref> (Castelli & Cover, 1995) </ref>. However, unlabeled data does contain information about the joint distribution over features other than the class label. Because of this they can sometimes be used|together with a sample of labeled data|to significantly increase classification accuracy in certain problem settings. <p> For instance, in Figure 1, the means, variances, and mixture parameter can be learned with unlabeled data alone. Labeled data must be used to determine which Gaussian belongs to which class. This problem is known to converge exponentially quickly in the number of labeled samples <ref> (Castelli & Cover, 1995) </ref>. Informally, as long as there are enough labeled examples to determine the TEXT CLASSIFICATION FROM LABELED AND UNLABELED DOCUMENTS USING EM 5 class of each component, the parameter estimation can be done with unlabeled data alone.
Reference: <author> Cheeseman, P., & Stutz, J. </author> <year> (1996). </year> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In Fayyad, U., Piatetsky-Shapiro, G., Smyth, P., & Uthurusamy, R. (Eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Whereas we focus on data where the class labels are missing, they focus on data where features other than the class labels are missing. The AutoClass project <ref> (Cheeseman & Stutz, 1996) </ref> investigates the combination of Expectation-Maximization with a naive Bayes generative model. The emphasis of their research is the discovery of novel clustering for unsupervised learning over unlabeled data.
Reference: <author> Cohen, W. W., & Singer, Y. </author> <year> (1996). </year> <title> Context-sensitive learning methods for text categorization. </title> <booktitle> In SIGIR '96: Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pp. 307-315. </pages>
Reference-contexts: A variety of statistical techniques other than naive Bayes have been applied to text classification, including Support Vector Machines (Joachims, 1998), k nearest neighbor (Yang, 1994, 1999), TFIDF/Rocchio (Salton, 1991; Rocchio, 1971), exponential gradient and covering algorithms <ref> (Cohen & Singer, 1996) </ref>. We use naive Bayes in this paper because is has a strong probabilistic foundation for Expectation-Maximization, and is more efficient for large data sets.
Reference: <author> Cover, T. M., & Thomas, J. A. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: The sum of this quantity over all words is the Kullback-Leibler divergence between the distribution of words in c j and the distribution of words in :c j <ref> (Cover & Thomas, 1991) </ref>.
Reference: <author> Craven, M., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T., Nigam, K., & Slattery, S. </author> <year> (1998). </year> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <booktitle> In Proceedings of the Fifteenth National Conference on Artificial Intellligence (AAAI-98), </booktitle> <pages> pp. 509-516. </pages>
Reference-contexts: The use of each non-overlapping training set comprises a new trial of the given experiment. Results are reported as averages over all trials of the experiment. The WebKB data set <ref> (Craven et al., 1998) </ref> contains 8145 web pages gathered from university computer science departments. The collection includes the entirety of four departments, and additionally, an assortment of pages from other universities. The pages are divided into seven categories: student, faculty, staff, course, project, department and other. <p> In this paper, we use the four most populous non-other categories: student, faculty, course and project|all together containing 4199 pages. The task is to classify a web page into the appropriate one of the four categories. For consistency with previous studies with this data set <ref> (Craven et al., 1998) </ref>, when tokenizing the WebKB data, numbers were converted into a time or a phone number token, if appropriate, or otherwise a sequence-of-length-n token. We did not use stemming or a stoplist; we found that using a stoplist actually hurt performance.
Reference: <author> Dagan, I., & Engelson, S. P. </author> <year> (1995). </year> <title> Committee-based sampling for training probabilistic classifiers. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference (ICML '95), </booktitle> <pages> pp. 150-157. </pages>
Reference: <author> Dempster, A. P., Laird, N. M., & Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39 (1), </volume> <pages> 1-38. </pages>
Reference-contexts: This paper uses Expectation-Maximization (EM) to learn classifiers that take advantage of both labeled and unlabeled data. EM is a class of iterative algorithms for maximum likelihood or maximum a posteriori estimation in problems with incomplete data <ref> (Dempster, Laird, & Rubin, 1977) </ref>. In our case, the unlabeled data are considered incomplete because they come without class labels. <p> However, by augmenting this small set with a large set of unlabeled data, and combining the two sets with EM, we can improve the parameter estimates. EM is a class of iterative algorithms for maximum likelihood or maximum a posteriori estimation in problems with incomplete data <ref> (Dempster et al., 1977) </ref>. In our case, the unlabeled data are considered incomplete because they come without class labels. Applying EM to naive Bayes is quite straightforward. First, the naive Bayes parameters, ^ , are estimated from just the labeled documents. <p> Its application to classification is not new in the statistics literature. The idea of using an EM-like procedure to improve a classifier by "treating the unclassified data as incomplete" is mentioned by R. J. A. Little among the published responses to the original EM paper <ref> (Dempster et al., 1977) </ref>.
Reference: <author> Dietterich, T. G. </author> <year> (1998). </year> <title> Approximate statistical tests for comparing supervised classification learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 10 (7). </volume>
Reference: <author> Domingos, P., & Pazzani, M. </author> <year> (1997). </year> <title> On the optimality of the simple Bayesian classifier under zero-one loss. </title> <journal> Machine Learning, </journal> <volume> 29, </volume> <pages> 103-130. </pages>
Reference: <author> Friedman, J. H. </author> <year> (1997). </year> <title> On bias, variance, </title> <booktitle> 0/1-loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery, </booktitle> <volume> 1 (1), </volume> <pages> 55-77. </pages> <note> 32 NIGAM, </note> <author> MCCALLUM, THRUN AND MITCHELL Ghahramani, Z., & Jordan, M. I. </author> <year> (1994). </year> <title> Supervised learning from incomplete data via an EM approach. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <pages> pp. 120-127. </pages>
Reference: <author> Jaakkola, T. S., & Jordan, M. I. </author> <year> (1998). </year> <title> Improving the mean field approximation via the use of mixture distributions. </title> <editor> In Jordan, M. I. (Ed.), </editor> <title> Learning in Graphical Models. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> Joachims, T. </author> <year> (1997). </year> <title> A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference (ICML '97), </booktitle> <pages> pp. 143-151. </pages>
Reference-contexts: We use a stoplist, but do not stem. In Reuters, classifiers for different categories perform best with widely varying vocabulary sizes (which are chosen by average mutual information with the class variable). This variance in optimal vocabulary size is unsurprising. As previously noted <ref> (Joachims, 1997) </ref>, categories like "wheat" and "corn" are known for a strong correspondence between a small set of words (like their title words) and the cate 18 NIGAM, MCCALLUM, THRUN AND MITCHELL gories, while categories like "acq" are known for more complex characteristics.
Reference: <author> Joachims, T. </author> <year> (1998). </year> <title> Text categorization with Support Vector Machines: Learning with many relevant features. </title> <booktitle> In Machine Learning: ECML-98, Tenth European Conference on Machine Learning, </booktitle> <pages> pp. 137-142. </pages>
Reference-contexts: The precision-recall breakeven point is defined as the precision and recall value at which the two are equal <ref> (e.g. Joachims, 1998) </ref>. The algorithm used for experiments with EM is described in Table 2. In this section, when leave-one-out cross-validation is performed in conjunction with EM, we make one simplification for computational efficiency. <p> A variety of statistical techniques other than naive Bayes have been applied to text classification, including Support Vector Machines <ref> (Joachims, 1998) </ref>, k nearest neighbor (Yang, 1994, 1999), TFIDF/Rocchio (Salton, 1991; Rocchio, 1971), exponential gradient and covering algorithms (Cohen & Singer, 1996). We use naive Bayes in this paper because is has a strong probabilistic foundation for Expectation-Maximization, and is more efficient for large data sets.
Reference: <author> Koller, D., & Sahami, M. </author> <year> (1997). </year> <title> Hierarchically classifying documents using very few words. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference (ICML '97), </booktitle> <pages> pp. 170-178. </pages>
Reference: <author> Lang, K. </author> <year> (1995). </year> <title> Newsweeder: Learning to filter netnews. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference (ICML '95), </booktitle> <pages> pp. 331-339. </pages>
Reference: <author> Larkey, L. S., & Croft, W. B. </author> <year> (1996). </year> <title> Combining classifiers in text categorization. </title> <booktitle> In SIGIR '96: Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pp. 289-297. </pages>
Reference: <author> Lewis, D. D. </author> <year> (1992). </year> <title> An evaluation of phrasal and clustered representations on a text categorization task. </title> <booktitle> In SIGIR '92: Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pp. 37-50. </pages>
Reference: <author> Lewis, D. D. </author> <year> (1995). </year> <title> A sequential algorithm for training text classifiers: Corrigendum and additional data. </title> <journal> SIGIR Forum, </journal> <volume> 29 (2), </volume> <pages> 13-19. </pages>
Reference: <author> Lewis, D. D. </author> <year> (1998). </year> <title> Naive (Bayes) at forty: The independence assumption in information retrieval. </title> <booktitle> In Machine Learning: ECML-98, Tenth European Conference on Machine Learning, </booktitle> <pages> pp. 4-15. </pages>
Reference: <author> Lewis, D. D., & Gale, W. A. </author> <year> (1994). </year> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In SIGIR '94: Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pp. 3-12. </pages>
Reference: <author> Lewis, D. D., & Knowles, K. A. </author> <year> (1997). </year> <title> Threading electronic mail: A preliminary study. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 33 (2), </volume> <pages> 209-217. </pages>
Reference: <author> Lewis, D. D., & Ringuette, M. </author> <year> (1994). </year> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pp. 81-93. </pages>
Reference: <author> Li, H., & Yamanishi, K. </author> <year> (1997). </year> <title> Document classification using a finite mixture model. </title> <booktitle> In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 39-47. </pages>
Reference-contexts: John Lafferty provided insightful discussions about EM. We thank Jason Rennie for comments on an earlier draft. This research was supported in part by the Darpa HPKB program under contract F30602-97-1-0215. Notes 1. This assumption will be relaxed in Section 5.3.2 by making this a many-to-one correspondence. Other work <ref> (Li & Yamanishi, 1997) </ref> relaxes this assumption in a one-to-many fashion. 2.
Reference: <author> Liere, R., & Tadepalli, P. </author> <year> (1997). </year> <title> Active learning with committees for text categorization. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97), </booktitle> <pages> pp. 591-596. </pages>
Reference: <author> McCallum, A., & Nigam, K. </author> <year> (1998). </year> <title> A comparison of event models for naive Bayes text classification. In AAAI-98 Workshop on Learning for Text Categorization. </title> <type> Tech. rep. </type> <month> WS-98-05, </month> <title> AAAI Press. http://www.cs.cmu.edu/~mccallum. TEXT CLASSIFICATION FROM LABELED AND UNLABELED DOCUMENTS USING EM 33 McCallum, </title> <editor> A., Rosenfeld, R., Mitchell, T., & Ng, A. </editor> <year> (1998). </year> <title> Improving text clasification by shrinkage in a hierarchy of classes. </title> <booktitle> In Machine Learning: Proceedings of the Fifteenth International Conference (ICML '98), </booktitle> <pages> pp. 359-367. </pages>
Reference-contexts: It is a multinomial (or in lan TEXT CLASSIFICATION FROM LABELED AND UNLABELED DOCUMENTS USING EM 9 guage modeling terms, "unigram") model, where the classifier is a mixture of multi-nomials <ref> (McCallum & Nigam, 1998) </ref>. This formulation has been used by numerous practitioners of naive Bayes text classification (Lewis & Gale, 1994; Joachims, 1997; Li & Yamanishi, 1997; Mitchell, 1997; McCallum et al., 1998; Lewis, 1998). <p> Empirical comparisons show that the multinomial formulation yields classifiers with consistently higher accuracy <ref> (McCallum & Nigam, 1998) </ref>. 5. Incorporating Unlabeled Data with EM We now proceed to the main topic of this paper: how unlabeled data can be used to improve a text classifier. <p> Three such examples applied to text are "Query By Committee" (Dagan & Engelson, 1995; Liere & Tadepalli, 1997), relevance sampling and uncertainty sampling (Lewis & Gale, 1994; Lewis, 1995). Recent work by some of the authors combines active learning with Expectation-Maximization <ref> (McCallum & Nigam, 1998) </ref>. EM is applied to the unlabeled documents both to help inform the algorithm's choice of documents for labeling requests, and also to boost accuracy using the documents that remain unlabeled (as in this paper). <p> using EM: (1) active learning could use an explicit model of unlabeled data and incorporate EM, both to improve selection of examples for which to request a label and to improve classification accuracy using the examples that remain unlabeled at the end; initial study in this area has already begun <ref> (McCallum & Nigam, 1998) </ref>; (2) an incremental learning algorithm that re-trains throughout the testing phase could use the unlabeled test data received early in the testing phase in order to improve performance on the later test data.
Reference: <author> McCallum, A. K., & Nigam, K. </author> <year> (1998). </year> <title> Employing EM in pool-based active learning for text classification. </title> <booktitle> In Machine Learning: Proceedings of the Fifteenth International Conference (ICML '98), </booktitle> <pages> pp. 350-358. </pages>
Reference-contexts: It is a multinomial (or in lan TEXT CLASSIFICATION FROM LABELED AND UNLABELED DOCUMENTS USING EM 9 guage modeling terms, "unigram") model, where the classifier is a mixture of multi-nomials <ref> (McCallum & Nigam, 1998) </ref>. This formulation has been used by numerous practitioners of naive Bayes text classification (Lewis & Gale, 1994; Joachims, 1997; Li & Yamanishi, 1997; Mitchell, 1997; McCallum et al., 1998; Lewis, 1998). <p> Empirical comparisons show that the multinomial formulation yields classifiers with consistently higher accuracy <ref> (McCallum & Nigam, 1998) </ref>. 5. Incorporating Unlabeled Data with EM We now proceed to the main topic of this paper: how unlabeled data can be used to improve a text classifier. <p> Three such examples applied to text are "Query By Committee" (Dagan & Engelson, 1995; Liere & Tadepalli, 1997), relevance sampling and uncertainty sampling (Lewis & Gale, 1994; Lewis, 1995). Recent work by some of the authors combines active learning with Expectation-Maximization <ref> (McCallum & Nigam, 1998) </ref>. EM is applied to the unlabeled documents both to help inform the algorithm's choice of documents for labeling requests, and also to boost accuracy using the documents that remain unlabeled (as in this paper). <p> using EM: (1) active learning could use an explicit model of unlabeled data and incorporate EM, both to improve selection of examples for which to request a label and to improve classification accuracy using the examples that remain unlabeled at the end; initial study in this area has already begun <ref> (McCallum & Nigam, 1998) </ref>; (2) an incremental learning algorithm that re-trains throughout the testing phase could use the unlabeled test data received early in the testing phase in order to improve performance on the later test data.
Reference: <author> McLachlan, G. J., & Krishnan, T. </author> <year> (1997). </year> <title> The EM Algorithm and Extensions. </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: Consider when an infinite amount of unlabeled data is available, along with a finite number of labeled samples. It is well known that unlabeled data alone, when generated from a mixture of two Gaussians, are sufficient to recover the original mixture components <ref> (McLachlan & Krishnan, 1997, Section 2.7) </ref>. However, it is impossible to assign class labels to each of the Gaussians without any labeled data. Thus, the remaining learning problem is the problem of assigning class labels to the two Gaussians.
Reference: <author> McLachlan, G., & Basford, K. </author> <year> (1988). </year> <title> Mixture Models. </title> <publisher> Marcel Dekker, </publisher> <address> New York. </address>
Reference: <author> Miller, D. J., & Uyar, H. S. </author> <year> (1997). </year> <title> A mixture of experts classifier with learning based on both labelled and unlabelled data. </title> <booktitle> In Advances in Neural Information Processing Systems 9, </booktitle> <pages> pp. 571-577. </pages>
Reference: <author> Mitchell, T. M. </author> <year> (1997). </year> <title> Machine Learning. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference: <author> Ng, A. Y. </author> <year> (1997). </year> <title> Preventing "overfitting" of cross-validation data. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference (ICML '97), </booktitle> <pages> pp. 245-253. </pages>
Reference: <author> Pazzani, M. J., Muramatsu, J., & Billsus, D. </author> <year> (1996). </year> <title> Syskill & Webert: Identifying interesting Web sites. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pp. 54-59. </pages>
Reference: <author> Rissanen, J. </author> <year> (1983). </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> Annals of Statistics, </journal> <volume> 11 (2), </volume> <pages> 416-431. </pages>
Reference-contexts: The computationally expensive, but complete, cross-validation should perform better. Other model selection methods may perform better, while also remaining compu-tationally efficient. These include: more robust methods of cross-validation, such as that of Ng (1997); Minimum Description Length <ref> (Rissanen, 1983) </ref>; and Schuur-man's metric-based approach, which also uses unlabeled data (1997). Research on improved methods of model selection for our algorithm is an area of future work. 7. Related Work Expectation-Maximization is a well-known family of algorithms with a long history and many applications.
Reference: <author> Robertson, S. E., & Sparck-Jones, K. </author> <year> (1976). </year> <title> Relevance weighting of search terms. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 27 (3), </volume> <pages> 129-146. </pages>
Reference: <author> Rocchio, J. </author> <year> (1971). </year> <title> Relevance feedback in information retrieval. </title> <editor> In Salton, G. (Ed.), </editor> <title> The SMART Retrieval System:Experiments in Automatic Document Processing. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Sahami, M., Dumais, S., Heckerman, D., & Horvitz, E. </author> <year> (1998). </year> <title> A Baysian approach to filtering junk e-mail. In AAAI-98 Workshop on Learning for Text Categorization. </title> <type> Tech. rep. </type> <address> WS-98-05, </address> <publisher> AAAI Press. </publisher> <address> http://robotics.stanford.edu/users/sahami/papers.html. </address>
Reference: <author> Salton, G. </author> <year> (1991). </year> <title> Developments in automatic text retrieval. </title> <journal> Science, </journal> <volume> 253 (5023), </volume> <pages> 974-980. </pages>
Reference: <author> Schuurmans, D. </author> <year> (1997). </year> <title> A new metric-based approach to model selection. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97), </booktitle> <pages> pp. 552-558. </pages>
Reference: <author> Shahshahani, B., & Landgrebe, D. </author> <year> (1994). </year> <title> The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon. </title> <journal> IEEE Transactions on Geoscience and Remote Sensing, </journal> <volume> 32 (5), </volume> <pages> 1087-1095. </pages>
Reference: <author> Shavlik, J., & Eliassi-Rad, T. </author> <year> (1998). </year> <title> Intelligent agents for web-based tasks: An advice-taking approach. In AAAI-98 Workshop on Learning for Text Categorization. </title> <type> Tech. rep. </type> <address> WS-98-05, </address> <publisher> AAAI Press. </publisher> <address> http://www.cs.wisc.edu/~shavlik/mlrg/publications.html. </address>
Reference: <author> Stolcke, A., & Omohundro, S. M. </author> <year> (1994). </year> <title> Best-first model merging for hidden Markov model induction. </title> <type> Tech. rep. </type> <institution> TR-94-003, ICSI, University of California, Berkeley. </institution> <note> http://www.icsi.berkeley.edu/techreports/1994.html. 34 NIGAM, </note> <author> MCCALLUM, THRUN AND MITCHELL Yang, Y. </author> <year> (1994). </year> <title> Expert network: Effective and efficient learning from human decisions in text categorization and retrieval.. </title> <booktitle> In SIGIR '94: Proceedings of the Seventeenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pp. 13-22. </pages>
Reference: <author> Yang, Y. </author> <year> (1999). </year> <title> An evaluation of statistical approaches to text categorization. </title> <note> Journal of Information Retrieval. To appear. </note>
Reference: <author> Yang, Y., & Pederson, J. O. </author> <year> (1997). </year> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In Machine Learning: Proceedings of the Fourteenth International Conference (ICML '97), </booktitle> <pages> pp. 412-420. </pages>
References-found: 44

