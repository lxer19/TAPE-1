URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/SmaKro91.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: smagt@fwi.uva.nl  
Title: A real-time learning neural robot controller  
Author: P. Patrick van der Smagt Ben J. A. Krose 
Address: Kruislaan 403, 1098 SJ Amsterdam THE NETHERLANDS  
Affiliation: University of Amsterdam Department of Computer Systems  
Abstract: A neurally based adaptive controller for a 6 degrees of freedom (DOF) robot manipulator with only rotary joints and a hand-held camera is described. The task of the system is to place the manipulator directly above an object that is observed by the camera (i.e., 2D hand-eye coordination). The requirement of adaptivity results in a system which does not make use of any inverse kinematics formulas or other detailed knowledge of the plant; instead, it should be self-supervising and adapt on-line. The proposed neural system will directly translate the preprocessed sensory data to joint displacements. It controls the plant in a feedback loop. The robot arm may make a sequence of moves before the target is reached, when in the meantime the network learns from experience. The network is shown to adapt quickly (in only tens of trials) and form a correct mapping from input to output domain.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. H. Levis, S. I. Marcus, W. R. Perkins, P. Kokotovic, M. Athans, R. W. Brockett & A. S. Willsky, </author> <title> "Challenges to control: A collective view," </title> <journal> IEEE Transactions on Automatic Control AC-32 (1987), </journal> <pages> 275-285. </pages>
Reference-contexts: Although non-neural adaptive controllers have been investigated, real-time computational requirements grow out of bound when the number of state variables increases <ref> [1] </ref>. fl From: Proceedings of the 1991 International Conference on Artificial Neural Networks, ICANN-91, Espoo, Finland, June 24-28, 1991, pp. 351-356. y This research has been partly sponsored by the Dutch Foundation for Neural Networks.
Reference: [2] <author> G. Josin, </author> <title> "Neural-space generalization of a topological transformation," </title> <booktitle> Biological Cybernetics 59 (1988), </booktitle> <pages> 283-290. </pages>
Reference: [3] <author> D. Psaltis, A. Sideris & A. A. Yamamura, </author> <title> "A multilayer neural network controller," </title> <journal> IEEE Control Systems Magazine 8 (Apr., </journal> <year> 1988), </year> <pages> 17-21. </pages>
Reference: [4] <author> H. J. Ritter, T. M. Martinetz & K. J. Schulten, </author> <title> "Topology-conserving maps for learning visuo-motor-coordination," </title> <booktitle> Neural Networks 2 (1989), </booktitle> <pages> 159-168. </pages>
Reference: [5] <author> M. Kuperstein, </author> <title> "INFANT neural controller for adaptive sensory-motor coordination," </title> <booktitle> Neural Networks 4 (1991), </booktitle> <pages> 131-145. </pages>
Reference: [6] <author> M. J. D. Powell, </author> <title> "Restart procedures for the conjugate gradient method," </title> <booktitle> Mathematical Programming 12 (Apr., </booktitle> <year> 1977), </year> <pages> 241-254. </pages>
Reference-contexts: The feed-forward network used consists of three inputs, ten hidden units with sigmoid activation function, and two output units. A bias input is provided to the hidden and output units. The network is trained with an implementation of the conjugate gradient optimisation technique with improvements as suggested by Powell <ref> [6] </ref>. This technique is described in the appendix. Since the conjugate gradient algorithm has to be performed over a number of training samples instead of one at a time, a set of training samples (the short term memory or STM) is built up and maintained at all times. <p> A learning curve is depicted in figure 6. move is plotted. Appendix: The conjugate gradient algorithm The proposed algorithm is an improvement on the error back-propagation algorithm for supervised learning of a feed-forward network. The algorithm is based on conjugate gradient optimisation <ref> [6, 8-12] </ref>. Conjugate gradient optimisation is a standard technique from numerical analysis. It replaces the primitive steepest descent method with a direction set minimisation method. <p> Although only n iterations are needed for a quadratic system with n degrees of freedom, due to the fact that we are not minimising quadratic systems, as well as a result of round off errors, the n directions have to be followed several times (see figure 7). Powell <ref> [6] </ref> introduced some improvements to correct for behaviour in non-quadratic systems. The resulting cost is O (n) which is significantly better than the linear convergence 2 of steepest descent. The hills on the left are very steep, resulting in a large search vector u i .
Reference: [7] <author> B. J. A. Krose, M. J. van der Korst & F. C. A. Groen, </author> <title> "Learning strategies for a vision based neural controller for a robot arm," </title> <booktitle> in IEEE International Workshop on Intelligent Motor Control, </booktitle> <editor> O. Kaynak, ed., Istanbul, </editor> <month> 20-22 Aug., </month> <year> 1990, </year> <pages> 199-203. </pages>
Reference-contexts: The whole procedure is repeated until the target position is reached 1 . New learning samples are constructed as follows. With the input (x; 3 ) the network gives a joint angle displacement , resulting in a new position x 0 . It can be shown <ref> [7] </ref> that the position that would have been reached with this equals x T x 0 , with T the rotation matrix of the second camera image with respect to the first image.
Reference: [8] <author> W. H. Press, B. P. Flannery, S. A. Teukolsky & W. T. Vetterling, </author> <title> Numerical Recipes: </title> <booktitle> The Art of Scientific Computing, </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference: [9] <author> J. Stoer & R. </author> <title> Bulirsch, Introduction to Numerical Analysis, </title> <publisher> Springer-Verlag, </publisher> <address> New York-Heidel-berg-Berlin, </address> <year> 1980, </year> <title> Einfuhrung in die Numerische Mathematik, </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg-Ber-lin, </address> <year> 1972, 1976. </year>
Reference: [10] <author> M. R. Hestenes & E. </author> <title> Stiefel, "Methods of conjugate gradients for solving linear systems," </title> <institution> Nat. Bur. Standards J. Res. </institution> <month> 49 </month> <year> (1952), </year> <pages> 409-436. </pages>
Reference: [11] <author> E. Polak, </author> <title> Computational Methods in Optimization, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference: [12] <author> E. Barnard & R. Cole, </author> <title> "A neural-net training program based on conjugate-gradient optimization," </title> <institution> Oregon Graduate Center, </institution> <type> Tech. rep. CSE 89-014, </type> <year> 1989. </year>
Reference: [13] <author> P. Patrick van der Smagt & B. J. A. Krose, </author> <title> "A real-time learning robot controller," </title> <institution> Dept. of Computer Systems, University of Amsterdam, </institution> <type> internal report, </type> <address> Amsterdam, The Netherlands, </address> <note> (in preparation). </note>
Reference-contexts: This is different from gradient descent which directly minimises in the direction of the steepest descent. A more detailed discussion is provided in <ref> [13] </ref>. Although only n iterations are needed for a quadratic system with n degrees of freedom, due to the fact that we are not minimising quadratic systems, as well as a result of round off errors, the n directions have to be followed several times (see figure 7).
Reference: [14] <author> Peter van Summeren, </author> <month> Sept., </month> <year> 1990, </year> <type> personal communication. </type>
Reference-contexts: The hills on the left are very steep, resulting in a large search vector u i . When the quadratic portion is entered the new search direction is constructed from the previous direction and the gradient, resulting in a spiraling minimisation <ref> [14] </ref>. This problem can be overcome by detecting such spiraling minimisations and restaring the algorithm with u 0 = rf . 2 A method is said to converge linearly if E i+1 = cE i with c &lt; 1.
References-found: 14

