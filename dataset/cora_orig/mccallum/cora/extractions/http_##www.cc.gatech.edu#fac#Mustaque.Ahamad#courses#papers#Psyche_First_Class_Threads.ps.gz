URL: http://www.cc.gatech.edu/fac/Mustaque.Ahamad/courses/papers/Psyche_First_Class_Threads.ps.gz
Refering-URL: http://www.cs.gatech.edu/computing/classes/cs6420_97_fall/reading.html
Root-URL: 
Email: -marsh,scott,leblanc,markatos-@cs.rochester.edu  
Title: First-Class User-Level Threads  
Author: Brian D. Marsh Michael L. Scott Thomas J. LeBlanc Evangelos P. Markatos 
Note: This work was supported in part by NSF grant number CCR-9005633, NSF IIP grant number CDA-8822724, and a DARPA/NASA Graduate Research Assistantship in Parallel Processing.  
Address: Rochester, NY 14627-0226  
Affiliation: Computer Science Department University of Rochester  
Abstract: It is often desirable, for reasons of clarity, portability, and efficiency, to write parallel programs in which the number of processes is independent of the number of available processors. Several modern operating systems support more than one process in an address space, but the overhead of creating and synchronizing kernel processes can be high. Many runtime environments implement lightweight processes (threads) in user space, but this approach usually results in second-class status for threads, making it difficult or impossible to perform scheduling operations at appropriate times (e.g. when the current thread blocks in the kernel). In addition, a lack of common assumptions may also make it difficult for parallel programs or library routines that use dissimilar thread packages to communicate with each other, or to synchronize access to shared data. We describe a set of kernel mechanisms and conventions designed to accord first-class status to user-level threads, allowing them to be used in any reasonable way that traditional kernel-provided processes can be used, while leaving the details of their implementation to user-level code. The key features of our approach are (1) shared memory for asynchronous communication between the kernel and the user, (2) software interrupts for events that might require action on the part of a user-level scheduler, and (3) a scheduler interface convention that facilitates interactions in user space between dissimilar kinds of threads. We have incorporated these mechanisms in the Psyche parallel operating system, and have used them to implement several different kinds of user-level threads. We argue for our approach in terms of both flexibility and performance. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, A. Tevanian and M. Young, </author> <title> ``Mach: A New Kernel Foundation for UNIX Development,'' </title> <booktitle> Proceedings of the Summer 1986 USENIX Technical Conference, </booktitle> <month> June </month> <year> 1986, </year> <pages> pp. 93-112. </pages>
Reference-contexts: We now describe these mechanisms and their use in more detail. 3. Mechanisms In Psyche, kernel processes are used to implement the virtual processors that execute user-level threads. In many respects our notion of virtual processor resembles the kernel-implemented threads of multiprocessor operating systems such as Mach <ref> [1] </ref> and Topaz (Taos) [24]. Virtual processors are created in response to a system call, very much like traditional kernel-implemented processes. To obtain true parallelism within an application, one creates a virtual processor (often in the same address space) on each of several different physical processors. <p> Agora is a collection of libraries and interface compilers that allow users to connect distributed programs with stylized shared memory abstractions, much as an RPC stub generator allows users to connect those programs with a message-passing abstraction. Agora is built on top of Mach <ref> [1] </ref>, and uses Mach's kernel-implemented threads. 6. Performance Implications The two goals of our work are flexibility and performance for user-level threads. To achieve acceptable performance, features provided by a thread package must be cheap enough to use frequently.
Reference: [2] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska and H. M. Levy, </author> <title> ``Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism,'' </title> <booktitle> Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: An ability to request temporary non-preemption at the end of the quantum does not suffice, however, for a program that requires asynchronous notification to trigger an explicit action. At the University of Washington, Anderson et al. <ref> [2] </ref> have explored user-level scheduling in the context of the Topaz operating system on the DEC SRC Firefly multiprocessor workstation, an UMA machine. <p> Weiser, Demers, and Hauser [27] report a context switch time of 77 ms for user-level threads in the Portable Common Runtime on a SPARC-based workstation. The context switch time for the thread package used in the Psyche experiments is 51 ms. Anderson <ref> [2] </ref> reports a time of 37 ms in his FastThreads package on the CVAX processor. Comparable times for kernel-implemented processes are at least an order of magnitude slower in each case: 550 ms in Psyche on the Butterfly, 441 ms in Topaz on the CVAX.
Reference: [3] <institution> BBN Advanced Computers Incorporated, ``Inside the Butterfly Plus,'' </institution> <address> Cambridge, MA, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: Our conclusions are presented in section 7. 2. Rationale Our approach was developed as part of the Psyche parallel operating system [19-21], running on the BBN Butterfly Plus multiprocessor <ref> [3] </ref>. The design of our thread mechanisms was heavily influenced by the need to support multi-model parallel programming, the primary goal of Psyche. In particular, we have attempted to ensure that kernel assumptions about the nature of threads are minimized. <p> Our performance figures were derived from our implementation of Psyche on the BBN Butterfly Plus multiprocessor <ref> [3] </ref>, which contains MC68020 processors clocked at 16 MHz. Our experiments quantify the performance advantages of first-class user-level threads over both kernel processes and conventional user-level threads. 6.1. Comparison to Kernel-Implemented Processes It is widely recognized that kernel-implemented processes are inherently more expensive than user-implemented threads.
Reference: [4] <author> B. N. Bershad, E. D. Lazowska, H. M. Levy and D. B. Wagner, </author> <title> ``An Open Environment for Building Parallel Programming Systems,'' </title> <booktitle> Proceedings of the First ACM Conference on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <month> 19-21 July </month> <year> 1988, </year> <pages> pp. 1-9. </pages>
Reference-contexts: It may or may not need to save floating point registers on context switches. Features such as these, provided by the kernel but often unused in user space, can incur unwarranted costs. To overcome these problems, it has become commonplace to construct lightweight thread packages in user space <ref> [4, 9, 23, 27] </ref>. These packages multiplex a potentially large number of user-defined threads on top of a single kernel-implemented process. Within limits, they also allow the user to implement customized processes, communication, and scheduling inside an application. <p> User-level scheduling (and cooperation with the kernel scheduler) is only one aspect of multi-model parallel programming. The more general problem has been addressed in part by the Presto <ref> [4] </ref> and Agora [5] programming environments, at Washington and at CMU, respectively. Presto is an unusually flexible user-level thread package. It is constructed in an object-oriented style, with an internal structure into which users can plug a wide variety of process and communication abstractions.
Reference: [5] <author> R. Bisiani and A. Forin, </author> <title> ``Multilanguage Parallel Programming of Heterogeneous Machines,'' </title> <journal> IEEE Transactions on Computers 37:8 (August 1988), </journal> <pages> pp. 930-945. </pages>
Reference-contexts: User-level scheduling (and cooperation with the kernel scheduler) is only one aspect of multi-model parallel programming. The more general problem has been addressed in part by the Presto [4] and Agora <ref> [5] </ref> programming environments, at Washington and at CMU, respectively. Presto is an unusually flexible user-level thread package. It is constructed in an object-oriented style, with an internal structure into which users can plug a wide variety of process and communication abstractions.
Reference: [6] <author> D. L. Black, </author> <title> ``Scheduling Support for Concurrency and Parallelism in the Mach Operating System,'' </title> <booktitle> Computer 23:5 (May 1990), </booktitle> <pages> pp. 35-43. </pages>
Reference-contexts: Since we are using a NUMA machine, we also want to be able to reach a clean point at preemption time without requiring thread migration. All of the mechanisms discussed above allow user programs to control their scheduling behavior. Black <ref> [6] </ref> has proposed instead that programs provide the operating system with scheduler hints.
Reference: [7] <author> P. Brinch Hansen, </author> <title> ``Distributed Processes: A Concurrent Programming Concept,'' </title> <journal> Communications of the ACM 21:11 (November 1978), </journal> <pages> pp. 934-941. </pages>
Reference-contexts: Users want, and different run-time environments define, threads of various kinds, many of which may be incompatible with the kernel's notion of process. Some environments expect one thread to run at a time, as in the coroutine-like scheduling of Distributed Processes <ref> [7] </ref> and Lynx [22]. Some want to build cactus stacks, with dynamic allocation of activation records, as in Mesa [12]. Some want to use customized scheduling policies, such as priority-based scheduling, within an application.
Reference: [8] <author> M. Crovella, P. Das, C. Dubnicki, T. J. LeBlanc and E. P. Markatos, </author> <title> ``Multiprogramming on Multiprocessors,'' </title> <type> TR 385, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> February </month> <year> 1991 </year> <month> (revised May </month> <year> 1991). </year>
Reference-contexts: The various approaches to dealing with preemption are principally motivated by the unpredictable nature of kernel scheduling. There is less of a need for special mechanisms that deal with preemption however, if the machine is shared using physical partitions instead of time-slicing. Under processor partitioning <ref> [8, 18, 26] </ref> each application receives a set of dedicated processors for a relatively long period of time; preemption is only used to reallocate processors for medium-term scheduling. Processor partitioning may be the preferred scheduling policy for self-contained, compute-intensive parallel programs, particularly on machines with large numbers of processors.
Reference: [9] <author> T. W. Doeppner, Jr., </author> <title> ``Threads: A System for the Support of Concurrent Programming,'' </title> <type> Technical Report CS-87-11, </type> <institution> Department of Computer Science, Brown University, </institution> <year> 1987. </year>
Reference-contexts: It may or may not need to save floating point registers on context switches. Features such as these, provided by the kernel but often unused in user space, can incur unwarranted costs. To overcome these problems, it has become commonplace to construct lightweight thread packages in user space <ref> [4, 9, 23, 27] </ref>. These packages multiplex a potentially large number of user-defined threads on top of a single kernel-implemented process. Within limits, they also allow the user to implement customized processes, communication, and scheduling inside an application.
Reference: [10] <author> J. Edler, J. Lipkis and E. Schonberg, </author> <title> ``Process Management for Highly Parallel UNIX Systems,'' </title> <note> Ultracomputer Note #136, </note> <author> Courant Institute, N. Y. U., </author> <month> April </month> <year> 1988. </year>
Reference-contexts: In the next section we present the rationale for our approach and a brief overview of the specific mechanisms we propose. We describe our mechanisms in more detail in section 3. We discuss how our mechanisms support the hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 Edler et al. <ref> [10] </ref> note that blocking system calls in Unix include not only read, write, open, and close, but also ioctl, mkdir, rmdir, rename, link, unlink, stat, and others. construction of first-class user-level threads in section 4. We discuss related work in section 5. <p> None of the Unix user structure is shared between processes however, and none of it is writable in user mode, so it cannot be used to convey information from a thread package to the kernel. As part of the Symunix project at New York University, Edler et al. <ref> [10] </ref> have proposed a set of parallel programming extensions to the Unix kernel interface, including support for user-level scheduling. In particular, they describe a new ``meta-system-call'' that provides an asynchronous wrapper for existing blocking calls, and a quantum-extending mechanism designed to avoid preemption during critical sections.
Reference: [11] <author> R. H. Halstead, Jr., </author> <title> ``Multilisp: A Language for Concurrent Symbolic Computation,'' </title> <journal> ACM Transactions on Programming Languages and Systems 7:4 (October 1985), </journal> <pages> pp. 501-538. </pages>
Reference-contexts: To ensure the integrity of scheduling within the thread package, the kernel provides software interrupts at every point where a scheduler action might be required. In our experiments with Psyche, we have successfully ported or implemented Multilisp futures <ref> [11] </ref>, Uniform System tasks [25], Lynx threads [22], heavyweight single-threaded programs, and two different thread libraries. Performance. As in all user-level thread packages, the ability to create, destroy, schedule, and synchronize threads without the assistance of the kernel keeps the cost of these operations low. <p> Similar effects can occur in programs with condition synchronization. One of the most common models of parallel programming employs a collection of worker processes, one per processor, which repeatedly dequeue and execute tasks from a central work queue <ref> [11, 25, 26] </ref>. One of the things that a task may do is generate more tasks. It will often do so only if it is the last task of a certain kind to finish.
Reference: [12] <author> B. W. Lampson and D. D. Redell, </author> <title> ``Experience with Processes and Monitors in Mesa,'' </title> <journal> Communications of the ACM 23:2 (February 1980), </journal> <pages> pp. 105-117. </pages>
Reference-contexts: Some environments expect one thread to run at a time, as in the coroutine-like scheduling of Distributed Processes [7] and Lynx [22]. Some want to build cactus stacks, with dynamic allocation of activation records, as in Mesa <ref> [12] </ref>. Some want to use customized scheduling policies, such as priority-based scheduling, within an application. Some want to have processes interact using communication or synchronization mechanisms that are difficult to implement with kernel-provided operations.
Reference: [13] <author> S. J. Leffler, M. K. McKusick, M. J. Karels and J. S. Quarterman, </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System, </title> <publisher> The Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: For example, our use of shared data between the kernel and user is not new; the user structure (u-dot) of Unix 4.3BSD, which is readable in user space, contains information about the current process, and is also used to implement machine-dependent mechanisms such as the ``signal trampoline'' <ref> [13] </ref>. None of the Unix user structure is shared between processes however, and none of it is writable in user mode, so it cannot be used to convey information from a thread package to the kernel.
Reference: [14] <author> S. </author> <title> Leutenegger, ``Issues in Multiprogrammed Multiprocessor Scheduling,'' </title> <type> Ph. D. Thesis, TR 954, </type> <institution> Department of Computer Sciences, University of Wisconsin Madison, </institution> <month> August </month> <year> 1990. </year>
Reference-contexts: Zahorjan et al. [28] report performance degradations in the neighborhood of 25% when processes may be preempted at arbitrary times, while sharing a lock that is in use 75% of the time. Leutenneger <ref> [14] </ref> describes the variation in performance degradation as a function of lock utilization. For a lock that is in use 50% of the time, he reports that round-robin scheduling performs 10% worse than a processor allocation scheme in which the processes of a given application always run concurrently.
Reference: [15] <author> E. Markatos, M. Crovella, P. Das, C. Dubnicki and T. J. LeBlanc, </author> <title> ``The Effects of Multiprogramming on Barrier Synchronization,'' </title> <type> TR 380, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: The Gaussian elimination program has only 512 barriers, but each is a tree barrier, which can introduce context switches for each level in the tree <ref> [15] </ref>. The performance advantage of user-level threads is substantial in these cases, and we would expect many parallel applications to produce comparable results on other systems.
Reference: [16] <author> B. D. Marsh, </author> <title> ``Multi-Model Parallel Programming,'' </title> <type> Ph. D. Thesis, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: The first port took over a month, mainly because it uncovered kernel bugs, while the other two ports took less than a month each. All of the packages were integrated into a general system for cross-model synchronization and communication over the course of a two-month period <ref> [16] </ref>. 4. Discussion Returning to the issues enumerated in section 1, we now consider the degree to which our mechanisms support the construction of first-class user-level threads. Semantic flexibility.
Reference: [17] <author> B. Marsh, C. Brown, T. LeBlanc, M. Scott, T. Becker, P. Das, J. Karlsson and C. Quiroz, </author> <title> ``The Rochester Checkers Player: Multi-Model Parallel Programming for Animate Vision,'' </title> <type> TR 374, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: Support for first-class threads in Psyche has allowed us to construct a wide variety of user-level thread packages, and to employ them in the construction of multi-model programs. Our largest demonstration, now running in our robot lab, employs four different user-level process models in an integrated checkers-playing program <ref> [17] </ref>. A vision module, using a central queue of Uniform System tasks, analyzes video camera input to determine the most recent move of a human opponent on a conventional checkers set. A strategy module, using message passing between multi-threaded Lynx processes, performs parallel alpha-beta search to chose an appropriate counter-move.
Reference: [18] <author> C. McCann, R. Vaswani and J. Zahorjan, </author> <title> ``A Dynamic Processor Allocation Policy for Multipro-grammed Shared Memory Multiprocessors,'' </title> <type> TR 90-03-02, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> March </month> <year> 1990 </year> <month> (Revised February </month> <year> 1991). </year>
Reference-contexts: The various approaches to dealing with preemption are principally motivated by the unpredictable nature of kernel scheduling. There is less of a need for special mechanisms that deal with preemption however, if the machine is shared using physical partitions instead of time-slicing. Under processor partitioning <ref> [8, 18, 26] </ref> each application receives a set of dedicated processors for a relatively long period of time; preemption is only used to reallocate processors for medium-term scheduling. Processor partitioning may be the preferred scheduling policy for self-contained, compute-intensive parallel programs, particularly on machines with large numbers of processors.
Reference: [19] <author> M. L. Scott, T. J. LeBlanc and B. D. Marsh, </author> <title> ``Design Rationale for Psyche, a General-Purpose Multiprocessor Operating System,'' </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1988, </year> <pages> pp. 255-262. </pages>
Reference: [20] <author> M. L. Scott, T. J. LeBlanc and B. D. Marsh, </author> <title> ``Evolution of an Operating System for Large-Scale Shared-Memory Multiprocessors,'' </title> <type> TR 309, </type> <institution> Computer Science Department, University of Rochester, </institution> <month> March </month> <year> 1989. </year>
Reference: [21] <author> M. L. Scott, T. J. LeBlanc and B. D. Marsh, </author> <booktitle> ``Multi-Model Parallel Programming in Psyche,'' Proceedings of the Second ACM Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> March </month> <year> 1990, </year> <pages> pp. 70-78. </pages>
Reference-contexts: Lack of conventions for sharing among thread packages. When using user-level thread packages, a program may need to synchronize access to data shared among more than one kind of thread. This claim is one of the premises behind multi-model parallel programming <ref> [21] </ref>, the simultaneous use of more than one model of parallelism, both in different applications and in different pieces of a single application. Spin locks are an easily implemented solution, but are not always appropriate. <p> If the thread packages lie in distinct but overlapping address spaces, then the unblock routine will be invoked via PPC. Further details can be found in <ref> [21] </ref>. 3.4. Putting it All Together By mirroring the behavior of a physical machine, with memory and interrupts, our approach provides the writers of thread packages with a familiar model of concurrent programming. System implementors are accustomed to using this model in operating systems, and in signal-based programs in Unix.
Reference: [22] <author> M. L. Scott, </author> <title> ``The Lynx Distributed Programming Language: Motivation, Design, and Experience,'' </title> <booktitle> Computer Languages 16:3/4 (1991), </booktitle> <pages> pp. 209-233. </pages>
Reference-contexts: Users want, and different run-time environments define, threads of various kinds, many of which may be incompatible with the kernel's notion of process. Some environments expect one thread to run at a time, as in the coroutine-like scheduling of Distributed Processes [7] and Lynx <ref> [22] </ref>. Some want to build cactus stacks, with dynamic allocation of activation records, as in Mesa [12]. Some want to use customized scheduling policies, such as priority-based scheduling, within an application. Some want to have processes interact using communication or synchronization mechanisms that are difficult to implement with kernel-provided operations. <p> To ensure the integrity of scheduling within the thread package, the kernel provides software interrupts at every point where a scheduler action might be required. In our experiments with Psyche, we have successfully ported or implemented Multilisp futures [11], Uniform System tasks [25], Lynx threads <ref> [22] </ref>, heavyweight single-threaded programs, and two different thread libraries. Performance. As in all user-level thread packages, the ability to create, destroy, schedule, and synchronize threads without the assistance of the kernel keeps the cost of these operations low.
Reference: [23] <author> Sun Microsystems, Inc., </author> <title> ``Lightweight Processes,'' in SunOS Programming Utilities and Libraries, </title> <month> 27 March </month> <year> 1990. </year> <title> Sun Part Number 800-3847-10. </title>
Reference-contexts: It may or may not need to save floating point registers on context switches. Features such as these, provided by the kernel but often unused in user space, can incur unwarranted costs. To overcome these problems, it has become commonplace to construct lightweight thread packages in user space <ref> [4, 9, 23, 27] </ref>. These packages multiplex a potentially large number of user-defined threads on top of a single kernel-implemented process. Within limits, they also allow the user to implement customized processes, communication, and scheduling inside an application.
Reference: [24] <author> C. P. Thacker and L. C. Stewart, ``Firefly: </author> <title> A Multiprocessor Workstation,'' </title> <journal> IEEE Transactions on Computers 37:8 (August 1988), </journal> <pages> pp. 909-920. </pages>
Reference-contexts: Mechanisms In Psyche, kernel processes are used to implement the virtual processors that execute user-level threads. In many respects our notion of virtual processor resembles the kernel-implemented threads of multiprocessor operating systems such as Mach [1] and Topaz (Taos) <ref> [24] </ref>. Virtual processors are created in response to a system call, very much like traditional kernel-implemented processes. To obtain true parallelism within an application, one creates a virtual processor (often in the same address space) on each of several different physical processors.
Reference: [25] <author> R. H. Thomas and W. Crowther, </author> <title> ``The Uniform System: An Approach to Runtime Support for Large Scale Shared Memory Parallel Processors,'' </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1988, </year> <pages> pp. 245-254. </pages>
Reference-contexts: To ensure the integrity of scheduling within the thread package, the kernel provides software interrupts at every point where a scheduler action might be required. In our experiments with Psyche, we have successfully ported or implemented Multilisp futures [11], Uniform System tasks <ref> [25] </ref>, Lynx threads [22], heavyweight single-threaded programs, and two different thread libraries. Performance. As in all user-level thread packages, the ability to create, destroy, schedule, and synchronize threads without the assistance of the kernel keeps the cost of these operations low. <p> Similar effects can occur in programs with condition synchronization. One of the most common models of parallel programming employs a collection of worker processes, one per processor, which repeatedly dequeue and execute tasks from a central work queue <ref> [11, 25, 26] </ref>. One of the things that a task may do is generate more tasks. It will often do so only if it is the last task of a certain kind to finish.
Reference: [26] <author> A. Tucker and A. Gupta, </author> <title> ``Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multiprocessors,'' </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> 3-6 December </month> <year> 1989, </year> <pages> pp. 159-166. </pages>
Reference-contexts: The various approaches to dealing with preemption are principally motivated by the unpredictable nature of kernel scheduling. There is less of a need for special mechanisms that deal with preemption however, if the machine is shared using physical partitions instead of time-slicing. Under processor partitioning <ref> [8, 18, 26] </ref> each application receives a set of dedicated processors for a relatively long period of time; preemption is only used to reallocate processors for medium-term scheduling. Processor partitioning may be the preferred scheduling policy for self-contained, compute-intensive parallel programs, particularly on machines with large numbers of processors. <p> Similar effects can occur in programs with condition synchronization. One of the most common models of parallel programming employs a collection of worker processes, one per processor, which repeatedly dequeue and execute tasks from a central work queue <ref> [11, 25, 26] </ref>. One of the things that a task may do is generate more tasks. It will often do so only if it is the last task of a certain kind to finish. <p> Barrier programs can be expected to suffer from inopportune preemption more than do programs based on spin locks, because the probability is high that a process will be working on something critical (i.e. progress towards the barrier) at any given time. Tucker and Gupta <ref> [26] </ref> observe that the impact of preemption on work-queue based programs can be reduced by introducing a mechanism to preempt worker processes only after they finish a task and before another task is removed from the work queue.
Reference: [27] <author> M. Weiser, A. Demers and C. Hauser, </author> <title> ``The Portable Common Runtime Approach to Interoperability,'' </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <month> 3-6 December </month> <year> 1989, </year> <pages> pp. 114-122. </pages>
Reference-contexts: It may or may not need to save floating point registers on context switches. Features such as these, provided by the kernel but often unused in user space, can incur unwarranted costs. To overcome these problems, it has become commonplace to construct lightweight thread packages in user space <ref> [4, 9, 23, 27] </ref>. These packages multiplex a potentially large number of user-defined threads on top of a single kernel-implemented process. Within limits, they also allow the user to implement customized processes, communication, and scheduling inside an application. <p> The difference can be attributed not only to trap overhead, but also to the degree of functionality that must be designed into a process abstraction meant to meet the needs of disparate applications. The resulting difference in context switch time is often substantial. Weiser, Demers, and Hauser <ref> [27] </ref> report a context switch time of 77 ms for user-level threads in the Portable Common Runtime on a SPARC-based workstation. The context switch time for the thread package used in the Psyche experiments is 51 ms.
Reference: [28] <author> J. Zahorjan, E. D. Lazowska and D. L. Eager, </author> <title> ``The Effect of Scheduling Discipline on Spin Overhead in Shared Memory Parallel Systems,'' </title> <journal> IEEE Transactions on Parallel and Distributed Systems 2:2 (April 1991), </journal> <pages> pp. 180-198. </pages>
Reference-contexts: If kernel-level scheduling is not coordinated with user-level synchronization, threads may also block for locks that are held by threads on preempted virtual processors, or for conditions that can be made true only by threads on preempted virtual processors. Zahorjan et al. <ref> [28] </ref> report performance degradations in the neighborhood of 25% when processes may be preempted at arbitrary times, while sharing a lock that is in use 75% of the time. Leutenneger [14] describes the variation in performance degradation as a function of lock utilization.
References-found: 28

