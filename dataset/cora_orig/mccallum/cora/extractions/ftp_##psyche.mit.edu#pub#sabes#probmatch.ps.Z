URL: ftp://psyche.mit.edu/pub/sabes/probmatch.ps.Z
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: sabes@psyche.mit.edu  jordan@psyche.mit.edu  
Title: Reinforcement Learning by Probability Matching  
Author: Philip N. Sabes Michael I. Jordan 
Address: Cambridge, MA 02139  
Affiliation: Department of Brain and Cognitive Sciences Massachusetts Institute of Technology  
Abstract: We present a new algorithm for associative reinforcement learning. The algorithm is based upon the idea of matching a network's output probability with a probability distribution derived from the environment's reward signal. This Probability Matching algorithm is shown to perform faster and be less susceptible to local minima than previously existing algorithms. We use Probability Matching to train mixture of experts networks, an architecture for which other reinforcement learning rules fail to converge reliably on even simple problems. This architecture is particularly well suited for our algorithm as it can compute arbitrarily complex functions yet calculation of the output probability is simple.
Abstract-found: 1
Intro-found: 1
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87. </pages>
Reference-contexts: Although general purpose algorithms such as REINFORCE (Williams, 1992) and Generalized Learning Automata (Phansalkar, 1991) exist, they are generally slow and have trouble with local minima. As an example, when we attempted to apply these algorithms to mixture of experts networks <ref> (Jacobs et al., 1991) </ref>, the algorithms typically converged to the local minimum which places the entire burden of the task on one expert. Here we present a new reinforcement learning algorithm which has faster and more reliable convergence properties than previous algorithms. <p> We can generalize the linear model by considering a conditional output distribution in the form of a mixture of Gaussian experts <ref> (Jacobs et al., 1991) </ref>, p (yjx) = i=1 i ) 1 1 i Expert i has mean i = w T i x and covariance 2 i I. <p> We note that the PM update rules are equivalent to the supervised learning gradient descent update rules in <ref> (Jacobs et al., 1991) </ref> modulated by the difference between the actual and expected rewards. 1 This fact implies that the REINFORCE step is in the direction of the gradient of J R (), as shown by (Williams, 1992).
Reference: <author> Phansalkar, V. V. </author> <year> (1991). </year> <title> Learning automata algorithms for connectionist systems local and global convergence. </title> <type> PhD Thesis, </type> <institution> Dept. of Electrical Engineering, India Institute of Science, Bangalore. </institution>
Reference-contexts: 1 INTRODUCTION The problem of learning associative networks from scalar reinforcement signals is notoriously difficult. Although general purpose algorithms such as REINFORCE (Williams, 1992) and Generalized Learning Automata <ref> (Phansalkar, 1991) </ref> exist, they are generally slow and have trouble with local minima.
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal credit assignment in reinforcement learning. </title> <type> PhD Thesis, </type> <institution> Dept. of Computer and Information Science, University of Mas-sachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: In each case, networks were trained using Probability Matching, REINFORCE, and REINFORCE with reinforcement comparison (REINF-COMP), where a running average of the reward is used as a reinforcement baseline <ref> (Sutton, 1984) </ref>. In the first two examples an optimal output function y fl (x) was chosen and used to calculate a noisy error, " = ky y fl (x) zk, where z was i.i.d. zero-mean Gaussian with = :1.
Reference: <author> Williams, R. J. </author> <year> (1992). </year> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256. </pages>
Reference-contexts: 1 INTRODUCTION The problem of learning associative networks from scalar reinforcement signals is notoriously difficult. Although general purpose algorithms such as REINFORCE <ref> (Williams, 1992) </ref> and Generalized Learning Automata (Phansalkar, 1991) exist, they are generally slow and have trouble with local minima. <p> Note that any quantity which does not depend on y or r can be added to the difference in the update rule, and the expected step will still point along the direction of the gradient. The form of Equation 2 is similar to the REINFORCE algorithm <ref> (Williams, 1992) </ref>, whose update rule is 4 = ff (r b)r log p (yjx); where b, the reinforcement baseline, is a quantity which does not depend on y or r. <p> PM update rules are equivalent to the supervised learning gradient descent update rules in (Jacobs et al., 1991) modulated by the difference between the actual and expected rewards. 1 This fact implies that the REINFORCE step is in the direction of the gradient of J R (), as shown by <ref> (Williams, 1992) </ref>. See Williams and Peng, 1991, for a similar REINFORCE plus entropy update rule. Table 1: Convergence times and gate entropies for the linear example (standard errors in parentheses).
Reference: <author> Williams, R. J. and Peng, J. </author> <year> (1991). </year> <title> Function optimization using connectionist reinforcement learning algorithms. </title> <journal> Connection Science, </journal> <volume> 3 </volume> <pages> 241-268. </pages>
References-found: 5

