URL: http://www.cis.upenn.edu/~daes/papers/kr92.ps.gz
Refering-URL: http://www.cis.upenn.edu/~daes/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: greiner@learning.siemens.com  dale@cs.toronto.edu  
Title: Learning Useful Horn Approximations  
Author: Russell Greiner Dale Schuurmans 
Note: 25-29 October 1992, Cambridge, MA.  
Address: College Road East  Princeton, NJ 08540  Toronto, Ontario M5S 1A4  
Affiliation: 755  Siemens Corporate Research  Department of Computer Science University of Toronto  
Abstract: While the task of answering queries from an arbitrary propositional theory is intractable in general, it can typically be performed efficiently if the theory is Horn. This suggests that it may be more efficient to answer queries using a "Horn approximation"; i.e., a horn theory that is semantically similar to the original theory. The utility of any such approximation depends on how often it produces answers to the queries that the system actually encounters; we therefore seek an approximation whose expected "coverage" is maximal. Unfortunately, there are several obstacles to achieving this goal in practice: (i) The optimal approximation depends on the query distribution, which is typically not known a priori; (ii) identifying the optimal approximation is intractable, even given the query distribution; and (iii) the optimal approximation might be too large to guarantee tractable inference. This paper presents an approach that overcomes (or side-steps) each of these obstacles. We define a learning process, AdComp, that uses observed queries to estimate the query distribution "online", and then uses these estimates to hill-climb, efficiently, in the space of size-bounded Horn approximations, until reaching one that is, with provably high probability, effectively at a local optimum. Appears in the Proceedings of the Third International Conference on Knowledge Representation and Reasoning, 
Abstract-found: 1
Intro-found: 1
Reference: [Bol85] <author> B. Bollobas. </author> <title> Random Graphs. </title> <publisher> Academic Press, </publisher> <year> 1985. </year>
Reference-contexts: Appendix A. 2.4 Finding a good Horn Strengthening A "horn-strengthening" of the clause fl = f a 1 ; : : : ; a k ; 7 Notice d (T; Q) = 1 jQj 2Q d (T; ), for any theory T and any set of queries Q. 8 See <ref> [Bol85, p. 12] </ref>.
Reference: [Che52] <author> Herman Chernoff. </author> <title> A measure of asymptotic efficiency for tests of a hypothesis based on the sums of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23 </volume> <pages> 493-507, </pages> <year> 1952. </year>
Reference-contexts: Now let Y n = n i=1 = d (S fi ; f i g n i=1 ) be the sample mean of n samples. 7 This average will tend to the population mean as n ! 1; i.e., = lim n;1 Y n . Chernoff bounds <ref> [Che52] </ref> provide the probable rate of convergence: the probability that "Y n is more than + fi" goes to 0 exponentially fast as n increases; and, for a fixed n, exponentially as fi increases.
Reference: [Coo71] <author> Stephen A. Cook. </author> <title> The complexity of theorem-proving procedures. </title> <booktitle> In STOC71, </booktitle> <pages> pages 151-58, </pages> <year> 1971. </year>
Reference-contexts: PS ff (S; W; ) ? Y ? S j= - ffi fl 8 &gt; : N : : : N addressing j= ? tractable (assuming P 6= N P ) <ref> [Coo71, GJ79] </ref>. We describe a technique that "approximates" an arbitrary theory, transforming it into a representation that admits more efficient, if less categorical, reasoning [EBBK89].
Reference: [DB88] <author> Thomas Dean and Mark Boddy. </author> <title> An analysis of time-dependent planning. </title> <booktitle> In Proceedings of AAAI-88, </booktitle> <pages> pages 49-54, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The only part of this process that is not necessarily bounded by a polynomial is the number of it erations required. However, this is not necessarily problematic, as AdComp is essentially an anytime system <ref> [DB88] </ref>, returning successively better and better horn approximations. In fact, our AdComp can be viewed as a natural extension of the anytime compiler discussed in [SK91], as each system runs in parallel with a performance system that is using the current best approximation to return responses to the queries presented.
Reference: [DE92] <author> Mukesh Dalal and David Etherington. </author> <title> Tractable approximate deduction using limited vocabulary. </title> <booktitle> In Proceedings of CSCSI-92, </booktitle> <address> Vancouver, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: For example, [KS92] suggests a way of shrinking the size of some horn weakening by adding new vocabulary terms to the initial theory; it would be easy to also include transformations that implement this idea. Other recent papers, including <ref> [DE92] </ref>, propose other techniques for finding good (not necessarily horn) approximations. Extension 5. The set of T W transformations described in Subsection 2.5 will not always allow AdComp to explore the entire space of K-sized weakenings.
Reference: [DG84] <author> William F. Dowling and Jean H. Gallier. </author> <title> Linear time algorithms for testing the satisfiabil-ity of propositional horn formula. </title> <journal> Journal of Logic Programming, </journal> <volume> 3 </volume> <pages> 267-84, </pages> <year> 1984. </year>
Reference-contexts: Notice that these are the correct answers; i.e., W j= guarantees that j= , and S 6j= guarantees that 6j= . Moreover, these tests are efficient; in fact, linear in the size of S, W and <ref> [DG84] </ref>, provided : is Horn. 2 1 We call each such S a "Strengthening" of the initial theory, and each such W an "W eakening".
Reference: [EBBK89] <author> David W. Etherington, Alex Borgida, Ronald J. Brachman, and Henry Kautz. </author> <title> Vivid knowledge and tractable reasoning: Preliminary report. </title> <booktitle> In Proceedings of IJCAI-89, </booktitle> <pages> pages 1146-52, </pages> <year> 1989. </year>
Reference-contexts: We describe a technique that "approximates" an arbitrary theory, transforming it into a representation that admits more efficient, if less categorical, reasoning <ref> [EBBK89] </ref>.
Reference: [GE91] <author> Russell Greiner and Charles Elkan. </author> <title> Measuring and improving the effectiveness of representations. </title> <booktitle> In Proceedings of IJCAI-91, </booktitle> <pages> pages 518-24, </pages> <address> Sydney, Australia, </address> <month> August </month> <year> 1991. </year> <title> 13 This relies on the fact that the distribution of queries is stationary, meaning that, for any given pair of strength-enings S j and S 0 , the values of the random variables i = d(S j ; i )d(S 0 ; i ) are drawn from a stationary distribution. </title>
Reference-contexts: : or are these two equally bad? We can provide the user with greater flexibility by allowing him to specify his own scoring function, c i : Approx 1 () fi Q 7! &lt;, where c i (; ) indicates how well the approximation does at solving . 12 Following <ref> [GE91] </ref>, we allow this scoring function to be a combina tion of various factors, including accuracy, categoricity and efficiency. Given any such c i , our goal is to find the performance system (call it PS c i ) whose expected c i score is maximal.
Reference: [GJ79] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: PS ff (S; W; ) ? Y ? S j= - ffi fl 8 &gt; : N : : : N addressing j= ? tractable (assuming P 6= N P ) <ref> [Coo71, GJ79] </ref>. We describe a technique that "approximates" an arbitrary theory, transforming it into a representation that admits more efficient, if less categorical, reasoning [EBBK89].
Reference: [GJ92] <author> Russell Greiner and Igor Jurisica. </author> <title> A statistical approach to solving the EBL utility problem. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <address> San Jose, </address> <year> 1992. </year>
Reference-contexts: Extension 2. AdComp works in "batch" mode | using a collection of n j (Equation 1) samples to decide whether to iterate from S j to S j+1 or to stop improving the strengthening, and also whether to iterate from W j to W j+1 , etc. <ref> [GJ92] </ref> presents PALO, a related algorithm (but designed for a different task) that can make these decisions after each individual sample. PALO can potentially require fewer samples on each iteration than AdComp, as PALO will consider climbing to a (probabilistically) better element or terminating, after seeing each sample.
Reference: [GS92] <author> Russell Greiner and Dale Schuurmans. </author> <title> Learning useful horn approximations. </title> <type> Technical report, </type> <institution> Siemens Corporate Research, </institution> <year> 1992. </year>
Reference-contexts: The particular c i function specified will determine whether the optimal performance system should use ff = SN D or ff = GU E or ff = IDK, or possibly some other pre-defined ff function. (See <ref> [GS92] </ref>.) Future Work. First, we are currently implementing AdComp and plan to test it empirically in order to determine just how categorical and efficient it really is, both on real world problems and on "hard" cases [MSL92].
Reference: [KS92] <author> Henry Kautz and Bart Selman. </author> <title> Speeding inference by acquiring new concepts. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <address> San Jose, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: It is easy to see that this w s will have the largest possible D [ ] value over all weakenings, for any distribution. Unfortunately, it can also be exponentially larger than the original theory <ref> [KS92] </ref>. As mentioned above, we avoid this potential blowup by considering only weakenings of size at most K = K (), where K () is a user-supplied polynomial function. 9 Our goal, therefore, is to find the weakening of this size that is maximally categorical, over the distribution of queries. <p> We can consider using other transformations, especially when seeking an optimal weakening. For example, <ref> [KS92] </ref> suggests a way of shrinking the size of some horn weakening by adding new vocabulary terms to the initial theory; it would be easy to also include transformations that implement this idea. Other recent papers, including [DE92], propose other techniques for finding good (not necessarily horn) approximations. Extension 5.
Reference: [MSL92] <author> David Mitchell, Bart Selman, and Hector Levesque. </author> <title> Hard and easy distribution of sat problems. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <address> San Jose, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: First, we are currently implementing AdComp and plan to test it empirically in order to determine just how categorical and efficient it really is, both on real world problems and on "hard" cases <ref> [MSL92] </ref>. We also plan to compare AdComp with other theorem proving processes, including incomplete systems like gsat [SLM92]. We anticipate that these studies will give us insights on many of the issues mentioned above, including the different ways of handling the "plateau problem" mentioned in Extension 5.
Reference: [SK91] <author> Bart Selman and Henry Kautz. </author> <title> Knowledge compilation using horn approximations. </title> <booktitle> In Proceedings of AAAI-91, </booktitle> <pages> pages 904-09, </pages> <address> Ana-heim, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: We describe a technique that "approximates" an arbitrary theory, transforming it into a representation that admits more efficient, if less categorical, reasoning [EBBK89]. In particular, our work extends the "knowledge compilation" method of Selman and Kautz <ref> [SK91] </ref>: Given a general propositional theory , their compiler will compute a pair of "bracketing" Horn theories S and W , with the property that S j= j= W . 1 Figure 1 shows how the resulting "compiled system" PS = PS ff (S; W; ) uses these bracketing theories to <p> Of course, we want this problematic situation to occur rarely; we therefore prefer S and W theories that cover a maximal number of queries, as this means a minimal number of queries will fall through to the final ff stage. <ref> [SK91] </ref> suggests restricting S (resp., W ) to be a "weakest Strengthening", s w (resp., a "strongest Weakening", w s ), which are the obvious extrema: s w (; S) () 8 T [S j= T j= & Horn (T )] ) S T w s (; W ) () 8 <p> Issue 2: Intractable Compilation. The task of finding either extremum is intractable <ref> [SK91, p906] </ref>, meaning they cannot be found efficiently (if P 6= N P ). Issue 3: Multiple Strengthenings. <p> _ :d are fl 1 a _ :c _ :d and fl 2 b _ :c _ :d.) We can write = H [ N , where each element of H is a Horn clause, and each element of N = ffl i g m i=1 is a non-Horn clause. <ref> [SK91] </ref> proves that each weakest strengthening is of the form S o = H [ 0 N , where 0 g m i=1 such that each fl i 0 2 0 N is a horn-strengthening of some fl i 2 N . <p> It will terminate on reaching an S j which is an *-local optimum. (Notice this S j is not necessarily a weakest strengthening.) 2.5 Finding a good Horn Weakening <ref> [SK91] </ref> proves that there is a unique optimal weakening, w s , and presents the lub algorithm for computing it. Their algorithm is equivalent to InitialWN ( H ; N ; 1), using the process shown in the set of all horn implicates of the initial theory. <p> However, this is not necessarily problematic, as AdComp is essentially an anytime system [DB88], returning successively better and better horn approximations. In fact, our AdComp can be viewed as a natural extension of the anytime compiler discussed in <ref> [SK91] </ref>, as each system runs in parallel with a performance system that is using the current best approximation to return responses to the queries presented. <p> The final task is to extend these ideas from propositional logic to full first-order predicate calculus. 4 Conclusion It is often critical for an agent to use its knowledge to produce answers efficiently. Unfortunately, this task is intractable in general. <ref> [SK91] </ref> presents a way around this problem, describing an algorithm that produces a semantically similar Horn approximation from which many queries can be answered efficiently. Our paper extends that work by providing a more utilitarian objective function, one that prefers approximations that produce answers efficiently to most anticipated queries.
Reference: [SLM92] <author> Bart Selman, Hector Levesque, and David Mitchell. </author> <title> A new method for solving hard sat-isfiability problems. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <pages> pages 440-46, </pages> <address> San Jose, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: We also plan to compare AdComp with other theorem proving processes, including incomplete systems like gsat <ref> [SLM92] </ref>. We anticipate that these studies will give us insights on many of the issues mentioned above, including the different ways of handling the "plateau problem" mentioned in Extension 5.
References-found: 15

