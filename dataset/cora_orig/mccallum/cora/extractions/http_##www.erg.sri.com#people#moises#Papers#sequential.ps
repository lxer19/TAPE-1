URL: http://www.erg.sri.com/people/moises/Papers/sequential.ps
Refering-URL: http://www.erg.sri.com/projects/hpkb/
Root-URL: 
Email: nir@cs.berkeley.edu  moises@erg.sri.com  
Title: Sequential Update of Bayesian Network Structure  
Author: Nir Friedman Moises Goldszmidt 
Address: Berkeley, CA 94720  333 Ravenswood Ave, EK329 Menlo Park, CA 94025  
Affiliation: University of California Computer Science Division  SRI International  
Abstract: There is an obvious need for improving the performance and accuracy of a Bayesian network as new data is observed. Because of errors in model construction and changes in the dynamics of the domains, we cannot afford to ignore the information in new data. While sequential update of parameters for a fixed structure can be accomplished using standard techniques, sequential update of network structure is still an open problem. In this paper, we investigate sequential update of Bayesian networks were both parameters and structure are expected to change. We introduce a new approach that allows for the flexible manipulation of the tradeoff between the quality of the learned networks and the amount of information that is maintained about past observations. We formally describe our approach including the necessary modifications to the scoring functions for learning Bayesian networks, evaluate its effectiveness through and empirical study, and extend it to the case of missing data.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Beinlich, G. Suermondt, R. Chavez, and G. Cooper. </author> <title> The ALARM monitoring system. </title> <booktitle> In Euro. Conf. on AI and Medicine. </booktitle> <year> 1989. </year>
Reference-contexts: The former representation grows linearly with the number of instances collected, and will become infeasible when the network is expected to perform for long periods of time. A good example of such a network, is the alarm network <ref> [1] </ref> which is part of a system for monitoring of intensive care patients. In this example, the domain consists of 37 variables that can have 2 53:95 distinct instantiations. Clearly, we cannot store counts for each possible instantiation observed in the data. <p> This procedure uses the neighbor frontier set we described above. The datasets used in the experiments were generated from two networks: the alarm network of <ref> [1] </ref> and the insurance network of [13]. The alarm networks contains 37 variables, and the insurance network contains 26 variables. From each network we sampled 5 training sets, each consisting of 10,000 instances.
Reference: [2] <author> W. Buntine. </author> <title> Theory refinement on Bayesian networks. </title> <booktitle> In UAI '91. </booktitle>
Reference-contexts: We are currently in the process of evaluating the effectiveness of this procedure. 6 Discussion Previous work on the sequential update of Bayesian networks have been mostly restricted to updating the parameters assuming a fixed structure [15]. The two notable exceptions are the approaches by Buntine <ref> [2] </ref> and by Lam and Bacchus [10]. Buntine's method assumes that a total order on the variables is given, and it maintains sufficient statistics for the possible parents of each node using lattice structures.
Reference: [3] <author> G. F. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9, </volume> <year> 1992. </year>
Reference-contexts: The two main scoring functions commonly used to learn Bayesian networks are the Bayesian score <ref> [3, 8] </ref>, and the one based on the principle of minimal description length (MDL) [9, 5]. These scores are asymptotically equivalent as the sample size increases.
Reference: [4] <author> N. Friedman. </author> <title> Learning belief networks in the presence of missing values and hidden variables. </title> <note> In ML `97. </note>
Reference-contexts: Finally, we examine how to extend these methods to deal with incomplete data in sequential update. To this end, we propose a combination of two generalizations of the expectation maximization algorithm: incremental EM [12] and model-selection EM <ref> [4] </ref>. The rest of this paper is organized as follows: In Section 2, we briefly review the current practice of learning Bayesian networks. In Section 3, we describe our approach for incremental update and develop the necessary theoretical foundations. <p> The second restriction of the standard EM is that it deals only with learning parameters in a fixed structure. Fried-man <ref> [4] </ref> shows that if use the expected sufficient statistics to evaluate alternative structures using the MDL score and choose structures that are assigned a higher score than the current model, then we are bound to improve the marginal score of network with respect to the observed data. <p> Simi lar results apply to the Bayesian score as well. It follows then that we can use the expected sufficient statistics in our search procedure to evaluate new models. There is no intrinsic difficulty in casting the results of <ref> [4] </ref> in the incremental framework of [12]. Combining the two techniques we get a simple modification of our approach that deals with incomplete data: Set B to be initial network. Let F initial search frontier for B.
Reference: [5] <author> N. Friedman and M. Goldszmidt. </author> <title> Learning Bayesian networks with local structure. </title> <booktitle> In UAI '96. </booktitle>
Reference-contexts: The two main scoring functions commonly used to learn Bayesian networks are the Bayesian score [3, 8], and the one based on the principle of minimal description length (MDL) <ref> [9, 5] </ref>. These scores are asymptotically equivalent as the sample size increases. Furthermore they are both asymptotically correct: with probability equal to one the learned distribution converges to the underlying distribution as the number of samples increases [7, 6]. <p> These scores are asymptotically equivalent as the sample size increases. Furthermore they are both asymptotically correct: with probability equal to one the learned distribution converges to the underlying distribution as the number of samples increases [7, 6]. In this paper we use the MDL score described in <ref> [5] </ref>, which we denote as S MDL , and the BDe variant of the Bayesian introduced by Heckerman et. al [8], which we denote as S BDe . Details about these scores for batch learning can be found in [5, 8]. <p> In this paper we use the MDL score described in [5], which we denote as S MDL , and the BDe variant of the Bayesian introduced by Heckerman et. al [8], which we denote as S BDe . Details about these scores for batch learning can be found in <ref> [5, 8] </ref>. What is of interest for the purposes of this paper is to understand what information (from the training data) is needed to compute these scores. When the data is complete, namely, each instance assigns values to all the variables of interest, both scores have two attractive properties. <p> This proposal satisfies a basic correctness property, and furthermore, as our experimental results show, performs well in practice. Our proposal is best motivated in the MDL setting. This score can be casted in information theoretic terms <ref> [5, 9] </ref> as: S MDL (G j D) = N i + 2 i where H D (X i j pa (X i )) is the empirical conditional en tropy of X i given its parents, and is equal to x i ;pa (x i ) N N (x i ;
Reference: [6] <author> D. Geiger, D. Heckerman, and C. Meek. </author> <title> Asymptotic model selection for directed graphs with hidden variables. </title> <booktitle> In UAI '96. </booktitle>
Reference-contexts: These scores are asymptotically equivalent as the sample size increases. Furthermore they are both asymptotically correct: with probability equal to one the learned distribution converges to the underlying distribution as the number of samples increases <ref> [7, 6] </ref>. In this paper we use the MDL score described in [5], which we denote as S MDL , and the BDe variant of the Bayesian introduced by Heckerman et. al [8], which we denote as S BDe .
Reference: [7] <author> D. Heckerman. </author> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <year> 1995. </year>
Reference-contexts: 1 Introduction Recently, there has been a great deal of effort in developing methods for learning Bayesian networks from data for density estimation, data analysis, and pattern classification (see <ref> [7] </ref> for a tutorial and an overview). This body of work, which includes both theoretical and experimental results, has concentrated mostly on batch learning methods. In this setting, the total corpus of data is fully available to the learning algorithm which outputs a model after multiple inspections of the data. <p> These scores are asymptotically equivalent as the sample size increases. Furthermore they are both asymptotically correct: with probability equal to one the learned distribution converges to the underlying distribution as the number of samples increases <ref> [7, 6] </ref>. In this paper we use the MDL score described in [5], which we denote as S MDL , and the BDe variant of the Bayesian introduced by Heckerman et. al [8], which we denote as S BDe . <p> As it turns out, we the optimal choice of parameters X i jpa (X i ) is a function of N X i ;pa (X i ) ; see, for example, <ref> [7] </ref>. Since the selection of parameters has a closed form, we can focus on choosing the best structure G for the network. The parameters are then easily computed from these sufficient statistics. The second property is decomposability of the score as signed to a structure.
Reference: [8] <author> D. Heckerman, D. Geiger, and D. M. Chickering. </author> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20, </volume> <year> 1995. </year>
Reference-contexts: The two main scoring functions commonly used to learn Bayesian networks are the Bayesian score <ref> [3, 8] </ref>, and the one based on the principle of minimal description length (MDL) [9, 5]. These scores are asymptotically equivalent as the sample size increases. <p> In this paper we use the MDL score described in [5], which we denote as S MDL , and the BDe variant of the Bayesian introduced by Heckerman et. al <ref> [8] </ref>, which we denote as S BDe . Details about these scores for batch learning can be found in [5, 8]. What is of interest for the purposes of this paper is to understand what information (from the training data) is needed to compute these scores. <p> In this paper we use the MDL score described in [5], which we denote as S MDL , and the BDe variant of the Bayesian introduced by Heckerman et. al [8], which we denote as S BDe . Details about these scores for batch learning can be found in <ref> [5, 8] </ref>. What is of interest for the purposes of this paper is to understand what information (from the training data) is needed to compute these scores. When the data is complete, namely, each instance assigns values to all the variables of interest, both scores have two attractive properties. <p> This approach, however, is infeasible when we also attempt to update the structure of the network. The BDe score is based on assumptions that allow us to compactly represent a prior using a single network and an equivalent sample size <ref> [8] </ref>. Unfortunately, the posterior cannot be compactly represented. The conjugate form for this prior essentially requires storing a complete network, which is equivalent to storing the counts for all possible assignments to U. Since we cannot realize the exact Bayesian method, we can resort to the following approximation.
Reference: [9] <author> W. Lam and F. Bacchus. </author> <title> Learning Bayesian belief networks. An approach based on the MDL principle. </title> <journal> Comp. Int., </journal> <volume> 10, </volume> <year> 1994. </year>
Reference-contexts: The two main scoring functions commonly used to learn Bayesian networks are the Bayesian score [3, 8], and the one based on the principle of minimal description length (MDL) <ref> [9, 5] </ref>. These scores are asymptotically equivalent as the sample size increases. Furthermore they are both asymptotically correct: with probability equal to one the learned distribution converges to the underlying distribution as the number of samples increases [7, 6]. <p> This proposal satisfies a basic correctness property, and furthermore, as our experimental results show, performs well in practice. Our proposal is best motivated in the MDL setting. This score can be casted in information theoretic terms <ref> [5, 9] </ref> as: S MDL (G j D) = N i + 2 i where H D (X i j pa (X i )) is the empirical conditional en tropy of X i given its parents, and is equal to x i ;pa (x i ) N N (x i ;
Reference: [10] <author> W. Lam and F. Bacchus. </author> <title> Using new data to refine a Bayesian network. </title> <booktitle> In UAI '94. </booktitle>
Reference-contexts: This procedure is space efficient since we we only need to store the new instances that have been observed since we last performed the update of the MAP. An approach similar in spirit was proposed by Lam and Bacchus <ref> [10] </ref> in the context of the MDL score. Unfortunately, by using the MAP model as the prior for the next iteration of learning, we are loosing information, and are strongly biasing the learning process toward the MAP model itself. <p> The two notable exceptions are the approaches by Buntine [2] and by Lam and Bacchus <ref> [10] </ref>. Buntine's method assumes that a total order on the variables is given, and it maintains sufficient statistics for the possible parents of each node using lattice structures. He imposes restrictions on the size of the lattices in order to bound the amount of information that is maintained.
Reference: [11] <author> S. L. Lauritzen. </author> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Comp. Stat. and Data Analysis, </journal> <volume> 19, </volume> <year> 1995. </year>
Reference-contexts: Moreover, in order to evaluate the optimal choice of parameters for a given candidate network structure, we must perform a nonlinear optimization using either Expectation Maximization EM <ref> [11] </ref> or gradient descent [13]. In this paper we focus on the EM procedure. The standard use of EM is for batch learning. In addition, it is restricted to induce the parameters under the assumption of a fixed structure.
Reference: [12] <author> R. M. Neal and G. E. Hinton. </author> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <type> Unpublished manuscript, </type> <year> 1994. </year>
Reference-contexts: We empirically evaluate these extended scoring functions in conjunction with the incremental learning procedure. Finally, we examine how to extend these methods to deal with incomplete data in sequential update. To this end, we propose a combination of two generalizations of the expectation maximization algorithm: incremental EM <ref> [12] </ref> and model-selection EM [4]. The rest of this paper is organized as follows: In Section 2, we briefly review the current practice of learning Bayesian networks. In Section 3, we describe our approach for incremental update and develop the necessary theoretical foundations. <p> As described the procedure is a batch learning method in that we must retain all of the training set. Neal and Hin-ton <ref> [12] </ref> essentially show that we also improve this probability if we use an incremental update of the sufficient statistics. In their approach, new incoming data cases are used to continuously recompute the sufficient statistics. <p> Simi lar results apply to the Bayesian score as well. It follows then that we can use the expected sufficient statistics in our search procedure to evaluate new models. There is no intrinsic difficulty in casting the results of [4] in the incremental framework of <ref> [12] </ref>. Combining the two techniques we get a simple modification of our approach that deals with incomplete data: Set B to be initial network. Let F initial search frontier for B. Let S = Suff (B) [ S B 0 2F Suff (B 0 ).
Reference: [13] <author> S. Russell, J. Binder, D. Koller, and K. </author> <title> Kanazawa. Local learning in probabilistic networks with hidden variables. </title> <booktitle> In IJCAI '95. </booktitle>
Reference-contexts: This procedure uses the neighbor frontier set we described above. The datasets used in the experiments were generated from two networks: the alarm network of [1] and the insurance network of <ref> [13] </ref>. The alarm networks contains 37 variables, and the insurance network contains 26 variables. From each network we sampled 5 training sets, each consisting of 10,000 instances. The results reported in Figures 1 and 2 are averages over the results of running the algorithms on all 5 datasets. <p> Moreover, in order to evaluate the optimal choice of parameters for a given candidate network structure, we must perform a nonlinear optimization using either Expectation Maximization EM [11] or gradient descent <ref> [13] </ref>. In this paper we focus on the EM procedure. The standard use of EM is for batch learning. In addition, it is restricted to induce the parameters under the assumption of a fixed structure.
Reference: [14] <author> G. Schwarz. </author> <title> Estimating the dimension of a model. </title> <journal> Ann. of Stat., </journal> <volume> 6, </volume> <year> 1978. </year>
Reference-contexts: This modification is motivated by the asymptotic equivalence between the Bayesian score and the MDL score. A a general result by Schwarz <ref> [14] </ref> shows that S BDe (G j D) = S MDL (G j D) + O (1): Thus, by Lemma 3.1, the average Bayesian score is also asymptotically correct.
Reference: [15] <author> D. J. Spiegelhalter and S. L. Lauritzen. </author> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20, </volume> <year> 1990. </year>
Reference-contexts: If we make the assumption that the structure of the network is fixed and we use conjugate priors, we can efficiently represent the posterior and update it after each iteration using a closed-form formula <ref> [15] </ref>. This approach, however, is infeasible when we also attempt to update the structure of the network. The BDe score is based on assumptions that allow us to compactly represent a prior using a single network and an equivalent sample size [8]. Unfortunately, the posterior cannot be compactly represented. <p> Of course, we can run parallel executions of this procedure. We are currently in the process of evaluating the effectiveness of this procedure. 6 Discussion Previous work on the sequential update of Bayesian networks have been mostly restricted to updating the parameters assuming a fixed structure <ref> [15] </ref>. The two notable exceptions are the approaches by Buntine [2] and by Lam and Bacchus [10]. Buntine's method assumes that a total order on the variables is given, and it maintains sufficient statistics for the possible parents of each node using lattice structures.
References-found: 15

