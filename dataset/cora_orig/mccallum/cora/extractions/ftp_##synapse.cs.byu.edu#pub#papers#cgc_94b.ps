URL: ftp://synapse.cs.byu.edu/pub/papers/cgc_94b.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: AN INCREMENTAL LEARNING MODEL FOR COMMONSENSE REASONING  
Author: Christophe Giraud-Carrier Tony Martinez 
Address: Provo, UT 84602  Provo, UT 84602  
Affiliation: Department of Computer Science Brigham Young University  Department of Computer Science Brigham Young University  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brewka, G. </author> <title> Nonmonotonic Reasoning: Logical Foundations of Commonsense. </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: We follow the characterization of the brittleness problem given in [20]. Insightful (and more general) discussions of commonsense, its implications, its nature and its manifestations can be found in <ref> [1, 5, 12] </ref>. Before discussing reasoning, we first briefly address the problem of representation in the attribute-value language. Reasoning tasks are often expressed in English and have simple, direct counterparts in the classical first-order logic language (FOL).
Reference: [2] <author> Desjardins, M. </author> <title> Workshop Summary: AAAI'92: Workshop on Constraining Learning with Prior Knowledge. </title> <journal> SIGART Bulletin, </journal> <volume> Vol. 4, No. 3, </volume> <month> July </month> <year> 1993, </year> <pages> 11-13. </pages>
Reference-contexts: Rather, built-in mechanisms (e.g., pain), and social structures (e.g., the family, school) account for much of humans ability to efficiently learn complex problems. Much effort has recently been expended in understanding the sources and use of prior knowledge in learning <ref> [2] </ref>. One way to use prior knowledge is as an initial learning bias, whose purpose is both to prune the input space and to constrain the search for correct An Incremental Learning Model for Commonsense Reasoning generalizations.
Reference: [3] <author> Dietterich, T.G., and Michalski, </author> <title> R.S. A Comparative Review of Selected Methods for Learning from Examples. </title> <editor> In Michalski, R.S., Carbonell, J.G., and Mitchell, T.M., (Eds.). </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Tioga Publishing Company, </publisher> <address> Palo Alto, CA, </address> <year> 1983, </year> <note> Chapter 3. </note>
Reference-contexts: Section 4 contains preliminary results on several datasets and tasks. Finally, Section 5 concludes the paper. 2. PRECEPT-DRIVEN LEARNING Learning from examples is the process of inductively discovering general rules from exposure to specific examples. This mechanism has been thoroughly studied and several models have been proposed <ref> [3, 14, 16] </ref>. However useful, inductive learning does not take advantage of an important source of knowledge, namely prior knowledge. Human learning is not the sole result of exposure to random examples.
Reference: [4] <author> Elman, J.L. </author> <title> Incremental Learning, or the Importance of Starting Small. </title> <type> CRL Technical Report 9101, </type> <institution> La Jolla, CA: University of California, San Diego, Center for Research in Language, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Incrementality Assuming that all the knowledge about a certain task is available a priori amounts to programming. Though achievable in limited domains, this assumption often does not hold. Humans tend to learn over time. The need for such incremental learning in artificial systems is argued elsewhere <ref> [4, 8] </ref>. It is inherent to PDL2. Incrementality tends to reduce overall algorithmic complexity and provides a natural mechanism to incorporate chronology as a learning bias (e.g., priority given to newer rules as in [13]).
Reference: [5] <author> Forguson, L. </author> <title> Common Sense. </title> <publisher> Routledge, </publisher> <year> 1989. </year>
Reference-contexts: We follow the characterization of the brittleness problem given in [20]. Insightful (and more general) discussions of commonsense, its implications, its nature and its manifestations can be found in <ref> [1, 5, 12] </ref>. Before discussing reasoning, we first briefly address the problem of representation in the attribute-value language. Reasoning tasks are often expressed in English and have simple, direct counterparts in the classical first-order logic language (FOL).
Reference: [6] <author> Ginsberg, </author> <title> M.L. </title> <booktitle> (Ed.). Readings in Nonmonotonic Reasoning. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, CA, </address> <year> 1987. </year>
Reference-contexts: 1. INTRODUCTION Much effort has been devoted to understanding learning and reasoning in artificial intelligence <ref> [6, 19, 22] </ref>. However, very few models attempt to integrate these two complementary processes. Rather, there is a vast body of research in machine learning, often focusing on inductive learning from examples, quite isolated from the work on reasoning in artificial intelligence.
Reference: [7] <author> Giraud-Carrier, C., and Martinez, T.R. </author> <title> Using Precepts to Augment Training Set Learning. </title> <booktitle> In Proceedings of the First New Zealand International TwoStream Conference on Artificial Neural Networks and Expert Systems, </booktitle> <year> 1993, </year> <pages> 46-51. </pages>
Reference-contexts: In particular, it increases learning speed by pruning and constraining the search in the input space, reduces memory requirements, and improves overall predictive accuracy. The precept-driven learning algorithm (PDLA) discussed in <ref> [7] </ref> is a proof of concept for the above contention. The system presented here, called PDL2, is an extension of PDLA. In particular, PDL2 provides mechanisms to handle noise. It also retains exceptions to exceptions in the knowledge base, thus accounting for linear inheritance and cancellation of inheritance. <p> However, CONSYDERR does not support learning, and it is not clear how such a skill could be (easily) integrated in the model. The next section presents an overview of PDL2. Some of the more original aspects of the algorithm are then discussed. For more details, see <ref> [7] </ref>. 2.1. PDL2 - Algorithmic Overview Let A be an application having inputs i 1 n output o, with domains I 1 n , O, respectively. Then I=I ... I is the input space and O is the output space for A. <p> The question of what constitutes enough evidence is naturally the key to properly handling truly incorrect guesses. The danger is that information might be lost if a precept is mistakenly deleted. In <ref> [7] </ref>, enough evidence is defined as the existence of examples (or other precepts) contradicting the targeted precept on more than half of the input space it covers.
Reference: [8] <author> Giraud-Carrier, C., and Martinez, T.R. </author> <title> Seven Desirable Properties for Artificial Learning Systems. </title> <booktitle> In Proceedings of the Seventh Florida AI Research Symposium, </booktitle> <year> 1994, </year> <pages> 16-20. </pages>
Reference-contexts: Incrementality Assuming that all the knowledge about a certain task is available a priori amounts to programming. Though achievable in limited domains, this assumption often does not hold. Humans tend to learn over time. The need for such incremental learning in artificial systems is argued elsewhere <ref> [4, 8] </ref>. It is inherent to PDL2. Incrementality tends to reduce overall algorithmic complexity and provides a natural mechanism to incorporate chronology as a learning bias (e.g., priority given to newer rules as in [13]).
Reference: [9] <author> Hall, L.O., and Romaniuk, S.G. </author> <title> A Hybrid Connectionist, Symbolic Learning System. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <year> 1990, </year> <pages> 783-788. </pages>
Reference-contexts: Of interest is the fact that new knowledge may cause generalization of the existing knowledge base. Indeed, one of the main advantages of the proposed system over similar models, such as <ref> [9, 20] </ref> is its ability to generalize. Rules need not all be given a priori. Based on a set of training examples, the system inductively generalizes and synthesizes rules that can then be used in reasoning.
Reference: [10] <author> Horty, J.F., Thomason, R.H., and Touretzky, </author> <note> D.S. </note>
Reference-contexts: Because of the way it deals with exceptions, PDL2 handles both linear and treestructured inheritance, including exceptions to exceptions. 3.3. The Inheritance Problem The inheritance problem is one of the most challenging problems of reasoning. It has received much attention and various solutions have been proposed <ref> [10, 21, 23] </ref>. The combination of default (rule-based) reasoning and similarity-based reasoning in PDL2 provides a natural way (see [20], Appendix A) of handling the problem of inheritance.
References-found: 10

