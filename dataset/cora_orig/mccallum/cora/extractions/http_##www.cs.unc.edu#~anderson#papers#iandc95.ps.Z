URL: http://www.cs.unc.edu/~anderson/papers/iandc95.ps.Z
Refering-URL: http://www.cs.unc.edu/~anderson/papers.html
Root-URL: http://www.cs.unc.edu
Title: Time/Contention Trade-offs for Multiprocessor Synchronization  
Author: James H. Anderson Jae-Heon Yang 
Keyword: Concurrent programs, lower bounds, mutual exclusion, remote memory references, shared memory, time complexity.  
Date: May 1994 Revised May 1995  
Address: Chapel Hill, North Carolina 27599-3175  College Park, Maryland 20742-3255  
Affiliation: Department of Computer Science The University of North Carolina  Department of Computer Science The University of Maryland  
Abstract: We establish trade-offs between time complexity and write- and access-contention for solutions to the mutual exclusion problem. The write-contention (access-contention) of a concurrent program is the number of processes that may be simultaneously enabled to write (access by reading and/or writing) the same shared variable. Our notion of time complexity distinguishes between local and remote accesses of shared memory. We show that, for any N-process mutual exclusion algorithm, if write-contention is w, and if at most v remote variables can be accessed by a single atomic operation, then there exists an execution involving only one process in which that process executes (log vw N ) remote operations for entry into its critical section. We further show that, among these operations, ( log vw N) distinct remote variables are accessed. For algorithms with access-contention c, we show that the latter bound can be improved to (log vc N ). The last two of these bounds imply that a trade-off between contention and time complexity exists even if coherent caching techniques are employed. In most shared-memory multiprocessors, an atomic operation may access only a constant number of remote variables. In fact, most commonly-available synchronization primitives (e.g., read, write, test-and-set, load-and-store, compare-and-swap, and fetch-and-add) access only one remote variable. In this case, the first and the last of our bounds are asymptotically tight. Our results have a number of important implications regarding specific concurrent programming problems. For example, the time bounds that we establish apply not only to the mutual exclusion problem, but also to a class of decision problems that includes the leader-election problem. Also, because the execution that establishes these bounds involves only one process, it follows that "fast mutual exclusion" requires arbitrarily high write-contention. Although such conclusions are interesting in their own right, we believe that the most important contribution of our work is to identify a time complexity measure for asynchronous concurrent programs that strikes a balance between being conceptually simple and having a tangible connection to real performance. fl A preliminary version of this paper was presented at the 26th ACM Symposium on Theory of Computing [20]. Work supported, in part, by NSF Contracts CCR-9109497 and CCR-9216421, by the NASA Center for Excellence in Space Data and Information Sciences (CESDIS), and by an IBM Fund Award. p
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Alur and G. Taubenfeld, </author> <title> "Results about Fast Mutual Exclusion", </title> <booktitle> Proceedings of the Thirteenth IEEE Real-Time Systems Symposium, </booktitle> <month> December, </month> <year> 1992, </year> <pages> pp. 12-21. 25 </pages>
Reference-contexts: Consider the following computation. H 0 = H Y 3 ffi L (1) ffi L (2) ffi ffi L (jY 3j) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : :; [R jY 3j ; W jY 3j ; jY 3j]i We will use H 0 to construct the computation G mentioned at the beginning of the proof. <p> We show this below. Without loss of generality, assume the processes are numbered so that Z = f1; 2; : : :; jZjg. The compu tation G we seek is defined as follows. G = H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : :; [R jZj ; W jZj ; jZj]i Observe that, because H 0 satisfies (C2) through (C4), G also satisfies (C2) through (C4). We now show that G satisfies (C1). Condition (C1). <p> Hence, we conclude that G satisfies (C1). 2 To complete the proof of the lemma, we need to show that G is actually a computation in C. This is established in the following claim. Claim 1: G 2 C. Proof: The proof is by induction on the subsequence h <ref> [R 1 ; W 1 ; 1] </ref>; : : :; [R jZj ; W jZj ; jZj]i. Induction Base. We use Lemmas 2, 3, and 4 to establish the base case. Because H satisfies (C1), by Lemma 2, H Z 2 C. Consider j 2 Z. <p> By Lemma 4, it follows that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) 2 C. Induction Hypothesis. Assume that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : :; [R j1 ; W j1 ; j 1]i 2 C : (5) Induction Step. <p> We use (P2) to prove that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : :; [R j ; W j ; j]i 2 C. Because j 2 Z, the following holds. <p> (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : :; [R j ; W j ; j]i 2 C. Because j 2 Z, the following holds. H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; [R 2 ; W 2 ; 2]; : : :; [R j1 ; W j1 ; j 1]i [j] H ffi L (j) (6) Consider x in R j :var. <p> Because G satisfies (C1), x is not written by a process other than j in H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : :; [R j1 ; W j1 ; j 1]i. <p> Hence, we have the following. (8x : x 2 R j :var :: value (x; H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : :; [R j1 ; W j1 ; j 1]i = value (x; H ffi L (j))) (7) 12 By (1), (5), (6), (7), and (P2), we conclude that H Z ffi L (1) ffi L <p> 2 ; W 2 ; 2]; : : :; [R j1 ; W j1 ; j 1]i = value (x; H ffi L (j))) (7) 12 By (1), (5), (6), (7), and (P2), we conclude that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : :; [R j ; W j ; j]i 2 C. 2 By construction, each process in Z executes r + 1 remote events in G. As shown above, G satisfies conditions (C1) through (C4). <p> By (8), we have jZj d (n 1)=(2v + 1)vce. The computation G we seek is defined as follows. G = H Z ffi F (1) ffi F (2) ffi ffi F (jZj) ffi h <ref> [R 1 ; W 1 ; 1] </ref>; [R 2 ; W 2 ; 2]; : : :; [R jZj ; W jZj ; jZj]i Because H satisfies (C5), H also satisfies (C1). Thus, by Lemma 2, H Z 2 C. <p> It is interesting to note that there exist read/write mutual exclusion algorithms with write-contention N that have O (1) time complexity in the absence of 22 competition <ref> [1, 12, 19] </ref>. Thus, establishing the above-mentioned lower bound for read/write algorithms will require proof techniques that differ from those given in this paper. We do not know whether the bound given in Theorem 5 is tight.
Reference: [2] <author> J. Anderson, </author> <title> "A Fine-Grained Solution to the Mutual Exclusion Problem", </title> <journal> Acta Informatica, </journal> <volume> Vol. 30, No. 3, </volume> <year> 1993, </year> <pages> pp. 249-265. </pages>
Reference-contexts: Thus, we have the following corollary. Corollary 1: For any system S satisfying the conditions of Theorem 1, there exist (N ) processes i in P for which the conclusion of the theorem holds. 2 Similar corollaries apply to the theorems in the following sections. In <ref> [2] </ref>, a mutual exclusion algorithm requiring O (N ) remote memory references per critical section acquisition is given that employs only single-reader, single-writer variables. Thus, if v and k are taken to be positive constants, then the bound of Theorem 1 is asymptotically tight. <p> Consider the following computation. H 0 = H Y 3 ffi L (1) ffi L (2) ffi ffi L (jY 3j) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : :; [R jY 3j ; W jY 3j ; jY 3j]i We will use H 0 to construct the computation G mentioned at the beginning of the proof. <p> The compu tation G we seek is defined as follows. G = H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : :; [R jZj ; W jZj ; jZj]i Observe that, because H 0 satisfies (C2) through (C4), G also satisfies (C2) through (C4). We now show that G satisfies (C1). Condition (C1). <p> By Lemma 4, it follows that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) 2 C. Induction Hypothesis. Assume that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : :; [R j1 ; W j1 ; j 1]i 2 C : (5) Induction Step. <p> We use (P2) to prove that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : :; [R j ; W j ; j]i 2 C. Because j 2 Z, the following holds. <p> 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : :; [R j ; W j ; j]i 2 C. Because j 2 Z, the following holds. H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; [R 2 ; W 2 ; 2]; : : :; [R j1 ; W j1 ; j 1]i [j] H ffi L (j) (6) Consider x in R j :var. <p> Because G satisfies (C1), x is not written by a process other than j in H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : :; [R j1 ; W j1 ; j 1]i. <p> Hence, we have the following. (8x : x 2 R j :var :: value (x; H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : :; [R j1 ; W j1 ; j 1]i = value (x; H ffi L (j))) (7) 12 By (1), (5), (6), (7), and (P2), we conclude that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h <p> : :; [R j1 ; W j1 ; j 1]i = value (x; H ffi L (j))) (7) 12 By (1), (5), (6), (7), and (P2), we conclude that H Z ffi L (1) ffi L (2) ffi ffi L (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : :; [R j ; W j ; j]i 2 C. 2 By construction, each process in Z executes r + 1 remote events in G. As shown above, G satisfies conditions (C1) through (C4). <p> By (8), we have jZj d (n 1)=(2v + 1)vce. The computation G we seek is defined as follows. G = H Z ffi F (1) ffi F (2) ffi ffi F (jZj) ffi h [R 1 ; W 1 ; 1]; <ref> [R 2 ; W 2 ; 2] </ref>; : : :; [R jZj ; W jZj ; jZj]i Because H satisfies (C5), H also satisfies (C1). Thus, by Lemma 2, H Z 2 C. It is straightforward to use this fact to prove that G 2 C.
Reference: [3] <author> T. Anderson, </author> <title> "The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors", </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Vol. 1, No. 1, </volume> <month> January, </month> <year> 1990, </year> <pages> pp. 6-16. </pages>
Reference-contexts: In a recent paper, we proposed a time measure for concurrent programs that distinguishes between local and remote accesses of shared memory [19]. This measure is motivated by recent work on scalable synchronization constructs <ref> [3, 8, 15] </ref>. Informally, a shared variable access is local if does not require a traversal of the global interconnect between processors and shared memory, and is remote otherwise. Although the notion of a locally accessible shared variable may seem counterintuitive, there are two mainstream architectural paradigms that support it. <p> First, it is conceptually simple. In fact, this measure is a natural descendent of the standard time complexity measure used in sequential programming. Second, this measure has a tangible connection with real performance, as demonstrated by a number of recently-published performance studies of synchronization algorithms <ref> [3, 8, 15, 19] </ref>. In each of these studies, those algorithms that minimize remote memory references exhibited the best performance under contention. <p> Using our time measure, all solutions to such problems in which processes busy-wait on remote variables are deemed as being equally "bad" | all have unbounded time complexity. 2 among processes <ref> [3, 10, 11, 17] </ref>. Performance problems associated with high access-contention can be partially alleviated by employing coherent caching techniques to reduce concurrent reads of the same memory location. However, even when such techniques are employed, limiting write-contention is still an important concern.
Reference: [4] <author> J. Burns and N. Lynch, </author> <title> "Bounds on Shared Memory for Mutual Exclusion", </title> <journal> Information and Computation, </journal> <volume> Vol. 107, </volume> <year> 1993, </year> <pages> pp. 171-184. </pages>
Reference-contexts: In this paper, we consider bounds on time for mutual exclusion, a subject that has received scant attention in the literature. Past work on the complexity of mutual exclusion has almost exclusively focused on space requirements <ref> [4] </ref>; the limited work on time bounds that has been done has focused on partially synchronous models [14]. The lack of prior work on time bounds for mutual exclusion within asynchronous models is probably due to difficulties associated with measuring the time spent within busy-waiting constructs.
Reference: [5] <author> K. Chandy and J. Misra, </author> <title> "How Processes Learn", </title> <journal> Distributed Computing, </journal> <volume> Vol. 1, No. 1, </volume> <year> 1986, </year> <pages> pp. 40-52. </pages>
Reference-contexts: The above-mentioned time bounds are then established in Sections 4 and 5. Concluding remarks appear in Section 6. 3 2 Shared-Memory Systems Our model of a shared-memory system is similar to that given by Merritt and Taubenfeld in [16]; much of our notation is borrowed from Chandy and Misra <ref> [5] </ref>. A system S = (C; P; V ) consists of a set of computations C, a set of processes P = f1; 2; : : :; N g, and a set of variables V . A computation is a finite sequence of events.
Reference: [6] <author> E. Dijkstra, </author> <title> "Solution of a Problem in Concurrent Programming Control", </title> <journal> Communications of the ACM , Vol. </journal> <volume> 8, No. 9, </volume> <year> 1965, </year> <pages> pp. 569. </pages>
Reference-contexts: 1 Introduction The mutual exclusion problem is a fundamental paradigm for coordinating accesses to shared data on asynchronous shared-memory multiprocessing systems <ref> [6] </ref>. In this problem, accesses to shared data are abstracted as "critical sections" of code, and it is required that at most one process executes its critical section at any time.
Reference: [7] <author> C. Dwork, M. Herlihy, and O. Waarts, </author> <title> "Contention in Shared Memory Algorithms", </title> <booktitle> Proceedings of the 25th ACM Symposium on Theory of Computing , May, </booktitle> <year> 1993, </year> <pages> pp. 174-183. </pages>
Reference-contexts: For algorithms with access-contention c, the latter bound is improved to (log c N ). It can be shown that the first and last of these bounds are asymptotically tight. Related work includes previous research by Dwork et al. given in <ref> [7] </ref>, where it is shown that solving mutual exclusion with access-contention c requires ((log 2 N )=c) memory references. Our work extends that of Dwork et al. in several directions.
Reference: [8] <author> G. Graunke and S. Thakkar, </author> <title> "Synchronization Algorithms for Shared-Memory Multiprocessors", </title> <journal> IEEE Computer, </journal> <volume> Vol. 23, No. 6, </volume> <month> June </month> <year> 1990, </year> <pages> pp. 60-69. </pages>
Reference-contexts: In a recent paper, we proposed a time measure for concurrent programs that distinguishes between local and remote accesses of shared memory [19]. This measure is motivated by recent work on scalable synchronization constructs <ref> [3, 8, 15] </ref>. Informally, a shared variable access is local if does not require a traversal of the global interconnect between processors and shared memory, and is remote otherwise. Although the notion of a locally accessible shared variable may seem counterintuitive, there are two mainstream architectural paradigms that support it. <p> First, it is conceptually simple. In fact, this measure is a natural descendent of the standard time complexity measure used in sequential programming. Second, this measure has a tangible connection with real performance, as demonstrated by a number of recently-published performance studies of synchronization algorithms <ref> [3, 8, 15, 19] </ref>. In each of these studies, those algorithms that minimize remote memory references exhibited the best performance under contention.
Reference: [9] <author> M. Herlihy, </author> <title> "Wait-Free Synchronization", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 13, No. 1, </volume> <year> 1991, </year> <pages> pp. 124-149. </pages>
Reference-contexts: Our bounds imply that tradeoffs exist between contention and time complexity and between atomicity and time complexity in any multiprocessor setting, even if blocking is used for synchronization within a processor. For wait-free algorithms, Herlihy has characterized synchronization primitives by consensus number <ref> [9] </ref>. Such a characterization is not applicable when waiting is introduced. One way of determining the power of synchronization primitives in this case is to compare the time complexity of mutual exclusion using such primitives.
Reference: [10] <author> M. Herlihy, B-H. Lim, and N. Shavit, </author> <title> "Low Contention Load Balancing on Large-Scale Multiprocessors", </title> <booktitle> Proceedings of the 3rd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July, </month> <year> 1992, </year> <pages> pp. 219-227. </pages>
Reference-contexts: Using our time measure, all solutions to such problems in which processes busy-wait on remote variables are deemed as being equally "bad" | all have unbounded time complexity. 2 among processes <ref> [3, 10, 11, 17] </ref>. Performance problems associated with high access-contention can be partially alleviated by employing coherent caching techniques to reduce concurrent reads of the same memory location. However, even when such techniques are employed, limiting write-contention is still an important concern.
Reference: [11] <author> M. Herlihy, N. Shavit, and O. Waarts, </author> <title> "Low Contention Linearizable Counting", </title> <booktitle> Proceedings of the 32nd IEEE Symposium on Foundations of Computer Science, </booktitle> <month> October, </month> <year> 1991, </year> <pages> pp. 526-535. </pages>
Reference-contexts: Using our time measure, all solutions to such problems in which processes busy-wait on remote variables are deemed as being equally "bad" | all have unbounded time complexity. 2 among processes <ref> [3, 10, 11, 17] </ref>. Performance problems associated with high access-contention can be partially alleviated by employing coherent caching techniques to reduce concurrent reads of the same memory location. However, even when such techniques are employed, limiting write-contention is still an important concern.
Reference: [12] <author> L. Lamport, </author> <title> "A Fast Mutual Exclusion Algorithm", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 5, No. 1, </volume> <month> February, </month> <year> 1987, </year> <pages> pp. 1-11. </pages>
Reference-contexts: Also, because the execution that establishes these bounds involves only one process, it follows that so-called fast mutual exclusion algorithms | i.e., algorithms that require a process to execute only a constant number of remote memory references in the absence of competition <ref> [12] </ref> | require arbitrarily high write-contention in the worst case. These bounds apply not only to the mutual exclusion problem, but also to a class of decision problems that includes the leader-election problem. In most shared-memory multiprocessors, an atomic operation may access only a constant number of remote variables. <p> It is interesting to note that there exist read/write mutual exclusion algorithms with write-contention N that have O (1) time complexity in the absence of 22 competition <ref> [1, 12, 19] </ref>. Thus, establishing the above-mentioned lower bound for read/write algorithms will require proof techniques that differ from those given in this paper. We do not know whether the bound given in Theorem 5 is tight.
Reference: [13] <author> B.-H. Lim and A. Agarwal, </author> <title> "Waiting Algorithms for Synchronization in Large-Scale Multiprocessors", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 11, No. 3, </volume> <month> August, </month> <year> 1993, </year> <pages> pp. 253-294. </pages>
Reference-contexts: Although the time bounds we establish are oriented towards programs that busy-wait, they also have implications regarding mutual exclusion mechanisms that are based on blocking. In particular, while blocking can be used to synchronize multiple processes on a single processor, busy-waiting is still fundamental for synchronization across processors <ref> [13] </ref>. Our bounds imply that tradeoffs exist between contention and time complexity and between atomicity and time complexity in any multiprocessor setting, even if blocking is used for synchronization within a processor. For wait-free algorithms, Herlihy has characterized synchronization primitives by consensus number [9].
Reference: [14] <author> N. Lynch and N. Shavit, </author> <title> "Timing-Based Mutual Exclusion", </title> <booktitle> Proceedings of the Thirteenth IEEE Real-Time Systems Symposium, </booktitle> <month> December, </month> <year> 1992, </year> <pages> pp. 2-11. </pages>
Reference-contexts: Past work on the complexity of mutual exclusion has almost exclusively focused on space requirements [4]; the limited work on time bounds that has been done has focused on partially synchronous models <ref> [14] </ref>. The lack of prior work on time bounds for mutual exclusion within asynchronous models is probably due to difficulties associated with measuring the time spent within busy-waiting constructs.
Reference: [15] <author> J. Mellor-Crummey and M. Scott, </author> <title> "Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 9, No. 1, </volume> <month> February, </month> <year> 1991, </year> <pages> pp. 21-65. </pages>
Reference-contexts: In a recent paper, we proposed a time measure for concurrent programs that distinguishes between local and remote accesses of shared memory [19]. This measure is motivated by recent work on scalable synchronization constructs <ref> [3, 8, 15] </ref>. Informally, a shared variable access is local if does not require a traversal of the global interconnect between processors and shared memory, and is remote otherwise. Although the notion of a locally accessible shared variable may seem counterintuitive, there are two mainstream architectural paradigms that support it. <p> First, it is conceptually simple. In fact, this measure is a natural descendent of the standard time complexity measure used in sequential programming. Second, this measure has a tangible connection with real performance, as demonstrated by a number of recently-published performance studies of synchronization algorithms <ref> [3, 8, 15, 19] </ref>. In each of these studies, those algorithms that minimize remote memory references exhibited the best performance under contention. <p> If v is taken to be a positive constant, then we can further show that the lower bound of Theorem 3 is asymptotically tight for solutions to the mutual exclusion problem for any value of w. In particular, an algorithm by Mellor-Crummey and Scott given in <ref> [15] </ref> solves the mutual exclusion problem for w processes, in O (1) time, with access-contention (and hence write-contention) w. By applying this solution within a balanced w-ary tree with N leaves, it is possible to obtain an N -process fi (log w N ) mutual exclusion algorithm with access-contention w.
Reference: [16] <author> M. Merritt and G. Taubenfeld, </author> <title> "Knowledge in Shared Memory Systems", </title> <booktitle> Proceedings of the Tenth ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August, </month> <year> 1991, </year> <pages> pp. 189-200. </pages>
Reference-contexts: The above-mentioned time bounds are then established in Sections 4 and 5. Concluding remarks appear in Section 6. 3 2 Shared-Memory Systems Our model of a shared-memory system is similar to that given by Merritt and Taubenfeld in <ref> [16] </ref>; much of our notation is borrowed from Chandy and Misra [5]. A system S = (C; P; V ) consists of a set of computations C, a set of processes P = f1; 2; : : :; N g, and a set of variables V .
Reference: [17] <author> G. Pfister and A. Norton, </author> <title> "Hot Spot Contention and Combining in Multistage Interconnection Networks", </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-34, No. 11, </volume> <month> November, </month> <year> 1985, </year> <pages> pp. 943-948. </pages>
Reference-contexts: Using our time measure, all solutions to such problems in which processes busy-wait on remote variables are deemed as being equally "bad" | all have unbounded time complexity. 2 among processes <ref> [3, 10, 11, 17] </ref>. Performance problems associated with high access-contention can be partially alleviated by employing coherent caching techniques to reduce concurrent reads of the same memory location. However, even when such techniques are employed, limiting write-contention is still an important concern.
Reference: [18] <author> P. Turan, </author> <title> "On an extremal problem in graph theory" (in Hungarian), </title> <journal> Mat. Fiz. </journal> <volume> Lapok , Vol. 48, </volume> <year> 1941, </year> <pages> pp. 436-452. </pages>
Reference-contexts: Proof: If there are two processes that do not have a remote event after H, then we can extend H by executing those processes and violate the Exclusion requirement. A formal proof is presented in the appendix. 2 The next theorem by Turan <ref> [18] </ref> will be used in subsequent lemmas. 9 Theorem 2 (Turan): Let G = hV; Ei be an undirected multigraph, 4 where V is a set of vertices and E is a set of edges.
Reference: [19] <author> J.-H. Yang and J. Anderson, </author> <title> "Fast, Scalable Synchronization with Minimal Hardware Support", </title> <booktitle> Pro--ceedings of the Twelfth ACM Symposium on Principles of Distributed Computing, </booktitle> <month> August, </month> <year> 1993, </year> <pages> pp. </pages> <month> 171-182. </month> <title> A revised version entitled "A Fast, Scalable Mutual Exclusion Algorithm" is scheduled to appear in Distributed Computing </title> . 
Reference-contexts: In other words, the standard sequential programming metric yields no useful information concerning the performance of such algorithms under contention. In a recent paper, we proposed a time measure for concurrent programs that distinguishes between local and remote accesses of shared memory <ref> [19] </ref>. This measure is motivated by recent work on scalable synchronization constructs [3, 8, 15]. Informally, a shared variable access is local if does not require a traversal of the global interconnect between processors and shared memory, and is remote otherwise. <p> First, it is conceptually simple. In fact, this measure is a natural descendent of the standard time complexity measure used in sequential programming. Second, this measure has a tangible connection with real performance, as demonstrated by a number of recently-published performance studies of synchronization algorithms <ref> [3, 8, 15, 19] </ref>. In each of these studies, those algorithms that minimize remote memory references exhibited the best performance under contention. <p> Note that Mellor-Crummey and Scott's algorithm uses load-and-store and compare-and-swap. Even with weaker atomic operations, logarithmic behavior can be achieved. In particular, an N -process fi (log 2 N ) mutual exclusion algorithm based on read/write atomicity has been given previously by us in <ref> [19] </ref>. <p> For instance, it is possible to solve the mutual exclusion problem with O (1) time complexity using load-and-store or fetch-and-add, while the best-known upper bound for read/write algorithms is O (log 2 N ) <ref> [19] </ref>. If a lower-bound result could be proved showing that this gap is fundamental, then this would establish that reads and writes are weaker than read-modify-writes from a performance standpoint. <p> It is interesting to note that there exist read/write mutual exclusion algorithms with write-contention N that have O (1) time complexity in the absence of 22 competition <ref> [1, 12, 19] </ref>. Thus, establishing the above-mentioned lower bound for read/write algorithms will require proof techniques that differ from those given in this paper. We do not know whether the bound given in Theorem 5 is tight. <p> We do not know whether the bound given in Theorem 5 is tight. We conjecture that this bound can be improved to (log vw N ), which has a matching algorithm when v is taken to be a constant <ref> [19] </ref>. One may be interested in determining the effect of contention on space requirements. It is quite easy to show that solving the minimal mutual exclusion problem with write-contention w requires at least N=w variables. In particular, it can be shown that every process writes a variable before eating.
Reference: [20] <author> J.-H. Yang and J. Anderson, </author> <title> "Time Bounds for Mutual Exclusion and Related Problems", </title> <booktitle> Proceedings of the 26th Annual ACM Symposium on Theory of Computing, ACM, </booktitle> <address> New York, </address> <pages> pp. 224-233, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: We would like to thank Faith Fich and Samir Khuller for informing us of Turan's theorem. By using this theorem, we were able to obtain slightly better bounds than we originally reported in <ref> [20] </ref>. We would also like to thank Sanglyul Min for his helpful comments on an earlier draft of this paper. Appendix: Additional Proofs In this section, we give full proofs of Lemmas 2, 3, 4, and 5.
References-found: 20

