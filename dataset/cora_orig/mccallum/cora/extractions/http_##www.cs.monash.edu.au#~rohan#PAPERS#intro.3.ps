URL: http://www.cs.monash.edu.au/~rohan/PAPERS/intro.3.ps
Refering-URL: http://www.cs.monash.edu.au/~rohan/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: (rohan@cs.monash.edu.au)  (jono@cs.monash.edu.au)  
Title: MDL and MML Similarities and Differences (Introduction to Minimum Encoding Inference Part III)  
Author: Rohan Baxter and Jonathan Oliver . ROHAN A. BAXTER JONATHAN J. OLIVER 
Address: Clayton, Victoria, 3168, AUSTRALIA  
Affiliation: Computer Science Department Monash University  
Note: (C) Copyright  
Date: March 1995  
Pubnum: Amended-  
Abstract: Tech Report 207 Department of Computer Science, Monash University, Clayton, Vic. 3168, Australia Abstract: This paper continues the introduction to minimum encoding inductive inference given by Oliver and Hand. This series of papers was written with the objective of providing an introduction to this area for statisticians. We describe the message length estimates used in Wallace's Minimum Message Length (MML) inference and Rissanen's Minimum Description Length (MDL) inference. The differences in the message length estimates of the two approaches are explained. The implications of these differences for applications are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: [AWY92] <author> L. Allison, C.S. Wallace, and C.N. Yee. </author> <title> Finite-state models in the alignment of macromolecules. </title> <journal> Journal of Molecular Evolution, </journal> <volume> 35 </volume> <pages> 77-89, </pages> <year> 1992. </year>
Reference-contexts: Computationally and cognitively it is often simpler to base prediction on a single model. More accurate prediction can also be done using a weighted combination of predictions from more than one model. The weights assigned to models are based on their message lengths <ref> [AWY92, OH96] </ref>. In both MDL and MML prediction, if we need to make a single-valued guess at the value of x, we use a loss function, l (^x; x). The loss function expresses our judgement of how bad it is to guess ^x when the real value is x.
Reference: [BC91] <author> A.R. </author> <title> Barron and T.M. Cover. Minimum complexity density estimation. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> 37 </volume> <pages> 1034-1054, </pages> <year> 1991. </year>
Reference-contexts: In a formal Bayesian setting, the prior is assigned before seeing the data. In this case, even with an extreme prior which assigns probability * to the true model, the MML procedure will recover the true model given sufficient data <ref> [BC91] </ref>. The MDL and MML treatment of priors is discussed in more detail in Dowe [Dow95]. Leaving aside the issue of the prior coming before the data, one essential difference between MDL and MML is the willingness to risk the scenario of an extreme prior, rather than lack of invariance.
Reference: [BO95] <author> R. Baxter and J. Oliver. </author> <title> Universal codes in MDL inductive inference. </title> <note> Submitted to COLT-95, </note> <year> 1995. </year>
Reference-contexts: Two part messages are used in MDL as approximations to the stochastic complexity expression of Equation (3), where better one part estimates are not available. Recently, algorithmic complexity theorists, Li and Vitanyi [LV93] have justified two part MDL messages as feasible approximations to Solmonoff inductive inference| see <ref> [BO95] </ref> for further discussion of this point. MDL does not employ a prior probability distribution (or density) on parameter values. We now examine how MDL uses universal codes to estimate the optimal model code length.
Reference: [Bun94] <author> W. Buntine. </author> <title> Internet:Machine Learning List, </title> <type> 6(27), </type> <year> 1994. </year>
Reference: [CS88] <author> J.H. Conway and N.J.A. Sloane. </author> <title> Sphere Packings, Lattices and Groups. </title> <address> Springer-Verlag,New York, </address> <year> 1988. </year>
Reference-contexts: 2 P rob () + 2 k log 2 k + 2 where F () = E ( @ 2 log e P rob (Dj) @ 2 ) is the Fisher information matrix and k is the optimal k dimensional quantising lattice constant (known values and bounds are given in <ref> [CS88, pages 59-61] </ref>). The derivation of the M M L (W F ) AOPV differs from the M M L (OH) AOPV in the following respect. M M L (OH) minimizes the AOPV for each parameter separately.
Reference: [Dow95] <author> D.L. Dowe. </author> <title> Priors and information-theoretic inductive inference: MML and MDL (in preparation). </title> <type> Technical report, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, AUSTRALIA, </address> <year> 1995. </year>
Reference-contexts: In this case, even with an extreme prior which assigns probability * to the true model, the MML procedure will recover the true model given sufficient data [BC91]. The MDL and MML treatment of priors is discussed in more detail in Dowe <ref> [Dow95] </ref>. Leaving aside the issue of the prior coming before the data, one essential difference between MDL and MML is the willingness to risk the scenario of an extreme prior, rather than lack of invariance.
Reference: [EH81] <author> B.S. </author> <title> Everitt and D.J. Hand. Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: The loss function expresses our judgement of how bad it is to guess ^x when the real value is x. Consider the problem of estimating a density function which consists of a mixture of normal distributions <ref> [EH81] </ref>. We may consider each set of mixture distributions with the same number of components as a model class. The model class whose predictive code has the minimum length may not necessarily contain the `best' single model.
Reference: [Eli75] <author> P. Elias. </author> <title> Universal codeword sets and representations of the integers. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> 21 </volume> <pages> 194-203, </pages> <year> 1975. </year>
Reference-contexts: The log fl function is defined as log fl (n) = log 2 (n) + log 2 log 2 (n) + ::::, including only positive terms. The log fl code encodes positive integers n with code length CodeLength (n) = log fl (n) + log (c), where c 2:8665064 <ref> [Eli75, Ris83] </ref>.
Reference: [Goo85] <author> I.J. </author> <title> Good. Weight of evidence: a brief survey. </title> <editor> In J.M. Bernado et al., editors, </editor> <booktitle> Bayesian Statistics 2, </booktitle> <pages> pages 249-269. </pages> <publisher> Elsevier, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: The stochastic complexity (SC) of data D, relative to the model class of Equation (2) is [Ris89, page 59]: SC = log P rob (D) (3) P rob (D) is also called the evidence <ref> [Goo85] </ref> and the marginal likelihood [KR93]. Approximations to Equation (2) are the basis of the Bayesian inference methods of Schwarz [Sch78] and MacKay [Mac92]. These approaches are discussed in Oliver and Baxter [OB94].
Reference: [Jef61] <author> H. Jeffreys. </author> <title> Theory of Probability. </title> <address> Cambridge, </address> <year> 1961. </year>
Reference-contexts: The code length for is then approximately: CodeLength () log fl ( ) + log 2 2 (6) 3.1.3 Parameter Transformations A separate difficulty is that the above mappings are not invariant under linear and nonlinear parameter transformations. Such invariance is an important property of an inductive inference technique <ref> [Jef61, page 391] </ref>. If, in the above example, we instead choose transmit (x; exp y) (i.e., the pH and the temperature) then the message length estimate changes if we use the mappings described above to code x and exp (y) with log fl.
Reference: [KR93] <author> R.E. Kass and A.E. Raftery. </author> <title> Bayes factors and model uncertainty. </title> <type> Technical Report 571, </type> <institution> Dept. of Statistics, Carnegie-Mellon University, </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The stochastic complexity (SC) of data D, relative to the model class of Equation (2) is [Ris89, page 59]: SC = log P rob (D) (3) P rob (D) is also called the evidence [Goo85] and the marginal likelihood <ref> [KR93] </ref>. Approximations to Equation (2) are the basis of the Bayesian inference methods of Schwarz [Sch78] and MacKay [Mac92]. These approaches are discussed in Oliver and Baxter [OB94].
Reference: [LV93] <author> Ming Li and Paul Vitanyi. </author> <title> An Introduction to Kolmogorov Complexity and its applications. </title> <publisher> Springer Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: P rob () is interpreted as subjective probabilities. Hence MML selects P rob () according to prior information, or, equivalently, to maximise its expected coding efficiency. The MDL literature is characterised by a lack of acceptance of subjective probabilities <ref> [Ris83, Ris87, Ris89, LV93, Ris93b] </ref>: "In our view the parameter is generated by our selecting the model class, and it has no other `inherent' meaning"[Ris89, page 54]. "In the MDL principle for statistical inference there is no need for the awkward Bayesian interpretations of the meaning of the prior probability on <p> Two part messages are used in MDL as approximations to the stochastic complexity expression of Equation (3), where better one part estimates are not available. Recently, algorithmic complexity theorists, Li and Vitanyi <ref> [LV93] </ref> have justified two part MDL messages as feasible approximations to Solmonoff inductive inference| see [BO95] for further discussion of this point. MDL does not employ a prior probability distribution (or density) on parameter values. We now examine how MDL uses universal codes to estimate the optimal model code length. <p> MDL does not employ a prior probability distribution (or density) on parameter values. We now examine how MDL uses universal codes to estimate the optimal model code length. We now describe the Universal Code MDL (UC-MDL) procedure <ref> [Ris83, Ris89, LV93] </ref>: UC-MDL inductive inference procedure: 1. Partition the hypothesis space into regions with centres i ; i 2 fN g and volumes AOP V ( i ), where AOPV stands for Accuracy of Parameter Value [OH94, Section 3]. <p> An obvious way is the following: Mapping 1: Mapped Integer : 1 2 3 4 5 6 7 8 9 10 ... Parameter Value: 0 1 -1 2 -2 3 -3 4 -4 5 ... 5 This approach is similar to that advocated by Li and Vitanyi <ref> [LV93, page 310] </ref> and Rissanen [Ris89]. 9 This mapping onto the log fl code implies a probability distribution where positive integers are equally likely to negative integers a priori. If we believe a priori this is not the case, we could choose a different mapping to reflect this. <p> For example, for classification trees [QR89], the null tree is the obvious model to be mapped onto the origin. The choice of origin and a mapping involves the same issues involved in the choice of prior probability distribution on hypotheses. This conclusion differs from that of Li and Vitanyi <ref> [LV93, page 311] </ref> who state: "its genesis [the log fl universal code] shows that it expresses no prior knowledge about the true value of the parameter." Rissanen has considered the origin problem [Ris83, page 427] "Does this invalidate the whole process? We think not.
Reference: [Mac92] <author> David J.C. MacKay. </author> <title> Bayesian Modeling and Neural Networks. </title> <type> PhD thesis, </type> <institution> Dept. of Computation and Neural Systems, CalTech, </institution> <year> 1992. </year> <month> 25 </month>
Reference-contexts: A typical example is the following passage from MacKay "Although some of the earliest work on complex model comparison involved the MDL framework [PW82], MDL has no apparent advantages, and in my work I approximate the evidence directly" <ref> [Mac92, page 17] </ref>. Here MacKay has classified the MML results of Patrick and Wallace as MDL. <p> Approximations to Equation (2) are the basis of the Bayesian inference methods of Schwarz [Sch78] and MacKay <ref> [Mac92] </ref>. These approaches are discussed in Oliver and Baxter [OB94]. If a single model within a model class needs to be chosen, then a non-coding principle, such as the maximum likelihood estimator, is used.
Reference: [OB94] <author> J.J. Oliver and R.A. Baxter. </author> <title> MML and Bayesianism: similarities and differences. </title> <type> Technical report TR 206, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, AUSTRALIA, </address> <year> 1994. </year>
Reference-contexts: Approximations to Equation (2) are the basis of the Bayesian inference methods of Schwarz [Sch78] and MacKay [Mac92]. These approaches are discussed in Oliver and Baxter <ref> [OB94] </ref>. If a single model within a model class needs to be chosen, then a non-coding principle, such as the maximum likelihood estimator, is used.
Reference: [OH94] <author> J.J. Oliver and D.J. </author> <title> Hand. Introduction to minimum encoding inference. </title> <type> Technical report TR 4-94, </type> <institution> Dept. of Statistics, Open University, Walton Hall, Milton Keynes, MK7 6AA, UK, </institution> <year> 1994. </year> <note> Also available as TR 205 Dept. </note> <institution> Computer Science, Monash Uni., Clayton, </institution> <address> Vic 3168 AUSTRALIA. </address>
Reference-contexts: The term code length is used to mean the length of a message which describes a single datum or parameter. A code length may be fractional| see Oliver and Hand <ref> [OH94, Section 2.8] </ref> for details. In this paper we consider message length estimates for sufficiently regular models 1 and identically and independently distributed data. <p> We now describe the Universal Code MDL (UC-MDL) procedure [Ris83, Ris89, LV93]: UC-MDL inductive inference procedure: 1. Partition the hypothesis space into regions with centres i ; i 2 fN g and volumes AOP V ( i ), where AOPV stands for Accuracy of Parameter Value <ref> [OH94, Section 3] </ref>. In general, the size of a region may be a function of its location. 2. Reorder the indexing to meet criteria described in Section 3.1, so f (i) = j; fi; j 2 N g. 3. <p> Further discussion of this issue can be found in <ref> [WB68, WB75, Ris83, WF87, WD93, OH94] </ref>. The point we wish to emphasize is that the UC-MDL inductive inference procedure is not fully specified until we specify f and the universal code to be used. <p> Each mapping implies a different prior probability distribution on the parameters. 3.1.1 Mapping Parameters onto Positive Integers Let us start by considering how to do the mapping for a single limited precision real number, such as y in our soil example. We must state parameters to a limited precision <ref> [OH94, Section 3:0] </ref>. So we can convert y to an integer by dividing by the accuracy of parameter value. For example, the limited precision real 2:25 has AOP V = :01 and 2:25 Next consider how to map the resulting integer onto the positive integers. <p> The first three message length estimates have a two part structure, and the last two message length estimates have a one part structure. The use of a simple example will help ground the discussion. We use the height example from Section 3.2 of Oliver and Hand <ref> [OH94] </ref>. The following set of heights are assumed to be from a normal distribution. We will calculate various MML and MDL estimates of the minimum message length to transmit the data. <p> The receiver also believes that is uniformly distributed in the range 100:::244 cm and that is uniformly distributed in the range 0:::50 cm. 18 4.0.1 MML (OH) Code Oliver and Hand <ref> [OH94] </ref>, following the derivations in Wallace and Boulton [WB68], derive the following expression for this model: M M L (OH) = log 2 range AOP V range AOP V s 2 + N N 2s 2 log 2 e bits (17) where s is the sample standard deviation, s is the
Reference: [OH96] <author> J.J. Oliver and D.J. </author> <title> Hand. Averaging over decision trees. </title> <journal> Journal of Classification, </journal> <note> To appear in 1996. An extended version is available as Technical Report TR 5-94, </note> <institution> Dept. of Statistics, Open University, Walton Hall, Milton Keynes, MK7 6AA, UK. </institution>
Reference-contexts: Computationally and cognitively it is often simpler to base prediction on a single model. More accurate prediction can also be done using a weighted combination of predictions from more than one model. The weights assigned to models are based on their message lengths <ref> [AWY92, OH96] </ref>. In both MDL and MML prediction, if we need to make a single-valued guess at the value of x, we use a loss function, l (^x; x). The loss function expresses our judgement of how bad it is to guess ^x when the real value is x.
Reference: [PW82] <author> J.D. Patrick and C.S. Wallace. </author> <title> Stone circle geometries: an information theory approach. In D.C. </title> <editor> Heggie, editor, </editor> <booktitle> Archaeoastronomy in the Old World, </booktitle> <pages> pages 231-264. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1982. </year>
Reference-contexts: 1 Introduction The Minimum Description Length (MDL) and the Minimum Message Length (MML) principles for model selection and inductive inference are not often distinguished in the literature. A typical example is the following passage from MacKay "Although some of the earliest work on complex model comparison involved the MDL framework <ref> [PW82] </ref>, MDL has no apparent advantages, and in my work I approximate the evidence directly" [Mac92, page 17]. Here MacKay has classified the MML results of Patrick and Wallace as MDL.
Reference: [QR89] <author> J.R. Quinlan and R. Rivest. </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80 </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: In the log fl code, this parameter value is assigned a prior probability of 0:5. The choice of the parameter value to be mapped to the integer 1 is called the origin problem. For many model classes, there is an obvious origin. For example, for classification trees <ref> [QR89] </ref>, the null tree is the obvious model to be mapped onto the origin. The choice of origin and a mapping involves the same issues involved in the choice of prior probability distribution on hypotheses. <p> As P rob (n) is a finite entropy distribution, n=1 Inequalities (15) and (16) show that Inequality (14) holds and therefore Inequality (8) holds. QED. 3.2.3 Other Universal Codes for Integers We may use the Quinlan and Rivest code for binary trees to map trees onto positive integers <ref> [QR89] </ref> 10 . The mapping chosen here is to map the tree with one node onto 1, the two trees with two nodes onto 2 and 3, the five trees with three nodes onto 4; 5; 6; 7; 8 and so on.
Reference: [Ris78] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: Part of the confusion can be traced to their related origins. MDL's most cited source paper, Rissanen <ref> [Ris78] </ref> , which derives the k 2 log 2 n complexity term, references MML's most cited source paper, Wallace and Boulton [WB68], and the message length derivation there is preceded by the phrase "Following a suggestion by Boulton and Wallace (private correspondence)" [Ris78, page 470]. <p> MDL's most cited source paper, Rissanen [Ris78] , which derives the k 2 log 2 n complexity term, references MML's most cited source paper, Wallace and Boulton [WB68], and the message length derivation there is preceded by the phrase "Following a suggestion by Boulton and Wallace (private correspondence)" <ref> [Ris78, page 470] </ref>. Wallace, Freeman and Rissanens' Royal Statistical Society meeting review papers on MML and MDL also appeared side by side in 1987 [Ris87, WF87]. In the following, we use the term MDL to cover a number of different message length estimates as espoused by Rissanen [Ris87, Ris89]. <p> = 144cm, range = 50cm, s = 5:23, s = 5:37: M M L (OH) = log 2 34:64 + log 2 16:58 + 20 log 2 13:45 + 20 fl :499 fl 1:443 = 98:55 4.1 MDL (1) The first MDL two part message length estimate we consider is <ref> [Ris78] </ref>: M DL (1) = log 2 P rob (xj) + k=2 log 2 (N ) + log 2 fl (k) (21) The first term is the negative log of the likelihood and is a term common to both MDL and MML message length expressions. k is the number of distinct
Reference: [Ris83] <author> J. Rissanen. </author> <title> A universal prior for the integers and estimation by MDL. </title> <journal> Ann. of Statistics, </journal> <volume> 11(2) </volume> <pages> 416-431, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: The later stochastic complexity literature does not use this indexing. One use of this indexing was to make the connection between MDL and maximum likelihood inductive inference <ref> [Ris83, page 418] </ref>. <p> P rob () is interpreted as subjective probabilities. Hence MML selects P rob () according to prior information, or, equivalently, to maximise its expected coding efficiency. The MDL literature is characterised by a lack of acceptance of subjective probabilities <ref> [Ris83, Ris87, Ris89, LV93, Ris93b] </ref>: "In our view the parameter is generated by our selecting the model class, and it has no other `inherent' meaning"[Ris89, page 54]. "In the MDL principle for statistical inference there is no need for the awkward Bayesian interpretations of the meaning of the prior probability on <p> MDL does not employ a prior probability distribution (or density) on parameter values. We now examine how MDL uses universal codes to estimate the optimal model code length. We now describe the Universal Code MDL (UC-MDL) procedure <ref> [Ris83, Ris89, LV93] </ref>: UC-MDL inductive inference procedure: 1. Partition the hypothesis space into regions with centres i ; i 2 fN g and volumes AOP V ( i ), where AOPV stands for Accuracy of Parameter Value [OH94, Section 3]. <p> Further discussion of this issue can be found in <ref> [WB68, WB75, Ris83, WF87, WD93, OH94] </ref>. The point we wish to emphasize is that the UC-MDL inductive inference procedure is not fully specified until we specify f and the universal code to be used. <p> This conclusion differs from that of Li and Vitanyi [LV93, page 311] who state: "its genesis [the log fl universal code] shows that it expresses no prior knowledge about the true value of the parameter." Rissanen has considered the origin problem <ref> [Ris83, page 427] </ref> "Does this invalidate the whole process? We think not. <p> The choice of mapping may affect the result of inductive inference made on finite data. 3.1.2 Implicit Mappings The explicit mappings given above are difficult to work with. A more convenient approach was outlined in <ref> [Ris83] </ref>. The method given in Section 3.1.1 for converting parameters to integers is convenient only if the parameters have uniform precision throughout the parameter space. Consider the single parameter case. <p> MML requires the specification of P rob (). This, along with an AOP V (), implies an origin (the most probable ) and a mapping (an enumeration of models from most probable to least probable). 6 Rissanen <ref> [Ris83, page 424] </ref> chooses the quadratic norm jjjj M () = p (; M ()). <p> The log fl function is defined as log fl (n) = log 2 (n) + log 2 log 2 (n) + ::::, including only positive terms. The log fl code encodes positive integers n with code length CodeLength (n) = log fl (n) + log (c), where c 2:8665064 <ref> [Eli75, Ris83] </ref>. <p> Initial call is decode (codeword,0,1); */ decode (char *codeword, int nextDigit, int n) - if (codeword [nextDigit] == '0') return n; else - /* interpret codeword [nextDigit,...,nextDigit+n] as a binary number and return its decimal value */ m = convertToDecimal (codeword, nextDigit, nextDigit+n); - decode (codeword,nextDigit+n+1,m); - 13 gets large <ref> [Ris83] </ref>. The specific theorem is that for any universal code [Ris83, Theorem 2]: U nivCodeLength (n) &lt; log (n) + r (n) where r (n) log (n) ! 0 as n ! 1. The `tail off' requirement is required to avoid assigning non-zero probabilities to infinite messages. <p> The specific theorem is that for any universal code <ref> [Ris83, Theorem 2] </ref>: U nivCodeLength (n) &lt; log (n) + r (n) where r (n) log (n) ! 0 as n ! 1. The `tail off' requirement is required to avoid assigning non-zero probabilities to infinite messages. <p> Since on a finite set of data, the estimated code lengths using different universal codes may be different, care is needed in choosing an appropriate one to represent our prior belief about a distribution of positive integers. 3.2.2 Useful Property of Universal Codes Rissanen <ref> [Ris83, Theorem 2] </ref> showed the asymptotic equivalence of universal codes. However the property that makes universal codes truly useful is only hinted at. Consider the problem of encoding an integer sampled from a finite entropy distribution. <p> Formally, we are required to show that n=1 P rob (n) log 2 U nivP rob (n) &lt; 1 (8) PROOF: From Theorem 2 of <ref> [Ris83] </ref>, we have log 2 (n) &lt; log 2 U nivP rob (n) &lt; log 2 (n) + r (n) (9) where r (n) log 2 n ! 0 and r (n) ! 1 as n ! 1. <p> 2 N log 2 P rob (xj) (23) This explains the difference in message length estimates between M DL (1) and M M L (OH) of 4:55 bits for the non-asymptotic case of N = 20. 19 4.2 MDL (2) A more accurate MDL message length estimate is the following <ref> [Ris83, page 426] </ref>: M DL (2) = log 2 P rob (xj) + k log 2 jM ()j + log 2 C (k) (24) where C (k even ) = (2) ( 1 2 k)! 2 ( 1 C (k odd ) = ( 1 2 (k + 1))! (26) M <p> where C (k even ) = (2) ( 1 2 k)! 2 ( 1 C (k odd ) = ( 1 2 (k + 1))! (26) M () = @ 2 (27) This approximation is invariant under linear transformations, but is not invariant under nonlinear transformations of the parameter space <ref> [Ris83, page 427] </ref>. This code suffers from the origin problem discussed in Section 3.1. The code is actually derived by a counting argument of the type described there. The log terms in M DL (2) are used as a convenient approximation to log fl.
Reference: [Ris87] <author> J. Rissanen. </author> <title> Stochastic complexity. </title> <journal> J. R. Statist. Soc. B, </journal> <volume> 49(3) </volume> <pages> 223-239, </pages> <year> 1987. </year>
Reference-contexts: Wallace, Freeman and Rissanens' Royal Statistical Society meeting review papers on MML and MDL also appeared side by side in 1987 <ref> [Ris87, WF87] </ref>. In the following, we use the term MDL to cover a number of different message length estimates as espoused by Rissanen [Ris87, Ris89]. MDL message length estimates are widely used in many areas of data modelling and artificial intelligence. <p> Wallace, Freeman and Rissanens' Royal Statistical Society meeting review papers on MML and MDL also appeared side by side in 1987 [Ris87, WF87]. In the following, we use the term MDL to cover a number of different message length estimates as espoused by Rissanen <ref> [Ris87, Ris89] </ref>. MDL message length estimates are widely used in many areas of data modelling and artificial intelligence. MDL codes include two part messages and one part messages. The asymptotic equivalence of the different MDL codes justifies the use of a single name [Ris87]. <p> MDL message length estimates are widely used in many areas of data modelling and artificial intelligence. MDL codes include two part messages and one part messages. The asymptotic equivalence of the different MDL codes justifies the use of a single name <ref> [Ris87] </ref>. MML estimates are two part message length estimates developed by Wallace and co-workers (see e.g., [WB68, WB75, WF87]). MML estimates have also had many applications. In this paper, we use the term message length specifically to mean the length of a bit string which describes a theory and data. <p> P rob () is interpreted as subjective probabilities. Hence MML selects P rob () according to prior information, or, equivalently, to maximise its expected coding efficiency. The MDL literature is characterised by a lack of acceptance of subjective probabilities <ref> [Ris83, Ris87, Ris89, LV93, Ris93b] </ref>: "In our view the parameter is generated by our selecting the model class, and it has no other `inherent' meaning"[Ris89, page 54]. "In the MDL principle for statistical inference there is no need for the awkward Bayesian interpretations of the meaning of the prior probability on <p> This code suffers from the origin problem discussed in Section 3.1. The code is actually derived by a counting argument of the type described there. The log terms in M DL (2) are used as a convenient approximation to log fl. This approximation is used by Rissanen <ref> [Ris87] </ref>.
Reference: [Ris89] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> N.J., </address> <year> 1989. </year>
Reference-contexts: Wallace, Freeman and Rissanens' Royal Statistical Society meeting review papers on MML and MDL also appeared side by side in 1987 [Ris87, WF87]. In the following, we use the term MDL to cover a number of different message length estimates as espoused by Rissanen <ref> [Ris87, Ris89] </ref>. MDL message length estimates are widely used in many areas of data modelling and artificial intelligence. MDL codes include two part messages and one part messages. The asymptotic equivalence of the different MDL codes justifies the use of a single name [Ris87]. <p> One use of this indexing was to make the connection between MDL and maximum likelihood inductive inference [Ris83, page 418]. Maximum likelihood is used by MDL to choose between models within a model class when the number of parameters, k, is the same. 3 MDL Principle <ref> [Ris89, page 9] </ref> "We may take it as an axiom that a model or a model class, which permits the shortest encoding of the data, captures best all the properties in the data we wish to learn". (In practice, we note MDL methods have concentrated on selecting model classes, rather than <p> A model class is specified by choosing P rob (Dj) and P rob () <ref> [Ris89, page 54] </ref>. In a parametric probabilistic model class framework, we have: P rob (D) = P rob (Dj)P rob ()d (2) where D is the data and are continuous model class parameters. <p> In a parametric probabilistic model class framework, we have: P rob (D) = P rob (Dj)P rob ()d (2) where D is the data and are continuous model class parameters. The stochastic complexity (SC) of data D, relative to the model class of Equation (2) is <ref> [Ris89, page 59] </ref>: SC = log P rob (D) (3) P rob (D) is also called the evidence [Goo85] and the marginal likelihood [KR93]. Approximations to Equation (2) are the basis of the Bayesian inference methods of Schwarz [Sch78] and MacKay [Mac92]. <p> P rob () is interpreted as subjective probabilities. Hence MML selects P rob () according to prior information, or, equivalently, to maximise its expected coding efficiency. The MDL literature is characterised by a lack of acceptance of subjective probabilities <ref> [Ris83, Ris87, Ris89, LV93, Ris93b] </ref>: "In our view the parameter is generated by our selecting the model class, and it has no other `inherent' meaning"[Ris89, page 54]. "In the MDL principle for statistical inference there is no need for the awkward Bayesian interpretations of the meaning of the prior probability on <p> MDL does not employ a prior probability distribution (or density) on parameter values. We now examine how MDL uses universal codes to estimate the optimal model code length. We now describe the Universal Code MDL (UC-MDL) procedure <ref> [Ris83, Ris89, LV93] </ref>: UC-MDL inductive inference procedure: 1. Partition the hypothesis space into regions with centres i ; i 2 fN g and volumes AOP V ( i ), where AOPV stands for Accuracy of Parameter Value [OH94, Section 3]. <p> Parameter Value: 0 1 -1 2 -2 3 -3 4 -4 5 ... 5 This approach is similar to that advocated by Li and Vitanyi [LV93, page 310] and Rissanen <ref> [Ris89] </ref>. 9 This mapping onto the log fl code implies a probability distribution where positive integers are equally likely to negative integers a priori. If we believe a priori this is not the case, we could choose a different mapping to reflect this. <p> A `fix' needs to be made to this procedure just in case P rob (x t+1 jx t ) = 0 <ref> [Ris89, page 69] </ref>. The fixes used are ad hoc. Another difficulty is that the PMDL message length estimate is dependent on the ordering of the data. In some applications the data will have a specific order, such as time series data.
Reference: [Ris93a] <author> J. Rissanen. </author> <title> Fisher information and stochastic complexity. </title> <type> Research report rj 9547, </type> <institution> IBM ARC, </institution> <address> San Jose, CA 95120-6099, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: Instead all the models in the assumed model class are used to encode the data. This difference arise from MML and MDL's respective formulations of the objectives for inductive inference. We summarize them as follows: 4 MDL Objectives: <ref> [Ris93a, page 13] </ref> "The main problem in statistical inference is and remains to find the best model class and the best model in it as judged by shortest code length..." MML Objectives: The end product of inductive inference is the model that gives the shortest two part message length.
Reference: [Ris93b] <author> J. Rissanen. </author> <title> Information theory and neural nets. </title> <editor> In P. Smolensky et al., editors, </editor> <booktitle> Mathematical Perspectives on Neural Networks. </booktitle> <publisher> Lawrence Erlbaum, </publisher> <year> 1993. </year>
Reference-contexts: P rob () is interpreted as subjective probabilities. Hence MML selects P rob () according to prior information, or, equivalently, to maximise its expected coding efficiency. The MDL literature is characterised by a lack of acceptance of subjective probabilities <ref> [Ris83, Ris87, Ris89, LV93, Ris93b] </ref>: "In our view the parameter is generated by our selecting the model class, and it has no other `inherent' meaning"[Ris89, page 54]. "In the MDL principle for statistical inference there is no need for the awkward Bayesian interpretations of the meaning of the prior probability on <p> The code is actually derived by a counting argument of the type described there. The log terms in M DL (2) are used as a convenient approximation to log fl. This approximation is used by Rissanen [Ris87]. A different derivation, but using similar arguments, is given in <ref> [Ris93b] </ref>. 4.2.1 Calculations for the Example First, consider the log 2 C (k) term with k = 2, log 2 (2) 1! 2 = log 2 4 = 3:65 (28) Second calculate the log p jM ()j term, @ 2 log P rob (xj) N 20 @ 2 log P rob
Reference: [Sch78] <author> G. Schwarz. </author> <title> Estimating dimension of a model. </title> <journal> Ann. Stat., </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: Approximations to Equation (2) are the basis of the Bayesian inference methods of Schwarz <ref> [Sch78] </ref> and MacKay [Mac92]. These approaches are discussed in Oliver and Baxter [OB94]. If a single model within a model class needs to be chosen, then a non-coding principle, such as the maximum likelihood estimator, is used.
Reference: [Sor83] <author> R. Sorfkin. </author> <title> A quantitative Occam's razor. </title> <journal> International Journal of Theoretical Physics, </journal> <volume> 22(12) </volume> <pages> 1091-1103, </pages> <year> 1983. </year>
Reference-contexts: These estimates dominate the minimum encoding inference literature, although Sorfkin independently developed a similar two part message length estimate <ref> [Sor83] </ref>. The first three message length estimates have a two part structure, and the last two message length estimates have a one part structure. The use of a simple example will help ground the discussion. We use the height example from Section 3.2 of Oliver and Hand [OH94].
Reference: [WB68] <author> C.S. Wallace and D.M. Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11(2) </volume> <pages> 195-209, </pages> <year> 1968. </year>
Reference-contexts: Part of the confusion can be traced to their related origins. MDL's most cited source paper, Rissanen [Ris78] , which derives the k 2 log 2 n complexity term, references MML's most cited source paper, Wallace and Boulton <ref> [WB68] </ref>, and the message length derivation there is preceded by the phrase "Following a suggestion by Boulton and Wallace (private correspondence)" [Ris78, page 470]. Wallace, Freeman and Rissanens' Royal Statistical Society meeting review papers on MML and MDL also appeared side by side in 1987 [Ris87, WF87]. <p> MDL codes include two part messages and one part messages. The asymptotic equivalence of the different MDL codes justifies the use of a single name [Ris87]. MML estimates are two part message length estimates developed by Wallace and co-workers (see e.g., <ref> [WB68, WB75, WF87] </ref>). MML estimates have also had many applications. In this paper, we use the term message length specifically to mean the length of a bit string which describes a theory and data. <p> MML Principle (paraphrasing <ref> [WB68, WF87] </ref>) For each model in the set of models considered, we consider the message consisting of a description of a model, followed by the data encoded under the assumption that the model is true. <p> Further discussion of this issue can be found in <ref> [WB68, WB75, Ris83, WF87, WD93, OH94] </ref>. The point we wish to emphasize is that the UC-MDL inductive inference procedure is not fully specified until we specify f and the universal code to be used. <p> The receiver also believes that is uniformly distributed in the range 100:::244 cm and that is uniformly distributed in the range 0:::50 cm. 18 4.0.1 MML (OH) Code Oliver and Hand [OH94], following the derivations in Wallace and Boulton <ref> [WB68] </ref>, derive the following expression for this model: M M L (OH) = log 2 range AOP V range AOP V s 2 + N N 2s 2 log 2 e bits (17) where s is the sample standard deviation, s is the unbiased sample standard deviation, * is the accuracy
Reference: [WB75] <author> C.S. Wallace and D.M. Boulton. </author> <title> An invariant Bayes method for point estimation. </title> <journal> Classification Society Bulletin, </journal> <volume> 3(3) </volume> <pages> 11-34, </pages> <year> 1975. </year>
Reference-contexts: MDL codes include two part messages and one part messages. The asymptotic equivalence of the different MDL codes justifies the use of a single name [Ris87]. MML estimates are two part message length estimates developed by Wallace and co-workers (see e.g., <ref> [WB68, WB75, WF87] </ref>). MML estimates have also had many applications. In this paper, we use the term message length specifically to mean the length of a bit string which describes a theory and data. <p> Further discussion of this issue can be found in <ref> [WB68, WB75, Ris83, WF87, WD93, OH94] </ref>. The point we wish to emphasize is that the UC-MDL inductive inference procedure is not fully specified until we specify f and the universal code to be used.
Reference: [WD93] <author> C.S. Wallace and D.L. Dowe. </author> <title> MML estimation of the von Mises concentration parameter. </title> <type> Technical Report 93/193, </type> <institution> Dept. of Computer Science,Monash University,Clayton 3168, Australia, </institution> <year> 1993. </year>
Reference-contexts: Further discussion of this issue can be found in <ref> [WB68, WB75, Ris83, WF87, WD93, OH94] </ref>. The point we wish to emphasize is that the UC-MDL inductive inference procedure is not fully specified until we specify f and the universal code to be used.
Reference: [WF87] <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> J. R. Statist. Soc B, </journal> <volume> 49(3) </volume> <pages> 240-265, </pages> <year> 1987. </year>
Reference-contexts: Wallace, Freeman and Rissanens' Royal Statistical Society meeting review papers on MML and MDL also appeared side by side in 1987 <ref> [Ris87, WF87] </ref>. In the following, we use the term MDL to cover a number of different message length estimates as espoused by Rissanen [Ris87, Ris89]. MDL message length estimates are widely used in many areas of data modelling and artificial intelligence. <p> MDL codes include two part messages and one part messages. The asymptotic equivalence of the different MDL codes justifies the use of a single name [Ris87]. MML estimates are two part message length estimates developed by Wallace and co-workers (see e.g., <ref> [WB68, WB75, WF87] </ref>). MML estimates have also had many applications. In this paper, we use the term message length specifically to mean the length of a bit string which describes a theory and data. <p> MML Principle (paraphrasing <ref> [WB68, WF87] </ref>) For each model in the set of models considered, we consider the message consisting of a description of a model, followed by the data encoded under the assumption that the model is true. <p> The shortest two part message cannot be expected to be as short as the shortest one part message encoding the data, because it encodes not just the data, but also a choice of model not implied by the data <ref> [WF87, page 260] </ref>. So while MML and MDL are often grouped under the `shortest data description length', `minimum description length' or `minimum encoding inference' umbrella, we see that MML uses the slightly longer two part message structure. <p> Typically it is applied to coding a model indexed by a positive integer. 2.3 A Specific Objection to Priors We now consider one specific objection to priors as used by MML "A similar code-length (similar to MDL) with a prior is discussed in Wallace and Freeman <ref> [WF87] </ref>. The authors advocate the principle of minimizing the mean code length in the spirit of Shannon's work, which strictly speaking does not allow it to be used to select the model class. <p> Further discussion of this issue can be found in <ref> [WB68, WB75, Ris83, WF87, WD93, OH94] </ref>. The point we wish to emphasize is that the UC-MDL inductive inference procedure is not fully specified until we specify f and the universal code to be used. <p> M DL (2) = 88:68 + 0:05 + 3:65 = 92:38 (35) 4.3 MML (WF) The M M L (W F ) is a feasible two part message length estimate proposed by <ref> [WF87] </ref>.
Reference: [WF92] <author> C.S. Wallace and P.R. Freeman. </author> <title> Single factor analysis by MML estimation. </title> <journal> J. R. Statist. Soc. B., </journal> <volume> 54(1) </volume> <pages> 185-209, </pages> <year> 1992. </year>
Reference-contexts: A code length may be fractional| see Oliver and Hand [OH94, Section 2.8] for details. In this paper we consider message length estimates for sufficiently regular models 1 and identically and independently distributed data. Message length estimates can, of course, be made for other cases, see Wallace and Freeman <ref> [WF92] </ref> for an example. 2 Differences The MML and MDL principles for inductive inference are similar, but distinct. We state these principles, but first define the terms: model and model class. In any inductive inference problem, a set of models is entertained.
Reference: [WP93] <author> C.S. Wallace and J.D. Patrick. </author> <title> Coding decision trees. </title> <journal> Machine Learning, </journal> <volume> 11 </volume> <pages> 7-22, </pages> <year> 1993. </year> <month> 26 </month>
Reference-contexts: Appendix A contains the code used to calculate the code lengths for the binary tree universal code. For a large integer such as 10 20 the difference is only 3 bits. Wallace and Patrick <ref> [WP93] </ref> described codes for n-arity trees. These are also universal codes. In Table 3, we show the code lengths for the first nine powers of ten. The differences in code lengths are still bounded, but are larger. Higher arity tree codes assign less probability to smaller integers.
References-found: 32

