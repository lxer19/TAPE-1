URL: http://www.cs.cornell.edu/Info/People/csun/papers/pp95.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/csun/sun.html
Root-URL: 
Email: csun@cs.cornell.edu  
Title: multifrontal solution of sparse linear least squares problems on distributed-memory multiprocessors 1  
Author: Chunguang Sun 
Address: Ithaca, NY 14853-3801  
Affiliation: Advanced Computing Research Institute Cornell Theory Center Cornell University  
Note: Parallel  
Abstract: This paper appears in Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, D. H. Bailey, P. E. Bjtrstad, J. R. Gilbert, M. V. Mascagni, R. S. Schreiber, H. D. Simon, V. J. Torczon, and L. T. Watson, eds., SIAM, Philadelphia, 1995, pp.418-423. Abstract We describe the issues involved in the design and implementation of efficient parallel algorithms for solving sparse linear least squares problems on distributed-memory multiprocessors. We consider both the QR factorization method and the method of corrected semi-normal equations. The sparse QR factorization is accomplished by a parallel multi-frontal scheme recently introduced. A new parallel algorithm for solving the related sparse triangular systems is proposed. Experimental results on an Intel iPSC/860 machine are described. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bj orck, </author> <title> Stability analysis of the method of seminormal equations for linear least squares problems, </title> <journal> Linear Algebra Appl., </journal> <volume> 88/89 (1987), </volume> <pages> pp. 31-48. </pages>
Reference-contexts: If A is large and sparse, it is often too expensive to store Q in the main memory. Once R is computed, the semi-normal equations R t Rx = A t b can be used to handle new right hand sides without storing Q. Bjorck <ref> [1] </ref> shows that the method of semi-normal equations (SNE) is numerically instable and proposes that an iterative refinement step be added to the SNE method as follows. 8 &gt; : R t Rffix = A t r; x x + ffix: (3) Bjorck [1] shows that this new method | the <p> Bjorck <ref> [1] </ref> shows that the method of semi-normal equations (SNE) is numerically instable and proposes that an iterative refinement step be added to the SNE method as follows. 8 &gt; : R t Rffix = A t r; x x + ffix: (3) Bjorck [1] shows that this new method | the method of corrected semi-normal equations (CSNE) is, in general, as accurate as the QR factorization method.
Reference: [2] <author> G. Golub, </author> <title> Numerical methods for solving linear least squares problems, </title> <journal> Numer. Math., </journal> <volume> 7 (1965), </volume> <pages> pp. 206-216. </pages>
Reference-contexts: 1 Introduction Consider the numerical solution of large and sparse linear least squares problems min kAx bk 2 ;(1) where A is an M fi N matrix with full column rank. The QR factorization method <ref> [2] </ref> for solving (1) is to reduce A to an upper triangular matrix R by an orthogonal matrix Q Q t A = R # and form Q t b.
Reference: [3] <author> A. Pothen and C. Sun, </author> <title> A mapping algorithm for parallel sparse Cholesky factorization, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 14 (1993), </volume> <pages> pp. 1253-1257. </pages>
Reference-contexts: A sparse QR factorization is decomposed into a set of computational tasks according to the supernodal tree structure. A task is to form the frontal matrix of a supernode. The computational tasks are mapped onto node processors by the proportional mapping scheme described in <ref> [3] </ref>. If a task associated with a supernode S is partitioned among a set of p processors, then the frontal matrix F of the supernode S is partitioned among the set of processors by the equal-volume partitioning strategy introduced in [4].
Reference: [4] <author> C. Sun, </author> <title> Parallel sparse orthogonal factorization on distributed-memory multiprocessors, </title> <type> Tech. Report CTC93TR162, </type> <institution> Advanced Computing Research Institute, Center for Theory and Simulation in Science and Engineering, Cornell University, </institution> <address> Ithaca, NY, </address> <month> Dec. </month> <year> 1993. </year> <title> [5] , Parallel multifrontal solution of sparse linear least squares problems on distributed-memory multiprocessors, </title> <type> Tech. Report CTC94TR185, </type> <institution> Advanced Computing Research Institute, Center for Theory and Simulation in Science and Engineering, Cornell University, </institution> <address> Ithaca, NY, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: The major tasks involved are sparse QR factorization and sparse triangular solution. The sparse QR factorization is accomplished by a parallel multifrontal scheme described in <ref> [4] </ref>. A new parallel multifrontal algorithm for solving the related sparse triangular systems is proposed. <p> Numerical experiments on an Intel iPSC/860 machine are described. 2 Parallel multifrontal sparse QR factorization The first step in a direct method for solving a sparse linear least squares problem is to compute a sparse QR factorization. The parallel algorithm proposed in <ref> [4] </ref> is used to perform parallel sparse QR factorization. The supernodal structure of R is fully exploited in the numeric factorization. Let S = fi; i + 1; ; jg be a supernode consisting of rows i; i + 1; ; j of R. <p> If a task associated with a supernode S is partitioned among a set of p processors, then the frontal matrix F of the supernode S is partitioned among the set of processors by the equal-volume partitioning strategy introduced in <ref> [4] </ref>. In other words, the rows of F are partitioned into p blocks such that every block consists of a set of adjacent rows of F and contains approximately jF j=p numerical values of F . <p> The parallel sparse QR factorization has been discussed in <ref> [4] </ref>. Central to our approaches is a new parallel multifrontal algorithm for solving the related sparse triangular systems. The choice among the three approaches depends on the availability of the right hand sides, the size of the primary storage space and the desired accuracy for the computed solutions. <p> The choice among the three approaches depends on the availability of the right hand sides, the size of the primary storage space and the desired accuracy for the computed solutions. We are currently experimenting a generalized scheme of our equal-volume partitioning strategy proposed in <ref> [4] </ref>. Assume that a frontal matrix F is mapped to p processors. Our generalized scheme partitions F into k p blocks such that every block consists of a set of adjacent rows of F and contains approximately jF j=k numerical values of F .
References-found: 4

