URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR91205-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: ken@rice.edu kremer@rice.edu  
Title: Automatic Data Alignment and Distribution for Loosely Synchronous Problems in an Interactive Programming Environment  
Author: Ken Kennedy Ulrich Kremer 
Address: P.O. Box 1892 Houston, Texas 77251  
Affiliation: Department of Computer Science Center for Research on Parallel Computation Rice University  
Abstract: An approach to distributed memory parallel programming that has recently become popular is one where the programmer explicitly specifies the data layout using language extensions, and a compiler generates all the communication. While this frees the programmer from the tedium of thinking about message-passing, no assistance is provided in determining the data layout scheme that gives a satisfactory performance on the target machine. We wish to provide automatic data alignment and distribution techniques for a large class of scientific computations, known in the literature as loosely synchronous problems. We propose an interactive software tool that allows the user to select regions of the sequential input program, then responds with a data decomposition scheme and diagnostic information for the selected region. The proposed tool allows the user to obtain insights into the characteristics of the program executing on a distributed memory machine and the behavior of the underlying compilation system without actually compiling and executing the program on the machine. An empirical study of actual application programs will show whether automatic techniques are able to generate data decomposition schemes that are close to optimal. If automatic techniques will fail to do so, we want to answer the questions (1) how user interaction can help to overcome the deficiencies of automatic techniques, and (2) whether, in particular, there is a data-parallel programming style that allows automatic detection of efficient data alignment and distribution schemes. 
Abstract-found: 1
Intro-found: 1
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Such non-local data items must be obtained by communication. For the given example we assume that the Fortran D compiler performs the following communication optimizations. Message vectorization uses the results of data dependence analysis <ref> [AK87, KKP + 81] </ref> to combine element messages into vectors. The level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence determines the outermost loop where element messages resulting from the same array reference may be legally combined [BFKK90, Ger90].
Reference: [ASU86] <author> A. V. Aho, R. Sethi, and J. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: The merging problem can be formulated as a single-source shortest paths problem over the phase control flow graph. The phase control flow graph is similar to the control flow graph <ref> [ASU86] </ref> where all nodes associated with a phase are substituted by nodes representing the set of reasonable data decomposition schemes for the phase. The static performance estimator is used to predict the costs for these reasonable decomposition schemes.
Reference: [BFKK90] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence determines the outermost loop where element messages resulting from the same array reference may be legally combined <ref> [BFKK90, Ger90] </ref>. Message coalescing ensures that each data value is sent to a processor only once. A detailed description of different communication optimizations can be found in [HKT92a]. This paper also discusses an alternative approach to generating messages for the red-black example, called vector message pipelining. <p> Li, Chen, and Choo for the Crystal project at Yale University [CCL89, LC90a, LC91b, LC91a, LC90b], and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project <ref> [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91] </ref>. Knobe, Lukas, Natarajan and Steele deal with the problem of automatic data layout for SIMD architectures such as the Connection machine. They discuss an algorithm that uses heuristics to solve conflicting requirements between minimizing communication and maximizing parallelism. <p> Balasundaram, Fox, Kennedy, and Kremer proposed an interactive performance estimation tool that helps the user to understand the effects of a user specified decomposition with respect to communication time and overall execution time <ref> [BFKK90] </ref> without actually compiling and executing the program. The described techniques are based on the knowledge about the compiler, the actual problem size, and the number of processors to be used. <p> The proposed automatic techniques will be implemented as part of the ParaScope parallel programming environment adapted to distributed memory multiprocessors <ref> [BKK + 89, KMT91, BFKK90, HKK + 91] </ref>. A prototype of the machine module of the static performance estimator is already available [BFKK91]. 7 Acknowledgement We wish to thank Seema Hiranandani and Chau-Wen Tseng for many discussions and helpful comments on the content of this paper.
Reference: [BFKK91] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: It predicts the performance of a node program using Express communication routines for different numbers of processors and data sizes [EXP89]. The prototype performance estimator has proved quite precise, especially in predicting the relative performances of different data decompositions <ref> [BFKK91] </ref>. Figure 7 and Figure 8 show the estimated execution times for our red-black example using the training set approach. <p> Li, Chen, and Choo for the Crystal project at Yale University [CCL89, LC90a, LC91b, LC91a, LC90b], and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project <ref> [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91] </ref>. Knobe, Lukas, Natarajan and Steele deal with the problem of automatic data layout for SIMD architectures such as the Connection machine. They discuss an algorithm that uses heuristics to solve conflicting requirements between minimizing communication and maximizing parallelism. <p> The described techniques are based on the knowledge about the compiler, the actual problem size, and the number of processors to be used. In <ref> [BFKK91] </ref>, techniques are discussed 19 that allow static performance estimation with high accuracy for a large class of loosely synchronous problems. Some approaches to automatic data decomposition concentrate on single loop nests. The iteration space is first partitioned into sets of iterations that can be executed independently. <p> The proposed automatic techniques will be implemented as part of the ParaScope parallel programming environment adapted to distributed memory multiprocessors [BKK + 89, KMT91, BFKK90, HKK + 91]. A prototype of the machine module of the static performance estimator is already available <ref> [BFKK91] </ref>. 7 Acknowledgement We wish to thank Seema Hiranandani and Chau-Wen Tseng for many discussions and helpful comments on the content of this paper.
Reference: [BKK + 89] <author> V. Balasundaram, K. Kennedy, U. Kremer, K. S. M c Kinley, and J. Subhlok. </author> <title> The ParaScope Editor: An interactive parallel programming tool. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: The proposed automatic techniques will be implemented as part of the ParaScope parallel programming environment adapted to distributed memory multiprocessors <ref> [BKK + 89, KMT91, BFKK90, HKK + 91] </ref>. A prototype of the machine module of the static performance estimator is already available [BFKK91]. 7 Acknowledgement We wish to thank Seema Hiranandani and Chau-Wen Tseng for many discussions and helpful comments on the content of this paper.
Reference: [CCL88] <author> M. Chen, Y. Choo, and J. Li. </author> <title> Compiling parallel programs by optimizing performance. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 1(2) </volume> <pages> 171-207, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: The reason is that traditional programming languages support single name spaces and, as a result, most programmers feel more comfortable working with a shared memory programming model. To this end, a number of researchers <ref> [CK88, CCL88, KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref> have proposed using a traditional sequential or parallel shared-memory language extended with annotations specifying how the data is to be mapped onto the distributed memory machine.
Reference: [CCL89] <author> M. Chen, Y. Choo, and J. Li. </author> <title> Theory and pragmatics of compiling efficient parallel code. </title> <type> Technical Report YALEU/DCS/TR-760, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: Knobe, Lukas at Compass, Steele at Thinking Machines Corporation, and Natarajan at Mo torola [KLS90, KN90], 2. Li, Chen, and Choo for the Crystal project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>, and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91].
Reference: [CHK92] <author> K. Cooper, M. W. Hall, and K. Kennedy. </author> <title> Procedure cloning. </title> <booktitle> In Proceedings of the 1992 IEEE International Conference on Computer Language, </booktitle> <address> Oakland, CA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Interprocedural analysis is used to allow the merging of computation phases across procedure boundaries. Interprocedural phase merging is compiler dependent. In particular, the automatic data partitioner has to know whether and when the compiler performs interprocedural optimizations such as procedure cloning <ref> [CHK92, HHKT91] </ref> or procedure inlining [Hal91]. In the following we will assume that the compiler performs procedure cloning for every distinct pattern of entry and exit 15 Algorithm DECOMP Input: program segment without procedure calls; problem sizes and number of processors to be used.
Reference: [CHZ91] <author> B. Chapman, H. Herbeck, and H. Zima. </author> <title> Automatic support for data distribution. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Our approach to automatic data alignment and distribution is closely related to the work done by Gupta and Banerjee at the University of Illinois at Urbana-Champaign [GB90, GB91, GB92], Wholey at Carnegie Mellon University [Who91], and Chapman, Herbeck, and Zima at the University of Vienna <ref> [CHZ91] </ref>. The described techniques for automatic data decomposition work on whole programs taking machine characteristics and problem characteristics into account. The automatic data partitioner is an integral part of the compiler. The major contributions of our research are 1. the development of new techniques for automatic data alignment and distribution.
Reference: [CK88] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multipro cessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: The reason is that traditional programming languages support single name spaces and, as a result, most programmers feel more comfortable working with a shared memory programming model. To this end, a number of researchers <ref> [CK88, CCL88, KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref> have proposed using a traditional sequential or parallel shared-memory language extended with annotations specifying how the data is to be mapped onto the distributed memory machine.
Reference: [CKK89] <author> D. Callahan, K. Kennedy, and U. Kremer. </author> <title> A dynamic study of vectorization in PFC. </title> <type> Technical Report TR89-97, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: In particular, we want to investigate whether there is a data-parallel programming style for sequential programs that facilitates automatic data alignment and distribution. The existence of a usable programming style in the context of vectorization has been observed by some researchers <ref> [CKK89, Wol89] </ref> and is partially responsible for the success of automatic vectorization. We believe that for 2 regular loosely synchronous problems written in a data-parallel programming style, the automatic data partitioner can determine an efficient decomposition scheme in most cases.
Reference: [CLR90] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The availability of fast collective communication routines will be crucial for the profitability of realignment and redistribution. The merging problem for a linear phase control flow graph can be solved as a single-source shortest paths problem in a directed acyclic graph <ref> [CLR90] </ref>. For example, Figure 9 shows a three phase problem with four reasonable decompositions for each phase. Each decomposition scheme is represented by a node. The node is labeled with the predicted cost of the decomposition scheme for the phase.
Reference: [D'H89] <author> E. D'Hollander. </author> <title> Partitioning and labeling of index sets in do loops with constant de pendence. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <address> 21 St. Charles, IL, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Some approaches to automatic data decomposition concentrate on single loop nests. The iteration space is first partitioned into sets of iterations that can be executed independently. The data mapping is determined by the iterations that are assigned to the different processors <ref> [RS89, Ram90, D'H89, KKBP91] </ref>. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils [SS90, HA90, IFKF90, GAY91]. This more abstract representation is used to find a good data mapping.
Reference: [EXP89] <institution> Parasoft Corporation. </institution> <note> Express User's Manual, </note> <year> 1989. </year>
Reference-contexts: Figure 4 illustrates the resulting compute-communicate sequence for the block-partitioning and column-partitioning schemes. We assume in our example that the compiler generates an EXPRESS 1 node programs <ref> [EXP89] </ref> where communication is performed by calls to vector-send and vector-receive communication routines KXVWRI and KXVREA, respectively. Execution of KXVWRI is non-blocking in the sense that the sending processor does not wait until the message is received. <p> A prototype of the machine module has been implemented for a common class of loosely synchronous scientific problems [FJL + 88]. It predicts the performance of a node program using Express communication routines for different numbers of processors and data sizes <ref> [EXP89] </ref>. The prototype performance estimator has proved quite precise, especially in predicting the relative performances of different data decompositions [BFKK91]. Figure 7 and Figure 8 show the estimated execution times for our red-black example using the training set approach.
Reference: [FF88] <author> W. Furmanski and G. Fox. </author> <title> Optimal communication algorithms for regular decomposi tions on the hypercube. </title> <type> Technical Report C3P-314B, </type> <institution> California Institute of Technology, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: For instance, fast collective communication routines as the Crystal router package developed at Caltech <ref> [FF88] </ref>, are crucial for determining the profitability of realignment and redistribution. In addition, if the compiler generates a node program, the characteristics of the target machine node compiler have to be considered. 2. machine characteristics, such as communication and computation costs, and machine topol ogy.
Reference: [FHK + 90] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> For tran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: The 1 Sets Training Estimator Performance Static Partitioner Data Automatic EnvironmentFortranUser Fortran D Fortran D Compiler Message Passing Fortran Fortran D language and its compiler <ref> [FHK + 90, HKT92b] </ref> follow this approach. Given a Fortran D program the compiler mechanically generates the node program for a given distributed-memory target machine. A problem with this approach is that it does not support the user in the decision process about a good data layout. <p> In addition, we believe that our two-phase strategy for specifying data decomposition is natural and conducive to writing modular and portable code. Fortran D bears similarities to both CM Fortran [TMC89] and Kali [KM91]. The complete language is described in detail elsewhere <ref> [FHK + 90] </ref>. 3 Why is Finding a Good Data Layout Hard? The choice of a good data decomposition scheme depends on many factors. <p> Li, Chen, and Choo for the Crystal project at Yale University [CCL89, LC90a, LC91b, LC91a, LC90b], and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project <ref> [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91] </ref>. Knobe, Lukas, Natarajan and Steele deal with the problem of automatic data layout for SIMD architectures such as the Connection machine. They discuss an algorithm that uses heuristics to solve conflicting requirements between minimizing communication and maximizing parallelism.
Reference: [FJL + 88] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: We want to propose the development of automatic techniques for data alignment and distribution for regular loosely synchronous problems that deal with realignment/redistribution, data replication, procedure calls, and control flow. Loosely synchronous problems represent a large class of scientific computations <ref> [FJL + 88] </ref>. They can be characterized by computation intensive regions that have substantial parallelism, with a synchronization point between the regions. We restrict our proposed system to regular loosely synchronous problems with arrays as their major data structures. <p> In many cases the accurate prediction of the crossover point at which one data decomposition scheme is preferable over another will be sufficient. A prototype of the machine module has been implemented for a common class of loosely synchronous scientific problems <ref> [FJL + 88] </ref>. It predicts the performance of a node program using Express communication routines for different numbers of processors and data sizes [EXP89]. The prototype performance estimator has proved quite precise, especially in predicting the relative performances of different data decompositions [BFKK91].
Reference: [GAY91] <author> E. Gabber, A. Averbuch, and A. Yehudai. </author> <title> Experience with a portable parallelizing Pascal compiler. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The data mapping is determined by the iterations that are assigned to the different processors [RS89, Ram90, D'H89, KKBP91]. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils <ref> [SS90, HA90, IFKF90, GAY91] </ref>. This more abstract representation is used to find a good data mapping.
Reference: [GB90] <author> M. Gupta and P. Banerjee. </author> <title> Automatic data partitioning on distributed mem ory multiprocessors. </title> <type> Technical Report CRHC-90-14, </type> <institution> Center for Reliable and High-Performance Computing, Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: This more abstract representation is used to find a good data mapping. Our approach to automatic data alignment and distribution is closely related to the work done by Gupta and Banerjee at the University of Illinois at Urbana-Champaign <ref> [GB90, GB91, GB92] </ref>, Wholey at Carnegie Mellon University [Who91], and Chapman, Herbeck, and Zima at the University of Vienna [CHZ91]. The described techniques for automatic data decomposition work on whole programs taking machine characteristics and problem characteristics into account. The automatic data partitioner is an integral part of the compiler.
Reference: [GB91] <author> M. Gupta and P. Banerjee. </author> <title> Automatic data partitioning on distributed memory mul tiprocessors. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, OR, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: This more abstract representation is used to find a good data mapping. Our approach to automatic data alignment and distribution is closely related to the work done by Gupta and Banerjee at the University of Illinois at Urbana-Champaign <ref> [GB90, GB91, GB92] </ref>, Wholey at Carnegie Mellon University [Who91], and Chapman, Herbeck, and Zima at the University of Vienna [CHZ91]. The described techniques for automatic data decomposition work on whole programs taking machine characteristics and problem characteristics into account. The automatic data partitioner is an integral part of the compiler.
Reference: [GB92] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> April </month> <year> 1992. </year>
Reference-contexts: This more abstract representation is used to find a good data mapping. Our approach to automatic data alignment and distribution is closely related to the work done by Gupta and Banerjee at the University of Illinois at Urbana-Champaign <ref> [GB90, GB91, GB92] </ref>, Wholey at Carnegie Mellon University [Who91], and Chapman, Herbeck, and Zima at the University of Vienna [CHZ91]. The described techniques for automatic data decomposition work on whole programs taking machine characteristics and problem characteristics into account. The automatic data partitioner is an integral part of the compiler.
Reference: [Ger89] <author> M. Gerndt. </author> <title> Automatic Parallelization for Distributed-Memory Multiprocessing Systems. </title> <type> PhD thesis, </type> <institution> University of Bonn, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: The reason is that traditional programming languages support single name spaces and, as a result, most programmers feel more comfortable working with a shared memory programming model. To this end, a number of researchers <ref> [CK88, CCL88, KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref> have proposed using a traditional sequential or parallel shared-memory language extended with annotations specifying how the data is to be mapped onto the distributed memory machine.
Reference: [Ger90] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency| Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence determines the outermost loop where element messages resulting from the same array reference may be legally combined <ref> [BFKK90, Ger90] </ref>. Message coalescing ensures that each data value is sent to a processor only once. A detailed description of different communication optimizations can be found in [HKT92a]. This paper also discusses an alternative approach to generating messages for the red-black example, called vector message pipelining.
Reference: [HA90] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The data mapping is determined by the iterations that are assigned to the different processors [RS89, Ram90, D'H89, KKBP91]. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils <ref> [SS90, HA90, IFKF90, GAY91] </ref>. This more abstract representation is used to find a good data mapping.
Reference: [Hal91] <author> M. W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Interprocedural analysis is used to allow the merging of computation phases across procedure boundaries. Interprocedural phase merging is compiler dependent. In particular, the automatic data partitioner has to know whether and when the compiler performs interprocedural optimizations such as procedure cloning [CHK92, HHKT91] or procedure inlining <ref> [Hal91] </ref>. In the following we will assume that the compiler performs procedure cloning for every distinct pattern of entry and exit 15 Algorithm DECOMP Input: program segment without procedure calls; problem sizes and number of processors to be used.
Reference: [HHKT91] <author> M. W. Hall, S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Interprocedural compilation of Fortran D for MIMD distributed-memory machines. </title> <type> Technical Report TR91-169, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> November </month> <year> 1991. </year> <month> 22 </month>
Reference-contexts: For such programs recompilation for different problem sizes and number of processors used can be avoided. In the reminder of this section, we will examine the relationship between the listed factors using the characteristics of the Fortran D compiler <ref> [HKT92b, HKT91, HHKT91] </ref>. Figure 3 shows a Fortran D code for pointwise red-black relaxation using a block-partitioning scheme for the N fi N , double precision array v. Substituting line 4 by DISTRIBUTE d (:,BLOCK) specifies a column partitioning scheme for array v. <p> Interprocedural analysis is used to allow the merging of computation phases across procedure boundaries. Interprocedural phase merging is compiler dependent. In particular, the automatic data partitioner has to know whether and when the compiler performs interprocedural optimizations such as procedure cloning <ref> [CHK92, HHKT91] </ref> or procedure inlining [Hal91]. In the following we will assume that the compiler performs procedure cloning for every distinct pattern of entry and exit 15 Algorithm DECOMP Input: program segment without procedure calls; problem sizes and number of processors to be used. <p> Li, Chen, and Choo for the Crystal project at Yale University [CCL89, LC90a, LC91b, LC91a, LC90b], and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project <ref> [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91] </ref>. Knobe, Lukas, Natarajan and Steele deal with the problem of automatic data layout for SIMD architectures such as the Connection machine. They discuss an algorithm that uses heuristics to solve conflicting requirements between minimizing communication and maximizing parallelism.
Reference: [HKK + 91] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Li, Chen, and Choo for the Crystal project at Yale University [CCL89, LC90a, LC91b, LC91a, LC90b], and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project <ref> [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91] </ref>. Knobe, Lukas, Natarajan and Steele deal with the problem of automatic data layout for SIMD architectures such as the Connection machine. They discuss an algorithm that uses heuristics to solve conflicting requirements between minimizing communication and maximizing parallelism. <p> The proposed automatic techniques will be implemented as part of the ParaScope parallel programming environment adapted to distributed memory multiprocessors <ref> [BKK + 89, KMT91, BFKK90, HKK + 91] </ref>. A prototype of the machine module of the static performance estimator is already available [BFKK91]. 7 Acknowledgement We wish to thank Seema Hiranandani and Chau-Wen Tseng for many discussions and helpful comments on the content of this paper.
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albu-querque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: For such programs recompilation for different problem sizes and number of processors used can be avoided. In the reminder of this section, we will examine the relationship between the listed factors using the characteristics of the Fortran D compiler <ref> [HKT92b, HKT91, HHKT91] </ref>. Figure 3 shows a Fortran D code for pointwise red-black relaxation using a block-partitioning scheme for the N fi N , double precision array v. Substituting line 4 by DISTRIBUTE d (:,BLOCK) specifies a column partitioning scheme for array v. <p> Distribution analysis is compiler, machine, and problem dependent. For instance, the compiler may not be able to generate efficient wavefront computations [Lam74] for a subset of distributions. Transformations like loop interchange and strip-mining can substantially improve the degree of parallelism induced by the wavefront <ref> [HKT91] </ref>. Distributions that sequentialize the computation are eliminated. Another consideration in our pruning heuristic are the sizes of the dimensions of the decomposition. If the size of a dimension is smaller than a machine dependent threshold, the dimension will always be localized. <p> Li, Chen, and Choo for the Crystal project at Yale University [CCL89, LC90a, LC91b, LC91a, LC90b], and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project <ref> [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91] </ref>. Knobe, Lukas, Natarajan and Steele deal with the problem of automatic data layout for SIMD architectures such as the Connection machine. They discuss an algorithm that uses heuristics to solve conflicting requirements between minimizing communication and maximizing parallelism.
Reference: [HKT92a] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Message coalescing ensures that each data value is sent to a processor only once. A detailed description of different communication optimizations can be found in <ref> [HKT92a] </ref>. This paper also discusses an alternative approach to generating messages for the red-black example, called vector message pipelining. Using message vectorization, the Fortran D compiler generates communication statements before the first loop nest at line 7 and after the second loop nest between line 17 and line 19.
Reference: [HKT92b] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Compilers and Runtime Software for Scalable Multiprocessors. </title> <publisher> Elsevier, </publisher> <address> Amsterdam, The Netherlands, </address> <note> to appear 1992. </note>
Reference-contexts: The 1 Sets Training Estimator Performance Static Partitioner Data Automatic EnvironmentFortranUser Fortran D Fortran D Compiler Message Passing Fortran Fortran D language and its compiler <ref> [FHK + 90, HKT92b] </ref> follow this approach. Given a Fortran D program the compiler mechanically generates the node program for a given distributed-memory target machine. A problem with this approach is that it does not support the user in the decision process about a good data layout. <p> For such programs recompilation for different problem sizes and number of processors used can be avoided. In the reminder of this section, we will examine the relationship between the listed factors using the characteristics of the Fortran D compiler <ref> [HKT92b, HKT91, HHKT91] </ref>. Figure 3 shows a Fortran D code for pointwise red-black relaxation using a block-partitioning scheme for the N fi N , double precision array v. Substituting line 4 by DISTRIBUTE d (:,BLOCK) specifies a column partitioning scheme for array v. <p> Li, Chen, and Choo for the Crystal project at Yale University [CCL89, LC90a, LC91b, LC91a, LC90b], and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project <ref> [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91] </ref>. Knobe, Lukas, Natarajan and Steele deal with the problem of automatic data layout for SIMD architectures such as the Connection machine. They discuss an algorithm that uses heuristics to solve conflicting requirements between minimizing communication and maximizing parallelism.
Reference: [IFKF90] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic paral lelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The data mapping is determined by the iterations that are assigned to the different processors [RS89, Ram90, D'H89, KKBP91]. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils <ref> [SS90, HA90, IFKF90, GAY91] </ref>. This more abstract representation is used to find a good data mapping.
Reference: [Kar87] <author> A. Karp. </author> <title> Programming for parallelism. </title> <journal> IEEE Computer, </journal> <volume> 20(5) </volume> <pages> 43-57, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: To execute a program on a distributed memory machine the program's data and code have to be mapped into the local memories of the machine's processors. The Fortran D compiler generates a node program according to the single program, multiple data (SPMD) programming model <ref> [Kar87] </ref>, exploiting the data parallelism inherent in the program, as opposed to its functional parallelism. 5 Each processor executes only those program statement instances that define a value of a data item that has been mapped onto that processor by the decomposition, alignment and distribution specifications.
Reference: [KKBP91] <author> D. Kulkarni, K. Kumar, A. Basu, and A. Paulraj. </author> <title> Loop partitioning for distributed memory multiprocessors as unimodular transformations. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Some approaches to automatic data decomposition concentrate on single loop nests. The iteration space is first partitioned into sets of iterations that can be executed independently. The data mapping is determined by the iterations that are assigned to the different processors <ref> [RS89, Ram90, D'H89, KKBP91] </ref>. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils [SS90, HA90, IFKF90, GAY91]. This more abstract representation is used to find a good data mapping.
Reference: [KKP + 81] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: Such non-local data items must be obtained by communication. For the given example we assume that the Fortran D compiler performs the following communication optimizations. Message vectorization uses the results of data dependence analysis <ref> [AK87, KKP + 81] </ref> to combine element messages into vectors. The level of the deepest loop-carried true dependence or loop enclosing a loop-independent true dependence determines the outermost loop where element messages resulting from the same array reference may be legally combined [BFKK90, Ger90].
Reference: [KLS90] <author> K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 102-118, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: Alignment analysis is largely machine-independent; it is performed by analyzing the array access patterns of computations in the phase. We intend to build on the inter-dimensional and intra-dimensional alignment techniques of Li and Chen [LC90a] and Knobe et al <ref> [KLS90] </ref>. The alignment problems can be formulated as an optimization problem on an undirected, weighted graph. Some instances of the problem of alignment have been shown to be NP-complete [LC90a]. <p> Knobe, Lukas at Compass, Steele at Thinking Machines Corporation, and Natarajan at Mo torola <ref> [KLS90, KN90] </ref>, 2. Li, Chen, and Choo for the Crystal project at Yale University [CCL89, LC90a, LC91b, LC91a, LC90b], and 3.
Reference: [KM91] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> Oc-tober </month> <year> 1991. </year>
Reference-contexts: As a result, it should be quite usable by computational scientists. In addition, we believe that our two-phase strategy for specifying data decomposition is natural and conducive to writing modular and portable code. Fortran D bears similarities to both CM Fortran [TMC89] and Kali <ref> [KM91] </ref>. The complete language is described in detail elsewhere [FHK + 90]. 3 Why is Finding a Good Data Layout Hard? The choice of a good data decomposition scheme depends on many factors.
Reference: [KMT91] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Interactive parallel programming using the ParaScope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The proposed automatic techniques will be implemented as part of the ParaScope parallel programming environment adapted to distributed memory multiprocessors <ref> [BKK + 89, KMT91, BFKK90, HKK + 91] </ref>. A prototype of the machine module of the static performance estimator is already available [BFKK91]. 7 Acknowledgement We wish to thank Seema Hiranandani and Chau-Wen Tseng for many discussions and helpful comments on the content of this paper.
Reference: [KMV90] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year> <month> 23 </month>
Reference-contexts: The reason is that traditional programming languages support single name spaces and, as a result, most programmers feel more comfortable working with a shared memory programming model. To this end, a number of researchers <ref> [CK88, CCL88, KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref> have proposed using a traditional sequential or parallel shared-memory language extended with annotations specifying how the data is to be mapped onto the distributed memory machine. <p> In addition, we will allow regular problems that give rise to wavefront style computations [Lam74].We do not handle representations of data objects as they occur, for instance, in irregular problems like sparse matrix and unstructured mesh computations <ref> [SCMB90, WSHB91, KMV90] </ref>. We will investigate the feasibility of these new automatic techniques in the context of an interactive system. The proposed automatic data decomposition tool is part of the Fortran D programming environment as shown in Figure 1.
Reference: [KN90] <author> K. Knobe and V. Natarajan. </author> <title> Data optimization: Minimizing residual interprocessor data motion on SIMD machines. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: Knobe, Lukas at Compass, Steele at Thinking Machines Corporation, and Natarajan at Mo torola <ref> [KLS90, KN90] </ref>, 2. Li, Chen, and Choo for the Crystal project at Yale University [CCL89, LC90a, LC91b, LC91a, LC90b], and 3.
Reference: [KZBG88] <author> U. Kremer, H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Advanced tools and techniques for automatic parallelization. </title> <journal> Parallel Computing, </journal> <volume> 7 </volume> <pages> 387-393, </pages> <year> 1988. </year>
Reference-contexts: The reason is that traditional programming languages support single name spaces and, as a result, most programmers feel more comfortable working with a shared memory programming model. To this end, a number of researchers <ref> [CK88, CCL88, KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref> have proposed using a traditional sequential or parallel shared-memory language extended with annotations specifying how the data is to be mapped onto the distributed memory machine.
Reference: [Lam74] <author> L. Lamport. </author> <title> The parallel execution of DO loops. </title> <journal> Communications of the ACM, </journal> <volume> 17(2) </volume> <pages> 83-93, </pages> <month> February </month> <year> 1974. </year>
Reference-contexts: Distribution analysis follows alignment analysis. It applies heuristics to prune unprofitable choices in the search space of possible distributions. Distribution analysis is compiler, machine, and problem dependent. For instance, the compiler may not be able to generate efficient wavefront computations <ref> [Lam74] </ref> for a subset of distributions. Transformations like loop interchange and strip-mining can substantially improve the degree of parallelism induced by the wavefront [HKT91]. Distributions that sequentialize the computation are eliminated. Another consideration in our pruning heuristic are the sizes of the dimensions of the decomposition.
Reference: [LC90a] <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: Alignment analysis is largely machine-independent; it is performed by analyzing the array access patterns of computations in the phase. We intend to build on the inter-dimensional and intra-dimensional alignment techniques of Li and Chen <ref> [LC90a] </ref> and Knobe et al [KLS90]. The alignment problems can be formulated as an optimization problem on an undirected, weighted graph. Some instances of the problem of alignment have been shown to be NP-complete [LC90a]. <p> We intend to build on the inter-dimensional and intra-dimensional alignment techniques of Li and Chen <ref> [LC90a] </ref> and Knobe et al [KLS90]. The alignment problems can be formulated as an optimization problem on an undirected, weighted graph. Some instances of the problem of alignment have been shown to be NP-complete [LC90a]. One major challenge in our proposed work will be to define the appropriate weights of the graph and to come up with a heuristic that solves the alignment problem in a way that is suitable for loosely synchronous problems. Alignment analysis is compiler dependent. <p> Knobe, Lukas at Compass, Steele at Thinking Machines Corporation, and Natarajan at Mo torola [KLS90, KN90], 2. Li, Chen, and Choo for the Crystal project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>, and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91].
Reference: [LC90b] <author> J. Li and M. Chen. </author> <title> Synthesis of explicit communication from shared-memory program references. </title> <type> Technical Report YALEU/DCS/TR-755, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: Knobe, Lukas at Compass, Steele at Thinking Machines Corporation, and Natarajan at Mo torola [KLS90, KN90], 2. Li, Chen, and Choo for the Crystal project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>, and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91].
Reference: [LC91a] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Knobe, Lukas at Compass, Steele at Thinking Machines Corporation, and Natarajan at Mo torola [KLS90, KN90], 2. Li, Chen, and Choo for the Crystal project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>, and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91].
Reference: [LC91b] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(4) </volume> <pages> 213-221, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Knobe, Lukas at Compass, Steele at Thinking Machines Corporation, and Natarajan at Mo torola [KLS90, KN90], 2. Li, Chen, and Choo for the Crystal project at Yale University <ref> [CCL89, LC90a, LC91b, LC91a, LC90b] </ref>, and 3. Balasundaram, Fox, Kennedy, and Kremer at Caltech and Rice University in the context of the Fortran D distributed memory compiler and environment project [FHK + 90, HKT92b, HKT91, HHKT91, BFKK90, BFKK91, HKK + 91].
Reference: [PSvG91] <author> E. Paalvast, H. Sips, and A. van Gemund. </author> <title> Automatic parallel program generation and optimization from data decompositions. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The reason is that traditional programming languages support single name spaces and, as a result, most programmers feel more comfortable working with a shared memory programming model. To this end, a number of researchers <ref> [CK88, CCL88, KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref> have proposed using a traditional sequential or parallel shared-memory language extended with annotations specifying how the data is to be mapped onto the distributed memory machine.
Reference: [RA90] <author> R. Ruhl and M. Annaratone. </author> <title> Parallelization of fortran code on distributed-memory parallel processors. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The reason is that traditional programming languages support single name spaces and, as a result, most programmers feel more comfortable working with a shared memory programming model. To this end, a number of researchers <ref> [CK88, CCL88, KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref> have proposed using a traditional sequential or parallel shared-memory language extended with annotations specifying how the data is to be mapped onto the distributed memory machine.
Reference: [Ram90] <author> J. Ramanujam. </author> <title> Compile-time Techniques for Parallel Execution of Loops on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, Ohio State University, Columbus, OH, </institution> <year> 1990. </year>
Reference-contexts: Some approaches to automatic data decomposition concentrate on single loop nests. The iteration space is first partitioned into sets of iterations that can be executed independently. The data mapping is determined by the iterations that are assigned to the different processors <ref> [RS89, Ram90, D'H89, KKBP91] </ref>. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils [SS90, HA90, IFKF90, GAY91]. This more abstract representation is used to find a good data mapping.
Reference: [RP89] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: The reason is that traditional programming languages support single name spaces and, as a result, most programmers feel more comfortable working with a shared memory programming model. To this end, a number of researchers <ref> [CK88, CCL88, KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref> have proposed using a traditional sequential or parallel shared-memory language extended with annotations specifying how the data is to be mapped onto the distributed memory machine.
Reference: [RS89] <author> J. Ramanujam and P. Sadayappan. </author> <title> A methodology for parallelizing programs for mul ticomputers and complex memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: Some approaches to automatic data decomposition concentrate on single loop nests. The iteration space is first partitioned into sets of iterations that can be executed independently. The data mapping is determined by the iterations that are assigned to the different processors <ref> [RS89, Ram90, D'H89, KKBP91] </ref>. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils [SS90, HA90, IFKF90, GAY91]. This more abstract representation is used to find a good data mapping.
Reference: [RSW88] <author> M. Rosing, R. Schnabel, and R. Weaver. Dino: </author> <title> Summary and examples. </title> <type> Technical 24 Report CU-CS-386-88, </type> <institution> Dept. of Computer Science, University of Colorado, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: The reason is that traditional programming languages support single name spaces and, as a result, most programmers feel more comfortable working with a shared memory programming model. To this end, a number of researchers <ref> [CK88, CCL88, KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref> have proposed using a traditional sequential or parallel shared-memory language extended with annotations specifying how the data is to be mapped onto the distributed memory machine.
Reference: [SCMB90] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 303-312, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: In addition, we will allow regular problems that give rise to wavefront style computations [Lam74].We do not handle representations of data objects as they occur, for instance, in irregular problems like sparse matrix and unstructured mesh computations <ref> [SCMB90, WSHB91, KMV90] </ref>. We will investigate the feasibility of these new automatic techniques in the context of an interactive system. The proposed automatic data decomposition tool is part of the Fortran D programming environment as shown in Figure 1.
Reference: [SS90] <author> L. Snyder and D. Socha. </author> <title> An algorithm producing balanced partitionings of data arrays. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: The data mapping is determined by the iterations that are assigned to the different processors [RS89, Ram90, D'H89, KKBP91]. Other techniques for single loop nests are based on recognizing specific computation patterns in the loop, called stencils <ref> [SS90, HA90, IFKF90, GAY91] </ref>. This more abstract representation is used to find a good data mapping.
Reference: [Tar74] <author> R. E. Tarjan. </author> <title> Testing flow graph reducibility. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 355-365, </pages> <year> 1974. </year>
Reference-contexts: The merging of phases in a strongly connected component of the phase control flow graph should 14 be done before merging any of its phases with a phase outside of the strongly connected component. This suggests a hierarchical algorithm for merging phases based on, for example, Tarjan intervals <ref> [Tar74] </ref>. Assuming that the innermost loop bodies can be represented by a linear phase control flow subgraph, the merging problem is solved by adding a shadow copy of the first phase after the last phase in the linear subgraph keeping the subgraph acyclic.
Reference: [TMC89] <institution> Thinking Machines Corporation, </institution> <address> Cambridge, MA. </address> <note> CM Fortran Reference Manual, ver sion 5.2-0.6 edition, </note> <month> September </month> <year> 1989. </year>
Reference-contexts: As a result, it should be quite usable by computational scientists. In addition, we believe that our two-phase strategy for specifying data decomposition is natural and conducive to writing modular and portable code. Fortran D bears similarities to both CM Fortran <ref> [TMC89] </ref> and Kali [KM91]. The complete language is described in detail elsewhere [FHK + 90]. 3 Why is Finding a Good Data Layout Hard? The choice of a good data decomposition scheme depends on many factors.
Reference: [Who91] <author> S. Wholey. </author> <title> Automatic Data Mapping for Distributed-Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: This more abstract representation is used to find a good data mapping. Our approach to automatic data alignment and distribution is closely related to the work done by Gupta and Banerjee at the University of Illinois at Urbana-Champaign [GB90, GB91, GB92], Wholey at Carnegie Mellon University <ref> [Who91] </ref>, and Chapman, Herbeck, and Zima at the University of Vienna [CHZ91]. The described techniques for automatic data decomposition work on whole programs taking machine characteristics and problem characteristics into account. The automatic data partitioner is an integral part of the compiler.
Reference: [Wol89] <author> M. J. Wolfe. </author> <title> Semi-automatic domain decomposition. </title> <booktitle> In Proceedings of the 4th Con ference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: In particular, we want to investigate whether there is a data-parallel programming style for sequential programs that facilitates automatic data alignment and distribution. The existence of a usable programming style in the context of vectorization has been observed by some researchers <ref> [CKK89, Wol89] </ref> and is partially responsible for the success of automatic vectorization. We believe that for 2 regular loosely synchronous problems written in a data-parallel programming style, the automatic data partitioner can determine an efficient decomposition scheme in most cases.
Reference: [WSHB91] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: In addition, we will allow regular problems that give rise to wavefront style computations [Lam74].We do not handle representations of data objects as they occur, for instance, in irregular problems like sparse matrix and unstructured mesh computations <ref> [SCMB90, WSHB91, KMV90] </ref>. We will investigate the feasibility of these new automatic techniques in the context of an interactive system. The proposed automatic data decomposition tool is part of the Fortran D programming environment as shown in Figure 1.
Reference: [ZBG88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 25 </month>
Reference-contexts: The reason is that traditional programming languages support single name spaces and, as a result, most programmers feel more comfortable working with a shared memory programming model. To this end, a number of researchers <ref> [CK88, CCL88, KMV90, PSvG91, RA90, RP89, RSW88, ZBG88, KZBG88, Ger89] </ref> have proposed using a traditional sequential or parallel shared-memory language extended with annotations specifying how the data is to be mapped onto the distributed memory machine.
References-found: 59

