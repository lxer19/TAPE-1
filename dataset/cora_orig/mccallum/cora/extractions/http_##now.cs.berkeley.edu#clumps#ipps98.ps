URL: http://now.cs.berkeley.edu/clumps/ipps98.ps
Refering-URL: http://now.cs.berkeley.edu/clumps/index.html
Root-URL: 
Email: fstevel,cullerg@CS.Berkeley.EDU  
Title: Managing Concurrent Access for Shared Memory Active Messages  
Author: Steven S. Lumetta and David E. Culler 
Address: Berkeley  
Affiliation: Computer Science Division University of California at  
Abstract: Passing messages through shared memory plays an important role on symmetric multiprocessors and on Clumps. The management of concurrent access to message queues is an important aspect of design for shared memory message-passing systems. Using both microbenchmarks and applications, this paper compares the performance of concurrent access algorithms for passing active messages on a Sun Enterprise 5000 server. The paper presents a new lock-free algorithm that provides many of the advantages of non-blocking algorithms while avoiding the overhead of true non-blocking behavior. The lock-free algorithm couples synchronization tightly to the data structure and demonstrates application performance superior to all others studied. The success of this algorithm implies that other practical problems might also benefit from a reexamination of the non-blocking literature. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson. </author> <title> The Performance of a Spin Lock Alternative for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: This system is part of a multi-protocol communication layer designed for Clumps [15]. The literature separates concurrent access algorithms into three disjoint groups. Traditional algorithms <ref> [1, 18, 20] </ref> are locking: a process must obtain a mutually exclusive lock to enter a critical section, thereby preventing other processes from entering concurrently. <p> Given hardware support for FETCH&ADD, the Ticket lock precludes starvation and is strictly fair [18]. Our implementation uses a FETCH&ADD operator built from CAS and hence cannot prevent starvation. It is fair only to processes that successfully obtain tickets. Fourth is the Anderson Lock <ref> [1] </ref>, which improves on the Ticket Lock by dividing the service counter into a separate flag for each process waiting on the lock. The lock operation obtains a slot assignment with CAS, then waits for the assigned slot to contain a lock indicator.
Reference: [2] <author> E. A. Brewer, F. T. Chong, L. T. Liu, S. D. Sharma, and J. D. Kubiatowicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1995. </year>
Reference-contexts: Non-blocking algorithms [9, 10, 17, 19] hence guarantee that some process makes progress in a finite amount of time, which implies that they do not enforce mutual exclusion. The remaining algorithms do not use locks but can still result in blocking behavior <ref> [2, 12, 22] </ref>. We follow Valois [22] and adopt the term lock-free for this third category. Non-blocking algorithms are advantageous on multipro-grammed systems, since locks interact poorly with timesharing. These algorithms follow a common design strategy and are simpler than their optimized locking counterparts. <p> Separate queues deliver superior results for one-to-one communication, but can be detrimental for more complex communication patterns. Polling each additional queue incurs a significant fraction of total message overhead in user-level communication layers [15]. Both Brewer et. al. <ref> [2] </ref> and Karamcheti and Chien [12] address concurrent message queues on the Cray T3D, a NUMA machine, with algorithms very similar to ours.
Reference: [3] <author> E. A. Brewer and B. C. Kuszmaul. </author> <title> How to Get Good Performance from the CM-5 Data Network. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: Application run input parameters. mensions and typifies regular applications that rely primarily on bulk communication. The communication pattern is all-to-all, but is scheduled into many one-to-one phases to improve performance <ref> [3] </ref>. CON finds the connected components of a distributed graph. CON performs a large amount of fine-grained communication in a statistically well-defined pattern. The balance between computation and communication depends strongly on the input parameters. We selected both a communication-bound run and a second, computation-bound run.
Reference: [4] <author> G. T. Byrd. </author> <title> Models of Communication Latency in Shared Memory Multiprocessors. </title> <type> Tech. Report CSL-TR-93-596, </type> <institution> Stanford University, </institution> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Cheriton and Kutter touch briefly on shared message segments, but only to mention their use in establishing a private segment for client-server communication [5]. Byrd builds optimistic high-level models of non-concurrent shared memory communication to aid in the design of a system that minimizes end-to-end latency <ref> [4] </ref>. A study by Lim et. al. [13] uses shared memory to route network traffic between SMP's through a proxy process, but again the data structures are duplicated for each communicating pair. Separate queues deliver superior results for one-to-one communication, but can be detrimental for more complex communication patterns.
Reference: [5] <author> D. R. Cheriton and R. A. Kutter. </author> <title> Optimized Memory-Based Messaging: Leveraging the Memory System for High-Performance Communication. </title> <journal> Computing Systems, </journal> <volume> 9(3) </volume> <pages> 179-215, </pages> <year> 1996. </year>
Reference-contexts: However, these systems often avoid concurrent access by creating separate data structures for each sender-receiver pair. Cheriton and Kutter touch briefly on shared message segments, but only to mention their use in establishing a private segment for client-server communication <ref> [5] </ref>. Byrd builds optimistic high-level models of non-concurrent shared memory communication to aid in the design of a system that minimizes end-to-end latency [4].
Reference: [6] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Supercomputing, </booktitle> <pages> pp. 262-73, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: The effect of this tradeoff at the application level is unclear, however. Applications are the natural metric for message-passing, since they use intrinsically interesting communication patterns. We now present execution times for three applications drawn from the Split-C application suite <ref> [6] </ref>. These programs are written in a bulk synchronous styleprocessors proceed through a sequence of coarse-grained phases, performing a global synchronization between each phase.
Reference: [7] <author> D. E. Culler, R. M. Karp, D. A. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Principles and Practice of Par. Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Communication stress test times in seconds. 4.1. LogP microbenchmarks We first measure point-to-point communication performance in terms of LogP. Assuming a small, fixed message length, the LogP model <ref> [7] </ref> characterizes communication networks as a set of four parameters: L, an upper bound on the network latency (wire time) between processors; o, the processor busy-time required to inject a message into the network or to pull one out; g, the minimum time between message injections for large numbers of messages;
Reference: [8] <author> D. E. Culler, L. T. Liu, R. P. Martin, and C. O. Yoshikawa. </author> <title> Assessing Fast Network Interfaces. </title> <journal> IEEE Micro, </journal> <volume> 16(1) </volume> <pages> 35-43, </pages> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: The overhead o is often separated into send overhead, o s , and receive overhead, o r . LogP parameters were measured using a microbench-mark from the suite described in <ref> [8] </ref> and appear in Table 1. The test uses one process as an RPC server and a second as a client and illustrates performance in the absence of contention.
Reference: [9] <author> M. Greenwald and D. Cheriton. </author> <title> The Synergy between Non-Blocking Synchronization and Operating System Structure. </title> <booktitle> In Operating Systems Design and Impl., </booktitle> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: When such locks are used, a process stalled inside a critical section can delay all others for an arbitrary amount of time, a behavior termed blocking. Non-blocking algorithms <ref> [9, 10, 17, 19] </ref> hence guarantee that some process makes progress in a finite amount of time, which implies that they do not enforce mutual exclusion. The remaining algorithms do not use locks but can still result in blocking behavior [2, 12, 22]. <p> Massalin and Pu provide evidence that non blocking approaches can be efficient when tailored to operating system data structures [17], and Greenwald and Cheriton show that more general approaches can also be successful in this regime <ref> [9] </ref>. Both kernel implementations make use of the double-compare-and-swap (DCAS) instruction, which performs simultaneous CAS operations on two independent words and is not generally available. Valois differentiates between non-blocking and lock-free algorithms.
Reference: [10] <author> M. Herlihy. </author> <title> A Methodology for Implementing Highly Concurrent Data Objects. </title> <type> Tech. Report CRL 91/10, </type> <institution> DEC CRL, </institution> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: When such locks are used, a process stalled inside a critical section can delay all others for an arbitrary amount of time, a behavior termed blocking. Non-blocking algorithms <ref> [9, 10, 17, 19] </ref> hence guarantee that some process makes progress in a finite amount of time, which implies that they do not enforce mutual exclusion. The remaining algorithms do not use locks but can still result in blocking behavior [2, 12, 22]. <p> Only the lock-free algorithm is both fast and robust to multiprogramming. 5. Related Work The locking algorithms used in our work are drawn from a survey by Mellor-Crummey and Scott [18]. Herlihy is a good source for the theory of non-blocking behavior as well as some general practical approaches <ref> [10] </ref>. More practical implementations of non-blocking data structures are also abundant. Michael and Scott survey a variety of such algorithms and evaluate their performance on an SGI Challenge [19].
Reference: [11] <author> M. P. Herlihy. </author> <title> Impossibility and Universality Results for Wait-Free Synchronization. </title> <booktitle> In Symposium on Principles of Distributed Computing, </booktitle> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Cache coherence is maintained with an invalidation protocol. Memory accesses on the Enterprise incur 300 nanoseconds of latency, while obtaining a line from another processor's L2 cache requires 480 nanoseconds. The Enterprise supports both the universal <ref> [11] </ref> COMPARE&SWAP primitive (CAS in the text) and the less powerful TEST&SET primitive (T&S), each at a cost of roughly 90 nanoseconds for cached data. This work arose in the context of passing active messages through shared memory [15].
Reference: [12] <author> V. Karamcheti and A. A. Chien. </author> <title> A Comparison of Architectural Support for Messaging in the TMC CM-5 and the Cray T3D. </title> <booktitle> In Int. Symp. on Comp. Arch., </booktitle> <pages> pp. 298-307, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Non-blocking algorithms [9, 10, 17, 19] hence guarantee that some process makes progress in a finite amount of time, which implies that they do not enforce mutual exclusion. The remaining algorithms do not use locks but can still result in blocking behavior <ref> [2, 12, 22] </ref>. We follow Valois [22] and adopt the term lock-free for this third category. Non-blocking algorithms are advantageous on multipro-grammed systems, since locks interact poorly with timesharing. These algorithms follow a common design strategy and are simpler than their optimized locking counterparts. <p> Separate queues deliver superior results for one-to-one communication, but can be detrimental for more complex communication patterns. Polling each additional queue incurs a significant fraction of total message overhead in user-level communication layers [15]. Both Brewer et. al. [2] and Karamcheti and Chien <ref> [12] </ref> address concurrent message queues on the Cray T3D, a NUMA machine, with algorithms very similar to ours. Their algorithms use remote FETCH&INCREMENT support to claim queue entries from a static queue, but rely on the receiver to reset the queue after all entries have been claimed and processed. <p> The main difference between these models lies in the need for bulk data transfers, since a thread can send a block of data by passing a pointer to the block in a short message. Karamcheti and Chien <ref> [12] </ref> describe an interesting pull-based algorithm using dynamic allocation and a link-based queue in the context of the T3D. 6.
Reference: [13] <author> B.-H. Lim, P. Heidelberger, P. Pattnaik, and M. Snir. </author> <title> Message Proxies for Efficient, Protected Communication on SMP Clusters. </title> <type> Tech. Report #RC 20522 (90972), </type> <institution> IBM Almaden, </institution> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Byrd builds optimistic high-level models of non-concurrent shared memory communication to aid in the design of a system that minimizes end-to-end latency [4]. A study by Lim et. al. <ref> [13] </ref> uses shared memory to route network traffic between SMP's through a proxy process, but again the data structures are duplicated for each communicating pair. Separate queues deliver superior results for one-to-one communication, but can be detrimental for more complex communication patterns.
Reference: [14] <author> L. T. Liu and D. E. Culler. </author> <title> Evaluation of the Intel Paragon on Active Message Communication. </title> <booktitle> In Intel Supercomputer Users Group Conference, </booktitle> <month> Jun. </month> <year> 1995. </year>
Reference-contexts: The test uses one process as an RPC server and a second as a client and illustrates performance in the absence of contention. The negative latency values indicate overlap in time between the send and receive overheads <ref> [14] </ref>, in this instance due to the poll operation. With the exception of the Posix mutex, the various algorithms are nearly equivalent, with send overhead and gap rising slightly as the complexity of the algorithm increases.
Reference: [15] <author> S. S. Lumetta, A. M. Mainwaring, and D. E. Culler. </author> <title> Multi-Protocol Active Messages on a Cluster of SMP's. </title> <booktitle> In SC97: High Performance Networking and Computing, </booktitle> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: The platform for these tests is a shared memory version of Active Messages-II [16] that operates on a Sun Enterprise 5000 server running the Solaris 2.5 operating system. This system is part of a multi-protocol communication layer designed for Clumps <ref> [15] </ref>. The literature separates concurrent access algorithms into three disjoint groups. Traditional algorithms [1, 18, 20] are locking: a process must obtain a mutually exclusive lock to enter a critical section, thereby preventing other processes from entering concurrently. <p> The Enterprise supports both the universal [11] COMPARE&SWAP primitive (CAS in the text) and the less powerful TEST&SET primitive (T&S), each at a cost of roughly 90 nanoseconds for cached data. This work arose in the context of passing active messages through shared memory <ref> [15] </ref>. Active messages are similar to a highly-optimized RPC mechanism in which each communication endpoint acts as both client and server. Individual messages in the request-reply protocol can be either short messages of up to eight words or bulk data transfers of up to 8 kB. <p> Separate queues deliver superior results for one-to-one communication, but can be detrimental for more complex communication patterns. Polling each additional queue incurs a significant fraction of total message overhead in user-level communication layers <ref> [15] </ref>. Both Brewer et. al. [2] and Karamcheti and Chien [12] address concurrent message queues on the Cray T3D, a NUMA machine, with algorithms very similar to ours.
Reference: [16] <author> A. M. Mainwaring and D. E. Culler. </author> <title> Active Message Applications Programming Interface and Communication Subsystem Organization. </title> <type> Tech. Report #CSD-96-918, </type> <address> U. C. Berkeley, </address> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: Real applications are the primary tool for our investigation, but a set of microbenchmarks helps to characterize performance in the extremes of high and low contention. The platform for these tests is a shared memory version of Active Messages-II <ref> [16] </ref> that operates on a Sun Enterprise 5000 server running the Solaris 2.5 operating system. This system is part of a multi-protocol communication layer designed for Clumps [15]. The literature separates concurrent access algorithms into three disjoint groups.
Reference: [17] <author> H. Massalin and C. Pu. </author> <title> A Lock-free Multiprocessor OS Kernel. </title> <type> Tech. Report CUCS-005-91, </type> <institution> Columbia University, </institution> <month> Jun. </month> <year> 1991. </year>
Reference-contexts: When such locks are used, a process stalled inside a critical section can delay all others for an arbitrary amount of time, a behavior termed blocking. Non-blocking algorithms <ref> [9, 10, 17, 19] </ref> hence guarantee that some process makes progress in a finite amount of time, which implies that they do not enforce mutual exclusion. The remaining algorithms do not use locks but can still result in blocking behavior [2, 12, 22]. <p> More practical implementations of non-blocking data structures are also abundant. Michael and Scott survey a variety of such algorithms and evaluate their performance on an SGI Challenge [19]. Massalin and Pu provide evidence that non blocking approaches can be efficient when tailored to operating system data structures <ref> [17] </ref>, and Greenwald and Cheriton show that more general approaches can also be successful in this regime [9]. Both kernel implementations make use of the double-compare-and-swap (DCAS) instruction, which performs simultaneous CAS operations on two independent words and is not generally available. Valois differentiates between non-blocking and lock-free algorithms.
Reference: [18] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Trans. on Comp. Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: This system is part of a multi-protocol communication layer designed for Clumps [15]. The literature separates concurrent access algorithms into three disjoint groups. Traditional algorithms <ref> [1, 18, 20] </ref> are locking: a process must obtain a mutually exclusive lock to enter a critical section, thereby preventing other processes from entering concurrently. <p> Concurrent Algorithms This section describes the six concurrent access algorithms that we evaluate in our study. Five are locking algorithms, with four drawn from the literature <ref> [18] </ref> and the fifth being the Solaris implementation of Posix mutexes. The last is our lock-free algorithm. <p> We found that placing both counters on a single cache line results in slightly better performance, and we report measurements only for that layout. The Ticket Lock exemplifies the CAS starvation phenomena on modern machines. Given hardware support for FETCH&ADD, the Ticket lock precludes starvation and is strictly fair <ref> [18] </ref>. Our implementation uses a FETCH&ADD operator built from CAS and hence cannot prevent starvation. It is fair only to processes that successfully obtain tickets. <p> Only the lock-free algorithm is both fast and robust to multiprogramming. 5. Related Work The locking algorithms used in our work are drawn from a survey by Mellor-Crummey and Scott <ref> [18] </ref>. Herlihy is a good source for the theory of non-blocking behavior as well as some general practical approaches [10]. More practical implementations of non-blocking data structures are also abundant. Michael and Scott survey a variety of such algorithms and evaluate their performance on an SGI Challenge [19].
Reference: [19] <author> M. M. Michael and M. L. Scott. </author> <title> Relative Performance of Preemption-Safe Locking and Non-Blocking Synchronization on Multiprogrammed Shared Memory Multiprocessors. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <year> 1997. </year>
Reference-contexts: When such locks are used, a process stalled inside a critical section can delay all others for an arbitrary amount of time, a behavior termed blocking. Non-blocking algorithms <ref> [9, 10, 17, 19] </ref> hence guarantee that some process makes progress in a finite amount of time, which implies that they do not enforce mutual exclusion. The remaining algorithms do not use locks but can still result in blocking behavior [2, 12, 22]. <p> As the name implies, spin locks spin in a tight loop while waiting for a lock, potentially wasting valuable cycles. Spin locks also interact poorly with the operating system scheduler and admit deadlock when used with preemptive threads. Numerous preemption-safe locking solutions exist <ref> [19] </ref> and are supported in modern thread packages through integration with the operating system. The Solaris implementation of Posix mutexes, our final locking algorithm, exemplifies these solutions. A process that tries to obtain a lock held by another process enqueues itself on the lock and gives up its proces sor. <p> Herlihy is a good source for the theory of non-blocking behavior as well as some general practical approaches [10]. More practical implementations of non-blocking data structures are also abundant. Michael and Scott survey a variety of such algorithms and evaluate their performance on an SGI Challenge <ref> [19] </ref>. Massalin and Pu provide evidence that non blocking approaches can be efficient when tailored to operating system data structures [17], and Greenwald and Cheriton show that more general approaches can also be successful in this regime [9].
Reference: [20] <author> D. P. Reed and R. K. Kanodia. </author> <title> Synchronization with Eventcounts and Sequencers. </title> <journal> Communications of the ACM, </journal> <volume> 22(2) </volume> <pages> 115-23, </pages> <month> Feb. </month> <year> 1979. </year>
Reference-contexts: This system is part of a multi-protocol communication layer designed for Clumps [15]. The literature separates concurrent access algorithms into three disjoint groups. Traditional algorithms <ref> [1, 18, 20] </ref> are locking: a process must obtain a mutually exclusive lock to enter a critical section, thereby preventing other processes from entering concurrently. <p> The second, Test & Test & Set, waits until a lock is released before making another attempt to obtain it, thereby reducing the amount of cache-coherence traffic generated while a lock is held. Third, the Ticket Lock <ref> [20] </ref>, further reduces cache-coherence traffic by ordering the processes waiting on a lock. To obtain a lock, a process obtains a ticket and waits for a service counter to show its ticket number. The ticket counter is incremented atomically to ensure that each process receives a different ticket.
Reference: [21] <author> A. Singhal, D. Broniarczyk, F. Cerauskis, J. Price, L. Yuan, C. Cheng, D. Doblar, S. Fosth, N. Agarwal, K. Harvey, E. Hagersten, and B. Liencres. Gigaplane: </author> <title> A High Performance Bus for Large SMPs. </title> <booktitle> In Hot Interconnects IV, </booktitle> <pages> pp. 41-52, </pages> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Our experimental platform is a Sun Enterprise 5000 server containing eight 167 MHz UltraSPARC processors with 512 kB of L2 cache each and a total of 512 MB of main memory in two banks. Processors are connected via Sun's Gigaplane Interconnect <ref> [21] </ref>, which is representative of many modern cache-coherent system interconnects. Its most unusual characteristic is support for more than one outstanding transaction on a single cache line, effectively pipelining concurrent memory traffic between processors. Cache coherence is maintained with an invalidation protocol.
Reference: [22] <author> J. D. Valois. </author> <title> Implementing Lock-Free Queues. </title> <booktitle> In International Conf. on Par. and Dist. Computing Sys., </booktitle> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: Non-blocking algorithms [9, 10, 17, 19] hence guarantee that some process makes progress in a finite amount of time, which implies that they do not enforce mutual exclusion. The remaining algorithms do not use locks but can still result in blocking behavior <ref> [2, 12, 22] </ref>. We follow Valois [22] and adopt the term lock-free for this third category. Non-blocking algorithms are advantageous on multipro-grammed systems, since locks interact poorly with timesharing. These algorithms follow a common design strategy and are simpler than their optimized locking counterparts. <p> Non-blocking algorithms [9, 10, 17, 19] hence guarantee that some process makes progress in a finite amount of time, which implies that they do not enforce mutual exclusion. The remaining algorithms do not use locks but can still result in blocking behavior [2, 12, 22]. We follow Valois <ref> [22] </ref> and adopt the term lock-free for this third category. Non-blocking algorithms are advantageous on multipro-grammed systems, since locks interact poorly with timesharing. These algorithms follow a common design strategy and are simpler than their optimized locking counterparts. A typical non-blocking operation works as follows. <p> Both kernel implementations make use of the double-compare-and-swap (DCAS) instruction, which performs simultaneous CAS operations on two independent words and is not generally available. Valois differentiates between non-blocking and lock-free algorithms. The lock-free, array-based queue algorithm presented in <ref> [22] </ref> is similar to ours but requires significantly more complex operations and unaligned CAS instructions, making it infeasible on a real machine. The idea of passing messages through shared memory is not novel. Numerous applications make use of these techniques, as do a number of operating systems.
References-found: 22

