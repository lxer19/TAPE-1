URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-08.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Title: MULTI-COORDINATION METHODS FOR PARALLEL SOLUTION OF BLOCK-ANGULAR PROGRAMS  
Author: By Golbon Zakeri 
Degree: A thesis submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  
Date: 1995  
Address: WISCONSIN MADISON  
Affiliation: (Computer Sciences and Mathematics) at the UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Agha Iqbal Ali and J. Kennington. </author> <title> MNETGEN program documentation. </title> <type> Technical Report IEOR 77003, </type> <institution> Department of Industrial Engineering and Operations Research, Southern Methodist University, Dallas, Texas 75275, </institution> <year> 1977. </year>
Reference-contexts: The upper bounds u are assumed to be finite. If the function D is separable (e.g. the BAP is linear) then we can rewrite the coupling constraints as: D <ref> [1] </ref> (x [1] ) + + D [K] (x [k] ) d and if D is linear then: D [1] x [1] + + D [K] x [k] d 4 Also if c is separable (e.g. the BAP is linear) then we can rewrite the objective as: and if c is <p> The upper bounds u are assumed to be finite. If the function D is separable (e.g. the BAP is linear) then we can rewrite the coupling constraints as: D <ref> [1] </ref> (x [1] ) + + D [K] (x [k] ) d and if D is linear then: D [1] x [1] + + D [K] x [k] d 4 Also if c is separable (e.g. the BAP is linear) then we can rewrite the objective as: and if c is linear then: <p> The upper bounds u are assumed to be finite. If the function D is separable (e.g. the BAP is linear) then we can rewrite the coupling constraints as: D <ref> [1] </ref> (x [1] ) + + D [K] (x [k] ) d and if D is linear then: D [1] x [1] + + D [K] x [k] d 4 Also if c is separable (e.g. the BAP is linear) then we can rewrite the objective as: and if c is linear then: cx = c [1] x [1] + + c [K] x [K] Problems with block-angular structure come <p> If the function D is separable (e.g. the BAP is linear) then we can rewrite the coupling constraints as: D <ref> [1] </ref> (x [1] ) + + D [K] (x [k] ) d and if D is linear then: D [1] x [1] + + D [K] x [k] d 4 Also if c is separable (e.g. the BAP is linear) then we can rewrite the objective as: and if c is linear then: cx = c [1] x [1] + + c [K] x [K] Problems with block-angular structure come up very <p> D [K] (x [k] ) d and if D is linear then: D <ref> [1] </ref> x [1] + + D [K] x [k] d 4 Also if c is separable (e.g. the BAP is linear) then we can rewrite the objective as: and if c is linear then: cx = c [1] x [1] + + c [K] x [K] Problems with block-angular structure come up very often in large-scale optimization and have important practical applications. Two well-known types of BAP are multicommodity network flow problems and problems arising from multi-period financial modeling. <p> (x [k] ) d and if D is linear then: D <ref> [1] </ref> x [1] + + D [K] x [k] d 4 Also if c is separable (e.g. the BAP is linear) then we can rewrite the objective as: and if c is linear then: cx = c [1] x [1] + + c [K] x [K] Problems with block-angular structure come up very often in large-scale optimization and have important practical applications. Two well-known types of BAP are multicommodity network flow problems and problems arising from multi-period financial modeling. <p> They define: h [k] (x [k] ) = &gt; &gt; &lt; c [k] (x [k] ) if x [k] satisfies the network constraints of block k +1 otherwise from which they form closed, proper, convex, extended real valued functions: G 1 (x <ref> [1] </ref> ; ; x [K] ; ~ d [1] ; ; ~ d [K] ) = &gt; &gt; &lt; P K +1 otherwise and 8 &gt; &gt; : P K +1 otherwise The above yields an equivalent form for the BAP: min G 1 (x [1] ; ; x [K] ; <p> They define: h [k] (x [k] ) = &gt; &gt; &lt; c [k] (x [k] ) if x [k] satisfies the network constraints of block k +1 otherwise from which they form closed, proper, convex, extended real valued functions: G 1 (x <ref> [1] </ref> ; ; x [K] ; ~ d [1] ; ; ~ d [K] ) = &gt; &gt; &lt; P K +1 otherwise and 8 &gt; &gt; : P K +1 otherwise The above yields an equivalent form for the BAP: min G 1 (x [1] ; ; x [K] ; ~ d [1] ; ; ~ d [K] <p> valued functions: G 1 (x <ref> [1] </ref> ; ; x [K] ; ~ d [1] ; ; ~ d [K] ) = &gt; &gt; &lt; P K +1 otherwise and 8 &gt; &gt; : P K +1 otherwise The above yields an equivalent form for the BAP: min G 1 (x [1] ; ; x [K] ; ~ d [1] ; ; ~ d [K] ) + G 2 (y [1] ; ; y [K] ; d [1] ; ; d [K] ) subject to x [k] = y [k] (8k = 1; ; K) ~ d [k] = d [k] (8k <p> x [K] ; ~ d <ref> [1] </ref> ; ; ~ d [K] ) = &gt; &gt; &lt; P K +1 otherwise and 8 &gt; &gt; : P K +1 otherwise The above yields an equivalent form for the BAP: min G 1 (x [1] ; ; x [K] ; ~ d [1] ; ; ~ d [K] ) + G 2 (y [1] ; ; y [K] ; d [1] ; ; d [K] ) subject to x [k] = y [k] (8k = 1; ; K) ~ d [k] = d [k] (8k = 1; ; K) Now in order to <p> ) = &gt; &gt; &lt; P K +1 otherwise and 8 &gt; &gt; : P K +1 otherwise The above yields an equivalent form for the BAP: min G 1 (x <ref> [1] </ref> ; ; x [K] ; ~ d [1] ; ; ~ d [K] ) + G 2 (y [1] ; ; y [K] ; d [1] ; ; d [K] ) subject to x [k] = y [k] (8k = 1; ; K) ~ d [k] = d [k] (8k = 1; ; K) Now in order to solve the linearly-constrained convex problem: min F 1 (w) + F <p> +1 otherwise and 8 &gt; &gt; : P K +1 otherwise The above yields an equivalent form for the BAP: min G 1 (x <ref> [1] </ref> ; ; x [K] ; ~ d [1] ; ; ~ d [K] ) + G 2 (y [1] ; ; y [K] ; d [1] ; ; d [K] ) subject to x [k] = y [k] (8k = 1; ; K) ~ d [k] = d [k] (8k = 1; ; K) Now in order to solve the linearly-constrained convex problem: min F 1 (w) + F 2 (z) subject to Aw + b <p> Dantzig-Wolfe Decomposition The Dantzig-Wolfe decomposition method [3] solves the BAP when it is a linear program. Suppose the coupling constraints are put into the objective in the form of the Lagrangian relaxation: min cx (Dx d) min (c <ref> [1] </ref> D [1] )x [1] + + (c [K] D [K] )x [K] s/t Block constraints s/t Block constraints If the vector is chosen as the vector of optimal dual variables then the objective value of the above is the same as the optimal objective for the original problem. <p> Dantzig-Wolfe Decomposition The Dantzig-Wolfe decomposition method [3] solves the BAP when it is a linear program. Suppose the coupling constraints are put into the objective in the form of the Lagrangian relaxation: min cx (Dx d) min (c <ref> [1] </ref> D [1] )x [1] + + (c [K] D [K] )x [K] s/t Block constraints s/t Block constraints If the vector is chosen as the vector of optimal dual variables then the objective value of the above is the same as the optimal objective for the original problem. <p> Dantzig-Wolfe Decomposition The Dantzig-Wolfe decomposition method [3] solves the BAP when it is a linear program. Suppose the coupling constraints are put into the objective in the form of the Lagrangian relaxation: min cx (Dx d) min (c <ref> [1] </ref> D [1] )x [1] + + (c [K] D [K] )x [K] s/t Block constraints s/t Block constraints If the vector is chosen as the vector of optimal dual variables then the objective value of the above is the same as the optimal objective for the original problem. <p> Then the original 11 BAP may be written as: min c <ref> [1] </ref> X 1 1 + + c [K] X K K k 0; e k = 1 (8k = 1; : : : ; K) Now, at the first iteration, for each block k the subproblem min ~c [k] x [k] 0 x [k] u [k] is solved in order to <p> Subsequently the restricted master min (c <ref> [1] </ref> ~ X 1 ) 1 + + (c [K] ~ X K ) K 0; e k = 1 (8k = 1; : : : ; K) is solved where ~ X k is an approximation (in iteration one this is just the one column obtained so far,) of X <p> Coordinator Problem Once the subproblem is solved and search directions y t [k] for each block k are determined then we need to determine what stepsize is to be taken in each block. Consider the search matrix at iteration t: Y t = B B B @ <ref> [1] </ref> x t . . . 0 : : : y t [K] C C C A the Schultz-Meyer decomposition method finds the relevant stepsizes by approx imately solving the problem below (we will denote this problem SMC). min f (x t + Y t w) subject to 0 x t <p> k = x t + (y t k x t )w that f (x t+1 " x2E (k;t) # where 2 (0; 1) is independent of t: To obtain a computationally checkable condition for (28) suppose 0 is not optimal for coordinator i, and let f t k (~) = <ref> [1] </ref> ; : : : ; x t [k] + (y t [k] ; : : : ; x t [K] ), then: [ min f (x) f (x t )] = [ min f (x) f t and x2E (k;t) 0~1 k (~) Now f is convex hence f t <p> follow from above that: [ min f (x) f (x t )] = [ min f (x) f t [f t k (0) + minfrf t k (~)(0 ~); rf t Thus, if f~ i g is a sequence of reals converging to a minimizer of f t k on <ref> [0; 1] </ref>, then [f t k (0) + minfrf t k (~ i )(0 ~ i ); rf t converges to [ min f (x) f t Therefore, the sufficient decrease condition is satisfied by any point x t+1 k corre sponding to w k = ~ i such that: f <p> We stopped at pds-40 mainly because the particular network solver we used does not work very efficiently for problems larger than those in pds-40. We also tested our code on another group of problems generated by MNETGEN <ref> [1] </ref> which is a derivative of NETGEN [13] discussed later in this section. 61 4.2 Analysis of the Results 4.2.1 Single-variable multi-coordination In table (8) we present the solution results of the PDS problems we tested using single-variable multi-coordination method. <p> Hence we refer to p;q as the group of blocks whose search direction is considered by the coordinator on node p at time t with t mod n = q. Define Y (t) as: Y (t) = B B B @ (t) <ref> [1] </ref> : : : 0 . . . 0 (t) [K] C C C A Let p be the index of our processor and ^w t p be the solution to the following: min rf (x t )Y (t) w + w 0 H t w 77 Given 0 &lt; fl
Reference: [2] <author> Chunhui Chen and O.L.Mangasarian. </author> <title> A class of smoothing functions for nonlinear and mixed complimentarity problems. </title> <type> Technical Report TR94-11, </type> <institution> Center for Parallel Optimization, Computer Sciences Department, University of Wisconsin, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Then the problem: min x c (x) A <ref> [2] </ref> x [2] = b [2] A [K] x [K] = b [K] 0 x u is called a block-angular programming problem, denoted (BAP). The theory can be extended to non-zero lower bounds on x, however we assume 0 lower bounds for convenience. <p> Then the problem: min x c (x) A <ref> [2] </ref> x [2] = b [2] A [K] x [K] = b [K] 0 x u is called a block-angular programming problem, denoted (BAP). The theory can be extended to non-zero lower bounds on x, however we assume 0 lower bounds for convenience. <p> Then the problem: min x c (x) A <ref> [2] </ref> x [2] = b [2] A [K] x [K] = b [K] 0 x u is called a block-angular programming problem, denoted (BAP). The theory can be extended to non-zero lower bounds on x, however we assume 0 lower bounds for convenience. <p> Once the approximate solution of (7) is determined they adjust the penalty parameters. The steps are repeated in a loop until the termination criteria is met. Mangasarian and Chen have since developed a class of smoothing functions one of which is the smoothed exact penalty function <ref> [2] </ref>. Augmented Lagrangian Meyer and Zakarian [24] solve the BAP using an augmented Lagrangian approach.
Reference: [3] <author> G.B. Dantzig and P. Wolfe. </author> <title> A decomposition principle for linear programs. </title> <journal> Operations Research, </journal> <volume> 8 </volume> <pages> 101-111, </pages> <year> 1960. </year>
Reference-contexts: We will discuss a few price directed decomposition schemes below-the most famous of which is perhaps Dantzig-Wolfe. The last such example discussed here is barrier decomposition methods on which the work of this thesis is based. Dantzig-Wolfe Decomposition The Dantzig-Wolfe decomposition method <ref> [3] </ref> solves the BAP when it is a linear program.
Reference: [4] <author> Renato DeLeone, Manlio Gaudioso, and Maria Flavia Monaco. </author> <title> Nonsmooth optimization methods for parallel decomposition of multicommodity flow problems. </title> <type> Technical Report 1080, </type> <institution> Centre for Parallel Optimization, Computer Sciences Department, University of Wisconsin, </institution> <year> 1992. </year> <month> 90 </month>
Reference-contexts: Then the master problem is: min P K subject to y [k] 2 V [k] (8k = 1; ; K) P K Note that the objective of the master problem is usually nondifferentiable. Bundle Methods De Leone et al. <ref> [4] </ref> apply a resource directed decomposition method to multicom-modity network flow problems where c and D are linear. They use the bundle method to solve the nondifferentiable master problem (2). They do not solve the subproblems (1) to optimality; rather, they terminate with an approximately optimal solution.
Reference: [5] <author> Renato DeLeone, Robert R. Meyer, and Spyridon Kontogiorgis. </author> <title> Alternating direction splittings for block-angular parallel optimization. </title> <type> Technical Report 1217, </type> <institution> Centre for Parallel Optimization, Computer Sciences Department, University of Wisconsin, </institution> <year> 1994. </year>
Reference-contexts: The algorithm continues in a loop in this fashion until the termination criteria are met. 8 1.3.2 Alternating Directions Method De Leone, Meyer and Kontogiorgis in <ref> [5] </ref> use the "Alternating Directions" method to solve the BAP.
Reference: [6] <author> M.C. Ferris and O.L. Mangasarian. </author> <title> Parallel variable distribution. </title> <type> Technical Report 1175, </type> <institution> Centre for Parallel Optimization, University of Wisconsin, Computer Sciences Dept., </institution> <year> 1993. </year>
Reference-contexts: This approach allows use of parallelism on the time consuming portion of the algorithm (step 1 of the augmented Lagrangian algorithm) in an efficient manner. This is an application of the parallel variable distribution method developed by Ferris and Mangasarian <ref> [6] </ref> to the augmented Lagrangian formulation of the BAP. Barrier Function Methods We dedicate the next chapter to explanation of barrier function methods and Schultz-Meyer decomposition scheme for solution of barrier problems. The method developed here is a decomposition method related to Schultz-Meyer which uses parallel coordination.
Reference: [7] <author> A.V. </author> <title> Fiacco and G.P. McCormick. Nonlinear Programming: Sequential Unconstrained Minimization Techniques. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: In order to remove the coupling constraints we may place them in the objective in the form of a barrier function. Barrier functions were first introduced by Frisch [9] and later analyzed in great detail by Fiacco and McCormick <ref> [7] </ref>. <p> This hope in fact materializes through the following result obtained by Fiacco and McCormick (theorem 8 of <ref> [7] </ref>) for general constrained problems whose sets of local minima satisfy some specified properties. Before we present the theorem we have to make some definitions and introduce the concept of compact perturbation superset for a set of local minimizers. Definition 2 (Isolated Set) (from [7]) A set X Y is an <p> Fiacco and McCormick (theorem 8 of <ref> [7] </ref>) for general constrained problems whose sets of local minima satisfy some specified properties. Before we present the theorem we have to make some definitions and introduce the concept of compact perturbation superset for a set of local minimizers. Definition 2 (Isolated Set) (from [7]) A set X Y is an isolated (sub)set of Y if there exists some set E such that X E ffi and ; = (EnX) " Y . <p> The above result on the existence of compact perturbation set is due Fiacco and McCormick (theorem 7 of <ref> [7] </ref>).
Reference: [8] <author> L.R. Ford and D.R. Fulkerson. </author> <title> Flows in Networks. </title> <publisher> Princton University Press, </publisher> <address> Princton, NJ, </address> <year> 1962. </year>
Reference-contexts: Two well-known types of BAP are multicommodity network flow problems and problems arising from multi-period financial modeling. In the case of the multicommodity network flow problems the matrices A [k] are node-arc incidence matrices for the underlying network structure. For more information on network flow problems see <ref> [8] </ref> and for more on multicommodity network problems see [12]. 1.3 A Survey of Solution Methods for Block Angular Programs 1.3.1 Resource Directive Decomposition The BAP may be viewed as a problem of optimally allocating the scarce resource d to the blocks.
Reference: [9] <author> K.R. Frisch. </author> <title> The logarithmic potential method of convex programming. </title> <type> Unpublished manuscript, </type> <institution> University Institute of Economics, Oslo, </institution> <year> 1955. </year>
Reference-contexts: In order to remove the coupling constraints we may place them in the objective in the form of a barrier function. Barrier functions were first introduced by Frisch <ref> [9] </ref> and later analyzed in great detail by Fiacco and McCormick [7].
Reference: [10] <author> M.D. Grigoriadis and L.G. Khachiyan. </author> <title> An exponential-function reduction method for block-angular convex programs. </title> <type> Technical Report 211, </type> <institution> Laboratory for Computer Sciences Research, Rutgers, Computer Sciences Dept., </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: In the last section of this chapter we present our computational results for each of the above mentioned schemes and compare them to those of De Leone [14], Zenios and Pinar [18], McBride and Mamer [15], Schultz and Meyer [20] and Grigoriadis and Khachiyan <ref> [10] </ref>. 53 4.1 The Implementation 4.1.1 Algorithm and Implementation As we discussed in chapter 2, our algorithm follows the basic three phase method of Schultz and Meyer. Figure (4) presents a sketch of the three phase method. <p> Results obtained by Zenios and Pinar (in column labeled ZP) have been implemented on the Cray-YMP with 8 processors using the vector units hence the processors are 2-4 times faster than the nodes of CM5. Grigoriadis and Khachiyan <ref> [10] </ref> implemented their algorithm on the IBM RS 6000-550 which is 3 times faster than a node on the CM-5. McBride and Mamer [15] implemented their algorithm on the HP-730 work station which is also three times as fast as a node of the CM5.
Reference: [11] <author> W. Karush. </author> <title> Minima of functions of several variables with inequalities as side conditions. </title> <type> Master's thesis, </type> <institution> University of Chicago, </institution> <year> 1939. </year> <month> 91 </month>
Reference-contexts: A KKT point for NLP is a tuple (x; w; v) that satisfies the Karush-Kuhn Tucker conditions <ref> [11] </ref>: g (x) 0 h (x) = 0 wg (x) = 0 A point x for which there exists (w; v) satisfying KKT conditions is also referred to as a KKT point for NLP.
Reference: [12] <author> J.L. Kennington and R.V. Helgason. </author> <title> Algorithms for Network Programming. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: In the case of the multicommodity network flow problems the matrices A [k] are node-arc incidence matrices for the underlying network structure. For more information on network flow problems see [8] and for more on multicommodity network problems see <ref> [12] </ref>. 1.3 A Survey of Solution Methods for Block Angular Programs 1.3.1 Resource Directive Decomposition The BAP may be viewed as a problem of optimally allocating the scarce resource d to the blocks.
Reference: [13] <author> D. Klingman, A. Napier, and J. Stutz. </author> <title> NETGEN|A program for generation of large-scale (un)capacitated assignment, transportation and minimum cost network problems. </title> <journal> Management Science, </journal> <volume> 20 </volume> <pages> 814-822, </pages> <year> 1974. </year>
Reference-contexts: We stopped at pds-40 mainly because the particular network solver we used does not work very efficiently for problems larger than those in pds-40. We also tested our code on another group of problems generated by MNETGEN [1] which is a derivative of NETGEN <ref> [13] </ref> discussed later in this section. 61 4.2 Analysis of the Results 4.2.1 Single-variable multi-coordination In table (8) we present the solution results of the PDS problems we tested using single-variable multi-coordination method.
Reference: [14] <author> R. De Leone, R.R. Meyer, S. Kontogiorgis, A. Zakarian, and G. Zakeri. </author> <title> Coordination in coarse grained decomposition. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4(4) </volume> <pages> 777-793, </pages> <year> 1994. </year>
Reference-contexts: If all the 12 subproblems have nonnegative optimal objectives then the current solution of the restricted master is an optimal solution for the the original BAP. For a more detailed discussion of the Dantzig-Wolfe decomposition method see <ref> [14] </ref>. Simplicial Decomposition This method originally developed by Von Hohenbalken [22] considers the problem s/t x 2 X where X is a compact, convex, nonempty set and f is differentiable. <p> We will then present the values of the parameters in our code. In the last section of this chapter we present our computational results for each of the above mentioned schemes and compare them to those of De Leone <ref> [14] </ref>, Zenios and Pinar [18], McBride and Mamer [15], Schultz and Meyer [20] and Grigoriadis and Khachiyan [10]. 53 4.1 The Implementation 4.1.1 Algorithm and Implementation As we discussed in chapter 2, our algorithm follows the basic three phase method of Schultz and Meyer.
Reference: [15] <author> Richard D. McBride and John W. Mamer. </author> <title> Solving multicommodity flow problems with a primal embedded network simplex algorithm. </title> <note> submitted. </note>
Reference-contexts: We will then present the values of the parameters in our code. In the last section of this chapter we present our computational results for each of the above mentioned schemes and compare them to those of De Leone [14], Zenios and Pinar [18], McBride and Mamer <ref> [15] </ref>, Schultz and Meyer [20] and Grigoriadis and Khachiyan [10]. 53 4.1 The Implementation 4.1.1 Algorithm and Implementation As we discussed in chapter 2, our algorithm follows the basic three phase method of Schultz and Meyer. Figure (4) presents a sketch of the three phase method. <p> Grigoriadis and Khachiyan [10] implemented their algorithm on the IBM RS 6000-550 which is 3 times faster than a node on the CM-5. McBride and Mamer <ref> [15] </ref> implemented their algorithm on the HP-730 work station which is also three times as fast as a node of the CM5.
Reference: [16] <author> Deepankar Medhi. </author> <title> Parallel bundle-based decomposition for large-scale structured mathematical programming problems. </title> <journal> Annals of Operations Research, </journal> <volume> 22 </volume> <pages> 101-127, </pages> <year> 1990. </year>
Reference-contexts: If certain descent conditions are met then ^y = y j + s j is used as a new iterate, otherwise ^y is added to the bundle and the current iterate remains unchanged. The termination condition is of the type jv j j ffi. Medhi <ref> [16] </ref> also uses bundle methods to solve BAP. However, he applies the 7 bundle method to the dual (Rockafellar dual [19]) of BAP which is a nonsmooth, concave problem.
Reference: [17] <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.4 release notes, appendix to MINOS 5.1 user's guide. </title> <type> Technical report, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: Since the upper bounds on the subproblems are changed from each iteration to the next (we adjust the decoupled resource allocation) we can not use "hot starting". That is, we need to start with an all-artificial basis at every iteration. We used the optimization package MINOS <ref> [17] </ref> in the form of a subroutine (MINOS 5.4) in order to solve the coordinator problems for both single-variable and group coordination. MINOS solves the above using a reduced-gradient algorithm in conjunction with a quasi-Newton algorithm. MINOS requires any domain constraint for the objective function to be specified explicitly.
Reference: [18] <author> M.C. Pnar and S.A. Zenios. </author> <title> Parallel decomposition of multicommodity network flows using a linear-quadratic penalty algorithm. </title> <journal> ORSA Journal on Computing, </journal> <volume> 4 </volume> <pages> 235-249, </pages> <year> 1992. </year>
Reference-contexts: A modification of simplicial decomposition is the restricted simplicial decomposition in which there are only a fixed (predetermined) number of columns of Y are stored. Restricted simplicial decomposition uses less memory than the ordinary simplicial decomposition. Smoothed Exact Penalty Zenios and Pinar <ref> [18] </ref> introduce the following linear quadratic penalty function in the solution of BAP: ~p (*; t) = &gt; &gt; &gt; &gt; &gt; &gt; &lt; 0 if t 0 2* if 0 t * 2 ) if t * As the parameter * # 0; ~p (*; t) approaches the piecewise linear <p> We will then present the values of the parameters in our code. In the last section of this chapter we present our computational results for each of the above mentioned schemes and compare them to those of De Leone [14], Zenios and Pinar <ref> [18] </ref>, McBride and Mamer [15], Schultz and Meyer [20] and Grigoriadis and Khachiyan [10]. 53 4.1 The Implementation 4.1.1 Algorithm and Implementation As we discussed in chapter 2, our algorithm follows the basic three phase method of Schultz and Meyer. Figure (4) presents a sketch of the three phase method.
Reference: [19] <author> R.T. Rockafellar. </author> <title> Conjugate Duality and Optimization, </title> <booktitle> volume 16 of Conference Board of the Mathematical Sciences. </booktitle> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1974. </year> <month> 92 </month>
Reference-contexts: The termination condition is of the type jv j j ffi. Medhi [16] also uses bundle methods to solve BAP. However, he applies the 7 bundle method to the dual (Rockafellar dual <ref> [19] </ref>) of BAP which is a nonsmooth, concave problem.
Reference: [20] <author> G.L. Schultz. </author> <title> Barrier Decomposition for the parallel optimization of block-angular programs. </title> <type> PhD thesis, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1991. </year>
Reference-contexts: by (11) and by definition of ^ and ^x we get: c (x) t:J c (x fl ) So by feasibility of x for (9) and the above we obtain: c (x fl ) c (x) c (x fl ) + * (12) 2.3 Schultz-Meyer Decomposition Scheme Schultz and Meyer <ref> [20] </ref> use the barrier function techniques to transform the BAP to the BP. They then apply a three phase decomposition scheme to solve the BP. In order to solve the BP one needs to start with a point that satisfies the mutual constraints (as well as block and bound constraints). <p> ), then they choose i+1 such that the following hold: D (x i ) &lt; i+1 i and i+1 d (13) and in the limit either 1 = d or (9j) such that lim inf i j D j: (x i )g = 0 They show (in theorem 2.7 of <ref> [20] </ref>) that one can obtain a strictly feasible point ^x using shifted barrier methods provided the original BAP has a strictly feasible point. Theorem 4 (Finite Feasibility) Suppose c (:) is bounded from below on B. Also let i+1 be chosen to satisfy (13) and (14). <p> On the other hand if no such point exists then f t (x i ; i ) ! +1. 25 They propose the following algorithm for choosing i which they prove (in the orem 2.9 of <ref> [20] </ref>) satisfies the conditions (13) and (14): 1 8 &gt; &gt; : D j: (x 0 ) + fi if D j: (x 0 ) d j where fi &gt; 0 is a constant. <p> Ordinarily there would be an equivalent of R (x t ) for the lower bounds on the variables, however since the matrix of coupling constraints is non-negative the lower bounds remain at zero for the decoupled 28 resource allocation (see <ref> [20] </ref>). If the matrix G t is chosen appropriately (e.g. taking G t to be a block diagonal approximation of the hessian of f (x t ) or G t = 0) then (17) is decomposable over the blocks. Hence the subproblem can be solved in parallel. <p> In the last section of this chapter we present our computational results for each of the above mentioned schemes and compare them to those of De Leone [14], Zenios and Pinar [18], McBride and Mamer [15], Schultz and Meyer <ref> [20] </ref> and Grigoriadis and Khachiyan [10]. 53 4.1 The Implementation 4.1.1 Algorithm and Implementation As we discussed in chapter 2, our algorithm follows the basic three phase method of Schultz and Meyer. Figure (4) presents a sketch of the three phase method. <p> : f1; : : : ; Kg fi Z + ! Z is called a culling function if y (k;t) [k] is a search direction used for block k at time t and (8k) lim jjx t (k;t) (For a more detailed discussion of the above definition see Schultz's thesis <ref> [20] </ref>). We now construct a particular culling function for our algorithm. We divide the blocks into n disjoint groups, n 1 of which contain P blocks, while the last group contains the remaining blocks. At each iteration we concern ourselves with only one group. <p> Then for t sufficiently large the subproblem solutions with kth block given by y [k] solve subproblems of the form (22). The proof for the above lemma is presented in Schultz's thesis <ref> [20] </ref>. <p> 71 1. fx t g converges and (8k) lim t!1 (k; t) = +1 2. lim t!1 jjx t+1 x t jj = 0 and (9T ) (8 (k; t)) t (k; t) T The proof for the above is very short and simple, it may also be found in <ref> [20] </ref>.
Reference: [21] <author> Rudy Setiono. </author> <title> An interior dual proximal point algorithm for linear programs. </title> <type> Technical Report TR879, </type> <institution> Center for Parallel Optimization, Computer Sciences Department, University of Wisconsin, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: McBride and Mamer [15] implemented their algorithm on the HP-730 work station which is also three times as fast as a node of the CM5. Another refrence for solution of the PDS problems (only up to pds.10) is <ref> [21] </ref>. 65 problem SM ZP GK MM MC pds.10 1711 408 123 40 45 pds.30 19380 7504 756 838 654 4.3 The MNETGEN problems Another set of problems we considered were those produced by MNETGEN.
Reference: [22] <author> B. von Hohenbalken. </author> <title> Simplicial decomposition in nonlinear programming algorithms. </title> <journal> Mathematical Programming, </journal> <volume> 13 </volume> <pages> 49-68, </pages> <year> 1977. </year>
Reference-contexts: If all the 12 subproblems have nonnegative optimal objectives then the current solution of the restricted master is an optimal solution for the the original BAP. For a more detailed discussion of the Dantzig-Wolfe decomposition method see [14]. Simplicial Decomposition This method originally developed by Von Hohenbalken <ref> [22] </ref> considers the problem s/t x 2 X where X is a compact, convex, nonempty set and f is differentiable.
Reference: [23] <author> Armand Zakarian. </author> <title> Private communication. </title>
Reference-contexts: The column labeled total var. contains the total number of arcs. The block constraint matrices for these block-angular problems are node-arc incidence matrices. We take advantage of this fact in our code and use a very efficient network flow solver NSM <ref> [23] </ref> to solve the subproblems. NSM uses the network simplex method to solve the subproblems. Since the upper bounds on the subproblems are changed from each iteration to the next (we adjust the decoupled resource allocation) we can not use "hot starting".
Reference: [24] <author> Armand Zakarian. </author> <title> Parallel solution of multi-commodity network flow programs via nonlinear jacobi algorithm. </title> <type> Unpublished manuscript. </type>
Reference-contexts: The steps are repeated in a loop until the termination criteria is met. Mangasarian and Chen have since developed a class of smoothing functions one of which is the smoothed exact penalty function [2]. Augmented Lagrangian Meyer and Zakarian <ref> [24] </ref> solve the BAP using an augmented Lagrangian approach.
References-found: 24

