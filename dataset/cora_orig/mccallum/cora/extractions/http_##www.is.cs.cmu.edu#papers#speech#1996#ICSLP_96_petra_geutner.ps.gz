URL: http://www.is.cs.cmu.edu/papers/speech/1996/ICSLP_96_petra_geutner.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: pgeutner@ira.uka.de  
Title: INTRODUCING LINGUISTIC CONSTRAINTS INTO STATISTICAL LANGUAGE MODELING  
Author: Petra Geutner 
Address: (USA)  
Affiliation: Interactive Systems Laboratories University of Karlsruhe (Germany) Carnegie Mellon University  
Abstract: Building robust stochastic language models is a major issue in speech recognition systems. Conventional word-based n-gram models do not capture any linguistic constraints inherent in speech. In this paper the notion of function and content words (open/closed word classes) is used to provide linguistic knowledge that can be incorporated into language models. Function words are articles, prepositions, personal pronouns content words are nouns, verbs, adjectives and adverbs. Based on this class definition resulting in function and content word markers, a new language model is defined. A combination of the word-based model with this new model will be introduced. The combined model shows modest improvements both in perplexity results and recognition performance. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> P. Geutner, B. Suhm, F.-D. But, L. Mayfield, T. Kemp, A. E. McNair, I. Rogina, T. Schultz, T. Sloboda, W. Ward, M. Woszczyna and A. Waibel. </author> <title> Integrating Different Learning Approaches into a Multilingual Spoken Language Translation System. </title> <editor> In Stefan Wermter, Ellen Riloff and Gabriele Scheler, editors, </editor> <title> Connectionist, statistical, </title> <booktitle> and symbolic approaches to learning for natural language processing, </booktitle> <pages> pages 117-131. </pages> <publisher> Springer, </publisher> <address> Berlin Hei-delberg, </address> <month> March </month> <year> 1996. </year> <booktitle> Lecture Notes in Artificial Intelligence. </booktitle>
Reference-contexts: Table 1 shows a detailed description of all available data. All recognition results reported have been performed with the JANUS system <ref> [1] </ref>. Training Test #dialogues 608 8 #Utterances 10735 110 #Words 281160 2346 Vocabulary Size 5442 543 Table 1: GSST Database 3.2. Perplexity Results As baseline a word-based trigram language model was trained and the perplexity of 67.2 was used as reference for all further experiments.
Reference: 2. <author> X. Huang, F. Alleva, H.W. Hon, M.-Y. Hwang and R. Rosenfeld. </author> <title> The SPHINX-II Speech Recognition System: an Overview. </title> <booktitle> Computer, Speech and Language, </booktitle> <volume> 7 </volume> <pages> 137-148, </pages> <year> 1993. </year>
Reference-contexts: One approach is to make use of syntactic and semantic knowledge that is inherent in the notion of function and content words. Many attempts have been made to incorporate more than local constraints into language modeling <ref> [2, 6] </ref>. Here, the prediction of the next word is extended not only to the (n-1) last words but to longer-term dependencies.
Reference: 3. <author> R. Isotani and S. Matsunaga. </author> <title> Speech Recognition using a stochastic language model integrating local and global constraints. </title> <booktitle> ARPA SLT Workshop, </booktitle> <pages> pages 87-92, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Many attempts have been made to incorporate more than local constraints into language modeling [2, 6]. Here, the prediction of the next word is extended not only to the (n-1) last words but to longer-term dependencies. Motivated by the work of <ref> [3, 5] </ref> a similar approach has been implemented: the next word is predicted through the last function/content word pair, wherever these have been found in the word history. Based on this idea a separate language model is trained.
Reference: 4. <author> R. Isotani and S. Sagayama. </author> <title> Speech Recognition using particle N-grams and content-word N-grams. </title> <booktitle> Proceed ings of Eurospeech'93, </booktitle> <pages> pages 1955-1958, </pages> <month> September </month> <year> 1993. </year> <institution> Berlin, Germany. </institution>
Reference-contexts: FUNCTION AND CONTENT WORDS The notion of function and content words (sometimes also referred to as open and closed word classes) is well known. Isotani et al. already used this distinction for the Japanese language in order to differentiate between particles and content words <ref> [4] </ref>. The same way of class distinction is possible for the German language: function words can be thought of as articles, prepositions, personal pronouns; content words are nouns, verbs, adjectives and adverbs briefly everything that cannot be captured or enumerated within a closed class.
Reference: 5. <author> R. Isotani and S. Matsunaga. </author> <title> A Stochastic Language Model for Speech Recognition Integrating Local and Global Constraints. </title> <booktitle> Proceedings of the IEEE 1994 International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <pages> pages 5-8, </pages> <month> May </month> <year> 1994. </year> <institution> Adelaide, Aus-tralia. </institution>
Reference-contexts: Many attempts have been made to incorporate more than local constraints into language modeling [2, 6]. Here, the prediction of the next word is extended not only to the (n-1) last words but to longer-term dependencies. Motivated by the work of <ref> [3, 5] </ref> a similar approach has been implemented: the next word is predicted through the last function/content word pair, wherever these have been found in the word history. Based on this idea a separate language model is trained.
Reference: 6. <author> R. Lau, R. Rosenfeld and S. Roukos. </author> <title> Trigger-based Language Models: A Maximum Entropy Approach. </title> <booktitle> Proceedings of the IEEE 1993 International Conference on Acoustics, Speech and Signal Processing (ICASSP), </booktitle> <pages> pages 45-48, </pages> <month> May </month> <year> 1993. </year> <institution> Minneapolis, Minnesota. </institution>
Reference-contexts: One approach is to make use of syntactic and semantic knowledge that is inherent in the notion of function and content words. Many attempts have been made to incorporate more than local constraints into language modeling <ref> [2, 6] </ref>. Here, the prediction of the next word is extended not only to the (n-1) last words but to longer-term dependencies.
References-found: 6

