URL: ftp://ftp.csd.uwo.ca/pub/ling/papers/ijcai91-ge.ps.Z
Refering-URL: http://www.csd.uwo.ca/faculty/ling/sub-pub.html
Root-URL: 
Email: Email: ling@csd.uwo.ca  
Title: Inductive Learning from Good Examples  
Author: Xiaofeng (Charles) Ling 
Address: Ontario London, Ontario, Canada N6A 5B7  
Affiliation: Department of Computer Science University of Western  
Abstract: We study what kind of data may ease the computational complexity of learning of Horn clause theories (in Gold's paradigm) and Boolean functions (in PAC-learning paradigm). We give several definitions of good data (basic and generative representative sets), and develop data-driven algorithms that learn faster from good examples, and degenerate to learn in the limit from the "worst" possible examples. We show that Horn clause theories, k-term DNF and general DNF Boolean functions are polynomially learnable from generative rep resentative presentations.
Abstract-found: 1
Intro-found: 1
Reference: [ Angluin, 1981 ] <author> D. Angluin. </author> <title> A note on the number of queries needed to identify regular languages. </title> <journal> Information and Control, </journal> <volume> 51, </volume> <year> 1981. </year>
Reference-contexts: However, most of them are heuristic. They learn faster from some good data, but fail to learn (in the limit) from "bad" data; little is done to characterize the "good" data for efficient learning. Some theoretical studies on learning from "good" examples have been explored. For example, Angluin <ref> [ Angluin, 1981 ] </ref> defines "representatives" for live states as good strings for identifying a DFA polynomially. Rivest and Sloan [ Rivest and Sloan, 1988 ] show how to learn an arbitrary concept by first learning its relevant subconcepts. <p> To be successful in learning, the learner must receive data that go through (use or exercise) every "live" component of one of the finite characterizations 4 . For example, data that go through all "live" transactions in a DFA (see <ref> [ Angluin, 1981 ] </ref> ), all "live" production rules in a grammar for a language, or all "live" Horn clauses in a logic program 5 .
Reference: [ Angluin, 1988 ] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4), </volume> <year> 1988. </year>
Reference-contexts: It is still open if general DNF is PAC-learnable. Human learning, on the other hand, is efficient, interactive, and data-dependent. The study of the interaction between learners and teachers may reveal the essence of efficient human learning. Learning equipped with teachers answering various types of queries <ref> [ Angluin, 1988 ] </ref> is one of such studies. We believe that the current learning model does not reflects the quality of the examples in the role of efficient learning. Here we present some results on what kind of data may ease the computational complexity of learning 1 .
Reference: [ Banerji, 1988 ] <author> R.B. Banerji. </author> <title> Learning theories in a subset of a polyadic logic. </title> <booktitle> In Proceedings of First Workshop on Computational Learning Theory, </booktitle> <year> 1988. </year>
Reference: [ Blum and Blum, 1975 ] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year>
Reference-contexts: However, the computational time and the number of examples needed for convergence depend on the example series, and can not be specified a priori. Although there is an enumeration algorithm that identifies any h-easy <ref> [ Blum and Blum, 1975 ] </ref> model of first order theories in the limit, such an algorithm is extremely inefficient in practice. Shapiro in his seminal work [ Shapiro, 1981 ] presented an incremental method MIS which searches the hypothesis space from general to specific. <p> A total recursive function h is used to bound the resources in proving. For more details see <ref> [ Shapiro, 1981, Blum and Blum, 1975 ] </ref> . Read a set S of positive data T = initial (S) 1: read more examples.
Reference: [ Blumer et al., 1989 ] <author> A. Blumer, A. Ehren-feucht, D. Haussler, and M. Warmuth. </author> <title> Learnabil-ity and the vapnik-chervonenkis dimension. </title> <journal> JACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: However, MIS has two major shortcomings. First, the refinement of the refuted clauses may introduce a large number of faulty clauses, which need to be removed using a large number of negative examples. Second, the algorithm is still very inefficient (exponential). Approximate identification, or PAC-learning (Probably Approximately Correct learning) <ref> [ Valiant, 1984, Blumer et al., 1989 ] </ref> assumes that the examples of the target concept are drawn randomly according to some fixed distribution for learning and for testing the conjecture. <p> Draw enough random examples according to a simple and important theorem due to <ref> [ Valiant, 1984, Blumer et al., 1989 ] </ref> : If C contains a finite number of Boolean functions, then any polynomial algorithm that requests sample size at least 1=" ln (jCj=ffi) and outputs any consistent function in C PAC-identifies C.
Reference: [ Buntine, 1988 ] <author> W.L. Buntine. </author> <title> Generalized subsump-tion and its applications to induction and redundancy. </title> <journal> Artificial Intelligence, </journal> <volume> 36(2) </volume> <pages> 149-176, </pages> <year> 1988. </year>
Reference-contexts: The resulting clauses can be very long. Some techniques of logical reduction of clauses <ref> [ Buntine, 1988, Muggleton and Feng, 1990 ] </ref> and syntactic restriction (such as ij-determination [ Muggleton and Feng, 1990 ] ) may be applied.
Reference: [ Freivalds et al., 1989 ] <author> R. Freivalds, E.B. Kinber, and R. Wiehagen. </author> <title> Inductive inference from good examples. </title> <booktitle> In Proceedings of International Workshop of Analogical and Inductive Inference, </booktitle> <year> 1989. </year>
Reference-contexts: For example, Angluin [ Angluin, 1981 ] defines "representatives" for live states as good strings for identifying a DFA polynomially. Rivest and Sloan [ Rivest and Sloan, 1988 ] show how to learn an arbitrary concept by first learning its relevant subconcepts. Freivalds et al <ref> [ Freivalds et al., 1989 ] </ref> study good examples in recursive theoretic inductive inference. 2 Shapiro's Model Inference System Shapiro's Model Inference System (MIS) [ Shapiro, 1981 ] starts with the most general conjecture (a theory with an empty clause).
Reference: [ Gold, 1967 ] <author> E. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and Control, </journal> <volume> 10 </volume> <pages> 47-474, </pages> <year> 1967. </year>
Reference-contexts: 1 Introduction In any inductive learning model, how data of the target theory are supplied to the learning programs is a crucial assumption. Identification in the limit <ref> [ Gold, 1967 ] </ref> assumes that the series of examples is an admissible enumeration of all (positive and/or negative) examples of the target concept, and requires the learning algorithm to produce a correct hypothesis in some finite time.
Reference: [ Ishizaka, 1988 ] <author> H. Ishizaka. </author> <title> Model inference incorporating generalization. </title> <journal> Journal of Information Processing, </journal> <volume> 11(3), </volume> <year> 1988. </year>
Reference: [ Ling and Dawes, 1990 ] <author> X.C. Ling and M. Dawes. </author> <title> Learning with representative presentations | the inverse of shapiro's mis. </title> <type> Technical Report 263, </type> <institution> Department of Computer Science, University of Western On-tario, </institution> <year> 1990. </year>
Reference-contexts: abstraction operators A for the class of theories is complete, then the constructive algorithm above identifies any h-easy model 8 of that class in the limit given an enumeration of positive and negative examples of the model. 7 Most proofs of the paper are omitted and can be supplied from <ref> [ Ling and Dawes, 1990, Ling, 1991 ] </ref> . 8 The attempted derivation of an atom from T may not halt. A total recursive function h is used to bound the resources in proving. For more details see [ Shapiro, 1981, Blum and Blum, 1975 ] .
Reference: [ Ling, 1989 ] <author> X.C. Ling. </author> <title> Learning and inventing of horn clause theories a constructive method. </title> <editor> In Z.W. Ras, editor, </editor> <booktitle> Methodologies for Intelligent Systems, </booktitle> <volume> 4, </volume> <pages> pages 323-331. </pages> <publisher> North-Holland, </publisher> <year> 1989. </year>
Reference: [ Ling, 1991 ] <author> X.C. Ling. </author> <title> Inductive learning from good examples. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Western Ontario, </institution> <year> 1991. </year> <month> Forthcoming. </month>
Reference-contexts: abstraction operators A for the class of theories is complete, then the constructive algorithm above identifies any h-easy model 8 of that class in the limit given an enumeration of positive and negative examples of the model. 7 Most proofs of the paper are omitted and can be supplied from <ref> [ Ling and Dawes, 1990, Ling, 1991 ] </ref> . 8 The attempted derivation of an atom from T may not halt. A total recursive function h is used to bound the resources in proving. For more details see [ Shapiro, 1981, Blum and Blum, 1975 ] . <p> For proofs of the following theorems, see <ref> [ Ling, 1991 ] </ref> . Theorem 8.1 k-term-DNF is PAC-learnable (by k-term DNF) from generative representative presentations with a sample size O (" 1 kr ln (p (n)=ffi)); and time com plexity O (" 1 k 2 rn p (n) (r+1)(k+1) ln (p (n)=ffi)).
Reference: [ Muggleton and Buntine, 1988 ] <author> S. Muggle-ton and W. Buntine. </author> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the fifth international conference on Machine Learning, </booktitle> <year> 1988. </year>
Reference: [ Muggleton and Feng, 1990 ] <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of first International Conference on Algorithmic Learning Theory. </booktitle> <address> OHMSHA Tokyo, </address> <year> 1990. </year>
Reference-contexts: The resulting clauses can be very long. Some techniques of logical reduction of clauses <ref> [ Buntine, 1988, Muggleton and Feng, 1990 ] </ref> and syntactic restriction (such as ij-determination [ Muggleton and Feng, 1990 ] ) may be applied. <p> The resulting clauses can be very long. Some techniques of logical reduction of clauses [ Buntine, 1988, Muggleton and Feng, 1990 ] and syntactic restriction (such as ij-determination <ref> [ Muggleton and Feng, 1990 ] </ref> ) may be applied. Clearly the total number of possible clauses from the lgg of at most r most specific clauses is polynomial, so is the maximum number of faulty clauses in the conjecture.
Reference: [ Plotkin, 1970 ] <author> G.D. Plotkin. </author> <title> A note on inductive generalization. </title> <editor> In B. Meltzer and D. Michie, editors, </editor> <booktitle> Machine Intelligence 5, </booktitle> <pages> pages 153-163. </pages> <publisher> Elsevier North-Holland, </publisher> <year> 1970. </year>
Reference-contexts: The least general generalization (lgg) of a set of clauses is taken from Protkin's work <ref> [ Plotkin, 1970 ] </ref> . We define a set of ground atoms as a generative representative set for a logic program LP if it is the union of generative representative sets for all clauses in LP.
Reference: [ Rivest and Sloan, 1988 ] <author> R.L. Rivest and R. Sloan. </author> <title> Learning complicated concepts reliably and usefully. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory (COLT-88). </booktitle> <publisher> Morgan Kauf-mann, </publisher> <year> 1988. </year>
Reference-contexts: Some theoretical studies on learning from "good" examples have been explored. For example, Angluin [ Angluin, 1981 ] defines "representatives" for live states as good strings for identifying a DFA polynomially. Rivest and Sloan <ref> [ Rivest and Sloan, 1988 ] </ref> show how to learn an arbitrary concept by first learning its relevant subconcepts.
Reference: [ Shapiro, 1981 ] <author> E. Shapiro. </author> <title> Inductive inference of theories from facts. </title> <type> Technical Report TR 192, </type> <institution> Computer Science Department, Yale University, </institution> <year> 1981. </year>
Reference-contexts: Although there is an enumeration algorithm that identifies any h-easy [ Blum and Blum, 1975 ] model of first order theories in the limit, such an algorithm is extremely inefficient in practice. Shapiro in his seminal work <ref> [ Shapiro, 1981 ] </ref> presented an incremental method MIS which searches the hypothesis space from general to specific. However, MIS has two major shortcomings. <p> Rivest and Sloan [ Rivest and Sloan, 1988 ] show how to learn an arbitrary concept by first learning its relevant subconcepts. Freivalds et al [ Freivalds et al., 1989 ] study good examples in recursive theoretic inductive inference. 2 Shapiro's Model Inference System Shapiro's Model Inference System (MIS) <ref> [ Shapiro, 1981 ] </ref> starts with the most general conjecture (a theory with an empty clause). When the conjecture T proves a negative example (T is too strong) the backtracing algorithm is invoked to remove faulty clauses from T . <p> Removing a clause may overly specilize T , in which case the refinements of faulty clauses are added into T . However, the possible refinements of any clause are infinite. So which refinements of which clauses should be added into T? As Shapiro pointed out <ref> [ Shapiro, 1981 ] </ref> , MIS identifies the theory in the limit "...as long as it [the refinement] is `exhaustive' in some natural way". <p> We define mgs (ff) = fp (ff)g where p is a refinement operator complete for the theories <ref> [ Shapiro, 1981 ] </ref> . <p> A total recursive function h is used to bound the resources in proving. For more details see <ref> [ Shapiro, 1981, Blum and Blum, 1975 ] </ref> . Read a set S of positive data T = initial (S) 1: read more examples. <p> The algorithm identifies any h-easy models in the limit, and learns faster given a good set of data. 6 Various Abstraction Operators We design abstraction operators for various sub-classes of clauses, similar to Shapiro's refinement operators for sub-classes of clauses <ref> [ Shapiro, 1981 ] </ref> . Thus, the algorithm is more efficient if the class of models to be inferred is known. 6.1 Abstraction Operators for Atoms We first discuss the simplest class of models, which have logic programs containing only unit clauses.
Reference: [ Valiant, 1984 ] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Comm. ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: However, MIS has two major shortcomings. First, the refinement of the refuted clauses may introduce a large number of faulty clauses, which need to be removed using a large number of negative examples. Second, the algorithm is still very inefficient (exponential). Approximate identification, or PAC-learning (Probably Approximately Correct learning) <ref> [ Valiant, 1984, Blumer et al., 1989 ] </ref> assumes that the examples of the target concept are drawn randomly according to some fixed distribution for learning and for testing the conjecture. <p> Draw enough random examples according to a simple and important theorem due to <ref> [ Valiant, 1984, Blumer et al., 1989 ] </ref> : If C contains a finite number of Boolean functions, then any polynomial algorithm that requests sample size at least 1=" ln (jCj=ffi) and outputs any consistent function in C PAC-identifies C.
Reference: [ Winston, 1984 ] <author> P.H. Winston. </author> <booktitle> Artificial Intelligence. </booktitle> <address> MA: Addison-Wisley, 2 edition, </address> <year> 1984. </year>
References-found: 19

