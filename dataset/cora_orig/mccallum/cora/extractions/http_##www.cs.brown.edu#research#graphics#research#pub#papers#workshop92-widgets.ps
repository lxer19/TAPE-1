URL: http://www.cs.brown.edu/research/graphics/research/pub/papers/workshop92-widgets.ps
Refering-URL: http://www.cs.brown.edu/research/graphics/research/pub/
Root-URL: http://www.cs.brown.edu/
Title: Three-Dimensional Widgets  
Author: D. Brookshire Conner, Scott S. Snibbe, Kenneth P. Herndon, Daniel C. Robbins, Robert C. Zeleznik, Andries van Dam 
Keyword: User Interface Design, Widgets, 3D Interaction, Virtual Reality  
Address: Providence, RI  
Affiliation: Computer Science Department Brown University  
Abstract: The 3D components of today's user interfaces are still underdeveloped. Direct interaction with 3D objects has been limited thus far to gestural picking, manipulation with linear transformations, and simple camera motion. Further, there are no toolkits for building 3D user interfaces. We present a system which allows experimentation with 3D widgets, encapsulated 3D geometry and behavior. Our widgets are first-class objects in the same 3D environment used to develop the application. This integration of widgets and application objects provides a higher bandwidth between interface and application than exists in more traditional UI toolkit-based interfaces. We hope to allow user-interface designers to build highly interactive 3D environments more easily than is possible with today's tools. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alan H. Barr. </author> <title> Global and local deformations of solid primitives. </title> <booktitle> In Proceedings of the ACM SIGGRAPH, Computer Graphics, </booktitle> <volume> volume 18(3), </volume> <pages> pages 21-30, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: To construct a more complex widget, we start with the simple rotation and translation handle widgets discussed in Section 4.2. By rearranging them and changing their connections, we combine them to form a rack for specifying high-level deformations such as twists, tapers and bends <ref> [1] </ref>, shown in Color Plate IV. Different handles specify the parameters to three deformations. The distance between the two upright handles specifies the range over which the deformation applies.
Reference: [2] <author> Eric A. Bier. </author> <title> Snap-dragging in three dimensions. </title> <booktitle> In Proceedings of the ACM SIGGRAPH, Computer Graphics, </booktitle> <volume> volume 24(4), </volume> <pages> pages 193-204, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: The flexibility of the system allows the widget designer and user to explore a wide range of options. 4.3 Snapping With a more intricate ATN (Figure 2) we can perform simple snap-dragging <ref> [2] </ref>. A mouse's coordinates are used to generate a ray from the camera through the projection of the mouse's position onto the viewplane. If this ray intersects an object, the ATN lets the user choose a point on an object to snap to a point on another object.
Reference: [3] <author> Jeff Butterworth, Andrew Davidson, Stephen Hench, and T. Marc Olano. 3DM: </author> <title> A three dimensional modeler using a head-mounted display. </title> <booktitle> In Proceedings of the 1992 Symposium on Interactive 3D Graphics, </booktitle> <year> 1992. </year>
Reference-contexts: Some virtual-reality systems make use of menus floating in 3-space with 3D icons instead of 2D pixmap icons <ref> [3] </ref>. Besides the additional options for its position, however, such a menu provides no more expressive power than its 2D equivalent. There are many reasons for the underutilization of 3D. First, almost all interaction techniques must be created from scratch, since essentially no toolkits of 3D interaction techniques exist.
Reference: [4] <author> Stuart K. Card, George G. Robertson, and Jock D. Mackinlay. </author> <title> The information visualizer, an information workspace. </title> <booktitle> In Human Factors in Computing Systems, Proceedings of the ACM SIGCHI, </booktitle> <pages> pages 181-188. </pages> <publisher> Addison Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Most paradigms and metaphors for 3D interfaces are less developed than those for 2D interfaces. Some 3D metaphors are the natural analogs of those familiar in 2D, such as 3D menus and rooms [14] <ref> [4] </ref>. However, research in 3D interfaces must develop new metaphors and interaction techniques to take advantage of the greater possibilities of 3D. The cone tree and perspective wall, designed at Xerox PARC [22] [13], demonstrate the potential of 3D representation and interactive animation.
Reference: [5] <author> James Foley, Won Chui Kim, Srdjan Kovacevic, and Kevin Murray. </author> <title> Designing interfaces at a high level of abstraction. </title> <journal> IEEE Software, </journal> <pages> pages 25-32, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: In other words, a user interface can be built by starting with simple widgets and using them to bootstrap more complex ones. Finally, we hope to develop a high-level UIDS (user interface design system) <ref> [5] </ref> for our system. As previously noted, our system currently has no tools for making high-level specifications of an interface. Most commercial UIMSs, having been built on top of a widget toolkit, focus on appearance and geometry of widgets. Some research-level UIDSs handle behavior and sequencing.
Reference: [6] <author> Tinsley Galyean, Melissa Gold, William Hsu, Henry Kauf-man, and Mark Stern. </author> <title> Manipulation of virtual three-dimensional objects using two-dimensional input devices. Class project, </title> <institution> Brown University, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: We can easily change the kind of object that mouse coordinates are mapped to, so as to produce a virtual cube or virtual donut. This sort of modification of the interface can be done at run time. 4.2 Handles Object handles <ref> [6] </ref> are a 3D widget that contains more visual geometry than the virtual sphere widget. We can build handles with an arbitrarily complex appearance. Once they are built, we are free to establish dependencies on them or use them as a controller.
Reference: [7] <author> Mark Green. </author> <title> A survey of three dialogue models. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 5(3) </volume> <pages> 244-375, </pages> <year> 1986. </year>
Reference-contexts: In addition to the data-oriented mechanisms of dependencies and controllers, we provide a dialog model that uses augmented transition networks (ATNs). We use ATNs because the sequencing of an interface is explicitly declared and is more easily visualized in a hierarchical ATN than in context-free grammars or event systems <ref> [7] </ref>. A simple transition network is a finite-state automaton (FSA). A complex interface can be described as an FSA but the complexity produces a combinatorial explosion of FSA states. <p> Therefore, some events that can happen at any time, such as an abort or help request, are especially cumbersome to specify, requiring an additional arc from every state in the ATN. By contrast, event systems have greater expressivenessthan ATNs <ref> [7] </ref>, since they can easily handle an abort or help event by simply adding a new event handler to process this event. This would seem to make event systems a better choice. However, notions of current state, history, or context are more difficult to express in event systems.
Reference: [8] <author> Mark Green and Robert Jacob. </author> <title> SIGGRAPH '90 workshop report: Software architectures and metaphors for non-WIMP user interfaces. </title> <journal> Computer Graphics, </journal> <volume> 25(3) </volume> <pages> 229-235, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: These techniques currently appear only in systems designed explicitly to present or use them, such as demos or prototypes, but in the future, these techniques should be as accessible as any other component in the widget designer's repertoire <ref> [8] </ref>. 2.3 Integrating the application and the user interface User interfaces were originally designed by application programmers using the same tools they used to build applications. This produced interfaces that were tightly integrated with the application. <p> In particular, as has been noted by those critiquing WIMP interfaces <ref> [8] </ref>, today's toolkits are not oriented towards highly interactive applications. Such highly interactive applications require a high bandwidth between the application and the user interface, particularly for semantic feedback [8]. <p> In particular, as has been noted by those critiquing WIMP interfaces <ref> [8] </ref>, today's toolkits are not oriented towards highly interactive applications. Such highly interactive applications require a high bandwidth between the application and the user interface, particularly for semantic feedback [8]. Prior UI research indicates that this may be best accomplished if the application and the interface are part of the same development environment, with the same tools being used to build both [18]. An integrated environment has additional software engineering benefits.
Reference: [9] <author> Paul Haeberli. dynadraw. </author> <note> posted to comp.graphics, </note> <year> 1990. </year> <title> GL program. </title>
Reference-contexts: However, the user can gain more expressive power through interaction techniques that interpret and process movements and make possible more sophisticated interaction. For example, a calligraphic drawing program can attach a pen to a cursor by means of a simulated spring <ref> [9] </ref>, a simple motion-control technique that makes possible a whole new range of drawings not easily created with a rigid pen-cursor linkage. Both 2D and 3D widgets can benefit from more sophisticated reaction to user input.
Reference: [10] <author> Brent Halperin and Van Nguyen. </author> <title> A model for object-based inheritance. </title> <editor> In Peter Wegner and Bruce Shriver, editors, </editor> <booktitle> Research Directions in Object-Oriented Programming. </booktitle> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Thus, both a widget's geometry and behavior are specified in the same unified framework, the framework of the application objects it controls. The underlying construction technique we use is delegation, where one object (the child) is created from a pre-existing object (the parent) [24] <ref> [10] </ref>. If the parent object is changed, the child changes as well.
Reference: [11] <author> Scott H. Hudson. </author> <title> Graphical specification of flexible user interface displays. </title> <booktitle> In Proceedings of the ACM Symposium on User Interface Software and Technology, </booktitle> <pages> pages 105-114, </pages> <year> 1989. </year>
Reference-contexts: By employing controllers, widgets can make use of general constraints, hardware devices, and simulation techniques. 3.2 A dialog model for sequencing Some researchers choose to separate UI design into two broad categories: data-oriented UI design, usually supported through constraints, and dialog-oriented UI design <ref> [11] </ref>. We find both models useful. In addition to the data-oriented mechanisms of dependencies and controllers, we provide a dialog model that uses augmented transition networks (ATNs).
Reference: [12] <author> Robert J. K. Jacob. </author> <title> A specification language for direct-manipulation user interfaces. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 5(4) </volume> <pages> 283-317, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: We would like a dialog model that combines the best features of both ATNs and event handlers. Thus, we modify the ATN model to allow possibly disconnected components of the state graph and more than one active state <ref> [12] </ref>. We can now represent a set of event handlers as a group of disconnected states in an ATN, one state per event handler, each with a single arc back to itself. The arc's input tokens represent the corresponding event handler's events, and the arc's action represents the handler routine.
Reference: [13] <author> Jock D. Mackinlay, George G. Robertson, and Stuart K. Card. </author> <title> The perspective wall: Detail and context smoothly integrated. </title> <booktitle> In Human Factors in Computing Systems, Proceedings of the ACM SIGCHI. ACM SIGCHI, </booktitle> <year> 1991. </year>
Reference-contexts: However, research in 3D interfaces must develop new metaphors and interaction techniques to take advantage of the greater possibilities of 3D. The cone tree and perspective wall, designed at Xerox PARC [22] <ref> [13] </ref>, demonstrate the potential of 3D representation and interactive animation.
Reference: [14] <author> Microelectronics and Computer Technology Corporation. </author> <title> An introduction to the visual metaphors team's software releases. Video tape, </title> <note> 1986. TR# HI-344-86. </note>
Reference-contexts: Thus, application developers are left to implement basic interactive techniques such as virtual sphere rotation themselves. Most paradigms and metaphors for 3D interfaces are less developed than those for 2D interfaces. Some 3D metaphors are the natural analogs of those familiar in 2D, such as 3D menus and rooms <ref> [14] </ref> [4]. However, research in 3D interfaces must develop new metaphors and interaction techniques to take advantage of the greater possibilities of 3D. The cone tree and perspective wall, designed at Xerox PARC [22] [13], demonstrate the potential of 3D representation and interactive animation.
Reference: [15] <author> Richard S. Millman and George D. Parker. </author> <title> Elements of Differential Geometry. </title> <publisher> Prentice-Hall, </publisher> <year> 1977. </year>
Reference-contexts: If this ray intersects an object, the ATN lets the user choose a point on an object to snap to a point on another object. Since this is done with ray intersection, the point to snap includes a complete Frenet frame <ref> [15] </ref> defined by the surface normal and tangents. When the user releases the mouse button and clicks again, the ATN begins checking to see if the ray specified by the mouse intersects another object. If so, this new object becomes the object to snap to.
Reference: [16] <author> Brad A. Myers. </author> <title> Encapsulating interactive behaviors. </title> <booktitle> In Proceedings of CHI '89 (Austin, </booktitle> <address> TX, April 30-May 4, </address> <year> 1989), </year> <pages> pages 319-324. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Although this definition is somewhat vague and general, it has the advantage of covering all the areas of the interface literature we have explored, from general constructs such as Garnet's Interaction Objects <ref> [16] </ref> and the Interactive Objects of Xerox's 3D Rooms [21] to very specific kinds of widgets such as those found in the X Toolkit or the Macintosh Toolkit. The extent to which a 2D widget should be classified as consisting of behavior or of geometry varies widely.
Reference: [17] <author> Brad A. Myers. </author> <title> User-interface tools: Introduction and survey. </title> <journal> IEEE Software, </journal> <pages> pages 15-23, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Finally, we believe such a toolkit is intrinsically more difficult to create than its 2D counterpart because of the inherent complexity of 3D interaction. Widget toolkits are well known for 2D applications (e.g., the Macintosh Programmer's Toolbox, OSF/Motif, XView) <ref> [17] </ref>. However, 3D graphics libraries such as PHIGS+ and SGI's GL provide very little support for interaction beyond simple device handling. The industry standard PHIGS+ provides only six widgets (pick, locator, stroke, choice, valuator, and string). <p> However, research in 3D interfaces must develop new metaphors and interaction techniques to take advantage of the greater possibilities of 3D. The cone tree and perspective wall, designed at Xerox PARC [22] [13], demonstrate the potential of 3D representation and interactive animation. User interfaces are inherently difficult to program <ref> [17] </ref>. 3D interfaces complicate interface design and implementation, since the interface must take into account such issues as a richer collection of primitives, attributes, and rendering styles, multiple coordinate systems, viewing projections, visibility determination, and lighting and shading. <p> This produced interfaces that were tightly integrated with the application. Recently, however, interface design is more often done by specialists using UI development tools <ref> [17] </ref>. While this separation produces more consistent interfaces and more modular programs, it can also produce interfaces that are not as helpful as they could be if they were more specialized to the application the interface designer is not only aided but also limited by the toolkit and its metaphors.
Reference: [18] <author> Brad A. Myers, Dario A. Guise, Roger B. Dannenberg, Brad Vander Zanden, David S. Kosbie, Edward Pervin, An-drew Mickish, and Philippe Marchal. Garnet: </author> <title> Comprehensive support for graphical, highly interactive user interfaces. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 71-85, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Prior UI research indicates that this may be best accomplished if the application and the interface are part of the same development environment, with the same tools being used to build both <ref> [18] </ref>. An integrated environment has additional software engineering benefits. First, only a single paradigm must be learned, rather than one for the interface and another for the application. Also, separate paradigms can be hard to integrate at several levels: the conceptual level, the code implementation level, and the compile-debug level. <p> For example, a cube can become a simple slider by constraining it to move only along its x axis, and a torus's inner radius can then depend on the x position of the cube. To provide multi-way constraints and cyclical constraint networks <ref> [18] </ref>, we use controllers [25], objects whose primary purpose is to control other objects. Thus, our dynamic constraint solver is encapsulated as a controller. Additionally, we encapsulate physical devices as controllers that filter and pass values to objects.
Reference: [19] <author> Randy Pausch. </author> <type> personal communication, </type> <year> 1991. </year>
Reference-contexts: In virtual-reality systems, 3D interaction is especially crucial. However, the significant difficulties of 3D input and display have led research in virtual worlds to concentrate far more on the development of new devices and device-handling techniques than on higher-level techniques for 3D interaction <ref> [19] </ref>. Such interaction goes no further than a straightforward interpretation of device data, such as using a Polhemus for a head tracker or a DataGlove for simple gestural recognition of commands such as select, translate and rotate.
Reference: [20] <author> Cary B. Phillips, Jianmin Zhao, and Norman I. Badler. </author> <title> Interactive real-time articulated figure manipulation using multiple kinematic constraints. </title> <booktitle> In Special Issue on the 1990 Symposium on Interactive 3D Graphics, Computer Graphics, </booktitle> <pages> pages 245-250. </pages> <publisher> ACM SIGGRAPH, ACM Press, </publisher> <month> March </month> <year> 1990. </year>
Reference-contexts: Color Plate I shows various handles being used to translate, rotate and scale an object. The same kind of constrained motion can be produced by holding down various modifier keys or different combinations of buttons <ref> [20] </ref>. However, a user presented with such an interface has no easy way to determine what the possible actions are. Handles allow constrained motion through intuitive direct manipulation: when a particular handle is selected, motion is constrained along or around the axis it describes.
Reference: [21] <author> George G. Robertson, Stuart K. Card, and Jock D. Mackinlay. </author> <title> The cognitive coprocessor architecture for interactive user interfaces. </title> <booktitle> In Proceedings of the ACM Symposium on User Interface Software and Technology, </booktitle> <pages> pages 10-18, </pages> <year> 1989. </year>
Reference-contexts: Although this definition is somewhat vague and general, it has the advantage of covering all the areas of the interface literature we have explored, from general constructs such as Garnet's Interaction Objects [16] and the Interactive Objects of Xerox's 3D Rooms <ref> [21] </ref> to very specific kinds of widgets such as those found in the X Toolkit or the Macintosh Toolkit. The extent to which a 2D widget should be classified as consisting of behavior or of geometry varies widely.
Reference: [22] <author> George G. Robertson, Jock D. Mackinlay, and Stuart K. Card. </author> <title> Cone trees: Animated 3D visualizations of hierarchical information. </title> <booktitle> In Human Factors in Computing Systems, Proceedings of the ACM SIGCHI. ACM SIGCHI, </booktitle> <year> 1991. </year>
Reference-contexts: However, research in 3D interfaces must develop new metaphors and interaction techniques to take advantage of the greater possibilities of 3D. The cone tree and perspective wall, designed at Xerox PARC <ref> [22] </ref> [13], demonstrate the potential of 3D representation and interactive animation.
Reference: [23] <author> Dean Rubine. </author> <title> Specifying gestures by example. </title> <booktitle> In Proceedings of the ACM SIGGRAPH, Computer Graphics, </booktitle> <pages> pages 329-337. </pages> <publisher> ACM SIGGRAPH, Addison-Wesley, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: As noted, common 2D widgets rarely take advantage of all the degrees of freedom available to them. The use of multiple degrees of freedom to enhance interaction is thus largely unexplored potential, even in 2D <ref> [23] </ref>, and 3D, with its greater degrees of freedom, has correspondingly greater potential. This potential must of course be handled with restraint: while we would like to be able to use several degrees of freedom simultaneously, using too many may make the widget too difficult to use. <p> Gestural interfaces, on the other hand, allow the user to specify operation, operand, and parameters in a single action <ref> [23] </ref>, providing a faster interface and commands that do not depend on previous or further actions. In addition to providing better input, a tighter integration between application and interface lets the application provide semantic feedback while the user is interacting.
Reference: [24] <author> Peter Wegner. </author> <title> The object-oriented classification paradigm. </title> <editor> In Peter Wegner and Bruce Shriver, editors, </editor> <booktitle> Research Directions in Object-Oriented Programming. </booktitle> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Thus, both a widget's geometry and behavior are specified in the same unified framework, the framework of the application objects it controls. The underlying construction technique we use is delegation, where one object (the child) is created from a pre-existing object (the parent) <ref> [24] </ref> [10]. If the parent object is changed, the child changes as well.
Reference: [25] <author> Robert C. Zeleznik, D. Brookshire Conner, Matthias M. Wloka, Daniel G. Aliaga, Nathan T. Huang, Philip M. Hub-bard, Brian Knep, Henry Kaufman, John F. Hughes, and An-dries van Dam. </author> <title> An object-oriented framework for the integration of interactive animation techniques. </title> <booktitle> In Proceedings of the ACM SIGGRAPH, Computer Graphics, </booktitle> <pages> pages 105-112. </pages> <publisher> ACM SIGGRAPH, Addison-Wesley, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: Existing UI toolkits do allow callbacks to alter a widget based on application feedback, but the mechanisms to do so are often clumsy and hard to use. Our interfaces are constructed in an environment called UGA <ref> [25] </ref> in which widgets can actively depend on the state of other widgets, in the same way that any other objects (e.g., the application's objects) in our system can depend on each other. Our widgets are not external to the application model. <p> The system supports the behavioral aspects of widgets through one-way constraints called dependencies <ref> [25] </ref>. An object can be explicitly related to another object by using a dependency. Since widgets are first-class objects in UGA, they can use this dependency mechanism as easily as application objects can. <p> For example, a cube can become a simple slider by constraining it to move only along its x axis, and a torus's inner radius can then depend on the x position of the cube. To provide multi-way constraints and cyclical constraint networks [18], we use controllers <ref> [25] </ref>, objects whose primary purpose is to control other objects. Thus, our dynamic constraint solver is encapsulated as a controller. Additionally, we encapsulate physical devices as controllers that filter and pass values to objects. Finally, we can use controllers to encapsulate simulation methods, such as inverse kinematics or collision detection.
References-found: 25

