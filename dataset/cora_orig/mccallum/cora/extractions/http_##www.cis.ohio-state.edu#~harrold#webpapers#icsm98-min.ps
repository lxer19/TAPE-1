URL: http://www.cis.ohio-state.edu/~harrold/webpapers/icsm98-min.ps
Refering-URL: http://www.cis.ohio-state.edu/~harrold/allpapers.html
Root-URL: 
Title: An Empirical Study of the Effects of Minimization on the Fault Detection Capabilities of Test Suites  
Author: Gregg Rothermel Mary Jean Harrold Jeffery Ostrin Christie Hong 
Keyword: software testing, test suite minimization, fault detection, empirical studies  
Note: Research Paper  
Address: Corvallis, OR  Columbus, OH  Corvallis, OR  Columbus, OH  
Affiliation: Department of Computer Science Oregon State University  Department of Computer and Information Science Ohio State University  Department of Computer Science Oregon State University  Department of Computer and Information Science Ohio State University  
Email: grother@cs.orst.edu  harrold@cis.ohio-state.edu  ostrinj@ucs.orst.edu  hongc@cis.ohio-state.edu  
Date: May 25, 1998  
Abstract: Test suite minimization techniques attempt to reduce the cost of saving and reusing tests during software maintenance, by eliminating redundant tests from test suites. A potential drawback of these techniques is that in minimizing a test suite, they might reduce the ability of that test suite to reveal faults in the software. A recent study showed that minimization can reduce test suite size without significantly reducing the fault detection capabilities of test suites. To further investigate this issue, we performed an experiment in which we compared the costs and benefits of minimizing test suites of various sizes for several programs. In contrast to the previous study, our results reveal that the fault-detection capabilities of test suites can be severely compromised by minimization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Balcer, W. Hasling, and T. </author> <title> Ostrand. Automatic generation of test scripts from formal test specifications. </title> <booktitle> In Proc. of the 3rd Symp. on Softw. Testing, Analysis, and Verification, </booktitle> <pages> pages 210-218, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: initial set of black-box test cases "according to good testing practices, based on the tester's understanding of the program's functionality and knowledge of special values and boundary points that are easily observable in the code" [9, p. 194], using the category partition method and the Siemens Test Specification Language tool <ref> [1, 10] </ref>. They then augmented this set with manually-created white-box test cases to ensure that each executable statement, edge, and definition-use pair in the base program or its control flow graph was exercised by at least 30 test cases.
Reference: [2] <author> T.Y. Chen and M.F. Lau. </author> <title> Dividing strategies for the optimization of a test suite. </title> <journal> Information Processing Letters, </journal> <volume> 60(3) </volume> <pages> 135-141, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: Therefore, researchers have investigated the notion that when several test cases in a test suite execute the same program components, that test suite can be reduced to a smaller suite that guarantees equivalent coverage. This research has resulted in the creation of various test suite minimization algorithms (e.g., <ref> [2, 6, 8] </ref>). The motivation for test suite minimization is straightforward: by reducing test suite size we reduce the costs of executing, validating, and managing those test suites over future releases of the software. <p> However, this subset of the test suite is the minimum cardinality hitting set of the T i s, and the problem of finding the minimum cardinality hitting set is NP-complete [4]. Thus, minimization techniques resort to heuristics. Several test suite minimization heuristics have been proposed (e.g. <ref> [2, 6, 8] </ref>); in this work we utilize the methodology of Harrold, Gupta, and Soffa, described further in Reference [6]. 2.2 Previous empirical work A number of empirical studies of software testing have been performed.
Reference: [3] <author> P.G. Frankl and S.N. Weiss. </author> <title> An experimental comparison of the effectiveness of branch testing and data flow testing. </title> <journal> IEEE Trans. on Softw. Eng., </journal> <volume> 19(8) </volume> <pages> 774-787, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Some of these studies, such as those reported in References <ref> [3, 9, 12] </ref>, provide only indirect data about the effects of test suite minimization through consideration of the effects of test suite size on costs and benefits of testing.
Reference: [4] <author> M.R. Garey and D.S. Johnson. </author> <title> Computers and Intractability. W.H. </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: A maximum reduction is achieved by finding the smallest representative set of test cases. However, this subset of the test suite is the minimum cardinality hitting set of the T i s, and the problem of finding the minimum cardinality hitting set is NP-complete <ref> [4] </ref>. Thus, minimization techniques resort to heuristics.
Reference: [5] <author> T.L. Graves, M.J. Harrold, J-M Kim, A. Porter, and G. Rothermel. </author> <title> An empirical study of regression test selection techniques. </title> <booktitle> In The 20th Int'l. Conf. on Softw. </booktitle> <address> Eng., </address> <month> April </month> <year> 1998. </year>
Reference-contexts: Some of these studies, such as those reported in References [3, 9, 12], provide only indirect data about the effects of test suite minimization through consideration of the effects of test suite size on costs and benefits of testing. Other studies, such as the study reported in Reference <ref> [5] </ref>, provide only indirect data about the effects of test suite minimization through a comparison of regression test selection techniques that practice or do not practice minimization. 1 A recent study by Wong, Horgan, London, and Mathur [13], however, directly examines the costs and benefits of test suite minimization.
Reference: [6] <author> M. J. Harrold, R. Gupta, and M. L. Soffa. </author> <title> A methodology for controlling the size of a test suite. </title> <journal> ACM Trans. on Softw. Eng. and Methodology, </journal> <volume> 2(3) </volume> <pages> 270-285, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Therefore, researchers have investigated the notion that when several test cases in a test suite execute the same program components, that test suite can be reduced to a smaller suite that guarantees equivalent coverage. This research has resulted in the creation of various test suite minimization algorithms (e.g., <ref> [2, 6, 8] </ref>). The motivation for test suite minimization is straightforward: by reducing test suite size we reduce the costs of executing, validating, and managing those test suites over future releases of the software. <p> However, this subset of the test suite is the minimum cardinality hitting set of the T i s, and the problem of finding the minimum cardinality hitting set is NP-complete [4]. Thus, minimization techniques resort to heuristics. Several test suite minimization heuristics have been proposed (e.g. <ref> [2, 6, 8] </ref>); in this work we utilize the methodology of Harrold, Gupta, and Soffa, described further in Reference [6]. 2.2 Previous empirical work A number of empirical studies of software testing have been performed. <p> Thus, minimization techniques resort to heuristics. Several test suite minimization heuristics have been proposed (e.g. [2, 6, 8]); in this work we utilize the methodology of Harrold, Gupta, and Soffa, described further in Reference <ref> [6] </ref>. 2.2 Previous empirical work A number of empirical studies of software testing have been performed. <p> To perform the experiments, we required a test suite minimization tool. We implemented the algorithm of Harrold, Gupta and Soffa <ref> [6] </ref> within the Aristotle program analysis system [7]. 3.4 Experimental design 3.4.1 Variables. <p> In both cases, however, all subject programs were of size less than 1000 lines of code. * The WHLM study used ATAC for minimization, whereas our study utilized the algorithm of Reference <ref> [6] </ref>. Reference [13] reports that the ATAC approach achieved minimal test selection on the cases studied; we 15 have not yet determined whether our algorithm was equally successful.
Reference: [7] <author> M.J. Harrold and G. Rothermel. Aristotle: </author> <title> A system for research on and development of program analysis based tools. </title> <type> Technical Report OSU-CISRC-3/97-TR17, </type> <institution> The Ohio State University, </institution> <month> Mar </month> <year> 1997. </year>
Reference-contexts: To perform the experiments, we required a test suite minimization tool. We implemented the algorithm of Harrold, Gupta and Soffa [6] within the Aristotle program analysis system <ref> [7] </ref>. 3.4 Experimental design 3.4.1 Variables.
Reference: [8] <author> J.R. Horgan and S.A. London. ATAC: </author> <title> A data flow coverage testing tool for C. </title> <booktitle> In Proc. of the Symp. on Assessment of Quality Softw. Dev. Tools, </booktitle> <pages> pages 2-10, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Therefore, researchers have investigated the notion that when several test cases in a test suite execute the same program components, that test suite can be reduced to a smaller suite that guarantees equivalent coverage. This research has resulted in the creation of various test suite minimization algorithms (e.g., <ref> [2, 6, 8] </ref>). The motivation for test suite minimization is straightforward: by reducing test suite size we reduce the costs of executing, validating, and managing those test suites over future releases of the software. <p> However, this subset of the test suite is the minimum cardinality hitting set of the T i s, and the problem of finding the minimum cardinality hitting set is NP-complete [4]. Thus, minimization techniques resort to heuristics. Several test suite minimization heuristics have been proposed (e.g. <ref> [2, 6, 8] </ref>); in this work we utilize the methodology of Harrold, Gupta, and Soffa, described further in Reference [6]. 2.2 Previous empirical work A number of empirical studies of software testing have been performed. <p> The researchers minimized these test suites using ATAC <ref> [8] </ref>, an heuristic-based minization tool that found "exact solutions for minimizations of all test suites examined" [13, page 42]. This minimization was done with respect to all-uses dataflow coverage (i.e., size was reduced while keeping all-uses coverage constant).
Reference: [9] <author> M. Hutchins, H. Foster, T. Goradia, and T. </author> <title> Ostrand. Experiments on the effectiveness of dataflow- and controlflow-based test adequacy criteria. </title> <booktitle> In Proc. of the 16th Int'l. Conf. on Softw. Eng., </booktitle> <pages> pages 191-200, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Some of these studies, such as those reported in References <ref> [3, 9, 12] </ref>, provide only indirect data about the effects of test suite minimization through consideration of the effects of test suite size on costs and benefits of testing. <p> Each program has a variety of versions, each containing one fault. Each program also has a large universe of inputs. These programs, versions, and inputs were assembled by researchers at Siemens Corporate Research for a study of the fault-detection capabilities of control-flow and data-flow coverage criteria <ref> [9] </ref>. We describe the other data in the table in the following paragraphs. <p> In a few cases they modified between two and five lines of code. Their goal was to introduce faults that were as realistic as possible, based on their experience with real programs. Ten people performed the fault seeding, working "mostly without knowledge of each other's work" <ref> [9, p. 196] </ref>. For each base program, the researchers at Siemens created a large test pool containing possible test cases for the program. <p> To populate these test pools, they first created an initial set of black-box test cases "according to good testing practices, based on the tester's understanding of the program's functionality and knowledge of special values and boundary points that are easily observable in the code" <ref> [9, p. 194] </ref>, using the category partition method and the Siemens Test Specification Language tool [1, 10]. <p> To obtain meaningful results with the seeded versions of the programs, the researchers retained only faults that were "neither too easy nor too hard to detect" <ref> [9, p. 196] </ref>, which they defined as being detectable by at least three and at most 350 test cases in the test pool associated with each program. 6 boxplots (explained in the caption for those unfamiliar with the notation) illustrate that the range of difficulty of the faults varies within and
Reference: [10] <author> T.J. Ostrand and M.J. Balcer. </author> <title> The category-partition method for specifying and generating functional tests. </title> <journal> Comm. of the ACM, </journal> <volume> 31(6), </volume> <month> June </month> <year> 1988. </year>
Reference-contexts: initial set of black-box test cases "according to good testing practices, based on the tester's understanding of the program's functionality and knowledge of special values and boundary points that are easily observable in the code" [9, p. 194], using the category partition method and the Siemens Test Specification Language tool <ref> [1, 10] </ref>. They then augmented this set with manually-created white-box test cases to ensure that each executable statement, edge, and definition-use pair in the base program or its control flow graph was exercised by at least 30 test cases.
Reference: [11] <author> G. Rothermel and M.J. Harrold. </author> <title> Analyzing regression test selection techniques. </title> <journal> IEEE Trans. on Softw. Eng., </journal> <volume> 22(8) </volume> <pages> 529-551, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The problems of regression test selection and test suite minimization are thus related, but dissimilar. For further discussion of regression test selection see Reference <ref> [11] </ref>. 2 The WHLM study involved ten common C UNIX utility programs, including nine programs ranging in size from 90 to 289 lines of code, and one program of 842 lines of code.
Reference: [12] <author> W. E. Wong, J. R. Horgan, S. London, and A. P. Mathur. </author> <title> Effect of test set size and block coverage on the fault detection effectiveness. </title> <booktitle> In Proc. of the Fifth Intl. Symp. on Softw. Rel. Engr., </booktitle> <pages> pages 230-238, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Some of these studies, such as those reported in References <ref> [3, 9, 12] </ref>, provide only indirect data about the effects of test suite minimization through consideration of the effects of test suite size on costs and benefits of testing.
Reference: [13] <author> W. E. Wong, J. R. Horgan, S. London, and A. P. Mathur. </author> <title> Effect of test set minimization on fault detection effectiveness. </title> <booktitle> In 17th Int'l. Conf. on Softw. Eng., </booktitle> <pages> pages 41-50, </pages> <month> April </month> <year> 1995. </year> <month> 18 </month>
Reference-contexts: This tradeoff between the time required to execute, validate, and manage test suites, and the fault detection effectiveness of test suites, is central to any decision to employ test suite minimization. A recent study <ref> [13] </ref> suggests that test suite minimization may produce dramatic savings in test suite size, at little cost to the fault-detection effectiveness of those test suites. To further explore this issue, we performed an experiment. Our experiment, however, revealed results quite different from those of the previous study. <p> Other studies, such as the study reported in Reference [5], provide only indirect data about the effects of test suite minimization through a comparison of regression test selection techniques that practice or do not practice minimization. 1 A recent study by Wong, Horgan, London, and Mathur <ref> [13] </ref>, however, directly examines the costs and benefits of test suite minimization. <p> The researchers did not attempt to achieve any particular coverage of the code; measurements taken after the test suites were generated indicated that the tests achieved block coverage between 50% to 95%. Reference <ref> [13] </ref> reports test suite sizes in terms of averages over groups of test cases that achieved similar coverage: 89 test suites belonged to groups in which average test suite size ranged from 12 to 27 test cases, and 933 test suites belonged to groups in which average test suite size ranged <p> The researchers minimized these test suites using ATAC [8], an heuristic-based minization tool that found "exact solutions for minimizations of all test suites examined" <ref> [13, page 42] </ref>. This minimization was done with respect to all-uses dataflow coverage (i.e., size was reduced while keeping all-uses coverage constant). The researchers measured the reduction in test suite size achieved through minimization, and the reduction in fault-detection effectiveness of the minimized test suites. <p> This observation leads us to believe that test cases that do not add coverage to a test set are likely to be ineffective in detecting additional faults. <ref> [13, page 50] </ref> 2.3 Open questions The WHLM study leaves a number of open questions. Many of these questions concern the extent to which the results observed in that study generalize to other testing situations. Among the open questions are the following, which motivate the present work: 1. <p> In both cases, however, all subject programs were of size less than 1000 lines of code. * The WHLM study used ATAC for minimization, whereas our study utilized the algorithm of Reference [6]. Reference <ref> [13] </ref> reports that the ATAC approach achieved minimal test selection on the cases studied; we 15 have not yet determined whether our algorithm was equally successful. <p> The authors of the WHLM study suggest that, had they minimized with respect to block coverage, "both the size and effectiveness reductions of the minimized test sets would have been substantially greater than for minimization with respect to all-uses coverage" <ref> [13, page 43] </ref>. * In our opinion, the factor likely to be most responsible for differences in results in the two studies involves the types of test suites utilized. The WHLM study used test suites that were not coverage-adequate, and that were relatively small compared to our test suites.
References-found: 13

