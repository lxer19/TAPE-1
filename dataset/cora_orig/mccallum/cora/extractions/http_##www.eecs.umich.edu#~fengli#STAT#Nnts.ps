URL: http://www.eecs.umich.edu/~fengli/STAT/Nnts.ps
Refering-URL: http://www.eecs.umich.edu/~fengli/STAT/index-7.htm
Root-URL: http://www.eecs.umich.edu
Email: E-mail: cc@maths.bath.ac.uk  
Title: Time Series Forecasting with Neural Networks: A Case Study  
Author: By JULIAN FARAWAY and CHRIS CHATFIELD 
Date: December 19th, 1995  
Address: USA  UK  UK.  
Affiliation: University of Michigan,  University of Bath,  of the Statistics Group, University of Bath.  
Pubnum: Research Report 95-06  
Abstract: SUMMARY Using the famous airline data as the main example, a variety of neural network (NN) models are fitted and the resulting forecasts are compared with those obtained from the (Box-Jenkins) seasonal ARIMA model called the airline model. The results suggest that there is plenty of scope for going badly wrong with NN models and that it is unwise to apply them blindly in `black-box' mode. Rather the wise analyst needs to use traditional modelling skills to select a good NN model, for example in making a careful choice of input variables. The BIC criterion is recommended for comparing different NN models. Great care is also needed when fitting a NN model and using it to produce forecasts. Methods of examining the response surface implied by a NN model are examined as well as alternative procedures using Generalized Additive Models and Projection Pursuit Regression. Keywords: ARIMA model; Box-Jenkins forecasting; Airline model; Generalized additive model; Projection pursuit regression 
Abstract-found: 1
Intro-found: 1
Reference: <author> Box, G. E. P. , G. M. Jenkins, G. M. and Reinsel, G. C. </author> <title> (1994) Time Series Analysis, </title> <booktitle> Forecasting and Control (3rd ed.). </booktitle> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: One seasonal and one non-seasonal moving average terms are then fitted. The resulting model is a special type of seasonal ARIMA model, often called the airline model. The model is of order (0,1,1) fi (0,1,1) 12 in the usual notation <ref> (see for example Box et al., 1994, p. 333) </ref>.
Reference: <author> Brown, R. G. </author> <title> (1962) Smoothing, Forecasting and Prediction of Discrete Time Series. </title>
Reference: <editor> Englewood Cliffs, </editor> <address> N.J.: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Chambers, J. and Hastie, T. </author> <title> (1992) Statistical Models in S. </title> <address> Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks/Cole. </publisher>
Reference: <author> Chatfield, C. </author> <title> (1993) Neural networks: Forecasting breakthrough or passing fad? Int. </title> <journal> J. of Forecasting, </journal> <volume> 9, </volume> <pages> 1-3. </pages>
Reference: <author> Chatfield, C. </author> <title> (1995) Model uncertainty, data mining and statistical inference (with discussion). </title> <journal> J. R. Statist. Soc. A, </journal> <volume> 158, </volume> <pages> 419-466. </pages> <address> 19 Chatfield, C. </address> <booktitle> (1996) Time Series Analysis (5th edn.) </booktitle> <address> London: </address> <publisher> Chapman & Hall. </publisher>
Reference-contexts: The over-optimism caused by choosing the best of many fitted models and then behaving as if the selected model were known to be true has been well documented in work on model uncertainty <ref> (e.g. Chatfield, 1995) </ref>. 5 Alternative ways of looking into the `black box' One major criticism levelled at NN models is that they provide a "black-box" approach which may produce satisfactory predictions but generally provides little insight into the structure of the data.
Reference: <author> Chatfield, C. and Faraway, J. </author> <title> (1996) Forecasting sales data with neural nets: A case study. Recherche et Applications en Marketing, </title> <note> (to appear). </note>
Reference: <author> Chatfield, C. and Prothero, D. L. </author> <title> (1973) Box-Jenkins seasonal forecasting: problems in a case study (with discussion). </title> <journal> J. R. Statist. Soc. A, </journal> <volume> 36, </volume> <pages> 295-336. </pages>
Reference: <author> Chatfield, C. and Yar, M. </author> <title> (1988) Holt-Winters forecasting: some practical issues. </title> <journal> The Statistician, </journal> <volume> 37, </volume> <pages> 129-140. </pages>
Reference: <author> Cheng, B. and Titterington, M. </author> <title> (1994) Neural networks: a review from a statistical perspective (with discussion). </title> <journal> Statistical Science, </journal> <volume> 9, </volume> <pages> 2-54. </pages> <editor> de Groot, C. and Wurtz, D. </editor> <title> (1991) Analysis of univariate time series with connectionist nets: A case study of two classical examples. </title> <journal> Neurocomputing, </journal> <volume> 3, </volume> <pages> 177-192. </pages>
Reference: <author> Friedman, J. and Stuetzle, W. </author> <title> (1981) Projection pursuit regression. </title> <journal> J. Am. Statist. Ass., </journal> <volume> 76, </volume> <pages> 817-823. </pages>
Reference-contexts: An alternative approach is provided by Projection Pursuit Regression (PPR) <ref> (see Friedman and Stuetzle, 1981) </ref>.
Reference: <author> Gershenfeld, N. A. and Weigend, A. S. </author> <title> (1994) The future of time series: learning and understanding. In Time Series Prediction (eds A.S. Weigend and N.A. </title> <booktitle> Gershenfeld), </booktitle> <pages> pp. 1-70. </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <editor> Gorr, W. L., Nagin, D. and Szczypula, J. </editor> <title> (1994) Comparative study of artificial neural network and statistical models for predicting student grade point averages. </title> <journal> Int. J. of Forecasting, </journal> <volume> 10, </volume> <pages> 17-34. </pages>
Reference-contexts: There NN models seem generally more appropriate, though, for the one financial time series (exchange rate data), there was a "crucial difference between training and test set performance" <ref> (Gershenfeld and Weigend, 1994, p. 40) </ref>.
Reference: <author> Harvey, A. </author> <booktitle> (1993) Time Series Models (2nd ed.). </booktitle> <address> Hemel Hempstead, U.K.: </address> <publisher> Harvester Wheatsheaf. </publisher>
Reference: <author> Hastie, T. and Tibshirani, R. </author> <title> (1990) Generalized Additive Models. </title> <publisher> London: Chapman Hall. </publisher>
Reference-contexts: Such plots will not necessarily be useful for all data and all NN models but they are valuable here. We now go on to consider some alternative modern statistical methods which can give similar results more directly. Generalized Additive Models (GAM) <ref> (Hastie and Tibshirani, 1990) </ref> can be used to represent the predicted response in terms of a sum of functions of the chosen inputs.
Reference: <author> Hertz, J., Krogh, A. and Palmer, R. </author> <title> (1991) Introduction to the Theory of Neural Computation. </title> <address> Redwood City, CA: </address> <publisher> Addison Wesley. </publisher>
Reference: <author> Hill, T., Marquez, L., O'Connor, M. and Remus, W. </author> <title> (1994) Artificial neural network models for forecasting and decision making. </title> <journal> Int. J. of Forecasting, </journal> <volume> 10, </volume> <pages> 5-15. </pages>
Reference: <author> Hill, T., O'Connor, M. and Remus, W. </author> <title> (1996) Neural network models for time series forecasts. </title> <journal> Man. </journal> <note> Science, (to appear). </note>
Reference: <author> Hoptroff, R. </author> <title> (1993) The principles and practice of time series forecasting and business modelling using neural nets. </title> <journal> Neural Computing and Applications, </journal> <volume> 1, </volume> <pages> 59-66. </pages>
Reference-contexts: NN modelling is non-parametric in character and it has been suggested that the whole process can be completely automated on a computer "so that people with little knowledge of either forecasting or neural nets can prepare reasonable forecasts in a short space of time" <ref> (Hoptroff, 1993) </ref>. This black-box character can be seen as an advantage or disadvantage. Certainly black boxes can sometimes give silly results and NN models obtained in this way are no exception.
Reference: <author> Priestley, M. B. </author> <title> (1981) Spectral Analysis and Time Series. </title> <publisher> London: Academic Press. </publisher>
Reference-contexts: Model selection criteria such as the AIC and BIC, defined in Section 2, provide a fairer comparison <ref> (see for example Priestley, 1981) </ref>. For NN models, the number of parameters, p, is the number of weights, while n, the number of effective observations, depends on the maximum lag. The results for a range of models are shown in Table 3. The symbols are defined in Section 2.
Reference: <author> Ripley, B. </author> <title> (1993) Statistical aspects of neural networks. In Chaos and Networks Statistical and Probabilistic Aspects (eds O. </title> <editor> Barndorff-Nielsen, J. Jensen, and W. </editor> <booktitle> Kendall), </booktitle> <pages> pp. 40-123. </pages> <address> London: </address> <publisher> Chapman and Hall. 20 Ripley, </publisher> <editor> B. D. </editor> <title> (1995) Statistical ideas for selecting network architectures. </title> <booktitle> In Neural Networks: Artificial Intelligence and Industrial Applications (eds B. </booktitle> <editor> Kappen and S. </editor> <booktitle> Gielen), </booktitle> <pages> pp. 183-190. </pages> <address> Berlin: </address> <publisher> Springer. </publisher>
Reference: <author> Ripley, B. D. </author> <title> (1996) Pattern Recognition and Neural Networks. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference: <author> Tang, Z., de Almeida, C. and Fishwick, P. A. </author> <title> (1991) Time series forecasting using neural networks versus Box-Jenkins methodology. </title> <journal> Simulation, </journal> <volume> 57, </volume> <pages> 303-310. </pages>
Reference: <author> Venables, W. N. and Ripley, B. D. </author> <title> (1994) Modern Applied Statistics with S-Plus. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The minimization is no easy task as the objective function often has several local minima and the number of weights may be large. Various numerical algorithms have been proposed but we used some software written in the S-Plus language by Brian Ripley <ref> (See Venables and Ripley, 1994) </ref>. The training algorithm for selecting the weights may take several hundred iterations to converge, but may still converge to a local minimum.
References-found: 23

