URL: ftp://ftp.eecs.umich.edu/people/pynadath/aaai96ex.ps.Z
Refering-URL: http://ai.eecs.umich.edu/people/pynadath/planrec.html
Root-URL: http://www.cs.umich.edu
Email: fpynadath,wellmang@umich.edu  
Title: Generalized Queries on Probabilistic Context-Free Grammars  
Author: David V. Pynadath and Michael P. Wellman 
Note: Extended version of paper in the Proceedings of the Thirteenth National Conference on Arti ficial Intelligence (1996) pages 1285-1290  
Date: June 24, 1997  
Address: 1101 Beal Avenue Ann Arbor, MI 48109  
Affiliation: Artificial Intelligence Laboratory University of Michigan  
Abstract: Probabilistic context-free grammars (PCFGs) provide a simple way to represent a particular class of distributions over sentences in a context-free language. Efficient parsing algorithms for answering particular queries about a PCFG (i.e., calculating the probability of a given sentence, or finding the most likely parse) have been developed, and applied to a variety of pattern-recognition problems. We extend the class of queries that can be answered in several ways: (1) allowing missing tokens in a sentence or sentence fragment, (2) supporting queries about intermediate structure, such as the presence of particular nonterminals, and (3) flexible conditioning on a variety of types of evidence. Our method works by constructing a Bayesian network to represent the distribution of parse trees induced by a given PCFG. The network structure mirrors that of the chart in a standard parser, and is generated using a similar dynamic-programming approach. We present an algorithm for constructing Bayesian networks from PCFGs, and show how queries or patterns of queries on the network correspond to interesting queries on PCFGs. The network formalism also supports extensions to encode various context sensitivities within the probabilistic dependency structure. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ezra Black, Fred Jelinek, John Lafferty, David M. Magerman, Robert Mercer, and Salim Roukos. </author> <title> Towards history-based grammars: Using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 3137, </pages> <year> 1993. </year>
Reference-contexts: The domain of each P ijk variable therefore consists of productions, augmented with the j and k indices of each of the symbols on the right-hand side. In the previous example, the domain of P 141 would require two possible values, S ! np <ref> [1; 3] </ref>vp [3; 1] and S ! np [2; 1]vp [2; 1], where the numbers in brackets correspond to the j and k values, respectively, of the associated symbol. <p> The domain of each P ijk variable therefore consists of productions, augmented with the j and k indices of each of the symbols on the right-hand side. In the previous example, the domain of P 141 would require two possible values, S ! np [1; 3]vp <ref> [3; 1] </ref> and S ! np [2; 1]vp [2; 1], where the numbers in brackets correspond to the j and k values, respectively, of the associated symbol. If we know that P 141 is the former, then N 113 = np and N 231 = vp with probability one. <p> In the previous example, the domain of P 141 would require two possible values, S ! np [1; 3]vp [3; 1] and S ! np <ref> [2; 1] </ref>vp [2; 1], where the numbers in brackets correspond to the j and k values, respectively, of the associated symbol. If we know that P 141 is the former, then N 113 = np and N 231 = vp with probability one. <p> In the previous example, the domain of P 141 would require two possible values, S ! np [1; 3]vp [3; 1] and S ! np <ref> [2; 1] </ref>vp [2; 1], where the numbers in brackets correspond to the j and k values, respectively, of the associated symbol. If we know that P 141 is the former, then N 113 = np and N 231 = vp with probability one. <p> The other abstractions and decompositions proceed along similar lines, with additional summation required when multiple productions or multiple levels of abstraction are possible. The final table is shown in Fig. 5, which lists only the nonzero values. 10 COMPUTE-BETA (grammar,length) for each symbol x 2TERMINALS (grammar) fi <ref> [x; 1; 1] </ref> 1:0 for j 1 to length kmax [j] 0 for each symbol E 2NONTERMINALS (grammar) fi [E; j; 1] 0:0 then /* Decomposition phase */ for each production E ! E 1 E m (p) 2DECOMP-PRODS (grammar) for each sequence fj t g m t=1 such that P <p> The final table is shown in Fig. 5, which lists only the nonzero values. 10 COMPUTE-BETA (grammar,length) for each symbol x 2TERMINALS (grammar) fi [x; 1; 1] 1:0 for j 1 to length kmax [j] 0 for each symbol E 2NONTERMINALS (grammar) fi <ref> [E; j; 1] </ref> 0:0 then /* Decomposition phase */ for each production E ! E 1 E m (p) 2DECOMP-PRODS (grammar) for each sequence fj t g m t=1 such that P for each sequence fk t g m t=1 such that 1 k t kmax [j t ] result p <p> (p) 2DECOMP-PRODS (grammar) for each sequence fj t g m t=1 such that P for each sequence fk t g m t=1 such that 1 k t kmax [j t ] result p for t 1 to m result resultfi [E t ; j t ; k t ] fi <ref> [E; j; 1] </ref> fi [E; j; 1]+result /* Abstraction Phase */ while fi [E 0 ; j; kmax [j] + 1] &gt; 0 for some E 0 kmax [j] kmax [j] + 1 for each production E ! E 0 (p) 2ABSTRACT-PRODS (grammar) if fi [E 0 ; j; kmax [j]] <p> each sequence fj t g m t=1 such that P for each sequence fk t g m t=1 such that 1 k t kmax [j t ] result p for t 1 to m result resultfi [E t ; j t ; k t ] fi <ref> [E; j; 1] </ref> fi [E; j; 1]+result /* Abstraction Phase */ while fi [E 0 ; j; kmax [j] + 1] &gt; 0 for some E 0 kmax [j] kmax [j] + 1 for each production E ! E 0 (p) 2ABSTRACT-PRODS (grammar) if fi [E 0 ; j; kmax [j]] &gt; 0 then fi <p> More global models of context sensitivity will likely require a radically different grammatical form and probabilistic interpretation framework. The History-Based Grammar (HBG) <ref> [1] </ref> provides a rich model of context sensitivity by conditioning the production probabilities on (potentially) the entire parse tree available at the current expansion point. Since our Bayesian networks represent all positions of the parse tree, it is theoretically possible to represent these conditional probabilities by introducing the appropriate links.
Reference: [2] <author> Ted Briscoe and John Carroll. </author> <title> Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. </title> <booktitle> Computational Linguistics, </booktitle> <address> 19(1):2559, </address> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: The flexible framework of our Bayesian-network representation supports further extensions to context-sensitive probabilities, as in the probabilistic parse tables of Briscoe & Carroll <ref> [2] </ref>. Section 4 explores several possible ways to relax the independence assumptions of the PCFG model within our approach. <p> In the previous example, the domain of P 141 would require two possible values, S ! np [1; 3]vp [3; 1] and S ! np <ref> [2; 1] </ref>vp [2; 1], where the numbers in brackets correspond to the j and k values, respectively, of the associated symbol. If we know that P 141 is the former, then N 113 = np and N 231 = vp with probability one. <p> In the previous example, the domain of P 141 would require two possible values, S ! np [1; 3]vp [3; 1] and S ! np <ref> [2; 1] </ref>vp [2; 1], where the numbers in brackets correspond to the j and k values, respectively, of the associated symbol. If we know that P 141 is the former, then N 113 = np and N 231 = vp with probability one. <p> However, once we have the network, we can again use any of the query algorithms from Section 3.4. Thus, we have a unified framework for performing inference, regardless of the form of the language model used to generate the networks. Probabilistic parse tables <ref> [2] </ref> and stochastic programs [14] provide alternate frameworks for introducing context sensitivity. The former approach uses the 27 finite-state machine of the chart parser as the underlying structure and introduces context sensitivity into the transition probabilities.
Reference: [3] <author> Eugene Charniak. </author> <title> Statistical Language Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Most pattern-recognition problems start from observations generated by some structured stochastic process. Probabilistic context-free grammars (PCFGs) <ref> [10, 3] </ref> have provided a useful method for modeling uncertainty in a wide range of structures, including natural languages [3], programming languages [22], images [6], speech signals [17], and RNA sequences [20]. <p> 1 Introduction Most pattern-recognition problems start from observations generated by some structured stochastic process. Probabilistic context-free grammars (PCFGs) [10, 3] have provided a useful method for modeling uncertainty in a wide range of structures, including natural languages <ref> [3] </ref>, programming languages [22], images [6], speech signals [17], and RNA sequences [20]. Domains like plan recognition, where non-probabilistic grammars have provided useful models [21], may also benefit from an explicit stochastic model. <p> The sum of probabilities p over all expansions of a given nonterminal E must be one. The examples in this paper will use the sample grammar (from Charniak <ref> [3] </ref>) shown in Fig. 1. This definition of the PCFG model prohibits rules of the form E ! ", where " represents the empty string. However, we can rewrite any PCFG to eliminate such rules and still represent the original distribution [3], as long as we note the probability Pr (S <p> this paper will use the sample grammar (from Charniak <ref> [3] </ref>) shown in Fig. 1. This definition of the PCFG model prohibits rules of the form E ! ", where " represents the empty string. However, we can rewrite any PCFG to eliminate such rules and still represent the original distribution [3], as long as we note the probability Pr (S ! "). For clarity, the algorithm descriptions in this paper assume Pr (S ! ") = 0, but a negligible amount of additional bookkeeping can correct for any nonzero probability. <p> np pp: 0.0014 vp!verb np: 0.00216 3 vp!verb pp: 0.016 np!noun pp: 0.036 2 np!noun np: 0.0018 vp! verb np: 0.024 pp!prep np: 0.2 np!noun: 0.02 np!noun: 0.18 np!noun: 0.2 1 verb!swat: 0.2 verb!flies: 0.4 prep!like: 1.0 noun!ants: 0.5 noun!swat: 0.05 noun!flies: 0.45 verb!like: 0.4 i=1 2 3 4 ity <ref> [3] </ref>, the probability of a subtree rooted by a particular symbol appearing amid a particular string. Given these probabilities we can compute the probability of any particular nonterminal symbol appearing in the parse tree as the root of a subtree covering some subsequence. <p> The domain of each P ijk variable therefore consists of productions, augmented with the j and k indices of each of the symbols on the right-hand side. In the previous example, the domain of P 141 would require two possible values, S ! np <ref> [1; 3] </ref>vp [3; 1] and S ! np [2; 1]vp [2; 1], where the numbers in brackets correspond to the j and k values, respectively, of the associated symbol. <p> The domain of each P ijk variable therefore consists of productions, augmented with the j and k indices of each of the symbols on the right-hand side. In the previous example, the domain of P 141 would require two possible values, S ! np [1; 3]vp <ref> [3; 1] </ref> and S ! np [2; 1]vp [2; 1], where the numbers in brackets correspond to the j and k values, respectively, of the associated symbol. If we know that P 141 is the former, then N 113 = np and N 231 = vp with probability one. <p> The PCFG specifies the relative probabilities of different productions for each nonterminal, but we must compute the probability, fi (E; j; k) (analogous to the inside probability <ref> [3] </ref>), that each symbol E t on the right-hand side is the root node of a subtree, at abstraction level k t , with a terminal subsequence length j t . 3.2 Calculating fi 3.2.1 Algorithm We can calculate the values for fi with a modified version of the dynamic programming
Reference: [4] <author> Eugene Charniak and Glenn Carroll. </author> <title> Context-sensitive statistics for improved grammatical language models. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 728733, </pages> <year> 1994. </year>
Reference-contexts: Alternatively, we may introduce additional dependencies on other nodes in the network. A PCFG extension that conditions the production probabilities on the parent of the left-hand side symbol has already proved useful in modeling natural language <ref> [4] </ref>. In this case, each production has a set of associated probabilities, one for each nonterminal symbol that is a possible parent of the symbol 23 on the left-hand side. This new probability structure requires modifications to both the dynamic programming and the network generation algorithms.
Reference: [5] <author> Eugene Charniak and Solomon Eyal Shimony. </author> <title> Cost-based abduction and MAP explanation. </title> <journal> Artificial Intelligence, </journal> <volume> 66:345374, </volume> <year> 1994. </year>
Reference-contexts: Alternate network algorithms can compute the most probable state of the random variables given the evidence, instead of a conditional probability <ref> [18, 5, 8] </ref>. For example, consider the case of possible four-word sentences beginning with the phrase Swat flies. . . ..
Reference: [6] <author> Pilip A. Chou. </author> <title> Recognition of equations using a two-dimensional stochastic context-free grammar. </title> <booktitle> In Proceedings SPIE, Visual Communications and Image Processing IV, </booktitle> <pages> pages 852863, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Most pattern-recognition problems start from observations generated by some structured stochastic process. Probabilistic context-free grammars (PCFGs) [10, 3] have provided a useful method for modeling uncertainty in a wide range of structures, including natural languages [3], programming languages [22], images <ref> [6] </ref>, speech signals [17], and RNA sequences [20]. Domains like plan recognition, where non-probabilistic grammars have provided useful models [21], may also benefit from an explicit stochastic model.
Reference: [7] <author> Adnan Darwiche and Gregory Provan. </author> <title> Query DAGs: A practical paradigm for implementing belief-network inference. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 6:147176, </volume> <year> 1997. </year> <month> 29 </month>
Reference-contexts: Unfortunately, such compilation can itself be prohibitive and will often produce networks of exponential size. There exist Bayesian network algorithms <ref> [9, 7] </ref> that offer greater flexibility in compilation, possibly allowing us to to limit the size of the resulting networks, while still providing acceptable query response times.
Reference: [8] <author> Rina Dechter. </author> <title> Bucket elimination: A unifying framework for probabilis-tic inference. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 211219, </pages> <year> 1996. </year>
Reference-contexts: The standard Bayesian network algorithms <ref> [18, 16, 8] </ref> can return joint probabilities of the form Pr (X i 1 j 1 k 1 = x 1 ; : : : ; X i m j m k m = x m ) or conditional probabilities of the form Pr (X ijk = xjX i 1 j <p> Alternate network algorithms can compute the most probable state of the random variables given the evidence, instead of a conditional probability <ref> [18, 5, 8] </ref>. For example, consider the case of possible four-word sentences beginning with the phrase Swat flies. . . ..
Reference: [9] <author> Rina Dechter. </author> <title> Topological parameters for time-space tradeoff. </title> <booktitle> In Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 220227, </pages> <year> 1996. </year>
Reference-contexts: Unfortunately, such compilation can itself be prohibitive and will often produce networks of exponential size. There exist Bayesian network algorithms <ref> [9, 7] </ref> that offer greater flexibility in compilation, possibly allowing us to to limit the size of the resulting networks, while still providing acceptable query response times.
Reference: [10] <author> Rafael C. Gonzalez and Michael S. Thomason. </author> <title> Syntactic pattern recognition: An introduction. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1978. </year>
Reference-contexts: 1 Introduction Most pattern-recognition problems start from observations generated by some structured stochastic process. Probabilistic context-free grammars (PCFGs) <ref> [10, 3] </ref> have provided a useful method for modeling uncertainty in a wide range of structures, including natural languages [3], programming languages [22], images [6], speech signals [17], and RNA sequences [20].
Reference: [11] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1979. </year>
Reference-contexts: Therefore, the probability of a given derivation is simply the product 3 of the probabilities of the individual productions involved. We define the parse tree representation of each such derivation as for non-probabilistic context-free grammars <ref> [11] </ref>. The probability of a string in the language is the sum taken over all its possible derivations. 2.1 Standard PCFG Algorithms Since the number of possible derivations grows exponentially with the string's length, direct enumeration would not be computationally viable. <p> If we restrict the types of context sensitivity, then we are more likely to find such a network generation algorithm. In the non-stochastic case, context-sensitive grammars <ref> [11] </ref> provide a more structured model than the general unrestricted grammar by allowing only productions of the form ff 1 Aff 2 ! ff 1 Bff 2 , where the ffs are arbitrary sequences of terminal and/or nonterminal symbols.
Reference: [12] <author> Frederick Jelinek, John D. Lafferty, and R. L. Mercer. </author> <title> Basic methods of probabilistic context free grammars. </title> <editor> In P. Laface and R. DeMori, editors, </editor> <booktitle> Speech Recognition and Understanding, </booktitle> <pages> pages 345360. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference-contexts: Instead, the standard dynamic programming approach used for both probabilistic and non-probabilistic CFGs <ref> [12] </ref> exploits the common production sequences shared across derivations. The central structure is a table, or chart, storing previous results for each subsequence in the input sentence. Each entry in the chart corresponds to a subsequence x i x i+j1 of the observation string x 1 x L . <p> For example, in the sentence Swat flies like ants, we can compute the probability that like ants is a prepositional phrase, using a combination of inside and outside probabilities. The Left-to-Right Inside (LRI) algorithm <ref> [12] </ref> specifies how we can use inside probabilities to obtain the probability of a given initial subsequence, such as the probability of a sentence (of any length) beginning with the words Swat flies. <p> The probability of an initial subsequence like Swat flies. . . , as computed by the LRI algorithm <ref> [12] </ref>, corresponds to the probability Pr (N 111 = swat; N 211 = like). Since the Bayesian network represents the distribution over strings of bounded length, we can find initial subsequence probabilities only over completions of length bounded by n L.
Reference: [13] <author> Finn V. Jensen. </author> <title> An introduction to Bayesian networks. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: Therefore, abstraction productions connect nodes whose indices match in the i and j components, while decomposition productions connect nodes whose indices differ. 6 3 Bayesian Networks for PCFGs A Bayesian network <ref> [18, 16, 13] </ref> is a directed acyclic graph where nodes represent random variables, and associated with each node is a specification of the distribution of its variable conditioned on its predecessors in the graph.
Reference: [14] <author> Daphne Koller, David McAllester, and Avi Pfeffer. </author> <title> Effective Bayesian inference for stochastic programs. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <year> 1997. </year>
Reference-contexts: However, once we have the network, we can again use any of the query algorithms from Section 3.4. Thus, we have a unified framework for performing inference, regardless of the form of the language model used to generate the networks. Probabilistic parse tables [2] and stochastic programs <ref> [14] </ref> provide alternate frameworks for introducing context sensitivity. The former approach uses the 27 finite-state machine of the chart parser as the underlying structure and introduces context sensitivity into the transition probabilities.
Reference: [15] <author> David M. Magerman and Mitchell P. Marcus. Pearl: </author> <title> A probabilistic chart parser. </title> <booktitle> In Proceedings of the Second International Workshop on Parsing Technologies, </booktitle> <pages> pages 193199, </pages> <year> 1991. </year>
Reference-contexts: Although the proposed PCSG model cannot account for dependence on position or parent symbol, described earlier in this section, we could make similar extensions to account for these types of dependencies. The result would be similar to the context-sensitive probabilities of PEARL <ref> [15] </ref>. However, PEARL conditions the probabilities on a part-of-speech trigram, as well as on the sibling and parent nonterminal symbols.
Reference: [16] <author> Richard E. </author> <title> Neapolitan. Probabilistic Reasoning in Expert Systems: Theory and Algorithms. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Therefore, abstraction productions connect nodes whose indices match in the i and j components, while decomposition productions connect nodes whose indices differ. 6 3 Bayesian Networks for PCFGs A Bayesian network <ref> [18, 16, 13] </ref> is a directed acyclic graph where nodes represent random variables, and associated with each node is a specification of the distribution of its variable conditioned on its predecessors in the graph. <p> The standard Bayesian network algorithms <ref> [18, 16, 8] </ref> can return joint probabilities of the form Pr (X i 1 j 1 k 1 = x 1 ; : : : ; X i m j m k m = x m ) or conditional probabilities of the form Pr (X ijk = xjX i 1 j
Reference: [17] <author> Hermann Ney. </author> <title> Stochastic grammars and pattern recognition. </title> <editor> In P. Laface and R. DeMori, editors, </editor> <booktitle> Speech Recognition and Understanding, </booktitle> <pages> pages 319 344. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Most pattern-recognition problems start from observations generated by some structured stochastic process. Probabilistic context-free grammars (PCFGs) [10, 3] have provided a useful method for modeling uncertainty in a wide range of structures, including natural languages [3], programming languages [22], images [6], speech signals <ref> [17] </ref>, and RNA sequences [20]. Domains like plan recognition, where non-probabilistic grammars have provided useful models [21], may also benefit from an explicit stochastic model. Once we have created a PCFG model of a process, we can apply existing PCFG parsing algorithms to answer a variety of queries.
Reference: [18] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1987. </year> <month> 30 </month>
Reference-contexts: Therefore, abstraction productions connect nodes whose indices match in the i and j components, while decomposition productions connect nodes whose indices differ. 6 3 Bayesian Networks for PCFGs A Bayesian network <ref> [18, 16, 13] </ref> is a directed acyclic graph where nodes represent random variables, and associated with each node is a specification of the distribution of its variable conditioned on its predecessors in the graph. <p> The standard Bayesian network algorithms <ref> [18, 16, 8] </ref> can return joint probabilities of the form Pr (X i 1 j 1 k 1 = x 1 ; : : : ; X i m j m k m = x m ) or conditional probabilities of the form Pr (X ijk = xjX i 1 j <p> Alternate network algorithms can compute the most probable state of the random variables given the evidence, instead of a conditional probability <ref> [18, 5, 8] </ref>. For example, consider the case of possible four-word sentences beginning with the phrase Swat flies. . . ..
Reference: [19] <author> David V. Pynadath and Michael P. Wellman. </author> <title> Accounting for context in plan recognition, with application to traffic monitoring. </title> <booktitle> In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 472 481, </pages> <year> 1995. </year>
Reference-contexts: In many cases, we may wish to account for external influences, such as explicit context representation in natural language problems or influences of the current world state in planning, as required by many plan recognition problems <ref> [19] </ref>. For instance, if we are processing multiple sentences, we may want to draw links from the symbol nodes of one sentence to the production nodes of another, to reflect thematic connections.
Reference: [20] <author> Yasubumi Sakakibara, Michael Brown, Rebecca C. Underwood, I. Saira Mian, and David Haussler. </author> <title> Stochastic context-free grammars for modeling RNA. </title> <booktitle> In Proceedings of the 27th Hawaii International Conference on System Sciences, </booktitle> <pages> pages 284293, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Most pattern-recognition problems start from observations generated by some structured stochastic process. Probabilistic context-free grammars (PCFGs) [10, 3] have provided a useful method for modeling uncertainty in a wide range of structures, including natural languages [3], programming languages [22], images [6], speech signals [17], and RNA sequences <ref> [20] </ref>. Domains like plan recognition, where non-probabilistic grammars have provided useful models [21], may also benefit from an explicit stochastic model. Once we have created a PCFG model of a process, we can apply existing PCFG parsing algorithms to answer a variety of queries.
Reference: [21] <author> Marc Vilain. </author> <title> Getting serious about parsing plans: A grammatical analysis of plan recognition. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 190197, </pages> <year> 1990. </year>
Reference-contexts: Probabilistic context-free grammars (PCFGs) [10, 3] have provided a useful method for modeling uncertainty in a wide range of structures, including natural languages [3], programming languages [22], images [6], speech signals [17], and RNA sequences [20]. Domains like plan recognition, where non-probabilistic grammars have provided useful models <ref> [21] </ref>, may also benefit from an explicit stochastic model. Once we have created a PCFG model of a process, we can apply existing PCFG parsing algorithms to answer a variety of queries.
Reference: [22] <author> C. S. Wetherell. </author> <title> Probabilistic languages: A review and some open questions. </title> <journal> Computing Surveys, </journal> <volume> 12(4):361379, </volume> <year> 1980. </year> <month> 31 </month>
Reference-contexts: 1 Introduction Most pattern-recognition problems start from observations generated by some structured stochastic process. Probabilistic context-free grammars (PCFGs) [10, 3] have provided a useful method for modeling uncertainty in a wide range of structures, including natural languages [3], programming languages <ref> [22] </ref>, images [6], speech signals [17], and RNA sequences [20]. Domains like plan recognition, where non-probabilistic grammars have provided useful models [21], may also benefit from an explicit stochastic model.
References-found: 22

