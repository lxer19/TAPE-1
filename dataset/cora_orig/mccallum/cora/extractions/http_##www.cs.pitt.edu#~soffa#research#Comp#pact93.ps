URL: http://www.cs.pitt.edu/~soffa/research/Comp/pact93.ps
Refering-URL: http://www.cs.pitt.edu/~soffa/research/ilp.html
Root-URL: 
Title: Compliation Techniques for Fine and Medium Grain Parallelism, URSA: A Unified ReSource Allocator for Registers
Author: David A. Berson, Rajiv Gupta, and Mary Lou Soffa 
Keyword: Keyword Codes: C.1.1; D.3.4 Keywords: Single Data Stream Architectures; Programming Languages, Processor  
Address: Pittsburgh Pittsburgh, PA 15260  
Affiliation: Department of Computer Science, University of  
Date: January 1993  
Note: Presented at IFIP WG 10.3(Concurrent Systems) Working Conference on Architectures and  Orlando, Fl.,  
Abstract: The division of instruction scheduling and register allocation and assignment into separate phases can adversely affect the performance of these tasks and thus the quality of the code generated for load/store fine grained parallel architectures. Improved performance in one phase can deteriorate the performance of the other phase, possibly resulting in poorer overall performance. In this paper we present an approach that partitions instruction scheduling and register allocation and assignment into a new set of phases in an attempt to construct phases with minimal interaction. This approach uses a technique that unifies the problems of allocating registers and functional units. The technique, which consists of three phases, operates on a dependence DAG representation of the program. The first phase carries out the measurement of resource requirements and identifies regions with excess requirements. The second phase applies transformations that reduce the requirements to levels supported by the target machine. The final phase carries out the assignment of resources. 
Abstract-found: 1
Intro-found: 1
Reference: [AiN88] <author> A. Aiken and A. Nicolau, </author> <title> A Development Environment for Horizontal Microcode, </title> <journal> IEEE Trans. on Software Engineering 14, </journal> <month> 5 (May </month> <year> 1988), </year> <pages> 584-594. </pages>
Reference-contexts: By constructing DAGs of traces, which are basic block sequences, trace scheduling allows code motion across basic blocks. Other techniques that also allow code motion across basic blocks include percolation scheduling and region scheduling <ref> [AiN88, GuS90] </ref>. However, they do not use an explicit DAG representation during code motion. In this work we use DAGs since they represent a partial order on the execution of instructions, which provides a basis for measuring resource requirements.
Reference: [BGS92] <author> D. A. Berson, R. Gupta and M. L. Soffa, </author> <title> URSA: A Unified ReSource Allocator for Registers and Functional Units in VLIW Architectures, </title> <type> Technical Report 92-21, </type> <institution> University of Pittsburgh, Computer Science Department, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: The single root and single leaf represent the entry to and exit from the basic block. Details of the algorithms described in the subsequent sections can be found in <ref> [BGS92] </ref>. 3. Measuring Resource Requirements The measurements of register and functional unit requirements use the same algorithm and a common data structure, a Reuse DAG, which indicates which instructions can reuse a resource used by a previous instruction. <p> This is accomplished by finding the minimum sized set of descendants that kill all of their ancestors. Theorem 2 Defining Kill () for all nodes in the DAG is NP-Complete. Proof: By reduction to the Minimum cover problem <ref> [BGS92] </ref>. The sub-DAG -B, C, E, F- of the DAG in Figure 2 (b) is an example of the difficult case. An optimal solution to the minimum cover problem for this sub-DAG will choose the same node to kill both B and C. Let the solution be F.
Reference: [BEH91] <author> D. G. Bradlee, S. J. Eggers and R. R. Henry, </author> <title> Integrating Register Allocation and Instruction Scheduling for RISCs, </title> <booktitle> Proc. Fourth International Conf. on ASPLOS, </booktitle> <address> Santa Clara, CA, </address> <month> April 8-11, </month> <year> 1991, </year> <pages> 122-131. </pages>
Reference-contexts: The relative benefit of several possible transformations can be compared by determining the amount that each transformation reduces the resource requirements and its impact on the critical path through the DAG. Several techniques have been proposed to address the issue of interaction between the two problems for pipelined architectures <ref> [BEH91, GoH88, SwB90] </ref>. However all of these techniques still attempt to solve the problems separately while taking into account the interactions to a limited extent.
Reference: [Dil50] <author> R. P. </author> <title> Dilworth, A Decomposition Theorem for Partially Ordered Sets, </title> <booktitle> Annuals of Mathematics 51(1950), </booktitle> <pages> 161-166. </pages>
Reference-contexts: Proof: see <ref> [Dil50] </ref>. The DAG in Figure 2 (b) can be minimally decomposed into a set of four chains, such as - -A, B, E, I, K-, -C, F-, -D, G, J-, -H -. Thus, at most four nodes at a time can execute in parallel.
Reference: [Fis81] <author> J. A. Fisher, </author> <title> Trace Scheduling: A Technique for Global Microcode Compaction, </title> <journal> IEEE Trans. on Computers C-30, </journal> <month> 7 (July </month> <year> 1981), </year> <pages> 478-490. </pages>
Reference-contexts: The major steps of URSA are summarized in Figure 1. The basic structure that is used for representing parallelism in straight line code is the dependence DAG. A DAG representation is suitable for exploiting parallelism present within basic blocks as well as parallelism across basic block boundaries <ref> [Fis81] </ref>. By constructing DAGs of traces, which are basic block sequences, trace scheduling allows code motion across basic blocks. Other techniques that also allow code motion across basic blocks include percolation scheduling and region scheduling [AiN88, GuS90]. However, they do not use an explicit DAG representation during code motion.
Reference: [FoF65] <author> L. R. Ford and D. R. Fulkerson, </author> <title> Flows in Networks, </title> <publisher> Princeton University Press, </publisher> <address> Princeton, N.J., </address> <year> 1965. </year>
Reference-contexts: Clearly, all chains in a Reuse R DAG are allocation chains. Therefore, by Theorem 1, a minimum decomposition of a Reuse R DAG into allocation chains gives the maximum resource requirements of R for the original DAG. Ford and Fulkerson <ref> [FoF65] </ref> have shown that the problem of finding a minimum chain decomposition can be solved by transforming it into a maximum bipartite graph matching problem. The bipartite graph represents all possible pairs of nodes (a , b ) CanReuse R . <p> The computation of the excessive chain sets requires that the projection of the minimum decomposition for the DAG onto any hammock also be a minimum decomposition. The decomposition algorithm in <ref> [FoF65] </ref> only guarantees minimum decomposition for the entire DAG, but not for all hammocks nested within the DAG. A bipartite matching algorithm based on augmenting paths can be modified to find matchings that use only edges that will result in a decomposition that is minimal for all nested hammocks.
Reference: [GoH88] <author> J. R. Goodman and W. Hsu, </author> <title> Code Scheduling and Register Allocation in Large Basic Blocks, </title> <booktitle> Proc. of the ACM Supercomputing Conference, </booktitle> <year> 1988, </year> <pages> 442-452. </pages>
Reference-contexts: The relative benefit of several possible transformations can be compared by determining the amount that each transformation reduces the resource requirements and its impact on the critical path through the DAG. Several techniques have been proposed to address the issue of interaction between the two problems for pipelined architectures <ref> [BEH91, GoH88, SwB90] </ref>. However all of these techniques still attempt to solve the problems separately while taking into account the interactions to a limited extent. <p> Our technique uses more information from a larger region of the program when performing allocation and scheduling. The effective goal of our technique is to maximize the average utilization of the resources by minimizing the execution time, without ever exceeding the limits of the target architecture. Goodman and Hsu <ref> [GoH88] </ref> also present a DAG driven technique. However, their technique uses prepass scheduling and does not have a mechanism for inserting spill code. Section 2 gives an overview of the URSA technique. Section 3 presents the techniques for measuring the resource requirements.
Reference: [GuS90] <author> R. Gupta and M. L. Soffa, </author> <title> Region Scheduling: An Approach for Detecting and Redistributing Parallelism, </title> <journal> IEEE Trans. on Software Engineering 16, </journal> <month> 4 (April </month> <year> 1990), </year> <pages> 421-431. </pages>
Reference-contexts: By constructing DAGs of traces, which are basic block sequences, trace scheduling allows code motion across basic blocks. Other techniques that also allow code motion across basic blocks include percolation scheduling and region scheduling <ref> [AiN88, GuS90] </ref>. However, they do not use an explicit DAG representation during code motion. In this work we use DAGs since they represent a partial order on the execution of instructions, which provides a basis for measuring resource requirements.
Reference: [SwB90] <author> P. Sweany and S. Beaty, </author> <title> Post-Compaction Register Assignment in a Retargetable Compiler, </title> <booktitle> Proc. of the 23rd Annual Workshop on Microprogramming and Microarchitecture, </booktitle> <month> Nov. </month> <year> 1990, </year> <pages> 107-116. </pages>
Reference-contexts: The relative benefit of several possible transformations can be compared by determining the amount that each transformation reduces the resource requirements and its impact on the critical path through the DAG. Several techniques have been proposed to address the issue of interaction between the two problems for pipelined architectures <ref> [BEH91, GoH88, SwB90] </ref>. However all of these techniques still attempt to solve the problems separately while taking into account the interactions to a limited extent.
References-found: 9

