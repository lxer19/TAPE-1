URL: http://www.icsi.berkeley.edu/~bilmes/icassp98.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/~bilmes/
Root-URL: http://www.icsi.berkeley.edu
Email: &lt;bilmes@cs.berkeley.edu&gt;  
Title: MAXIMUM MUTUAL INFORMATION BASED REDUCTION STRATEGIES FOR CROSS-CORRELATION BASED JOINT DISTRIBUTIONAL MODELING  
Author: Jeff A. Bilmes 
Address: 1947 Center Street, Suite 600 Berkeley, CA 94704, USA  Berkeley, CA 94720, USA  
Affiliation: International Computer Science Institute  CS Division, Department of EECS University of California at Berkeley  
Date: 1998  
Note: ICASSP'98 IEEE International Conference on Acoustics, Speech, and Signal Processing, Seattle,  
Abstract: In maximum-likelihood based speech recognition systems, it is important to accurately estimate the joint distribution of feature vectors given a particular acoustic model. In previous work, we showed we can boost accuracy in this task by modeling the joint distribution of time-localized feature vectors along with statistics relating those feature vectors to their surrounding context. In this work, we evaluate information preserving reduction strategies for those statistics. We claim that those statistics corresponding to spectro-temporal loci in speech with relatively large mutual information are most useful in estimating the information contained in the feature-vector joint distribution. Furthermore, we claim that such statistics are most likely to generalize. Using an EM algorithm to compute mutual information between pairs of points in the time-frequency grid, we verify these hypothesesusing both overlap plots and speech recognition word error results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jeff A. Bilmes. </author> <title> Joint distributional modeling with cross-correlation based features. </title> <booktitle> In Proc. </booktitle> <address> ASRU, Santa Barbara, </address> <month> December </month> <year> 1997. </year> <note> IEEE. </note>
Reference-contexts: There have been many attempts to model the feature-vector joint distribution more accurately. Some involve estimating the short-time joint distribution of feature vectors [5] and others add to that a conditional dependence on additional variables along with a distribution of those variables [7]. In <ref> [1] </ref>, we showed that we can model information contained in the feature-vector joint distribution by modeling the joint distribution of time-localized feature vectors along with statistics relating those feature vectors to their surrounding context, i.e.: P (X t ; dt [ E s [X t X l ])jQ t ); (1) <p> Using compressed envelopes as feature channels, we demonstrated a significant word error rate reduction in a noisy test condition with such a model. In <ref> [1] </ref>, we chose a data-independent reduction strategy M (). In this paper, we argue that we can obtain a better reduction strategy by retaining MCG coefficients corresponding only to pairs of time-feature points in training data with a strong statistical dependence (i.e., large mutual information). <p> Assuming, as we argue in <ref> [1] </ref>, that the joint distribution of X t and the statistics S E s [X t X T l ] captures important information about the feature-vector joint distribution, what fixed size subset of the statistics can best represent this distribution? With motivation from Chow's results, we argue that a subset containing <p> The MCG parameters are the same as in <ref> [1] </ref>. Each word error rate (WER) displayed is obtained using data from 200 speakers totaling 2600 examples from 4 jackknifed cuts scores shown are the average of 4 tests in which 150 speakers were used for training and 50 different speakers used for testing. <p> The number of hidden units of the ANN is adjusted to equalize the number of free parameters in each case. As can be seen, the addition of strong MCG features significantly reduces the error rate (at 200 strong MCG features, the WER is insignificantly different then our best result <ref> [1] </ref>). The addition of weak MCG features, however, significantly increases the error rate probably because the number of hidden units decreases while adding useless input features.
Reference: [2] <author> C.K. Chow and C.N. Liu. </author> <title> Approximating discrete probability distributions with dependence trees. </title> <journal> IEEE Trans. on Info. Theory, </journal> <volume> 14 </volume> <pages> 462-467, </pages> <year> 1968. </year>
Reference-contexts: Chow showed that the best tree-dependent approximation (one whose graph is a tree) of such a distribution, in terms of least Kullback-Leibler distance, can be constructed by finding a maximum weight spanning tree of the original graph with edge weights set as the mutual information between the corresponding two variables <ref> [2] </ref>.
Reference: [3] <author> T.M. Cover and J.A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: distribution, what fixed size subset of the statistics can best represent this distribution? With motivation from Chow's results, we argue that a subset containing strong statistics are better than a subset containing weak ones, where we define a strong and weak statistic according to the relative magnitude of mutual information <ref> [3] </ref> between the corresponding features elements in a training set. <p> But because I (X t;i ; Z) I (X t;i ; Q t ) I (X t;i ; ZjQ t ) I (X t;i ; Z) + I (X t;i ; Q t jZ) <ref> [3] </ref>, choosing Z with a large I (X t;i ; Z) can produce a range of larger possible values for I (X t;i ; ZjQ t ) than if we chose Z otherwise. An even simpler heuristic, therefore, is to choose Z with large I (X t;i ; Z). <p> In Section 4 we provide empirical evidence for this phenomena in real speech data. 3. MUTUAL INFORMATION COMPUTATION USING EM The mutual information between two continuous random variables X and Y is defined as <ref> [3] </ref>: I (X; Y ) = p (x; y) log p (x)p (y) dxdy: In practice, we have access only to samples f (x i ; y i ) : 1 i N g drawn from the distributions governing X and Y . We therefore must use an estimation method.
Reference: [4] <author> Solomon Kullback. </author> <title> Information Theory And Statistics. </title> <publisher> Dover, </publisher> <year> 1968. </year>
Reference-contexts: We therefore must use an estimation method. One method computes a 2-dimensional histogram and then performs a discrete version of the above computation. Another method assumes that X and Y are jointly Gaussian distributed with correlation coefficient xy . If so, the quantity can be computed analytically <ref> [4] </ref> as I l (X; Y ) = 1 2 log 2 (1 2 xy ): Because xy captures the linear dependence between X and Y regardless of their joint distribution, we call this the linear mutual information.
Reference: [5] <author> N. Morgan and H. Bourlard. </author> <title> Continuous speech recognition. </title> <journal> IEEE Signal Processing Magazine, </journal> <volume> 12(3), </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: This is the conditional independence problem associated with HMMs. There have been many attempts to model the feature-vector joint distribution more accurately. Some involve estimating the short-time joint distribution of feature vectors <ref> [5] </ref> and others add to that a conditional dependence on additional variables along with a distribution of those variables [7]. <p> SPEECH RECOGNITION RESULTS To lend further evidence to our hypothesis, we evaluated both strong and weak MCG features in a hybrid artificial neural network/hidden Markov model (ANN/HMM) speech recognition system <ref> [5] </ref> using strong and weak MCG features used in addition to baseline. digits+. The MCG parameters are the same as in [1]. <p> Before each ANN training, all weights were set to small random values. The far left of Figure 5 shows the baseline score consisting of 1 frame of JRASTA <ref> [5] </ref> features plus deltas (17 total) and a 572 hidden unit ANN probability estimator. Moving to the right, each point shows the addition of MCG features, in increments of 20, starting from the strongest and moving down or the weakest and moving up in strength.
Reference: [6] <author> A.C. Morris. </author> <title> An information-theoretical study of speech processing in the peripheral auditory system and cochlear nu-cleas. </title> <type> PhD thesis, </type> <institution> Institut National Polytechnique De Greno-ble, </institution> <year> 1992. </year>
Reference-contexts: We can instead use numerical integration, or even simpler, sample the resulting distribution for the discrete computation. sections of digits+. While the first non-parametric method is fairly simple, it suffers from several problems including the need for bias correction <ref> [6] </ref>, a large number of histogram bins, and large amounts of training data. Because we need to compute thousands of mutual information values, this approach is not viable since thousands of simultaneously maintained 2-dimensional histograms would be required.
Reference: [7] <author> M. Ostendorf, V. Digalakis, and O. Kimball. </author> <title> From HMM's to segment models: A unified view of stochastic modeling for speech recognition. </title> <journal> IEEE Trans. Speech and Audio Proc., </journal> <volume> 4(5) </volume> <pages> 360-378, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: There have been many attempts to model the feature-vector joint distribution more accurately. Some involve estimating the short-time joint distribution of feature vectors [5] and others add to that a conditional dependence on additional variables along with a distribution of those variables <ref> [7] </ref>.
Reference: [8] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Mor-gan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: a collection of random variables y 1 ; : : : ; y N (governed by P (y 1 ; : : : ; y N )) can be described as a graph where each node represents a variable and each edge rep-resents a dependence relation between its two variables <ref> [8] </ref>.
References-found: 8

