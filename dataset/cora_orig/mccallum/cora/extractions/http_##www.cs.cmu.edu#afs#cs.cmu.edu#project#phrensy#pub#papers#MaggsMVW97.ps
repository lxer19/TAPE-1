URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/phrensy/pub/papers/MaggsMVW97.ps
Refering-URL: http://www.cs.cmu.edu/~bmm/MaggsMVW97.html
Root-URL: 
Email: (email:  
Title: Exploiting Locality for Data Management in Systems of Limited Bandwidth  
Author: Bruce M. Maggs Friedhelm Meyer auf der Heide flfl Berthold Vocking flfl Matthias Westermann flfl 
Note: ffmadh, voecking, marsug@uni-paderborn.de). Supported in part by DFG-Sonderforschungsbereich 376, by EU ESPRIT Long Term Research Project 20244 (ALCOM-IT), and by DFG Leib-niz Grant Me872/6-1.  
Address: 33095 Paderborn, Ger-many  
Affiliation: flfl Department of Mathematics and Computer Science, and Heinz Nixdorf Institute, University of Paderborn,  
Abstract: This paper deals with data management in computer systems in which the computing nodes are connected by a relatively sparse network. We consider the problem of placing and accessing a set of shared objects that are read and written from the nodes in the network. These objects are, e.g., global variables in a parallel program, pages or cache lines in a virtual shared memory system, shared files in a distributed file system, or pages in the World Wide Web. A data management strategy consists of a placement strategy that maps the objects (possibly dynamically and with redundancy) to the nodes, and an access strategy that describes how reads and writes are handled by the system (including the routing). We investigate static and dynamic data management strategies. In the static model, we assume that we are given an application for which the rates of read and write acesses for all node-object pairs are known. The goal is to calculate a static placement of the objects to the nodes in the network and to specify the routing such that the network congestion is minimized. We introduce efficient algorithms that calculate optimal or close-to-optimal solutions for tree-connected networks, meshes of arbitrary dimension, and internet-like clustered networks. These algorithms take time only linear in the input size. In the dynamic model, we assume no knowledge about fl School of Computer Science, Carnegie Mellon University, Pitts-burgh, PA 15213 (email: bmm@cs.cmu.edu). Supported in part by the Air Force Materiel Command (AFMC) and ARPA under Contract F196828-93-C-0193, by ARPA Contracts F33615-93-1-1330 and N00014-95-1-1246, and by an NSF National Young Investigator Award, No. CCR-94-57766, with matching funds provided by NEC Research Institute and Sun Microsystems. This research was conducted in part while he was visiting the Heinz Nixdorf Institute, with support provided by DFG-Sonderforschungsbereich 376 "Massive Par-allelitat: Algorithmen, Entwurfsmethoden, Anwendungen". the access pattern. An adversary specifies accesses at run-time. Here we devolop dynamic caching strategies that also aim to minimize the congestion on trees, meshes, and clustered networks. These strategies are investigated in a competitive model. For example, we achieve competitive ratio 3 for tree-connected networks and competitive ratio O(d log n) for d-dimensional meshes of size n. Further, we present an (log n=d) lower bound for the competitive ratio for on-line routing in meshes, which implies that the achieved upper bound on the competive ratio for meshes of constant dimension is optimal. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Andrews, T. Leighton, P. T. Metaxas, and L. Zhang. </author> <title> Automatic methods for hiding latency in high bandwidth networks. </title> <booktitle> In Proc. of the 28th ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 257-265, </pages> <year> 1996. </year>
Reference-contexts: But whereas several standard methods are known for hiding latency, e.g., pipelined routing (see, e.g., [7, 8]), redundant computation (see, e.g., <ref> [1, 2, 15, 21, 22, 24] </ref>) or slackness (see, e.g., [30]) the only way to bypass the bandwidth bottleneck is to reduce the communication load by exploiting locality. The principle of locality is already known from sequential computation.
Reference: [2] <author> M. Andrews, T. Leighton, P. T. Metaxas, and L. Zhang. </author> <title> Improved methods for hiding latency in high bandwidth networks. </title> <booktitle> In Proc. of the 8th ACM Symp. on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 52-61, </pages> <year> 1996. </year>
Reference-contexts: But whereas several standard methods are known for hiding latency, e.g., pipelined routing (see, e.g., [7, 8]), redundant computation (see, e.g., <ref> [1, 2, 15, 21, 22, 24] </ref>) or slackness (see, e.g., [30]) the only way to bypass the bandwidth bottleneck is to reduce the communication load by exploiting locality. The principle of locality is already known from sequential computation.
Reference: [3] <author> Y. Aumann and R. Rabani. </author> <title> An O(log k) approximate min-cut max-flow theorem and approximation algorithm. </title> <journal> SIAM Journal on Computing, </journal> <note> to appear, </note> <year> 1997. </year>
Reference-contexts: Lemma 4.3 In the general case, q = (ff= log ffi). If K is a planar graph or a constant genus graph then q = (ff). Proof. First we consider the general case. According to <ref> [3] </ref> any multicommodity flow problem can be satisfied up to a factor O (S= log k) with S denoting the minimum cut ratio and k denoting the number of commodities. It is easy to check that the minimum cut ratio of our multicommodity flow problem is ff.
Reference: [4] <author> B. Awerbuch, Y. Bartal, and A. Fiat. </author> <title> Competitive distributed file allocation. </title> <booktitle> In Proc. of the 25th ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 164-173, </pages> <year> 1993. </year>
Reference-contexts: For instance, this yields N -processor PRAM sim ulations for the p p N mesh with slowdown fi ( p though this bound cannot be improved for general PRAM simulations, it is not satisfactory for applications including locality. Awerbuch et al. investigate dynamic data management in arbitrary networks. In <ref> [4] </ref> they present a dynamic caching strategy that minimizes the total communication load up to a polylogarithmic factor. In [5] they adapt their scheme to systems with limited memory capacities.
Reference: [5] <author> B. Awerbuch, Y. Bartal, and A. Fiat. </author> <title> Distributed paging for general networks. </title> <booktitle> In Proc. of the 7th ACM Symp. on Discrete Algorithms (SODA), </booktitle> <pages> pages 574-583, </pages> <year> 1996. </year>
Reference-contexts: Awerbuch et al. investigate dynamic data management in arbitrary networks. In [4] they present a dynamic caching strategy that minimizes the total communication load up to a polylogarithmic factor. In <ref> [5] </ref> they adapt their scheme to systems with limited memory capacities. However, their strategies uses the concept of a global leader that eventually is involved in nearly any access issued by one of the processors. This shows the importance of considering the congestion rather than the total communication load.
Reference: [6] <author> Y. Bartal, A. Fiat, and Y. Rabani. </author> <title> Competitive algorithms for distributed data management. </title> <booktitle> In Proc. of the 24th ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 39-50, </pages> <year> 1992. </year>
Reference-contexts: This shows the importance of considering the congestion rather than the total communication load. Better results are known for dynamic data management on tree-connected networks. Note that minimizing the total load in trees also minimizes the congestion. Here Bartal et al. <ref> [6] </ref> describe a randomized fi (1)-competitive dynamic strategy for trees, and Lund et al. [19] describe a deterministic but centralized strategy with same competitive ratio. (The competitive ratio given in both papers is 3.
Reference: [7] <author> R. J. Cole, B. M. Maggs, and R. K. Sitaraman. </author> <title> On the benefit of supporting virtual channels in wormhole routers. </title> <booktitle> In Proc. of the 8th ACM Symp. on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 131-141, </pages> <year> 1996. </year>
Reference-contexts: Usually, the buses and links are the bottleneck in these systems, since improving communication bandwidth and latency is often more expensive or more difficult than increasing processor speed and memory capacity. But whereas several standard methods are known for hiding latency, e.g., pipelined routing (see, e.g., <ref> [7, 8] </ref>), redundant computation (see, e.g., [1, 2, 15, 21, 22, 24]) or slackness (see, e.g., [30]) the only way to bypass the bandwidth bottleneck is to reduce the communication load by exploiting locality. The principle of locality is already known from sequential computation.
Reference: [8] <editor> R. Cypher, F. Meyer auf der Heide, C. Scheideler, and B. Vocking. </editor> <title> Universal algorithms for store-and-forward and wormhole routing. </title> <booktitle> In Proc. of the 26th ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 356-365, </pages> <year> 1996. </year>
Reference-contexts: Usually, the buses and links are the bottleneck in these systems, since improving communication bandwidth and latency is often more expensive or more difficult than increasing processor speed and memory capacity. But whereas several standard methods are known for hiding latency, e.g., pipelined routing (see, e.g., <ref> [7, 8] </ref>), redundant computation (see, e.g., [1, 2, 15, 21, 22, 24]) or slackness (see, e.g., [30]) the only way to bypass the bandwidth bottleneck is to reduce the communication load by exploiting locality. The principle of locality is already known from sequential computation. <p> Minimizing the congestion overcomes this drawback, because the congestion measure captures bottlenecks in the system. Therefore, it is a lower bound on the execution time of a given application. Moreover, several results on store-and-forward- and wormhole-routing (see, e.g., <ref> [8, 17, 23, 25, 28] </ref>) indicate that the congestion yields a good estimate for the execution time taken by coarse grained applications with high communication load. Hence, minimizing the congestion seems to be a good heuristic for efficient data management.
Reference: [9] <author> D. Dowdy and D. Foster. </author> <title> Comparative models of the file assignment problem. </title> <journal> Computing Surveys, </journal> <volume> 14(2) </volume> <pages> 287-313, </pages> <year> 1982. </year>
Reference-contexts: The first approaches to solve the problem concentrated on modeling it by mixed integer programs and solving these programs efficiently by using heuristics. Here several models with different cost functions and constraints have been developed. The 1981 survey paper by Dowdy and Foster <ref> [9] </ref> gives an overview of this work. Most theoretical work in the area of distributed data management concerns PRAM simulations. In [13], Kar-lin and Upfal present a probabilistic emulation on an N - node butterfly.
Reference: [10] <author> A. J. van de Goor. </author> <title> Computer Architecture and Design. </title> <publisher> Ad-dison Wesley, </publisher> <year> 1994. </year>
Reference-contexts: The principle of locality is already known from sequential computation. Two kinds of locality are usually distinguished: temporal and spatial locality (see, e.g., <ref> [10] </ref>). Temporal locality means that in the near future, a program is more likely to reference those data objects that have been referenced in the recent past. This locality can be due to instruction references in program loops, or data references in working stacks.
Reference: [11] <author> T. Hagerup and C. Rub. </author> <title> A guided tour of Chernoff bounds. </title> <journal> Information Processing Letters, </journal> <volume> 33 </volume> <pages> 305-308, </pages> <year> 1989/90. </year>
Reference-contexts: Then the static strategy for trees guarantees that for every ` 2 E x;j , K (`) = O (). As a consequence, the maximum weight in the sum of random variables in equation 1 is at most O (). Applying a Chernoff-Hoeffding bound <ref> [11] </ref> to this sum yields that L j deviates by at most O ( log n) from E [L j ], w.h.p., for 1 j 3. Since L = L 1 + L 2 + L 3 , it follows L = E [L] + O ( log n), w.h.p.. <p> Then L = x2X i=1 t X X A i (x) : The variables A i (x) in the sum S i = P dependent, for 1 i t . Applying a Chernoff-Hoeffding bound <ref> [11] </ref> to this sum yields that it deviates by at most O (log n) from E [S i ], w.h.p., for 1 i t .
Reference: [12] <author> D. Karger, E. Lehman, T. Leighton, M. Levine, D. Lewin, and R. Panigrahy. </author> <title> Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the world wide web. </title> <booktitle> In Proc. of the 29th ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 654-655, </pages> <year> 1997. </year>
Reference-contexts: Plaxton and Rajaraman [26] show how to balance the pages among several caches by embedding a random cache tree for each page into the network. This balances the load well and ensures fast responses even for popular pages. Karger et al. <ref> [12] </ref> use a similar technique to relieve hot spots in the WWW. Note that the technique of embedding a random tree for each object is similar to our access tree strategy. The main differences to our approach are the following. <p> The main differences to our approach are the following. The strategy in [26] uses a uniform embedding of the tree nodes onto the nodes in the Internet, which completely dissolves topological locality. The strategy in <ref> [12] </ref> pays attention to topological locality. In fact, they use a model similar to our Internet model. However, they consider the latencies instead of the bandwidths to be the main problem for data transmission in the Internet.
Reference: [13] <author> A. Karlin and E. Upfal. </author> <title> Parallel hashing-an efficient implementation of shared memory. </title> <booktitle> In Proc. of the 18th ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 160-168, </pages> <year> 1986. </year>
Reference-contexts: Here several models with different cost functions and constraints have been developed. The 1981 survey paper by Dowdy and Foster [9] gives an overview of this work. Most theoretical work in the area of distributed data management concerns PRAM simulations. In <ref> [13] </ref>, Kar-lin and Upfal present a probabilistic emulation on an N - node butterfly. Their algorithm emulates one step of an N - processor EREW PRAM in time fi (log N ), w.h.p..
Reference: [14] <author> P. Klein, S. A. Plotkin, and S. Rao. </author> <title> Excluded minors, network decomposition, and multicommodity flow. </title> <booktitle> In Proc. of the 25th ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 682-690, </pages> <year> 1993. </year>
Reference-contexts: Then we can translate the above multicommodity flow problem into a uniform multicommodity problem in which each node sends the same amount of data to each other node such that the flow and the cut ratio in both problems is nearly equivalent. According to <ref> [14] </ref>, the flow in uniform multicom-modity factor can be satisfied up to a factor q = O (S) with S denoting the minimum cut ratio. Applying, S = O (ff) yields the lemma.
Reference: [15] <author> R. R. Koch, F. T. Leighton, B. M. Maggs, S. B. Rao, A. L. Rosenberg, and E. J. Schwabe. </author> <title> Work-preserving emulations of fixed-connection networks. </title> <journal> Journal of the ACM, </journal> <volume> 44(1) </volume> <pages> 104-147, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: But whereas several standard methods are known for hiding latency, e.g., pipelined routing (see, e.g., [7, 8]), redundant computation (see, e.g., <ref> [1, 2, 15, 21, 22, 24] </ref>) or slackness (see, e.g., [30]) the only way to bypass the bandwidth bottleneck is to reduce the communication load by exploiting locality. The principle of locality is already known from sequential computation.
Reference: [16] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-28(9):690-691, </volume> <year> 1979. </year>
Reference-contexts: Then diam (T ) signposts for each object are sufficient. Theorem 2.3 The dynamic caching strategy for trees is 3-competitive. Remark: In order to achieve a sequential consistent strategy according to the definition of Lamport <ref> [16] </ref>, it is necessary to acknowledge the invalidations before updating a copy. Then the strategy becomes 4-competitive. Proof.
Reference: [17] <author> F. T. Leighton, B. M. Maggs, A. G. Ranade, and S. B. Rao. </author> <title> Randomized routing and sorting on fixed-connection networks. </title> <journal> Journal of Algorithms, </journal> <volume> 17 </volume> <pages> 157-205, </pages> <year> 1994. </year>
Reference-contexts: Minimizing the congestion overcomes this drawback, because the congestion measure captures bottlenecks in the system. Therefore, it is a lower bound on the execution time of a given application. Moreover, several results on store-and-forward- and wormhole-routing (see, e.g., <ref> [8, 17, 23, 25, 28] </ref>) indicate that the congestion yields a good estimate for the execution time taken by coarse grained applications with high communication load. Hence, minimizing the congestion seems to be a good heuristic for efficient data management.
Reference: [18] <author> T. Leighton, F. Makedon, S. Plotkin, C. Stein, E. Tardos, and S. Tragoudas. </author> <title> Fast approximation algorithms for multi-commodity flow problems. </title> <journal> Journal of Computer and System Science, </journal> <volume> 50 </volume> <pages> 228-243, </pages> <year> 1995. </year>
Reference-contexts: Applying, S = O (ff) yields the lemma. The maximum flow for cluster K can be calculated efficiently by a randomized approximation scheme in time O (ffi 2 jV K j jE K j log 4 jV K j), see <ref> [18] </ref>. Alternatively, it can be calculated deterministically based on linear programming. Let f i;j : E K ! IR, represent the flow of commodity ` i;j on the respective edges.
Reference: [19] <author> C. Lund, N. Reingold, J. Westbrook, and D. Yan. </author> <title> On-line distributed data management. </title> <booktitle> In Proc. of the 2nd European Symposium on Algorithms (ESA), </booktitle> <year> 1996. </year>
Reference-contexts: Better results are known for dynamic data management on tree-connected networks. Note that minimizing the total load in trees also minimizes the congestion. Here Bartal et al. [6] describe a randomized fi (1)-competitive dynamic strategy for trees, and Lund et al. <ref> [19] </ref> describe a deterministic but centralized strategy with same competitive ratio. (The competitive ratio given in both papers is 3.
Reference: [20] <editor> B. Maggs, F. Meyer auf der Heide, B. Vocking, and M. West-ermann. </editor> <title> Exploiting locality for networks of limited bandwidth. </title> <type> Technical Report tr-rsfb-97-042, </type> <institution> University of Pader-born, </institution> <year> 1997. </year>
Reference-contexts: Since it is not clear how to realize this in an asynchronous setting, we initially restrict ourselves to strategies that update or invalidate all copies in case of a write. In a full version of this paper <ref> [20] </ref>, we consider a more general model, including, e.g., strategies using the majority trick. 1.2 Description of the dynamic model In the dynamic model, there is no knowledge about the access patterns of an application in advance. <p> For simplicity, we assume that each object fits into one routing packet such that each migration of a copy along an edge increases the load of this edge by one. Also request, update, and invalidation messages are assumed to have size one. In a full version of this paper <ref> [20] </ref>, we discuss also more complicated models including non-uniform message sizes and slice-wise accesses to larger objects. We use the competitive ratio as a measure for the efficiency of a dynamic data management strategy. <p> Further, the access tree can be used on these networks for efficient dynamic data management, e.g., for WWW pages. The characteristics of this topology and the results for it are described in more detail in Section 4. In a full version of this paper <ref> [20] </ref>, we show that most of the static results hold even in more general models capturing, e.g., the majority trick. Further, we show that the dynamic access tree strategy can be extended to handle nonuniform object sizes and slice-wise access to large objects. <p> These links are allowed to have arbitrary bandwidth. Note, however, that all algorithms described in this section can also be applied to networks with more complicate bus connections. This is described in a full version of this paper <ref> [20] </ref>. In the following, the network is modeled by a graph T = (V; E). Let diam (T ) denote the diameter of this graph and degree (T ) its maximum node degree. The advantage of trees is that there is only one simple path between any pair of nodes. <p> This yields the following result. Theorem 3.4 The dynamic access tree strategy is O (d log n)-competitive, for meshes of size n and dimension d. Due to space limitations we omit the proof for this theorem. It can be found in a full version of the paper <ref> [20] </ref>. 4 Data management on clustered networks A clustered network G = (V; E) is a network that consists of several small subnetworks, i.e., clusters, that are organized hierarchically. The cluster tree T (G) describes this hierarchical structure.
Reference: [21] <editor> F. Meyer auf der Heide. </editor> <title> Efficiency of universal parallel computers. </title> <journal> Acta Informatica, </journal> <volume> 19 </volume> <pages> 269-296, </pages> <year> 1983. </year>
Reference-contexts: But whereas several standard methods are known for hiding latency, e.g., pipelined routing (see, e.g., [7, 8]), redundant computation (see, e.g., <ref> [1, 2, 15, 21, 22, 24] </ref>) or slackness (see, e.g., [30]) the only way to bypass the bandwidth bottleneck is to reduce the communication load by exploiting locality. The principle of locality is already known from sequential computation.
Reference: [22] <editor> F. Meyer auf der Heide. </editor> <title> Efficient simulations among several models of parallel computers. </title> <journal> SIAM Journal on Computing, </journal> <volume> 15(1) </volume> <pages> 106-119, </pages> <month> Feb. </month> <year> 1986. </year>
Reference-contexts: But whereas several standard methods are known for hiding latency, e.g., pipelined routing (see, e.g., [7, 8]), redundant computation (see, e.g., <ref> [1, 2, 15, 21, 22, 24] </ref>) or slackness (see, e.g., [30]) the only way to bypass the bandwidth bottleneck is to reduce the communication load by exploiting locality. The principle of locality is already known from sequential computation.
Reference: [23] <editor> F. Meyer auf der Heide and B. Vocking. </editor> <title> A packet routing protocol for arbitrary networks. </title> <booktitle> In Proc. of the 12th Symp. on Theoretical Aspects of Computer Science (STACS), </booktitle> <pages> pages 291-302, </pages> <year> 1995. </year>
Reference-contexts: Minimizing the congestion overcomes this drawback, because the congestion measure captures bottlenecks in the system. Therefore, it is a lower bound on the execution time of a given application. Moreover, several results on store-and-forward- and wormhole-routing (see, e.g., <ref> [8, 17, 23, 25, 28] </ref>) indicate that the congestion yields a good estimate for the execution time taken by coarse grained applications with high communication load. Hence, minimizing the congestion seems to be a good heuristic for efficient data management.
Reference: [24] <editor> F. Meyer auf der Heide and R. </editor> <title> Wanka. Time-optimal simulations of networks by universal parallel computers. </title> <booktitle> In Proc. of the 6th Symp. on Theoretical Aspects of Computer Science (STACS), </booktitle> <pages> pages 120-131, </pages> <year> 1989. </year>
Reference-contexts: But whereas several standard methods are known for hiding latency, e.g., pipelined routing (see, e.g., [7, 8]), redundant computation (see, e.g., <ref> [1, 2, 15, 21, 22, 24] </ref>) or slackness (see, e.g., [30]) the only way to bypass the bandwidth bottleneck is to reduce the communication load by exploiting locality. The principle of locality is already known from sequential computation.
Reference: [25] <author> R. Ostrovsky and Y. Rabani. </author> <title> Universal O(congestion + dilation+log 1+* n) local control packet switching algorithms. </title> <booktitle> In Proc. of the 29th ACM Symp. on Theory of Computing (STOC), </booktitle> <pages> pages 644-653, </pages> <year> 1997. </year>
Reference-contexts: Minimizing the congestion overcomes this drawback, because the congestion measure captures bottlenecks in the system. Therefore, it is a lower bound on the execution time of a given application. Moreover, several results on store-and-forward- and wormhole-routing (see, e.g., <ref> [8, 17, 23, 25, 28] </ref>) indicate that the congestion yields a good estimate for the execution time taken by coarse grained applications with high communication load. Hence, minimizing the congestion seems to be a good heuristic for efficient data management.
Reference: [26] <author> C. G. Plaxton and R. Rajaraman. </author> <title> Fast fault-tolerant concurrent access to shared objects. </title> <booktitle> In Proc. of the 37th IEEE Symp. on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 570-579, </pages> <year> 1996. </year>
Reference-contexts: Several recent papers deal with the distribution of pages in the WWW. Plaxton and Rajaraman <ref> [26] </ref> show how to balance the pages among several caches by embedding a random cache tree for each page into the network. This balances the load well and ensures fast responses even for popular pages. Karger et al. [12] use a similar technique to relieve hot spots in the WWW. <p> Karger et al. [12] use a similar technique to relieve hot spots in the WWW. Note that the technique of embedding a random tree for each object is similar to our access tree strategy. The main differences to our approach are the following. The strategy in <ref> [26] </ref> uses a uniform embedding of the tree nodes onto the nodes in the Internet, which completely dissolves topological locality. The strategy in [12] pays attention to topological locality. In fact, they use a model similar to our Internet model.
Reference: [27] <author> A. G. Ranade. </author> <title> How to emulate shared memory. </title> <journal> Journal of Computer and System Science, </journal> <volume> 42 </volume> <pages> 307-326, </pages> <year> 1991. </year>
Reference-contexts: Most theoretical work in the area of distributed data management concerns PRAM simulations. In [13], Kar-lin and Upfal present a probabilistic emulation on an N - node butterfly. Their algorithm emulates one step of an N - processor EREW PRAM in time fi (log N ), w.h.p.. Ranade <ref> [27] </ref> showed how combining could be used to improve this result, i.e., he showed that a CRCW PRAM step can be emulated in the same time. Both strategies use random hash functions to distribute the memory cells uniformly among the processors. This scheme can also be adapted to other networks.
Reference: [28] <author> C. Scheideler and B. Vocking. </author> <title> Universal continuous routing strategies. </title> <booktitle> In Proc. of the 8th ACM Symp. on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 142-151, </pages> <year> 1996. </year>
Reference-contexts: Minimizing the congestion overcomes this drawback, because the congestion measure captures bottlenecks in the system. Therefore, it is a lower bound on the execution time of a given application. Moreover, several results on store-and-forward- and wormhole-routing (see, e.g., <ref> [8, 17, 23, 25, 28] </ref>) indicate that the congestion yields a good estimate for the execution time taken by coarse grained applications with high communication load. Hence, minimizing the congestion seems to be a good heuristic for efficient data management.
Reference: [29] <author> E. Upfal and A. Wigderson. </author> <title> How to share memory in a distributed system. </title> <journal> Journal of the ACM, </journal> <volume> 34 </volume> <pages> 116-127, </pages> <year> 1987. </year>
Reference-contexts: In particular, it does not include strategies that allow only a fraction of the copies to be updated in case of a write, which, e.g., is implemented in strategies using the majority trick introduced in <ref> [29] </ref>. However, all strategies using such techniques add time stamps to the copies. This requires that there is some definition of uniform time among different nodes.
Reference: [30] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33, </volume> <year> 1990. </year>
Reference-contexts: But whereas several standard methods are known for hiding latency, e.g., pipelined routing (see, e.g., [7, 8]), redundant computation (see, e.g., [1, 2, 15, 21, 22, 24]) or slackness (see, e.g., <ref> [30] </ref>) the only way to bypass the bandwidth bottleneck is to reduce the communication load by exploiting locality. The principle of locality is already known from sequential computation. Two kinds of locality are usually distinguished: temporal and spatial locality (see, e.g., [10]).
References-found: 30

