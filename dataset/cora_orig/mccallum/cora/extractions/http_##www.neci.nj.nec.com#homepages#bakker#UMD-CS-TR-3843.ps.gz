URL: http://www.neci.nj.nec.com/homepages/bakker/UMD-CS-TR-3843.ps.gz
Refering-URL: http://www.neci.nj.nec.com/homepages/bakker/
Root-URL: 
Title: Neural Learning of Chaotic Dynamics: The Error Propagation Algorithm trains a neural network to identify
Author: Rembrandt Bakker, Jaap C. Schouten, Cor M. van den Bleek C. Lee Giles 
Address: College Park, MD 20742  4 Independence Way Julianalaan 136, 2628 BL Delft, The Netherlands Princeton, NJ 08540  College Park, MD 20742  
Affiliation: Institute for Advanced Computer Studies, University of Maryland,  Delft University of Technology, NEC Research Institute Department of Chemical Process Technology  Institute for Advanced Computer Studies University of Maryland  
Note: Also with the  An algorithm is introduced that  
Email: r.bakker@stm.tudelft.nl giles@research.nj.nec.com  
Phone: 2  
Date: 1,2  
Web: http://www.neci.nj.com/homepages/giles.html  
Abstract: Technical Report UMIACS-TR-97-77 and CS-TR-3843 Abstract 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.S. Weigend, and N.A. Gershenfeld, </author> <title> Time Series Prediction: Forecasting the Future and Understanding the Past, </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year> <month> 8 </month>
Reference-contexts: A variety of data-driven analysis methods for this type of timeseries was collected in 1991 during the Santa Fe timeserie s competition <ref> [1] </ref>. The methods focused either on characterization or prediction of the timeseries. <p> Our estimate of S is the average of ten evaluations, and we choose a threshold of . 5. Laser data results We applied the training algorithm to the benchmark laser data (set A) from the Santa Fe time series competition <ref> [1] </ref>. The first 3000 points were used for training, the next 3000 for testing. We took an embedding of 40 and used PCA to reduce the 40 delays to 16 principal components. First a linear model was fitted to make onestep-ahead predictions. <p> In all cases, the common stopping criterion stop when error on test set is at its minimum does not yield a model with an acceptable Diks test result. 6 is taken from the Santa Fe timeseries competition <ref> [1] </ref>. The network is trained with 90% error propagation. Experience has shown that the Diks test result of each training session is sensitive for the initial weights of the MLP, the curves are not well reproducible. Trial and error is required. <p> The three largest exponents are, averaged over the ten evaluations and expressed in nats per 40 samples: 0.57, -0.06 and -0.88. The largest exponent is 20% higher than a rough estimate by Hbner et al. in <ref> [1] </ref>. The second exponent must be zero according to Haken [14]. To verify this, we estimate, from the ten evaluations, a standard deviation of the second exponent of 0.04, which is of the same order as its magnitude. 1 conjecture [12] as indicated.
Reference: [2] <author> J.C. Principe, A. Rathie, and J.M. Kuo, </author> <title> Prediction of Chaotic Time Series with Neural Networks and the Issue of Dynami c Modeling, </title> <journal> Int. J. Bifurcation and Chaos, </journal> <volume> Vol. 2, </volume> <year> 1992, </year> <pages> pp. 989-996. </pages>
Reference-contexts: In concise neural network jargon we formulate our goal: train a network to learn the chaotic attractor. A number of authors addressed this issue [2-6]. The common approach is to identify a prediction model, generate a timeseries with it and investigate it s characteristics. Principe et al. <ref> [2] </ref> found that in many cases this approach fails; learning of the attractor is not guaranteed and remains a matter of trial and error. To improve the common approach, Kuo and Principe [3] proposed to minimize multi-step instead of just onestep-ahead prediction errors. <p> This differs from the usual goal of making good short-term predictions. The question arises what happens if we simply minimize short-term prediction errors; will the prediction model approximate the attractor of the real system? Principe et al. <ref> [2] </ref> showed in an experimental study that this is not necessarily the case, and later Kuo & Principe [3] and also Deco and Schrmann [4] proposed, as a remedy, to use trajectory learning, i.e., minimize multi-step instead of onestep-ahead prediction errors.
Reference: [3] <author> J.M. Kuo, and J.C. Principe, </author> <title> Reconstructed Dynamics and Chaotic Signal Modeling, </title> <booktitle> In Proc. IEEE Intl Conf. Neura l Networks, </booktitle> <volume> Vol. 5, </volume> <year> 1994, </year> <pages> pp. 3131-3136. </pages>
Reference-contexts: Principe et al. [2] found that in many cases this approach fails; learning of the attractor is not guaranteed and remains a matter of trial and error. To improve the common approach, Kuo and Principe <ref> [3] </ref> proposed to minimize multi-step instead of just onestep-ahead prediction errors. <p> The question arises what happens if we simply minimize short-term prediction errors; will the prediction model approximate the attractor of the real system? Principe et al. [2] showed in an experimental study that this is not necessarily the case, and later Kuo & Principe <ref> [3] </ref> and also Deco and Schrmann [4] proposed, as a remedy, to use trajectory learning, i.e., minimize multi-step instead of onestep-ahead prediction errors. This approach still minimizes prediction errors (and can still fail) but it leaves more room for optimization as it makes the prediction horizon user-adjustable.
Reference: [4] <author> G. Deco, B. Schrmann, </author> <title> Neural Learning of Chaotic System Behavior, </title> <journal> IEICE Trans. Fundamentals, </journal> <volume> Vol. E77-A, </volume> <year> 1994, </year> <pages> pp. 1840-1845. </pages>
Reference-contexts: The question arises what happens if we simply minimize short-term prediction errors; will the prediction model approximate the attractor of the real system? Principe et al. [2] showed in an experimental study that this is not necessarily the case, and later Kuo & Principe [3] and also Deco and Schrmann <ref> [4] </ref> proposed, as a remedy, to use trajectory learning, i.e., minimize multi-step instead of onestep-ahead prediction errors. This approach still minimizes prediction errors (and can still fail) but it leaves more room for optimization as it makes the prediction horizon user-adjustable.
Reference: [5] <author> R. Rico-Martnez, K. Krischer, I.G. Kevrekidis, M.C. Kube, and J.L. Hudson, </author> <title> Discrete vs. Continuous-Time Nonlinear Signal Processing Of Cu Electrodissolution Data, </title> <journal> Chem. Eng. Comm., </journal> <volume> Vol 118, </volume> <year> 1992, </year> <pages> pp. 25-48. </pages>
Reference: [6] <author> L.A. Aguirre, S.A. Billings, </author> <title> Validating Identified Nonlinear Models with Chaotic Dynamics, </title> <journal> Int. J. Bifurcation and Chaos, </journal> <volume> Vol. 4, </volume> <year> 1994, </year> <pages> pp. 109-125. </pages>
Reference: [7] <author> F. Takens, </author> <title> Detecting strange attractors in turbulence, </title> <booktitle> Lecture notes in Mathematics, </booktitle> <volume> Vol. 898, </volume> <year> 1981, </year> <pages> pp. 365-381. </pages>
Reference-contexts: In this case, we extract the state from the timeseries of the single measured variable y by taking a delay vecto r , where m is the number of delays, called embedding dimension. Takens <ref> [7] </ref> showed that this method of delays will lead to evolutions of the type of Eq. (1) if we choose , where D is the dimension of the systems attractor. The choice of the delay time - and the embedding m is highly empirical.
Reference: [8] <author> D.S. Broomhead, </author> <title> G.P. King, Extracting qualitative dynamics from experimental data, </title> <journal> Physica D, </journal> <volume> Vol. 20, </volume> <year> 1986, </year> <pages> pp. 217-236. </pages>
Reference-contexts: A convenient way to reduce the dimension of a set of linearly correlated variables is Principal Component Analysis (PCA), as introduced in this context by Broomhea d and King <ref> [8] </ref>. PCA transforms the set of delay vectors to a new set of uncorrelated variables of reduced dimension. The transformation matrix U is computed by singular value decomposition (SVD) of the covariance matrix S of the input data.
Reference: [9] <author> J.E. Jackson, </author> <title> A Users Guide to Principal Components, </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Intuitively, one would like the reconstruction of recent measurements to be more accurate than that of older ones. This can be done by using weighted PCA <ref> [9] </ref>. In our experience, the standard weighting profile shown in Fig. 1 works well. The curve is computed where w (d) is the weight for the d delay.
Reference: [10] <editor> R. Bakker, R.J. de Korte, J.C. Schouten, F. Takens , and C.M. van den Bleek, </editor> <title> Neural Networks for Prediction and Control of Chaotic Fluidized Bed Hydrodynamics: A First Step, </title> <booktitle> Fractals, </booktitle> <volume> Vol. 5, No. </volume> <pages> 3, </pages> <note> (to appear). </note>
Reference-contexts: Fortunately, these three operations can be done in a single matrix computation that we call update formula, see Bakker et al. <ref> [10] </ref> for a derivation, with and 4. Training algorithm The objective of our training procedure is to find a set of parameters for the model such that it will approximate the attractor of the real system. This differs from the usual goal of making good short-term predictions. <p> Equivalently, defining th e prediction error , Equation (10) and the illustration in Fig. 2 make clear why we call this algorithm error propagation (EP). For minimization of the error we use conjugate gradients; gradients are computed with backpropagation through time. In a preliminary version of the algorithm <ref> [10] </ref>, the parameter was taken as an additional output of the prediction model, to make it statedependent. The idea was that the training procedure would automatically choose large in regions of the embedding space where the predictability is high, and small where predictability is low.
Reference: [11] <author> C. Diks, W.R. van Zwet, F. Takens, and J. de Goede, </author> <title> Detecting differences between delay vector distributions, </title> <journal> Physical Review E, </journal> <volume> Vol. 53, </volume> <year> 1996, </year> <pages> pp. 2169-2176. </pages>
Reference-contexts: The idea is to make successive predictions dependent on each other by propagating previous predictio n errors. To achieve our goal of attractor learning, we extend, in section 4.2, the training procedure with a test developed by Diks et al. <ref> [11] </ref>, that measures the distance between the model and real system attractor. Training can be stopped after the distance has reached its minimum. 4.1. Error Propagation The common way to train a prediction model is to minimize the squared difference between the predicted and measured future value. <p> Therefore, w e monitor a comparison between the model and measured attractor and stop training when the difference is at a minimum. The test we use was recently developed by Diks et al. <ref> [11] </ref>. It is comparable to visual comparison of attractors plotted in the em bedding-space. The following null hypothesis is tested: the two sets of delay vectors (model generated , measured) are drawn from the same multidimensional probability distribution. <p> Under the null hypothesis, the ratio is a random variable with zero mean and unit variance. For details we refer to <ref> [11] </ref>. Special attention is needed for the choice of the bandwidth parameter that is used by the Gaussian kernels. It determines the length scale at which the two sets are compared.
Reference: [12] <author> J. Kaplan, and J. Yorke, </author> <title> Chaotic Behavior of Multidimensional Difference Equations, </title> <booktitle> Lecture notes in mathematics, </booktitle> <volume> Vol. 730, </volume> <year> 1979. </year>
Reference-contexts: The second exponent must be zero according to Haken [14]. To verify this, we estimate, from the ten evaluations, a standard deviation of the second exponent of 0.04, which is of the same order as its magnitude. 1 conjecture <ref> [12] </ref> as indicated. From the Lyapunov spectrum, we estimate the information dimension D of the system to be 2.6, using the Kaplan-Yorke 1 conjecture [12]: take that (real) value of i where the interpolated curve vs. i crosses zero, see Fig 5. <p> we estimate, from the ten evaluations, a standard deviation of the second exponent of 0.04, which is of the same order as its magnitude. 1 conjecture <ref> [12] </ref> as indicated. From the Lyapunov spectrum, we estimate the information dimension D of the system to be 2.6, using the Kaplan-Yorke 1 conjecture [12]: take that (real) value of i where the interpolated curve vs. i crosses zero, see Fig 5.
Reference: [13] <author> H.F. von Bremen, F.E. Udwadia, W. Proskurowski, </author> <title> An efficient QR Based Method for the Computation of Lyapuno v Exponents, </title> <journal> Physica D, </journal> <volume> Vol. 101, </volume> <year> 1997, </year> <pages> pp. 1-16. </pages>
Reference-contexts: Table 1. NRMSE of 0% and 90% EP networks, run in 0% and 90% EP mode. trained with: run with: 0% EP 90% EP 0% EP 0.07 0.27 M 7 We computed the Lyapunov spectrum of our model using the method by Von Bremen et al. <ref> [13] </ref>, using series o f tangent maps along ten different model generated trajectories of length 6,000. The three largest exponents are, averaged over the ten evaluations and expressed in nats per 40 samples: 0.57, -0.06 and -0.88.
Reference: [14] <author> H. Haken, </author> <title> At Least One Lyapunov Exponent vanishes if the Trajectory of an Attractor does not contain a Fixed Point, </title> <journal> Physics Letters, </journal> <volume> Vol. 94A, </volume> <year> (1983), </year> <pages> pp. 71. </pages>
Reference-contexts: The three largest exponents are, averaged over the ten evaluations and expressed in nats per 40 samples: 0.57, -0.06 and -0.88. The largest exponent is 20% higher than a rough estimate by Hbner et al. in [1]. The second exponent must be zero according to Haken <ref> [14] </ref>. To verify this, we estimate, from the ten evaluations, a standard deviation of the second exponent of 0.04, which is of the same order as its magnitude. 1 conjecture [12] as indicated.
Reference: [15] <author> R. Bakker, J.C. Schouten, F. Takens, and C.M. van den Bleek, </author> <title> Neural network model to control an experimental chaoti c pendulum, </title> <journal> Physical Review E, </journal> <volume> Vol. 4-A, </volume> <year> 1996, </year> <pages> pp. 3545-3552. </pages>
Reference-contexts: The computation of the Lyapunov spectrum is only a first minor application of the model. The model can play a n essential role in chaos control and synchronization, as demonstrated in the control of an experimental chaotic pendulum <ref> [15] </ref>. The selected model for the laser data has 56 sigmoidal nodes and over 1400 weights. In our experience, this kind of large size network helps training to quickly converge to an acceptable local optimum.
Reference: [16] <author> T. Lin, C.L. Giles, B.G. Horne, S.Y. Kung, </author> <title> A Delay Damage Model Selection Algorithm for NARX Neural Networks, </title> <journal> IEEE Trans. </journal> <note> Signal Proc., (in press) </note> . 
Reference-contexts: Therefore, a pruning technique to reduce the size of the model will be the next feature added to the algorithm. Also , pruning will improve the generalization performance of the model <ref> [16] </ref> and may enhance learning of the system attractor. Acknowledgements This work is supported by the Netherlands Foundation for Chemical Research (SON) with financial aid from th e Netherlands Organization for Scientific Research (NWO).
References-found: 16

