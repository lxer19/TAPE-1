URL: ftp://ftp.cis.ufl.edu/cis/tech-reports/tr91/tr91-002.ps
Refering-URL: http://www.cis.ufl.edu/tech-reports/tech-reports/tr91-abstracts.html
Root-URL: http://www.cis.ufl.edu
Title: Non-Blocking Algorithms for Concurrent Data Structures  
Author: Sundeep Prakash Yann-Hang Lee Theodore Johnson 
Date: July 1, 1991  
Address: Gainesville, FL 32611  
Affiliation: Dept. of Computer and Information Sciences University of Florida  
Abstract: Non-blocking algorithms for concurrent data structure guarantee that a data structure is always accessible, in contrast to blocking algorithms in which a slow or halted process can render part or all of the data structure inaccessible to other processes. In this paper, we first develop a method to design non-blocking algorithms for any concurrent data structure, using the compare&swap operation as the basic synchronization primitive. We use the example of queues to demonstrate the method. In this general method, many processes are allowed to concurrently access the data structure, but modifications to the data structure are made in serial order. We then deal with the problem of increasing the achieved concurrency of access (number of modifications that can be simultaneously made) to the data structure. For increasing the achieved concurrency in the general case, we give a method to convert any locking algorithm into a non-blocking algorithm. This transformation is done by having the locks contain the information required to complete the operation. This allows processes blocked by a lock to complete the operation of the process holding the lock. The achieved concurrency of the obtained non-blocking algorithm turns out to be the same as the equivalent locking algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C.S. Ellis and T.J. Olson. </author> <title> Concurrent Dynamic Storage Allocation. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pp. 502-511. </pages>
Reference-contexts: Blocking algorithms are those in which a process 1 trying to read or modify the data structure isolates or locks part or all of the da-ta structure to prevent interference from other processes <ref> [1, 2, 3, 4] </ref>. The problem with the blocking approach is that in an asynchronous system with processes having different speeds, a slower process might prevent faster processes from accessing the data structure.
Reference: [2] <author> S.F. Hummel. </author> <title> SMARTS - Shared-memory Multiprocessor Ada Run Time Supervisor. </title> <type> Technical Report 495, </type> <institution> NYU, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: Blocking algorithms are those in which a process 1 trying to read or modify the data structure isolates or locks part or all of the da-ta structure to prevent interference from other processes <ref> [1, 2, 3, 4] </ref>. The problem with the blocking approach is that in an asynchronous system with processes having different speeds, a slower process might prevent faster processes from accessing the data structure. <p> We shall deal with lists in this paper, although the methods described can be easily applied to any data structure. Lists are interesting because they are required for many operating system functions <ref> [2] </ref> and can be used to make more complex data structures, such as buddy system memory managers [6]. Non-blocking implementations are desirable due to their robustness and continued fast operation in the presence of processes with varying speeds. <p> Non-blocking implementations are desirable due to their robustness and continued fast operation in the presence of processes with varying speeds. Many concurrent implementations are in existence <ref> [2, 6, 7, 8] </ref> but non-blocking implementations are few. We give a brief description of some important ones.
Reference: [3] <author> Y. Mond and Y. Raz. </author> <title> Concurrency Control in B + -Trees Databases using Preparatory Operations. </title> <booktitle> 11th International Conference on Very Large Databases, </booktitle> <pages> pp. 331-334, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: Blocking algorithms are those in which a process 1 trying to read or modify the data structure isolates or locks part or all of the da-ta structure to prevent interference from other processes <ref> [1, 2, 3, 4] </ref>. The problem with the blocking approach is that in an asynchronous system with processes having different speeds, a slower process might prevent faster processes from accessing the data structure.
Reference: [4] <author> P. Tang, P.-C. Yew, C.-Q. Zhu. </author> <title> A Parallel Linked List for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of 13th Annual International Computer Software & Applications Conference, </booktitle> <pages> pp. 130-135, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Blocking algorithms are those in which a process 1 trying to read or modify the data structure isolates or locks part or all of the da-ta structure to prevent interference from other processes <ref> [1, 2, 3, 4] </ref>. The problem with the blocking approach is that in an asynchronous system with processes having different speeds, a slower process might prevent faster processes from accessing the data structure.
Reference: [5] <author> M. Herlihy. </author> <title> A Methodology for Implementing Highly Concurrent Data Structures. </title> <booktitle> In Proceedings of the 2nd ACM SIGPLAN on the Principles and Practice of Parallel Programming, </booktitle> <pages> pp. 197-206, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: Such an algorithm guarantees that some active process will be able to complete an operation in a finite number of steps <ref> [5] </ref>, making the algorithm robust with respect to process failures. 1.1 Review of Current Work Shared data structures are required in a number of multiprocessing applications. We shall deal with lists in this paper, although the methods described can be easily applied to any data structure. <p> We give a brief description of some important ones. Lamport [9] gives a wait-free implementation for FIFO queues but restricts the concurrency to a single enqueuer and a single dequeuer (an algorithm is wait-free if every process can complete an operation in a finite number of steps <ref> [5] </ref>). Gottlieb et al. [7] give a blocking algorithm for enqueuing and dequeuing using the replace-add and swap operations for synchronization. This implementation allows a high degree of parallelism limited only by the predefined maximum queue size. <p> Herlihy and Wing [10] give a non-blocking algorithm using the compare&swap operation. This also permits an arbitrary number of enqueuers and dequeuers but is impractical as it requires an infinite array size for continued operation. Herlihy <ref> [5] </ref> presents a general methodology for automatically transforming sequential code for any data structure into a concurrent non-blocking or wait-free implementation in which the memory requirements for each concurrent process grow in proportion to the total number of concurrent processes. <p> System latency is defined by Herlihy in <ref> [5] </ref> as the maximum number of steps a system can take without completing an operation. <p> elements to be held by a process, the total reserve can be limited by O (n) and can be kept in shared pools (one for the operation information blocks and one for the elements) which can be maintained as non-blocking stacks (as mentioned earlier). 2.5 Comparison with Herlihy's Methodology Herlihy <ref> [5] </ref> gives a general methodology for transforming sequential code for any data structure into a concurrent non-blocking or wait-free implementation (an algorithm is wait-free if every concurrent process can complete an operation in a finite number of steps [5]). Two protocols are presented for this purpose. <p> as non-blocking stacks (as mentioned earlier). 2.5 Comparison with Herlihy's Methodology Herlihy <ref> [5] </ref> gives a general methodology for transforming sequential code for any data structure into a concurrent non-blocking or wait-free implementation (an algorithm is wait-free if every concurrent process can complete an operation in a finite number of steps [5]). Two protocols are presented for this purpose.
Reference: [6] <author> J. Wilson. </author> <title> Operating System Data Structures for Shared-memory (MIMD) Machines with Fetch-and-Add. </title> <type> Ph.D Thesis, </type> <institution> NYU, </institution> <year> 1988. </year>
Reference-contexts: We shall deal with lists in this paper, although the methods described can be easily applied to any data structure. Lists are interesting because they are required for many operating system functions [2] and can be used to make more complex data structures, such as buddy system memory managers <ref> [6] </ref>. Non-blocking implementations are desirable due to their robustness and continued fast operation in the presence of processes with varying speeds. Many concurrent implementations are in existence [2, 6, 7, 8] but non-blocking implementations are few. We give a brief description of some important ones. <p> Non-blocking implementations are desirable due to their robustness and continued fast operation in the presence of processes with varying speeds. Many concurrent implementations are in existence <ref> [2, 6, 7, 8] </ref> but non-blocking implementations are few. We give a brief description of some important ones.
Reference: [7] <author> A. Gottlieb, B.D. Lubachevsky, and L. Rudolph. </author> <title> Basic Techniques for the Efficient Coordination of Very Large Numbers of Cooperating Sequential Processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 164-189, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: Non-blocking implementations are desirable due to their robustness and continued fast operation in the presence of processes with varying speeds. Many concurrent implementations are in existence <ref> [2, 6, 7, 8] </ref> but non-blocking implementations are few. We give a brief description of some important ones. <p> Lamport [9] gives a wait-free implementation for FIFO queues but restricts the concurrency to a single enqueuer and a single dequeuer (an algorithm is wait-free if every process can complete an operation in a finite number of steps [5]). Gottlieb et al. <ref> [7] </ref> give a blocking algorithm for enqueuing and dequeuing using the replace-add and swap operations for synchronization. This implementation allows a high degree of parallelism limited only by the predefined maximum queue size. However, it is possible for an enqueuer or dequeuer to block other dequeuers and enqueuers.
Reference: [8] <author> J. M. Stone. </author> <title> A Simple and Correct Shared-Queue Algorithm Using Compare--and-Swap. </title> <booktitle> In Proceedings of the IEEE Computer Society and ACM SIGARCH Supercomputing `90 Conference, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: Non-blocking implementations are desirable due to their robustness and continued fast operation in the presence of processes with varying speeds. Many concurrent implementations are in existence <ref> [2, 6, 7, 8] </ref> but non-blocking implementations are few. We give a brief description of some important ones. <p> This implementation allows a high degree of parallelism limited only by the predefined maximum queue size. However, it is possible for an enqueuer or dequeuer to block other dequeuers and enqueuers. Stone <ref> [8] </ref> gives a `non-delaying' implementation for the same, using the compare&swap operation for synchronization, which allows an arbitrary number of enqueuers and dequeuers, but it is possible for a faulty or slow enqueuer to block all the dequeuers. Herlihy and Wing [10] give a non-blocking algorithm using the compare&swap operation. <p> It puts the desired new value of C into B1, assigns to B2 the value A2 +1, and then executes the CSDBL instruction. Although the A-B-A problem can still occur, the probability is much lower <ref> [8] </ref> and is acceptable for a large class of applications.
Reference: [9] <author> L. Lamport. </author> <title> Specifying Concurrent Program Modules. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 190-222, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: Non-blocking implementations are desirable due to their robustness and continued fast operation in the presence of processes with varying speeds. Many concurrent implementations are in existence [2, 6, 7, 8] but non-blocking implementations are few. We give a brief description of some important ones. Lamport <ref> [9] </ref> gives a wait-free implementation for FIFO queues but restricts the concurrency to a single enqueuer and a single dequeuer (an algorithm is wait-free if every process can complete an operation in a finite number of steps [5]).
Reference: [10] <author> M. Herlihy and J. Wing. </author> <title> Axioms for Concurrent Objects. </title> <booktitle> In 14th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 13-26, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Stone [8] gives a `non-delaying' implementation for the same, using the compare&swap operation for synchronization, which allows an arbitrary number of enqueuers and dequeuers, but it is possible for a faulty or slow enqueuer to block all the dequeuers. Herlihy and Wing <ref> [10] </ref> give a non-blocking algorithm using the compare&swap operation. This also permits an arbitrary number of enqueuers and dequeuers but is impractical as it requires an infinite array size for continued operation.
Reference: [11] <author> R. Kent Treiber. </author> <title> Systems Programming: Coping with parallelism. </title> <institution> IBM Almaden Research Center, </institution> <address> San Jose, California, RJ 5118, </address> <month> April </month> <year> 1986. </year>
Reference-contexts: Herlihy [5] presents a general methodology for automatically transforming sequential code for any data structure into a concurrent non-blocking or wait-free implementation in which the memory requirements for each concurrent process grow in proportion to the total number of concurrent processes. Treiber <ref> [11] </ref> gives a non-blocking algorithm for concurrent FIFO access to a shared queue. <p> An assumption made here is that the objects in the queue and the blocks storing the operation information are never destroyed, but returned to a shared pool (which can easily be maintained as a non-blocking stack <ref> [11] </ref>). We must make this assumption because some slower processes may read an object even after it is dequeued, or may try to read an operation information block even after it is no longer current. The enqueue procedure is shown in Fig. 2.
Reference: [12] <author> M.P. Herlihy. </author> <title> Impossibility and Universality Results for Wait-Free Synchronization. </title> <booktitle> Seventh ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 276-290, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: As proved by Herlihy <ref> [12] </ref>, it is impossible to design non-blocking or wait-free implementations of many simple data structures using other well known synchronization primitives i.e. read, write, test&set, fetch&add and swap. However, the compare&swap operation in its simple form has a standard difficulty (the A-B-A problem [13]).
Reference: [13] <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, New York, </address> <booktitle> System/370 Principles of Operations, </booktitle> <pages> pp. </pages> <address> 7-13,14, </address> <month> May </month> <year> 1983. </year>
Reference-contexts: As proved by Herlihy [12], it is impossible to design non-blocking or wait-free implementations of many simple data structures using other well known synchronization primitives i.e. read, write, test&set, fetch&add and swap. However, the compare&swap operation in its simple form has a standard difficulty (the A-B-A problem <ref> [13] </ref>). The solution is to use the modified compare&swap operation (also described in [13]), which is what we have done. <p> However, the compare&swap operation in its simple form has a standard difficulty (the A-B-A problem <ref> [13] </ref>). The solution is to use the modified compare&swap operation (also described in [13]), which is what we have done.
Reference: [14] <author> C.-Q Zhu and P.-C. Yew. </author> <title> A Synchronization Scheme and its Applications for Large Multiprocessor Systems. </title> <booktitle> Proceedings of the 4th International Conference on Distributed Computing Systems, </booktitle> <pages> pp. 486-493, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: In the next section we describe the compare&swap operation, the A-B-A problem and its solution. 1.3 The A-B-A Problem We use the implementation of the compare&swap operation found in the IBM/370 architecture (the synchronization primitive proposed for the Cedar supercomputer at the University of Illinois <ref> [14] </ref> has all the capabilities of compare&swap). It is a three operand atomic instruction of the form CS (A,B,C).
Reference: [15] <author> D. Shasha and N. Goodman. </author> <title> Concurrent Search Structure Algorithms. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 13(1) </volume> <pages> 53-90, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: We show the correctness of these algorithms by claiming that the operations done by the concurrent process using these algorithms are decisive operation serializable (concurrent actions are decisive operation serializable <ref> [15] </ref> if they are serializable [15] and there is an operation dec (a) in each of the actions such that if dec (a i ) occurs before dec (a j ) then a i comes before a j in the serial order). <p> We show the correctness of these algorithms by claiming that the operations done by the concurrent process using these algorithms are decisive operation serializable (concurrent actions are decisive operation serializable <ref> [15] </ref> if they are serializable [15] and there is an operation dec (a) in each of the actions such that if dec (a i ) occurs before dec (a j ) then a i comes before a j in the serial order).

References-found: 15

