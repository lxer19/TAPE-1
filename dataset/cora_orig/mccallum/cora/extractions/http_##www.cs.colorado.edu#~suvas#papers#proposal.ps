URL: http://www.cs.colorado.edu/~suvas/papers/proposal.ps
Refering-URL: http://www.cs.colorado.edu/~suvas/Papers.html
Root-URL: http://www.cs.colorado.edu
Title: Dependence Driven Execution for Data Parallelism  
Degree: Thesis Proposal Suvas Vajracharya  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> L.M. Adams. </author> <title> Iterative Methods for Solving Partial Differential Equations of Elliptic Type. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <address> Cambridge, Mass., </address> <year> 1950. </year>
Reference-contexts: This is possible because the "red" points in the iteration space depend only on the "black" points and vice versa. This allows the use of doall loops. Such a parallel implementation of the SOR method is referred to as the Red/Black SOR computation in literature <ref> [1, 2, 53] </ref>. Red/Black SOR is an example of an algorithm that does not suffer from load-imbalance and therefore does well under static scheduling schemes. Red/Black SOR is representative of class of PDE solvers where each element depends only on its neighboring elements.
Reference: [2] <author> L.M. Adams. </author> <title> Iterative Algorithms for Large Sparse Linear Systems on Parallel Computers. </title> <type> PhD thesis, </type> <institution> Univ. of Virginia, Charlottesville, </institution> <year> 1982. </year>
Reference-contexts: This is possible because the "red" points in the iteration space depend only on the "black" points and vice versa. This allows the use of doall loops. Such a parallel implementation of the SOR method is referred to as the Red/Black SOR computation in literature <ref> [1, 2, 53] </ref>. Red/Black SOR is an example of an algorithm that does not suffer from load-imbalance and therefore does well under static scheduling schemes. Red/Black SOR is representative of class of PDE solvers where each element depends only on its neighboring elements. <p> I also assume, for the sake of simplicity, that the matrix dimension, N is a power of two. Data-structures: Two complete quadtree are required. They can be stored in a simple array of quadtrees, QUAD <ref> [2] </ref>. Each node in the quadtree consists of: 1. enable cnt: This enable counter is used to determine when an node is ready to be scheduled. The counter is decremented until it becomes 0, at which point it can be scheduled.
Reference: [3] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Actors continue with their computations based on the message it picks up from its mailbox. The metaphor for mailboxes is appropriate because of asynchrony of the system. The notion of "actors" was first described by Hewitt [17] and further developed by Agha <ref> [3] </ref>. Several implementation exists: Charm++ [45], Threaded-C [40], Concurrent Smalltalk (CST) [22] and ABCL/1 [64]. Because of asynchrony, message driven systems have much potential for parallelism. For example, it is an ideal model for tolerating latency by overlapping computation with communication.
Reference: [4] <author> F. Allen, M. Burke, J. Ferrante, W. Hsieh, and V. Sarkar. </author> <title> A framework for determining useful parallelism. </title> <booktitle> In ACM Int. Conf. Supercomputing, </booktitle> <pages> pages 207-215, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Schedulers are called static if the assignment of work is done at compile time otherwise they are called dynamic. Another distinguishing feature of loop schedulers is whether the scheduler is distributed or centralized. In a parallel program, three types of loops may be distinguished: doser, doall <ref> [4] </ref> and doacross [21]. Doser loops must be executed serially while iterations in a doall loop are completely independent and therefore can run in any order or in parallel. In a doacross loop, values assigned in one iterations are used in another iteration and therefore only partial parallelism is possible.
Reference: [5] <author> J.R. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 19(6) </volume> <pages> 233-246, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: This transformation then allows the application of the methods described in the previous section. A hybrid loop consisting of doser and doall can be transformed such that all doalls are nested within the doser loop by pushing them inside. This method, called the loop interchange <ref> [5, 63] </ref>, is motivated by the fact that we can apply loop coalescing to inner doall loops once the doser has been pushed out. Another transformation, called loop distribution, can convert a multi-way nested doall loop to a one-way nested loop, again allowing loop coalescing.
Reference: [6] <author> T.E. Anderson, E.D. Lazowska, and H.M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> ACM Trans. Comput. Syst., </journal> <volume> 10(1) </volume> <pages> 52-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: If a worker blocks due to a trap to the operating system caused by an I/O, remote memory access or a page fault, a new worker is created in place of the blocked one to overlap other computations with the trap. This is made possible by scheduler activations <ref> [6] </ref>, which allows communications between a user-level thread and the operating systems. Chores also supports applications that contain both task and data parallelism.
Reference: [7] <author> Robert Babb. </author> <title> Parallel processing with large-grain data flow techniques. </title> <journal> IEEE Computer, </journal> <volume> 17(7) </volume> <pages> 55-61, </pages> <month> jul </month> <year> 1984. </year>
Reference-contexts: This improved performance would come from using data level synchronization at the granularity of chunks of iterations instead of entire loops. 3.2.2 Large-Grain Data flow In <ref> [7, 29] </ref>, Babb and DiNucci describe a large-grain data flow, or LGDF, where the nodes in the dataflow graph consist of program blocks corresponding to 5 to 50 (or even more) statements in a higher level programming language. <p> In <ref> [7] </ref>, Babb describes the steps involved in modeling and implementing a Fortran program using LGDF techniques as follows: 1. Draw on paper the data flow graph, perhaps in a hierarchical fashion, i.e., each node may itself be exploded to reveal a more detailed data flow graph. 2. <p> Expand the data flow macros in step 2. 6. Compile and execute. A disadvantage of this approach is the amount of effort required of the programmer. For example, consider the graph shown in Figure 8 for a simple matrix/vector multiplication, an example taken from <ref> [7] </ref>. The central node, node P01 has to be further expanded (not shown) to specify an equally complex graph computing the inner product. 3.3 Data-driven Runtime Systems 3.3.1 Chores The Chores system [23] is implemented on top of Presto [14] and runs on the Sequent Symmetry.
Reference: [8] <author> C.F. Baillie, Dirk Grunwald, and Suvas Vajracharya. </author> <title> Application of an object-oriented runtime system to a grand challenge 3d multi-grid code. </title> <booktitle> In Proc. Twenty-Ninth Annual Hawaii International Conference on System Sciences, </booktitle> <pages> pages 256-260, </pages> <year> 1996. </year>
Reference-contexts: This example serves as kernel for a grand challenge application, Quasi-Geostropic Multigrid Solver or simply QGMG. A detailed explaination of QGMG and its implementation on Dude can be found in <ref> [8] </ref>. Figure 13 shows the performance of two Multigrid solver being solved simultaneously. Each loop independently solves a matrix size of 1024x1024.
Reference: [9] <author> Vasanth Balasundaram. </author> <title> A mechanism for keeping useful internal information in parallel programming tools: The data access descriptor. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 9 </volume> <pages> 154-170, </pages> <year> 1990. </year>
Reference-contexts: To achieve this, it is necessary to describe regions of computations (such as the non-intersecting regions) using data descriptors. In this paper, data descriptors consist of simple rectangles although more sophisticated and exact methods available from compiler literature <ref> [9, 18, 10] </ref> could be used. Dude is an ideal run-time system that can take advantage of such patterns recognized by the compiler.
Reference: [10] <author> Vasanth Balasundaram and Ken Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of teh ACM SIGPLAN Symposium on Compiler Construction, </booktitle> <pages> pages 41-53, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: To achieve this, it is necessary to describe regions of computations (such as the non-intersecting regions) using data descriptors. In this paper, data descriptors consist of simple rectangles although more sophisticated and exact methods available from compiler literature <ref> [9, 18, 10] </ref> could be used. Dude is an ideal run-time system that can take advantage of such patterns recognized by the compiler.
Reference: [11] <author> U. Banerjee. </author> <title> An introduction to a formal theory of dependence analysis. </title> <journal> J. Supercomp., </journal> <volume> 2(2) </volume> <pages> 133-149, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: General loop scheduling is possible if the programmer or the compiler passes application-specific dependencies to the scheduler. Parallelizing compilers can make this available in the form of dependence vectors <ref> [63, 11] </ref>. By passing this information to the run-time layer, a dependence-driven run-time system can exploit the application-specific parallelism yet remain a general system. The compiler describes the parallelism declaratively in the form of dependence vectors. All the procedural details of synchronization and scheduling are handled by the run-time system. <p> Each vector in the set represents an arc from the source of the to the target of the dependency. How these vectors are computed is beyond the scope of this thesis but a good description can be found in [63] <ref> [11] </ref>. space. A compiler can compute this vector by taking the difference between the iteration vectors of the source and target iterations. 4.1.4 Data Parallel Operation Data parallel operation and dependence satisfaction is handled by the runtime system.
Reference: [12] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the 17th International Conference on Computer Architecture, </booktitle> <pages> pages 168-176, </pages> <year> 1990. </year>
Reference-contexts: Using an entry-consistency DSM without the help of the run-time system would require the user to be responsible for identifying the data objects, explicitly providing synchronization objects, and for maximum performance, specifying whether the object is read-only or read-write access. * In Munin <ref> [12] </ref>, Bennett et al. proposed using a type-specific memory coherence. These types included write-once objects, private objects, write-many objects, result objects, synchronization objects, migratory objects and producer-consumer objects.
Reference: [13] <author> M.J. Berger and P. Colella. </author> <title> Local adaptive mesh refinement for shock hydrodynamics. </title> <journal> Journal of Computational Physics, </journal> <volume> 82 </volume> <pages> 64-84, </pages> <year> 1989. </year>
Reference-contexts: AMR techniques allow computational resource such as a CPU and memory to be used only when needed, i.e. for those areas of problem grid where the error is unacceptable. Examples include the shock waves in computation fluid dynamics <ref> [13] </ref> where regions containing shock waves need more computational efforts that other areas of the computational space. Figure 1 shows another example taken from [48]. The black circles in the figure represent points of interest with high error, such as atomic nuclei in materials design applications.
Reference: [14] <author> B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> PRESTO: A System for Object-Oriented Parallel Programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year> <month> 38 </month>
Reference-contexts: The central node, node P01 has to be further expanded (not shown) to specify an equally complex graph computing the inner product. 3.3 Data-driven Runtime Systems 3.3.1 Chores The Chores system [23] is implemented on top of Presto <ref> [14] </ref> and runs on the Sequent Symmetry. A per-processor worker (a user-level thread) grab chunks of work from a central queue using the 13 guided self-scheduling method.
Reference: [15] <author> Brian N. Bershad, Mathew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The midway distribured shared memory system. </title> <booktitle> In Proceedings 38th IEEE Computer Society Society International Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: This is called lazy release consistency [46]. 30 In both release and lazy release consistencies, the entire memory is made consistent at each synchronization point. A weaker model would be to make only the necessary subset of memory consistent. There are two approaches to this basic idea. In Midway <ref> [15] </ref>, an entry consistency is introduced. In this model, the user associates an user-defined data object with a synchronization object. This association allows the system to update only the data object associated with the synchronization object, upon its acquisition. <p> This is possible because the run-time system often knows which processors are producers and which processors are consumers of the data described by descriptors. In Red/Black SOR, for example, the coherence operations can be limited to only the neighboring processors. * Several works, e.g, <ref> [15] </ref> have suggested the use of multiple policies to support a given program. However, little work has been done to automate when and which of these policies are appropriately used. This research proposes to have the run-time system direct the use of a given policy.
Reference: [16] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Furthermore, Dude is more appropriate for NUMA multiprocessors and DSM multicomputers since the run-time system reduces communication by using affinity information and by directing the DSM layer. Chores was designed with the assumption that memory accesses are uniform such as in the Sequent Symmetry. 3.3.2 Cilk Cilk <ref> [16] </ref> is a C-based data-driven runtime system using a work-stealing scheduler. A pre-processor translates Cilk program to a C program. As in data-driven models, threads are ready when all the arguments that it requires are available. If the arguments are not available, then the thread is waiting.
Reference: [17] <author> P. Bishop C. Hewitt and R. Steiger. </author> <title> A universal actor formalism for artificial intelligence. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 235-245. </pages> <publisher> SIAM, </publisher> <year> 1973. </year>
Reference-contexts: In the second part, actors pick up the message from remote actor. Actors continue with their computations based on the message it picks up from its mailbox. The metaphor for mailboxes is appropriate because of asynchrony of the system. The notion of "actors" was first described by Hewitt <ref> [17] </ref> and further developed by Agha [3]. Several implementation exists: Charm++ [45], Threaded-C [40], Concurrent Smalltalk (CST) [22] and ABCL/1 [64]. Because of asynchrony, message driven systems have much potential for parallelism. For example, it is an ideal model for tolerating latency by overlapping computation with communication.
Reference: [18] <author> David Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> April </month> <year> 1987. </year>
Reference-contexts: To achieve this, it is necessary to describe regions of computations (such as the non-intersecting regions) using data descriptors. In this paper, data descriptors consist of simple rectangles although more sophisticated and exact methods available from compiler literature <ref> [9, 18, 10] </ref> could be used. Dude is an ideal run-time system that can take advantage of such patterns recognized by the compiler.
Reference: [19] <author> David Cann. </author> <title> Retire fortran?: a debate rekindled. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 81-89, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Sisal has been ported to various machines including Cray C90, Y-MP, or X-MP, Cray 2 running UNICOS, and SGI multi-processor running IRIX. In <ref> [19] </ref>, Cann reports favorable performance comparisons of Fortran and Sisal on the Cray Y-MP/864 computer. The benchmarks included the Livermore Loops (24 scientific kernels), the SIMPLE hydrodynamics program, and a weather-prediction program. One of the reasons for improved performance is the lack of inter-procedural analysis on the Fortran compiler.
Reference: [20] <author> D.E. Culler, A. Sah, K.E. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: a compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: But even with the use of the Dagger language, the message model seems unnatural for shared memory MIMDs since users need to explicitly send messages. For example, in stencil applications, user are responsible for sending and receiving boundaries of blocks even on shared memory multiprocessors. 3.4.2 Tam Tam <ref> [20] </ref> is a complier-controlled threaded abstract machine. It evolved from graph-based execution for dataflow languages and provides a bridge between dataflow models and the control flow models typically employed by standard multiprocessors.
Reference: [21] <author> R. Cytron. </author> <title> Doacross: Beyond vectorization for multiprocessors. </title> <booktitle> In Int. Conf. Parallel Procesing, </booktitle> <pages> pages 836-834, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Schedulers are called static if the assignment of work is done at compile time otherwise they are called dynamic. Another distinguishing feature of loop schedulers is whether the scheduler is distributed or centralized. In a parallel program, three types of loops may be distinguished: doser, doall [4] and doacross <ref> [21] </ref>. Doser loops must be executed serially while iterations in a doall loop are completely independent and therefore can run in any order or in parallel. In a doacross loop, values assigned in one iterations are used in another iteration and therefore only partial parallelism is possible.
Reference: [22] <author> W. Dally and A. Chien. </author> <title> Object oriented concurrent programming in CST. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Computers, </booktitle> <pages> pages 434-439, </pages> <year> 1988. </year> <note> SIAM. </note>
Reference-contexts: The metaphor for mailboxes is appropriate because of asynchrony of the system. The notion of "actors" was first described by Hewitt [17] and further developed by Agha [3]. Several implementation exists: Charm++ [45], Threaded-C [40], Concurrent Smalltalk (CST) <ref> [22] </ref> and ABCL/1 [64]. Because of asynchrony, message driven systems have much potential for parallelism. For example, it is an ideal model for tolerating latency by overlapping computation with communication.
Reference: [23] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support fo shared memory parallel computing. </title> <journal> ACM. Trans on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The central node, node P01 has to be further expanded (not shown) to specify an equally complex graph computing the inner product. 3.3 Data-driven Runtime Systems 3.3.1 Chores The Chores system <ref> [23] </ref> is implemented on top of Presto [14] and runs on the Sequent Symmetry. A per-processor worker (a user-level thread) grab chunks of work from a central queue using the 13 guided self-scheduling method.
Reference: [24] <author> Dawson R. Engler, Gregory R. Andrews, and David K. Lowenthal. </author> <title> Efficient support for fine-grain parallelism. </title> <type> TR 93-13, </type> <institution> Univ. Arizona, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: One of the consequence of using a shared-memory model together with the use of high-level application-specific dependence 18 information is that run-time system can direct communication at a higher-level than would be possible. 3.5.2 Filaments The Filaments work <ref> [24] </ref> makes the case that extremely fine grain threads are not as expensive as previously thought.
Reference: [25] <author> Zhixi Fang, Peiyi Tang, Pen chung Yew, and Chuan qi Zhu. </author> <title> Dynamic processor self-scheduling for general parallel nested loops. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 919-929, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: In contrast, this thesis proposes the use of tree-like recursive data structures (presented later) to represent multidimensional iteration space. The iterations spaces are manipulated during run-time. This allows the proposed run-time system to schedule multi-dimensional loops with dependencies across iterations and across dimensions. Runtime Methods: Fang et. al. <ref> [25] </ref> described a two-level scheduling scheme for general arbitrarily nested parallel loops. Each loop doser, doall or doacross is termed a task. An extremely large-grain data-flow graph, where each node is an entire loop, is created.
Reference: [26] <author> John T. Feo and David C. Cann. </author> <title> A report on the sisal language project. </title> <journal> Jounal of Parallel and Distributed Computing, </journal> <volume> 10(4) </volume> <pages> 81-89, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Another is that the Sisal compiler is able to avoid to problem with arrays in static single assignment languages by using an optimization called copy elimination. Although Sisal is based on a dataflow model, the runtime system, as described, in <ref> [26] </ref> does not aggressively take advantage of the independencies in the loop iterations. Data-level synchronization occur at very high granularity since an entire loop would be a node in the dependence graph.
Reference: [27] <author> D.D Gajski, D.A. Padua, D. J. Kuck, and R.H. Kuhn. </author> <title> A second opinion on data flow machines and languages. </title> <journal> Computer, </journal> <volume> 15(6) </volume> <pages> 58-69, </pages> <month> February </month> <year> 1982. </year>
Reference-contexts: The research focuses on the scheduling of loops in multiprocessors, i.e., it focuses on data-parallelism. The motivation is that most of the parallelism in scientific and engineering applications come from loops <ref> [27] </ref>. There are a number of existing run-time systems for parallel computers. <p> Since dataflow model requires a single static assignment policy, data cannot be overwritten. Writing to a single element in a large array is difficult to make efficient since simply copying the entire array would be prohibitively expensive. A comprehensive critique of the dataflow approach can be found in <ref> [27] </ref>. Of the two principles of data flow model, only asynchrony is relevant to the proposed work.
Reference: [28] <author> D. Gannon and J.K. Lee. </author> <title> Object oriented parallelism. </title> <booktitle> In Proceedings of 1991 Japan Society for Parallel Processing, </booktitle> <pages> pages 13-23, </pages> <year> 1991. </year>
Reference-contexts: In the absence of hints from the user, the runtime will decompose the data using a default method. The decomposition methods are similar to the data decomposition and distribution utilities available in HPF [41] and pC++ <ref> [28] </ref>. One important difference, however, is that in the process of data decomposition, Dude takes flat data and creates objects called Iterates which is a tuple consisting of both data and operation. Iterates defines the granularity of parallelism in Dude.
Reference: [29] <editor> Dennis B. Gannon and editors Robert J. Douglass. </editor> <booktitle> The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 335-349. </pages> <publisher> Scientific Computation Series. MIT Press, </publisher> <year> 1987. </year> <month> 39 </month>
Reference-contexts: This improved performance would come from using data level synchronization at the granularity of chunks of iterations instead of entire loops. 3.2.2 Large-Grain Data flow In <ref> [7, 29] </ref>, Babb and DiNucci describe a large-grain data flow, or LGDF, where the nodes in the dataflow graph consist of program blocks corresponding to 5 to 50 (or even more) statements in a higher level programming language.
Reference: [30] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Performance evaluations of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <year> 1991. </year>
Reference-contexts: This can be weakened (and thereby consistency traffic can be reduced) by taking advantage of the fact that parallel programs only expect consistencies at the synchronization points. This is the idea behind release consistency <ref> [30] </ref> which uses a delayed update queue to buffer the writes to a memory location until the program reaches a synchronization points. More specifically, remote memory is invalidated or updated at the release of a synchronization object.
Reference: [31] <author> Rick Gillette. </author> <title> Memory channel an optimized cluster interconnect. In Hot Interconnects '95, </title> <month> August </month> <year> 1995. </year> <note> (to appear). </note>
Reference-contexts: Currently there are two boxes, each with 4 nodes connected by memory channels <ref> [31] </ref>.
Reference: [32] <author> Susan Graham, Steven Lucco, and Oliver Sharp. </author> <title> Orchestrating interactions among parallel computations. </title> <booktitle> In Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> April </month> <year> 1993. </year> <note> ACM, ACM. </note>
Reference-contexts: The current size of the batch is exactly half of the previous batch: R 0 = N; C i = dR i1 =2P e; R i = R i1 P C i Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling [61], tapering <ref> [32, 54] </ref>, and safe self-scheduling [52]. A distributed version of tapering has also been proposed by Graham and Lucco. Hybrid Methods: There are two problems with the centralized dynamic schedulers described above. First, a centralized data structure incurs heavy synchronization costs and makes the algorithm un-scalable.
Reference: [33] <author> A. S. Grimshaw. </author> <title> Easy to use object-oriented parallel programming with mentat. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Cilk provides low level primitives such as spawning threads and sending arguments to waiting threads. Programmers must use these primitives to express parallelism. High level abstractions for data-parallelism such as scheduling loops and declarative dependence specification are absent in Cilk. 3.3.3 Mentat Mentat <ref> [34, 33] </ref> is an adaptation of the dataflow model for MIMD architectures and imperative languages such as C++. This adaptation involves three important modifications to original dataflow model. First, granularity needs to be larger to amortize the overhead.
Reference: [34] <author> A. S. Grimshaw, W. T. Strayer, and P. Narayan. </author> <title> Dynamic object-oriented parallel processing. </title> <booktitle> IEEE Parallel and Distributed Technology: Systems and Applications, </booktitle> <pages> pages 33-47, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Cilk provides low level primitives such as spawning threads and sending arguments to waiting threads. Programmers must use these primitives to express parallelism. High level abstractions for data-parallelism such as scheduling loops and declarative dependence specification are absent in Cilk. 3.3.3 Mentat Mentat <ref> [34, 33] </ref> is an adaptation of the dataflow model for MIMD architectures and imperative languages such as C++. This adaptation involves three important modifications to original dataflow model. First, granularity needs to be larger to amortize the overhead.
Reference: [35] <author> Dirk Grunwald. </author> <title> A users guide to awesime: An object oriented parallel programming and simulation system. </title> <type> Technical Report CU-CS-552-91, </type> <institution> University of Colorado, Boulder, </institution> <year> 1991. </year>
Reference-contexts: In Dude synchronizations are handled by the system; user need only specify the dependencies. Thirdly, in Dude, the system does the scheduling and in the case of dynamic applications, the system also chooses the appropriate granularity. 3.5.3 Awesime Awesime <ref> [35] </ref> (A Widely Extensible Simulation Environment), is an object-oriented thread package for shared-address parallel architectures. The Awesime library currently runs on workstations using Digital Alpha AXP, SPARC, Intel `x86', MIPS R3000 and Motorola 68K processors, the SGI Power Challenge as well as the KSR-1 massively parallel processor. <p> Dude is based on Awesime. Awesime provides efficient mechanism for task parallelism. Dude provides data parallelism. 4 Current Status of Proposed Work The proposed work, the Dude runtime system, is built on the top of an existing thread library, Awesime <ref> [35] </ref>, a thread library implemented in C++. Together they provide a powerful way to 19 take advantage of both task and data parallelism in applications. In this paper, we concentrate on Dude's data parallelism since this is the most novel aspect of the runtime system.
Reference: [36] <author> Dirk Grunwald and Suvas Vajracharya. </author> <title> Efficient barriers for distributed shared memory computers. </title> <booktitle> In Intl. Parallel Processing Symposium. IEEE, IEEE Computer Society, </booktitle> <month> April </month> <year> 1994. </year> <note> (to appear). </note>
Reference-contexts: The improved performance is largely due to a more efficient barrier <ref> [36] </ref> which takes the hierarchical interconnect topology of the KSR-1 into account. 5.1.3 Awesime Threads-Dynamic Block Scheduling This is the chunking method described in section 3.1.1.
Reference: [37] <author> J. Gurd, C.C. Kirkham, and A.P.W. Boehm. </author> <title> The manchester prototype dataflow computer. </title> <journal> Communication of the ACM, </journal> <volume> 28 </volume> <pages> 34-52, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Both architectures and languages using this model has been built. Dataflow architectures include the classical Manchester Dataflow Machine <ref> [37, 38] </ref>. Dataflow languages include Sisal, VAL, Id,and LAU. A description of Sisal is included below. 3.2.1 Sisal Sisal, a descendant of VAL, is an applicative language that has been specially designed for efficient execution of scientific data-parallel computations.
Reference: [38] <author> J. Gurd, C.C. Kirkham, and A.P.W. Boehm. </author> <title> The Manchester Dataflow Computing System, </title> <publisher> pages 516,517,519,520,529. North-Holland, </publisher> <year> 1987. </year>
Reference-contexts: Both architectures and languages using this model has been built. Dataflow architectures include the classical Manchester Dataflow Machine <ref> [37, 38] </ref>. Dataflow languages include Sisal, VAL, Id,and LAU. A description of Sisal is included below. 3.2.1 Sisal Sisal, a descendant of VAL, is an applicative language that has been specially designed for efficient execution of scientific data-parallel computations.
Reference: [39] <author> A. Gursoy and L. V. Kale. Dagger: </author> <title> Cominging the benefits of synchronous and asynchronous communication styles. </title> <type> Technical Report 9303, </type> <institution> Parallel Programming Laboratory, Department of Computer Science, University of Illinois, </institution> <year> 1993. </year>
Reference-contexts: Charm++ supports abstractions for sharing informations other than simply messages. Specific abstract shared objects such as "read-only objects", "write-once" objects and "accumulator objects" and "monotonic" (or idempotent) objects are also provided. To simplify the complexity of sending asynchronous messages and preserving their ordering, a coordination language called Dagger <ref> [39] </ref> for specifying synchronization constraints has been implemented. But even with the use of the Dagger language, the message model seems unnatural for shared memory MIMDs since users need to explicitly send messages.
Reference: [40] <author> Y. Zhou Halbherr and C.F. Joerg. </author> <title> Mimd-style parallel programming based on continuation passing threads. </title> <type> Technical Report CSG Memo 355, </type> <institution> MIT Laboratory for computer science, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Actors continue with their computations based on the message it picks up from its mailbox. The metaphor for mailboxes is appropriate because of asynchrony of the system. The notion of "actors" was first described by Hewitt [17] and further developed by Agha [3]. Several implementation exists: Charm++ [45], Threaded-C <ref> [40] </ref>, Concurrent Smalltalk (CST) [22] and ABCL/1 [64]. Because of asynchrony, message driven systems have much potential for parallelism. For example, it is an ideal model for tolerating latency by overlapping computation with communication.
Reference: [41] <author> High Performance Fortran Forum HPFF. </author> <title> Draft high performance fortran specificition, </title> <note> version 0.4. In Proceedings of 1991 Japan Society for Parallel Processing, page Available from anonymous ftp site titan.cs.rice.edu, </note> <year> 1992. </year>
Reference-contexts: A programmer or compiler chooses the decomposition method such as by BLOCK or CYCLIC. In the absence of hints from the user, the runtime will decompose the data using a default method. The decomposition methods are similar to the data decomposition and distribution utilities available in HPF <ref> [41] </ref> and pC++ [28]. One important difference, however, is that in the process of data decomposition, Dude takes flat data and creates objects called Iterates which is a tuple consisting of both data and operation. Iterates defines the granularity of parallelism in Dude.
Reference: [42] <author> S. F. Hummel, Edith Schonberg, and L. E. Flynn. </author> <title> Factoring, a method for scheduling parallel loops. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The size of the ith chunk, C i , is selected using the following rule: R 0 = N; C i = d p where R i is the number of remaining iterations. A variation on this method is factoring <ref> [42] </ref> where at each scheduling operation the scheduler computes the size of a batch of chunks with the motivation to reduce the number of scheduling operations.
Reference: [43] <author> S.F. Hummel and E. Schonberg. </author> <title> Low-overhead schedulign of nested parallelism. </title> <journal> IBM Journal of Research and Development, </journal> <pages> pages 743-765, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Consequently, the contribution of this paper is task parallelism, not data parallelism. In contrast, the proposed work schedules multiple loops simultaneously, even in the presence of dependencies. Hummel and Schonberg <ref> [43] </ref> presents another attempt on scheduling nested parallelism. This work addresses the problem of nested doall loops with private variables in the inner loops. Private variable make it difficult to simply coalesce the loops. The solution was to create a tree of activation stacks.
Reference: [44] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> Crl: High-performance all-software distributed shared memory. </title> <booktitle> In SIGOPS, </booktitle> <pages> pages 213-228, </pages> <month> December </month> <year> 1995. </year> <month> 40 </month>
Reference-contexts: This association allows the system to update only the data object associated with the synchronization object, upon its acquisition. A problem with this approach is that providing the synchronization objects explicitly for every data object and using these synchronization objects, can be burdensome to the programmer. In CRL <ref> [44] </ref>, the synchronization objects are associated with regions of memory and these synchronization objects are implicit in the rgn start op and rgn end op which bracket the memory operations.
Reference: [45] <author> Laxmikant Kale and Sanjeev Krishnan. Charm++: </author> <title> A portable concurrent object-oriented system based on c++. </title> <type> Technical report, </type> <institution> Univ. of Illinois, Urbana-Champaign, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: Actors continue with their computations based on the message it picks up from its mailbox. The metaphor for mailboxes is appropriate because of asynchrony of the system. The notion of "actors" was first described by Hewitt [17] and further developed by Agha [3]. Several implementation exists: Charm++ <ref> [45] </ref>, Threaded-C [40], Concurrent Smalltalk (CST) [22] and ABCL/1 [64]. Because of asynchrony, message driven systems have much potential for parallelism. For example, it is an ideal model for tolerating latency by overlapping computation with communication.
Reference: [46] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th International Conference on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <year> 1992. </year>
Reference-contexts: More specifically, remote memory is invalidated or updated at the release of a synchronization object. A variation of this is to pull the data instead of pushing by making local memory consistent upon acquiring a synchronization object. This is called lazy release consistency <ref> [46] </ref>. 30 In both release and lazy release consistencies, the entire memory is made consistent at each synchronization point. A weaker model would be to make only the necessary subset of memory consistent. There are two approaches to this basic idea. In Midway [15], an entry consistency is introduced.
Reference: [47] <institution> Kendall Square Research, </institution> <address> Boston, MA. </address> <booktitle> The KSR-1 System Architecture Manual, </booktitle> <month> April </month> <year> 1992. </year>
Reference-contexts: when the amount of work in a single loop is insufficient to fully utilize the available hardware resources such as CPUs. 5 Preliminary Results from Current System In this section I describe the performance of the Red/Black SOR and the Multigrid solver on the KSR-1, a parallel cache-only memory architecture <ref> [47] </ref>. Preliminary results indicate that Dude performs well on shared multiprocessors such as the KSR-1, SGI and a simulated distributed shared memory system. Speedup curves for the Red/Black SOR and a Multigrid application show that Dude performs the best of all the methods compared.
Reference: [48] <author> Scott R. Kohn. </author> <title> A Parallel Software Infrastructure for Dynamic Block-Irregular Scientific Calculations. </title> <type> PhD thesis, </type> <institution> UCSD CSE Dept., </institution> <month> June </month> <year> 1995. </year> <note> Tech. Report CS95-429. </note>
Reference-contexts: Examples include the shock waves in computation fluid dynamics [13] where regions containing shock waves need more computational efforts that other areas of the computational space. Figure 1 shows another example taken from <ref> [48] </ref>. The black circles in the figure represent points of interest with high error, such as atomic nuclei in materials design applications. The grids around the black circle are further refined to reduce the error in those areas. <p> A technique for creating regions of computations on the fly is necessary for AMR's. This implies that computation over regions as expressed by 5 loops need to be first class. The thesis by Kohn and the resulting run-time system, LPARX <ref> [48, 49] </ref> was specially designed for attacking such problems. 2. Mean Value Analysis and Pre-Conditioned Conjugate Gradient: Both of these applications require the traversal of a grid from one corner to another. This method of traversing the matrix is known as the wavefront computation. <p> Tam provides very low-level primitives and is not intended for users-level programming directly. Furthermore Tam is intended to support functional languages. 3.5 Control-Driven Thread Packages 3.5.1 LPARX LPARX <ref> [48, 49] </ref> is a library designed specifically for dynamic irregular scientific calculations. It consists of three parts: numerical operations, grid management facilities, and display routines. The numerical operations define the elliptic partial differential equations to be solved. The display library provides data visualization utilities.
Reference: [49] <author> S.R. Kohn and S.B. Baden. </author> <title> A parallel software infrastructure for structured adaptive mesh methods. </title> <booktitle> In Supercomputing '95, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: A technique for creating regions of computations on the fly is necessary for AMR's. This implies that computation over regions as expressed by 5 loops need to be first class. The thesis by Kohn and the resulting run-time system, LPARX <ref> [48, 49] </ref> was specially designed for attacking such problems. 2. Mean Value Analysis and Pre-Conditioned Conjugate Gradient: Both of these applications require the traversal of a grid from one corner to another. This method of traversing the matrix is known as the wavefront computation. <p> Tam provides very low-level primitives and is not intended for users-level programming directly. Furthermore Tam is intended to support functional languages. 3.5 Control-Driven Thread Packages 3.5.1 LPARX LPARX <ref> [48, 49] </ref> is a library designed specifically for dynamic irregular scientific calculations. It consists of three parts: numerical operations, grid management facilities, and display routines. The numerical operations define the elliptic partial differential equations to be solved. The display library provides data visualization utilities.
Reference: [50] <author> C. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 11 </volume> <pages> 1001-10016, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: There are several dynamic scheduling methods. The simplest method is self-scheduling [60] where each processor grabs one iteration from the central data structure and executes that iteration. To alleviate the high cost of N synchronizations for N iterations in this approach, fixed-size chunking <ref> [50] </ref> was proposed, where each process grabs chunks of K iterations instead of one iteration. If the processors do not start simultaneously or if one processor gets an expensive chunk, the potential for load imbalance still exists.
Reference: [51] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <year> 1979. </year>
Reference-contexts: Research efforts have succeeded to some extent by relaxing the requirement that memory be sequential consistency. According to Lamport <ref> [51] </ref>, this means that the "result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each processor appear in this sequence in the order specified by its program".
Reference: [52] <author> Liu and et al. </author> <title> Scheduling parallel loops with variable length iteration execution times on parallel computers. </title> <booktitle> In Proc. 5th Intl. Conf. Parallel and Distributed Computing and System, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: The current size of the batch is exactly half of the previous batch: R 0 = N; C i = dR i1 =2P e; R i = R i1 P C i Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling [61], tapering [32, 54], and safe self-scheduling <ref> [52] </ref>. A distributed version of tapering has also been proposed by Graham and Lucco. Hybrid Methods: There are two problems with the centralized dynamic schedulers described above. First, a centralized data structure incurs heavy synchronization costs and makes the algorithm un-scalable.
Reference: [53] <author> L.M. Adams and H.F. Jordan. </author> <title> Is SOR Color Blind? SIAM Sci. </title> <journal> Stat. Computation, </journal> <volume> 7(2) </volume> <pages> 490-506, </pages> <year> 1986. </year>
Reference-contexts: This is possible because the "red" points in the iteration space depend only on the "black" points and vice versa. This allows the use of doall loops. Such a parallel implementation of the SOR method is referred to as the Red/Black SOR computation in literature <ref> [1, 2, 53] </ref>. Red/Black SOR is an example of an algorithm that does not suffer from load-imbalance and therefore does well under static scheduling schemes. Red/Black SOR is representative of class of PDE solvers where each element depends only on its neighboring elements.
Reference: [54] <author> Steven Lucco. </author> <title> A dynamic scheduling method for irregular parallel programs. </title> <booktitle> In Proceedings of ACM SIGPLAN '92 Conference on Porgramming Language Design and Implementation, </booktitle> <pages> pages 200-211. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: The current size of the batch is exactly half of the previous batch: R 0 = N; C i = dR i1 =2P e; R i = R i1 P C i Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling [61], tapering <ref> [32, 54] </ref>, and safe self-scheduling [52]. A distributed version of tapering has also been proposed by Graham and Lucco. Hybrid Methods: There are two problems with the centralized dynamic schedulers described above. First, a centralized data structure incurs heavy synchronization costs and makes the algorithm un-scalable.
Reference: [55] <author> E.P Markatos and T. J. LeBlanc. </author> <title> Load Balancing vs Locality Management in Shared Memory Multiprocessors. </title> <booktitle> In Intl. Conference on Parallel Processing, </booktitle> <pages> pages 258-257, </pages> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: As a result, poor data locality incurs much communication. A compromise between purely static scheduling, which has the potential for poor load balance on irregular problems, and dynamic scheduling, is affinity loop scheduling <ref> [55] </ref>, which takes affinity of data to processors into account. This scheme is most like fixed-sized chunking except that chunks have an affinity to a particular processor which is remembered for subsequent scheduling to promote data locality.
Reference: [56] <author> Srinivasan Parthasarathy Mohammed Javeed Zaki, Wei Li. </author> <title> Customized dynamic load balancing for network of workstations. </title> <type> Technical Report 602, </type> <institution> Univ. of Rochester, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: When iterations in these local queues are exhausted, work may be stolen from remote queues to balance the load. Alternatively, adaptive methods can be used to determine whether to use static or dynamic and centralized or distributed schedulers. In Customized Load Balancing for Network of Workstations <ref> [56] </ref>, Zaki et al. describe a hybrid compile/run-time system that determines the appropriate scheduler by monitoring processor performance during run-time. 3.1.2 Multiply-Nested Hybrid Loop Scheduling The previous methods discussed loop scheduling for a simple single doall loop. In real applications however, the structure of loops can be quite complicated.
Reference: [57] <author> C. D. Polochronopoulous and D. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12) </volume> <pages> 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: If the processors do not start simultaneously or if one processor gets an expensive chunk, the potential for load imbalance still exists. In guided-self scheduling <ref> [57] </ref>, a variable chunk size as determined during runtime is used. The size of the ith chunk, C i , is selected using the following rule: R 0 = N; C i = d p where R i is the number of remaining iterations.
Reference: [58] <author> C. D. Polychronopoulos. </author> <title> Loop coalescing: A compiler transformation for parallel machines. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1987. </year>
Reference-contexts: Static Methods: A special case of multiply-nested loops are the one-way (or perfectly) nested doall loops in which there exists exactly one loop at each nest level as shown in Figure 5. Given a one-way nested doall loop, a compile time transformation called loop coalescing <ref> [58] </ref> can be used to coalesce m doall loops into a single doall with N = Q m i=1 (N i ) iterations. This transformation then allows the application of the methods described in the previous section.
Reference: [59] <author> Hanan Samet. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference: [60] <author> P. Tang and P.C. Yew. </author> <title> Processor self-scheduling for multiple nested parallel loops. </title> <booktitle> In Proc. Int. Conf. on Parallel Processing, </booktitle> <pages> pages 528-535. </pages> <publisher> IEEE, </publisher> <month> August </month> <year> 1986. </year> <month> 41 </month>
Reference-contexts: The (P 2 ) contention for the data structure can be reduced to (P log P ) by using a combining tree. There are several dynamic scheduling methods. The simplest method is self-scheduling <ref> [60] </ref> where each processor grabs one iteration from the central data structure and executes that iteration. To alleviate the high cost of N synchronizations for N iterations in this approach, fixed-size chunking [50] was proposed, where each process grabs chunks of K iterations instead of one iteration.
Reference: [61] <author> T.H. Tzen and L.M. Ni. </author> <title> Trapezoid self-scheduling: A practical scheduling scheme for parallel computers. </title> <journal> IEEE Transactions Parrallel Distributed Systems, </journal> <volume> 4 </volume> <pages> 87-98, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The current size of the batch is exactly half of the previous batch: R 0 = N; C i = dR i1 =2P e; R i = R i1 P C i Other methods in this model include adaptive guided self-scheduling, trapezoidal self-scheduling <ref> [61] </ref>, tapering [32, 54], and safe self-scheduling [52]. A distributed version of tapering has also been proposed by Graham and Lucco. Hybrid Methods: There are two problems with the centralized dynamic schedulers described above. First, a centralized data structure incurs heavy synchronization costs and makes the algorithm un-scalable.
Reference: [62] <author> Chien-Min Wang and Sheng-De Wang. </author> <title> A hybrid scheme for efficiently executing loops on multiprocessors. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 625-637, </pages> <year> 1992. </year>
Reference-contexts: The solution was to create a tree of activation stacks. This is easily and cheaply implemented in Dude by using C++ objects to create activations for the inner loops. Hybrid Methods: A optimized compile/run-time version of loop interchange and loop distribution called IGSS and MGSS was proposed in <ref> [62] </ref>. These schemes also allowed the scheduling of hybrid loops consisting of doser and doalls. This method is useful in cases where the number of iterations of a doall loop is much larger than the number of processors involved.
Reference: [63] <author> M.J. Wolfe. </author> <title> Optimizing supercompilers for supercomputers. </title> <type> PhD thesis, </type> <institution> Univ. Illinois, Urbana, </institution> <month> April </month> <year> 1987. </year> <type> Rep. 329. </type>
Reference-contexts: General loop scheduling is possible if the programmer or the compiler passes application-specific dependencies to the scheduler. Parallelizing compilers can make this available in the form of dependence vectors <ref> [63, 11] </ref>. By passing this information to the run-time layer, a dependence-driven run-time system can exploit the application-specific parallelism yet remain a general system. The compiler describes the parallelism declaratively in the form of dependence vectors. All the procedural details of synchronization and scheduling are handled by the run-time system. <p> This transformation then allows the application of the methods described in the previous section. A hybrid loop consisting of doser and doall can be transformed such that all doalls are nested within the doser loop by pushing them inside. This method, called the loop interchange <ref> [5, 63] </ref>, is motivated by the fact that we can apply loop coalescing to inner doall loops once the doser has been pushed out. Another transformation, called loop distribution, can convert a multi-way nested doall loop to a one-way nested loop, again allowing loop coalescing. <p> Each vector in the set represents an arc from the source of the to the target of the dependency. How these vectors are computed is beyond the scope of this thesis but a good description can be found in <ref> [63] </ref> [11]. space. A compiler can compute this vector by taking the difference between the iteration vectors of the source and target iterations. 4.1.4 Data Parallel Operation Data parallel operation and dependence satisfaction is handled by the runtime system.
Reference: [64] <author> A. Yonezawa. </author> <title> ABCL: An Object Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> 42 </month>
Reference-contexts: The metaphor for mailboxes is appropriate because of asynchrony of the system. The notion of "actors" was first described by Hewitt [17] and further developed by Agha [3]. Several implementation exists: Charm++ [45], Threaded-C [40], Concurrent Smalltalk (CST) [22] and ABCL/1 <ref> [64] </ref>. Because of asynchrony, message driven systems have much potential for parallelism. For example, it is an ideal model for tolerating latency by overlapping computation with communication.
References-found: 64

