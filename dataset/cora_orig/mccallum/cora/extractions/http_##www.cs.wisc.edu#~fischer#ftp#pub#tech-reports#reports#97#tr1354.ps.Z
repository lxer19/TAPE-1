URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/reports/97/tr1354.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/reports/97/
Root-URL: http://www.cs.wisc.edu
Title: IMAGE-BASED TRANSFORMATION OF VIEWPOINT AND SCENE APPEARANCE  
Author: By Steven Maxwell Seitz 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy (Computer Sciences) at the  
Date: 1997  
Address: WISCONSIN MADISON  
Affiliation: UNIVERSITY OF  
Abstract-found: 0
Intro-found: 1
Reference: [AB91] <author> Edward H. Adelson and J. R. Bergen. </author> <title> The Plenoptic Function and the Elements of Early Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: A key feature of these types of editing operations is that they apply to the space of all views of the scene, rather than just one image. It is therefore convenient to cast them in terms of the plenoptic function <ref> [AB91, MB95b] </ref>, which encodes scene appearance from all possible viewpoints. Within this framework, our goal is to recover a scene's plenoptic function from a discrete set of images and to determine how it should be modified in response to basic image editing operations like painting, scissoring, and morphing.
Reference: [AL91] <author> Nicholas Ayache and Francis Lustman. </author> <title> Trinocular stereo vision for robotics. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(1) </volume> <pages> 73-85, </pages> <year> 1991. </year>
Reference-contexts: Recently, a number of researchers have devised techniques to improve the performance of binocular stereo in the presence of occlusion, e.g., using Bayesian techniques [BM92, GLY92] and masks [NMSO96] to detect half-occluded regions. Trinocular <ref> [SD88, AL91] </ref> and multiple-baseline stereo methods [OK85b] enable larger cumulative baselines and have been shown to improve matching accuracy. Still, individual views must generally be close together. Another approach for integrating disparate views is to arrange cameras in clusters and to compute stereo reconstructions from each cluster [KRN97].
Reference: [App95] <institution> Apple Computer, Inc. </institution> <note> QuickTime VR, version 1.0, </note> <year> 1995. </year>
Reference-contexts: McMillan and Bishop's Plenoptic Modeling approach also supported interpolation of mosaics to allow camera translation using an elegant image-space algorithm to resolve visibility order. Apple Computer's commercialization of Chen's system, QuickTime VR <ref> [App95] </ref>, has brought image-based scene visualization to the mainstream, enabling interactive viewing on conventional PC's of real scenes like the Great Wall of China or the interior of a new car.
Reference: [AS95] <author> Serge Ayer and Harpreet S. Sawhney. </author> <title> Layered representation of motion video using robust maximum-likelihood estimation of mixture models and mdl encoding. </title> <booktitle> In Proc. Fifth Int. Conf. on Computer Vision, </booktitle> <pages> pages 777-784, </pages> <year> 1995. </year>
Reference-contexts: Recently, researchers in computer vision and computer graphics have advocated this layering paradigm as an effective way for representing and rendering more complicated 3D scenes by manually [TK96, LS97] or automatically <ref> [WA94, DP91, AS95] </ref> segmenting the scene into a series of layers. In the last few years, computer-based image metamorphosis or morphing [Wol90, BN92] has emerged as a popular means for producing fascinating visual effects.
Reference: [AS97] <author> Shai Avidan and Amnon Shashua. </author> <title> Novel view synthesis in tensor space. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 1034-1040, </pages> <year> 1997. </year>
Reference-contexts: The problems of acquiring the representation and synthesizing views are both achieved via simple sampling operations. While elegant and relatively simple to construct, these approaches require many more input views compared to correspondence-based techniques. Most closely-related to the approach in this chapter are the so called view interpolation methods <ref> [CW93, LF94, MB95b, WHH95, BP96, Sch96, AS97] </ref>, which use image warping to produce new views from a small number of basis views. Most of these methods require advance knowledge of camera parameters to produce correct perspective views, with the exceptions of [LF94, AS97]. <p> Most of these methods require advance knowledge of camera parameters to produce correct perspective views, with the exceptions of <ref> [LF94, AS97] </ref>. Furthermore, all of these methods require 18 dense pixel correspondence maps as input. This latter requirement is a serious limitation, given that image-based computation of correct correspondence maps is known to be an ill-posed problem [PTK85, VP89]. <p> For Lambertian scenes, this problem reduces to that of computing the optical flow fields C si : I s ) I i and using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow <ref> [AS97] </ref> p stereo [OLC93, KRN97, Sch96] p feature tracking [TK92, BTZ96] p contour tracking [CB92, SF95] p volume intersection [MKKJ96, MTG97] p p EPI analysis [BBM87, KTOT95] p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. <p> Rather, we seek to capture their relative strengths and weaknesses and identify the conditions in which each approach is most applicable. For comparison, we also list voxel coloring, the technique presented in this chapter. 61 6.1.1 Optical Flow Avidan and Shashua <ref> [AS97] </ref> used optical flow correlation in conjunction with a trilinear tensor technique to generate new views from uncalibrated basis images. They achieved good results by correlating closely spaced frontal images of human faces.
Reference: [BA83] <author> P. J. Burt and E. H. Adelson. </author> <title> The laplacian pyramid as a compact image code. </title> <journal> IEEE Trans. Communications, </journal> <volume> 31 </volume> <pages> 532-540, </pages> <year> 1983. </year>
Reference-contexts: We note that a number of techniques have been developed that are less susceptible to the aforementioned problems. For instance, mixture-methods and parameterized models have been proposed [WA94, DP95, BA96] to avoid smoothing over flow discontinuities. Other researchers have used coarse-to-fine schemes <ref> [BA83] </ref> to enable the recovery of flow fields with larger image displacements. 6.1.2 Stereo Because of its relatively high accuracy, stereo is by far the most popular correspondence technique for view synthesis practitioners [OLC93, KRN97, WHH95, SD95b, MB95b, Sch96, DTM96].
Reference: [BA96] <author> Michael J. Black and P. Anandan. </author> <title> Estimating optical flow in segmented images using variable-order parametric models with local deformations. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 18(10) </volume> <pages> 972-986, </pages> <year> 1996. </year>
Reference-contexts: Furthermore, most optical flow techniques use gradient methods, window correlation, or velocity tuned filters, all of which require small image displacements. We note that a number of techniques have been developed that are less susceptible to the aforementioned problems. For instance, mixture-methods and parameterized models have been proposed <ref> [WA94, DP95, BA96] </ref> to avoid smoothing over flow discontinuities.
Reference: [Bas92] <author> Ronen Basri. </author> <title> On the uniqueness of correspondence under orthographic and perspective projections. </title> <booktitle> In Proc. Image Understanding Workshop, </booktitle> <pages> pages 875-884, </pages> <year> 1992. </year>
Reference-contexts: These approaches are evaluated with respect to their use for producing pixel correspondence maps under various conditions. A check indicates that a technique is robust with respect to a particular condition, i.e., the presence of that condition imposes virtually no performance penalty. As noted in <ref> [PTK85, Bas92] </ref> however, optical flow computation is an inherently ill-posed problem and cannot be solved without additional assumptions on scene structure and camera placement. In Section 3.5 we used the assumption of monotonicity to enable a solution sufficient for view synthesis.
Reference: [BB89] <author> H. Harlyn Baker and Robert C. Bolles. </author> <title> Generalizing epipolar-plane image analysis on the spatiotemporal surface. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 3(1) </volume> <pages> 33-49, </pages> <year> 1989. </year>
Reference-contexts: A constraint of the method is that the basis views must all lie along a line and be closely-spaced. Epipolar plane analysis is a class of techniques introduced by Bolles, Baker, and 64 Marimont <ref> [BBM87, BB89] </ref>, to track and reconstruct features in closely-spaced views of a rigid scene. Stacking sequential images one on top of another forms an XY T cube, called an epipolar volume.
Reference: [BBM87] <author> Robert C. Bolles, H. Harlyn Baker, and David H. Marimont. </author> <title> Epipolar-plane image analysis: An approach to determining structure from motion. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 1(1) </volume> <pages> 7-55, </pages> <year> 1987. </year>
Reference-contexts: using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo [OLC93, KRN97, Sch96] p feature tracking [TK92, BTZ96] p contour tracking [CB92, SF95] p volume intersection [MKKJ96, MTG97] p p EPI analysis <ref> [BBM87, KTOT95] </ref> p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. These approaches are evaluated with respect to their use for producing pixel correspondence maps under various conditions. <p> A constraint of the method is that the basis views must all lie along a line and be closely-spaced. Epipolar plane analysis is a class of techniques introduced by Bolles, Baker, and 64 Marimont <ref> [BBM87, BB89] </ref>, to track and reconstruct features in closely-spaced views of a rigid scene. Stacking sequential images one on top of another forms an XY T cube, called an epipolar volume.
Reference: [BCZ93] <author> Andrew Blake, Rupert Curwen, and Andrew Zisserman. </author> <title> A framework for spatiotemporal control in the tracking of visual contours. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 11(2) </volume> <pages> 127-145, </pages> <year> 1993. </year> <month> 110 </month>
Reference-contexts: Solving the last problem requires combinatorial algorithms [Cox93] to keep 63 track of all possible associations over a long image sequence. The search can be simplified by using heuristic pruning techniques [Cox93] or by imposing the assumption of a rigid scene <ref> [BCZ93, SD95a] </ref>. 6.1.4 Contour Tracking and Volume Intersection Moezzi et al. [MKKJ96, MTG97] applied a volume intersection technique to derive texture-mapped models for the purpose of view synthesis, using background subtraction to obtain silhouettes.
Reference: [BFB94] <author> J. L. Barron, D. J. Fleet, and S. S. Beauchemin. </author> <title> Performance of optical flow techniques. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 12(1) </volume> <pages> 43-77, </pages> <year> 1994. </year>
Reference-contexts: Faces present an ideal match for optical flow techniques due to the existence of individual views from which all points on the face are visible. A variety of techniques exist for computing optical flow fields C ij (see <ref> [BFB94] </ref> for a survey and comparison). While it has been shown that optical flow estimates can be directly measured only at brightness discontinuities [Mar82, PTK85], smoothness conditions can be imposed to derive dense flow fields.
Reference: [BK96] <author> Peter N. Belhumeur and David J. Kriegman. </author> <title> What is the set of images of an object under all possible lighting conditions. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 270-277, </pages> <year> 1996. </year>
Reference-contexts: More recently, there has been increasing attention in modeling reflectance and illumination via analysis of basis images <ref> [War92, BK96, DNvGK97, SWI97, SK98] </ref>. However, the problems of image-based editing and animation have yet to be studied. This chapter addresses the editing problem and describes several 3D editing operations that can be performed on photographs.
Reference: [Bla97] <author> Black Diamond, Inc. </author> <note> Surround Video, version 1.0, </note> <year> 1997. </year>
Reference-contexts: The success of QuickTime VR and newer systems like Surround Video <ref> [Bla97] </ref>, IPIX [Int97], SmoothMove [Inf97], and RealVR [Liv97] has helped spawn an emerging subfield of computer graphics, often called image-based rendering, and has attracted growing interest in image-based representations. Debevec et al. [DTM96] exploited a mixture of manual and automatic methods to reconstruct high-quality 3D models of architectural scenes.
Reference: [BM92] <author> Peter N. Belhumeur and David Mumford. </author> <title> A Bayesian treatment of the stereo correspondence problem using half-occluded regions. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 506-512, </pages> <year> 1992. </year>
Reference-contexts: Recently, a number of researchers have devised techniques to improve the performance of binocular stereo in the presence of occlusion, e.g., using Bayesian techniques <ref> [BM92, GLY92] </ref> and masks [NMSO96] to detect half-occluded regions. Trinocular [SD88, AL91] and multiple-baseline stereo methods [OK85b] enable larger cumulative baselines and have been shown to improve matching accuracy. Still, individual views must generally be close together.
Reference: [BN92] <author> Thaddeus Beier and Shawn Neely. </author> <title> Feature-based image metamorphosis. </title> <booktitle> In Proc. SIGGRAPH 92, </booktitle> <pages> pages 35-42, </pages> <year> 1992. </year>
Reference-contexts: In the last few years, computer-based image metamorphosis or morphing <ref> [Wol90, BN92] </ref> has emerged as a popular means for producing fascinating visual effects. These techniques combine 2D interpolations of shape and color to produce transitions between a pair of basis images. <p> Two maps are required because the correspondence may not be one-to-one. In practice, C 01 and C 10 are partially specified by having the user provide a sparse set of matching features or regions in the two images. The remaining correspondences are determined automatically by interpolation <ref> [Wol90, BN92, LCSW92] </ref>. <p> Existing morphing methods vary principally in how the correspondence maps are computed. In addition, some techniques allow finer control over interpolation rates and methods. For instance, Beier and Neely <ref> [BN92] </ref> suggested two different methods of interpolating line features, using linear interpolation of endpoints, per Eqs. (3.2) and (3.3), or of position and angle. <p> The dashed line shows the linear path of one feature during the course of the transformation. This example is indicative of the types of distortions that can arise with image morphing techniques. <ref> [Wol90, BN92, LCSW92] </ref>. To illustrate the potentially severe 3D distortions incurred by image morphing, it is useful to consider interpolating between two different views of a planar shape. <p> For instance, it is possible to interpolate between views of different objects like human faces that have approximately the same shape. In the interactive approach, off-the-shelf image morphing programs can be used to compute the image correspondence and morph of the prewarped image. In one popular image morphing technique <ref> [BN92] </ref> a user interactively 44 specifies a set of matching line segments in the two images. From these lines, the morph program interpolates a dense pixel correspondence. The endpoints of these same line segments are used to automatically prewarp the images, using the method described in Appendix A. <p> Resampling effects can be reduced by supersampling the input images [Wol90] or by composing the image transformations into one aggregate warp for each image. The latter approach is especially compatible with image morphing techniques that employ inverse mapping, such as the Beier and Neely method <ref> [BN92] </ref>, since the inverse postwarp, morph, and prewarp can be directly concatenated into a single inverse map. <p> These features were used to automatically prewarp the images to achieve parallelism using the method described in Appendix A. Inspection of the prewarped images confirms that corresponding features, e.g., in Fig. 3.9, do in fact occupy the same scanlines. An implementation of the Beier and Neely field-morphing algorithm <ref> [BN92] </ref> was used to compute the intermediate images, based on the same set of features used to prewarp the images. The resulting images were postwarped by selecting a quadrilateral region delimited by four control 48 Bottom: morph between views of two different people. <p> The image morph was computed by linearly interpolating corresponding pixels in the two original images. The change in orientation between the original images caused the in-between images to contract. In addition, the bending effects seen in Fig. 3.5 are also present. Image morphing techniques such as <ref> [BN92] </ref> that preserve lines can reduce bending effects, but only when line features are present. An interesting side-effect is that a large hole appears in the image morph, between the stick and propeller, but not in the view morph, since the eye-level is constant throughout the transition. <p> View morphs may also produce holes, but only as a result of a change in visibility. Fig. 3.13 illustrates distortions that can occur in facial image morphs. In this example, Beier and Neely's method <ref> [BN92] </ref> was used to compute an image morph between an image of Leonardo da Vinci's Mona Lisa and its mirror reflection. <p> While the motion of rays is determined, the motion of voxels along rays is not. Our implementation of plenoptic image morphing fixed this variable by constraining voxels to move parallel to the image plane. We use an implementation of the Beier and Neely method <ref> [BN92] </ref> to generate image warps. The voxel warp is computed by inverse-mapping each voxel in the warped scene to determine the corresponding voxel (if any) in the unwarped scene.
Reference: [BP93] <author> Roberto Brunelli and Tomaso Poggio. </author> <title> Face recognition: Features versus templates. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(10) </volume> <pages> 1042-1052, </pages> <year> 1993. </year>
Reference-contexts: The stereo correspondence techniques used in our implementation were much more likely to generate erroneous correspondences at these features than were human operators, and consequently tended to produce poorer results. Possible improvements could be obtained by using specialized feature detectors <ref> [BP93, PMS94, Per96] </ref> to facilitate automatic correspondence of key features like the eyes, nose, mouth.
Reference: [BP96] <author> David Beymer and Tomaso Poggio. </author> <title> Image representations for visual learning. </title> <journal> Science, </journal> (272):1905-1909, 1996. 
Reference-contexts: The problem is represented schematically in Fig. 3.1. This problem is interesting because (1) it has applications of practical importance, such as stereo viewing [MB95a] and teleconferencing <ref> [BP96] </ref>, (2) it is amenable to a thorough bottom-up analysis, and (3) it provides a base case for the more general problem of view synthesis from arbitrary sets of views. <p> Poggio and Brunelli [PB92] proposed a computational learning framework for synthesizing 2D and 3D transformations from a set of basis images. Their technique, which uses hyper basis functions to learn parameterized models from images, is very general and can model a variety of interesting transformations. Later work <ref> [BP96, VP96] </ref> demonstrated the utility of this framework for the difficult case of synthesizing new views of human faces. <p> The problems of acquiring the representation and synthesizing views are both achieved via simple sampling operations. While elegant and relatively simple to construct, these approaches require many more input views compared to correspondence-based techniques. Most closely-related to the approach in this chapter are the so called view interpolation methods <ref> [CW93, LF94, MB95b, WHH95, BP96, Sch96, AS97] </ref>, which use image warping to produce new views from a small number of basis views. Most of these methods require advance knowledge of camera parameters to produce correct perspective views, with the exceptions of [LF94, AS97]. <p> This approach is in essence the reduction used in all previous approaches to view synthesis, for instance <ref> [CW93, LF94, MB95b, BP96] </ref>. While complete optical flow is sufficient to synthesize new views, its automatic recovery from basis images is problematic and error-prone. In particular, the flow within low-contrast regions of nearly uniform color is locally ambiguous, a condition known as the aperture problem [Mar82]. <p> One way of doing this is to normalize the images, using the procedure in Fig. 3.13, to convert both observed and reference images to front-facing views prior to recognition. An alternative approach that has recently been explored by several researchers <ref> [BP96, PMS94, MN90] </ref> is to estimate both pose and identity by acquiring images of each reference face from several different viewpoints.
Reference: [BTZ96] <author> Paul Beardsley, Phil Torr, and Andrew Zisserman. </author> <title> 3D model acquisition from extended image sequences. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <pages> pages 683-695, </pages> <year> 1996. </year>
Reference-contexts: Indeed, a great deal of recent work <ref> [TK92, BTZ96, Sze96, MKKJ96, KRN97] </ref> has been devoted to the problem of obtaining high-quality texture-mapped 3D models from two or more basis images, and significant progress has been made in this area. <p> that of computing the optical flow fields C si : I s ) I i and using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo [OLC93, KRN97, Sch96] p feature tracking <ref> [TK92, BTZ96] </ref> p contour tracking [CB92, SF95] p volume intersection [MKKJ96, MTG97] p p EPI analysis [BBM87, KTOT95] p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. <p> Integration of noisier stereo-derived models is significantly more difficult, if fine detail is to be preserved. 6.1.3 Feature Tracking Several researchers have applied feature tracking techniques in association with shape-from-motion methods to recover high quality texture-mapped 3D models <ref> [TK92, BTZ96] </ref>, suitable for view synthesis. However, because feature tracking does not yield dense correspondence fields, accuracy is ensured only in places where features have been detected and reliably tracked.
Reference: [CB92] <author> R. Cipolla and A. Blake. </author> <title> Surface shape from the deformation of apparent contours. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 9(2) </volume> <pages> 83-112, </pages> <year> 1992. </year>
Reference-contexts: flow fields C si : I s ) I i and using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo [OLC93, KRN97, Sch96] p feature tracking [TK92, BTZ96] p contour tracking <ref> [CB92, SF95] </ref> p volume intersection [MKKJ96, MTG97] p p EPI analysis [BBM87, KTOT95] p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. These approaches are evaluated with respect to their use for producing pixel correspondence maps under various conditions. <p> These approaches can be characterized by whether or not they use temporal derivatives of the contour. While temporal derivatives provide useful shape and correspondence information <ref> [CB92, SF95] </ref>, computation of temporal derivatives requires closely space views. Alternatively, surface shape can be inferred directly from the silhouette using volume intersection [MA91, Sze93] and other direct approaches [KD95a, Kut97].
Reference: [Che95] <author> Shenchang Eric Chen. </author> <title> Quicktime VR | An image-based approach to virtual environment navigation. </title> <booktitle> In Proc. SIGGRAPH 95, </booktitle> <pages> pages 29-38, </pages> <year> 1995. </year>
Reference-contexts: For brevity, we discuss only a few here that are representative of the dominant approaches that currently exist. Comparisons with approaches in this thesis will be discussed in later sections and chapters. 17 Chen <ref> [Che95] </ref>, and McMillan and Bishop [MB95b] developed systems similar to Lippman's Movie-Maps but using computer-based rather than optical methods. Both of these systems provided panoramic visualization of scenes using one or more cylindrical image mosaics, each of which stored scene appearance from a single camera position. <p> Image reprojection has been used previously in a number of applications [Wol90]. Our use of reprojection is most closely related to the techniques used for rectifying stereo views to simplify 3D shape reconstruction [Fau93]. Image mosaic techniques <ref> [Gre86, MB95b, Che95, Sze96, SS97, KAI + 95, MP97] </ref> also rely heavily on reprojection methods to project images onto a planar, cylindrical, or spherical manifold.
Reference: [CL96] <author> Brian Curless and Marc Levoy. </author> <title> A volumetric method for building complex models from range images. </title> <booktitle> In Proc. SIGGRAPH 96, </booktitle> <pages> pages 303-312, </pages> <year> 1996. </year>
Reference-contexts: Still, individual views must generally be close together. Another approach for integrating disparate views is to arrange cameras in clusters and to compute stereo reconstructions from each cluster [KRN97]. The resulting partial models can be subsequently merged <ref> [TL94, CL96] </ref> to form a more complete reconstruction. This split and merge approach has proven most successful when the partial reconstructions to be merged are individually quite accurate, e.g., as obtainable by high-end laser range scanners.
Reference: [Col96] <author> Robert T. Collins. </author> <title> A space-sweep approach to true multi-image matching. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 358-363, </pages> <year> 1996. </year>
Reference-contexts: I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo [OLC93, KRN97, Sch96] p feature tracking [TK92, BTZ96] p contour tracking [CB92, SF95] p volume intersection [MKKJ96, MTG97] p p EPI analysis [BBM87, KTOT95] p p Space-Sweep <ref> [Col96] </ref> p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. These approaches are evaluated with respect to their use for producing pixel correspondence maps under various conditions. <p> Votes are accumulated when two or more subspaces intersect, indicating the possible presence of a point or line feature in the scene. A restriction of this technique is that it detects correspondences only for features that appear in all images. Collins <ref> [Col96, Col97] </ref> proposed a Space-Sweep approach in which features are matched by sweeping a plane through the scene and accumulating votes for points on the plane that project to features in the images. <p> This reverse-mapping strategy is similar to that of Collins <ref> [Col96] </ref>, who used a diffusion operator in place of the texture-mapping formulation. 80 6.5 Experimental Results In order to evaluate the performance of the voxel coloring algorithm for view synthesis, it was applied to images of a variety of real scenes.
Reference: [Col97] <author> Robert T. Collins. </author> <title> Multi-image focus of attention for rapid site model construction. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 575-581, </pages> <year> 1997. </year>
Reference-contexts: Votes are accumulated when two or more subspaces intersect, indicating the possible presence of a point or line feature in the scene. A restriction of this technique is that it detects correspondences only for features that appear in all images. Collins <ref> [Col96, Col97] </ref> proposed a Space-Sweep approach in which features are matched by sweeping a plane through the scene and accumulating votes for points on the plane that project to features in the images.
Reference: [Cox93] <author> Ingemar J. Cox. </author> <title> A review of statistical data association techniques for motion correspondence. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 10(1) </volume> <pages> 53-66, </pages> <year> 1993. </year> <month> 111 </month>
Reference-contexts: Problems occur, however, when features disappear (e.g., due to occlusion or detection failure), when the image displacement is large (e.g., due to large view separation), and when two similar features appear close together in an image. Solving the last problem requires combinatorial algorithms <ref> [Cox93] </ref> to keep 63 track of all possible associations over a long image sequence. The search can be simplified by using heuristic pruning techniques [Cox93] or by imposing the assumption of a rigid scene [BCZ93, SD95a]. 6.1.4 Contour Tracking and Volume Intersection Moezzi et al. [MKKJ96, MTG97] applied a volume intersection <p> Solving the last problem requires combinatorial algorithms <ref> [Cox93] </ref> to keep 63 track of all possible associations over a long image sequence. The search can be simplified by using heuristic pruning techniques [Cox93] or by imposing the assumption of a rigid scene [BCZ93, SD95a]. 6.1.4 Contour Tracking and Volume Intersection Moezzi et al. [MKKJ96, MTG97] applied a volume intersection technique to derive texture-mapped models for the purpose of view synthesis, using background subtraction to obtain silhouettes.
Reference: [CW93] <author> Shenchang Eric Chen and Lance Williams. </author> <title> View interpolation for image synthesis. </title> <booktitle> In Proc. SIGGRAPH 93, </booktitle> <pages> pages 279-288, </pages> <year> 1993. </year>
Reference-contexts: These images were optically corrected at playback time using a viewing apparatus employing a conical mirror. By moving one's head relative to this device, the user could effect a rotation of the virtual camera about its optical center. In their seminal paper on view synthesis, Chen and Williams <ref> [CW93] </ref>, described how new views of a scene could be quickly computed from a set of basis images and range-maps via image warping in software on desktop computers. <p> The problems of acquiring the representation and synthesizing views are both achieved via simple sampling operations. While elegant and relatively simple to construct, these approaches require many more input views compared to correspondence-based techniques. Most closely-related to the approach in this chapter are the so called view interpolation methods <ref> [CW93, LF94, MB95b, WHH95, BP96, Sch96, AS97] </ref>, which use image warping to produce new views from a small number of basis views. Most of these methods require advance knowledge of camera parameters to produce correct perspective views, with the exceptions of [LF94, AS97]. <p> This approach is in essence the reduction used in all previous approaches to view synthesis, for instance <ref> [CW93, LF94, MB95b, BP96] </ref>. While complete optical flow is sufficient to synthesize new views, its automatic recovery from basis images is problematic and error-prone. In particular, the flow within low-contrast regions of nearly uniform color is locally ambiguous, a condition known as the aperture problem [Mar82]. <p> Alternatively, we could produce the same two images by moving the camera instead of the object, as shown in Fig. 3.6. Chen and Williams <ref> [CW93] </ref> previously considered this special case, arguing that linear image interpolation should produce new perspective views when the camera moves parallel to the image plane. <p> The opposite case, of an occluded surface suddenly becoming visible, gives rise to a hole; a region of I s having no correspondence in I 0 . Folds can be resolved using Z-buffer techniques <ref> [CW93] </ref>, provided depth information is available. In the absence of 3D shape information, we use point disparity instead. The disparity of corresponding points p 0 and p 1 in two parallel views is defined to be the difference of their x-coordinates 2 . <p> An alternative method using a Painter's method instead of Z-buffering is presented in [MB95b]. Unlike folds, holes cannot always be eliminated using image information alone. Chen and Williams <ref> [CW93] </ref> suggested different methods for filling holes, using a designated background color, interpolation with neighboring pixels, or additional images for better surface coverage. <p> This scene can either be reprojected to synthesize new views as in [SD97b], or used to compute correspondence maps for image-warping methods such as <ref> [CW93, LF94, MB95b, SD96c, Sch96] </ref>. The approach has the unique feature that it guarantees a consistent scene and set of flow fields when all assumptions are met.
Reference: [CWS95] <author> R. Chellappa, C. L. Wilson, and S. Sirohey. </author> <title> Human and machine recognition of faces: A survey. </title> <journal> Proc. IEEE, </journal> <volume> 83(5) </volume> <pages> 705-740, </pages> <year> 1995. </year>
Reference-contexts: These capabilities would add increased flexibility in the image acquisition process and provide the photographer with a powerful new set of image post-processing tools. Photo correction also has potential uses in face recognition and image database applications. Most face recognition techniques <ref> [SI92, CWS95] </ref> use 2D pattern recognition methods to correlate an observed face with a database of reference images of known faces. In order to maintain low error rates, image acquisition must be carefully controlled to ensure that the facial pose is the same (e.g., frontal) in all views.
Reference: [DNvGK97] <author> Kristin J. Dana, Shree K. Nayar, Bram van Ginneken, and Jan J. Koen-derink. </author> <title> Reflectance and texture of real-world surfaces. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 151-157, </pages> <year> 1997. </year>
Reference-contexts: More recently, there has been increasing attention in modeling reflectance and illumination via analysis of basis images <ref> [War92, BK96, DNvGK97, SWI97, SK98] </ref>. However, the problems of image-based editing and animation have yet to be studied. This chapter addresses the editing problem and describes several 3D editing operations that can be performed on photographs.
Reference: [DP91] <author> Trevor Darrell and Alex Pentland. </author> <title> Robust estimation of a multi-layered motion representation. </title> <booktitle> In Proc. IEEE Workshop on Visual Motion, </booktitle> <pages> pages 173-178, </pages> <year> 1991. </year>
Reference-contexts: Recently, researchers in computer vision and computer graphics have advocated this layering paradigm as an effective way for representing and rendering more complicated 3D scenes by manually [TK96, LS97] or automatically <ref> [WA94, DP91, AS95] </ref> segmenting the scene into a series of layers. In the last few years, computer-based image metamorphosis or morphing [Wol90, BN92] has emerged as a popular means for producing fascinating visual effects.
Reference: [DP95] <author> Trevor Darrell and Alex P. Pentland. </author> <title> Cooperative robust estimation using layers of support. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(5) </volume> <pages> 474-487, </pages> <year> 1995. </year>
Reference-contexts: Furthermore, most optical flow techniques use gradient methods, window correlation, or velocity tuned filters, all of which require small image displacements. We note that a number of techniques have been developed that are less susceptible to the aforementioned problems. For instance, mixture-methods and parameterized models have been proposed <ref> [WA94, DP95, BA96] </ref> to avoid smoothing over flow discontinuities.
Reference: [DTM96] <author> Paul E. Debevec, Camillo J. Taylor, and Jitendra Malik. </author> <title> Modeling and rendering architecture from photographs: A hybrid geometry- and image-based approach. </title> <booktitle> In Proc. SIGGRAPH 96, </booktitle> <pages> pages 11-20, </pages> <year> 1996. </year>
Reference-contexts: The success of QuickTime VR and newer systems like Surround Video [Bla97], IPIX [Int97], SmoothMove [Inf97], and RealVR [Liv97] has helped spawn an emerging subfield of computer graphics, often called image-based rendering, and has attracted growing interest in image-based representations. Debevec et al. <ref> [DTM96] </ref> exploited a mixture of manual and automatic methods to reconstruct high-quality 3D models of architectural scenes. A view-dependent texture mapping approach was used to model varying illumination and reflectance. <p> While the method developed in this chapter can operate fully automatically, a key feature is the use of user-interaction to help specify new views and control the interpolation. In this respect, our approach has strong similarities to Debevec et al.'s <ref> [DTM96] </ref> Facade system which included an important manual component. <p> Another possibility would be to use a mixture of automatic and manual techniques, for instance using user-specified features to bootstrap a stereo refinement procedure, as in <ref> [DTM96] </ref>. 53 Chapter 4 Single-View Morphing When an object has bilateral symmetry, view morphs can be computed from a single image by interpolating the image with its mirror reflection. For example, Fig. 3.13 depicts a view morph between an image of Leonardo da Vinci's Mona Lisa and its reflection. <p> Other researchers have used coarse-to-fine schemes [BA83] to enable the recovery of flow fields with larger image displacements. 6.1.2 Stereo Because of its relatively high accuracy, stereo is by far the most popular correspondence technique for view synthesis practitioners <ref> [OLC93, KRN97, WHH95, SD95b, MB95b, Sch96, DTM96] </ref>. However, problems with occlusions, widely separated views, and the need for specialized camera rigs make it impractical for many applications. Stereo represents an adaptation of optical flow to the problem of matching two calibrated views of the same rigid scene.
Reference: [DZLF94] <author> R. Deriche, Z. Zhang, Q.-T. Luong, and O. Faugeras. </author> <title> Robust recovery of the epipolar geometry for an uncalibrated stereo rig. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <pages> pages 567-576, </pages> <year> 1994. </year>
Reference-contexts: To solve the problem of mapping points between uncalibrated views, they developed techniques based on the fundamental matrix <ref> [LH81, DZLF94] </ref>. A second contribution was a ray-tracing method for resolving occlusion relationships in synthesized views. Poggio and Brunelli [PB92] proposed a computational learning framework for synthesizing 2D and 3D transformations from a set of basis images.
Reference: [Fau93] <author> Olivier Faugeras. </author> <title> Three-Dimensional Computer Vision, A Geometric Viewpoint. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: Reprojection can also be performed through texture-mapping and can therefore exploit current graphics hardware. Image reprojection has been used previously in a number of applications [Wol90]. Our use of reprojection is most closely related to the techniques used for rectifying stereo views to simplify 3D shape reconstruction <ref> [Fau93] </ref>. Image mosaic techniques [Gre86, MB95b, Che95, Sze96, SS97, KAI + 95, MP97] also rely heavily on reprojection methods to project images onto a planar, cylindrical, or spherical manifold. <p> The final positions of the control points for the image in the center of Fig. 3.9 were computed automatically by calibrating the two images based on their known focal lengths and interpolating the changes in orientation <ref> [Fau93] </ref>. Fig. 3.11 shows results for interpolating human faces in varying poses. The first example shows selected frames from a morph computed by interpolating views of the same person facing in two different directions. The resulting animation depicts the subject continuously turning his head from right to left.
Reference: [FM95] <author> Olivier Faugeras and Mourrain. </author> <title> On the geometry and algebra of the point and line correspondences between N images. </title> <booktitle> In Proc. Fifth Int. Conf. on Computer Vision, </booktitle> <pages> pages 951-956, </pages> <year> 1995. </year>
Reference-contexts: However, as noted in Chapter 5, the additional images introduce a whole range of new problems, like occlusion, calibration, correspondence, and representational issues. Whereas the two-image problem has been thoroughly studied in computer vision, theories of multi-image projective geometry, calibration, and correspondence have only recently begun to emerge <ref> [Sha94, Har94, LV94, Tri95, FM95, HA95] </ref>.
Reference: [Fre92] <author> John E. Freund. </author> <title> Mathematical Statistics. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: In particular, suppose the sensor error (accuracy of irradiance measurement) is normally distributed 2 with standard deviation 0 . The consistency of a voxel can be estimated using the likelihood ratio test, distributed as 2 with n 1 degrees of freedom <ref> [Fre92] </ref>: V = 2 (6.14) If 0 is unknown, it can be estimated by imaging a homogeneous surface and computing the standard deviation s 0 of m 0 image pixels.
Reference: [FS75] <author> R. W. Floyd and L. Steinberg. </author> <title> An adaptive algorithm for spatial gray scale. </title> <note> In SID 75, Int. Symp. Dig. Tech. Papers, page 36. </note> <year> 1975. </year>
Reference-contexts: For view synthesis tasks, this is a potential disadvantage, since structured noise can produce perceptible artifacts. The fact that these errors are deterministic and well-understood indicates that they are potentially detectable, and thus could be attenuated. One solution would be to perform a post-processing phase, analogous to dithering <ref> [FS75] </ref>, in which errors are diffused to mitigate these artifacts. An alternative method would be to identify textureless regions and other error sources in the basis images and treat these features specially in the reconstruction process.
Reference: [GGSC96] <author> Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F. Cohen. </author> <booktitle> The lumigraph. In Proc. SIGGRAPH 96, </booktitle> <pages> pages 43-54, </pages> <year> 1996. </year>
Reference-contexts: Observe that for N input images, a naive image-based approach would store all N images and N 2 correspondence maps, resulting in an expensive and highly redundant representation. As the number of images becomes very large, 3D or even 4D <ref> [LH96, GGSC96] </ref> representations can offer distinct advantages. A new solution for large numbers of images is presented in Chapter 6. 3.2 Related Work on View Synthesis Lippman [Lip80] was one of the first to propose the use of an image-based representation for computer-aided 3D scene visualization. <p> A view-dependent texture mapping approach was used to model varying illumination and reflectance. A major contribution of this work was to show the importance of user-interaction to aid view synthesis algorithms and facilitate 3D model construction. Levoy and Hanrahan [LH96] and Gortler et al. <ref> [GGSC96] </ref> developed novel ray-based approaches for view synthesis. These methods, termed light field and Lumigraph respectively, represent the visible scene using a four-dimensional ray space in which any image is a two-dimensional sample. The problems of acquiring the representation and synthesizing views are both achieved via simple sampling operations. <p> Existing image-based representations were designed primarily for view synthesis, not for editing, and some cannot be easily extended to support editing operations. For instance, a recent direction in image-based view synthesis has been toward ray-based plenoptic representations, in particular the light field [LH96] and Lumigraph <ref> [GGSC96, SCG97] </ref>, in which light rays are represented separately. An advantage of these models is that views can be synthesized without having to compute a dense pixel correspondence. Performing image editing operations within a ray-based representation is difficult, however, for two reasons.
Reference: [GLY92] <author> Davi Geiger, Bruce Landendorf, and Alan Yuille. </author> <title> Occlusions and binocular stereo. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <pages> pages 425-433, </pages> <year> 1992. </year>
Reference-contexts: Recently, a number of researchers have devised techniques to improve the performance of binocular stereo in the presence of occlusion, e.g., using Bayesian techniques <ref> [BM92, GLY92] </ref> and masks [NMSO96] to detect half-occluded regions. Trinocular [SD88, AL91] and multiple-baseline stereo methods [OK85b] enable larger cumulative baselines and have been shown to improve matching accuracy. Still, individual views must generally be close together.
Reference: [Gre86] <author> Ned Greene. </author> <title> Environment mapping and other applications of world projections. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 6(11) </volume> <pages> 21-29, </pages> <month> November </month> <year> 1986. </year> <month> 112 </month>
Reference-contexts: Image reprojection has been used previously in a number of applications [Wol90]. Our use of reprojection is most closely related to the techniques used for rectifying stereo views to simplify 3D shape reconstruction [Fau93]. Image mosaic techniques <ref> [Gre86, MB95b, Che95, Sze96, SS97, KAI + 95, MP97] </ref> also rely heavily on reprojection methods to project images onto a planar, cylindrical, or spherical manifold.
Reference: [HA95] <author> Anders Heyden and Kalle Astrom. </author> <title> A canonical framework for sequences of images. </title> <booktitle> In Proc. IEEE Workshop on Representation of Visual Scenes, </booktitle> <pages> pages 45-52, </pages> <year> 1995. </year>
Reference-contexts: However, as noted in Chapter 5, the additional images introduce a whole range of new problems, like occlusion, calibration, correspondence, and representational issues. Whereas the two-image problem has been thoroughly studied in computer vision, theories of multi-image projective geometry, calibration, and correspondence have only recently begun to emerge <ref> [Sha94, Har94, LV94, Tri95, FM95, HA95] </ref>.
Reference: [Har94] <author> Richard I. </author> <title> Hartley. Projective reconstruction and invariants from multiple images. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intell., </journal> <volume> 16(10) </volume> <pages> 1036-1041, </pages> <year> 1994. </year>
Reference-contexts: However, as noted in Chapter 5, the additional images introduce a whole range of new problems, like occlusion, calibration, correspondence, and representational issues. Whereas the two-image problem has been thoroughly studied in computer vision, theories of multi-image projective geometry, calibration, and correspondence have only recently begun to emerge <ref> [Sha94, Har94, LV94, Tri95, FM95, HA95] </ref>.
Reference: [Har95] <author> Richard I. </author> <title> Hartley. In defence of the 8-point algorithm. </title> <booktitle> In Proc. Fifth Int. Conf. on Computer Vision, </booktitle> <pages> pages 1064-1070, </pages> <year> 1995. </year>
Reference: [Hec89] <author> Paul S. Heckbert. </author> <title> Fundamentals of texture mapping and image warping. </title> <type> Master's thesis, </type> <institution> University of California, Dept. CS, Berkeley, </institution> <address> CA, </address> <month> May </month> <year> 1989. </year>
Reference: [HiAA97] <author> Youichi Horry, Ken ichi Anjyo, and Kiyoshi Arai. </author> <title> Tour into the picture: Using a spidery mesh interface to make animation from a single image. </title> <booktitle> In Proc. SIGGRAPH 97, </booktitle> <pages> pages 225-232, </pages> <year> 1997. </year>
Reference: [Hil83] <author> Ellen Catherine Hildreth. </author> <title> The Measurement of Visual Motion. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1983. </year>
Reference-contexts: In particular, the flow within low-contrast regions of nearly uniform color is locally ambiguous, a condition known as the aperture problem [Mar82]. As a result, complete optical flow is generally not measurable from two images [PTK85, VP89]. In response to the aperture problem many researchers instead use normal flow <ref> [Hil83] </ref>, which provides only the motion component in the gradient direction but has the advantage of being measurable. In this section we introduce another measurable variety of flow called boundary flow that is particularly well-suited to synthesizing new views of a scene.
Reference: [HSW92] <author> Glenn E. Healey, Steven A. Shafer, and Lawrence B. Wolff, </author> <title> editors. Physics-based vision: </title> <booktitle> Principles and Practice, COLOR, </booktitle> <pages> pages 205-299. </pages> <publisher> Jones and Bartlett, </publisher> <address> Boston, MA, </address> <year> 1992. </year>
Reference-contexts: They therefore provide absolute radiance information that is constant across all consistent scenes. This is to be distinguished from methods that attempt to solve the color constancy problem <ref> [HSW92] </ref>, by computing surface reflectance independent of scene illumination. In contrast, color invariants encode radiance directly, which is sufficient to synthesize new views of the scene with illumination held constant. A common theme that underlies this work is the use of novel constraints that simplify problems that are otherwise intractable. <p> An alternative method would be to identify textureless regions and other error sources in the basis images and treat these features specially in the reconstruction process. The notion of color invariance bears resemblence to the problem of color constancy (c.f. <ref> [HSW92] </ref>) which has a long history in the computer vision literature. The color constancy problem is to determine, from one or more images, a description of scene material properties (e.g., surface reflectance) that does not depend on scene illumination.
Reference: [IAH95] <author> Michal Irani, P. Anandan, and Steve Hsu. </author> <title> Mosaic based representations of video sequences and their applications. </title> <booktitle> In Proc. Fifth Int. Conf. on Computer Vision, </booktitle> <pages> pages 605-611, </pages> <year> 1995. </year>
Reference: [Inf97] <institution> Infinite Pictures, Inc. SmoothMove, </institution> <note> version 1.0, </note> <year> 1997. </year>
Reference-contexts: The success of QuickTime VR and newer systems like Surround Video [Bla97], IPIX [Int97], SmoothMove <ref> [Inf97] </ref>, and RealVR [Liv97] has helped spawn an emerging subfield of computer graphics, often called image-based rendering, and has attracted growing interest in image-based representations. Debevec et al. [DTM96] exploited a mixture of manual and automatic methods to reconstruct high-quality 3D models of architectural scenes.
Reference: [Int97] <institution> Interactive Pictures Corporation, Inc. IPIX, </institution> <note> version 1.0, </note> <year> 1997. </year>
Reference-contexts: The success of QuickTime VR and newer systems like Surround Video [Bla97], IPIX <ref> [Int97] </ref>, SmoothMove [Inf97], and RealVR [Liv97] has helped spawn an emerging subfield of computer graphics, often called image-based rendering, and has attracted growing interest in image-based representations. Debevec et al. [DTM96] exploited a mixture of manual and automatic methods to reconstruct high-quality 3D models of architectural scenes.
Reference: [KAI + 95] <author> Rakesh Kumar, P. Anandan, Michal Irani, James Bergen, and Keith Hanna. </author> <title> Representation of scenes from collections of images. </title> <booktitle> In Proc. IEEE Workshop on Representation of Visual Scenes, </booktitle> <pages> pages 10-17, </pages> <year> 1995. </year>
Reference-contexts: Image reprojection has been used previously in a number of applications [Wol90]. Our use of reprojection is most closely related to the techniques used for rectifying stereo views to simplify 3D shape reconstruction [Fau93]. Image mosaic techniques <ref> [Gre86, MB95b, Che95, Sze96, SS97, KAI + 95, MP97] </ref> also rely heavily on reprojection methods to project images onto a planar, cylindrical, or spherical manifold.
Reference: [KD95a] <author> Kiriakos N. Kutulakos and Charles R. Dyer. </author> <title> Affine surface reconstruction by purposive viewpoint control. </title> <booktitle> In Proc. Fifth Int. Conf. on Computer Vision, </booktitle> <pages> pages 894-901, </pages> <year> 1995. </year>
Reference-contexts: While temporal derivatives provide useful shape and correspondence information [CB92, SF95], computation of temporal derivatives requires closely space views. Alternatively, surface shape can be inferred directly from the silhouette using volume intersection [MA91, Sze93] and other direct approaches <ref> [KD95a, Kut97] </ref>. A fundamental shortcoming of contour-based methods is that they fail at concavities and compute only the visual hull [Lau95, KD95b], an approximation of the true shape.
Reference: [KD95b] <author> Kiriakos N. Kutulakos and Charles R. Dyer. </author> <title> Global surface reconstruction by purposive control of observer motion. </title> <journal> Artificial Intelligence, </journal> <volume> 78 </volume> <pages> 147-177, </pages> <year> 1995. </year> <month> 113 </month>
Reference-contexts: Alternatively, surface shape can be inferred directly from the silhouette using volume intersection [MA91, Sze93] and other direct approaches [KD95a, Kut97]. A fundamental shortcoming of contour-based methods is that they fail at concavities and compute only the visual hull <ref> [Lau95, KD95b] </ref>, an approximation of the true shape. Furthermore, they require the detection of occluding contours or silhouettes, a difficult problem in itself. 6.1.5 EPI Analysis Motivated by applications in holography, Katayama et al. [KTOT95] adapted epipolar plane (EPI) analysis techniques for view synthesis. <p> A primary contribution of this chapter was the voxel coloring framework for analyzing ambiguities in image correspondence and scene reconstruction. A similar theory was previously developed for the special case of volume intersection, i.e., reconstruction from silhouettes <ref> [Lau95, KD95b, Lau97] </ref>. The results in this chapter can be viewed as a generalization of the volume intersection problem for the broader case of textured objects and scenes.
Reference: [KNR95] <author> Takeo Kanade, P. J. Narayanan, and Peter W. Rander. </author> <title> Virtualized reality: Concepts and early results. </title> <booktitle> In Proc. IEEE Workshop on Representation of Visual Scenes, </booktitle> <pages> pages 69-76, </pages> <year> 1995. </year>
Reference: [KRN97] <author> Takeo Kanade, Peter Rander, and P. J. Narayanan. </author> <title> Virtualized reality: Constructing virtual worlds from real scenes. </title> <journal> IEEE Multimedia, </journal> <volume> 4(1) </volume> <pages> 34-46, </pages> <year> 1997. </year>
Reference-contexts: Indeed, a great deal of recent work <ref> [TK92, BTZ96, Sze96, MKKJ96, KRN97] </ref> has been devoted to the problem of obtaining high-quality texture-mapped 3D models from two or more basis images, and significant progress has been made in this area. <p> Lambertian scenes, this problem reduces to that of computing the optical flow fields C si : I s ) I i and using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo <ref> [OLC93, KRN97, Sch96] </ref> p feature tracking [TK92, BTZ96] p contour tracking [CB92, SF95] p volume intersection [MKKJ96, MTG97] p p EPI analysis [BBM87, KTOT95] p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. <p> Other researchers have used coarse-to-fine schemes [BA83] to enable the recovery of flow fields with larger image displacements. 6.1.2 Stereo Because of its relatively high accuracy, stereo is by far the most popular correspondence technique for view synthesis practitioners <ref> [OLC93, KRN97, WHH95, SD95b, MB95b, Sch96, DTM96] </ref>. However, problems with occlusions, widely separated views, and the need for specialized camera rigs make it impractical for many applications. Stereo represents an adaptation of optical flow to the problem of matching two calibrated views of the same rigid scene. <p> Trinocular [SD88, AL91] and multiple-baseline stereo methods [OK85b] enable larger cumulative baselines and have been shown to improve matching accuracy. Still, individual views must generally be close together. Another approach for integrating disparate views is to arrange cameras in clusters and to compute stereo reconstructions from each cluster <ref> [KRN97] </ref>. The resulting partial models can be subsequently merged [TL94, CL96] to form a more complete reconstruction. This split and merge approach has proven most successful when the partial reconstructions to be merged are individually quite accurate, e.g., as obtainable by high-end laser range scanners.
Reference: [KS96] <author> Sing Bing Kang and Richard Szeliski. </author> <title> 3-D scene data recovery using omnidirectional multibaseline stereo. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 364-370, </pages> <year> 1996. </year>
Reference-contexts: The linear case is easily generalized for any set of cameras satisfying the ordinal visibility constraint. Fig. 6.6 (b) shows a layer partition for the case of outward-facing cameras. This type of camera geometry is useful for acquiring panoramic scene visualizations, as in <ref> [MB95b, KS96] </ref>. One valid set of layers corresponds to a series of rectangles radiating outward from the camera volume. Layer 0 is the axis-aligned bounding box B of the camera centers and the subsequent layers are determined by uniformly expanding the box one unit at a time. <p> In particular, the room interior is highly concave, making accurate reconstruction by volume intersection or other contour-based methods impractical. Furthermore, the numerous cameras and large amount of occlusion would create difficulty for most stereo approaches. Notable exceptions include panorama-based stereo approaches <ref> [MB95b, KS96] </ref> that are well-suited for room reconstructions. However, these methods require that a panoramic image be constructed for each camera location prior to the stereo matching step, a requirement that is avoided by the voxel coloring approach.
Reference: [KTOT95] <author> Akihiro Katayama, Koichiro Tanaka, Takahiro Oshino, and Hideyuki Tamura. </author> <title> A viewpoint dependent stereoscopic display using interpolation of multi-viewpoint images. </title> <booktitle> In Proc. SPIE Vol. 2409A, </booktitle> <pages> pages 21-30, </pages> <year> 1995. </year>
Reference-contexts: using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo [OLC93, KRN97, Sch96] p feature tracking [TK92, BTZ96] p contour tracking [CB92, SF95] p volume intersection [MKKJ96, MTG97] p p EPI analysis <ref> [BBM87, KTOT95] </ref> p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. These approaches are evaluated with respect to their use for producing pixel correspondence maps under various conditions. <p> Furthermore, they require the detection of occluding contours or silhouettes, a difficult problem in itself. 6.1.5 EPI Analysis Motivated by applications in holography, Katayama et al. <ref> [KTOT95] </ref> adapted epipolar plane (EPI) analysis techniques for view synthesis. A key feature of their technique is that occlusions are explicitly modeled during the image correlation step. A constraint of the method is that the basis views must all lie along a line and be closely-spaced. <p> When two lines cross in the epipolar volume, the line with larger slope always corresponds to the occluder, i.e., the point which is closer to the axis of camera translation. Therefore, occlusions can be resolved by fitting lines in the order of decreasing slope <ref> [KTOT95] </ref>. 6.1.6 Scene Space Methods Determining correspondences using local image correlation methods, e.g., feature tracking or optical flow, is problematic when the motion between images is large. An alternative approach is to perform matching in scene space by detecting correspondences that are consistent with a rigid 3D scene. <p> Notice that for any two voxels P and Q, P can occlude Q from a basis viewpoint only if Q is in a higher layer than P. The simplification of visibility relationships for this special case of colinear views was previously noted by Katayama et al. <ref> [KTOT95] </ref>. The linear case is easily generalized for any set of cameras satisfying the ordinal visibility constraint. Fig. 6.6 (b) shows a layer partition for the case of outward-facing cameras. This type of camera geometry is useful for acquiring panoramic scene visualizations, as in [MB95b, KS96].
Reference: [Kut97] <author> Kiriakos N. Kutulakos. </author> <title> Shape from the light field boundary. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 53-59, </pages> <year> 1997. </year>
Reference-contexts: While temporal derivatives provide useful shape and correspondence information [CB92, SF95], computation of temporal derivatives requires closely space views. Alternatively, surface shape can be inferred directly from the silhouette using volume intersection [MA91, Sze93] and other direct approaches <ref> [KD95a, Kut97] </ref>. A fundamental shortcoming of contour-based methods is that they fail at concavities and compute only the visual hull [Lau95, KD95b], an approximation of the true shape.
Reference: [KvD76] <author> Jan J. Koenderink and Andrea J. van Doorn. </author> <title> The singularities of the visual mapping. </title> <journal> Biological Cybernetics, </journal> <volume> 24 </volume> <pages> 51-59, </pages> <year> 1976. </year>
Reference-contexts: Furthermore, the in-between views are the only views that can be predicted with certainty due to the requirement that the visible endpoints of each surface remain fixed. The monotonicity constraint is closely related to the theory of aspect graphs and visual events <ref> [KvD76, KvD79] </ref>. The constraint dictates that no changes in visibility, i.e., no occlusions, may occur within a monotonic range of viewspace. In other words, all views within a monotonic range are topologically equivalent; the same scene points are visible in every view.
Reference: [KvD79] <author> Jan J. Koenderink and Andrea J. van Doorn. </author> <title> The internal representation of solid shape with respect to vision. </title> <journal> Biological Cybernetics, </journal> <volume> 32 </volume> <pages> 211-216, </pages> <year> 1979. </year>
Reference-contexts: Furthermore, the in-between views are the only views that can be predicted with certainty due to the requirement that the visible endpoints of each surface remain fixed. The monotonicity constraint is closely related to the theory of aspect graphs and visual events <ref> [KvD76, KvD79] </ref>. The constraint dictates that no changes in visibility, i.e., no occlusions, may occur within a monotonic range of viewspace. In other words, all views within a monotonic range are topologically equivalent; the same scene points are visible in every view.
Reference: [Las87] <author> John Lasseter. </author> <title> Principles of traditional animation applied to 3D computer animation. </title> <booktitle> In Proc. SIGGRAPH 87, </booktitle> <pages> pages 35-44, </pages> <year> 1987. </year>
Reference-contexts: The success and continued popularity of their efforts attests to the efficacy of 2D techniques for producing realistic visual effects. Whereas traditional animation focused primarily on 2D effects, many of which featured artistic distortions and exaggerations <ref> [Las87] </ref>, certain 3D effects (i.e., scene transformations) were also employed. These included simulations of 3D camera translations by moving a small window over a background image. Translating the window conveys a frontoparallel motion of the camera, and scaling the window emulates forward or backward motion. <p> When the assumptions are violated, the correctness proofs no longer apply and the results may not be entirely physically correct. However, for visualization applications, the primary goal is perceptual realism; physical correctness is only one means for achieving this goal and is not strictly necessary <ref> [Las87] </ref>. An important feature of these algorithms is that they produce realistic results even for significant violations of the assumptions. The view morphing algorithm in particular is shown to produce highly realistic viewpoint transitions in situations where the shading, visibility, and shape differ dramatically in the two input images.
Reference: [Lau95] <author> Aldo Laurentini. </author> <title> How far 3D shapes can be understood from 2D silhouettes. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(2) </volume> <pages> 188-195, </pages> <year> 1995. </year>
Reference-contexts: Alternatively, surface shape can be inferred directly from the silhouette using volume intersection [MA91, Sze93] and other direct approaches [KD95a, Kut97]. A fundamental shortcoming of contour-based methods is that they fail at concavities and compute only the visual hull <ref> [Lau95, KD95b] </ref>, an approximation of the true shape. Furthermore, they require the detection of occluding contours or silhouettes, a difficult problem in itself. 6.1.5 EPI Analysis Motivated by applications in holography, Katayama et al. [KTOT95] adapted epipolar plane (EPI) analysis techniques for view synthesis. <p> For instance, consider the set of voxels that are contained in every consistent scene. Laurentini <ref> [Lau95] </ref> described how these invariants, called hard points, could be recovered by volume intersection from binary images. Hard points provide absolute information about the true scene but are relatively rare; some images may yield none (see, for example, Fig. 6.2). <p> In (b), the frequency is so low that the quantized texture pattern is uniform. Consequently, the problem reduces to reconstruction from silhouettes and the result is similar to what would be obtained by volume intersection <ref> [MA91, Sze93, Lau95] </ref>. Specifically, volume intersection would yield a closed diamond-shaped region; the reconstructed V-shaped cusp surface in (b) corresponds to the set of surfaces of this diamond that are visible from the basis views. Doubling results in a slightly better reconstruction consisting of two cusps, as shown in (c). <p> A primary contribution of this chapter was the voxel coloring framework for analyzing ambiguities in image correspondence and scene reconstruction. A similar theory was previously developed for the special case of volume intersection, i.e., reconstruction from silhouettes <ref> [Lau95, KD95b, Lau97] </ref>. The results in this chapter can be viewed as a generalization of the volume intersection problem for the broader case of textured objects and scenes.
Reference: [Lau97] <author> Aldo Laurentini. </author> <title> How many 2D silhouettes does it take to reconstruct a 3D object. </title> <booktitle> Computer Vision and Image Understanding, </booktitle> <volume> 67(1) </volume> <pages> 81-87, </pages> <year> 1997. </year>
Reference-contexts: A primary contribution of this chapter was the voxel coloring framework for analyzing ambiguities in image correspondence and scene reconstruction. A similar theory was previously developed for the special case of volume intersection, i.e., reconstruction from silhouettes <ref> [Lau95, KD95b, Lau97] </ref>. The results in this chapter can be viewed as a generalization of the volume intersection problem for the broader case of textured objects and scenes.
Reference: [LCSW92] <author> Seung-Yong Lee, Kyung-Yong Chwa, Sung Yong Shin, and George Wol-berg. </author> <title> Image metamorphosis using snakes and free-form deformations. </title> <booktitle> In Proc. SIGGRAPH 92, </booktitle> <pages> pages 439-448, </pages> <year> 1992. </year>
Reference-contexts: Two maps are required because the correspondence may not be one-to-one. In practice, C 01 and C 10 are partially specified by having the user provide a sparse set of matching features or regions in the two images. The remaining correspondences are determined automatically by interpolation <ref> [Wol90, BN92, LCSW92] </ref>. <p> The dashed line shows the linear path of one feature during the course of the transformation. This example is indicative of the types of distortions that can arise with image morphing techniques. <ref> [Wol90, BN92, LCSW92] </ref>. To illustrate the potentially severe 3D distortions incurred by image morphing, it is useful to consider interpolating between two different views of a planar shape.
Reference: [LF94] <author> Stephane Laveau and Olivier Faugeras. </author> <title> 3-D scene representation as a collection of images. </title> <booktitle> In Proc. Int. Conf. on Pattern Recognition, </booktitle> <pages> pages 689-691, </pages> <year> 1994. </year>
Reference-contexts: This observation led to the development of novel hardware systems that use view synthesis techniques to achieve real-time rendering rates for synthetic 3D scenes [RP94, TK96, LS97]. Laveau and Faugeras <ref> [LF94] </ref> were among the first to develop view synthesis techniques that operate on photographs and therefore apply to real scenes. <p> The problems of acquiring the representation and synthesizing views are both achieved via simple sampling operations. While elegant and relatively simple to construct, these approaches require many more input views compared to correspondence-based techniques. Most closely-related to the approach in this chapter are the so called view interpolation methods <ref> [CW93, LF94, MB95b, WHH95, BP96, Sch96, AS97] </ref>, which use image warping to produce new views from a small number of basis views. Most of these methods require advance knowledge of camera parameters to produce correct perspective views, with the exceptions of [LF94, AS97]. <p> Most of these methods require advance knowledge of camera parameters to produce correct perspective views, with the exceptions of <ref> [LF94, AS97] </ref>. Furthermore, all of these methods require 18 dense pixel correspondence maps as input. This latter requirement is a serious limitation, given that image-based computation of correct correspondence maps is known to be an ill-posed problem [PTK85, VP89]. <p> This approach is in essence the reduction used in all previous approaches to view synthesis, for instance <ref> [CW93, LF94, MB95b, BP96] </ref>. While complete optical flow is sufficient to synthesize new views, its automatic recovery from basis images is problematic and error-prone. In particular, the flow within low-contrast regions of nearly uniform color is locally ambiguous, a condition known as the aperture problem [Mar82]. <p> The use of image control points bears resemblance to the view synthesis work of Laveau and Faugeras <ref> [LF94] </ref>, who used five pairs of corresponding image points to specify projection parameters. <p> This scene can either be reprojected to synthesize new views as in [SD97b], or used to compute correspondence maps for image-warping methods such as <ref> [CW93, LF94, MB95b, SD96c, Sch96] </ref>. The approach has the unique feature that it guarantees a consistent scene and set of flow fields when all assumptions are met.
Reference: [LF96] <author> Quang-Tuan Luong and Olivier Faugeras. </author> <title> The fundamental matrix: Theory, algorithms, and stability analysis. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 17(1) </volume> <pages> 43-75, </pages> <year> 1996. </year> <month> 114 </month>
Reference-contexts: This instability is particularly prominent when feature positions are imprecise, or when the features lie on a variety of degenerate 45 3D configurations <ref> [LF96] </ref>. Poor prewarps are easily detected by the scanline property: corresponding points in two prewarped images should appear in the same scanline (i.e., have the same y coordinate).
Reference: [LH81] <author> H. C. Longuet-Higgins. </author> <title> A computer algorithm for reconstructing a scene from two projections. </title> <journal> Nature, </journal> <volume> 293 </volume> <pages> 133-135, </pages> <year> 1981. </year>
Reference-contexts: To solve the problem of mapping points between uncalibrated views, they developed techniques based on the fundamental matrix <ref> [LH81, DZLF94] </ref>. A second contribution was a ray-tracing method for resolving occlusion relationships in synthesized views. Poggio and Brunelli [PB92] proposed a computational learning framework for synthesizing 2D and 3D transformations from a set of basis images.
Reference: [LH91] <author> David Laur and Pat Hanrahan. </author> <title> Hierarchical splatting: A progressive refinement algorithm for volume rendering. </title> <booktitle> In Proc. SIGGRAPH 91, </booktitle> <year> 1991. </year>
Reference-contexts: For simplicity, our implementation used a square mask to approximate voxel footprints, and used Eq. (6.13) to test voxel consistency. Alternative footprint models are discussed in the volume rendering literature, e.g., <ref> [Wes90, LH91] </ref>. While our implementation did not make use of this, additional speedups are possible by exploiting the uniform discretization of space and simple layer geometry. Choosing planar or polyhedral layers enables the use of texture-mapping graphics hardware to calculate voxel footprints, an entire layer at a time.
Reference: [LH96] <author> Marc Levoy and Pat Hanrahan. </author> <title> Light field rendering. </title> <booktitle> In Proc. SIGGRAPH 96, </booktitle> <year> 1996. </year>
Reference-contexts: Observe that for N input images, a naive image-based approach would store all N images and N 2 correspondence maps, resulting in an expensive and highly redundant representation. As the number of images becomes very large, 3D or even 4D <ref> [LH96, GGSC96] </ref> representations can offer distinct advantages. A new solution for large numbers of images is presented in Chapter 6. 3.2 Related Work on View Synthesis Lippman [Lip80] was one of the first to propose the use of an image-based representation for computer-aided 3D scene visualization. <p> A view-dependent texture mapping approach was used to model varying illumination and reflectance. A major contribution of this work was to show the importance of user-interaction to aid view synthesis algorithms and facilitate 3D model construction. Levoy and Hanrahan <ref> [LH96] </ref> and Gortler et al. [GGSC96] developed novel ray-based approaches for view synthesis. These methods, termed light field and Lumigraph respectively, represent the visible scene using a four-dimensional ray space in which any image is a two-dimensional sample. <p> Existing image-based representations were designed primarily for view synthesis, not for editing, and some cannot be easily extended to support editing operations. For instance, a recent direction in image-based view synthesis has been toward ray-based plenoptic representations, in particular the light field <ref> [LH96] </ref> and Lumigraph [GGSC96, SCG97], in which light rays are represented separately. An advantage of these models is that views can be synthesized without having to compute a dense pixel correspondence. Performing image editing operations within a ray-based representation is difficult, however, for two reasons.
Reference: [Lip80] <author> A. Lippman. Movie-maps: </author> <title> an application of the optical videodisc to computer graphics. </title> <booktitle> In Proc. SIGGRAPH 80, </booktitle> <pages> pages 32-42, </pages> <year> 1980. </year>
Reference-contexts: As the number of images becomes very large, 3D or even 4D [LH96, GGSC96] representations can offer distinct advantages. A new solution for large numbers of images is presented in Chapter 6. 3.2 Related Work on View Synthesis Lippman <ref> [Lip80] </ref> was one of the first to propose the use of an image-based representation for computer-aided 3D scene visualization. His Movie-Map approach enabled interactive virtual exploration of an environment by selectively accessing views that were previously captured and stored on laser disk.
Reference: [Liv97] <institution> Live Picture, Inc. </institution> <note> RealSpace Viewer, version 2.0, </note> <year> 1997. </year>
Reference-contexts: The success of QuickTime VR and newer systems like Surround Video [Bla97], IPIX [Int97], SmoothMove [Inf97], and RealVR <ref> [Liv97] </ref> has helped spawn an emerging subfield of computer graphics, often called image-based rendering, and has attracted growing interest in image-based representations. Debevec et al. [DTM96] exploited a mixture of manual and automatic methods to reconstruct high-quality 3D models of architectural scenes.
Reference: [LS97] <author> Jed Lengyel and John Snyder. </author> <title> Rendering with coherent layers. </title> <booktitle> In Proc. SIGGRAPH 97, </booktitle> <pages> pages 233-242, </pages> <year> 1997. </year>
Reference-contexts: Recently, researchers in computer vision and computer graphics have advocated this layering paradigm as an effective way for representing and rendering more complicated 3D scenes by manually <ref> [TK96, LS97] </ref> or automatically [WA94, DP91, AS95] segmenting the scene into a series of layers. In the last few years, computer-based image metamorphosis or morphing [Wol90, BN92] has emerged as a popular means for producing fascinating visual effects. <p> This observation led to the development of novel hardware systems that use view synthesis techniques to achieve real-time rendering rates for synthetic 3D scenes <ref> [RP94, TK96, LS97] </ref>. Laveau and Faugeras [LF94] were among the first to develop view synthesis techniques that operate on photographs and therefore apply to real scenes.
Reference: [LV94] <author> Q-.T. Luong and T. Vieville. </author> <title> Canonic representations for the geometries of multiple projective views. </title> <booktitle> In Proc. Third European Conf. on Computer Vision, </booktitle> <pages> pages 589-597, </pages> <year> 1994. </year>
Reference-contexts: However, as noted in Chapter 5, the additional images introduce a whole range of new problems, like occlusion, calibration, correspondence, and representational issues. Whereas the two-image problem has been thoroughly studied in computer vision, theories of multi-image projective geometry, calibration, and correspondence have only recently begun to emerge <ref> [Sha94, Har94, LV94, Tri95, FM95, HA95] </ref>.
Reference: [MA91] <author> W. N. Martin and J. K. Aggarwal. </author> <title> Volumetric description of objects from multiple views. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 5(2) </volume> <pages> 150-158, </pages> <year> 1991. </year>
Reference-contexts: These approaches can be characterized by whether or not they use temporal derivatives of the contour. While temporal derivatives provide useful shape and correspondence information [CB92, SF95], computation of temporal derivatives requires closely space views. Alternatively, surface shape can be inferred directly from the silhouette using volume intersection <ref> [MA91, Sze93] </ref> and other direct approaches [KD95a, Kut97]. A fundamental shortcoming of contour-based methods is that they fail at concavities and compute only the visual hull [Lau95, KD95b], an approximation of the true shape. <p> In (b), the frequency is so low that the quantized texture pattern is uniform. Consequently, the problem reduces to reconstruction from silhouettes and the result is similar to what would be obtained by volume intersection <ref> [MA91, Sze93, Lau95] </ref>. Specifically, volume intersection would yield a closed diamond-shaped region; the reconstructed V-shaped cusp surface in (b) corresponds to the set of surfaces of this diamond that are visible from the basis views. Doubling results in a slightly better reconstruction consisting of two cusps, as shown in (c).
Reference: [Mar82] <author> David Marr. </author> <title> Vision. </title> <editor> W. H. </editor> <publisher> Freeman Co., </publisher> <address> San Francisco, CA, </address> <year> 1982. </year>
Reference-contexts: is indeed solvable, i.e., given two perspective views of a static scene, under what conditions may new views be unambiguously predicted? We point out that this question is nontrival, given that basic quantities like optical flow and shape are not uniquely computable due to inherent ambiguities (e.g., the aperture problem <ref> [Mar82] </ref>). The second goal is to develop an algorithm to produce correct, high-quality, synthetic views of a scene from two basis images. <p> This suggests that view synthesis is also ill-posed. However, it can be shown that this ambiguity is an artifact of 3D reconstruction and is avoidable for the task of synthesizing new 2D views. By extracting only the information needed for view synthesis, well-known limitations like the aperture problem <ref> [Mar82] </ref> are eliminated. <p> While complete optical flow is sufficient to synthesize new views, its automatic recovery from basis images is problematic and error-prone. In particular, the flow within low-contrast regions of nearly uniform color is locally ambiguous, a condition known as the aperture problem <ref> [Mar82] </ref>. As a result, complete optical flow is generally not measurable from two images [PTK85, VP89]. In response to the aperture problem many researchers instead use normal flow [Hil83], which provides only the motion component in the gradient direction but has the advantage of being measurable. <p> In particular, two views are sufficient to model appearance for the continuum of in-between views. In contrast, shape reconstruction is not generally possible from two views, due to ambiguities in the correspondence problem <ref> [Mar82, PTK85, VP89] </ref>. A second contribution was the view morphing algorithm for synthesizing high-quality in-between views by warping two basis images. While the uniqueness result demonstrates that in-between views are theoretically determined, this algorithm provides a practical method for synthesizing high-quality in-between views from two basis images. <p> A variety of techniques exist for computing optical flow fields C ij (see [BFB94] for a survey and comparison). While it has been shown that optical flow estimates can be directly measured only at brightness discontinuities <ref> [Mar82, PTK85] </ref>, smoothness conditions can be imposed to derive dense flow fields. Imposing smoothness is problematic, however, as it tends to produce poor results at motion discontinuities and occlusion boundaries.
Reference: [MB95a] <author> Leonard McMillan and Gary Bishop. </author> <title> Head-tracked stereoscopic display using image warping. </title> <booktitle> In Proc. SPIE Vol. 2409A, </booktitle> <pages> pages 21-30, </pages> <year> 1995. </year>
Reference-contexts: The problem is represented schematically in Fig. 3.1. This problem is interesting because (1) it has applications of practical importance, such as stereo viewing <ref> [MB95a] </ref> and teleconferencing [BP96], (2) it is amenable to a thorough bottom-up analysis, and (3) it provides a base case for the more general problem of view synthesis from arbitrary sets of views. <p> Bottom: view morph yields a more realistic transition that preserves the underlying 3D geometry. 51 also interesting in its own right and has immediate applications of practical importance such as teleconferencing [TP94], face recognition [Sei97, SY97], stereo display <ref> [MB95a] </ref>, and special effects [SD96c]. A main contribution was the uniqueness result for in-between views, which proved that all views of a scene between two cameras are uniquely determined from two basis images.
Reference: [MB95b] <author> Leonard McMillan and Gary Bishop. </author> <booktitle> Plenoptic modeling. In Proc. </booktitle> <volume> SIG-GRAPH 95, </volume> <pages> pages 39-46, </pages> <year> 1995. </year>
Reference-contexts: For brevity, we discuss only a few here that are representative of the dominant approaches that currently exist. Comparisons with approaches in this thesis will be discussed in later sections and chapters. 17 Chen [Che95], and McMillan and Bishop <ref> [MB95b] </ref> developed systems similar to Lippman's Movie-Maps but using computer-based rather than optical methods. Both of these systems provided panoramic visualization of scenes using one or more cylindrical image mosaics, each of which stored scene appearance from a single camera position. <p> The problems of acquiring the representation and synthesizing views are both achieved via simple sampling operations. While elegant and relatively simple to construct, these approaches require many more input views compared to correspondence-based techniques. Most closely-related to the approach in this chapter are the so called view interpolation methods <ref> [CW93, LF94, MB95b, WHH95, BP96, Sch96, AS97] </ref>, which use image warping to produce new views from a small number of basis views. Most of these methods require advance knowledge of camera parameters to produce correct perspective views, with the exceptions of [LF94, AS97]. <p> This approach is in essence the reduction used in all previous approaches to view synthesis, for instance <ref> [CW93, LF94, MB95b, BP96] </ref>. While complete optical flow is sufficient to synthesize new views, its automatic recovery from basis images is problematic and error-prone. In particular, the flow within low-contrast regions of nearly uniform color is locally ambiguous, a condition known as the aperture problem [Mar82]. <p> Furthermore, since the interpolation is computed one scanline at a time, Z-buffering may be performed at the scanline level, thereby avoiding the large memory requirements commonly associated with Z-buffering algorithms. An alternative method using a Painter's method instead of Z-buffering is presented in <ref> [MB95b] </ref>. Unlike folds, holes cannot always be eliminated using image information alone. Chen and Williams [CW93] suggested different methods for filling holes, using a designated background color, interpolation with neighboring pixels, or additional images for better surface coverage. <p> Image reprojection has been used previously in a number of applications [Wol90]. Our use of reprojection is most closely related to the techniques used for rectifying stereo views to simplify 3D shape reconstruction [Fau93]. Image mosaic techniques <ref> [Gre86, MB95b, Che95, Sze96, SS97, KAI + 95, MP97] </ref> also rely heavily on reprojection methods to project images onto a planar, cylindrical, or spherical manifold. <p> Other researchers have used coarse-to-fine schemes [BA83] to enable the recovery of flow fields with larger image displacements. 6.1.2 Stereo Because of its relatively high accuracy, stereo is by far the most popular correspondence technique for view synthesis practitioners <ref> [OLC93, KRN97, WHH95, SD95b, MB95b, Sch96, DTM96] </ref>. However, problems with occlusions, widely separated views, and the need for specialized camera rigs make it impractical for many applications. Stereo represents an adaptation of optical flow to the problem of matching two calibrated views of the same rigid scene. <p> This scene can either be reprojected to synthesize new views as in [SD97b], or used to compute correspondence maps for image-warping methods such as <ref> [CW93, LF94, MB95b, SD96c, Sch96] </ref>. The approach has the unique feature that it guarantees a consistent scene and set of flow fields when all assumptions are met. <p> The linear case is easily generalized for any set of cameras satisfying the ordinal visibility constraint. Fig. 6.6 (b) shows a layer partition for the case of outward-facing cameras. This type of camera geometry is useful for acquiring panoramic scene visualizations, as in <ref> [MB95b, KS96] </ref>. One valid set of layers corresponds to a series of rectangles radiating outward from the camera volume. Layer 0 is the axis-aligned bounding box B of the camera centers and the subsequent layers are determined by uniformly expanding the box one unit at a time. <p> In particular, the room interior is highly concave, making accurate reconstruction by volume intersection or other contour-based methods impractical. Furthermore, the numerous cameras and large amount of occlusion would create difficulty for most stereo approaches. Notable exceptions include panorama-based stereo approaches <ref> [MB95b, KS96] </ref> that are well-suited for room reconstructions. However, these methods require that a panoramic image be constructed for each camera location prior to the stereo matching step, a requirement that is avoided by the voxel coloring approach. <p> A key feature of these types of editing operations is that they apply to the space of all views of the scene, rather than just one image. It is therefore convenient to cast them in terms of the plenoptic function <ref> [AB91, MB95b] </ref>, which encodes scene appearance from all possible viewpoints. Within this framework, our goal is to recover a scene's plenoptic function from a discrete set of images and to determine how it should be modified in response to basic image editing operations like painting, scissoring, and morphing.
Reference: [MKKJ96] <author> Saied Moezzi, Arun Katkere, Don Y. Kuramura, and Ramesh Jain. </author> <title> Reality modeling and visualization from multiple video sequences. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 16(6) </volume> <pages> 58-63, </pages> <year> 1996. </year>
Reference-contexts: Indeed, a great deal of recent work <ref> [TK92, BTZ96, Sze96, MKKJ96, KRN97] </ref> has been devoted to the problem of obtaining high-quality texture-mapped 3D models from two or more basis images, and significant progress has been made in this area. <p> I s ) I i and using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo [OLC93, KRN97, Sch96] p feature tracking [TK92, BTZ96] p contour tracking [CB92, SF95] p volume intersection <ref> [MKKJ96, MTG97] </ref> p p EPI analysis [BBM87, KTOT95] p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. These approaches are evaluated with respect to their use for producing pixel correspondence maps under various conditions. <p> The search can be simplified by using heuristic pruning techniques [Cox93] or by imposing the assumption of a rigid scene [BCZ93, SD95a]. 6.1.4 Contour Tracking and Volume Intersection Moezzi et al. <ref> [MKKJ96, MTG97] </ref> applied a volume intersection technique to derive texture-mapped models for the purpose of view synthesis, using background subtraction to obtain silhouettes. Volume intersection techniques are attractive because, unlike most methods, they permit the input cameras to be arbitrarily far apart.
Reference: [MN90] <author> Hiroshi Murase and Shree K. Nayar. </author> <title> Visual learning and recognition of 3-D objects from appearance. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 14(1) </volume> <pages> 171-183, </pages> <year> 1990. </year>
Reference-contexts: One way of doing this is to normalize the images, using the procedure in Fig. 3.13, to convert both observed and reference images to front-facing views prior to recognition. An alternative approach that has recently been explored by several researchers <ref> [BP96, PMS94, MN90] </ref> is to estimate both pose and identity by acquiring images of each reference face from several different viewpoints.
Reference: [MP95] <author> Steve Mann and Rosalind Picard. </author> <title> Video orbits of the projective group; A new perspective on image mosaicing. </title> <journal> A.I. </journal> <volume> Memo No. 338, </volume> <publisher> MIT Media Lab, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference: [MP97] <author> Steve Mann and Rosalind Picard. </author> <title> Video orbits of the projective group: A simple approach to featureless estimation of parameters. </title> <journal> IEEE Trans. on Image Processing, </journal> <volume> 6(9), </volume> <year> 1997. </year> <month> 115 </month>
Reference-contexts: Image reprojection has been used previously in a number of applications [Wol90]. Our use of reprojection is most closely related to the techniques used for rectifying stereo views to simplify 3D shape reconstruction [Fau93]. Image mosaic techniques <ref> [Gre86, MB95b, Che95, Sze96, SS97, KAI + 95, MP97] </ref> also rely heavily on reprojection methods to project images onto a planar, cylindrical, or spherical manifold.
Reference: [MTG97] <author> Saied Moezzi, Li-Cheng Tai, and Philippe Gerard. </author> <title> Virtual view generation for 3d digital video. </title> <journal> IEEE Multimedia, </journal> <volume> 4(1) </volume> <pages> 18-26, </pages> <year> 1997. </year>
Reference-contexts: I s ) I i and using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo [OLC93, KRN97, Sch96] p feature tracking [TK92, BTZ96] p contour tracking [CB92, SF95] p volume intersection <ref> [MKKJ96, MTG97] </ref> p p EPI analysis [BBM87, KTOT95] p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. These approaches are evaluated with respect to their use for producing pixel correspondence maps under various conditions. <p> The search can be simplified by using heuristic pruning techniques [Cox93] or by imposing the assumption of a rigid scene [BCZ93, SD95a]. 6.1.4 Contour Tracking and Volume Intersection Moezzi et al. <ref> [MKKJ96, MTG97] </ref> applied a volume intersection technique to derive texture-mapped models for the purpose of view synthesis, using background subtraction to obtain silhouettes. Volume intersection techniques are attractive because, unlike most methods, they permit the input cameras to be arbitrarily far apart.
Reference: [NMSO96] <author> Yuichi Nakamura, Tomohiko Matsuura, Kiyohide Satoh, and Yuichi Ohta. </author> <title> Occlusion detectable stereo-occlusion patterns in camera matrix. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 371-378, </pages> <year> 1996. </year>
Reference-contexts: Recently, a number of researchers have devised techniques to improve the performance of binocular stereo in the presence of occlusion, e.g., using Bayesian techniques [BM92, GLY92] and masks <ref> [NMSO96] </ref> to detect half-occluded regions. Trinocular [SD88, AL91] and multiple-baseline stereo methods [OK85b] enable larger cumulative baselines and have been shown to improve matching accuracy. Still, individual views must generally be close together.
Reference: [OK85a] <author> Yuichi Ohta and Takeo Kanade. </author> <title> Stereo by intra- and inter-scanline search using dynamic programming. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(2) </volume> <pages> 139-154, </pages> <year> 1985. </year>
Reference-contexts: The monotonicity constraint dictates that all visible scene points appear in the same order along conjugate epipolar lines of I 0 and I 1 . This constraint is used commonly in stereo matching <ref> [OK85a] </ref> because the fixed relative ordering of points along epipolar lines simplifies the correspondence problem. Despite its usual definition with respect to epipolar lines and images, mono-tonicity constrains only the location of the optical centers with respect to points in the scene|the image planes may be chosen arbitrarily. <p> We explored two different methods for computing correspondence, one based on stereo matching, and one that incorporates user interaction. The first approach is to use stereo matching techniques to solve for point correspondences within epipolar lines. For our experiments, we used a dynamic programming method <ref> [OK85a] </ref> that exploits the monotonicity constraint. The prewarped images are in an ideal configuration for stereo matching because conjugate epipolar lines are horizontal and occupy corresponding scanlines in the two images. Therefore the images can be correlated one scanline at a time. <p> Therefore the images can be correlated one scanline at a time. Within each scanline, corresponding pixels are determined using the monotonicity constraint, which strongly limits the search space by fixing the relative pixel ordering. Edge segments are extracted to maintain inter-scanline constraints, as described in <ref> [OK85a] </ref>. As described in Section 3.5, a full pixel-correspondence is not needed to synthesize new views. Rather, we need only to determine boundary flow, i.e., correspondences of pixels at the boundaries of uniform intervals in epipolar lines. <p> For each pair of conjugate epipolar lines l 0 and l 1 , match l 0 0 and l 0 1 using a scanline stereo matching technique, e.g., <ref> [OK85a] </ref>. The boundary flow is easily obtained from the resulting interval correspondence. There are a number of improvements that could be made to this approach. <p> The first set of images, shown in Fig. 3.10, represent interpolations of views that were processed using stereo matching techniques <ref> [OK85a] </ref> to automatically derive boundary flow information.
Reference: [OK85b] <author> Masatoshi Okutomi and Takeo Kanade. </author> <title> A multiple-baseline stereo. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(4) </volume> <pages> 353-363, </pages> <year> 1985. </year>
Reference-contexts: Recently, a number of researchers have devised techniques to improve the performance of binocular stereo in the presence of occlusion, e.g., using Bayesian techniques [BM92, GLY92] and masks [NMSO96] to detect half-occluded regions. Trinocular [SD88, AL91] and multiple-baseline stereo methods <ref> [OK85b] </ref> enable larger cumulative baselines and have been shown to improve matching accuracy. Still, individual views must generally be close together. Another approach for integrating disparate views is to arrange cameras in clusters and to compute stereo reconstructions from each cluster [KRN97].
Reference: [OLC93] <author> Maximilian Ott, John P. Lewis, and Ingemar Cox. </author> <title> Teleconferencing eye contact using a virtual camera. </title> <booktitle> In Proc. </booktitle> <volume> INTERCHI `93, </volume> <pages> pages 109-110, </pages> <year> 1993. </year>
Reference-contexts: Laveau and Faugeras [LF94] were among the first to develop view synthesis techniques that operate on photographs and therefore apply to real scenes. While others <ref> [TK92, OLC93] </ref> had devised methods for synthesizing views from known (a priori or derived) depth maps and camera positions, they were the first to demonstrate the feasibility of view synthesis directly from uncalibrated images and correspondence maps. <p> Lambertian scenes, this problem reduces to that of computing the optical flow fields C si : I s ) I i and using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo <ref> [OLC93, KRN97, Sch96] </ref> p feature tracking [TK92, BTZ96] p contour tracking [CB92, SF95] p volume intersection [MKKJ96, MTG97] p p EPI analysis [BBM87, KTOT95] p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. <p> Other researchers have used coarse-to-fine schemes [BA83] to enable the recovery of flow fields with larger image displacements. 6.1.2 Stereo Because of its relatively high accuracy, stereo is by far the most popular correspondence technique for view synthesis practitioners <ref> [OLC93, KRN97, WHH95, SD95b, MB95b, Sch96, DTM96] </ref>. However, problems with occlusions, widely separated views, and the need for specialized camera rigs make it impractical for many applications. Stereo represents an adaptation of optical flow to the problem of matching two calibrated views of the same rigid scene.
Reference: [PB92] <author> Tomaso Poggio and Roberto Brunelli. </author> <title> A novel approach to graphics. </title> <journal> A.I. </journal> <volume> Memo No. 1354, </volume> <publisher> M.I.T., </publisher> <address> Cambridge, MA, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: To solve the problem of mapping points between uncalibrated views, they developed techniques based on the fundamental matrix [LH81, DZLF94]. A second contribution was a ray-tracing method for resolving occlusion relationships in synthesized views. Poggio and Brunelli <ref> [PB92] </ref> proposed a computational learning framework for synthesizing 2D and 3D transformations from a set of basis images. Their technique, which uses hyper basis functions to learn parameterized models from images, is very general and can model a variety of interesting transformations.
Reference: [Per96] <author> Pietro Perona. </author> <title> Recognition of planar object classes. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 223-230, </pages> <year> 1996. </year>
Reference-contexts: The stereo correspondence techniques used in our implementation were much more likely to generate erroneous correspondences at these features than were human operators, and consequently tended to produce poorer results. Possible improvements could be obtained by using specialized feature detectors <ref> [BP93, PMS94, Per96] </ref> to facilitate automatic correspondence of key features like the eyes, nose, mouth.
Reference: [PMS94] <author> Alex Pentland, Baback Moghaddam, and Thad Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 84-91, </pages> <year> 1994. </year>
Reference-contexts: The stereo correspondence techniques used in our implementation were much more likely to generate erroneous correspondences at these features than were human operators, and consequently tended to produce poorer results. Possible improvements could be obtained by using specialized feature detectors <ref> [BP93, PMS94, Per96] </ref> to facilitate automatic correspondence of key features like the eyes, nose, mouth. <p> One way of doing this is to normalize the images, using the procedure in Fig. 3.13, to convert both observed and reference images to front-facing views prior to recognition. An alternative approach that has recently been explored by several researchers <ref> [BP96, PMS94, MN90] </ref> is to estimate both pose and identity by acquiring images of each reference face from several different viewpoints.
Reference: [PTK85] <author> Tomaso Poggio, Vincent Torre, and Christof Koch. </author> <title> Computational vision and regularization theory. </title> <journal> Nature, </journal> <volume> 317 </volume> <pages> 314-319, </pages> <year> 1985. </year>
Reference-contexts: This conclusion is especially unfortunate in light of the fact that 3D reconstruction from photographs is generally ambiguous|a number of different scenes may be consistent with a given set of images; it is an ill-posed problem <ref> [PTK85] </ref>. This suggests that view synthesis is also ill-posed. However, it can be shown that this ambiguity is an artifact of 3D reconstruction and is avoidable for the task of synthesizing new 2D views. <p> Furthermore, all of these methods require 18 dense pixel correspondence maps as input. This latter requirement is a serious limitation, given that image-based computation of correct correspondence maps is known to be an ill-posed problem <ref> [PTK85, VP89] </ref>. In contrast, this chapter presents a view interpolation technique that operates directly on a pair of images, without knowledge of camera parameters or dense correspondence. It has the unique property of requiring only measurable image information, thus establishing that view interpolation is a well-posed problem. <p> In particular, the flow within low-contrast regions of nearly uniform color is locally ambiguous, a condition known as the aperture problem [Mar82]. As a result, complete optical flow is generally not measurable from two images <ref> [PTK85, VP89] </ref>. In response to the aperture problem many researchers instead use normal flow [Hil83], which provides only the motion component in the gradient direction but has the advantage of being measurable. <p> This result demonstrates that view synthesis under monotonicity is an intrinsically well-posed problem| and is therefore much easier than 3D reconstruction and related motion analysis tasks requiring smoothness conditions and regularization techniques <ref> [PTK85] </ref>. To see why the monotonicity constraint is so crucial to view synthesis, observe that it is required not only to make the correspondence problem solvable, but also to predict the appearance of uniform surfaces whose shapes are unknown. <p> In particular, two views are sufficient to model appearance for the continuum of in-between views. In contrast, shape reconstruction is not generally possible from two views, due to ambiguities in the correspondence problem <ref> [Mar82, PTK85, VP89] </ref>. A second contribution was the view morphing algorithm for synthesizing high-quality in-between views by warping two basis images. While the uniqueness result demonstrates that in-between views are theoretically determined, this algorithm provides a practical method for synthesizing high-quality in-between views from two basis images. <p> These approaches are evaluated with respect to their use for producing pixel correspondence maps under various conditions. A check indicates that a technique is robust with respect to a particular condition, i.e., the presence of that condition imposes virtually no performance penalty. As noted in <ref> [PTK85, Bas92] </ref> however, optical flow computation is an inherently ill-posed problem and cannot be solved without additional assumptions on scene structure and camera placement. In Section 3.5 we used the assumption of monotonicity to enable a solution sufficient for view synthesis. <p> A variety of techniques exist for computing optical flow fields C ij (see [BFB94] for a survey and comparison). While it has been shown that optical flow estimates can be directly measured only at brightness discontinuities <ref> [Mar82, PTK85] </ref>, smoothness conditions can be imposed to derive dense flow fields. Imposing smoothness is problematic, however, as it tends to produce poor results at motion discontinuities and occlusion boundaries. <p> Furthermore, suppose all sources of error could be ignored, including those due to camera calibration, image quantization, and noise. While we could not hope to measure the flow-fields corresponding to the true scene <ref> [PTK85] </ref>, we might reasonably expect to compute a consistent set of flow fields, i.e., one corresponding to some scene S 0 that appears identical to S from the basis viewpoints V 0 ; : : : ; V n . <p> Importantly, this result required neither pixel correspondence nor known camera positions. It therefore demonstrates that the view synthesis problem is well-posed under monotonicity. In contrast, the shape reconstruction problem is known to be ill-posed under the same conditions <ref> [PTK85] </ref>. 105 * Boundary flow. View synthesis requires computing correspondence information from the two basis views. However, the true optical flow field is not measurable from the two basis views [PTK85]. <p> In contrast, the shape reconstruction problem is known to be ill-posed under the same conditions <ref> [PTK85] </ref>. 105 * Boundary flow. View synthesis requires computing correspondence information from the two basis views. However, the true optical flow field is not measurable from the two basis views [PTK85]. An important contribution of this thesis was introducing a new type of flow field called boundary flow that is both measurable and sufficient for view synthesis. * The voxel coloring framework.
Reference: [Rob97] <author> Luc Robert. </author> <title> Realistic scene models from image sequences. </title> <booktitle> In Proc. Imagina 97 Conf., </booktitle> <pages> pages 8-13, Monaco, </pages> <year> 1997. </year>
Reference: [RP94] <author> Matthew Regan and Ronald Post. </author> <title> Priority rendering with a virtual reality address recalculation pipeline. </title> <booktitle> In Proc. SIGGRAPH 94, </booktitle> <pages> pages 155-162, </pages> <year> 1994. </year>
Reference-contexts: This observation led to the development of novel hardware systems that use view synthesis techniques to achieve real-time rendering rates for synthetic 3D scenes <ref> [RP94, TK96, LS97] </ref>. Laveau and Faugeras [LF94] were among the first to develop view synthesis techniques that operate on photographs and therefore apply to real scenes.
Reference: [RZFH95] <author> Luc Robert, Cyril Zeller, Olivier Faugeras, and Martial Hebert. </author> <title> Applications of non-metric vision to some visually guided robotics tasks. </title> <type> Technical Report 2584, </type> <institution> INRIA, Sophia-Antipolis, France, </institution> <month> June </month> <year> 1995. </year>
Reference: [Sam84] <author> H. Samet. </author> <title> The quadtree and related hierarchical data structures. </title> <journal> ACM Computing Surveys, </journal> <volume> 16 </volume> <pages> 187-260, </pages> <year> 1984. </year>
Reference-contexts: Accuracy and run-time also depend on the voxel resolution, a parameter that can be set by the user or determined automatically to match the pixel resolution, calibration accuracy, and computational resources. An extension would be to use hierarchical representations like octrees <ref> [Sam84, Sze93] </ref> in which the voxel resolution is locally adapted to match surface complexity. Maintaining the strict voxel visitation order within a hierarchical framework may be difficult, however, and is beyond the scope of this thesis.
Reference: [SCG97] <author> Peter-Pike Sloan, Michael F. Cohen, and Steven J. Gortler. </author> <title> Time critical lumigraph rendering. </title> <booktitle> In Proc. Symposium on Interactive 3D Graphics, </booktitle> <pages> pages 17-23, </pages> <year> 1997. </year> <month> 116 </month>
Reference-contexts: Existing image-based representations were designed primarily for view synthesis, not for editing, and some cannot be easily extended to support editing operations. For instance, a recent direction in image-based view synthesis has been toward ray-based plenoptic representations, in particular the light field [LH96] and Lumigraph <ref> [GGSC96, SCG97] </ref>, in which light rays are represented separately. An advantage of these models is that views can be synthesized without having to compute a dense pixel correspondence. Performing image editing operations within a ray-based representation is difficult, however, for two reasons.
Reference: [Sch96] <author> Daniel Scharstein. </author> <title> Stereo vision for view synthesis. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 852-858, </pages> <year> 1996. </year>
Reference-contexts: The problems of acquiring the representation and synthesizing views are both achieved via simple sampling operations. While elegant and relatively simple to construct, these approaches require many more input views compared to correspondence-based techniques. Most closely-related to the approach in this chapter are the so called view interpolation methods <ref> [CW93, LF94, MB95b, WHH95, BP96, Sch96, AS97] </ref>, which use image warping to produce new views from a small number of basis views. Most of these methods require advance knowledge of camera parameters to produce correct perspective views, with the exceptions of [LF94, AS97]. <p> Lambertian scenes, this problem reduces to that of computing the optical flow fields C si : I s ) I i and using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo <ref> [OLC93, KRN97, Sch96] </ref> p feature tracking [TK92, BTZ96] p contour tracking [CB92, SF95] p volume intersection [MKKJ96, MTG97] p p EPI analysis [BBM87, KTOT95] p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. <p> Other researchers have used coarse-to-fine schemes [BA83] to enable the recovery of flow fields with larger image displacements. 6.1.2 Stereo Because of its relatively high accuracy, stereo is by far the most popular correspondence technique for view synthesis practitioners <ref> [OLC93, KRN97, WHH95, SD95b, MB95b, Sch96, DTM96] </ref>. However, problems with occlusions, widely separated views, and the need for specialized camera rigs make it impractical for many applications. Stereo represents an adaptation of optical flow to the problem of matching two calibrated views of the same rigid scene. <p> This scene can either be reprojected to synthesize new views as in [SD97b], or used to compute correspondence maps for image-warping methods such as <ref> [CW93, LF94, MB95b, SD96c, Sch96] </ref>. The approach has the unique feature that it guarantees a consistent scene and set of flow fields when all assumptions are met.
Reference: [SD88] <author> C. V. Stewart and C. R. Dyer. </author> <title> The trinocular general support algorithm: a three-camera stereo algorithm for overcoming binocular matching errors. </title> <booktitle> In Proc. Second Int. Conf. on Computer Vision, </booktitle> <pages> pages 134-138, </pages> <year> 1988. </year>
Reference-contexts: Recently, a number of researchers have devised techniques to improve the performance of binocular stereo in the presence of occlusion, e.g., using Bayesian techniques [BM92, GLY92] and masks [NMSO96] to detect half-occluded regions. Trinocular <ref> [SD88, AL91] </ref> and multiple-baseline stereo methods [OK85b] enable larger cumulative baselines and have been shown to improve matching accuracy. Still, individual views must generally be close together. Another approach for integrating disparate views is to arrange cameras in clusters and to compute stereo reconstructions from each cluster [KRN97].
Reference: [SD95a] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> Complete structure from four point correspondences. </title> <booktitle> In Proc. Fifth Int. Conf. on Computer Vision, </booktitle> <pages> pages 330-337, </pages> <year> 1995. </year>
Reference-contexts: Solving the last problem requires combinatorial algorithms [Cox93] to keep 63 track of all possible associations over a long image sequence. The search can be simplified by using heuristic pruning techniques [Cox93] or by imposing the assumption of a rigid scene <ref> [BCZ93, SD95a] </ref>. 6.1.4 Contour Tracking and Volume Intersection Moezzi et al. [MKKJ96, MTG97] applied a volume intersection technique to derive texture-mapped models for the purpose of view synthesis, using background subtraction to obtain silhouettes. <p> To our knowledge, scene space methods have not been used previously for view synthesis. However, they are similar in spirit to the method presented in this chapter and are consequently mentioned here as related work. Seitz and Dyer <ref> [SD95a] </ref> proposed an approach that uses the correspondences of four point features in a sequence of views to determine the correspondences of all other features, using an assumption of scene rigidity. Each point or line feature in an image "votes" for the scene subspace that projects to that feature. <p> Correspondences are identified by modeling the statistical likelihood of accidental accumulation and thresholding the votes to achieve a desired false positive rate. As with <ref> [SD95a] </ref>, occlusions are not explicitly modeled in 65 the Space-Sweep approach. Zitnick and Webb [ZW96] described a scene space stereo technique that reconstructs scene regions that project unoccluded to the basis images.
Reference: [SD95b] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> Physically-valid view synthesis by image interpolation. </title> <booktitle> In Proc. IEEE Workshop on Representation of Visual Scenes, </booktitle> <pages> pages 18-25, </pages> <year> 1995. </year>
Reference-contexts: The last three years has seen an explosion in interest in view synthesis techniques and applications. Concurrent with the work in this thesis <ref> [SD95b, SD96c, SD96b, Sei97, SD97d, SD97a, SD97b, SD97c] </ref>, numerous other researchers [MP95, MP97, WHH95, IAH95, KNR95, SK95, KAI + 95, MB95a, MB95b, Che95, KTOT95, Sze96, MKKJ96, BP96, Sch96, VP96, DTM96, GGSC96, LH96, AS97, Rob97, HiAA97, SS97, KRN97] have developed other view synthesis techniques. <p> Other researchers have used coarse-to-fine schemes [BA83] to enable the recovery of flow fields with larger image displacements. 6.1.2 Stereo Because of its relatively high accuracy, stereo is by far the most popular correspondence technique for view synthesis practitioners <ref> [OLC93, KRN97, WHH95, SD95b, MB95b, Sch96, DTM96] </ref>. However, problems with occlusions, widely separated views, and the need for specialized camera rigs make it impractical for many applications. Stereo represents an adaptation of optical flow to the problem of matching two calibrated views of the same rigid scene.
Reference: [SD96a] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> Scene appearance representation by perspective view synthesis. </title> <type> Technical Report CS-TR-1298, </type> <institution> University of Wisconsin, Madison, WI, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: As noted in Section 3.5, a monotonic range may span no more than a single aspect of an aspect graph <ref> [SD96a] </ref>, thus limiting the range of views that may be predicted. In principle, a large range of views could be predicted by partitioning view space into monotonic ranges and using a different set of views to represent each range.
Reference: [SD96b] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> Toward image-based scene representation using view morphing. </title> <booktitle> In Proc. Int. Conf. on Pattern Recognition, </booktitle> <pages> pages 84-89, </pages> <year> 1996. </year>
Reference-contexts: The last three years has seen an explosion in interest in view synthesis techniques and applications. Concurrent with the work in this thesis <ref> [SD95b, SD96c, SD96b, Sei97, SD97d, SD97a, SD97b, SD97c] </ref>, numerous other researchers [MP95, MP97, WHH95, IAH95, KNR95, SK95, KAI + 95, MB95a, MB95b, Che95, KTOT95, Sze96, MKKJ96, BP96, Sch96, VP96, DTM96, GGSC96, LH96, AS97, Rob97, HiAA97, SS97, KRN97] have developed other view synthesis techniques.
Reference: [SD96c] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> View morphing. </title> <booktitle> In Proc. </booktitle> <volume> SIG-GRAPH 96, </volume> <pages> pages 21-30, </pages> <year> 1996. </year>
Reference-contexts: The last three years has seen an explosion in interest in view synthesis techniques and applications. Concurrent with the work in this thesis <ref> [SD95b, SD96c, SD96b, Sei97, SD97d, SD97a, SD97b, SD97c] </ref>, numerous other researchers [MP95, MP97, WHH95, IAH95, KNR95, SK95, KAI + 95, MB95a, MB95b, Che95, KTOT95, Sze96, MKKJ96, BP96, Sch96, VP96, DTM96, GGSC96, LH96, AS97, Rob97, HiAA97, SS97, KRN97] have developed other view synthesis techniques. <p> Bottom: view morph yields a more realistic transition that preserves the underlying 3D geometry. 51 also interesting in its own right and has immediate applications of practical importance such as teleconferencing [TP94], face recognition [Sei97, SY97], stereo display [MB95a], and special effects <ref> [SD96c] </ref>. A main contribution was the uniqueness result for in-between views, which proved that all views of a scene between two cameras are uniquely determined from two basis images. Furthermore, these in-between views can be synthesized without 3D shape information, dense pixel correspondence, or knowledge of camera positions. <p> This scene can either be reprojected to synthesize new views as in [SD97b], or used to compute correspondence maps for image-warping methods such as <ref> [CW93, LF94, MB95b, SD96c, Sch96] </ref>. The approach has the unique feature that it guarantees a consistent scene and set of flow fields when all assumptions are met.
Reference: [SD97a] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> Photorealistic scene reconstruction by voxel coloring. </title> <booktitle> In Proc. Image Understanding Workshop, </booktitle> <pages> pages 1067-1073, </pages> <year> 1997. </year>
Reference-contexts: The last three years has seen an explosion in interest in view synthesis techniques and applications. Concurrent with the work in this thesis <ref> [SD95b, SD96c, SD96b, Sei97, SD97d, SD97a, SD97b, SD97c] </ref>, numerous other researchers [MP95, MP97, WHH95, IAH95, KNR95, SK95, KAI + 95, MB95a, MB95b, Che95, KTOT95, Sze96, MKKJ96, BP96, Sch96, VP96, DTM96, GGSC96, LH96, AS97, Rob97, HiAA97, SS97, KRN97] have developed other view synthesis techniques.
Reference: [SD97b] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> Photorealistic scene reconstruction by voxel coloring. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf., </booktitle> <pages> pages 1067-1073, </pages> <year> 1997. </year>
Reference-contexts: The last three years has seen an explosion in interest in view synthesis techniques and applications. Concurrent with the work in this thesis <ref> [SD95b, SD96c, SD96b, Sei97, SD97d, SD97a, SD97b, SD97c] </ref>, numerous other researchers [MP95, MP97, WHH95, IAH95, KNR95, SK95, KAI + 95, MB95a, MB95b, Che95, KTOT95, Sze96, MKKJ96, BP96, Sch96, VP96, DTM96, GGSC96, LH96, AS97, Rob97, HiAA97, SS97, KRN97] have developed other view synthesis techniques. <p> (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo [OLC93, KRN97, Sch96] p feature tracking [TK92, BTZ96] p contour tracking [CB92, SF95] p volume intersection [MKKJ96, MTG97] p p EPI analysis [BBM87, KTOT95] p p Space-Sweep [Col96] p p voxel coloring <ref> [SD97b] </ref> p p p Table 6.1: Comparison of Correspondence Approaches. These approaches are evaluated with respect to their use for producing pixel correspondence maps under various conditions. <p> Rather than solve for the n 2 flow fields C ij directly, we instead reconstruct a colored, voxel-based 3D scene that is consistent with all of the basis views. This scene can either be reprojected to synthesize new views as in <ref> [SD97b] </ref>, or used to compute correspondence maps for image-warping methods such as [CW93, LF94, MB95b, SD96c, Sch96]. The approach has the unique feature that it guarantees a consistent scene and set of flow fields when all assumptions are met.
Reference: [SD97c] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> View-invariant analysis of cyclic motion. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 25(3), </volume> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: The last three years has seen an explosion in interest in view synthesis techniques and applications. Concurrent with the work in this thesis <ref> [SD95b, SD96c, SD96b, Sei97, SD97d, SD97a, SD97b, SD97c] </ref>, numerous other researchers [MP95, MP97, WHH95, IAH95, KNR95, SK95, KAI + 95, MB95a, MB95b, Che95, KTOT95, Sze96, MKKJ96, BP96, Sch96, VP96, DTM96, GGSC96, LH96, AS97, Rob97, HiAA97, SS97, KRN97] have developed other view synthesis techniques.
Reference: [SD97d] <author> Steven M. Seitz and Charles R. Dyer. </author> <title> View morphing: Uniquely predicting scene appearance from basis images. </title> <booktitle> In Proc. Image Understanding Workshop, </booktitle> <pages> pages 881-887, </pages> <year> 1997. </year>
Reference-contexts: The last three years has seen an explosion in interest in view synthesis techniques and applications. Concurrent with the work in this thesis <ref> [SD95b, SD96c, SD96b, Sei97, SD97d, SD97a, SD97b, SD97c] </ref>, numerous other researchers [MP95, MP97, WHH95, IAH95, KNR95, SK95, KAI + 95, MB95a, MB95b, Che95, KTOT95, Sze96, MKKJ96, BP96, Sch96, VP96, DTM96, GGSC96, LH96, AS97, Rob97, HiAA97, SS97, KRN97] have developed other view synthesis techniques.
Reference: [Sei97] <author> Steven M. Seitz. </author> <title> Bringing photographs to life with view morphing. </title> <booktitle> In Proc. Imagina 97 Conf., </booktitle> <pages> pages 153-158, Monaco, </pages> <year> 1997. </year>
Reference-contexts: The last three years has seen an explosion in interest in view synthesis techniques and applications. Concurrent with the work in this thesis <ref> [SD95b, SD96c, SD96b, Sei97, SD97d, SD97a, SD97b, SD97c] </ref>, numerous other researchers [MP95, MP97, WHH95, IAH95, KNR95, SK95, KAI + 95, MB95a, MB95b, Che95, KTOT95, Sze96, MKKJ96, BP96, Sch96, VP96, DTM96, GGSC96, LH96, AS97, Rob97, HiAA97, SS97, KRN97] have developed other view synthesis techniques. <p> Bottom: view morph yields a more realistic transition that preserves the underlying 3D geometry. 51 also interesting in its own right and has immediate applications of practical importance such as teleconferencing [TP94], face recognition <ref> [Sei97, SY97] </ref>, stereo display [MB95a], and special effects [SD96c]. A main contribution was the uniqueness result for in-between views, which proved that all views of a scene between two cameras are uniquely determined from two basis images.
Reference: [SF95] <author> W. Brent Seales and Olivier D. Faugeras. </author> <title> Building three-dimensional object models from image sequences. CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> 3(61) </volume> <pages> 308-324, </pages> <year> 1995. </year> <month> 117 </month>
Reference-contexts: flow fields C si : I s ) I i and using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo [OLC93, KRN97, Sch96] p feature tracking [TK92, BTZ96] p contour tracking <ref> [CB92, SF95] </ref> p volume intersection [MKKJ96, MTG97] p p EPI analysis [BBM87, KTOT95] p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. These approaches are evaluated with respect to their use for producing pixel correspondence maps under various conditions. <p> These approaches can be characterized by whether or not they use temporal derivatives of the contour. While temporal derivatives provide useful shape and correspondence information <ref> [CB92, SF95] </ref>, computation of temporal derivatives requires closely space views. Alternatively, surface shape can be inferred directly from the silhouette using volume intersection [MA91, Sze93] and other direct approaches [KD95a, Kut97].
Reference: [Sha94] <author> Amnon Shashua. </author> <title> Projective structure from uncalibrated images: Structure from motion and recognition. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 16(8) </volume> <pages> 778-790, </pages> <year> 1994. </year>
Reference-contexts: However, as noted in Chapter 5, the additional images introduce a whole range of new problems, like occlusion, calibration, correspondence, and representational issues. Whereas the two-image problem has been thoroughly studied in computer vision, theories of multi-image projective geometry, calibration, and correspondence have only recently begun to emerge <ref> [Sha94, Har94, LV94, Tri95, FM95, HA95] </ref>.
Reference: [Sho85] <author> Ken Shoemake. </author> <title> Animating rotation with quaternion curves. </title> <booktitle> In Proc. SIGGRAPH 85, </booktitle> <pages> pages 245-254, </pages> <year> 1985. </year>
Reference-contexts: the image plane normals are denoted by 3D unit vectors N 0 and N 1 , the axis D and angle of rotation are given by = cos 1 (N 0 N 1 ) Alternatively, if the orientations are expressed using quaternions, the interpolation is computed by spherical linear interpolation <ref> [Sho85] </ref>. In either case, camera parameters such as focal length and aspect ratio should be interpolated separately. 3.10 Uncalibrated View Morphing So far, we have assumed that the Euclidean camera positions for the two basis views and the synthesized view (s) are known.
Reference: [SI92] <author> A. Samal and P. Iyengar. </author> <title> Automatic recognition and analysis of human faces and facial expressions: A survey. </title> <journal> Pattern Recognition, </journal> <volume> 25(5) </volume> <pages> 65-77, </pages> <year> 1992. </year>
Reference-contexts: These capabilities would add increased flexibility in the image acquisition process and provide the photographer with a powerful new set of image post-processing tools. Photo correction also has potential uses in face recognition and image database applications. Most face recognition techniques <ref> [SI92, CWS95] </ref> use 2D pattern recognition methods to correlate an observed face with a database of reference images of known faces. In order to maintain low error rates, image acquisition must be carefully controlled to ensure that the facial pose is the same (e.g., frontal) in all views.
Reference: [SK95] <author> Richard Szeliski and Sing Bing Kang. </author> <title> Direct methods for visual scene reconstruction. </title> <booktitle> In Proc. IEEE Workshop on Representation of Visual Scenes, </booktitle> <year> 1995. </year> <note> To appear. </note>
Reference: [SK98] <author> Steven M. Seitz and Kiriakos N. Kutulakos. </author> <title> Plenoptic image editing. </title> <booktitle> In Proc. Sixth Int. Conf. on Computer Vision, </booktitle> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: More recently, there has been increasing attention in modeling reflectance and illumination via analysis of basis images <ref> [War92, BK96, DNvGK97, SWI97, SK98] </ref>. However, the problems of image-based editing and animation have yet to be studied. This chapter addresses the editing problem and describes several 3D editing operations that can be performed on photographs. <p> Second, the lack of correspondence information makes it difficult to propagate editing operations between images. Edit propagation therefore calls for a richer representation of the plenoptic function that encodes correspondence information. In <ref> [SK98] </ref>, we describe such a representation, 101 (e) (f ) (g) (h) a dinosaur toy. (b)-(d) show image painting, scissoring, and morphing operations, respectively, applied to image (a). (f)-(h) show images that were automatically generated by propagating the respective editing operations to image (e). <p> Illumination synthesis would then require varying the illumination in the basis images and reconstructing the reflectance function for each voxel. This idea is beyond the scope of this thesis, but is investigated further in <ref> [SK98] </ref>. Another application of interest is to apply view synthesis methods for large-scale environments, to provide walk-throughs of buildings such as museums, or visual exploration of entire cities. The voxel coloring approach is particularly suited for this type of application, based on its generality and ability to provide panoramic visualizations.
Reference: [SS97] <author> Richard Szeliski and Heung-Yeung Shum. </author> <title> Creating full view panoramic image mosaics and environment maps. </title> <booktitle> In Proc. SIGGRAPH 97, </booktitle> <pages> pages 251-258, </pages> <year> 1997. </year>
Reference-contexts: Image reprojection has been used previously in a number of applications [Wol90]. Our use of reprojection is most closely related to the techniques used for rectifying stereo views to simplify 3D shape reconstruction [Fau93]. Image mosaic techniques <ref> [Gre86, MB95b, Che95, Sze96, SS97, KAI + 95, MP97] </ref> also rely heavily on reprojection methods to project images onto a planar, cylindrical, or spherical manifold.
Reference: [SWI97] <author> Yoichi Sato, Mark D. Wheeler, and Katsushi Ikeuchi. </author> <title> Object shape and reflectance modeling from observation. </title> <booktitle> In Proc. SIGGRAPH 97, </booktitle> <pages> pages 379-387, </pages> <year> 1997. </year>
Reference-contexts: More recently, there has been increasing attention in modeling reflectance and illumination via analysis of basis images <ref> [War92, BK96, DNvGK97, SWI97, SK98] </ref>. However, the problems of image-based editing and animation have yet to be studied. This chapter addresses the editing problem and describes several 3D editing operations that can be performed on photographs.
Reference: [SY97] <author> W. Brent Seales and Cheng Jiun Yuan. </author> <title> Recognition using morphing. </title> <note> In preparation, </note> <year> 1997. </year>
Reference-contexts: Bottom: view morph yields a more realistic transition that preserves the underlying 3D geometry. 51 also interesting in its own right and has immediate applications of practical importance such as teleconferencing [TP94], face recognition <ref> [Sei97, SY97] </ref>, stereo display [MB95a], and special effects [SD96c]. A main contribution was the uniqueness result for in-between views, which proved that all views of a scene between two cameras are uniquely determined from two basis images. <p> These artifacts are relatively minor, however; the synthesized images clearly preserve the overall shape, color, and features of the original face and are easily recognizable to the human eye. Preliminary results by Seales and Yuan <ref> [SY97] </ref> indicate significant improvements in recognition accuracy when this approach is used to bootstrap 2D eigenface techniques [TP91]. 55 Chapter 5 N-View Morphing Chapter 3 focused on image synthesis from exactly two basis views.
Reference: [SZB95] <author> Larry S. Shapiro, Andrew Zisserman, and Michael Brady. </author> <title> 3D motion recovery via affine epipolar geometry. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 16 </volume> <pages> 147-182, </pages> <year> 1995. </year>
Reference: [Sze93] <author> Richard Szeliski. </author> <title> Rapid octree construction from image sequences. Computer Vision, Graphics, </title> <booktitle> and Image Processing: Image Understanding, </booktitle> <volume> 1(58) </volume> <pages> 23-32, </pages> <year> 1993. </year>
Reference-contexts: These approaches can be characterized by whether or not they use temporal derivatives of the contour. While temporal derivatives provide useful shape and correspondence information [CB92, SF95], computation of temporal derivatives requires closely space views. Alternatively, surface shape can be inferred directly from the silhouette using volume intersection <ref> [MA91, Sze93] </ref> and other direct approaches [KD95a, Kut97]. A fundamental shortcoming of contour-based methods is that they fail at concavities and compute only the visual hull [Lau95, KD95b], an approximation of the true shape. <p> Accuracy and run-time also depend on the voxel resolution, a parameter that can be set by the user or determined automatically to match the pixel resolution, calibration accuracy, and computational resources. An extension would be to use hierarchical representations like octrees <ref> [Sam84, Sze93] </ref> in which the voxel resolution is locally adapted to match surface complexity. Maintaining the strict voxel visitation order within a hierarchical framework may be difficult, however, and is beyond the scope of this thesis. <p> Calibrated images were captured with the aid of a computer-controlled pan-tilt head and a fixed overhead camera, as shown in Fig. 6.8. This strategy is similar to that in <ref> [Sze93] </ref>. An object was placed on the head and rotated 360 degrees in front of a color camera (Sony XC-999 with 1/2" CCD sensor and 12mm, F1.4 lens) positioned approximately 30cm horizontally from the object's center and 25cm vertically above it's base. <p> In (b), the frequency is so low that the quantized texture pattern is uniform. Consequently, the problem reduces to reconstruction from silhouettes and the result is similar to what would be obtained by volume intersection <ref> [MA91, Sze93, Lau95] </ref>. Specifically, volume intersection would yield a closed diamond-shaped region; the reconstructed V-shaped cusp surface in (b) corresponds to the set of surfaces of this diamond that are visible from the basis views. Doubling results in a slightly better reconstruction consisting of two cusps, as shown in (c).
Reference: [Sze96] <author> Richard Szeliski. </author> <title> Video mosaics for virtual environments. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 16(2) </volume> <pages> 22-30, </pages> <year> 1996. </year>
Reference-contexts: Indeed, a great deal of recent work <ref> [TK92, BTZ96, Sze96, MKKJ96, KRN97] </ref> has been devoted to the problem of obtaining high-quality texture-mapped 3D models from two or more basis images, and significant progress has been made in this area. <p> Image reprojection has been used previously in a number of applications [Wol90]. Our use of reprojection is most closely related to the techniques used for rectifying stereo views to simplify 3D shape reconstruction [Fau93]. Image mosaic techniques <ref> [Gre86, MB95b, Che95, Sze96, SS97, KAI + 95, MP97] </ref> also rely heavily on reprojection methods to project images onto a planar, cylindrical, or spherical manifold.
Reference: [TK92] <author> Carlo Tomasi and Takeo Kanade. </author> <title> Shape and motion from image streams under orthography: A factorization method. </title> <journal> Int. J. of Computer Vision, </journal> <volume> 9(2) </volume> <pages> 137-154, </pages> <year> 1992. </year>
Reference-contexts: Indeed, a great deal of recent work <ref> [TK92, BTZ96, Sze96, MKKJ96, KRN97] </ref> has been devoted to the problem of obtaining high-quality texture-mapped 3D models from two or more basis images, and significant progress has been made in this area. <p> Laveau and Faugeras [LF94] were among the first to develop view synthesis techniques that operate on photographs and therefore apply to real scenes. While others <ref> [TK92, OLC93] </ref> had devised methods for synthesizing views from known (a priori or derived) depth maps and camera positions, they were the first to demonstrate the feasibility of view synthesis directly from uncalibrated images and correspondence maps. <p> that of computing the optical flow fields C si : I s ) I i and using the relation color (p; I s ) = color (C si (p); I i ) 60 occlusion large view separation surface concavity optical flow [AS97] p stereo [OLC93, KRN97, Sch96] p feature tracking <ref> [TK92, BTZ96] </ref> p contour tracking [CB92, SF95] p volume intersection [MKKJ96, MTG97] p p EPI analysis [BBM87, KTOT95] p p Space-Sweep [Col96] p p voxel coloring [SD97b] p p p Table 6.1: Comparison of Correspondence Approaches. <p> Integration of noisier stereo-derived models is significantly more difficult, if fine detail is to be preserved. 6.1.3 Feature Tracking Several researchers have applied feature tracking techniques in association with shape-from-motion methods to recover high quality texture-mapped 3D models <ref> [TK92, BTZ96] </ref>, suitable for view synthesis. However, because feature tracking does not yield dense correspondence fields, accuracy is ensured only in places where features have been detected and reliably tracked.
Reference: [TK96] <author> Jay Torborg and James T. Kajiya. Talisman: </author> <title> Commodity realtime 3D graphics for the PC. </title> <booktitle> In Proc. SIGGRAPH 96, </booktitle> <pages> pages 353-363, </pages> <year> 1996. </year> <month> 118 </month>
Reference-contexts: Recently, researchers in computer vision and computer graphics have advocated this layering paradigm as an effective way for representing and rendering more complicated 3D scenes by manually <ref> [TK96, LS97] </ref> or automatically [WA94, DP91, AS95] segmenting the scene into a series of layers. In the last few years, computer-based image metamorphosis or morphing [Wol90, BN92] has emerged as a popular means for producing fascinating visual effects. <p> This observation led to the development of novel hardware systems that use view synthesis techniques to achieve real-time rendering rates for synthetic 3D scenes <ref> [RP94, TK96, LS97] </ref>. Laveau and Faugeras [LF94] were among the first to develop view synthesis techniques that operate on photographs and therefore apply to real scenes.
Reference: [TL94] <author> Greg Turk and Marc Levoy. </author> <title> Zippered polygon meshes from range images. </title> <booktitle> In Proc. SIGGRAPH 94, </booktitle> <pages> pages 311-318, </pages> <year> 1994. </year>
Reference-contexts: Still, individual views must generally be close together. Another approach for integrating disparate views is to arrange cameras in clusters and to compute stereo reconstructions from each cluster [KRN97]. The resulting partial models can be subsequently merged <ref> [TL94, CL96] </ref> to form a more complete reconstruction. This split and merge approach has proven most successful when the partial reconstructions to be merged are individually quite accurate, e.g., as obtainable by high-end laser range scanners.
Reference: [TP91] <author> Matthew Turk and Alex Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> J. of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: Preliminary results by Seales and Yuan [SY97] indicate significant improvements in recognition accuracy when this approach is used to bootstrap 2D eigenface techniques <ref> [TP91] </ref>. 55 Chapter 5 N-View Morphing Chapter 3 focused on image synthesis from exactly two basis views. The two-view algorithm can in fact be used to compute interpolations between three or more views by reducing the problem to a series of two-view interpolations.
Reference: [TP94] <author> Sebastian Toelg and Tomaso Poggio. </author> <title> Towards an example-based image compression architecture for video conferencing. </title> <journal> A.I. </journal> <volume> Memo No. 1494, </volume> <publisher> M.I.T., </publisher> <address> Cambridge, MA, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Holes can appear in view morphs as well. unnatural horizontal and vertical contraction of the face and torso. Bottom: view morph yields a more realistic transition that preserves the underlying 3D geometry. 51 also interesting in its own right and has immediate applications of practical importance such as teleconferencing <ref> [TP94] </ref>, face recognition [Sei97, SY97], stereo display [MB95a], and special effects [SD96c]. A main contribution was the uniqueness result for in-between views, which proved that all views of a scene between two cameras are uniquely determined from two basis images.
Reference: [Tri95] <author> Bill Triggs. </author> <title> Matching constraints and the joint image. </title> <booktitle> In Proc. 5th Int. Conf. on Computer Vision, </booktitle> <pages> pages 338-343, </pages> <year> 1995. </year>
Reference-contexts: However, as noted in Chapter 5, the additional images introduce a whole range of new problems, like occlusion, calibration, correspondence, and representational issues. Whereas the two-image problem has been thoroughly studied in computer vision, theories of multi-image projective geometry, calibration, and correspondence have only recently begun to emerge <ref> [Sha94, Har94, LV94, Tri95, FM95, HA95] </ref>.
Reference: [Tsa87] <author> R. Y. Tsai. </author> <title> A versatile camera calibration technique for high-accuracy 3D machine vision metrology using off-the-shelf cameras and lenses. </title> <journal> IEEE Trans. Robotics and Automation, </journal> <volume> 3(4) </volume> <pages> 323-344, </pages> <year> 1987. </year>
Reference-contexts: An object was placed on the head and rotated 360 degrees in front of a color camera (Sony XC-999 with 1/2" CCD sensor and 12mm, F1.4 lens) positioned approximately 30cm horizontally from the object's center and 25cm vertically above it's base. Tsai's method <ref> [Tsa87] </ref> was used to calibrate the camera with respect to the head, by rotating a known object and manually selecting image features for three pan positions. The calibration error was approximately 3%.
Reference: [UB91] <author> Shimon Ullman and Ronen Basri. </author> <title> Recognition by linear combinations of models. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 992-1006, </pages> <year> 1991. </year>
Reference: [VP89] <author> Alessandro Verri and Tomaso Poggio. </author> <title> Motion field and optical flow: Qualitative properties. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 11(5) </volume> <pages> 490-498, </pages> <year> 1989. </year>
Reference-contexts: Furthermore, all of these methods require 18 dense pixel correspondence maps as input. This latter requirement is a serious limitation, given that image-based computation of correct correspondence maps is known to be an ill-posed problem <ref> [PTK85, VP89] </ref>. In contrast, this chapter presents a view interpolation technique that operates directly on a pair of images, without knowledge of camera parameters or dense correspondence. It has the unique property of requiring only measurable image information, thus establishing that view interpolation is a well-posed problem. <p> In particular, the flow within low-contrast regions of nearly uniform color is locally ambiguous, a condition known as the aperture problem [Mar82]. As a result, complete optical flow is generally not measurable from two images <ref> [PTK85, VP89] </ref>. In response to the aperture problem many researchers instead use normal flow [Hil83], which provides only the motion component in the gradient direction but has the advantage of being measurable. <p> In particular, two views are sufficient to model appearance for the continuum of in-between views. In contrast, shape reconstruction is not generally possible from two views, due to ambiguities in the correspondence problem <ref> [Mar82, PTK85, VP89] </ref>. A second contribution was the view morphing algorithm for synthesizing high-quality in-between views by warping two basis images. While the uniqueness result demonstrates that in-between views are theoretically determined, this algorithm provides a practical method for synthesizing high-quality in-between views from two basis images.
Reference: [VP96] <author> Thomas Vetter and Tomaso Poggio. </author> <title> Image synthesis from a single example image. </title> <booktitle> In Proc. European Conf. on Computer Vision, </booktitle> <pages> pages 653-659, </pages> <year> 1996. </year>
Reference-contexts: Poggio and Brunelli [PB92] proposed a computational learning framework for synthesizing 2D and 3D transformations from a set of basis images. Their technique, which uses hyper basis functions to learn parameterized models from images, is very general and can model a variety of interesting transformations. Later work <ref> [BP96, VP96] </ref> demonstrated the utility of this framework for the difficult case of synthesizing new views of human faces.
Reference: [WA94] <author> John Y. A. Wang and Edward H. Adelson. </author> <title> Representing moving images with layers. </title> <journal> IEEE Trans. Image Processing, </journal> <volume> 3(5) </volume> <pages> 625-638, </pages> <year> 1994. </year>
Reference-contexts: Recently, researchers in computer vision and computer graphics have advocated this layering paradigm as an effective way for representing and rendering more complicated 3D scenes by manually [TK96, LS97] or automatically <ref> [WA94, DP91, AS95] </ref> segmenting the scene into a series of layers. In the last few years, computer-based image metamorphosis or morphing [Wol90, BN92] has emerged as a popular means for producing fascinating visual effects. <p> Furthermore, most optical flow techniques use gradient methods, window correlation, or velocity tuned filters, all of which require small image displacements. We note that a number of techniques have been developed that are less susceptible to the aforementioned problems. For instance, mixture-methods and parameterized models have been proposed <ref> [WA94, DP95, BA96] </ref> to avoid smoothing over flow discontinuities.
Reference: [Wal33] <author> Walt Disney Productions. </author> <title> Three little pigs. </title> <type> film, </type> <year> 1933. </year>
Reference-contexts: some simple image transformations that are capable of producing realistic 3D effects. 1.1 Examples of Image-Based Scene Transformations In the early part of this century, Walt Disney and other pioneers developed the cel animation techniques that have been used since to generate cartoons and films like the Three Little Pigs <ref> [Wal33] </ref>, Fantasia [Wal40a], and Aladdin [Wal92]. Cel animators employed a number of methods for conveying realistic motions, using paint and transparent sheets of celluloid as the basic medium. The success and continued popularity of their efforts attests to the efficacy of 2D techniques for producing realistic visual effects.
Reference: [Wal40a] <author> Walt Disney Productions. Fantasia. film, </author> <year> 1940. </year>
Reference-contexts: image transformations that are capable of producing realistic 3D effects. 1.1 Examples of Image-Based Scene Transformations In the early part of this century, Walt Disney and other pioneers developed the cel animation techniques that have been used since to generate cartoons and films like the Three Little Pigs [Wal33], Fantasia <ref> [Wal40a] </ref>, and Aladdin [Wal92]. Cel animators employed a number of methods for conveying realistic motions, using paint and transparent sheets of celluloid as the basic medium. The success and continued popularity of their efforts attests to the efficacy of 2D techniques for producing realistic visual effects.
Reference: [Wal40b] <author> Walt Disney Productions. Pinocchio. film, </author> <year> 1940. </year>
Reference-contexts: A more compelling effect can be achieved by designing the background image with a specific camera path in mind [WFH + 97]. This technique was used in Walt Disney's 1940 animated film, Pinocchio <ref> [Wal40b] </ref>, in which a window was moved in a fixed path along a special background image. While the background appears warped when viewed in its entirety, it appears quite natural through the sliding window and the resulting animation gives a realistic 3D effect.
Reference: [Wal92] <author> Walt Disney Productions. Aladdin. film, </author> <year> 1992. </year>
Reference-contexts: are capable of producing realistic 3D effects. 1.1 Examples of Image-Based Scene Transformations In the early part of this century, Walt Disney and other pioneers developed the cel animation techniques that have been used since to generate cartoons and films like the Three Little Pigs [Wal33], Fantasia [Wal40a], and Aladdin <ref> [Wal92] </ref>. Cel animators employed a number of methods for conveying realistic motions, using paint and transparent sheets of celluloid as the basic medium. The success and continued popularity of their efforts attests to the efficacy of 2D techniques for producing realistic visual effects.
Reference: [War92] <author> Gregory J. Ward. </author> <title> Measuring and modeling anisotropic reflection. </title> <booktitle> In Proc. SIGGRAPH 92, </booktitle> <pages> pages 265-272, </pages> <year> 1992. </year>
Reference-contexts: More recently, there has been increasing attention in modeling reflectance and illumination via analysis of basis images <ref> [War92, BK96, DNvGK97, SWI97, SK98] </ref>. However, the problems of image-based editing and animation have yet to be studied. This chapter addresses the editing problem and describes several 3D editing operations that can be performed on photographs.
Reference: [Wes90] <author> Lee Westover. </author> <title> Footprint evaluation for volume rendering. </title> <booktitle> In Proc. </booktitle> <volume> SIG-GRAPH 90, </volume> <pages> pages 367-376, </pages> <year> 1990. </year>
Reference-contexts: Neglecting occlusions, it is straightforward to compute a voxel's image projection, based on voxel shape and the known camera configuration. We use the term footprint, following <ref> [Wes90] </ref> to denote this projection, corresponding to the intersection with the image plane of all rays from the camera center intersecting the voxel. Accounting for occlusions is more difficult, however, and we must take care to include only the images and pixel positions from which V should be visible. <p> For simplicity, our implementation used a square mask to approximate voxel footprints, and used Eq. (6.13) to test voxel consistency. Alternative footprint models are discussed in the volume rendering literature, e.g., <ref> [Wes90, LH91] </ref>. While our implementation did not make use of this, additional speedups are possible by exploiting the uniform discretization of space and simple layer geometry. Choosing planar or polyhedral layers enables the use of texture-mapping graphics hardware to calculate voxel footprints, an entire layer at a time.
Reference: [WFH + 97] <author> Daniel N. Wood, Adam Finkelstein, John F. Hughes, Craig E. Thayer, and David H. Salesin. </author> <title> Multiperspective panoramas for cel animation. </title> <booktitle> In Proc. SIGGRAPH 97, </booktitle> <pages> pages 243-250, </pages> <year> 1997. </year> <month> 119 </month>
Reference-contexts: While sufficient to give an impression of camera movement, this technique does not model depth effects like parallax and occlusion and therefore is somewhat unrealistic. A more compelling effect can be achieved by designing the background image with a specific camera path in mind <ref> [WFH + 97] </ref>. This technique was used in Walt Disney's 1940 animated film, Pinocchio [Wal40b], in which a window was moved in a fixed path along a special background image. <p> While the background appears warped when viewed in its entirety, it appears quite natural through the sliding window and the resulting animation gives a realistic 3D effect. Wood et al. <ref> [WFH + 97] </ref> have recently developed computer algorithms for partially automating the creation of these warped 5 backgrounds, which they call multiperspective panoramas. Another way of simulating depth effects in animation is through layering, or trucking, i.e., compositing a series of background images and windows that move at different speeds.
Reference: [WHH95] <author> Tomas Werner, Roger David Hersch, and Vaclav Hlavac. </author> <title> Rendering real-world objects using view interpolation. </title> <booktitle> In Proc. Fifth Int. Conf. on Computer Vision, </booktitle> <pages> pages 957-962, </pages> <year> 1995. </year>
Reference-contexts: The problems of acquiring the representation and synthesizing views are both achieved via simple sampling operations. While elegant and relatively simple to construct, these approaches require many more input views compared to correspondence-based techniques. Most closely-related to the approach in this chapter are the so called view interpolation methods <ref> [CW93, LF94, MB95b, WHH95, BP96, Sch96, AS97] </ref>, which use image warping to produce new views from a small number of basis views. Most of these methods require advance knowledge of camera parameters to produce correct perspective views, with the exceptions of [LF94, AS97]. <p> Other researchers have used coarse-to-fine schemes [BA83] to enable the recovery of flow fields with larger image displacements. 6.1.2 Stereo Because of its relatively high accuracy, stereo is by far the most popular correspondence technique for view synthesis practitioners <ref> [OLC93, KRN97, WHH95, SD95b, MB95b, Sch96, DTM96] </ref>. However, problems with occlusions, widely separated views, and the need for specialized camera rigs make it impractical for many applications. Stereo represents an adaptation of optical flow to the problem of matching two calibrated views of the same rigid scene.
Reference: [Wol90] <author> George Wolberg. </author> <title> Digital Image Warping. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1990. </year>
Reference-contexts: In the last few years, computer-based image metamorphosis or morphing <ref> [Wol90, BN92] </ref> has emerged as a popular means for producing fascinating visual effects. These techniques combine 2D interpolations of shape and color to produce transitions between a pair of basis images. <p> Two maps are required because the correspondence may not be one-to-one. In practice, C 01 and C 10 are partially specified by having the user provide a sparse set of matching features or regions in the two images. The remaining correspondences are determined automatically by interpolation <ref> [Wol90, BN92, LCSW92] </ref>. <p> The dashed line shows the linear path of one feature during the course of the transformation. This example is indicative of the types of distortions that can arise with image morphing techniques. <ref> [Wol90, BN92, LCSW92] </ref>. To illustrate the potentially severe 3D distortions incurred by image morphing, it is useful to consider interpolating between two different views of a planar shape. <p> The operation of reprojection is very powerful because it allows the gaze direction to be modified after a photograph is taken or a scene rendered. Our use of projective transforms to compute reprojections takes advantage of an efficient scanline algorithm <ref> [Wol90] </ref>. Reprojection can also be performed through texture-mapping and can therefore exploit current graphics hardware. Image reprojection has been used previously in a number of applications [Wol90]. Our use of reprojection is most closely related to the techniques used for rectifying stereo views to simplify 3D shape reconstruction [Fau93]. <p> Our use of projective transforms to compute reprojections takes advantage of an efficient scanline algorithm <ref> [Wol90] </ref>. Reprojection can also be performed through texture-mapping and can therefore exploit current graphics hardware. Image reprojection has been used previously in a number of applications [Wol90]. Our use of reprojection is most closely related to the techniques used for rectifying stereo views to simplify 3D shape reconstruction [Fau93]. <p> Resampling effects can be reduced by supersampling the input images <ref> [Wol90] </ref> or by composing the image transformations into one aggregate warp for each image.
Reference: [ZW96] <author> C. Lawrence Zitnick and Jon A. Webb. </author> <title> Multi-baseline stereo using surface extraction. </title> <type> Technical Report CMU-CS-96-196, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> November </month> <year> 1996. </year> <month> 120 </month>
Reference-contexts: Correspondences are identified by modeling the statistical likelihood of accidental accumulation and thresholding the votes to achieve a desired false positive rate. As with [SD95a], occlusions are not explicitly modeled in 65 the Space-Sweep approach. Zitnick and Webb <ref> [ZW96] </ref> described a scene space stereo technique that reconstructs scene regions that project unoccluded to the basis images. They noted that the correspondence problem is fundamentally ill-posed in the presence of occlusion, but can be solved when occlusion does not occur.
References-found: 139

