URL: http://www.cs.wisc.edu/wpis/papers/scp91.ps
Refering-URL: http://www.cs.wisc.edu/~reps/
Root-URL: 
Title: ALGEBRAIC PROPERTIES OF PROGRAM INTEGRATION  
Author: Thomas REPS 
Address: 1210 W. Dayton Street, Madison, WI 53706  
Affiliation: Computer Sciences Department, University of Wisconsin-Madison,  
Abstract: The need to integrate several versions of a program into a common one arises frequently, but it is a tedious and time consuming task to merge programs by hand. The program-integration algorithm proposed by Horwitz, Prins, and Reps provides a way to create a semantics-based tool for integrating a base program with two or more variants. The integration algorithm is based on the assumption that any change in the behavior, rather than the text, of a program variant is significant and must be incorporated in the merged program. An integration system based on this algorithm will determine whether the variants incorporate interfering changes, and, if they do not, create an integrated program that includes all changes as well as all features of the base program that are preserved in all variants. To determine this information, the algorithm employs a program representation that is similar to the program dependence graphs that have been used previously in vectorizing and parallelizing compilers. This paper studies the algebraic properties of the program-integration operation, such as whether there are laws of associativity and distributivity. (For example, in this context associativity means: If three variants of a given base are to be integrated by a pair of two-variant integrations, the same result is produced no matter which two variants are integrated first.) To answer such questions, we reformulate the Horwitz-Prins-Reps integration algorithm as an operation in a Brouwerian algebra constructed from sets of dependence graphs. (A Brouwerian algebra is a distributive lattice with an operation a . - b characterized by a . - b i cdidi i c iff a i cdidi i b ciic c.) In this algebra, the program-integration operation can be defined solely in terms of ciic , cddc , and . - . By making use of the rich set of algebraic laws that hold in Brouwerian algebras, we have established a number of the integration operation's algebraic properties. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> V. Berzins, </author> <title> On merging software extensions, </title> <note> Acta Informatica 23 pp. </note> <month> 607-619 </month> <year> (1986). </year>
Reference-contexts: Example. The following example illustrates the HPR algorithm. The tags on statements are noted between square brackets. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii A Base B D (A, Base ) Pre (A, Base, B) D (B, Base) A [Base ]B iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii program <ref> [1] </ref> x := 0 program [1] x := 0; end (x, y) program [1] x := 0; [3] z := y program [1] x := 0 program [1] x := 0; [3] z := y program [1] x := 0; [3] z := y end (x, z) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c <p> Example. The following example illustrates the HPR algorithm. The tags on statements are noted between square brackets. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii A Base B D (A, Base ) Pre (A, Base, B) D (B, Base) A [Base ]B iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii program <ref> [1] </ref> x := 0 program [1] x := 0; end (x, y) program [1] x := 0; [3] z := y program [1] x := 0 program [1] x := 0; [3] z := y program [1] x := 0; [3] z := y end (x, z) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c <p> The tags on statements are noted between square brackets. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii A Base B D (A, Base ) Pre (A, Base, B) D (B, Base) A [Base ]B iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii program <ref> [1] </ref> x := 0 program [1] x := 0; end (x, y) program [1] x := 0; [3] z := y program [1] x := 0 program [1] x := 0; [3] z := y program [1] x := 0; [3] z := y end (x, z) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c <p> on statements are noted between square brackets. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii A Base B D (A, Base ) Pre (A, Base, B) D (B, Base) A [Base ]B iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii program <ref> [1] </ref> x := 0 program [1] x := 0; end (x, y) program [1] x := 0; [3] z := y program [1] x := 0 program [1] x := 0; [3] z := y program [1] x := 0; [3] z := y end (x, z) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c This example illustrates one subtlety of the HPR <p> square brackets. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii A Base B D (A, Base ) Pre (A, Base, B) D (B, Base) A [Base ]B iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii program <ref> [1] </ref> x := 0 program [1] x := 0; end (x, y) program [1] x := 0; [3] z := y program [1] x := 0 program [1] x := 0; [3] z := y program [1] x := 0; [3] z := y end (x, z) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c This example illustrates one subtlety of the HPR algorithm: an insertion made in <p> Base ) Pre (A, Base, B) D (B, Base) A [Base ]B iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii program <ref> [1] </ref> x := 0 program [1] x := 0; end (x, y) program [1] x := 0; [3] z := y program [1] x := 0 program [1] x := 0; [3] z := y program [1] x := 0; [3] z := y end (x, z) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c This example illustrates one subtlety of the HPR algorithm: an insertion made in one integrand can override a deletion in the other <p> Relation to Previous Work There has been previous work on merging functional programs <ref> [1] </ref>, logic programs [15], and specifications [4]. Different models of integration have been used in each case.
Reference: 2. <editor> H.B. </editor> <booktitle> Curry, Foundations of Mathematical Logic, </booktitle> <publisher> Dover Publications, Inc., </publisher> <address> New York, NY (1977). </address>
Reference-contexts: (L, ciic , cddc ) is distributive; that is, for all a, b, and c in L, (iv) a ciic (b cddc c) = (a ciic b) cddc (a ciic c ). (v) a cddc (b ciic c) = (a cddc b) ciic (a cddc c ). (For proofs, see <ref> [2] </ref>, page 143-145.) Remark. We use the symbol . to denote the general operation of pseudo-difference in an arbitrary Brouwerian algebra as well as a specific operation in the algebra (DCS, , , . - , G 1 ).
Reference: 3. <author> V. Donzeau-Gouge, G. Huet, G. Kahn, and B. Lang, </author> <title> Programming environments based on structured editors: The MENTOR experience, pp. 128-140 in Interactive Programming Environments, </title> <editor> ed. D. Barstow, E. San-dewall, and H. Shrobe,McGraw-Hill, </editor> <address> New York, NY (1984). </address>
Reference-contexts: A tagging facility meeting these requirements can be supported by language-based editors, such as those that can be created by such systems as MENTOR <ref> [3] </ref>, GANDALF [18], and the Synthesizer Generator 6 Component tags furnish the means for identifying how the program-dependence-graph vertices in different versions correspond. It is the tags that are used to determine identical vertices when operations are performed using vertices from different program dependence graphs. <p> Remark. Except where we wish to emphasize which program components have the same tag, we do not indicate program-component tags in our examples. When we do indicate program-component tags, the tag is placed between square brackets to the left of the component (e.g., <ref> [3] </ref> z := y). For all examples in which tags are not indicated explicitly, our convention is that components in different programs that have the same text also have the same tag. <p> The tags on statements are noted between square brackets. iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii A Base B D (A, Base ) Pre (A, Base, B) D (B, Base) A [Base ]B iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii program [1] x := 0 program [1] x := 0; end (x, y) program [1] x := 0; <ref> [3] </ref> z := y program [1] x := 0 program [1] x := 0; [3] z := y program [1] x := 0; [3] z := y end (x, z) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c This example illustrates <p> A Base B D (A, Base ) Pre (A, Base, B) D (B, Base) A [Base ]B iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii program [1] x := 0 program [1] x := 0; end (x, y) program [1] x := 0; <ref> [3] </ref> z := y program [1] x := 0 program [1] x := 0; [3] z := y program [1] x := 0; [3] z := y end (x, z) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c This example illustrates one subtlety of the HPR algorithm: an insertion made in one integrand can override <p> Base, B) D (B, Base) A [Base ]B iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii program [1] x := 0 program [1] x := 0; end (x, y) program [1] x := 0; <ref> [3] </ref> z := y program [1] x := 0 program [1] x := 0; [3] z := y program [1] x := 0; [3] z := y end (x, z) iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiic c c c c c c c c c c c c c c c c This example illustrates one subtlety of the HPR algorithm: an insertion made in one integrand can override a deletion in the other integrand.
Reference: 4. <author> M.S. Feather, </author> <title> Detecting interference when merging specification evolutions, </title> <type> Unpublished report, </type> <institution> Information Sciences Institute, University of Southern California, Marina del Rey, </institution> <address> CA (1989). </address>
Reference-contexts: Relation to Previous Work There has been previous work on merging functional programs [1], logic programs [15], and specifications <ref> [4] </ref>. Different models of integration have been used in each case. In Berzins's work on hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 13 Defining a good heuristic for this program-reconstitution problem remains an open question. - 50 - integrating functional programs, variants A and B are merged without regard to Base.
Reference: 5. <author> J. Ferrante, K. Ottenstein, and J. Warren, </author> <title> The program dependence graph and its use in optimization, </title> <journal> ACM Trans. Program. Lang. Syst. </journal> <pages> 9(3) pp. </pages> <month> 319-349 (July </month> <year> 1987). </year>
Reference-contexts: To determine this information, the HPR algorithm employs a program representation that is similar to the program dependence graphs that have been used previously in vectorizing and parallelizing compilers <ref> [5, 14] </ref>. It makes use of an operation on these graphs called program slicing [19, 28] to find potentially changed computations. <p> To determine this information, the HPR algorithm employs graphs that represent the dependences between program elements. 2.1. Program Dependence Graphs and Program Slicing The program dependence graphs used by the HPR algorithm are similar to those used previously for representing programs in vectorizing and parallelizing compilers <ref> [5, 14] </ref>. Different definitions of program dependence representations have been given, depending on the intended application, but all are variations - 6 - on a theme introduced in [13] and share the common feature of having an explicit representation of data dependences (see below). <p> Different definitions of program dependence representations have been given, depending on the intended application, but all are variations - 6 - on a theme introduced in [13] and share the common feature of having an explicit representation of data dependences (see below). The program dependence graphs defined in <ref> [5] </ref> introduced the additional feature of an explicit representation for control dependences (see below). 4 Definition 2.1. A directed graph G consists of a set of vertices V (G) and a set of edges E (G). <p> data dependence edge with source v and target w means (roughly) that the program's behavior might change if the relative order of the components represented by v and w were reversed. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 4 The definition of program dependence graph given here (which is taken from [10]) differs from that of <ref> [5] </ref> in two ways. First, our definition covers only the restricted language described earlier, and hence is less general than the one given in [5]. Second, because of the particular needs of the program-integration problem, we omit certain classes of data dependence edges and define one additional class. <p> represented by v and w were reversed. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 4 The definition of program dependence graph given here (which is taken from [10]) differs from that of <ref> [5] </ref> in two ways. First, our definition covers only the restricted language described earlier, and hence is less general than the one given in [5]. Second, because of the particular needs of the program-integration problem, we omit certain classes of data dependence edges and define one additional class. However, the structures we define and those defined in [5] share the feature of explicitly representing both control and data dependences; for this reason, despite their differences, <p> covers only the restricted language described earlier, and hence is less general than the one given in <ref> [5] </ref>. Second, because of the particular needs of the program-integration problem, we omit certain classes of data dependence edges and define one additional class. However, the structures we define and those defined in [5] share the feature of explicitly representing both control and data dependences; for this reason, despite their differences, we refer to our graphs as program dependence graphs, borrowing the term from [5]. 5 A method for determining control dependence edges for arbitrary programs is given in [5]; however, because we are <p> However, the structures we define and those defined in <ref> [5] </ref> share the feature of explicitly representing both control and data dependences; for this reason, despite their differences, we refer to our graphs as program dependence graphs, borrowing the term from [5]. 5 A method for determining control dependence edges for arbitrary programs is given in [5]; however, because we are assuming that programs include only assignment, conditional, and while statements, the control dependence edges can be determined in a much simpler fashion. <p> and those defined in <ref> [5] </ref> share the feature of explicitly representing both control and data dependences; for this reason, despite their differences, we refer to our graphs as program dependence graphs, borrowing the term from [5]. 5 A method for determining control dependence edges for arbitrary programs is given in [5]; however, because we are assuming that programs include only assignment, conditional, and while statements, the control dependence edges can be determined in a much simpler fashion. For the language under consideration here, the control dependence edges essentially represent the program's nesting structure.
Reference: 6. <author> A.J. Field and P.G. Harrison, </author> <title> Functional Programming, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA (1988). </address>
Reference-contexts: If no such buffer exists, the value is ^, the least Brouw value. The editor displays a type for each expression; these types are supplied using the Milner algorithm for polymorphic type inference (algorithm W) <ref> [6, 17] </ref>. An evaluation command added to the editor invokes the interpreter on the expression, andif the final result is a Brouwbuilds the corresponding program (if one exists). 6.2.
Reference: 7. <author> C.A.R. Hoare, </author> <title> A couple of novelties in the propositional calculus, </title> <journal> Zeitschr. f. math. Logik und Grundlagen d. Math. </journal> <pages> 31 pp. </pages> <month> 173-178 </month> <year> (1985). </year>
Reference-contexts: The notation a [base ]b that has been used here for the integration operation in Brouwerian algebras is taken from a paper by Hoare in which he investigated some of the properties of a [base ]b in Boolean algebras <ref> [7, 8] </ref>. However, nearly all of the questions examined in this work (for Brouwerian algebras) were not addressed by Hoare (for Boolean algebras). In unpublished work, Susan Horwitz and I found proofs of several algebraic properties of the HPR algorithm. <p> His deft manipulations of expressions involving a [base ]b made clear the benefits of understanding the elegant algebraic laws of this ternary operator. He also pointed me to <ref> [7] </ref>, which discusses properties of a [base ]b in Boolean algebras and articulates the advantages of the notation in which the base argument appears in the middle position.
Reference: 8. <author> C.A.R. Hoare, </author> <title> A couple of novelties in the propositional calculus, </title> <note> pp. 325-331 in Essays in Computing Science, </note> <editor> ed. C.B. Jones,Prentice-Hall, </editor> <address> New York, NY (1989). </address>
Reference-contexts: The notation a [base ]b that has been used here for the integration operation in Brouwerian algebras is taken from a paper by Hoare in which he investigated some of the properties of a [base ]b in Boolean algebras <ref> [7, 8] </ref>. However, nearly all of the questions examined in this work (for Brouwerian algebras) were not addressed by Hoare (for Boolean algebras). In unpublished work, Susan Horwitz and I found proofs of several algebraic properties of the HPR algorithm.
Reference: 9. <author> S. Horwitz, P. Pfeiffer, and T. Reps, </author> <title> Dependence analysis for pointer variables, </title> <booktitle> Proceedings of the ACM SIG-PLAN 89 Conference on Programming Language Design and Implementation, </booktitle> <address> (Portland, OR, </address> <month> June 21-23, </month> <year> 1989), </year> <journal> ACM SIGPLAN Notices 24(7) pp. </journal> <month> 28-40 (July </month> <year> 1989). </year>
Reference-contexts: initial state: (1) the program contains a non terminating loop, or (2) a fault occurs, such as division by zero. - 4 - Although the capabilities of our current integration algorithms are severely limited, recent research has made progress towards extending the set of language constructs to which they apply <ref> [9, 11] </ref>. Our hope is that such extensions will share with the basic integration algorithm a common set of algebraic properties. However, we would like to avoid having to re-prove that each property holds every time we enhance our techniques.
Reference: 10. <author> S. Horwitz, J. Prins, and T. Reps, </author> <title> Integrating non-interfering versions of programs, </title> <journal> ACM Trans. Program. Lang. Syst. </journal> <pages> 11(3) pp. </pages> <month> 345-387 (July </month> <year> 1989). </year>
Reference-contexts: a set of variants of P created, say, by modifying separate copies of Pthe goal is to determine whether the modifications inter fere, and, if they do not, to create an integrated program that includes all changes as well as all features of P that are preserved in all variants <ref> [10] </ref>. Opportunities for program integration arise in many situations: (1) A system may be customized by a user while simultaneously being upgraded by a maintainer. <p> The Horwitz-Prins-Reps Algorithm for Program Integration The first algorithm that meets the requirements given above was given by Horwitz, Prins, and Reps in <ref> [10] </ref>. Thus, that algorithmreferred to hereafter as the HPR algorithmis the first algorithm for semantics-based program-integration. The HPR algorithm represents a fundamental advance over text-based program-integration algorithms, and provides the first step in the creation of a theoretical foundation for building a semantics-based program-integration tool. <p> It makes use of an operation on these graphs called program slicing [19, 28] to find potentially changed computations. The HPR algorithm is summarized in Section 2, which also presents an example of an integration. (Full detailsand a more extensive examplecan be found in <ref> [10] </ref>.) This paper concerns not the HPR algorithm, but a close relative of it. (The revised integration algorithm is presented in Section 3.) We investigate the algorithm's algebraic properties, which are of particular interest when dealing with compositions of integrations. <p> Overview of the HPR Algorithm for Program Integration This section provides an overview of the HPR algorithm, which uses program dependence graphs to integrate programs <ref> [10] </ref>. It summarizes parts of [10], which contains a comprehensive description of the HPR algorithm. This summary is presented here because it is the starting point from which our new techniques are developed. <p> Overview of the HPR Algorithm for Program Integration This section provides an overview of the HPR algorithm, which uses program dependence graphs to integrate programs <ref> [10] </ref>. It summarizes parts of [10], which contains a comprehensive description of the HPR algorithm. This summary is presented here because it is the starting point from which our new techniques are developed. <p> A data dependence edge with source v and target w means (roughly) that the program's behavior might change if the relative order of the components represented by v and w were reversed. hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 4 The definition of program dependence graph given here (which is taken from <ref> [10] </ref>) differs from that of [5] in two ways. First, our definition covers only the restricted language described earlier, and hence is less general than the one given in [5]. <p> The final step of the HPR algorithm involves reconstituting a program from the merged program dependence graph. However, it is possible that there is no such programthe merged graph can be an infeasible program dependence graph; this is Type II interference. (The reader is referred to <ref> [10] </ref> for a discussion of reconstructing a program from the merged program dependence graph and the inherent difficulties of this problem.) If neither kind of interference occurs, a program whose program dependence graph is G M is returned as the result of the integration operation. Example.
Reference: 11. <author> S. Horwitz, T. Reps, and D. Binkley, </author> <title> Interprocedural slicing using dependence graphs, </title> <journal> ACM Trans. Program. Lang. Syst. </journal> <pages> 12(1) pp. </pages> <month> 26-60 (January </month> <year> 1990). </year>
Reference-contexts: initial state: (1) the program contains a non terminating loop, or (2) a fault occurs, such as division by zero. - 4 - Although the capabilities of our current integration algorithms are severely limited, recent research has made progress towards extending the set of language constructs to which they apply <ref> [9, 11] </ref>. Our hope is that such extensions will share with the basic integration algorithm a common set of algebraic properties. However, we would like to avoid having to re-prove that each property holds every time we enhance our techniques.
Reference: 12. <author> S. Horwitz and T. Reps, </author> <title> Efficient comparison of program slices, </title> <institution> TR-982, Computer Sciences Department, University of Wisconsin, Madison, </institution> <note> WI (December 1990). </note>
Reference-contexts: downwards-closed sets of untagged single-point slices for program integration is that, given slices s 1 and s 2 and vertices v 1 and v 2 , it is possible to test in linear time whether s 1 and s 2 are isomorphic with respect to v 1 and v 2 <ref> [12] </ref>. Furthermore, by using hashing techniques the slice-set manipulations needed to perform operations in the algebra of downwards-closed untagged single-point slices can be performed in linear expected time (i.e. expected time linear in the sum of the sizes of the argument sets).
Reference: 13. <author> D.J. Kuck, Y. Muraoka, and S.C. Chen, </author> <title> On the number of operations simultaneously executable in FORTRAN-like programs and their resulting speed-up, </title> <journal> IEEE Trans. on Computers C-21(12) pp. </journal> <month> 1293-1310 (December </month> <year> 1972). </year>
Reference-contexts: Different definitions of program dependence representations have been given, depending on the intended application, but all are variations - 6 - on a theme introduced in <ref> [13] </ref> and share the common feature of having an explicit representation of data dependences (see below). The program dependence graphs defined in [5] introduced the additional feature of an explicit representation for control dependences (see below). 4 Definition 2.1.
Reference: 14. <author> D.J. Kuck, R.H. Kuhn, B. Leasure, D.A. Padua, and M. Wolfe, </author> <title> Dependence graphs and compiler optimizations, pp. </title> <booktitle> 207-218 in Conference Record of the Eighth ACM Symposium on Principles of Programming Languages, </booktitle> <address> (Williamsburg, VA, </address> <month> January 26-28, </month> <year> 1981), </year> <booktitle> ACM, </booktitle> <address> New York, NY (1981). </address>
Reference-contexts: To determine this information, the HPR algorithm employs a program representation that is similar to the program dependence graphs that have been used previously in vectorizing and parallelizing compilers <ref> [5, 14] </ref>. It makes use of an operation on these graphs called program slicing [19, 28] to find potentially changed computations. <p> To determine this information, the HPR algorithm employs graphs that represent the dependences between program elements. 2.1. Program Dependence Graphs and Program Slicing The program dependence graphs used by the HPR algorithm are similar to those used previously for representing programs in vectorizing and parallelizing compilers <ref> [5, 14] </ref>. Different definitions of program dependence representations have been given, depending on the intended application, but all are variations - 6 - on a theme introduced in [13] and share the common feature of having an explicit representation of data dependences (see below).
Reference: 15. <author> A. Lakhotia and L. Sterling, </author> <title> Composing recursive logic programs with clausal join, </title> <journal> New Generation Computing 6(2) pp. </journal> <month> 211-225 </month> <year> (1988). </year>
Reference-contexts: Relation to Previous Work There has been previous work on merging functional programs [1], logic programs <ref> [15] </ref>, and specifications [4]. Different models of integration have been used in each case. In Berzins's work on hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 13 Defining a good heuristic for this program-reconstitution problem remains an open question. - 50 - integrating functional programs, variants A and B are merged without regard to Base. <p> Similarly, Lakhotia and Sterling's 1-1 join operation is a two-way merge. However, in their work there is no notion of interference, and the characterization of the semantic properties of the merged program was left as an open question in <ref> [15] </ref>. Feather's work on integrating specifications does take Base into account, but although the integration algorithm preserves syntactic modifications, it does not guarantee any semantic properties of the integrated specification.
Reference: 16. <author> J.C.C. McKinsey and A. Tarski, </author> <title> On closed elements in closure algebras, </title> <note> Annals of Mathematics 47(1) pp. </note> <month> 122-162 (January </month> <year> 1946). </year>
Reference-contexts: This paper uses lattice theory to provide such a framework. 1.3. An Overview of the Contents A novel feature of our study is the use of Brouwerian algebra, rather than, for example, Boolean algebra or relational algebra. A Brouwerian algebra <ref> [16] </ref> is a distributive lattice with a pseudo-difference operation, a . - b, characterized by a . - b i cdidi i c iff a i cdidi i b ciic c (see Section 3). <p> These laws provide a convenient way to establish the integration operation's algebraic pro perties through simple formula manipulations. (For examples, see Sections 4 and 5 and the appendices.) Definition 3.5. A Brouwerian algebra <ref> [16] </ref> is an algebra (L, ciic , cddc , . - , dcd ) where (i) (L, ciic , cddc ) is a lattice with greatest element dcd . (ii) L is closed under . (iii) For all a, b, and c in L, a - b i cdidi i c <p> But because z p and because cbeing an element of DCSis downwards closed, we conclude that z c. 3.3. Relationship Between Brouwerian and Boolean Algebras A Brouwerian algebra is similar, but not identical, to a Boolean algebra. The relationship between Boolean and Brouwerian algebras can be characterized as follows <ref> [16] </ref>: for all elements a, define the Brouwerian complement by a D a; a Brouwerian algebra is Boolean iff a = a. Example. It may be helpful to keep in mind the following example of a Brouwerian algebra that is not a Boolean algebra. <p> Double Brouwerian Algebras Definition 5.2. A double Brouwerian algebra <ref> [16] </ref> is an algebra (L, ciic , cddc , . . .- , dcd ) where both (L, ciic , cddc , . - , dcd ) and (L, cddc , ciic , . - dcd ) are Brouwerian algebras. <p> Further information about Brouwerian algebras can be found in <ref> [16] </ref> and [20].
Reference: 17. <author> R. Milner, </author> <title> A theory of type polymorphism in programming, </title> <journal> Journal of Computer and System Sciences 17 pp. </journal> <month> 348-375 </month> <year> (1978). </year>
Reference-contexts: If no such buffer exists, the value is ^, the least Brouw value. The editor displays a type for each expression; these types are supplied using the Milner algorithm for polymorphic type inference (algorithm W) <ref> [6, 17] </ref>. An evaluation command added to the editor invokes the interpreter on the expression, andif the final result is a Brouwbuilds the corresponding program (if one exists). 6.2.
Reference: 18. <author> D. Notkin, R.J. Ellison, B.J. Staudt, G.E. Kaiser, E. Kant, A.N. Habermann, V. Ambriola, and C. Montangero, </author> <title> Special issue on the GANDALF project, </title> <journal> Journal of Systems and Software 5(2)(May 1985). </journal>
Reference-contexts: A tagging facility meeting these requirements can be supported by language-based editors, such as those that can be created by such systems as MENTOR [3], GANDALF <ref> [18] </ref>, and the Synthesizer Generator 6 Component tags furnish the means for identifying how the program-dependence-graph vertices in different versions correspond. It is the tags that are used to determine identical vertices when operations are performed using vertices from different program dependence graphs.
Reference: 19. <author> K.J. Ottenstein and L.M. Ottenstein, </author> <title> The program dependence graph in a software development environment, </title> <booktitle> Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <address> (Pittsburgh, PA, </address> <month> Apr. </month> <pages> 23-25, </pages> <year> 1984), </year> <journal> ACM SIGPLAN Notices 19(5) pp. </journal> <month> 177-184 (May </month> <year> 1984). </year> <month> - 61 </month> - 
Reference-contexts: To determine this information, the HPR algorithm employs a program representation that is similar to the program dependence graphs that have been used previously in vectorizing and parallelizing compilers [5, 14]. It makes use of an operation on these graphs called program slicing <ref> [19, 28] </ref> to find potentially changed computations.
Reference: 20. <author> H. Rasiowa and R. Sikorski, </author> <title> The Mathematics of Metamathematics, </title> <publisher> Polish Scientific Publishers, </publisher> <address> Warsaw (1963). </address>
Reference-contexts: In addition, she provided many comments and helpful suggestions as this paper was being prepared. Appendix A: Algebraic Laws for Brouwerian Algebras This appendix covers the algebraic laws that hold for Brouwerian algebras. 14 The material presented hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 14 Propositions A.2-A.21 are taken from a list given in <ref> [20] </ref> (pp. 59-60). (In [20], the laws are expressed in dual form, using a pseudo-complement operator, rather than in the form given in Appendix A, where pseudo-difference is used.) - 51 - here makes the paper essentially self-contained. (Several of the easier proofs have been omitted and serve as simple exercises <p> Appendix A: Algebraic Laws for Brouwerian Algebras This appendix covers the algebraic laws that hold for Brouwerian algebras. 14 The material presented hhhhhhhhhhhhhhhhhhhhhhhhhhhhh 14 Propositions A.2-A.21 are taken from a list given in <ref> [20] </ref> (pp. 59-60). (In [20], the laws are expressed in dual form, using a pseudo-complement operator, rather than in the form given in Appendix A, where pseudo-difference is used.) - 51 - here makes the paper essentially self-contained. (Several of the easier proofs have been omitted and serve as simple exercises for the reader.) Not <p> Further information about Brouwerian algebras can be found in [16] and <ref> [20] </ref>.
Reference: 21. <author> T. Reps and T. Teitelbaum, </author> <title> The Synthesizer Generator: A System for Constructing Language-Based Editors, </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY (1988). </address>
Reference-contexts: The user interface for the integration tool incorporates a language-specific editor created using the Synthesizer Generator, a meta-system for creating interactive, language-based program-development systems <ref> [21] </ref>. The editor of the program-integration tool automatically supplies tags on program components (i.e., assignment statements and predicates) so that common components can be identified in different versions. Data-flow analysis of programs is carried out according to the editor's defining attribute grammar and used to construct program dependence graphs.
Reference: 22. <author> T. Reps and W. Yang, </author> <title> The semantics of program slicing, </title> <institution> TR-777, Computer Sciences Department, University of Wisconsin, Madison, </institution> <address> WI (June 1988). </address>
Reference-contexts: The paper is divided into seven sections and three appendices. Section 2 provides an overview of the HPR algorithm for program integration. It also reviews the results that were established in <ref> [22, 25] </ref> concerning the semantic properties of a program that results from an integration. Readers familiar with the HPR algorithm can skip directly to Section 3, which introduces the concepts from lattice theory on which this paper's results are based. <p> The significance of a slice is that it captures a portion of a program's behavior in the sense that, for any initial state on which the program halts, the program and the slice compute the same sequence of values for each element of the slice <ref> [22] </ref>. In our case a program point can be (1) an assignment statement, (2) a control predicate, or (3) a final use of a variable in an end statement. <p> Theorem 2.7. (Slicing Theorem <ref> [22] </ref>). Let Q be a slice of program P with respect to a set of vertices. <p> Semantic Properties of the Integrated Program The following theorem, Theorem 2.9, characterizes the execution behavior of the integrated program produced by the HPR algorithm in terms of the behaviors of the base program and the two variants <ref> [22, 25] </ref>. It shows that the integrated program produced by the HPR algorithm incorporates the changed behaviors of both variants A and B (with respect to base program Base) as well as the unchanged behavior of A, B, and Base. <p> Thus, the HPR algorithm meets the semantic criterion of the integration model that was introduced in Section 1.1. Theorem 2.9. (Integration Theorem <ref> [22, 25] </ref>). <p> This test is absent because, as shown in <ref> [22, 25] </ref>, it is unnecessary: Pre (a, base, b) is always a slice of D (a, base) g Pre (a, base, b) g D (b, base). - 25 - Recall from Section 2.2 that Type I interference is not the only way in which the merged dependence graph m created by
Reference: 23. <author> T. Reps and T. Bricker, </author> <title> Illustrating interference in interfering versions of programs, </title> <booktitle> Proceedings of the Second International Workshop on Software Configuration Management, </booktitle> <address> (Princeton, NJ, </address> <month> Oct. </month> <pages> 24-27, </pages> <year> 1989), </year> <note> ACM SIG-SOFT Software Engineering Notes 17(7) pp. </note> <month> 46-55 (November </month> <year> 1989). </year>
Reference-contexts: With the integration tool, one is able to display program slices and integrate programs; if interference is detected during integration (i.e., integration fails), the system provides an interactive facility to help the user diagnose the cause of interference <ref> [23] </ref>. The user interface for the integration tool incorporates a language-specific editor created using the Synthesizer Generator, a meta-system for creating interactive, language-based program-development systems [21].
Reference: 24. <author> T. Reps, </author> <title> Demonstration of a prototype tool for program integration, </title> <institution> TR-819, Computer Sciences Department, University of Wisconsin, Madison, </institution> <note> WI (January 1989). </note>
Reference-contexts: Proof. Immediate from Theorem 4.12, together with Theorem 5.7 and Lemma 5.8. ` Theorem 5.9 is illustrated in Figure 13. 6. Some Practical Considerations 6.1. Implementation A program-integration tool that uses the HPR algorithm has been demonstrable since the summer of 1987 <ref> [24, 27] </ref>. With the integration tool, one is able to display program slices and integrate programs; if interference is detected during integration (i.e., integration fails), the system provides an interactive facility to help the user diagnose the cause of interference [23].
Reference: 25. <author> T. Reps and W. Yang, </author> <title> The semantics of program slicing and program integration, pp. </title> <booktitle> 360-374 in Proceedings of the Colloquium on Current Issues in Programming Languages, </booktitle> <address> (Barcelona, Spain, March 13-17, </address> <year> 1989), </year> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Vol. 352, </volume> <publisher> Springer-Verlag, </publisher> <address> New York, NY (1989). </address>
Reference-contexts: The paper is divided into seven sections and three appendices. Section 2 provides an overview of the HPR algorithm for program integration. It also reviews the results that were established in <ref> [22, 25] </ref> concerning the semantic properties of a program that results from an integration. Readers familiar with the HPR algorithm can skip directly to Section 3, which introduces the concepts from lattice theory on which this paper's results are based. <p> Semantic Properties of the Integrated Program The following theorem, Theorem 2.9, characterizes the execution behavior of the integrated program produced by the HPR algorithm in terms of the behaviors of the base program and the two variants <ref> [22, 25] </ref>. It shows that the integrated program produced by the HPR algorithm incorporates the changed behaviors of both variants A and B (with respect to base program Base) as well as the unchanged behavior of A, B, and Base. <p> Thus, the HPR algorithm meets the semantic criterion of the integration model that was introduced in Section 1.1. Theorem 2.9. (Integration Theorem <ref> [22, 25] </ref>). <p> This test is absent because, as shown in <ref> [22, 25] </ref>, it is unnecessary: Pre (a, base, b) is always a slice of D (a, base) g Pre (a, base, b) g D (b, base). - 25 - Recall from Section 2.2 that Type I interference is not the only way in which the merged dependence graph m created by
Reference: 26. <author> T. Reps, </author> <title> Algebraic properties of program integration, pp. </title> <booktitle> 326-340 in Proceedings of the Third European Symposium on Programming, </booktitle> <address> (Copenhagen, Denmark, </address> <month> May 15-18, </month> <year> 1990), </year> <booktitle> Lecture Notes in Computer Science, </booktitle> <volume> Vol. 432, </volume> <editor> ed. N. Jones,Springer-Verlag, </editor> <address> New York, NY (1990). </address>
Reference-contexts: To appear in Science of Computer Programming. A preliminary version of this work appeared in Proceedings of the 3rd European Symposium on Programming (Copenhagen, Den-mark, May 15-18, 1990), Lecture Notes in Computer Science, Vol. 432, N. Jones (ed.), Springer-Verlag, New York, NY, 1990 <ref> [26] </ref>. - 2 - decomposition is not possible, the members of the programming team must work with multiple, separate copies of the source files, and the different versions of the files must ultimately be integrated to produce a common version. (3) Suppose a tree or dag of related versions of a
Reference: 27. <author> T. Reps, </author> <title> The Wisconsin program-integration system reference manual, </title> <type> Unpublished report, </type> <institution> Computer Sciences Department, University of Wisconsin, Madison, </institution> <address> WI (April 1990). </address>
Reference-contexts: Proof. Immediate from Theorem 4.12, together with Theorem 5.7 and Lemma 5.8. ` Theorem 5.9 is illustrated in Figure 13. 6. Some Practical Considerations 6.1. Implementation A program-integration tool that uses the HPR algorithm has been demonstrable since the summer of 1987 <ref> [24, 27] </ref>. With the integration tool, one is able to display program slices and integrate programs; if interference is detected during integration (i.e., integration fails), the system provides an interactive facility to help the user diagnose the cause of interference [23].
Reference: 28. <author> M. Weiser, </author> <title> Program slicing, </title> <journal> IEEE Transactions on Software Engineering SE-10(4) pp. </journal> <month> 352-357 (July </month> <year> 1984). </year>
Reference-contexts: To determine this information, the HPR algorithm employs a program representation that is similar to the program dependence graphs that have been used previously in vectorizing and parallelizing compilers [5, 14]. It makes use of an operation on these graphs called program slicing <ref> [19, 28] </ref> to find potentially changed computations.
References-found: 28

