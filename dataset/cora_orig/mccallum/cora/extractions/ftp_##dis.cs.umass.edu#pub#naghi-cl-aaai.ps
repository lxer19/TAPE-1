URL: ftp://dis.cs.umass.edu/pub/naghi-cl-aaai.ps
Refering-URL: http://dis.cs.umass.edu/research/meta-learn.html
Root-URL: 
Email: fnagendra,lesserg@cs.umass.edu  lander@bbtech.com  
Phone: 2  
Title: Cooperative Learning over Composite Search Spaces: Experiences with a Multi-agent Design System  
Author: M V Nagendra Prasad Susan E. Lander and Victor R. Lesser 
Address: Amherst, MA 01003  401 Main Street Amherst, MA 0l002  
Affiliation: 1 Department of Computer Science University of Massachusetts  Blackboard Technology Group, Inc.  
Abstract: We suggest the use of two learning techniques | short term and long term | to enhance search efficiency in a multi-agent design system by letting the agents learn about non-local requirements on the local search process. The first technique allows an agent to accumulate and apply constraining information about global problem solving, gathered as a result of agent communication, to further problem solving within the same problem instance. The second technique is used to classify problem instances and appropriately index and retrieve constraining information to apply to new problem instances. These techniques will be presented within the context of a multi-agent parametric-design application called STEAM. We show that learning conclusively improves solution quality and processing-time results. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Conry, S. E.; Kuwabara, K.; Lesser, V. R.; and Meyer, R. A. </author> <year> 1991. </year> <title> Multistage negotiation for distributed constraint satisfaction. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 21(6). </journal>
Reference-contexts: Distributed Search has been the explicit focus of research amongst a small group of DAI researchers for the past few years. Yokoo et al. (Yokoo, Durfee, & Ishida 1992), Conry et al. <ref> (Conry et al. 1991) </ref>, and Sycara et al. (Sycara et al. 1991) have investigated various issues in Distributed Search. However, implicit in all of these pieces of work are the assumptions that the agents have homogeneous local knowledge and representations and tightly integrated system-wide problem-solving strategies across all agents.
Reference: <author> Holland, J. H. </author> <year> 1985. </year> <title> Properties of bucket brigade algorithm. </title> <booktitle> In First International Conference on Genetic Algorithms and their Applications, </booktitle> <pages> 1-7. </pages>
Reference-contexts: In many complex real-world expert systems, it may be difficult to achieve such requirements. A related work presented in Weiss (Weiss 1994) uses classifier systems for learning an aspect of multi-agent systems that is different from that presented here. Multiple agents use a variant of Holland's <ref> (Holland 1985) </ref> bucket brigade algorithm to learn appropriate instantiations of hierarchical organizations for efficiently solving blocks-world problems. Conclusion Our paper investigates the role of learning in improving the efficiency of cooperative, distributed search among a set of heterogeneous agents for parametric design.
Reference: <author> Khedro, T., and Genesereth, M. </author> <year> 1993. </year> <title> Progressive negotiation: A strategy for resolving conflicts in cooperative distributed multi-disciplinary design. </title> <booktitle> In Proceedings of the Conflict Resolution Workshop, IJCAI-93. </booktitle>
Reference-contexts: We believe that Klein's work provides a general foundation for handling conflicts in design application systems. However, it falls short of embedding such conflict resolution mechanisms into the larger problem solving context that can involve studying issues like solution evaluation, information exchange and learning. Khedro and Genesereth <ref> (Khedro & Genesereth 1993) </ref> present a strategy called Progressive Negotiation for resolving conflicts among multi-agent systems. Using this strategy, the agents can provably converge to a mutually acceptable solution if one exists. However, the guarantee of convergence relies crucially on explicit declarative representation and exchange of all constraining information.
Reference: <author> Klein, M. </author> <year> 1991. </year> <title> Supporting conflict resolution in cooperative design systems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 21(6) </journal> <pages> 1379-1390. </pages>
Reference-contexts: However, implicit in all of these pieces of work are the assumptions that the agents have homogeneous local knowledge and representations and tightly integrated system-wide problem-solving strategies across all agents. Conflict management approaches are very similar to the Conflict Driven Learning mechanism presented here. Klein <ref> (Klein 1991) </ref> develops a theory of computational model of resolution of conflicts among groups of expert agents. Associated with each possible conflict is an advice for resolving the conflict.
Reference: <author> Lander, S. E., and Lesser, V. R. </author> <year> 1996. </year> <title> Sharing meta-information to guide cooperative search among heterogeneous reusable agents. </title> <note> To appear in IEEE Transactions on Knowledge and Data Engineering. </note>
Reference: <author> Lander, S. E. </author> <year> 1994. </year> <title> Distributed Search in Heterogeneous and Reusable Multi-Agent Systems. </title> <type> Ph.D. Dissertation, </type> <institution> University of Massachusetts. </institution>
Reference-contexts: In this situation, domain-specific strategies are used to relax one of the soft constraints c ik . S t 0 i = Space (C t i c ik ) How soft constraints are relaxed can strongly effect the system performance. Lander <ref> (Lander 1994) </ref> discusses some studies related to this issue. We will not attempt to further discuss this in the present paper. <p> This section introduces two machine learning techniques to exploit the situations that lead to conflicts so as to avoid similar conflicts in future. Conflict Driven Learning (CDL) CDL has been presented as negotiated search in Lan-der <ref> (Lander 1994) </ref>. Below, we reinterpret this process as a form of learning and provide a formal basis for the learning mechanisms. As discussed previously, purely local views that agents start out with are unlikely to lead to composite solutions that are mutually acceptable to all of them.
Reference: <author> Lesser, V. R. </author> <year> 1990. </year> <title> An overview of DAI: Distributed AI as distributed search. </title> <journal> Journal of the Japanese Society for Artificial Intelligence 5(4) </journal> <pages> 392-400. </pages>
Reference: <author> Lesser, V. R. </author> <year> 1991. </year> <title> A retrospective view of FA/C distributed problem solving. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 21(6) </journal> <pages> 1347-1362. </pages>
Reference: <author> Nagendra Prasad, M. V.; Lesser, V. R.; and Lander, S. E. </author> <year> 1995. </year> <title> Learning organizational roles in a heterogeneous multi-agent system. </title> <institution> Computer Science Technical Report 95-35, University of Massachusetts. </institution>
Reference-contexts: STEAM is one of the few complex multi-agent systems demonstrating the viability of such methods for interesting learning tasks in the domain of problem solving control, which is a notion that is not explicit in the above systems. Nagendra Prasad et al. <ref> (NagendraPrasad, Lesser, & Lander 1995) </ref> discuss organization role learning in STEAM for organizing the control of distributed search process among the agents. This work uses reinforcement learning to let the agents organize themselves to play appropriate roles in distributed search.
Reference: <author> Sen, S., and Sekaran, M. </author> <year> 1994. </year> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 426-431. </pages> <address> Seattle, WA: </address> <publisher> AAAI. </publisher>
Reference-contexts: More commonly, STEAM-like systems are aggregations of complex agents whose expertise is represented by a combination of declarative and procedural knowledge that cannot be captured as a set of explicit constraints. Previous work related to learning in multi-agent systems is limited. Tan (Tan 1993), and Sen and Sekaran <ref> (Sen & Sekaran 1994) </ref> represent work in multi-agent reinforcement learning systems. While these works highlight interesting aspects of multi-agent learning systems, they are primarily centered around toy problems on a grid world.
Reference: <author> Shaw, M. J., and Whinston, A. B. </author> <year> 1989. </year> <title> Learning and adaptation in DAI systems. </title> <editor> In Gasser, L., and Huhns, M., eds., </editor> <booktitle> Distributed Artificial Intelligence, </booktitle> <volume> volume 2, </volume> <pages> 413-429. </pages> <publisher> Pittman Publishing/Morgan Kauffmann Pub. </publisher>
Reference-contexts: This work uses reinforcement learning to let the agents organize themselves to play appropriate roles in distributed search. Shoham and Tennenholtz (Shoham & Tennenholtz 1992) discuss co-learning and the emergence of conventions in multi-agent systems with simple interactions. Shaw and Whinston <ref> (Shaw & Whinston 1989) </ref> discuss a classifier system based multi-agent learning system for resource and task allocation in Flexible Manufacturing Systems. However, genetic algorithms and classifier systems have specific representational requirements to achieve learning. In many complex real-world expert systems, it may be difficult to achieve such requirements.
Reference: <author> Shoham, Y., and Tennenholtz, M. </author> <year> 1992. </year> <title> Emergent conventions in multi-agent systems: Initial experimental results and observations. </title> <booktitle> In Proccedings of KR-92. </booktitle>
Reference-contexts: Nagendra Prasad et al. (NagendraPrasad, Lesser, & Lander 1995) discuss organization role learning in STEAM for organizing the control of distributed search process among the agents. This work uses reinforcement learning to let the agents organize themselves to play appropriate roles in distributed search. Shoham and Tennenholtz <ref> (Shoham & Tennenholtz 1992) </ref> discuss co-learning and the emergence of conventions in multi-agent systems with simple interactions. Shaw and Whinston (Shaw & Whinston 1989) discuss a classifier system based multi-agent learning system for resource and task allocation in Flexible Manufacturing Systems.
Reference: <author> Sycara, K.; Roth, S.; Sadeh, N.; and Fox, M. </author> <year> 1991. </year> <title> Distributed constrained heuristic search. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 21(6) </journal> <pages> 1446-1461. </pages>
Reference-contexts: Distributed Search has been the explicit focus of research amongst a small group of DAI researchers for the past few years. Yokoo et al. (Yokoo, Durfee, & Ishida 1992), Conry et al. (Conry et al. 1991), and Sycara et al. <ref> (Sycara et al. 1991) </ref> have investigated various issues in Distributed Search. However, implicit in all of these pieces of work are the assumptions that the agents have homogeneous local knowledge and representations and tightly integrated system-wide problem-solving strategies across all agents.
Reference: <author> Tan, M. </author> <year> 1993. </year> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 330-337. </pages>
Reference-contexts: More commonly, STEAM-like systems are aggregations of complex agents whose expertise is represented by a combination of declarative and procedural knowledge that cannot be captured as a set of explicit constraints. Previous work related to learning in multi-agent systems is limited. Tan <ref> (Tan 1993) </ref>, and Sen and Sekaran (Sen & Sekaran 1994) represent work in multi-agent reinforcement learning systems. While these works highlight interesting aspects of multi-agent learning systems, they are primarily centered around toy problems on a grid world.
Reference: <author> Weiss, G. </author> <year> 1994. </year> <title> Some studies in distributed machine learning and organizational design. </title> <type> Technical Report FKI-189-94, </type> <institution> Institut fur Informatik, TU Munchen. </institution>
Reference-contexts: However, genetic algorithms and classifier systems have specific representational requirements to achieve learning. In many complex real-world expert systems, it may be difficult to achieve such requirements. A related work presented in Weiss <ref> (Weiss 1994) </ref> uses classifier systems for learning an aspect of multi-agent systems that is different from that presented here. Multiple agents use a variant of Holland's (Holland 1985) bucket brigade algorithm to learn appropriate instantiations of hierarchical organizations for efficiently solving blocks-world problems.
Reference: <author> Yokoo, M.; Durfee, E. H.; and Ishida, T. </author> <year> 1992. </year> <title> Distributed cosntraint satisfaction for formalizing distributed problem solving. </title> <booktitle> In Proceedings of the Twelfth Conference on Distributed Computing Systems. </booktitle>
Reference-contexts: Related Work We classify the work relevant to the topic on hand into three categories: Distributed Search, Conflict Management and Multi-agent Learning. Distributed Search has been the explicit focus of research amongst a small group of DAI researchers for the past few years. Yokoo et al. <ref> (Yokoo, Durfee, & Ishida 1992) </ref>, Conry et al. (Conry et al. 1991), and Sycara et al. (Sycara et al. 1991) have investigated various issues in Distributed Search.
References-found: 16

