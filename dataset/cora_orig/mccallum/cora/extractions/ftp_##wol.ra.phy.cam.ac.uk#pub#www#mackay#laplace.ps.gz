URL: ftp://wol.ra.phy.cam.ac.uk/pub/www/mackay/laplace.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Email: mackay@mrao.cam.ac.uk  
Title: Choice of Basis for Laplace Approximation  
Author: David J.C. MacKay 
Date: February 23rd 1998  1998  
Note: Submitted to Machine Learning October 14th 1996 Accepted pending minor modifications  Revised version completed May 11th  
Address: Cambridge, CB3 0HE, United Kingdom.  
Affiliation: Cavendish Laboratory,  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Bridle, J. S. </author> <title> (1989) Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Neuro-computing: algorithms, architectures and applications, </title> <editor> ed. by F. Fougelman-Soulie and J. Herault. </editor> <publisher> Springer-Verlag. </publisher>
Reference-contexts: I would argue that the `1' terms in the traditional posterior probability are artefacts of the choice of basis. 2 A change of basis I suggest that maximum a posteriori parameter estimation and Laplace approximations would be better conducted in the `softmax' representation (widely used in neural networks <ref> (Bridle 1989) </ref>) in which the parameters p are replaced by parameters a: p i (a) = P : (9) [Please do not confuse p (a), the function defined in equation (9), with the probability density P (a).] The probability vector p has I components but only I 1 degrees of freedom;
Reference: <author> Chickering, D. M., and Heckerman, D., </author> <title> (1996) Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables. </title> <note> Microsoft Research Technical Report MSR-TR-96-08. </note>
Reference-contexts: The traditional MAP method for such models (Lee and Gauvain 1993) is to maximize the posterior probability of the parameters p, and the traditional Laplace method for such a model is, after maximizing in the p basis, to make the Gaussian approximation in the same basis <ref> (Chickering and Heckerman 1996) </ref>.
Reference: <author> Gelman, A. </author> <title> (1996) Bayesian model-building by pure thought: Some principles and examples. </title> <booktitle> Statistica Sinica 6: </booktitle> <pages> 215-232. </pages>
Reference: <author> Gelman, A., Carlin, J., Stern, H., and Rubin, D. </author> <title> (1995) Bayesian Data Analysis. </title> <publisher> London: Chapman and Hall. </publisher>
Reference: <author> Jeffreys, H. </author> <title> (1939) Theory of Probability. </title> <institution> Oxford Univ. </institution> <note> Press. 3rd edition reprinted in paperback 1985. </note>
Reference: <author> Lee, C. H., and Gauvain, J. L. </author> <title> (1993) Speaker adaptation based on MAP estimation of HMM parameters. </title> <booktitle> In IEEE Proceedings, </booktitle> <pages> pp. </pages> <note> II-558-561. 9 Lindley, </note> <author> D. V. </author> <title> (1980) Approximate Bayesian methods. In Bayesian Statistics, </title> <note> ed. by J. M. </note>
Reference-contexts: The traditional MAP method for such models <ref> (Lee and Gauvain 1993) </ref> is to maximize the posterior probability of the parameters p, and the traditional Laplace method for such a model is, after maximizing in the p basis, to make the Gaussian approximation in the same basis (Chickering and Heckerman 1996).
Reference: <editor> Bernardo, M. H. DeGroot, D. V. Lindley, and A. F. M. </editor> <volume> Smith, </volume> <pages> pp. 223-237. </pages> <address> Valencia: </address> <publisher> Valencia University Press. </publisher>
Reference: <author> MacKay, D. J. C. </author> <title> (1992) A practical Bayesian framework for backpropagation networks. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 448-472. </pages>
Reference: <author> MacKay, D. J. C., </author> <title> (1997) Ensemble learning for hidden Markov models. </title> <note> Available from http://wol.ra.phy.cam.ac.uk/. </note>
Reference-contexts: And deterministic Bayesian approximations that are basis independent are under development <ref> (MacKay 1997) </ref>. But if MAP methods are used, this paper offers a way of evaluating marginal likelihoods which satisfies these two desiderata: 1. We can make a Laplace approximation for any Dirichlet priors and any amount of data.
Reference: <author> MacKay, D. J. C., and Peto, L. </author> <title> (1995) A hierarchical Dirichlet language model. </title> <booktitle> Natural Language Engineering 1 (3): </booktitle> <pages> 1-19. </pages>
Reference: <author> Neal, R. M. </author> <title> (1992) Bayesian mixture modelling. In Maximum Entropy and Bayesian Methods, </title> <note> Seattle 1991 , ed. by C. </note> <author> Smith, G. Erickson, and P. </author> <month> Neudorfer, </month> <pages> pp. 197-211, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference-contexts: bins i (where both methods perform well) the traditional approximation can be a little more accurate. 4 Discussion This paper's aim is not to advocate the use of Laplace approximations; indeed a good case can be made for using other methods such as Markov chain Monte Carlo (see, for example, <ref> (Neal 1992) </ref>). And deterministic Bayesian approximations that are basis independent are under development (MacKay 1997). But if MAP methods are used, this paper offers a way of evaluating marginal likelihoods which satisfies these two desiderata: 1.
Reference: <author> O'Hagan, A. </author> <title> (1994) Bayesian Inference, </title> <journal> volume 2B of Kendall's Advanced Theory of Statistics. </journal>
Reference: <institution> Edward Arnold. </institution>
Reference: <author> Ripley, B. D. </author> <title> (1995) Pattern Recognition and Neural Networks. </title> <publisher> Cambridge. </publisher>
Reference: <author> Smith, A., and Spiegelhalter, D. </author> <title> (1980) Bayes factors and choice criteria for linear models. </title> <journal> Journal of the Royal Statistical Society B 42 (2): </journal> <pages> 213-220. </pages> <month> May 21, </month> <note> 1998 | Version 3.6 10 </note>
References-found: 15

