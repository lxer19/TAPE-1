URL: http://now.cs.berkeley.edu/Fastcomm/usenix.ps
Refering-URL: http://now.cs.berkeley.edu/Fastcomm/fastcomm.html
Root-URL: 
Title: High-Performance Local Area Communication With Fast Sockets  
Author: Steven H. Rodrigues Thomas E. Anderson David E. Culler 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California at Berkeley  
Abstract: Modern switched networks such as ATM and Myri-net enable low-latency, high-bandwidth communication. This performance has not been realized by current applications, because of the high processing overheads imposed by existing communications software. These overheads are usually not hidden with large packets; most network traffic is small. We have developed Fast Sockets, a local-area communication layer that utilizes a high-performance protocol and exports the Berkeley Sockets programming interface. Fast Sockets realizes round-trip transfer times of 60 microseconds and maximum transfer bandwidth of 33 MB/second between two UltraSPARC 1s connected by a Myrinet network. Fast Sockets obtains performance by collapsing protocol layers, using simple buffer management strategies, and utilizing knowledge of packet destinations for direct transfer into user buffers. Using receive posting, we make the Sockets API a single-copy communications layer and enable regular Sockets programs to exploit the performance of modern networks. Fast Sockets transparently reverts to standard TCP/IP protocols for wide-area communication. This work was supported in part by the Defense Advanced Research Projects Agency (N00600-93-C-2481, F30602-95-C-0014), the National Science Foundation (CDA 0401156), California MICRO, the AT&T Foundation, Digital Equipment Corporation, Exabyte, Hewlett Packard, Intel, IBM, Microsoft, Mitsubishi, Siemens Corporation, Sun Microsystems, and Xerox. Anderson and Culler were also supported by National Science Foundation Presidential Faculty Fellowships, and Rodrigues by a National Science Foundation Graduate Fellowship. The authors can be contacted at fsteverod, tea, cullerg@cs.berkeley.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [Abbott & Peterson 1993] <author> M. B. Abbott and L. L. Peter-son. </author> <title> Increasing network throughput by integrating protocol layers. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(5) </volume> <pages> 600-610, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Layering also restricts information transfer. Hidden implementation details of each layer can cause large, unforeseen impacts on performance [Clark 1982, Crowcroft et al. 1992]. Mechanisms have been proposed to overcome these difficulties [Clark & Tennenhouse 1990], but existing work has focused on message throughput, rather than protocol latency <ref> [Abbott & Peterson 1993] </ref>. Also, the number of programming interfaces and protocols is small: there are two programming interfaces (Berkeley Sockets and the System V Transport Layer Interface) and only a few data transfer protocols (TCP/IP and UDP/IP) in widespread usage.
Reference: [Amer et al. 1987] <author> P. D. Amer, R. N. Kumar, R. bin Kao, J. T. Phillips, and L. N. Cassel. </author> <title> Local area broadcast network measurement: Traffic characterization. </title> <booktitle> In Spring COMPCON, </booktitle> <pages> pp. 64-70, </pages> <address> San Francisco, California, </address> <month> February </month> <year> 1987. </year>
Reference: [Banks & Prudence 1993] <author> D. Banks and M. Prudence. </author> <title> A high-performance network architecture for a PA-RISC workstation. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 191-202, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: The HP/UX platform consists of two 99Mhz HP735's interconnected by an FDDI network and using the Medusa network adapter <ref> [Banks & Prudence 1993] </ref>. The Solaris platform is a collection of UltraSPARC 1's connected via a Myri-net network [Seitz 1994]. For all tests, there was no other load on the network links or switches. Our microbenchmarks were run against a variety of TCP/IP setups.
Reference: [Birrell & Nelson 1984] <author> A. D. Birrell and B. J. Nelson. </author> <title> Implementing remote procedure calls. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(1) </volume> <pages> 39-59, </pages> <month> February </month> <year> 1984. </year>
Reference-contexts: When an active message arrives at its destination, the handler is looked up and invoked with the data carried in the message. While conceptually similar to a remote procedure call <ref> [Birrell & Nelson 1984] </ref>, an active message is constrained in the amount and types of data that can be carried and passed to handler functions. These constraints enable the structuring of an Active Messages layer for high performance.
Reference: [Blumrich et al. 1994] <author> M. A. Blumrich, K. Li, R. D. Alpert, C. Dubnicki, E. W. Felten, and J. Sandberg. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 142-153, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Examples of communications layers with this memory management style are Hamlyn [Buzzard et al. 1996] and the SHRIMP network interface <ref> [Blumrich et al. 1994] </ref>. Also, the initial version of Generic Active Messages [Culler et al. 1994] offered only sender-based memory management for data transfer. Sender-based memory management has a major drawback for use in byte-stream and message-passing APIs such as Sockets.
Reference: [Brewer et al. 1995] <author> E. A. Brewer, F. T. Chong, L. T. Liu, S. D. Sharma, and J. D. Kubiatowicz. </author> <title> Remote Queues: Exposing message queues for optimization and atomicity. </title> <booktitle> In Proceedings of SPAA '95, </booktitle> <address> Santa Barbara, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: While this approach provides full compatibility with existing protocols, fundamental limitations of the protocol design may limit the performance gain. Recent systems, such as Active Messages [von Eicken et al. 1992], Remote Queues <ref> [Brewer et al. 1995] </ref>, and native U-Net [von Eicken et al. 1995], have used the first two methods; they implement new protocols and new programming interfaces to obtain improved local-area network performance. The protocols and interfaces are lightweight and provide programming abstractions that are similar to the underlying hardware. <p> Illinois Fast Messages [Pakin et al. 1995] provided an interface similar to that of previous versions of Active Messages, but did not allow processes to share the network. Remote Queues <ref> [Brewer et al. 1995] </ref> provided low-overhead communications similar to that of Active Messages, but separated the arrival of messages from the invocation of handlers. The SHRIMP project implemented a stream sockets layer that uses many of the same techniques as Fast Sockets [Damianakis et al. 1996].
Reference: [Buzzard et al. 1996] <author> G. Buzzard, D. Jacobson, M. Mackey, S. Marovich, and J. Wilkes. </author> <title> An implementation of the Hamlyn sender-managed interface architecture. </title> <booktitle> In Proceedings of the Second Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pp. 245-259, </pages> <address> Seattle, WA, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Examples of communications layers with this memory management style are Hamlyn <ref> [Buzzard et al. 1996] </ref> and the SHRIMP network interface [Blumrich et al. 1994]. Also, the initial version of Generic Active Messages [Culler et al. 1994] offered only sender-based memory management for data transfer. Sender-based memory management has a major drawback for use in byte-stream and message-passing APIs such as Sockets.
Reference: [Caceres et al. 1991] <author> R. Caceres, P. B. Danzig, S. Jamin, and D. J. Mitzel. </author> <title> Characteristics of wide-area TCP/IP conversations. </title> <booktitle> In Proceedings of ACM SIGCOMM '91, </booktitle> <month> September </month> <year> 1991. </year>
Reference: [Cheriton & Williamson 1987] <author> D. R. Cheriton and C. L. Williamson. </author> <title> Network measurement of the VMTP request-response protocol in the V distributed system. </title> <booktitle> In Proceedings of SIG-METRICS '87, </booktitle> <pages> pp. 216-225, </pages> <address> Banff, Alberta, Canada, </address> <month> May </month> <year> 1987. </year>
Reference: [Cheriton & Williamson 1989] <author> D. Cheriton and C. Williamson. VMTP: </author> <title> A transport layer for high-performance distributed computing. </title> <journal> IEEE Communications, </journal> <pages> pp. 37-44, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Previous work has focused on protocols, protocol and infrastructure implementations, and the underlying network device soft ware. The VMTP protocol <ref> [Cheriton & Williamson 1989] </ref> attempted to provide a general-purpose protocol optimized for small packets and request-response traffic.
Reference: [Chu 1996] <author> H.-K. J. Chu. </author> <title> Zero-copy TCP in Solaris. </title> <booktitle> In Proceedings of the 1996 USENIX Annual Technical Conference, </booktitle> <address> San Diego, CA, </address> <month> Jan-uary </month> <year> 1996. </year>
Reference-contexts: All of these systems realize latencies and throughput close to the physical limits of the network. However, none of them offer compatibility with existing applications. Other work has tried to improve performance by re-implementing TCP. Recent work includes zero-copy TCP for Solaris <ref> [Chu 1996] </ref> and a TCP interface for the U-Net interface [von Eicken et al. 1995]. These implementations can inter-operate with other TCP/IP implementations and improve throughput and latency relative to standard TCP/IP stacks. Both implementations can realize the full bandwidth of the network for large packets. <p> While Active Messages provides the equivalent of an ADC for Fast Sockets, fbufs are not needed, as receive posting allows for data transfer directly into the user application. Recently, the development of a zero-copy TCP stack in Solaris <ref> [Chu 1996] </ref> aggressively utilized hardware and operating system features such as direct memory access (DMA), page re-mapping, and copy-on-write pages to improve communications performance. To take full advantage of the zero-copy stack, user applications had to use page-aligned buffers and transfer sizes larger than a page.
Reference: [Claffy et al. 1992] <author> K. C. Claffy, G. C. Polyzos, and H.- W. Braun. </author> <title> Traffic characteristics of the T1 NSFNET backbone. </title> <type> Technical Report CS92-270, </type> <institution> University of California, </institution> <address> San Diego, </address> <month> July </month> <year> 1992. </year>
Reference: [Clark & Tennenhouse 1990] <author> D. D. Clark and D. L. Ten-nenhouse. </author> <title> Architectural considerations for a new generation of protocols. </title> <booktitle> In Proceedings of SIGCOMM '90, </booktitle> <pages> pp. 200-208, </pages> <address> Philadelphia, Pennsylvania, </address> <month> September </month> <year> 1990. </year>
Reference-contexts: Layering also restricts information transfer. Hidden implementation details of each layer can cause large, unforeseen impacts on performance [Clark 1982, Crowcroft et al. 1992]. Mechanisms have been proposed to overcome these difficulties <ref> [Clark & Tennenhouse 1990] </ref>, but existing work has focused on message throughput, rather than protocol latency [Abbott & Peterson 1993].
Reference: [Clark 1982] <author> D. D. Clark. </author> <title> Modularity and efficiency in protocol implementation. Request For Comments 817, </title> <type> IETF, </type> <month> July </month> <year> 1982. </year>
Reference-contexts: Layer transitions can be costly in time and programming effort. Each layer may use a different abstraction for data storage and transfer, requiring data transformation at every layer boundary. Layering also restricts information transfer. Hidden implementation details of each layer can cause large, unforeseen impacts on performance <ref> [Clark 1982, Crowcroft et al. 1992] </ref>. Mechanisms have been proposed to overcome these difficulties [Clark & Tennenhouse 1990], but existing work has focused on message throughput, rather than protocol latency [Abbott & Peterson 1993].
Reference: [Clark et al. 1989] <author> D. D. Clark, V. Jacobson, J. Romkey, and H. Salwen. </author> <title> An analysis of TCP processing overhead. </title> <journal> IEEE Communications, </journal> <pages> pp. 23-29, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: In contrast, Fast Sockets provides the same models as TCP/IP and maintains application compatibility. Other work <ref> [Clark et al. 1989, Watson & Mamrak 1987] </ref> argued that protocol implementations, rather than protocol designs, were to blame for poor performance, and that efficient implementations of general-purpose protocols could do as well as or better than special-purpose protocols for most applications.
Reference: [Crowcroft et al. 1992] <author> J. Crowcroft, I. Wakeman, Z. Wang, and D. </author> <title> Sirovica. </title> <journal> Is layering harmful? IEEE Network, </journal> <volume> 6(1) </volume> <pages> 20-24, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Layer transitions can be costly in time and programming effort. Each layer may use a different abstraction for data storage and transfer, requiring data transformation at every layer boundary. Layering also restricts information transfer. Hidden implementation details of each layer can cause large, unforeseen impacts on performance <ref> [Clark 1982, Crowcroft et al. 1992] </ref>. Mechanisms have been proposed to overcome these difficulties [Clark & Tennenhouse 1990], but existing work has focused on message throughput, rather than protocol latency [Abbott & Peterson 1993].
Reference: [Culler et al. 1994] <author> D. E. Culler, K. Keeton, L. T. Liu, A. Mainwaring, R. P. Martin, S. H. Rodrigues, and K. Wright. </author> <title> The Generic Active Message Interface Specification, </title> <month> February </month> <year> 1994. </year>
Reference-contexts: Examples of communications layers with this memory management style are Hamlyn [Buzzard et al. 1996] and the SHRIMP network interface [Blumrich et al. 1994]. Also, the initial version of Generic Active Messages <ref> [Culler et al. 1994] </ref> offered only sender-based memory management for data transfer. Sender-based memory management has a major drawback for use in byte-stream and message-passing APIs such as Sockets. With sender-based management, the sending and receiving endpoints must synchronize and agree on the destination of each packet. <p> The true test of Fast Sockets' usefulness, however, is how well its raw performance is exposed to applications. We present results from an FTP application to demonstrate the usefulness of Fast Sockets in a real-world environment. 4.1 Experimental Setup Fast Sockets has been implemented using Generic Active Messages (GAM) <ref> [Culler et al. 1994] </ref> on both HP/UX 9:0:x and Solaris 2:5. The HP/UX platform consists of two 99Mhz HP735's interconnected by an FDDI network and using the Medusa network adapter [Banks & Prudence 1993]. The Solaris platform is a collection of UltraSPARC 1's connected via a Myri-net network [Seitz 1994].
Reference: [Dahlin et al. 1994] <author> M. D. Dahlin, R. Y. Wang, D. A. Pat-terson, and T. E. Anderson. </author> <title> Cooperative caching: Using remote client memory to improve file system performance. </title> <booktitle> In Proceedings of the First Conference on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pp. 267-280, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Small packets are the rule even in applications considered bandwidth-intensive: 95% of all packets in a NFS trace performed at the Berkeley Computer Science Division carried less than 192 bytes of user data <ref> [Dahlin et al. 1994] </ref>; the mean packet size in the trace was 382 bytes. Processing overhead is the dominant transport cost for packets this small, limiting NFS performance on a high-bandwidth network.
Reference: [Damianakis et al. 1996] <author> S. N. Damianakis, C. Dubnicki, and E. W. Felten. </author> <title> Stream sockets on SHRIMP. </title> <type> Technical Report TR-513-96, </type> <institution> Princeton University, Princeton, NJ, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: Remote Queues [Brewer et al. 1995] provided low-overhead communications similar to that of Active Messages, but separated the arrival of messages from the invocation of handlers. The SHRIMP project implemented a stream sockets layer that uses many of the same techniques as Fast Sockets <ref> [Damianakis et al. 1996] </ref>. SHRIMP supports communication via shared memory and the execution of handler functions on data arrival.
Reference: [de Prycker 1993] <author> M. de Prycker. </author> <title> Asynchronous Transfer Mode: Solution for Broadband ISDN. </title> <publisher> Ellis Horwood Publishers, </publisher> <address> second edition, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction The development and deployment of high performance local-area networks such as ATM <ref> [de Prycker 1993] </ref>, Myrinet [Seitz 1994], and switched high-speed Ethernet has the potential to dramatically improve communication performance for network applications. These networks are capable of microsecond latencies and bandwidths of hundreds of megabits per second; their switched fabrics eliminate the contention seen on shared-bus networks such as traditional Ethernet.
Reference: [Druschel & Peterson 1993] <author> P. Druschel and L. L. Peter--son. Fbufs: </author> <title> A high-bandwith cross-domain data transfer facility. </title> <booktitle> In Proceedings of the Fourteenth Annual Symposium on Operating Systems Principles, </booktitle> <pages> pp. 189-202, </pages> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year>
Reference: [Druschel et al. 1993] <author> P. Druschel, M. B. Abbott, M. A. Pagels, and L. L. Peterson. </author> <title> Network subsystem design: A case for an integrated data path. </title> <journal> IEEE Network (Special Issue on End-System Support for High Speed Networks), </journal> <volume> 7(4) </volume> <pages> 8-17, </pages> <month> July </month> <year> 1993. </year>
Reference: [Druschel et al. 1994] <author> P. Druschel, L. L. Peterson, and B. S. Davie. </author> <title> Experiences with a high-speed network adapter: A software perspective. </title> <booktitle> In Proceedings of ACM SIGCOMM '94, </booktitle> <month> August </month> <year> 1994. </year>
Reference: [Edwards & Muir 1995] <author> A. Edwards and S. Muir. </author> <title> Experiences in implementing a high performance TCP in user-space. </title> <booktitle> In ACM SIGCOMM '95, </booktitle> <address> Cambridge, MA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Both systems, however, attempted only to match in-kernel performance, rather than better it. Further, both systems utilized in-kernel facilities for message transmission, limiting the possible performance improvement. Edwards and Muir <ref> [Edwards & Muir 1995] </ref> attempted to build an entirely user-level solution, but utilized a TCP stack that had been built for the HP/UX kernel.
Reference: [Feldmeier 1986] <author> D. C. Feldmeier. </author> <title> Traffic measurement on a token-ring network. </title> <booktitle> In Proceedings of the 1986 Computer Networking Conference, </booktitle> <pages> pp. 236-243, </pages> <month> November </month> <year> 1986. </year>
Reference: [Felten et al. 1996] <author> E. W. Felten, R. D. Alpert, A. Bilas, M. A. Blumrich, D. W. Clark, S. N. Dami-anakis, C. Dubnicki, L. Iftode, and K. Li. </author> <title> Early experience with message-passing on the SHRIMP multicomputer. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture (ISCA '96), </booktitle> <pages> pp. 296-307, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: SHRIMP supports communication via shared memory and the execution of handler functions on data arrival. The SHRIMP network had hardware latencies (4-9 ms one-way) much lower than the Fast Sockets Myri-net, but its maximum bandwidth (22 MB/s) was also lower than that of the Myrinet <ref> [Felten et al. 1996] </ref>. It used a custom-designed network interface for its memory-mapped communication model. The interface provided in-order, reliable delivery, which allowed for extremely low overheads (7 ms over the hardware latency); Fast Sockets incurs substantial overhead to ensure in-order delivery.
Reference: [Forin et al. 1991] <author> A. Forin, D. Golub, and B. N. Bershad. </author> <title> An I/O system for Mach 3.0. </title> <booktitle> In Proceedings of the USENIX Mach Symposium, </booktitle> <pages> pp. 163-176, </pages> <address> Monterey, CA, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: An alternative to reducing internal operating system costs is to bypass the operating system altogether, and use either in a user-level library (like Fast Sockets) or a separate user-level server. Mach 3.0 used the latter approach <ref> [Forin et al. 1991] </ref>, which yielded poor networking performance [Maeda & Bershad 1992]. Both [Maeda & Bershad 1993a] and [Thekkath et al. 1993] explored building TCP into a user-level library linked with existing applications. Both systems, however, attempted only to match in-kernel performance, rather than better it.
Reference: [Gusella 1990] <author> R. Gusella. </author> <title> A measurement study of diskless workstation traffic on an Ethernet. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(9) </volume> <pages> 1557-1568, </pages> <month> September </month> <year> 1990. </year>
Reference: [Hutchinson & Peterson 1991] <author> N. Hutchinson and L. L. Peterson. </author> <title> The x-kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference: [Kay & Pasquale 1993] <author> J. Kay and J. Pasquale. </author> <title> The importance of non-data-touching overheads in TCP/IP. </title> <booktitle> In Proceedings of the 1993 SIG-COMM, </booktitle> <pages> pp. 259-268, </pages> <address> San Francisco, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: Chain ing makes adding and removing packet headers easy. The mbuf abstraction is not cheap, however: 15% of the processing time for small TCP packets is consumed by mbuf management <ref> [Kay & Pasquale 1993] </ref>. Additionally, to take advantage of the mbuf abstraction, user data must be copied into and out of mbufs, which consumes even more time in the data transfer critical path. <p> Other work [Clark et al. 1989, Watson & Mamrak 1987] argued that protocol implementations, rather than protocol designs, were to blame for poor performance, and that efficient implementations of general-purpose protocols could do as well as or better than special-purpose protocols for most applications. The measurements made in <ref> [Kay & Pasquale 1993] </ref> lend credence to these arguments; they found that memory operations and operating system overheads played a dominant role in the cost of large packets. For small packets, however, protocol costs were significant, amounting for up to 33% of processing time for single-byte messages. <p> Edwards and Muir [Edwards & Muir 1995] attempted to build an entirely user-level solution, but utilized a TCP stack that had been built for the HP/UX kernel. Their solution replicated the organization of the kernel at user-level with worse performance than the in-kernel TCP stack. <ref> [Kay & Pasquale 1993] </ref> showed that interfacing to the network card itself was a major cost for small packets. Recent work has focused on reduc ing this portion of the protocol cost, and on utiliz-ing the message coprocessors that are appearing on high-performance network controllers such as Myri-net [Seitz 1994].
Reference: [Keeton et al. 1995] <author> K. Keeton, D. A. Patterson, and T. E. Anderson. </author> <title> LogP quantified: The case for low-overhead local area networks. In Hot Interconnects III, </title> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Further, observed bandwidth and round-trip latencies on next-generation network technologies such as Myrinet and ATM do not begin to approach the raw capabilities of these networks <ref> [Keeton et al. 1995] </ref>. In this section, we describe a number of features and problems of commercial TCP implementations, and how these features affect communication performance. 2.1 Built for the Wide Area TCP/IP was originally designed, and is usually implemented, for wide-area networks.
Reference: [Kleinrock & Naylor 1974] <author> L. Kleinrock and W. E. Nay-lor. </author> <title> On measured behavior of the ARPA network. </title> <booktitle> In AFIPS Proceedings, </booktitle> <volume> volume 43, </volume> <pages> pp. 767-780, </pages> <year> 1974. </year>
Reference: [Liu & Culler 1995] <author> L. T. Liu and D. E. Culler. </author> <title> Evaluation of the Intel Paragon on Active Message communication. </title> <booktitle> In Proceedings of the 1995 Intel Supercomputer Users Group Conference, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: The actual network device interface, Active Messages, remains a distinct layer from Fast Sockets. This facilitates the portability of Fast Sockets between different operating systems and network hardware. Active Messages implementations are available for the Intel Paragon <ref> [Liu & Culler 1995] </ref>, FDDI [Martin 1994], Myrinet [Mainwaring & Culler 1995], and ATM [von Eicken et al. 1995]. Layering costs are kept low because Active Messages is a thin layer, and all of its implementation-dependent constants (such as maximum packet size) are exposed to higher layers.
Reference: [Maeda & Bershad 1992] <author> C. Maeda and B. N. Bershad. </author> <title> Networking performance for microkernels. </title> <booktitle> In Proceedings of the Third Workshop on Workstation Operating Systems, </booktitle> <pages> pp. 154-159, </pages> <year> 1992. </year>
Reference-contexts: An alternative to reducing internal operating system costs is to bypass the operating system altogether, and use either in a user-level library (like Fast Sockets) or a separate user-level server. Mach 3.0 used the latter approach [Forin et al. 1991], which yielded poor networking performance <ref> [Maeda & Bershad 1992] </ref>. Both [Maeda & Bershad 1993a] and [Thekkath et al. 1993] explored building TCP into a user-level library linked with existing applications. Both systems, however, attempted only to match in-kernel performance, rather than better it.
Reference: [Maeda & Bershad 1993a] <author> C. Maeda and B. Bershad. </author> <title> Protocol service decomposition for high-performance networking. </title> <booktitle> In Proceedings of the Fourteenth Symposium on Operating Systems Principles, </booktitle> <pages> pp. 244-255, </pages> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Mach 3.0 used the latter approach [Forin et al. 1991], which yielded poor networking performance [Maeda & Bershad 1992]. Both <ref> [Maeda & Bershad 1993a] </ref> and [Thekkath et al. 1993] explored building TCP into a user-level library linked with existing applications. Both systems, however, attempted only to match in-kernel performance, rather than better it. Further, both systems utilized in-kernel facilities for message transmission, limiting the possible performance improvement.
Reference: [Maeda & Bershad 1993b] <author> C. Maeda and B. Bershad. </author> <title> Service without servers. </title> <booktitle> In Workshop on Workstation Operating Systems IV, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: Some global state can be maintained by simply using existing facilities. For example, the port name spaces can use the in-kernel name management functions. Other shared or global state can be maintained by using a server process to store this state, as described in <ref> [Maeda & Bershad 1993b] </ref>. Finally, using Active Messages limits Fast Sockets communication to the local-area domain. Fast Sockets supports wide-area communication by automatically switching to standard network protocols for non-local addresses. It is a reasonable trade-off, as endpoint processing overheads are generally not the limiting factor for internet-work communication. <p> User-level operation also causes problems for socket termination; standard TCP/IP sockets are gracefully shut down on process termination. These problems are not insurmountable. Sharing Fast Sockets requires an Active Messages layer that allows endpoint sharing and either the use of a dedicated server process <ref> [Maeda & Bershad 1993b] </ref> or the use of shared memory for every Fast Socket's state.
Reference: [Mainwaring & Culler 1995] <author> A. Mainwaring and D. Culler. </author> <title> Active Messages: </title> <booktitle> Organization and Applications Programming Interface, </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: The actual network device interface, Active Messages, remains a distinct layer from Fast Sockets. This facilitates the portability of Fast Sockets between different operating systems and network hardware. Active Messages implementations are available for the Intel Paragon [Liu & Culler 1995], FDDI [Martin 1994], Myrinet <ref> [Mainwaring & Culler 1995] </ref>, and ATM [von Eicken et al. 1995]. Layering costs are kept low because Active Messages is a thin layer, and all of its implementation-dependent constants (such as maximum packet size) are exposed to higher layers. The Fast Sockets layer stays lightweight by exploiting Active Message handlers.
Reference: [Martin 1994] <author> R. P. Martin. HPAM: </author> <title> An Active Message layer for a network of HP workstations. </title> <booktitle> In Hot Interconnects II, </booktitle> <pages> pp. 40-58, </pages> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The actual network device interface, Active Messages, remains a distinct layer from Fast Sockets. This facilitates the portability of Fast Sockets between different operating systems and network hardware. Active Messages implementations are available for the Intel Paragon [Liu & Culler 1995], FDDI <ref> [Martin 1994] </ref>, Myrinet [Mainwaring & Culler 1995], and ATM [von Eicken et al. 1995]. Layering costs are kept low because Active Messages is a thin layer, and all of its implementation-dependent constants (such as maximum packet size) are exposed to higher layers.
Reference: [Nagle 1984] <author> J. Nagle. </author> <title> Congestion control in IP/TCP in-ternetworks. Request For Comments 896, </title> <institution> Network Working Group, </institution> <month> January </month> <year> 1984. </year>
Reference-contexts: TCP/IP is only about 50-60% that of Fast Sockets. Another reason is that TCP defaults to batching small data writes in order to maximize throughput (the Na-gle algorithm) <ref> [Nagle 1984] </ref>, and these tests do not disable the algorithm (unlike our microbenchmarks); however, this behavior trades off small-packet bandwidth for higher round-trip latencies, as data is held at the sender in an attempt to coalesce data.
Reference: [Pagels et al. 1994] <author> M. A. Pagels, P. Druschel, and L. L. Peterson. </author> <title> Cache and TLB effectiveness in processing network I/O. </title> <type> Technical Report 94-08, </type> <institution> University of Arizona, </institution> <month> March </month> <year> 1994. </year>
Reference: [Pakin et al. 1995] <author> S. Pakin, M. Lauria, and A. Chien. </author> <title> High-performance messaging on workstations: Illinois Fast Messages(FM) for Myri-net. </title> <booktitle> In Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <year> 1995. </year>
Reference-contexts: Active Messages [von Eicken et al. 1992] is the base upon which Fast Sockets is built and is discussed above. Illinois Fast Messages <ref> [Pakin et al. 1995] </ref> provided an interface similar to that of previous versions of Active Messages, but did not allow processes to share the network.
Reference: [Peterson 1993] <author> L. L. Peterson. </author> <title> Life on the OS/network boundary. </title> <journal> Operating Systems Review, </journal> <volume> 27(2) </volume> <pages> 94-98, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: For small packets, however, protocol costs were significant, amounting for up to 33% of processing time for single-byte messages. The concept of reducing infrastructure costs was explored further in the x-kernel <ref> [Hutchinson & Pe-terson 1991, Peterson 1993] </ref>, an operating system designed for high-performance communications. The original, stand-alone version of the x-kernel performed significantly better at communication tasks than did BSD Unix on the same hardware (Sun 3's), for large files.
Reference: [Postel 1981a] <author> J. Postel. </author> <title> Internet protocol. Request For Comments 791, </title> <institution> IETF Network Working Group, </institution> <month> September </month> <year> 1981. </year>
Reference-contexts: Unfortunately, this raw network capacity goes unused, due to current network communication software. Most of these networks run the TCP/IP protocol suite <ref> [Postel 1981b, Postel 1981c, Postel 1981a] </ref>. TCP/IP is the default protocol suite for Internet traffic, and provides inter-operability among a wide variety of computing platforms and network technologies. The TCP protocol provides the abstraction of a reliable, ordered byte stream. The UDP protocol provides an unreliable, unordered datagram service.
Reference: [Postel 1981b] <author> J. Postel. </author> <title> Transmission control protocol. Request For CommentsC 793, </title> <institution> IETF Network Working Group, </institution> <month> September </month> <year> 1981. </year>
Reference-contexts: Unfortunately, this raw network capacity goes unused, due to current network communication software. Most of these networks run the TCP/IP protocol suite <ref> [Postel 1981b, Postel 1981c, Postel 1981a] </ref>. TCP/IP is the default protocol suite for Internet traffic, and provides inter-operability among a wide variety of computing platforms and network technologies. The TCP protocol provides the abstraction of a reliable, ordered byte stream. The UDP protocol provides an unreliable, unordered datagram service.
Reference: [Postel 1981c] <author> J. Postel. </author> <title> User datagram protocol. Request For Comments 768, </title> <institution> IETF Network Working Group, </institution> <month> August </month> <year> 1981. </year>
Reference-contexts: Unfortunately, this raw network capacity goes unused, due to current network communication software. Most of these networks run the TCP/IP protocol suite <ref> [Postel 1981b, Postel 1981c, Postel 1981a] </ref>. TCP/IP is the default protocol suite for Internet traffic, and provides inter-operability among a wide variety of computing platforms and network technologies. The TCP protocol provides the abstraction of a reliable, ordered byte stream. The UDP protocol provides an unreliable, unordered datagram service.
Reference: [Schoch & Hupp 1980] <author> J. F. Schoch and J. A. Hupp. </author> <title> Per--formance of an Ethernet local network. </title> <journal> Communications of the ACM, </journal> <volume> 23(12) </volume> <pages> 711-720, </pages> <month> De-cember </month> <year> 1980. </year>
Reference: [Seitz 1994] <author> C. Seitz. Myrinet: </author> <title> A gigabit-per-second local area network. In Hot Interconnects II, </title> <institution> Stanford University, Stanford, </institution> <address> CA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The development and deployment of high performance local-area networks such as ATM [de Prycker 1993], Myrinet <ref> [Seitz 1994] </ref>, and switched high-speed Ethernet has the potential to dramatically improve communication performance for network applications. These networks are capable of microsecond latencies and bandwidths of hundreds of megabits per second; their switched fabrics eliminate the contention seen on shared-bus networks such as traditional Ethernet. <p> The HP/UX platform consists of two 99Mhz HP735's interconnected by an FDDI network and using the Medusa network adapter [Banks & Prudence 1993]. The Solaris platform is a collection of UltraSPARC 1's connected via a Myri-net network <ref> [Seitz 1994] </ref>. For all tests, there was no other load on the network links or switches. Our microbenchmarks were run against a variety of TCP/IP setups. The standard HP/UX TCP/IP stack is well-tuned, but there is also a single-copy stack designed for use on the Medusa network interface. <p> Recent work has focused on reduc ing this portion of the protocol cost, and on utiliz-ing the message coprocessors that are appearing on high-performance network controllers such as Myri-net <ref> [Seitz 1994] </ref>. Active Messages [von Eicken et al. 1992] is the base upon which Fast Sockets is built and is discussed above. Illinois Fast Messages [Pakin et al. 1995] provided an interface similar to that of previous versions of Active Messages, but did not allow processes to share the network.
Reference: [Stevens 1990] <author> W. R. Stevens. </author> <title> UNIX Network Programming. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: Recovering Fast Sockets state lost during an exec () call can be done via a dedicated server process, where the Fast Socket's state is migrated to and from the server before and after the exec () similar to the method used by the user-level Transport Layer Interface <ref> [Stevens 1990] </ref>. The Fast Sockets library is currently single-threaded. This is problematic for current versions of Active Messages because an application must explicitly touch the network to receive messages. Since a user application could engage in an arbitrarily long computation, it is difficult to implement operations such as asynchronous I/O.
Reference: [Thekkath et al. 1993] <author> C. A. Thekkath, T. Nguyen, E. Moy, and E. D. Lazowska. </author> <title> Implementing network protocols at user-level. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <pages> pp. 554-565, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Mach 3.0 used the latter approach [Forin et al. 1991], which yielded poor networking performance [Maeda & Bershad 1992]. Both [Maeda & Bershad 1993a] and <ref> [Thekkath et al. 1993] </ref> explored building TCP into a user-level library linked with existing applications. Both systems, however, attempted only to match in-kernel performance, rather than better it. Further, both systems utilized in-kernel facilities for message transmission, limiting the possible performance improvement.
Reference: [von Eicken et al. 1992] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the Nineteenth ISCA, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: While this approach provides full compatibility with existing protocols, fundamental limitations of the protocol design may limit the performance gain. Recent systems, such as Active Messages <ref> [von Eicken et al. 1992] </ref>, Remote Queues [Brewer et al. 1995], and native U-Net [von Eicken et al. 1995], have used the first two methods; they implement new protocols and new programming interfaces to obtain improved local-area network performance. <p> Recent work has focused on reduc ing this portion of the protocol cost, and on utiliz-ing the message coprocessors that are appearing on high-performance network controllers such as Myri-net [Seitz 1994]. Active Messages <ref> [von Eicken et al. 1992] </ref> is the base upon which Fast Sockets is built and is discussed above. Illinois Fast Messages [Pakin et al. 1995] provided an interface similar to that of previous versions of Active Messages, but did not allow processes to share the network.
Reference: [von Eicken et al. 1995] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the Fifteenth SOSP, </booktitle> <pages> pp. 40-53, </pages> <address> Copper Mountain, CO, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: While this approach provides full compatibility with existing protocols, fundamental limitations of the protocol design may limit the performance gain. Recent systems, such as Active Messages [von Eicken et al. 1992], Remote Queues [Brewer et al. 1995], and native U-Net <ref> [von Eicken et al. 1995] </ref>, have used the first two methods; they implement new protocols and new programming interfaces to obtain improved local-area network performance. The protocols and interfaces are lightweight and provide programming abstractions that are similar to the underlying hardware. <p> However, none of them offer compatibility with existing applications. Other work has tried to improve performance by re-implementing TCP. Recent work includes zero-copy TCP for Solaris [Chu 1996] and a TCP interface for the U-Net interface <ref> [von Eicken et al. 1995] </ref>. These implementations can inter-operate with other TCP/IP implementations and improve throughput and latency relative to standard TCP/IP stacks. Both implementations can realize the full bandwidth of the network for large packets. However, both systems have round-trip latencies considerably higher than the raw network. <p> This facilitates the portability of Fast Sockets between different operating systems and network hardware. Active Messages implementations are available for the Intel Paragon [Liu & Culler 1995], FDDI [Martin 1994], Myrinet [Mainwaring & Culler 1995], and ATM <ref> [von Eicken et al. 1995] </ref>. Layering costs are kept low because Active Messages is a thin layer, and all of its implementation-dependent constants (such as maximum packet size) are exposed to higher layers. The Fast Sockets layer stays lightweight by exploiting Active Message handlers. <p> The SHRIMP communication model also deals poorly with non-word-aligned data, which required programming complexity to work around; Fast Sockets transparently handles this un-aligned data without extra data structures or other difficulties in the data path. U-Net <ref> [von Eicken et al. 1995] </ref> is a user-level network interface developed at Cornell. It virtualized the network interface, allowing multiple processes to share the interface. U-Net emphasized improving the implementation of existing communications protocols whereas Fast Sockets uses a new protocol just for local-area use.
Reference: [Watson & Mamrak 1987] <author> R. M. Watson and S. A. Mam-rak. </author> <title> Gaining efficiency in transport services by appropriate design and implementation choices. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(2) </volume> <pages> 97-120, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: In contrast, Fast Sockets provides the same models as TCP/IP and maintains application compatibility. Other work <ref> [Clark et al. 1989, Watson & Mamrak 1987] </ref> argued that protocol implementations, rather than protocol designs, were to blame for poor performance, and that efficient implementations of general-purpose protocols could do as well as or better than special-purpose protocols for most applications.
References-found: 52

