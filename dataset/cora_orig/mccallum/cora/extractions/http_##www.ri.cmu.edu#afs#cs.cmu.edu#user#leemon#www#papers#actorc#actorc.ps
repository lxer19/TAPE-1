URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/actorc/actorc.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/leemon/www/papers/index.html
Root-URL: 
Email: rjw@ccs.neu.edu  bairdlc@wL.wpafb.af.mil  
Title: Analysis of Some Incremental Variants of Policy Iteration: First Steps Toward Understanding Actor-Critic Learning Systems  
Author: Ronald J. Williams Leemon C. Baird, III 
Note: Wright-Patterson Air Force Base,  September 8, 1993  
Address: Boston, MA 02115  OH 45433-6543  
Affiliation: College of Computer Science Northeastern University  Wright Laboratory  
Abstract: Northeastern University College of Computer Science Technical Report NU-CCS-93-11 fl We gratefully acknowledge the substantial contributions to this effort provided by Andy Barto, who sparked our original interest in these questions and whose continued encouragement and insightful comments and criticisms have helped us greatly. Recent discussions with Satinder Singh and Vijay Gullapalli have also had a helpful impact on this work. Special thanks also to Rich Sutton, who has influenced our thinking on this subject in numerous ways. This work was supported by Grant IRI-8921275 from the National Science Foundation and by the U. S. Air Force. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1991). </year> <title> Real-time learning and control using asynchronous dynamic programming (COINS Technical Report 91-57). </title> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Barto, A. G., Sutton, R. S., & Anderson, C. W. </author> <year> (1983). </year> <title> Neuronlike elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13, </volume> <pages> 835-846. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning and sequential decision making (COINS Technical Report 89-95). </title> <address> Amherst, MA: </address> <institution> University of Massachusetts, Department of Computer and Information Science. </institution>
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. Engle-wood Cliffs, NJ: Prentice Hall. 8 I thank Vijay Gullapalli for first bringing this issue, along with some limited results along these lines, to my attention. Discussions with Peter Dayan have helped convince me of the the validity of the simpler result described here. </title> <note> 41 Bertsekas, </note> <author> D. P. & Tsitsiklis, J. N. </author> <year> (1989). </year> <title> Parallel and Distributed Computation: Numerical Methods. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference: <author> Dayan, P. </author> <year> (1992). </year> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8, </booktitle> <pages> 341-362. </pages>
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibility of general-purpose learning algorithms applied to rule-based systems. </title> <editor> In: R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.) </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, Volume II. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Jaakola, T., Jordan, M. I., & Singh, S. P. </author> <year> (1993). </year> <title> On the convergence of stochastic iterative dynamic programming algorithms (Working Paper). </title> <institution> Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology. </institution>
Reference: <author> Moore, A. W. & Atkeson, C. G., </author> <title> (1992) Memory-based reinforcement learning: Converging with less data and less real time, </title> <editor> In: S. J. Hanson, J. D. Cowan, & C. L. Giles (Eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Narendra, K. S. & Thathatchar, M. A. L. </author> <year> (1989). </year> <title> Learning Automata: An Introduction. </title> <address> Engle-wood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: In this case policy updates can correspond to small steps in this simplex, as in stochastic learning automata <ref> (Narendra & Thathatchar, 1989) </ref>. This is actually the way the actor works in the ASE/ACE architecture of Barto, Sutton, and Anderson (1983). Perhaps the theoretical tools recently developed by Jaakola, Jordan, and Singh (1993) can be used or extended to provide an analysis of such algorithms.
Reference: <author> Peng, J. & Williams, R. J. </author> <year> (1992). </year> <title> Efficient learning and planning within the Dyna framework, </title> <booktitle> Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <address> Hon-olulu, HI. </address>
Reference: <author> Puterman, M. L. & Shin, M. C. </author> <year> (1978). </year> <title> Modified policy iteration algorithms for discounted Markov decision problems. </title> <journal> Management Science, </journal> <volume> 24, </volume> <pages> 1127-1137. </pages>
Reference: <author> Ross, S. </author> <year> 1983). </year> <title> Introduction to Stochastic Dynamic Programming. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Some important elementary results from the theory of dynamic programming <ref> (see, e.g., Ross, 1983, or Bertsekas, 1987) </ref> that we will need are given by the following. Proposition 2.5.1 For any stationary policy , v (x) = L v for all x 2 X.
Reference: <author> Samuel, A. L. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 3, </volume> <pages> 210-229. </pages> <note> Reprinted in: </note> <editor> E. A. Feigenbaum and J. Feldman (Eds.) </editor> <booktitle> (1963). Computers and Thought. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Singh, S. P. </author> <year> (1993). </year> <title> Learning Control in Dynamic Environments. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Singh, S. P. & Gullapalli, V. </author> <year> (1993). </year> <title> Asynchronous policy iteration with single-sided updates (Working Paper). </title> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference: <author> Sutton, R. S. </author> <year> (1984). </year> <title> Temporal credit assignment in reinforcement learning. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <note> Amherst (also COINS Technical Report 84-02). </note>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> Proceedings of the Seventh International Conference in 42 Machine Learning, </booktitle> <pages> 216-224. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1991). </year> <title> Planning by incremental dynamic programming. </title> <booktitle> Proceedings of the 8th International Machine Learning Workshop. </booktitle>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> Ph.D. Dissertation, </type> <institution> Cambridge University, </institution> <address> Cambridge, England. </address>
Reference-contexts: 1 Introduction 1.1 Overview The purpose of this paper is to present some results in the theory of incremental dynamic programming <ref> (Watkins, 1989) </ref>, which, in its general form, combines elements of the theory of dynamic programming with features appropriate for on-line learning in the absence of an a priori model of the environment. <p> It is traditional to carry this out synchronously or by sweeping through the states cyclically, but more arbitrary ordering is also permissible (Bertsekas, 1987; Bertsekas & Tsitsik-lis, 1989). This result also serves as the starting point for analysis of the Q-learning algorithm <ref> (Watkins, 1989) </ref>. Here we restate this result in our terms and give a proof. 12 Before doing this, however, we introduce another local operator on value functions.
Reference: <author> Watkins, C. J. C. H. & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
Reference: <author> Werbos, P. J. </author> <year> (1987). </year> <title> Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 17, </volume> <pages> 7-20. </pages>
Reference: <author> Williams, R. J. & Baird, L. C., </author> <title> III (1990). A mathematical analysis of actor-critic architectures for learning optimal controls through incremental dynamic programming. </title> <booktitle> Proceedings of the Sixth Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> August 15-17, </pages> <address> New Haven, CT, </address> <pages> 96-101. </pages>
Reference-contexts: processing machines with asynchronous communication be 1 tween processors, the main issue being the extent to which the final result obtained is independent of the precise order in which updating occurs among processors. 1 Many of the results proved here first appeared in summary form in a short workshop paper <ref> (Williams & Baird, 1990) </ref>. 1.2 Incremental Dynamic Programming The theory of dynamic programming provides a general framework within which to define, and sometimes to solve, sequential decision tasks, in which an appropriate sequence of actions must be chosen in order to receive optimal cumulative rewards over time.
Reference: <author> Witten, I. H. </author> <year> (1977). </year> <title> An adaptive optimal controller for discrete-time Markov environments. </title> <journal> Information and Control, </journal> <volume> 34, </volume> <pages> 286-295. </pages>
References-found: 24

