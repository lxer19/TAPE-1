URL: http://www.cs.duke.edu/~magda/clustering-survey.ps
Refering-URL: http://www.cs.duke.edu/~magda/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Note: Chapter 1  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. K. Agarwal and M. Sharir, </author> <title> Planar geometric location problems, </title> <journal> Al-gorithmica, </journal> <volume> 11 (1994), </volume> <pages> 185-195. </pages>
Reference-contexts: An O (n d+1 ) algorithm exists for the d-dimensional case [18] (using the fact that there exists a hyperplane separating the two clusters). For the planar case, Agarwal and Sharir <ref> [1] </ref> gave a deterministic O (n 2 log n) algorithm for the associated decision problem of type (P3), which, plugged into a parametric search algorithm, yields a running time of O (n 2 log 3 n).
Reference: [2] <author> P. K. Agarwal and M. Sharir, </author> <title> Efficient algorithms for geometric optimization, </title> <type> Tech. Rep. </type> <institution> CS-1996-19, Dept. Computer Science, Duke University, </institution> <year> 1996. </year>
Reference-contexts: This fact has determined researchers to study the problem for the case when k is a small fixed number, so that a fast exact algorithm can be designed. We review here the most important results, and refer the reader to <ref> [2] </ref> for a more comprehensive survey. Euclidian 1-center Given the input set S, we want to compute the ball of smallest radius that covers S.
Reference: [3] <author> R. Agrawal, A. Ghosh, T. Imielinski, B. Iyer, and A. Swami, </author> <title> An interval classifier for database mining applications, </title> <booktitle> Proceedings of the 18th Conference on Very Large Databases, </booktitle> <publisher> Morgan Kauffman, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: After that, classification algorithms are employed in order to find a classification function for each group, that would allow efficient retrieval of all objects in the database that belong to that group. See <ref> [3, 62] </ref> for more information on classification algorithms. Recently, the advent of spatial databases has triggered studies in using clustering algorithms on the spatial attributes of the data [56, 44]. In a spatial database, objects of more complex types are allowed, such as points, lines, polygons.
Reference: [4] <author> M. R. Anderberg, </author> <title> Cluster Analysis for Applications, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The various heuristics currently used can be classified into two categories: hierarchical clustering and partitional clustering. The next two subsections present a summary of these categories, which is largely based on [41] and <ref> [4] </ref>. 2.6.1 Hierarchical Clustering Heuristics If C and C 0 are two partitions (clusterings) of the input, we say that C is nested into C 0 if every component of C is a proper subset of C 0 .
Reference: [5] <author> J. Bar-Ilan, G. Kortsarz, and D. Peleg, </author> <title> How to allocate network centers, </title> <journal> J. Algorithms, </journal> <volume> 15 (1993), </volume> <pages> 385-415. </pages>
Reference: [6] <author> M. Berger and I. Rigoutsos, </author> <title> An algorithm for point clustering and grid generation, </title> <journal> IEEE Trans. Systems, Man and Cybernetics., </journal> <volume> 21 (1991), </volume> <pages> 1278-1286. </pages>
Reference-contexts: There is also a cost determined by boundary conditions for each rectangular subgrid, thus making it necessary to use as few rectangles as possible. Two approaches are studied in <ref> [6] </ref>: a partitioning heuristic that chooses k initial points as cluster centers (k is the number of clusters), determines the clusters corresponding to them and then iteratively refines the cluster centers and the clusters (the general form of this method is detailed in the next section); and a divisive heuristic that
Reference: [7] <author> A. Bookstein, S. T. Klein, and T. Raita, </author> <title> Detecting content-bearing words by serial clustering, </title> <booktitle> Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1995, </year> <pages> pp. 319-327. </pages> <note> Extended Abstract. </note>
Reference-contexts: When such a word is encountered, it is simply ignored, and no position is assigned for it in the vector. We mention here a related idea proposed by Bookstein et al. <ref> [7] </ref>: they argue that whether a word is relevant or not for a set of documents may depend on the documents themselves (i.e. some words may be relevant for a corpus, and irrelevant for another one).
Reference: [8] <author> R. A. Botafogo, </author> <title> Cluster analysis for hypertext systems, </title> <booktitle> Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1993, </year> <pages> pp. 116-125. </pages>
Reference-contexts: For example, in the life sciences, the data to be clustered consists of life forms and the clusters themselves are species, or subspecies of a given species. The need for using clustering alghorithms is determined by the scientists' wish to study various characteristics of the life forms. <ref> [8] </ref> mentions a situation where mammals are clustered by similarities in the proportion of three chemicals in their milk. The "natural association" of the animals depends on the percentages of these chemicals in their milk. <p> An interesting and recent application of clustering is identifying nodes of information on the Internet that are highly related to each other. In particular, hypertext links are clustered according to similarities in the information 21 they contain. In <ref> [8] </ref>, the hypertext is seen as a directed graph, which is then transformed into an undirected graph.
Reference: [9] <author> T. Brinkhoff and H.-P. Kriegel, </author> <title> The impact of global clusterings on spatial database systems, </title> <booktitle> Proceedings of the International Conference on Very Large Databases, </booktitle> <publisher> Morgan Kauffman, </publisher> <month> September </month> <year> 1994. </year> <month> 51 </month>
Reference-contexts: The clustering relies heavily on the spatial access method used in the database, which is usually an R fl -tree. Objects are clustered together if they are assigned to adjacent leaves <ref> [9] </ref>, or in a bottom-up manner that minimizes the number of I/O's on the path from the root to the leaf [16]. 2.5.4 Image Processing Applications An image processing system transforms continuous images taken from a camera into digital data, and then processes that data to determine features in an image.
Reference: [10] <author> P. Brucker, </author> <title> On the complexity of clustering problems, in: Optimization and Operations Research, </title> <publisher> Springer-Verlag, </publisher> <year> 1977. </year>
Reference-contexts: That is why heuristics are widely used in applications, although they can have significant drawbacks. 2.1 Complexity Results In one dimension, pairwise and central clustering can be solved in polynomial time by dynamic programming <ref> [10] </ref>. In two dimensions, central clustering is NP-complete [26, 54], even when an approximate solution is sought [24, 29, 46]. For metrics satisfying the triangle inequality, pairwise clustering reduces to central clustering, and is thus also NP-complete.
Reference: [11] <author> B. Chazelle and J. Matousek, </author> <title> On linear-time deterministic algorithms for optimization problems in fixed dimension, </title> <booktitle> Proc. 4th ACM-SIAM Sympos. Discrete Algorithms, </booktitle> <year> 1993, </year> <pages> pp. 281-290. </pages>
Reference-contexts: Deterministic algorithms for this problem run in linear time for the planar case [21], and in time d O (d) n in d dimensions <ref> [11] </ref> (using a derandomization of Clarkson's algorithm for LP-type problems [12]). <p> There are also various extensions to this problem, such as the weighted 1-center problem, for which a randomized algorithm in time O (n log n) was proposed by Megiddo and Zemel [55], and the smallest enclosing ellipsoid of S <ref> [11, 67] </ref>. Euclidian 2-center For the Euclidian 2-center problem, we want to cover S by two balls of same smallest radius. An O (n d+1 ) algorithm exists for the d-dimensional case [18] (using the fact that there exists a hyperplane separating the two clusters).
Reference: [12] <author> K. L. Clarkson, </author> <title> A Las Vegas algorithm for linear programming when the dimension is small, </title> <booktitle> Proc. 29th Annu. IEEE Sympos. </booktitle> <institution> Found. Comput. Sci., </institution> <year> 1988, </year> <pages> pp. 452-456. </pages>
Reference-contexts: Deterministic algorithms for this problem run in linear time for the planar case [21], and in time d O (d) n in d dimensions [11] (using a derandomization of Clarkson's algorithm for LP-type problems <ref> [12] </ref>). There are also various extensions to this problem, such as the weighted 1-center problem, for which a randomized algorithm in time O (n log n) was proposed by Megiddo and Zemel [55], and the smallest enclosing ellipsoid of S [11, 67].
Reference: [13] <author> G. Coleman and H. Andrews, </author> <title> Image segmentation by clustering, </title> <booktitle> Proceedings of the IEEE., </booktitle> <year> 1979, </year> <pages> pp. 773-785. </pages>
Reference-contexts: The problem becomes more complex, due to the fact that clusters of the feature vectors are not necessarily connected in the xy plane. Various methods for finding an initial clustering and refining it to observe the spatial constraints have 23 been studied <ref> [13, 43, 66, 61] </ref>. Jain and Dubes [41] discuss three types of images for which segmentation algorithms are employed, each of which determines a distinct feature vector. In the case of textured images, the vector of a pixel must reflect textural qualities such as coarseness or regularity.
Reference: [14] <author> D. R. Cutting, D. R. Karger, and J. O. Pedersen, </author> <title> Constant interaction-time scatter/gather browsing of very large document collections, </title> <booktitle> Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1993, </year> <pages> pp. 126-134. </pages>
Reference-contexts: The slower, more precise algortihms are used off-line to organize the documents, while faster algorithms are used online for reclustering at user's request <ref> [15, 14, 32] </ref>. Two problems are central for implementing such a clustering algorithm: 1. How to represent a document as a d dimensional object? 2. What should the distance between two documents be? The answer to the first question is the vector space representation of documents.
Reference: [15] <author> D. R. Cutting, D. R. Karger, J. O. Pedersen, and J. W. Tukey, Scatter/gather: </author> <title> A cluster-based approach to browsing large document collections, </title> <booktitle> Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1992, </year> <pages> pp. 318-329. </pages>
Reference-contexts: The slower, more precise algortihms are used off-line to organize the documents, while faster algorithms are used online for reclustering at user's request <ref> [15, 14, 32] </ref>. Two problems are central for implementing such a clustering algorithm: 1. How to represent a document as a d dimensional object? 2. What should the distance between two documents be? The answer to the first question is the vector space representation of documents.
Reference: [16] <author> A. A. Diwan, S. Rane, S. Seshadri, and S. Sudarshan, </author> <title> Clustering techniques for minimizing external path length., </title> <booktitle> Proceedings of the International Conference on Very Large Databases, </booktitle> <publisher> Morgan Kauffman, </publisher> <year> 1996. </year>
Reference-contexts: Objects are clustered together if they are assigned to adjacent leaves [9], or in a bottom-up manner that minimizes the number of I/O's on the path from the root to the leaf <ref> [16] </ref>. 2.5.4 Image Processing Applications An image processing system transforms continuous images taken from a camera into digital data, and then processes that data to determine features in an image.
Reference: [17] <author> Z. Drezner, </author> <title> The p-centre problems | Heuristic and optimal algorithms, </title> <journal> J. Oper. Res. Soc., </journal> <volume> 35 (1984), </volume> <pages> 741-748. </pages>
Reference-contexts: Since it takes O (n) time to verify whether a specific choice of balls covers the entire set S, this naive algorithm works in O (n dk+2 ). For the planar case, Drezner <ref> [17] </ref> gave an algorithm that runs in O (n 2k+1 log n) time, and can be improved to O (n 2k1 log n) by combining it with the result that the Euclidian 1-center problem can be solved in O (n) time [53, 21]. <p> There exists a k-clustering of S of smallest size so that each ball in the cluster is defined by at most d + 1 points of S <ref> [17] </ref>. Then, w fl is the diameter of one of the balls defined by d + 1 input points. There are O (n d+1 ) diameters of balls determined by d + 1 points of S, and we can compute each one in time O (d 3 ).
Reference: [18] <author> Z. Drezner, </author> <title> The planar two-center and two-median problem, </title> <journal> Transp. Sci., </journal> <volume> 18 (1984), </volume> <pages> 351-361. </pages>
Reference-contexts: Euclidian 2-center For the Euclidian 2-center problem, we want to cover S by two balls of same smallest radius. An O (n d+1 ) algorithm exists for the d-dimensional case <ref> [18] </ref> (using the fact that there exists a hyperplane separating the two clusters).
Reference: [19] <author> Z. Drezner, </author> <title> On the rectangular p-center problem, </title> <institution> Naval Res. Logist. Q., </institution> <month> 34 </month> <year> (1987), </year> <pages> 229-234. </pages>
Reference-contexts: A recent result by Agarwal et al. gives an O (n 4=3 log 5 n) algorithm for the discrete Euclidian 2-center problem. Rectiliniar k-center For k = 1, the problem is trivially solved in linear time. For the planar case, linear algorithms are known for k = 2 <ref> [19] </ref>, and for k = 3 [65]. An O (n log n) algorithm is known for k = 4, and an O (n log 5 n) algorithm was proposed for k = 5 [65].
Reference: [20] <author> Z. Drezner, ed., </author> <title> Facility Location, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: The book by Drezner <ref> [20] </ref> describes many other variants of clustering problems. 14 The weighted clustering is an expansion of discrete k-center clustering, for which a positive weight w (p) is assigned to each point p 2 S.
Reference: [21] <author> M. E. Dyer, </author> <title> On a multidimensional search technique and its application to the Euclidean one-centre problem, </title> <journal> SIAM J. Comput., </journal> <volume> 15 (1986), </volume> <pages> 725-738. </pages>
Reference-contexts: For the planar case, Drezner [17] gave an algorithm that runs in O (n 2k+1 log n) time, and can be improved to O (n 2k1 log n) by combining it with the result that the Euclidian 1-center problem can be solved in O (n) time <ref> [53, 21] </ref>. <p> This is an LP-type problem of combinatorial dimension d+1 [64, 67], and can be solved in expected running time O (d 2 n)+ 2 O ( d log d) [28]. Deterministic algorithms for this problem run in linear time for the planar case <ref> [21] </ref>, and in time d O (d) n in d dimensions [11] (using a derandomization of Clarkson's algorithm for LP-type problems [12]).
Reference: [22] <author> M. E. Dyer and A. M. Frieze, </author> <title> A simple heuristic for the p-centre prob-lem, </title> <journal> Oper. Res. Lett., </journal> <volume> 3 (1985), </volume> <pages> 285-288. </pages>
Reference-contexts: The problem is to find a subset C S of k cluster centers, so that the cluster size is minimized, where the cluster size is defined as: cluster size = max s i 2S c j 2C Dyer and Frieze <ref> [22] </ref> adapted the Greedy Algorithm algorithm of Gonzalez (see Section 3) to produce a solution whose approximation factor of the cluster size is bounded by minf3; 1 + ffg, where ff is the ratio between the maximum and minimum weights of the points in S.
Reference: [23] <author> D. Eppstein, </author> <title> Faster construction of planar two-centers, </title> <booktitle> Proc. 8th ACM-SIAM Sympos. Discrete Algorithms, </booktitle> <year> 1997. </year>
Reference-contexts: This result was recently improved by Sharir [63], who gave an O (n log 9 n) deterministic algorithm. Using randomization, Eppstein <ref> [23] </ref> has proposed an algorithm with expected running time O (n log 2 n). A recent result by Agarwal et al. gives an O (n 4=3 log 5 n) algorithm for the discrete Euclidian 2-center problem. Rectiliniar k-center For k = 1, the problem is trivially solved in linear time.
Reference: [24] <author> T. Feder and D. H. Greene, </author> <title> Optimal algorithms for approximate clustering, </title> <booktitle> Proc. 20th Annu. ACM Sympos. Theory Comput., </booktitle> <year> 1988, </year> <pages> pp. 434-444. </pages>
Reference-contexts: In two dimensions, central clustering is NP-complete [26, 54], even when an approximate solution is sought <ref> [24, 29, 46] </ref>. For metrics satisfying the triangle inequality, pairwise clustering reduces to central clustering, and is thus also NP-complete. <p> Gonzalez [29] proved that pairwise L 2 clustering is NP-hard for an approximation factor ff &lt; 1:732 in 2 dimensions, and for ff &lt; 2 in 3 dimensions. The best known bounds for the approximation factors were proved by Feder and Greene <ref> [24] </ref>: 1.82 for central L 2 clustering, 1.97 for pairwise L 2 clustering and 2 for pairwise and central L 1 and L 1 clustering (all results are in d 2 dimensions). <p> Feder and Greene <ref> [24] </ref> proved that the suppliers problem is NP-hard in d 2 dimensions for approximation factors in the cluster size less than 2.65 for the L 2 and less than 3 for the L 1 and L 1 metrics. <p> It has been later improved to O (n log k) by Feder and Greene <ref> [24] </ref> by using a box decomposition scheme that first groups the points into rectangular boxes, producing a solution with an approximation factor of 6, and then refines the clustering to obtain the desired approximation factor of 2. <p> The improvement comes from an algorithm that finds the exact solution on a strip. We will present this algorithm in more detail in the last part of this paper. Very similar ideas appear in <ref> [24] </ref>, which also introduces the idea of ap proximating both the cluster size and the number of clusters in order to reduce the running time. In [30], Gonzalez also proposes an O (dn + n log k) algorithm that computes a clustering with 2 d1 k fl clusters. <p> Although this is superpolynomial in k, we expect it to be fairly fast for problems in which k is not very large. If we only allow polynomial time, it is unlikely that one can achieve an approximation factor better than 2 <ref> [24] </ref>. In the following, w fl denotes the optimal cluster size of a k-clustering of S. <p> We present the algorithm for the L 1 metric, although it can also be used for the L 2 metric. The algorithm proceeds as follows: First, run the Feder-Greene algorithm <ref> [24] </ref>, in time O (n log k). The algorithm returns a cover of S by k hypercubes of size w 0 , so that w fl w 0 2w fl . Then, draw a grid of step w 0 *=6.
Reference: [25] <author> M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huang, B. Dom, M. Gorkani, J. Hafner, D. Lee, D. Petkovic, D. Steele, and P. Yanker, </author> <title> Query by image and video content: </title> <booktitle> The QBIC system, Computer, 28 (1995), </booktitle> <pages> 23-32. </pages>
Reference-contexts: Although in most of the cases it is considered that the collection consists of text documents, recent trends try to address the problem of multimedia collections <ref> [25, 52, 31] </ref>. Document clustering has been used in connection with a number of applications, such as query methods, browsing methods, text categorization, and hypertext analysis.
Reference: [26] <author> R. J. Fowler, M. S. Paterson, and S. L. Tanimoto, </author> <title> Optimal packing and covering in the plane are NP-complete, </title> <journal> Inform. Process. Lett., </journal> <volume> 12 (1981), </volume> <pages> 133-137. </pages>
Reference-contexts: That is why heuristics are widely used in applications, although they can have significant drawbacks. 2.1 Complexity Results In one dimension, pairwise and central clustering can be solved in polynomial time by dynamic programming [10]. In two dimensions, central clustering is NP-complete <ref> [26, 54] </ref>, even when an approximate solution is sought [24, 29, 46]. For metrics satisfying the triangle inequality, pairwise clustering reduces to central clustering, and is thus also NP-complete.
Reference: [27] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness, </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> New York, NY, </address> <year> 1979. </year>
Reference-contexts: We present here the main ideas of the proof by Megiddo and Supowit [54] for rectiliniar k-center (the proof for Euclidian k-center is quite similar). They showed that the 3-SAT problem <ref> [27] </ref> can be reduced to the following square-hitting problem: (SH) Given a set of n squares of fixed side-length w and an integer k, is there a set of k points in the plane so that each square covers at least one such point (i.e. is "hit" by at least one
Reference: [28] <author> B. Gartner, </author> <title> A subexponential algorithm for abstract optimization problems, </title> <journal> SIAM J. Comput., </journal> <volume> 24 (1995), </volume> <pages> 1018-1035. </pages>
Reference-contexts: Euclidian 1-center Given the input set S, we want to compute the ball of smallest radius that covers S. This is an LP-type problem of combinatorial dimension d+1 [64, 67], and can be solved in expected running time O (d 2 n)+ 2 O ( d log d) <ref> [28] </ref>. Deterministic algorithms for this problem run in linear time for the planar case [21], and in time d O (d) n in d dimensions [11] (using a derandomization of Clarkson's algorithm for LP-type problems [12]).
Reference: [29] <author> T. Gonzalez, </author> <title> Clustering to minimize the maximum intercluster distance, </title> <type> Theoret. </type> <institution> Comput. Sci., </institution> <month> 38 </month> <year> (1985), </year> <pages> 293-306. </pages>
Reference-contexts: In two dimensions, central clustering is NP-complete [26, 54], even when an approximate solution is sought <ref> [24, 29, 46] </ref>. For metrics satisfying the triangle inequality, pairwise clustering reduces to central clustering, and is thus also NP-complete. <p> They also proved that k-median is NP-hard. Later, Ko et al. [46] improved the bound for the L 1 metric to 2. Gonzalez <ref> [29] </ref> proved that pairwise L 2 clustering is NP-hard for an approximation factor ff &lt; 1:732 in 2 dimensions, and for ff &lt; 2 in 3 dimensions. <p> We will review the most important results in both directions in the next subsections. 2.3.1 Approximating the Cluster Size We review two general techniques that generate constant approximation factors for a number of clustering problems: the Greedy Algorithm and the 10 Bottleneck Approach. The Greedy Algorithm, due to Gonzalez <ref> [29] </ref>, works for both pairwise and central clustering under any L q metric, as well as for discrete central clustering, and achieves an approximation factor of 2. In view of the known lower bound results, it's unlikely one can do better for the L 1 and L 1 met-rics. <p> This is the min-sum problem for S and k with the square-error distance function. Adapting the argument of <ref> [29] </ref>, Lin and Storer [47] proved that this problem is NP-complete. The problem was already known to be NP-complete for the Euclidian and the rectiliniar metrics [54], when d 2. We are not aware of any approximation algorithm for the min-sum problem with the square-error metric.
Reference: [30] <author> T. Gonzalez, </author> <title> Covering a set of points in multidimensional space, </title> <journal> Inform. Process. Lett., </journal> <volume> 40 (1991), </volume> <pages> 181-188. </pages>
Reference-contexts: This last problem is solved by brute force, by simply analyzing all the (bounded number of) possible ways to cover the points in a square of size l D by disks of diameter D. An improvement of this method was later proposed by Gonzalez <ref> [30] </ref>, whose algorithm generates a cover of (1 + *)k fl clusters in time O (d d=2+1 (2d=*) 2d2 n B 1 d (2=*) d1 +1 ) for covering with disks, and in time O (d (d=*) 2d2 n B 2 d (1=*) d1 +1 ) for covering with squares or <p> We will present this algorithm in more detail in the last part of this paper. Very similar ideas appear in [24], which also introduces the idea of ap proximating both the cluster size and the number of clusters in order to reduce the running time. In <ref> [30] </ref>, Gonzalez also proposes an O (dn + n log k) algorithm that computes a clustering with 2 d1 k fl clusters. He divides the space into strips of width D, and generates an exact cover of the input points lying in each strip. <p> The basic idea is to "decompose" the plane into "thin" horizontal strips, each of width at most p k, apply the (modified) algorithm of <ref> [30] </ref> on each strip, and then determine the optimal cover of the entire set from the optimal covers on the strips. <p> Modified Gonzalez's Algorithm: Definition 8 We say that an anchored cover C of S is an l-cover with respect to S if a vertical line through any point of S intersects at most 2l 1 squares of C (l is a positive integer). The proof of Lemma 2 in <ref> [30] </ref> directly implies the following Lemma 9 If all points of S lie in a horizontal strip of height l, then the optimal cover of S is an l-cover. <p> Gonzalez's algorithm works in time O (d (2l d1 1)n 2dl d1 44 on strips for which j q i q l for any q 2 f1; : : : ; d 1g (note: this is different from the result stated in <ref> [30] </ref> because there is a typo in that paper). We extend the notations from the previous subsection as follows: Definition 12 Let C be a cover of S.
Reference: [31] <author> K.-A. Han and S.-H. Myaeng, </author> <title> Image organization and retrieval with automatically constructed feature vectors, </title> <booktitle> Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (H.-P. </booktitle> <editor> Frei, D. Harman, P. Schauble, and R. Wilkinson, eds.), </editor> <publisher> ACM Press, </publisher> <month> August </month> <year> 1996, </year> <pages> pp. 157-165. </pages>
Reference-contexts: Although in most of the cases it is considered that the collection consists of text documents, recent trends try to address the problem of multimedia collections <ref> [25, 52, 31] </ref>. Document clustering has been used in connection with a number of applications, such as query methods, browsing methods, text categorization, and hypertext analysis.
Reference: [32] <author> M. A. Hearst and J. O. Pedersen, </author> <title> Reexamining the cluster hypothesis: </title> <booktitle> Scatter/gather on retrieval results, Proceedings of the Nineteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (H.-P. </booktitle> <editor> Frei, D. Harman, P. Schauble, and R. Wilkinson, eds.), </editor> <publisher> ACM Press, </publisher> <month> August </month> <year> 1996, </year> <pages> pp. 76-84. 53 </pages>
Reference-contexts: The slower, more precise algortihms are used off-line to organize the documents, while faster algorithms are used online for reclustering at user's request <ref> [15, 14, 32] </ref>. Two problems are central for implementing such a clustering algorithm: 1. How to represent a document as a d dimensional object? 2. What should the distance between two documents be? The answer to the first question is the vector space representation of documents.
Reference: [33] <author> J. Hershberger, </author> <title> A faster algorithm for the two-center decision problem, </title> <journal> Inform. Process. Lett., </journal> <volume> 47 (1993), </volume> <pages> 23-29. </pages>
Reference-contexts: The running time of the decision procedure was later improved to O (n 2 ) <ref> [33] </ref>, and the result was used to obtain a running time 16 of O (n 2 log n) for the optimization problem [42]. This result was recently improved by Sharir [63], who gave an O (n log 9 n) deterministic algorithm.
Reference: [34] <author> D. S. Hochbaum and W. Maass, </author> <title> Approximation schemes for covering and packing problems in image processing and VLSI, </title> <journal> J. ACM, </journal> <volume> 31 (1984), </volume> <pages> 130-136. </pages>
Reference-contexts: See also [46] for a related algorithm. 2.3.2 Approximating the Number of Clusters One of the most important techniques that allows various approximate clus-terings was proposed by Hochbaum and Maass <ref> [34] </ref> and is called the Shifting Strategy.
Reference: [35] <author> D. S. Hochbaum and D. Shmoys, </author> <title> A best possible heuristic for the k-center problem, </title> <journal> Math. Oper. Res., </journal> <volume> 10 (1985), </volume> <pages> 180-184. </pages>
Reference-contexts: Feder and Greene also proved that their algorithm is optimal under the algebraic decision tree model. A different approximation algorithm, which achieves the approximation factor of 2 for discrete k-center clustering, was proposed by Hochbaum and Shmoys <ref> [35] </ref> in the context of clustering on graphs. Although the running time is not efficient, the algorithm is significant as being an application of a 11 general method, called a unified approach to bottleneck problems [36], which can be used to solve other types of NP-complete problems defined on graphs. <p> The reader is referred to [36] for the complete collection of implementations of the decision procedure, and to <ref> [35] </ref> for a more ellaborate presentation of the algorithm in the particular case of discrete k-center clustering. <p> Plesnik [58] adapted the algorithm by Hochbaum and Shmoys <ref> [35, 36] </ref> to get an approximation factor of at most 2 for the same problem, defined in the context of weighted graphs.
Reference: [36] <author> D. S. Hochbaum and D. Shmoys, </author> <title> A unified approach to approximation algorithms for bottleneck problems, </title> <journal> J. ACM, </journal> <volume> 33 (1986), </volume> <pages> 533-550. </pages>
Reference-contexts: In the case of a generic metric satisfying the triangle inequality a bound of 3 was previously proven by H.Karloff (communicated in <ref> [36] </ref>). We present here the main ideas of the proof by Megiddo and Supowit [54] for rectiliniar k-center (the proof for Euclidian k-center is quite similar). <p> NP-completeness results for pairwise and discrete center clustering approximations with a factor ff &lt; 2 were also proved by Hochbaum and Shmoys <ref> [36] </ref>, but their proof is for a nongeometric version of the clustering problem, which uses a generic metric satisfying the triangle inequality. <p> Although the running time is not efficient, the algorithm is significant as being an application of a 11 general method, called a unified approach to bottleneck problems <ref> [36] </ref>, which can be used to solve other types of NP-complete problems defined on graphs. <p> Indeed, if a k-clustering was possible in G i , then at least 2 points of H would end up in the same cluster, so there would be a path of length 2 between them, contradiction. The reader is referred to <ref> [36] </ref> for the complete collection of implementations of the decision procedure, and to [35] for a more ellaborate presentation of the algorithm in the particular case of discrete k-center clustering. <p> Plesnik [58] adapted the algorithm by Hochbaum and Shmoys <ref> [35, 36] </ref> to get an approximation factor of at most 2 for the same problem, defined in the context of weighted graphs. <p> Bar-Ilan et al.[5] give an algorithm that achieves an approximation factor of at most 10 for the capacitated discrete k-center problem in the context of graphs. Their approach is to use the general technique by Hochbaum and Shmoys <ref> [36] </ref> in a first phase, to obtain an initial set of cluster centers for the unca-pacitated case. The second phase only takes into consideration the clusters that have more points than their capacity.
Reference: [37] <author> W. L. Hsu and G. L. Nemhauser, </author> <title> Easy and hard bottleneck location problems, </title> <journal> Discr. Appl. Math., </journal> <volume> 1 (1979), </volume> <pages> 209-215. </pages>
Reference-contexts: In the case of a generic metric satisfying the triangle inequality, a bound of 2 was proved by Hsu and Nemhauser <ref> [37] </ref>. A relaxation of discrete center clustering, known as the suppliers problem is defined by considering two (not necessarily disjoint) sets S cust and S sup instead of S, such that S cust contains all points we want to cluster, and S sup contains all possible candidates to cluster centers.
Reference: [38] <author> R. Z. Hwang, R. C. Chang, and R. C. T. Lee, </author> <title> The generalized searching over separators strategy to solve some NP-Hard problems in subexpo-nential time, </title> <journal> Algorithmica, </journal> <volume> 9 (1993), </volume> <pages> 398-423. </pages>
Reference-contexts: Using this method, the problem can be solved by doing a binary search on the set of possible cluster sizes and using a divide-and-conquer approach at each step. A different algorithm that also runs in n O ( p k) time was given in <ref> [38] </ref>, as an application of a general method called the search over separators strategy. The method also applies to the Euclidian discrete k-center problem, and the Euclidian discrete k-median problem. <p> For d = 2, our algorithm solves the Euclidian k-center problem in n O ( k) , but it is simpler than the ones described in [39] and <ref> [38] </ref>, and the constant hidden in the big-Oh notation is small and easily computable. <p> If k is part of the input, most problems of type (P1) are NP-hard (see the previous chapter). For the planar case and under the L 2 metric, there exist two subexpo-nential algorithms due to Hwang et al. <ref> [38, 39] </ref> that solve (P1) exactly. We propose a subexponential algorithm that solves problems of type (P1) under the L 1 or L 2 metric for any dimension d 2. The algorithm uses a decision procedure that answers queries of type (P3) in subexponential time.
Reference: [39] <author> R. Z. Hwang, R. C. T. Lee, and R. C. Chang, </author> <title> The slab dividing approach to solve the Euclidean p-center problem, </title> <journal> Algorithmica, </journal> <volume> 9 (1993), </volume> <pages> 1-22. </pages>
Reference-contexts: These results were later improved to n O ( p k) by Hwang et al. <ref> [39] </ref>, who introduced the slab dividing method: they showed that one can find a slab that divides the input points of S into two disjoint sets so that the optimal 9 clustering of each set has at most 2k=3 clusters. <p> For d = 2, our algorithm solves the Euclidian k-center problem in n O ( k) , but it is simpler than the ones described in <ref> [39] </ref> and [38], and the constant hidden in the big-Oh notation is small and easily computable. <p> If k is part of the input, most problems of type (P1) are NP-hard (see the previous chapter). For the planar case and under the L 2 metric, there exist two subexpo-nential algorithms due to Hwang et al. <ref> [38, 39] </ref> that solve (P1) exactly. We propose a subexponential algorithm that solves problems of type (P1) under the L 1 or L 2 metric for any dimension d 2. The algorithm uses a decision procedure that answers queries of type (P3) in subexponential time.
Reference: [40] <author> M. Iwayama and T. Tokunaga, </author> <title> Cluster-based text categorization: A comparison of category search strategies, </title> <booktitle> Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1995, </year> <pages> pp. 273-280. </pages>
Reference-contexts: are assigned the same category into a cluster labeled by that category; see the documents as vectors (in the sense presented above) and cluster them using the Eu-clidian distance as criterion of similarity; use a probabilistic algorithm, that estimates the most likely set of clusters from the given training data <ref> [40] </ref>. An interesting and recent application of clustering is identifying nodes of information on the Internet that are highly related to each other. In particular, hypertext links are clustered according to similarities in the information 21 they contain.
Reference: [41] <author> A. K. Jain and R. C. Dubes, </author> <title> Algorithms for Clustering Data, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: The problem becomes more complex, due to the fact that clusters of the feature vectors are not necessarily connected in the xy plane. Various methods for finding an initial clustering and refining it to observe the spatial constraints have 23 been studied [13, 43, 66, 61]. Jain and Dubes <ref> [41] </ref> discuss three types of images for which segmentation algorithms are employed, each of which determines a distinct feature vector. In the case of textured images, the vector of a pixel must reflect textural qualities such as coarseness or regularity. <p> The various heuristics currently used can be classified into two categories: hierarchical clustering and partitional clustering. The next two subsections present a summary of these categories, which is largely based on <ref> [41] </ref> and [4]. 2.6.1 Hierarchical Clustering Heuristics If C and C 0 are two partitions (clusterings) of the input, we say that C is nested into C 0 if every component of C is a proper subset of C 0 .
Reference: [42] <author> J. W. Jaromczyk and M. Kowaluk, </author> <title> An efficient algorithm for the Eu-clidean two-center problem, </title> <booktitle> Proc. 10th Annu. ACM Sympos. </booktitle> <institution> Comput. Geom., </institution> <year> 1994, </year> <pages> pp. 303-311. </pages>
Reference-contexts: The running time of the decision procedure was later improved to O (n 2 ) [33], and the result was used to obtain a running time 16 of O (n 2 log n) for the optimization problem <ref> [42] </ref>. This result was recently improved by Sharir [63], who gave an O (n log 9 n) deterministic algorithm. Using randomization, Eppstein [23] has proposed an algorithm with expected running time O (n log 2 n).
Reference: [43] <author> J. Jolion, P. Meer, and S. Batauche, </author> <title> Robust clustering with applications in computer vision, </title> <journal> IEEE Trans. Pattern Analysis Mach. Intell., </journal> <volume> 13 (1991), </volume> <pages> 791-802. </pages>
Reference-contexts: The problem becomes more complex, due to the fact that clusters of the feature vectors are not necessarily connected in the xy plane. Various methods for finding an initial clustering and refining it to observe the spatial constraints have 23 been studied <ref> [13, 43, 66, 61] </ref>. Jain and Dubes [41] discuss three types of images for which segmentation algorithms are employed, each of which determines a distinct feature vector. In the case of textured images, the vector of a pixel must reflect textural qualities such as coarseness or regularity.
Reference: [44] <author> L. Kaufman and P. J. Rousseeuw, </author> <title> Finding Groups in Data: An Introduction to Cluster Analysis, </title> <publisher> John Wiley & Sons, </publisher> <year> 1990. </year>
Reference-contexts: See [3, 62] for more information on classification algorithms. Recently, the advent of spatial databases has triggered studies in using clustering algorithms on the spatial attributes of the data <ref> [56, 44] </ref>. In a spatial database, objects of more complex types are allowed, such as points, lines, polygons. The user may want to receive as answer to a query all objects that are spatially close to each other.
Reference: [45] <author> M. T. Ko and R. C. T. Lee, </author> <title> On weighted rectilinear 2-center and 3-center problems, </title> <journal> Inform. Sci., </journal> <volume> 54 (1991), </volume> <pages> 169-190. 54 </pages>
Reference-contexts: An O (n log n) algorithm is known for k = 4, and an O (n log 5 n) algorithm was proposed for k = 5 [65]. For the rectiliniar 2-center weighted problem, Ko and Lee gave an O (n log n) algorithm in the planar case <ref> [45] </ref>. 2.5 Applications Clustering algorithms have been employed in many fields of human knowledge that require finding a "natural association" among some specific data. The way the term "natural association" is defined depends on the field and the particular application that uses it, and can vary quite a lot.
Reference: [46] <author> M. T. Ko, R. C. T. Lee, and J. S. Chang, </author> <title> An optimal approximation algorithm for the rectilinear m-center problem, </title> <journal> Algorithmica, </journal> <volume> 5 (1990), </volume> <pages> 341-352. </pages>
Reference-contexts: In two dimensions, central clustering is NP-complete [26, 54], even when an approximate solution is sought <ref> [24, 29, 46] </ref>. For metrics satisfying the triangle inequality, pairwise clustering reduces to central clustering, and is thus also NP-complete. <p> They also proved that k-median is NP-hard. Later, Ko et al. <ref> [46] </ref> improved the bound for the L 1 metric to 2. Gonzalez [29] proved that pairwise L 2 clustering is NP-hard for an approximation factor ff &lt; 1:732 in 2 dimensions, and for ff &lt; 2 in 3 dimensions. <p> The reader is referred to [36] for the complete collection of implementations of the decision procedure, and to [35] for a more ellaborate presentation of the algorithm in the particular case of discrete k-center clustering. See also <ref> [46] </ref> for a related algorithm. 2.3.2 Approximating the Number of Clusters One of the most important techniques that allows various approximate clus-terings was proposed by Hochbaum and Maass [34] and is called the Shifting Strategy.
Reference: [47] <author> J. Lin and J. A. Storer, </author> <title> Geometric clustering to minimize the mean-square error, </title> <type> Unpublished Manuscript, </type> <year> (1997). </year>
Reference-contexts: This is the min-sum problem for S and k with the square-error distance function. Adapting the argument of [29], Lin and Storer <ref> [47] </ref> proved that this problem is NP-complete. The problem was already known to be NP-complete for the Euclidian and the rectiliniar metrics [54], when d 2. We are not aware of any approximation algorithm for the min-sum problem with the square-error metric. <p> We are not aware of any approximation algorithm for the min-sum problem with the square-error metric. The current approach to solving this problem is by using heuristics (see Section 2.6). However, <ref> [47] </ref> proves that solutions obtained by the best known heuristic are not within any constant factor of the optimal. The authors suggest using the Greedy Algorithm algorithm (see Subsection 2.3.1) for the min-max problem, starting from the observation that the min-max objective function upperbounds the min-sum objective function.
Reference: [48] <author> Y. Linde, A. Buzo, and R. M. Gray, </author> <title> An algorithm for vector quantiser design, </title> <journal> IEEE Transactions on Communications, </journal> <volume> COM-28 (1980), </volume> <pages> 84-95. </pages>
Reference-contexts: D (q) = E [d (X; q (X)]: The dissimilarity distance d (X; q (X)) is usually defined as the square error d (X; q (X)) = i=1 although other distances such as the Euclidian or the L 1 distance have also been considered <ref> [48] </ref>. When the distribution of X is known, D (q) can be computed as D (q) = i=1 Many times, however, nothing is known about the distribution law of the random variable X.
Reference: [49] <author> R. Lupton, F. M. Maley, and N. Young, </author> <title> Data collection for the sloan digital sky survey: A network-flow heuristic, </title> <booktitle> Proceedings of the Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <address> ACM/SIAM, </address> <month> January </month> <year> 1996, </year> <pages> pp. 296-303. </pages>
Reference-contexts: think that some of the geometric approximation algorithms could also be used in this context (for example the Greedy Algorithm). 2.5.5 Other Applications A recent project of the Astrophysical Research Consortium and the Sloan Foundation is aimed to provide a much better map of the universe than is currently available <ref> [49] </ref>. A telescope containing 660 optical fibers will take a series of "snapshots" from different regions of the sky. The maximum number of galaxies for which data can be gathered in a single snapshot is 660. <p> This is the capacitated clustering problem, for which the best known approximation algorithm by Bar-Ilan et al. achieves an approximation factor of log n. In this case, however, n is too large and the algorithm is not practical. The approach in <ref> [49] </ref> is heuristical, and it takes into account the fact that the points are distributed densely and somewhat uniformly in the plane. The plane is first covered by disks, so that the cover has (almost) minimum density.
Reference: [50] <author> J. MacQueen, </author> <title> Some methods for classification and analysis of multi-variate observations, </title> <booktitle> Proceedings of the Fifth Berkeley Symposium on Math., Stat. and Prob., </booktitle> <year> 1967, </year> <pages> pp. 281-296. </pages>
Reference-contexts: of the approximation factor), but their low time complexity makes them the 17 primary choice in most of the situations we encountered. 2.5.1 Data Compression Applications The min-sum center clustering has one important application in vector quantization, a method used for compression of digitized data such as images or speech <ref> [51, 50] </ref>. The problem consists in coding a continuous-value random vector by a vector from a finite set of code vectors.
Reference: [51] <author> J. Makhoul, S. Roucos, and H. Gish, </author> <title> Vector quantization in speech coding, </title> <booktitle> Proceedings of the IEEE, 73 (1985), </booktitle> <pages> 1551-1588. </pages>
Reference-contexts: of the approximation factor), but their low time complexity makes them the 17 primary choice in most of the situations we encountered. 2.5.1 Data Compression Applications The min-sum center clustering has one important application in vector quantization, a method used for compression of digitized data such as images or speech <ref> [51, 50] </ref>. The problem consists in coding a continuous-value random vector by a vector from a finite set of code vectors.
Reference: [52] <author> C. Meghini, </author> <title> An image retrieval model based on classical logic, </title> <booktitle> Proceedings of the Eighteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1995, </year> <pages> pp. 300-308. </pages>
Reference-contexts: Although in most of the cases it is considered that the collection consists of text documents, recent trends try to address the problem of multimedia collections <ref> [25, 52, 31] </ref>. Document clustering has been used in connection with a number of applications, such as query methods, browsing methods, text categorization, and hypertext analysis.
Reference: [53] <author> N. Megiddo, </author> <title> Linear-time algorithms for linear programming in R 3 and related problems, </title> <journal> SIAM J. Comput., </journal> <volume> 12 (1983), </volume> <pages> 759-776. </pages>
Reference-contexts: For the planar case, Drezner [17] gave an algorithm that runs in O (n 2k+1 log n) time, and can be improved to O (n 2k1 log n) by combining it with the result that the Euclidian 1-center problem can be solved in O (n) time <ref> [53, 21] </ref>.
Reference: [54] <author> N. Megiddo and K. J. Supowit, </author> <title> On the complexity of some common geometric location problems, </title> <journal> SIAM J. Comput., </journal> <volume> 13 (1984), </volume> <pages> 182-196. </pages>
Reference-contexts: That is why heuristics are widely used in applications, although they can have significant drawbacks. 2.1 Complexity Results In one dimension, pairwise and central clustering can be solved in polynomial time by dynamic programming [10]. In two dimensions, central clustering is NP-complete <ref> [26, 54] </ref>, even when an approximate solution is sought [24, 29, 46]. For metrics satisfying the triangle inequality, pairwise clustering reduces to central clustering, and is thus also NP-complete. <p> There are a number of papers investigating the reduction of various NP-complete problems to (P3), whose main aim is in fact to improve the best known bounds for the approximation factor of 5 the optimal cluster size. Megiddo and Supowit <ref> [54] </ref> showed that even if we allow the cluster size to be within a factor ff of the optimal size, central L 2 clustering remains NP-hard for ff &lt; 1:154, and L 1 clustering remains NP-hard for ff &lt; 1:5. They also proved that k-median is NP-hard. <p> In the case of a generic metric satisfying the triangle inequality a bound of 3 was previously proven by H.Karloff (communicated in [36]). We present here the main ideas of the proof by Megiddo and Supowit <ref> [54] </ref> for rectiliniar k-center (the proof for Euclidian k-center is quite similar). <p> This is the min-sum problem for S and k with the square-error distance function. Adapting the argument of [29], Lin and Storer [47] proved that this problem is NP-complete. The problem was already known to be NP-complete for the Euclidian and the rectiliniar metrics <ref> [54] </ref>, when d 2. We are not aware of any approximation algorithm for the min-sum problem with the square-error metric. The current approach to solving this problem is by using heuristics (see Section 2.6).
Reference: [55] <author> N. Megiddo and E. Zemel, </author> <title> A randomized O(n log n) algorithm for the weighted Euclidean 1-center problem, </title> <journal> J. Algorithms, </journal> <volume> 7 (1986), </volume> <pages> 358-368. </pages>
Reference-contexts: There are also various extensions to this problem, such as the weighted 1-center problem, for which a randomized algorithm in time O (n log n) was proposed by Megiddo and Zemel <ref> [55] </ref>, and the smallest enclosing ellipsoid of S [11, 67]. Euclidian 2-center For the Euclidian 2-center problem, we want to cover S by two balls of same smallest radius.
Reference: [56] <author> R. T. Ng and J. Han, </author> <title> Efficient and Effective Clustering Methods for Spatial Data Mining, </title> <booktitle> Proceedings of the Twentieth International Conference on Very Large Databases, </booktitle> <year> 1994, </year> <pages> pp. 144-155. </pages>
Reference-contexts: See [3, 62] for more information on classification algorithms. Recently, the advent of spatial databases has triggered studies in using clustering algorithms on the spatial attributes of the data <ref> [56, 44] </ref>. In a spatial database, objects of more complex types are allowed, such as points, lines, polygons. The user may want to receive as answer to a query all objects that are spatially close to each other. <p> The user may want to receive as answer to a query all objects that are spatially close to each other. The answer can be farther used for a spatial join with different maps (i.e. the answer is overlaid with maps depicting various information on the same terrain). <ref> [56] </ref> discusses two clustering algorithms that can be used on objects with both spatial and nonspatial attributes. The spatial attributes are considered to be the x and y coordinates (each object is a point in space), and the metric is either the Euclidian or the L 1 metric.
Reference: [57] <author> D. Nussbaum, </author> <title> Rectiliniar p-piercing problems, </title> <booktitle> Proceedings of the Annual International Symposium on Symbolic and Algebraic Computation, </booktitle> <year> 1997. </year> <note> to appear. 55 </note>
Reference-contexts: Sharir and Welzl [65] showed that the rectiliniar k-center problem in the plane can be solved in O (n k4 log 5 n), for k 5. The result was recently improved to O (n k4 log n) by Nussbaum <ref> [57] </ref>.
Reference: [58] <author> J. Plesnik, </author> <title> A heuristic for the p-center problem in graphs, </title> <journal> Discr. Appl. Math., </journal> <volume> 17 (1987), </volume> <pages> 263-268. </pages>
Reference-contexts: Plesnik <ref> [58] </ref> adapted the algorithm by Hochbaum and Shmoys [35, 36] to get an approximation factor of at most 2 for the same problem, defined in the context of weighted graphs.
Reference: [59] <author> F. P. Preparata and M. I. Shamos, </author> <title> Computational Geometry: An Introduction, </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference: [60] <author> P. Raghavan, </author> <title> Information retrieval algorithms: A survey, </title> <booktitle> Proceedings of the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <month> January </month> <year> 1997, </year> <pages> pp. 11-18. </pages>
Reference-contexts: Document clustering has been used in order to reduce the query time, by matching the query against the clusters, rather than all the documents in the corpus. The intuition is that closely related documents 19 will be relevant to the same queries (see <ref> [60] </ref> for a comprehensive survey). When a browsing method is used, the system provides some general information about the corpus, organized in a small number of topics that are considered representative for the entire collection.
Reference: [61] <author> P. Schroeter and J. Bigun, </author> <title> Hierarchical image segmentation by multidimensional clustering and orientation-adaptive boundary refinement, </title> <journal> Pattern Recognition., </journal> <volume> 28 (1995), </volume> <pages> 695-709. </pages>
Reference-contexts: The problem becomes more complex, due to the fact that clusters of the feature vectors are not necessarily connected in the xy plane. Various methods for finding an initial clustering and refining it to observe the spatial constraints have 23 been studied <ref> [13, 43, 66, 61] </ref>. Jain and Dubes [41] discuss three types of images for which segmentation algorithms are employed, each of which determines a distinct feature vector. In the case of textured images, the vector of a pixel must reflect textural qualities such as coarseness or regularity.
Reference: [62] <author> J. Shafer, R. Agrawal, and M. Mehta, Sprint: </author> <title> A scalable parallel classifier for data mining., </title> <booktitle> Proceedings of the International Conference on Very Large Databases, </booktitle> <publisher> Morgan Kauffman, </publisher> <year> 1996. </year>
Reference-contexts: After that, classification algorithms are employed in order to find a classification function for each group, that would allow efficient retrieval of all objects in the database that belong to that group. See <ref> [3, 62] </ref> for more information on classification algorithms. Recently, the advent of spatial databases has triggered studies in using clustering algorithms on the spatial attributes of the data [56, 44]. In a spatial database, objects of more complex types are allowed, such as points, lines, polygons.
Reference: [63] <author> M. Sharir, </author> <title> A near-linear algorithm for the planar 2-center problem, </title> <booktitle> Proc. 12th Annu. ACM Sympos. </booktitle> <institution> Comput. Geom., </institution> <year> 1996, </year> <pages> pp. 106-112. </pages>
Reference-contexts: The running time of the decision procedure was later improved to O (n 2 ) [33], and the result was used to obtain a running time 16 of O (n 2 log n) for the optimization problem [42]. This result was recently improved by Sharir <ref> [63] </ref>, who gave an O (n log 9 n) deterministic algorithm. Using randomization, Eppstein [23] has proposed an algorithm with expected running time O (n log 2 n). A recent result by Agarwal et al. gives an O (n 4=3 log 5 n) algorithm for the discrete Euclidian 2-center problem.
Reference: [64] <author> M. Sharir and E. Welzl, </author> <title> A combinatorial bound for linear programming and related problems, </title> <journal> Proc. 9th Sympos. Theoret. Aspects Comput. Sci., Lecture Notes Comput. Sci., </journal> <volume> Vol. 577, </volume> <publisher> Springer-Verlag, </publisher> <year> 1992, </year> <pages> pp. 569-579. </pages>
Reference-contexts: We review here the most important results, and refer the reader to [2] for a more comprehensive survey. Euclidian 1-center Given the input set S, we want to compute the ball of smallest radius that covers S. This is an LP-type problem of combinatorial dimension d+1 <ref> [64, 67] </ref>, and can be solved in expected running time O (d 2 n)+ 2 O ( d log d) [28].
Reference: [65] <author> M. Sharir and E. Welzl, </author> <title> Rectilinear and polygonal p-piercing and p-center problems, </title> <booktitle> Proc. 12th Annu. ACM Sympos. </booktitle> <institution> Comput. Geom., </institution> <year> 1996, </year> <pages> pp. 122-132. </pages>
Reference-contexts: A different algorithm that also runs in n O ( p k) time was given in [38], as an application of a general method called the search over separators strategy. The method also applies to the Euclidian discrete k-center problem, and the Euclidian discrete k-median problem. Sharir and Welzl <ref> [65] </ref> showed that the rectiliniar k-center problem in the plane can be solved in O (n k4 log 5 n), for k 5. The result was recently improved to O (n k4 log n) by Nussbaum [57]. <p> Rectiliniar k-center For k = 1, the problem is trivially solved in linear time. For the planar case, linear algorithms are known for k = 2 [19], and for k = 3 <ref> [65] </ref>. An O (n log n) algorithm is known for k = 4, and an O (n log 5 n) algorithm was proposed for k = 5 [65]. <p> For the planar case, linear algorithms are known for k = 2 [19], and for k = 3 <ref> [65] </ref>. An O (n log n) algorithm is known for k = 4, and an O (n log 5 n) algorithm was proposed for k = 5 [65]. For the rectiliniar 2-center weighted problem, Ko and Lee gave an O (n log n) algorithm in the planar case [45]. 2.5 Applications Clustering algorithms have been employed in many fields of human knowledge that require finding a "natural association" among some specific data.
Reference: [66] <author> M. Spann and R. Wilson, </author> <title> Quadtree approach to image segmentation which statistical and spatial information, </title> <journal> Pattern Recognition., </journal> <volume> 18 (1985), </volume> <pages> 257-269. </pages>
Reference-contexts: The problem becomes more complex, due to the fact that clusters of the feature vectors are not necessarily connected in the xy plane. Various methods for finding an initial clustering and refining it to observe the spatial constraints have 23 been studied <ref> [13, 43, 66, 61] </ref>. Jain and Dubes [41] discuss three types of images for which segmentation algorithms are employed, each of which determines a distinct feature vector. In the case of textured images, the vector of a pixel must reflect textural qualities such as coarseness or regularity.
Reference: [67] <author> E. Welzl, </author> <title> Smallest enclosing disks, balls and ellipsoids, </title> <type> Report B 91-09, </type> <institution> Fachbereich Mathematik, Freie Universitat Berlin, </institution> <address> Berlin, Germany, </address> <year> 1991. </year>
Reference-contexts: We review here the most important results, and refer the reader to [2] for a more comprehensive survey. Euclidian 1-center Given the input set S, we want to compute the ball of smallest radius that covers S. This is an LP-type problem of combinatorial dimension d+1 <ref> [64, 67] </ref>, and can be solved in expected running time O (d 2 n)+ 2 O ( d log d) [28]. <p> There are also various extensions to this problem, such as the weighted 1-center problem, for which a randomized algorithm in time O (n log n) was proposed by Megiddo and Zemel [55], and the smallest enclosing ellipsoid of S <ref> [11, 67] </ref>. Euclidian 2-center For the Euclidian 2-center problem, we want to cover S by two balls of same smallest radius. An O (n d+1 ) algorithm exists for the d-dimensional case [18] (using the fact that there exists a hyperplane separating the two clusters).
References-found: 67

