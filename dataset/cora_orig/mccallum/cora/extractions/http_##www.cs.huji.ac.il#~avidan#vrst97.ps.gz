URL: http://www.cs.huji.ac.il/~avidan/vrst97.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~avidan/index.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Image-Based View Synthesis by Combining Trilinear Tensors and Learning Techniques  
Author: S. Avidan T. Evgeniou A. Shashua T. Poggio 
Address: Jerusalem 91904 Cambridge, MA 02139 Israel USA  
Affiliation: Hebrew University Artificial Intelligence Laboratory Institute of Computer Science M.I.T  
Abstract: We present a new method for rendering novel images of flexible 3D objects from a small number of example images in correspondence. The strength of the method is the ability to synthesize images whose viewing position is significantly far away from the viewing cone of the example images (view extrapolation), yet without ever modeling the 3D structure of the scene. The method relies on synthesizing a chain of trilinear tensors that governs the warping function from the example images to the novel image, together with a multi-dimensional interpolation function that synthesizes the non-rigid motions of the viewed object from the virtual camera position. We show that two closely spaced example images alone are sufficient in practice to synthesize a significant viewing cone, thus demonstrating the ability of representing an object by a relatively small number of model images for the purpose of cheap and fast viewers that can run on standard hardware. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adelson, E.H. and J.R. </author> <title> Bergen The Plenoptic Function and the Elements of Early Vision In Computational Models of Visual Processing, Chapter 1, </title> <editor> Edited by Michael Landy and J. Anthony Movshon. </editor> <publisher> The MIT Press, </publisher> <address> Cambridgem, Mass. </address> <year> 1991. </year>
Reference-contexts: However, interpolation may produce physically-invalid images. Seitz and Dyer [20] proposed a physically-valid view interpolation method. The method involves recovering the epipolar geometry between the two acquired images and having interpolation done along the rectified epipolar lines. Interpolation can also be performed directly on the plenoptic function <ref> [1] </ref> which represents the amount of light emitted at each point in space as a function of direction. Levoy et al. [15] and Gortler et al. [10] interpolate between a dense set of several thousands of example images to reconstruct a reduced plenoptic function (under an occlusion-free world assumption).
Reference: [2] <author> E.B. Barrett, P.M. Payton, and G. Gheen. </author> <title> Robust algebraic invariant methods with applications in geometry and imaging. </title> <booktitle> In Proceedings of the SPIE on Remote Sensing, </booktitle> <address> San Diego, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: There are alternative ways of performing reprojection, but if we would like to do it without recovering first a 3D model of the scene, the trilinear tensor generally provides the best results since it is free from singular configurations (see <ref> [2, 21, 23] </ref>). In image-based rendering we would like to obtain the tensor via user specification of the location of a virtual camera, rather than by the specification of (at least) seven matching points.
Reference: [3] <author> J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hin-gorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In Proceedings of the European Conference on Computer Vision, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1992. </year>
Reference: [4] <author> D. Beymer, A. Shashua, and T. Poggio. </author> <title> Example based image analysis and synthesis. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1431, </pages> <institution> Artificial Intelligence Laboratory, Mas-sachusetts Institute of Technology, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: This is the basis for the QuickTimeVR system [7]. The fixed position constraint can be relaxed by computing the optical flow between the example images and using it to interpolate between the cylinders constructed at different locations (cf. <ref> [4, 5, 6] </ref>) (originally proposed 1 for views, not mosaics, but the principle is the same). However, interpolation may produce physically-invalid images. Seitz and Dyer [20] proposed a physically-valid view interpolation method. <p> This simple technique can be used to control several non-rigid degrees of freedom such as facial expressions, as shown by <ref> [4, 5] </ref>. It can be combined directly with the algebraic technique described earlier to control the position of the virtual camera. That is, we first perform Using the four example images on the left (framed) we generate new images as shown on the right.
Reference: [5] <author> D. Beymer, and T. Poggio. </author> <title> Image Representations for Visual Learning. </title> <journal> Science, </journal> <volume> 272, </volume> <month> pages </month> <year> 1905 1909, 1996. </year>
Reference-contexts: This is the basis for the QuickTimeVR system [7]. The fixed position constraint can be relaxed by computing the optical flow between the example images and using it to interpolate between the cylinders constructed at different locations (cf. <ref> [4, 5, 6] </ref>) (originally proposed 1 for views, not mosaics, but the principle is the same). However, interpolation may produce physically-invalid images. Seitz and Dyer [20] proposed a physically-valid view interpolation method. <p> This idea of generating virtual views of an object by using class-specific knowledge has been discussed before (see references in <ref> [5] </ref>). Suppose that we have two views Img ref and Img p of the prototype. We take Img ref to appear in the same pose as Img nov . Img p is a slightly transformed (i.e., rotated) view of Img ref (see diagram in Fig. 4). <p> A multidimensional interpolation technique such as Radial Basis Functions or splines is then used to interpolate the n example pairs (r i ; (S i ; T i )) (see for instance <ref> [5] </ref>). <p> This simple technique can be used to control several non-rigid degrees of freedom such as facial expressions, as shown by <ref> [4, 5] </ref>. It can be combined directly with the algebraic technique described earlier to control the position of the virtual camera. That is, we first perform Using the four example images on the left (framed) we generate new images as shown on the right.
Reference: [6] <author> S.E. Chen and L. Williams. </author> <title> View interpolation for image synthesis. </title> <booktitle> In SIGGRAPH, </booktitle> <pages> pages 279288, </pages> <address> Anahiem, CA, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: This is the basis for the QuickTimeVR system [7]. The fixed position constraint can be relaxed by computing the optical flow between the example images and using it to interpolate between the cylinders constructed at different locations (cf. <ref> [4, 5, 6] </ref>) (originally proposed 1 for views, not mosaics, but the principle is the same). However, interpolation may produce physically-invalid images. Seitz and Dyer [20] proposed a physically-valid view interpolation method.
Reference: [7] <author> Shenchang Eric Chen. </author> <title> QuickTimeVR an image-based approach to virtual environment navigation. </title> <booktitle> In SIGGRAPH, </booktitle> <pages> pages 2938, </pages> <year> 1995. </year>
Reference-contexts: The mosaic is mapped to a virtual cylinder that allows the user to look continuously at all directions but not to move. This is the basis for the QuickTimeVR system <ref> [7] </ref>. The fixed position constraint can be relaxed by computing the optical flow between the example images and using it to interpolate between the cylinders constructed at different locations (cf. [4, 5, 6]) (originally proposed 1 for views, not mosaics, but the principle is the same).
Reference: [8] <author> O.D. Faugeras. </author> <title> What can be seen in three dimensions with an uncalibrated stereo rig? In Proceedings of the European Conference on Computer Vision, </title> <booktitle> pages 563578, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: We next consider how to obtain the seed tensor that starts the process. 2.3 The Seed Tensor of Two Views Given two acquired images we can construct a special tensor composed of the elements of the fundamental matrix <ref> [8] </ref> that can serve as a seed tensor that starts the chain of tensors, as follows.
Reference: [9] <author> O.D. Faugeras and B. Mourrain. </author> <title> On the geometry and algebra of the point and line correspondences between N images. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: In space, this constraint is an intersection between a ray and two planes. became prominent in [21] and the underlying theory has been studied intensively in <ref> [24, 12, 9, 25, 13, 22] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
Reference: [10] <author> Gortler, S.J., Grzeszczuk, R., Szeliski, R. and Co--hen, M. </author> <booktitle> The Lumigraph In SIGGRAPH, </booktitle> <pages> pages 43 - 54, </pages> <year> 1996. </year>
Reference-contexts: Interpolation can also be performed directly on the plenoptic function [1] which represents the amount of light emitted at each point in space as a function of direction. Levoy et al. [15] and Gortler et al. <ref> [10] </ref> interpolate between a dense set of several thousands of example images to reconstruct a reduced plenoptic function (under an occlusion-free world assumption). They considerably increase the number of example images to avoid computing optical flow between the model images.
Reference: [11] <author> W.E.L. </author> <title> Grimson. Why stereo vision is not always about 3D reconstruction. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1435, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Depth maps are easily provided for synthetic environments, whereas for real scenes the process is fragile especially under small base-line situations that arise due to the requirement of dense correspondence between the model images/mosaics <ref> [11] </ref>. In this paper we propose a new view-synthesis method that makes use of the recent development of multi-linear matching constraints, known as trilinearities, that were first introduced in [21].
Reference: [12] <author> R. </author> <title> Hartley. A linear method for reconstruction from lines and points. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 882 887, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: In space, this constraint is an intersection between a ray and two planes. became prominent in [21] and the underlying theory has been studied intensively in <ref> [24, 12, 9, 25, 13, 22] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
Reference: [13] <author> A. </author> <title> Heyden. Reconstruction from image sequences by means of relative depths. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 10581063, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: In space, this constraint is an intersection between a ray and two planes. became prominent in [21] and the underlying theory has been studied intensively in <ref> [24, 12, 9, 25, 13, 22] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
Reference: [14] <author> S. Laveau and O.D. Faugeras. </author> <title> 3-d scene representation as a collection of images. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <pages> pages 689691, </pages> <address> Jerusalem, Israel, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: The alternative approach, along the lines of this paper, is to reduce the number of acquired (model) images by exploiting the 3D-from-2D geometry of the problem with the aid of corresponding points between the model images. Laveau and Faugeras <ref> [14] </ref> were the first to use the epipolar constraint for view synthesis, allowing them to extrapolate, as well as interpolate, between the example images. <p> to singularities that arise under certain camera motions (like when the virtual camera center is collinear with the centers of the model cameras) and the relation between translational and rotational parameters of the virtual camera and the epipolar constraint is somewhat indirect and hence requires the specification of matching points <ref> [14] </ref>. The singular camera motions can be relaxed by using the depth map of the environment.
Reference: [15] <author> M. Levoy and P. Hanrahan. </author> <title> Light field rendering. </title> <booktitle> In SIGGRAPH, </booktitle> <pages> pages 3142, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Interpolation can also be performed directly on the plenoptic function [1] which represents the amount of light emitted at each point in space as a function of direction. Levoy et al. <ref> [15] </ref> and Gortler et al. [10] interpolate between a dense set of several thousands of example images to reconstruct a reduced plenoptic function (under an occlusion-free world assumption). They considerably increase the number of example images to avoid computing optical flow between the model images.
Reference: [16] <author> B.D. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Proceedings IJCAI, </booktitle> <pages> pages 674679, </pages> <address> Van-couver, Canada, </address> <year> 1981. </year>
Reference: [17] <author> Leonard McMillan and Gary Bishop. </author> <title> Plenoptic modeling: An image-based rendering system. </title> <booktitle> In SIGGRAPH, </booktitle> <pages> pages 3946, </pages> <year> 1995. </year>
Reference-contexts: The singular camera motions can be relaxed by using the depth map of the environment. McMillan and Bishop <ref> [17] </ref> use a full depth map (3D reconstruction of the camera motion and the environment) together with the epipolar constraint to provide a direct connection between the virtual camera motion and the re-projection engine.
Reference: [18] <author> Torr P.H.S., Zisserman A., and Murray D. </author> <title> Motion clustering using the trilinear constraint over three views. In Workshop on Geometrical Modeling and Invariants for Computer Vision. </title> <publisher> Xidian University Press., </publisher> <year> 1995. </year>
Reference: [19] <author> B. Rousso, S. Avidan, A. Shashua, and S. Pe-leg. </author> <title> Robust recovery of camera rotation from three frames. </title> <booktitle> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> San-Francisco, CA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: The matrix a j i representing the rotational component of camera motion between the two model views 1,2 can be rep resented in closed form as a function of the tensor ff jk described in <ref> [19] </ref>: X = det @ j3 ff 2 + ff 3 j3 j2 1 Y = det @ j3 ff 2 + ff 3 j3 j2 1 Z = det @ j2 ff 2 + ff 3 j3 j2 1 K = det @ j2 ff 2 + ff 3 j3
Reference: [20] <author> Steven M. Seitz, Charles R. Dyers. </author> <title> View morphing. </title> <booktitle> In SIGGRAPH, </booktitle> <pages> pages 2130, </pages> <year> 1996. </year>
Reference-contexts: However, interpolation may produce physically-invalid images. Seitz and Dyer <ref> [20] </ref> proposed a physically-valid view interpolation method. The method involves recovering the epipolar geometry between the two acquired images and having interpolation done along the rectified epipolar lines. <p> Methods that estimate 3D struc ture are very noisy with small baselines. Although our technique implicitly estimates the 3D structure, not doing so in an explicit way means avoiding noisy steps and therefore generating less noisy virtual images. Moreover, morphing techniques, such as <ref> [20] </ref>, require large baselines since they cannot perform extrapolation.
Reference: [21] <author> A. Shashua. </author> <title> Algebraic functions for recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(8):779789, </volume> <year> 1995. </year>
Reference-contexts: In this paper we propose a new view-synthesis method that makes use of the recent development of multi-linear matching constraints, known as trilinearities, that were first introduced in <ref> [21] </ref>. The trilinearities provide a general (not subject to singular camera configurations) warping function from model images to novel synthesized images governed directly by the camera parameters of the virtual camera. <p> In space, this constraint is an intersection between a ray and two planes. became prominent in <ref> [21] </ref> and the underlying theory has been studied intensively in [24, 12, 9, 25, 13, 22]. <p> There are alternative ways of performing reprojection, but if we would like to do it without recovering first a 3D model of the scene, the trilinear tensor generally provides the best results since it is free from singular configurations (see <ref> [2, 21, 23] </ref>). In image-based rendering we would like to obtain the tensor via user specification of the location of a virtual camera, rather than by the specification of (at least) seven matching points.
Reference: [22] <author> A. Shashua and S. Avidan. </author> <title> The rank4 constraint in multiple view geometry. </title> <booktitle> In Proceedings of the Euro-pean Conference on Computer Vision, </booktitle> <address> Cambridge, UK, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: In space, this constraint is an intersection between a ray and two planes. became prominent in [21] and the underlying theory has been studied intensively in <ref> [24, 12, 9, 25, 13, 22] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
Reference: [23] <author> A. Shashua and S.J. Maybank. </author> <title> Degenerate n point configurations of three views: </title> <type> Do critical surfaces exist? Technical Report TR 96-19, </type> <institution> Hebrew University of Jerusalem, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: There are alternative ways of performing reprojection, but if we would like to do it without recovering first a 3D model of the scene, the trilinear tensor generally provides the best results since it is free from singular configurations (see <ref> [2, 21, 23] </ref>). In image-based rendering we would like to obtain the tensor via user specification of the location of a virtual camera, rather than by the specification of (at least) seven matching points.
Reference: [24] <author> A. Shashua and M. Werman. </author> <title> On the trilinear tensor of three perspective views and its underlying geometry. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: In space, this constraint is an intersection between a ray and two planes. became prominent in [21] and the underlying theory has been studied intensively in <ref> [24, 12, 9, 25, 13, 22] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
Reference: [25] <author> B. Triggs. </author> <title> Matching constraints and the joint image. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 338343, </pages> <address> Cambridge, MA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: In space, this constraint is an intersection between a ray and two planes. became prominent in [21] and the underlying theory has been studied intensively in <ref> [24, 12, 9, 25, 13, 22] </ref>. One can readily see that given two views in full correspondence and the tensor (recovered using 7 matching points with a third view), the entire third view can be synthesized by means of forward warping.
References-found: 25

