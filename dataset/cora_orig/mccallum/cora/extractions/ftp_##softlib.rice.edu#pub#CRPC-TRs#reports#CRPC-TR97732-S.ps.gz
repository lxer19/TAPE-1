URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR97732-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Array Combining Scatter Functions on Coarse-Grained, Distributed-Memory Parallel Machines  Parallel Programming  
Author: Seungjo Bae, Khaled A. Alsabti, ; and Sanjay Ranka 
Address: Yusong P.O. Box 106, Taejon 305-600, Korea  Building, #301  Gainesville, FL 32611  
Affiliation: Computer System Department, Computer Technology Division Electronics and Telecommunications Research Institute  CSE  Department of Computer Science University of Florida  Syracuse University.  
Note: Corresponding author  1 He is currently visiting from  
Pubnum: Section  
Email: Email: sbae@computer.etri.re.kr  Email: kalsabti, ranka@cis.ufl.edu  
Phone: Phone: +82-42-860-5667  Phone: (352) 392-1526  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Seungjo Bae. </author> <title> Runtime Support for Unstructured Data Accesses on Coarse-Grained Distributed-Memory Parallel Machines. </title> <type> PhD thesis, </type> <institution> Syracuse University, </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: Now we define the communication primitive, prefix-reduction-sum, which performs vector prefix-sum and reduction-sum simultaneously where the prefix operation is exclusive. 1 The time taken to perform the vector prefix-reduction-sum is modeled as t lg P + M where M is the vector size <ref> [1] </ref>. Readers are referred to [1] for detailed algorithms. Transportation The transportation is a communication primitive that performs many-to-many personalized communication with possibly high variance in message sizes (i.e., with highly non-uniform messages) [15]. <p> Now we define the communication primitive, prefix-reduction-sum, which performs vector prefix-sum and reduction-sum simultaneously where the prefix operation is exclusive. 1 The time taken to perform the vector prefix-reduction-sum is modeled as t lg P + M where M is the vector size <ref> [1] </ref>. Readers are referred to [1] for detailed algorithms. Transportation The transportation is a communication primitive that performs many-to-many personalized communication with possibly high variance in message sizes (i.e., with highly non-uniform messages) [15]. <p> However, all schemes presented in this paper can be easily extended to multidimensional arrays. For more details, readers are referred to <ref> [1, 2] </ref>. <p> On the other hand, in the two-stage algorithm it is O (3L (c p + c l + c b )). Therefore, we have more indexing overhead in the two-stage algorithm. For optimization schemes to decrease indexing overhead, readers are referred to <ref> [1] </ref>. 4.3 Modeling In this section we present modeling for two schemes, NLC-OSC and NLC-TSC, and then compare the two schemes based on our modeling. <p> The criteria should be based on ff (the maximum number of writes among all target processor). Readers are referred to <ref> [1] </ref> for more details. 5 Local Combining Schemes In the direct algorithm the presence of hot processors, due to the variance in incoming traffic on target processors, degrades the performance of the last two steps, including the transportation between the source and target processors and message decomposition on target processors.
Reference: [2] <author> Seungjo Bae, Khaled A. Alsabti, and Sanjay Ranka. </author> <title> Performance Evaluation of Random Access Read/Write on Coarse-Grained Distributed Memory Parallel Machines. </title> <type> Technical report, </type> <institution> School of Computer and Information Science, Syracuse University, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: However, all schemes presented in this paper can be easily extended to multidimensional arrays. For more details, readers are referred to <ref> [1, 2] </ref>. <p> For succinctness, only partial results are presented in Figures 8 through 10. Full results are available in <ref> [2] </ref>. For bucket-based algorithms, only the best results with optimal K values are shown. Although the optimal K values are not shown in figures, our experimental results showed that they are 1,2, or 4 in most cases.
Reference: [3] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Also, incoming traffic on target processors is bounded by L. In Stage II we deal with dense buckets. After identifying which buckets are located on each intermediate processor, we resolve collision by performing direct address hashing in Step 6 <ref> [3] </ref>. To do so we first assign a hash table of size L 0 =K to each bucket. The number of elements of a dense bucket is greater than L=K. Hence, on each intermediate processor, there exists at most K 1 full buckets and two partial buckets. <p> Later we will explain how all the local combining schemes can be applied to the two-stage algorithm. 5.1 Whole Local Combining (WLC) To perform collision resolution locally we can perform hashing during the first scan. The two hashing techniques, open address hashing and direct address hashing, can be used <ref> [3] </ref>. Whole Local Combining Using Open Address Hashing We perform open address hashing during the first scan [3]. In open address hashing each element of Il 0 is used as a key. <p> The two hashing techniques, open address hashing and direct address hashing, can be used <ref> [3] </ref>. Whole Local Combining Using Open Address Hashing We perform open address hashing during the first scan [3]. In open address hashing each element of Il 0 is used as a key. <p> The element of a hash table is a global index, Il 0 (i) (hashing key), with a datum (satellite information). Whole Local Combining Using Direct Address Hashing The whole local combining scheme using open address hashing entails a significant hashing overhead. To reduce such an overhead, direct address hashing <ref> [3] </ref> can be used instead of open address hashing. We can use a hash table of size N 0 where each entry is a datum. Note that we do not need to store any index used as a hashing key. <p> However, it should be noted that Lcount (i) N 0 =KP does not necessarily mean that there are no hot pointers. For each locally dense pointer-set P S i we first reset Lcount (i) to N 0 =KP and then perform direct address hashing <ref> [3] </ref> during message composition. Note that the size of a hash table is N 0 =P K and each entry of a hash table is a local index (hashing key), valid on a target processor with a datum. Whenever a collision occurs, we need only to combine two colliding elements.
Reference: [4] <institution> Thinking Machines Corporation. </institution> <note> CMMD version 3.0 Reference manual, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: For the transportation primitive, linear permutation using active messages and a randomized distributed algorithm using active messages were used in the direct algorithm and the two-stage algorithm, respectively [21, 22, 15]. Also, for the vector prefix-reduction sum, a CM-5 global function was used <ref> [4] </ref>. 6.1 Performance Evaluation of Ten Schemes In each category we used various hot degrees ffi = 0:0; 0:2; 0:4; 0:6; 0:8; 1:0 and fixed the value of H to 1 in order to compare the performances of ten different schemes for the array combining scatter functions.
Reference: [5] <author> C. H. Koelbel et al. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction In High Performance Fortran (HPF) array combining scatter functions are generalized array reduction functions. Elements in a subset of a source array are combined, and then the resulting value is sent to a position in a resulting array. At this time each subset is non-overlapping <ref> [5, 8] </ref>. <p> functions are defined in High Performance Fortran (HPF) and have the form XXX SCATTER (ARRAY, BASE, INDX 0 , : : : , INDX r 0 1 , MASK) where XXX is one of the reduction operations defined in HPF, and r 0 is the rank of a base array <ref> [5, 8] </ref>. Arguments are explained in Table 1. Each array combining scatter function returns a resulting array that is conformable with the base array and has the same type as the source array.
Reference: [6] <author> C. Leiserson et al. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proc. of 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> San Diego, CA, </address> <year> 1994. </year>
Reference-contexts: It follows that the underlying interconnection network can be regarded as a virtual crossbar network in the two-level model. These assumptions closely model the behavior of the CM-5 on which our experimental results are presented <ref> [6] </ref>. Although our algorithms are analyzed under these assumptions, most of them are architecture-independent and can be efficiently implemented on meshes and hypercubes with wormhole routing [6, 13]. <p> These assumptions closely model the behavior of the CM-5 on which our experimental results are presented [6]. Although our algorithms are analyzed under these assumptions, most of them are architecture-independent and can be efficiently implemented on meshes and hypercubes with wormhole routing <ref> [6, 13] </ref>.
Reference: [7] <editor> Tilak Agerwala et al. </editor> <title> IBM SP2 System Architecture. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 152-184, </pages> <year> 1995. </year> <month> 19 </month>
Reference-contexts: We expect this to be the case in other machines such as the IBM SP2 <ref> [7, 18] </ref> and the Intel Paragon [19]. 17 6.2 Experimental Threshold Hot Degree with H = 1; 2; 4 In the previous experiment we fixed H to one.
Reference: [8] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification: </title> <note> Version 1.1, Novem--ber 1994. </note>
Reference-contexts: 1 Introduction In High Performance Fortran (HPF) array combining scatter functions are generalized array reduction functions. Elements in a subset of a source array are combined, and then the resulting value is sent to a position in a resulting array. At this time each subset is non-overlapping <ref> [5, 8] </ref>. <p> functions are defined in High Performance Fortran (HPF) and have the form XXX SCATTER (ARRAY, BASE, INDX 0 , : : : , INDX r 0 1 , MASK) where XXX is one of the reduction operations defined in HPF, and r 0 is the rank of a base array <ref> [5, 8] </ref>. Arguments are explained in Table 1. Each array combining scatter function returns a resulting array that is conformable with the base array and has the same type as the source array.
Reference: [9] <author> V. Kumar, A. Grama, G. Karypis, and A. Gupta. </author> <title> Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1994. </year>
Reference-contexts: Under the assumption that there is no node contention, the time taken to send a message from one processor to another is modeled as t + m, where m is the size of the messages <ref> [9, 17] </ref>. 2.1 Communication Primitives Vector Reduction and Prefix Sum The vector reduction-sum computes an element-wise reduction sum of the local vector located in each processor, and the vector prefix-sum computes an element-wise exclusive prefix sum of the local vector located in each processor.
Reference: [10] <author> Wei-Keng Liao and Sanjay Ranka. </author> <title> Hough Transformation. </title> <type> Technical report, </type> <institution> School of Computer and Information Science, Syracuse University, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: On coarse-grained, distributed-memory parallel machines all elements are distributed among processors, thus the presence of hot spots may cause a high variance in incoming traffic on target processors. Such hot spots are inherent features of applications such as histogramming, geometric hashing [14], the Hough transformation <ref> [20, 10] </ref>, and database join [11]. In this paper we present two global combining algorithms for the array combining scatter functions on coarse-grained, distributed-memory parallel machines: direct (one-stage) algorithm and extended two-stage algorithm. 1 In the direct algorithm all source data are sent directly to target processors from source processors.
Reference: [11] <author> H. Lu, B. Ooi, and K. Tan. </author> <title> Query Processing in Parallel Relational Database Systems. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Such hot spots are inherent features of applications such as histogramming, geometric hashing [14], the Hough transformation [20, 10], and database join <ref> [11] </ref>. In this paper we present two global combining algorithms for the array combining scatter functions on coarse-grained, distributed-memory parallel machines: direct (one-stage) algorithm and extended two-stage algorithm. 1 In the direct algorithm all source data are sent directly to target processors from source processors.
Reference: [12] <author> D. Nassimi and S. Sahni. </author> <title> Data Broadcasting in SIMD Computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(2):101-107, </volume> <year> 1981. </year>
Reference-contexts: which has the same type as the source array INDX i Index array conformable with the source array where 0 i &lt; r 0 Table 1: Description of arguments used in array combining scatter functions The array combining scatter function can be defined in terms of random access write (RAW) <ref> [12] </ref>. In random access write (RAW) each element i (0 i &lt; N ) may need to write (or send) a datum, D (i), to another element [12]. <p> of arguments used in array combining scatter functions The array combining scatter function can be defined in terms of random access write (RAW) <ref> [12] </ref>. In random access write (RAW) each element i (0 i &lt; N ) may need to write (or send) a datum, D (i), to another element [12]. At this time I (i) indicates a pointer such that element i needs to write a datum, D (i), to an element, I (i). Thus parallel execution of the do-loop, do (i = 0 : n 1) A (I (i)) = D (i); results in RAW [16].
Reference: [13] <author> Lionel M. Ni and Philip K. McKinley. </author> <title> A survey of wormhole routing techniques in direct networks. </title> <journal> IEEE Computer, </journal> <volume> 26(2) </volume> <pages> 62-76, </pages> <year> 1993. </year>
Reference-contexts: These assumptions closely model the behavior of the CM-5 on which our experimental results are presented [6]. Although our algorithms are analyzed under these assumptions, most of them are architecture-independent and can be efficiently implemented on meshes and hypercubes with wormhole routing <ref> [6, 13] </ref>.
Reference: [14] <author> V. K. Prasanna and C. Wang. </author> <title> Scalable Data Parallel Object Recognition Using Geometric Hashing on the CM-5. </title> <booktitle> In Proc. of Scalable High Performance Computing Conference, </booktitle> <year> 1994. </year>
Reference-contexts: On coarse-grained, distributed-memory parallel machines all elements are distributed among processors, thus the presence of hot spots may cause a high variance in incoming traffic on target processors. Such hot spots are inherent features of applications such as histogramming, geometric hashing <ref> [14] </ref>, the Hough transformation [20, 10], and database join [11].
Reference: [15] <author> Ravi V. Shankar, Khaled A. Alsabti, and Sanjay Ranka. </author> <title> The transportation primitive. </title> <booktitle> In Proc. of Frontiers 95, </booktitle> <year> 1995. </year>
Reference-contexts: Readers are referred to [1] for detailed algorithms. Transportation The transportation is a communication primitive that performs many-to-many personalized communication with possibly high variance in message sizes (i.e., with highly non-uniform messages) <ref> [15] </ref>. The transportation primitive with P processors is represented by communication matrix, P fi P matrix = ( ij ) where each row index i and column index j ( 0 i; j &lt; P ) indicate a source processor and a target processor numbers, respectively. <p> A matrix entry ij gives the size of message sent by processor i to processor j. Also, each row and column are called send vector sendl and receive vector recvl <ref> [15] </ref>. <p> If the overall traffic bound is t, communication can be done in time O (2t) when t O (P 2 ) <ref> [15] </ref>. Generally, the transportation primitive requires a preliminary communication in order to exchange sendl vector, which can be done in time O (t P ). 1 If a control network is available, as on the CM-5, we do not need to combine reduction-sum with prefix-sum. <p> Also, for exact timing on the CM-5, processor 0 was always selected as a hot processor. For the transportation primitive, linear permutation using active messages and a randomized distributed algorithm using active messages were used in the direct algorithm and the two-stage algorithm, respectively <ref> [21, 22, 15] </ref>.
Reference: [16] <author> Ravi V. Shankar and Sanjay Ranka. </author> <title> Random Data Accesses on a Coarse-Grained Parallel Machine II. One-to-Many and Many-to-one Mappings. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 44(1) </volume> <pages> 24-34, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: At this time I (i) indicates a pointer such that element i needs to write a datum, D (i), to an element, I (i). Thus parallel execution of the do-loop, do (i = 0 : n 1) A (I (i)) = D (i); results in RAW <ref> [16] </ref>. Figure 1 shows an example of a RAW. Assume that two elements i and j (0 i; j &lt; N ) need to write the data, D (i) and D (j), to the same target element (i.e., I (i) = I (j)). <p> We use the term collision to refer to this situation. There are several methods that can be used to resolve the collision if the number of data elements causing a collision is bounded <ref> [16] </ref>. In this paper we assume collisions are resolved by using a predefined binary associative operator. In Figure 1 collisions are resolved using a binary associative operator +. <p> 3 4 5 6 7 Pointer array 7 7 1 1 2 6 3 3 After RAW - D (2) + D (3) D (4) D (6) + D (7) - D (5) D (0) + D (1) We call a target element where a collision occurs a hot spot <ref> [16] </ref>. In Figure 1 hot spots are elements 1, 3, and 7. On coarse-grained, distributed-memory parallel machines all elements are distributed among processors, thus the presence of hot spots may cause a high variance in incoming traffic on target processors. <p> However, in the two-stage algorithm they are sent via intermediate processors in order to prevent a high variance in incoming traffic due to the presence of hot spots <ref> [16] </ref>. Also, we present four local combining (collision resolution) schemes, whole local combining using open address hashing, whole local combining using direct address hashing, partial local combining without compression, and partial local combining with compression, to reduce outgoing traffic as well as variance in incoming traffic. <p> 0 ), respectively. 4 Note that in Step 2 we need to scan two arrays, A l and Il 0 , while we need to do only one array, Il 0 , in Step 1. 4.2 Two-Stage Algorithm First, we introduce Shankar and Ranka's two-stage algorithm for random access write <ref> [16] </ref> and then present our extended two-stage algorithm for the array combining scatter functions. 4.2.1 Two-Stage Algorithm for Random Access Write We assume all arrays of rank one have the same shape (N ) and are distributed using BLOCK distribution among P processors. <p> The basic idea is that, during RAW, these buckets are dynamically stretched or shrunk to reduce communication overhead. A high-level description of the algorithm is given in Figure 5. For more details, readers are referred to <ref> [16] </ref>. 1. Preprocessing of Stage I: In every source processor a bucket vector of size KP is created. <p> Thus our extension will deal with different values of N and N 0 , and with block-cyclically distributed base arrays. First, we assume that a hybrid algorithm <ref> [16] </ref> is used in our presentation. In the hybrid algorithm each bucket is classified as a dense or a sparse bucket, and then only dense buckets are processed through two-stage communication. For ease of presentation, the extended two-stage algorithm may be divided into eight steps, as presented in Figure 6.
Reference: [17] <author> Ravi V. Shankar and Sanjay Ranka. </author> <title> Random Data Accesses on a Coarse-Grained Parallel Machine I. One-to-One Mappings. </title> <journal> Journal of Parallel and Distributed, </journal> <volume> 44(1) </volume> <pages> 14-23, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: Under the assumption that there is no node contention, the time taken to send a message from one processor to another is modeled as t + m, where m is the size of the messages <ref> [9, 17] </ref>. 2.1 Communication Primitives Vector Reduction and Prefix Sum The vector reduction-sum computes an element-wise reduction sum of the local vector located in each processor, and the vector prefix-sum computes an element-wise exclusive prefix sum of the local vector located in each processor.
Reference: [18] <author> Marc Snir. </author> <title> The Communication Software and Parallel Environment of the IBM SP2. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 205-221, </pages> <year> 1995. </year>
Reference-contexts: We expect this to be the case in other machines such as the IBM SP2 <ref> [7, 18] </ref> and the Intel Paragon [19]. 17 6.2 Experimental Threshold Hot Degree with H = 1; 2; 4 In the previous experiment we fixed H to one.
Reference: [19] <institution> Intel Corporation Supercomputer System Division. Paragon XP/S Product Overview, </institution> <year> 1991. </year>
Reference-contexts: We expect this to be the case in other machines such as the IBM SP2 [7, 18] and the Intel Paragon <ref> [19] </ref>. 17 6.2 Experimental Threshold Hot Degree with H = 1; 2; 4 In the previous experiment we fixed H to one. By using two more values of H (2 and 4), we compared performances of two schemes, NLC-OSC and NLC-TSC, to evaluate the experimental threshold for hot degrees.
Reference: [20] <author> S. Tanimoto. </author> <title> The Elementary of Artificial Intelligence Using Common LISP. </title> <publisher> Freeman and Company, </publisher> <year> 1990. </year>
Reference-contexts: On coarse-grained, distributed-memory parallel machines all elements are distributed among processors, thus the presence of hot spots may cause a high variance in incoming traffic on target processors. Such hot spots are inherent features of applications such as histogramming, geometric hashing [14], the Hough transformation <ref> [20, 10] </ref>, and database join [11]. In this paper we present two global combining algorithms for the array combining scatter functions on coarse-grained, distributed-memory parallel machines: direct (one-stage) algorithm and extended two-stage algorithm. 1 In the direct algorithm all source data are sent directly to target processors from source processors.
Reference: [21] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proc. of ISCA 1992, </booktitle> <pages> pages 256-266, </pages> <year> 1992. </year>
Reference-contexts: Also, for exact timing on the CM-5, processor 0 was always selected as a hot processor. For the transportation primitive, linear permutation using active messages and a randomized distributed algorithm using active messages were used in the direct algorithm and the two-stage algorithm, respectively <ref> [21, 22, 15] </ref>.

References-found: 21

